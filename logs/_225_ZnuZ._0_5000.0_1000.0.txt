Wed Feb 1 13:37:09 EST 2023
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  0% |  0% |
numbers of X: 25508404
numbers of Z: 30856
shape of features
(30856,)
shape of features
(30856,)
ZX	Vol	Parts	Cubes	Eps
Z	0.02500837342801806	30856	30.856	0.09323563648458022
X	0.0221149764784922	426	0.426	0.3730430565651091
X	0.022494603292135686	10402	10.402	0.12931646662927795
X	0.023032874652100927	1632	1.632	0.24166253669661716
X	0.022403060511337638	4116	4.116	0.17590465345268358
X	0.023965703980088435	26597	26.597	0.09658711166222556
X	0.0224727966285248	15516	15.516	0.113142503349859
X	0.023110047689495095	40819	40.819	0.08272678757216159
X	0.022541988893152433	23492	23.492	0.09863341721088194
X	0.02248468094278019	6119	6.119	0.1543128499044501
X	0.022328369622812216	11487	11.487	0.12480063562134931
X	0.023022307453153812	2989	2.989	0.1974881211797118
X	0.02249345087785071	65541	65.541	0.07001337420695908
X	0.023149842289845346	4719	4.719	0.16991537818418093
X	0.023230562400166445	174814	174.814	0.05103026667937364
X	0.02318260510638831	17314	17.314	0.1102186129765539
X	0.02273756391658441	20943	20.943	0.10277835918249573
X	0.02328391721720506	44630	44.63	0.0805025653102671
X	0.022414795225303055	32400	32.4	0.08844295716329749
X	0.023298758241981174	114359	114.359	0.058842006723008086
X	0.023291785340535404	89756	89.756	0.06378419757714997
X	0.022684278059319597	6972	6.972	0.14817986492741328
X	0.023573848186902505	165897	165.897	0.052183193077447995
X	0.022472014713103963	8225	8.225	0.13979834647615985
X	0.023239826598746495	21683	21.683	0.10233821503811606
X	0.022425261362539646	8595	8.595	0.13766722050492827
X	0.023129667533002194	60189	60.189	0.0727027358803406
X	0.023279359183304208	42552	42.552	0.08178688757560346
X	0.022471363923338906	4344	4.344	0.17294706871179338
X	0.023024374306920677	45901	45.901	0.0794551215903879
X	0.024632808584621756	902247	902.247	0.03011129926110688
X	0.022189176967905767	4686	4.686	0.16792407700005724
X	0.023941728269718236	490510	490.51	0.036545665700664615
X	0.02240833605921438	12274	12.274	0.12221968592473294
X	0.022609685995596537	4818	4.818	0.16742073533337112
X	0.022190919830596623	4818	4.818	0.16638065752245262
X	0.022480393986189055	71820	71.82	0.0678973523232694
X	0.02402851961807079	65618	65.618	0.07154314187192777
X	0.021881787782342534	904	0.904	0.28927092906151447
X	0.023899851743535226	5496	5.496	0.16322355315525652
X	0.02172275355354492	1866	1.866	0.22663887821242967
X	0.02234783129322914	2462	2.462	0.20860070819774953
X	0.02083691558634417	759	0.759	0.3016689086508759
X	0.02046353801373	693	0.693	0.3090882502364078
X	0.022636805530733193	2901	2.901	0.19834555415740704
X	0.021276105704703396	338	0.338	0.39779423026202326
X	0.021809016747973448	524	0.524	0.34655188571153495
X	0.022490092994744817	2788	2.788	0.20055469221079902
X	0.02315475893803751	2001	2.001	0.22618530806330597
X	0.022534619426922673	1642	1.642	0.2394192274228931
X	0.02298325378208248	9230	9.23	0.1355407664642693
X	0.02261396333720793	8098	8.098	0.14082063675896656
X	0.02233614039943488	2301	2.301	0.21331947395817652
X	0.022313288745413606	3028	3.028	0.19459689433740793
X	0.02298896022451728	1989	1.989	0.22609702534993453
X	0.022519037776165672	4443	4.443	0.17177416208897595
X	0.022434942361676313	3084	3.084	0.19376270834119963
X	0.02227101175081122	1176	1.176	0.26654929719995546
X	0.022642636226469753	2614	2.614	0.20537163881539278
X	0.021833451284341495	1598	1.598	0.23906456350495484
X	0.022196393977681497	1592	1.592	0.24068357506579938
X	0.023227245204389846	5850	5.85	0.15834849724837277
X	0.021855604504677	947	0.947	0.28471102271832466
X	0.024193037230275105	11245	11.245	0.12909526439770644
X	0.022055715175548673	2906	2.906	0.1965207946536916
X	0.022363850796352166	1043	1.043	0.2778139776296989
X	0.02236603829321802	1540	1.54	0.24398106034742767
X	0.0222350946324838	708	0.708	0.31550184854905217
X	0.02174059189745082	1488	1.488	0.2444684454258182
X	0.022570626253552873	1807	1.807	0.23202163647559387
X	0.022327909565239206	1260	1.26	0.26071089054619434
X	0.02204860329156243	2700	2.7	0.2013751018664359
X	0.022026478608814785	2216	2.216	0.21500989076277705
X	0.02227064245501546	854	0.854	0.2965456958658186
X	0.022044331910763347	1619	1.619	0.2387903090049392
X	0.022995242398382215	2024	2.024	0.2248066537864537
X	0.02239775244243778	4695	4.695	0.1683408882982636
X	0.022286943322379467	709	0.709	0.31559837458568807
X	0.021989147187959607	362	0.362	0.3930971893240936
X	0.021942356260018836	902	0.902	0.2897514212204824
X	0.02393624755599134	5514	5.514	0.1631284721679722
X	0.022199673779366703	2084	2.084	0.22003053732448852
X	0.020731839864119934	395	0.395	0.3744095491239332
X	0.023527673533178556	4651	4.651	0.17166333620647162
X	0.02127302177465161	684	0.684	0.31447854947001663
X	0.021931532759324503	1339	1.339	0.2539585288649354
X	0.02122616628208724	502	0.502	0.34838219274577975
X	0.021900099866466256	2037	2.037	0.22070814235115027
X	0.02147589403132466	491	0.491	0.3523356488532968
X	0.023781666728901772	2781	2.781	0.2044939356148295
X	0.022457273235066726	2207	2.207	0.21669633358392065
X	0.0214105906619621	871	0.871	0.29076145347413057
X	0.02220400026396001	652	0.652	0.32413645350055287
X	0.0222177159802164	567	0.567	0.3396556035993349
X	0.022885990061937464	3399	3.399	0.18883080597368435
X	0.02233116437547682	2324	2.324	0.21259762808759672
X	0.022212445250719436	1837	1.837	0.22952451129005677
X	0.022228960282881068	2326	2.326	0.21221193880049358
X	0.022606190830269626	5989	5.989	0.15570086504415384
X	0.022435052837045943	5930	5.93	0.15582033526207542
X	0.02249304621730254	1376	1.376	0.25379123195754155
X	0.023046560319002603	10086	10.086	0.13171312409475996
X	0.02250636203585943	3077	3.077	0.19411507681717996
X	0.022293338671009084	2828	2.828	0.19902084669301445
X	0.02136730678011836	1025	1.025	0.27521703178483237
X	0.021630842783881984	2278	2.278	0.21175803925581146
X	0.022255941626295017	1449	1.449	0.24857600046459705
X	0.022371838078675068	4022	4.022	0.17718210909934817
X	0.022101130495517683	431	0.431	0.37151732657310116
X	0.022009457645809794	1376	1.376	0.2519592473263989
X	0.022129339098892697	827	0.827	0.299103143970737
X	0.022117695812869684	1307	1.307	0.25673678460738
X	0.021745162876049405	1157	1.157	0.26587434643476404
X	0.021456028946066553	899	0.899	0.28791428837555827
X	0.022383486754131293	2981	2.981	0.19581919204905562
X	0.02301736936511864	2562	2.562	0.2078861328812431
X	0.02187774991411744	790	0.79	0.3025463055827717
X	0.02194722817253981	649	0.649	0.3233788860331932
X	0.02131538963335666	700	0.7	0.31227106558566037
X	0.02247221755468448	4499	4.499	0.1709398476236641
X	0.02192153883950363	658	0.658	0.32177209729825795
X	0.022447770311973186	5413	5.413	0.1606614667905177
X	0.021822496424900695	1692	1.692	0.23451359490015322
X	0.0214377367474979	870	0.87	0.29099569248144513
X	0.022932923383704385	2023	2.023	0.22464039069925804
X	0.022051258183437446	900	0.9	0.29044481077417683
X	0.022317052439060816	1806	1.806	0.23119212100668352
X	0.02229572249936228	952	0.952	0.28610681363672413
X	0.021815689067389975	1239	1.239	0.26015521909899697
X	0.023437319805495467	12053	12.053	0.12481619235085765
X	0.02241274613674017	1317	1.317	0.2572190317124946
X	0.022428253996832948	2812	2.812	0.1997990335402928
X	0.022314228985986614	1020	1.02	0.27967945922733833
X	0.022333118229450593	2484	2.484	0.2079373912983638
X	0.02146320876957091	767	0.767	0.30359859821311475
X	0.021885098058995585	1380	1.38	0.2512405893526232
X	0.021792188672646706	1073	1.073	0.27283518475950147
X	0.022995496106022938	523	0.523	0.352950416085165
X	0.021979887326670243	679	0.679	0.3187023231089114
X	0.021997110991873455	766	0.766	0.3062285130097349
X	0.02224415201746307	1194	1.194	0.2650964076099973
X	0.02216405586669822	2788	2.788	0.1995808290107944
X	0.02170616583671425	1069	1.069	0.27281514156043774
X	0.023935948072443484	4679	4.679	0.1723055221791423
X	0.02318494138063666	14891	14.891	0.11590283158084107
X	0.02396363108256794	7739	7.739	0.14575470835663196
X	0.021866274794188553	681	0.681	0.31784049608524395
X	0.02168715101836314	655	0.655	0.32110992901826624
X	0.021325465943778946	1698	1.698	0.23244503867532745
X	0.021863197080375926	822	0.822	0.29850202662088987
X	0.021799270155455172	1342	1.342	0.25325798426731416
X	0.021122947122678617	1274	1.274	0.2549930603744499
X	0.022829798413449818	4058	4.058	0.17785388025404816
X	0.023059544211307726	3120	3.12	0.19478942471358457
X	0.022484198140728915	5742	5.742	0.15761761024205828
X	0.021494249148411186	1225	1.225	0.2598535952652151
X	0.023032094435853124	14432	14.432	0.11686083114121638
X	0.021535220924554486	992	0.992	0.27896253853574166
X	0.02194384497087441	1037	1.037	0.27659521116181923
X	0.022227921778942773	1966	1.966	0.22444250925097042
X	0.020516618618196703	220	0.22	0.4534830249003093
X	0.023836985155801578	4598	4.598	0.17307225531451356
X	0.02206847520482	701	0.701	0.31575587248455433
X	0.022308887161543674	3373	3.373	0.18770990441485183
X	0.02167145875059959	877	0.877	0.2912702872215563
X	0.02207921780826156	1541	1.541	0.24288107793908437
X	0.02206062213003106	1052	1.052	0.2757617365741576
X	0.022241455578009803	1116	1.116	0.2711229848669743
X	0.021734892394264974	1293	1.293	0.25616492037309274
X	0.022146357166170413	2529	2.529	0.20611890350210982
X	0.020160862631389477	231	0.231	0.44357362011999507
X	0.021055977345321032	753	0.753	0.3035243069594903
X	0.020707894708462946	913	0.913	0.28306644467762415
X	0.022744917034107463	2484	2.484	0.20920766318390596
X	0.021494994438919543	908	0.908	0.2871334766820591
X	0.023082265753509357	3474	3.474	0.18799640347156868
X	0.022515365270391136	1571	1.571	0.24290377855849818
X	0.021849179378406008	1750	1.75	0.23198809242719975
X	0.022020642088214307	2591	2.591	0.20407392701462612
X	0.022255728561226876	3455	3.455	0.18606495748735705
X	0.021735507420579368	1391	1.391	0.2500043418199378
X	0.022506823194092824	2343	2.343	0.21257587247565135
X	0.02204111198359811	1528	1.528	0.2434277298330621
X	0.022164499282382524	2008	2.008	0.22265446330966351
X	0.022513073734437852	1455	1.455	0.2491861691133309
X	0.022777335737365453	5235	5.235	0.1632533414424762
X	0.02255417792841779	2464	2.464	0.20918414592521326
X	0.022173408620979714	1591	1.591	0.24065086493183627
X	0.022539408150916818	4039	4.039	0.17737383320069558
X	0.022581354576934165	3524	3.524	0.18573968163510376
X	0.024243572379626772	15742	15.742	0.11548145196224015
X	0.022085692256678117	675	0.675	0.31984218808424836
X	0.02133912093777551	336	0.336	0.39897506037605274
X	0.022236054843096317	1097	1.097	0.27265724132147123
X	0.021898489952523997	701	0.701	0.3149430653197066
X	0.022036753212783033	1640	1.64	0.23773944983137935
X	0.023106077307970145	1982	1.982	0.2267466384502952
X	0.021909308106244134	1089	1.089	0.27197788633170517
X	0.02109522245609463	708	0.708	0.3100156493786778
X	0.022323320395732607	1636	1.636	0.2389599751289473
X	0.021989213747804798	524	0.524	0.34750373061205053
X	0.02238313097317548	4046	4.046	0.17686082743401277
X	0.022088922803956895	886	0.886	0.29213276673711536
X	0.02248933419025041	6900	6.9	0.14826631796301284
X	0.0223131322023263	3882	3.882	0.17913000410044688
X	0.022198425978990365	1559	1.559	0.24237734391634197
X	0.02113988776730967	260	0.26	0.4332212752571546
X	0.023829173756757784	3150	3.15	0.1963056547902728
X	0.022249070248807564	2868	2.868	0.19796005847373457
X	0.022837731335685525	1709	1.709	0.23730322056252529
X	0.024830099335749034	5427	5.427	0.16601202425590716
X	0.02233606245468823	7362	7.362	0.14476723092529556
X	0.02141874499097265	978	0.978	0.2797811097538328
X	0.021789807863313897	709	0.709	0.3132341144875969
X	0.02153469784391352	514	0.514	0.3473163442899759
X	0.021145663412588455	549	0.549	0.3377144604138998
X	0.022022982542156885	506	0.506	0.35175629037362566
X	0.02238489125194616	418	0.418	0.3769290145277852
X	0.020913895184001767	431	0.431	0.36474209279635195
X	0.022012845409818318	3435	3.435	0.18574472147646703
X	0.022321153258913217	969	0.969	0.28453189137723317
X	0.02118948557726335	958	0.958	0.28070596065133063
X	0.022209520437379296	875	0.875	0.29388474461453806
X	0.022157126877467653	2322	2.322	0.21210475744971732
X	0.02157468194804971	1016	1.016	0.2769173911074197
X	0.022131463027243076	3229	3.229	0.1899535261220559
X	0.022004850555415015	2796	2.796	0.19891174107279141
X	0.021926117261818305	711	0.711	0.3135913277234971
X	0.021569991232530186	556	0.556	0.33852576303894427
X	0.022348024495209604	5571	5.571	0.15889198602627827
X	0.0200310489496829	394	0.394	0.37045530936421434
X	0.022703126086366293	596	0.596	0.336470092164706
X	0.022287505626158624	1101	1.101	0.2725365259441822
X	0.021417304484642886	1421	1.421	0.2470151428002505
X	0.02306020717927028	3689	3.689	0.18421211259470724
X	0.02227624695639192	1449	1.449	0.24865157404389607
X	0.02132161057596089	632	0.632	0.3231228292986074
X	0.02231217889693108	1554	1.554	0.24305076348661112
X	0.022824609288285655	3091	3.091	0.19473086020863292
X	0.020977562948380074	728	0.728	0.3065781631876829
X	0.022486229508817273	7658	7.658	0.14319694707287653
X	0.022559523282804052	52942	52.942	0.07525058112538224
X	0.022530828365112568	1925	1.925	0.2270468469456355
X	0.02288627254564636	2597	2.597	0.20655436254848583
X	0.021545604110546887	404	0.404	0.3764082239284736
X	0.023202877492594	3183	3.183	0.19389580041404067
X	0.02320430306746328	8034	8.034	0.1424116660573351
X	0.022428743024405264	87770	87.77	0.06345798517547283
X	0.022398035394733735	1827	1.827	0.23058114587602718
X	0.022494978013789597	40824	40.824	0.08198291181419662
X	0.022339486357504986	41283	41.283	0.08148931076970255
X	0.022324548595037245	4926	4.926	0.16548657191149896
X	0.023217043805786278	51918	51.918	0.07647092125737379
X	0.022170783746545443	784	0.784	0.3046643657222185
X	0.022431733978775507	3054	3.054	0.19438583049724686
X	0.0229934467455437	79298	79.298	0.06618820669788923
X	0.023506655233794936	238458	238.458	0.046194788194846986
X	0.022181049825942344	2542	2.542	0.20587432285106885
X	0.022548368504784654	21038	21.038	0.10233799192902927
X	0.02239198735035456	9123	9.123	0.1348916255894678
X	0.02244591065383753	24132	24.132	0.09761456825221324
X	0.022305078528217417	26350	26.35	0.09459634989892088
X	0.0236614940325256	17988	17.988	0.10956867962674496
X	0.022817966252998857	23920	23.92	0.09844007125491204
X	0.02316247761089246	13016	13.016	0.12118136055600184
X	0.02140322702415589	3160	3.16	0.1892041792655788
X	0.02329161410647887	130811	130.811	0.05625810597575018
X	0.02231102883239471	1541	1.541	0.24372812804877306
X	0.022197047255639823	1414	1.414	0.2503890115490786
X	0.02255034642825372	12767	12.767	0.1208800933087302
X	0.021866040896991033	17851	17.851	0.10699640895728867
X	0.022833686397813434	220797	220.797	0.046938328629353404
X	0.022863339820198388	58120	58.12	0.07327199593405666
X	0.022385426160928774	427	0.427	0.37426495571807467
X	0.02246450206947045	8507	8.507	0.13822081667680525
X	0.022954797889863435	11036	11.036	0.12764993377458025
X	0.023238274071714964	11595	11.595	0.1260794693193535
X	0.023145949440259057	49022	49.022	0.07786841321495472
X	0.022201345856513514	12617	12.617	0.12072792411353635
X	0.022412492656512437	23088	23.088	0.09901506620098907
X	0.02211126494559858	436	0.436	0.3701482493960944
X	0.022064796967786952	2894	2.894	0.19681905227928118
X	0.022314852409202602	41638	41.638	0.08122718188258383
X	0.022391310962682123	25110	25.11	0.0962522601512747
X	0.02243265904569351	19698	19.698	0.10442862044964138
X	0.02245270592959447	4525	4.525	0.17056242743799238
X	0.023270615368436845	217235	217.235	0.0474926084610654
X	0.022428097494482883	3771	3.771	0.18118071434674732
X	0.02251876011819013	17438	17.438	0.10889700456443423
X	0.02327808866271725	77196	77.196	0.06705803762977806
X	0.023028776724639842	1351	1.351	0.25735849410637923
X	0.022388601546791714	28132	28.132	0.09267063085479484
X	0.023840711023717383	155887	155.887	0.05347733644208123
X	0.022540009468383648	147968	147.968	0.05340666143494434
X	0.022761793306060155	9115	9.115	0.1356698258225501
X	0.022024552469040122	6573	6.573	0.14964004153171362
X	0.021796890010348492	1618	1.618	0.2379424916989314
X	0.022283978508319063	2487	2.487	0.20770118861583609
X	0.02326225834598488	33233	33.233	0.08878932935540851
X	0.022509099371420798	6867	6.867	0.14854693329432647
X	0.02324732508974929	16722	16.722	0.11160789657106715
X	0.02331472627456432	40383	40.383	0.08326783008889677
X	0.023180885957714156	21626	21.626	0.1023413819838823
X	0.02236732292050392	9353	9.353	0.13372759200997933
X	0.022636012751188532	3441	3.441	0.1873721696892313
X	0.023216538533685015	88617	88.617	0.06398725433864386
X	0.023295509390295517	12840	12.84	0.12196514297092581
X	0.022738752276723277	7004	7.004	0.14807218930508756
X	0.02437454609127448	18719	18.719	0.10919882472409853
X	0.023566169588384343	224742	224.742	0.04715577850318314
X	0.022452850568559723	4898	4.898	0.16611833456536845
X	0.023030856159885153	79313	79.313	0.06621990731347381
X	0.02219105935380699	2313	2.313	0.21248786557442043
X	0.022161669872608262	21698	21.698	0.10070729371984456
X	0.022604609450798244	7108	7.108	0.14705615153178456
X	0.023848919038626914	84321	84.321	0.06564137206477898
X	0.023142071233667932	27722	27.722	0.09415843299850431
X	0.022158169333032645	2479	2.479	0.20753234022190897
X	0.023011523044873465	87085	87.085	0.06417029008999292
X	0.02319883239186586	81516	81.516	0.06577705455054195
X	0.0224389914983782	2377	2.377	0.21134471110656677
X	0.023246194488048724	41617	41.617	0.08235570681005798
X	0.023226773035096366	110577	110.577	0.0594439998387339
X	0.02408088956668204	256037	256.037	0.045476803594911464
X	0.023114532491636683	60227	60.227	0.07267158420486919
X	0.02169785269926095	801	0.801	0.3003272551243428
X	0.022253536323407045	2912	2.912	0.19697112016263058
X	0.02272336557824566	6179	6.179	0.15435409417614362
X	0.022448110867052527	12028	12.028	0.12312005279521307
X	0.022522662826341657	6749	6.749	0.14943767511900974
X	0.022001248575154165	1182	1.182	0.2650187596770493
X	0.02326755526216824	45017	45.017	0.08025240721190714
X	0.023000355862847002	64789	64.789	0.07080725520311427
X	0.02211107192920708	9445	9.445	0.13278100304508936
X	0.02225270442943864	2172	2.172	0.21719058388398185
X	0.022309570459754503	9955	9.955	0.1308629324799082
X	0.024329309213182264	10600	10.6	0.13190906367393068
X	0.022498588607908425	6681	6.681	0.14988951957828203
X	0.02247721614801634	8619	8.619	0.13764545648057566
X	0.02240941938115081	1815	1.815	0.2311273420542594
X	0.022121266324787787	50028	50.028	0.0761843233147629
X	0.022987931486938333	19040	19.04	0.10648235559783294
X	0.022684199534029652	10603	10.603	0.12885409719809565
X	0.022563318690864743	21584	21.584	0.10149010030664846
X	0.02207133305853025	3689	3.689	0.1815404062798452
X	0.024080845557524374	376806	376.806	0.03998078413468937
X	0.022429176256271468	4968	4.968	0.16527629887068107
X	0.022441810309572034	56745	56.745	0.07340237220601607
X	0.022617309564526577	791	0.791	0.3057887377715287
X	0.02244677040215131	1799	1.799	0.23193924208972586
X	0.022480067803845975	2998	2.998	0.19572906858492062
X	0.023309832860194085	6106	6.106	0.15628865480767298
X	0.022413367086061932	3741	3.741	0.18162395642933532
X	0.022478046326290867	26500	26.5	0.09466095814053212
X	0.022041999353326354	2432	2.432	0.2084950770308246
X	0.02260321479157215	2620	2.62	0.2050955827769399
X	0.023919792506065317	207157	207.157	0.0486951750812932
X	0.022592181494465707	23105	23.105	0.09925462018812016
X	0.022495933627223307	22666	22.666	0.09974926689145865
X	0.023483590721841387	63588	63.588	0.07174579391918318
X	0.0230357305609133	99873	99.873	0.06132694039083082
X	0.022894928870483326	4157	4.157	0.17659826182409372
X	0.022507091552749863	3000	3.0	0.1957639447059753
X	0.023198380353400844	9519	9.519	0.13457222900528584
X	0.020945915484784253	402	0.402	0.37350042293197133
X	0.022005462104867358	2157	2.157	0.21688363457941856
X	0.023203607721914304	11780	11.78	0.12535357995375232
X	0.021667214446603297	1667	1.667	0.2351197767084048
X	0.022453744523029385	1565	1.565	0.24299172178946427
X	0.02184795607315096	1729	1.729	0.23291919130002767
X	0.022372569306105095	2977	2.977	0.19587499985621737
X	0.023720554282387483	217771	217.771	0.04775750126751002
X	0.02210440711214598	7279	7.279	0.14481127714722336
X	0.022818771898350798	42066	42.066	0.08155558047792133
X	0.022349158541603183	4304	4.304	0.17316613797214095
X	0.022786065612574864	1848	1.848	0.2310233048377091
X	0.022325973880025524	10999	10.999	0.12661516834465222
X	0.023338449612391707	328057	328.057	0.04143565716065367
X	0.023970183330022116	280845	280.845	0.04402860103747716
X	0.022512362323988334	10398	10.398	0.12936707304377654
X	0.02283121968228363	18765	18.765	0.10675630031141428
X	0.022257662312006845	1175	1.175	0.26657160950372344
X	0.022207728885577198	23157	23.157	0.0986144636689822
X	0.02304958014455439	114702	114.702	0.05857298306898875
X	0.02311685367584245	23933	23.933	0.09885011852980281
X	0.022339098589931976	5400	5.4	0.16053040264435972
X	0.022471543333224295	45453	45.453	0.07907213209002051
X	0.022963616842577514	225660	225.66	0.046686921305905245
X	0.022195422958587008	15671	15.671	0.11230236569086265
X	0.021631049091217942	3928	3.928	0.17659105504953582
X	0.024772854868743022	12154	12.154	0.12679017719287053
X	0.02282888441605148	12761	12.761	0.12139477472702852
X	0.022472405636530256	1622	1.622	0.2401778236560874
X	0.023233622801488947	102779	102.779	0.060916860874931436
X	0.02239649882599109	10469	10.469	0.12885214243104182
X	0.021674187777165003	485	0.485	0.3548681569875579
X	0.02308607283175592	13867	13.867	0.118519328430686
X	0.023810060727433177	16173	16.173	0.11376010184021752
X	0.022049726085356772	464	0.464	0.3622110868406464
X	0.02262822345469417	65953	65.953	0.0700065433750667
X	0.023192821958575634	82726	82.726	0.06544912671474935
X	0.023332203113635694	78579	78.579	0.06671391340912988
X	0.022977471784579675	88336	88.336	0.06383440831392162
X	0.022481565877075598	62325	62.325	0.07118493454330266
X	0.021914907019438597	8814	8.814	0.13547386735552042
X	0.021767013438152683	6530	6.53	0.14938095148816424
X	0.02319750523325043	26524	26.524	0.09563145292444622
X	0.023309500713076744	64496	64.496	0.07123062091477096
X	0.023156706339773493	2160	2.16	0.22049953554593724
X	0.023303100978440107	31927	31.927	0.09003648562680615
X	0.023511884479340862	128933	128.933	0.05670758038969625
X	0.023167148020553707	40199	40.199	0.08321835140478959
X	0.022891485647335393	30027	30.027	0.09135255077782908
X	0.02251919161685455	41908	41.908	0.08129901560408101
X	0.023070477032216243	6361	6.361	0.15364215606602516
X	0.022392775537024585	1908	1.908	0.2272531187956203
X	0.022111417157347817	11795	11.795	0.12330278263182178
X	0.022882294539387064	87886	87.886	0.0638547620620086
X	0.023025029057367745	57368	57.368	0.07376383581081401
X	0.023299025353685006	330303	330.303	0.04131823346431548
X	0.02400733544995177	94863	94.863	0.06325318064912405
X	0.02317877935892057	44561	44.561	0.08042268054587534
X	0.022355874106342186	13369	13.369	0.11869458618457007
X	0.02308180405825835	86454	86.454	0.06439145215588285
X	0.02210750923218468	964	0.964	0.28411070232692354
X	0.02219772171953525	4432	4.432	0.17109456618928895
X	0.023929168078518196	171468	171.468	0.051869831753411276
X	0.022362670374664623	8138	8.138	0.14006683832914796
X	0.02239544513369072	3260	3.26	0.19009937964392545
X	0.02318027878333794	11990	11.99	0.1245756486700641
X	0.02385831080832233	82685	82.685	0.06608014496700361
X	0.02214802699845827	14811	14.811	0.11435369991414492
X	0.02345064510832082	47559	47.559	0.0790024743055179
X	0.021928234986341343	2196	2.196	0.21533953962985092
X	0.022270718406799606	4732	4.732	0.16758294965689177
X	0.021999406890889712	2203	2.203	0.21534369386430716
X	0.02241436070469188	9492	9.492	0.13316483102593293
X	0.023073605229147778	19608	19.608	0.10557491000555483
X	0.022473440624959415	29108	29.108	0.09173866168443348
X	0.02205561904771116	1714	1.714	0.23433445265928016
X	0.02324358937566054	82591	82.591	0.06553251315194047
X	0.021933433673490436	1130	1.13	0.2687464445816248
X	0.023802965200390512	17774	17.774	0.11022547066486492
X	0.02216244209031493	3617	3.617	0.182988178881254
X	0.022528183936204538	17609	17.609	0.1085584990283304
X	0.022799260199435484	2583	2.583	0.20666429440722975
X	0.023045293782900402	16926	16.926	0.11083475632245686
X	0.022525681742190797	8911	8.911	0.13622301871199188
X	0.023819815537321487	16459	16.459	0.11311277304420064
X	0.02257079277708355	53069	53.069	0.07520302377616601
X	0.022561024170017666	137576	137.576	0.05473587237008679
X	0.022282975980499352	3721	3.721	0.18159525957989522
X	0.021595569707980267	4484	4.484	0.16887538568021818
X	0.022547628221054765	69309	69.309	0.06877602593324149
X	0.02242993917416047	5344	5.344	0.16130724807390076
X	0.02406763012967835	376474	376.474	0.039985216031284715
X	0.02159803416535468	711	0.711	0.31201935951419035
X	0.022485486517609527	60705	60.705	0.07181678210500181
X	0.02407510024036286	147979	147.979	0.05459120390700714
X	0.02193129405186192	1201	1.201	0.26333408940561004
X	0.022302548614467402	108249	108.249	0.05906227896526334
X	0.022570611539410507	16954	16.954	0.11000785985765575
X	0.023184000184860505	345092	345.092	0.04065224116925924
X	0.02205963872113015	3474	3.474	0.18517805880989513
X	0.02190387276787969	13270	13.27	0.11818163529838062
X	0.02240960988130562	2390	2.39	0.2108687053938124
X	0.022191514231448984	2551	2.551	0.2056642594944229
X	0.022218356755613813	5173	5.173	0.16255106496483496
X	0.022626415571308678	16910	16.91	0.11019385685968562
X	0.022128369848747054	5800	5.8	0.15625735764230586
X	0.022516642063930014	44183	44.183	0.07987596924234565
X	0.02283058395118236	9246	9.246	0.13516192763471163
X	0.022442738156576072	851	0.851	0.2976565388493186
X	0.022392313929307104	6239	6.239	0.15310686457957165
X	0.024106797755391957	84108	84.108	0.0659326815714791
X	0.023746254574881435	22313	22.313	0.10209685979719042
X	0.02332479677646465	147160	147.16	0.05411811939557338
X	0.022477989299714268	18562	18.562	0.10658863419114481
X	0.023234895109747872	6996	6.996	0.1491982188463371
X	0.022052281387074768	2448	2.448	0.20807218956555695
X	0.02238671230884101	3193	3.193	0.19139494643840257
X	0.02325684903089112	224921	224.921	0.04693609519069544
X	0.023293978986148746	7151	7.151	0.14823780208812287
X	0.023129810770764134	7245	7.245	0.14724635853728363
X	0.022357643754148185	24866	24.866	0.09651764147175432
X	0.022606705836594198	21355	21.355	0.10191682894098336
X	0.022428911930534656	1681	1.681	0.2371811465088964
X	0.02253182993308958	3781	3.781	0.18129949556260083
X	0.023033568083949362	202025	202.025	0.048489983292359415
X	0.022497189536547512	8331	8.331	0.139254865316193
X	0.022163062432294354	22500	22.5	0.099498320619816
X	0.023314681337353357	19546	19.546	0.1060532209808821
X	0.024222872061369968	49661	49.661	0.07871727623642312
X	0.02240638553739616	72500	72.5	0.0676100513760989
X	0.02352697452076972	128617	128.617	0.0567661232205713
X	0.022798429877105834	52881	52.881	0.07554431014718123
X	0.022701197718108225	3303	3.303	0.1901282857575483
X	0.023246569403785163	32061	32.061	0.08983810910003853
X	0.02248150120229828	7542	7.542	0.1439172747786917
X	0.022912669628500172	5327	5.327	0.16262887789285224
X	0.022324157255634542	10948	10.948	0.12680803167839122
X	0.022487028840804144	2497	2.497	0.20805165659696762
X	0.022463421079898772	32034	32.034	0.08884266460726789
X	0.022099096192464123	6586	6.586	0.14971004077320996
X	0.021237888430668123	1150	1.15	0.2643247508661516
X	0.022766385444501623	10725	10.725	0.12851848415302738
X	0.02312978899421941	3445	3.445	0.1886517398557632
X	0.022439494814614477	70471	70.471	0.06828639936934236
X	0.022522296052145955	6174	6.174	0.15393900646287378
X	0.02262940995066971	12691	12.691	0.12126229833308395
X	0.02202825117638827	4840	4.84	0.1657212572417321
X	0.022454269856984354	13312	13.312	0.11903788709088337
X	0.02243565908834509	2070	2.07	0.22130411815075388
X	0.022324216194886063	3812	3.812	0.1802496551803787
X	0.023030226410821315	14745	14.745	0.11602488417713974
X	0.024037214171311973	75240	75.24	0.06836153125117181
X	0.0220891172096086	4839	4.839	0.16588517649347947
X	0.022258248900448346	837	0.837	0.29848454257415286
X	0.021711662460628715	2027	2.027	0.22043460664927908
X	0.022536669699904546	128352	128.352	0.05599669130409105
X	0.024735171166296043	357466	357.466	0.04105443312229569
X	0.022194613103120726	11662	11.662	0.12392478684360855
X	0.022309323536865315	3676	3.676	0.18240509556486764
X	0.022496377077216082	45047	45.047	0.07933818051660702
X	0.024510290321569147	71134	71.134	0.07010630451927666
X	0.021674791515081483	1487	1.487	0.24427629261992256
X	0.022366472778329618	8177	8.177	0.1398517271807001
X	0.022826150454400337	9811	9.811	0.1325072985207178
X	0.022542178190363134	68432	68.432	0.06906301732491145
X	0.022336176066083152	46308	46.308	0.07842435097187374
X	0.02323032028808792	120211	120.211	0.05781452953319673
X	0.02234879047018598	4799	4.799	0.16699408606513605
X	0.02313062251595501	2447	2.447	0.21143868571323618
X	0.021946836799915843	1064	1.064	0.2742479787275382
X	0.022632468795951283	19825	19.825	0.10451362503622313
X	0.023105752574737015	7734	7.734	0.14402525413993858
X	0.022369800168175722	17024	17.024	0.1095301034778867
X	0.022964682438989175	57940	57.94	0.07345600716787294
X	0.023305683512956612	255469	255.469	0.04501679226506855
X	0.02226845300556115	4556	4.556	0.16970792003381213
X	0.02236838076645529	1641	1.641	0.23887754219312346
X	0.02231182079086425	8041	8.041	0.14052113543130904
X	0.02396348299242683	403131	403.131	0.039027246828169805
X	0.02247467389541064	18603	18.603	0.1065050345540955
X	0.02159672636840461	1512	1.512	0.242630491630357
X	0.021190660126679434	1842	1.842	0.2257450762791743
X	0.02254519842832953	21112	21.112	0.10221349260973489
X	0.02236102457915964	897	0.897	0.2921234483835122
X	0.02236648868847768	31224	31.224	0.08947528501147879
X	0.023739559147558403	2338	2.338	0.2165422845269669
X	0.02256198821945994	9649	9.649	0.13272875495656392
X	0.02243907904343175	6101	6.101	0.15435996371271146
X	0.021940086804999363	2524	2.524	0.2056125710324077
X	0.022157260467383456	1560	1.56	0.24217565790030177
X	0.02322167854554253	121826	121.826	0.05755077918832612
X	0.024143291363621852	295639	295.639	0.043385530977667404
X	0.023028379050651863	69229	69.229	0.06928806563885875
X	0.022340257939967148	30465	30.465	0.09017697586331244
X	0.02312543750760539	14499	14.499	0.11683796101732138
X	0.022291675258493047	7196	7.196	0.14577526883495767
X	0.022980695834578894	5710	5.71	0.15906538786060903
X	0.023231574039990675	47183	47.183	0.07896434338753033
X	0.024971882057892452	18990	18.99	0.10955749746526108
X	0.02402549753145239	51780	51.78	0.07741707904339953
X	0.022479458700089924	7057	7.057	0.14713700465203605
X	0.023262940452560384	449056	449.056	0.03727818006898578
X	0.022768647655251426	9806	9.806	0.1324184344978224
X	0.023741730276769124	27186	27.186	0.09558484012086292
X	0.02354999164171742	106880	106.88	0.060399234916023024
X	0.023258029756516415	33431	33.431	0.08860832318475398
X	0.022836983957864213	57768	57.768	0.07339229553642213
X	0.023521700575192536	155430	155.43	0.05328986541320347
X	0.02323239128816416	30106	30.106	0.09172342971019405
X	0.023293629198383455	24423	24.423	0.09843420757881016
X	0.023096931759099056	5811	5.811	0.15840460587193173
X	0.022588830675125356	21906	21.906	0.1010284187225148
X	0.022934811058852422	3692	3.692	0.1838277866229409
X	0.023321573321574186	88291	88.291	0.06416238072140272
X	0.024492549848899592	559567	559.567	0.03524200718551699
X	0.02317206441664074	70802	70.802	0.06891383210283725
X	0.022747002166761895	85013	85.013	0.0644386266025537
X	0.023237083305164268	12303	12.303	0.12361092708154386
X	0.02246160035584674	16541	16.541	0.11073705719341208
X	0.02467432296360793	38598	38.598	0.08614435317252232
X	0.02213340222820026	5819	5.819	0.15609893489456408
X	0.023357302870040012	61789	61.789	0.07230536262576971
X	0.02313510723698936	140680	140.68	0.05478727739414086
X	0.022298195198472318	13013	13.013	0.11966415940087574
X	0.02324610077316178	240872	240.872	0.045869210574310114
X	0.02248824034373507	107088	107.088	0.05943900838201445
X	0.022990134000056273	15706	15.706	0.11354254813738492
X	0.02314254548845731	1655	1.655	0.24091899489565702
X	0.02366734877259675	35507	35.507	0.0873531445814027
X	0.022513177751764767	50968	50.968	0.07615756843597461
X	0.022691036601743684	1754	1.754	0.23475143142364438
X	0.023902250866022694	290789	290.789	0.043479740366389644
X	0.023307144868659826	20203	20.203	0.10487961165298795
X	0.02235303787581657	11356	11.356	0.12532481053310135
X	0.023060597539534683	3858	3.858	0.1814830673997265
X	0.022415120462315037	1428	1.428	0.2503826836146326
X	0.021844633101897076	1308	1.308	0.25561068465799613
X	0.02312976110853802	4447	4.447	0.1732612084945206
X	0.022385503684332703	12786	12.786	0.1205250693886647
X	0.022523920994972448	19146	19.146	0.10556549300423643
X	0.024136368926020783	1851601	1851.601	0.023534673424449905
X	0.022424855986678344	5010	5.01	0.16480256960178716
X	0.022494532959771645	18306	18.306	0.10710946273017635
X	0.02218670736463733	2432	2.432	0.20895034611658475
X	0.02258390743638367	8044	8.044	0.14107249296876986
X	0.024228112300766977	6027	6.027	0.1590032966911613
X	0.022935459106861057	86379	86.379	0.06427366852257074
X	0.02222204091124706	19856	19.856	0.10382391299726297
X	0.0245227408482305	484475	484.475	0.036991267483414414
X	0.022288265469913537	3337	3.337	0.18832444161193929
X	0.02342022904669471	122803	122.803	0.05756087500705912
X	0.022500755787960514	19414	19.414	0.10504144647773518
X	0.02247520434298004	79010	79.01	0.06576687340400547
X	0.02386828430434431	230754	230.754	0.04694153871531615
X	0.022393584905263893	8872	8.872	0.1361551365935929
X	0.022361983187672237	3255	3.255	0.19010189288073015
X	0.023196019878787687	61845	61.845	0.07211677426805477
X	0.02232561847976023	5606	5.606	0.15850761637792485
X	0.02236991756433438	21221	21.221	0.10177306316433833
X	0.022601999669925252	43392	43.392	0.08045982163152386
X	0.022596800150022113	14324	14.324	0.11641105114899475
X	0.02495295901248468	35291	35.291	0.08908803150994862
X	0.022870648562170026	9603	9.603	0.13354383707307949
X	0.02390986074084001	69615	69.615	0.07003117391684356
X	0.02304547091063125	198312	198.312	0.04879914326949239
X	0.021953289878531693	7128	7.128	0.14549362805540808
X	0.022470347337303707	3909	3.909	0.17913538098922846
X	0.022665395239223753	86644	86.644	0.06395506347918596
X	0.022009427298174324	23165	23.165	0.09830874360901855
X	0.023031699505158088	75635	75.635	0.06727717157102198
X	0.021953526432914615	18387	18.387	0.1060875626997926
X	0.022254149354602296	31832	31.832	0.08875286012861046
X	0.02238784902472966	16094	16.094	0.11163047965976264
X	0.02317631520427292	56678	56.678	0.07422381806201701
X	0.022512844923066933	35025	35.025	0.08630131474045934
X	0.022820684876950912	13525	13.525	0.11905029248847131
X	0.022391589472506315	4031	4.031	0.177102234888588
X	0.024962433971895737	17466	17.466	0.1126413441933364
X	0.022798216039227527	22587	22.587	0.10031074078364759
X	0.022576033873892173	47148	47.148	0.07823386585851527
X	0.023183559075680593	64628	64.628	0.07105366172343726
X	0.022388933432365424	5025	5.025	0.1645504646588416
X	0.023052940250394807	18748	18.748	0.10713312995787647
X	0.02296382189313244	24186	24.186	0.09828638768688765
X	0.022998000867676057	70244	70.244	0.06892238285890222
X	0.02318470059260191	26277	26.277	0.09591250567556284
X	0.022864951659269613	53383	53.383	0.07537993703645493
X	0.022489089699129865	12989	12.989	0.12007853663037578
X	0.023296729330167222	14690	14.69	0.11661588684700964
X	0.022458243540635702	1655	1.655	0.2385206200520688
X	0.022426458563428692	6330	6.33	0.15244703991636516
X	0.02318614949684402	36310	36.31	0.08611275046511102
X	0.023078688130728796	22344	22.344	0.10108422798503636
X	0.0229860605231002	25236	25.236	0.09693516243305446
X	0.022227281658954573	844	0.844	0.2975189518835978
X	0.022870872872032846	2955	2.955	0.19780704335677146
X	0.022221648612482863	30413	30.413	0.09006837720646357
X	0.02286750138555063	23271	23.271	0.09941865634534784
X	0.022441238508552473	11458	11.458	0.12511594712528326
X	0.022389910865179767	1716	1.716	0.23542090617553924
X	0.021494690622160497	1629	1.629	0.23630343681915963
X	0.022332381441066817	12442	12.442	0.12152947264180404
X	0.022494360825449505	19077	19.077	0.10564635699168984
X	0.02290350603316436	55558	55.558	0.07442493346077915
X	0.022519913232382668	2109	2.109	0.22020645444668735
X	0.022483407257066098	12099	12.099	0.12294311619489191
X	0.02282492115131889	9185	9.185	0.13544928237386172
X	0.022189873566789278	5851	5.851	0.15594624067140228
X	0.023246694879774626	196203	196.203	0.04911549484052504
X	0.02219004421527865	10898	10.898	0.1267468344121771
X	0.022377565863655816	45336	45.336	0.07902961777241217
X	0.023056602230444086	104696	104.696	0.060388615464461226
X	0.022976415160581916	23289	23.289	0.09955058344300882
X	0.02315786600975641	80594	80.594	0.06598804703244086
X	0.023932223015865554	113785	113.785	0.0594701922512149
X	0.022151484661974058	910	0.91	0.2898147482633868
X	0.022681544069099425	21156	21.156	0.10234807671244464
X	0.02314367752044051	8027	8.027	0.14232888117341808
X	0.02322376422126599	134563	134.563	0.05567612282700445
X	0.02294908919765208	90442	90.442	0.0633089482684978
X	0.022080633926538237	13417	13.417	0.11806432974714037
X	0.022548862183412104	23775	23.775	0.09825048484732742
X	0.0231857456793824	6208	6.208	0.15515166951987905
X	0.022042774609270895	8420	8.42	0.13782184395191563
X	0.022359675951355112	5540	5.54	0.15921546399663
X	0.022146494473092488	5111	5.111	0.16302956526607854
X	0.022802605333232372	140609	140.609	0.05453271458689319
X	0.024769561661374978	210369	210.369	0.0490131023754469
X	0.021760872123444176	5614	5.614	0.15708496858611204
X	0.023092804808446065	9412	9.412	0.13487504069710507
X	0.022835347235039955	11718	11.718	0.12490640496263287
X	0.023060430720413697	91499	91.499	0.06316604553201656
X	0.022487741584947575	7716	7.716	0.14284044932954987
X	0.02235675411902948	4942	4.942	0.16538724130210858
X	0.02331664014260356	27039	27.039	0.09518279381282908
X	0.023176666495189856	66517	66.517	0.07036760544308374
X	0.022418694724421533	4163	4.163	0.17528091733549805
X	0.023114543816899945	25083	25.083	0.09731249692557582
X	0.02253966645178742	2462	2.462	0.20919588884719142
time for making epsilon is 1.3527705669403076
epsilons are
[0.3730430565651091, 0.12931646662927795, 0.24166253669661716, 0.17590465345268358, 0.09658711166222556, 0.113142503349859, 0.08272678757216159, 0.09863341721088194, 0.1543128499044501, 0.12480063562134931, 0.1974881211797118, 0.07001337420695908, 0.16991537818418093, 0.05103026667937364, 0.1102186129765539, 0.10277835918249573, 0.0805025653102671, 0.08844295716329749, 0.058842006723008086, 0.06378419757714997, 0.14817986492741328, 0.052183193077447995, 0.13979834647615985, 0.10233821503811606, 0.13766722050492827, 0.0727027358803406, 0.08178688757560346, 0.17294706871179338, 0.0794551215903879, 0.03011129926110688, 0.16792407700005724, 0.036545665700664615, 0.12221968592473294, 0.16742073533337112, 0.16638065752245262, 0.0678973523232694, 0.07154314187192777, 0.28927092906151447, 0.16322355315525652, 0.22663887821242967, 0.20860070819774953, 0.3016689086508759, 0.3090882502364078, 0.19834555415740704, 0.39779423026202326, 0.34655188571153495, 0.20055469221079902, 0.22618530806330597, 0.2394192274228931, 0.1355407664642693, 0.14082063675896656, 0.21331947395817652, 0.19459689433740793, 0.22609702534993453, 0.17177416208897595, 0.19376270834119963, 0.26654929719995546, 0.20537163881539278, 0.23906456350495484, 0.24068357506579938, 0.15834849724837277, 0.28471102271832466, 0.12909526439770644, 0.1965207946536916, 0.2778139776296989, 0.24398106034742767, 0.31550184854905217, 0.2444684454258182, 0.23202163647559387, 0.26071089054619434, 0.2013751018664359, 0.21500989076277705, 0.2965456958658186, 0.2387903090049392, 0.2248066537864537, 0.1683408882982636, 0.31559837458568807, 0.3930971893240936, 0.2897514212204824, 0.1631284721679722, 0.22003053732448852, 0.3744095491239332, 0.17166333620647162, 0.31447854947001663, 0.2539585288649354, 0.34838219274577975, 0.22070814235115027, 0.3523356488532968, 0.2044939356148295, 0.21669633358392065, 0.29076145347413057, 0.32413645350055287, 0.3396556035993349, 0.18883080597368435, 0.21259762808759672, 0.22952451129005677, 0.21221193880049358, 0.15570086504415384, 0.15582033526207542, 0.25379123195754155, 0.13171312409475996, 0.19411507681717996, 0.19902084669301445, 0.27521703178483237, 0.21175803925581146, 0.24857600046459705, 0.17718210909934817, 0.37151732657310116, 0.2519592473263989, 0.299103143970737, 0.25673678460738, 0.26587434643476404, 0.28791428837555827, 0.19581919204905562, 0.2078861328812431, 0.3025463055827717, 0.3233788860331932, 0.31227106558566037, 0.1709398476236641, 0.32177209729825795, 0.1606614667905177, 0.23451359490015322, 0.29099569248144513, 0.22464039069925804, 0.29044481077417683, 0.23119212100668352, 0.28610681363672413, 0.26015521909899697, 0.12481619235085765, 0.2572190317124946, 0.1997990335402928, 0.27967945922733833, 0.2079373912983638, 0.30359859821311475, 0.2512405893526232, 0.27283518475950147, 0.352950416085165, 0.3187023231089114, 0.3062285130097349, 0.2650964076099973, 0.1995808290107944, 0.27281514156043774, 0.1723055221791423, 0.11590283158084107, 0.14575470835663196, 0.31784049608524395, 0.32110992901826624, 0.23244503867532745, 0.29850202662088987, 0.25325798426731416, 0.2549930603744499, 0.17785388025404816, 0.19478942471358457, 0.15761761024205828, 0.2598535952652151, 0.11686083114121638, 0.27896253853574166, 0.27659521116181923, 0.22444250925097042, 0.4534830249003093, 0.17307225531451356, 0.31575587248455433, 0.18770990441485183, 0.2912702872215563, 0.24288107793908437, 0.2757617365741576, 0.2711229848669743, 0.25616492037309274, 0.20611890350210982, 0.44357362011999507, 0.3035243069594903, 0.28306644467762415, 0.20920766318390596, 0.2871334766820591, 0.18799640347156868, 0.24290377855849818, 0.23198809242719975, 0.20407392701462612, 0.18606495748735705, 0.2500043418199378, 0.21257587247565135, 0.2434277298330621, 0.22265446330966351, 0.2491861691133309, 0.1632533414424762, 0.20918414592521326, 0.24065086493183627, 0.17737383320069558, 0.18573968163510376, 0.11548145196224015, 0.31984218808424836, 0.39897506037605274, 0.27265724132147123, 0.3149430653197066, 0.23773944983137935, 0.2267466384502952, 0.27197788633170517, 0.3100156493786778, 0.2389599751289473, 0.34750373061205053, 0.17686082743401277, 0.29213276673711536, 0.14826631796301284, 0.17913000410044688, 0.24237734391634197, 0.4332212752571546, 0.1963056547902728, 0.19796005847373457, 0.23730322056252529, 0.16601202425590716, 0.14476723092529556, 0.2797811097538328, 0.3132341144875969, 0.3473163442899759, 0.3377144604138998, 0.35175629037362566, 0.3769290145277852, 0.36474209279635195, 0.18574472147646703, 0.28453189137723317, 0.28070596065133063, 0.29388474461453806, 0.21210475744971732, 0.2769173911074197, 0.1899535261220559, 0.19891174107279141, 0.3135913277234971, 0.33852576303894427, 0.15889198602627827, 0.37045530936421434, 0.336470092164706, 0.2725365259441822, 0.2470151428002505, 0.18421211259470724, 0.24865157404389607, 0.3231228292986074, 0.24305076348661112, 0.19473086020863292, 0.3065781631876829, 0.14319694707287653, 0.07525058112538224, 0.2270468469456355, 0.20655436254848583, 0.3764082239284736, 0.19389580041404067, 0.1424116660573351, 0.06345798517547283, 0.23058114587602718, 0.08198291181419662, 0.08148931076970255, 0.16548657191149896, 0.07647092125737379, 0.3046643657222185, 0.19438583049724686, 0.06618820669788923, 0.046194788194846986, 0.20587432285106885, 0.10233799192902927, 0.1348916255894678, 0.09761456825221324, 0.09459634989892088, 0.10956867962674496, 0.09844007125491204, 0.12118136055600184, 0.1892041792655788, 0.05625810597575018, 0.24372812804877306, 0.2503890115490786, 0.1208800933087302, 0.10699640895728867, 0.046938328629353404, 0.07327199593405666, 0.37426495571807467, 0.13822081667680525, 0.12764993377458025, 0.1260794693193535, 0.07786841321495472, 0.12072792411353635, 0.09901506620098907, 0.3701482493960944, 0.19681905227928118, 0.08122718188258383, 0.0962522601512747, 0.10442862044964138, 0.17056242743799238, 0.0474926084610654, 0.18118071434674732, 0.10889700456443423, 0.06705803762977806, 0.25735849410637923, 0.09267063085479484, 0.05347733644208123, 0.05340666143494434, 0.1356698258225501, 0.14964004153171362, 0.2379424916989314, 0.20770118861583609, 0.08878932935540851, 0.14854693329432647, 0.11160789657106715, 0.08326783008889677, 0.1023413819838823, 0.13372759200997933, 0.1873721696892313, 0.06398725433864386, 0.12196514297092581, 0.14807218930508756, 0.10919882472409853, 0.04715577850318314, 0.16611833456536845, 0.06621990731347381, 0.21248786557442043, 0.10070729371984456, 0.14705615153178456, 0.06564137206477898, 0.09415843299850431, 0.20753234022190897, 0.06417029008999292, 0.06577705455054195, 0.21134471110656677, 0.08235570681005798, 0.0594439998387339, 0.045476803594911464, 0.07267158420486919, 0.3003272551243428, 0.19697112016263058, 0.15435409417614362, 0.12312005279521307, 0.14943767511900974, 0.2650187596770493, 0.08025240721190714, 0.07080725520311427, 0.13278100304508936, 0.21719058388398185, 0.1308629324799082, 0.13190906367393068, 0.14988951957828203, 0.13764545648057566, 0.2311273420542594, 0.0761843233147629, 0.10648235559783294, 0.12885409719809565, 0.10149010030664846, 0.1815404062798452, 0.03998078413468937, 0.16527629887068107, 0.07340237220601607, 0.3057887377715287, 0.23193924208972586, 0.19572906858492062, 0.15628865480767298, 0.18162395642933532, 0.09466095814053212, 0.2084950770308246, 0.2050955827769399, 0.0486951750812932, 0.09925462018812016, 0.09974926689145865, 0.07174579391918318, 0.06132694039083082, 0.17659826182409372, 0.1957639447059753, 0.13457222900528584, 0.37350042293197133, 0.21688363457941856, 0.12535357995375232, 0.2351197767084048, 0.24299172178946427, 0.23291919130002767, 0.19587499985621737, 0.04775750126751002, 0.14481127714722336, 0.08155558047792133, 0.17316613797214095, 0.2310233048377091, 0.12661516834465222, 0.04143565716065367, 0.04402860103747716, 0.12936707304377654, 0.10675630031141428, 0.26657160950372344, 0.0986144636689822, 0.05857298306898875, 0.09885011852980281, 0.16053040264435972, 0.07907213209002051, 0.046686921305905245, 0.11230236569086265, 0.17659105504953582, 0.12679017719287053, 0.12139477472702852, 0.2401778236560874, 0.060916860874931436, 0.12885214243104182, 0.3548681569875579, 0.118519328430686, 0.11376010184021752, 0.3622110868406464, 0.0700065433750667, 0.06544912671474935, 0.06671391340912988, 0.06383440831392162, 0.07118493454330266, 0.13547386735552042, 0.14938095148816424, 0.09563145292444622, 0.07123062091477096, 0.22049953554593724, 0.09003648562680615, 0.05670758038969625, 0.08321835140478959, 0.09135255077782908, 0.08129901560408101, 0.15364215606602516, 0.2272531187956203, 0.12330278263182178, 0.0638547620620086, 0.07376383581081401, 0.04131823346431548, 0.06325318064912405, 0.08042268054587534, 0.11869458618457007, 0.06439145215588285, 0.28411070232692354, 0.17109456618928895, 0.051869831753411276, 0.14006683832914796, 0.19009937964392545, 0.1245756486700641, 0.06608014496700361, 0.11435369991414492, 0.0790024743055179, 0.21533953962985092, 0.16758294965689177, 0.21534369386430716, 0.13316483102593293, 0.10557491000555483, 0.09173866168443348, 0.23433445265928016, 0.06553251315194047, 0.2687464445816248, 0.11022547066486492, 0.182988178881254, 0.1085584990283304, 0.20666429440722975, 0.11083475632245686, 0.13622301871199188, 0.11311277304420064, 0.07520302377616601, 0.05473587237008679, 0.18159525957989522, 0.16887538568021818, 0.06877602593324149, 0.16130724807390076, 0.039985216031284715, 0.31201935951419035, 0.07181678210500181, 0.05459120390700714, 0.26333408940561004, 0.05906227896526334, 0.11000785985765575, 0.04065224116925924, 0.18517805880989513, 0.11818163529838062, 0.2108687053938124, 0.2056642594944229, 0.16255106496483496, 0.11019385685968562, 0.15625735764230586, 0.07987596924234565, 0.13516192763471163, 0.2976565388493186, 0.15310686457957165, 0.0659326815714791, 0.10209685979719042, 0.05411811939557338, 0.10658863419114481, 0.1491982188463371, 0.20807218956555695, 0.19139494643840257, 0.04693609519069544, 0.14823780208812287, 0.14724635853728363, 0.09651764147175432, 0.10191682894098336, 0.2371811465088964, 0.18129949556260083, 0.048489983292359415, 0.139254865316193, 0.099498320619816, 0.1060532209808821, 0.07871727623642312, 0.0676100513760989, 0.0567661232205713, 0.07554431014718123, 0.1901282857575483, 0.08983810910003853, 0.1439172747786917, 0.16262887789285224, 0.12680803167839122, 0.20805165659696762, 0.08884266460726789, 0.14971004077320996, 0.2643247508661516, 0.12851848415302738, 0.1886517398557632, 0.06828639936934236, 0.15393900646287378, 0.12126229833308395, 0.1657212572417321, 0.11903788709088337, 0.22130411815075388, 0.1802496551803787, 0.11602488417713974, 0.06836153125117181, 0.16588517649347947, 0.29848454257415286, 0.22043460664927908, 0.05599669130409105, 0.04105443312229569, 0.12392478684360855, 0.18240509556486764, 0.07933818051660702, 0.07010630451927666, 0.24427629261992256, 0.1398517271807001, 0.1325072985207178, 0.06906301732491145, 0.07842435097187374, 0.05781452953319673, 0.16699408606513605, 0.21143868571323618, 0.2742479787275382, 0.10451362503622313, 0.14402525413993858, 0.1095301034778867, 0.07345600716787294, 0.04501679226506855, 0.16970792003381213, 0.23887754219312346, 0.14052113543130904, 0.039027246828169805, 0.1065050345540955, 0.242630491630357, 0.2257450762791743, 0.10221349260973489, 0.2921234483835122, 0.08947528501147879, 0.2165422845269669, 0.13272875495656392, 0.15435996371271146, 0.2056125710324077, 0.24217565790030177, 0.05755077918832612, 0.043385530977667404, 0.06928806563885875, 0.09017697586331244, 0.11683796101732138, 0.14577526883495767, 0.15906538786060903, 0.07896434338753033, 0.10955749746526108, 0.07741707904339953, 0.14713700465203605, 0.03727818006898578, 0.1324184344978224, 0.09558484012086292, 0.060399234916023024, 0.08860832318475398, 0.07339229553642213, 0.05328986541320347, 0.09172342971019405, 0.09843420757881016, 0.15840460587193173, 0.1010284187225148, 0.1838277866229409, 0.06416238072140272, 0.03524200718551699, 0.06891383210283725, 0.0644386266025537, 0.12361092708154386, 0.11073705719341208, 0.08614435317252232, 0.15609893489456408, 0.07230536262576971, 0.05478727739414086, 0.11966415940087574, 0.045869210574310114, 0.05943900838201445, 0.11354254813738492, 0.24091899489565702, 0.0873531445814027, 0.07615756843597461, 0.23475143142364438, 0.043479740366389644, 0.10487961165298795, 0.12532481053310135, 0.1814830673997265, 0.2503826836146326, 0.25561068465799613, 0.1732612084945206, 0.1205250693886647, 0.10556549300423643, 0.023534673424449905, 0.16480256960178716, 0.10710946273017635, 0.20895034611658475, 0.14107249296876986, 0.1590032966911613, 0.06427366852257074, 0.10382391299726297, 0.036991267483414414, 0.18832444161193929, 0.05756087500705912, 0.10504144647773518, 0.06576687340400547, 0.04694153871531615, 0.1361551365935929, 0.19010189288073015, 0.07211677426805477, 0.15850761637792485, 0.10177306316433833, 0.08045982163152386, 0.11641105114899475, 0.08908803150994862, 0.13354383707307949, 0.07003117391684356, 0.04879914326949239, 0.14549362805540808, 0.17913538098922846, 0.06395506347918596, 0.09830874360901855, 0.06727717157102198, 0.1060875626997926, 0.08875286012861046, 0.11163047965976264, 0.07422381806201701, 0.08630131474045934, 0.11905029248847131, 0.177102234888588, 0.1126413441933364, 0.10031074078364759, 0.07823386585851527, 0.07105366172343726, 0.1645504646588416, 0.10713312995787647, 0.09828638768688765, 0.06892238285890222, 0.09591250567556284, 0.07537993703645493, 0.12007853663037578, 0.11661588684700964, 0.2385206200520688, 0.15244703991636516, 0.08611275046511102, 0.10108422798503636, 0.09693516243305446, 0.2975189518835978, 0.19780704335677146, 0.09006837720646357, 0.09941865634534784, 0.12511594712528326, 0.23542090617553924, 0.23630343681915963, 0.12152947264180404, 0.10564635699168984, 0.07442493346077915, 0.22020645444668735, 0.12294311619489191, 0.13544928237386172, 0.15594624067140228, 0.04911549484052504, 0.1267468344121771, 0.07902961777241217, 0.060388615464461226, 0.09955058344300882, 0.06598804703244086, 0.0594701922512149, 0.2898147482633868, 0.10234807671244464, 0.14232888117341808, 0.05567612282700445, 0.0633089482684978, 0.11806432974714037, 0.09825048484732742, 0.15515166951987905, 0.13782184395191563, 0.15921546399663, 0.16302956526607854, 0.05453271458689319, 0.0490131023754469, 0.15708496858611204, 0.13487504069710507, 0.12490640496263287, 0.06316604553201656, 0.14284044932954987, 0.16538724130210858, 0.09518279381282908, 0.07036760544308374, 0.17528091733549805, 0.09731249692557582, 0.20919588884719142]
0.09323563648458022
Making ranges
torch.Size([52024, 2])
We keep 9.12e+06/9.52e+08 =  0% of the original kernel matrix.

torch.Size([1381, 2])
We keep 1.50e+04/1.81e+05 =  8% of the original kernel matrix.

torch.Size([11594, 2])
We keep 4.97e+05/1.31e+07 =  3% of the original kernel matrix.

torch.Size([19569, 2])
We keep 2.89e+06/1.08e+08 =  2% of the original kernel matrix.

torch.Size([32307, 2])
We keep 4.39e+06/3.21e+08 =  1% of the original kernel matrix.

torch.Size([4159, 2])
We keep 1.28e+05/2.66e+06 =  4% of the original kernel matrix.

torch.Size([16977, 2])
We keep 1.19e+06/5.04e+07 =  2% of the original kernel matrix.

torch.Size([6673, 2])
We keep 1.93e+06/1.69e+07 = 11% of the original kernel matrix.

torch.Size([19159, 2])
We keep 2.21e+06/1.27e+08 =  1% of the original kernel matrix.

torch.Size([44067, 2])
We keep 1.29e+07/7.07e+08 =  1% of the original kernel matrix.

torch.Size([50482, 2])
We keep 9.24e+06/8.21e+08 =  1% of the original kernel matrix.

torch.Size([28376, 2])
We keep 4.60e+06/2.41e+08 =  1% of the original kernel matrix.

torch.Size([39259, 2])
We keep 5.92e+06/4.79e+08 =  1% of the original kernel matrix.

torch.Size([68621, 2])
We keep 1.85e+07/1.67e+09 =  1% of the original kernel matrix.

torch.Size([61856, 2])
We keep 1.28e+07/1.26e+09 =  1% of the original kernel matrix.

torch.Size([41953, 2])
We keep 6.60e+06/5.52e+08 =  1% of the original kernel matrix.

torch.Size([49325, 2])
We keep 8.13e+06/7.25e+08 =  1% of the original kernel matrix.

torch.Size([12636, 2])
We keep 1.50e+06/3.74e+07 =  4% of the original kernel matrix.

torch.Size([26285, 2])
We keep 2.95e+06/1.89e+08 =  1% of the original kernel matrix.

torch.Size([12596, 2])
We keep 5.93e+06/1.32e+08 =  4% of the original kernel matrix.

torch.Size([25730, 2])
We keep 4.51e+06/3.54e+08 =  1% of the original kernel matrix.

torch.Size([6307, 2])
We keep 4.03e+05/8.93e+06 =  4% of the original kernel matrix.

torch.Size([19546, 2])
We keep 1.80e+06/9.22e+07 =  1% of the original kernel matrix.

torch.Size([104671, 2])
We keep 6.79e+07/4.30e+09 =  1% of the original kernel matrix.

torch.Size([73671, 2])
We keep 1.89e+07/2.02e+09 =  0% of the original kernel matrix.

torch.Size([11093, 2])
We keep 6.57e+05/2.23e+07 =  2% of the original kernel matrix.

torch.Size([24923, 2])
We keep 2.46e+06/1.46e+08 =  1% of the original kernel matrix.

torch.Size([283700, 2])
We keep 2.86e+08/3.06e+10 =  0% of the original kernel matrix.

torch.Size([126884, 2])
We keep 4.36e+07/5.39e+09 =  0% of the original kernel matrix.

torch.Size([31755, 2])
We keep 4.58e+06/3.00e+08 =  1% of the original kernel matrix.

torch.Size([41837, 2])
We keep 6.37e+06/5.34e+08 =  1% of the original kernel matrix.

torch.Size([36532, 2])
We keep 7.08e+06/4.39e+08 =  1% of the original kernel matrix.

torch.Size([45509, 2])
We keep 7.40e+06/6.46e+08 =  1% of the original kernel matrix.

torch.Size([75411, 2])
We keep 2.30e+07/1.99e+09 =  1% of the original kernel matrix.

torch.Size([64825, 2])
We keep 1.36e+07/1.38e+09 =  0% of the original kernel matrix.

torch.Size([53855, 2])
We keep 1.46e+07/1.05e+09 =  1% of the original kernel matrix.

torch.Size([54568, 2])
We keep 1.06e+07/1.00e+09 =  1% of the original kernel matrix.

torch.Size([189728, 2])
We keep 1.33e+08/1.31e+10 =  1% of the original kernel matrix.

torch.Size([101949, 2])
We keep 3.02e+07/3.53e+09 =  0% of the original kernel matrix.

torch.Size([153745, 2])
We keep 1.03e+08/8.06e+09 =  1% of the original kernel matrix.

torch.Size([90941, 2])
We keep 2.46e+07/2.77e+09 =  0% of the original kernel matrix.

torch.Size([10910, 2])
We keep 1.90e+06/4.86e+07 =  3% of the original kernel matrix.

torch.Size([23260, 2])
We keep 3.26e+06/2.15e+08 =  1% of the original kernel matrix.

torch.Size([279762, 2])
We keep 1.92e+08/2.75e+10 =  0% of the original kernel matrix.

torch.Size([126364, 2])
We keep 4.06e+07/5.12e+09 =  0% of the original kernel matrix.

torch.Size([17926, 2])
We keep 1.61e+06/6.77e+07 =  2% of the original kernel matrix.

torch.Size([31056, 2])
We keep 3.61e+06/2.54e+08 =  1% of the original kernel matrix.

torch.Size([35494, 2])
We keep 2.14e+07/4.70e+08 =  4% of the original kernel matrix.

torch.Size([44333, 2])
We keep 7.80e+06/6.69e+08 =  1% of the original kernel matrix.

torch.Size([9090, 2])
We keep 7.69e+06/7.39e+07 = 10% of the original kernel matrix.

torch.Size([20825, 2])
We keep 3.72e+06/2.65e+08 =  1% of the original kernel matrix.

torch.Size([105104, 2])
We keep 3.27e+07/3.62e+09 =  0% of the original kernel matrix.

torch.Size([75622, 2])
We keep 1.73e+07/1.86e+09 =  0% of the original kernel matrix.

torch.Size([59412, 2])
We keep 2.70e+07/1.81e+09 =  1% of the original kernel matrix.

torch.Size([55732, 2])
We keep 1.32e+07/1.31e+09 =  1% of the original kernel matrix.

torch.Size([10353, 2])
We keep 7.08e+05/1.89e+07 =  3% of the original kernel matrix.

torch.Size([24354, 2])
We keep 2.27e+06/1.34e+08 =  1% of the original kernel matrix.

torch.Size([61753, 2])
We keep 6.59e+07/2.11e+09 =  3% of the original kernel matrix.

torch.Size([57246, 2])
We keep 1.42e+07/1.42e+09 =  1% of the original kernel matrix.

torch.Size([1590782, 2])
We keep 3.47e+09/8.14e+11 =  0% of the original kernel matrix.

torch.Size([315504, 2])
We keep 1.89e+08/2.78e+10 =  0% of the original kernel matrix.

torch.Size([9246, 2])
We keep 9.87e+05/2.20e+07 =  4% of the original kernel matrix.

torch.Size([22649, 2])
We keep 2.44e+06/1.45e+08 =  1% of the original kernel matrix.

torch.Size([640782, 2])
We keep 2.18e+09/2.41e+11 =  0% of the original kernel matrix.

torch.Size([186855, 2])
We keep 1.10e+08/1.51e+10 =  0% of the original kernel matrix.

torch.Size([17700, 2])
We keep 8.96e+06/1.51e+08 =  5% of the original kernel matrix.

torch.Size([30311, 2])
We keep 4.98e+06/3.79e+08 =  1% of the original kernel matrix.

torch.Size([11563, 2])
We keep 7.27e+05/2.32e+07 =  3% of the original kernel matrix.

torch.Size([25429, 2])
We keep 2.47e+06/1.49e+08 =  1% of the original kernel matrix.

torch.Size([9647, 2])
We keep 8.62e+05/2.32e+07 =  3% of the original kernel matrix.

torch.Size([23040, 2])
We keep 2.47e+06/1.49e+08 =  1% of the original kernel matrix.

torch.Size([99620, 2])
We keep 8.80e+07/5.16e+09 =  1% of the original kernel matrix.

torch.Size([69831, 2])
We keep 2.05e+07/2.22e+09 =  0% of the original kernel matrix.

torch.Size([111106, 2])
We keep 5.07e+07/4.31e+09 =  1% of the original kernel matrix.

torch.Size([77539, 2])
We keep 1.89e+07/2.02e+09 =  0% of the original kernel matrix.

torch.Size([2393, 2])
We keep 5.23e+04/8.17e+05 =  6% of the original kernel matrix.

torch.Size([13572, 2])
We keep 8.03e+05/2.79e+07 =  2% of the original kernel matrix.

torch.Size([12135, 2])
We keep 9.47e+05/3.02e+07 =  3% of the original kernel matrix.

torch.Size([25843, 2])
We keep 2.78e+06/1.70e+08 =  1% of the original kernel matrix.

torch.Size([4108, 2])
We keep 1.88e+05/3.48e+06 =  5% of the original kernel matrix.

torch.Size([16283, 2])
We keep 1.28e+06/5.76e+07 =  2% of the original kernel matrix.

torch.Size([6235, 2])
We keep 2.31e+05/6.06e+06 =  3% of the original kernel matrix.

torch.Size([19662, 2])
We keep 1.54e+06/7.60e+07 =  2% of the original kernel matrix.

torch.Size([1494, 2])
We keep 4.92e+04/5.76e+05 =  8% of the original kernel matrix.

torch.Size([10422, 2])
We keep 6.97e+05/2.34e+07 =  2% of the original kernel matrix.

torch.Size([1657, 2])
We keep 7.52e+04/4.80e+05 = 15% of the original kernel matrix.

torch.Size([11032, 2])
We keep 6.68e+05/2.14e+07 =  3% of the original kernel matrix.

torch.Size([6942, 2])
We keep 3.34e+05/8.42e+06 =  3% of the original kernel matrix.

torch.Size([20235, 2])
We keep 1.75e+06/8.95e+07 =  1% of the original kernel matrix.

torch.Size([1181, 2])
We keep 1.07e+04/1.14e+05 =  9% of the original kernel matrix.

torch.Size([10851, 2])
We keep 4.27e+05/1.04e+07 =  4% of the original kernel matrix.

torch.Size([1352, 2])
We keep 2.29e+04/2.75e+05 =  8% of the original kernel matrix.

torch.Size([11039, 2])
We keep 5.71e+05/1.62e+07 =  3% of the original kernel matrix.

torch.Size([6925, 2])
We keep 3.56e+05/7.77e+06 =  4% of the original kernel matrix.

torch.Size([20555, 2])
We keep 1.67e+06/8.60e+07 =  1% of the original kernel matrix.

torch.Size([4871, 2])
We keep 1.86e+05/4.00e+06 =  4% of the original kernel matrix.

torch.Size([17742, 2])
We keep 1.37e+06/6.17e+07 =  2% of the original kernel matrix.

torch.Size([4146, 2])
We keep 1.32e+05/2.70e+06 =  4% of the original kernel matrix.

torch.Size([16585, 2])
We keep 1.20e+06/5.07e+07 =  2% of the original kernel matrix.

torch.Size([14335, 2])
We keep 3.08e+06/8.52e+07 =  3% of the original kernel matrix.

torch.Size([27124, 2])
We keep 3.98e+06/2.85e+08 =  1% of the original kernel matrix.

torch.Size([16610, 2])
We keep 1.92e+06/6.56e+07 =  2% of the original kernel matrix.

torch.Size([29886, 2])
We keep 3.65e+06/2.50e+08 =  1% of the original kernel matrix.

torch.Size([5678, 2])
We keep 2.30e+05/5.29e+06 =  4% of the original kernel matrix.

torch.Size([18765, 2])
We keep 1.48e+06/7.10e+07 =  2% of the original kernel matrix.

torch.Size([6959, 2])
We keep 3.65e+05/9.17e+06 =  3% of the original kernel matrix.

torch.Size([20337, 2])
We keep 1.79e+06/9.34e+07 =  1% of the original kernel matrix.

torch.Size([5086, 2])
We keep 1.93e+05/3.96e+06 =  4% of the original kernel matrix.

torch.Size([18225, 2])
We keep 1.37e+06/6.14e+07 =  2% of the original kernel matrix.

torch.Size([10772, 2])
We keep 6.23e+05/1.97e+07 =  3% of the original kernel matrix.

torch.Size([24630, 2])
We keep 2.33e+06/1.37e+08 =  1% of the original kernel matrix.

torch.Size([5494, 2])
We keep 5.41e+05/9.51e+06 =  5% of the original kernel matrix.

torch.Size([17481, 2])
We keep 1.81e+06/9.52e+07 =  1% of the original kernel matrix.

torch.Size([3086, 2])
We keep 7.72e+04/1.38e+06 =  5% of the original kernel matrix.

torch.Size([15128, 2])
We keep 9.55e+05/3.63e+07 =  2% of the original kernel matrix.

torch.Size([6540, 2])
We keep 2.58e+05/6.83e+06 =  3% of the original kernel matrix.

torch.Size([20114, 2])
We keep 1.62e+06/8.07e+07 =  2% of the original kernel matrix.

torch.Size([4240, 2])
We keep 1.16e+05/2.55e+06 =  4% of the original kernel matrix.

torch.Size([17007, 2])
We keep 1.15e+06/4.93e+07 =  2% of the original kernel matrix.

torch.Size([4164, 2])
We keep 1.29e+05/2.53e+06 =  5% of the original kernel matrix.

torch.Size([16716, 2])
We keep 1.17e+06/4.91e+07 =  2% of the original kernel matrix.

torch.Size([12632, 2])
We keep 1.02e+06/3.42e+07 =  2% of the original kernel matrix.

torch.Size([26301, 2])
We keep 2.84e+06/1.81e+08 =  1% of the original kernel matrix.

torch.Size([2471, 2])
We keep 5.60e+04/8.97e+05 =  6% of the original kernel matrix.

torch.Size([13796, 2])
We keep 8.15e+05/2.92e+07 =  2% of the original kernel matrix.

torch.Size([22091, 2])
We keep 2.79e+06/1.26e+08 =  2% of the original kernel matrix.

torch.Size([34582, 2])
We keep 4.70e+06/3.47e+08 =  1% of the original kernel matrix.

torch.Size([7362, 2])
We keep 3.30e+05/8.44e+06 =  3% of the original kernel matrix.

torch.Size([21080, 2])
We keep 1.74e+06/8.97e+07 =  1% of the original kernel matrix.

torch.Size([2685, 2])
We keep 7.16e+04/1.09e+06 =  6% of the original kernel matrix.

torch.Size([14307, 2])
We keep 8.99e+05/3.22e+07 =  2% of the original kernel matrix.

torch.Size([3791, 2])
We keep 1.19e+05/2.37e+06 =  5% of the original kernel matrix.

torch.Size([16112, 2])
We keep 1.14e+06/4.75e+07 =  2% of the original kernel matrix.

torch.Size([1899, 2])
We keep 3.68e+04/5.01e+05 =  7% of the original kernel matrix.

torch.Size([12557, 2])
We keep 6.96e+05/2.18e+07 =  3% of the original kernel matrix.

torch.Size([3916, 2])
We keep 1.10e+05/2.21e+06 =  4% of the original kernel matrix.

torch.Size([16323, 2])
We keep 1.11e+06/4.59e+07 =  2% of the original kernel matrix.

torch.Size([4532, 2])
We keep 1.57e+05/3.27e+06 =  4% of the original kernel matrix.

torch.Size([17509, 2])
We keep 1.26e+06/5.58e+07 =  2% of the original kernel matrix.

torch.Size([3459, 2])
We keep 8.57e+04/1.59e+06 =  5% of the original kernel matrix.

torch.Size([15684, 2])
We keep 9.87e+05/3.89e+07 =  2% of the original kernel matrix.

torch.Size([5943, 2])
We keep 4.77e+05/7.29e+06 =  6% of the original kernel matrix.

torch.Size([18535, 2])
We keep 1.67e+06/8.33e+07 =  2% of the original kernel matrix.

torch.Size([5924, 2])
We keep 2.04e+05/4.91e+06 =  4% of the original kernel matrix.

torch.Size([19234, 2])
We keep 1.44e+06/6.84e+07 =  2% of the original kernel matrix.

torch.Size([2456, 2])
We keep 4.53e+04/7.29e+05 =  6% of the original kernel matrix.

torch.Size([13958, 2])
We keep 7.77e+05/2.64e+07 =  2% of the original kernel matrix.

torch.Size([3554, 2])
We keep 1.52e+05/2.62e+06 =  5% of the original kernel matrix.

torch.Size([15224, 2])
We keep 1.16e+06/5.00e+07 =  2% of the original kernel matrix.

torch.Size([4997, 2])
We keep 1.78e+05/4.10e+06 =  4% of the original kernel matrix.

torch.Size([18006, 2])
We keep 1.36e+06/6.25e+07 =  2% of the original kernel matrix.

torch.Size([9962, 2])
We keep 9.33e+05/2.20e+07 =  4% of the original kernel matrix.

torch.Size([23388, 2])
We keep 2.47e+06/1.45e+08 =  1% of the original kernel matrix.

torch.Size([2147, 2])
We keep 3.61e+04/5.03e+05 =  7% of the original kernel matrix.

torch.Size([13227, 2])
We keep 6.91e+05/2.19e+07 =  3% of the original kernel matrix.

torch.Size([1061, 2])
We keep 1.22e+04/1.31e+05 =  9% of the original kernel matrix.

torch.Size([10214, 2])
We keep 4.54e+05/1.12e+07 =  4% of the original kernel matrix.

torch.Size([2391, 2])
We keep 4.94e+04/8.14e+05 =  6% of the original kernel matrix.

torch.Size([13637, 2])
We keep 7.98e+05/2.78e+07 =  2% of the original kernel matrix.

torch.Size([12173, 2])
We keep 8.92e+05/3.04e+07 =  2% of the original kernel matrix.

torch.Size([25754, 2])
We keep 2.80e+06/1.70e+08 =  1% of the original kernel matrix.

torch.Size([5554, 2])
We keep 1.77e+05/4.34e+06 =  4% of the original kernel matrix.

torch.Size([18881, 2])
We keep 1.39e+06/6.43e+07 =  2% of the original kernel matrix.

torch.Size([1084, 2])
We keep 1.47e+04/1.56e+05 =  9% of the original kernel matrix.

torch.Size([10020, 2])
We keep 4.73e+05/1.22e+07 =  3% of the original kernel matrix.

torch.Size([10664, 2])
We keep 7.26e+05/2.16e+07 =  3% of the original kernel matrix.

torch.Size([24492, 2])
We keep 2.44e+06/1.44e+08 =  1% of the original kernel matrix.

torch.Size([2005, 2])
We keep 3.35e+04/4.68e+05 =  7% of the original kernel matrix.

torch.Size([12863, 2])
We keep 6.74e+05/2.11e+07 =  3% of the original kernel matrix.

torch.Size([3433, 2])
We keep 9.43e+04/1.79e+06 =  5% of the original kernel matrix.

torch.Size([15536, 2])
We keep 1.03e+06/4.13e+07 =  2% of the original kernel matrix.

torch.Size([1489, 2])
We keep 1.98e+04/2.52e+05 =  7% of the original kernel matrix.

torch.Size([11681, 2])
We keep 5.53e+05/1.55e+07 =  3% of the original kernel matrix.

torch.Size([4364, 2])
We keep 2.40e+05/4.15e+06 =  5% of the original kernel matrix.

torch.Size([16406, 2])
We keep 1.37e+06/6.29e+07 =  2% of the original kernel matrix.

torch.Size([1510, 2])
We keep 2.07e+04/2.41e+05 =  8% of the original kernel matrix.

torch.Size([11687, 2])
We keep 5.51e+05/1.52e+07 =  3% of the original kernel matrix.

torch.Size([6501, 2])
We keep 3.06e+05/7.73e+06 =  3% of the original kernel matrix.

torch.Size([20118, 2])
We keep 1.72e+06/8.58e+07 =  2% of the original kernel matrix.

torch.Size([5702, 2])
We keep 2.21e+05/4.87e+06 =  4% of the original kernel matrix.

torch.Size([19136, 2])
We keep 1.45e+06/6.81e+07 =  2% of the original kernel matrix.

torch.Size([2621, 2])
We keep 4.36e+04/7.59e+05 =  5% of the original kernel matrix.

torch.Size([14380, 2])
We keep 7.69e+05/2.69e+07 =  2% of the original kernel matrix.

torch.Size([1949, 2])
We keep 2.98e+04/4.25e+05 =  7% of the original kernel matrix.

torch.Size([13000, 2])
We keep 6.49e+05/2.01e+07 =  3% of the original kernel matrix.

torch.Size([1597, 2])
We keep 2.42e+04/3.21e+05 =  7% of the original kernel matrix.

torch.Size([12002, 2])
We keep 5.99e+05/1.75e+07 =  3% of the original kernel matrix.

torch.Size([7468, 2])
We keep 4.67e+05/1.16e+07 =  4% of the original kernel matrix.

torch.Size([20813, 2])
We keep 1.97e+06/1.05e+08 =  1% of the original kernel matrix.

torch.Size([5441, 2])
We keep 2.27e+05/5.40e+06 =  4% of the original kernel matrix.

torch.Size([18524, 2])
We keep 1.49e+06/7.17e+07 =  2% of the original kernel matrix.

torch.Size([4525, 2])
We keep 1.55e+05/3.37e+06 =  4% of the original kernel matrix.

torch.Size([17170, 2])
We keep 1.28e+06/5.67e+07 =  2% of the original kernel matrix.

torch.Size([5891, 2])
We keep 2.23e+05/5.41e+06 =  4% of the original kernel matrix.

torch.Size([19200, 2])
We keep 1.50e+06/7.18e+07 =  2% of the original kernel matrix.

torch.Size([11318, 2])
We keep 1.41e+06/3.59e+07 =  3% of the original kernel matrix.

torch.Size([24655, 2])
We keep 2.91e+06/1.85e+08 =  1% of the original kernel matrix.

torch.Size([13542, 2])
We keep 9.71e+05/3.52e+07 =  2% of the original kernel matrix.

torch.Size([27379, 2])
We keep 2.88e+06/1.83e+08 =  1% of the original kernel matrix.

torch.Size([3894, 2])
We keep 8.88e+04/1.89e+06 =  4% of the original kernel matrix.

torch.Size([16731, 2])
We keep 1.04e+06/4.25e+07 =  2% of the original kernel matrix.

torch.Size([19232, 2])
We keep 2.22e+06/1.02e+08 =  2% of the original kernel matrix.

torch.Size([32186, 2])
We keep 4.28e+06/3.11e+08 =  1% of the original kernel matrix.

torch.Size([7422, 2])
We keep 4.13e+05/9.47e+06 =  4% of the original kernel matrix.

torch.Size([21116, 2])
We keep 1.82e+06/9.49e+07 =  1% of the original kernel matrix.

torch.Size([7156, 2])
We keep 3.05e+05/8.00e+06 =  3% of the original kernel matrix.

torch.Size([20852, 2])
We keep 1.70e+06/8.73e+07 =  1% of the original kernel matrix.

torch.Size([2835, 2])
We keep 6.72e+04/1.05e+06 =  6% of the original kernel matrix.

torch.Size([14438, 2])
We keep 8.67e+05/3.16e+07 =  2% of the original kernel matrix.

torch.Size([5232, 2])
We keep 2.77e+05/5.19e+06 =  5% of the original kernel matrix.

torch.Size([17952, 2])
We keep 1.47e+06/7.03e+07 =  2% of the original kernel matrix.

torch.Size([3531, 2])
We keep 1.18e+05/2.10e+06 =  5% of the original kernel matrix.

torch.Size([15659, 2])
We keep 1.09e+06/4.47e+07 =  2% of the original kernel matrix.

torch.Size([9960, 2])
We keep 5.18e+05/1.62e+07 =  3% of the original kernel matrix.

torch.Size([23812, 2])
We keep 2.18e+06/1.24e+08 =  1% of the original kernel matrix.

torch.Size([1291, 2])
We keep 2.18e+04/1.86e+05 = 11% of the original kernel matrix.

torch.Size([11015, 2])
We keep 5.16e+05/1.33e+07 =  3% of the original kernel matrix.

torch.Size([3490, 2])
We keep 9.23e+04/1.89e+06 =  4% of the original kernel matrix.

torch.Size([15647, 2])
We keep 1.05e+06/4.25e+07 =  2% of the original kernel matrix.

torch.Size([2333, 2])
We keep 4.63e+04/6.84e+05 =  6% of the original kernel matrix.

torch.Size([13657, 2])
We keep 7.54e+05/2.55e+07 =  2% of the original kernel matrix.

torch.Size([3288, 2])
We keep 1.10e+05/1.71e+06 =  6% of the original kernel matrix.

torch.Size([15294, 2])
We keep 1.03e+06/4.03e+07 =  2% of the original kernel matrix.

torch.Size([2773, 2])
We keep 9.08e+04/1.34e+06 =  6% of the original kernel matrix.

torch.Size([14068, 2])
We keep 9.39e+05/3.57e+07 =  2% of the original kernel matrix.

torch.Size([2384, 2])
We keep 5.25e+04/8.08e+05 =  6% of the original kernel matrix.

torch.Size([13525, 2])
We keep 7.95e+05/2.77e+07 =  2% of the original kernel matrix.

torch.Size([7273, 2])
We keep 3.20e+05/8.89e+06 =  3% of the original kernel matrix.

torch.Size([20907, 2])
We keep 1.76e+06/9.20e+07 =  1% of the original kernel matrix.

torch.Size([5612, 2])
We keep 2.90e+05/6.56e+06 =  4% of the original kernel matrix.

torch.Size([18498, 2])
We keep 1.61e+06/7.91e+07 =  2% of the original kernel matrix.

torch.Size([1750, 2])
We keep 5.24e+04/6.24e+05 =  8% of the original kernel matrix.

torch.Size([11397, 2])
We keep 7.34e+05/2.44e+07 =  3% of the original kernel matrix.

torch.Size([1794, 2])
We keep 2.79e+04/4.21e+05 =  6% of the original kernel matrix.

torch.Size([12335, 2])
We keep 6.46e+05/2.00e+07 =  3% of the original kernel matrix.

torch.Size([2197, 2])
We keep 3.17e+04/4.90e+05 =  6% of the original kernel matrix.

torch.Size([13356, 2])
We keep 6.73e+05/2.16e+07 =  3% of the original kernel matrix.

torch.Size([9614, 2])
We keep 7.20e+05/2.02e+07 =  3% of the original kernel matrix.

torch.Size([23046, 2])
We keep 2.37e+06/1.39e+08 =  1% of the original kernel matrix.

torch.Size([2048, 2])
We keep 2.93e+04/4.33e+05 =  6% of the original kernel matrix.

torch.Size([13067, 2])
We keep 6.59e+05/2.03e+07 =  3% of the original kernel matrix.

torch.Size([12991, 2])
We keep 8.06e+05/2.93e+07 =  2% of the original kernel matrix.

torch.Size([26582, 2])
We keep 2.67e+06/1.67e+08 =  1% of the original kernel matrix.

torch.Size([4229, 2])
We keep 1.42e+05/2.86e+06 =  4% of the original kernel matrix.

torch.Size([16808, 2])
We keep 1.20e+06/5.22e+07 =  2% of the original kernel matrix.

torch.Size([2502, 2])
We keep 4.75e+04/7.57e+05 =  6% of the original kernel matrix.

torch.Size([13896, 2])
We keep 7.76e+05/2.68e+07 =  2% of the original kernel matrix.

torch.Size([4856, 2])
We keep 1.86e+05/4.09e+06 =  4% of the original kernel matrix.

torch.Size([17724, 2])
We keep 1.37e+06/6.24e+07 =  2% of the original kernel matrix.

torch.Size([2412, 2])
We keep 5.06e+04/8.10e+05 =  6% of the original kernel matrix.

torch.Size([13808, 2])
We keep 8.07e+05/2.78e+07 =  2% of the original kernel matrix.

torch.Size([4514, 2])
We keep 1.48e+05/3.26e+06 =  4% of the original kernel matrix.

torch.Size([17257, 2])
We keep 1.27e+06/5.57e+07 =  2% of the original kernel matrix.

torch.Size([2596, 2])
We keep 5.18e+04/9.06e+05 =  5% of the original kernel matrix.

torch.Size([14259, 2])
We keep 8.25e+05/2.94e+07 =  2% of the original kernel matrix.

torch.Size([3325, 2])
We keep 7.35e+04/1.54e+06 =  4% of the original kernel matrix.

torch.Size([15508, 2])
We keep 9.71e+05/3.82e+07 =  2% of the original kernel matrix.

torch.Size([24429, 2])
We keep 2.60e+06/1.45e+08 =  1% of the original kernel matrix.

torch.Size([36528, 2])
We keep 4.84e+06/3.72e+08 =  1% of the original kernel matrix.

torch.Size([3488, 2])
We keep 9.10e+04/1.73e+06 =  5% of the original kernel matrix.

torch.Size([15797, 2])
We keep 1.02e+06/4.06e+07 =  2% of the original kernel matrix.

torch.Size([6603, 2])
We keep 3.51e+05/7.91e+06 =  4% of the original kernel matrix.

torch.Size([19946, 2])
We keep 1.70e+06/8.68e+07 =  1% of the original kernel matrix.

torch.Size([2593, 2])
We keep 7.23e+04/1.04e+06 =  6% of the original kernel matrix.

torch.Size([13876, 2])
We keep 8.69e+05/3.15e+07 =  2% of the original kernel matrix.

torch.Size([6217, 2])
We keep 2.49e+05/6.17e+06 =  4% of the original kernel matrix.

torch.Size([19560, 2])
We keep 1.55e+06/7.66e+07 =  2% of the original kernel matrix.

torch.Size([1859, 2])
We keep 4.89e+04/5.88e+05 =  8% of the original kernel matrix.

torch.Size([11982, 2])
We keep 7.15e+05/2.37e+07 =  3% of the original kernel matrix.

torch.Size([3759, 2])
We keep 1.84e+05/1.90e+06 =  9% of the original kernel matrix.

torch.Size([16194, 2])
We keep 1.06e+06/4.26e+07 =  2% of the original kernel matrix.

torch.Size([2957, 2])
We keep 6.38e+04/1.15e+06 =  5% of the original kernel matrix.

torch.Size([14833, 2])
We keep 8.82e+05/3.31e+07 =  2% of the original kernel matrix.

torch.Size([1618, 2])
We keep 2.25e+04/2.74e+05 =  8% of the original kernel matrix.

torch.Size([12137, 2])
We keep 5.87e+05/1.61e+07 =  3% of the original kernel matrix.

torch.Size([1873, 2])
We keep 3.10e+04/4.61e+05 =  6% of the original kernel matrix.

torch.Size([12489, 2])
We keep 6.69e+05/2.10e+07 =  3% of the original kernel matrix.

torch.Size([2183, 2])
We keep 3.92e+04/5.87e+05 =  6% of the original kernel matrix.

torch.Size([13344, 2])
We keep 7.29e+05/2.36e+07 =  3% of the original kernel matrix.

torch.Size([3199, 2])
We keep 8.18e+04/1.43e+06 =  5% of the original kernel matrix.

torch.Size([15252, 2])
We keep 9.61e+05/3.68e+07 =  2% of the original kernel matrix.

torch.Size([6981, 2])
We keep 3.13e+05/7.77e+06 =  4% of the original kernel matrix.

torch.Size([20652, 2])
We keep 1.69e+06/8.60e+07 =  1% of the original kernel matrix.

torch.Size([2632, 2])
We keep 7.97e+04/1.14e+06 =  6% of the original kernel matrix.

torch.Size([13735, 2])
We keep 9.00e+05/3.30e+07 =  2% of the original kernel matrix.

torch.Size([10460, 2])
We keep 7.12e+05/2.19e+07 =  3% of the original kernel matrix.

torch.Size([24178, 2])
We keep 2.47e+06/1.44e+08 =  1% of the original kernel matrix.

torch.Size([26311, 2])
We keep 4.47e+06/2.22e+08 =  2% of the original kernel matrix.

torch.Size([37736, 2])
We keep 5.77e+06/4.59e+08 =  1% of the original kernel matrix.

torch.Size([16183, 2])
We keep 1.60e+06/5.99e+07 =  2% of the original kernel matrix.

torch.Size([29680, 2])
We keep 3.55e+06/2.39e+08 =  1% of the original kernel matrix.

torch.Size([2034, 2])
We keep 3.10e+04/4.64e+05 =  6% of the original kernel matrix.

torch.Size([13053, 2])
We keep 6.73e+05/2.10e+07 =  3% of the original kernel matrix.

torch.Size([2031, 2])
We keep 3.10e+04/4.29e+05 =  7% of the original kernel matrix.

torch.Size([13084, 2])
We keep 6.52e+05/2.02e+07 =  3% of the original kernel matrix.

torch.Size([4050, 2])
We keep 1.89e+05/2.88e+06 =  6% of the original kernel matrix.

torch.Size([16253, 2])
We keep 1.19e+06/5.24e+07 =  2% of the original kernel matrix.

torch.Size([2125, 2])
We keep 4.60e+04/6.76e+05 =  6% of the original kernel matrix.

torch.Size([12949, 2])
We keep 7.55e+05/2.54e+07 =  2% of the original kernel matrix.

torch.Size([3531, 2])
We keep 1.01e+05/1.80e+06 =  5% of the original kernel matrix.

torch.Size([15677, 2])
We keep 1.03e+06/4.14e+07 =  2% of the original kernel matrix.

torch.Size([3293, 2])
We keep 8.46e+04/1.62e+06 =  5% of the original kernel matrix.

torch.Size([15001, 2])
We keep 9.73e+05/3.93e+07 =  2% of the original kernel matrix.

torch.Size([9417, 2])
We keep 6.11e+05/1.65e+07 =  3% of the original kernel matrix.

torch.Size([23282, 2])
We keep 2.20e+06/1.25e+08 =  1% of the original kernel matrix.

torch.Size([7918, 2])
We keep 3.25e+05/9.73e+06 =  3% of the original kernel matrix.

torch.Size([21924, 2])
We keep 1.82e+06/9.63e+07 =  1% of the original kernel matrix.

torch.Size([9417, 2])
We keep 1.61e+06/3.30e+07 =  4% of the original kernel matrix.

torch.Size([22224, 2])
We keep 2.83e+06/1.77e+08 =  1% of the original kernel matrix.

torch.Size([3347, 2])
We keep 7.69e+04/1.50e+06 =  5% of the original kernel matrix.

torch.Size([15558, 2])
We keep 9.74e+05/3.78e+07 =  2% of the original kernel matrix.

torch.Size([27742, 2])
We keep 4.28e+06/2.08e+08 =  2% of the original kernel matrix.

torch.Size([39218, 2])
We keep 5.61e+06/4.45e+08 =  1% of the original kernel matrix.

torch.Size([3095, 2])
We keep 5.41e+04/9.84e+05 =  5% of the original kernel matrix.

torch.Size([15426, 2])
We keep 8.46e+05/3.06e+07 =  2% of the original kernel matrix.

torch.Size([2723, 2])
We keep 6.88e+04/1.08e+06 =  6% of the original kernel matrix.

torch.Size([14233, 2])
We keep 8.75e+05/3.20e+07 =  2% of the original kernel matrix.

torch.Size([4907, 2])
We keep 1.82e+05/3.87e+06 =  4% of the original kernel matrix.

torch.Size([17853, 2])
We keep 1.33e+06/6.07e+07 =  2% of the original kernel matrix.

torch.Size([703, 2])
We keep 9.26e+03/4.84e+04 = 19% of the original kernel matrix.

torch.Size([8694, 2])
We keep 3.32e+05/6.79e+06 =  4% of the original kernel matrix.

torch.Size([9997, 2])
We keep 8.51e+05/2.11e+07 =  4% of the original kernel matrix.

torch.Size([23815, 2])
We keep 2.44e+06/1.42e+08 =  1% of the original kernel matrix.

torch.Size([2098, 2])
We keep 3.74e+04/4.91e+05 =  7% of the original kernel matrix.

torch.Size([13172, 2])
We keep 6.84e+05/2.16e+07 =  3% of the original kernel matrix.

torch.Size([7550, 2])
We keep 5.23e+05/1.14e+07 =  4% of the original kernel matrix.

torch.Size([21059, 2])
We keep 1.95e+06/1.04e+08 =  1% of the original kernel matrix.

torch.Size([2489, 2])
We keep 5.00e+04/7.69e+05 =  6% of the original kernel matrix.

torch.Size([13876, 2])
We keep 7.90e+05/2.71e+07 =  2% of the original kernel matrix.

torch.Size([3361, 2])
We keep 1.33e+05/2.37e+06 =  5% of the original kernel matrix.

torch.Size([14942, 2])
We keep 1.14e+06/4.75e+07 =  2% of the original kernel matrix.

torch.Size([3012, 2])
We keep 6.65e+04/1.11e+06 =  6% of the original kernel matrix.

torch.Size([15132, 2])
We keep 8.89e+05/3.25e+07 =  2% of the original kernel matrix.

torch.Size([3081, 2])
We keep 6.66e+04/1.25e+06 =  5% of the original kernel matrix.

torch.Size([15333, 2])
We keep 9.15e+05/3.44e+07 =  2% of the original kernel matrix.

torch.Size([3325, 2])
We keep 8.14e+04/1.67e+06 =  4% of the original kernel matrix.

torch.Size([15441, 2])
We keep 9.99e+05/3.99e+07 =  2% of the original kernel matrix.

torch.Size([6445, 2])
We keep 2.65e+05/6.40e+06 =  4% of the original kernel matrix.

torch.Size([19904, 2])
We keep 1.59e+06/7.80e+07 =  2% of the original kernel matrix.

torch.Size([867, 2])
We keep 7.24e+03/5.34e+04 = 13% of the original kernel matrix.

torch.Size([9602, 2])
We keep 3.44e+05/7.13e+06 =  4% of the original kernel matrix.

torch.Size([2188, 2])
We keep 3.73e+04/5.67e+05 =  6% of the original kernel matrix.

torch.Size([13290, 2])
We keep 6.97e+05/2.32e+07 =  3% of the original kernel matrix.

torch.Size([2449, 2])
We keep 5.57e+04/8.34e+05 =  6% of the original kernel matrix.

torch.Size([13492, 2])
We keep 7.97e+05/2.82e+07 =  2% of the original kernel matrix.

torch.Size([6224, 2])
We keep 2.46e+05/6.17e+06 =  3% of the original kernel matrix.

torch.Size([19783, 2])
We keep 1.56e+06/7.66e+07 =  2% of the original kernel matrix.

torch.Size([2604, 2])
We keep 5.70e+04/8.24e+05 =  6% of the original kernel matrix.

torch.Size([14266, 2])
We keep 8.05e+05/2.80e+07 =  2% of the original kernel matrix.

torch.Size([7948, 2])
We keep 4.76e+05/1.21e+07 =  3% of the original kernel matrix.

torch.Size([21689, 2])
We keep 1.97e+06/1.07e+08 =  1% of the original kernel matrix.

torch.Size([3991, 2])
We keep 2.04e+05/2.47e+06 =  8% of the original kernel matrix.

torch.Size([16621, 2])
We keep 1.16e+06/4.85e+07 =  2% of the original kernel matrix.

torch.Size([4406, 2])
We keep 1.52e+05/3.06e+06 =  4% of the original kernel matrix.

torch.Size([17014, 2])
We keep 1.23e+06/5.40e+07 =  2% of the original kernel matrix.

torch.Size([6054, 2])
We keep 2.86e+05/6.71e+06 =  4% of the original kernel matrix.

torch.Size([19292, 2])
We keep 1.60e+06/7.99e+07 =  1% of the original kernel matrix.

torch.Size([6968, 2])
We keep 5.25e+05/1.19e+07 =  4% of the original kernel matrix.

torch.Size([20115, 2])
We keep 1.95e+06/1.07e+08 =  1% of the original kernel matrix.

torch.Size([3610, 2])
We keep 1.31e+05/1.93e+06 =  6% of the original kernel matrix.

torch.Size([15787, 2])
We keep 1.06e+06/4.29e+07 =  2% of the original kernel matrix.

torch.Size([5702, 2])
We keep 2.26e+05/5.49e+06 =  4% of the original kernel matrix.

torch.Size([18997, 2])
We keep 1.51e+06/7.23e+07 =  2% of the original kernel matrix.

torch.Size([3933, 2])
We keep 1.12e+05/2.33e+06 =  4% of the original kernel matrix.

torch.Size([16493, 2])
We keep 1.11e+06/4.71e+07 =  2% of the original kernel matrix.

torch.Size([3941, 2])
We keep 2.32e+05/4.03e+06 =  5% of the original kernel matrix.

torch.Size([15538, 2])
We keep 1.34e+06/6.20e+07 =  2% of the original kernel matrix.

torch.Size([3967, 2])
We keep 1.06e+05/2.12e+06 =  4% of the original kernel matrix.

torch.Size([16743, 2])
We keep 1.09e+06/4.49e+07 =  2% of the original kernel matrix.

torch.Size([12068, 2])
We keep 7.84e+05/2.74e+07 =  2% of the original kernel matrix.

torch.Size([25848, 2])
We keep 2.63e+06/1.62e+08 =  1% of the original kernel matrix.

torch.Size([6292, 2])
We keep 2.45e+05/6.07e+06 =  4% of the original kernel matrix.

torch.Size([19743, 2])
We keep 1.56e+06/7.60e+07 =  2% of the original kernel matrix.

torch.Size([4275, 2])
We keep 1.16e+05/2.53e+06 =  4% of the original kernel matrix.

torch.Size([16954, 2])
We keep 1.16e+06/4.91e+07 =  2% of the original kernel matrix.

torch.Size([7409, 2])
We keep 8.15e+05/1.63e+07 =  4% of the original kernel matrix.

torch.Size([20131, 2])
We keep 2.21e+06/1.25e+08 =  1% of the original kernel matrix.

torch.Size([8595, 2])
We keep 4.08e+05/1.24e+07 =  3% of the original kernel matrix.

torch.Size([22506, 2])
We keep 1.99e+06/1.09e+08 =  1% of the original kernel matrix.

torch.Size([27220, 2])
We keep 5.33e+06/2.48e+08 =  2% of the original kernel matrix.

torch.Size([38576, 2])
We keep 6.10e+06/4.86e+08 =  1% of the original kernel matrix.

torch.Size([1626, 2])
We keep 3.63e+04/4.56e+05 =  7% of the original kernel matrix.

torch.Size([11575, 2])
We keep 6.77e+05/2.08e+07 =  3% of the original kernel matrix.

torch.Size([981, 2])
We keep 1.06e+04/1.13e+05 =  9% of the original kernel matrix.

torch.Size([10098, 2])
We keep 4.31e+05/1.04e+07 =  4% of the original kernel matrix.

torch.Size([2997, 2])
We keep 6.96e+04/1.20e+06 =  5% of the original kernel matrix.

torch.Size([14861, 2])
We keep 9.15e+05/3.38e+07 =  2% of the original kernel matrix.

torch.Size([2005, 2])
We keep 3.70e+04/4.91e+05 =  7% of the original kernel matrix.

torch.Size([12791, 2])
We keep 6.92e+05/2.16e+07 =  3% of the original kernel matrix.

torch.Size([3489, 2])
We keep 1.74e+05/2.69e+06 =  6% of the original kernel matrix.

torch.Size([14959, 2])
We keep 1.18e+06/5.06e+07 =  2% of the original kernel matrix.

torch.Size([4869, 2])
We keep 1.86e+05/3.93e+06 =  4% of the original kernel matrix.

torch.Size([17906, 2])
We keep 1.36e+06/6.12e+07 =  2% of the original kernel matrix.

torch.Size([2818, 2])
We keep 7.10e+04/1.19e+06 =  5% of the original kernel matrix.

torch.Size([14480, 2])
We keep 9.12e+05/3.36e+07 =  2% of the original kernel matrix.

torch.Size([1911, 2])
We keep 3.74e+04/5.01e+05 =  7% of the original kernel matrix.

torch.Size([12277, 2])
We keep 6.65e+05/2.18e+07 =  3% of the original kernel matrix.

torch.Size([4237, 2])
We keep 1.41e+05/2.68e+06 =  5% of the original kernel matrix.

torch.Size([16936, 2])
We keep 1.19e+06/5.05e+07 =  2% of the original kernel matrix.

torch.Size([1715, 2])
We keep 2.18e+04/2.75e+05 =  7% of the original kernel matrix.

torch.Size([12397, 2])
We keep 5.73e+05/1.62e+07 =  3% of the original kernel matrix.

torch.Size([10153, 2])
We keep 5.10e+05/1.64e+07 =  3% of the original kernel matrix.

torch.Size([24155, 2])
We keep 2.18e+06/1.25e+08 =  1% of the original kernel matrix.

torch.Size([2553, 2])
We keep 4.68e+04/7.85e+05 =  5% of the original kernel matrix.

torch.Size([14116, 2])
We keep 7.86e+05/2.73e+07 =  2% of the original kernel matrix.

torch.Size([16096, 2])
We keep 1.21e+06/4.76e+07 =  2% of the original kernel matrix.

torch.Size([29503, 2])
We keep 3.20e+06/2.13e+08 =  1% of the original kernel matrix.

torch.Size([9139, 2])
We keep 5.15e+05/1.51e+07 =  3% of the original kernel matrix.

torch.Size([22909, 2])
We keep 2.13e+06/1.20e+08 =  1% of the original kernel matrix.

torch.Size([4064, 2])
We keep 1.32e+05/2.43e+06 =  5% of the original kernel matrix.

torch.Size([16643, 2])
We keep 1.15e+06/4.81e+07 =  2% of the original kernel matrix.

torch.Size([878, 2])
We keep 8.31e+03/6.76e+04 = 12% of the original kernel matrix.

torch.Size([9849, 2])
We keep 3.77e+05/8.02e+06 =  4% of the original kernel matrix.

torch.Size([7024, 2])
We keep 4.19e+05/9.92e+06 =  4% of the original kernel matrix.

torch.Size([20507, 2])
We keep 1.87e+06/9.72e+07 =  1% of the original kernel matrix.

torch.Size([7260, 2])
We keep 3.06e+05/8.23e+06 =  3% of the original kernel matrix.

torch.Size([20883, 2])
We keep 1.71e+06/8.85e+07 =  1% of the original kernel matrix.

torch.Size([3940, 2])
We keep 1.53e+05/2.92e+06 =  5% of the original kernel matrix.

torch.Size([16226, 2])
We keep 1.22e+06/5.27e+07 =  2% of the original kernel matrix.

torch.Size([12173, 2])
We keep 8.57e+05/2.95e+07 =  2% of the original kernel matrix.

torch.Size([25995, 2])
We keep 2.76e+06/1.67e+08 =  1% of the original kernel matrix.

torch.Size([16590, 2])
We keep 1.41e+06/5.42e+07 =  2% of the original kernel matrix.

torch.Size([29984, 2])
We keep 3.36e+06/2.27e+08 =  1% of the original kernel matrix.

torch.Size([2597, 2])
We keep 5.95e+04/9.56e+05 =  6% of the original kernel matrix.

torch.Size([13985, 2])
We keep 8.20e+05/3.02e+07 =  2% of the original kernel matrix.

torch.Size([2031, 2])
We keep 3.40e+04/5.03e+05 =  6% of the original kernel matrix.

torch.Size([12892, 2])
We keep 6.82e+05/2.19e+07 =  3% of the original kernel matrix.

torch.Size([1521, 2])
We keep 2.08e+04/2.64e+05 =  7% of the original kernel matrix.

torch.Size([11825, 2])
We keep 5.62e+05/1.59e+07 =  3% of the original kernel matrix.

torch.Size([1672, 2])
We keep 2.49e+04/3.01e+05 =  8% of the original kernel matrix.

torch.Size([11960, 2])
We keep 5.81e+05/1.69e+07 =  3% of the original kernel matrix.

torch.Size([1377, 2])
We keep 2.41e+04/2.56e+05 =  9% of the original kernel matrix.

torch.Size([11005, 2])
We keep 5.56e+05/1.56e+07 =  3% of the original kernel matrix.

torch.Size([1309, 2])
We keep 1.48e+04/1.75e+05 =  8% of the original kernel matrix.

torch.Size([11216, 2])
We keep 5.01e+05/1.29e+07 =  3% of the original kernel matrix.

torch.Size([1260, 2])
We keep 1.63e+04/1.86e+05 =  8% of the original kernel matrix.

torch.Size([10723, 2])
We keep 4.94e+05/1.33e+07 =  3% of the original kernel matrix.

torch.Size([7430, 2])
We keep 5.89e+05/1.18e+07 =  4% of the original kernel matrix.

torch.Size([20613, 2])
We keep 1.96e+06/1.06e+08 =  1% of the original kernel matrix.

torch.Size([2554, 2])
We keep 7.47e+04/9.39e+05 =  7% of the original kernel matrix.

torch.Size([14119, 2])
We keep 8.38e+05/2.99e+07 =  2% of the original kernel matrix.

torch.Size([1870, 2])
We keep 7.80e+04/9.18e+05 =  8% of the original kernel matrix.

torch.Size([11435, 2])
We keep 8.16e+05/2.96e+07 =  2% of the original kernel matrix.

torch.Size([1988, 2])
We keep 5.55e+04/7.66e+05 =  7% of the original kernel matrix.

torch.Size([12100, 2])
We keep 7.88e+05/2.70e+07 =  2% of the original kernel matrix.

torch.Size([5655, 2])
We keep 2.38e+05/5.39e+06 =  4% of the original kernel matrix.

torch.Size([18786, 2])
We keep 1.50e+06/7.16e+07 =  2% of the original kernel matrix.

torch.Size([2418, 2])
We keep 7.74e+04/1.03e+06 =  7% of the original kernel matrix.

torch.Size([13290, 2])
We keep 8.56e+05/3.13e+07 =  2% of the original kernel matrix.

torch.Size([7048, 2])
We keep 4.24e+05/1.04e+07 =  4% of the original kernel matrix.

torch.Size([20353, 2])
We keep 1.86e+06/9.96e+07 =  1% of the original kernel matrix.

torch.Size([6943, 2])
We keep 2.95e+05/7.82e+06 =  3% of the original kernel matrix.

torch.Size([20275, 2])
We keep 1.68e+06/8.63e+07 =  1% of the original kernel matrix.

torch.Size([2092, 2])
We keep 3.60e+04/5.06e+05 =  7% of the original kernel matrix.

torch.Size([13188, 2])
We keep 6.95e+05/2.19e+07 =  3% of the original kernel matrix.

torch.Size([1457, 2])
We keep 2.72e+04/3.09e+05 =  8% of the original kernel matrix.

torch.Size([11110, 2])
We keep 5.96e+05/1.72e+07 =  3% of the original kernel matrix.

torch.Size([11953, 2])
We keep 8.87e+05/3.10e+07 =  2% of the original kernel matrix.

torch.Size([25430, 2])
We keep 2.74e+06/1.72e+08 =  1% of the original kernel matrix.

torch.Size([1346, 2])
We keep 1.43e+04/1.55e+05 =  9% of the original kernel matrix.

torch.Size([11076, 2])
We keep 4.69e+05/1.22e+07 =  3% of the original kernel matrix.

torch.Size([1870, 2])
We keep 2.63e+04/3.55e+05 =  7% of the original kernel matrix.

torch.Size([12859, 2])
We keep 6.16e+05/1.84e+07 =  3% of the original kernel matrix.

torch.Size([2461, 2])
We keep 7.97e+04/1.21e+06 =  6% of the original kernel matrix.

torch.Size([13001, 2])
We keep 8.94e+05/3.40e+07 =  2% of the original kernel matrix.

torch.Size([3624, 2])
We keep 1.11e+05/2.02e+06 =  5% of the original kernel matrix.

torch.Size([15759, 2])
We keep 1.07e+06/4.38e+07 =  2% of the original kernel matrix.

torch.Size([8471, 2])
We keep 4.80e+05/1.36e+07 =  3% of the original kernel matrix.

torch.Size([22201, 2])
We keep 2.06e+06/1.14e+08 =  1% of the original kernel matrix.

torch.Size([3876, 2])
We keep 1.13e+05/2.10e+06 =  5% of the original kernel matrix.

torch.Size([16382, 2])
We keep 1.09e+06/4.47e+07 =  2% of the original kernel matrix.

torch.Size([1571, 2])
We keep 3.80e+04/3.99e+05 =  9% of the original kernel matrix.

torch.Size([11390, 2])
We keep 6.10e+05/1.95e+07 =  3% of the original kernel matrix.

torch.Size([3750, 2])
We keep 1.22e+05/2.41e+06 =  5% of the original kernel matrix.

torch.Size([16019, 2])
We keep 1.14e+06/4.80e+07 =  2% of the original kernel matrix.

torch.Size([7620, 2])
We keep 3.58e+05/9.55e+06 =  3% of the original kernel matrix.

torch.Size([21499, 2])
We keep 1.79e+06/9.54e+07 =  1% of the original kernel matrix.

torch.Size([1295, 2])
We keep 1.51e+05/5.30e+05 = 28% of the original kernel matrix.

torch.Size([10047, 2])
We keep 7.08e+05/2.25e+07 =  3% of the original kernel matrix.

torch.Size([15285, 2])
We keep 2.18e+06/5.86e+07 =  3% of the original kernel matrix.

torch.Size([28795, 2])
We keep 3.48e+06/2.36e+08 =  1% of the original kernel matrix.

torch.Size([50293, 2])
We keep 8.65e+07/2.80e+09 =  3% of the original kernel matrix.

torch.Size([47238, 2])
We keep 1.60e+07/1.63e+09 =  0% of the original kernel matrix.

torch.Size([4729, 2])
We keep 1.77e+05/3.71e+06 =  4% of the original kernel matrix.

torch.Size([17740, 2])
We keep 1.19e+06/5.94e+07 =  1% of the original kernel matrix.

torch.Size([5844, 2])
We keep 2.82e+05/6.74e+06 =  4% of the original kernel matrix.

torch.Size([18903, 2])
We keep 1.62e+06/8.01e+07 =  2% of the original kernel matrix.

torch.Size([1063, 2])
We keep 1.92e+04/1.63e+05 = 11% of the original kernel matrix.

torch.Size([9941, 2])
We keep 4.97e+05/1.25e+07 =  3% of the original kernel matrix.

torch.Size([7695, 2])
We keep 3.77e+05/1.01e+07 =  3% of the original kernel matrix.

torch.Size([21445, 2])
We keep 1.88e+06/9.82e+07 =  1% of the original kernel matrix.

torch.Size([17404, 2])
We keep 1.49e+06/6.45e+07 =  2% of the original kernel matrix.

torch.Size([30608, 2])
We keep 3.55e+06/2.48e+08 =  1% of the original kernel matrix.

torch.Size([87061, 2])
We keep 2.62e+08/7.70e+09 =  3% of the original kernel matrix.

torch.Size([62460, 2])
We keep 2.44e+07/2.71e+09 =  0% of the original kernel matrix.

torch.Size([4910, 2])
We keep 1.46e+05/3.34e+06 =  4% of the original kernel matrix.

torch.Size([18076, 2])
We keep 1.25e+06/5.64e+07 =  2% of the original kernel matrix.

torch.Size([54731, 2])
We keep 3.07e+07/1.67e+09 =  1% of the original kernel matrix.

torch.Size([52892, 2])
We keep 1.28e+07/1.26e+09 =  1% of the original kernel matrix.

torch.Size([60149, 2])
We keep 3.19e+07/1.70e+09 =  1% of the original kernel matrix.

torch.Size([56053, 2])
We keep 1.28e+07/1.27e+09 =  1% of the original kernel matrix.

torch.Size([9569, 2])
We keep 1.04e+06/2.43e+07 =  4% of the original kernel matrix.

torch.Size([22956, 2])
We keep 2.53e+06/1.52e+08 =  1% of the original kernel matrix.

torch.Size([60985, 2])
We keep 1.07e+08/2.70e+09 =  3% of the original kernel matrix.

torch.Size([56116, 2])
We keep 1.47e+07/1.60e+09 =  0% of the original kernel matrix.

torch.Size([2143, 2])
We keep 4.13e+04/6.15e+05 =  6% of the original kernel matrix.

torch.Size([13281, 2])
We keep 7.36e+05/2.42e+07 =  3% of the original kernel matrix.

torch.Size([7314, 2])
We keep 3.62e+05/9.33e+06 =  3% of the original kernel matrix.

torch.Size([20815, 2])
We keep 1.80e+06/9.42e+07 =  1% of the original kernel matrix.

torch.Size([100977, 2])
We keep 9.94e+07/6.29e+09 =  1% of the original kernel matrix.

torch.Size([67628, 2])
We keep 2.24e+07/2.45e+09 =  0% of the original kernel matrix.

torch.Size([396430, 2])
We keep 4.68e+08/5.69e+10 =  0% of the original kernel matrix.

torch.Size([152134, 2])
We keep 5.69e+07/7.36e+09 =  0% of the original kernel matrix.

torch.Size([5665, 2])
We keep 4.61e+05/6.46e+06 =  7% of the original kernel matrix.

torch.Size([18670, 2])
We keep 1.59e+06/7.84e+07 =  2% of the original kernel matrix.

torch.Size([37233, 2])
We keep 7.44e+06/4.43e+08 =  1% of the original kernel matrix.

torch.Size([45978, 2])
We keep 7.50e+06/6.49e+08 =  1% of the original kernel matrix.

torch.Size([15540, 2])
We keep 2.97e+06/8.32e+07 =  3% of the original kernel matrix.

torch.Size([28652, 2])
We keep 3.95e+06/2.81e+08 =  1% of the original kernel matrix.

torch.Size([35276, 2])
We keep 2.77e+07/5.82e+08 =  4% of the original kernel matrix.

torch.Size([43448, 2])
We keep 7.97e+06/7.45e+08 =  1% of the original kernel matrix.

torch.Size([34241, 2])
We keep 3.26e+07/6.94e+08 =  4% of the original kernel matrix.

torch.Size([42450, 2])
We keep 8.90e+06/8.13e+08 =  1% of the original kernel matrix.

torch.Size([24158, 2])
We keep 9.23e+06/3.24e+08 =  2% of the original kernel matrix.

torch.Size([35416, 2])
We keep 6.70e+06/5.55e+08 =  1% of the original kernel matrix.

torch.Size([37989, 2])
We keep 1.29e+07/5.72e+08 =  2% of the original kernel matrix.

torch.Size([46057, 2])
We keep 8.30e+06/7.38e+08 =  1% of the original kernel matrix.

torch.Size([24107, 2])
We keep 5.49e+06/1.69e+08 =  3% of the original kernel matrix.

torch.Size([36433, 2])
We keep 5.19e+06/4.02e+08 =  1% of the original kernel matrix.

torch.Size([4620, 2])
We keep 1.56e+06/9.99e+06 = 15% of the original kernel matrix.

torch.Size([16369, 2])
We keep 1.85e+06/9.75e+07 =  1% of the original kernel matrix.

torch.Size([127292, 2])
We keep 3.63e+08/1.71e+10 =  2% of the original kernel matrix.

torch.Size([76929, 2])
We keep 3.43e+07/4.04e+09 =  0% of the original kernel matrix.

torch.Size([3754, 2])
We keep 1.34e+05/2.37e+06 =  5% of the original kernel matrix.

torch.Size([16021, 2])
We keep 1.15e+06/4.75e+07 =  2% of the original kernel matrix.

torch.Size([3824, 2])
We keep 1.00e+05/2.00e+06 =  5% of the original kernel matrix.

torch.Size([16407, 2])
We keep 1.07e+06/4.36e+07 =  2% of the original kernel matrix.

torch.Size([23440, 2])
We keep 4.38e+06/1.63e+08 =  2% of the original kernel matrix.

torch.Size([35518, 2])
We keep 5.04e+06/3.94e+08 =  1% of the original kernel matrix.

torch.Size([26387, 2])
We keep 1.02e+07/3.19e+08 =  3% of the original kernel matrix.

torch.Size([36513, 2])
We keep 6.48e+06/5.51e+08 =  1% of the original kernel matrix.

torch.Size([345706, 2])
We keep 4.70e+08/4.88e+10 =  0% of the original kernel matrix.

torch.Size([140276, 2])
We keep 5.35e+07/6.81e+09 =  0% of the original kernel matrix.

torch.Size([102809, 2])
We keep 3.33e+07/3.38e+09 =  0% of the original kernel matrix.

torch.Size([74691, 2])
We keep 1.70e+07/1.79e+09 =  0% of the original kernel matrix.

torch.Size([1357, 2])
We keep 1.49e+04/1.82e+05 =  8% of the original kernel matrix.

torch.Size([11502, 2])
We keep 5.09e+05/1.32e+07 =  3% of the original kernel matrix.

torch.Size([17479, 2])
We keep 2.47e+06/7.24e+07 =  3% of the original kernel matrix.

torch.Size([30711, 2])
We keep 3.75e+06/2.62e+08 =  1% of the original kernel matrix.

torch.Size([20095, 2])
We keep 2.97e+06/1.22e+08 =  2% of the original kernel matrix.

torch.Size([32557, 2])
We keep 4.56e+06/3.41e+08 =  1% of the original kernel matrix.

torch.Size([18980, 2])
We keep 3.49e+06/1.34e+08 =  2% of the original kernel matrix.

torch.Size([32021, 2])
We keep 4.71e+06/3.58e+08 =  1% of the original kernel matrix.

torch.Size([82228, 2])
We keep 3.45e+07/2.40e+09 =  1% of the original kernel matrix.

torch.Size([67284, 2])
We keep 1.46e+07/1.51e+09 =  0% of the original kernel matrix.

torch.Size([17167, 2])
We keep 1.41e+07/1.59e+08 =  8% of the original kernel matrix.

torch.Size([29928, 2])
We keep 5.06e+06/3.89e+08 =  1% of the original kernel matrix.

torch.Size([40000, 2])
We keep 9.13e+06/5.33e+08 =  1% of the original kernel matrix.

torch.Size([46986, 2])
We keep 8.08e+06/7.12e+08 =  1% of the original kernel matrix.

torch.Size([1324, 2])
We keep 1.85e+04/1.90e+05 =  9% of the original kernel matrix.

torch.Size([11156, 2])
We keep 5.22e+05/1.35e+07 =  3% of the original kernel matrix.

torch.Size([7187, 2])
We keep 3.23e+05/8.38e+06 =  3% of the original kernel matrix.

torch.Size([20899, 2])
We keep 1.73e+06/8.93e+07 =  1% of the original kernel matrix.

torch.Size([47730, 2])
We keep 3.44e+07/1.73e+09 =  1% of the original kernel matrix.

torch.Size([46426, 2])
We keep 1.30e+07/1.28e+09 =  1% of the original kernel matrix.

torch.Size([34529, 2])
We keep 1.18e+07/6.31e+08 =  1% of the original kernel matrix.

torch.Size([42356, 2])
We keep 8.63e+06/7.75e+08 =  1% of the original kernel matrix.

torch.Size([30402, 2])
We keep 8.07e+06/3.88e+08 =  2% of the original kernel matrix.

torch.Size([40299, 2])
We keep 7.11e+06/6.08e+08 =  1% of the original kernel matrix.

torch.Size([9438, 2])
We keep 7.11e+05/2.05e+07 =  3% of the original kernel matrix.

torch.Size([22781, 2])
We keep 2.37e+06/1.40e+08 =  1% of the original kernel matrix.

torch.Size([296677, 2])
We keep 2.96e+08/4.72e+10 =  0% of the original kernel matrix.

torch.Size([130140, 2])
We keep 5.19e+07/6.70e+09 =  0% of the original kernel matrix.

torch.Size([9267, 2])
We keep 4.44e+05/1.42e+07 =  3% of the original kernel matrix.

torch.Size([23198, 2])
We keep 2.07e+06/1.16e+08 =  1% of the original kernel matrix.

torch.Size([32226, 2])
We keep 4.62e+06/3.04e+08 =  1% of the original kernel matrix.

torch.Size([42260, 2])
We keep 6.38e+06/5.38e+08 =  1% of the original kernel matrix.

torch.Size([117773, 2])
We keep 6.88e+07/5.96e+09 =  1% of the original kernel matrix.

torch.Size([78756, 2])
We keep 2.16e+07/2.38e+09 =  0% of the original kernel matrix.

torch.Size([3520, 2])
We keep 9.25e+04/1.83e+06 =  5% of the original kernel matrix.

torch.Size([15882, 2])
We keep 1.05e+06/4.17e+07 =  2% of the original kernel matrix.

torch.Size([46854, 2])
We keep 1.40e+07/7.91e+08 =  1% of the original kernel matrix.

torch.Size([50026, 2])
We keep 9.05e+06/8.68e+08 =  1% of the original kernel matrix.

torch.Size([206451, 2])
We keep 3.42e+08/2.43e+10 =  1% of the original kernel matrix.

torch.Size([104613, 2])
We keep 4.00e+07/4.81e+09 =  0% of the original kernel matrix.

torch.Size([203746, 2])
We keep 3.00e+08/2.19e+10 =  1% of the original kernel matrix.

torch.Size([103275, 2])
We keep 3.79e+07/4.57e+09 =  0% of the original kernel matrix.

torch.Size([17442, 2])
We keep 2.24e+06/8.31e+07 =  2% of the original kernel matrix.

torch.Size([30317, 2])
We keep 3.98e+06/2.81e+08 =  1% of the original kernel matrix.

torch.Size([13354, 2])
We keep 1.56e+06/4.32e+07 =  3% of the original kernel matrix.

torch.Size([26631, 2])
We keep 3.10e+06/2.03e+08 =  1% of the original kernel matrix.

torch.Size([3403, 2])
We keep 1.78e+05/2.62e+06 =  6% of the original kernel matrix.

torch.Size([14682, 2])
We keep 1.17e+06/4.99e+07 =  2% of the original kernel matrix.

torch.Size([6542, 2])
We keep 2.34e+05/6.19e+06 =  3% of the original kernel matrix.

torch.Size([20129, 2])
We keep 1.56e+06/7.67e+07 =  2% of the original kernel matrix.

torch.Size([46394, 2])
We keep 1.88e+07/1.10e+09 =  1% of the original kernel matrix.

torch.Size([49382, 2])
We keep 1.09e+07/1.03e+09 =  1% of the original kernel matrix.

torch.Size([15153, 2])
We keep 1.26e+06/4.72e+07 =  2% of the original kernel matrix.

torch.Size([28590, 2])
We keep 3.18e+06/2.12e+08 =  1% of the original kernel matrix.

torch.Size([27775, 2])
We keep 5.35e+06/2.80e+08 =  1% of the original kernel matrix.

torch.Size([39047, 2])
We keep 6.20e+06/5.16e+08 =  1% of the original kernel matrix.

torch.Size([50429, 2])
We keep 4.72e+07/1.63e+09 =  2% of the original kernel matrix.

torch.Size([52390, 2])
We keep 1.12e+07/1.25e+09 =  0% of the original kernel matrix.

torch.Size([31148, 2])
We keep 1.90e+07/4.68e+08 =  4% of the original kernel matrix.

torch.Size([41169, 2])
We keep 7.71e+06/6.67e+08 =  1% of the original kernel matrix.

torch.Size([14086, 2])
We keep 4.16e+06/8.75e+07 =  4% of the original kernel matrix.

torch.Size([26606, 2])
We keep 4.05e+06/2.89e+08 =  1% of the original kernel matrix.

torch.Size([8571, 2])
We keep 4.25e+05/1.18e+07 =  3% of the original kernel matrix.

torch.Size([22565, 2])
We keep 1.91e+06/1.06e+08 =  1% of the original kernel matrix.

torch.Size([57761, 2])
We keep 1.91e+08/7.85e+09 =  2% of the original kernel matrix.

torch.Size([48165, 2])
We keep 2.44e+07/2.73e+09 =  0% of the original kernel matrix.

torch.Size([19224, 2])
We keep 1.86e+07/1.65e+08 = 11% of the original kernel matrix.

torch.Size([31988, 2])
We keep 5.10e+06/3.96e+08 =  1% of the original kernel matrix.

torch.Size([11229, 2])
We keep 4.41e+06/4.91e+07 =  8% of the original kernel matrix.

torch.Size([24679, 2])
We keep 3.26e+06/2.16e+08 =  1% of the original kernel matrix.

torch.Size([31322, 2])
We keep 6.52e+06/3.50e+08 =  1% of the original kernel matrix.

torch.Size([41715, 2])
We keep 6.95e+06/5.78e+08 =  1% of the original kernel matrix.

torch.Size([275568, 2])
We keep 5.78e+08/5.05e+10 =  1% of the original kernel matrix.

torch.Size([120580, 2])
We keep 5.40e+07/6.93e+09 =  0% of the original kernel matrix.

torch.Size([10597, 2])
We keep 8.98e+05/2.40e+07 =  3% of the original kernel matrix.

torch.Size([24024, 2])
We keep 2.51e+06/1.51e+08 =  1% of the original kernel matrix.

torch.Size([104770, 2])
We keep 1.20e+08/6.29e+09 =  1% of the original kernel matrix.

torch.Size([72475, 2])
We keep 2.20e+07/2.45e+09 =  0% of the original kernel matrix.

torch.Size([4732, 2])
We keep 3.01e+05/5.35e+06 =  5% of the original kernel matrix.

torch.Size([16967, 2])
We keep 1.48e+06/7.14e+07 =  2% of the original kernel matrix.

torch.Size([20467, 2])
We keep 1.44e+07/4.71e+08 =  3% of the original kernel matrix.

torch.Size([28806, 2])
We keep 7.74e+06/6.70e+08 =  1% of the original kernel matrix.

torch.Size([15060, 2])
We keep 1.33e+06/5.05e+07 =  2% of the original kernel matrix.

torch.Size([29037, 2])
We keep 3.13e+06/2.19e+08 =  1% of the original kernel matrix.

torch.Size([83061, 2])
We keep 1.75e+08/7.11e+09 =  2% of the original kernel matrix.

torch.Size([60292, 2])
We keep 2.36e+07/2.60e+09 =  0% of the original kernel matrix.

torch.Size([43871, 2])
We keep 9.68e+06/7.69e+08 =  1% of the original kernel matrix.

torch.Size([49223, 2])
We keep 9.28e+06/8.55e+08 =  1% of the original kernel matrix.

torch.Size([6242, 2])
We keep 2.44e+05/6.15e+06 =  3% of the original kernel matrix.

torch.Size([19711, 2])
We keep 1.54e+06/7.65e+07 =  2% of the original kernel matrix.

torch.Size([150632, 2])
We keep 6.53e+07/7.58e+09 =  0% of the original kernel matrix.

torch.Size([90638, 2])
We keep 2.39e+07/2.69e+09 =  0% of the original kernel matrix.

torch.Size([145229, 2])
We keep 6.10e+07/6.64e+09 =  0% of the original kernel matrix.

torch.Size([88357, 2])
We keep 2.25e+07/2.52e+09 =  0% of the original kernel matrix.

torch.Size([5238, 2])
We keep 2.58e+05/5.65e+06 =  4% of the original kernel matrix.

torch.Size([18124, 2])
We keep 1.51e+06/7.33e+07 =  2% of the original kernel matrix.

torch.Size([67467, 2])
We keep 3.08e+07/1.73e+09 =  1% of the original kernel matrix.

torch.Size([61411, 2])
We keep 1.29e+07/1.28e+09 =  1% of the original kernel matrix.

torch.Size([184451, 2])
We keep 9.10e+07/1.22e+10 =  0% of the original kernel matrix.

torch.Size([101008, 2])
We keep 2.92e+07/3.41e+09 =  0% of the original kernel matrix.

torch.Size([410047, 2])
We keep 4.20e+08/6.56e+10 =  0% of the original kernel matrix.

torch.Size([156563, 2])
We keep 6.05e+07/7.90e+09 =  0% of the original kernel matrix.

torch.Size([60097, 2])
We keep 2.13e+08/3.63e+09 =  5% of the original kernel matrix.

torch.Size([53942, 2])
We keep 1.77e+07/1.86e+09 =  0% of the original kernel matrix.

torch.Size([1955, 2])
We keep 4.60e+04/6.42e+05 =  7% of the original kernel matrix.

torch.Size([12432, 2])
We keep 7.30e+05/2.47e+07 =  2% of the original kernel matrix.

torch.Size([7043, 2])
We keep 3.24e+05/8.48e+06 =  3% of the original kernel matrix.

torch.Size([20626, 2])
We keep 1.71e+06/8.99e+07 =  1% of the original kernel matrix.

torch.Size([8485, 2])
We keep 1.75e+06/3.82e+07 =  4% of the original kernel matrix.

torch.Size([21873, 2])
We keep 2.84e+06/1.91e+08 =  1% of the original kernel matrix.

torch.Size([23588, 2])
We keep 3.37e+06/1.45e+08 =  2% of the original kernel matrix.

torch.Size([35725, 2])
We keep 4.83e+06/3.71e+08 =  1% of the original kernel matrix.

torch.Size([13842, 2])
We keep 2.23e+06/4.55e+07 =  4% of the original kernel matrix.

torch.Size([27527, 2])
We keep 3.15e+06/2.08e+08 =  1% of the original kernel matrix.

torch.Size([3451, 2])
We keep 7.24e+04/1.40e+06 =  5% of the original kernel matrix.

torch.Size([15949, 2])
We keep 9.47e+05/3.65e+07 =  2% of the original kernel matrix.

torch.Size([76282, 2])
We keep 2.39e+07/2.03e+09 =  1% of the original kernel matrix.

torch.Size([65185, 2])
We keep 1.37e+07/1.39e+09 =  0% of the original kernel matrix.

torch.Size([108777, 2])
We keep 4.48e+07/4.20e+09 =  1% of the original kernel matrix.

torch.Size([75932, 2])
We keep 1.87e+07/2.00e+09 =  0% of the original kernel matrix.

torch.Size([6009, 2])
We keep 9.54e+06/8.92e+07 = 10% of the original kernel matrix.

torch.Size([15682, 2])
We keep 4.00e+06/2.91e+08 =  1% of the original kernel matrix.

torch.Size([5030, 2])
We keep 2.27e+05/4.72e+06 =  4% of the original kernel matrix.

torch.Size([17604, 2])
We keep 1.42e+06/6.70e+07 =  2% of the original kernel matrix.

torch.Size([17684, 2])
We keep 2.49e+06/9.91e+07 =  2% of the original kernel matrix.

torch.Size([30349, 2])
We keep 4.21e+06/3.07e+08 =  1% of the original kernel matrix.

torch.Size([20894, 2])
We keep 2.57e+06/1.12e+08 =  2% of the original kernel matrix.

torch.Size([33668, 2])
We keep 4.48e+06/3.27e+08 =  1% of the original kernel matrix.

torch.Size([14513, 2])
We keep 1.43e+06/4.46e+07 =  3% of the original kernel matrix.

torch.Size([28159, 2])
We keep 3.15e+06/2.06e+08 =  1% of the original kernel matrix.

torch.Size([17939, 2])
We keep 1.82e+06/7.43e+07 =  2% of the original kernel matrix.

torch.Size([31144, 2])
We keep 3.78e+06/2.66e+08 =  1% of the original kernel matrix.

torch.Size([4539, 2])
We keep 1.87e+05/3.29e+06 =  5% of the original kernel matrix.

torch.Size([17400, 2])
We keep 1.26e+06/5.60e+07 =  2% of the original kernel matrix.

torch.Size([56530, 2])
We keep 9.05e+07/2.50e+09 =  3% of the original kernel matrix.

torch.Size([52331, 2])
We keep 1.50e+07/1.54e+09 =  0% of the original kernel matrix.

torch.Size([34501, 2])
We keep 5.08e+06/3.63e+08 =  1% of the original kernel matrix.

torch.Size([44061, 2])
We keep 6.84e+06/5.87e+08 =  1% of the original kernel matrix.

torch.Size([20793, 2])
We keep 3.20e+06/1.12e+08 =  2% of the original kernel matrix.

torch.Size([33585, 2])
We keep 4.42e+06/3.27e+08 =  1% of the original kernel matrix.

torch.Size([29817, 2])
We keep 1.37e+07/4.66e+08 =  2% of the original kernel matrix.

torch.Size([39474, 2])
We keep 7.73e+06/6.66e+08 =  1% of the original kernel matrix.

torch.Size([6981, 2])
We keep 7.02e+05/1.36e+07 =  5% of the original kernel matrix.

torch.Size([19411, 2])
We keep 2.04e+06/1.14e+08 =  1% of the original kernel matrix.

torch.Size([572526, 2])
We keep 3.44e+09/1.42e+11 =  2% of the original kernel matrix.

torch.Size([182090, 2])
We keep 8.66e+07/1.16e+10 =  0% of the original kernel matrix.

torch.Size([11308, 2])
We keep 8.26e+05/2.47e+07 =  3% of the original kernel matrix.

torch.Size([24910, 2])
We keep 2.55e+06/1.53e+08 =  1% of the original kernel matrix.

torch.Size([96699, 2])
We keep 4.62e+07/3.22e+09 =  1% of the original kernel matrix.

torch.Size([72058, 2])
We keep 1.67e+07/1.75e+09 =  0% of the original kernel matrix.

torch.Size([2124, 2])
We keep 4.59e+04/6.26e+05 =  7% of the original kernel matrix.

torch.Size([13083, 2])
We keep 7.36e+05/2.44e+07 =  3% of the original kernel matrix.

torch.Size([4296, 2])
We keep 1.60e+05/3.24e+06 =  4% of the original kernel matrix.

torch.Size([16964, 2])
We keep 1.26e+06/5.55e+07 =  2% of the original kernel matrix.

torch.Size([7335, 2])
We keep 3.24e+05/8.99e+06 =  3% of the original kernel matrix.

torch.Size([21008, 2])
We keep 1.77e+06/9.25e+07 =  1% of the original kernel matrix.

torch.Size([13831, 2])
We keep 1.14e+06/3.73e+07 =  3% of the original kernel matrix.

torch.Size([27914, 2])
We keep 2.80e+06/1.88e+08 =  1% of the original kernel matrix.

torch.Size([8182, 2])
We keep 9.14e+05/1.40e+07 =  6% of the original kernel matrix.

torch.Size([21652, 2])
We keep 2.07e+06/1.15e+08 =  1% of the original kernel matrix.

torch.Size([36234, 2])
We keep 2.73e+07/7.02e+08 =  3% of the original kernel matrix.

torch.Size([44464, 2])
We keep 9.16e+06/8.18e+08 =  1% of the original kernel matrix.

torch.Size([6264, 2])
We keep 2.32e+05/5.91e+06 =  3% of the original kernel matrix.

torch.Size([19600, 2])
We keep 1.53e+06/7.50e+07 =  2% of the original kernel matrix.

torch.Size([6014, 2])
We keep 2.79e+05/6.86e+06 =  4% of the original kernel matrix.

torch.Size([19230, 2])
We keep 1.61e+06/8.08e+07 =  1% of the original kernel matrix.

torch.Size([321196, 2])
We keep 3.52e+08/4.29e+10 =  0% of the original kernel matrix.

torch.Size([135468, 2])
We keep 5.06e+07/6.39e+09 =  0% of the original kernel matrix.

torch.Size([37644, 2])
We keep 1.04e+07/5.34e+08 =  1% of the original kernel matrix.

torch.Size([45499, 2])
We keep 8.10e+06/7.13e+08 =  1% of the original kernel matrix.

torch.Size([32214, 2])
We keep 1.08e+07/5.14e+08 =  2% of the original kernel matrix.

torch.Size([41506, 2])
We keep 7.99e+06/6.99e+08 =  1% of the original kernel matrix.

torch.Size([99618, 2])
We keep 5.43e+07/4.04e+09 =  1% of the original kernel matrix.

torch.Size([73027, 2])
We keep 1.84e+07/1.96e+09 =  0% of the original kernel matrix.

torch.Size([129460, 2])
We keep 1.19e+08/9.97e+09 =  1% of the original kernel matrix.

torch.Size([76847, 2])
We keep 2.70e+07/3.08e+09 =  0% of the original kernel matrix.

torch.Size([9680, 2])
We keep 5.38e+05/1.73e+07 =  3% of the original kernel matrix.

torch.Size([23397, 2])
We keep 2.25e+06/1.28e+08 =  1% of the original kernel matrix.

torch.Size([7380, 2])
We keep 3.21e+05/9.00e+06 =  3% of the original kernel matrix.

torch.Size([20950, 2])
We keep 1.77e+06/9.26e+07 =  1% of the original kernel matrix.

torch.Size([13757, 2])
We keep 4.01e+06/9.06e+07 =  4% of the original kernel matrix.

torch.Size([26348, 2])
We keep 4.10e+06/2.94e+08 =  1% of the original kernel matrix.

torch.Size([1069, 2])
We keep 1.71e+04/1.62e+05 = 10% of the original kernel matrix.

torch.Size([9917, 2])
We keep 4.81e+05/1.24e+07 =  3% of the original kernel matrix.

torch.Size([5620, 2])
We keep 1.92e+05/4.65e+06 =  4% of the original kernel matrix.

torch.Size([18820, 2])
We keep 1.41e+06/6.66e+07 =  2% of the original kernel matrix.

torch.Size([18346, 2])
We keep 4.10e+06/1.39e+08 =  2% of the original kernel matrix.

torch.Size([31104, 2])
We keep 4.86e+06/3.63e+08 =  1% of the original kernel matrix.

torch.Size([4043, 2])
We keep 1.88e+05/2.78e+06 =  6% of the original kernel matrix.

torch.Size([16151, 2])
We keep 1.20e+06/5.14e+07 =  2% of the original kernel matrix.

torch.Size([3961, 2])
We keep 1.22e+05/2.45e+06 =  4% of the original kernel matrix.

torch.Size([16597, 2])
We keep 1.15e+06/4.83e+07 =  2% of the original kernel matrix.

torch.Size([4413, 2])
We keep 1.46e+05/2.99e+06 =  4% of the original kernel matrix.

torch.Size([17063, 2])
We keep 1.22e+06/5.34e+07 =  2% of the original kernel matrix.

torch.Size([7121, 2])
We keep 3.51e+05/8.86e+06 =  3% of the original kernel matrix.

torch.Size([20746, 2])
We keep 1.77e+06/9.19e+07 =  1% of the original kernel matrix.

torch.Size([263125, 2])
We keep 6.03e+08/4.74e+10 =  1% of the original kernel matrix.

torch.Size([116218, 2])
We keep 5.34e+07/6.72e+09 =  0% of the original kernel matrix.

torch.Size([12752, 2])
We keep 1.77e+06/5.30e+07 =  3% of the original kernel matrix.

torch.Size([25448, 2])
We keep 3.32e+06/2.25e+08 =  1% of the original kernel matrix.

torch.Size([66968, 2])
We keep 3.23e+07/1.77e+09 =  1% of the original kernel matrix.

torch.Size([60122, 2])
We keep 1.31e+07/1.30e+09 =  1% of the original kernel matrix.

torch.Size([7364, 2])
We keep 1.98e+06/1.85e+07 = 10% of the original kernel matrix.

torch.Size([20306, 2])
We keep 2.23e+06/1.33e+08 =  1% of the original kernel matrix.

torch.Size([4414, 2])
We keep 1.70e+05/3.42e+06 =  4% of the original kernel matrix.

torch.Size([16979, 2])
We keep 1.29e+06/5.70e+07 =  2% of the original kernel matrix.

torch.Size([18435, 2])
We keep 3.79e+06/1.21e+08 =  3% of the original kernel matrix.

torch.Size([31041, 2])
We keep 4.58e+06/3.39e+08 =  1% of the original kernel matrix.

torch.Size([486005, 2])
We keep 1.42e+09/1.08e+11 =  1% of the original kernel matrix.

torch.Size([163866, 2])
We keep 7.22e+07/1.01e+10 =  0% of the original kernel matrix.

torch.Size([464455, 2])
We keep 4.36e+08/7.89e+10 =  0% of the original kernel matrix.

torch.Size([165687, 2])
We keep 6.52e+07/8.67e+09 =  0% of the original kernel matrix.

torch.Size([19966, 2])
We keep 3.65e+06/1.08e+08 =  3% of the original kernel matrix.

torch.Size([33080, 2])
We keep 4.29e+06/3.21e+08 =  1% of the original kernel matrix.

torch.Size([31715, 2])
We keep 7.99e+06/3.52e+08 =  2% of the original kernel matrix.

torch.Size([41963, 2])
We keep 6.86e+06/5.79e+08 =  1% of the original kernel matrix.

torch.Size([3040, 2])
We keep 8.06e+04/1.38e+06 =  5% of the original kernel matrix.

torch.Size([14934, 2])
We keep 9.46e+05/3.63e+07 =  2% of the original kernel matrix.

torch.Size([16376, 2])
We keep 5.70e+07/5.36e+08 = 10% of the original kernel matrix.

torch.Size([26207, 2])
We keep 8.04e+06/7.15e+08 =  1% of the original kernel matrix.

torch.Size([123756, 2])
We keep 5.28e+08/1.32e+10 =  4% of the original kernel matrix.

torch.Size([76146, 2])
We keep 3.08e+07/3.54e+09 =  0% of the original kernel matrix.

torch.Size([35459, 2])
We keep 1.80e+07/5.73e+08 =  3% of the original kernel matrix.

torch.Size([43993, 2])
We keep 8.14e+06/7.38e+08 =  1% of the original kernel matrix.

torch.Size([12123, 2])
We keep 9.56e+05/2.92e+07 =  3% of the original kernel matrix.

torch.Size([25775, 2])
We keep 2.69e+06/1.67e+08 =  1% of the original kernel matrix.

torch.Size([71379, 2])
We keep 3.28e+07/2.07e+09 =  1% of the original kernel matrix.

torch.Size([62058, 2])
We keep 1.38e+07/1.40e+09 =  0% of the original kernel matrix.

torch.Size([329452, 2])
We keep 4.79e+08/5.09e+10 =  0% of the original kernel matrix.

torch.Size([137093, 2])
We keep 5.53e+07/6.96e+09 =  0% of the original kernel matrix.

torch.Size([26315, 2])
We keep 4.91e+06/2.46e+08 =  1% of the original kernel matrix.

torch.Size([36973, 2])
We keep 5.93e+06/4.84e+08 =  1% of the original kernel matrix.

torch.Size([7279, 2])
We keep 7.77e+05/1.54e+07 =  5% of the original kernel matrix.

torch.Size([19727, 2])
We keep 2.15e+06/1.21e+08 =  1% of the original kernel matrix.

torch.Size([24675, 2])
We keep 2.95e+06/1.48e+08 =  1% of the original kernel matrix.

torch.Size([37249, 2])
We keep 4.95e+06/3.75e+08 =  1% of the original kernel matrix.

torch.Size([24515, 2])
We keep 2.95e+06/1.63e+08 =  1% of the original kernel matrix.

torch.Size([36505, 2])
We keep 5.11e+06/3.94e+08 =  1% of the original kernel matrix.

torch.Size([4099, 2])
We keep 1.62e+05/2.63e+06 =  6% of the original kernel matrix.

torch.Size([16677, 2])
We keep 1.15e+06/5.00e+07 =  2% of the original kernel matrix.

torch.Size([149879, 2])
We keep 1.59e+08/1.06e+10 =  1% of the original kernel matrix.

torch.Size([89311, 2])
We keep 2.79e+07/3.17e+09 =  0% of the original kernel matrix.

torch.Size([16427, 2])
We keep 3.97e+06/1.10e+08 =  3% of the original kernel matrix.

torch.Size([28803, 2])
We keep 4.40e+06/3.23e+08 =  1% of the original kernel matrix.

torch.Size([1476, 2])
We keep 1.91e+04/2.35e+05 =  8% of the original kernel matrix.

torch.Size([11600, 2])
We keep 5.45e+05/1.50e+07 =  3% of the original kernel matrix.

torch.Size([26310, 2])
We keep 3.38e+06/1.92e+08 =  1% of the original kernel matrix.

torch.Size([38078, 2])
We keep 5.28e+06/4.28e+08 =  1% of the original kernel matrix.

torch.Size([28092, 2])
We keep 5.13e+06/2.62e+08 =  1% of the original kernel matrix.

torch.Size([39428, 2])
We keep 6.15e+06/4.99e+08 =  1% of the original kernel matrix.

torch.Size([1455, 2])
We keep 1.89e+04/2.15e+05 =  8% of the original kernel matrix.

torch.Size([11484, 2])
We keep 5.35e+05/1.43e+07 =  3% of the original kernel matrix.

torch.Size([87099, 2])
We keep 1.16e+08/4.35e+09 =  2% of the original kernel matrix.

torch.Size([65233, 2])
We keep 1.89e+07/2.04e+09 =  0% of the original kernel matrix.

torch.Size([140719, 2])
We keep 7.18e+07/6.84e+09 =  1% of the original kernel matrix.

torch.Size([86857, 2])
We keep 2.29e+07/2.55e+09 =  0% of the original kernel matrix.

torch.Size([127813, 2])
We keep 1.29e+08/6.17e+09 =  2% of the original kernel matrix.

torch.Size([82719, 2])
We keep 2.22e+07/2.42e+09 =  0% of the original kernel matrix.

torch.Size([132376, 2])
We keep 1.44e+08/7.80e+09 =  1% of the original kernel matrix.

torch.Size([82993, 2])
We keep 2.41e+07/2.73e+09 =  0% of the original kernel matrix.

torch.Size([97043, 2])
We keep 6.37e+07/3.88e+09 =  1% of the original kernel matrix.

torch.Size([70958, 2])
We keep 1.80e+07/1.92e+09 =  0% of the original kernel matrix.

torch.Size([13606, 2])
We keep 3.18e+06/7.77e+07 =  4% of the original kernel matrix.

torch.Size([25814, 2])
We keep 3.87e+06/2.72e+08 =  1% of the original kernel matrix.

torch.Size([11972, 2])
We keep 1.85e+06/4.26e+07 =  4% of the original kernel matrix.

torch.Size([25181, 2])
We keep 3.08e+06/2.01e+08 =  1% of the original kernel matrix.

torch.Size([45884, 2])
We keep 9.02e+06/7.04e+08 =  1% of the original kernel matrix.

torch.Size([51048, 2])
We keep 8.88e+06/8.18e+08 =  1% of the original kernel matrix.

torch.Size([113326, 2])
We keep 4.50e+07/4.16e+09 =  1% of the original kernel matrix.

torch.Size([78209, 2])
We keep 1.84e+07/1.99e+09 =  0% of the original kernel matrix.

torch.Size([5388, 2])
We keep 2.05e+05/4.67e+06 =  4% of the original kernel matrix.

torch.Size([18667, 2])
We keep 1.42e+06/6.66e+07 =  2% of the original kernel matrix.

torch.Size([45017, 2])
We keep 2.73e+07/1.02e+09 =  2% of the original kernel matrix.

torch.Size([49609, 2])
We keep 9.64e+06/9.85e+08 =  0% of the original kernel matrix.

torch.Size([168457, 2])
We keep 3.54e+08/1.66e+10 =  2% of the original kernel matrix.

torch.Size([94729, 2])
We keep 3.25e+07/3.98e+09 =  0% of the original kernel matrix.

torch.Size([59981, 2])
We keep 3.51e+07/1.62e+09 =  2% of the original kernel matrix.

torch.Size([56788, 2])
We keep 1.25e+07/1.24e+09 =  1% of the original kernel matrix.

torch.Size([35241, 2])
We keep 2.40e+07/9.02e+08 =  2% of the original kernel matrix.

torch.Size([41885, 2])
We keep 9.87e+06/9.27e+08 =  1% of the original kernel matrix.

torch.Size([30429, 2])
We keep 1.11e+08/1.76e+09 =  6% of the original kernel matrix.

torch.Size([34619, 2])
We keep 1.31e+07/1.29e+09 =  1% of the original kernel matrix.

torch.Size([12555, 2])
We keep 1.31e+06/4.05e+07 =  3% of the original kernel matrix.

torch.Size([26220, 2])
We keep 3.06e+06/1.96e+08 =  1% of the original kernel matrix.

torch.Size([4880, 2])
We keep 1.67e+05/3.64e+06 =  4% of the original kernel matrix.

torch.Size([17825, 2])
We keep 1.31e+06/5.89e+07 =  2% of the original kernel matrix.

torch.Size([12502, 2])
We keep 7.97e+06/1.39e+08 =  5% of the original kernel matrix.

torch.Size([24263, 2])
We keep 4.86e+06/3.64e+08 =  1% of the original kernel matrix.

torch.Size([158599, 2])
We keep 6.76e+07/7.72e+09 =  0% of the original kernel matrix.

torch.Size([91887, 2])
We keep 2.40e+07/2.71e+09 =  0% of the original kernel matrix.

torch.Size([98165, 2])
We keep 3.43e+07/3.29e+09 =  1% of the original kernel matrix.

torch.Size([73107, 2])
We keep 1.66e+07/1.77e+09 =  0% of the original kernel matrix.

torch.Size([485953, 2])
We keep 9.97e+08/1.09e+11 =  0% of the original kernel matrix.

torch.Size([164525, 2])
We keep 7.62e+07/1.02e+10 =  0% of the original kernel matrix.

torch.Size([117442, 2])
We keep 1.92e+08/9.00e+09 =  2% of the original kernel matrix.

torch.Size([77369, 2])
We keep 2.62e+07/2.93e+09 =  0% of the original kernel matrix.

torch.Size([64673, 2])
We keep 3.88e+07/1.99e+09 =  1% of the original kernel matrix.

torch.Size([58663, 2])
We keep 1.38e+07/1.37e+09 =  1% of the original kernel matrix.

torch.Size([19366, 2])
We keep 2.98e+06/1.79e+08 =  1% of the original kernel matrix.

torch.Size([32427, 2])
We keep 5.13e+06/4.13e+08 =  1% of the original kernel matrix.

torch.Size([139019, 2])
We keep 9.39e+07/7.47e+09 =  1% of the original kernel matrix.

torch.Size([85258, 2])
We keep 2.37e+07/2.67e+09 =  0% of the original kernel matrix.

torch.Size([2345, 2])
We keep 6.21e+04/9.29e+05 =  6% of the original kernel matrix.

torch.Size([13292, 2])
We keep 8.46e+05/2.97e+07 =  2% of the original kernel matrix.

torch.Size([9119, 2])
We keep 7.70e+05/1.96e+07 =  3% of the original kernel matrix.

torch.Size([22316, 2])
We keep 2.34e+06/1.37e+08 =  1% of the original kernel matrix.

torch.Size([273203, 2])
We keep 2.71e+08/2.94e+10 =  0% of the original kernel matrix.

torch.Size([124410, 2])
We keep 4.33e+07/5.29e+09 =  0% of the original kernel matrix.

torch.Size([17352, 2])
We keep 1.70e+06/6.62e+07 =  2% of the original kernel matrix.

torch.Size([30523, 2])
We keep 3.58e+06/2.51e+08 =  1% of the original kernel matrix.

torch.Size([7627, 2])
We keep 4.15e+05/1.06e+07 =  3% of the original kernel matrix.

torch.Size([21175, 2])
We keep 1.88e+06/1.01e+08 =  1% of the original kernel matrix.

torch.Size([19542, 2])
We keep 3.30e+06/1.44e+08 =  2% of the original kernel matrix.

torch.Size([32658, 2])
We keep 4.77e+06/3.70e+08 =  1% of the original kernel matrix.

torch.Size([101776, 2])
We keep 1.88e+08/6.84e+09 =  2% of the original kernel matrix.

torch.Size([71778, 2])
We keep 2.30e+07/2.55e+09 =  0% of the original kernel matrix.

torch.Size([23725, 2])
We keep 6.98e+06/2.19e+08 =  3% of the original kernel matrix.

torch.Size([35061, 2])
We keep 5.72e+06/4.57e+08 =  1% of the original kernel matrix.

torch.Size([70145, 2])
We keep 3.64e+07/2.26e+09 =  1% of the original kernel matrix.

torch.Size([61777, 2])
We keep 1.45e+07/1.47e+09 =  0% of the original kernel matrix.

torch.Size([3424, 2])
We keep 6.86e+05/4.82e+06 = 14% of the original kernel matrix.

torch.Size([14423, 2])
We keep 1.38e+06/6.78e+07 =  2% of the original kernel matrix.

torch.Size([8516, 2])
We keep 2.98e+06/2.24e+07 = 13% of the original kernel matrix.

torch.Size([21533, 2])
We keep 2.40e+06/1.46e+08 =  1% of the original kernel matrix.

torch.Size([5338, 2])
We keep 2.21e+05/4.85e+06 =  4% of the original kernel matrix.

torch.Size([18209, 2])
We keep 1.44e+06/6.80e+07 =  2% of the original kernel matrix.

torch.Size([17197, 2])
We keep 6.99e+06/9.01e+07 =  7% of the original kernel matrix.

torch.Size([30348, 2])
We keep 4.09e+06/2.93e+08 =  1% of the original kernel matrix.

torch.Size([34881, 2])
We keep 6.50e+06/3.84e+08 =  1% of the original kernel matrix.

torch.Size([44166, 2])
We keep 7.11e+06/6.05e+08 =  1% of the original kernel matrix.

torch.Size([35858, 2])
We keep 3.47e+07/8.47e+08 =  4% of the original kernel matrix.

torch.Size([42233, 2])
We keep 9.95e+06/8.98e+08 =  1% of the original kernel matrix.

torch.Size([3916, 2])
We keep 4.25e+05/2.94e+06 = 14% of the original kernel matrix.

torch.Size([16245, 2])
We keep 1.22e+06/5.29e+07 =  2% of the original kernel matrix.

torch.Size([134766, 2])
We keep 1.03e+08/6.82e+09 =  1% of the original kernel matrix.

torch.Size([84252, 2])
We keep 2.30e+07/2.55e+09 =  0% of the original kernel matrix.

torch.Size([2775, 2])
We keep 7.97e+04/1.28e+06 =  6% of the original kernel matrix.

torch.Size([14233, 2])
We keep 9.20e+05/3.49e+07 =  2% of the original kernel matrix.

torch.Size([32262, 2])
We keep 7.58e+06/3.16e+08 =  2% of the original kernel matrix.

torch.Size([42385, 2])
We keep 6.60e+06/5.48e+08 =  1% of the original kernel matrix.

torch.Size([6234, 2])
We keep 1.06e+06/1.31e+07 =  8% of the original kernel matrix.

torch.Size([18862, 2])
We keep 2.04e+06/1.12e+08 =  1% of the original kernel matrix.

torch.Size([31320, 2])
We keep 5.48e+06/3.10e+08 =  1% of the original kernel matrix.

torch.Size([41782, 2])
We keep 6.43e+06/5.43e+08 =  1% of the original kernel matrix.

torch.Size([6190, 2])
We keep 3.42e+05/6.67e+06 =  5% of the original kernel matrix.

torch.Size([19575, 2])
We keep 1.63e+06/7.97e+07 =  2% of the original kernel matrix.

torch.Size([26033, 2])
We keep 1.24e+07/2.86e+08 =  4% of the original kernel matrix.

torch.Size([37421, 2])
We keep 6.40e+06/5.22e+08 =  1% of the original kernel matrix.

torch.Size([17093, 2])
We keep 2.45e+06/7.94e+07 =  3% of the original kernel matrix.

torch.Size([30057, 2])
We keep 3.84e+06/2.75e+08 =  1% of the original kernel matrix.

torch.Size([27145, 2])
We keep 7.88e+06/2.71e+08 =  2% of the original kernel matrix.

torch.Size([38558, 2])
We keep 6.27e+06/5.08e+08 =  1% of the original kernel matrix.

torch.Size([85961, 2])
We keep 4.16e+07/2.82e+09 =  1% of the original kernel matrix.

torch.Size([68400, 2])
We keep 1.57e+07/1.64e+09 =  0% of the original kernel matrix.

torch.Size([200916, 2])
We keep 2.61e+08/1.89e+10 =  1% of the original kernel matrix.

torch.Size([104169, 2])
We keep 3.55e+07/4.25e+09 =  0% of the original kernel matrix.

torch.Size([8414, 2])
We keep 5.73e+05/1.38e+07 =  4% of the original kernel matrix.

torch.Size([22020, 2])
We keep 2.06e+06/1.15e+08 =  1% of the original kernel matrix.

torch.Size([5842, 2])
We keep 2.08e+06/2.01e+07 = 10% of the original kernel matrix.

torch.Size([16565, 2])
We keep 2.38e+06/1.38e+08 =  1% of the original kernel matrix.

torch.Size([95337, 2])
We keep 8.11e+07/4.80e+09 =  1% of the original kernel matrix.

torch.Size([68100, 2])
We keep 1.95e+07/2.14e+09 =  0% of the original kernel matrix.

torch.Size([11401, 2])
We keep 9.60e+05/2.86e+07 =  3% of the original kernel matrix.

torch.Size([24783, 2])
We keep 2.70e+06/1.65e+08 =  1% of the original kernel matrix.

torch.Size([638617, 2])
We keep 8.55e+08/1.42e+11 =  0% of the original kernel matrix.

torch.Size([193969, 2])
We keep 8.53e+07/1.16e+10 =  0% of the original kernel matrix.

torch.Size([1845, 2])
We keep 3.60e+04/5.06e+05 =  7% of the original kernel matrix.

torch.Size([12264, 2])
We keep 6.74e+05/2.19e+07 =  3% of the original kernel matrix.

torch.Size([98870, 2])
We keep 4.54e+07/3.69e+09 =  1% of the original kernel matrix.

torch.Size([72256, 2])
We keep 1.77e+07/1.87e+09 =  0% of the original kernel matrix.

torch.Size([243789, 2])
We keep 1.59e+08/2.19e+10 =  0% of the original kernel matrix.

torch.Size([117415, 2])
We keep 3.75e+07/4.57e+09 =  0% of the original kernel matrix.

torch.Size([3175, 2])
We keep 7.33e+04/1.44e+06 =  5% of the original kernel matrix.

torch.Size([15106, 2])
We keep 9.57e+05/3.71e+07 =  2% of the original kernel matrix.

torch.Size([137420, 2])
We keep 1.88e+08/1.17e+10 =  1% of the original kernel matrix.

torch.Size([82544, 2])
We keep 2.87e+07/3.34e+09 =  0% of the original kernel matrix.

torch.Size([18266, 2])
We keep 8.27e+06/2.87e+08 =  2% of the original kernel matrix.

torch.Size([28496, 2])
We keep 6.37e+06/5.23e+08 =  1% of the original kernel matrix.

torch.Size([461426, 2])
We keep 1.39e+09/1.19e+11 =  1% of the original kernel matrix.

torch.Size([157307, 2])
We keep 7.99e+07/1.06e+10 =  0% of the original kernel matrix.

torch.Size([8892, 2])
We keep 4.10e+05/1.21e+07 =  3% of the original kernel matrix.

torch.Size([22750, 2])
We keep 1.95e+06/1.07e+08 =  1% of the original kernel matrix.

torch.Size([19709, 2])
We keep 6.03e+06/1.76e+08 =  3% of the original kernel matrix.

torch.Size([31488, 2])
We keep 5.23e+06/4.09e+08 =  1% of the original kernel matrix.

torch.Size([4400, 2])
We keep 5.81e+05/5.71e+06 = 10% of the original kernel matrix.

torch.Size([16334, 2])
We keep 1.51e+06/7.37e+07 =  2% of the original kernel matrix.

torch.Size([6286, 2])
We keep 2.66e+05/6.51e+06 =  4% of the original kernel matrix.

torch.Size([19748, 2])
We keep 1.60e+06/7.87e+07 =  2% of the original kernel matrix.

torch.Size([9504, 2])
We keep 1.16e+06/2.68e+07 =  4% of the original kernel matrix.

torch.Size([22665, 2])
We keep 2.61e+06/1.60e+08 =  1% of the original kernel matrix.

torch.Size([28071, 2])
We keep 6.37e+06/2.86e+08 =  2% of the original kernel matrix.

torch.Size([39202, 2])
We keep 6.10e+06/5.22e+08 =  1% of the original kernel matrix.

torch.Size([9834, 2])
We keep 1.99e+06/3.36e+07 =  5% of the original kernel matrix.

torch.Size([22659, 2])
We keep 2.83e+06/1.79e+08 =  1% of the original kernel matrix.

torch.Size([61998, 2])
We keep 3.71e+07/1.95e+09 =  1% of the original kernel matrix.

torch.Size([56857, 2])
We keep 1.36e+07/1.36e+09 =  1% of the original kernel matrix.

torch.Size([18343, 2])
We keep 1.97e+06/8.55e+07 =  2% of the original kernel matrix.

torch.Size([31046, 2])
We keep 4.01e+06/2.85e+08 =  1% of the original kernel matrix.

torch.Size([2558, 2])
We keep 4.33e+04/7.24e+05 =  5% of the original kernel matrix.

torch.Size([14339, 2])
We keep 7.77e+05/2.63e+07 =  2% of the original kernel matrix.

torch.Size([10114, 2])
We keep 1.77e+06/3.89e+07 =  4% of the original kernel matrix.

torch.Size([22605, 2])
We keep 3.02e+06/1.93e+08 =  1% of the original kernel matrix.

torch.Size([95504, 2])
We keep 1.44e+08/7.07e+09 =  2% of the original kernel matrix.

torch.Size([68934, 2])
We keep 2.33e+07/2.60e+09 =  0% of the original kernel matrix.

torch.Size([19751, 2])
We keep 1.89e+07/4.98e+08 =  3% of the original kernel matrix.

torch.Size([30653, 2])
We keep 7.87e+06/6.88e+08 =  1% of the original kernel matrix.

torch.Size([249651, 2])
We keep 1.85e+08/2.17e+10 =  0% of the original kernel matrix.

torch.Size([118552, 2])
We keep 3.73e+07/4.54e+09 =  0% of the original kernel matrix.

torch.Size([29144, 2])
We keep 1.47e+07/3.45e+08 =  4% of the original kernel matrix.

torch.Size([39424, 2])
We keep 6.74e+06/5.73e+08 =  1% of the original kernel matrix.

torch.Size([12791, 2])
We keep 1.64e+06/4.89e+07 =  3% of the original kernel matrix.

torch.Size([26489, 2])
We keep 3.24e+06/2.16e+08 =  1% of the original kernel matrix.

torch.Size([6082, 2])
We keep 2.82e+05/5.99e+06 =  4% of the original kernel matrix.

torch.Size([19571, 2])
We keep 1.54e+06/7.55e+07 =  2% of the original kernel matrix.

torch.Size([5606, 2])
We keep 6.33e+05/1.02e+07 =  6% of the original kernel matrix.

torch.Size([17666, 2])
We keep 1.85e+06/9.85e+07 =  1% of the original kernel matrix.

torch.Size([338707, 2])
We keep 5.76e+08/5.06e+10 =  1% of the original kernel matrix.

torch.Size([139358, 2])
We keep 5.28e+07/6.94e+09 =  0% of the original kernel matrix.

torch.Size([15202, 2])
We keep 1.39e+06/5.11e+07 =  2% of the original kernel matrix.

torch.Size([28741, 2])
We keep 3.31e+06/2.21e+08 =  1% of the original kernel matrix.

torch.Size([14902, 2])
We keep 1.70e+06/5.25e+07 =  3% of the original kernel matrix.

torch.Size([28444, 2])
We keep 3.35e+06/2.24e+08 =  1% of the original kernel matrix.

torch.Size([32311, 2])
We keep 1.77e+07/6.18e+08 =  2% of the original kernel matrix.

torch.Size([40643, 2])
We keep 8.65e+06/7.67e+08 =  1% of the original kernel matrix.

torch.Size([25623, 2])
We keep 1.31e+07/4.56e+08 =  2% of the original kernel matrix.

torch.Size([35133, 2])
We keep 7.57e+06/6.59e+08 =  1% of the original kernel matrix.

torch.Size([4023, 2])
We keep 1.61e+05/2.83e+06 =  5% of the original kernel matrix.

torch.Size([16289, 2])
We keep 1.20e+06/5.19e+07 =  2% of the original kernel matrix.

torch.Size([8275, 2])
We keep 5.31e+05/1.43e+07 =  3% of the original kernel matrix.

torch.Size([21563, 2])
We keep 2.10e+06/1.17e+08 =  1% of the original kernel matrix.

torch.Size([267186, 2])
We keep 5.93e+08/4.08e+10 =  1% of the original kernel matrix.

torch.Size([120120, 2])
We keep 4.96e+07/6.23e+09 =  0% of the original kernel matrix.

torch.Size([15673, 2])
We keep 2.31e+06/6.94e+07 =  3% of the original kernel matrix.

torch.Size([28997, 2])
We keep 3.69e+06/2.57e+08 =  1% of the original kernel matrix.

torch.Size([23537, 2])
We keep 2.54e+07/5.06e+08 =  5% of the original kernel matrix.

torch.Size([33721, 2])
We keep 7.83e+06/6.94e+08 =  1% of the original kernel matrix.

torch.Size([32504, 2])
We keep 1.03e+07/3.82e+08 =  2% of the original kernel matrix.

torch.Size([42499, 2])
We keep 7.08e+06/6.03e+08 =  1% of the original kernel matrix.

torch.Size([65857, 2])
We keep 1.24e+08/2.47e+09 =  5% of the original kernel matrix.

torch.Size([59832, 2])
We keep 1.52e+07/1.53e+09 =  0% of the original kernel matrix.

torch.Size([115966, 2])
We keep 1.28e+08/5.26e+09 =  2% of the original kernel matrix.

torch.Size([78177, 2])
We keep 2.00e+07/2.24e+09 =  0% of the original kernel matrix.

torch.Size([162062, 2])
We keep 2.48e+08/1.65e+10 =  1% of the original kernel matrix.

torch.Size([90662, 2])
We keep 3.35e+07/3.97e+09 =  0% of the original kernel matrix.

torch.Size([91554, 2])
We keep 4.24e+07/2.80e+09 =  1% of the original kernel matrix.

torch.Size([70789, 2])
We keep 1.54e+07/1.63e+09 =  0% of the original kernel matrix.

torch.Size([7050, 2])
We keep 5.93e+05/1.09e+07 =  5% of the original kernel matrix.

torch.Size([20604, 2])
We keep 1.89e+06/1.02e+08 =  1% of the original kernel matrix.

torch.Size([54410, 2])
We keep 1.21e+07/1.03e+09 =  1% of the original kernel matrix.

torch.Size([55195, 2])
We keep 1.04e+07/9.89e+08 =  1% of the original kernel matrix.

torch.Size([12835, 2])
We keep 2.51e+06/5.69e+07 =  4% of the original kernel matrix.

torch.Size([26034, 2])
We keep 3.37e+06/2.33e+08 =  1% of the original kernel matrix.

torch.Size([11252, 2])
We keep 1.19e+06/2.84e+07 =  4% of the original kernel matrix.

torch.Size([24897, 2])
We keep 2.70e+06/1.64e+08 =  1% of the original kernel matrix.

torch.Size([20513, 2])
We keep 2.90e+06/1.20e+08 =  2% of the original kernel matrix.

torch.Size([33205, 2])
We keep 4.49e+06/3.38e+08 =  1% of the original kernel matrix.

torch.Size([6155, 2])
We keep 2.44e+05/6.24e+06 =  3% of the original kernel matrix.

torch.Size([19744, 2])
We keep 1.55e+06/7.70e+07 =  2% of the original kernel matrix.

torch.Size([32879, 2])
We keep 4.45e+07/1.03e+09 =  4% of the original kernel matrix.

torch.Size([40093, 2])
We keep 1.05e+07/9.88e+08 =  1% of the original kernel matrix.

torch.Size([13854, 2])
We keep 1.31e+06/4.34e+07 =  3% of the original kernel matrix.

torch.Size([27108, 2])
We keep 3.10e+06/2.03e+08 =  1% of the original kernel matrix.

torch.Size([2868, 2])
We keep 9.12e+04/1.32e+06 =  6% of the original kernel matrix.

torch.Size([13940, 2])
We keep 9.47e+05/3.55e+07 =  2% of the original kernel matrix.

torch.Size([20368, 2])
We keep 2.99e+06/1.15e+08 =  2% of the original kernel matrix.

torch.Size([32935, 2])
We keep 4.49e+06/3.31e+08 =  1% of the original kernel matrix.

torch.Size([5329, 2])
We keep 6.63e+05/1.19e+07 =  5% of the original kernel matrix.

torch.Size([17371, 2])
We keep 1.82e+06/1.06e+08 =  1% of the original kernel matrix.

torch.Size([59978, 2])
We keep 1.72e+08/4.97e+09 =  3% of the original kernel matrix.

torch.Size([49211, 2])
We keep 2.03e+07/2.17e+09 =  0% of the original kernel matrix.

torch.Size([10191, 2])
We keep 1.78e+06/3.81e+07 =  4% of the original kernel matrix.

torch.Size([23084, 2])
We keep 2.90e+06/1.91e+08 =  1% of the original kernel matrix.

torch.Size([24355, 2])
We keep 4.55e+06/1.61e+08 =  2% of the original kernel matrix.

torch.Size([36418, 2])
We keep 5.00e+06/3.92e+08 =  1% of the original kernel matrix.

torch.Size([7229, 2])
We keep 4.46e+06/2.34e+07 = 19% of the original kernel matrix.

torch.Size([19248, 2])
We keep 2.46e+06/1.49e+08 =  1% of the original kernel matrix.

torch.Size([21075, 2])
We keep 5.26e+06/1.77e+08 =  2% of the original kernel matrix.

torch.Size([33416, 2])
We keep 5.28e+06/4.11e+08 =  1% of the original kernel matrix.

torch.Size([4905, 2])
We keep 2.12e+05/4.28e+06 =  4% of the original kernel matrix.

torch.Size([17735, 2])
We keep 1.39e+06/6.39e+07 =  2% of the original kernel matrix.

torch.Size([9177, 2])
We keep 5.01e+05/1.45e+07 =  3% of the original kernel matrix.

torch.Size([22890, 2])
We keep 2.10e+06/1.18e+08 =  1% of the original kernel matrix.

torch.Size([28442, 2])
We keep 4.01e+06/2.17e+08 =  1% of the original kernel matrix.

torch.Size([39691, 2])
We keep 5.63e+06/4.55e+08 =  1% of the original kernel matrix.

torch.Size([115310, 2])
We keep 8.10e+07/5.66e+09 =  1% of the original kernel matrix.

torch.Size([78751, 2])
We keep 2.12e+07/2.32e+09 =  0% of the original kernel matrix.

torch.Size([10621, 2])
We keep 8.08e+05/2.34e+07 =  3% of the original kernel matrix.

torch.Size([24110, 2])
We keep 2.46e+06/1.49e+08 =  1% of the original kernel matrix.

torch.Size([2560, 2])
We keep 4.20e+04/7.01e+05 =  5% of the original kernel matrix.

torch.Size([14202, 2])
We keep 7.54e+05/2.58e+07 =  2% of the original kernel matrix.

torch.Size([4858, 2])
We keep 1.96e+05/4.11e+06 =  4% of the original kernel matrix.

torch.Size([17601, 2])
We keep 1.35e+06/6.25e+07 =  2% of the original kernel matrix.

torch.Size([176665, 2])
We keep 2.69e+08/1.65e+10 =  1% of the original kernel matrix.

torch.Size([96875, 2])
We keep 3.33e+07/3.96e+09 =  0% of the original kernel matrix.

torch.Size([564434, 2])
We keep 1.64e+09/1.28e+11 =  1% of the original kernel matrix.

torch.Size([182116, 2])
We keep 8.28e+07/1.10e+10 =  0% of the original kernel matrix.

torch.Size([18135, 2])
We keep 4.14e+06/1.36e+08 =  3% of the original kernel matrix.

torch.Size([30128, 2])
We keep 4.73e+06/3.60e+08 =  1% of the original kernel matrix.

torch.Size([8149, 2])
We keep 5.17e+05/1.35e+07 =  3% of the original kernel matrix.

torch.Size([21679, 2])
We keep 2.06e+06/1.13e+08 =  1% of the original kernel matrix.

torch.Size([54890, 2])
We keep 4.13e+07/2.03e+09 =  2% of the original kernel matrix.

torch.Size([51940, 2])
We keep 1.39e+07/1.39e+09 =  0% of the original kernel matrix.

torch.Size([120624, 2])
We keep 4.93e+07/5.06e+09 =  0% of the original kernel matrix.

torch.Size([80578, 2])
We keep 2.03e+07/2.19e+09 =  0% of the original kernel matrix.

torch.Size([3985, 2])
We keep 1.01e+05/2.21e+06 =  4% of the original kernel matrix.

torch.Size([16637, 2])
We keep 1.11e+06/4.59e+07 =  2% of the original kernel matrix.

torch.Size([14394, 2])
We keep 2.20e+06/6.69e+07 =  3% of the original kernel matrix.

torch.Size([27845, 2])
We keep 3.61e+06/2.52e+08 =  1% of the original kernel matrix.

torch.Size([20478, 2])
We keep 1.99e+06/9.63e+07 =  2% of the original kernel matrix.

torch.Size([33265, 2])
We keep 4.12e+06/3.03e+08 =  1% of the original kernel matrix.

torch.Size([56830, 2])
We keep 1.75e+08/4.68e+09 =  3% of the original kernel matrix.

torch.Size([49086, 2])
We keep 1.97e+07/2.11e+09 =  0% of the original kernel matrix.

torch.Size([44573, 2])
We keep 1.30e+08/2.14e+09 =  6% of the original kernel matrix.

torch.Size([46862, 2])
We keep 1.44e+07/1.43e+09 =  1% of the original kernel matrix.

torch.Size([198887, 2])
We keep 1.47e+08/1.45e+10 =  1% of the original kernel matrix.

torch.Size([104719, 2])
We keep 3.15e+07/3.71e+09 =  0% of the original kernel matrix.

torch.Size([11481, 2])
We keep 7.86e+05/2.30e+07 =  3% of the original kernel matrix.

torch.Size([25446, 2])
We keep 2.48e+06/1.48e+08 =  1% of the original kernel matrix.

torch.Size([5927, 2])
We keep 2.99e+05/5.99e+06 =  4% of the original kernel matrix.

torch.Size([19356, 2])
We keep 1.53e+06/7.55e+07 =  2% of the original kernel matrix.

torch.Size([2996, 2])
We keep 6.13e+04/1.13e+06 =  5% of the original kernel matrix.

torch.Size([15095, 2])
We keep 8.90e+05/3.28e+07 =  2% of the original kernel matrix.

torch.Size([32849, 2])
We keep 9.27e+06/3.93e+08 =  2% of the original kernel matrix.

torch.Size([42362, 2])
We keep 7.19e+06/6.12e+08 =  1% of the original kernel matrix.

torch.Size([16120, 2])
We keep 1.99e+06/5.98e+07 =  3% of the original kernel matrix.

torch.Size([29625, 2])
We keep 3.50e+06/2.39e+08 =  1% of the original kernel matrix.

torch.Size([25128, 2])
We keep 8.53e+06/2.90e+08 =  2% of the original kernel matrix.

torch.Size([36263, 2])
We keep 6.30e+06/5.25e+08 =  1% of the original kernel matrix.

torch.Size([93231, 2])
We keep 7.63e+07/3.36e+09 =  2% of the original kernel matrix.

torch.Size([70750, 2])
We keep 1.71e+07/1.79e+09 =  0% of the original kernel matrix.

torch.Size([424187, 2])
We keep 4.07e+08/6.53e+10 =  0% of the original kernel matrix.

torch.Size([159196, 2])
We keep 6.02e+07/7.88e+09 =  0% of the original kernel matrix.

torch.Size([10088, 2])
We keep 7.32e+05/2.08e+07 =  3% of the original kernel matrix.

torch.Size([23555, 2])
We keep 2.40e+06/1.41e+08 =  1% of the original kernel matrix.

torch.Size([4096, 2])
We keep 1.41e+05/2.69e+06 =  5% of the original kernel matrix.

torch.Size([16565, 2])
We keep 1.19e+06/5.06e+07 =  2% of the original kernel matrix.

torch.Size([17543, 2])
We keep 1.58e+06/6.47e+07 =  2% of the original kernel matrix.

torch.Size([30821, 2])
We keep 3.56e+06/2.48e+08 =  1% of the original kernel matrix.

torch.Size([439621, 2])
We keep 4.79e+09/1.63e+11 =  2% of the original kernel matrix.

torch.Size([158485, 2])
We keep 9.16e+07/1.24e+10 =  0% of the original kernel matrix.

torch.Size([33985, 2])
We keep 6.19e+06/3.46e+08 =  1% of the original kernel matrix.

torch.Size([43355, 2])
We keep 6.77e+06/5.74e+08 =  1% of the original kernel matrix.

torch.Size([3103, 2])
We keep 2.62e+05/2.29e+06 = 11% of the original kernel matrix.

torch.Size([14380, 2])
We keep 1.12e+06/4.67e+07 =  2% of the original kernel matrix.

torch.Size([4045, 2])
We keep 1.96e+05/3.39e+06 =  5% of the original kernel matrix.

torch.Size([15972, 2])
We keep 1.24e+06/5.68e+07 =  2% of the original kernel matrix.

torch.Size([35837, 2])
We keep 8.23e+06/4.46e+08 =  1% of the original kernel matrix.

torch.Size([44710, 2])
We keep 7.54e+06/6.51e+08 =  1% of the original kernel matrix.

torch.Size([2353, 2])
We keep 5.48e+04/8.05e+05 =  6% of the original kernel matrix.

torch.Size([13389, 2])
We keep 7.98e+05/2.77e+07 =  2% of the original kernel matrix.

torch.Size([31476, 2])
We keep 4.22e+07/9.75e+08 =  4% of the original kernel matrix.

torch.Size([37670, 2])
We keep 1.03e+07/9.63e+08 =  1% of the original kernel matrix.

torch.Size([5592, 2])
We keep 2.39e+05/5.47e+06 =  4% of the original kernel matrix.

torch.Size([18888, 2])
We keep 1.53e+06/7.21e+07 =  2% of the original kernel matrix.

torch.Size([20728, 2])
We keep 1.87e+06/9.31e+07 =  2% of the original kernel matrix.

torch.Size([33609, 2])
We keep 4.07e+06/2.98e+08 =  1% of the original kernel matrix.

torch.Size([12396, 2])
We keep 1.57e+06/3.72e+07 =  4% of the original kernel matrix.

torch.Size([25867, 2])
We keep 2.97e+06/1.88e+08 =  1% of the original kernel matrix.

torch.Size([6475, 2])
We keep 2.36e+05/6.37e+06 =  3% of the original kernel matrix.

torch.Size([20045, 2])
We keep 1.57e+06/7.79e+07 =  2% of the original kernel matrix.

torch.Size([4131, 2])
We keep 1.15e+05/2.43e+06 =  4% of the original kernel matrix.

torch.Size([16780, 2])
We keep 1.15e+06/4.81e+07 =  2% of the original kernel matrix.

torch.Size([207865, 2])
We keep 1.15e+08/1.48e+10 =  0% of the original kernel matrix.

torch.Size([107150, 2])
We keep 3.16e+07/3.76e+09 =  0% of the original kernel matrix.

torch.Size([454354, 2])
We keep 7.39e+08/8.74e+10 =  0% of the original kernel matrix.

torch.Size([163438, 2])
We keep 6.97e+07/9.12e+09 =  0% of the original kernel matrix.

torch.Size([105508, 2])
We keep 9.78e+07/4.79e+09 =  2% of the original kernel matrix.

torch.Size([74967, 2])
We keep 1.98e+07/2.14e+09 =  0% of the original kernel matrix.

torch.Size([40193, 2])
We keep 5.01e+07/9.28e+08 =  5% of the original kernel matrix.

torch.Size([45798, 2])
We keep 1.02e+07/9.40e+08 =  1% of the original kernel matrix.

torch.Size([27782, 2])
We keep 3.47e+06/2.10e+08 =  1% of the original kernel matrix.

torch.Size([39171, 2])
We keep 5.57e+06/4.47e+08 =  1% of the original kernel matrix.

torch.Size([14696, 2])
We keep 2.04e+06/5.18e+07 =  3% of the original kernel matrix.

torch.Size([28190, 2])
We keep 3.33e+06/2.22e+08 =  1% of the original kernel matrix.

torch.Size([12419, 2])
We keep 1.05e+06/3.26e+07 =  3% of the original kernel matrix.

torch.Size([26188, 2])
We keep 2.81e+06/1.76e+08 =  1% of the original kernel matrix.

torch.Size([80061, 2])
We keep 2.48e+07/2.23e+09 =  1% of the original kernel matrix.

torch.Size([66914, 2])
We keep 1.43e+07/1.46e+09 =  0% of the original kernel matrix.

torch.Size([33078, 2])
We keep 8.17e+06/3.61e+08 =  2% of the original kernel matrix.

torch.Size([43429, 2])
We keep 7.00e+06/5.86e+08 =  1% of the original kernel matrix.

torch.Size([68836, 2])
We keep 8.69e+07/2.68e+09 =  3% of the original kernel matrix.

torch.Size([60294, 2])
We keep 1.55e+07/1.60e+09 =  0% of the original kernel matrix.

torch.Size([14307, 2])
We keep 1.82e+06/4.98e+07 =  3% of the original kernel matrix.

torch.Size([27562, 2])
We keep 3.29e+06/2.18e+08 =  1% of the original kernel matrix.

torch.Size([687276, 2])
We keep 1.61e+09/2.02e+11 =  0% of the original kernel matrix.

torch.Size([195724, 2])
We keep 1.01e+08/1.39e+10 =  0% of the original kernel matrix.

torch.Size([19300, 2])
We keep 3.05e+06/9.62e+07 =  3% of the original kernel matrix.

torch.Size([32287, 2])
We keep 4.17e+06/3.03e+08 =  1% of the original kernel matrix.

torch.Size([42323, 2])
We keep 1.32e+07/7.39e+08 =  1% of the original kernel matrix.

torch.Size([48716, 2])
We keep 9.33e+06/8.39e+08 =  1% of the original kernel matrix.

torch.Size([163620, 2])
We keep 1.57e+08/1.14e+10 =  1% of the original kernel matrix.

torch.Size([94286, 2])
We keep 2.84e+07/3.30e+09 =  0% of the original kernel matrix.

torch.Size([57547, 2])
We keep 1.32e+07/1.12e+09 =  1% of the original kernel matrix.

torch.Size([57599, 2])
We keep 1.08e+07/1.03e+09 =  1% of the original kernel matrix.

torch.Size([74885, 2])
We keep 1.11e+08/3.34e+09 =  3% of the original kernel matrix.

torch.Size([62223, 2])
We keep 1.67e+07/1.78e+09 =  0% of the original kernel matrix.

torch.Size([228970, 2])
We keep 2.84e+08/2.42e+10 =  1% of the original kernel matrix.

torch.Size([111472, 2])
We keep 3.97e+07/4.80e+09 =  0% of the original kernel matrix.

torch.Size([51476, 2])
We keep 1.05e+07/9.06e+08 =  1% of the original kernel matrix.

torch.Size([54502, 2])
We keep 1.01e+07/9.29e+08 =  1% of the original kernel matrix.

torch.Size([39586, 2])
We keep 1.20e+07/5.96e+08 =  2% of the original kernel matrix.

torch.Size([47402, 2])
We keep 8.52e+06/7.54e+08 =  1% of the original kernel matrix.

torch.Size([11224, 2])
We keep 1.70e+06/3.38e+07 =  5% of the original kernel matrix.

torch.Size([24877, 2])
We keep 2.86e+06/1.79e+08 =  1% of the original kernel matrix.

torch.Size([36929, 2])
We keep 8.90e+06/4.80e+08 =  1% of the original kernel matrix.

torch.Size([45202, 2])
We keep 7.63e+06/6.76e+08 =  1% of the original kernel matrix.

torch.Size([8476, 2])
We keep 5.32e+05/1.36e+07 =  3% of the original kernel matrix.

torch.Size([22494, 2])
We keep 2.03e+06/1.14e+08 =  1% of the original kernel matrix.

torch.Size([155753, 2])
We keep 1.04e+08/7.80e+09 =  1% of the original kernel matrix.

torch.Size([91464, 2])
We keep 2.41e+07/2.72e+09 =  0% of the original kernel matrix.

torch.Size([965702, 2])
We keep 2.18e+09/3.13e+11 =  0% of the original kernel matrix.

torch.Size([238306, 2])
We keep 1.23e+08/1.73e+10 =  0% of the original kernel matrix.

torch.Size([93001, 2])
We keep 1.54e+08/5.01e+09 =  3% of the original kernel matrix.

torch.Size([69634, 2])
We keep 2.04e+07/2.18e+09 =  0% of the original kernel matrix.

torch.Size([94883, 2])
We keep 1.06e+08/7.23e+09 =  1% of the original kernel matrix.

torch.Size([62890, 2])
We keep 2.36e+07/2.62e+09 =  0% of the original kernel matrix.

torch.Size([18636, 2])
We keep 9.08e+06/1.51e+08 =  6% of the original kernel matrix.

torch.Size([31165, 2])
We keep 4.86e+06/3.80e+08 =  1% of the original kernel matrix.

torch.Size([18014, 2])
We keep 1.39e+07/2.74e+08 =  5% of the original kernel matrix.

torch.Size([29380, 2])
We keep 6.19e+06/5.10e+08 =  1% of the original kernel matrix.

torch.Size([62191, 2])
We keep 1.96e+07/1.49e+09 =  1% of the original kernel matrix.

torch.Size([59987, 2])
We keep 1.24e+07/1.19e+09 =  1% of the original kernel matrix.

torch.Size([10364, 2])
We keep 1.39e+06/3.39e+07 =  4% of the original kernel matrix.

torch.Size([23368, 2])
We keep 2.79e+06/1.80e+08 =  1% of the original kernel matrix.

torch.Size([76560, 2])
We keep 1.07e+08/3.82e+09 =  2% of the original kernel matrix.

torch.Size([61608, 2])
We keep 1.83e+07/1.91e+09 =  0% of the original kernel matrix.

torch.Size([237982, 2])
We keep 1.44e+08/1.98e+10 =  0% of the original kernel matrix.

torch.Size([115462, 2])
We keep 3.57e+07/4.34e+09 =  0% of the original kernel matrix.

torch.Size([12749, 2])
We keep 8.44e+06/1.69e+08 =  4% of the original kernel matrix.

torch.Size([24038, 2])
We keep 5.16e+06/4.02e+08 =  1% of the original kernel matrix.

torch.Size([254747, 2])
We keep 1.30e+09/5.80e+10 =  2% of the original kernel matrix.

torch.Size([113803, 2])
We keep 5.83e+07/7.43e+09 =  0% of the original kernel matrix.

torch.Size([125807, 2])
We keep 2.43e+08/1.15e+10 =  2% of the original kernel matrix.

torch.Size([80312, 2])
We keep 2.85e+07/3.30e+09 =  0% of the original kernel matrix.

torch.Size([27990, 2])
We keep 4.65e+06/2.47e+08 =  1% of the original kernel matrix.

torch.Size([39175, 2])
We keep 5.92e+06/4.85e+08 =  1% of the original kernel matrix.

torch.Size([3911, 2])
We keep 1.29e+05/2.74e+06 =  4% of the original kernel matrix.

torch.Size([16317, 2])
We keep 1.20e+06/5.11e+07 =  2% of the original kernel matrix.

torch.Size([57890, 2])
We keep 1.75e+07/1.26e+09 =  1% of the original kernel matrix.

torch.Size([56990, 2])
We keep 1.14e+07/1.10e+09 =  1% of the original kernel matrix.

torch.Size([89770, 2])
We keep 2.84e+07/2.60e+09 =  1% of the original kernel matrix.

torch.Size([70200, 2])
We keep 1.52e+07/1.57e+09 =  0% of the original kernel matrix.

torch.Size([4049, 2])
We keep 1.54e+05/3.08e+06 =  5% of the original kernel matrix.

torch.Size([16446, 2])
We keep 1.23e+06/5.41e+07 =  2% of the original kernel matrix.

torch.Size([474888, 2])
We keep 4.69e+08/8.46e+10 =  0% of the original kernel matrix.

torch.Size([169277, 2])
We keep 6.81e+07/8.97e+09 =  0% of the original kernel matrix.

torch.Size([28568, 2])
We keep 1.28e+07/4.08e+08 =  3% of the original kernel matrix.

torch.Size([39576, 2])
We keep 7.03e+06/6.23e+08 =  1% of the original kernel matrix.

torch.Size([21314, 2])
We keep 3.32e+06/1.29e+08 =  2% of the original kernel matrix.

torch.Size([33981, 2])
We keep 4.69e+06/3.50e+08 =  1% of the original kernel matrix.

torch.Size([9065, 2])
We keep 5.14e+05/1.49e+07 =  3% of the original kernel matrix.

torch.Size([23121, 2])
We keep 2.09e+06/1.19e+08 =  1% of the original kernel matrix.

torch.Size([3590, 2])
We keep 1.10e+05/2.04e+06 =  5% of the original kernel matrix.

torch.Size([15808, 2])
We keep 1.09e+06/4.41e+07 =  2% of the original kernel matrix.

torch.Size([3255, 2])
We keep 9.21e+04/1.71e+06 =  5% of the original kernel matrix.

torch.Size([15165, 2])
We keep 1.02e+06/4.04e+07 =  2% of the original kernel matrix.

torch.Size([8053, 2])
We keep 1.09e+06/1.98e+07 =  5% of the original kernel matrix.

torch.Size([21041, 2])
We keep 2.42e+06/1.37e+08 =  1% of the original kernel matrix.

torch.Size([21541, 2])
We keep 6.83e+06/1.63e+08 =  4% of the original kernel matrix.

torch.Size([34032, 2])
We keep 5.15e+06/3.95e+08 =  1% of the original kernel matrix.

torch.Size([32232, 2])
We keep 9.19e+06/3.67e+08 =  2% of the original kernel matrix.

torch.Size([41762, 2])
We keep 6.93e+06/5.91e+08 =  1% of the original kernel matrix.

torch.Size([2417833, 2])
We keep 2.84e+10/3.43e+12 =  0% of the original kernel matrix.

torch.Size([358100, 2])
We keep 3.70e+08/5.71e+10 =  0% of the original kernel matrix.

torch.Size([9485, 2])
We keep 1.05e+06/2.51e+07 =  4% of the original kernel matrix.

torch.Size([22882, 2])
We keep 2.56e+06/1.55e+08 =  1% of the original kernel matrix.

torch.Size([31295, 2])
We keep 5.90e+06/3.35e+08 =  1% of the original kernel matrix.

torch.Size([41624, 2])
We keep 6.60e+06/5.65e+08 =  1% of the original kernel matrix.

torch.Size([5720, 2])
We keep 2.88e+05/5.91e+06 =  4% of the original kernel matrix.

torch.Size([18681, 2])
We keep 1.55e+06/7.50e+07 =  2% of the original kernel matrix.

torch.Size([17287, 2])
We keep 1.72e+06/6.47e+07 =  2% of the original kernel matrix.

torch.Size([30273, 2])
We keep 3.63e+06/2.48e+08 =  1% of the original kernel matrix.

torch.Size([11979, 2])
We keep 1.18e+06/3.63e+07 =  3% of the original kernel matrix.

torch.Size([25594, 2])
We keep 2.99e+06/1.86e+08 =  1% of the original kernel matrix.

torch.Size([85726, 2])
We keep 2.20e+08/7.46e+09 =  2% of the original kernel matrix.

torch.Size([61513, 2])
We keep 2.39e+07/2.67e+09 =  0% of the original kernel matrix.

torch.Size([25356, 2])
We keep 2.20e+07/3.94e+08 =  5% of the original kernel matrix.

torch.Size([35881, 2])
We keep 7.30e+06/6.13e+08 =  1% of the original kernel matrix.

torch.Size([835834, 2])
We keep 1.35e+09/2.35e+11 =  0% of the original kernel matrix.

torch.Size([221735, 2])
We keep 1.08e+08/1.49e+10 =  0% of the original kernel matrix.

torch.Size([7507, 2])
We keep 4.32e+05/1.11e+07 =  3% of the original kernel matrix.

torch.Size([20910, 2])
We keep 1.92e+06/1.03e+08 =  1% of the original kernel matrix.

torch.Size([200328, 2])
We keep 1.87e+08/1.51e+10 =  1% of the original kernel matrix.

torch.Size([105291, 2])
We keep 3.23e+07/3.79e+09 =  0% of the original kernel matrix.

torch.Size([27849, 2])
We keep 1.55e+07/3.77e+08 =  4% of the original kernel matrix.

torch.Size([38490, 2])
We keep 6.88e+06/5.99e+08 =  1% of the original kernel matrix.

torch.Size([123577, 2])
We keep 8.42e+07/6.24e+09 =  1% of the original kernel matrix.

torch.Size([80824, 2])
We keep 2.21e+07/2.44e+09 =  0% of the original kernel matrix.

torch.Size([317865, 2])
We keep 5.00e+08/5.32e+10 =  0% of the original kernel matrix.

torch.Size([133341, 2])
We keep 5.52e+07/7.12e+09 =  0% of the original kernel matrix.

torch.Size([18397, 2])
We keep 2.20e+06/7.87e+07 =  2% of the original kernel matrix.

torch.Size([31220, 2])
We keep 3.88e+06/2.74e+08 =  1% of the original kernel matrix.

torch.Size([7343, 2])
We keep 5.18e+05/1.06e+07 =  4% of the original kernel matrix.

torch.Size([20760, 2])
We keep 1.88e+06/1.00e+08 =  1% of the original kernel matrix.

torch.Size([107878, 2])
We keep 3.76e+07/3.82e+09 =  0% of the original kernel matrix.

torch.Size([76588, 2])
We keep 1.79e+07/1.91e+09 =  0% of the original kernel matrix.

torch.Size([10338, 2])
We keep 1.28e+06/3.14e+07 =  4% of the original kernel matrix.

torch.Size([23655, 2])
We keep 2.76e+06/1.73e+08 =  1% of the original kernel matrix.

torch.Size([27697, 2])
We keep 1.81e+07/4.50e+08 =  4% of the original kernel matrix.

torch.Size([36670, 2])
We keep 7.60e+06/6.55e+08 =  1% of the original kernel matrix.

torch.Size([72813, 2])
We keep 2.37e+07/1.88e+09 =  1% of the original kernel matrix.

torch.Size([63037, 2])
We keep 1.34e+07/1.34e+09 =  1% of the original kernel matrix.

torch.Size([14303, 2])
We keep 1.69e+07/2.05e+08 =  8% of the original kernel matrix.

torch.Size([26657, 2])
We keep 5.49e+06/4.42e+08 =  1% of the original kernel matrix.

torch.Size([56920, 2])
We keep 2.47e+07/1.25e+09 =  1% of the original kernel matrix.

torch.Size([57616, 2])
We keep 1.15e+07/1.09e+09 =  1% of the original kernel matrix.

torch.Size([18275, 2])
We keep 3.02e+06/9.22e+07 =  3% of the original kernel matrix.

torch.Size([30950, 2])
We keep 4.14e+06/2.96e+08 =  1% of the original kernel matrix.

torch.Size([106213, 2])
We keep 8.70e+07/4.85e+09 =  1% of the original kernel matrix.

torch.Size([75282, 2])
We keep 2.00e+07/2.15e+09 =  0% of the original kernel matrix.

torch.Size([285776, 2])
We keep 4.73e+08/3.93e+10 =  1% of the original kernel matrix.

torch.Size([124625, 2])
We keep 4.87e+07/6.12e+09 =  0% of the original kernel matrix.

torch.Size([15126, 2])
We keep 1.48e+06/5.08e+07 =  2% of the original kernel matrix.

torch.Size([28514, 2])
We keep 3.26e+06/2.20e+08 =  1% of the original kernel matrix.

torch.Size([8346, 2])
We keep 6.32e+05/1.53e+07 =  4% of the original kernel matrix.

torch.Size([21870, 2])
We keep 2.16e+06/1.21e+08 =  1% of the original kernel matrix.

torch.Size([108001, 2])
We keep 1.65e+08/7.51e+09 =  2% of the original kernel matrix.

torch.Size([73321, 2])
We keep 2.41e+07/2.67e+09 =  0% of the original kernel matrix.

torch.Size([36446, 2])
We keep 9.91e+06/5.37e+08 =  1% of the original kernel matrix.

torch.Size([44304, 2])
We keep 8.09e+06/7.15e+08 =  1% of the original kernel matrix.

torch.Size([113806, 2])
We keep 2.51e+08/5.72e+09 =  4% of the original kernel matrix.

torch.Size([77035, 2])
We keep 2.15e+07/2.33e+09 =  0% of the original kernel matrix.

torch.Size([25138, 2])
We keep 1.43e+07/3.38e+08 =  4% of the original kernel matrix.

torch.Size([35317, 2])
We keep 6.81e+06/5.67e+08 =  1% of the original kernel matrix.

torch.Size([38892, 2])
We keep 2.22e+07/1.01e+09 =  2% of the original kernel matrix.

torch.Size([44188, 2])
We keep 1.04e+07/9.82e+08 =  1% of the original kernel matrix.

torch.Size([22091, 2])
We keep 8.80e+06/2.59e+08 =  3% of the original kernel matrix.

torch.Size([33220, 2])
We keep 6.11e+06/4.97e+08 =  1% of the original kernel matrix.

torch.Size([61523, 2])
We keep 8.76e+07/3.21e+09 =  2% of the original kernel matrix.

torch.Size([54112, 2])
We keep 1.67e+07/1.75e+09 =  0% of the original kernel matrix.

torch.Size([45632, 2])
We keep 3.44e+07/1.23e+09 =  2% of the original kernel matrix.

torch.Size([48715, 2])
We keep 1.13e+07/1.08e+09 =  1% of the original kernel matrix.

torch.Size([24813, 2])
We keep 4.41e+06/1.83e+08 =  2% of the original kernel matrix.

torch.Size([36608, 2])
We keep 5.38e+06/4.17e+08 =  1% of the original kernel matrix.

torch.Size([9231, 2])
We keep 6.05e+05/1.62e+07 =  3% of the original kernel matrix.

torch.Size([22852, 2])
We keep 2.19e+06/1.24e+08 =  1% of the original kernel matrix.

torch.Size([31865, 2])
We keep 5.00e+06/3.05e+08 =  1% of the original kernel matrix.

torch.Size([42561, 2])
We keep 6.59e+06/5.39e+08 =  1% of the original kernel matrix.

torch.Size([24921, 2])
We keep 1.83e+07/5.10e+08 =  3% of the original kernel matrix.

torch.Size([35175, 2])
We keep 7.96e+06/6.97e+08 =  1% of the original kernel matrix.

torch.Size([64545, 2])
We keep 6.99e+07/2.22e+09 =  3% of the original kernel matrix.

torch.Size([57729, 2])
We keep 1.44e+07/1.45e+09 =  0% of the original kernel matrix.

torch.Size([112958, 2])
We keep 6.09e+07/4.18e+09 =  1% of the original kernel matrix.

torch.Size([77966, 2])
We keep 1.84e+07/1.99e+09 =  0% of the original kernel matrix.

torch.Size([10684, 2])
We keep 9.05e+05/2.53e+07 =  3% of the original kernel matrix.

torch.Size([24192, 2])
We keep 2.56e+06/1.55e+08 =  1% of the original kernel matrix.

torch.Size([32434, 2])
We keep 4.79e+06/3.51e+08 =  1% of the original kernel matrix.

torch.Size([42448, 2])
We keep 6.76e+06/5.78e+08 =  1% of the original kernel matrix.

torch.Size([37711, 2])
We keep 1.39e+07/5.85e+08 =  2% of the original kernel matrix.

torch.Size([45490, 2])
We keep 8.41e+06/7.46e+08 =  1% of the original kernel matrix.

torch.Size([120275, 2])
We keep 1.03e+08/4.93e+09 =  2% of the original kernel matrix.

torch.Size([80538, 2])
We keep 1.96e+07/2.17e+09 =  0% of the original kernel matrix.

torch.Size([42259, 2])
We keep 1.34e+07/6.90e+08 =  1% of the original kernel matrix.

torch.Size([48694, 2])
We keep 9.05e+06/8.11e+08 =  1% of the original kernel matrix.

torch.Size([93596, 2])
We keep 2.84e+07/2.85e+09 =  0% of the original kernel matrix.

torch.Size([71675, 2])
We keep 1.57e+07/1.65e+09 =  0% of the original kernel matrix.

torch.Size([17034, 2])
We keep 9.15e+06/1.69e+08 =  5% of the original kernel matrix.

torch.Size([29126, 2])
We keep 5.19e+06/4.01e+08 =  1% of the original kernel matrix.

torch.Size([27414, 2])
We keep 3.55e+06/2.16e+08 =  1% of the original kernel matrix.

torch.Size([38878, 2])
We keep 5.54e+06/4.53e+08 =  1% of the original kernel matrix.

torch.Size([4060, 2])
We keep 1.44e+05/2.74e+06 =  5% of the original kernel matrix.

torch.Size([16450, 2])
We keep 1.19e+06/5.11e+07 =  2% of the original kernel matrix.

torch.Size([14234, 2])
We keep 1.17e+06/4.01e+07 =  2% of the original kernel matrix.

torch.Size([27645, 2])
We keep 3.01e+06/1.95e+08 =  1% of the original kernel matrix.

torch.Size([61788, 2])
We keep 1.42e+07/1.32e+09 =  1% of the original kernel matrix.

torch.Size([59232, 2])
We keep 1.13e+07/1.12e+09 =  1% of the original kernel matrix.

torch.Size([33059, 2])
We keep 1.00e+07/4.99e+08 =  2% of the original kernel matrix.

torch.Size([41913, 2])
We keep 7.85e+06/6.89e+08 =  1% of the original kernel matrix.

torch.Size([44666, 2])
We keep 7.75e+06/6.37e+08 =  1% of the original kernel matrix.

torch.Size([50951, 2])
We keep 8.58e+06/7.79e+08 =  1% of the original kernel matrix.

torch.Size([2703, 2])
We keep 4.22e+04/7.12e+05 =  5% of the original kernel matrix.

torch.Size([14713, 2])
We keep 7.61e+05/2.60e+07 =  2% of the original kernel matrix.

torch.Size([6975, 2])
We keep 3.76e+05/8.73e+06 =  4% of the original kernel matrix.

torch.Size([20582, 2])
We keep 1.77e+06/9.12e+07 =  1% of the original kernel matrix.

torch.Size([47949, 2])
We keep 2.45e+07/9.25e+08 =  2% of the original kernel matrix.

torch.Size([51193, 2])
We keep 1.01e+07/9.38e+08 =  1% of the original kernel matrix.

torch.Size([38169, 2])
We keep 1.07e+07/5.42e+08 =  1% of the original kernel matrix.

torch.Size([45896, 2])
We keep 8.20e+06/7.18e+08 =  1% of the original kernel matrix.

torch.Size([22794, 2])
We keep 2.92e+06/1.31e+08 =  2% of the original kernel matrix.

torch.Size([35196, 2])
We keep 4.65e+06/3.54e+08 =  1% of the original kernel matrix.

torch.Size([4311, 2])
We keep 1.33e+05/2.94e+06 =  4% of the original kernel matrix.

torch.Size([17061, 2])
We keep 1.22e+06/5.29e+07 =  2% of the original kernel matrix.

torch.Size([3332, 2])
We keep 1.75e+05/2.65e+06 =  6% of the original kernel matrix.

torch.Size([14543, 2])
We keep 1.16e+06/5.03e+07 =  2% of the original kernel matrix.

torch.Size([24118, 2])
We keep 3.53e+06/1.55e+08 =  2% of the original kernel matrix.

torch.Size([36121, 2])
We keep 4.98e+06/3.84e+08 =  1% of the original kernel matrix.

torch.Size([32992, 2])
We keep 9.16e+06/3.64e+08 =  2% of the original kernel matrix.

torch.Size([42655, 2])
We keep 6.89e+06/5.89e+08 =  1% of the original kernel matrix.

torch.Size([73106, 2])
We keep 6.57e+07/3.09e+09 =  2% of the original kernel matrix.

torch.Size([61258, 2])
We keep 1.65e+07/1.71e+09 =  0% of the original kernel matrix.

torch.Size([4901, 2])
We keep 2.20e+05/4.45e+06 =  4% of the original kernel matrix.

torch.Size([17852, 2])
We keep 1.39e+06/6.51e+07 =  2% of the original kernel matrix.

torch.Size([17869, 2])
We keep 9.74e+06/1.46e+08 =  6% of the original kernel matrix.

torch.Size([30158, 2])
We keep 4.80e+06/3.73e+08 =  1% of the original kernel matrix.

torch.Size([13844, 2])
We keep 3.27e+06/8.44e+07 =  3% of the original kernel matrix.

torch.Size([26230, 2])
We keep 3.93e+06/2.83e+08 =  1% of the original kernel matrix.

torch.Size([11776, 2])
We keep 1.34e+06/3.42e+07 =  3% of the original kernel matrix.

torch.Size([25113, 2])
We keep 2.86e+06/1.81e+08 =  1% of the original kernel matrix.

torch.Size([283223, 2])
We keep 4.48e+08/3.85e+10 =  1% of the original kernel matrix.

torch.Size([125831, 2])
We keep 4.79e+07/6.05e+09 =  0% of the original kernel matrix.

torch.Size([15444, 2])
We keep 8.87e+06/1.19e+08 =  7% of the original kernel matrix.

torch.Size([28474, 2])
We keep 4.43e+06/3.36e+08 =  1% of the original kernel matrix.

torch.Size([70349, 2])
We keep 2.83e+07/2.06e+09 =  1% of the original kernel matrix.

torch.Size([61275, 2])
We keep 1.38e+07/1.40e+09 =  0% of the original kernel matrix.

torch.Size([182982, 2])
We keep 1.75e+08/1.10e+10 =  1% of the original kernel matrix.

torch.Size([99883, 2])
We keep 2.80e+07/3.23e+09 =  0% of the original kernel matrix.

torch.Size([39904, 2])
We keep 1.01e+07/5.42e+08 =  1% of the original kernel matrix.

torch.Size([47465, 2])
We keep 8.13e+06/7.19e+08 =  1% of the original kernel matrix.

torch.Size([135076, 2])
We keep 1.07e+08/6.50e+09 =  1% of the original kernel matrix.

torch.Size([85152, 2])
We keep 2.25e+07/2.49e+09 =  0% of the original kernel matrix.

torch.Size([192737, 2])
We keep 1.25e+08/1.29e+10 =  0% of the original kernel matrix.

torch.Size([102907, 2])
We keep 3.01e+07/3.51e+09 =  0% of the original kernel matrix.

torch.Size([2304, 2])
We keep 5.37e+04/8.28e+05 =  6% of the original kernel matrix.

torch.Size([13234, 2])
We keep 8.04e+05/2.81e+07 =  2% of the original kernel matrix.

torch.Size([35578, 2])
We keep 9.49e+06/4.48e+08 =  2% of the original kernel matrix.

torch.Size([44381, 2])
We keep 7.51e+06/6.53e+08 =  1% of the original kernel matrix.

torch.Size([17419, 2])
We keep 1.63e+06/6.44e+07 =  2% of the original kernel matrix.

torch.Size([30795, 2])
We keep 3.58e+06/2.48e+08 =  1% of the original kernel matrix.

torch.Size([216048, 2])
We keep 2.46e+08/1.81e+10 =  1% of the original kernel matrix.

torch.Size([109696, 2])
We keep 3.51e+07/4.15e+09 =  0% of the original kernel matrix.

torch.Size([140681, 2])
We keep 1.07e+08/8.18e+09 =  1% of the original kernel matrix.

torch.Size([86915, 2])
We keep 2.47e+07/2.79e+09 =  0% of the original kernel matrix.

torch.Size([19526, 2])
We keep 5.73e+06/1.80e+08 =  3% of the original kernel matrix.

torch.Size([31218, 2])
We keep 5.30e+06/4.14e+08 =  1% of the original kernel matrix.

torch.Size([42180, 2])
We keep 8.70e+06/5.65e+08 =  1% of the original kernel matrix.

torch.Size([48924, 2])
We keep 8.16e+06/7.34e+08 =  1% of the original kernel matrix.

torch.Size([13241, 2])
We keep 1.06e+06/3.85e+07 =  2% of the original kernel matrix.

torch.Size([27178, 2])
We keep 2.98e+06/1.92e+08 =  1% of the original kernel matrix.

torch.Size([7901, 2])
We keep 4.41e+06/7.09e+07 =  6% of the original kernel matrix.

torch.Size([19663, 2])
We keep 3.72e+06/2.60e+08 =  1% of the original kernel matrix.

torch.Size([10964, 2])
We keep 3.46e+06/3.07e+07 = 11% of the original kernel matrix.

torch.Size([24520, 2])
We keep 2.75e+06/1.71e+08 =  1% of the original kernel matrix.

torch.Size([10563, 2])
We keep 9.26e+05/2.61e+07 =  3% of the original kernel matrix.

torch.Size([23820, 2])
We keep 2.60e+06/1.58e+08 =  1% of the original kernel matrix.

torch.Size([237599, 2])
We keep 2.31e+08/1.98e+10 =  1% of the original kernel matrix.

torch.Size([115387, 2])
We keep 3.58e+07/4.34e+09 =  0% of the original kernel matrix.

torch.Size([297071, 2])
We keep 5.93e+08/4.43e+10 =  1% of the original kernel matrix.

torch.Size([130032, 2])
We keep 5.15e+07/6.49e+09 =  0% of the original kernel matrix.

torch.Size([9000, 2])
We keep 1.53e+06/3.15e+07 =  4% of the original kernel matrix.

torch.Size([21240, 2])
We keep 2.79e+06/1.73e+08 =  1% of the original kernel matrix.

torch.Size([17936, 2])
We keep 2.07e+06/8.86e+07 =  2% of the original kernel matrix.

torch.Size([31191, 2])
We keep 3.93e+06/2.90e+08 =  1% of the original kernel matrix.

torch.Size([22696, 2])
We keep 3.30e+06/1.37e+08 =  2% of the original kernel matrix.

torch.Size([35138, 2])
We keep 4.77e+06/3.62e+08 =  1% of the original kernel matrix.

torch.Size([130272, 2])
We keep 1.88e+08/8.37e+09 =  2% of the original kernel matrix.

torch.Size([83068, 2])
We keep 2.50e+07/2.82e+09 =  0% of the original kernel matrix.

torch.Size([16594, 2])
We keep 1.62e+06/5.95e+07 =  2% of the original kernel matrix.

torch.Size([29853, 2])
We keep 3.48e+06/2.38e+08 =  1% of the original kernel matrix.

torch.Size([11452, 2])
We keep 7.65e+05/2.44e+07 =  3% of the original kernel matrix.

torch.Size([25354, 2])
We keep 2.48e+06/1.52e+08 =  1% of the original kernel matrix.

torch.Size([44559, 2])
We keep 1.30e+07/7.31e+08 =  1% of the original kernel matrix.

torch.Size([50172, 2])
We keep 9.18e+06/8.34e+08 =  1% of the original kernel matrix.

torch.Size([116539, 2])
We keep 5.38e+07/4.42e+09 =  1% of the original kernel matrix.

torch.Size([79022, 2])
We keep 1.90e+07/2.05e+09 =  0% of the original kernel matrix.

torch.Size([8598, 2])
We keep 8.04e+05/1.73e+07 =  4% of the original kernel matrix.

torch.Size([22088, 2])
We keep 2.22e+06/1.28e+08 =  1% of the original kernel matrix.

torch.Size([40577, 2])
We keep 1.59e+07/6.29e+08 =  2% of the original kernel matrix.

torch.Size([47342, 2])
We keep 8.66e+06/7.74e+08 =  1% of the original kernel matrix.

torch.Size([6206, 2])
We keep 3.03e+05/6.06e+06 =  5% of the original kernel matrix.

torch.Size([19814, 2])
We keep 1.49e+06/7.60e+07 =  1% of the original kernel matrix.

time for making ranges is 3.8144643306732178
Sorting X and nu_X
time for sorting X is 0.07463312149047852
Sorting Z and nu_Z
time for sorting Z is 0.0002701282501220703
Starting Optim
sum tnu_Z before tensor(21207878., device='cuda:0')
c= tensor(497.2061, device='cuda:0')
c= tensor(37517.3789, device='cuda:0')
c= tensor(39658.0742, device='cuda:0')
c= tensor(55392.0469, device='cuda:0')
c= tensor(253658.9688, device='cuda:0')
c= tensor(320103.5625, device='cuda:0')
c= tensor(663010.4375, device='cuda:0')
c= tensor(764180.5000, device='cuda:0')
c= tensor(788359.5000, device='cuda:0')
c= tensor(1771425.8750, device='cuda:0')
c= tensor(1776751.7500, device='cuda:0')
c= tensor(3384293., device='cuda:0')
c= tensor(3392710.2500, device='cuda:0')
c= tensor(10305727., device='cuda:0')
c= tensor(10415794., device='cuda:0')
c= tensor(10522948., device='cuda:0')
c= tensor(11036321., device='cuda:0')
c= tensor(11451035., device='cuda:0')
c= tensor(14652554., device='cuda:0')
c= tensor(16803904., device='cuda:0')
c= tensor(16827386., device='cuda:0')
c= tensor(22902580., device='cuda:0')
c= tensor(22923240., device='cuda:0')
c= tensor(23262526., device='cuda:0')
c= tensor(23428654., device='cuda:0')
c= tensor(24136580., device='cuda:0')
c= tensor(24768870., device='cuda:0')
c= tensor(24782672., device='cuda:0')
c= tensor(26436102., device='cuda:0')
c= tensor(1.4515e+08, device='cuda:0')
c= tensor(1.4516e+08, device='cuda:0')
c= tensor(2.1782e+08, device='cuda:0')
c= tensor(2.1796e+08, device='cuda:0')
c= tensor(2.1797e+08, device='cuda:0')
c= tensor(2.1799e+08, device='cuda:0')
c= tensor(2.1951e+08, device='cuda:0')
c= tensor(2.2063e+08, device='cuda:0')
c= tensor(2.2063e+08, device='cuda:0')
c= tensor(2.2065e+08, device='cuda:0')
c= tensor(2.2065e+08, device='cuda:0')
c= tensor(2.2065e+08, device='cuda:0')
c= tensor(2.2065e+08, device='cuda:0')
c= tensor(2.2065e+08, device='cuda:0')
c= tensor(2.2066e+08, device='cuda:0')
c= tensor(2.2066e+08, device='cuda:0')
c= tensor(2.2066e+08, device='cuda:0')
c= tensor(2.2066e+08, device='cuda:0')
c= tensor(2.2067e+08, device='cuda:0')
c= tensor(2.2067e+08, device='cuda:0')
c= tensor(2.2071e+08, device='cuda:0')
c= tensor(2.2073e+08, device='cuda:0')
c= tensor(2.2074e+08, device='cuda:0')
c= tensor(2.2074e+08, device='cuda:0')
c= tensor(2.2075e+08, device='cuda:0')
c= tensor(2.2075e+08, device='cuda:0')
c= tensor(2.2076e+08, device='cuda:0')
c= tensor(2.2076e+08, device='cuda:0')
c= tensor(2.2077e+08, device='cuda:0')
c= tensor(2.2077e+08, device='cuda:0')
c= tensor(2.2077e+08, device='cuda:0')
c= tensor(2.2078e+08, device='cuda:0')
c= tensor(2.2079e+08, device='cuda:0')
c= tensor(2.2082e+08, device='cuda:0')
c= tensor(2.2082e+08, device='cuda:0')
c= tensor(2.2083e+08, device='cuda:0')
c= tensor(2.2083e+08, device='cuda:0')
c= tensor(2.2083e+08, device='cuda:0')
c= tensor(2.2083e+08, device='cuda:0')
c= tensor(2.2083e+08, device='cuda:0')
c= tensor(2.2084e+08, device='cuda:0')
c= tensor(2.2084e+08, device='cuda:0')
c= tensor(2.2084e+08, device='cuda:0')
c= tensor(2.2084e+08, device='cuda:0')
c= tensor(2.2085e+08, device='cuda:0')
c= tensor(2.2085e+08, device='cuda:0')
c= tensor(2.2086e+08, device='cuda:0')
c= tensor(2.2086e+08, device='cuda:0')
c= tensor(2.2086e+08, device='cuda:0')
c= tensor(2.2086e+08, device='cuda:0')
c= tensor(2.2087e+08, device='cuda:0')
c= tensor(2.2088e+08, device='cuda:0')
c= tensor(2.2088e+08, device='cuda:0')
c= tensor(2.2089e+08, device='cuda:0')
c= tensor(2.2089e+08, device='cuda:0')
c= tensor(2.2089e+08, device='cuda:0')
c= tensor(2.2089e+08, device='cuda:0')
c= tensor(2.2089e+08, device='cuda:0')
c= tensor(2.2089e+08, device='cuda:0')
c= tensor(2.2090e+08, device='cuda:0')
c= tensor(2.2090e+08, device='cuda:0')
c= tensor(2.2090e+08, device='cuda:0')
c= tensor(2.2090e+08, device='cuda:0')
c= tensor(2.2090e+08, device='cuda:0')
c= tensor(2.2091e+08, device='cuda:0')
c= tensor(2.2091e+08, device='cuda:0')
c= tensor(2.2092e+08, device='cuda:0')
c= tensor(2.2092e+08, device='cuda:0')
c= tensor(2.2094e+08, device='cuda:0')
c= tensor(2.2095e+08, device='cuda:0')
c= tensor(2.2095e+08, device='cuda:0')
c= tensor(2.2099e+08, device='cuda:0')
c= tensor(2.2099e+08, device='cuda:0')
c= tensor(2.2100e+08, device='cuda:0')
c= tensor(2.2100e+08, device='cuda:0')
c= tensor(2.2100e+08, device='cuda:0')
c= tensor(2.2100e+08, device='cuda:0')
c= tensor(2.2101e+08, device='cuda:0')
c= tensor(2.2101e+08, device='cuda:0')
c= tensor(2.2101e+08, device='cuda:0')
c= tensor(2.2101e+08, device='cuda:0')
c= tensor(2.2102e+08, device='cuda:0')
c= tensor(2.2102e+08, device='cuda:0')
c= tensor(2.2102e+08, device='cuda:0')
c= tensor(2.2102e+08, device='cuda:0')
c= tensor(2.2103e+08, device='cuda:0')
c= tensor(2.2103e+08, device='cuda:0')
c= tensor(2.2103e+08, device='cuda:0')
c= tensor(2.2103e+08, device='cuda:0')
c= tensor(2.2104e+08, device='cuda:0')
c= tensor(2.2104e+08, device='cuda:0')
c= tensor(2.2105e+08, device='cuda:0')
c= tensor(2.2105e+08, device='cuda:0')
c= tensor(2.2105e+08, device='cuda:0')
c= tensor(2.2106e+08, device='cuda:0')
c= tensor(2.2106e+08, device='cuda:0')
c= tensor(2.2106e+08, device='cuda:0')
c= tensor(2.2106e+08, device='cuda:0')
c= tensor(2.2106e+08, device='cuda:0')
c= tensor(2.2110e+08, device='cuda:0')
c= tensor(2.2110e+08, device='cuda:0')
c= tensor(2.2110e+08, device='cuda:0')
c= tensor(2.2111e+08, device='cuda:0')
c= tensor(2.2111e+08, device='cuda:0')
c= tensor(2.2111e+08, device='cuda:0')
c= tensor(2.2112e+08, device='cuda:0')
c= tensor(2.2112e+08, device='cuda:0')
c= tensor(2.2112e+08, device='cuda:0')
c= tensor(2.2112e+08, device='cuda:0')
c= tensor(2.2112e+08, device='cuda:0')
c= tensor(2.2112e+08, device='cuda:0')
c= tensor(2.2113e+08, device='cuda:0')
c= tensor(2.2113e+08, device='cuda:0')
c= tensor(2.2114e+08, device='cuda:0')
c= tensor(2.2120e+08, device='cuda:0')
c= tensor(2.2122e+08, device='cuda:0')
c= tensor(2.2122e+08, device='cuda:0')
c= tensor(2.2122e+08, device='cuda:0')
c= tensor(2.2122e+08, device='cuda:0')
c= tensor(2.2123e+08, device='cuda:0')
c= tensor(2.2123e+08, device='cuda:0')
c= tensor(2.2123e+08, device='cuda:0')
c= tensor(2.2124e+08, device='cuda:0')
c= tensor(2.2124e+08, device='cuda:0')
c= tensor(2.2126e+08, device='cuda:0')
c= tensor(2.2126e+08, device='cuda:0')
c= tensor(2.2133e+08, device='cuda:0')
c= tensor(2.2133e+08, device='cuda:0')
c= tensor(2.2133e+08, device='cuda:0')
c= tensor(2.2133e+08, device='cuda:0')
c= tensor(2.2133e+08, device='cuda:0')
c= tensor(2.2134e+08, device='cuda:0')
c= tensor(2.2134e+08, device='cuda:0')
c= tensor(2.2135e+08, device='cuda:0')
c= tensor(2.2135e+08, device='cuda:0')
c= tensor(2.2135e+08, device='cuda:0')
c= tensor(2.2135e+08, device='cuda:0')
c= tensor(2.2136e+08, device='cuda:0')
c= tensor(2.2136e+08, device='cuda:0')
c= tensor(2.2136e+08, device='cuda:0')
c= tensor(2.2136e+08, device='cuda:0')
c= tensor(2.2136e+08, device='cuda:0')
c= tensor(2.2136e+08, device='cuda:0')
c= tensor(2.2137e+08, device='cuda:0')
c= tensor(2.2137e+08, device='cuda:0')
c= tensor(2.2137e+08, device='cuda:0')
c= tensor(2.2138e+08, device='cuda:0')
c= tensor(2.2138e+08, device='cuda:0')
c= tensor(2.2139e+08, device='cuda:0')
c= tensor(2.2140e+08, device='cuda:0')
c= tensor(2.2140e+08, device='cuda:0')
c= tensor(2.2140e+08, device='cuda:0')
c= tensor(2.2140e+08, device='cuda:0')
c= tensor(2.2141e+08, device='cuda:0')
c= tensor(2.2141e+08, device='cuda:0')
c= tensor(2.2142e+08, device='cuda:0')
c= tensor(2.2142e+08, device='cuda:0')
c= tensor(2.2142e+08, device='cuda:0')
c= tensor(2.2143e+08, device='cuda:0')
c= tensor(2.2144e+08, device='cuda:0')
c= tensor(2.2152e+08, device='cuda:0')
c= tensor(2.2152e+08, device='cuda:0')
c= tensor(2.2152e+08, device='cuda:0')
c= tensor(2.2152e+08, device='cuda:0')
c= tensor(2.2152e+08, device='cuda:0')
c= tensor(2.2152e+08, device='cuda:0')
c= tensor(2.2153e+08, device='cuda:0')
c= tensor(2.2153e+08, device='cuda:0')
c= tensor(2.2153e+08, device='cuda:0')
c= tensor(2.2153e+08, device='cuda:0')
c= tensor(2.2153e+08, device='cuda:0')
c= tensor(2.2154e+08, device='cuda:0')
c= tensor(2.2154e+08, device='cuda:0')
c= tensor(2.2155e+08, device='cuda:0')
c= tensor(2.2156e+08, device='cuda:0')
c= tensor(2.2156e+08, device='cuda:0')
c= tensor(2.2156e+08, device='cuda:0')
c= tensor(2.2157e+08, device='cuda:0')
c= tensor(2.2157e+08, device='cuda:0')
c= tensor(2.2157e+08, device='cuda:0')
c= tensor(2.2158e+08, device='cuda:0')
c= tensor(2.2160e+08, device='cuda:0')
c= tensor(2.2160e+08, device='cuda:0')
c= tensor(2.2160e+08, device='cuda:0')
c= tensor(2.2160e+08, device='cuda:0')
c= tensor(2.2161e+08, device='cuda:0')
c= tensor(2.2161e+08, device='cuda:0')
c= tensor(2.2161e+08, device='cuda:0')
c= tensor(2.2161e+08, device='cuda:0')
c= tensor(2.2161e+08, device='cuda:0')
c= tensor(2.2162e+08, device='cuda:0')
c= tensor(2.2162e+08, device='cuda:0')
c= tensor(2.2162e+08, device='cuda:0')
c= tensor(2.2162e+08, device='cuda:0')
c= tensor(2.2162e+08, device='cuda:0')
c= tensor(2.2163e+08, device='cuda:0')
c= tensor(2.2163e+08, device='cuda:0')
c= tensor(2.2163e+08, device='cuda:0')
c= tensor(2.2164e+08, device='cuda:0')
c= tensor(2.2165e+08, device='cuda:0')
c= tensor(2.2165e+08, device='cuda:0')
c= tensor(2.2165e+08, device='cuda:0')
c= tensor(2.2165e+08, device='cuda:0')
c= tensor(2.2165e+08, device='cuda:0')
c= tensor(2.2166e+08, device='cuda:0')
c= tensor(2.2166e+08, device='cuda:0')
c= tensor(2.2166e+08, device='cuda:0')
c= tensor(2.2166e+08, device='cuda:0')
c= tensor(2.2167e+08, device='cuda:0')
c= tensor(2.2168e+08, device='cuda:0')
c= tensor(2.2171e+08, device='cuda:0')
c= tensor(2.2377e+08, device='cuda:0')
c= tensor(2.2378e+08, device='cuda:0')
c= tensor(2.2378e+08, device='cuda:0')
c= tensor(2.2378e+08, device='cuda:0')
c= tensor(2.2379e+08, device='cuda:0')
c= tensor(2.2381e+08, device='cuda:0')
c= tensor(2.3005e+08, device='cuda:0')
c= tensor(2.3006e+08, device='cuda:0')
c= tensor(2.3063e+08, device='cuda:0')
c= tensor(2.3129e+08, device='cuda:0')
c= tensor(2.3131e+08, device='cuda:0')
c= tensor(2.3416e+08, device='cuda:0')
c= tensor(2.3416e+08, device='cuda:0')
c= tensor(2.3417e+08, device='cuda:0')
c= tensor(2.3588e+08, device='cuda:0')
c= tensor(2.4989e+08, device='cuda:0')
c= tensor(2.4989e+08, device='cuda:0')
c= tensor(2.5001e+08, device='cuda:0')
c= tensor(2.5008e+08, device='cuda:0')
c= tensor(2.5082e+08, device='cuda:0')
c= tensor(2.5148e+08, device='cuda:0')
c= tensor(2.5167e+08, device='cuda:0')
c= tensor(2.5195e+08, device='cuda:0')
c= tensor(2.5207e+08, device='cuda:0')
c= tensor(2.5210e+08, device='cuda:0')
c= tensor(2.6298e+08, device='cuda:0')
c= tensor(2.6298e+08, device='cuda:0')
c= tensor(2.6299e+08, device='cuda:0')
c= tensor(2.6305e+08, device='cuda:0')
c= tensor(2.6320e+08, device='cuda:0')
c= tensor(2.7841e+08, device='cuda:0')
c= tensor(2.7906e+08, device='cuda:0')
c= tensor(2.7906e+08, device='cuda:0')
c= tensor(2.7910e+08, device='cuda:0')
c= tensor(2.7914e+08, device='cuda:0')
c= tensor(2.7925e+08, device='cuda:0')
c= tensor(2.8092e+08, device='cuda:0')
c= tensor(2.8201e+08, device='cuda:0')
c= tensor(2.8220e+08, device='cuda:0')
c= tensor(2.8220e+08, device='cuda:0')
c= tensor(2.8221e+08, device='cuda:0')
c= tensor(2.8291e+08, device='cuda:0')
c= tensor(2.8322e+08, device='cuda:0')
c= tensor(2.8335e+08, device='cuda:0')
c= tensor(2.8337e+08, device='cuda:0')
c= tensor(3.0248e+08, device='cuda:0')
c= tensor(3.0248e+08, device='cuda:0')
c= tensor(3.0259e+08, device='cuda:0')
c= tensor(3.0427e+08, device='cuda:0')
c= tensor(3.0427e+08, device='cuda:0')
c= tensor(3.0454e+08, device='cuda:0')
c= tensor(3.1296e+08, device='cuda:0')
c= tensor(3.2634e+08, device='cuda:0')
c= tensor(3.2637e+08, device='cuda:0')
c= tensor(3.2639e+08, device='cuda:0')
c= tensor(3.2639e+08, device='cuda:0')
c= tensor(3.2639e+08, device='cuda:0')
c= tensor(3.2691e+08, device='cuda:0')
c= tensor(3.2692e+08, device='cuda:0')
c= tensor(3.2717e+08, device='cuda:0')
c= tensor(3.3185e+08, device='cuda:0')
c= tensor(3.3226e+08, device='cuda:0')
c= tensor(3.3236e+08, device='cuda:0')
c= tensor(3.3236e+08, device='cuda:0')
c= tensor(3.3837e+08, device='cuda:0')
c= tensor(3.3940e+08, device='cuda:0')
c= tensor(3.3949e+08, device='cuda:0')
c= tensor(3.3959e+08, device='cuda:0')
c= tensor(3.5983e+08, device='cuda:0')
c= tensor(3.5984e+08, device='cuda:0')
c= tensor(3.6219e+08, device='cuda:0')
c= tensor(3.6220e+08, device='cuda:0')
c= tensor(3.6348e+08, device='cuda:0')
c= tensor(3.6358e+08, device='cuda:0')
c= tensor(3.6871e+08, device='cuda:0')
c= tensor(3.6915e+08, device='cuda:0')
c= tensor(3.6915e+08, device='cuda:0')
c= tensor(3.7112e+08, device='cuda:0')
c= tensor(3.7245e+08, device='cuda:0')
c= tensor(3.7245e+08, device='cuda:0')
c= tensor(3.7295e+08, device='cuda:0')
c= tensor(3.7693e+08, device='cuda:0')
c= tensor(3.8912e+08, device='cuda:0')
c= tensor(3.9377e+08, device='cuda:0')
c= tensor(3.9377e+08, device='cuda:0')
c= tensor(3.9378e+08, device='cuda:0')
c= tensor(3.9388e+08, device='cuda:0')
c= tensor(3.9395e+08, device='cuda:0')
c= tensor(3.9397e+08, device='cuda:0')
c= tensor(3.9398e+08, device='cuda:0')
c= tensor(3.9447e+08, device='cuda:0')
c= tensor(3.9528e+08, device='cuda:0')
c= tensor(3.9546e+08, device='cuda:0')
c= tensor(3.9547e+08, device='cuda:0')
c= tensor(3.9553e+08, device='cuda:0')
c= tensor(3.9556e+08, device='cuda:0')
c= tensor(3.9558e+08, device='cuda:0')
c= tensor(3.9560e+08, device='cuda:0')
c= tensor(3.9561e+08, device='cuda:0')
c= tensor(4.0014e+08, device='cuda:0')
c= tensor(4.0024e+08, device='cuda:0')
c= tensor(4.0029e+08, device='cuda:0')
c= tensor(4.0066e+08, device='cuda:0')
c= tensor(4.0067e+08, device='cuda:0')
c= tensor(6.0295e+08, device='cuda:0')
c= tensor(6.0296e+08, device='cuda:0')
c= tensor(6.0410e+08, device='cuda:0')
c= tensor(6.0410e+08, device='cuda:0')
c= tensor(6.0410e+08, device='cuda:0')
c= tensor(6.0411e+08, device='cuda:0')
c= tensor(6.0414e+08, device='cuda:0')
c= tensor(6.0415e+08, device='cuda:0')
c= tensor(6.0465e+08, device='cuda:0')
c= tensor(6.0465e+08, device='cuda:0')
c= tensor(6.0466e+08, device='cuda:0')
c= tensor(6.1408e+08, device='cuda:0')
c= tensor(6.1431e+08, device='cuda:0')
c= tensor(6.1460e+08, device='cuda:0')
c= tensor(6.1575e+08, device='cuda:0')
c= tensor(6.1800e+08, device='cuda:0')
c= tensor(6.1800e+08, device='cuda:0')
c= tensor(6.1801e+08, device='cuda:0')
c= tensor(6.1807e+08, device='cuda:0')
c= tensor(6.1807e+08, device='cuda:0')
c= tensor(6.1807e+08, device='cuda:0')
c= tensor(6.1813e+08, device='cuda:0')
c= tensor(6.1813e+08, device='cuda:0')
c= tensor(6.1813e+08, device='cuda:0')
c= tensor(6.1813e+08, device='cuda:0')
c= tensor(6.1814e+08, device='cuda:0')
c= tensor(6.3749e+08, device='cuda:0')
c= tensor(6.3753e+08, device='cuda:0')
c= tensor(6.3822e+08, device='cuda:0')
c= tensor(6.3825e+08, device='cuda:0')
c= tensor(6.3825e+08, device='cuda:0')
c= tensor(6.3830e+08, device='cuda:0')
c= tensor(7.0803e+08, device='cuda:0')
c= tensor(7.2300e+08, device='cuda:0')
c= tensor(7.2307e+08, device='cuda:0')
c= tensor(7.2320e+08, device='cuda:0')
c= tensor(7.2320e+08, device='cuda:0')
c= tensor(7.2428e+08, device='cuda:0')
c= tensor(7.3592e+08, device='cuda:0')
c= tensor(7.3628e+08, device='cuda:0')
c= tensor(7.3630e+08, device='cuda:0')
c= tensor(7.3727e+08, device='cuda:0')
c= tensor(7.5156e+08, device='cuda:0')
c= tensor(7.5164e+08, device='cuda:0')
c= tensor(7.5165e+08, device='cuda:0')
c= tensor(7.5169e+08, device='cuda:0')
c= tensor(7.5175e+08, device='cuda:0')
c= tensor(7.5175e+08, device='cuda:0')
c= tensor(7.5557e+08, device='cuda:0')
c= tensor(7.5562e+08, device='cuda:0')
c= tensor(7.5562e+08, device='cuda:0')
c= tensor(7.5567e+08, device='cuda:0')
c= tensor(7.5574e+08, device='cuda:0')
c= tensor(7.5574e+08, device='cuda:0')
c= tensor(7.5775e+08, device='cuda:0')
c= tensor(7.5933e+08, device='cuda:0')
c= tensor(7.6175e+08, device='cuda:0')
c= tensor(7.6476e+08, device='cuda:0')
c= tensor(7.6600e+08, device='cuda:0')
c= tensor(7.6604e+08, device='cuda:0')
c= tensor(7.6607e+08, device='cuda:0')
c= tensor(7.6622e+08, device='cuda:0')
c= tensor(7.6707e+08, device='cuda:0')
c= tensor(7.6708e+08, device='cuda:0')
c= tensor(7.6940e+08, device='cuda:0')
c= tensor(7.9183e+08, device='cuda:0')
c= tensor(7.9274e+08, device='cuda:0')
c= tensor(7.9314e+08, device='cuda:0')
c= tensor(7.9697e+08, device='cuda:0')
c= tensor(7.9699e+08, device='cuda:0')
c= tensor(7.9700e+08, device='cuda:0')
c= tensor(7.9717e+08, device='cuda:0')
c= tensor(7.9846e+08, device='cuda:0')
c= tensor(7.9949e+08, device='cuda:0')
c= tensor(8.3410e+08, device='cuda:0')
c= tensor(8.3904e+08, device='cuda:0')
c= tensor(8.4005e+08, device='cuda:0')
c= tensor(8.4025e+08, device='cuda:0')
c= tensor(8.4197e+08, device='cuda:0')
c= tensor(8.4197e+08, device='cuda:0')
c= tensor(8.4198e+08, device='cuda:0')
c= tensor(8.4950e+08, device='cuda:0')
c= tensor(8.4952e+08, device='cuda:0')
c= tensor(8.4952e+08, device='cuda:0')
c= tensor(8.4964e+08, device='cuda:0')
c= tensor(8.5360e+08, device='cuda:0')
c= tensor(8.5375e+08, device='cuda:0')
c= tensor(8.5441e+08, device='cuda:0')
c= tensor(8.5442e+08, device='cuda:0')
c= tensor(8.5445e+08, device='cuda:0')
c= tensor(8.5445e+08, device='cuda:0')
c= tensor(8.5460e+08, device='cuda:0')
c= tensor(8.5470e+08, device='cuda:0')
c= tensor(8.5531e+08, device='cuda:0')
c= tensor(8.5532e+08, device='cuda:0')
c= tensor(8.5725e+08, device='cuda:0')
c= tensor(8.5725e+08, device='cuda:0')
c= tensor(8.5741e+08, device='cuda:0')
c= tensor(8.5743e+08, device='cuda:0')
c= tensor(8.5750e+08, device='cuda:0')
c= tensor(8.5750e+08, device='cuda:0')
c= tensor(8.5770e+08, device='cuda:0')
c= tensor(8.5773e+08, device='cuda:0')
c= tensor(8.5786e+08, device='cuda:0')
c= tensor(8.5907e+08, device='cuda:0')
c= tensor(8.6663e+08, device='cuda:0')
c= tensor(8.6664e+08, device='cuda:0')
c= tensor(8.6667e+08, device='cuda:0')
c= tensor(8.6931e+08, device='cuda:0')
c= tensor(8.6933e+08, device='cuda:0')
c= tensor(9.0006e+08, device='cuda:0')
c= tensor(9.0006e+08, device='cuda:0')
c= tensor(9.0095e+08, device='cuda:0')
c= tensor(9.0426e+08, device='cuda:0')
c= tensor(9.0426e+08, device='cuda:0')
c= tensor(9.0924e+08, device='cuda:0')
c= tensor(9.0964e+08, device='cuda:0')
c= tensor(9.4725e+08, device='cuda:0')
c= tensor(9.4726e+08, device='cuda:0')
c= tensor(9.4735e+08, device='cuda:0')
c= tensor(9.4736e+08, device='cuda:0')
c= tensor(9.4736e+08, device='cuda:0')
c= tensor(9.4738e+08, device='cuda:0')
c= tensor(9.4769e+08, device='cuda:0')
c= tensor(9.4772e+08, device='cuda:0')
c= tensor(9.4844e+08, device='cuda:0')
c= tensor(9.4846e+08, device='cuda:0')
c= tensor(9.4846e+08, device='cuda:0')
c= tensor(9.4848e+08, device='cuda:0')
c= tensor(9.5167e+08, device='cuda:0')
c= tensor(9.5204e+08, device='cuda:0')
c= tensor(9.5643e+08, device='cuda:0')
c= tensor(9.5664e+08, device='cuda:0')
c= tensor(9.5667e+08, device='cuda:0')
c= tensor(9.5668e+08, device='cuda:0')
c= tensor(9.5669e+08, device='cuda:0')
c= tensor(9.7402e+08, device='cuda:0')
c= tensor(9.7404e+08, device='cuda:0')
c= tensor(9.7406e+08, device='cuda:0')
c= tensor(9.7447e+08, device='cuda:0')
c= tensor(9.7512e+08, device='cuda:0')
c= tensor(9.7512e+08, device='cuda:0')
c= tensor(9.7513e+08, device='cuda:0')
c= tensor(1.0011e+09, device='cuda:0')
c= tensor(1.0011e+09, device='cuda:0')
c= tensor(1.0019e+09, device='cuda:0')
c= tensor(1.0020e+09, device='cuda:0')
c= tensor(1.0046e+09, device='cuda:0')
c= tensor(1.0083e+09, device='cuda:0')
c= tensor(1.0141e+09, device='cuda:0')
c= tensor(1.0153e+09, device='cuda:0')
c= tensor(1.0153e+09, device='cuda:0')
c= tensor(1.0157e+09, device='cuda:0')
c= tensor(1.0157e+09, device='cuda:0')
c= tensor(1.0157e+09, device='cuda:0')
c= tensor(1.0158e+09, device='cuda:0')
c= tensor(1.0158e+09, device='cuda:0')
c= tensor(1.0171e+09, device='cuda:0')
c= tensor(1.0171e+09, device='cuda:0')
c= tensor(1.0171e+09, device='cuda:0')
c= tensor(1.0171e+09, device='cuda:0')
c= tensor(1.0172e+09, device='cuda:0')
c= tensor(1.0213e+09, device='cuda:0')
c= tensor(1.0213e+09, device='cuda:0')
c= tensor(1.0214e+09, device='cuda:0')
c= tensor(1.0215e+09, device='cuda:0')
c= tensor(1.0216e+09, device='cuda:0')
c= tensor(1.0216e+09, device='cuda:0')
c= tensor(1.0216e+09, device='cuda:0')
c= tensor(1.0217e+09, device='cuda:0')
c= tensor(1.0240e+09, device='cuda:0')
c= tensor(1.0240e+09, device='cuda:0')
c= tensor(1.0240e+09, device='cuda:0')
c= tensor(1.0240e+09, device='cuda:0')
c= tensor(1.0322e+09, device='cuda:0')
c= tensor(1.0852e+09, device='cuda:0')
c= tensor(1.0853e+09, device='cuda:0')
c= tensor(1.0853e+09, device='cuda:0')
c= tensor(1.0860e+09, device='cuda:0')
c= tensor(1.0868e+09, device='cuda:0')
c= tensor(1.0868e+09, device='cuda:0')
c= tensor(1.0869e+09, device='cuda:0')
c= tensor(1.0870e+09, device='cuda:0')
c= tensor(1.0938e+09, device='cuda:0')
c= tensor(1.0967e+09, device='cuda:0')
c= tensor(1.1001e+09, device='cuda:0')
c= tensor(1.1001e+09, device='cuda:0')
c= tensor(1.1001e+09, device='cuda:0')
c= tensor(1.1001e+09, device='cuda:0')
c= tensor(1.1003e+09, device='cuda:0')
c= tensor(1.1003e+09, device='cuda:0')
c= tensor(1.1005e+09, device='cuda:0')
c= tensor(1.1022e+09, device='cuda:0')
c= tensor(1.1123e+09, device='cuda:0')
c= tensor(1.1123e+09, device='cuda:0')
c= tensor(1.1123e+09, device='cuda:0')
c= tensor(1.1123e+09, device='cuda:0')
c= tensor(1.2648e+09, device='cuda:0')
c= tensor(1.2649e+09, device='cuda:0')
c= tensor(1.2649e+09, device='cuda:0')
c= tensor(1.2649e+09, device='cuda:0')
c= tensor(1.2651e+09, device='cuda:0')
c= tensor(1.2651e+09, device='cuda:0')
c= tensor(1.2659e+09, device='cuda:0')
c= tensor(1.2659e+09, device='cuda:0')
c= tensor(1.2659e+09, device='cuda:0')
c= tensor(1.2660e+09, device='cuda:0')
c= tensor(1.2660e+09, device='cuda:0')
c= tensor(1.2660e+09, device='cuda:0')
c= tensor(1.2689e+09, device='cuda:0')
c= tensor(1.2919e+09, device='cuda:0')
c= tensor(1.2940e+09, device='cuda:0')
c= tensor(1.2951e+09, device='cuda:0')
c= tensor(1.2952e+09, device='cuda:0')
c= tensor(1.2952e+09, device='cuda:0')
c= tensor(1.2952e+09, device='cuda:0')
c= tensor(1.2956e+09, device='cuda:0')
c= tensor(1.2957e+09, device='cuda:0')
c= tensor(1.2975e+09, device='cuda:0')
c= tensor(1.2975e+09, device='cuda:0')
c= tensor(1.3505e+09, device='cuda:0')
c= tensor(1.3505e+09, device='cuda:0')
c= tensor(1.3508e+09, device='cuda:0')
c= tensor(1.3575e+09, device='cuda:0')
c= tensor(1.3578e+09, device='cuda:0')
c= tensor(1.3598e+09, device='cuda:0')
c= tensor(1.3668e+09, device='cuda:0')
c= tensor(1.3671e+09, device='cuda:0')
c= tensor(1.3673e+09, device='cuda:0')
c= tensor(1.3673e+09, device='cuda:0')
c= tensor(1.3675e+09, device='cuda:0')
c= tensor(1.3675e+09, device='cuda:0')
c= tensor(1.3695e+09, device='cuda:0')
c= tensor(1.4275e+09, device='cuda:0')
c= tensor(1.4317e+09, device='cuda:0')
c= tensor(1.4338e+09, device='cuda:0')
c= tensor(1.4340e+09, device='cuda:0')
c= tensor(1.4342e+09, device='cuda:0')
c= tensor(1.4346e+09, device='cuda:0')
c= tensor(1.4346e+09, device='cuda:0')
c= tensor(1.4366e+09, device='cuda:0')
c= tensor(1.4403e+09, device='cuda:0')
c= tensor(1.4404e+09, device='cuda:0')
c= tensor(1.4801e+09, device='cuda:0')
c= tensor(1.4872e+09, device='cuda:0')
c= tensor(1.4873e+09, device='cuda:0')
c= tensor(1.4873e+09, device='cuda:0')
c= tensor(1.4877e+09, device='cuda:0')
c= tensor(1.4882e+09, device='cuda:0')
c= tensor(1.4882e+09, device='cuda:0')
c= tensor(1.4999e+09, device='cuda:0')
c= tensor(1.5003e+09, device='cuda:0')
c= tensor(1.5004e+09, device='cuda:0')
c= tensor(1.5004e+09, device='cuda:0')
c= tensor(1.5004e+09, device='cuda:0')
c= tensor(1.5004e+09, device='cuda:0')
c= tensor(1.5004e+09, device='cuda:0')
c= tensor(1.5005e+09, device='cuda:0')
c= tensor(1.5007e+09, device='cuda:0')
c= tensor(2.6814e+09, device='cuda:0')
c= tensor(2.6815e+09, device='cuda:0')
c= tensor(2.6816e+09, device='cuda:0')
c= tensor(2.6816e+09, device='cuda:0')
c= tensor(2.6817e+09, device='cuda:0')
c= tensor(2.6817e+09, device='cuda:0')
c= tensor(2.6872e+09, device='cuda:0')
c= tensor(2.6876e+09, device='cuda:0')
c= tensor(2.7317e+09, device='cuda:0')
c= tensor(2.7317e+09, device='cuda:0')
c= tensor(2.7363e+09, device='cuda:0')
c= tensor(2.7367e+09, device='cuda:0')
c= tensor(2.7387e+09, device='cuda:0')
c= tensor(2.7552e+09, device='cuda:0')
c= tensor(2.7552e+09, device='cuda:0')
c= tensor(2.7552e+09, device='cuda:0')
c= tensor(2.7560e+09, device='cuda:0')
c= tensor(2.7560e+09, device='cuda:0')
c= tensor(2.7563e+09, device='cuda:0')
c= tensor(2.7567e+09, device='cuda:0')
c= tensor(2.7574e+09, device='cuda:0')
c= tensor(2.7578e+09, device='cuda:0')
c= tensor(2.7579e+09, device='cuda:0')
c= tensor(2.7597e+09, device='cuda:0')
c= tensor(2.7712e+09, device='cuda:0')
c= tensor(2.7712e+09, device='cuda:0')
c= tensor(2.7712e+09, device='cuda:0')
c= tensor(2.7758e+09, device='cuda:0')
c= tensor(2.7759e+09, device='cuda:0')
c= tensor(2.7800e+09, device='cuda:0')
c= tensor(2.7803e+09, device='cuda:0')
c= tensor(2.7809e+09, device='cuda:0')
c= tensor(2.7810e+09, device='cuda:0')
c= tensor(2.7847e+09, device='cuda:0')
c= tensor(2.7854e+09, device='cuda:0')
c= tensor(2.7854e+09, device='cuda:0')
c= tensor(2.7854e+09, device='cuda:0')
c= tensor(2.7855e+09, device='cuda:0')
c= tensor(2.7859e+09, device='cuda:0')
c= tensor(2.7875e+09, device='cuda:0')
c= tensor(2.7889e+09, device='cuda:0')
c= tensor(2.7889e+09, device='cuda:0')
c= tensor(2.7891e+09, device='cuda:0')
c= tensor(2.7894e+09, device='cuda:0')
c= tensor(2.7914e+09, device='cuda:0')
c= tensor(2.7916e+09, device='cuda:0')
c= tensor(2.7923e+09, device='cuda:0')
c= tensor(2.7927e+09, device='cuda:0')
c= tensor(2.7928e+09, device='cuda:0')
c= tensor(2.7928e+09, device='cuda:0')
c= tensor(2.7928e+09, device='cuda:0')
c= tensor(2.7931e+09, device='cuda:0')
c= tensor(2.7933e+09, device='cuda:0')
c= tensor(2.7935e+09, device='cuda:0')
c= tensor(2.7935e+09, device='cuda:0')
c= tensor(2.7935e+09, device='cuda:0')
c= tensor(2.7939e+09, device='cuda:0')
c= tensor(2.7941e+09, device='cuda:0')
c= tensor(2.7942e+09, device='cuda:0')
c= tensor(2.7942e+09, device='cuda:0')
c= tensor(2.7942e+09, device='cuda:0')
c= tensor(2.7942e+09, device='cuda:0')
c= tensor(2.7944e+09, device='cuda:0')
c= tensor(2.7959e+09, device='cuda:0')
c= tensor(2.7959e+09, device='cuda:0')
c= tensor(2.7965e+09, device='cuda:0')
c= tensor(2.7966e+09, device='cuda:0')
c= tensor(2.7966e+09, device='cuda:0')
c= tensor(2.8096e+09, device='cuda:0')
c= tensor(2.8098e+09, device='cuda:0')
c= tensor(2.8107e+09, device='cuda:0')
c= tensor(2.8147e+09, device='cuda:0')
c= tensor(2.8149e+09, device='cuda:0')
c= tensor(2.8169e+09, device='cuda:0')
c= tensor(2.8196e+09, device='cuda:0')
c= tensor(2.8196e+09, device='cuda:0')
c= tensor(2.8198e+09, device='cuda:0')
c= tensor(2.8198e+09, device='cuda:0')
c= tensor(2.8248e+09, device='cuda:0')
c= tensor(2.8273e+09, device='cuda:0')
c= tensor(2.8274e+09, device='cuda:0')
c= tensor(2.8276e+09, device='cuda:0')
c= tensor(2.8277e+09, device='cuda:0')
c= tensor(2.8281e+09, device='cuda:0')
c= tensor(2.8281e+09, device='cuda:0')
c= tensor(2.8281e+09, device='cuda:0')
c= tensor(2.8344e+09, device='cuda:0')
c= tensor(2.8518e+09, device='cuda:0')
c= tensor(2.8518e+09, device='cuda:0')
c= tensor(2.8519e+09, device='cuda:0')
c= tensor(2.8519e+09, device='cuda:0')
c= tensor(2.8557e+09, device='cuda:0')
c= tensor(2.8557e+09, device='cuda:0')
c= tensor(2.8557e+09, device='cuda:0')
c= tensor(2.8560e+09, device='cuda:0')
c= tensor(2.8569e+09, device='cuda:0')
c= tensor(2.8569e+09, device='cuda:0')
c= tensor(2.8572e+09, device='cuda:0')
c= tensor(2.8572e+09, device='cuda:0')
memory (bytes)
4310765568
time for making loss 2 is 14.335710287094116
p0 True
it  0 : 1716769792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 59% |
shape of L is 
torch.Size([])
memory (bytes)
4311093248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  9% |
memory (bytes)
4311605248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% | 10% |
error is  16668444000.0
relative error loss 5.8338485
shape of L is 
torch.Size([])
memory (bytes)
4518629376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  5% | 10% |
memory (bytes)
4518633472
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  16668374000.0
relative error loss 5.8338237
shape of L is 
torch.Size([])
memory (bytes)
4525027328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
4525031424
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  16667746000.0
relative error loss 5.8336043
shape of L is 
torch.Size([])
memory (bytes)
4526874624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
memory (bytes)
4527132672
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  16663645000.0
relative error loss 5.832169
shape of L is 
torch.Size([])
memory (bytes)
4529246208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
memory (bytes)
4529250304
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  16620571000.0
relative error loss 5.817093
shape of L is 
torch.Size([])
memory (bytes)
4531372032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% | 10% |
memory (bytes)
4531376128
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  16195623000.0
relative error loss 5.668364
shape of L is 
torch.Size([])
memory (bytes)
4533465088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 10% |
memory (bytes)
4533465088
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  14221370000.0
relative error loss 4.9773884
shape of L is 
torch.Size([])
memory (bytes)
4535607296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 10% |
memory (bytes)
4535644160
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  5286798300.0
relative error loss 1.8503455
shape of L is 
torch.Size([])
memory (bytes)
4537778176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 30% | 10% |
memory (bytes)
4537778176
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  41648095000.0
relative error loss 14.576566
shape of L is 
torch.Size([])
memory (bytes)
4539777024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 10% |
memory (bytes)
4539908096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% | 10% |
error is  4691503600.0
relative error loss 1.6419961
shape of L is 
torch.Size([])
memory (bytes)
4542058496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
memory (bytes)
4542058496
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  4528351000.0
relative error loss 1.5848938
time to take a step is 251.79862785339355
it  1 : 2323278848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
4544032768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% | 10% |
memory (bytes)
4544217088
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  4528351000.0
relative error loss 1.5848938
shape of L is 
torch.Size([])
memory (bytes)
4546125824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 10% |
memory (bytes)
4546334720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 10% |
error is  3462460400.0
relative error loss 1.211839
shape of L is 
torch.Size([])
memory (bytes)
4548464640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
memory (bytes)
4548468736
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  2784583000.0
relative error loss 0.9745861
shape of L is 
torch.Size([])
memory (bytes)
4550369280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 10% |
memory (bytes)
4550524928
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  5065238500.0
relative error loss 1.7728009
shape of L is 
torch.Size([])
memory (bytes)
4552560640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 10% |
memory (bytes)
4552560640
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  2587527000.0
relative error loss 0.90561783
shape of L is 
torch.Size([])
memory (bytes)
4554866688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 32% | 11% |
memory (bytes)
4554870784
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  2454729700.0
relative error loss 0.8591396
shape of L is 
torch.Size([])
memory (bytes)
4556906496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4556906496
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  2386580700.0
relative error loss 0.8352879
shape of L is 
torch.Size([])
memory (bytes)
4558921728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 11% |
memory (bytes)
4559036416
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  2492784400.0
relative error loss 0.8724585
shape of L is 
torch.Size([])
memory (bytes)
4561186816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
4561186816
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  2257568800.0
relative error loss 0.79013455
shape of L is 
torch.Size([])
memory (bytes)
4563312640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4563312640
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  2108426900.0
relative error loss 0.7379359
time to take a step is 225.9907751083374
it  2 : 2496566784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
4565438464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 11% |
memory (bytes)
4565438464
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  2108426900.0
relative error loss 0.7379359
shape of L is 
torch.Size([])
memory (bytes)
4567576576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 30% | 11% |
memory (bytes)
4567576576
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1966070000.0
relative error loss 0.6881119
shape of L is 
torch.Size([])
memory (bytes)
4569686016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 11% |
memory (bytes)
4569694208
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1788384300.0
relative error loss 0.62592304
shape of L is 
torch.Size([])
memory (bytes)
4571828224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
4571828224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% | 11% |
error is  1659264000.0
relative error loss 0.58073175
shape of L is 
torch.Size([])
memory (bytes)
4573888512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 11% |
memory (bytes)
4573937664
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  2012545500.0
relative error loss 0.704378
shape of L is 
torch.Size([])
memory (bytes)
4575936512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 11% |
memory (bytes)
4576059392
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1568380900.0
relative error loss 0.54892325
shape of L is 
torch.Size([])
memory (bytes)
4578185216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 11% |
memory (bytes)
4578185216
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1456055300.0
relative error loss 0.50961
shape of L is 
torch.Size([])
memory (bytes)
4580139008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 32% | 11% |
memory (bytes)
4580352000
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1386299800.0
relative error loss 0.48519602
shape of L is 
torch.Size([])
memory (bytes)
4582486016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 11% |
memory (bytes)
4582494208
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1302585600.0
relative error loss 0.4558966
shape of L is 
torch.Size([])
memory (bytes)
4584632320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 11% |
memory (bytes)
4584640512
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1235038600.0
relative error loss 0.43225557
time to take a step is 284.00442242622375
it  3 : 2496566784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
4586749952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 11% |
memory (bytes)
4586786816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 11% |
error is  1235038600.0
relative error loss 0.43225557
shape of L is 
torch.Size([])
memory (bytes)
4588920832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4588929024
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1174464600.0
relative error loss 0.4110551
shape of L is 
torch.Size([])
memory (bytes)
4590858240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 11% |
memory (bytes)
4591071232
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1113123500.0
relative error loss 0.38958606
shape of L is 
torch.Size([])
memory (bytes)
4593209344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 32% | 11% |
memory (bytes)
4593209344
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1047439000.0
relative error loss 0.3665969
shape of L is 
torch.Size([])
memory (bytes)
4595347456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 11% |
memory (bytes)
4595347456
| ID | GPU  | MEM |
-------------------
|  0 |   2% |  0% |
|  1 | 100% | 11% |
error is  973823000.0
relative error loss 0.3408318
shape of L is 
torch.Size([])
memory (bytes)
4597428224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 11% |
memory (bytes)
4597489664
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  910391940.0
relative error loss 0.31863132
shape of L is 
torch.Size([])
memory (bytes)
4599623680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4599631872
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  854135200.0
relative error loss 0.29894182
shape of L is 
torch.Size([])
memory (bytes)
4601786368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
4601786368
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  801511800.0
relative error loss 0.280524
shape of L is 
torch.Size([])
memory (bytes)
4603858944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 11% |
memory (bytes)
4603924480
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  740169500.0
relative error loss 0.25905457
shape of L is 
torch.Size([])
memory (bytes)
4606066688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 11% |
memory (bytes)
4606066688
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  692886300.0
relative error loss 0.24250574
time to take a step is 235.03235125541687
c= tensor(497.2061, device='cuda:0')
c= tensor(37517.3789, device='cuda:0')
c= tensor(39658.0742, device='cuda:0')
c= tensor(55392.0469, device='cuda:0')
c= tensor(253658.9688, device='cuda:0')
c= tensor(320103.5625, device='cuda:0')
c= tensor(663010.4375, device='cuda:0')
c= tensor(764180.5000, device='cuda:0')
c= tensor(788359.5000, device='cuda:0')
c= tensor(1771425.8750, device='cuda:0')
c= tensor(1776751.7500, device='cuda:0')
c= tensor(3384293., device='cuda:0')
c= tensor(3392710.2500, device='cuda:0')
c= tensor(10305727., device='cuda:0')
c= tensor(10415794., device='cuda:0')
c= tensor(10522948., device='cuda:0')
c= tensor(11036321., device='cuda:0')
c= tensor(11451035., device='cuda:0')
c= tensor(14652554., device='cuda:0')
c= tensor(16803904., device='cuda:0')
c= tensor(16827386., device='cuda:0')
c= tensor(22902580., device='cuda:0')
c= tensor(22923240., device='cuda:0')
c= tensor(23262526., device='cuda:0')
c= tensor(23428654., device='cuda:0')
c= tensor(24136580., device='cuda:0')
c= tensor(24768870., device='cuda:0')
c= tensor(24782672., device='cuda:0')
c= tensor(26436102., device='cuda:0')
c= tensor(1.4515e+08, device='cuda:0')
c= tensor(1.4516e+08, device='cuda:0')
c= tensor(2.1782e+08, device='cuda:0')
c= tensor(2.1796e+08, device='cuda:0')
c= tensor(2.1797e+08, device='cuda:0')
c= tensor(2.1799e+08, device='cuda:0')
c= tensor(2.1951e+08, device='cuda:0')
c= tensor(2.2063e+08, device='cuda:0')
c= tensor(2.2063e+08, device='cuda:0')
c= tensor(2.2065e+08, device='cuda:0')
c= tensor(2.2065e+08, device='cuda:0')
c= tensor(2.2065e+08, device='cuda:0')
c= tensor(2.2065e+08, device='cuda:0')
c= tensor(2.2065e+08, device='cuda:0')
c= tensor(2.2066e+08, device='cuda:0')
c= tensor(2.2066e+08, device='cuda:0')
c= tensor(2.2066e+08, device='cuda:0')
c= tensor(2.2066e+08, device='cuda:0')
c= tensor(2.2067e+08, device='cuda:0')
c= tensor(2.2067e+08, device='cuda:0')
c= tensor(2.2071e+08, device='cuda:0')
c= tensor(2.2073e+08, device='cuda:0')
c= tensor(2.2074e+08, device='cuda:0')
c= tensor(2.2074e+08, device='cuda:0')
c= tensor(2.2075e+08, device='cuda:0')
c= tensor(2.2075e+08, device='cuda:0')
c= tensor(2.2076e+08, device='cuda:0')
c= tensor(2.2076e+08, device='cuda:0')
c= tensor(2.2077e+08, device='cuda:0')
c= tensor(2.2077e+08, device='cuda:0')
c= tensor(2.2077e+08, device='cuda:0')
c= tensor(2.2078e+08, device='cuda:0')
c= tensor(2.2079e+08, device='cuda:0')
c= tensor(2.2082e+08, device='cuda:0')
c= tensor(2.2082e+08, device='cuda:0')
c= tensor(2.2083e+08, device='cuda:0')
c= tensor(2.2083e+08, device='cuda:0')
c= tensor(2.2083e+08, device='cuda:0')
c= tensor(2.2083e+08, device='cuda:0')
c= tensor(2.2083e+08, device='cuda:0')
c= tensor(2.2084e+08, device='cuda:0')
c= tensor(2.2084e+08, device='cuda:0')
c= tensor(2.2084e+08, device='cuda:0')
c= tensor(2.2084e+08, device='cuda:0')
c= tensor(2.2085e+08, device='cuda:0')
c= tensor(2.2085e+08, device='cuda:0')
c= tensor(2.2086e+08, device='cuda:0')
c= tensor(2.2086e+08, device='cuda:0')
c= tensor(2.2086e+08, device='cuda:0')
c= tensor(2.2086e+08, device='cuda:0')
c= tensor(2.2087e+08, device='cuda:0')
c= tensor(2.2088e+08, device='cuda:0')
c= tensor(2.2088e+08, device='cuda:0')
c= tensor(2.2089e+08, device='cuda:0')
c= tensor(2.2089e+08, device='cuda:0')
c= tensor(2.2089e+08, device='cuda:0')
c= tensor(2.2089e+08, device='cuda:0')
c= tensor(2.2089e+08, device='cuda:0')
c= tensor(2.2089e+08, device='cuda:0')
c= tensor(2.2090e+08, device='cuda:0')
c= tensor(2.2090e+08, device='cuda:0')
c= tensor(2.2090e+08, device='cuda:0')
c= tensor(2.2090e+08, device='cuda:0')
c= tensor(2.2090e+08, device='cuda:0')
c= tensor(2.2091e+08, device='cuda:0')
c= tensor(2.2091e+08, device='cuda:0')
c= tensor(2.2092e+08, device='cuda:0')
c= tensor(2.2092e+08, device='cuda:0')
c= tensor(2.2094e+08, device='cuda:0')
c= tensor(2.2095e+08, device='cuda:0')
c= tensor(2.2095e+08, device='cuda:0')
c= tensor(2.2099e+08, device='cuda:0')
c= tensor(2.2099e+08, device='cuda:0')
c= tensor(2.2100e+08, device='cuda:0')
c= tensor(2.2100e+08, device='cuda:0')
c= tensor(2.2100e+08, device='cuda:0')
c= tensor(2.2100e+08, device='cuda:0')
c= tensor(2.2101e+08, device='cuda:0')
c= tensor(2.2101e+08, device='cuda:0')
c= tensor(2.2101e+08, device='cuda:0')
c= tensor(2.2101e+08, device='cuda:0')
c= tensor(2.2102e+08, device='cuda:0')
c= tensor(2.2102e+08, device='cuda:0')
c= tensor(2.2102e+08, device='cuda:0')
c= tensor(2.2102e+08, device='cuda:0')
c= tensor(2.2103e+08, device='cuda:0')
c= tensor(2.2103e+08, device='cuda:0')
c= tensor(2.2103e+08, device='cuda:0')
c= tensor(2.2103e+08, device='cuda:0')
c= tensor(2.2104e+08, device='cuda:0')
c= tensor(2.2104e+08, device='cuda:0')
c= tensor(2.2105e+08, device='cuda:0')
c= tensor(2.2105e+08, device='cuda:0')
c= tensor(2.2105e+08, device='cuda:0')
c= tensor(2.2106e+08, device='cuda:0')
c= tensor(2.2106e+08, device='cuda:0')
c= tensor(2.2106e+08, device='cuda:0')
c= tensor(2.2106e+08, device='cuda:0')
c= tensor(2.2106e+08, device='cuda:0')
c= tensor(2.2110e+08, device='cuda:0')
c= tensor(2.2110e+08, device='cuda:0')
c= tensor(2.2110e+08, device='cuda:0')
c= tensor(2.2111e+08, device='cuda:0')
c= tensor(2.2111e+08, device='cuda:0')
c= tensor(2.2111e+08, device='cuda:0')
c= tensor(2.2112e+08, device='cuda:0')
c= tensor(2.2112e+08, device='cuda:0')
c= tensor(2.2112e+08, device='cuda:0')
c= tensor(2.2112e+08, device='cuda:0')
c= tensor(2.2112e+08, device='cuda:0')
c= tensor(2.2112e+08, device='cuda:0')
c= tensor(2.2113e+08, device='cuda:0')
c= tensor(2.2113e+08, device='cuda:0')
c= tensor(2.2114e+08, device='cuda:0')
c= tensor(2.2120e+08, device='cuda:0')
c= tensor(2.2122e+08, device='cuda:0')
c= tensor(2.2122e+08, device='cuda:0')
c= tensor(2.2122e+08, device='cuda:0')
c= tensor(2.2122e+08, device='cuda:0')
c= tensor(2.2123e+08, device='cuda:0')
c= tensor(2.2123e+08, device='cuda:0')
c= tensor(2.2123e+08, device='cuda:0')
c= tensor(2.2124e+08, device='cuda:0')
c= tensor(2.2124e+08, device='cuda:0')
c= tensor(2.2126e+08, device='cuda:0')
c= tensor(2.2126e+08, device='cuda:0')
c= tensor(2.2133e+08, device='cuda:0')
c= tensor(2.2133e+08, device='cuda:0')
c= tensor(2.2133e+08, device='cuda:0')
c= tensor(2.2133e+08, device='cuda:0')
c= tensor(2.2133e+08, device='cuda:0')
c= tensor(2.2134e+08, device='cuda:0')
c= tensor(2.2134e+08, device='cuda:0')
c= tensor(2.2135e+08, device='cuda:0')
c= tensor(2.2135e+08, device='cuda:0')
c= tensor(2.2135e+08, device='cuda:0')
c= tensor(2.2135e+08, device='cuda:0')
c= tensor(2.2136e+08, device='cuda:0')
c= tensor(2.2136e+08, device='cuda:0')
c= tensor(2.2136e+08, device='cuda:0')
c= tensor(2.2136e+08, device='cuda:0')
c= tensor(2.2136e+08, device='cuda:0')
c= tensor(2.2136e+08, device='cuda:0')
c= tensor(2.2137e+08, device='cuda:0')
c= tensor(2.2137e+08, device='cuda:0')
c= tensor(2.2137e+08, device='cuda:0')
c= tensor(2.2138e+08, device='cuda:0')
c= tensor(2.2138e+08, device='cuda:0')
c= tensor(2.2139e+08, device='cuda:0')
c= tensor(2.2140e+08, device='cuda:0')
c= tensor(2.2140e+08, device='cuda:0')
c= tensor(2.2140e+08, device='cuda:0')
c= tensor(2.2140e+08, device='cuda:0')
c= tensor(2.2141e+08, device='cuda:0')
c= tensor(2.2141e+08, device='cuda:0')
c= tensor(2.2142e+08, device='cuda:0')
c= tensor(2.2142e+08, device='cuda:0')
c= tensor(2.2142e+08, device='cuda:0')
c= tensor(2.2143e+08, device='cuda:0')
c= tensor(2.2144e+08, device='cuda:0')
c= tensor(2.2152e+08, device='cuda:0')
c= tensor(2.2152e+08, device='cuda:0')
c= tensor(2.2152e+08, device='cuda:0')
c= tensor(2.2152e+08, device='cuda:0')
c= tensor(2.2152e+08, device='cuda:0')
c= tensor(2.2152e+08, device='cuda:0')
c= tensor(2.2153e+08, device='cuda:0')
c= tensor(2.2153e+08, device='cuda:0')
c= tensor(2.2153e+08, device='cuda:0')
c= tensor(2.2153e+08, device='cuda:0')
c= tensor(2.2153e+08, device='cuda:0')
c= tensor(2.2154e+08, device='cuda:0')
c= tensor(2.2154e+08, device='cuda:0')
c= tensor(2.2155e+08, device='cuda:0')
c= tensor(2.2156e+08, device='cuda:0')
c= tensor(2.2156e+08, device='cuda:0')
c= tensor(2.2156e+08, device='cuda:0')
c= tensor(2.2157e+08, device='cuda:0')
c= tensor(2.2157e+08, device='cuda:0')
c= tensor(2.2157e+08, device='cuda:0')
c= tensor(2.2158e+08, device='cuda:0')
c= tensor(2.2160e+08, device='cuda:0')
c= tensor(2.2160e+08, device='cuda:0')
c= tensor(2.2160e+08, device='cuda:0')
c= tensor(2.2160e+08, device='cuda:0')
c= tensor(2.2161e+08, device='cuda:0')
c= tensor(2.2161e+08, device='cuda:0')
c= tensor(2.2161e+08, device='cuda:0')
c= tensor(2.2161e+08, device='cuda:0')
c= tensor(2.2161e+08, device='cuda:0')
c= tensor(2.2162e+08, device='cuda:0')
c= tensor(2.2162e+08, device='cuda:0')
c= tensor(2.2162e+08, device='cuda:0')
c= tensor(2.2162e+08, device='cuda:0')
c= tensor(2.2162e+08, device='cuda:0')
c= tensor(2.2163e+08, device='cuda:0')
c= tensor(2.2163e+08, device='cuda:0')
c= tensor(2.2163e+08, device='cuda:0')
c= tensor(2.2164e+08, device='cuda:0')
c= tensor(2.2165e+08, device='cuda:0')
c= tensor(2.2165e+08, device='cuda:0')
c= tensor(2.2165e+08, device='cuda:0')
c= tensor(2.2165e+08, device='cuda:0')
c= tensor(2.2165e+08, device='cuda:0')
c= tensor(2.2166e+08, device='cuda:0')
c= tensor(2.2166e+08, device='cuda:0')
c= tensor(2.2166e+08, device='cuda:0')
c= tensor(2.2166e+08, device='cuda:0')
c= tensor(2.2167e+08, device='cuda:0')
c= tensor(2.2168e+08, device='cuda:0')
c= tensor(2.2171e+08, device='cuda:0')
c= tensor(2.2377e+08, device='cuda:0')
c= tensor(2.2378e+08, device='cuda:0')
c= tensor(2.2378e+08, device='cuda:0')
c= tensor(2.2378e+08, device='cuda:0')
c= tensor(2.2379e+08, device='cuda:0')
c= tensor(2.2381e+08, device='cuda:0')
c= tensor(2.3005e+08, device='cuda:0')
c= tensor(2.3006e+08, device='cuda:0')
c= tensor(2.3063e+08, device='cuda:0')
c= tensor(2.3129e+08, device='cuda:0')
c= tensor(2.3131e+08, device='cuda:0')
c= tensor(2.3416e+08, device='cuda:0')
c= tensor(2.3416e+08, device='cuda:0')
c= tensor(2.3417e+08, device='cuda:0')
c= tensor(2.3588e+08, device='cuda:0')
c= tensor(2.4989e+08, device='cuda:0')
c= tensor(2.4989e+08, device='cuda:0')
c= tensor(2.5001e+08, device='cuda:0')
c= tensor(2.5008e+08, device='cuda:0')
c= tensor(2.5082e+08, device='cuda:0')
c= tensor(2.5148e+08, device='cuda:0')
c= tensor(2.5167e+08, device='cuda:0')
c= tensor(2.5195e+08, device='cuda:0')
c= tensor(2.5207e+08, device='cuda:0')
c= tensor(2.5210e+08, device='cuda:0')
c= tensor(2.6298e+08, device='cuda:0')
c= tensor(2.6298e+08, device='cuda:0')
c= tensor(2.6299e+08, device='cuda:0')
c= tensor(2.6305e+08, device='cuda:0')
c= tensor(2.6320e+08, device='cuda:0')
c= tensor(2.7841e+08, device='cuda:0')
c= tensor(2.7906e+08, device='cuda:0')
c= tensor(2.7906e+08, device='cuda:0')
c= tensor(2.7910e+08, device='cuda:0')
c= tensor(2.7914e+08, device='cuda:0')
c= tensor(2.7925e+08, device='cuda:0')
c= tensor(2.8092e+08, device='cuda:0')
c= tensor(2.8201e+08, device='cuda:0')
c= tensor(2.8220e+08, device='cuda:0')
c= tensor(2.8220e+08, device='cuda:0')
c= tensor(2.8221e+08, device='cuda:0')
c= tensor(2.8291e+08, device='cuda:0')
c= tensor(2.8322e+08, device='cuda:0')
c= tensor(2.8335e+08, device='cuda:0')
c= tensor(2.8337e+08, device='cuda:0')
c= tensor(3.0248e+08, device='cuda:0')
c= tensor(3.0248e+08, device='cuda:0')
c= tensor(3.0259e+08, device='cuda:0')
c= tensor(3.0427e+08, device='cuda:0')
c= tensor(3.0427e+08, device='cuda:0')
c= tensor(3.0454e+08, device='cuda:0')
c= tensor(3.1296e+08, device='cuda:0')
c= tensor(3.2634e+08, device='cuda:0')
c= tensor(3.2637e+08, device='cuda:0')
c= tensor(3.2639e+08, device='cuda:0')
c= tensor(3.2639e+08, device='cuda:0')
c= tensor(3.2639e+08, device='cuda:0')
c= tensor(3.2691e+08, device='cuda:0')
c= tensor(3.2692e+08, device='cuda:0')
c= tensor(3.2717e+08, device='cuda:0')
c= tensor(3.3185e+08, device='cuda:0')
c= tensor(3.3226e+08, device='cuda:0')
c= tensor(3.3236e+08, device='cuda:0')
c= tensor(3.3236e+08, device='cuda:0')
c= tensor(3.3837e+08, device='cuda:0')
c= tensor(3.3940e+08, device='cuda:0')
c= tensor(3.3949e+08, device='cuda:0')
c= tensor(3.3959e+08, device='cuda:0')
c= tensor(3.5983e+08, device='cuda:0')
c= tensor(3.5984e+08, device='cuda:0')
c= tensor(3.6219e+08, device='cuda:0')
c= tensor(3.6220e+08, device='cuda:0')
c= tensor(3.6348e+08, device='cuda:0')
c= tensor(3.6358e+08, device='cuda:0')
c= tensor(3.6871e+08, device='cuda:0')
c= tensor(3.6915e+08, device='cuda:0')
c= tensor(3.6915e+08, device='cuda:0')
c= tensor(3.7112e+08, device='cuda:0')
c= tensor(3.7245e+08, device='cuda:0')
c= tensor(3.7245e+08, device='cuda:0')
c= tensor(3.7295e+08, device='cuda:0')
c= tensor(3.7693e+08, device='cuda:0')
c= tensor(3.8912e+08, device='cuda:0')
c= tensor(3.9377e+08, device='cuda:0')
c= tensor(3.9377e+08, device='cuda:0')
c= tensor(3.9378e+08, device='cuda:0')
c= tensor(3.9388e+08, device='cuda:0')
c= tensor(3.9395e+08, device='cuda:0')
c= tensor(3.9397e+08, device='cuda:0')
c= tensor(3.9398e+08, device='cuda:0')
c= tensor(3.9447e+08, device='cuda:0')
c= tensor(3.9528e+08, device='cuda:0')
c= tensor(3.9546e+08, device='cuda:0')
c= tensor(3.9547e+08, device='cuda:0')
c= tensor(3.9553e+08, device='cuda:0')
c= tensor(3.9556e+08, device='cuda:0')
c= tensor(3.9558e+08, device='cuda:0')
c= tensor(3.9560e+08, device='cuda:0')
c= tensor(3.9561e+08, device='cuda:0')
c= tensor(4.0014e+08, device='cuda:0')
c= tensor(4.0024e+08, device='cuda:0')
c= tensor(4.0029e+08, device='cuda:0')
c= tensor(4.0066e+08, device='cuda:0')
c= tensor(4.0067e+08, device='cuda:0')
c= tensor(6.0295e+08, device='cuda:0')
c= tensor(6.0296e+08, device='cuda:0')
c= tensor(6.0410e+08, device='cuda:0')
c= tensor(6.0410e+08, device='cuda:0')
c= tensor(6.0410e+08, device='cuda:0')
c= tensor(6.0411e+08, device='cuda:0')
c= tensor(6.0414e+08, device='cuda:0')
c= tensor(6.0415e+08, device='cuda:0')
c= tensor(6.0465e+08, device='cuda:0')
c= tensor(6.0465e+08, device='cuda:0')
c= tensor(6.0466e+08, device='cuda:0')
c= tensor(6.1408e+08, device='cuda:0')
c= tensor(6.1431e+08, device='cuda:0')
c= tensor(6.1460e+08, device='cuda:0')
c= tensor(6.1575e+08, device='cuda:0')
c= tensor(6.1800e+08, device='cuda:0')
c= tensor(6.1800e+08, device='cuda:0')
c= tensor(6.1801e+08, device='cuda:0')
c= tensor(6.1807e+08, device='cuda:0')
c= tensor(6.1807e+08, device='cuda:0')
c= tensor(6.1807e+08, device='cuda:0')
c= tensor(6.1813e+08, device='cuda:0')
c= tensor(6.1813e+08, device='cuda:0')
c= tensor(6.1813e+08, device='cuda:0')
c= tensor(6.1813e+08, device='cuda:0')
c= tensor(6.1814e+08, device='cuda:0')
c= tensor(6.3749e+08, device='cuda:0')
c= tensor(6.3753e+08, device='cuda:0')
c= tensor(6.3822e+08, device='cuda:0')
c= tensor(6.3825e+08, device='cuda:0')
c= tensor(6.3825e+08, device='cuda:0')
c= tensor(6.3830e+08, device='cuda:0')
c= tensor(7.0803e+08, device='cuda:0')
c= tensor(7.2300e+08, device='cuda:0')
c= tensor(7.2307e+08, device='cuda:0')
c= tensor(7.2320e+08, device='cuda:0')
c= tensor(7.2320e+08, device='cuda:0')
c= tensor(7.2428e+08, device='cuda:0')
c= tensor(7.3592e+08, device='cuda:0')
c= tensor(7.3628e+08, device='cuda:0')
c= tensor(7.3630e+08, device='cuda:0')
c= tensor(7.3727e+08, device='cuda:0')
c= tensor(7.5156e+08, device='cuda:0')
c= tensor(7.5164e+08, device='cuda:0')
c= tensor(7.5165e+08, device='cuda:0')
c= tensor(7.5169e+08, device='cuda:0')
c= tensor(7.5175e+08, device='cuda:0')
c= tensor(7.5175e+08, device='cuda:0')
c= tensor(7.5557e+08, device='cuda:0')
c= tensor(7.5562e+08, device='cuda:0')
c= tensor(7.5562e+08, device='cuda:0')
c= tensor(7.5567e+08, device='cuda:0')
c= tensor(7.5574e+08, device='cuda:0')
c= tensor(7.5574e+08, device='cuda:0')
c= tensor(7.5775e+08, device='cuda:0')
c= tensor(7.5933e+08, device='cuda:0')
c= tensor(7.6175e+08, device='cuda:0')
c= tensor(7.6476e+08, device='cuda:0')
c= tensor(7.6600e+08, device='cuda:0')
c= tensor(7.6604e+08, device='cuda:0')
c= tensor(7.6607e+08, device='cuda:0')
c= tensor(7.6622e+08, device='cuda:0')
c= tensor(7.6707e+08, device='cuda:0')
c= tensor(7.6708e+08, device='cuda:0')
c= tensor(7.6940e+08, device='cuda:0')
c= tensor(7.9183e+08, device='cuda:0')
c= tensor(7.9274e+08, device='cuda:0')
c= tensor(7.9314e+08, device='cuda:0')
c= tensor(7.9697e+08, device='cuda:0')
c= tensor(7.9699e+08, device='cuda:0')
c= tensor(7.9700e+08, device='cuda:0')
c= tensor(7.9717e+08, device='cuda:0')
c= tensor(7.9846e+08, device='cuda:0')
c= tensor(7.9949e+08, device='cuda:0')
c= tensor(8.3410e+08, device='cuda:0')
c= tensor(8.3904e+08, device='cuda:0')
c= tensor(8.4005e+08, device='cuda:0')
c= tensor(8.4025e+08, device='cuda:0')
c= tensor(8.4197e+08, device='cuda:0')
c= tensor(8.4197e+08, device='cuda:0')
c= tensor(8.4198e+08, device='cuda:0')
c= tensor(8.4950e+08, device='cuda:0')
c= tensor(8.4952e+08, device='cuda:0')
c= tensor(8.4952e+08, device='cuda:0')
c= tensor(8.4964e+08, device='cuda:0')
c= tensor(8.5360e+08, device='cuda:0')
c= tensor(8.5375e+08, device='cuda:0')
c= tensor(8.5441e+08, device='cuda:0')
c= tensor(8.5442e+08, device='cuda:0')
c= tensor(8.5445e+08, device='cuda:0')
c= tensor(8.5445e+08, device='cuda:0')
c= tensor(8.5460e+08, device='cuda:0')
c= tensor(8.5470e+08, device='cuda:0')
c= tensor(8.5531e+08, device='cuda:0')
c= tensor(8.5532e+08, device='cuda:0')
c= tensor(8.5725e+08, device='cuda:0')
c= tensor(8.5725e+08, device='cuda:0')
c= tensor(8.5741e+08, device='cuda:0')
c= tensor(8.5743e+08, device='cuda:0')
c= tensor(8.5750e+08, device='cuda:0')
c= tensor(8.5750e+08, device='cuda:0')
c= tensor(8.5770e+08, device='cuda:0')
c= tensor(8.5773e+08, device='cuda:0')
c= tensor(8.5786e+08, device='cuda:0')
c= tensor(8.5907e+08, device='cuda:0')
c= tensor(8.6663e+08, device='cuda:0')
c= tensor(8.6664e+08, device='cuda:0')
c= tensor(8.6667e+08, device='cuda:0')
c= tensor(8.6931e+08, device='cuda:0')
c= tensor(8.6933e+08, device='cuda:0')
c= tensor(9.0006e+08, device='cuda:0')
c= tensor(9.0006e+08, device='cuda:0')
c= tensor(9.0095e+08, device='cuda:0')
c= tensor(9.0426e+08, device='cuda:0')
c= tensor(9.0426e+08, device='cuda:0')
c= tensor(9.0924e+08, device='cuda:0')
c= tensor(9.0964e+08, device='cuda:0')
c= tensor(9.4725e+08, device='cuda:0')
c= tensor(9.4726e+08, device='cuda:0')
c= tensor(9.4735e+08, device='cuda:0')
c= tensor(9.4736e+08, device='cuda:0')
c= tensor(9.4736e+08, device='cuda:0')
c= tensor(9.4738e+08, device='cuda:0')
c= tensor(9.4769e+08, device='cuda:0')
c= tensor(9.4772e+08, device='cuda:0')
c= tensor(9.4844e+08, device='cuda:0')
c= tensor(9.4846e+08, device='cuda:0')
c= tensor(9.4846e+08, device='cuda:0')
c= tensor(9.4848e+08, device='cuda:0')
c= tensor(9.5167e+08, device='cuda:0')
c= tensor(9.5204e+08, device='cuda:0')
c= tensor(9.5643e+08, device='cuda:0')
c= tensor(9.5664e+08, device='cuda:0')
c= tensor(9.5667e+08, device='cuda:0')
c= tensor(9.5668e+08, device='cuda:0')
c= tensor(9.5669e+08, device='cuda:0')
c= tensor(9.7402e+08, device='cuda:0')
c= tensor(9.7404e+08, device='cuda:0')
c= tensor(9.7406e+08, device='cuda:0')
c= tensor(9.7447e+08, device='cuda:0')
c= tensor(9.7512e+08, device='cuda:0')
c= tensor(9.7512e+08, device='cuda:0')
c= tensor(9.7513e+08, device='cuda:0')
c= tensor(1.0011e+09, device='cuda:0')
c= tensor(1.0011e+09, device='cuda:0')
c= tensor(1.0019e+09, device='cuda:0')
c= tensor(1.0020e+09, device='cuda:0')
c= tensor(1.0046e+09, device='cuda:0')
c= tensor(1.0083e+09, device='cuda:0')
c= tensor(1.0141e+09, device='cuda:0')
c= tensor(1.0153e+09, device='cuda:0')
c= tensor(1.0153e+09, device='cuda:0')
c= tensor(1.0157e+09, device='cuda:0')
c= tensor(1.0157e+09, device='cuda:0')
c= tensor(1.0157e+09, device='cuda:0')
c= tensor(1.0158e+09, device='cuda:0')
c= tensor(1.0158e+09, device='cuda:0')
c= tensor(1.0171e+09, device='cuda:0')
c= tensor(1.0171e+09, device='cuda:0')
c= tensor(1.0171e+09, device='cuda:0')
c= tensor(1.0171e+09, device='cuda:0')
c= tensor(1.0172e+09, device='cuda:0')
c= tensor(1.0213e+09, device='cuda:0')
c= tensor(1.0213e+09, device='cuda:0')
c= tensor(1.0214e+09, device='cuda:0')
c= tensor(1.0215e+09, device='cuda:0')
c= tensor(1.0216e+09, device='cuda:0')
c= tensor(1.0216e+09, device='cuda:0')
c= tensor(1.0216e+09, device='cuda:0')
c= tensor(1.0217e+09, device='cuda:0')
c= tensor(1.0240e+09, device='cuda:0')
c= tensor(1.0240e+09, device='cuda:0')
c= tensor(1.0240e+09, device='cuda:0')
c= tensor(1.0240e+09, device='cuda:0')
c= tensor(1.0322e+09, device='cuda:0')
c= tensor(1.0852e+09, device='cuda:0')
c= tensor(1.0853e+09, device='cuda:0')
c= tensor(1.0853e+09, device='cuda:0')
c= tensor(1.0860e+09, device='cuda:0')
c= tensor(1.0868e+09, device='cuda:0')
c= tensor(1.0868e+09, device='cuda:0')
c= tensor(1.0869e+09, device='cuda:0')
c= tensor(1.0870e+09, device='cuda:0')
c= tensor(1.0938e+09, device='cuda:0')
c= tensor(1.0967e+09, device='cuda:0')
c= tensor(1.1001e+09, device='cuda:0')
c= tensor(1.1001e+09, device='cuda:0')
c= tensor(1.1001e+09, device='cuda:0')
c= tensor(1.1001e+09, device='cuda:0')
c= tensor(1.1003e+09, device='cuda:0')
c= tensor(1.1003e+09, device='cuda:0')
c= tensor(1.1005e+09, device='cuda:0')
c= tensor(1.1022e+09, device='cuda:0')
c= tensor(1.1123e+09, device='cuda:0')
c= tensor(1.1123e+09, device='cuda:0')
c= tensor(1.1123e+09, device='cuda:0')
c= tensor(1.1123e+09, device='cuda:0')
c= tensor(1.2648e+09, device='cuda:0')
c= tensor(1.2649e+09, device='cuda:0')
c= tensor(1.2649e+09, device='cuda:0')
c= tensor(1.2649e+09, device='cuda:0')
c= tensor(1.2651e+09, device='cuda:0')
c= tensor(1.2651e+09, device='cuda:0')
c= tensor(1.2659e+09, device='cuda:0')
c= tensor(1.2659e+09, device='cuda:0')
c= tensor(1.2659e+09, device='cuda:0')
c= tensor(1.2660e+09, device='cuda:0')
c= tensor(1.2660e+09, device='cuda:0')
c= tensor(1.2660e+09, device='cuda:0')
c= tensor(1.2689e+09, device='cuda:0')
c= tensor(1.2919e+09, device='cuda:0')
c= tensor(1.2940e+09, device='cuda:0')
c= tensor(1.2951e+09, device='cuda:0')
c= tensor(1.2952e+09, device='cuda:0')
c= tensor(1.2952e+09, device='cuda:0')
c= tensor(1.2952e+09, device='cuda:0')
c= tensor(1.2956e+09, device='cuda:0')
c= tensor(1.2957e+09, device='cuda:0')
c= tensor(1.2975e+09, device='cuda:0')
c= tensor(1.2975e+09, device='cuda:0')
c= tensor(1.3505e+09, device='cuda:0')
c= tensor(1.3505e+09, device='cuda:0')
c= tensor(1.3508e+09, device='cuda:0')
c= tensor(1.3575e+09, device='cuda:0')
c= tensor(1.3578e+09, device='cuda:0')
c= tensor(1.3598e+09, device='cuda:0')
c= tensor(1.3668e+09, device='cuda:0')
c= tensor(1.3671e+09, device='cuda:0')
c= tensor(1.3673e+09, device='cuda:0')
c= tensor(1.3673e+09, device='cuda:0')
c= tensor(1.3675e+09, device='cuda:0')
c= tensor(1.3675e+09, device='cuda:0')
c= tensor(1.3695e+09, device='cuda:0')
c= tensor(1.4275e+09, device='cuda:0')
c= tensor(1.4317e+09, device='cuda:0')
c= tensor(1.4338e+09, device='cuda:0')
c= tensor(1.4340e+09, device='cuda:0')
c= tensor(1.4342e+09, device='cuda:0')
c= tensor(1.4346e+09, device='cuda:0')
c= tensor(1.4346e+09, device='cuda:0')
c= tensor(1.4366e+09, device='cuda:0')
c= tensor(1.4403e+09, device='cuda:0')
c= tensor(1.4404e+09, device='cuda:0')
c= tensor(1.4801e+09, device='cuda:0')
c= tensor(1.4872e+09, device='cuda:0')
c= tensor(1.4873e+09, device='cuda:0')
c= tensor(1.4873e+09, device='cuda:0')
c= tensor(1.4877e+09, device='cuda:0')
c= tensor(1.4882e+09, device='cuda:0')
c= tensor(1.4882e+09, device='cuda:0')
c= tensor(1.4999e+09, device='cuda:0')
c= tensor(1.5003e+09, device='cuda:0')
c= tensor(1.5004e+09, device='cuda:0')
c= tensor(1.5004e+09, device='cuda:0')
c= tensor(1.5004e+09, device='cuda:0')
c= tensor(1.5004e+09, device='cuda:0')
c= tensor(1.5004e+09, device='cuda:0')
c= tensor(1.5005e+09, device='cuda:0')
c= tensor(1.5007e+09, device='cuda:0')
c= tensor(2.6814e+09, device='cuda:0')
c= tensor(2.6815e+09, device='cuda:0')
c= tensor(2.6816e+09, device='cuda:0')
c= tensor(2.6816e+09, device='cuda:0')
c= tensor(2.6817e+09, device='cuda:0')
c= tensor(2.6817e+09, device='cuda:0')
c= tensor(2.6872e+09, device='cuda:0')
c= tensor(2.6876e+09, device='cuda:0')
c= tensor(2.7317e+09, device='cuda:0')
c= tensor(2.7317e+09, device='cuda:0')
c= tensor(2.7363e+09, device='cuda:0')
c= tensor(2.7367e+09, device='cuda:0')
c= tensor(2.7387e+09, device='cuda:0')
c= tensor(2.7552e+09, device='cuda:0')
c= tensor(2.7552e+09, device='cuda:0')
c= tensor(2.7552e+09, device='cuda:0')
c= tensor(2.7560e+09, device='cuda:0')
c= tensor(2.7560e+09, device='cuda:0')
c= tensor(2.7563e+09, device='cuda:0')
c= tensor(2.7567e+09, device='cuda:0')
c= tensor(2.7574e+09, device='cuda:0')
c= tensor(2.7578e+09, device='cuda:0')
c= tensor(2.7579e+09, device='cuda:0')
c= tensor(2.7597e+09, device='cuda:0')
c= tensor(2.7712e+09, device='cuda:0')
c= tensor(2.7712e+09, device='cuda:0')
c= tensor(2.7712e+09, device='cuda:0')
c= tensor(2.7758e+09, device='cuda:0')
c= tensor(2.7759e+09, device='cuda:0')
c= tensor(2.7800e+09, device='cuda:0')
c= tensor(2.7803e+09, device='cuda:0')
c= tensor(2.7809e+09, device='cuda:0')
c= tensor(2.7810e+09, device='cuda:0')
c= tensor(2.7847e+09, device='cuda:0')
c= tensor(2.7854e+09, device='cuda:0')
c= tensor(2.7854e+09, device='cuda:0')
c= tensor(2.7854e+09, device='cuda:0')
c= tensor(2.7855e+09, device='cuda:0')
c= tensor(2.7859e+09, device='cuda:0')
c= tensor(2.7875e+09, device='cuda:0')
c= tensor(2.7889e+09, device='cuda:0')
c= tensor(2.7889e+09, device='cuda:0')
c= tensor(2.7891e+09, device='cuda:0')
c= tensor(2.7894e+09, device='cuda:0')
c= tensor(2.7914e+09, device='cuda:0')
c= tensor(2.7916e+09, device='cuda:0')
c= tensor(2.7923e+09, device='cuda:0')
c= tensor(2.7927e+09, device='cuda:0')
c= tensor(2.7928e+09, device='cuda:0')
c= tensor(2.7928e+09, device='cuda:0')
c= tensor(2.7928e+09, device='cuda:0')
c= tensor(2.7931e+09, device='cuda:0')
c= tensor(2.7933e+09, device='cuda:0')
c= tensor(2.7935e+09, device='cuda:0')
c= tensor(2.7935e+09, device='cuda:0')
c= tensor(2.7935e+09, device='cuda:0')
c= tensor(2.7939e+09, device='cuda:0')
c= tensor(2.7941e+09, device='cuda:0')
c= tensor(2.7942e+09, device='cuda:0')
c= tensor(2.7942e+09, device='cuda:0')
c= tensor(2.7942e+09, device='cuda:0')
c= tensor(2.7942e+09, device='cuda:0')
c= tensor(2.7944e+09, device='cuda:0')
c= tensor(2.7959e+09, device='cuda:0')
c= tensor(2.7959e+09, device='cuda:0')
c= tensor(2.7965e+09, device='cuda:0')
c= tensor(2.7966e+09, device='cuda:0')
c= tensor(2.7966e+09, device='cuda:0')
c= tensor(2.8096e+09, device='cuda:0')
c= tensor(2.8098e+09, device='cuda:0')
c= tensor(2.8107e+09, device='cuda:0')
c= tensor(2.8147e+09, device='cuda:0')
c= tensor(2.8149e+09, device='cuda:0')
c= tensor(2.8169e+09, device='cuda:0')
c= tensor(2.8196e+09, device='cuda:0')
c= tensor(2.8196e+09, device='cuda:0')
c= tensor(2.8198e+09, device='cuda:0')
c= tensor(2.8198e+09, device='cuda:0')
c= tensor(2.8248e+09, device='cuda:0')
c= tensor(2.8273e+09, device='cuda:0')
c= tensor(2.8274e+09, device='cuda:0')
c= tensor(2.8276e+09, device='cuda:0')
c= tensor(2.8277e+09, device='cuda:0')
c= tensor(2.8281e+09, device='cuda:0')
c= tensor(2.8281e+09, device='cuda:0')
c= tensor(2.8281e+09, device='cuda:0')
c= tensor(2.8344e+09, device='cuda:0')
c= tensor(2.8518e+09, device='cuda:0')
c= tensor(2.8518e+09, device='cuda:0')
c= tensor(2.8519e+09, device='cuda:0')
c= tensor(2.8519e+09, device='cuda:0')
c= tensor(2.8557e+09, device='cuda:0')
c= tensor(2.8557e+09, device='cuda:0')
c= tensor(2.8557e+09, device='cuda:0')
c= tensor(2.8560e+09, device='cuda:0')
c= tensor(2.8569e+09, device='cuda:0')
c= tensor(2.8569e+09, device='cuda:0')
c= tensor(2.8572e+09, device='cuda:0')
c= tensor(2.8572e+09, device='cuda:0')
time to make c is 10.649364233016968
time for making loss is 10.649458885192871
p0 True
it  0 : 1717140992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
4608270336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 10% |
memory (bytes)
4608462848
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  692886300.0
relative error loss 0.24250574
shape of L is 
torch.Size([])
memory (bytes)
4635336704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 30% | 10% |
memory (bytes)
4635336704
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  678743040.0
relative error loss 0.23755571
shape of L is 
torch.Size([])
memory (bytes)
4638760960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
4638769152
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  668980500.0
relative error loss 0.23413888
shape of L is 
torch.Size([])
memory (bytes)
4641959936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 32% | 10% |
memory (bytes)
4642045952
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  661186560.0
relative error loss 0.23141105
shape of L is 
torch.Size([])
memory (bytes)
4645236736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
4645236736
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  652096000.0
relative error loss 0.22822942
shape of L is 
torch.Size([])
memory (bytes)
4648419328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 10% |
memory (bytes)
4648419328
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  645005300.0
relative error loss 0.22574772
shape of L is 
torch.Size([])
memory (bytes)
4651589632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
4651671552
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  638570750.0
relative error loss 0.22349566
shape of L is 
torch.Size([])
memory (bytes)
4654850048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 10% |
memory (bytes)
4654878720
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  634321400.0
relative error loss 0.22200842
shape of L is 
torch.Size([])
memory (bytes)
4657934336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 10% |
memory (bytes)
4658077696
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  630526200.0
relative error loss 0.22068012
shape of L is 
torch.Size([])
memory (bytes)
4661313536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 10% |
memory (bytes)
4661313536
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  626777600.0
relative error loss 0.21936813
time to take a step is 286.17992520332336
it  1 : 2500274176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
4664295424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
4664532992
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  626777600.0
relative error loss 0.21936813
shape of L is 
torch.Size([])
memory (bytes)
4667740160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
4667740160
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  623634940.0
relative error loss 0.21826823
shape of L is 
torch.Size([])
memory (bytes)
4670914560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
4670914560
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  620926200.0
relative error loss 0.21732019
shape of L is 
torch.Size([])
memory (bytes)
4674166784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
4674166784
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  618450940.0
relative error loss 0.21645387
shape of L is 
torch.Size([])
memory (bytes)
4677279744
| ID | GPU | MEM |
------------------
|  0 |  2% |  0% |
|  1 | 11% | 11% |
memory (bytes)
4677378048
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  616580350.0
relative error loss 0.21579917
shape of L is 
torch.Size([])
memory (bytes)
4680605696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 11% |
memory (bytes)
4680605696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  614716400.0
relative error loss 0.2151468
shape of L is 
torch.Size([])
memory (bytes)
4683792384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 11% |
memory (bytes)
4683821056
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  612862460.0
relative error loss 0.21449792
shape of L is 
torch.Size([])
memory (bytes)
4686950400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 10% |
memory (bytes)
4687065088
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  611612160.0
relative error loss 0.21406034
shape of L is 
torch.Size([])
memory (bytes)
4690259968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 30% | 10% |
memory (bytes)
4690272256
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  610222100.0
relative error loss 0.21357381
shape of L is 
torch.Size([])
memory (bytes)
4693438464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 34% | 11% |
memory (bytes)
4693487616
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  609166100.0
relative error loss 0.21320422
time to take a step is 282.1135685443878
it  2 : 2500274176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
4696694784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
4696694784
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  609166100.0
relative error loss 0.21320422
shape of L is 
torch.Size([])
memory (bytes)
4699848704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 10% |
memory (bytes)
4699914240
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  608102140.0
relative error loss 0.21283184
shape of L is 
torch.Size([])
memory (bytes)
4703125504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
4703125504
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  606534400.0
relative error loss 0.21228315
shape of L is 
torch.Size([])
memory (bytes)
4706164736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 11% |
memory (bytes)
4706336768
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  605193500.0
relative error loss 0.21181384
shape of L is 
torch.Size([])
memory (bytes)
4709548032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
4709548032
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  603918100.0
relative error loss 0.21136746
shape of L is 
torch.Size([])
memory (bytes)
4712706048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 34% | 11% |
memory (bytes)
4712755200
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  602528000.0
relative error loss 0.21088094
shape of L is 
torch.Size([])
memory (bytes)
4715970560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
4715978752
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  601066750.0
relative error loss 0.2103695
shape of L is 
torch.Size([])
memory (bytes)
4719185920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
4719194112
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  600004600.0
relative error loss 0.20999776
shape of L is 
torch.Size([])
memory (bytes)
4722339840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 11% |
memory (bytes)
4722401280
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  598655740.0
relative error loss 0.20952567
shape of L is 
torch.Size([])
memory (bytes)
4725620736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
4725620736
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  597775100.0
relative error loss 0.20921744
time to take a step is 282.60866141319275
it  3 : 2500274176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
4728635392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 30% | 10% |
memory (bytes)
4728823808
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  597775100.0
relative error loss 0.20921744
shape of L is 
torch.Size([])
memory (bytes)
4732039168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
4732039168
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  596965100.0
relative error loss 0.20893396
shape of L is 
torch.Size([])
memory (bytes)
4735258624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
4735258624
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  596374000.0
relative error loss 0.20872708
shape of L is 
torch.Size([])
memory (bytes)
4738482176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 34% | 11% |
memory (bytes)
4738482176
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  595689700.0
relative error loss 0.20848759
shape of L is 
torch.Size([])
memory (bytes)
4741513216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
4741681152
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  595122940.0
relative error loss 0.2082892
shape of L is 
torch.Size([])
memory (bytes)
4744753152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 11% |
memory (bytes)
4744753152
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  594704640.0
relative error loss 0.2081428
shape of L is 
torch.Size([])
memory (bytes)
4748124160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
4748124160
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  594226700.0
relative error loss 0.20797552
shape of L is 
torch.Size([])
memory (bytes)
4751257600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 10% |
memory (bytes)
4751331328
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  593672450.0
relative error loss 0.20778154
shape of L is 
torch.Size([])
memory (bytes)
4754530304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 34% | 11% |
memory (bytes)
4754530304
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  593287700.0
relative error loss 0.20764688
shape of L is 
torch.Size([])
memory (bytes)
4757585920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
4757757952
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  592938240.0
relative error loss 0.20752458
time to take a step is 282.4067552089691
it  4 : 2500274176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
4760965120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 11% |
memory (bytes)
4760965120
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  592938240.0
relative error loss 0.20752458
shape of L is 
torch.Size([])
memory (bytes)
4764176384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
4764176384
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  592654100.0
relative error loss 0.20742512
shape of L is 
torch.Size([])
memory (bytes)
4767309824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
4767309824
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  592018200.0
relative error loss 0.20720255
shape of L is 
torch.Size([])
memory (bytes)
4770611200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
4770611200
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  591672600.0
relative error loss 0.2070816
shape of L is 
torch.Size([])
memory (bytes)
4773822464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 10% |
memory (bytes)
4773822464
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  591099900.0
relative error loss 0.20688117
shape of L is 
torch.Size([])
memory (bytes)
4777029632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
4777033728
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  590572000.0
relative error loss 0.20669642
shape of L is 
torch.Size([])
memory (bytes)
4780154880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4780253184
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  590109440.0
relative error loss 0.20653452
shape of L is 
torch.Size([])
memory (bytes)
4783464448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
4783464448
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  589264400.0
relative error loss 0.20623875
shape of L is 
torch.Size([])
memory (bytes)
4786651136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
4786659328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% | 11% |
error is  588696600.0
relative error loss 0.20604002
shape of L is 
torch.Size([])
memory (bytes)
4789796864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 34% | 11% |
memory (bytes)
4789891072
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  588140800.0
relative error loss 0.2058455
time to take a step is 282.79477405548096
it  5 : 2500274176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
4793106432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 10% |
memory (bytes)
4793106432
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  588140800.0
relative error loss 0.2058455
shape of L is 
torch.Size([])
memory (bytes)
4796264448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
4796309504
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  587645700.0
relative error loss 0.20567222
shape of L is 
torch.Size([])
memory (bytes)
4799541248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 11% |
memory (bytes)
4799541248
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  587096600.0
relative error loss 0.20548004
shape of L is 
torch.Size([])
memory (bytes)
4802686976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
4802748416
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  586528260.0
relative error loss 0.20528112
shape of L is 
torch.Size([])
memory (bytes)
4805881856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 11% |
memory (bytes)
4805963776
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  586144800.0
relative error loss 0.20514691
shape of L is 
torch.Size([])
memory (bytes)
4809146368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
4809175040
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  585768700.0
relative error loss 0.20501529
shape of L is 
torch.Size([])
memory (bytes)
4812349440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 11% |
memory (bytes)
4812394496
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  585447700.0
relative error loss 0.20490293
shape of L is 
torch.Size([])
memory (bytes)
4815609856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 11% |
memory (bytes)
4815609856
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  585200640.0
relative error loss 0.20481646
shape of L is 
torch.Size([])
memory (bytes)
4818812928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
4818812928
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  585040900.0
relative error loss 0.20476055
shape of L is 
torch.Size([])
memory (bytes)
4822036480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 10% |
memory (bytes)
4822036480
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  584887800.0
relative error loss 0.20470698
time to take a step is 283.8645133972168
it  6 : 2500274176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
4825231360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4825243648
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  584887800.0
relative error loss 0.20470698
shape of L is 
torch.Size([])
memory (bytes)
4828278784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 10% |
memory (bytes)
4828446720
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  584613100.0
relative error loss 0.20461084
shape of L is 
torch.Size([])
memory (bytes)
4831641600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
4831670272
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  584682500.0
relative error loss 0.20463511
shape of L is 
torch.Size([])
memory (bytes)
4834844672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 11% |
memory (bytes)
4834873344
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  584475900.0
relative error loss 0.20456281
shape of L is 
torch.Size([])
memory (bytes)
4838076416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
4838100992
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  584306700.0
relative error loss 0.2045036
shape of L is 
torch.Size([])
memory (bytes)
4841279488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
4841308160
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  584103200.0
relative error loss 0.20443235
shape of L is 
torch.Size([])
memory (bytes)
4844347392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% | 11% |
memory (bytes)
4844515328
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  583936260.0
relative error loss 0.20437394
shape of L is 
torch.Size([])
memory (bytes)
4847726592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
4847726592
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  583719700.0
relative error loss 0.20429814
shape of L is 
torch.Size([])
memory (bytes)
4850778112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
4850909184
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  583584000.0
relative error loss 0.20425065
shape of L is 
torch.Size([])
memory (bytes)
4854161408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 11% |
memory (bytes)
4854161408
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  583447550.0
relative error loss 0.20420289
time to take a step is 282.7432961463928
it  7 : 2500274176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
4857376768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
4857376768
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  583447550.0
relative error loss 0.20420289
shape of L is 
torch.Size([])
memory (bytes)
4860588032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
4860588032
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  583299600.0
relative error loss 0.20415111
shape of L is 
torch.Size([])
memory (bytes)
4863803392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
4863803392
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  583090400.0
relative error loss 0.20407791
shape of L is 
torch.Size([])
memory (bytes)
4867014656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
4867014656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% | 11% |
error is  583000300.0
relative error loss 0.20404637
shape of L is 
torch.Size([])
memory (bytes)
4870213632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
4870213632
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  582897150.0
relative error loss 0.20401026
shape of L is 
torch.Size([])
memory (bytes)
4873445376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 34% | 11% |
memory (bytes)
4873445376
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  582690050.0
relative error loss 0.20393777
shape of L is 
torch.Size([])
memory (bytes)
4876623872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 34% | 11% |
memory (bytes)
4876652544
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  582531600.0
relative error loss 0.20388232
shape of L is 
torch.Size([])
memory (bytes)
4879724544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
4879867904
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  582390000.0
relative error loss 0.20383276
shape of L is 
torch.Size([])
memory (bytes)
4883087360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
4883087360
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  582302460.0
relative error loss 0.20380212
shape of L is 
torch.Size([])
memory (bytes)
4886179840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 11% |
memory (bytes)
4886298624
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  581955600.0
relative error loss 0.20368072
time to take a step is 284.87585282325745
it  8 : 2500274176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
4889513984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4889513984
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  581955600.0
relative error loss 0.20368072
shape of L is 
torch.Size([])
memory (bytes)
4892704768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
4892704768
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  581765600.0
relative error loss 0.20361423
shape of L is 
torch.Size([])
memory (bytes)
4895944704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 33% | 11% |
memory (bytes)
4895944704
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  581582100.0
relative error loss 0.20355
shape of L is 
torch.Size([])
memory (bytes)
4899086336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
4899160064
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  581293600.0
relative error loss 0.20344901
shape of L is 
torch.Size([])
memory (bytes)
4902383616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% | 11% |
memory (bytes)
4902383616
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  580987900.0
relative error loss 0.20334204
shape of L is 
torch.Size([])
memory (bytes)
4905558016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% | 10% |
memory (bytes)
4905586688
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  581014800.0
relative error loss 0.20335144
shape of L is 
torch.Size([])
memory (bytes)
4908707840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
4908802048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% | 11% |
error is  580827650.0
relative error loss 0.20328595
shape of L is 
torch.Size([])
memory (bytes)
4912025600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 10% |
memory (bytes)
4912025600
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  580591900.0
relative error loss 0.20320342
shape of L is 
torch.Size([])
memory (bytes)
4915191808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
4915236864
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  580391200.0
relative error loss 0.20313318
shape of L is 
torch.Size([])
memory (bytes)
4918448128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 11% |
memory (bytes)
4918448128
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  580137200.0
relative error loss 0.2030443
time to take a step is 283.71637511253357
it  9 : 2500274176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
4921581568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
4921667584
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  580137200.0
relative error loss 0.2030443
shape of L is 
torch.Size([])
memory (bytes)
4924870656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 33% | 11% |
memory (bytes)
4924870656
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  579954940.0
relative error loss 0.2029805
shape of L is 
torch.Size([])
memory (bytes)
4928081920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
4928086016
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  579792400.0
relative error loss 0.20292361
shape of L is 
torch.Size([])
memory (bytes)
4931293184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 11% |
memory (bytes)
4931293184
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  579542500.0
relative error loss 0.20283617
shape of L is 
torch.Size([])
memory (bytes)
4934524928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 11% |
memory (bytes)
4934524928
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  579312400.0
relative error loss 0.20275562
shape of L is 
torch.Size([])
memory (bytes)
4937564160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 33% | 11% |
memory (bytes)
4937736192
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  579199740.0
relative error loss 0.20271619
shape of L is 
torch.Size([])
memory (bytes)
4940947456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 11% |
memory (bytes)
4940947456
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  579029500.0
relative error loss 0.20265661
shape of L is 
torch.Size([])
memory (bytes)
4944150528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 34% | 11% |
memory (bytes)
4944150528
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  578966800.0
relative error loss 0.20263466
shape of L is 
torch.Size([])
memory (bytes)
4947382272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
4947382272
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  578830600.0
relative error loss 0.202587
shape of L is 
torch.Size([])
memory (bytes)
4950581248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 34% | 11% |
memory (bytes)
4950597632
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  578776600.0
relative error loss 0.20256808
time to take a step is 283.77324318885803
it  10 : 2500274176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
4953817088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 11% |
memory (bytes)
4953817088
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  578776600.0
relative error loss 0.20256808
shape of L is 
torch.Size([])
memory (bytes)
4957024256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 33% | 11% |
memory (bytes)
4957024256
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  578604800.0
relative error loss 0.20250797
shape of L is 
torch.Size([])
memory (bytes)
4960161792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 11% |
memory (bytes)
4960247808
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  578498800.0
relative error loss 0.20247087
shape of L is 
torch.Size([])
memory (bytes)
4963463168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 11% |
memory (bytes)
4963463168
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  578391800.0
relative error loss 0.20243342
shape of L is 
torch.Size([])
memory (bytes)
4966580224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
4966670336
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  578246660.0
relative error loss 0.20238262
shape of L is 
torch.Size([])
memory (bytes)
4969893888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 34% | 11% |
memory (bytes)
4969893888
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  578110200.0
relative error loss 0.20233487
shape of L is 
torch.Size([])
memory (bytes)
4973023232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 33% | 11% |
memory (bytes)
4973117440
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  578015500.0
relative error loss 0.20230171
shape of L is 
torch.Size([])
memory (bytes)
4976324608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 11% |
memory (bytes)
4976324608
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  577900000.0
relative error loss 0.2022613
shape of L is 
torch.Size([])
memory (bytes)
4979531776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 11% |
memory (bytes)
4979531776
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  577758200.0
relative error loss 0.20221166
shape of L is 
torch.Size([])
memory (bytes)
4982681600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
4982681600
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  577766400.0
relative error loss 0.20221452
shape of L is 
torch.Size([])
memory (bytes)
4985962496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
4985962496
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  577670900.0
relative error loss 0.20218112
time to take a step is 313.3190314769745
it  11 : 2500274688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
4989177856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
4989177856
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  577670900.0
relative error loss 0.20218112
shape of L is 
torch.Size([])
memory (bytes)
4992389120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
4992389120
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  577549800.0
relative error loss 0.20213874
shape of L is 
torch.Size([])
memory (bytes)
4995518464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 11% |
memory (bytes)
4995612672
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  577466600.0
relative error loss 0.2021096
shape of L is 
torch.Size([])
memory (bytes)
4998823936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
4998823936
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  577401600.0
relative error loss 0.20208685
shape of L is 
torch.Size([])
memory (bytes)
5001936896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5002027008
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  577333250.0
relative error loss 0.20206293
shape of L is 
torch.Size([])
memory (bytes)
5005250560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5005250560
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  577276400.0
relative error loss 0.20204304
shape of L is 
torch.Size([])
memory (bytes)
5008334848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5008453632
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  577179900.0
relative error loss 0.20200926
shape of L is 
torch.Size([])
memory (bytes)
5011673088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5011673088
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  577192200.0
relative error loss 0.20201357
shape of L is 
torch.Size([])
memory (bytes)
5014888448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5014888448
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  577107700.0
relative error loss 0.20198399
shape of L is 
torch.Size([])
memory (bytes)
5018071040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 11% |
memory (bytes)
5018071040
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  577024260.0
relative error loss 0.20195478
time to take a step is 296.5183506011963
it  12 : 2500274176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5021327360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 11% |
memory (bytes)
5021327360
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  577024260.0
relative error loss 0.20195478
shape of L is 
torch.Size([])
memory (bytes)
5024358400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 33% | 11% |
memory (bytes)
5024526336
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  576938500.0
relative error loss 0.20192477
shape of L is 
torch.Size([])
memory (bytes)
5027741696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5027741696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% | 11% |
error is  576851700.0
relative error loss 0.2018944
shape of L is 
torch.Size([])
memory (bytes)
5030821888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 32% | 11% |
memory (bytes)
5030944768
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  576750100.0
relative error loss 0.20185882
shape of L is 
torch.Size([])
memory (bytes)
5034156032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 32% | 11% |
memory (bytes)
5034156032
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  576652800.0
relative error loss 0.20182478
shape of L is 
torch.Size([])
memory (bytes)
5037363200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5037363200
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  576561400.0
relative error loss 0.20179279
shape of L is 
torch.Size([])
memory (bytes)
5040582656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5040582656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% | 11% |
error is  576461300.0
relative error loss 0.20175776
shape of L is 
torch.Size([])
memory (bytes)
5043798016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% | 11% |
memory (bytes)
5043798016
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  576377100.0
relative error loss 0.20172828
shape of L is 
torch.Size([])
memory (bytes)
5046927360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5047005184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 11% |
error is  576290800.0
relative error loss 0.20169808
shape of L is 
torch.Size([])
memory (bytes)
5050224640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 33% | 11% |
memory (bytes)
5050224640
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  576230900.0
relative error loss 0.20167711
time to take a step is 289.33177733421326
it  13 : 2500274176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5053407232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
5053407232
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  576230900.0
relative error loss 0.20167711
shape of L is 
torch.Size([])
memory (bytes)
5056655360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
5056655360
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  576161800.0
relative error loss 0.20165293
shape of L is 
torch.Size([])
memory (bytes)
5059874816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5059874816
| ID | GPU | MEM |
------------------
|  0 | 23% |  0% |
|  1 | 98% | 11% |
error is  576097800.0
relative error loss 0.20163053
shape of L is 
torch.Size([])
memory (bytes)
5063012352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 11% |
memory (bytes)
5063094272
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  576067600.0
relative error loss 0.20161995
shape of L is 
torch.Size([])
memory (bytes)
5066305536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5066305536
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  576025100.0
relative error loss 0.20160508
shape of L is 
torch.Size([])
memory (bytes)
5069496320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5069524992
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  575986700.0
relative error loss 0.20159164
shape of L is 
torch.Size([])
memory (bytes)
5072711680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5072736256
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  575955460.0
relative error loss 0.20158072
shape of L is 
torch.Size([])
memory (bytes)
5075959808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 11% |
memory (bytes)
5075959808
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  575932700.0
relative error loss 0.20157273
shape of L is 
torch.Size([])
memory (bytes)
5079162880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% | 11% |
memory (bytes)
5079162880
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  575849200.0
relative error loss 0.20154352
shape of L is 
torch.Size([])
memory (bytes)
5082382336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5082382336
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  575819800.0
relative error loss 0.20153323
time to take a step is 286.38955569267273
it  14 : 2500274176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5085470720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 11% |
memory (bytes)
5085605888
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  575819800.0
relative error loss 0.20153323
shape of L is 
torch.Size([])
memory (bytes)
5088817152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5088817152
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  575776000.0
relative error loss 0.20151791
shape of L is 
torch.Size([])
memory (bytes)
5092036608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 11% |
memory (bytes)
5092036608
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  575721500.0
relative error loss 0.20149882
shape of L is 
torch.Size([])
memory (bytes)
5095190528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5095256064
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  575654900.0
relative error loss 0.20147553
shape of L is 
torch.Size([])
memory (bytes)
5098459136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5098459136
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  575590400.0
relative error loss 0.20145294
shape of L is 
torch.Size([])
memory (bytes)
5101649920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 11% |
memory (bytes)
5101678592
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  575526140.0
relative error loss 0.20143045
shape of L is 
torch.Size([])
memory (bytes)
5104902144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5104902144
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  575464200.0
relative error loss 0.20140877
shape of L is 
torch.Size([])
memory (bytes)
5108121600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5108121600
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  575350300.0
relative error loss 0.2013689
shape of L is 
torch.Size([])
memory (bytes)
5111222272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 11% |
memory (bytes)
5111320576
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  575276300.0
relative error loss 0.20134301
shape of L is 
torch.Size([])
memory (bytes)
5114544128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 34% | 11% |
memory (bytes)
5114544128
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  575208200.0
relative error loss 0.20131917
time to take a step is 284.12408447265625
sum tnnu_Z after tensor(7511672., device='cuda:0')
shape of features
(7714,)
shape of features
(7714,)
number of orig particles 30856
number of new particles after remove low mass 30856
tnuZ shape should be parts x labs
torch.Size([30856, 702])
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
error without small mass is  692845250.0
relative error without small mass is  0.2424914
nnu_Z shape should be number of particles by maxV
(30856, 702)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
shape of features
(30856,)
Wed Feb 1 15:07:29 EST 2023
