Tue Jan 31 12:07:05 EST 2023
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  0% |  0% |
numbers of X: 4773981
numbers of Z: 15081
shape of features
(15081,)
shape of features
(15081,)
ZX	Vol	Parts	Cubes	Eps
Z	0.01998268352698272	15081	15.081	0.10983511863828856
X	0.012034448132909353	185	0.185	0.4021778521068764
X	0.011986456926484803	318	0.318	0.3352904906626487
X	0.012417461145637472	67	0.067	0.5701458957404725
X	0.01173340121289	117	0.117	0.46460015800249965
X	0.017037677392135714	3784	3.784	0.1651276325031674
X	0.01261719642607116	2366	2.366	0.1747093071096221
X	0.012445477528469763	509	0.509	0.2902448979196377
X	0.012980013355208452	6552	6.552	0.12559347690284822
X	0.012788146327687561	3167	3.167	0.15924036995325008
X	0.011745687158777603	930	0.93	0.2328798040223186
X	0.012470520410740444	575	0.575	0.2788724278297356
X	0.012191755709651702	1523	1.523	0.20004242758045734
X	0.01244017750614106	365	0.365	0.32422356064847146
X	0.013125268628209076	67035	67.035	0.058067820943583925
X	0.01255600990281699	329	0.329	0.33668098095478294
X	0.013024624212604313	5194	5.194	0.13585879308933202
X	0.01226433723243647	241	0.241	0.3705746272264336
X	0.01234452218478023	1612	1.612	0.19710746946220403
X	0.013090644590177895	58164	58.164	0.06082782045902098
X	0.013130178193989575	59699	59.699	0.060362588416302605
X	0.011548553966085392	611	0.611	0.26637622869364697
X	0.013205809571668482	18346	18.346	0.08962060686797589
X	0.012208813405954685	457	0.457	0.29894118522270463
X	0.01186883783527813	452	0.452	0.2972281377199747
X	0.012585776380414384	246	0.246	0.3712344756745645
X	0.012716777118226203	2068	2.068	0.18320704717986253
X	0.01312813641380858	28245	28.245	0.07746172556256352
X	0.011741310445126302	1058	1.058	0.22305414655428335
X	0.012754445754680571	32012	32.012	0.07358392127830322
X	0.013119987335531985	143898	143.898	0.04500832864513893
X	0.012713409880896474	2485	2.485	0.17231055658991576
X	0.01298950393012566	39572	39.572	0.06898185281235955
X	0.011950764838698402	815	0.815	0.2447631482449548
X	0.012834166411879873	3238	3.238	0.1582572472440058
X	0.012905014518664883	121	0.121	0.4742322038048422
X	0.013786488138272912	100545	100.545	0.051566112411347426
X	0.014636896915015518	157	0.157	0.453436476603649
X	0.012540642321818701	409	0.409	0.31299122203931573
X	0.011238779009536001	441	0.441	0.29427961604602443
X	0.012051827475796239	427	0.427	0.30446801582738686
X	0.01161311606283658	171	0.171	0.40799128407905927
X	0.01090471811540352	247	0.247	0.35343196893394424
X	0.008528566616015159	55	0.055	0.5372434370304109
X	0.01130138491824349	74	0.074	0.534523309998407
X	0.012926247025872601	157	0.157	0.4350350675993038
X	0.011731728955532443	87	0.087	0.5127994956297696
X	0.010949388501320849	103	0.103	0.4737154474513717
X	0.009944389396074721	35	0.035	0.6574105851249038
X	0.012220314794478916	83	0.083	0.5280415538338139
X	0.011798278933988405	106	0.106	0.4810286364337186
X	0.012556502300681869	8360	8.36	0.11452161601038824
X	0.011612128277197322	86	0.086	0.5130241283652776
X	0.012181812831001634	1381	1.381	0.2066201859015752
X	0.010980345120919948	133	0.133	0.4354331104810232
X	0.01302024695498825	425	0.425	0.3129032666883626
X	0.015552191698098204	1518	1.518	0.21719005889084955
X	0.012593176098925863	373	0.373	0.32320307312537017
X	0.011911930349036481	212	0.212	0.383014687434268
X	0.01310843062997055	446	0.446	0.30860665018464273
X	0.006767539982862499	19	0.019	0.7088577247698726
X	0.01983848243302519	1369	1.369	0.24380072889618581
X	0.012657046728780423	204	0.204	0.39588350378469794
X	0.012194912004431643	363	0.363	0.322669078371234
X	0.01252953759947526	616	0.616	0.2729720055230663
X	0.012188857193278772	486	0.486	0.29271315438235695
X	0.00838796726392456	39	0.039	0.5991432992908776
X	0.0116790480521022	607	0.607	0.26796181951324133
X	0.011920759979405168	154	0.154	0.42618131747866556
X	0.01127470687944531	75	0.075	0.5317179628205057
X	0.011229753860190731	75	0.075	0.5310103563948579
X	0.011434105175260402	143	0.143	0.4308129158799325
X	0.01088544854413986	151	0.151	0.4161882299800055
X	0.0110182389221433	119	0.119	0.4523991094718607
X	0.010135502196973562	50	0.05	0.5874331116262556
X	0.013097722433975573	1519	1.519	0.2050592366118891
X	0.012195964580766123	650	0.65	0.2657261320574761
X	0.010838196645917569	39	0.039	0.6525755626396221
X	0.011633781391424668	179	0.179	0.40205853815096376
X	0.0121440439109584	140	0.14	0.4426683084813191
X	0.012287505164581169	251	0.251	0.3658165340129161
X	0.012188090790552084	419	0.419	0.30754397211315465
X	0.011298603581106003	292	0.292	0.33823143900829933
X	0.011519694311989051	346	0.346	0.32170288230034033
X	0.009010044062760002	70	0.07	0.5049049459134024
X	0.011950767589963999	465	0.465	0.29510797718184706
X	0.013706247762537963	292	0.292	0.3607269381888438
X	0.011013285922193437	114	0.114	0.4588499488912506
X	0.012414061459595109	138	0.138	0.44806903813962584
X	0.011236952733729152	133	0.133	0.4387990086248773
X	0.011412146572695271	257	0.257	0.3541153389646174
X	0.012229941290668801	529	0.529	0.28487624316323773
X	0.012013232738592	105	0.105	0.48546381445080405
X	0.013040809257246311	368	0.368	0.3284623340698959
X	0.012376167516125297	1708	1.708	0.19350822869346768
X	0.010954189259988478	87	0.087	0.5012106945767031
X	0.005648972980691139	18	0.018	0.6795670775671995
X	0.015439909594691664	584	0.584	0.29790466358889967
X	0.010633780469513044	101	0.101	0.47219578434533116
X	0.011954506914355003	64	0.064	0.5716329194566128
X	0.011715111034515842	112	0.112	0.4711683707457481
X	0.012287737077928032	2426	2.426	0.17173561560434053
X	0.011578148458829539	251	0.251	0.3586370312311524
X	0.012145617688166131	167	0.167	0.41741467342943317
X	0.0105739608856077	63	0.063	0.5516103208071685
X	0.012939116837602826	1327	1.327	0.21363770497547074
X	0.01153811875236172	619	0.619	0.265143785096233
X	0.0117432008948454	469	0.469	0.29255299549909164
X	0.011552949168806642	86	0.086	0.5121511321447051
X	0.011620407089544099	710	0.71	0.25389510698053225
X	0.013839599647961104	189	0.189	0.41836361559838525
X	0.014710340741158623	234	0.234	0.39762067514105537
X	0.011769504357427199	120	0.12	0.4611677999396978
X	0.00809089756104354	28	0.028	0.6611188759186166
X	0.011030685605921686	77	0.077	0.5232436025642374
X	0.012602921017244067	2595	2.595	0.16934717286140666
X	0.014541502352509051	294	0.294	0.36707430271850583
X	0.01337163722156359	182	0.182	0.41882995244141924
X	0.011231895516364802	192	0.192	0.3881956001190509
X	0.012247790092735815	327	0.327	0.3345825812348062
X	0.012143197923098177	758	0.758	0.252089422107503
X	0.012936175303493438	586	0.586	0.28052361406959286
X	0.012377658038877673	263	0.263	0.36104464717110596
X	0.0101495103997164	63	0.063	0.5441285511129428
X	0.01289980783714356	137	0.137	0.45494010610086943
X	0.012221795482171945	1108	1.108	0.2226033780449239
X	0.013669234172976266	527	0.527	0.2960132761088399
X	0.012045900125139047	55	0.055	0.6027800305556994
X	0.010223629380571352	54	0.054	0.5742095957957769
X	0.01270279643236852	2561	2.561	0.1705414377467256
X	0.010267979083676998	86	0.086	0.4924122418282572
X	0.012252063973139627	224	0.224	0.37959493810388556
X	0.012956944960048244	178	0.178	0.4175362668116425
X	0.011598747388542899	181	0.181	0.40016967236131357
X	0.01135678308488009	203	0.203	0.3824603613027995
X	0.0115165378888686	67	0.067	0.5560097142245811
X	0.012740990943065347	458	0.458	0.303002289219453
X	0.011941857405734783	587	0.587	0.2729886515393189
X	0.012596387680539303	235	0.235	0.37704459435949716
X	0.012287536507731943	2914	2.914	0.16155674377497858
X	0.009921216648102901	110	0.11	0.4484603822184853
X	0.011991076095053519	525	0.525	0.2837263156533065
X	0.008877311640405299	48	0.048	0.569744414508255
X	0.01743982429913173	2696	2.696	0.18632626244620612
X	0.01214429636507626	697	0.697	0.2592466737964207
X	0.011312522596992959	378	0.378	0.31047208148380984
X	0.012211250052272183	239	0.239	0.3710684879071158
X	0.014406013082543462	155	0.155	0.4529713166727682
X	0.01193616698014714	61	0.061	0.5805572018721501
X	0.011371298364170324	30	0.03	0.7237072667107024
X	0.011553176983235477	271	0.271	0.3493365646881845
X	0.011287360782306983	75	0.075	0.5319168088443884
X	0.012108666122485911	594	0.594	0.27317227517774484
X	0.01185020625044592	1304	1.304	0.20868093436872778
X	0.01147867080412648	237	0.237	0.3645141722834265
X	0.010656086503219081	67	0.067	0.5418024670308972
X	0.01387222448043368	1973	1.973	0.1915753705402858
X	0.012350923118990819	466	0.466	0.2981520734288742
X	0.012647804184718668	606	0.606	0.2753261760619482
X	0.011952203404774984	186	0.186	0.40053918555865725
X	0.010366089299047759	59	0.059	0.5600854767455938
X	0.010385902636893601	54	0.054	0.577231685417972
X	0.01114282344156075	80	0.08	0.518364304259055
X	0.0117852869747481	299	0.299	0.3403217125104231
X	0.008678849520020702	23	0.023	0.7226224349075233
X	0.012162049638925686	298	0.298	0.3442945667104883
X	0.011317758140423998	59	0.059	0.5767259136751515
X	0.012667224719684718	189	0.189	0.40619997310318806
X	0.011895417231071643	402	0.402	0.3093032918527301
X	0.011512957114171798	287	0.287	0.34232214653046344
X	0.010885772939650689	91	0.091	0.4927265446909514
X	0.011918020870706524	149	0.149	0.4308630893441141
X	0.011762727711346213	520	0.52	0.2828144313809125
X	0.012585510051111061	200	0.2	0.39775313229435577
X	0.011201676981898	124	0.124	0.4486976413420677
X	0.012898806716485332	4372	4.372	0.14342442824584584
X	0.01364242857576141	112	0.112	0.49570643343706006
X	0.011857282110645982	216	0.216	0.3800526810303465
X	0.01167190608959697	268	0.268	0.35183226446269034
X	0.011355627252113457	579	0.579	0.2696771738399955
X	0.012217729753935685	191	0.191	0.3999315955664469
X	0.01655081664006524	460	0.46	0.3301316773989094
X	0.011525725107523437	305	0.305	0.33557490247918703
X	0.01127140783336612	221	0.221	0.37084744977183887
X	0.01099993632455718	88	0.088	0.49999903521870176
X	0.011568229611619199	1095	1.095	0.21942406695870312
X	0.012814649184195441	744	0.744	0.25825253621263766
X	0.01161394917381348	140	0.14	0.4361313393367209
X	0.01076732038985174	124	0.124	0.4428214506192763
X	0.0118577157302916	541	0.541	0.27985597899054715
X	0.0124861162655631	2273	2.273	0.17644498248034435
X	0.0061416603277425	19	0.019	0.6862948356862913
X	0.011859383886568861	107	0.107	0.480351830342183
X	0.012115242543515137	655	0.655	0.2644621987378242
X	0.014469724734511203	466	0.466	0.3143099413287151
X	0.012532746828252	163	0.163	0.4252257483050623
X	0.0113776184301164	143	0.143	0.43010231069308064
X	0.011511124864344987	81	0.081	0.5218482707212525
X	0.012776937635631456	108	0.108	0.49090901386429653
X	0.01153269722978432	436	0.436	0.2979532546652716
X	0.011051436941387044	70	0.07	0.540472460985803
X	0.012732531960251819	232	0.232	0.3800221669362219
X	0.011612016322533352	61	0.061	0.5752534971181531
X	0.012870896205552892	398	0.398	0.31859716379919273
X	0.0120790570831091	110	0.11	0.47886538995593775
X	0.010584583984599178	123	0.123	0.441492056442161
X	0.01138459078590808	210	0.21	0.3784710891545261
X	0.011319002169187497	268	0.268	0.34824999300567056
X	0.01071858721255728	258	0.258	0.3463426269454105
X	0.011855838903944689	544	0.544	0.279325850595055
X	0.011660478986455001	86	0.086	0.5137351875673568
X	0.011773596614012977	369	0.369	0.31717157728811707
X	0.012204813031014202	863	0.863	0.2418281081432018
X	0.01163870283238498	68	0.068	0.5552201540830621
X	0.011689616249139003	470	0.47	0.29190002494099376
X	0.01080965277416602	284	0.284	0.33638075004121193
X	0.010921662261622238	54	0.054	0.5869912951265858
X	0.009726870000428097	48	0.048	0.5873684630912259
X	0.01154729365704733	257	0.257	0.35550771424715516
X	0.012103002030368001	6597	6.597	0.12241901309346491
X	0.01115399127658656	68	0.068	0.5474029446238352
X	0.010357149463447287	34	0.034	0.6728533858845447
X	0.01015361684873586	48	0.048	0.5958356869052989
X	0.011794511542959154	343	0.343	0.32518358394433167
X	0.011410301430110601	186	0.186	0.39439196439704993
X	0.011218397684191042	65	0.065	0.5567657246309128
X	0.01104514164056456	119	0.119	0.452767010784113
X	0.010883751285625118	37	0.037	0.6650571418303592
X	0.00910829456598075	66	0.066	0.5167704961529934
X	0.011506309405056349	285	0.285	0.3430549809053705
X	0.012502124788702502	277	0.277	0.3560420996269839
X	0.00719974620773331	26	0.026	0.6518003859068575
X	0.01298460989991039	286	0.286	0.3567420397305249
X	0.011338570386623193	39	0.039	0.6624674807465365
X	0.01239889324849461	147	0.147	0.4385527736108555
X	0.010927978338007195	394	0.394	0.3027014731582302
X	0.013230667018052646	268	0.268	0.36684475590424453
X	0.01668909459416121	863	0.863	0.2684153366683372
X	0.013116454749859023	651	0.651	0.2721104113922868
X	0.012157902956953598	189	0.189	0.40068119110644784
X	0.012267034278163945	866	0.866	0.24195832104975265
X	0.012804355723381101	1037	1.037	0.2311317769558427
X	0.013302845091131993	722	0.722	0.26412070353313283
X	0.0115319830094258	303	0.303	0.33637247839998563
X	0.010983152458680048	77	0.077	0.5224909377721728
X	0.01275205898009569	523	0.523	0.2899743217472552
X	0.012811753146028478	1619	1.619	0.1992754986188749
X	0.012486303893110805	1070	1.07	0.2268210592806414
X	0.012332936756610253	590	0.59	0.2754682167425648
X	0.012276956440657016	2005	2.005	0.1829470647711821
X	0.01245188853569838	6134	6.134	0.12661849609868894
X	0.016116229723314852	337	0.337	0.3629759490689908
X	0.012375375444443093	875	0.875	0.24183367565348207
X	0.01099494923460584	433	0.433	0.29392404156544016
X	0.011082693061608282	202	0.202	0.3799834183361828
X	0.01291637328286284	15889	15.889	0.09332858346709764
X	0.01307683337460864	18981	18.981	0.08832042839783218
X	0.012101255993279781	168	0.168	0.41607701220431514
X	0.01291787812554484	7738	7.738	0.1186280757486225
X	0.012620243560670166	18076	18.076	0.08871324552424761
X	0.01298470974977358	1881	1.881	0.1904062282576293
X	0.011904447763662162	3237	3.237	0.15435552696671476
X	0.014400264331194757	3616	3.616	0.158506598927119
X	0.013076577296134657	2592	2.592	0.17150878198724806
X	0.012938944427764632	2279	2.279	0.17839588844159263
X	0.01001181090857109	90	0.09	0.4809390520565285
X	0.011891683761787388	649	0.649	0.2636328663455907
X	0.01089708711615392	183	0.183	0.3904988666061901
X	0.011428113498569282	269	0.269	0.3489319640427065
X	0.012079376028178044	697	0.697	0.25878389245989203
X	0.011859511229109092	1264	1.264	0.2109145653128183
X	0.012319499902970881	7525	7.525	0.11785884788838784
X	0.015704690670870377	560	0.56	0.3038182001205234
X	0.011628298707135378	270	0.27	0.35052376815230507
X	0.010512973285692243	319	0.319	0.3206109069068987
X	0.012912227438208934	1420	1.42	0.20872330404437436
X	0.012823324509966335	1075	1.075	0.22848834720893155
X	0.013110441164235178	14082	14.082	0.09764522008699829
X	0.012373412370230801	17099	17.099	0.08977861986894185
X	0.011535997314731311	674	0.674	0.25771028422439574
X	0.00795286044181881	15	0.015	0.8093642810147366
X	0.011878828117371504	72	0.072	0.5484626199995021
X	0.011951719267355167	5845	5.845	0.12692542394349465
X	0.01224981248052467	4467	4.467	0.13997092394305657
X	0.012057191781524673	1029	1.029	0.22713127109951478
X	0.010599048035235458	67	0.067	0.5408340412690227
X	0.013885983204376411	10315	10.315	0.11041696384756096
X	0.016197113720009943	510	0.51	0.3166813481362147
X	0.012474503961720201	463	0.463	0.2997878987287119
X	0.013123835711362893	38204	38.204	0.07003535207514065
X	0.011776607417856709	602	0.602	0.2694477546066466
X	0.012575032552447086	1516	1.516	0.20242771471786777
X	0.0159260790519524	19983	19.983	0.09271486982990562
X	0.015915372229485216	55606	55.606	0.06590196765819342
X	0.01201637414733899	1293	1.293	0.21024465926795055
X	0.011789789980390606	579	0.579	0.2730711598831576
X	0.01286273185479977	160	0.16	0.4315897062036192
X	0.01117399235673342	172	0.172	0.40200054421325415
X	0.01305045313649536	6362	6.362	0.12706052405543938
X	0.011227225997538741	122	0.122	0.4514789898973014
X	0.012402870242257301	167	0.167	0.42034115155205537
X	0.013202450018162445	2495	2.495	0.1742587765920591
X	0.013243149987298948	2790	2.79	0.16805922823053684
X	0.014047814541559268	136	0.136	0.4691985041786676
X	0.019763555650034955	527	0.527	0.3347224347656012
X	0.013203443853125727	72620	72.62	0.05665135816385905
X	0.012607271979948115	769	0.769	0.2540377518726054
X	0.012408229147426692	444	0.444	0.30346523469405134
X	0.012581540456855502	972	0.972	0.23479518392961854
X	0.012482853699460356	23593	23.593	0.08088045224396642
X	0.012612447230751006	343	0.343	0.3325332529127435
X	0.012646886283897261	304	0.304	0.3465000832709029
X	0.011838681742002258	300	0.3	0.3404557684523835
X	0.011878334336251113	265	0.265	0.3552247552626137
X	0.012975872219481132	1406	1.406	0.2097572979173045
X	0.01683067324493159	4949	4.949	0.15038159204225027
X	0.012878019617345473	7641	7.641	0.11900529745849968
X	0.012497370279151921	203	0.203	0.39485793190304014
X	0.012995323909259188	15469	15.469	0.09435718077158105
X	0.01293760262252749	5075	5.075	0.13660688457655212
X	0.012163206508604599	252	0.252	0.3640959486587492
X	0.012883491958526175	8264	8.264	0.11595276094792478
X	0.014565356222519856	9648	9.648	0.11471704159945253
X	0.014601061586016997	1492	1.492	0.2138972510718035
X	0.012948923123843452	2677	2.677	0.16912004821545187
X	0.011815721469497399	195	0.195	0.3927732278706911
X	0.012872917899885122	1165	1.165	0.22273255110574347
X	0.01281736429862957	536	0.536	0.288100920675818
X	0.011924961155979298	692	0.692	0.25829557109118534
X	0.012068753498343174	421	0.421	0.30605074456295045
X	0.011670392499152043	133	0.133	0.4443698728249046
X	0.012936441074235304	950	0.95	0.23879784493029024
X	0.0121348077843154	2218	2.218	0.17620725534922113
X	0.01216913029407243	3533	3.533	0.15102144958789182
X	0.012174951184064428	294	0.294	0.3459712144441336
X	0.012355773115297815	628	0.628	0.26996249115111354
X	0.01400968525321344	1725	1.725	0.20100788454941604
X	0.011474266946045721	659	0.659	0.25918697813794894
X	0.012344340277982003	882	0.882	0.24099041332427948
X	0.01235273054592336	398	0.398	0.3142630337599655
X	0.01529041475898698	69442	69.442	0.0603854557622318
X	0.012607380792346575	1482	1.482	0.20413879910178084
X	0.012601805204343001	212	0.212	0.3902704181499236
X	0.012284355159455432	10112	10.112	0.10670180538494012
X	0.011934528996037438	165	0.165	0.4166522965087975
X	0.01588283878605209	28594	28.594	0.08220238248858622
X	0.012578210499739015	2399	2.399	0.17372521993808035
X	0.012416064720139097	1068	1.068	0.22653617937288859
X	0.01682916452551077	337	0.337	0.3682512404593794
X	0.012102417052130967	314	0.314	0.3377905447664022
X	0.01264266461447828	522	0.522	0.2893272714484363
X	0.012985691048896086	1160	1.16	0.22370156665639648
X	0.01120751870740896	240	0.24	0.36010797844869347
X	0.01217969692986576	1293	1.293	0.2111929025352252
X	0.011048656697188882	57	0.057	0.5787329478877713
X	0.01216201606110543	95	0.095	0.5039962701494027
X	0.01305762844304489	15282	15.282	0.09489165846356037
X	0.012514053808437014	3846	3.846	0.1481822708886541
X	0.012994944857915953	8292	8.292	0.11615512660613776
X	0.012870511723137478	3110	3.11	0.16055055400076343
X	0.015675427174534635	41337	41.337	0.07238137183087949
X	0.01806865049821441	1785	1.785	0.2163195806346913
X	0.01252096551336357	1114	1.114	0.22400147401993
X	0.01450462179494694	676	0.676	0.2778774070718396
X	0.011588697939255961	62	0.062	0.571760722465554
X	0.01034510629338799	60	0.06	0.5565803919674611
X	0.01299602235470473	1660	1.66	0.19856412540181098
X	0.010849106582264149	640	0.64	0.25688455924915865
X	0.011912488749111255	278	0.278	0.3499338077644761
X	0.01176538349769426	413	0.413	0.3054114714231187
X	0.012007200258984002	2185	2.185	0.17646697357017646
X	0.012923499297433985	131512	131.512	0.04614644471092612
X	0.011886970755052368	195	0.195	0.39356112586383735
X	0.012266636810924112	1178	1.178	0.21837021691079983
X	0.014048543795083242	366	0.366	0.33732620060081264
X	0.010669050968976402	75	0.075	0.5220212418748373
X	0.011939549344804097	713	0.713	0.25583863835586634
X	0.013479673603622604	11502	11.502	0.10543108381599134
X	0.013218100269875225	139987	139.987	0.0455365836836545
X	0.011876125302124303	821	0.821	0.24365605293736625
X	0.013052499464088383	5367	5.367	0.13447886332214803
X	0.011286031771721407	82	0.082	0.5163083380908157
X	0.015631440603171124	818	0.818	0.26735105384715085
X	0.012999320938948535	110488	110.488	0.04900065115697183
X	0.013278613326802639	12247	12.247	0.10273245553914673
X	0.011786366506597814	946	0.946	0.23182642345187227
X	0.01222791975714912	6941	6.941	0.12077494412428251
X	0.012931656858955202	36961	36.961	0.07046446405936226
X	0.012255246450953454	2992	2.992	0.16000006288883104
X	0.01273216818510281	446	0.446	0.30562520611447547
X	0.0107961015652578	126	0.126	0.4408580805874291
X	0.012841639981560009	2273	2.273	0.17810400472642596
X	0.015783899469991175	266	0.266	0.39004156456480077
X	0.012873132115970314	11527	11.527	0.10375028803345489
X	0.01202064950577651	3928	3.928	0.14518402033045252
X	0.01456841367482149	183	0.183	0.43018347658717154
X	0.013046741095774975	4344	4.344	0.1442793163715888
X	0.0114880920016528	450	0.45	0.2944501616319455
X	0.010010286706805998	94	0.094	0.4739940473157559
X	0.013145084131554815	3304	3.304	0.15845566745256834
X	0.012760851455180638	6940	6.94	0.12251052310575539
X	0.01807056713630142	16570	16.57	0.1029318451925402
X	0.012968425266041058	2862	2.862	0.16547759666541254
X	0.012844482437643254	35987	35.987	0.07093441451413443
X	0.012369383437011185	4171	4.171	0.14367105719638465
X	0.011830029924215385	3075	3.075	0.15669203023834885
X	0.012841129920276206	3557	3.557	0.1534050977001601
X	0.018768236717250038	3235	3.235	0.17968689155996947
X	0.012639195956080622	433	0.433	0.30790058704048556
X	0.013073732390579976	10309	10.309	0.1082416366884272
X	0.015844390264751644	1263	1.263	0.2323578736357629
X	0.014569443824964607	5067	5.067	0.14219926654491905
X	0.013446335368770154	8381	8.381	0.11706741609878303
X	0.0116283744288342	341	0.341	0.3242810292776721
X	0.011711982727712199	137	0.137	0.4405243834446403
X	0.010352035455181421	71	0.071	0.526327183314224
X	0.012231396418153021	14827	14.827	0.09378662725598925
X	0.0155411871044872	43380	43.38	0.07102286932023864
X	0.013088920451329355	19585	19.585	0.08742992062928466
X	0.012378342909073501	5297	5.297	0.13270201340288626
X	0.012969932124819852	44893	44.893	0.06610784878052485
X	0.011850843137613769	2180	2.18	0.17583186612870957
X	0.014513614089062783	90	0.09	0.544307544399138
X	0.013082727260045725	36610	36.61	0.07096314471512308
X	0.014932853728544793	269	0.269	0.3814727414020806
X	0.016083337925516132	713	0.713	0.28255015662181465
X	0.013057221855782102	54103	54.103	0.0622600997272003
X	0.014453505892263362	1661	1.661	0.20568432247181462
X	0.011284526932868879	236	0.236	0.36295862259973416
X	0.012345496431674392	413	0.413	0.3103507759351805
X	0.014081714705961838	38589	38.589	0.07146016352026172
X	0.01186940605881334	3194	3.194	0.15489284564941683
X	0.012917686950366294	5531	5.531	0.13267640612042358
X	0.011537711265110379	95	0.095	0.49522058293125876
X	0.011499371105573761	432	0.432	0.2985818795897048
X	0.01310461770350971	342	0.342	0.33713156501704833
X	0.012834510097624029	9054	9.054	0.1123344427517355
X	0.012700548523950549	794	0.794	0.25196138786939487
X	0.01271714639691968	3208	3.208	0.15826508077070736
X	0.012043691029812663	726	0.726	0.25503962527455537
X	0.013065432020627871	9112	9.112	0.11276388271420083
X	0.016524609687057178	170	0.17	0.45979131427995457
X	0.018080911183807082	10852	10.852	0.11855053255395363
X	0.011645670241200801	1674	1.674	0.19089871788921173
X	0.01272992040645675	2231	2.231	0.1786934797210353
X	0.011974764933991862	418	0.418	0.30598272290559375
X	0.012491986868241508	2544	2.544	0.1699696854439192
X	0.01684304930149913	208	0.208	0.43263248717564734
X	0.013070211985052819	2281	2.281	0.17894481349208383
X	0.012963449259421191	1928	1.928	0.18874314217408264
X	0.01364158660155819	1757	1.757	0.19801482561287634
X	0.014494513841826614	165	0.165	0.44453562369888044
X	0.014661840844644818	299	0.299	0.3660208041701824
X	0.01456455870804414	3252	3.252	0.16483480974871972
X	0.011543150613059218	838	0.838	0.2397140270703858
X	0.01319838872247233	211497	211.497	0.039664827317159465
X	0.012232163777758051	99	0.099	0.49806883060752377
X	0.011992175098490938	1625	1.625	0.19469204128206516
X	0.013099056438780657	17951	17.951	0.09002928246235448
X	0.011514673210638645	498	0.498	0.28488795742046846
X	0.012902290013578073	7784	7.784	0.11834629259621422
X	0.013821116114462722	371	0.371	0.333982005823065
X	0.013066246840455997	12532	12.532	0.10140129250192738
X	0.011884339709085237	1418	1.418	0.20312635506563148
X	0.012007959169715409	814	0.814	0.24525334111876773
X	0.012497407325370035	128	0.128	0.4604720968118824
X	0.012073210995087198	100	0.1	0.4942434587452938
X	0.01210670393695834	548	0.548	0.28059620168818655
X	0.012767925637578932	326	0.326	0.3395999217600362
X	0.012422153316994288	2468	2.468	0.17137622674723435
X	0.012757837822580342	5293	5.293	0.13407826898494307
X	0.012171319525119996	1128	1.128	0.22097485353659868
X	0.012888985497112348	245	0.245	0.3747004519147549
X	0.015214325906132848	323	0.323	0.36114648940329386
X	0.01271821448116926	9011	9.011	0.11217197977391691
X	0.014152837643168763	1530	1.53	0.20991849204018304
X	0.013034520997139204	14996	14.996	0.09543475173942972
X	0.01654360412560722	608	0.608	0.30077531016769193
X	0.01954388606314171	106	0.106	0.5691608495627799
X	0.010855398298295666	149	0.149	0.41765706428422805
X	0.01416117356369746	510	0.51	0.30281428342919015
X	0.01679279433382396	2567	2.567	0.187024728720464
X	0.01245704488240111	4315	4.315	0.1423895365181717
X	0.012792857071825566	414	0.414	0.3138020204729984
X	0.012466222911044744	7346	7.346	0.11927826722985724
X	0.01227269144291688	1194	1.194	0.21742617690558716
X	0.014642255755390144	227	0.227	0.40104586577630835
X	0.01516967219056859	462	0.462	0.3202172622599108
X	0.012847423479900873	1217	1.217	0.2193689486339398
X	0.016418320694241522	1062	1.062	0.2491158760776501
X	0.0113603041079538	1575	1.575	0.1932130545369274
X	0.012464770273294963	307	0.307	0.3437019275732881
X	0.012960519743414556	4456	4.456	0.14274439971851483
X	0.012946093007193243	14973	14.973	0.09526717769955308
X	0.01528752163343182	13020	13.02	0.10549746727620703
X	0.013050307447512392	25371	25.371	0.08012371041081859
X	0.01298317809068532	1150	1.15	0.22433363387773855
X	0.01304893709068948	6852	6.852	0.12395173049955766
X	0.012896013492721875	1492	1.492	0.20522434831009373
X	0.012910666089800443	623	0.623	0.2746756752250373
X	0.01105894841979319	87	0.087	0.5028033855536364
X	0.014400684410429122	563	0.563	0.2946399141960149
X	0.01393771383778519	371	0.371	0.33491855695052886
X	0.011952667769737604	778	0.778	0.24859654227398487
X	0.010216758105825358	49	0.049	0.5929785862702837
X	0.012298604348543763	1168	1.168	0.21918171923967594
X	0.012842525410402178	635	0.635	0.2724534026440091
X	0.012787040647769365	37450	37.45	0.06989392150263846
X	0.0086670281825283	49	0.049	0.5613383446188706
X	0.012108838291041464	91	0.091	0.5105289361055001
X	0.01352983140932829	2172	2.172	0.1839970477230036
X	0.014201973564529919	510	0.51	0.3031048190660553
X	0.012018239336934973	1339	1.339	0.20781972086796305
X	0.010745291505097709	150	0.15	0.4153131167707421
X	0.013012797835798829	4163	4.163	0.14621370786343282
X	0.012839676696455839	9587	9.587	0.11022762590963965
X	0.01319193268514373	180	0.18	0.41848379024779225
X	0.00916286108358264	70	0.07	0.507743478636488
X	0.010257365489375279	24	0.024	0.7532541439969015
X	0.01285391521160362	4175	4.175	0.14547654682911454
X	0.01397548086205673	162552	162.552	0.04413584856458732
X	0.011101404632020022	255	0.255	0.35178656602477787
X	0.011875741036858805	169	0.169	0.4126590158111586
X	0.01233027182457898	4366	4.366	0.141350160674522
X	0.012997654689482819	10367	10.367	0.10782942733889117
X	0.012000301868985363	363	0.363	0.3209434489027075
X	0.01248451286894372	279	0.279	0.3550224358053505
X	0.011236225442952639	166	0.166	0.40754051076405295
X	0.016589150763015148	216181	216.181	0.0424947759890588
X	0.011877324324626952	3438	3.438	0.15117184530765101
X	0.012329363246166845	1074	1.074	0.2255859888854932
X	0.012649847112572285	1115	1.115	0.22470020557184514
X	0.012459328303687412	1570	1.57	0.1994642151248642
X	0.014166859490502313	205	0.205	0.4103677676977145
X	0.013641395273617551	18229	18.229	0.09078873873088317
X	0.012765630476347191	221	0.221	0.38655977966104477
X	0.012664361123811634	2135	2.135	0.1810207954058408
X	0.013024660134980694	15353	15.353	0.09466535108757451
X	0.013246228550233625	139452	139.452	0.045627061390523777
X	0.011792952271008062	433	0.433	0.30086953500659264
X	0.011747834319590776	272	0.272	0.3508565885490626
X	0.012996807793815074	11778	11.778	0.10333680952495915
X	0.0120605801496201	1176	1.176	0.21726359386413774
X	0.013012941173865363	9219	9.219	0.1121752941897692
X	0.00992461490766642	71	0.071	0.5189813744609755
X	0.011624770998827472	104	0.104	0.48170781149523856
X	0.012233456386116982	885	0.885	0.23999487581990214
X	0.01163833098767991	176	0.176	0.4043828034650787
X	0.01214898493760532	719	0.719	0.25660810396201844
X	0.010434178728422472	168	0.168	0.3960192800802387
X	0.012982937203870297	1403	1.403	0.20994478686788032
X	0.012017764325384461	794	0.794	0.24736280302399527
X	0.012101931020134803	441	0.441	0.3016282593748152
X	0.013122632434081613	196	0.196	0.40605821173533135
X	0.013090239305101139	26767	26.767	0.07878604426244291
X	0.013155520081925725	50427	50.427	0.06389717092270146
X	0.012973330265022086	2828	2.828	0.16615905861581778
X	0.011945903047470481	269	0.269	0.35412416913763745
X	0.013004774927549999	3057	3.057	0.16203262476568328
X	0.011471440788512643	493	0.493	0.28548963772891944
X	0.01530213816215369	132	0.132	0.4875951710502417
X	0.01317176035081102	16593	16.593	0.09259189426935467
X	0.014187734040268922	1455	1.455	0.21364038328916732
X	0.01291318222813017	5537	5.537	0.1326130464761297
X	0.012291449098080522	1240	1.24	0.2148129539606745
X	0.012925432761228841	12103	12.103	0.10221564022308129
X	0.012600988157980478	696	0.696	0.2625821024175914
X	0.013020213786172142	12028	12.028	0.1026774084341258
X	0.013056185166375642	25415	25.415	0.08008946555196125
X	0.014813852557878676	1992	1.992	0.19519089761433223
X	0.012719140577169072	2942	2.942	0.162906482457681
X	0.017361541679874536	798	0.798	0.2791655338233616
X	0.013983769126719002	6527	6.527	0.1289150270683396
X	0.012916555211622783	1515	1.515	0.20428886267447663
X	0.012184805599301806	447	0.447	0.30095647663429537
X	0.011554631618553	675	0.675	0.25772157517092287
X	0.013003745024427811	707	0.707	0.26396702462172444
X	0.013125268255729974	10043	10.043	0.10933223895608128
X	0.014285677369134931	67442	67.442	0.059610596218093996
X	0.01297320754773204	9251	9.251	0.11193164722512082
X	0.015918443464119705	1363	1.363	0.22688280414072737
X	0.01573282067921297	846	0.846	0.26493870541942655
X	0.012118338884384553	115	0.115	0.47233313983726366
X	0.012034146613823196	2219	2.219	0.17569227455052916
X	0.012369188920878403	431	0.431	0.3061643719455612
X	0.013085621635928163	57059	57.059	0.061210143956525825
X	0.012815142365236641	1953	1.953	0.18721489440914235
X	0.018323587467095153	304	0.304	0.39208386805181367
X	0.012975302547833746	127848	127.848	0.04664529580758952
X	0.011250193322156179	319	0.319	0.327936507718946
X	0.012233264737399127	1193	1.193	0.2172537643589635
X	0.012609226738829844	102	0.102	0.4981530123254255
X	0.010599448366212521	94	0.094	0.48311642549175726
X	0.018829705290079515	554	0.554	0.3239250884441291
X	0.010815453341035711	197	0.197	0.3800664204784943
X	0.01325172560406314	184065	184.065	0.04160067778444011
X	0.013235942625197115	5885	5.885	0.13101955988870112
X	0.012601292366029419	1408	1.408	0.20762077652539068
X	0.01318022896344425	3237	3.237	0.1596835154626082
X	0.01108778904202563	254	0.254	0.35210355715752045
X	0.012848391553507145	466	0.466	0.30210247423091185
X	0.012121156877249966	1874	1.874	0.18631943640918716
X	0.01314270147307839	1578	1.578	0.20270255124221553
X	0.012206951997324803	247	0.247	0.36697523829887224
X	0.01206600858003335	2216	2.216	0.17592651159112557
X	0.011122296363276222	484	0.484	0.28430404175603396
X	0.012937905675200907	648	0.648	0.27128742179904497
X	0.012338960242998494	587	0.587	0.2759816167874808
X	0.011597406713744434	542	0.542	0.2776219926732994
X	0.01352501170639078	3579	3.579	0.1557609726337887
X	0.012266362986053481	4107	4.107	0.14401145440228458
X	0.012332948469792753	1202	1.202	0.2172972097701215
X	0.01320496364658936	131711	131.711	0.046455633588677145
X	0.01556120831145968	995	0.995	0.2500768048717659
X	0.013072931952295681	21111	21.111	0.08523567372452884
X	0.01158783787273112	404	0.404	0.30610729648841256
X	0.012914764367537492	5278	5.278	0.13475318337293968
X	0.012886350681584448	56794	56.794	0.06099241499147302
X	0.012967503882435064	5524	5.524	0.1329028330993823
X	0.011917031144859053	271	0.271	0.35296604431033746
X	0.012896803205429058	4527	4.527	0.14176113862393053
X	0.012628335551523954	1333	1.333	0.21159496110099402
X	0.012394246805848348	3985	3.985	0.14597011093574425
X	0.012656802237532322	22267	22.267	0.08283631324635198
X	0.011722566027459493	457	0.457	0.2949185992507436
X	0.013323006131682844	609	0.609	0.27968015075183644
X	0.012171979733345802	1202	1.202	0.21634768426324857
X	0.012809437617159839	21665	21.665	0.08393126907517559
X	0.013056395466543119	6511	6.511	0.12610293659143842
X	0.01233843851071973	2219	2.219	0.1771608000846995
X	0.010504592638121934	107	0.107	0.4613159591701482
X	0.01292619437154228	55228	55.228	0.06162691890508312
X	0.012536654633088364	8562	8.562	0.11355392392165223
X	0.013051220187825723	82311	82.311	0.05412497967198987
X	0.01303426797308953	546	0.546	0.2879373483870684
X	0.014661803937191747	10448	10.448	0.11195704175832638
X	0.012619602587071654	956	0.956	0.236335663291882
X	0.012233588798879123	10705	10.705	0.1045496064471917
X	0.01681957943772087	3310	3.31	0.17192104654338064
X	0.012040287593774882	1214	1.214	0.2148519953058537
X	0.011747798957132679	982	0.982	0.22870816669893868
X	0.011990780773819128	3219	3.219	0.15501563205045973
X	0.012019618118053188	761	0.761	0.2509007758697491
X	0.01273313016514437	4916	4.916	0.13733285062657818
X	0.012973609668668153	1665	1.665	0.19825106537634984
X	0.012637625323147137	835	0.835	0.24735821216779452
X	0.01288187844162441	825	0.825	0.24994344379886868
X	0.013129069871686796	15198	15.198	0.0952392221696413
X	0.016419461490590134	1200	1.2	0.23918056519020284
X	0.01302836904054131	17935	17.935	0.08989376224368477
X	0.01308078188603314	11322	11.322	0.10493092899495851
X	0.014395659886063793	10953	10.953	0.10953834204824754
X	0.013031714335320002	1771	1.771	0.19450360870860894
X	0.010929370725386542	101	0.101	0.4765311130024735
X	0.013219001516168212	1209	1.209	0.2219511760262668
X	0.0131633702773372	2884	2.884	0.16587869649894726
X	0.011905940559948801	266	0.266	0.35505368503029544
X	0.01194811012718952	1075	1.075	0.2231671416040526
X	0.014075720805549143	149	0.149	0.4554368060938061
X	0.011776787119618107	390	0.39	0.31140152797877385
X	0.012810094954188443	24580	24.58	0.08047422880259322
X	0.012236118413155386	712	0.712	0.25806031665229834
X	0.012961170378114655	3619	3.619	0.15299780920895809
X	0.012292204485319157	104	0.104	0.4907558429219206
X	0.014367728773734505	312	0.312	0.358435631522882
X	0.012388339491797436	1270	1.27	0.21366651135692197
X	0.012811631739791043	5774	5.774	0.1304295179111275
X	0.012903954574397717	13806	13.806	0.09777285993406382
X	0.012358978959226103	278	0.278	0.3542522350510494
X	0.011340998143161902	978	0.978	0.22634494938588862
X	0.01474859370665594	119	0.119	0.49857965203156335
X	0.011454710991773368	722	0.722	0.2512748251991506
X	0.012948855341106022	1897	1.897	0.1896944753701513
X	0.011692549121172478	958	0.958	0.23024024108069038
X	0.012082674886562596	823	0.823	0.244861826607998
X	0.014056623209712724	9621	9.621	0.11347153494582529
X	0.01689610439204793	773	0.773	0.279599053396362
X	0.013125171849008783	6348	6.348	0.12739606881666984
X	0.013352498174875074	14788	14.788	0.09665352445389329
X	0.009658822506253358	217	0.217	0.3543954606665898
X	0.012909073673252603	1139	1.139	0.2246244974260462
X	0.012943779967796974	2272	2.272	0.1786011557344742
X	0.012650258292040108	8548	8.548	0.11395803705311087
X	0.013050641442355194	11373	11.373	0.10469331212991642
X	0.01245721403156533	340	0.34	0.3321345102568427
X	0.0128235781074793	2217	2.217	0.1795069120583851
X	0.012839375264028853	2666	2.666	0.16887340070476686
X	0.013680167665922999	365	0.365	0.3346566956464848
X	0.008929356257011807	85	0.085	0.47184614180266343
X	0.011085395199814402	695	0.695	0.25172255158136536
X	0.013159052053594649	18653	18.653	0.08902089812849202
X	0.013021720670496553	79240	79.24	0.054774022524929154
X	0.01208474201195325	2033	2.033	0.18114791300895783
X	0.012468364705950365	276	0.276	0.3561504288989958
X	0.014135785114320634	313	0.313	0.3561163108396049
X	0.013708013961688546	7331	7.331	0.12319803271470035
X	0.012057407522159981	201	0.201	0.3914587101231477
X	0.012363791696701818	362	0.362	0.32444994359779966
X	0.012679000031640804	2516	2.516	0.171444986100397
X	0.0132202016460206	27932	27.932	0.077931316945631
X	0.013240716227364113	91	0.091	0.5259648182612477
X	0.013030976801637196	3946	3.946	0.14891552041812095
X	0.01284022871208966	400	0.4	0.31781246204058416
time for making epsilon is 0.2947559356689453
epsilons are
[0.4021778521068764, 0.3352904906626487, 0.5701458957404725, 0.46460015800249965, 0.1651276325031674, 0.1747093071096221, 0.2902448979196377, 0.12559347690284822, 0.15924036995325008, 0.2328798040223186, 0.2788724278297356, 0.20004242758045734, 0.32422356064847146, 0.058067820943583925, 0.33668098095478294, 0.13585879308933202, 0.3705746272264336, 0.19710746946220403, 0.06082782045902098, 0.060362588416302605, 0.26637622869364697, 0.08962060686797589, 0.29894118522270463, 0.2972281377199747, 0.3712344756745645, 0.18320704717986253, 0.07746172556256352, 0.22305414655428335, 0.07358392127830322, 0.04500832864513893, 0.17231055658991576, 0.06898185281235955, 0.2447631482449548, 0.1582572472440058, 0.4742322038048422, 0.051566112411347426, 0.453436476603649, 0.31299122203931573, 0.29427961604602443, 0.30446801582738686, 0.40799128407905927, 0.35343196893394424, 0.5372434370304109, 0.534523309998407, 0.4350350675993038, 0.5127994956297696, 0.4737154474513717, 0.6574105851249038, 0.5280415538338139, 0.4810286364337186, 0.11452161601038824, 0.5130241283652776, 0.2066201859015752, 0.4354331104810232, 0.3129032666883626, 0.21719005889084955, 0.32320307312537017, 0.383014687434268, 0.30860665018464273, 0.7088577247698726, 0.24380072889618581, 0.39588350378469794, 0.322669078371234, 0.2729720055230663, 0.29271315438235695, 0.5991432992908776, 0.26796181951324133, 0.42618131747866556, 0.5317179628205057, 0.5310103563948579, 0.4308129158799325, 0.4161882299800055, 0.4523991094718607, 0.5874331116262556, 0.2050592366118891, 0.2657261320574761, 0.6525755626396221, 0.40205853815096376, 0.4426683084813191, 0.3658165340129161, 0.30754397211315465, 0.33823143900829933, 0.32170288230034033, 0.5049049459134024, 0.29510797718184706, 0.3607269381888438, 0.4588499488912506, 0.44806903813962584, 0.4387990086248773, 0.3541153389646174, 0.28487624316323773, 0.48546381445080405, 0.3284623340698959, 0.19350822869346768, 0.5012106945767031, 0.6795670775671995, 0.29790466358889967, 0.47219578434533116, 0.5716329194566128, 0.4711683707457481, 0.17173561560434053, 0.3586370312311524, 0.41741467342943317, 0.5516103208071685, 0.21363770497547074, 0.265143785096233, 0.29255299549909164, 0.5121511321447051, 0.25389510698053225, 0.41836361559838525, 0.39762067514105537, 0.4611677999396978, 0.6611188759186166, 0.5232436025642374, 0.16934717286140666, 0.36707430271850583, 0.41882995244141924, 0.3881956001190509, 0.3345825812348062, 0.252089422107503, 0.28052361406959286, 0.36104464717110596, 0.5441285511129428, 0.45494010610086943, 0.2226033780449239, 0.2960132761088399, 0.6027800305556994, 0.5742095957957769, 0.1705414377467256, 0.4924122418282572, 0.37959493810388556, 0.4175362668116425, 0.40016967236131357, 0.3824603613027995, 0.5560097142245811, 0.303002289219453, 0.2729886515393189, 0.37704459435949716, 0.16155674377497858, 0.4484603822184853, 0.2837263156533065, 0.569744414508255, 0.18632626244620612, 0.2592466737964207, 0.31047208148380984, 0.3710684879071158, 0.4529713166727682, 0.5805572018721501, 0.7237072667107024, 0.3493365646881845, 0.5319168088443884, 0.27317227517774484, 0.20868093436872778, 0.3645141722834265, 0.5418024670308972, 0.1915753705402858, 0.2981520734288742, 0.2753261760619482, 0.40053918555865725, 0.5600854767455938, 0.577231685417972, 0.518364304259055, 0.3403217125104231, 0.7226224349075233, 0.3442945667104883, 0.5767259136751515, 0.40619997310318806, 0.3093032918527301, 0.34232214653046344, 0.4927265446909514, 0.4308630893441141, 0.2828144313809125, 0.39775313229435577, 0.4486976413420677, 0.14342442824584584, 0.49570643343706006, 0.3800526810303465, 0.35183226446269034, 0.2696771738399955, 0.3999315955664469, 0.3301316773989094, 0.33557490247918703, 0.37084744977183887, 0.49999903521870176, 0.21942406695870312, 0.25825253621263766, 0.4361313393367209, 0.4428214506192763, 0.27985597899054715, 0.17644498248034435, 0.6862948356862913, 0.480351830342183, 0.2644621987378242, 0.3143099413287151, 0.4252257483050623, 0.43010231069308064, 0.5218482707212525, 0.49090901386429653, 0.2979532546652716, 0.540472460985803, 0.3800221669362219, 0.5752534971181531, 0.31859716379919273, 0.47886538995593775, 0.441492056442161, 0.3784710891545261, 0.34824999300567056, 0.3463426269454105, 0.279325850595055, 0.5137351875673568, 0.31717157728811707, 0.2418281081432018, 0.5552201540830621, 0.29190002494099376, 0.33638075004121193, 0.5869912951265858, 0.5873684630912259, 0.35550771424715516, 0.12241901309346491, 0.5474029446238352, 0.6728533858845447, 0.5958356869052989, 0.32518358394433167, 0.39439196439704993, 0.5567657246309128, 0.452767010784113, 0.6650571418303592, 0.5167704961529934, 0.3430549809053705, 0.3560420996269839, 0.6518003859068575, 0.3567420397305249, 0.6624674807465365, 0.4385527736108555, 0.3027014731582302, 0.36684475590424453, 0.2684153366683372, 0.2721104113922868, 0.40068119110644784, 0.24195832104975265, 0.2311317769558427, 0.26412070353313283, 0.33637247839998563, 0.5224909377721728, 0.2899743217472552, 0.1992754986188749, 0.2268210592806414, 0.2754682167425648, 0.1829470647711821, 0.12661849609868894, 0.3629759490689908, 0.24183367565348207, 0.29392404156544016, 0.3799834183361828, 0.09332858346709764, 0.08832042839783218, 0.41607701220431514, 0.1186280757486225, 0.08871324552424761, 0.1904062282576293, 0.15435552696671476, 0.158506598927119, 0.17150878198724806, 0.17839588844159263, 0.4809390520565285, 0.2636328663455907, 0.3904988666061901, 0.3489319640427065, 0.25878389245989203, 0.2109145653128183, 0.11785884788838784, 0.3038182001205234, 0.35052376815230507, 0.3206109069068987, 0.20872330404437436, 0.22848834720893155, 0.09764522008699829, 0.08977861986894185, 0.25771028422439574, 0.8093642810147366, 0.5484626199995021, 0.12692542394349465, 0.13997092394305657, 0.22713127109951478, 0.5408340412690227, 0.11041696384756096, 0.3166813481362147, 0.2997878987287119, 0.07003535207514065, 0.2694477546066466, 0.20242771471786777, 0.09271486982990562, 0.06590196765819342, 0.21024465926795055, 0.2730711598831576, 0.4315897062036192, 0.40200054421325415, 0.12706052405543938, 0.4514789898973014, 0.42034115155205537, 0.1742587765920591, 0.16805922823053684, 0.4691985041786676, 0.3347224347656012, 0.05665135816385905, 0.2540377518726054, 0.30346523469405134, 0.23479518392961854, 0.08088045224396642, 0.3325332529127435, 0.3465000832709029, 0.3404557684523835, 0.3552247552626137, 0.2097572979173045, 0.15038159204225027, 0.11900529745849968, 0.39485793190304014, 0.09435718077158105, 0.13660688457655212, 0.3640959486587492, 0.11595276094792478, 0.11471704159945253, 0.2138972510718035, 0.16912004821545187, 0.3927732278706911, 0.22273255110574347, 0.288100920675818, 0.25829557109118534, 0.30605074456295045, 0.4443698728249046, 0.23879784493029024, 0.17620725534922113, 0.15102144958789182, 0.3459712144441336, 0.26996249115111354, 0.20100788454941604, 0.25918697813794894, 0.24099041332427948, 0.3142630337599655, 0.0603854557622318, 0.20413879910178084, 0.3902704181499236, 0.10670180538494012, 0.4166522965087975, 0.08220238248858622, 0.17372521993808035, 0.22653617937288859, 0.3682512404593794, 0.3377905447664022, 0.2893272714484363, 0.22370156665639648, 0.36010797844869347, 0.2111929025352252, 0.5787329478877713, 0.5039962701494027, 0.09489165846356037, 0.1481822708886541, 0.11615512660613776, 0.16055055400076343, 0.07238137183087949, 0.2163195806346913, 0.22400147401993, 0.2778774070718396, 0.571760722465554, 0.5565803919674611, 0.19856412540181098, 0.25688455924915865, 0.3499338077644761, 0.3054114714231187, 0.17646697357017646, 0.04614644471092612, 0.39356112586383735, 0.21837021691079983, 0.33732620060081264, 0.5220212418748373, 0.25583863835586634, 0.10543108381599134, 0.0455365836836545, 0.24365605293736625, 0.13447886332214803, 0.5163083380908157, 0.26735105384715085, 0.04900065115697183, 0.10273245553914673, 0.23182642345187227, 0.12077494412428251, 0.07046446405936226, 0.16000006288883104, 0.30562520611447547, 0.4408580805874291, 0.17810400472642596, 0.39004156456480077, 0.10375028803345489, 0.14518402033045252, 0.43018347658717154, 0.1442793163715888, 0.2944501616319455, 0.4739940473157559, 0.15845566745256834, 0.12251052310575539, 0.1029318451925402, 0.16547759666541254, 0.07093441451413443, 0.14367105719638465, 0.15669203023834885, 0.1534050977001601, 0.17968689155996947, 0.30790058704048556, 0.1082416366884272, 0.2323578736357629, 0.14219926654491905, 0.11706741609878303, 0.3242810292776721, 0.4405243834446403, 0.526327183314224, 0.09378662725598925, 0.07102286932023864, 0.08742992062928466, 0.13270201340288626, 0.06610784878052485, 0.17583186612870957, 0.544307544399138, 0.07096314471512308, 0.3814727414020806, 0.28255015662181465, 0.0622600997272003, 0.20568432247181462, 0.36295862259973416, 0.3103507759351805, 0.07146016352026172, 0.15489284564941683, 0.13267640612042358, 0.49522058293125876, 0.2985818795897048, 0.33713156501704833, 0.1123344427517355, 0.25196138786939487, 0.15826508077070736, 0.25503962527455537, 0.11276388271420083, 0.45979131427995457, 0.11855053255395363, 0.19089871788921173, 0.1786934797210353, 0.30598272290559375, 0.1699696854439192, 0.43263248717564734, 0.17894481349208383, 0.18874314217408264, 0.19801482561287634, 0.44453562369888044, 0.3660208041701824, 0.16483480974871972, 0.2397140270703858, 0.039664827317159465, 0.49806883060752377, 0.19469204128206516, 0.09002928246235448, 0.28488795742046846, 0.11834629259621422, 0.333982005823065, 0.10140129250192738, 0.20312635506563148, 0.24525334111876773, 0.4604720968118824, 0.4942434587452938, 0.28059620168818655, 0.3395999217600362, 0.17137622674723435, 0.13407826898494307, 0.22097485353659868, 0.3747004519147549, 0.36114648940329386, 0.11217197977391691, 0.20991849204018304, 0.09543475173942972, 0.30077531016769193, 0.5691608495627799, 0.41765706428422805, 0.30281428342919015, 0.187024728720464, 0.1423895365181717, 0.3138020204729984, 0.11927826722985724, 0.21742617690558716, 0.40104586577630835, 0.3202172622599108, 0.2193689486339398, 0.2491158760776501, 0.1932130545369274, 0.3437019275732881, 0.14274439971851483, 0.09526717769955308, 0.10549746727620703, 0.08012371041081859, 0.22433363387773855, 0.12395173049955766, 0.20522434831009373, 0.2746756752250373, 0.5028033855536364, 0.2946399141960149, 0.33491855695052886, 0.24859654227398487, 0.5929785862702837, 0.21918171923967594, 0.2724534026440091, 0.06989392150263846, 0.5613383446188706, 0.5105289361055001, 0.1839970477230036, 0.3031048190660553, 0.20781972086796305, 0.4153131167707421, 0.14621370786343282, 0.11022762590963965, 0.41848379024779225, 0.507743478636488, 0.7532541439969015, 0.14547654682911454, 0.04413584856458732, 0.35178656602477787, 0.4126590158111586, 0.141350160674522, 0.10782942733889117, 0.3209434489027075, 0.3550224358053505, 0.40754051076405295, 0.0424947759890588, 0.15117184530765101, 0.2255859888854932, 0.22470020557184514, 0.1994642151248642, 0.4103677676977145, 0.09078873873088317, 0.38655977966104477, 0.1810207954058408, 0.09466535108757451, 0.045627061390523777, 0.30086953500659264, 0.3508565885490626, 0.10333680952495915, 0.21726359386413774, 0.1121752941897692, 0.5189813744609755, 0.48170781149523856, 0.23999487581990214, 0.4043828034650787, 0.25660810396201844, 0.3960192800802387, 0.20994478686788032, 0.24736280302399527, 0.3016282593748152, 0.40605821173533135, 0.07878604426244291, 0.06389717092270146, 0.16615905861581778, 0.35412416913763745, 0.16203262476568328, 0.28548963772891944, 0.4875951710502417, 0.09259189426935467, 0.21364038328916732, 0.1326130464761297, 0.2148129539606745, 0.10221564022308129, 0.2625821024175914, 0.1026774084341258, 0.08008946555196125, 0.19519089761433223, 0.162906482457681, 0.2791655338233616, 0.1289150270683396, 0.20428886267447663, 0.30095647663429537, 0.25772157517092287, 0.26396702462172444, 0.10933223895608128, 0.059610596218093996, 0.11193164722512082, 0.22688280414072737, 0.26493870541942655, 0.47233313983726366, 0.17569227455052916, 0.3061643719455612, 0.061210143956525825, 0.18721489440914235, 0.39208386805181367, 0.04664529580758952, 0.327936507718946, 0.2172537643589635, 0.4981530123254255, 0.48311642549175726, 0.3239250884441291, 0.3800664204784943, 0.04160067778444011, 0.13101955988870112, 0.20762077652539068, 0.1596835154626082, 0.35210355715752045, 0.30210247423091185, 0.18631943640918716, 0.20270255124221553, 0.36697523829887224, 0.17592651159112557, 0.28430404175603396, 0.27128742179904497, 0.2759816167874808, 0.2776219926732994, 0.1557609726337887, 0.14401145440228458, 0.2172972097701215, 0.046455633588677145, 0.2500768048717659, 0.08523567372452884, 0.30610729648841256, 0.13475318337293968, 0.06099241499147302, 0.1329028330993823, 0.35296604431033746, 0.14176113862393053, 0.21159496110099402, 0.14597011093574425, 0.08283631324635198, 0.2949185992507436, 0.27968015075183644, 0.21634768426324857, 0.08393126907517559, 0.12610293659143842, 0.1771608000846995, 0.4613159591701482, 0.06162691890508312, 0.11355392392165223, 0.05412497967198987, 0.2879373483870684, 0.11195704175832638, 0.236335663291882, 0.1045496064471917, 0.17192104654338064, 0.2148519953058537, 0.22870816669893868, 0.15501563205045973, 0.2509007758697491, 0.13733285062657818, 0.19825106537634984, 0.24735821216779452, 0.24994344379886868, 0.0952392221696413, 0.23918056519020284, 0.08989376224368477, 0.10493092899495851, 0.10953834204824754, 0.19450360870860894, 0.4765311130024735, 0.2219511760262668, 0.16587869649894726, 0.35505368503029544, 0.2231671416040526, 0.4554368060938061, 0.31140152797877385, 0.08047422880259322, 0.25806031665229834, 0.15299780920895809, 0.4907558429219206, 0.358435631522882, 0.21366651135692197, 0.1304295179111275, 0.09777285993406382, 0.3542522350510494, 0.22634494938588862, 0.49857965203156335, 0.2512748251991506, 0.1896944753701513, 0.23024024108069038, 0.244861826607998, 0.11347153494582529, 0.279599053396362, 0.12739606881666984, 0.09665352445389329, 0.3543954606665898, 0.2246244974260462, 0.1786011557344742, 0.11395803705311087, 0.10469331212991642, 0.3321345102568427, 0.1795069120583851, 0.16887340070476686, 0.3346566956464848, 0.47184614180266343, 0.25172255158136536, 0.08902089812849202, 0.054774022524929154, 0.18114791300895783, 0.3561504288989958, 0.3561163108396049, 0.12319803271470035, 0.3914587101231477, 0.32444994359779966, 0.171444986100397, 0.077931316945631, 0.5259648182612477, 0.14891552041812095, 0.31781246204058416]
0.10983511863828856
Making ranges
torch.Size([20573, 2])
We keep 5.39e+06/2.27e+08 =  2% of the original kernel matrix.

torch.Size([513, 2])
We keep 5.75e+03/3.42e+04 = 16% of the original kernel matrix.

torch.Size([3976, 2])
We keep 2.31e+05/2.79e+06 =  8% of the original kernel matrix.

torch.Size([688, 2])
We keep 1.49e+04/1.01e+05 = 14% of the original kernel matrix.

torch.Size([4213, 2])
We keep 3.18e+05/4.80e+06 =  6% of the original kernel matrix.

torch.Size([137, 2])
We keep 1.68e+03/4.49e+03 = 37% of the original kernel matrix.

torch.Size([2437, 2])
We keep 1.36e+05/1.01e+06 = 13% of the original kernel matrix.

torch.Size([314, 2])
We keep 3.85e+03/1.37e+04 = 28% of the original kernel matrix.

torch.Size([3221, 2])
We keep 1.83e+05/1.76e+06 = 10% of the original kernel matrix.

torch.Size([5579, 2])
We keep 9.89e+05/1.43e+07 =  6% of the original kernel matrix.

torch.Size([9810, 2])
We keep 1.99e+06/5.71e+07 =  3% of the original kernel matrix.

torch.Size([4596, 2])
We keep 3.85e+05/5.60e+06 =  6% of the original kernel matrix.

torch.Size([8794, 2])
We keep 1.31e+06/3.57e+07 =  3% of the original kernel matrix.

torch.Size([948, 2])
We keep 3.59e+04/2.59e+05 = 13% of the original kernel matrix.

torch.Size([4688, 2])
We keep 4.53e+05/7.68e+06 =  5% of the original kernel matrix.

torch.Size([11251, 2])
We keep 1.55e+06/4.29e+07 =  3% of the original kernel matrix.

torch.Size([13820, 2])
We keep 2.81e+06/9.88e+07 =  2% of the original kernel matrix.

torch.Size([4966, 2])
We keep 1.77e+06/1.00e+07 = 17% of the original kernel matrix.

torch.Size([9106, 2])
We keep 1.62e+06/4.78e+07 =  3% of the original kernel matrix.

torch.Size([1514, 2])
We keep 2.16e+05/8.65e+05 = 24% of the original kernel matrix.

torch.Size([5271, 2])
We keep 6.38e+05/1.40e+07 =  4% of the original kernel matrix.

torch.Size([1324, 2])
We keep 3.45e+04/3.31e+05 = 10% of the original kernel matrix.

torch.Size([5423, 2])
We keep 4.71e+05/8.67e+06 =  5% of the original kernel matrix.

torch.Size([2298, 2])
We keep 2.25e+05/2.32e+06 =  9% of the original kernel matrix.

torch.Size([6386, 2])
We keep 9.62e+05/2.30e+07 =  4% of the original kernel matrix.

torch.Size([1063, 2])
We keep 1.66e+04/1.33e+05 = 12% of the original kernel matrix.

torch.Size([5151, 2])
We keep 3.60e+05/5.50e+06 =  6% of the original kernel matrix.

torch.Size([98857, 2])
We keep 8.36e+07/4.49e+09 =  1% of the original kernel matrix.

torch.Size([42226, 2])
We keep 1.93e+07/1.01e+09 =  1% of the original kernel matrix.

torch.Size([829, 2])
We keep 1.59e+04/1.08e+05 = 14% of the original kernel matrix.

torch.Size([4657, 2])
We keep 3.44e+05/4.96e+06 =  6% of the original kernel matrix.

torch.Size([9354, 2])
We keep 1.35e+06/2.70e+07 =  5% of the original kernel matrix.

torch.Size([12515, 2])
We keep 2.34e+06/7.83e+07 =  2% of the original kernel matrix.

torch.Size([585, 2])
We keep 1.20e+04/5.81e+04 = 20% of the original kernel matrix.

torch.Size([3917, 2])
We keep 2.63e+05/3.63e+06 =  7% of the original kernel matrix.

torch.Size([3228, 2])
We keep 2.40e+05/2.60e+06 =  9% of the original kernel matrix.

torch.Size([7635, 2])
We keep 1.01e+06/2.43e+07 =  4% of the original kernel matrix.

torch.Size([85084, 2])
We keep 6.44e+07/3.38e+09 =  1% of the original kernel matrix.

torch.Size([38643, 2])
We keep 1.71e+07/8.77e+08 =  1% of the original kernel matrix.

torch.Size([87320, 2])
We keep 6.19e+07/3.56e+09 =  1% of the original kernel matrix.

torch.Size([39183, 2])
We keep 1.74e+07/9.00e+08 =  1% of the original kernel matrix.

torch.Size([1247, 2])
We keep 5.18e+04/3.73e+05 = 13% of the original kernel matrix.

torch.Size([5009, 2])
We keep 5.07e+05/9.21e+06 =  5% of the original kernel matrix.

torch.Size([26035, 2])
We keep 7.56e+06/3.37e+08 =  2% of the original kernel matrix.

torch.Size([21796, 2])
We keep 6.17e+06/2.77e+08 =  2% of the original kernel matrix.

torch.Size([1140, 2])
We keep 2.42e+04/2.09e+05 = 11% of the original kernel matrix.

torch.Size([5170, 2])
We keep 4.08e+05/6.89e+06 =  5% of the original kernel matrix.

torch.Size([1155, 2])
We keep 2.47e+04/2.04e+05 = 12% of the original kernel matrix.

torch.Size([5249, 2])
We keep 3.92e+05/6.82e+06 =  5% of the original kernel matrix.

torch.Size([552, 2])
We keep 1.38e+04/6.05e+04 = 22% of the original kernel matrix.

torch.Size([3785, 2])
We keep 2.92e+05/3.71e+06 =  7% of the original kernel matrix.

torch.Size([3788, 2])
We keep 3.02e+05/4.28e+06 =  7% of the original kernel matrix.

torch.Size([8264, 2])
We keep 1.19e+06/3.12e+07 =  3% of the original kernel matrix.

torch.Size([39648, 2])
We keep 1.86e+07/7.98e+08 =  2% of the original kernel matrix.

torch.Size([26277, 2])
We keep 8.96e+06/4.26e+08 =  2% of the original kernel matrix.

torch.Size([2272, 2])
We keep 9.23e+04/1.12e+06 =  8% of the original kernel matrix.

torch.Size([6674, 2])
We keep 7.29e+05/1.60e+07 =  4% of the original kernel matrix.

torch.Size([39255, 2])
We keep 2.62e+07/1.02e+09 =  2% of the original kernel matrix.

torch.Size([25248, 2])
We keep 1.02e+07/4.83e+08 =  2% of the original kernel matrix.

torch.Size([209034, 2])
We keep 2.62e+08/2.07e+10 =  1% of the original kernel matrix.

torch.Size([63943, 2])
We keep 3.71e+07/2.17e+09 =  1% of the original kernel matrix.

torch.Size([4992, 2])
We keep 3.85e+05/6.18e+06 =  6% of the original kernel matrix.

torch.Size([9332, 2])
We keep 1.34e+06/3.75e+07 =  3% of the original kernel matrix.

torch.Size([50763, 2])
We keep 4.42e+07/1.57e+09 =  2% of the original kernel matrix.

torch.Size([28886, 2])
We keep 1.23e+07/5.97e+08 =  2% of the original kernel matrix.

torch.Size([1693, 2])
We keep 6.52e+04/6.64e+05 =  9% of the original kernel matrix.

torch.Size([5745, 2])
We keep 6.04e+05/1.23e+07 =  4% of the original kernel matrix.

torch.Size([6523, 2])
We keep 5.66e+05/1.05e+07 =  5% of the original kernel matrix.

torch.Size([10587, 2])
We keep 1.61e+06/4.88e+07 =  3% of the original kernel matrix.

torch.Size([362, 2])
We keep 3.44e+03/1.46e+04 = 23% of the original kernel matrix.

torch.Size([3502, 2])
We keep 1.70e+05/1.82e+06 =  9% of the original kernel matrix.

torch.Size([134072, 2])
We keep 1.62e+08/1.01e+10 =  1% of the original kernel matrix.

torch.Size([49537, 2])
We keep 2.75e+07/1.52e+09 =  1% of the original kernel matrix.

torch.Size([409, 2])
We keep 6.14e+03/2.46e+04 = 24% of the original kernel matrix.

torch.Size([3825, 2])
We keep 2.31e+05/2.37e+06 =  9% of the original kernel matrix.

torch.Size([953, 2])
We keep 2.28e+04/1.67e+05 = 13% of the original kernel matrix.

torch.Size([4798, 2])
We keep 3.92e+05/6.17e+06 =  6% of the original kernel matrix.

torch.Size([1092, 2])
We keep 2.44e+04/1.94e+05 = 12% of the original kernel matrix.

torch.Size([4930, 2])
We keep 4.11e+05/6.65e+06 =  6% of the original kernel matrix.

torch.Size([822, 2])
We keep 2.85e+04/1.82e+05 = 15% of the original kernel matrix.

torch.Size([4414, 2])
We keep 3.94e+05/6.44e+06 =  6% of the original kernel matrix.

torch.Size([521, 2])
We keep 4.89e+03/2.92e+04 = 16% of the original kernel matrix.

torch.Size([3987, 2])
We keep 2.13e+05/2.58e+06 =  8% of the original kernel matrix.

torch.Size([136, 2])
We keep 4.20e+04/6.10e+04 = 68% of the original kernel matrix.

torch.Size([1534, 2])
We keep 2.37e+05/3.73e+06 =  6% of the original kernel matrix.

torch.Size([85, 2])
We keep 9.45e+02/3.02e+03 = 31% of the original kernel matrix.

torch.Size([1769, 2])
We keep 1.04e+05/8.29e+05 = 12% of the original kernel matrix.

torch.Size([212, 2])
We keep 1.17e+03/5.48e+03 = 21% of the original kernel matrix.

torch.Size([2916, 2])
We keep 1.20e+05/1.12e+06 = 10% of the original kernel matrix.

torch.Size([266, 2])
We keep 5.43e+03/2.46e+04 = 22% of the original kernel matrix.

torch.Size([2992, 2])
We keep 1.77e+05/2.37e+06 =  7% of the original kernel matrix.

torch.Size([283, 2])
We keep 2.04e+03/7.57e+03 = 26% of the original kernel matrix.

torch.Size([3292, 2])
We keep 1.41e+05/1.31e+06 = 10% of the original kernel matrix.

torch.Size([302, 2])
We keep 2.46e+03/1.06e+04 = 23% of the original kernel matrix.

torch.Size([3278, 2])
We keep 1.64e+05/1.55e+06 = 10% of the original kernel matrix.

torch.Size([98, 2])
We keep 5.19e+02/1.22e+03 = 42% of the original kernel matrix.

torch.Size([2152, 2])
We keep 8.50e+04/5.28e+05 = 16% of the original kernel matrix.

torch.Size([239, 2])
We keep 1.73e+03/6.89e+03 = 25% of the original kernel matrix.

torch.Size([3139, 2])
We keep 1.34e+05/1.25e+06 = 10% of the original kernel matrix.

torch.Size([339, 2])
We keep 2.51e+03/1.12e+04 = 22% of the original kernel matrix.

torch.Size([3402, 2])
We keep 1.62e+05/1.60e+06 = 10% of the original kernel matrix.

torch.Size([9693, 2])
We keep 2.93e+06/6.99e+07 =  4% of the original kernel matrix.

torch.Size([12570, 2])
We keep 3.37e+06/1.26e+08 =  2% of the original kernel matrix.

torch.Size([271, 2])
We keep 1.93e+03/7.40e+03 = 26% of the original kernel matrix.

torch.Size([3143, 2])
We keep 1.45e+05/1.30e+06 = 11% of the original kernel matrix.

torch.Size([2767, 2])
We keep 2.32e+05/1.91e+06 = 12% of the original kernel matrix.

torch.Size([7090, 2])
We keep 8.57e+05/2.08e+07 =  4% of the original kernel matrix.

torch.Size([448, 2])
We keep 3.90e+03/1.77e+04 = 22% of the original kernel matrix.

torch.Size([3696, 2])
We keep 1.86e+05/2.01e+06 =  9% of the original kernel matrix.

torch.Size([889, 2])
We keep 2.58e+04/1.81e+05 = 14% of the original kernel matrix.

torch.Size([4653, 2])
We keep 4.13e+05/6.41e+06 =  6% of the original kernel matrix.

torch.Size([2217, 2])
We keep 2.11e+05/2.30e+06 =  9% of the original kernel matrix.

torch.Size([6594, 2])
We keep 1.01e+06/2.29e+07 =  4% of the original kernel matrix.

torch.Size([897, 2])
We keep 1.99e+04/1.39e+05 = 14% of the original kernel matrix.

torch.Size([4726, 2])
We keep 3.65e+05/5.63e+06 =  6% of the original kernel matrix.

torch.Size([555, 2])
We keep 7.25e+03/4.49e+04 = 16% of the original kernel matrix.

torch.Size([3959, 2])
We keep 2.50e+05/3.20e+06 =  7% of the original kernel matrix.

torch.Size([1052, 2])
We keep 2.77e+04/1.99e+05 = 13% of the original kernel matrix.

torch.Size([5131, 2])
We keep 4.17e+05/6.73e+06 =  6% of the original kernel matrix.

torch.Size([69, 2])
We keep 1.67e+02/3.61e+02 = 46% of the original kernel matrix.

torch.Size([1992, 2])
We keep 5.27e+04/2.87e+05 = 18% of the original kernel matrix.

torch.Size([1927, 2])
We keep 1.79e+05/1.87e+06 =  9% of the original kernel matrix.

torch.Size([6629, 2])
We keep 9.81e+05/2.06e+07 =  4% of the original kernel matrix.

torch.Size([566, 2])
We keep 6.72e+03/4.16e+04 = 16% of the original kernel matrix.

torch.Size([4157, 2])
We keep 2.37e+05/3.08e+06 =  7% of the original kernel matrix.

torch.Size([947, 2])
We keep 1.66e+04/1.32e+05 = 12% of the original kernel matrix.

torch.Size([4747, 2])
We keep 3.54e+05/5.47e+06 =  6% of the original kernel matrix.

torch.Size([1355, 2])
We keep 3.80e+04/3.79e+05 = 10% of the original kernel matrix.

torch.Size([5454, 2])
We keep 5.02e+05/9.29e+06 =  5% of the original kernel matrix.

torch.Size([1138, 2])
We keep 2.94e+04/2.36e+05 = 12% of the original kernel matrix.

torch.Size([5092, 2])
We keep 4.44e+05/7.33e+06 =  6% of the original kernel matrix.

torch.Size([152, 2])
We keep 5.41e+02/1.52e+03 = 35% of the original kernel matrix.

torch.Size([2537, 2])
We keep 8.27e+04/5.88e+05 = 14% of the original kernel matrix.

torch.Size([1219, 2])
We keep 3.58e+04/3.68e+05 =  9% of the original kernel matrix.

torch.Size([5242, 2])
We keep 4.86e+05/9.15e+06 =  5% of the original kernel matrix.

torch.Size([476, 2])
We keep 5.07e+03/2.37e+04 = 21% of the original kernel matrix.

torch.Size([3899, 2])
We keep 2.16e+05/2.32e+06 =  9% of the original kernel matrix.

torch.Size([258, 2])
We keep 1.28e+03/5.62e+03 = 22% of the original kernel matrix.

torch.Size([3225, 2])
We keep 1.21e+05/1.13e+06 = 10% of the original kernel matrix.

torch.Size([237, 2])
We keep 1.52e+03/5.62e+03 = 27% of the original kernel matrix.

torch.Size([3035, 2])
We keep 1.38e+05/1.13e+06 = 12% of the original kernel matrix.

torch.Size([475, 2])
We keep 3.94e+03/2.04e+04 = 19% of the original kernel matrix.

torch.Size([3869, 2])
We keep 1.90e+05/2.16e+06 =  8% of the original kernel matrix.

torch.Size([437, 2])
We keep 4.48e+03/2.28e+04 = 19% of the original kernel matrix.

torch.Size([3659, 2])
We keep 2.04e+05/2.28e+06 =  8% of the original kernel matrix.

torch.Size([397, 2])
We keep 2.98e+03/1.42e+04 = 21% of the original kernel matrix.

torch.Size([3585, 2])
We keep 1.73e+05/1.79e+06 =  9% of the original kernel matrix.

torch.Size([146, 2])
We keep 7.64e+02/2.50e+03 = 30% of the original kernel matrix.

torch.Size([2440, 2])
We keep 9.86e+04/7.54e+05 = 13% of the original kernel matrix.

torch.Size([3025, 2])
We keep 1.70e+05/2.31e+06 =  7% of the original kernel matrix.

torch.Size([7506, 2])
We keep 9.77e+05/2.29e+07 =  4% of the original kernel matrix.

torch.Size([1553, 2])
We keep 4.43e+04/4.22e+05 = 10% of the original kernel matrix.

torch.Size([5719, 2])
We keep 5.18e+05/9.80e+06 =  5% of the original kernel matrix.

torch.Size([112, 2])
We keep 6.15e+02/1.52e+03 = 40% of the original kernel matrix.

torch.Size([2370, 2])
We keep 8.97e+04/5.88e+05 = 15% of the original kernel matrix.

torch.Size([361, 2])
We keep 6.56e+03/3.20e+04 = 20% of the original kernel matrix.

torch.Size([3299, 2])
We keep 2.33e+05/2.70e+06 =  8% of the original kernel matrix.

torch.Size([460, 2])
We keep 3.65e+03/1.96e+04 = 18% of the original kernel matrix.

torch.Size([3862, 2])
We keep 1.88e+05/2.11e+06 =  8% of the original kernel matrix.

torch.Size([627, 2])
We keep 9.78e+03/6.30e+04 = 15% of the original kernel matrix.

torch.Size([4180, 2])
We keep 2.89e+05/3.79e+06 =  7% of the original kernel matrix.

torch.Size([900, 2])
We keep 2.20e+04/1.76e+05 = 12% of the original kernel matrix.

torch.Size([4627, 2])
We keep 3.93e+05/6.32e+06 =  6% of the original kernel matrix.

torch.Size([674, 2])
We keep 1.35e+04/8.53e+04 = 15% of the original kernel matrix.

torch.Size([4036, 2])
We keep 3.12e+05/4.40e+06 =  7% of the original kernel matrix.

torch.Size([949, 2])
We keep 1.59e+04/1.20e+05 = 13% of the original kernel matrix.

torch.Size([4885, 2])
We keep 3.35e+05/5.22e+06 =  6% of the original kernel matrix.

torch.Size([266, 2])
We keep 1.40e+03/4.90e+03 = 28% of the original kernel matrix.

torch.Size([3085, 2])
We keep 1.23e+05/1.06e+06 = 11% of the original kernel matrix.

torch.Size([1037, 2])
We keep 2.86e+04/2.16e+05 = 13% of the original kernel matrix.

torch.Size([4858, 2])
We keep 4.30e+05/7.01e+06 =  6% of the original kernel matrix.

torch.Size([572, 2])
We keep 1.15e+04/8.53e+04 = 13% of the original kernel matrix.

torch.Size([4047, 2])
We keep 3.09e+05/4.40e+06 =  7% of the original kernel matrix.

torch.Size([360, 2])
We keep 2.75e+03/1.30e+04 = 21% of the original kernel matrix.

torch.Size([3472, 2])
We keep 1.68e+05/1.72e+06 =  9% of the original kernel matrix.

torch.Size([441, 2])
We keep 4.99e+03/1.90e+04 = 26% of the original kernel matrix.

torch.Size([3727, 2])
We keep 2.07e+05/2.08e+06 =  9% of the original kernel matrix.

torch.Size([433, 2])
We keep 3.53e+03/1.77e+04 = 19% of the original kernel matrix.

torch.Size([3767, 2])
We keep 1.85e+05/2.01e+06 =  9% of the original kernel matrix.

torch.Size([622, 2])
We keep 1.28e+04/6.60e+04 = 19% of the original kernel matrix.

torch.Size([4011, 2])
We keep 2.88e+05/3.88e+06 =  7% of the original kernel matrix.

torch.Size([1151, 2])
We keep 3.59e+04/2.80e+05 = 12% of the original kernel matrix.

torch.Size([4903, 2])
We keep 4.64e+05/7.98e+06 =  5% of the original kernel matrix.

torch.Size([377, 2])
We keep 2.65e+03/1.10e+04 = 24% of the original kernel matrix.

torch.Size([3699, 2])
We keep 1.65e+05/1.58e+06 = 10% of the original kernel matrix.

torch.Size([675, 2])
We keep 2.62e+04/1.35e+05 = 19% of the original kernel matrix.

torch.Size([3974, 2])
We keep 3.79e+05/5.55e+06 =  6% of the original kernel matrix.

torch.Size([3205, 2])
We keep 2.24e+05/2.92e+06 =  7% of the original kernel matrix.

torch.Size([7504, 2])
We keep 1.03e+06/2.58e+07 =  3% of the original kernel matrix.

torch.Size([226, 2])
We keep 1.87e+03/7.57e+03 = 24% of the original kernel matrix.

torch.Size([2789, 2])
We keep 1.39e+05/1.31e+06 = 10% of the original kernel matrix.

torch.Size([88, 2])
We keep 1.50e+02/3.24e+02 = 46% of the original kernel matrix.

torch.Size([1968, 2])
We keep 4.61e+04/2.71e+05 = 16% of the original kernel matrix.

torch.Size([1083, 2])
We keep 3.61e+04/3.41e+05 = 10% of the original kernel matrix.

torch.Size([5279, 2])
We keep 5.09e+05/8.81e+06 =  5% of the original kernel matrix.

torch.Size([387, 2])
We keep 2.56e+03/1.02e+04 = 25% of the original kernel matrix.

torch.Size([3498, 2])
We keep 1.56e+05/1.52e+06 = 10% of the original kernel matrix.

torch.Size([223, 2])
We keep 1.20e+03/4.10e+03 = 29% of the original kernel matrix.

torch.Size([3144, 2])
We keep 1.24e+05/9.65e+05 = 12% of the original kernel matrix.

torch.Size([293, 2])
We keep 2.51e+03/1.25e+04 = 20% of the original kernel matrix.

torch.Size([3239, 2])
We keep 1.64e+05/1.69e+06 =  9% of the original kernel matrix.

torch.Size([4920, 2])
We keep 3.70e+05/5.89e+06 =  6% of the original kernel matrix.

torch.Size([9077, 2])
We keep 1.33e+06/3.66e+07 =  3% of the original kernel matrix.

torch.Size([681, 2])
We keep 9.89e+03/6.30e+04 = 15% of the original kernel matrix.

torch.Size([4283, 2])
We keep 2.82e+05/3.79e+06 =  7% of the original kernel matrix.

torch.Size([458, 2])
We keep 5.37e+03/2.79e+04 = 19% of the original kernel matrix.

torch.Size([3782, 2])
We keep 2.23e+05/2.52e+06 =  8% of the original kernel matrix.

torch.Size([246, 2])
We keep 1.14e+03/3.97e+03 = 28% of the original kernel matrix.

torch.Size([3189, 2])
We keep 1.20e+05/9.50e+05 = 12% of the original kernel matrix.

torch.Size([2349, 2])
We keep 1.42e+05/1.76e+06 =  8% of the original kernel matrix.

torch.Size([6664, 2])
We keep 8.18e+05/2.00e+07 =  4% of the original kernel matrix.

torch.Size([1494, 2])
We keep 3.95e+04/3.83e+05 = 10% of the original kernel matrix.

torch.Size([5647, 2])
We keep 5.02e+05/9.34e+06 =  5% of the original kernel matrix.

torch.Size([1302, 2])
We keep 2.59e+04/2.20e+05 = 11% of the original kernel matrix.

torch.Size([5508, 2])
We keep 4.26e+05/7.07e+06 =  6% of the original kernel matrix.

torch.Size([209, 2])
We keep 1.78e+03/7.40e+03 = 24% of the original kernel matrix.

torch.Size([2827, 2])
We keep 1.38e+05/1.30e+06 = 10% of the original kernel matrix.

torch.Size([1334, 2])
We keep 5.36e+04/5.04e+05 = 10% of the original kernel matrix.

torch.Size([5181, 2])
We keep 5.51e+05/1.07e+07 =  5% of the original kernel matrix.

torch.Size([433, 2])
We keep 6.05e+03/3.57e+04 = 16% of the original kernel matrix.

torch.Size([3793, 2])
We keep 2.42e+05/2.85e+06 =  8% of the original kernel matrix.

torch.Size([502, 2])
We keep 8.92e+03/5.48e+04 = 16% of the original kernel matrix.

torch.Size([3902, 2])
We keep 2.86e+05/3.53e+06 =  8% of the original kernel matrix.

torch.Size([299, 2])
We keep 3.08e+03/1.44e+04 = 21% of the original kernel matrix.

torch.Size([3169, 2])
We keep 1.76e+05/1.81e+06 =  9% of the original kernel matrix.

torch.Size([115, 2])
We keep 3.18e+02/7.84e+02 = 40% of the original kernel matrix.

torch.Size([2293, 2])
We keep 6.78e+04/4.22e+05 = 16% of the original kernel matrix.

torch.Size([290, 2])
We keep 1.67e+03/5.93e+03 = 28% of the original kernel matrix.

torch.Size([3362, 2])
We keep 1.37e+05/1.16e+06 = 11% of the original kernel matrix.

torch.Size([3865, 2])
We keep 4.53e+05/6.73e+06 =  6% of the original kernel matrix.

torch.Size([8125, 2])
We keep 1.41e+06/3.91e+07 =  3% of the original kernel matrix.

torch.Size([523, 2])
We keep 1.86e+04/8.64e+04 = 21% of the original kernel matrix.

torch.Size([3860, 2])
We keep 3.36e+05/4.43e+06 =  7% of the original kernel matrix.

torch.Size([504, 2])
We keep 5.66e+03/3.31e+04 = 17% of the original kernel matrix.

torch.Size([4012, 2])
We keep 2.29e+05/2.74e+06 =  8% of the original kernel matrix.

torch.Size([526, 2])
We keep 6.52e+03/3.69e+04 = 17% of the original kernel matrix.

torch.Size([3828, 2])
We keep 2.35e+05/2.90e+06 =  8% of the original kernel matrix.

torch.Size([724, 2])
We keep 1.68e+04/1.07e+05 = 15% of the original kernel matrix.

torch.Size([4223, 2])
We keep 3.38e+05/4.93e+06 =  6% of the original kernel matrix.

torch.Size([1501, 2])
We keep 5.79e+04/5.75e+05 = 10% of the original kernel matrix.

torch.Size([5565, 2])
We keep 5.94e+05/1.14e+07 =  5% of the original kernel matrix.

torch.Size([1411, 2])
We keep 4.03e+04/3.43e+05 = 11% of the original kernel matrix.

torch.Size([5636, 2])
We keep 4.98e+05/8.84e+06 =  5% of the original kernel matrix.

torch.Size([570, 2])
We keep 1.21e+04/6.92e+04 = 17% of the original kernel matrix.

torch.Size([3949, 2])
We keep 3.08e+05/3.97e+06 =  7% of the original kernel matrix.

torch.Size([170, 2])
We keep 1.19e+03/3.97e+03 = 30% of the original kernel matrix.

torch.Size([2714, 2])
We keep 1.21e+05/9.50e+05 = 12% of the original kernel matrix.

torch.Size([275, 2])
We keep 3.65e+03/1.88e+04 = 19% of the original kernel matrix.

torch.Size([3115, 2])
We keep 1.72e+05/2.07e+06 =  8% of the original kernel matrix.

torch.Size([1713, 2])
We keep 1.19e+05/1.23e+06 =  9% of the original kernel matrix.

torch.Size([5555, 2])
We keep 7.71e+05/1.67e+07 =  4% of the original kernel matrix.

torch.Size([1181, 2])
We keep 3.35e+04/2.78e+05 = 12% of the original kernel matrix.

torch.Size([5308, 2])
We keep 4.87e+05/7.95e+06 =  6% of the original kernel matrix.

torch.Size([133, 2])
We keep 1.30e+03/3.02e+03 = 43% of the original kernel matrix.

torch.Size([2451, 2])
We keep 1.22e+05/8.29e+05 = 14% of the original kernel matrix.

torch.Size([173, 2])
We keep 8.58e+02/2.92e+03 = 29% of the original kernel matrix.

torch.Size([2719, 2])
We keep 1.01e+05/8.14e+05 = 12% of the original kernel matrix.

torch.Size([5295, 2])
We keep 3.76e+05/6.56e+06 =  5% of the original kernel matrix.

torch.Size([9604, 2])
We keep 1.39e+06/3.86e+07 =  3% of the original kernel matrix.

torch.Size([330, 2])
We keep 1.72e+03/7.40e+03 = 23% of the original kernel matrix.

torch.Size([3434, 2])
We keep 1.37e+05/1.30e+06 = 10% of the original kernel matrix.

torch.Size([613, 2])
We keep 8.37e+03/5.02e+04 = 16% of the original kernel matrix.

torch.Size([4166, 2])
We keep 2.59e+05/3.38e+06 =  7% of the original kernel matrix.

torch.Size([467, 2])
We keep 6.19e+03/3.17e+04 = 19% of the original kernel matrix.

torch.Size([3795, 2])
We keep 2.03e+05/2.68e+06 =  7% of the original kernel matrix.

torch.Size([454, 2])
We keep 6.16e+03/3.28e+04 = 18% of the original kernel matrix.

torch.Size([3619, 2])
We keep 2.23e+05/2.73e+06 =  8% of the original kernel matrix.

torch.Size([505, 2])
We keep 7.65e+03/4.12e+04 = 18% of the original kernel matrix.

torch.Size([3597, 2])
We keep 2.40e+05/3.06e+06 =  7% of the original kernel matrix.

torch.Size([272, 2])
We keep 1.22e+03/4.49e+03 = 27% of the original kernel matrix.

torch.Size([3279, 2])
We keep 1.24e+05/1.01e+06 = 12% of the original kernel matrix.

torch.Size([907, 2])
We keep 2.96e+04/2.10e+05 = 14% of the original kernel matrix.

torch.Size([4667, 2])
We keep 4.35e+05/6.91e+06 =  6% of the original kernel matrix.

torch.Size([1312, 2])
We keep 3.81e+04/3.45e+05 = 11% of the original kernel matrix.

torch.Size([5233, 2])
We keep 4.88e+05/8.85e+06 =  5% of the original kernel matrix.

torch.Size([536, 2])
We keep 8.30e+03/5.52e+04 = 15% of the original kernel matrix.

torch.Size([3991, 2])
We keep 2.72e+05/3.54e+06 =  7% of the original kernel matrix.

torch.Size([3838, 2])
We keep 5.81e+05/8.49e+06 =  6% of the original kernel matrix.

torch.Size([7843, 2])
We keep 1.50e+06/4.39e+07 =  3% of the original kernel matrix.

torch.Size([388, 2])
We keep 2.59e+03/1.21e+04 = 21% of the original kernel matrix.

torch.Size([3531, 2])
We keep 1.61e+05/1.66e+06 =  9% of the original kernel matrix.

torch.Size([1311, 2])
We keep 2.92e+04/2.76e+05 = 10% of the original kernel matrix.

torch.Size([5457, 2])
We keep 4.52e+05/7.92e+06 =  5% of the original kernel matrix.

torch.Size([191, 2])
We keep 7.34e+02/2.30e+03 = 31% of the original kernel matrix.

torch.Size([2735, 2])
We keep 9.52e+04/7.24e+05 = 13% of the original kernel matrix.

torch.Size([4009, 2])
We keep 4.98e+05/7.27e+06 =  6% of the original kernel matrix.

torch.Size([8407, 2])
We keep 1.57e+06/4.07e+07 =  3% of the original kernel matrix.

torch.Size([1788, 2])
We keep 4.92e+04/4.86e+05 = 10% of the original kernel matrix.

torch.Size([6110, 2])
We keep 5.60e+05/1.05e+07 =  5% of the original kernel matrix.

torch.Size([960, 2])
We keep 1.82e+04/1.43e+05 = 12% of the original kernel matrix.

torch.Size([4837, 2])
We keep 3.67e+05/5.70e+06 =  6% of the original kernel matrix.

torch.Size([568, 2])
We keep 1.11e+04/5.71e+04 = 19% of the original kernel matrix.

torch.Size([3825, 2])
We keep 2.78e+05/3.60e+06 =  7% of the original kernel matrix.

torch.Size([399, 2])
We keep 4.46e+03/2.40e+04 = 18% of the original kernel matrix.

torch.Size([3677, 2])
We keep 2.04e+05/2.34e+06 =  8% of the original kernel matrix.

torch.Size([138, 2])
We keep 1.16e+03/3.72e+03 = 31% of the original kernel matrix.

torch.Size([2345, 2])
We keep 1.16e+05/9.20e+05 = 12% of the original kernel matrix.

torch.Size([64, 2])
We keep 3.36e+02/9.00e+02 = 37% of the original kernel matrix.

torch.Size([2038, 2])
We keep 7.15e+04/4.52e+05 = 15% of the original kernel matrix.

torch.Size([870, 2])
We keep 1.06e+04/7.34e+04 = 14% of the original kernel matrix.

torch.Size([4777, 2])
We keep 2.93e+05/4.09e+06 =  7% of the original kernel matrix.

torch.Size([239, 2])
We keep 1.71e+03/5.62e+03 = 30% of the original kernel matrix.

torch.Size([3022, 2])
We keep 1.34e+05/1.13e+06 = 11% of the original kernel matrix.

torch.Size([1331, 2])
We keep 3.50e+04/3.53e+05 =  9% of the original kernel matrix.

torch.Size([5410, 2])
We keep 4.88e+05/8.96e+06 =  5% of the original kernel matrix.

torch.Size([2375, 2])
We keep 1.35e+05/1.70e+06 =  7% of the original kernel matrix.

torch.Size([6595, 2])
We keep 8.34e+05/1.97e+07 =  4% of the original kernel matrix.

torch.Size([557, 2])
We keep 8.34e+03/5.62e+04 = 14% of the original kernel matrix.

torch.Size([3917, 2])
We keep 2.60e+05/3.57e+06 =  7% of the original kernel matrix.

torch.Size([156, 2])
We keep 1.40e+03/4.49e+03 = 31% of the original kernel matrix.

torch.Size([2474, 2])
We keep 1.25e+05/1.01e+06 = 12% of the original kernel matrix.

torch.Size([3376, 2])
We keep 3.16e+05/3.89e+06 =  8% of the original kernel matrix.

torch.Size([7768, 2])
We keep 1.18e+06/2.98e+07 =  3% of the original kernel matrix.

torch.Size([1049, 2])
We keep 2.69e+04/2.17e+05 = 12% of the original kernel matrix.

torch.Size([4907, 2])
We keep 4.24e+05/7.03e+06 =  6% of the original kernel matrix.

torch.Size([1408, 2])
We keep 3.70e+04/3.67e+05 = 10% of the original kernel matrix.

torch.Size([5515, 2])
We keep 5.09e+05/9.14e+06 =  5% of the original kernel matrix.

torch.Size([558, 2])
We keep 7.24e+03/3.46e+04 = 20% of the original kernel matrix.

torch.Size([4049, 2])
We keep 2.45e+05/2.81e+06 =  8% of the original kernel matrix.

torch.Size([184, 2])
We keep 9.97e+02/3.48e+03 = 28% of the original kernel matrix.

torch.Size([2679, 2])
We keep 1.10e+05/8.90e+05 = 12% of the original kernel matrix.

torch.Size([146, 2])
We keep 9.78e+02/2.92e+03 = 33% of the original kernel matrix.

torch.Size([2437, 2])
We keep 1.09e+05/8.14e+05 = 13% of the original kernel matrix.

torch.Size([203, 2])
We keep 1.64e+03/6.40e+03 = 25% of the original kernel matrix.

torch.Size([2758, 2])
We keep 1.35e+05/1.21e+06 = 11% of the original kernel matrix.

torch.Size([769, 2])
We keep 1.36e+04/8.94e+04 = 15% of the original kernel matrix.

torch.Size([4424, 2])
We keep 3.10e+05/4.51e+06 =  6% of the original kernel matrix.

torch.Size([43, 2])
We keep 2.11e+02/5.29e+02 = 39% of the original kernel matrix.

torch.Size([1609, 2])
We keep 5.17e+04/3.47e+05 = 14% of the original kernel matrix.

torch.Size([740, 2])
We keep 1.59e+04/8.88e+04 = 17% of the original kernel matrix.

torch.Size([4343, 2])
We keep 3.31e+05/4.49e+06 =  7% of the original kernel matrix.

torch.Size([139, 2])
We keep 1.02e+03/3.48e+03 = 29% of the original kernel matrix.

torch.Size([2558, 2])
We keep 1.18e+05/8.90e+05 = 13% of the original kernel matrix.

torch.Size([595, 2])
We keep 6.87e+03/3.57e+04 = 19% of the original kernel matrix.

torch.Size([4264, 2])
We keep 2.37e+05/2.85e+06 =  8% of the original kernel matrix.

torch.Size([1016, 2])
We keep 2.20e+04/1.62e+05 = 13% of the original kernel matrix.

torch.Size([4847, 2])
We keep 3.87e+05/6.06e+06 =  6% of the original kernel matrix.

torch.Size([769, 2])
We keep 1.10e+04/8.24e+04 = 13% of the original kernel matrix.

torch.Size([4461, 2])
We keep 3.00e+05/4.33e+06 =  6% of the original kernel matrix.

torch.Size([270, 2])
We keep 2.22e+03/8.28e+03 = 26% of the original kernel matrix.

torch.Size([3014, 2])
We keep 1.50e+05/1.37e+06 = 10% of the original kernel matrix.

torch.Size([449, 2])
We keep 5.82e+03/2.22e+04 = 26% of the original kernel matrix.

torch.Size([3616, 2])
We keep 2.13e+05/2.25e+06 =  9% of the original kernel matrix.

torch.Size([1108, 2])
We keep 3.43e+04/2.70e+05 = 12% of the original kernel matrix.

torch.Size([4865, 2])
We keep 4.50e+05/7.84e+06 =  5% of the original kernel matrix.

torch.Size([450, 2])
We keep 7.74e+03/4.00e+04 = 19% of the original kernel matrix.

torch.Size([3660, 2])
We keep 2.51e+05/3.02e+06 =  8% of the original kernel matrix.

torch.Size([352, 2])
We keep 3.60e+03/1.54e+04 = 23% of the original kernel matrix.

torch.Size([3379, 2])
We keep 1.88e+05/1.87e+06 = 10% of the original kernel matrix.

torch.Size([4873, 2])
We keep 1.10e+06/1.91e+07 =  5% of the original kernel matrix.

torch.Size([8944, 2])
We keep 2.08e+06/6.59e+07 =  3% of the original kernel matrix.

torch.Size([270, 2])
We keep 2.88e+03/1.25e+04 = 22% of the original kernel matrix.

torch.Size([3185, 2])
We keep 1.72e+05/1.69e+06 = 10% of the original kernel matrix.

torch.Size([638, 2])
We keep 7.51e+03/4.67e+04 = 16% of the original kernel matrix.

torch.Size([4197, 2])
We keep 2.49e+05/3.26e+06 =  7% of the original kernel matrix.

torch.Size([696, 2])
We keep 1.24e+04/7.18e+04 = 17% of the original kernel matrix.

torch.Size([4318, 2])
We keep 3.03e+05/4.04e+06 =  7% of the original kernel matrix.

torch.Size([626, 2])
We keep 1.56e+05/3.35e+05 = 46% of the original kernel matrix.

torch.Size([3534, 2])
We keep 4.28e+05/8.73e+06 =  4% of the original kernel matrix.

torch.Size([422, 2])
We keep 7.46e+03/3.65e+04 = 20% of the original kernel matrix.

torch.Size([3493, 2])
We keep 2.50e+05/2.88e+06 =  8% of the original kernel matrix.

torch.Size([888, 2])
We keep 2.48e+04/2.12e+05 = 11% of the original kernel matrix.

torch.Size([4915, 2])
We keep 4.41e+05/6.94e+06 =  6% of the original kernel matrix.

torch.Size([772, 2])
We keep 1.21e+04/9.30e+04 = 12% of the original kernel matrix.

torch.Size([4455, 2])
We keep 3.13e+05/4.60e+06 =  6% of the original kernel matrix.

torch.Size([624, 2])
We keep 6.85e+03/4.88e+04 = 14% of the original kernel matrix.

torch.Size([4134, 2])
We keep 2.46e+05/3.33e+06 =  7% of the original kernel matrix.

torch.Size([284, 2])
We keep 2.39e+03/7.74e+03 = 30% of the original kernel matrix.

torch.Size([3154, 2])
We keep 1.48e+05/1.33e+06 = 11% of the original kernel matrix.

torch.Size([1978, 2])
We keep 1.03e+05/1.20e+06 =  8% of the original kernel matrix.

torch.Size([6034, 2])
We keep 7.37e+05/1.65e+07 =  4% of the original kernel matrix.

torch.Size([1601, 2])
We keep 5.52e+04/5.54e+05 =  9% of the original kernel matrix.

torch.Size([5815, 2])
We keep 5.74e+05/1.12e+07 =  5% of the original kernel matrix.

torch.Size([394, 2])
We keep 4.64e+03/1.96e+04 = 23% of the original kernel matrix.

torch.Size([3422, 2])
We keep 1.89e+05/2.11e+06 =  8% of the original kernel matrix.

torch.Size([286, 2])
We keep 3.10e+03/1.54e+04 = 20% of the original kernel matrix.

torch.Size([3085, 2])
We keep 1.68e+05/1.87e+06 =  9% of the original kernel matrix.

torch.Size([1046, 2])
We keep 5.83e+04/2.93e+05 = 19% of the original kernel matrix.

torch.Size([4738, 2])
We keep 4.63e+05/8.16e+06 =  5% of the original kernel matrix.

torch.Size([4424, 2])
We keep 3.03e+05/5.17e+06 =  5% of the original kernel matrix.

torch.Size([8661, 2])
We keep 1.25e+06/3.43e+07 =  3% of the original kernel matrix.

torch.Size([70, 2])
We keep 2.03e+02/3.61e+02 = 56% of the original kernel matrix.

torch.Size([1738, 2])
We keep 5.10e+04/2.87e+05 = 17% of the original kernel matrix.

torch.Size([346, 2])
We keep 2.52e+03/1.14e+04 = 21% of the original kernel matrix.

torch.Size([3458, 2])
We keep 1.65e+05/1.61e+06 = 10% of the original kernel matrix.

torch.Size([1389, 2])
We keep 5.04e+04/4.29e+05 = 11% of the original kernel matrix.

torch.Size([5421, 2])
We keep 5.40e+05/9.88e+06 =  5% of the original kernel matrix.

torch.Size([1071, 2])
We keep 2.74e+04/2.17e+05 = 12% of the original kernel matrix.

torch.Size([5198, 2])
We keep 4.42e+05/7.03e+06 =  6% of the original kernel matrix.

torch.Size([439, 2])
We keep 4.92e+03/2.66e+04 = 18% of the original kernel matrix.

torch.Size([3724, 2])
We keep 2.04e+05/2.46e+06 =  8% of the original kernel matrix.

torch.Size([503, 2])
We keep 3.91e+03/2.04e+04 = 19% of the original kernel matrix.

torch.Size([4005, 2])
We keep 1.91e+05/2.16e+06 =  8% of the original kernel matrix.

torch.Size([280, 2])
We keep 1.84e+03/6.56e+03 = 28% of the original kernel matrix.

torch.Size([3244, 2])
We keep 1.36e+05/1.22e+06 = 11% of the original kernel matrix.

torch.Size([347, 2])
We keep 2.54e+03/1.17e+04 = 21% of the original kernel matrix.

torch.Size([3524, 2])
We keep 1.62e+05/1.63e+06 =  9% of the original kernel matrix.

torch.Size([1156, 2])
We keep 2.42e+04/1.90e+05 = 12% of the original kernel matrix.

torch.Size([5110, 2])
We keep 3.97e+05/6.58e+06 =  6% of the original kernel matrix.

torch.Size([278, 2])
We keep 1.30e+03/4.90e+03 = 26% of the original kernel matrix.

torch.Size([3334, 2])
We keep 1.24e+05/1.06e+06 = 11% of the original kernel matrix.

torch.Size([719, 2])
We keep 8.99e+03/5.38e+04 = 16% of the original kernel matrix.

torch.Size([4593, 2])
We keep 2.73e+05/3.50e+06 =  7% of the original kernel matrix.

torch.Size([187, 2])
We keep 1.01e+03/3.72e+03 = 27% of the original kernel matrix.

torch.Size([2838, 2])
We keep 1.13e+05/9.20e+05 = 12% of the original kernel matrix.

torch.Size([1006, 2])
We keep 2.15e+04/1.58e+05 = 13% of the original kernel matrix.

torch.Size([5075, 2])
We keep 3.94e+05/6.00e+06 =  6% of the original kernel matrix.

torch.Size([348, 2])
We keep 2.41e+03/1.21e+04 = 19% of the original kernel matrix.

torch.Size([3543, 2])
We keep 1.65e+05/1.66e+06 =  9% of the original kernel matrix.

torch.Size([345, 2])
We keep 3.90e+03/1.51e+04 = 25% of the original kernel matrix.

torch.Size([3185, 2])
We keep 1.71e+05/1.85e+06 =  9% of the original kernel matrix.

torch.Size([435, 2])
We keep 8.58e+03/4.41e+04 = 19% of the original kernel matrix.

torch.Size([3440, 2])
We keep 2.50e+05/3.17e+06 =  7% of the original kernel matrix.

torch.Size([762, 2])
We keep 9.79e+03/7.18e+04 = 13% of the original kernel matrix.

torch.Size([4562, 2])
We keep 2.82e+05/4.04e+06 =  6% of the original kernel matrix.

torch.Size([682, 2])
We keep 1.02e+04/6.66e+04 = 15% of the original kernel matrix.

torch.Size([4163, 2])
We keep 2.73e+05/3.89e+06 =  7% of the original kernel matrix.

torch.Size([1480, 2])
We keep 3.04e+04/2.96e+05 = 10% of the original kernel matrix.

torch.Size([5745, 2])
We keep 4.56e+05/8.20e+06 =  5% of the original kernel matrix.

torch.Size([252, 2])
We keep 1.62e+03/7.40e+03 = 21% of the original kernel matrix.

torch.Size([3113, 2])
We keep 1.38e+05/1.30e+06 = 10% of the original kernel matrix.

torch.Size([931, 2])
We keep 1.70e+04/1.36e+05 = 12% of the original kernel matrix.

torch.Size([4754, 2])
We keep 3.56e+05/5.56e+06 =  6% of the original kernel matrix.

torch.Size([1350, 2])
We keep 2.82e+05/7.45e+05 = 37% of the original kernel matrix.

torch.Size([5064, 2])
We keep 6.10e+05/1.30e+07 =  4% of the original kernel matrix.

torch.Size([236, 2])
We keep 1.35e+03/4.62e+03 = 29% of the original kernel matrix.

torch.Size([3147, 2])
We keep 1.29e+05/1.03e+06 = 12% of the original kernel matrix.

torch.Size([868, 2])
We keep 3.65e+04/2.21e+05 = 16% of the original kernel matrix.

torch.Size([4122, 2])
We keep 4.30e+05/7.09e+06 =  6% of the original kernel matrix.

torch.Size([718, 2])
We keep 1.04e+04/8.07e+04 = 12% of the original kernel matrix.

torch.Size([4239, 2])
We keep 2.97e+05/4.28e+06 =  6% of the original kernel matrix.

torch.Size([223, 2])
We keep 9.08e+02/2.92e+03 = 31% of the original kernel matrix.

torch.Size([3124, 2])
We keep 1.08e+05/8.14e+05 = 13% of the original kernel matrix.

torch.Size([178, 2])
We keep 9.00e+02/2.30e+03 = 39% of the original kernel matrix.

torch.Size([2510, 2])
We keep 1.05e+05/7.24e+05 = 14% of the original kernel matrix.

torch.Size([510, 2])
We keep 1.80e+04/6.60e+04 = 27% of the original kernel matrix.

torch.Size([3594, 2])
We keep 2.80e+05/3.88e+06 =  7% of the original kernel matrix.

torch.Size([7952, 2])
We keep 2.27e+06/4.35e+07 =  5% of the original kernel matrix.

torch.Size([10996, 2])
We keep 2.86e+06/9.95e+07 =  2% of the original kernel matrix.

torch.Size([253, 2])
We keep 1.42e+03/4.62e+03 = 30% of the original kernel matrix.

torch.Size([3089, 2])
We keep 1.25e+05/1.03e+06 = 12% of the original kernel matrix.

torch.Size([109, 2])
We keep 4.78e+02/1.16e+03 = 41% of the original kernel matrix.

torch.Size([2365, 2])
We keep 8.46e+04/5.13e+05 = 16% of the original kernel matrix.

torch.Size([136, 2])
We keep 7.22e+02/2.30e+03 = 31% of the original kernel matrix.

torch.Size([2512, 2])
We keep 9.94e+04/7.24e+05 = 13% of the original kernel matrix.

torch.Size([858, 2])
We keep 1.74e+04/1.18e+05 = 14% of the original kernel matrix.

torch.Size([4552, 2])
We keep 3.52e+05/5.17e+06 =  6% of the original kernel matrix.

torch.Size([553, 2])
We keep 6.07e+03/3.46e+04 = 17% of the original kernel matrix.

torch.Size([3911, 2])
We keep 2.27e+05/2.81e+06 =  8% of the original kernel matrix.

torch.Size([216, 2])
We keep 1.41e+03/4.22e+03 = 33% of the original kernel matrix.

torch.Size([2815, 2])
We keep 1.22e+05/9.80e+05 = 12% of the original kernel matrix.

torch.Size([359, 2])
We keep 3.38e+03/1.42e+04 = 23% of the original kernel matrix.

torch.Size([3433, 2])
We keep 1.80e+05/1.79e+06 = 10% of the original kernel matrix.

torch.Size([136, 2])
We keep 4.99e+02/1.37e+03 = 36% of the original kernel matrix.

torch.Size([2706, 2])
We keep 9.10e+04/5.58e+05 = 16% of the original kernel matrix.

torch.Size([210, 2])
We keep 1.23e+03/4.36e+03 = 28% of the original kernel matrix.

torch.Size([2761, 2])
We keep 1.20e+05/9.95e+05 = 12% of the original kernel matrix.

torch.Size([733, 2])
We keep 1.22e+04/8.12e+04 = 14% of the original kernel matrix.

torch.Size([4352, 2])
We keep 3.07e+05/4.30e+06 =  7% of the original kernel matrix.

torch.Size([687, 2])
We keep 9.80e+03/7.67e+04 = 12% of the original kernel matrix.

torch.Size([4315, 2])
We keep 2.78e+05/4.18e+06 =  6% of the original kernel matrix.

torch.Size([92, 2])
We keep 2.32e+02/6.76e+02 = 34% of the original kernel matrix.

torch.Size([2077, 2])
We keep 5.78e+04/3.92e+05 = 14% of the original kernel matrix.

torch.Size([667, 2])
We keep 1.19e+04/8.18e+04 = 14% of the original kernel matrix.

torch.Size([4263, 2])
We keep 2.99e+05/4.31e+06 =  6% of the original kernel matrix.

torch.Size([108, 2])
We keep 5.15e+02/1.52e+03 = 33% of the original kernel matrix.

torch.Size([2312, 2])
We keep 8.74e+04/5.88e+05 = 14% of the original kernel matrix.

torch.Size([323, 2])
We keep 5.14e+03/2.16e+04 = 23% of the original kernel matrix.

torch.Size([3282, 2])
We keep 1.89e+05/2.22e+06 =  8% of the original kernel matrix.

torch.Size([1000, 2])
We keep 1.98e+04/1.55e+05 = 12% of the original kernel matrix.

torch.Size([4821, 2])
We keep 3.71e+05/5.94e+06 =  6% of the original kernel matrix.

torch.Size([608, 2])
We keep 1.12e+04/7.18e+04 = 15% of the original kernel matrix.

torch.Size([4057, 2])
We keep 3.07e+05/4.04e+06 =  7% of the original kernel matrix.

torch.Size([1396, 2])
We keep 7.08e+04/7.45e+05 =  9% of the original kernel matrix.

torch.Size([5755, 2])
We keep 6.70e+05/1.30e+07 =  5% of the original kernel matrix.

torch.Size([1399, 2])
We keep 4.50e+04/4.24e+05 = 10% of the original kernel matrix.

torch.Size([5647, 2])
We keep 5.21e+05/9.82e+06 =  5% of the original kernel matrix.

torch.Size([455, 2])
We keep 7.37e+03/3.57e+04 = 20% of the original kernel matrix.

torch.Size([3782, 2])
We keep 2.41e+05/2.85e+06 =  8% of the original kernel matrix.

torch.Size([1972, 2])
We keep 6.55e+04/7.50e+05 =  8% of the original kernel matrix.

torch.Size([6379, 2])
We keep 6.40e+05/1.31e+07 =  4% of the original kernel matrix.

torch.Size([1508, 2])
We keep 2.38e+05/1.08e+06 = 22% of the original kernel matrix.

torch.Size([5265, 2])
We keep 7.11e+05/1.56e+07 =  4% of the original kernel matrix.

torch.Size([1189, 2])
We keep 6.01e+04/5.21e+05 = 11% of the original kernel matrix.

torch.Size([4952, 2])
We keep 5.78e+05/1.09e+07 =  5% of the original kernel matrix.

torch.Size([774, 2])
We keep 1.33e+04/9.18e+04 = 14% of the original kernel matrix.

torch.Size([4465, 2])
We keep 3.12e+05/4.57e+06 =  6% of the original kernel matrix.

torch.Size([270, 2])
We keep 1.44e+03/5.93e+03 = 24% of the original kernel matrix.

torch.Size([3245, 2])
We keep 1.29e+05/1.16e+06 = 11% of the original kernel matrix.

torch.Size([956, 2])
We keep 3.41e+04/2.74e+05 = 12% of the original kernel matrix.

torch.Size([4615, 2])
We keep 4.55e+05/7.89e+06 =  5% of the original kernel matrix.

torch.Size([2809, 2])
We keep 4.76e+05/2.62e+06 = 18% of the original kernel matrix.

torch.Size([7216, 2])
We keep 1.04e+06/2.44e+07 =  4% of the original kernel matrix.

torch.Size([2021, 2])
We keep 1.23e+05/1.14e+06 = 10% of the original kernel matrix.

torch.Size([6202, 2])
We keep 7.11e+05/1.61e+07 =  4% of the original kernel matrix.

torch.Size([1492, 2])
We keep 3.78e+04/3.48e+05 = 10% of the original kernel matrix.

torch.Size([5743, 2])
We keep 4.97e+05/8.90e+06 =  5% of the original kernel matrix.

torch.Size([3453, 2])
We keep 2.56e+05/4.02e+06 =  6% of the original kernel matrix.

torch.Size([7642, 2])
We keep 1.15e+06/3.02e+07 =  3% of the original kernel matrix.

torch.Size([8958, 2])
We keep 1.90e+06/3.76e+07 =  5% of the original kernel matrix.

torch.Size([11989, 2])
We keep 2.65e+06/9.25e+07 =  2% of the original kernel matrix.

torch.Size([640, 2])
We keep 1.80e+04/1.14e+05 = 15% of the original kernel matrix.

torch.Size([4287, 2])
We keep 3.78e+05/5.08e+06 =  7% of the original kernel matrix.

torch.Size([1443, 2])
We keep 1.13e+05/7.66e+05 = 14% of the original kernel matrix.

torch.Size([5381, 2])
We keep 6.57e+05/1.32e+07 =  4% of the original kernel matrix.

torch.Size([860, 2])
We keep 2.71e+04/1.87e+05 = 14% of the original kernel matrix.

torch.Size([4307, 2])
We keep 4.00e+05/6.53e+06 =  6% of the original kernel matrix.

torch.Size([530, 2])
We keep 8.34e+03/4.08e+04 = 20% of the original kernel matrix.

torch.Size([3794, 2])
We keep 2.45e+05/3.05e+06 =  8% of the original kernel matrix.

torch.Size([18828, 2])
We keep 8.57e+06/2.52e+08 =  3% of the original kernel matrix.

torch.Size([18033, 2])
We keep 5.79e+06/2.40e+08 =  2% of the original kernel matrix.

torch.Size([26849, 2])
We keep 9.50e+06/3.60e+08 =  2% of the original kernel matrix.

torch.Size([21846, 2])
We keep 6.54e+06/2.86e+08 =  2% of the original kernel matrix.

torch.Size([457, 2])
We keep 5.05e+03/2.82e+04 = 17% of the original kernel matrix.

torch.Size([3765, 2])
We keep 1.97e+05/2.53e+06 =  7% of the original kernel matrix.

torch.Size([12835, 2])
We keep 2.07e+06/5.99e+07 =  3% of the original kernel matrix.

torch.Size([14797, 2])
We keep 3.23e+06/1.17e+08 =  2% of the original kernel matrix.

torch.Size([20268, 2])
We keep 1.80e+07/3.27e+08 =  5% of the original kernel matrix.

torch.Size([18003, 2])
We keep 6.47e+06/2.73e+08 =  2% of the original kernel matrix.

torch.Size([3343, 2])
We keep 2.76e+05/3.54e+06 =  7% of the original kernel matrix.

torch.Size([7784, 2])
We keep 1.10e+06/2.84e+07 =  3% of the original kernel matrix.

torch.Size([3877, 2])
We keep 1.33e+06/1.05e+07 = 12% of the original kernel matrix.

torch.Size([7783, 2])
We keep 1.66e+06/4.88e+07 =  3% of the original kernel matrix.

torch.Size([4828, 2])
We keep 1.24e+06/1.31e+07 =  9% of the original kernel matrix.

torch.Size([8580, 2])
We keep 1.91e+06/5.45e+07 =  3% of the original kernel matrix.

torch.Size([4614, 2])
We keep 6.29e+05/6.72e+06 =  9% of the original kernel matrix.

torch.Size([9147, 2])
We keep 1.26e+06/3.91e+07 =  3% of the original kernel matrix.

torch.Size([4484, 2])
We keep 4.28e+05/5.19e+06 =  8% of the original kernel matrix.

torch.Size([9049, 2])
We keep 1.22e+06/3.44e+07 =  3% of the original kernel matrix.

torch.Size([230, 2])
We keep 2.22e+03/8.10e+03 = 27% of the original kernel matrix.

torch.Size([2827, 2])
We keep 1.51e+05/1.36e+06 = 11% of the original kernel matrix.

torch.Size([1247, 2])
We keep 1.07e+05/4.21e+05 = 25% of the original kernel matrix.

torch.Size([5163, 2])
We keep 4.89e+05/9.79e+06 =  4% of the original kernel matrix.

torch.Size([483, 2])
We keep 8.27e+03/3.35e+04 = 24% of the original kernel matrix.

torch.Size([3537, 2])
We keep 2.18e+05/2.76e+06 =  7% of the original kernel matrix.

torch.Size([665, 2])
We keep 1.07e+04/7.24e+04 = 14% of the original kernel matrix.

torch.Size([4188, 2])
We keep 2.85e+05/4.06e+06 =  7% of the original kernel matrix.

torch.Size([1609, 2])
We keep 6.17e+04/4.86e+05 = 12% of the original kernel matrix.

torch.Size([5824, 2])
We keep 5.45e+05/1.05e+07 =  5% of the original kernel matrix.

torch.Size([2229, 2])
We keep 1.41e+05/1.60e+06 =  8% of the original kernel matrix.

torch.Size([6400, 2])
We keep 8.18e+05/1.91e+07 =  4% of the original kernel matrix.

torch.Size([11283, 2])
We keep 2.79e+06/5.66e+07 =  4% of the original kernel matrix.

torch.Size([13698, 2])
We keep 3.12e+06/1.13e+08 =  2% of the original kernel matrix.

torch.Size([1092, 2])
We keep 3.63e+04/3.14e+05 = 11% of the original kernel matrix.

torch.Size([5395, 2])
We keep 5.06e+05/8.45e+06 =  5% of the original kernel matrix.

torch.Size([762, 2])
We keep 1.12e+04/7.29e+04 = 15% of the original kernel matrix.

torch.Size([4392, 2])
We keep 2.93e+05/4.07e+06 =  7% of the original kernel matrix.

torch.Size([784, 2])
We keep 1.51e+04/1.02e+05 = 14% of the original kernel matrix.

torch.Size([4209, 2])
We keep 3.20e+05/4.81e+06 =  6% of the original kernel matrix.

torch.Size([2734, 2])
We keep 1.68e+05/2.02e+06 =  8% of the original kernel matrix.

torch.Size([7129, 2])
We keep 9.23e+05/2.14e+07 =  4% of the original kernel matrix.

torch.Size([2106, 2])
We keep 1.09e+05/1.16e+06 =  9% of the original kernel matrix.

torch.Size([6485, 2])
We keep 7.63e+05/1.62e+07 =  4% of the original kernel matrix.

torch.Size([21302, 2])
We keep 5.73e+06/1.98e+08 =  2% of the original kernel matrix.

torch.Size([19493, 2])
We keep 4.89e+06/2.12e+08 =  2% of the original kernel matrix.

torch.Size([16365, 2])
We keep 1.57e+07/2.92e+08 =  5% of the original kernel matrix.

torch.Size([15900, 2])
We keep 6.16e+06/2.58e+08 =  2% of the original kernel matrix.

torch.Size([1426, 2])
We keep 5.21e+04/4.54e+05 = 11% of the original kernel matrix.

torch.Size([5426, 2])
We keep 5.44e+05/1.02e+07 =  5% of the original kernel matrix.

torch.Size([53, 2])
We keep 9.30e+01/2.25e+02 = 41% of the original kernel matrix.

torch.Size([1867, 2])
We keep 4.21e+04/2.26e+05 = 18% of the original kernel matrix.

torch.Size([195, 2])
We keep 1.49e+03/5.18e+03 = 28% of the original kernel matrix.

torch.Size([2757, 2])
We keep 1.35e+05/1.09e+06 = 12% of the original kernel matrix.

torch.Size([9399, 2])
We keep 1.37e+06/3.42e+07 =  4% of the original kernel matrix.

torch.Size([12381, 2])
We keep 2.53e+06/8.81e+07 =  2% of the original kernel matrix.

torch.Size([7000, 2])
We keep 1.63e+06/2.00e+07 =  8% of the original kernel matrix.

torch.Size([10628, 2])
We keep 2.10e+06/6.74e+07 =  3% of the original kernel matrix.

torch.Size([1858, 2])
We keep 2.01e+05/1.06e+06 = 18% of the original kernel matrix.

torch.Size([5829, 2])
We keep 7.23e+05/1.55e+07 =  4% of the original kernel matrix.

torch.Size([206, 2])
We keep 1.32e+03/4.49e+03 = 29% of the original kernel matrix.

torch.Size([2800, 2])
We keep 1.25e+05/1.01e+06 = 12% of the original kernel matrix.

torch.Size([13403, 2])
We keep 3.57e+06/1.06e+08 =  3% of the original kernel matrix.

torch.Size([15092, 2])
We keep 3.95e+06/1.56e+08 =  2% of the original kernel matrix.

torch.Size([801, 2])
We keep 3.07e+04/2.60e+05 = 11% of the original kernel matrix.

torch.Size([4592, 2])
We keep 4.68e+05/7.69e+06 =  6% of the original kernel matrix.

torch.Size([1234, 2])
We keep 2.25e+04/2.14e+05 = 10% of the original kernel matrix.

torch.Size([5456, 2])
We keep 4.11e+05/6.98e+06 =  5% of the original kernel matrix.

torch.Size([54629, 2])
We keep 3.39e+07/1.46e+09 =  2% of the original kernel matrix.

torch.Size([30736, 2])
We keep 1.19e+07/5.76e+08 =  2% of the original kernel matrix.

torch.Size([1307, 2])
We keep 3.88e+04/3.62e+05 = 10% of the original kernel matrix.

torch.Size([5254, 2])
We keep 4.94e+05/9.08e+06 =  5% of the original kernel matrix.

torch.Size([3032, 2])
We keep 1.79e+05/2.30e+06 =  7% of the original kernel matrix.

torch.Size([7421, 2])
We keep 9.39e+05/2.29e+07 =  4% of the original kernel matrix.

torch.Size([22854, 2])
We keep 1.49e+07/3.99e+08 =  3% of the original kernel matrix.

torch.Size([19999, 2])
We keep 6.85e+06/3.01e+08 =  2% of the original kernel matrix.

torch.Size([63254, 2])
We keep 7.16e+07/3.09e+09 =  2% of the original kernel matrix.

torch.Size([32840, 2])
We keep 1.68e+07/8.39e+08 =  2% of the original kernel matrix.

torch.Size([2322, 2])
We keep 1.44e+05/1.67e+06 =  8% of the original kernel matrix.

torch.Size([6534, 2])
We keep 8.44e+05/1.95e+07 =  4% of the original kernel matrix.

torch.Size([1395, 2])
We keep 3.56e+04/3.35e+05 = 10% of the original kernel matrix.

torch.Size([5411, 2])
We keep 4.80e+05/8.73e+06 =  5% of the original kernel matrix.

torch.Size([388, 2])
We keep 6.67e+03/2.56e+04 = 26% of the original kernel matrix.

torch.Size([3437, 2])
We keep 2.20e+05/2.41e+06 =  9% of the original kernel matrix.

torch.Size([516, 2])
We keep 6.62e+03/2.96e+04 = 22% of the original kernel matrix.

torch.Size([3860, 2])
We keep 2.27e+05/2.59e+06 =  8% of the original kernel matrix.

torch.Size([9113, 2])
We keep 2.29e+06/4.05e+07 =  5% of the original kernel matrix.

torch.Size([12414, 2])
We keep 2.63e+06/9.59e+07 =  2% of the original kernel matrix.

torch.Size([417, 2])
We keep 3.61e+03/1.49e+04 = 24% of the original kernel matrix.

torch.Size([3726, 2])
We keep 1.87e+05/1.84e+06 = 10% of the original kernel matrix.

torch.Size([455, 2])
We keep 4.77e+03/2.79e+04 = 17% of the original kernel matrix.

torch.Size([3792, 2])
We keep 2.11e+05/2.52e+06 =  8% of the original kernel matrix.

torch.Size([3282, 2])
We keep 9.37e+05/6.23e+06 = 15% of the original kernel matrix.

torch.Size([7724, 2])
We keep 1.13e+06/3.76e+07 =  2% of the original kernel matrix.

torch.Size([5406, 2])
We keep 4.59e+05/7.78e+06 =  5% of the original kernel matrix.

torch.Size([9726, 2])
We keep 1.41e+06/4.21e+07 =  3% of the original kernel matrix.

torch.Size([345, 2])
We keep 4.38e+03/1.85e+04 = 23% of the original kernel matrix.

torch.Size([3473, 2])
We keep 2.08e+05/2.05e+06 = 10% of the original kernel matrix.

torch.Size([899, 2])
We keep 3.74e+04/2.78e+05 = 13% of the original kernel matrix.

torch.Size([5107, 2])
We keep 5.11e+05/7.95e+06 =  6% of the original kernel matrix.

torch.Size([64576, 2])
We keep 1.51e+08/5.27e+09 =  2% of the original kernel matrix.

torch.Size([31736, 2])
We keep 2.09e+07/1.10e+09 =  1% of the original kernel matrix.

torch.Size([1498, 2])
We keep 6.37e+04/5.91e+05 = 10% of the original kernel matrix.

torch.Size([5625, 2])
We keep 6.02e+05/1.16e+07 =  5% of the original kernel matrix.

torch.Size([1037, 2])
We keep 3.12e+04/1.97e+05 = 15% of the original kernel matrix.

torch.Size([4939, 2])
We keep 4.06e+05/6.70e+06 =  6% of the original kernel matrix.

torch.Size([2036, 2])
We keep 8.59e+04/9.45e+05 =  9% of the original kernel matrix.

torch.Size([6370, 2])
We keep 6.97e+05/1.47e+07 =  4% of the original kernel matrix.

torch.Size([23805, 2])
We keep 3.16e+07/5.57e+08 =  5% of the original kernel matrix.

torch.Size([19338, 2])
We keep 7.67e+06/3.56e+08 =  2% of the original kernel matrix.

torch.Size([754, 2])
We keep 2.00e+04/1.18e+05 = 16% of the original kernel matrix.

torch.Size([4304, 2])
We keep 3.40e+05/5.17e+06 =  6% of the original kernel matrix.

torch.Size([692, 2])
We keep 1.97e+04/9.24e+04 = 21% of the original kernel matrix.

torch.Size([4198, 2])
We keep 3.29e+05/4.58e+06 =  7% of the original kernel matrix.

torch.Size([701, 2])
We keep 1.42e+04/9.00e+04 = 15% of the original kernel matrix.

torch.Size([4207, 2])
We keep 3.12e+05/4.52e+06 =  6% of the original kernel matrix.

torch.Size([662, 2])
We keep 1.20e+04/7.02e+04 = 17% of the original kernel matrix.

torch.Size([4118, 2])
We keep 3.05e+05/4.00e+06 =  7% of the original kernel matrix.

torch.Size([1945, 2])
We keep 4.78e+05/1.98e+06 = 24% of the original kernel matrix.

torch.Size([6235, 2])
We keep 8.31e+05/2.12e+07 =  3% of the original kernel matrix.

torch.Size([4453, 2])
We keep 2.06e+06/2.45e+07 =  8% of the original kernel matrix.

torch.Size([8374, 2])
We keep 2.45e+06/7.46e+07 =  3% of the original kernel matrix.

torch.Size([10990, 2])
We keep 2.02e+06/5.84e+07 =  3% of the original kernel matrix.

torch.Size([13645, 2])
We keep 3.11e+06/1.15e+08 =  2% of the original kernel matrix.

torch.Size([483, 2])
We keep 8.74e+03/4.12e+04 = 21% of the original kernel matrix.

torch.Size([3863, 2])
We keep 2.58e+05/3.06e+06 =  8% of the original kernel matrix.

torch.Size([22625, 2])
We keep 5.59e+06/2.39e+08 =  2% of the original kernel matrix.

torch.Size([19994, 2])
We keep 5.53e+06/2.33e+08 =  2% of the original kernel matrix.

torch.Size([8628, 2])
We keep 1.23e+06/2.58e+07 =  4% of the original kernel matrix.

torch.Size([12073, 2])
We keep 2.25e+06/7.65e+07 =  2% of the original kernel matrix.

torch.Size([709, 2])
We keep 9.57e+03/6.35e+04 = 15% of the original kernel matrix.

torch.Size([4438, 2])
We keep 2.83e+05/3.80e+06 =  7% of the original kernel matrix.

torch.Size([13208, 2])
We keep 2.75e+06/6.83e+07 =  4% of the original kernel matrix.

torch.Size([15048, 2])
We keep 3.34e+06/1.25e+08 =  2% of the original kernel matrix.

torch.Size([12360, 2])
We keep 3.04e+06/9.31e+07 =  3% of the original kernel matrix.

torch.Size([14552, 2])
We keep 3.85e+06/1.46e+08 =  2% of the original kernel matrix.

torch.Size([2222, 2])
We keep 2.34e+05/2.23e+06 = 10% of the original kernel matrix.

torch.Size([6560, 2])
We keep 9.54e+05/2.25e+07 =  4% of the original kernel matrix.

torch.Size([3358, 2])
We keep 1.01e+06/7.17e+06 = 14% of the original kernel matrix.

torch.Size([7659, 2])
We keep 1.38e+06/4.04e+07 =  3% of the original kernel matrix.

torch.Size([515, 2])
We keep 7.01e+03/3.80e+04 = 18% of the original kernel matrix.

torch.Size([3868, 2])
We keep 2.26e+05/2.94e+06 =  7% of the original kernel matrix.

torch.Size([2612, 2])
We keep 1.05e+05/1.36e+06 =  7% of the original kernel matrix.

torch.Size([7159, 2])
We keep 7.91e+05/1.76e+07 =  4% of the original kernel matrix.

torch.Size([1070, 2])
We keep 4.32e+04/2.87e+05 = 15% of the original kernel matrix.

torch.Size([4947, 2])
We keep 4.43e+05/8.08e+06 =  5% of the original kernel matrix.

torch.Size([1679, 2])
We keep 5.03e+04/4.79e+05 = 10% of the original kernel matrix.

torch.Size([5996, 2])
We keep 5.41e+05/1.04e+07 =  5% of the original kernel matrix.

torch.Size([1108, 2])
We keep 2.04e+04/1.77e+05 = 11% of the original kernel matrix.

torch.Size([5228, 2])
We keep 3.86e+05/6.35e+06 =  6% of the original kernel matrix.

torch.Size([252, 2])
We keep 3.78e+03/1.77e+04 = 21% of the original kernel matrix.

torch.Size([2867, 2])
We keep 1.88e+05/2.01e+06 =  9% of the original kernel matrix.

torch.Size([2089, 2])
We keep 7.80e+04/9.02e+05 =  8% of the original kernel matrix.

torch.Size([6630, 2])
We keep 6.99e+05/1.43e+07 =  4% of the original kernel matrix.

torch.Size([4096, 2])
We keep 3.32e+05/4.92e+06 =  6% of the original kernel matrix.

torch.Size([8380, 2])
We keep 1.24e+06/3.34e+07 =  3% of the original kernel matrix.

torch.Size([3939, 2])
We keep 1.73e+06/1.25e+07 = 13% of the original kernel matrix.

torch.Size([7272, 2])
We keep 1.79e+06/5.33e+07 =  3% of the original kernel matrix.

torch.Size([649, 2])
We keep 1.31e+04/8.64e+04 = 15% of the original kernel matrix.

torch.Size([4103, 2])
We keep 3.06e+05/4.43e+06 =  6% of the original kernel matrix.

torch.Size([1285, 2])
We keep 4.54e+04/3.94e+05 = 11% of the original kernel matrix.

torch.Size([5302, 2])
We keep 4.93e+05/9.47e+06 =  5% of the original kernel matrix.

torch.Size([3114, 2])
We keep 2.29e+05/2.98e+06 =  7% of the original kernel matrix.

torch.Size([7511, 2])
We keep 1.06e+06/2.60e+07 =  4% of the original kernel matrix.

torch.Size([1419, 2])
We keep 4.19e+04/4.34e+05 =  9% of the original kernel matrix.

torch.Size([5448, 2])
We keep 5.09e+05/9.94e+06 =  5% of the original kernel matrix.

torch.Size([1975, 2])
We keep 7.17e+04/7.78e+05 =  9% of the original kernel matrix.

torch.Size([6380, 2])
We keep 6.52e+05/1.33e+07 =  4% of the original kernel matrix.

torch.Size([1029, 2])
We keep 1.88e+04/1.58e+05 = 11% of the original kernel matrix.

torch.Size([5092, 2])
We keep 3.80e+05/6.00e+06 =  6% of the original kernel matrix.

torch.Size([66868, 2])
We keep 1.97e+08/4.82e+09 =  4% of the original kernel matrix.

torch.Size([33478, 2])
We keep 1.98e+07/1.05e+09 =  1% of the original kernel matrix.

torch.Size([3067, 2])
We keep 1.71e+05/2.20e+06 =  7% of the original kernel matrix.

torch.Size([7572, 2])
We keep 8.95e+05/2.24e+07 =  4% of the original kernel matrix.

torch.Size([519, 2])
We keep 7.78e+03/4.49e+04 = 17% of the original kernel matrix.

torch.Size([3789, 2])
We keep 2.64e+05/3.20e+06 =  8% of the original kernel matrix.

torch.Size([13683, 2])
We keep 3.89e+06/1.02e+08 =  3% of the original kernel matrix.

torch.Size([15037, 2])
We keep 3.85e+06/1.52e+08 =  2% of the original kernel matrix.

torch.Size([421, 2])
We keep 4.59e+03/2.72e+04 = 16% of the original kernel matrix.

torch.Size([3719, 2])
We keep 2.15e+05/2.49e+06 =  8% of the original kernel matrix.

torch.Size([30691, 2])
We keep 2.47e+07/8.18e+08 =  3% of the original kernel matrix.

torch.Size([23211, 2])
We keep 9.53e+06/4.31e+08 =  2% of the original kernel matrix.

torch.Size([4591, 2])
We keep 3.48e+05/5.76e+06 =  6% of the original kernel matrix.

torch.Size([8748, 2])
We keep 1.32e+06/3.62e+07 =  3% of the original kernel matrix.

torch.Size([2025, 2])
We keep 1.26e+05/1.14e+06 = 11% of the original kernel matrix.

torch.Size([6288, 2])
We keep 7.37e+05/1.61e+07 =  4% of the original kernel matrix.

torch.Size([661, 2])
We keep 1.74e+04/1.14e+05 = 15% of the original kernel matrix.

torch.Size([4465, 2])
We keep 3.83e+05/5.08e+06 =  7% of the original kernel matrix.

torch.Size([763, 2])
We keep 1.40e+04/9.86e+04 = 14% of the original kernel matrix.

torch.Size([4496, 2])
We keep 3.24e+05/4.74e+06 =  6% of the original kernel matrix.

torch.Size([1267, 2])
We keep 2.92e+04/2.72e+05 = 10% of the original kernel matrix.

torch.Size([5441, 2])
We keep 4.59e+05/7.87e+06 =  5% of the original kernel matrix.

torch.Size([2244, 2])
We keep 1.58e+05/1.35e+06 = 11% of the original kernel matrix.

torch.Size([6716, 2])
We keep 7.32e+05/1.75e+07 =  4% of the original kernel matrix.

torch.Size([433, 2])
We keep 3.00e+04/5.76e+04 = 52% of the original kernel matrix.

torch.Size([3299, 2])
We keep 2.90e+05/3.62e+06 =  8% of the original kernel matrix.

torch.Size([2322, 2])
We keep 1.59e+05/1.67e+06 =  9% of the original kernel matrix.

torch.Size([6615, 2])
We keep 8.14e+05/1.95e+07 =  4% of the original kernel matrix.

torch.Size([155, 2])
We keep 9.29e+02/3.25e+03 = 28% of the original kernel matrix.

torch.Size([2611, 2])
We keep 1.12e+05/8.60e+05 = 13% of the original kernel matrix.

torch.Size([249, 2])
We keep 1.91e+03/9.02e+03 = 21% of the original kernel matrix.

torch.Size([3023, 2])
We keep 1.48e+05/1.43e+06 = 10% of the original kernel matrix.

torch.Size([22216, 2])
We keep 6.12e+06/2.34e+08 =  2% of the original kernel matrix.

torch.Size([19890, 2])
We keep 5.54e+06/2.30e+08 =  2% of the original kernel matrix.

torch.Size([6647, 2])
We keep 7.11e+05/1.48e+07 =  4% of the original kernel matrix.

torch.Size([10626, 2])
We keep 1.84e+06/5.80e+07 =  3% of the original kernel matrix.

torch.Size([12571, 2])
We keep 2.29e+06/6.88e+07 =  3% of the original kernel matrix.

torch.Size([14575, 2])
We keep 3.37e+06/1.25e+08 =  2% of the original kernel matrix.

torch.Size([5335, 2])
We keep 8.18e+05/9.67e+06 =  8% of the original kernel matrix.

torch.Size([9536, 2])
We keep 1.59e+06/4.69e+07 =  3% of the original kernel matrix.

torch.Size([51815, 2])
We keep 3.63e+07/1.71e+09 =  2% of the original kernel matrix.

torch.Size([29917, 2])
We keep 1.29e+07/6.23e+08 =  2% of the original kernel matrix.

torch.Size([2767, 2])
We keep 2.38e+05/3.19e+06 =  7% of the original kernel matrix.

torch.Size([7507, 2])
We keep 1.14e+06/2.69e+07 =  4% of the original kernel matrix.

torch.Size([2408, 2])
We keep 1.13e+05/1.24e+06 =  9% of the original kernel matrix.

torch.Size([6829, 2])
We keep 7.69e+05/1.68e+07 =  4% of the original kernel matrix.

torch.Size([1261, 2])
We keep 5.68e+04/4.57e+05 = 12% of the original kernel matrix.

torch.Size([5416, 2])
We keep 5.82e+05/1.02e+07 =  5% of the original kernel matrix.

torch.Size([145, 2])
We keep 1.19e+03/3.84e+03 = 30% of the original kernel matrix.

torch.Size([2502, 2])
We keep 1.24e+05/9.35e+05 = 13% of the original kernel matrix.

torch.Size([220, 2])
We keep 1.06e+03/3.60e+03 = 29% of the original kernel matrix.

torch.Size([2931, 2])
We keep 1.11e+05/9.05e+05 = 12% of the original kernel matrix.

torch.Size([2832, 2])
We keep 3.47e+05/2.76e+06 = 12% of the original kernel matrix.

torch.Size([7096, 2])
We keep 9.74e+05/2.50e+07 =  3% of the original kernel matrix.

torch.Size([1505, 2])
We keep 4.96e+04/4.10e+05 = 12% of the original kernel matrix.

torch.Size([5494, 2])
We keep 5.08e+05/9.65e+06 =  5% of the original kernel matrix.

torch.Size([856, 2])
We keep 1.09e+04/7.73e+04 = 14% of the original kernel matrix.

torch.Size([4696, 2])
We keep 3.00e+05/4.19e+06 =  7% of the original kernel matrix.

torch.Size([1013, 2])
We keep 2.07e+04/1.71e+05 = 12% of the original kernel matrix.

torch.Size([4885, 2])
We keep 3.73e+05/6.23e+06 =  5% of the original kernel matrix.

torch.Size([4554, 2])
We keep 3.00e+05/4.77e+06 =  6% of the original kernel matrix.

torch.Size([8780, 2])
We keep 1.21e+06/3.30e+07 =  3% of the original kernel matrix.

torch.Size([173602, 2])
We keep 2.48e+08/1.73e+10 =  1% of the original kernel matrix.

torch.Size([57036, 2])
We keep 3.45e+07/1.98e+09 =  1% of the original kernel matrix.

torch.Size([401, 2])
We keep 8.59e+03/3.80e+04 = 22% of the original kernel matrix.

torch.Size([3371, 2])
We keep 2.41e+05/2.94e+06 =  8% of the original kernel matrix.

torch.Size([2369, 2])
We keep 1.22e+05/1.39e+06 =  8% of the original kernel matrix.

torch.Size([6642, 2])
We keep 8.11e+05/1.78e+07 =  4% of the original kernel matrix.

torch.Size([839, 2])
We keep 1.66e+04/1.34e+05 = 12% of the original kernel matrix.

torch.Size([4825, 2])
We keep 3.62e+05/5.52e+06 =  6% of the original kernel matrix.

torch.Size([248, 2])
We keep 1.60e+03/5.62e+03 = 28% of the original kernel matrix.

torch.Size([2958, 2])
We keep 1.27e+05/1.13e+06 = 11% of the original kernel matrix.

torch.Size([1524, 2])
We keep 5.39e+04/5.08e+05 = 10% of the original kernel matrix.

torch.Size([5654, 2])
We keep 5.65e+05/1.08e+07 =  5% of the original kernel matrix.

torch.Size([11333, 2])
We keep 8.09e+06/1.32e+08 =  6% of the original kernel matrix.

torch.Size([13651, 2])
We keep 3.77e+06/1.73e+08 =  2% of the original kernel matrix.

torch.Size([201121, 2])
We keep 2.37e+08/1.96e+10 =  1% of the original kernel matrix.

torch.Size([62353, 2])
We keep 3.59e+07/2.11e+09 =  1% of the original kernel matrix.

torch.Size([1723, 2])
We keep 9.52e+04/6.74e+05 = 14% of the original kernel matrix.

torch.Size([5955, 2])
We keep 5.87e+05/1.24e+07 =  4% of the original kernel matrix.

torch.Size([9598, 2])
We keep 1.40e+06/2.88e+07 =  4% of the original kernel matrix.

torch.Size([12696, 2])
We keep 2.38e+06/8.09e+07 =  2% of the original kernel matrix.

torch.Size([286, 2])
We keep 1.69e+03/6.72e+03 = 25% of the original kernel matrix.

torch.Size([3395, 2])
We keep 1.43e+05/1.24e+06 = 11% of the original kernel matrix.

torch.Size([1334, 2])
We keep 6.64e+04/6.69e+05 =  9% of the original kernel matrix.

torch.Size([5500, 2])
We keep 6.42e+05/1.23e+07 =  5% of the original kernel matrix.

torch.Size([123042, 2])
We keep 2.28e+08/1.22e+10 =  1% of the original kernel matrix.

torch.Size([45897, 2])
We keep 2.99e+07/1.67e+09 =  1% of the original kernel matrix.

torch.Size([14378, 2])
We keep 1.23e+07/1.50e+08 =  8% of the original kernel matrix.

torch.Size([15683, 2])
We keep 4.59e+06/1.85e+08 =  2% of the original kernel matrix.

torch.Size([2141, 2])
We keep 8.50e+04/8.95e+05 =  9% of the original kernel matrix.

torch.Size([6433, 2])
We keep 6.82e+05/1.43e+07 =  4% of the original kernel matrix.

torch.Size([8278, 2])
We keep 3.29e+06/4.82e+07 =  6% of the original kernel matrix.

torch.Size([11377, 2])
We keep 2.82e+06/1.05e+08 =  2% of the original kernel matrix.

torch.Size([51161, 2])
We keep 2.89e+07/1.37e+09 =  2% of the original kernel matrix.

torch.Size([29314, 2])
We keep 1.15e+07/5.57e+08 =  2% of the original kernel matrix.

torch.Size([5520, 2])
We keep 4.91e+05/8.95e+06 =  5% of the original kernel matrix.

torch.Size([9561, 2])
We keep 1.51e+06/4.51e+07 =  3% of the original kernel matrix.

torch.Size([1000, 2])
We keep 3.05e+04/1.99e+05 = 15% of the original kernel matrix.

torch.Size([4794, 2])
We keep 4.18e+05/6.73e+06 =  6% of the original kernel matrix.

torch.Size([383, 2])
We keep 3.40e+03/1.59e+04 = 21% of the original kernel matrix.

torch.Size([3466, 2])
We keep 1.87e+05/1.90e+06 =  9% of the original kernel matrix.

torch.Size([4560, 2])
We keep 3.00e+05/5.17e+06 =  5% of the original kernel matrix.

torch.Size([8977, 2])
We keep 1.26e+06/3.43e+07 =  3% of the original kernel matrix.

torch.Size([609, 2])
We keep 1.18e+04/7.08e+04 = 16% of the original kernel matrix.

torch.Size([4252, 2])
We keep 3.15e+05/4.01e+06 =  7% of the original kernel matrix.

torch.Size([14548, 2])
We keep 4.87e+06/1.33e+08 =  3% of the original kernel matrix.

torch.Size([15620, 2])
We keep 4.39e+06/1.74e+08 =  2% of the original kernel matrix.

torch.Size([7015, 2])
We keep 1.04e+06/1.54e+07 =  6% of the original kernel matrix.

torch.Size([10692, 2])
We keep 1.90e+06/5.92e+07 =  3% of the original kernel matrix.

torch.Size([500, 2])
We keep 6.74e+03/3.35e+04 = 20% of the original kernel matrix.

torch.Size([4065, 2])
We keep 2.36e+05/2.76e+06 =  8% of the original kernel matrix.

torch.Size([6834, 2])
We keep 1.71e+06/1.89e+07 =  9% of the original kernel matrix.

torch.Size([10756, 2])
We keep 1.93e+06/6.55e+07 =  2% of the original kernel matrix.

torch.Size([1055, 2])
We keep 2.28e+04/2.02e+05 = 11% of the original kernel matrix.

torch.Size([4848, 2])
We keep 3.92e+05/6.79e+06 =  5% of the original kernel matrix.

torch.Size([296, 2])
We keep 2.35e+03/8.84e+03 = 26% of the original kernel matrix.

torch.Size([3121, 2])
We keep 1.50e+05/1.42e+06 = 10% of the original kernel matrix.

torch.Size([5614, 2])
We keep 6.42e+05/1.09e+07 =  5% of the original kernel matrix.

torch.Size([9744, 2])
We keep 1.65e+06/4.98e+07 =  3% of the original kernel matrix.

torch.Size([10543, 2])
We keep 3.14e+06/4.82e+07 =  6% of the original kernel matrix.

torch.Size([13380, 2])
We keep 2.93e+06/1.05e+08 =  2% of the original kernel matrix.

torch.Size([15406, 2])
We keep 2.05e+07/2.75e+08 =  7% of the original kernel matrix.

torch.Size([16122, 2])
We keep 5.75e+06/2.50e+08 =  2% of the original kernel matrix.

torch.Size([4647, 2])
We keep 5.56e+05/8.19e+06 =  6% of the original kernel matrix.

torch.Size([8856, 2])
We keep 1.49e+06/4.32e+07 =  3% of the original kernel matrix.

torch.Size([51147, 2])
We keep 3.71e+07/1.30e+09 =  2% of the original kernel matrix.

torch.Size([29423, 2])
We keep 1.13e+07/5.43e+08 =  2% of the original kernel matrix.

torch.Size([7313, 2])
We keep 8.36e+05/1.74e+07 =  4% of the original kernel matrix.

torch.Size([11011, 2])
We keep 2.00e+06/6.29e+07 =  3% of the original kernel matrix.

torch.Size([5405, 2])
We keep 6.98e+05/9.46e+06 =  7% of the original kernel matrix.

torch.Size([9344, 2])
We keep 1.59e+06/4.64e+07 =  3% of the original kernel matrix.

torch.Size([6701, 2])
We keep 6.16e+05/1.27e+07 =  4% of the original kernel matrix.

torch.Size([10668, 2])
We keep 1.75e+06/5.36e+07 =  3% of the original kernel matrix.

torch.Size([4136, 2])
We keep 6.36e+05/1.05e+07 =  6% of the original kernel matrix.

torch.Size([8752, 2])
We keep 1.76e+06/4.88e+07 =  3% of the original kernel matrix.

torch.Size([1062, 2])
We keep 2.50e+04/1.87e+05 = 13% of the original kernel matrix.

torch.Size([5063, 2])
We keep 3.99e+05/6.53e+06 =  6% of the original kernel matrix.

torch.Size([9972, 2])
We keep 1.22e+07/1.06e+08 = 11% of the original kernel matrix.

torch.Size([13035, 2])
We keep 3.51e+06/1.55e+08 =  2% of the original kernel matrix.

torch.Size([1978, 2])
We keep 1.74e+05/1.60e+06 = 10% of the original kernel matrix.

torch.Size([6337, 2])
We keep 8.35e+05/1.90e+07 =  4% of the original kernel matrix.

torch.Size([7571, 2])
We keep 1.55e+06/2.57e+07 =  6% of the original kernel matrix.

torch.Size([11389, 2])
We keep 2.37e+06/7.64e+07 =  3% of the original kernel matrix.

torch.Size([12338, 2])
We keep 3.58e+06/7.02e+07 =  5% of the original kernel matrix.

torch.Size([14570, 2])
We keep 3.13e+06/1.26e+08 =  2% of the original kernel matrix.

torch.Size([752, 2])
We keep 1.63e+04/1.16e+05 = 13% of the original kernel matrix.

torch.Size([4249, 2])
We keep 3.28e+05/5.14e+06 =  6% of the original kernel matrix.

torch.Size([363, 2])
We keep 3.67e+03/1.88e+04 = 19% of the original kernel matrix.

torch.Size([3270, 2])
We keep 1.75e+05/2.07e+06 =  8% of the original kernel matrix.

torch.Size([261, 2])
We keep 1.56e+03/5.04e+03 = 31% of the original kernel matrix.

torch.Size([3031, 2])
We keep 1.26e+05/1.07e+06 = 11% of the original kernel matrix.

torch.Size([18588, 2])
We keep 9.59e+06/2.20e+08 =  4% of the original kernel matrix.

torch.Size([17165, 2])
We keep 5.17e+06/2.24e+08 =  2% of the original kernel matrix.

torch.Size([55387, 2])
We keep 4.75e+07/1.88e+09 =  2% of the original kernel matrix.

torch.Size([30971, 2])
We keep 1.32e+07/6.54e+08 =  2% of the original kernel matrix.

torch.Size([28520, 2])
We keep 8.15e+06/3.84e+08 =  2% of the original kernel matrix.

torch.Size([22600, 2])
We keep 6.61e+06/2.95e+08 =  2% of the original kernel matrix.

torch.Size([7951, 2])
We keep 1.75e+06/2.81e+07 =  6% of the original kernel matrix.

torch.Size([11375, 2])
We keep 2.39e+06/7.99e+07 =  2% of the original kernel matrix.

torch.Size([59982, 2])
We keep 5.04e+07/2.02e+09 =  2% of the original kernel matrix.

torch.Size([31707, 2])
We keep 1.37e+07/6.77e+08 =  2% of the original kernel matrix.

torch.Size([3502, 2])
We keep 4.84e+05/4.75e+06 = 10% of the original kernel matrix.

torch.Size([7666, 2])
We keep 1.22e+06/3.29e+07 =  3% of the original kernel matrix.

torch.Size([301, 2])
We keep 2.15e+03/8.10e+03 = 26% of the original kernel matrix.

torch.Size([3544, 2])
We keep 1.53e+05/1.36e+06 = 11% of the original kernel matrix.

torch.Size([55474, 2])
We keep 2.71e+07/1.34e+09 =  2% of the original kernel matrix.

torch.Size([30942, 2])
We keep 1.13e+07/5.52e+08 =  2% of the original kernel matrix.

torch.Size([587, 2])
We keep 1.19e+04/7.24e+04 = 16% of the original kernel matrix.

torch.Size([4121, 2])
We keep 3.10e+05/4.06e+06 =  7% of the original kernel matrix.

torch.Size([1237, 2])
We keep 6.59e+04/5.08e+05 = 12% of the original kernel matrix.

torch.Size([5323, 2])
We keep 6.24e+05/1.08e+07 =  5% of the original kernel matrix.

torch.Size([79181, 2])
We keep 5.30e+07/2.93e+09 =  1% of the original kernel matrix.

torch.Size([37028, 2])
We keep 1.62e+07/8.16e+08 =  1% of the original kernel matrix.

torch.Size([3133, 2])
We keep 2.20e+05/2.76e+06 =  7% of the original kernel matrix.

torch.Size([7589, 2])
We keep 1.07e+06/2.50e+07 =  4% of the original kernel matrix.

torch.Size([600, 2])
We keep 1.18e+04/5.57e+04 = 21% of the original kernel matrix.

torch.Size([3951, 2])
We keep 2.65e+05/3.56e+06 =  7% of the original kernel matrix.

torch.Size([968, 2])
We keep 2.30e+04/1.71e+05 = 13% of the original kernel matrix.

torch.Size([4810, 2])
We keep 3.88e+05/6.23e+06 =  6% of the original kernel matrix.

torch.Size([45529, 2])
We keep 3.85e+07/1.49e+09 =  2% of the original kernel matrix.

torch.Size([27555, 2])
We keep 1.21e+07/5.82e+08 =  2% of the original kernel matrix.

torch.Size([6072, 2])
We keep 6.19e+05/1.02e+07 =  6% of the original kernel matrix.

torch.Size([9898, 2])
We keep 1.66e+06/4.82e+07 =  3% of the original kernel matrix.

torch.Size([9733, 2])
We keep 1.36e+06/3.06e+07 =  4% of the original kernel matrix.

torch.Size([12604, 2])
We keep 2.43e+06/8.34e+07 =  2% of the original kernel matrix.

torch.Size([281, 2])
We keep 2.25e+03/9.02e+03 = 24% of the original kernel matrix.

torch.Size([3157, 2])
We keep 1.47e+05/1.43e+06 = 10% of the original kernel matrix.

torch.Size([860, 2])
We keep 4.24e+04/1.87e+05 = 22% of the original kernel matrix.

torch.Size([4482, 2])
We keep 4.11e+05/6.51e+06 =  6% of the original kernel matrix.

torch.Size([858, 2])
We keep 1.72e+04/1.17e+05 = 14% of the original kernel matrix.

torch.Size([4825, 2])
We keep 3.41e+05/5.16e+06 =  6% of the original kernel matrix.

torch.Size([9774, 2])
We keep 5.44e+06/8.20e+07 =  6% of the original kernel matrix.

torch.Size([12183, 2])
We keep 3.79e+06/1.37e+08 =  2% of the original kernel matrix.

torch.Size([1443, 2])
We keep 7.53e+04/6.30e+05 = 11% of the original kernel matrix.

torch.Size([5273, 2])
We keep 6.30e+05/1.20e+07 =  5% of the original kernel matrix.

torch.Size([5607, 2])
We keep 9.68e+05/1.03e+07 =  9% of the original kernel matrix.

torch.Size([9667, 2])
We keep 1.62e+06/4.84e+07 =  3% of the original kernel matrix.

torch.Size([1668, 2])
We keep 5.68e+04/5.27e+05 = 10% of the original kernel matrix.

torch.Size([5846, 2])
We keep 5.70e+05/1.09e+07 =  5% of the original kernel matrix.

torch.Size([13513, 2])
We keep 3.19e+06/8.30e+07 =  3% of the original kernel matrix.

torch.Size([15295, 2])
We keep 3.66e+06/1.37e+08 =  2% of the original kernel matrix.

torch.Size([387, 2])
We keep 6.86e+03/2.89e+04 = 23% of the original kernel matrix.

torch.Size([3785, 2])
We keep 2.58e+05/2.56e+06 = 10% of the original kernel matrix.

torch.Size([13142, 2])
We keep 4.28e+06/1.18e+08 =  3% of the original kernel matrix.

torch.Size([15122, 2])
We keep 4.34e+06/1.64e+08 =  2% of the original kernel matrix.

torch.Size([2103, 2])
We keep 6.04e+05/2.80e+06 = 21% of the original kernel matrix.

torch.Size([5957, 2])
We keep 9.09e+05/2.52e+07 =  3% of the original kernel matrix.

torch.Size([3730, 2])
We keep 3.15e+05/4.98e+06 =  6% of the original kernel matrix.

torch.Size([8140, 2])
We keep 1.27e+06/3.36e+07 =  3% of the original kernel matrix.

torch.Size([1026, 2])
We keep 2.63e+04/1.75e+05 = 15% of the original kernel matrix.

torch.Size([4941, 2])
We keep 3.96e+05/6.30e+06 =  6% of the original kernel matrix.

torch.Size([4463, 2])
We keep 5.04e+05/6.47e+06 =  7% of the original kernel matrix.

torch.Size([8677, 2])
We keep 1.40e+06/3.84e+07 =  3% of the original kernel matrix.

torch.Size([475, 2])
We keep 9.98e+03/4.33e+04 = 23% of the original kernel matrix.

torch.Size([3923, 2])
We keep 2.89e+05/3.14e+06 =  9% of the original kernel matrix.

torch.Size([4275, 2])
We keep 4.59e+05/5.20e+06 =  8% of the original kernel matrix.

torch.Size([8693, 2])
We keep 1.32e+06/3.44e+07 =  3% of the original kernel matrix.

torch.Size([3791, 2])
We keep 2.54e+05/3.72e+06 =  6% of the original kernel matrix.

torch.Size([8226, 2])
We keep 1.15e+06/2.91e+07 =  3% of the original kernel matrix.

torch.Size([2420, 2])
We keep 3.27e+05/3.09e+06 = 10% of the original kernel matrix.

torch.Size([6572, 2])
We keep 1.10e+06/2.65e+07 =  4% of the original kernel matrix.

torch.Size([455, 2])
We keep 5.83e+03/2.72e+04 = 21% of the original kernel matrix.

torch.Size([4077, 2])
We keep 2.26e+05/2.49e+06 =  9% of the original kernel matrix.

torch.Size([604, 2])
We keep 1.38e+04/8.94e+04 = 15% of the original kernel matrix.

torch.Size([4184, 2])
We keep 3.39e+05/4.51e+06 =  7% of the original kernel matrix.

torch.Size([4894, 2])
We keep 7.93e+05/1.06e+07 =  7% of the original kernel matrix.

torch.Size([9258, 2])
We keep 1.69e+06/4.90e+07 =  3% of the original kernel matrix.

torch.Size([1911, 2])
We keep 6.22e+04/7.02e+05 =  8% of the original kernel matrix.

torch.Size([6182, 2])
We keep 6.22e+05/1.26e+07 =  4% of the original kernel matrix.

torch.Size([319323, 2])
We keep 4.94e+08/4.47e+10 =  1% of the original kernel matrix.

torch.Size([78339, 2])
We keep 5.24e+07/3.19e+09 =  1% of the original kernel matrix.

torch.Size([285, 2])
We keep 2.22e+03/9.80e+03 = 22% of the original kernel matrix.

torch.Size([3234, 2])
We keep 1.55e+05/1.49e+06 = 10% of the original kernel matrix.

torch.Size([3267, 2])
We keep 2.06e+05/2.64e+06 =  7% of the original kernel matrix.

torch.Size([7721, 2])
We keep 9.71e+05/2.45e+07 =  3% of the original kernel matrix.

torch.Size([25807, 2])
We keep 7.21e+06/3.22e+08 =  2% of the original kernel matrix.

torch.Size([21711, 2])
We keep 6.27e+06/2.71e+08 =  2% of the original kernel matrix.

torch.Size([1128, 2])
We keep 3.18e+04/2.48e+05 = 12% of the original kernel matrix.

torch.Size([4915, 2])
We keep 4.39e+05/7.51e+06 =  5% of the original kernel matrix.

torch.Size([9646, 2])
We keep 3.06e+06/6.06e+07 =  5% of the original kernel matrix.

torch.Size([12452, 2])
We keep 3.26e+06/1.17e+08 =  2% of the original kernel matrix.

torch.Size([716, 2])
We keep 2.28e+04/1.38e+05 = 16% of the original kernel matrix.

torch.Size([4243, 2])
We keep 3.93e+05/5.60e+06 =  7% of the original kernel matrix.

torch.Size([17301, 2])
We keep 5.27e+06/1.57e+08 =  3% of the original kernel matrix.

torch.Size([17478, 2])
We keep 4.66e+06/1.89e+08 =  2% of the original kernel matrix.

torch.Size([2907, 2])
We keep 1.86e+05/2.01e+06 =  9% of the original kernel matrix.

torch.Size([7179, 2])
We keep 8.86e+05/2.14e+07 =  4% of the original kernel matrix.

torch.Size([1557, 2])
We keep 7.97e+04/6.63e+05 = 12% of the original kernel matrix.

torch.Size([5609, 2])
We keep 6.32e+05/1.23e+07 =  5% of the original kernel matrix.

torch.Size([349, 2])
We keep 3.56e+03/1.64e+04 = 21% of the original kernel matrix.

torch.Size([3533, 2])
We keep 1.83e+05/1.93e+06 =  9% of the original kernel matrix.

torch.Size([311, 2])
We keep 2.58e+03/1.00e+04 = 25% of the original kernel matrix.

torch.Size([3309, 2])
We keep 1.54e+05/1.51e+06 = 10% of the original kernel matrix.

torch.Size([1165, 2])
We keep 3.04e+04/3.00e+05 = 10% of the original kernel matrix.

torch.Size([5158, 2])
We keep 4.48e+05/8.26e+06 =  5% of the original kernel matrix.

torch.Size([858, 2])
We keep 1.63e+04/1.06e+05 = 15% of the original kernel matrix.

torch.Size([4770, 2])
We keep 3.46e+05/4.92e+06 =  7% of the original kernel matrix.

torch.Size([4309, 2])
We keep 6.46e+05/6.09e+06 = 10% of the original kernel matrix.

torch.Size([8600, 2])
We keep 1.33e+06/3.72e+07 =  3% of the original kernel matrix.

torch.Size([8585, 2])
We keep 1.33e+06/2.80e+07 =  4% of the original kernel matrix.

torch.Size([11963, 2])
We keep 2.40e+06/7.98e+07 =  3% of the original kernel matrix.

torch.Size([2472, 2])
We keep 1.08e+05/1.27e+06 =  8% of the original kernel matrix.

torch.Size([6880, 2])
We keep 7.68e+05/1.70e+07 =  4% of the original kernel matrix.

torch.Size([666, 2])
We keep 9.77e+03/6.00e+04 = 16% of the original kernel matrix.

torch.Size([4384, 2])
We keep 2.84e+05/3.69e+06 =  7% of the original kernel matrix.

torch.Size([697, 2])
We keep 1.51e+04/1.04e+05 = 14% of the original kernel matrix.

torch.Size([4457, 2])
We keep 3.55e+05/4.87e+06 =  7% of the original kernel matrix.

torch.Size([6366, 2])
We keep 2.16e+07/8.12e+07 = 26% of the original kernel matrix.

torch.Size([9984, 2])
We keep 3.50e+06/1.36e+08 =  2% of the original kernel matrix.

torch.Size([2945, 2])
We keep 3.50e+05/2.34e+06 = 14% of the original kernel matrix.

torch.Size([7501, 2])
We keep 9.66e+05/2.31e+07 =  4% of the original kernel matrix.

torch.Size([22315, 2])
We keep 6.79e+06/2.25e+08 =  3% of the original kernel matrix.

torch.Size([20114, 2])
We keep 5.28e+06/2.26e+08 =  2% of the original kernel matrix.

torch.Size([1138, 2])
We keep 4.51e+04/3.70e+05 = 12% of the original kernel matrix.

torch.Size([5358, 2])
We keep 5.02e+05/9.17e+06 =  5% of the original kernel matrix.

torch.Size([262, 2])
We keep 3.47e+03/1.12e+04 = 30% of the original kernel matrix.

torch.Size([3484, 2])
We keep 2.00e+05/1.60e+06 = 12% of the original kernel matrix.

torch.Size([446, 2])
We keep 4.50e+03/2.22e+04 = 20% of the original kernel matrix.

torch.Size([3657, 2])
We keep 2.01e+05/2.25e+06 =  8% of the original kernel matrix.

torch.Size([990, 2])
We keep 3.82e+04/2.60e+05 = 14% of the original kernel matrix.

torch.Size([4714, 2])
We keep 4.73e+05/7.69e+06 =  6% of the original kernel matrix.

torch.Size([3473, 2])
We keep 6.50e+05/6.59e+06 =  9% of the original kernel matrix.

torch.Size([8040, 2])
We keep 1.47e+06/3.87e+07 =  3% of the original kernel matrix.

torch.Size([7564, 2])
We keep 9.81e+05/1.86e+07 =  5% of the original kernel matrix.

torch.Size([11076, 2])
We keep 2.06e+06/6.51e+07 =  3% of the original kernel matrix.

torch.Size([868, 2])
We keep 2.57e+04/1.71e+05 = 14% of the original kernel matrix.

torch.Size([4592, 2])
We keep 3.97e+05/6.24e+06 =  6% of the original kernel matrix.

torch.Size([10172, 2])
We keep 2.30e+06/5.40e+07 =  4% of the original kernel matrix.

torch.Size([12850, 2])
We keep 3.08e+06/1.11e+08 =  2% of the original kernel matrix.

torch.Size([2015, 2])
We keep 2.55e+05/1.43e+06 = 17% of the original kernel matrix.

torch.Size([6133, 2])
We keep 8.40e+05/1.80e+07 =  4% of the original kernel matrix.

torch.Size([563, 2])
We keep 9.05e+03/5.15e+04 = 17% of the original kernel matrix.

torch.Size([4184, 2])
We keep 2.87e+05/3.42e+06 =  8% of the original kernel matrix.

torch.Size([792, 2])
We keep 3.77e+04/2.13e+05 = 17% of the original kernel matrix.

torch.Size([4475, 2])
We keep 4.54e+05/6.97e+06 =  6% of the original kernel matrix.

torch.Size([2576, 2])
We keep 1.19e+05/1.48e+06 =  8% of the original kernel matrix.

torch.Size([7041, 2])
We keep 7.71e+05/1.84e+07 =  4% of the original kernel matrix.

torch.Size([1828, 2])
We keep 9.82e+04/1.13e+06 =  8% of the original kernel matrix.

torch.Size([6341, 2])
We keep 7.66e+05/1.60e+07 =  4% of the original kernel matrix.

torch.Size([2394, 2])
We keep 2.26e+05/2.48e+06 =  9% of the original kernel matrix.

torch.Size([6297, 2])
We keep 9.29e+05/2.38e+07 =  3% of the original kernel matrix.

torch.Size([818, 2])
We keep 1.63e+04/9.42e+04 = 17% of the original kernel matrix.

torch.Size([4650, 2])
We keep 3.01e+05/4.63e+06 =  6% of the original kernel matrix.

torch.Size([5821, 2])
We keep 6.75e+06/1.99e+07 = 34% of the original kernel matrix.

torch.Size([9797, 2])
We keep 1.83e+06/6.72e+07 =  2% of the original kernel matrix.

torch.Size([21596, 2])
We keep 6.76e+06/2.24e+08 =  3% of the original kernel matrix.

torch.Size([19568, 2])
We keep 5.49e+06/2.26e+08 =  2% of the original kernel matrix.

torch.Size([8447, 2])
We keep 4.17e+07/1.70e+08 = 24% of the original kernel matrix.

torch.Size([11570, 2])
We keep 5.04e+06/1.96e+08 =  2% of the original kernel matrix.

torch.Size([36881, 2])
We keep 1.74e+07/6.44e+08 =  2% of the original kernel matrix.

torch.Size([25590, 2])
We keep 8.25e+06/3.83e+08 =  2% of the original kernel matrix.

torch.Size([1930, 2])
We keep 1.65e+05/1.32e+06 = 12% of the original kernel matrix.

torch.Size([6198, 2])
We keep 7.73e+05/1.73e+07 =  4% of the original kernel matrix.

torch.Size([9920, 2])
We keep 2.49e+06/4.69e+07 =  5% of the original kernel matrix.

torch.Size([12938, 2])
We keep 2.93e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([2441, 2])
We keep 5.91e+05/2.23e+06 = 26% of the original kernel matrix.

torch.Size([6805, 2])
We keep 9.13e+05/2.25e+07 =  4% of the original kernel matrix.

torch.Size([1477, 2])
We keep 4.15e+04/3.88e+05 = 10% of the original kernel matrix.

torch.Size([5793, 2])
We keep 5.29e+05/9.40e+06 =  5% of the original kernel matrix.

torch.Size([240, 2])
We keep 1.51e+03/7.57e+03 = 19% of the original kernel matrix.

torch.Size([3036, 2])
We keep 1.31e+05/1.31e+06 = 10% of the original kernel matrix.

torch.Size([1212, 2])
We keep 3.85e+04/3.17e+05 = 12% of the original kernel matrix.

torch.Size([5356, 2])
We keep 5.03e+05/8.49e+06 =  5% of the original kernel matrix.

torch.Size([675, 2])
We keep 1.79e+04/1.38e+05 = 13% of the original kernel matrix.

torch.Size([4293, 2])
We keep 3.63e+05/5.60e+06 =  6% of the original kernel matrix.

torch.Size([1632, 2])
We keep 6.32e+04/6.05e+05 = 10% of the original kernel matrix.

torch.Size([5797, 2])
We keep 6.03e+05/1.17e+07 =  5% of the original kernel matrix.

torch.Size([124, 2])
We keep 8.55e+02/2.40e+03 = 35% of the original kernel matrix.

torch.Size([2269, 2])
We keep 9.89e+04/7.39e+05 = 13% of the original kernel matrix.

torch.Size([2323, 2])
We keep 1.47e+05/1.36e+06 = 10% of the original kernel matrix.

torch.Size([6523, 2])
We keep 8.09e+05/1.76e+07 =  4% of the original kernel matrix.

torch.Size([784, 2])
We keep 1.45e+05/4.03e+05 = 35% of the original kernel matrix.

torch.Size([4195, 2])
We keep 4.37e+05/9.58e+06 =  4% of the original kernel matrix.

torch.Size([42185, 2])
We keep 5.22e+07/1.40e+09 =  3% of the original kernel matrix.

torch.Size([26039, 2])
We keep 1.17e+07/5.65e+08 =  2% of the original kernel matrix.

torch.Size([173, 2])
We keep 7.67e+02/2.40e+03 = 31% of the original kernel matrix.

torch.Size([2638, 2])
We keep 9.52e+04/7.39e+05 = 12% of the original kernel matrix.

torch.Size([188, 2])
We keep 3.56e+03/8.28e+03 = 42% of the original kernel matrix.

torch.Size([2570, 2])
We keep 1.51e+05/1.37e+06 = 11% of the original kernel matrix.

torch.Size([3474, 2])
We keep 5.57e+05/4.72e+06 = 11% of the original kernel matrix.

torch.Size([7812, 2])
We keep 1.20e+06/3.28e+07 =  3% of the original kernel matrix.

torch.Size([917, 2])
We keep 4.29e+04/2.60e+05 = 16% of the original kernel matrix.

torch.Size([4549, 2])
We keep 4.51e+05/7.69e+06 =  5% of the original kernel matrix.

torch.Size([2614, 2])
We keep 1.40e+05/1.79e+06 =  7% of the original kernel matrix.

torch.Size([6929, 2])
We keep 8.50e+05/2.02e+07 =  4% of the original kernel matrix.

torch.Size([501, 2])
We keep 4.44e+03/2.25e+04 = 19% of the original kernel matrix.

torch.Size([3878, 2])
We keep 1.95e+05/2.26e+06 =  8% of the original kernel matrix.

torch.Size([8086, 2])
We keep 8.11e+05/1.73e+07 =  4% of the original kernel matrix.

torch.Size([11767, 2])
We keep 1.94e+06/6.28e+07 =  3% of the original kernel matrix.

torch.Size([13381, 2])
We keep 4.51e+06/9.19e+07 =  4% of the original kernel matrix.

torch.Size([14851, 2])
We keep 3.74e+06/1.45e+08 =  2% of the original kernel matrix.

torch.Size([504, 2])
We keep 6.03e+03/3.24e+04 = 18% of the original kernel matrix.

torch.Size([4032, 2])
We keep 2.42e+05/2.71e+06 =  8% of the original kernel matrix.

torch.Size([233, 2])
We keep 1.50e+03/4.90e+03 = 30% of the original kernel matrix.

torch.Size([2867, 2])
We keep 1.25e+05/1.06e+06 = 11% of the original kernel matrix.

torch.Size([128, 2])
We keep 2.60e+02/5.76e+02 = 45% of the original kernel matrix.

torch.Size([2777, 2])
We keep 6.50e+04/3.62e+05 = 17% of the original kernel matrix.

torch.Size([5644, 2])
We keep 1.58e+06/1.74e+07 =  9% of the original kernel matrix.

torch.Size([9635, 2])
We keep 1.94e+06/6.30e+07 =  3% of the original kernel matrix.

torch.Size([206420, 2])
We keep 4.13e+08/2.64e+10 =  1% of the original kernel matrix.

torch.Size([63356, 2])
We keep 4.20e+07/2.45e+09 =  1% of the original kernel matrix.

torch.Size([688, 2])
We keep 1.06e+04/6.50e+04 = 16% of the original kernel matrix.

torch.Size([4181, 2])
We keep 2.87e+05/3.85e+06 =  7% of the original kernel matrix.

torch.Size([514, 2])
We keep 5.38e+03/2.86e+04 = 18% of the original kernel matrix.

torch.Size([3927, 2])
We keep 2.21e+05/2.55e+06 =  8% of the original kernel matrix.

torch.Size([6619, 2])
We keep 1.47e+06/1.91e+07 =  7% of the original kernel matrix.

torch.Size([10320, 2])
We keep 2.01e+06/6.58e+07 =  3% of the original kernel matrix.

torch.Size([16236, 2])
We keep 3.11e+06/1.07e+08 =  2% of the original kernel matrix.

torch.Size([16955, 2])
We keep 3.99e+06/1.56e+08 =  2% of the original kernel matrix.

torch.Size([735, 2])
We keep 2.01e+04/1.32e+05 = 15% of the original kernel matrix.

torch.Size([4105, 2])
We keep 3.64e+05/5.47e+06 =  6% of the original kernel matrix.

torch.Size([467, 2])
We keep 2.66e+04/7.78e+04 = 34% of the original kernel matrix.

torch.Size([3297, 2])
We keep 2.86e+05/4.21e+06 =  6% of the original kernel matrix.

torch.Size([503, 2])
We keep 6.28e+03/2.76e+04 = 22% of the original kernel matrix.

torch.Size([3804, 2])
We keep 2.22e+05/2.50e+06 =  8% of the original kernel matrix.

torch.Size([160890, 2])
We keep 9.63e+08/4.67e+10 =  2% of the original kernel matrix.

torch.Size([50494, 2])
We keep 5.66e+07/3.26e+09 =  1% of the original kernel matrix.

torch.Size([4606, 2])
We keep 7.66e+05/1.18e+07 =  6% of the original kernel matrix.

torch.Size([8379, 2])
We keep 1.71e+06/5.18e+07 =  3% of the original kernel matrix.

torch.Size([2132, 2])
We keep 1.05e+05/1.15e+06 =  9% of the original kernel matrix.

torch.Size([6410, 2])
We keep 7.56e+05/1.62e+07 =  4% of the original kernel matrix.

torch.Size([2532, 2])
We keep 1.23e+05/1.24e+06 =  9% of the original kernel matrix.

torch.Size([7068, 2])
We keep 7.59e+05/1.68e+07 =  4% of the original kernel matrix.

torch.Size([2487, 2])
We keep 2.05e+05/2.46e+06 =  8% of the original kernel matrix.

torch.Size([6603, 2])
We keep 9.82e+05/2.37e+07 =  4% of the original kernel matrix.

torch.Size([509, 2])
We keep 7.68e+03/4.20e+04 = 18% of the original kernel matrix.

torch.Size([3897, 2])
We keep 2.63e+05/3.09e+06 =  8% of the original kernel matrix.

torch.Size([23781, 2])
We keep 8.94e+06/3.32e+08 =  2% of the original kernel matrix.

torch.Size([20126, 2])
We keep 6.35e+06/2.75e+08 =  2% of the original kernel matrix.

torch.Size([602, 2])
We keep 9.15e+03/4.88e+04 = 18% of the original kernel matrix.

torch.Size([4128, 2])
We keep 2.71e+05/3.33e+06 =  8% of the original kernel matrix.

torch.Size([2046, 2])
We keep 8.04e+05/4.56e+06 = 17% of the original kernel matrix.

torch.Size([5397, 2])
We keep 1.19e+06/3.22e+07 =  3% of the original kernel matrix.

torch.Size([21076, 2])
We keep 8.63e+06/2.36e+08 =  3% of the original kernel matrix.

torch.Size([19099, 2])
We keep 5.47e+06/2.32e+08 =  2% of the original kernel matrix.

torch.Size([192608, 2])
We keep 2.42e+08/1.94e+10 =  1% of the original kernel matrix.

torch.Size([61576, 2])
We keep 3.59e+07/2.10e+09 =  1% of the original kernel matrix.

torch.Size([1029, 2])
We keep 2.26e+04/1.87e+05 = 12% of the original kernel matrix.

torch.Size([4868, 2])
We keep 4.05e+05/6.53e+06 =  6% of the original kernel matrix.

torch.Size([774, 2])
We keep 1.12e+04/7.40e+04 = 15% of the original kernel matrix.

torch.Size([4432, 2])
We keep 3.00e+05/4.10e+06 =  7% of the original kernel matrix.

torch.Size([10294, 2])
We keep 1.98e+07/1.39e+08 = 14% of the original kernel matrix.

torch.Size([12897, 2])
We keep 4.17e+06/1.78e+08 =  2% of the original kernel matrix.

torch.Size([1809, 2])
We keep 2.69e+05/1.38e+06 = 19% of the original kernel matrix.

torch.Size([5705, 2])
We keep 7.27e+05/1.77e+07 =  4% of the original kernel matrix.

torch.Size([13941, 2])
We keep 4.72e+06/8.50e+07 =  5% of the original kernel matrix.

torch.Size([15344, 2])
We keep 3.51e+06/1.39e+08 =  2% of the original kernel matrix.

torch.Size([188, 2])
We keep 1.55e+03/5.04e+03 = 30% of the original kernel matrix.

torch.Size([2603, 2])
We keep 1.29e+05/1.07e+06 = 12% of the original kernel matrix.

torch.Size([342, 2])
We keep 2.16e+03/1.08e+04 = 19% of the original kernel matrix.

torch.Size([3428, 2])
We keep 1.57e+05/1.57e+06 =  9% of the original kernel matrix.

torch.Size([2019, 2])
We keep 7.30e+04/7.83e+05 =  9% of the original kernel matrix.

torch.Size([6337, 2])
We keep 6.44e+05/1.33e+07 =  4% of the original kernel matrix.

torch.Size([481, 2])
We keep 5.30e+03/3.10e+04 = 17% of the original kernel matrix.

torch.Size([3797, 2])
We keep 2.16e+05/2.65e+06 =  8% of the original kernel matrix.

torch.Size([1131, 2])
We keep 1.95e+05/5.17e+05 = 37% of the original kernel matrix.

torch.Size([4594, 2])
We keep 5.47e+05/1.08e+07 =  5% of the original kernel matrix.

torch.Size([443, 2])
We keep 6.61e+03/2.82e+04 = 23% of the original kernel matrix.

torch.Size([3480, 2])
We keep 2.05e+05/2.53e+06 =  8% of the original kernel matrix.

torch.Size([2990, 2])
We keep 1.37e+05/1.97e+06 =  6% of the original kernel matrix.

torch.Size([7595, 2])
We keep 8.82e+05/2.12e+07 =  4% of the original kernel matrix.

torch.Size([1776, 2])
We keep 5.95e+04/6.30e+05 =  9% of the original kernel matrix.

torch.Size([6030, 2])
We keep 5.97e+05/1.20e+07 =  4% of the original kernel matrix.

torch.Size([1223, 2])
We keep 2.27e+04/1.94e+05 = 11% of the original kernel matrix.

torch.Size([5392, 2])
We keep 4.10e+05/6.65e+06 =  6% of the original kernel matrix.

torch.Size([440, 2])
We keep 7.27e+03/3.84e+04 = 18% of the original kernel matrix.

torch.Size([3692, 2])
We keep 2.35e+05/2.96e+06 =  7% of the original kernel matrix.

torch.Size([38416, 2])
We keep 1.82e+07/7.16e+08 =  2% of the original kernel matrix.

torch.Size([26057, 2])
We keep 8.84e+06/4.04e+08 =  2% of the original kernel matrix.

torch.Size([75000, 2])
We keep 5.43e+07/2.54e+09 =  2% of the original kernel matrix.

torch.Size([35895, 2])
We keep 1.47e+07/7.60e+08 =  1% of the original kernel matrix.

torch.Size([4669, 2])
We keep 5.96e+05/8.00e+06 =  7% of the original kernel matrix.

torch.Size([8962, 2])
We keep 1.45e+06/4.26e+07 =  3% of the original kernel matrix.

torch.Size([574, 2])
We keep 1.22e+04/7.24e+04 = 16% of the original kernel matrix.

torch.Size([3906, 2])
We keep 2.88e+05/4.06e+06 =  7% of the original kernel matrix.

torch.Size([5962, 2])
We keep 4.93e+05/9.35e+06 =  5% of the original kernel matrix.

torch.Size([10084, 2])
We keep 1.55e+06/4.61e+07 =  3% of the original kernel matrix.

torch.Size([1039, 2])
We keep 3.93e+04/2.43e+05 = 16% of the original kernel matrix.

torch.Size([4776, 2])
We keep 4.35e+05/7.43e+06 =  5% of the original kernel matrix.

torch.Size([314, 2])
We keep 4.79e+03/1.74e+04 = 27% of the original kernel matrix.

torch.Size([3288, 2])
We keep 2.09e+05/1.99e+06 = 10% of the original kernel matrix.

torch.Size([24325, 2])
We keep 6.57e+06/2.75e+08 =  2% of the original kernel matrix.

torch.Size([20903, 2])
We keep 5.86e+06/2.50e+08 =  2% of the original kernel matrix.

torch.Size([2657, 2])
We keep 2.51e+05/2.12e+06 = 11% of the original kernel matrix.

torch.Size([7129, 2])
We keep 9.86e+05/2.19e+07 =  4% of the original kernel matrix.

torch.Size([8433, 2])
We keep 3.33e+06/3.07e+07 = 10% of the original kernel matrix.

torch.Size([11885, 2])
We keep 2.31e+06/8.35e+07 =  2% of the original kernel matrix.

torch.Size([2648, 2])
We keep 1.29e+05/1.54e+06 =  8% of the original kernel matrix.

torch.Size([7093, 2])
We keep 8.01e+05/1.87e+07 =  4% of the original kernel matrix.

torch.Size([16355, 2])
We keep 5.24e+06/1.46e+08 =  3% of the original kernel matrix.

torch.Size([16745, 2])
We keep 4.59e+06/1.83e+08 =  2% of the original kernel matrix.

torch.Size([1625, 2])
We keep 4.60e+04/4.84e+05 =  9% of the original kernel matrix.

torch.Size([5817, 2])
We keep 5.49e+05/1.05e+07 =  5% of the original kernel matrix.

torch.Size([16940, 2])
We keep 4.24e+06/1.45e+08 =  2% of the original kernel matrix.

torch.Size([16999, 2])
We keep 4.53e+06/1.81e+08 =  2% of the original kernel matrix.

torch.Size([33341, 2])
We keep 2.04e+07/6.46e+08 =  3% of the original kernel matrix.

torch.Size([24061, 2])
We keep 8.13e+06/3.83e+08 =  2% of the original kernel matrix.

torch.Size([3239, 2])
We keep 3.36e+05/3.97e+06 =  8% of the original kernel matrix.

torch.Size([7785, 2])
We keep 1.16e+06/3.00e+07 =  3% of the original kernel matrix.

torch.Size([4825, 2])
We keep 7.73e+05/8.66e+06 =  8% of the original kernel matrix.

torch.Size([8927, 2])
We keep 1.58e+06/4.44e+07 =  3% of the original kernel matrix.

torch.Size([1314, 2])
We keep 6.96e+04/6.37e+05 = 10% of the original kernel matrix.

torch.Size([5450, 2])
We keep 6.72e+05/1.20e+07 =  5% of the original kernel matrix.

torch.Size([10520, 2])
We keep 1.53e+06/4.26e+07 =  3% of the original kernel matrix.

torch.Size([13353, 2])
We keep 2.82e+06/9.84e+07 =  2% of the original kernel matrix.

torch.Size([3379, 2])
We keep 1.70e+05/2.30e+06 =  7% of the original kernel matrix.

torch.Size([7942, 2])
We keep 9.56e+05/2.28e+07 =  4% of the original kernel matrix.

torch.Size([1055, 2])
We keep 3.52e+04/2.00e+05 = 17% of the original kernel matrix.

torch.Size([4911, 2])
We keep 4.26e+05/6.74e+06 =  6% of the original kernel matrix.

torch.Size([1514, 2])
We keep 6.28e+04/4.56e+05 = 13% of the original kernel matrix.

torch.Size([5581, 2])
We keep 5.35e+05/1.02e+07 =  5% of the original kernel matrix.

torch.Size([1569, 2])
We keep 6.23e+04/5.00e+05 = 12% of the original kernel matrix.

torch.Size([5719, 2])
We keep 5.45e+05/1.07e+07 =  5% of the original kernel matrix.

torch.Size([15477, 2])
We keep 2.81e+06/1.01e+08 =  2% of the original kernel matrix.

torch.Size([16180, 2])
We keep 3.77e+06/1.51e+08 =  2% of the original kernel matrix.

torch.Size([90169, 2])
We keep 9.24e+07/4.55e+09 =  2% of the original kernel matrix.

torch.Size([39874, 2])
We keep 1.94e+07/1.02e+09 =  1% of the original kernel matrix.

torch.Size([13606, 2])
We keep 6.11e+06/8.56e+07 =  7% of the original kernel matrix.

torch.Size([15195, 2])
We keep 3.59e+06/1.40e+08 =  2% of the original kernel matrix.

torch.Size([2183, 2])
We keep 1.49e+05/1.86e+06 =  8% of the original kernel matrix.

torch.Size([6630, 2])
We keep 9.32e+05/2.06e+07 =  4% of the original kernel matrix.

torch.Size([1454, 2])
We keep 6.92e+04/7.16e+05 =  9% of the original kernel matrix.

torch.Size([5709, 2])
We keep 6.85e+05/1.28e+07 =  5% of the original kernel matrix.

torch.Size([294, 2])
We keep 2.84e+03/1.32e+04 = 21% of the original kernel matrix.

torch.Size([3198, 2])
We keep 1.70e+05/1.73e+06 =  9% of the original kernel matrix.

torch.Size([4048, 2])
We keep 3.19e+05/4.92e+06 =  6% of the original kernel matrix.

torch.Size([8242, 2])
We keep 1.25e+06/3.35e+07 =  3% of the original kernel matrix.

torch.Size([1053, 2])
We keep 2.23e+04/1.86e+05 = 12% of the original kernel matrix.

torch.Size([4916, 2])
We keep 3.89e+05/6.50e+06 =  5% of the original kernel matrix.

torch.Size([70074, 2])
We keep 7.90e+07/3.26e+09 =  2% of the original kernel matrix.

torch.Size([34234, 2])
We keep 1.68e+07/8.61e+08 =  1% of the original kernel matrix.

torch.Size([3136, 2])
We keep 3.43e+05/3.81e+06 =  9% of the original kernel matrix.

torch.Size([7598, 2])
We keep 1.14e+06/2.95e+07 =  3% of the original kernel matrix.

torch.Size([692, 2])
We keep 1.60e+04/9.24e+04 = 17% of the original kernel matrix.

torch.Size([4720, 2])
We keep 3.48e+05/4.58e+06 =  7% of the original kernel matrix.

torch.Size([137170, 2])
We keep 3.25e+08/1.63e+10 =  1% of the original kernel matrix.

torch.Size([48888, 2])
We keep 3.42e+07/1.93e+09 =  1% of the original kernel matrix.

torch.Size([802, 2])
We keep 1.92e+04/1.02e+05 = 18% of the original kernel matrix.

torch.Size([4359, 2])
We keep 3.25e+05/4.81e+06 =  6% of the original kernel matrix.

torch.Size([2576, 2])
We keep 1.26e+05/1.42e+06 =  8% of the original kernel matrix.

torch.Size([6965, 2])
We keep 7.95e+05/1.80e+07 =  4% of the original kernel matrix.

torch.Size([264, 2])
We keep 2.86e+03/1.04e+04 = 27% of the original kernel matrix.

torch.Size([3191, 2])
We keep 1.80e+05/1.54e+06 = 11% of the original kernel matrix.

torch.Size([299, 2])
We keep 2.59e+03/8.84e+03 = 29% of the original kernel matrix.

torch.Size([3179, 2])
We keep 1.47e+05/1.42e+06 = 10% of the original kernel matrix.

torch.Size([1092, 2])
We keep 4.33e+04/3.07e+05 = 14% of the original kernel matrix.

torch.Size([5526, 2])
We keep 5.31e+05/8.35e+06 =  6% of the original kernel matrix.

torch.Size([611, 2])
We keep 6.86e+03/3.88e+04 = 17% of the original kernel matrix.

torch.Size([4066, 2])
We keep 2.37e+05/2.97e+06 =  7% of the original kernel matrix.

torch.Size([270337, 2])
We keep 3.64e+08/3.39e+10 =  1% of the original kernel matrix.

torch.Size([72408, 2])
We keep 4.66e+07/2.78e+09 =  1% of the original kernel matrix.

torch.Size([8125, 2])
We keep 1.81e+06/3.46e+07 =  5% of the original kernel matrix.

torch.Size([11899, 2])
We keep 2.43e+06/8.88e+07 =  2% of the original kernel matrix.

torch.Size([2298, 2])
We keep 2.45e+05/1.98e+06 = 12% of the original kernel matrix.

torch.Size([6381, 2])
We keep 9.16e+05/2.12e+07 =  4% of the original kernel matrix.

torch.Size([5763, 2])
We keep 5.80e+05/1.05e+07 =  5% of the original kernel matrix.

torch.Size([10027, 2])
We keep 1.59e+06/4.88e+07 =  3% of the original kernel matrix.

torch.Size([755, 2])
We keep 1.10e+04/6.45e+04 = 17% of the original kernel matrix.

torch.Size([4470, 2])
We keep 2.85e+05/3.83e+06 =  7% of the original kernel matrix.

torch.Size([907, 2])
We keep 2.83e+04/2.17e+05 = 13% of the original kernel matrix.

torch.Size([4563, 2])
We keep 4.26e+05/7.03e+06 =  6% of the original kernel matrix.

torch.Size([3471, 2])
We keep 3.07e+05/3.51e+06 =  8% of the original kernel matrix.

torch.Size([7690, 2])
We keep 1.12e+06/2.83e+07 =  3% of the original kernel matrix.

torch.Size([3139, 2])
We keep 2.38e+05/2.49e+06 =  9% of the original kernel matrix.

torch.Size([7605, 2])
We keep 9.78e+05/2.38e+07 =  4% of the original kernel matrix.

torch.Size([613, 2])
We keep 9.68e+03/6.10e+04 = 15% of the original kernel matrix.

torch.Size([4087, 2])
We keep 2.81e+05/3.73e+06 =  7% of the original kernel matrix.

torch.Size([1723, 2])
We keep 2.05e+06/4.91e+06 = 41% of the original kernel matrix.

torch.Size([5165, 2])
We keep 1.11e+06/3.34e+07 =  3% of the original kernel matrix.

torch.Size([1067, 2])
We keep 2.82e+04/2.34e+05 = 12% of the original kernel matrix.

torch.Size([4847, 2])
We keep 4.24e+05/7.30e+06 =  5% of the original kernel matrix.

torch.Size([1414, 2])
We keep 4.35e+04/4.20e+05 = 10% of the original kernel matrix.

torch.Size([5532, 2])
We keep 5.44e+05/9.77e+06 =  5% of the original kernel matrix.

torch.Size([1236, 2])
We keep 3.87e+04/3.45e+05 = 11% of the original kernel matrix.

torch.Size([5140, 2])
We keep 4.82e+05/8.85e+06 =  5% of the original kernel matrix.

torch.Size([1215, 2])
We keep 3.72e+04/2.94e+05 = 12% of the original kernel matrix.

torch.Size([5149, 2])
We keep 4.61e+05/8.17e+06 =  5% of the original kernel matrix.

torch.Size([5383, 2])
We keep 7.69e+05/1.28e+07 =  6% of the original kernel matrix.

torch.Size([9289, 2])
We keep 1.82e+06/5.40e+07 =  3% of the original kernel matrix.

torch.Size([1710, 2])
We keep 7.55e+06/1.69e+07 = 44% of the original kernel matrix.

torch.Size([4556, 2])
We keep 1.92e+06/6.19e+07 =  3% of the original kernel matrix.

torch.Size([1770, 2])
We keep 1.54e+05/1.44e+06 = 10% of the original kernel matrix.

torch.Size([5569, 2])
We keep 8.31e+05/1.81e+07 =  4% of the original kernel matrix.

torch.Size([185283, 2])
We keep 2.28e+08/1.73e+10 =  1% of the original kernel matrix.

torch.Size([59898, 2])
We keep 3.45e+07/1.99e+09 =  1% of the original kernel matrix.

torch.Size([1427, 2])
We keep 1.26e+05/9.90e+05 = 12% of the original kernel matrix.

torch.Size([5442, 2])
We keep 7.34e+05/1.50e+07 =  4% of the original kernel matrix.

torch.Size([30255, 2])
We keep 1.04e+07/4.46e+08 =  2% of the original kernel matrix.

torch.Size([23230, 2])
We keep 7.18e+06/3.18e+08 =  2% of the original kernel matrix.

torch.Size([1047, 2])
We keep 2.05e+04/1.63e+05 = 12% of the original kernel matrix.

torch.Size([4992, 2])
We keep 3.68e+05/6.09e+06 =  6% of the original kernel matrix.

torch.Size([8066, 2])
We keep 1.57e+06/2.79e+07 =  5% of the original kernel matrix.

torch.Size([11475, 2])
We keep 2.37e+06/7.96e+07 =  2% of the original kernel matrix.

torch.Size([63606, 2])
We keep 6.86e+07/3.23e+09 =  2% of the original kernel matrix.

torch.Size([33139, 2])
We keep 1.66e+07/8.57e+08 =  1% of the original kernel matrix.

torch.Size([9083, 2])
We keep 1.50e+06/3.05e+07 =  4% of the original kernel matrix.

torch.Size([12311, 2])
We keep 2.41e+06/8.33e+07 =  2% of the original kernel matrix.

torch.Size([770, 2])
We keep 1.07e+04/7.34e+04 = 14% of the original kernel matrix.

torch.Size([4463, 2])
We keep 2.85e+05/4.09e+06 =  6% of the original kernel matrix.

torch.Size([7900, 2])
We keep 1.07e+06/2.05e+07 =  5% of the original kernel matrix.

torch.Size([11452, 2])
We keep 2.11e+06/6.83e+07 =  3% of the original kernel matrix.

torch.Size([1918, 2])
We keep 3.19e+05/1.78e+06 = 17% of the original kernel matrix.

torch.Size([5947, 2])
We keep 8.00e+05/2.01e+07 =  3% of the original kernel matrix.

torch.Size([6492, 2])
We keep 1.32e+06/1.59e+07 =  8% of the original kernel matrix.

torch.Size([10118, 2])
We keep 1.85e+06/6.01e+07 =  3% of the original kernel matrix.

torch.Size([31233, 2])
We keep 1.19e+07/4.96e+08 =  2% of the original kernel matrix.

torch.Size([23245, 2])
We keep 7.46e+06/3.36e+08 =  2% of the original kernel matrix.

torch.Size([911, 2])
We keep 2.65e+04/2.09e+05 = 12% of the original kernel matrix.

torch.Size([4564, 2])
We keep 3.88e+05/6.89e+06 =  5% of the original kernel matrix.

torch.Size([1221, 2])
We keep 4.56e+04/3.71e+05 = 12% of the original kernel matrix.

torch.Size([5163, 2])
We keep 4.92e+05/9.18e+06 =  5% of the original kernel matrix.

torch.Size([2589, 2])
We keep 1.29e+05/1.44e+06 =  8% of the original kernel matrix.

torch.Size([6869, 2])
We keep 8.02e+05/1.81e+07 =  4% of the original kernel matrix.

torch.Size([30029, 2])
We keep 1.10e+07/4.69e+08 =  2% of the original kernel matrix.

torch.Size([22933, 2])
We keep 7.20e+06/3.27e+08 =  2% of the original kernel matrix.

torch.Size([9008, 2])
We keep 1.94e+06/4.24e+07 =  4% of the original kernel matrix.

torch.Size([12275, 2])
We keep 2.69e+06/9.82e+07 =  2% of the original kernel matrix.

torch.Size([4575, 2])
We keep 3.23e+05/4.92e+06 =  6% of the original kernel matrix.

torch.Size([8907, 2])
We keep 1.23e+06/3.35e+07 =  3% of the original kernel matrix.

torch.Size([325, 2])
We keep 2.77e+03/1.14e+04 = 24% of the original kernel matrix.

torch.Size([3240, 2])
We keep 1.63e+05/1.61e+06 = 10% of the original kernel matrix.

torch.Size([72325, 2])
We keep 6.71e+07/3.05e+09 =  2% of the original kernel matrix.

torch.Size([34875, 2])
We keep 1.61e+07/8.33e+08 =  1% of the original kernel matrix.

torch.Size([12431, 2])
We keep 3.51e+06/7.33e+07 =  4% of the original kernel matrix.

torch.Size([14141, 2])
We keep 3.51e+06/1.29e+08 =  2% of the original kernel matrix.

torch.Size([106477, 2])
We keep 1.37e+08/6.78e+09 =  2% of the original kernel matrix.

torch.Size([43568, 2])
We keep 2.26e+07/1.24e+09 =  1% of the original kernel matrix.

torch.Size([1041, 2])
We keep 4.03e+04/2.98e+05 = 13% of the original kernel matrix.

torch.Size([4875, 2])
We keep 4.68e+05/8.23e+06 =  5% of the original kernel matrix.

torch.Size([12215, 2])
We keep 5.06e+06/1.09e+08 =  4% of the original kernel matrix.

torch.Size([14250, 2])
We keep 4.21e+06/1.58e+08 =  2% of the original kernel matrix.

torch.Size([1803, 2])
We keep 1.56e+05/9.14e+05 = 17% of the original kernel matrix.

torch.Size([5980, 2])
We keep 6.88e+05/1.44e+07 =  4% of the original kernel matrix.

torch.Size([14702, 2])
We keep 4.82e+06/1.15e+08 =  4% of the original kernel matrix.

torch.Size([15474, 2])
We keep 4.20e+06/1.61e+08 =  2% of the original kernel matrix.

torch.Size([4275, 2])
We keep 7.55e+05/1.10e+07 =  6% of the original kernel matrix.

torch.Size([8885, 2])
We keep 1.76e+06/4.99e+07 =  3% of the original kernel matrix.

torch.Size([2298, 2])
We keep 1.30e+05/1.47e+06 =  8% of the original kernel matrix.

torch.Size([6606, 2])
We keep 8.19e+05/1.83e+07 =  4% of the original kernel matrix.

torch.Size([1983, 2])
We keep 9.02e+04/9.64e+05 =  9% of the original kernel matrix.

torch.Size([6188, 2])
We keep 6.73e+05/1.48e+07 =  4% of the original kernel matrix.

torch.Size([4929, 2])
We keep 1.03e+06/1.04e+07 =  9% of the original kernel matrix.

torch.Size([8976, 2])
We keep 1.65e+06/4.85e+07 =  3% of the original kernel matrix.

torch.Size([1183, 2])
We keep 1.34e+05/5.79e+05 = 23% of the original kernel matrix.

torch.Size([4911, 2])
We keep 5.50e+05/1.15e+07 =  4% of the original kernel matrix.

torch.Size([8286, 2])
We keep 1.74e+06/2.42e+07 =  7% of the original kernel matrix.

torch.Size([11694, 2])
We keep 2.31e+06/7.41e+07 =  3% of the original kernel matrix.

torch.Size([3447, 2])
We keep 1.98e+05/2.77e+06 =  7% of the original kernel matrix.

torch.Size([7965, 2])
We keep 9.90e+05/2.51e+07 =  3% of the original kernel matrix.

torch.Size([2053, 2])
We keep 6.03e+04/6.97e+05 =  8% of the original kernel matrix.

torch.Size([6573, 2])
We keep 6.12e+05/1.26e+07 =  4% of the original kernel matrix.

torch.Size([1670, 2])
We keep 7.05e+04/6.81e+05 = 10% of the original kernel matrix.

torch.Size([5869, 2])
We keep 6.41e+05/1.24e+07 =  5% of the original kernel matrix.

torch.Size([20233, 2])
We keep 9.84e+06/2.31e+08 =  4% of the original kernel matrix.

torch.Size([18621, 2])
We keep 5.35e+06/2.29e+08 =  2% of the original kernel matrix.

torch.Size([2078, 2])
We keep 1.25e+05/1.44e+06 =  8% of the original kernel matrix.

torch.Size([6637, 2])
We keep 8.59e+05/1.81e+07 =  4% of the original kernel matrix.

torch.Size([25386, 2])
We keep 1.01e+07/3.22e+08 =  3% of the original kernel matrix.

torch.Size([21313, 2])
We keep 6.38e+06/2.70e+08 =  2% of the original kernel matrix.

torch.Size([17690, 2])
We keep 4.84e+06/1.28e+08 =  3% of the original kernel matrix.

torch.Size([17680, 2])
We keep 4.26e+06/1.71e+08 =  2% of the original kernel matrix.

torch.Size([13032, 2])
We keep 5.41e+06/1.20e+08 =  4% of the original kernel matrix.

torch.Size([14413, 2])
We keep 4.32e+06/1.65e+08 =  2% of the original kernel matrix.

torch.Size([3002, 2])
We keep 3.92e+05/3.14e+06 = 12% of the original kernel matrix.

torch.Size([7519, 2])
We keep 9.78e+05/2.67e+07 =  3% of the original kernel matrix.

torch.Size([330, 2])
We keep 2.52e+03/1.02e+04 = 24% of the original kernel matrix.

torch.Size([3400, 2])
We keep 1.58e+05/1.52e+06 = 10% of the original kernel matrix.

torch.Size([2116, 2])
We keep 1.30e+05/1.46e+06 =  8% of the original kernel matrix.

torch.Size([6392, 2])
We keep 8.08e+05/1.82e+07 =  4% of the original kernel matrix.

torch.Size([4988, 2])
We keep 4.71e+05/8.32e+06 =  5% of the original kernel matrix.

torch.Size([9320, 2])
We keep 1.48e+06/4.35e+07 =  3% of the original kernel matrix.

torch.Size([633, 2])
We keep 1.27e+04/7.08e+04 = 17% of the original kernel matrix.

torch.Size([4040, 2])
We keep 2.86e+05/4.01e+06 =  7% of the original kernel matrix.

torch.Size([1845, 2])
We keep 1.31e+05/1.16e+06 = 11% of the original kernel matrix.

torch.Size([5828, 2])
We keep 7.58e+05/1.62e+07 =  4% of the original kernel matrix.

torch.Size([375, 2])
We keep 4.62e+03/2.22e+04 = 20% of the original kernel matrix.

torch.Size([3704, 2])
We keep 2.13e+05/2.25e+06 =  9% of the original kernel matrix.

torch.Size([953, 2])
We keep 1.87e+04/1.52e+05 = 12% of the original kernel matrix.

torch.Size([4838, 2])
We keep 3.71e+05/5.88e+06 =  6% of the original kernel matrix.

torch.Size([30830, 2])
We keep 2.18e+07/6.04e+08 =  3% of the original kernel matrix.

torch.Size([22699, 2])
We keep 8.07e+06/3.71e+08 =  2% of the original kernel matrix.

torch.Size([1566, 2])
We keep 5.82e+04/5.07e+05 = 11% of the original kernel matrix.

torch.Size([5701, 2])
We keep 5.66e+05/1.07e+07 =  5% of the original kernel matrix.

torch.Size([6874, 2])
We keep 7.76e+05/1.31e+07 =  5% of the original kernel matrix.

torch.Size([10806, 2])
We keep 1.81e+06/5.46e+07 =  3% of the original kernel matrix.

torch.Size([377, 2])
We keep 2.29e+03/1.08e+04 = 21% of the original kernel matrix.

torch.Size([3748, 2])
We keep 1.60e+05/1.57e+06 = 10% of the original kernel matrix.

torch.Size([716, 2])
We keep 1.61e+04/9.73e+04 = 16% of the original kernel matrix.

torch.Size([4400, 2])
We keep 3.43e+05/4.71e+06 =  7% of the original kernel matrix.

torch.Size([2682, 2])
We keep 1.43e+05/1.61e+06 =  8% of the original kernel matrix.

torch.Size([7166, 2])
We keep 8.28e+05/1.92e+07 =  4% of the original kernel matrix.

torch.Size([9984, 2])
We keep 2.12e+06/3.33e+07 =  6% of the original kernel matrix.

torch.Size([12894, 2])
We keep 2.47e+06/8.71e+07 =  2% of the original kernel matrix.

torch.Size([19942, 2])
We keep 5.96e+06/1.91e+08 =  3% of the original kernel matrix.

torch.Size([18611, 2])
We keep 5.09e+06/2.08e+08 =  2% of the original kernel matrix.

torch.Size([745, 2])
We keep 1.32e+04/7.73e+04 = 17% of the original kernel matrix.

torch.Size([4510, 2])
We keep 2.93e+05/4.19e+06 =  6% of the original kernel matrix.

torch.Size([1726, 2])
We keep 1.03e+05/9.56e+05 = 10% of the original kernel matrix.

torch.Size([5652, 2])
We keep 6.97e+05/1.47e+07 =  4% of the original kernel matrix.

torch.Size([375, 2])
We keep 3.28e+03/1.42e+04 = 23% of the original kernel matrix.

torch.Size([3910, 2])
We keep 1.80e+05/1.79e+06 = 10% of the original kernel matrix.

torch.Size([1195, 2])
We keep 1.12e+05/5.21e+05 = 21% of the original kernel matrix.

torch.Size([4839, 2])
We keep 5.79e+05/1.09e+07 =  5% of the original kernel matrix.

torch.Size([3334, 2])
We keep 2.97e+05/3.60e+06 =  8% of the original kernel matrix.

torch.Size([7803, 2])
We keep 1.05e+06/2.86e+07 =  3% of the original kernel matrix.

torch.Size([1986, 2])
We keep 9.38e+04/9.18e+05 = 10% of the original kernel matrix.

torch.Size([6328, 2])
We keep 5.63e+05/1.44e+07 =  3% of the original kernel matrix.

torch.Size([1774, 2])
We keep 7.28e+04/6.77e+05 = 10% of the original kernel matrix.

torch.Size([5954, 2])
We keep 6.15e+05/1.24e+07 =  4% of the original kernel matrix.

torch.Size([13067, 2])
We keep 4.28e+06/9.26e+07 =  4% of the original kernel matrix.

torch.Size([14930, 2])
We keep 3.90e+06/1.45e+08 =  2% of the original kernel matrix.

torch.Size([1291, 2])
We keep 6.92e+04/5.98e+05 = 11% of the original kernel matrix.

torch.Size([5604, 2])
We keep 6.48e+05/1.17e+07 =  5% of the original kernel matrix.

torch.Size([11086, 2])
We keep 2.49e+06/4.03e+07 =  6% of the original kernel matrix.

torch.Size([13707, 2])
We keep 2.76e+06/9.57e+07 =  2% of the original kernel matrix.

torch.Size([21538, 2])
We keep 5.34e+06/2.19e+08 =  2% of the original kernel matrix.

torch.Size([19821, 2])
We keep 5.34e+06/2.23e+08 =  2% of the original kernel matrix.

torch.Size([543, 2])
We keep 1.02e+04/4.71e+04 = 21% of the original kernel matrix.

torch.Size([3427, 2])
We keep 2.50e+05/3.27e+06 =  7% of the original kernel matrix.

torch.Size([2172, 2])
We keep 1.15e+05/1.30e+06 =  8% of the original kernel matrix.

torch.Size([6620, 2])
We keep 7.87e+05/1.72e+07 =  4% of the original kernel matrix.

torch.Size([4776, 2])
We keep 2.95e+05/5.16e+06 =  5% of the original kernel matrix.

torch.Size([9239, 2])
We keep 1.23e+06/3.43e+07 =  3% of the original kernel matrix.

torch.Size([11821, 2])
We keep 2.99e+06/7.31e+07 =  4% of the original kernel matrix.

torch.Size([14041, 2])
We keep 3.49e+06/1.29e+08 =  2% of the original kernel matrix.

torch.Size([16154, 2])
We keep 5.05e+06/1.29e+08 =  3% of the original kernel matrix.

torch.Size([16638, 2])
We keep 4.35e+06/1.72e+08 =  2% of the original kernel matrix.

torch.Size([729, 2])
We keep 1.97e+04/1.16e+05 = 17% of the original kernel matrix.

torch.Size([4284, 2])
We keep 3.49e+05/5.13e+06 =  6% of the original kernel matrix.

torch.Size([4006, 2])
We keep 3.25e+05/4.92e+06 =  6% of the original kernel matrix.

torch.Size([8413, 2])
We keep 1.23e+06/3.34e+07 =  3% of the original kernel matrix.

torch.Size([5053, 2])
We keep 6.02e+05/7.11e+06 =  8% of the original kernel matrix.

torch.Size([9318, 2])
We keep 1.39e+06/4.02e+07 =  3% of the original kernel matrix.

torch.Size([710, 2])
We keep 2.87e+04/1.33e+05 = 21% of the original kernel matrix.

torch.Size([4316, 2])
We keep 3.84e+05/5.50e+06 =  6% of the original kernel matrix.

torch.Size([313, 2])
We keep 1.86e+03/7.22e+03 = 25% of the original kernel matrix.

torch.Size([3118, 2])
We keep 1.31e+05/1.28e+06 = 10% of the original kernel matrix.

torch.Size([1282, 2])
We keep 5.35e+04/4.83e+05 = 11% of the original kernel matrix.

torch.Size([5022, 2])
We keep 5.35e+05/1.05e+07 =  5% of the original kernel matrix.

torch.Size([27168, 2])
We keep 1.08e+07/3.48e+08 =  3% of the original kernel matrix.

torch.Size([22024, 2])
We keep 6.40e+06/2.81e+08 =  2% of the original kernel matrix.

torch.Size([113566, 2])
We keep 9.84e+07/6.28e+09 =  1% of the original kernel matrix.

torch.Size([45006, 2])
We keep 2.17e+07/1.20e+09 =  1% of the original kernel matrix.

torch.Size([3129, 2])
We keep 3.08e+05/4.13e+06 =  7% of the original kernel matrix.

torch.Size([7190, 2])
We keep 1.14e+06/3.07e+07 =  3% of the original kernel matrix.

torch.Size([762, 2])
We keep 1.18e+04/7.62e+04 = 15% of the original kernel matrix.

torch.Size([4544, 2])
We keep 2.96e+05/4.16e+06 =  7% of the original kernel matrix.

torch.Size([841, 2])
We keep 1.56e+04/9.80e+04 = 15% of the original kernel matrix.

torch.Size([4762, 2])
We keep 3.47e+05/4.72e+06 =  7% of the original kernel matrix.

torch.Size([8792, 2])
We keep 3.08e+06/5.37e+07 =  5% of the original kernel matrix.

torch.Size([11953, 2])
We keep 2.96e+06/1.11e+08 =  2% of the original kernel matrix.

torch.Size([542, 2])
We keep 7.51e+03/4.04e+04 = 18% of the original kernel matrix.

torch.Size([3957, 2])
We keep 2.43e+05/3.03e+06 =  8% of the original kernel matrix.

torch.Size([897, 2])
We keep 1.98e+04/1.31e+05 = 15% of the original kernel matrix.

torch.Size([4594, 2])
We keep 3.66e+05/5.46e+06 =  6% of the original kernel matrix.

torch.Size([4600, 2])
We keep 3.92e+05/6.33e+06 =  6% of the original kernel matrix.

torch.Size([8963, 2])
We keep 1.35e+06/3.79e+07 =  3% of the original kernel matrix.

torch.Size([41429, 2])
We keep 1.92e+07/7.80e+08 =  2% of the original kernel matrix.

torch.Size([27082, 2])
We keep 8.80e+06/4.21e+08 =  2% of the original kernel matrix.

torch.Size([196, 2])
We keep 2.26e+03/8.28e+03 = 27% of the original kernel matrix.

torch.Size([2769, 2])
We keep 1.61e+05/1.37e+06 = 11% of the original kernel matrix.

torch.Size([7370, 2])
We keep 8.46e+05/1.56e+07 =  5% of the original kernel matrix.

torch.Size([11149, 2])
We keep 1.88e+06/5.95e+07 =  3% of the original kernel matrix.

torch.Size([846, 2])
We keep 3.43e+04/1.60e+05 = 21% of the original kernel matrix.

torch.Size([4433, 2])
We keep 3.71e+05/6.03e+06 =  6% of the original kernel matrix.

time for making ranges is 1.5698292255401611
Sorting X and nu_X
time for sorting X is 0.03649139404296875
Sorting Z and nu_Z
time for sorting Z is 0.00026035308837890625
Starting Optim
sum tnu_Z before tensor(5728522.5000, device='cuda:0')
c= tensor(200.6332, device='cuda:0')
c= tensor(574.5375, device='cuda:0')
c= tensor(652.6404, device='cuda:0')
c= tensor(783.8591, device='cuda:0')
c= tensor(14781.3594, device='cuda:0')
c= tensor(19591.3008, device='cuda:0')
c= tensor(20348.6172, device='cuda:0')
c= tensor(40111.6406, device='cuda:0')
c= tensor(81543.5781, device='cuda:0')
c= tensor(98464.2656, device='cuda:0')
c= tensor(99263.6484, device='cuda:0')
c= tensor(103813.9062, device='cuda:0')
c= tensor(104242.3594, device='cuda:0')
c= tensor(2425885.2500, device='cuda:0')
c= tensor(2426291.2500, device='cuda:0')
c= tensor(2443467.2500, device='cuda:0')
c= tensor(2443839., device='cuda:0')
c= tensor(2447593.7500, device='cuda:0')
c= tensor(4042896.5000, device='cuda:0')
c= tensor(5285958.5000, device='cuda:0')
c= tensor(5286849., device='cuda:0')
c= tensor(5447675.5000, device='cuda:0')
c= tensor(5448263., device='cuda:0')
c= tensor(5448917., device='cuda:0')
c= tensor(5449291., device='cuda:0')
c= tensor(5453619., device='cuda:0')
c= tensor(5999790., device='cuda:0')
c= tensor(6001425.5000, device='cuda:0')
c= tensor(6613208., device='cuda:0')
c= tensor(13733029., device='cuda:0')
c= tensor(13740164., device='cuda:0')
c= tensor(14651565., device='cuda:0')
c= tensor(14653656., device='cuda:0')
c= tensor(14663172., device='cuda:0')
c= tensor(14663334., device='cuda:0')
c= tensor(18706810., device='cuda:0')
c= tensor(18706980., device='cuda:0')
c= tensor(18707530., device='cuda:0')
c= tensor(18708054., device='cuda:0')
c= tensor(18708692., device='cuda:0')
c= tensor(18708876., device='cuda:0')
c= tensor(18709684., device='cuda:0')
c= tensor(18709744., device='cuda:0')
c= tensor(18709824., device='cuda:0')
c= tensor(18710140., device='cuda:0')
c= tensor(18710236., device='cuda:0')
c= tensor(18710342., device='cuda:0')
c= tensor(18710378., device='cuda:0')
c= tensor(18710472., device='cuda:0')
c= tensor(18710582., device='cuda:0')
c= tensor(18774290., device='cuda:0')
c= tensor(18774380., device='cuda:0')
c= tensor(18778074., device='cuda:0')
c= tensor(18778216., device='cuda:0')
c= tensor(18778748., device='cuda:0')
c= tensor(18782260., device='cuda:0')
c= tensor(18782792., device='cuda:0')
c= tensor(18783022., device='cuda:0')
c= tensor(18783590., device='cuda:0')
c= tensor(18783610., device='cuda:0')
c= tensor(18786960., device='cuda:0')
c= tensor(18787216., device='cuda:0')
c= tensor(18787634., device='cuda:0')
c= tensor(18788460., device='cuda:0')
c= tensor(18789106., device='cuda:0')
c= tensor(18789146., device='cuda:0')
c= tensor(18789914., device='cuda:0')
c= tensor(18790072., device='cuda:0')
c= tensor(18790152., device='cuda:0')
c= tensor(18790230., device='cuda:0')
c= tensor(18790382., device='cuda:0')
c= tensor(18790548., device='cuda:0')
c= tensor(18790682., device='cuda:0')
c= tensor(18790734., device='cuda:0')
c= tensor(18793394., device='cuda:0')
c= tensor(18794346., device='cuda:0')
c= tensor(18794388., device='cuda:0')
c= tensor(18794584., device='cuda:0')
c= tensor(18794736., device='cuda:0')
c= tensor(18795012., device='cuda:0')
c= tensor(18795534., device='cuda:0')
c= tensor(18795882., device='cuda:0')
c= tensor(18796302., device='cuda:0')
c= tensor(18796374., device='cuda:0')
c= tensor(18797004., device='cuda:0')
c= tensor(18797346., device='cuda:0')
c= tensor(18797470., device='cuda:0')
c= tensor(18797618., device='cuda:0')
c= tensor(18797756., device='cuda:0')
c= tensor(18798112., device='cuda:0')
c= tensor(18798792., device='cuda:0')
c= tensor(18798904., device='cuda:0')
c= tensor(18799402., device='cuda:0')
c= tensor(18802882., device='cuda:0')
c= tensor(18802972., device='cuda:0')
c= tensor(18802992., device='cuda:0')
c= tensor(18803804., device='cuda:0')
c= tensor(18803910., device='cuda:0')
c= tensor(18803974., device='cuda:0')
c= tensor(18804096., device='cuda:0')
c= tensor(18809480., device='cuda:0')
c= tensor(18809760., device='cuda:0')
c= tensor(18809936., device='cuda:0')
c= tensor(18810002., device='cuda:0')
c= tensor(18813576., device='cuda:0')
c= tensor(18814390., device='cuda:0')
c= tensor(18814954., device='cuda:0')
c= tensor(18815048., device='cuda:0')
c= tensor(18816376., device='cuda:0')
c= tensor(18816586., device='cuda:0')
c= tensor(18816852., device='cuda:0')
c= tensor(18816986., device='cuda:0')
c= tensor(18817014., device='cuda:0')
c= tensor(18817092., device='cuda:0')
c= tensor(18825588., device='cuda:0')
c= tensor(18825972., device='cuda:0')
c= tensor(18826190., device='cuda:0')
c= tensor(18826404., device='cuda:0')
c= tensor(18826874., device='cuda:0')
c= tensor(18828090., device='cuda:0')
c= tensor(18828818., device='cuda:0')
c= tensor(18829180., device='cuda:0')
c= tensor(18829250., device='cuda:0')
c= tensor(18829456., device='cuda:0')
c= tensor(18831236., device='cuda:0')
c= tensor(18831864., device='cuda:0')
c= tensor(18831926., device='cuda:0')
c= tensor(18831982., device='cuda:0')
c= tensor(18837376., device='cuda:0')
c= tensor(18837466., device='cuda:0')
c= tensor(18837718., device='cuda:0')
c= tensor(18838010., device='cuda:0')
c= tensor(18838200., device='cuda:0')
c= tensor(18838436., device='cuda:0')
c= tensor(18838506., device='cuda:0')
c= tensor(18839124., device='cuda:0')
c= tensor(18839940., device='cuda:0')
c= tensor(18840224., device='cuda:0')
c= tensor(18849118., device='cuda:0')
c= tensor(18849232., device='cuda:0')
c= tensor(18849918., device='cuda:0')
c= tensor(18849966., device='cuda:0')
c= tensor(18855604., device='cuda:0')
c= tensor(18856562., device='cuda:0')
c= tensor(18857002., device='cuda:0')
c= tensor(18857284., device='cuda:0')
c= tensor(18857452., device='cuda:0')
c= tensor(18857516., device='cuda:0')
c= tensor(18857548., device='cuda:0')
c= tensor(18857850., device='cuda:0')
c= tensor(18857926., device='cuda:0')
c= tensor(18858772., device='cuda:0')
c= tensor(18861020., device='cuda:0')
c= tensor(18861288., device='cuda:0')
c= tensor(18861362., device='cuda:0')
c= tensor(18866056., device='cuda:0')
c= tensor(18866614., device='cuda:0')
c= tensor(18867448., device='cuda:0')
c= tensor(18867646., device='cuda:0')
c= tensor(18867706., device='cuda:0')
c= tensor(18867760., device='cuda:0')
c= tensor(18867846., device='cuda:0')
c= tensor(18868186., device='cuda:0')
c= tensor(18868210., device='cuda:0')
c= tensor(18868606., device='cuda:0')
c= tensor(18868668., device='cuda:0')
c= tensor(18868884., device='cuda:0')
c= tensor(18869454., device='cuda:0')
c= tensor(18869792., device='cuda:0')
c= tensor(18869890., device='cuda:0')
c= tensor(18870096., device='cuda:0')
c= tensor(18870804., device='cuda:0')
c= tensor(18871042., device='cuda:0')
c= tensor(18871182., device='cuda:0')
c= tensor(18893452., device='cuda:0')
c= tensor(18893570., device='cuda:0')
c= tensor(18893812., device='cuda:0')
c= tensor(18894130., device='cuda:0')
c= tensor(18897920., device='cuda:0')
c= tensor(18898208., device='cuda:0')
c= tensor(18898830., device='cuda:0')
c= tensor(18899180., device='cuda:0')
c= tensor(18899442., device='cuda:0')
c= tensor(18899536., device='cuda:0')
c= tensor(18901376., device='cuda:0')
c= tensor(18902432., device='cuda:0')
c= tensor(18902580., device='cuda:0')
c= tensor(18902718., device='cuda:0')
c= tensor(18903916., device='cuda:0')
c= tensor(18908376., device='cuda:0')
c= tensor(18908396., device='cuda:0')
c= tensor(18908506., device='cuda:0')
c= tensor(18909494., device='cuda:0')
c= tensor(18910160., device='cuda:0')
c= tensor(18910350., device='cuda:0')
c= tensor(18910504., device='cuda:0')
c= tensor(18910588., device='cuda:0')
c= tensor(18910712., device='cuda:0')
c= tensor(18911222., device='cuda:0')
c= tensor(18911294., device='cuda:0')
c= tensor(18911568., device='cuda:0')
c= tensor(18911630., device='cuda:0')
c= tensor(18912108., device='cuda:0')
c= tensor(18912222., device='cuda:0')
c= tensor(18912368., device='cuda:0')
c= tensor(18912606., device='cuda:0')
c= tensor(18912926., device='cuda:0')
c= tensor(18913228., device='cuda:0')
c= tensor(18913906., device='cuda:0')
c= tensor(18913996., device='cuda:0')
c= tensor(18914448., device='cuda:0')
c= tensor(18919752., device='cuda:0')
c= tensor(18919824., device='cuda:0')
c= tensor(18920452., device='cuda:0')
c= tensor(18920784., device='cuda:0')
c= tensor(18920840., device='cuda:0')
c= tensor(18920890., device='cuda:0')
c= tensor(18921382., device='cuda:0')
c= tensor(18956142., device='cuda:0')
c= tensor(18956214., device='cuda:0')
c= tensor(18956248., device='cuda:0')
c= tensor(18956296., device='cuda:0')
c= tensor(18956718., device='cuda:0')
c= tensor(18956938., device='cuda:0')
c= tensor(18957006., device='cuda:0')
c= tensor(18957128., device='cuda:0')
c= tensor(18957166., device='cuda:0')
c= tensor(18957234., device='cuda:0')
c= tensor(18957568., device='cuda:0')
c= tensor(18957924., device='cuda:0')
c= tensor(18957952., device='cuda:0')
c= tensor(18958304., device='cuda:0')
c= tensor(18958344., device='cuda:0')
c= tensor(18958582., device='cuda:0')
c= tensor(18959100., device='cuda:0')
c= tensor(18959406., device='cuda:0')
c= tensor(18960804., device='cuda:0')
c= tensor(18961646., device='cuda:0')
c= tensor(18961866., device='cuda:0')
c= tensor(18963440., device='cuda:0')
c= tensor(18967692., device='cuda:0')
c= tensor(18968886., device='cuda:0')
c= tensor(18969250., device='cuda:0')
c= tensor(18969330., device='cuda:0')
c= tensor(18970080., device='cuda:0')
c= tensor(18987296., device='cuda:0')
c= tensor(18990046., device='cuda:0')
c= tensor(18990792., device='cuda:0')
c= tensor(18995110., device='cuda:0')
c= tensor(19030868., device='cuda:0')
c= tensor(19031304., device='cuda:0')
c= tensor(19033400., device='cuda:0')
c= tensor(19033952., device='cuda:0')
c= tensor(19034282., device='cuda:0')
c= tensor(19182232., device='cuda:0')
c= tensor(19411398., device='cuda:0')
c= tensor(19411596., device='cuda:0')
c= tensor(19437758., device='cuda:0')
c= tensor(19817312., device='cuda:0')
c= tensor(19821622., device='cuda:0')
c= tensor(19843216., device='cuda:0')
c= tensor(19860054., device='cuda:0')
c= tensor(19884176., device='cuda:0')
c= tensor(19896374., device='cuda:0')
c= tensor(19896484., device='cuda:0')
c= tensor(19903248., device='cuda:0')
c= tensor(19903600., device='cuda:0')
c= tensor(19903938., device='cuda:0')
c= tensor(19904962., device='cuda:0')
c= tensor(19907712., device='cuda:0')
c= tensor(19963224., device='cuda:0')
c= tensor(19963968., device='cuda:0')
c= tensor(19964270., device='cuda:0')
c= tensor(19964682., device='cuda:0')
c= tensor(19967444., device='cuda:0')
c= tensor(19971980., device='cuda:0')
c= tensor(20146844., device='cuda:0')
c= tensor(21471236., device='cuda:0')
c= tensor(21472202., device='cuda:0')
c= tensor(21472216., device='cuda:0')
c= tensor(21472296., device='cuda:0')
c= tensor(21496498., device='cuda:0')
c= tensor(21524008., device='cuda:0')
c= tensor(21526612., device='cuda:0')
c= tensor(21526682., device='cuda:0')
c= tensor(21681502., device='cuda:0')
c= tensor(21682180., device='cuda:0')
c= tensor(21682794., device='cuda:0')
c= tensor(22489514., device='cuda:0')
c= tensor(22490350., device='cuda:0')
c= tensor(22493350., device='cuda:0')
c= tensor(22808862., device='cuda:0')
c= tensor(25018600., device='cuda:0')
c= tensor(25020902., device='cuda:0')
c= tensor(25021726., device='cuda:0')
c= tensor(25021916., device='cuda:0')
c= tensor(25022110., device='cuda:0')
c= tensor(25068868., device='cuda:0')
c= tensor(25069040., device='cuda:0')
c= tensor(25069234., device='cuda:0')
c= tensor(25109468., device='cuda:0')
c= tensor(25119282., device='cuda:0')
c= tensor(25119440., device='cuda:0')
c= tensor(25120156., device='cuda:0')
c= tensor(29030916., device='cuda:0')
c= tensor(29032062., device='cuda:0')
c= tensor(29032668., device='cuda:0')
c= tensor(29034320., device='cuda:0')
c= tensor(29761032., device='cuda:0')
c= tensor(29761454., device='cuda:0')
c= tensor(29761854., device='cuda:0')
c= tensor(29762404., device='cuda:0')
c= tensor(29762778., device='cuda:0')
c= tensor(29782102., device='cuda:0')
c= tensor(29814608., device='cuda:0')
c= tensor(29898316., device='cuda:0')
c= tensor(29898586., device='cuda:0')
c= tensor(30041840., device='cuda:0')
c= tensor(30060916., device='cuda:0')
c= tensor(30061198., device='cuda:0')
c= tensor(30104408., device='cuda:0')
c= tensor(30203162., device='cuda:0')
c= tensor(30206994., device='cuda:0')
c= tensor(30239190., device='cuda:0')
c= tensor(30239420., device='cuda:0')
c= tensor(30241026., device='cuda:0')
c= tensor(30242802., device='cuda:0')
c= tensor(30243804., device='cuda:0')
c= tensor(30244304., device='cuda:0')
c= tensor(30244444., device='cuda:0')
c= tensor(30245778., device='cuda:0')
c= tensor(30251614., device='cuda:0')
c= tensor(30276198., device='cuda:0')
c= tensor(30276562., device='cuda:0')
c= tensor(30277926., device='cuda:0')
c= tensor(30282060., device='cuda:0')
c= tensor(30282978., device='cuda:0')
c= tensor(30284244., device='cuda:0')
c= tensor(30284694., device='cuda:0')
c= tensor(39537408., device='cuda:0')
c= tensor(39541032., device='cuda:0')
c= tensor(39541260., device='cuda:0')
c= tensor(39656692., device='cuda:0')
c= tensor(39656868., device='cuda:0')
c= tensor(40405784., device='cuda:0')
c= tensor(40410740., device='cuda:0')
c= tensor(40413036., device='cuda:0')
c= tensor(40413452., device='cuda:0')
c= tensor(40413816., device='cuda:0')
c= tensor(40414468., device='cuda:0')
c= tensor(40419248., device='cuda:0')
c= tensor(40419744., device='cuda:0')
c= tensor(40422336., device='cuda:0')
c= tensor(40422396., device='cuda:0')
c= tensor(40422496., device='cuda:0')
c= tensor(40524780., device='cuda:0')
c= tensor(40545048., device='cuda:0')
c= tensor(40602344., device='cuda:0')
c= tensor(40613744., device='cuda:0')
c= tensor(41343372., device='cuda:0')
c= tensor(41348560., device='cuda:0')
c= tensor(41350332., device='cuda:0')
c= tensor(41351280., device='cuda:0')
c= tensor(41351344., device='cuda:0')
c= tensor(41351408., device='cuda:0')
c= tensor(41357104., device='cuda:0')
c= tensor(41358084., device='cuda:0')
c= tensor(41358408., device='cuda:0')
c= tensor(41358972., device='cuda:0')
c= tensor(41362832., device='cuda:0')
c= tensor(48587480., device='cuda:0')
c= tensor(48587740., device='cuda:0')
c= tensor(48590240., device='cuda:0')
c= tensor(48590712., device='cuda:0')
c= tensor(48590792., device='cuda:0')
c= tensor(48591828., device='cuda:0')
c= tensor(48924312., device='cuda:0')
c= tensor(56627460., device='cuda:0')
c= tensor(56629280., device='cuda:0')
c= tensor(56654708., device='cuda:0')
c= tensor(56654792., device='cuda:0')
c= tensor(56656084., device='cuda:0')
c= tensor(62640212., device='cuda:0')
c= tensor(62858640., device='cuda:0')
c= tensor(62860176., device='cuda:0')
c= tensor(62989296., device='cuda:0')
c= tensor(63646656., device='cuda:0')
c= tensor(63656172., device='cuda:0')
c= tensor(63656896., device='cuda:0')
c= tensor(63657040., device='cuda:0')
c= tensor(63664096., device='cuda:0')
c= tensor(63664432., device='cuda:0')
c= tensor(63783620., device='cuda:0')
c= tensor(63811000., device='cuda:0')
c= tensor(63811224., device='cuda:0')
c= tensor(63831308., device='cuda:0')
c= tensor(63831904., device='cuda:0')
c= tensor(63832004., device='cuda:0')
c= tensor(63840104., device='cuda:0')
c= tensor(63898568., device='cuda:0')
c= tensor(64306352., device='cuda:0')
c= tensor(64314968., device='cuda:0')
c= tensor(65182240., device='cuda:0')
c= tensor(65195388., device='cuda:0')
c= tensor(65205164., device='cuda:0')
c= tensor(65214220., device='cuda:0')
c= tensor(65222760., device='cuda:0')
c= tensor(65223268., device='cuda:0')
c= tensor(65882360., device='cuda:0')
c= tensor(65886496., device='cuda:0')
c= tensor(65928168., device='cuda:0')
c= tensor(66160212., device='cuda:0')
c= tensor(66160636., device='cuda:0')
c= tensor(66160808., device='cuda:0')
c= tensor(66160884., device='cuda:0')
c= tensor(66520896., device='cuda:0')
c= tensor(67793544., device='cuda:0')
c= tensor(68000904., device='cuda:0')
c= tensor(68036272., device='cuda:0')
c= tensor(69152048., device='cuda:0')
c= tensor(69158720., device='cuda:0')
c= tensor(69158824., device='cuda:0')
c= tensor(69629352., device='cuda:0')
c= tensor(69629712., device='cuda:0')
c= tensor(69630776., device='cuda:0')
c= tensor(70752232., device='cuda:0')
c= tensor(70755104., device='cuda:0')
c= tensor(70755424., device='cuda:0')
c= tensor(70755968., device='cuda:0')
c= tensor(71680568., device='cuda:0')
c= tensor(71690480., device='cuda:0')
c= tensor(71708272., device='cuda:0')
c= tensor(71708408., device='cuda:0')
c= tensor(71709328., device='cuda:0')
c= tensor(71709944., device='cuda:0')
c= tensor(71889576., device='cuda:0')
c= tensor(71890744., device='cuda:0')
c= tensor(71906128., device='cuda:0')
c= tensor(71907112., device='cuda:0')
c= tensor(71971640., device='cuda:0')
c= tensor(71971832., device='cuda:0')
c= tensor(72047696., device='cuda:0')
c= tensor(72070376., device='cuda:0')
c= tensor(72075136., device='cuda:0')
c= tensor(72075672., device='cuda:0')
c= tensor(72083688., device='cuda:0')
c= tensor(72083936., device='cuda:0')
c= tensor(72091264., device='cuda:0')
c= tensor(72095584., device='cuda:0')
c= tensor(72104184., device='cuda:0')
c= tensor(72104408., device='cuda:0')
c= tensor(72104744., device='cuda:0')
c= tensor(72120304., device='cuda:0')
c= tensor(72121488., device='cuda:0')
c= tensor(87766432., device='cuda:0')
c= tensor(87766536., device='cuda:0')
c= tensor(87770272., device='cuda:0')
c= tensor(87884488., device='cuda:0')
c= tensor(87885248., device='cuda:0')
c= tensor(87970056., device='cuda:0')
c= tensor(87970616., device='cuda:0')
c= tensor(88051112., device='cuda:0')
c= tensor(88054336., device='cuda:0')
c= tensor(88055576., device='cuda:0')
c= tensor(88055728., device='cuda:0')
c= tensor(88055832., device='cuda:0')
c= tensor(88056544., device='cuda:0')
c= tensor(88056944., device='cuda:0')
c= tensor(88072296., device='cuda:0')
c= tensor(88098568., device='cuda:0')
c= tensor(88100224., device='cuda:0')
c= tensor(88100512., device='cuda:0')
c= tensor(88100896., device='cuda:0')
c= tensor(88510072., device='cuda:0')
c= tensor(88515064., device='cuda:0')
c= tensor(88675928., device='cuda:0')
c= tensor(88676824., device='cuda:0')
c= tensor(88676936., device='cuda:0')
c= tensor(88677112., device='cuda:0')
c= tensor(88678344., device='cuda:0')
c= tensor(88688096., device='cuda:0')
c= tensor(88702216., device='cuda:0')
c= tensor(88702752., device='cuda:0')
c= tensor(88783328., device='cuda:0')
c= tensor(88796368., device='cuda:0')
c= tensor(88796640., device='cuda:0')
c= tensor(88797496., device='cuda:0')
c= tensor(88799704., device='cuda:0')
c= tensor(88801624., device='cuda:0')
c= tensor(88806968., device='cuda:0')
c= tensor(88807472., device='cuda:0')
c= tensor(88995344., device='cuda:0')
c= tensor(89132576., device='cuda:0')
c= tensor(90028992., device='cuda:0')
c= tensor(90339112., device='cuda:0')
c= tensor(90342992., device='cuda:0')
c= tensor(90384288., device='cuda:0')
c= tensor(90393056., device='cuda:0')
c= tensor(90393848., device='cuda:0')
c= tensor(90393944., device='cuda:0')
c= tensor(90394664., device='cuda:0')
c= tensor(90395168., device='cuda:0')
c= tensor(90396352., device='cuda:0')
c= tensor(90396400., device='cuda:0')
c= tensor(90398280., device='cuda:0')
c= tensor(90406528., device='cuda:0')
c= tensor(92197032., device='cuda:0')
c= tensor(92197080., device='cuda:0')
c= tensor(92197184., device='cuda:0')
c= tensor(92209168., device='cuda:0')
c= tensor(92210112., device='cuda:0')
c= tensor(92212576., device='cuda:0')
c= tensor(92212744., device='cuda:0')
c= tensor(92224656., device='cuda:0')
c= tensor(92335912., device='cuda:0')
c= tensor(92336112., device='cuda:0')
c= tensor(92336184., device='cuda:0')
c= tensor(92336208., device='cuda:0')
c= tensor(92370608., device='cuda:0')
c= tensor(1.0611e+08, device='cuda:0')
c= tensor(1.0611e+08, device='cuda:0')
c= tensor(1.0611e+08, device='cuda:0')
c= tensor(1.0615e+08, device='cuda:0')
c= tensor(1.0619e+08, device='cuda:0')
c= tensor(1.0619e+08, device='cuda:0')
c= tensor(1.0620e+08, device='cuda:0')
c= tensor(1.0620e+08, device='cuda:0')
c= tensor(1.3979e+08, device='cuda:0')
c= tensor(1.3981e+08, device='cuda:0')
c= tensor(1.3981e+08, device='cuda:0')
c= tensor(1.3981e+08, device='cuda:0')
c= tensor(1.3981e+08, device='cuda:0')
c= tensor(1.3981e+08, device='cuda:0')
c= tensor(1.4003e+08, device='cuda:0')
c= tensor(1.4003e+08, device='cuda:0')
c= tensor(1.4004e+08, device='cuda:0')
c= tensor(1.4024e+08, device='cuda:0')
c= tensor(1.4762e+08, device='cuda:0')
c= tensor(1.4763e+08, device='cuda:0')
c= tensor(1.4763e+08, device='cuda:0')
c= tensor(1.4793e+08, device='cuda:0')
c= tensor(1.4794e+08, device='cuda:0')
c= tensor(1.4813e+08, device='cuda:0')
c= tensor(1.4813e+08, device='cuda:0')
c= tensor(1.4813e+08, device='cuda:0')
c= tensor(1.4813e+08, device='cuda:0')
c= tensor(1.4813e+08, device='cuda:0')
c= tensor(1.4813e+08, device='cuda:0')
c= tensor(1.4813e+08, device='cuda:0')
c= tensor(1.4814e+08, device='cuda:0')
c= tensor(1.4814e+08, device='cuda:0')
c= tensor(1.4814e+08, device='cuda:0')
c= tensor(1.4814e+08, device='cuda:0')
c= tensor(1.4850e+08, device='cuda:0')
c= tensor(1.4974e+08, device='cuda:0')
c= tensor(1.4975e+08, device='cuda:0')
c= tensor(1.4975e+08, device='cuda:0')
c= tensor(1.4976e+08, device='cuda:0')
c= tensor(1.4976e+08, device='cuda:0')
c= tensor(1.4976e+08, device='cuda:0')
c= tensor(1.4987e+08, device='cuda:0')
c= tensor(1.4988e+08, device='cuda:0')
c= tensor(1.4992e+08, device='cuda:0')
c= tensor(1.4992e+08, device='cuda:0')
c= tensor(1.5002e+08, device='cuda:0')
c= tensor(1.5002e+08, device='cuda:0')
c= tensor(1.5009e+08, device='cuda:0')
c= tensor(1.5061e+08, device='cuda:0')
c= tensor(1.5062e+08, device='cuda:0')
c= tensor(1.5063e+08, device='cuda:0')
c= tensor(1.5063e+08, device='cuda:0')
c= tensor(1.5066e+08, device='cuda:0')
c= tensor(1.5067e+08, device='cuda:0')
c= tensor(1.5067e+08, device='cuda:0')
c= tensor(1.5067e+08, device='cuda:0')
c= tensor(1.5067e+08, device='cuda:0')
c= tensor(1.5071e+08, device='cuda:0')
c= tensor(1.5277e+08, device='cuda:0')
c= tensor(1.5298e+08, device='cuda:0')
c= tensor(1.5298e+08, device='cuda:0')
c= tensor(1.5298e+08, device='cuda:0')
c= tensor(1.5298e+08, device='cuda:0')
c= tensor(1.5299e+08, device='cuda:0')
c= tensor(1.5299e+08, device='cuda:0')
c= tensor(1.5462e+08, device='cuda:0')
c= tensor(1.5463e+08, device='cuda:0')
c= tensor(1.5463e+08, device='cuda:0')
c= tensor(1.6348e+08, device='cuda:0')
c= tensor(1.6348e+08, device='cuda:0')
c= tensor(1.6348e+08, device='cuda:0')
c= tensor(1.6348e+08, device='cuda:0')
c= tensor(1.6348e+08, device='cuda:0')
c= tensor(1.6348e+08, device='cuda:0')
c= tensor(1.6348e+08, device='cuda:0')
c= tensor(1.7282e+08, device='cuda:0')
c= tensor(1.7287e+08, device='cuda:0')
c= tensor(1.7288e+08, device='cuda:0')
c= tensor(1.7289e+08, device='cuda:0')
c= tensor(1.7289e+08, device='cuda:0')
c= tensor(1.7289e+08, device='cuda:0')
c= tensor(1.7290e+08, device='cuda:0')
c= tensor(1.7290e+08, device='cuda:0')
c= tensor(1.7290e+08, device='cuda:0')
c= tensor(1.7292e+08, device='cuda:0')
c= tensor(1.7293e+08, device='cuda:0')
c= tensor(1.7293e+08, device='cuda:0')
c= tensor(1.7293e+08, device='cuda:0')
c= tensor(1.7293e+08, device='cuda:0')
c= tensor(1.7294e+08, device='cuda:0')
c= tensor(1.7313e+08, device='cuda:0')
c= tensor(1.7313e+08, device='cuda:0')
c= tensor(1.7931e+08, device='cuda:0')
c= tensor(1.7931e+08, device='cuda:0')
c= tensor(1.7951e+08, device='cuda:0')
c= tensor(1.7951e+08, device='cuda:0')
c= tensor(1.7953e+08, device='cuda:0')
c= tensor(1.8203e+08, device='cuda:0')
c= tensor(1.8207e+08, device='cuda:0')
c= tensor(1.8207e+08, device='cuda:0')
c= tensor(1.8208e+08, device='cuda:0')
c= tensor(1.8209e+08, device='cuda:0')
c= tensor(1.8211e+08, device='cuda:0')
c= tensor(1.8232e+08, device='cuda:0')
c= tensor(1.8232e+08, device='cuda:0')
c= tensor(1.8232e+08, device='cuda:0')
c= tensor(1.8233e+08, device='cuda:0')
c= tensor(1.8255e+08, device='cuda:0')
c= tensor(1.8258e+08, device='cuda:0')
c= tensor(1.8258e+08, device='cuda:0')
c= tensor(1.8258e+08, device='cuda:0')
c= tensor(1.8467e+08, device='cuda:0')
c= tensor(1.8472e+08, device='cuda:0')
c= tensor(1.8758e+08, device='cuda:0')
c= tensor(1.8759e+08, device='cuda:0')
c= tensor(1.8767e+08, device='cuda:0')
c= tensor(1.8767e+08, device='cuda:0')
c= tensor(1.8782e+08, device='cuda:0')
c= tensor(1.8783e+08, device='cuda:0')
c= tensor(1.8784e+08, device='cuda:0')
c= tensor(1.8784e+08, device='cuda:0')
c= tensor(1.8786e+08, device='cuda:0')
c= tensor(1.8787e+08, device='cuda:0')
c= tensor(1.8789e+08, device='cuda:0')
c= tensor(1.8790e+08, device='cuda:0')
c= tensor(1.8790e+08, device='cuda:0')
c= tensor(1.8790e+08, device='cuda:0')
c= tensor(1.8823e+08, device='cuda:0')
c= tensor(1.8824e+08, device='cuda:0')
c= tensor(1.8845e+08, device='cuda:0')
c= tensor(1.8863e+08, device='cuda:0')
c= tensor(1.8912e+08, device='cuda:0')
c= tensor(1.8913e+08, device='cuda:0')
c= tensor(1.8913e+08, device='cuda:0')
c= tensor(1.8913e+08, device='cuda:0')
c= tensor(1.8914e+08, device='cuda:0')
c= tensor(1.8914e+08, device='cuda:0')
c= tensor(1.8914e+08, device='cuda:0')
c= tensor(1.8914e+08, device='cuda:0')
c= tensor(1.8914e+08, device='cuda:0')
c= tensor(1.8962e+08, device='cuda:0')
c= tensor(1.8962e+08, device='cuda:0')
c= tensor(1.8963e+08, device='cuda:0')
c= tensor(1.8963e+08, device='cuda:0')
c= tensor(1.8963e+08, device='cuda:0')
c= tensor(1.8963e+08, device='cuda:0')
c= tensor(1.8967e+08, device='cuda:0')
c= tensor(1.8980e+08, device='cuda:0')
c= tensor(1.8980e+08, device='cuda:0')
c= tensor(1.8981e+08, device='cuda:0')
c= tensor(1.8981e+08, device='cuda:0')
c= tensor(1.8981e+08, device='cuda:0')
c= tensor(1.8983e+08, device='cuda:0')
c= tensor(1.8984e+08, device='cuda:0')
c= tensor(1.8984e+08, device='cuda:0')
c= tensor(1.8992e+08, device='cuda:0')
c= tensor(1.8992e+08, device='cuda:0')
c= tensor(1.8997e+08, device='cuda:0')
c= tensor(1.9005e+08, device='cuda:0')
c= tensor(1.9005e+08, device='cuda:0')
c= tensor(1.9006e+08, device='cuda:0')
c= tensor(1.9006e+08, device='cuda:0')
c= tensor(1.9011e+08, device='cuda:0')
c= tensor(1.9023e+08, device='cuda:0')
c= tensor(1.9023e+08, device='cuda:0')
c= tensor(1.9023e+08, device='cuda:0')
c= tensor(1.9025e+08, device='cuda:0')
c= tensor(1.9025e+08, device='cuda:0')
c= tensor(1.9025e+08, device='cuda:0')
c= tensor(1.9025e+08, device='cuda:0')
c= tensor(1.9047e+08, device='cuda:0')
c= tensor(1.9286e+08, device='cuda:0')
c= tensor(1.9287e+08, device='cuda:0')
c= tensor(1.9287e+08, device='cuda:0')
c= tensor(1.9287e+08, device='cuda:0')
c= tensor(1.9293e+08, device='cuda:0')
c= tensor(1.9293e+08, device='cuda:0')
c= tensor(1.9293e+08, device='cuda:0')
c= tensor(1.9293e+08, device='cuda:0')
c= tensor(1.9332e+08, device='cuda:0')
c= tensor(1.9332e+08, device='cuda:0')
c= tensor(1.9333e+08, device='cuda:0')
c= tensor(1.9334e+08, device='cuda:0')
memory (bytes)
3149406208
time for making loss 2 is 14.719672679901123
p0 True
it  0 : 375389696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
3149672448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  3% |
memory (bytes)
3150397440
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  4% |
error is  2080840300.0
relative error loss 10.762857
shape of L is 
torch.Size([])
memory (bytes)
3435679744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  3% |  5% |
memory (bytes)
3435827200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  5% |
error is  2080814200.0
relative error loss 10.762723
shape of L is 
torch.Size([])
memory (bytes)
3437740032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  5% |
memory (bytes)
3437744128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  5% |
error is  2080695600.0
relative error loss 10.762109
shape of L is 
torch.Size([])
memory (bytes)
3438850048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  5% |  5% |
memory (bytes)
3438850048
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  5% |
error is  2079551500.0
relative error loss 10.756191
shape of L is 
torch.Size([])
memory (bytes)
3440680960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  5% |  5% |
memory (bytes)
3440685056
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  5% |
error is  2067985000.0
relative error loss 10.696365
shape of L is 
torch.Size([])
memory (bytes)
3442757632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  5% |  5% |
memory (bytes)
3442765824
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  5% |
error is  2010919400.0
relative error loss 10.401202
shape of L is 
torch.Size([])
memory (bytes)
3444875264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  5% |  5% |
memory (bytes)
3444875264
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  5% |
error is  1468238300.0
relative error loss 7.5942593
shape of L is 
torch.Size([])
memory (bytes)
3446960128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  5% |  5% |
memory (bytes)
3446964224
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  5% |
error is  489070940.0
relative error loss 2.5296516
shape of L is 
torch.Size([])
memory (bytes)
3449090048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  5% |  5% |
memory (bytes)
3449090048
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  5% |
error is  298489150.0
relative error loss 1.5438937
shape of L is 
torch.Size([])
memory (bytes)
3451191296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  5% |  5% |
memory (bytes)
3451195392
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  303615780.0
relative error loss 1.5704105
shape of L is 
torch.Size([])
memory (bytes)
3453321216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  5% |  5% |
memory (bytes)
3453321216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  6% |
error is  236549340.0
relative error loss 1.2235186
time to take a step is 248.82845330238342
it  1 : 756521472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  4% |  6% |
shape of L is 
torch.Size([])
memory (bytes)
3455459328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% |  5% |
memory (bytes)
3455463424
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  5% |
error is  236549340.0
relative error loss 1.2235186
shape of L is 
torch.Size([])
memory (bytes)
3457593344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% |  5% |
memory (bytes)
3457593344
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  202259250.0
relative error loss 1.046158
shape of L is 
torch.Size([])
memory (bytes)
3459715072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  5% |  5% |
memory (bytes)
3459719168
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  186405570.0
relative error loss 0.9641569
shape of L is 
torch.Size([])
memory (bytes)
3461816320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  5% |  5% |
memory (bytes)
3461816320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  495682600.0
relative error loss 2.5638494
shape of L is 
torch.Size([])
memory (bytes)
3463843840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  5% |  6% |
memory (bytes)
3463843840
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  171877650.0
relative error loss 0.8890133
shape of L is 
torch.Size([])
memory (bytes)
3466076160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  5% |  5% |
memory (bytes)
3466076160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  6% |
error is  160275520.0
relative error loss 0.8290029
shape of L is 
torch.Size([])
memory (bytes)
3468218368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  5% |
memory (bytes)
3468218368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  151357500.0
relative error loss 0.7828757
shape of L is 
torch.Size([])
memory (bytes)
3470327808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  5% |  5% |
memory (bytes)
3470327808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  137209470.0
relative error loss 0.70969695
shape of L is 
torch.Size([])
memory (bytes)
3472429056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  5% |
memory (bytes)
3472429056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  122833490.0
relative error loss 0.6353392
shape of L is 
torch.Size([])
memory (bytes)
3474579456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  5% |  5% |
memory (bytes)
3474579456
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  147062370.0
relative error loss 0.7606597
shape of L is 
torch.Size([])
memory (bytes)
3476692992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% |  6% |
memory (bytes)
3476697088
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  113577336.0
relative error loss 0.587463
time to take a step is 241.8269603252411
it  2 : 756521472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
shape of L is 
torch.Size([])
memory (bytes)
3478818816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  5% |
memory (bytes)
3478818816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% |  5% |
error is  113577336.0
relative error loss 0.587463
shape of L is 
torch.Size([])
memory (bytes)
3480895488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  5% |  5% |
memory (bytes)
3480895488
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  102734030.0
relative error loss 0.53137755
shape of L is 
torch.Size([])
memory (bytes)
3483021312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% |  5% |
memory (bytes)
3483021312
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  94356240.0
relative error loss 0.48804456
shape of L is 
torch.Size([])
memory (bytes)
3485147136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% |  5% |
memory (bytes)
3485147136
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  86900300.0
relative error loss 0.44947976
shape of L is 
torch.Size([])
memory (bytes)
3487252480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  5% |  6% |
memory (bytes)
3487256576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  81800610.0
relative error loss 0.4231023
shape of L is 
torch.Size([])
memory (bytes)
3489316864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  5% |  5% |
memory (bytes)
3489316864
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  69883920.0
relative error loss 0.3614649
shape of L is 
torch.Size([])
memory (bytes)
3491508224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  4% |  5% |
memory (bytes)
3491508224
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  58987200.0
relative error loss 0.30510312
shape of L is 
torch.Size([])
memory (bytes)
3493478400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  5% |  5% |
memory (bytes)
3493478400
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  49608976.0
relative error loss 0.25659555
shape of L is 
torch.Size([])
memory (bytes)
3495739392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  4% |  5% |
memory (bytes)
3495739392
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  43939904.0
relative error loss 0.22727306
shape of L is 
torch.Size([])
memory (bytes)
3497902080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  5% |
memory (bytes)
3497902080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  38149120.0
relative error loss 0.19732103
time to take a step is 221.94989466667175
it  3 : 756520960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  4% |  6% |
shape of L is 
torch.Size([])
memory (bytes)
3499814912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% |  5% |
memory (bytes)
3500023808
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  5% |
error is  38149120.0
relative error loss 0.19732103
shape of L is 
torch.Size([])
memory (bytes)
3501920256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  5% |  5% |
memory (bytes)
3501920256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% |  6% |
error is  33205392.0
relative error loss 0.17175028
shape of L is 
torch.Size([])
memory (bytes)
3504300032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  5% |  5% |
memory (bytes)
3504304128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  30375856.0
relative error loss 0.15711491
shape of L is 
torch.Size([])
memory (bytes)
3506442240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  5% |
memory (bytes)
3506446336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  28180224.0
relative error loss 0.1457583
shape of L is 
torch.Size([])
memory (bytes)
3508592640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  5% |  5% |
memory (bytes)
3508592640
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  24965536.0
relative error loss 0.12913077
shape of L is 
torch.Size([])
memory (bytes)
3510722560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3510722560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  23030336.0
relative error loss 0.119121216
shape of L is 
torch.Size([])
memory (bytes)
3512864768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  5% |  5% |
memory (bytes)
3512868864
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  21091568.0
relative error loss 0.10909321
shape of L is 
torch.Size([])
memory (bytes)
3515019264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  5% |
memory (bytes)
3515019264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  18976032.0
relative error loss 0.098150894
shape of L is 
torch.Size([])
memory (bytes)
3517153280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  4% |  5% |
memory (bytes)
3517157376
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  17733216.0
relative error loss 0.0917226
shape of L is 
torch.Size([])
memory (bytes)
3519283200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  5% |  5% |
memory (bytes)
3519283200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  16595568.0
relative error loss 0.08583827
time to take a step is 222.9288992881775
c= tensor(200.6332, device='cuda:0')
c= tensor(574.5375, device='cuda:0')
c= tensor(652.6404, device='cuda:0')
c= tensor(783.8591, device='cuda:0')
c= tensor(14781.3594, device='cuda:0')
c= tensor(19591.3008, device='cuda:0')
c= tensor(20348.6172, device='cuda:0')
c= tensor(40111.6406, device='cuda:0')
c= tensor(81543.5781, device='cuda:0')
c= tensor(98464.2656, device='cuda:0')
c= tensor(99263.6484, device='cuda:0')
c= tensor(103813.9062, device='cuda:0')
c= tensor(104242.3594, device='cuda:0')
c= tensor(2425885.2500, device='cuda:0')
c= tensor(2426291.2500, device='cuda:0')
c= tensor(2443467.2500, device='cuda:0')
c= tensor(2443839., device='cuda:0')
c= tensor(2447593.7500, device='cuda:0')
c= tensor(4042896.5000, device='cuda:0')
c= tensor(5285958.5000, device='cuda:0')
c= tensor(5286849., device='cuda:0')
c= tensor(5447675.5000, device='cuda:0')
c= tensor(5448263., device='cuda:0')
c= tensor(5448917., device='cuda:0')
c= tensor(5449291., device='cuda:0')
c= tensor(5453619., device='cuda:0')
c= tensor(5999790., device='cuda:0')
c= tensor(6001425.5000, device='cuda:0')
c= tensor(6613208., device='cuda:0')
c= tensor(13733029., device='cuda:0')
c= tensor(13740164., device='cuda:0')
c= tensor(14651565., device='cuda:0')
c= tensor(14653656., device='cuda:0')
c= tensor(14663172., device='cuda:0')
c= tensor(14663334., device='cuda:0')
c= tensor(18706810., device='cuda:0')
c= tensor(18706980., device='cuda:0')
c= tensor(18707530., device='cuda:0')
c= tensor(18708054., device='cuda:0')
c= tensor(18708692., device='cuda:0')
c= tensor(18708876., device='cuda:0')
c= tensor(18709684., device='cuda:0')
c= tensor(18709744., device='cuda:0')
c= tensor(18709824., device='cuda:0')
c= tensor(18710140., device='cuda:0')
c= tensor(18710236., device='cuda:0')
c= tensor(18710342., device='cuda:0')
c= tensor(18710378., device='cuda:0')
c= tensor(18710472., device='cuda:0')
c= tensor(18710582., device='cuda:0')
c= tensor(18774290., device='cuda:0')
c= tensor(18774380., device='cuda:0')
c= tensor(18778074., device='cuda:0')
c= tensor(18778216., device='cuda:0')
c= tensor(18778748., device='cuda:0')
c= tensor(18782260., device='cuda:0')
c= tensor(18782792., device='cuda:0')
c= tensor(18783022., device='cuda:0')
c= tensor(18783590., device='cuda:0')
c= tensor(18783610., device='cuda:0')
c= tensor(18786960., device='cuda:0')
c= tensor(18787216., device='cuda:0')
c= tensor(18787634., device='cuda:0')
c= tensor(18788460., device='cuda:0')
c= tensor(18789106., device='cuda:0')
c= tensor(18789146., device='cuda:0')
c= tensor(18789914., device='cuda:0')
c= tensor(18790072., device='cuda:0')
c= tensor(18790152., device='cuda:0')
c= tensor(18790230., device='cuda:0')
c= tensor(18790382., device='cuda:0')
c= tensor(18790548., device='cuda:0')
c= tensor(18790682., device='cuda:0')
c= tensor(18790734., device='cuda:0')
c= tensor(18793394., device='cuda:0')
c= tensor(18794346., device='cuda:0')
c= tensor(18794388., device='cuda:0')
c= tensor(18794584., device='cuda:0')
c= tensor(18794736., device='cuda:0')
c= tensor(18795012., device='cuda:0')
c= tensor(18795534., device='cuda:0')
c= tensor(18795882., device='cuda:0')
c= tensor(18796302., device='cuda:0')
c= tensor(18796374., device='cuda:0')
c= tensor(18797004., device='cuda:0')
c= tensor(18797346., device='cuda:0')
c= tensor(18797470., device='cuda:0')
c= tensor(18797618., device='cuda:0')
c= tensor(18797756., device='cuda:0')
c= tensor(18798112., device='cuda:0')
c= tensor(18798792., device='cuda:0')
c= tensor(18798904., device='cuda:0')
c= tensor(18799402., device='cuda:0')
c= tensor(18802882., device='cuda:0')
c= tensor(18802972., device='cuda:0')
c= tensor(18802992., device='cuda:0')
c= tensor(18803804., device='cuda:0')
c= tensor(18803910., device='cuda:0')
c= tensor(18803974., device='cuda:0')
c= tensor(18804096., device='cuda:0')
c= tensor(18809480., device='cuda:0')
c= tensor(18809760., device='cuda:0')
c= tensor(18809936., device='cuda:0')
c= tensor(18810002., device='cuda:0')
c= tensor(18813576., device='cuda:0')
c= tensor(18814390., device='cuda:0')
c= tensor(18814954., device='cuda:0')
c= tensor(18815048., device='cuda:0')
c= tensor(18816376., device='cuda:0')
c= tensor(18816586., device='cuda:0')
c= tensor(18816852., device='cuda:0')
c= tensor(18816986., device='cuda:0')
c= tensor(18817014., device='cuda:0')
c= tensor(18817092., device='cuda:0')
c= tensor(18825588., device='cuda:0')
c= tensor(18825972., device='cuda:0')
c= tensor(18826190., device='cuda:0')
c= tensor(18826404., device='cuda:0')
c= tensor(18826874., device='cuda:0')
c= tensor(18828090., device='cuda:0')
c= tensor(18828818., device='cuda:0')
c= tensor(18829180., device='cuda:0')
c= tensor(18829250., device='cuda:0')
c= tensor(18829456., device='cuda:0')
c= tensor(18831236., device='cuda:0')
c= tensor(18831864., device='cuda:0')
c= tensor(18831926., device='cuda:0')
c= tensor(18831982., device='cuda:0')
c= tensor(18837376., device='cuda:0')
c= tensor(18837466., device='cuda:0')
c= tensor(18837718., device='cuda:0')
c= tensor(18838010., device='cuda:0')
c= tensor(18838200., device='cuda:0')
c= tensor(18838436., device='cuda:0')
c= tensor(18838506., device='cuda:0')
c= tensor(18839124., device='cuda:0')
c= tensor(18839940., device='cuda:0')
c= tensor(18840224., device='cuda:0')
c= tensor(18849118., device='cuda:0')
c= tensor(18849232., device='cuda:0')
c= tensor(18849918., device='cuda:0')
c= tensor(18849966., device='cuda:0')
c= tensor(18855604., device='cuda:0')
c= tensor(18856562., device='cuda:0')
c= tensor(18857002., device='cuda:0')
c= tensor(18857284., device='cuda:0')
c= tensor(18857452., device='cuda:0')
c= tensor(18857516., device='cuda:0')
c= tensor(18857548., device='cuda:0')
c= tensor(18857850., device='cuda:0')
c= tensor(18857926., device='cuda:0')
c= tensor(18858772., device='cuda:0')
c= tensor(18861020., device='cuda:0')
c= tensor(18861288., device='cuda:0')
c= tensor(18861362., device='cuda:0')
c= tensor(18866056., device='cuda:0')
c= tensor(18866614., device='cuda:0')
c= tensor(18867448., device='cuda:0')
c= tensor(18867646., device='cuda:0')
c= tensor(18867706., device='cuda:0')
c= tensor(18867760., device='cuda:0')
c= tensor(18867846., device='cuda:0')
c= tensor(18868186., device='cuda:0')
c= tensor(18868210., device='cuda:0')
c= tensor(18868606., device='cuda:0')
c= tensor(18868668., device='cuda:0')
c= tensor(18868884., device='cuda:0')
c= tensor(18869454., device='cuda:0')
c= tensor(18869792., device='cuda:0')
c= tensor(18869890., device='cuda:0')
c= tensor(18870096., device='cuda:0')
c= tensor(18870804., device='cuda:0')
c= tensor(18871042., device='cuda:0')
c= tensor(18871182., device='cuda:0')
c= tensor(18893452., device='cuda:0')
c= tensor(18893570., device='cuda:0')
c= tensor(18893812., device='cuda:0')
c= tensor(18894130., device='cuda:0')
c= tensor(18897920., device='cuda:0')
c= tensor(18898208., device='cuda:0')
c= tensor(18898830., device='cuda:0')
c= tensor(18899180., device='cuda:0')
c= tensor(18899442., device='cuda:0')
c= tensor(18899536., device='cuda:0')
c= tensor(18901376., device='cuda:0')
c= tensor(18902432., device='cuda:0')
c= tensor(18902580., device='cuda:0')
c= tensor(18902718., device='cuda:0')
c= tensor(18903916., device='cuda:0')
c= tensor(18908376., device='cuda:0')
c= tensor(18908396., device='cuda:0')
c= tensor(18908506., device='cuda:0')
c= tensor(18909494., device='cuda:0')
c= tensor(18910160., device='cuda:0')
c= tensor(18910350., device='cuda:0')
c= tensor(18910504., device='cuda:0')
c= tensor(18910588., device='cuda:0')
c= tensor(18910712., device='cuda:0')
c= tensor(18911222., device='cuda:0')
c= tensor(18911294., device='cuda:0')
c= tensor(18911568., device='cuda:0')
c= tensor(18911630., device='cuda:0')
c= tensor(18912108., device='cuda:0')
c= tensor(18912222., device='cuda:0')
c= tensor(18912368., device='cuda:0')
c= tensor(18912606., device='cuda:0')
c= tensor(18912926., device='cuda:0')
c= tensor(18913228., device='cuda:0')
c= tensor(18913906., device='cuda:0')
c= tensor(18913996., device='cuda:0')
c= tensor(18914448., device='cuda:0')
c= tensor(18919752., device='cuda:0')
c= tensor(18919824., device='cuda:0')
c= tensor(18920452., device='cuda:0')
c= tensor(18920784., device='cuda:0')
c= tensor(18920840., device='cuda:0')
c= tensor(18920890., device='cuda:0')
c= tensor(18921382., device='cuda:0')
c= tensor(18956142., device='cuda:0')
c= tensor(18956214., device='cuda:0')
c= tensor(18956248., device='cuda:0')
c= tensor(18956296., device='cuda:0')
c= tensor(18956718., device='cuda:0')
c= tensor(18956938., device='cuda:0')
c= tensor(18957006., device='cuda:0')
c= tensor(18957128., device='cuda:0')
c= tensor(18957166., device='cuda:0')
c= tensor(18957234., device='cuda:0')
c= tensor(18957568., device='cuda:0')
c= tensor(18957924., device='cuda:0')
c= tensor(18957952., device='cuda:0')
c= tensor(18958304., device='cuda:0')
c= tensor(18958344., device='cuda:0')
c= tensor(18958582., device='cuda:0')
c= tensor(18959100., device='cuda:0')
c= tensor(18959406., device='cuda:0')
c= tensor(18960804., device='cuda:0')
c= tensor(18961646., device='cuda:0')
c= tensor(18961866., device='cuda:0')
c= tensor(18963440., device='cuda:0')
c= tensor(18967692., device='cuda:0')
c= tensor(18968886., device='cuda:0')
c= tensor(18969250., device='cuda:0')
c= tensor(18969330., device='cuda:0')
c= tensor(18970080., device='cuda:0')
c= tensor(18987296., device='cuda:0')
c= tensor(18990046., device='cuda:0')
c= tensor(18990792., device='cuda:0')
c= tensor(18995110., device='cuda:0')
c= tensor(19030868., device='cuda:0')
c= tensor(19031304., device='cuda:0')
c= tensor(19033400., device='cuda:0')
c= tensor(19033952., device='cuda:0')
c= tensor(19034282., device='cuda:0')
c= tensor(19182232., device='cuda:0')
c= tensor(19411398., device='cuda:0')
c= tensor(19411596., device='cuda:0')
c= tensor(19437758., device='cuda:0')
c= tensor(19817312., device='cuda:0')
c= tensor(19821622., device='cuda:0')
c= tensor(19843216., device='cuda:0')
c= tensor(19860054., device='cuda:0')
c= tensor(19884176., device='cuda:0')
c= tensor(19896374., device='cuda:0')
c= tensor(19896484., device='cuda:0')
c= tensor(19903248., device='cuda:0')
c= tensor(19903600., device='cuda:0')
c= tensor(19903938., device='cuda:0')
c= tensor(19904962., device='cuda:0')
c= tensor(19907712., device='cuda:0')
c= tensor(19963224., device='cuda:0')
c= tensor(19963968., device='cuda:0')
c= tensor(19964270., device='cuda:0')
c= tensor(19964682., device='cuda:0')
c= tensor(19967444., device='cuda:0')
c= tensor(19971980., device='cuda:0')
c= tensor(20146844., device='cuda:0')
c= tensor(21471236., device='cuda:0')
c= tensor(21472202., device='cuda:0')
c= tensor(21472216., device='cuda:0')
c= tensor(21472296., device='cuda:0')
c= tensor(21496498., device='cuda:0')
c= tensor(21524008., device='cuda:0')
c= tensor(21526612., device='cuda:0')
c= tensor(21526682., device='cuda:0')
c= tensor(21681502., device='cuda:0')
c= tensor(21682180., device='cuda:0')
c= tensor(21682794., device='cuda:0')
c= tensor(22489514., device='cuda:0')
c= tensor(22490350., device='cuda:0')
c= tensor(22493350., device='cuda:0')
c= tensor(22808862., device='cuda:0')
c= tensor(25018600., device='cuda:0')
c= tensor(25020902., device='cuda:0')
c= tensor(25021726., device='cuda:0')
c= tensor(25021916., device='cuda:0')
c= tensor(25022110., device='cuda:0')
c= tensor(25068868., device='cuda:0')
c= tensor(25069040., device='cuda:0')
c= tensor(25069234., device='cuda:0')
c= tensor(25109468., device='cuda:0')
c= tensor(25119282., device='cuda:0')
c= tensor(25119440., device='cuda:0')
c= tensor(25120156., device='cuda:0')
c= tensor(29030916., device='cuda:0')
c= tensor(29032062., device='cuda:0')
c= tensor(29032668., device='cuda:0')
c= tensor(29034320., device='cuda:0')
c= tensor(29761032., device='cuda:0')
c= tensor(29761454., device='cuda:0')
c= tensor(29761854., device='cuda:0')
c= tensor(29762404., device='cuda:0')
c= tensor(29762778., device='cuda:0')
c= tensor(29782102., device='cuda:0')
c= tensor(29814608., device='cuda:0')
c= tensor(29898316., device='cuda:0')
c= tensor(29898586., device='cuda:0')
c= tensor(30041840., device='cuda:0')
c= tensor(30060916., device='cuda:0')
c= tensor(30061198., device='cuda:0')
c= tensor(30104408., device='cuda:0')
c= tensor(30203162., device='cuda:0')
c= tensor(30206994., device='cuda:0')
c= tensor(30239190., device='cuda:0')
c= tensor(30239420., device='cuda:0')
c= tensor(30241026., device='cuda:0')
c= tensor(30242802., device='cuda:0')
c= tensor(30243804., device='cuda:0')
c= tensor(30244304., device='cuda:0')
c= tensor(30244444., device='cuda:0')
c= tensor(30245778., device='cuda:0')
c= tensor(30251614., device='cuda:0')
c= tensor(30276198., device='cuda:0')
c= tensor(30276562., device='cuda:0')
c= tensor(30277926., device='cuda:0')
c= tensor(30282060., device='cuda:0')
c= tensor(30282978., device='cuda:0')
c= tensor(30284244., device='cuda:0')
c= tensor(30284694., device='cuda:0')
c= tensor(39537408., device='cuda:0')
c= tensor(39541032., device='cuda:0')
c= tensor(39541260., device='cuda:0')
c= tensor(39656692., device='cuda:0')
c= tensor(39656868., device='cuda:0')
c= tensor(40405784., device='cuda:0')
c= tensor(40410740., device='cuda:0')
c= tensor(40413036., device='cuda:0')
c= tensor(40413452., device='cuda:0')
c= tensor(40413816., device='cuda:0')
c= tensor(40414468., device='cuda:0')
c= tensor(40419248., device='cuda:0')
c= tensor(40419744., device='cuda:0')
c= tensor(40422336., device='cuda:0')
c= tensor(40422396., device='cuda:0')
c= tensor(40422496., device='cuda:0')
c= tensor(40524780., device='cuda:0')
c= tensor(40545048., device='cuda:0')
c= tensor(40602344., device='cuda:0')
c= tensor(40613744., device='cuda:0')
c= tensor(41343372., device='cuda:0')
c= tensor(41348560., device='cuda:0')
c= tensor(41350332., device='cuda:0')
c= tensor(41351280., device='cuda:0')
c= tensor(41351344., device='cuda:0')
c= tensor(41351408., device='cuda:0')
c= tensor(41357104., device='cuda:0')
c= tensor(41358084., device='cuda:0')
c= tensor(41358408., device='cuda:0')
c= tensor(41358972., device='cuda:0')
c= tensor(41362832., device='cuda:0')
c= tensor(48587480., device='cuda:0')
c= tensor(48587740., device='cuda:0')
c= tensor(48590240., device='cuda:0')
c= tensor(48590712., device='cuda:0')
c= tensor(48590792., device='cuda:0')
c= tensor(48591828., device='cuda:0')
c= tensor(48924312., device='cuda:0')
c= tensor(56627460., device='cuda:0')
c= tensor(56629280., device='cuda:0')
c= tensor(56654708., device='cuda:0')
c= tensor(56654792., device='cuda:0')
c= tensor(56656084., device='cuda:0')
c= tensor(62640212., device='cuda:0')
c= tensor(62858640., device='cuda:0')
c= tensor(62860176., device='cuda:0')
c= tensor(62989296., device='cuda:0')
c= tensor(63646656., device='cuda:0')
c= tensor(63656172., device='cuda:0')
c= tensor(63656896., device='cuda:0')
c= tensor(63657040., device='cuda:0')
c= tensor(63664096., device='cuda:0')
c= tensor(63664432., device='cuda:0')
c= tensor(63783620., device='cuda:0')
c= tensor(63811000., device='cuda:0')
c= tensor(63811224., device='cuda:0')
c= tensor(63831308., device='cuda:0')
c= tensor(63831904., device='cuda:0')
c= tensor(63832004., device='cuda:0')
c= tensor(63840104., device='cuda:0')
c= tensor(63898568., device='cuda:0')
c= tensor(64306352., device='cuda:0')
c= tensor(64314968., device='cuda:0')
c= tensor(65182240., device='cuda:0')
c= tensor(65195388., device='cuda:0')
c= tensor(65205164., device='cuda:0')
c= tensor(65214220., device='cuda:0')
c= tensor(65222760., device='cuda:0')
c= tensor(65223268., device='cuda:0')
c= tensor(65882360., device='cuda:0')
c= tensor(65886496., device='cuda:0')
c= tensor(65928168., device='cuda:0')
c= tensor(66160212., device='cuda:0')
c= tensor(66160636., device='cuda:0')
c= tensor(66160808., device='cuda:0')
c= tensor(66160884., device='cuda:0')
c= tensor(66520896., device='cuda:0')
c= tensor(67793544., device='cuda:0')
c= tensor(68000904., device='cuda:0')
c= tensor(68036272., device='cuda:0')
c= tensor(69152048., device='cuda:0')
c= tensor(69158720., device='cuda:0')
c= tensor(69158824., device='cuda:0')
c= tensor(69629352., device='cuda:0')
c= tensor(69629712., device='cuda:0')
c= tensor(69630776., device='cuda:0')
c= tensor(70752232., device='cuda:0')
c= tensor(70755104., device='cuda:0')
c= tensor(70755424., device='cuda:0')
c= tensor(70755968., device='cuda:0')
c= tensor(71680568., device='cuda:0')
c= tensor(71690480., device='cuda:0')
c= tensor(71708272., device='cuda:0')
c= tensor(71708408., device='cuda:0')
c= tensor(71709328., device='cuda:0')
c= tensor(71709944., device='cuda:0')
c= tensor(71889576., device='cuda:0')
c= tensor(71890744., device='cuda:0')
c= tensor(71906128., device='cuda:0')
c= tensor(71907112., device='cuda:0')
c= tensor(71971640., device='cuda:0')
c= tensor(71971832., device='cuda:0')
c= tensor(72047696., device='cuda:0')
c= tensor(72070376., device='cuda:0')
c= tensor(72075136., device='cuda:0')
c= tensor(72075672., device='cuda:0')
c= tensor(72083688., device='cuda:0')
c= tensor(72083936., device='cuda:0')
c= tensor(72091264., device='cuda:0')
c= tensor(72095584., device='cuda:0')
c= tensor(72104184., device='cuda:0')
c= tensor(72104408., device='cuda:0')
c= tensor(72104744., device='cuda:0')
c= tensor(72120304., device='cuda:0')
c= tensor(72121488., device='cuda:0')
c= tensor(87766432., device='cuda:0')
c= tensor(87766536., device='cuda:0')
c= tensor(87770272., device='cuda:0')
c= tensor(87884488., device='cuda:0')
c= tensor(87885248., device='cuda:0')
c= tensor(87970056., device='cuda:0')
c= tensor(87970616., device='cuda:0')
c= tensor(88051112., device='cuda:0')
c= tensor(88054336., device='cuda:0')
c= tensor(88055576., device='cuda:0')
c= tensor(88055728., device='cuda:0')
c= tensor(88055832., device='cuda:0')
c= tensor(88056544., device='cuda:0')
c= tensor(88056944., device='cuda:0')
c= tensor(88072296., device='cuda:0')
c= tensor(88098568., device='cuda:0')
c= tensor(88100224., device='cuda:0')
c= tensor(88100512., device='cuda:0')
c= tensor(88100896., device='cuda:0')
c= tensor(88510072., device='cuda:0')
c= tensor(88515064., device='cuda:0')
c= tensor(88675928., device='cuda:0')
c= tensor(88676824., device='cuda:0')
c= tensor(88676936., device='cuda:0')
c= tensor(88677112., device='cuda:0')
c= tensor(88678344., device='cuda:0')
c= tensor(88688096., device='cuda:0')
c= tensor(88702216., device='cuda:0')
c= tensor(88702752., device='cuda:0')
c= tensor(88783328., device='cuda:0')
c= tensor(88796368., device='cuda:0')
c= tensor(88796640., device='cuda:0')
c= tensor(88797496., device='cuda:0')
c= tensor(88799704., device='cuda:0')
c= tensor(88801624., device='cuda:0')
c= tensor(88806968., device='cuda:0')
c= tensor(88807472., device='cuda:0')
c= tensor(88995344., device='cuda:0')
c= tensor(89132576., device='cuda:0')
c= tensor(90028992., device='cuda:0')
c= tensor(90339112., device='cuda:0')
c= tensor(90342992., device='cuda:0')
c= tensor(90384288., device='cuda:0')
c= tensor(90393056., device='cuda:0')
c= tensor(90393848., device='cuda:0')
c= tensor(90393944., device='cuda:0')
c= tensor(90394664., device='cuda:0')
c= tensor(90395168., device='cuda:0')
c= tensor(90396352., device='cuda:0')
c= tensor(90396400., device='cuda:0')
c= tensor(90398280., device='cuda:0')
c= tensor(90406528., device='cuda:0')
c= tensor(92197032., device='cuda:0')
c= tensor(92197080., device='cuda:0')
c= tensor(92197184., device='cuda:0')
c= tensor(92209168., device='cuda:0')
c= tensor(92210112., device='cuda:0')
c= tensor(92212576., device='cuda:0')
c= tensor(92212744., device='cuda:0')
c= tensor(92224656., device='cuda:0')
c= tensor(92335912., device='cuda:0')
c= tensor(92336112., device='cuda:0')
c= tensor(92336184., device='cuda:0')
c= tensor(92336208., device='cuda:0')
c= tensor(92370608., device='cuda:0')
c= tensor(1.0611e+08, device='cuda:0')
c= tensor(1.0611e+08, device='cuda:0')
c= tensor(1.0611e+08, device='cuda:0')
c= tensor(1.0615e+08, device='cuda:0')
c= tensor(1.0619e+08, device='cuda:0')
c= tensor(1.0619e+08, device='cuda:0')
c= tensor(1.0620e+08, device='cuda:0')
c= tensor(1.0620e+08, device='cuda:0')
c= tensor(1.3979e+08, device='cuda:0')
c= tensor(1.3981e+08, device='cuda:0')
c= tensor(1.3981e+08, device='cuda:0')
c= tensor(1.3981e+08, device='cuda:0')
c= tensor(1.3981e+08, device='cuda:0')
c= tensor(1.3981e+08, device='cuda:0')
c= tensor(1.4003e+08, device='cuda:0')
c= tensor(1.4003e+08, device='cuda:0')
c= tensor(1.4004e+08, device='cuda:0')
c= tensor(1.4024e+08, device='cuda:0')
c= tensor(1.4762e+08, device='cuda:0')
c= tensor(1.4763e+08, device='cuda:0')
c= tensor(1.4763e+08, device='cuda:0')
c= tensor(1.4793e+08, device='cuda:0')
c= tensor(1.4794e+08, device='cuda:0')
c= tensor(1.4813e+08, device='cuda:0')
c= tensor(1.4813e+08, device='cuda:0')
c= tensor(1.4813e+08, device='cuda:0')
c= tensor(1.4813e+08, device='cuda:0')
c= tensor(1.4813e+08, device='cuda:0')
c= tensor(1.4813e+08, device='cuda:0')
c= tensor(1.4813e+08, device='cuda:0')
c= tensor(1.4814e+08, device='cuda:0')
c= tensor(1.4814e+08, device='cuda:0')
c= tensor(1.4814e+08, device='cuda:0')
c= tensor(1.4814e+08, device='cuda:0')
c= tensor(1.4850e+08, device='cuda:0')
c= tensor(1.4974e+08, device='cuda:0')
c= tensor(1.4975e+08, device='cuda:0')
c= tensor(1.4975e+08, device='cuda:0')
c= tensor(1.4976e+08, device='cuda:0')
c= tensor(1.4976e+08, device='cuda:0')
c= tensor(1.4976e+08, device='cuda:0')
c= tensor(1.4987e+08, device='cuda:0')
c= tensor(1.4988e+08, device='cuda:0')
c= tensor(1.4992e+08, device='cuda:0')
c= tensor(1.4992e+08, device='cuda:0')
c= tensor(1.5002e+08, device='cuda:0')
c= tensor(1.5002e+08, device='cuda:0')
c= tensor(1.5009e+08, device='cuda:0')
c= tensor(1.5061e+08, device='cuda:0')
c= tensor(1.5062e+08, device='cuda:0')
c= tensor(1.5063e+08, device='cuda:0')
c= tensor(1.5063e+08, device='cuda:0')
c= tensor(1.5066e+08, device='cuda:0')
c= tensor(1.5067e+08, device='cuda:0')
c= tensor(1.5067e+08, device='cuda:0')
c= tensor(1.5067e+08, device='cuda:0')
c= tensor(1.5067e+08, device='cuda:0')
c= tensor(1.5071e+08, device='cuda:0')
c= tensor(1.5277e+08, device='cuda:0')
c= tensor(1.5298e+08, device='cuda:0')
c= tensor(1.5298e+08, device='cuda:0')
c= tensor(1.5298e+08, device='cuda:0')
c= tensor(1.5298e+08, device='cuda:0')
c= tensor(1.5299e+08, device='cuda:0')
c= tensor(1.5299e+08, device='cuda:0')
c= tensor(1.5462e+08, device='cuda:0')
c= tensor(1.5463e+08, device='cuda:0')
c= tensor(1.5463e+08, device='cuda:0')
c= tensor(1.6348e+08, device='cuda:0')
c= tensor(1.6348e+08, device='cuda:0')
c= tensor(1.6348e+08, device='cuda:0')
c= tensor(1.6348e+08, device='cuda:0')
c= tensor(1.6348e+08, device='cuda:0')
c= tensor(1.6348e+08, device='cuda:0')
c= tensor(1.6348e+08, device='cuda:0')
c= tensor(1.7282e+08, device='cuda:0')
c= tensor(1.7287e+08, device='cuda:0')
c= tensor(1.7288e+08, device='cuda:0')
c= tensor(1.7289e+08, device='cuda:0')
c= tensor(1.7289e+08, device='cuda:0')
c= tensor(1.7289e+08, device='cuda:0')
c= tensor(1.7290e+08, device='cuda:0')
c= tensor(1.7290e+08, device='cuda:0')
c= tensor(1.7290e+08, device='cuda:0')
c= tensor(1.7292e+08, device='cuda:0')
c= tensor(1.7293e+08, device='cuda:0')
c= tensor(1.7293e+08, device='cuda:0')
c= tensor(1.7293e+08, device='cuda:0')
c= tensor(1.7293e+08, device='cuda:0')
c= tensor(1.7294e+08, device='cuda:0')
c= tensor(1.7313e+08, device='cuda:0')
c= tensor(1.7313e+08, device='cuda:0')
c= tensor(1.7931e+08, device='cuda:0')
c= tensor(1.7931e+08, device='cuda:0')
c= tensor(1.7951e+08, device='cuda:0')
c= tensor(1.7951e+08, device='cuda:0')
c= tensor(1.7953e+08, device='cuda:0')
c= tensor(1.8203e+08, device='cuda:0')
c= tensor(1.8207e+08, device='cuda:0')
c= tensor(1.8207e+08, device='cuda:0')
c= tensor(1.8208e+08, device='cuda:0')
c= tensor(1.8209e+08, device='cuda:0')
c= tensor(1.8211e+08, device='cuda:0')
c= tensor(1.8232e+08, device='cuda:0')
c= tensor(1.8232e+08, device='cuda:0')
c= tensor(1.8232e+08, device='cuda:0')
c= tensor(1.8233e+08, device='cuda:0')
c= tensor(1.8255e+08, device='cuda:0')
c= tensor(1.8258e+08, device='cuda:0')
c= tensor(1.8258e+08, device='cuda:0')
c= tensor(1.8258e+08, device='cuda:0')
c= tensor(1.8467e+08, device='cuda:0')
c= tensor(1.8472e+08, device='cuda:0')
c= tensor(1.8758e+08, device='cuda:0')
c= tensor(1.8759e+08, device='cuda:0')
c= tensor(1.8767e+08, device='cuda:0')
c= tensor(1.8767e+08, device='cuda:0')
c= tensor(1.8782e+08, device='cuda:0')
c= tensor(1.8783e+08, device='cuda:0')
c= tensor(1.8784e+08, device='cuda:0')
c= tensor(1.8784e+08, device='cuda:0')
c= tensor(1.8786e+08, device='cuda:0')
c= tensor(1.8787e+08, device='cuda:0')
c= tensor(1.8789e+08, device='cuda:0')
c= tensor(1.8790e+08, device='cuda:0')
c= tensor(1.8790e+08, device='cuda:0')
c= tensor(1.8790e+08, device='cuda:0')
c= tensor(1.8823e+08, device='cuda:0')
c= tensor(1.8824e+08, device='cuda:0')
c= tensor(1.8845e+08, device='cuda:0')
c= tensor(1.8863e+08, device='cuda:0')
c= tensor(1.8912e+08, device='cuda:0')
c= tensor(1.8913e+08, device='cuda:0')
c= tensor(1.8913e+08, device='cuda:0')
c= tensor(1.8913e+08, device='cuda:0')
c= tensor(1.8914e+08, device='cuda:0')
c= tensor(1.8914e+08, device='cuda:0')
c= tensor(1.8914e+08, device='cuda:0')
c= tensor(1.8914e+08, device='cuda:0')
c= tensor(1.8914e+08, device='cuda:0')
c= tensor(1.8962e+08, device='cuda:0')
c= tensor(1.8962e+08, device='cuda:0')
c= tensor(1.8963e+08, device='cuda:0')
c= tensor(1.8963e+08, device='cuda:0')
c= tensor(1.8963e+08, device='cuda:0')
c= tensor(1.8963e+08, device='cuda:0')
c= tensor(1.8967e+08, device='cuda:0')
c= tensor(1.8980e+08, device='cuda:0')
c= tensor(1.8980e+08, device='cuda:0')
c= tensor(1.8981e+08, device='cuda:0')
c= tensor(1.8981e+08, device='cuda:0')
c= tensor(1.8981e+08, device='cuda:0')
c= tensor(1.8983e+08, device='cuda:0')
c= tensor(1.8984e+08, device='cuda:0')
c= tensor(1.8984e+08, device='cuda:0')
c= tensor(1.8992e+08, device='cuda:0')
c= tensor(1.8992e+08, device='cuda:0')
c= tensor(1.8997e+08, device='cuda:0')
c= tensor(1.9005e+08, device='cuda:0')
c= tensor(1.9005e+08, device='cuda:0')
c= tensor(1.9006e+08, device='cuda:0')
c= tensor(1.9006e+08, device='cuda:0')
c= tensor(1.9011e+08, device='cuda:0')
c= tensor(1.9023e+08, device='cuda:0')
c= tensor(1.9023e+08, device='cuda:0')
c= tensor(1.9023e+08, device='cuda:0')
c= tensor(1.9025e+08, device='cuda:0')
c= tensor(1.9025e+08, device='cuda:0')
c= tensor(1.9025e+08, device='cuda:0')
c= tensor(1.9025e+08, device='cuda:0')
c= tensor(1.9047e+08, device='cuda:0')
c= tensor(1.9286e+08, device='cuda:0')
c= tensor(1.9287e+08, device='cuda:0')
c= tensor(1.9287e+08, device='cuda:0')
c= tensor(1.9287e+08, device='cuda:0')
c= tensor(1.9293e+08, device='cuda:0')
c= tensor(1.9293e+08, device='cuda:0')
c= tensor(1.9293e+08, device='cuda:0')
c= tensor(1.9293e+08, device='cuda:0')
c= tensor(1.9332e+08, device='cuda:0')
c= tensor(1.9332e+08, device='cuda:0')
c= tensor(1.9333e+08, device='cuda:0')
c= tensor(1.9334e+08, device='cuda:0')
time to make c is 12.334060192108154
time for making loss is 12.334073305130005
p0 True
it  0 : 375571456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  4% |
shape of L is 
torch.Size([])
memory (bytes)
3521478656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  4% |
memory (bytes)
3521712128
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  5% |
error is  16595568.0
relative error loss 0.08583827
shape of L is 
torch.Size([])
memory (bytes)
3547897856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  5% |
memory (bytes)
3547897856
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  5% |
error is  16156480.0
relative error loss 0.08356715
shape of L is 
torch.Size([])
memory (bytes)
3551166464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  5% |
memory (bytes)
3551350784
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  5% |
error is  13796112.0
relative error loss 0.07135847
shape of L is 
torch.Size([])
memory (bytes)
3554357248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  5% |
memory (bytes)
3554357248
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  13239328.0
relative error loss 0.068478584
shape of L is 
torch.Size([])
memory (bytes)
3557789696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  5% |
memory (bytes)
3557814272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  12768848.0
relative error loss 0.0660451
shape of L is 
torch.Size([])
memory (bytes)
3560996864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% |  5% |
memory (bytes)
3560996864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  12357888.0
relative error loss 0.06391946
shape of L is 
torch.Size([])
memory (bytes)
3564056576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  5% |
memory (bytes)
3564056576
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  12127840.0
relative error loss 0.06272957
shape of L is 
torch.Size([])
memory (bytes)
3567427584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  5% |
memory (bytes)
3567493120
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  11876368.0
relative error loss 0.061428867
shape of L is 
torch.Size([])
memory (bytes)
3570581504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  5% |
memory (bytes)
3570581504
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  11726048.0
relative error loss 0.06065136
shape of L is 
torch.Size([])
memory (bytes)
3573866496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  5% |
memory (bytes)
3573919744
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  11609952.0
relative error loss 0.060050867
time to take a step is 268.5315418243408
it  1 : 758333952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  6% |
shape of L is 
torch.Size([])
memory (bytes)
3577098240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  5% |
memory (bytes)
3577151488
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  5% |
error is  11609952.0
relative error loss 0.060050867
shape of L is 
torch.Size([])
memory (bytes)
3580362752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  5% |
memory (bytes)
3580362752
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  11406160.0
relative error loss 0.05899678
shape of L is 
torch.Size([])
memory (bytes)
3583590400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
memory (bytes)
3583594496
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  11344224.0
relative error loss 0.058676425
shape of L is 
torch.Size([])
memory (bytes)
3586826240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
memory (bytes)
3586826240
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  11246336.0
relative error loss 0.058170114
shape of L is 
torch.Size([])
memory (bytes)
3590045696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% |  6% |
memory (bytes)
3590045696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% |  6% |
error is  11258592.0
relative error loss 0.058233507
shape of L is 
torch.Size([])
memory (bytes)
3593269248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3593269248
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  11193344.0
relative error loss 0.057896018
shape of L is 
torch.Size([])
memory (bytes)
3596517376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3596517376
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  11132784.0
relative error loss 0.05758278
shape of L is 
torch.Size([])
memory (bytes)
3599736832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3599736832
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  11043248.0
relative error loss 0.057119668
shape of L is 
torch.Size([])
memory (bytes)
3602964480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
memory (bytes)
3602964480
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  11016608.0
relative error loss 0.056981876
shape of L is 
torch.Size([])
memory (bytes)
3606188032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  6% |
memory (bytes)
3606188032
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10972384.0
relative error loss 0.056753132
time to take a step is 281.2250027656555
it  2 : 758333952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
shape of L is 
torch.Size([])
memory (bytes)
3609411584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  5% |
memory (bytes)
3609411584
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  5% |
error is  10972384.0
relative error loss 0.056753132
shape of L is 
torch.Size([])
memory (bytes)
3612573696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  5% |
memory (bytes)
3612631040
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10941616.0
relative error loss 0.05659399
shape of L is 
torch.Size([])
memory (bytes)
3615817728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3615870976
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10922640.0
relative error loss 0.05649584
shape of L is 
torch.Size([])
memory (bytes)
3619094528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
memory (bytes)
3619094528
| ID | GPU | MEM |
------------------
|  0 | 23% |  0% |
|  1 | 98% |  6% |
error is  10892336.0
relative error loss 0.056339096
shape of L is 
torch.Size([])
memory (bytes)
3622326272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3622326272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  10846096.0
relative error loss 0.056099925
shape of L is 
torch.Size([])
memory (bytes)
3625566208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  6% |
memory (bytes)
3625566208
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10828368.0
relative error loss 0.05600823
shape of L is 
torch.Size([])
memory (bytes)
3628793856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
memory (bytes)
3628793856
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10807328.0
relative error loss 0.055899404
shape of L is 
torch.Size([])
memory (bytes)
3632013312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% |  6% |
memory (bytes)
3632017408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  10772896.0
relative error loss 0.05572131
shape of L is 
torch.Size([])
memory (bytes)
3635245056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3635245056
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10753856.0
relative error loss 0.055622827
shape of L is 
torch.Size([])
memory (bytes)
3638468608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% |  6% |
memory (bytes)
3638472704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% |  6% |
error is  10736368.0
relative error loss 0.055532373
time to take a step is 274.0135498046875
it  3 : 758333952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  6% |
shape of L is 
torch.Size([])
memory (bytes)
3641688064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  5% |
memory (bytes)
3641688064
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  5% |
error is  10736368.0
relative error loss 0.055532373
shape of L is 
torch.Size([])
memory (bytes)
3644895232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% |  5% |
memory (bytes)
3644895232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  10726928.0
relative error loss 0.055483546
shape of L is 
torch.Size([])
memory (bytes)
3648139264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3648139264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  10708496.0
relative error loss 0.05538821
shape of L is 
torch.Size([])
memory (bytes)
3651346432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3651350528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  10712768.0
relative error loss 0.055410307
shape of L is 
torch.Size([])
memory (bytes)
3654582272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
memory (bytes)
3654582272
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10699968.0
relative error loss 0.0553441
shape of L is 
torch.Size([])
memory (bytes)
3657805824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3657805824
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10686768.0
relative error loss 0.055275824
shape of L is 
torch.Size([])
memory (bytes)
3661025280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3661025280
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10671264.0
relative error loss 0.055195633
shape of L is 
torch.Size([])
memory (bytes)
3664257024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3664257024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  10662016.0
relative error loss 0.0551478
shape of L is 
torch.Size([])
memory (bytes)
3667423232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3667476480
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10652400.0
relative error loss 0.05509806
shape of L is 
torch.Size([])
memory (bytes)
3670704128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3670708224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  10642512.0
relative error loss 0.055046916
time to take a step is 280.2154474258423
it  4 : 758333952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  6% |
shape of L is 
torch.Size([])
memory (bytes)
3673919488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  5% |
memory (bytes)
3673919488
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  5% |
error is  10642512.0
relative error loss 0.055046916
shape of L is 
torch.Size([])
memory (bytes)
3677097984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% |  5% |
memory (bytes)
3677155328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% |  6% |
error is  10635904.0
relative error loss 0.055012736
shape of L is 
torch.Size([])
memory (bytes)
3680399360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3680399360
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10635264.0
relative error loss 0.05500943
shape of L is 
torch.Size([])
memory (bytes)
3683594240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3683618816
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10628512.0
relative error loss 0.054974504
shape of L is 
torch.Size([])
memory (bytes)
3686789120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  6% |
memory (bytes)
3686842368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  10620368.0
relative error loss 0.054932382
shape of L is 
torch.Size([])
memory (bytes)
3690049536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3690053632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  10615632.0
relative error loss 0.054907884
shape of L is 
torch.Size([])
memory (bytes)
3693285376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% |  6% |
memory (bytes)
3693285376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  10610176.0
relative error loss 0.054879665
shape of L is 
torch.Size([])
memory (bytes)
3696463872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3696504832
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10604736.0
relative error loss 0.054851525
shape of L is 
torch.Size([])
memory (bytes)
3699724288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% |  6% |
memory (bytes)
3699724288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  6% |
error is  10599792.0
relative error loss 0.054825954
shape of L is 
torch.Size([])
memory (bytes)
3702927360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  6% |
memory (bytes)
3702951936
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10595760.0
relative error loss 0.0548051
time to take a step is 274.675110578537
it  5 : 758333952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
shape of L is 
torch.Size([])
memory (bytes)
3706122240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  5% |
memory (bytes)
3706179584
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  5% |
error is  10595760.0
relative error loss 0.0548051
shape of L is 
torch.Size([])
memory (bytes)
3709394944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% |  5% |
memory (bytes)
3709399040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% |  6% |
error is  10592624.0
relative error loss 0.05478888
shape of L is 
torch.Size([])
memory (bytes)
3712487424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3712638976
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10589984.0
relative error loss 0.054775223
shape of L is 
torch.Size([])
memory (bytes)
3715764224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  6% |
memory (bytes)
3715866624
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10590256.0
relative error loss 0.05477663
shape of L is 
torch.Size([])
memory (bytes)
3719036928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  6% |
memory (bytes)
3719090176
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10588192.0
relative error loss 0.054765955
shape of L is 
torch.Size([])
memory (bytes)
3722235904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3722235904
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10586208.0
relative error loss 0.05475569
shape of L is 
torch.Size([])
memory (bytes)
3725557760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3725557760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  10584304.0
relative error loss 0.054745845
shape of L is 
torch.Size([])
memory (bytes)
3728777216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  6% |
memory (bytes)
3728777216
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10582384.0
relative error loss 0.054735914
shape of L is 
torch.Size([])
memory (bytes)
3732017152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3732021248
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10580176.0
relative error loss 0.054724492
shape of L is 
torch.Size([])
memory (bytes)
3735187456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3735240704
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10578448.0
relative error loss 0.054715555
time to take a step is 273.0605523586273
it  6 : 758333952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
shape of L is 
torch.Size([])
memory (bytes)
3738411008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  5% |
memory (bytes)
3738464256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  5% |
error is  10578448.0
relative error loss 0.054715555
shape of L is 
torch.Size([])
memory (bytes)
3741675520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  5% |
memory (bytes)
3741675520
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10575872.0
relative error loss 0.05470223
shape of L is 
torch.Size([])
memory (bytes)
3744903168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3744903168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  10576528.0
relative error loss 0.054705624
shape of L is 
torch.Size([])
memory (bytes)
3748073472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  6% |
memory (bytes)
3748130816
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10573808.0
relative error loss 0.054691557
shape of L is 
torch.Size([])
memory (bytes)
3751354368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3751354368
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10571056.0
relative error loss 0.054677323
shape of L is 
torch.Size([])
memory (bytes)
3754520576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3754573824
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10567712.0
relative error loss 0.054660026
shape of L is 
torch.Size([])
memory (bytes)
3757797376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3757797376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  10564720.0
relative error loss 0.054644547
shape of L is 
torch.Size([])
memory (bytes)
3760959488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% |  6% |
memory (bytes)
3761012736
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10560768.0
relative error loss 0.054624107
shape of L is 
torch.Size([])
memory (bytes)
3764178944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3764232192
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10558832.0
relative error loss 0.054614093
shape of L is 
torch.Size([])
memory (bytes)
3767341056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% |  6% |
memory (bytes)
3767455744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% |  6% |
error is  10552768.0
relative error loss 0.05458273
time to take a step is 272.45926094055176
it  7 : 758333952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
shape of L is 
torch.Size([])
memory (bytes)
3770630144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% |  5% |
memory (bytes)
3770675200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  6% |
error is  10552768.0
relative error loss 0.05458273
shape of L is 
torch.Size([])
memory (bytes)
3773747200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3773898752
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10548880.0
relative error loss 0.054562617
shape of L is 
torch.Size([])
memory (bytes)
3777126400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3777126400
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10541248.0
relative error loss 0.054523144
shape of L is 
torch.Size([])
memory (bytes)
3780337664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% |  6% |
memory (bytes)
3780337664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  6% |
error is  10535440.0
relative error loss 0.054493103
shape of L is 
torch.Size([])
memory (bytes)
3783491584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% |  6% |
memory (bytes)
3783491584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  6% |
error is  10534160.0
relative error loss 0.05448648
shape of L is 
torch.Size([])
memory (bytes)
3786817536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3786817536
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10530736.0
relative error loss 0.05446877
shape of L is 
torch.Size([])
memory (bytes)
3789991936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3789991936
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10529680.0
relative error loss 0.05446331
shape of L is 
torch.Size([])
memory (bytes)
3793244160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% |  6% |
memory (bytes)
3793244160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% |  6% |
error is  10527504.0
relative error loss 0.054452054
shape of L is 
torch.Size([])
memory (bytes)
3796488192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  6% |
memory (bytes)
3796488192
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10528016.0
relative error loss 0.054454703
shape of L is 
torch.Size([])
memory (bytes)
3799650304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3799650304
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10526448.0
relative error loss 0.054446593
time to take a step is 272.3100039958954
it  8 : 758334464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
shape of L is 
torch.Size([])
memory (bytes)
3802943488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  5% |
memory (bytes)
3802943488
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10526448.0
relative error loss 0.054446593
shape of L is 
torch.Size([])
memory (bytes)
3806146560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  6% |
memory (bytes)
3806146560
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10524752.0
relative error loss 0.05443782
shape of L is 
torch.Size([])
memory (bytes)
3809382400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3809382400
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10522496.0
relative error loss 0.054426152
shape of L is 
torch.Size([])
memory (bytes)
3812605952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% |  6% |
memory (bytes)
3812605952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  6% |
error is  10521776.0
relative error loss 0.054422427
shape of L is 
torch.Size([])
memory (bytes)
3815845888
| ID | GPU | MEM |
------------------
|  0 |  1% |  0% |
|  1 |  8% |  6% |
memory (bytes)
3815845888
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10520336.0
relative error loss 0.05441498
shape of L is 
torch.Size([])
memory (bytes)
3819003904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3819003904
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10519680.0
relative error loss 0.054411586
shape of L is 
torch.Size([])
memory (bytes)
3822280704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3822280704
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10518976.0
relative error loss 0.054407943
shape of L is 
torch.Size([])
memory (bytes)
3825504256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  6% |
memory (bytes)
3825508352
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10517488.0
relative error loss 0.054400247
shape of L is 
torch.Size([])
memory (bytes)
3828736000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3828736000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  10516048.0
relative error loss 0.0543928
shape of L is 
torch.Size([])
memory (bytes)
3831959552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  6% |
memory (bytes)
3831959552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  10513152.0
relative error loss 0.05437782
time to take a step is 279.7005488872528
it  9 : 758333952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
shape of L is 
torch.Size([])
memory (bytes)
3835170816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  5% |
memory (bytes)
3835174912
| ID | GPU  | MEM |
-------------------
|  0 |   1% |  0% |
|  1 | 100% |  6% |
error is  10513152.0
relative error loss 0.05437782
shape of L is 
torch.Size([])
memory (bytes)
3838390272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  6% |
memory (bytes)
3838390272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% |  6% |
error is  10511776.0
relative error loss 0.054370705
shape of L is 
torch.Size([])
memory (bytes)
3841630208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3841630208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  10509104.0
relative error loss 0.054356884
shape of L is 
torch.Size([])
memory (bytes)
3844853760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3844853760
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10510320.0
relative error loss 0.054363173
shape of L is 
torch.Size([])
memory (bytes)
3848028160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% |  6% |
memory (bytes)
3848085504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  6% |
error is  10508064.0
relative error loss 0.054351505
shape of L is 
torch.Size([])
memory (bytes)
3851264000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  6% |
memory (bytes)
3851317248
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10506400.0
relative error loss 0.054342896
shape of L is 
torch.Size([])
memory (bytes)
3854499840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3854536704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  10504288.0
relative error loss 0.054331973
shape of L is 
torch.Size([])
memory (bytes)
3857723392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  6% |
memory (bytes)
3857764352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  10503744.0
relative error loss 0.05432916
shape of L is 
torch.Size([])
memory (bytes)
3860992000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3860992000
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10502624.0
relative error loss 0.054323364
shape of L is 
torch.Size([])
memory (bytes)
3864211456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3864211456
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10501888.0
relative error loss 0.05431956
time to take a step is 276.7656219005585
it  10 : 758333952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
shape of L is 
torch.Size([])
memory (bytes)
3867439104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  5% |
memory (bytes)
3867439104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% |  6% |
error is  10501888.0
relative error loss 0.05431956
shape of L is 
torch.Size([])
memory (bytes)
3870638080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  6% |
memory (bytes)
3870642176
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10501216.0
relative error loss 0.054316085
shape of L is 
torch.Size([])
memory (bytes)
3873824768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  6% |
memory (bytes)
3873878016
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10499872.0
relative error loss 0.05430913
shape of L is 
torch.Size([])
memory (bytes)
3877089280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% |  6% |
memory (bytes)
3877105664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% |  6% |
error is  10500448.0
relative error loss 0.05431211
shape of L is 
torch.Size([])
memory (bytes)
3880329216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3880333312
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10499232.0
relative error loss 0.05430582
shape of L is 
torch.Size([])
memory (bytes)
3883556864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3883556864
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10498368.0
relative error loss 0.05430135
shape of L is 
torch.Size([])
memory (bytes)
3886780416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% |  6% |
memory (bytes)
3886780416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% |  6% |
error is  10497696.0
relative error loss 0.054297876
shape of L is 
torch.Size([])
memory (bytes)
3890008064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  6% |
memory (bytes)
3890008064
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10496512.0
relative error loss 0.05429175
shape of L is 
torch.Size([])
memory (bytes)
3893231616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3893231616
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10495472.0
relative error loss 0.054286372
shape of L is 
torch.Size([])
memory (bytes)
3896471552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  6% |
memory (bytes)
3896471552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  10495008.0
relative error loss 0.054283973
time to take a step is 279.34075689315796
it  11 : 758333952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
shape of L is 
torch.Size([])
memory (bytes)
3899682816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  5% |
memory (bytes)
3899682816
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10495008.0
relative error loss 0.054283973
shape of L is 
torch.Size([])
memory (bytes)
3902898176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  6% |
memory (bytes)
3902898176
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10494496.0
relative error loss 0.054281324
shape of L is 
torch.Size([])
memory (bytes)
3906129920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% |  6% |
memory (bytes)
3906129920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% |  6% |
error is  10493648.0
relative error loss 0.05427694
shape of L is 
torch.Size([])
memory (bytes)
3909357568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  6% |
memory (bytes)
3909357568
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10493744.0
relative error loss 0.054277435
shape of L is 
torch.Size([])
memory (bytes)
3912589312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3912593408
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10493296.0
relative error loss 0.054275118
shape of L is 
torch.Size([])
memory (bytes)
3915821056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% |  6% |
memory (bytes)
3915821056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% |  6% |
error is  10493056.0
relative error loss 0.054273877
shape of L is 
torch.Size([])
memory (bytes)
3919048704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
memory (bytes)
3919048704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  10492864.0
relative error loss 0.054272883
shape of L is 
torch.Size([])
memory (bytes)
3922264064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3922264064
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10492496.0
relative error loss 0.05427098
shape of L is 
torch.Size([])
memory (bytes)
3925499904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  6% |
memory (bytes)
3925499904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  10492224.0
relative error loss 0.054269575
shape of L is 
torch.Size([])
memory (bytes)
3928727552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% |  6% |
memory (bytes)
3928727552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% |  6% |
error is  10491984.0
relative error loss 0.05426833
time to take a step is 276.5715126991272
it  12 : 758333952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  6% |
shape of L is 
torch.Size([])
memory (bytes)
3931947008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% |  5% |
memory (bytes)
3931947008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  10491984.0
relative error loss 0.05426833
shape of L is 
torch.Size([])
memory (bytes)
3935162368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% |  6% |
memory (bytes)
3935162368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% |  6% |
error is  10491856.0
relative error loss 0.05426767
shape of L is 
torch.Size([])
memory (bytes)
3938394112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% |  6% |
memory (bytes)
3938394112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  10491424.0
relative error loss 0.054265436
shape of L is 
torch.Size([])
memory (bytes)
3941617664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3941617664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  10491168.0
relative error loss 0.05426411
shape of L is 
torch.Size([])
memory (bytes)
3944828928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3944833024
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10490816.0
relative error loss 0.05426229
shape of L is 
torch.Size([])
memory (bytes)
3948056576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% |  6% |
memory (bytes)
3948056576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  6% |
error is  10490528.0
relative error loss 0.0542608
shape of L is 
torch.Size([])
memory (bytes)
3951288320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3951288320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  10489968.0
relative error loss 0.054257903
shape of L is 
torch.Size([])
memory (bytes)
3954507776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  6% |
memory (bytes)
3954507776
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10489136.0
relative error loss 0.0542536
shape of L is 
torch.Size([])
memory (bytes)
3957628928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3957628928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  10488256.0
relative error loss 0.05424905
shape of L is 
torch.Size([])
memory (bytes)
3960958976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  6% |
memory (bytes)
3960958976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  10487664.0
relative error loss 0.054245986
time to take a step is 274.62410140037537
it  13 : 758333952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
shape of L is 
torch.Size([])
memory (bytes)
3964125184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  5% |
memory (bytes)
3964178432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% |  6% |
error is  10487664.0
relative error loss 0.054245986
shape of L is 
torch.Size([])
memory (bytes)
3967328256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  6% |
memory (bytes)
3967381504
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10486608.0
relative error loss 0.054240525
shape of L is 
torch.Size([])
memory (bytes)
3970629632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3970629632
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10485472.0
relative error loss 0.05423465
shape of L is 
torch.Size([])
memory (bytes)
3973836800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3973853184
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10484288.0
relative error loss 0.054228526
shape of L is 
torch.Size([])
memory (bytes)
3977072640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3977072640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  10483600.0
relative error loss 0.054224968
shape of L is 
torch.Size([])
memory (bytes)
3980300288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% |  6% |
memory (bytes)
3980300288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% |  6% |
error is  10483280.0
relative error loss 0.05422331
shape of L is 
torch.Size([])
memory (bytes)
3983519744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3983527936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  10482480.0
relative error loss 0.054219175
shape of L is 
torch.Size([])
memory (bytes)
3986710528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
3986755584
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10481936.0
relative error loss 0.05421636
shape of L is 
torch.Size([])
memory (bytes)
3989979136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  6% |
memory (bytes)
3989979136
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10481632.0
relative error loss 0.054214787
shape of L is 
torch.Size([])
memory (bytes)
3993198592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  6% |
memory (bytes)
3993198592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% |  6% |
error is  10481344.0
relative error loss 0.054213297
time to take a step is 281.2795367240906
it  14 : 758333952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
shape of L is 
torch.Size([])
memory (bytes)
3996426240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  5% |
memory (bytes)
3996426240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  10481344.0
relative error loss 0.054213297
shape of L is 
torch.Size([])
memory (bytes)
3999608832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% |  6% |
memory (bytes)
3999633408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  6% |
error is  10480912.0
relative error loss 0.054211065
shape of L is 
torch.Size([])
memory (bytes)
4002869248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
4002869248
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10480688.0
relative error loss 0.054209907
shape of L is 
torch.Size([])
memory (bytes)
4006084608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
4006084608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  10480000.0
relative error loss 0.054206345
shape of L is 
torch.Size([])
memory (bytes)
4009259008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
4009259008
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10479888.0
relative error loss 0.054205768
shape of L is 
torch.Size([])
memory (bytes)
4012552192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
4012552192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  10479648.0
relative error loss 0.054204527
shape of L is 
torch.Size([])
memory (bytes)
4015714304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
4015714304
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10479168.0
relative error loss 0.054202043
shape of L is 
torch.Size([])
memory (bytes)
4018962432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
4018999296
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10478832.0
relative error loss 0.054200307
shape of L is 
torch.Size([])
memory (bytes)
4022185984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  6% |
memory (bytes)
4022185984
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10478368.0
relative error loss 0.054197904
shape of L is 
torch.Size([])
memory (bytes)
4025454592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% |  6% |
memory (bytes)
4025454592
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  10478144.0
relative error loss 0.054196745
time to take a step is 279.06280851364136
sum tnnu_Z after tensor(3664514., device='cuda:0')
shape of features
(3770,)
shape of features
(3770,)
number of orig particles 15081
number of new particles after remove low mass 15081
tnuZ shape should be parts x labs
torch.Size([15081, 702])
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
error without small mass is  16592664.0
relative error without small mass is  0.08582325
nnu_Z shape should be number of particles by maxV
(15081, 702)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
shape of features
(15081,)
Tue Jan 31 13:32:49 EST 2023
