Fri Feb 3 00:57:21 EST 2023
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  0% |  0% |
numbers of X: 29768163
numbers of Z: 14460
shape of features
(14460,)
shape of features
(14460,)
ZX	Vol	Parts	Cubes	Eps
Z	0.01500368749931114	14460	14.46	0.10123792589645873
X	0.012471915912481204	1371	1.371	0.20875260631708786
X	0.013906662413060685	17308	17.308	0.09296625616711836
X	0.014605718990011694	1927	1.927	0.19643268004067746
X	0.012467730460487268	1950	1.95	0.1856026892087744
X	0.012836366937883271	35696	35.696	0.0711116665546523
X	0.012842439821786057	30329	30.329	0.07509248608613082
X	0.012845180408662603	37749	37.749	0.06981438537510609
X	0.012866219171907256	42913	42.913	0.06692997912021796
X	0.012521016249441158	5884	5.884	0.12862408698969544
X	0.01251383659307157	32726	32.726	0.07258259945483746
X	0.012601746947016209	6414	6.414	0.1252469809992739
X	0.013261388051152862	112754	112.754	0.04899506542604503
X	0.012950064917100686	6702	6.702	0.1245534726917733
X	0.012616231944133305	103725	103.725	0.049546776775394145
X	0.012870419219656638	15909	15.909	0.0931786907200177
X	0.012581253233914258	50484	50.484	0.06292985271040258
X	0.012887203418691635	28520	28.52	0.07673668858546916
X	0.012741537440727	23721	23.721	0.0812885943463118
X	0.012893824124925976	328323	328.323	0.03399070284837628
X	0.013737393511041335	70307	70.307	0.05802768666547725
X	0.012786640477521925	16502	16.502	0.09184875851735447
X	0.01322082525194482	344051	344.051	0.03374519277627555
X	0.01339474211207912	17829	17.829	0.09090807408509767
X	0.01260954202560985	12948	12.948	0.09912096942824786
X	0.01232895694618574	15018	15.018	0.09363497064395686
X	0.012872817838326911	46660	46.66	0.06509929665530907
X	0.013914165817085362	23488	23.488	0.08398538397580195
X	0.01262391715282967	6239	6.239	0.12648131824644204
X	0.012536052397972673	21694	21.694	0.08329272186032256
X	0.013722049476347338	1382331	1382.331	0.02149161132527429
X	0.01250904741980979	6141	6.141	0.126763740921516
X	0.013894171889906368	233533	233.533	0.03903865109107597
X	0.013998163051442015	10037	10.037	0.11172637493612801
X	0.013475955256421784	6646	6.646	0.12657066866752453
X	0.012489093209237242	8982	8.982	0.11161415749247657
X	0.013642926731056236	59636	59.636	0.061159840889196786
X	0.013179987300532412	47037	47.037	0.06543726293255342
X	0.012410760698799076	1129	1.129	0.22234881536005394
X	0.012748638062898172	3212	3.212	0.1583298322569709
X	0.012516678502043395	2501	2.501	0.17105081003250808
X	0.013723892086570792	5136	5.136	0.13876632700447117
X	0.011737496164783544	1232	1.232	0.21199303143583723
X	0.012105714463669108	1066	1.066	0.22477313718125616
X	0.012917741473098918	2819	2.819	0.16609778371283904
X	0.012565027117394866	1047	1.047	0.22894908709642708
X	0.01252260240338362	1247	1.247	0.2157459802656522
X	0.012837250103810927	6610	6.61	0.12476425596210235
X	0.01243377237858812	4382	4.382	0.14157183017597375
X	0.012561983238479209	1689	1.689	0.19519836995208
X	0.01239502424929146	7606	7.606	0.11767847179550657
X	0.01283842217535674	10277	10.277	0.10769985008192438
X	0.013700999983754191	1654	1.654	0.20233557761860468
X	0.01253201524934005	4635	4.635	0.1393125982352926
X	0.012763816143461332	2054	2.054	0.1838484757197043
X	0.012547023184699638	5375	5.375	0.13265412031462936
X	0.014054903837343691	4549	4.549	0.14564781728665788
X	0.012495554518260031	3264	3.264	0.15643543273387125
X	0.012462219322370119	2280	2.28	0.176151703607428
X	0.01256225329940104	2529	2.529	0.17062378516721374
X	0.012269347932018471	1860	1.86	0.18754385813135724
X	0.01257694251126378	5179	5.179	0.13441355108679723
X	0.012525603492591648	1713	1.713	0.1940947397638672
X	0.013258452669329132	6348	6.348	0.12782583592482402
X	0.013217744477924015	6856	6.856	0.12445972804419987
X	0.012512142790819084	2572	2.572	0.16944167758162246
X	0.012515128030245253	2112	2.112	0.18095911390086794
X	0.012366738755103083	3091	3.091	0.15875182519510922
X	0.012513412899356173	7720	7.72	0.1174679775516784
X	0.012524465776420685	4455	4.455	0.14113579285824931
X	0.01247084626789277	1469	1.469	0.20399743647968077
X	0.012562183020930255	5481	5.481	0.13184644918837066
X	0.012460471663571395	2543	2.543	0.16984888774468634
X	0.013021971814209252	3137	3.137	0.1607141358399081
X	0.012269188974450002	1803	1.803	0.189498907122671
X	0.012778142847104117	2572	2.572	0.17063400781777197
X	0.01279346478436109	3647	3.647	0.15194420397126518
X	0.012622972904535772	1770	1.77	0.19248466159819536
X	0.011888542997122374	660	0.66	0.2621369441954021
X	0.012565148429637414	3372	3.372	0.15503390483936055
X	0.013223408069980088	10763	10.763	0.1071034235790337
X	0.012454554125141341	3585	3.585	0.15145362579182486
X	0.012342620604065704	1661	1.661	0.19513982703073304
X	0.01438151149006522	4073	4.073	0.15227550232936973
X	0.012853485158367981	1105	1.105	0.22657894004437376
X	0.012424623233664698	2531	2.531	0.1699535971007169
X	0.012113451100456148	1118	1.118	0.22127993183409284
X	0.012609608553927742	2107	2.107	0.18155673178968795
X	0.012286892180307103	1338	1.338	0.20940898475749806
X	0.012483065398125177	2512	2.512	0.17064773838657668
X	0.012522647682049663	2475	2.475	0.17167497241408797
X	0.012610262429755409	3942	3.942	0.1473451465027851
X	0.012225050496827462	1559	1.559	0.19867116974067975
X	0.012672868780089842	1339	1.339	0.21152650173273851
X	0.012627150600273062	3440	3.44	0.15425845330916482
X	0.012096569512168026	2465	2.465	0.16993458187565125
X	0.012471671725731567	3744	3.744	0.14934692222050924
X	0.012396657665936294	2323	2.323	0.17475051574872613
X	0.01236983288999624	9539	9.539	0.10904865865233561
X	0.012500701337548822	3877	3.877	0.14773368339865664
X	0.012480379679451113	3082	3.082	0.15939146210100064
X	0.012643034672111201	7775	7.775	0.11759358855328905
X	0.012454730994332351	3501	3.501	0.152656070946242
X	0.012592479673703026	4368	4.368	0.14232336704289295
X	0.01181310857892633	1495	1.495	0.19917783207114853
X	0.012428121920403072	3156	3.156	0.15791480080891376
X	0.012516945506553901	1379	1.379	0.2085985883553591
X	0.014361103991597377	5274	5.274	0.1396419486258113
X	0.012481688545403912	906	0.906	0.2397259497672192
X	0.013502266724192838	2295	2.295	0.18052665327270812
X	0.013345800338565366	1148	1.148	0.2265344441537483
X	0.012592547161694305	1395	1.395	0.20821553354863412
X	0.012344248309597981	1200	1.2	0.21748424488920398
X	0.012297336215456402	1263	1.263	0.213535080053833
X	0.012709970556789223	3176	3.176	0.15876496923901479
X	0.01251722542299615	3219	3.219	0.15725182664949722
X	0.012353286539825207	2002	2.002	0.1834169556161397
X	0.012779114043701906	2335	2.335	0.17622652886165668
X	0.012403688982015184	1402	1.402	0.20682400869289594
X	0.014495411161474724	5589	5.589	0.13739327516867206
X	0.01241834867269253	1428	1.428	0.20564202919976943
X	0.012556668165213275	10791	10.791	0.10518106766928437
X	0.012457581461215839	2809	2.809	0.16429612941114394
X	0.012374986771903745	1469	1.469	0.20347340331422334
X	0.012555494656631895	2136	2.136	0.18047242739523278
X	0.012525163477075526	1927	1.927	0.18662388646803724
X	0.012291892068574872	2236	2.236	0.17648818884797554
X	0.012555180498684251	1638	1.638	0.19716796517289392
X	0.012299041204466446	1850	1.85	0.18803261073040872
X	0.014526444632533629	11180	11.18	0.10912037333995757
X	0.012458890651761615	2303	2.303	0.17554770065613434
X	0.012798760550725779	4912	4.912	0.1376057300073249
X	0.012468843165951109	958	0.958	0.2352268454504918
X	0.012422564589719373	3090	3.09	0.15900749460392596
X	0.012378346304878949	1279	1.279	0.21310682630113562
X	0.012478170866202624	2462	2.462	0.171772759750368
X	0.012537992958641836	1638	1.638	0.1970779523991238
X	0.012344823870735893	1306	1.306	0.21143679823499417
X	0.01254196851715332	1528	1.528	0.20171929919356574
X	0.012397488900955307	1369	1.369	0.20843793406766448
X	0.012423435144634854	2253	2.253	0.17666892582597377
X	0.012483333461095224	2643	2.643	0.1677816532908677
X	0.012569335687576979	1194	1.194	0.21916405779589332
X	0.013982365279427643	4936	4.936	0.14149306821859436
X	0.012607826712311422	9776	9.776	0.10884947684093729
X	0.014518004482266339	4506	4.506	0.14769708122788552
X	0.012560019349177588	1986	1.986	0.18492843233219092
X	0.012522311128580401	1401	1.401	0.20753058880354272
X	0.01401228475525428	2400	2.4	0.18006635394575002
X	0.012525028169375077	1401	1.401	0.20754559743608217
X	0.012548172469632718	1589	1.589	0.19913710721741706
X	0.012825478143158386	1282	1.282	0.21547415184501614
X	0.012610119745721021	3027	3.027	0.16090477407859693
X	0.012531852926821706	3594	3.594	0.1516395215699329
X	0.012492982585430721	8160	8.16	0.11525470371204356
X	0.01254944554754873	3323	3.323	0.15572729230417667
X	0.012551636306471893	8274	8.274	0.11490218217951463
X	0.012527179824761822	3115	3.115	0.15902488379323818
X	0.012843281454485753	1884	1.884	0.18961165715272518
X	0.012878379383749763	5214	5.214	0.1351751092739552
X	0.011793468193261396	684	0.684	0.25834207656063407
X	0.01494399573715981	2837	2.837	0.1739946119255403
X	0.012445367208410725	2011	2.011	0.183596845442519
X	0.012374254337997347	2393	2.393	0.1729253963850213
X	0.014028294476695045	1250	1.25	0.22388841421804279
X	0.012313187489527545	1942	1.942	0.18508607921475553
X	0.012410395890082788	1444	1.444	0.2048359344655637
X	0.012529497654750213	3262	3.262	0.1566089462810416
X	0.012249089959702213	2616	2.616	0.16729720865726472
X	0.013494489972775836	4059	4.059	0.1492493589852163
X	0.0122788419964247	881	0.881	0.24065441533188142
X	0.01284467499474016	1428	1.428	0.20796885662677564
X	0.012474401135226564	1344	1.344	0.21015520245394315
X	0.01261164818034557	6582	6.582	0.12420468841539206
X	0.012472937665886665	2293	2.293	0.17586857003328576
X	0.014305734710500196	3990	3.99	0.15305437813285774
X	0.01473398099402261	2377	2.377	0.18369484736335784
X	0.012379146834987075	5215	5.215	0.1333968170768759
X	0.0123289823105031	2626	2.626	0.16744705069902527
X	0.012472299175945854	4606	4.606	0.1393822685585509
X	0.012531810932238666	3094	3.094	0.15940349583561514
X	0.012513433101869175	2936	2.936	0.16213376767275522
X	0.012391296478866755	1642	1.642	0.19614678697606056
X	0.012498820270192295	3184	3.184	0.15774853015481088
X	0.012407585811620643	1896	1.896	0.1870465281543601
X	0.012868880275887664	4875	4.875	0.13820445631639564
X	0.01250650888231188	3668	3.668	0.15051123626651006
X	0.012348609583918015	1710	1.71	0.1932890789387151
X	0.01417925171563518	3034	3.034	0.167190878061576
X	0.013792323458592094	3717	3.717	0.15481568458630177
X	0.012516232326518816	8128	8.128	0.1154773051416621
X	0.012367252852323457	1574	1.574	0.19880295588514088
X	0.012002536150504898	917	0.917	0.23566833254818384
X	0.01257210401061802	3394	3.394	0.15472674016951776
X	0.012534435881065543	1646	1.646	0.19673954110764524
X	0.012452456794123887	2777	2.777	0.16490217654306602
X	0.012764347377826531	2126	2.126	0.18175168739212388
X	0.012395282007699425	2151	2.151	0.17928281687902076
X	0.012401621548537347	1068	1.068	0.2264483047657358
X	0.012353939166790652	2367	2.367	0.17346122143757764
X	0.012512428863167203	1565	1.565	0.19995967697820252
X	0.012609790326421526	4109	4.109	0.14531950437627364
X	0.012405306940642144	2291	2.291	0.1756011982415924
X	0.01258881768574044	8123	8.123	0.11572383842748281
X	0.012580827745919712	3211	3.211	0.15764842521858474
X	0.013310808045179848	2812	2.812	0.16790477021645556
X	0.012339166218064235	1378	1.378	0.20765650120237453
X	0.01424209841266708	2712	2.712	0.1738180069000882
X	0.0135410338570574	2682	2.682	0.17155274971267204
X	0.012504565404532618	2670	2.67	0.1673089268956818
X	0.013939210760787473	5284	5.284	0.13817361664500244
X	0.012514516185386028	3066	3.066	0.1598136870766534
X	0.012329548193755079	2462	2.462	0.17108806112529074
X	0.012499893080642838	2639	2.639	0.16794057507792307
X	0.012573805740585517	3819	3.819	0.14876668855360395
X	0.012409906962330703	796	0.796	0.24981498513744568
X	0.012379010647724198	1249	1.249	0.2148034072999101
X	0.012448132693156796	1187	1.187	0.21888592196684345
X	0.012422589231763214	730	0.73	0.25721507692789086
X	0.012605339412496724	5943	5.943	0.12848416445016925
X	0.012830315209160585	2545	2.545	0.1714680385001229
X	0.012247141481689278	3338	3.338	0.1542349513686234
X	0.012784576740674288	1132	1.132	0.22436062570555163
X	0.012391469697804101	3820	3.82	0.14803116331576255
X	0.012715675010876492	2886	2.886	0.1639385278889939
X	0.012384498110255309	2866	2.866	0.16287986788220263
X	0.012988300843915274	3549	3.549	0.15410454470794105
X	0.012421260943422061	1552	1.552	0.20002824420391008
X	0.012467579101360376	890	0.89	0.24106308412228095
X	0.0137297547710658	3504	3.504	0.15765202769295986
X	0.011677948431986997	815	0.815	0.24288627894283418
X	0.012205034987693441	1306	1.306	0.21063568557917908
X	0.012456765345324541	1709	1.709	0.19388955189194793
X	0.014277894942855963	2879	2.879	0.17053263621179002
X	0.014193741089754242	4017	4.017	0.15231114480779362
X	0.012438938593597687	1397	1.397	0.20726643262353484
X	0.012258202990156505	1858	1.858	0.18755430324194275
X	0.01256876842919745	2952	2.952	0.16207851808415732
X	0.012597530270608528	5851	5.851	0.12912740920722057
X	0.012437418296359488	1349	1.349	0.20968760712006523
X	0.012526266394400428	5823	5.823	0.12908970781572382
X	0.012842912925512949	27282	27.282	0.07779095643722848
X	0.013050648370374838	9260	9.26	0.1121175762348957
X	0.012481179206365602	5867	5.867	0.12861151187747025
X	0.012220357763399995	822	0.822	0.24588801876625382
X	0.012470884916212984	3336	3.336	0.15519953290065702
X	0.012579218263390717	19116	19.116	0.08697989457614133
X	0.012529839986906795	101048	101.048	0.04986616092654664
X	0.012574235563660942	3416	3.416	0.15440258844700236
X	0.012583797117712512	44745	44.745	0.06551726079883662
X	0.012347123128691401	17769	17.769	0.08857291351170864
X	0.012571979425834386	16669	16.669	0.09102584202294553
X	0.014490394139524094	270757	270.757	0.03768485382243085
X	0.012433933940812162	1975	1.975	0.1846490816000829
X	0.01257485772494948	4791	4.791	0.13794066109541805
X	0.012577808781043545	174436	174.436	0.04162200108540832
X	0.013213310654619222	525522	525.522	0.02929590473227789
X	0.012557533878900698	4167	4.167	0.14444204195483273
X	0.014483170653551302	16636	16.636	0.0954856800337263
X	0.012507658867215896	6760	6.76	0.1227655302461195
X	0.012572636246400428	61865	61.865	0.05879320169101504
X	0.012651056787921732	19895	19.895	0.08599257501049008
X	0.014088444364534306	74732	74.732	0.05733926813137914
X	0.013230617151766311	29250	29.25	0.0767629065931677
X	0.012534293792940592	9497	9.497	0.10969103743979348
X	0.0125305943914098	4126	4.126	0.14481520039132284
X	0.013104266893196369	216613	216.613	0.03925630364082683
X	0.012499021683623466	4698	4.698	0.13856525213414608
X	0.012351406764642928	2140	2.14	0.17937732794649075
X	0.012451523462245869	41865	41.865	0.06675090096057429
X	0.012388819709475259	17605	17.605	0.08894699925065944
X	0.012597811190615945	129680	129.68	0.04596994996810564
X	0.012554142338194987	95983	95.983	0.050761086527372615
X	0.012492449411472196	1558	1.558	0.2001520528100437
X	0.013798839481879245	11676	11.676	0.10572624925425142
X	0.012525008685492306	5284	5.284	0.1333332327292711
X	0.012592395015699748	13056	13.056	0.09880207505344746
X	0.012886149750115037	65011	65.011	0.05830584707086215
X	0.012848046077372905	9062	9.062	0.11234084232662063
X	0.012565276739674966	55942	55.942	0.06078709042425506
X	0.012140262007380533	779	0.779	0.24978337765838796
X	0.013039215573133051	4604	4.604	0.14148336600709963
X	0.012988069191313604	25368	25.368	0.07999928795674915
X	0.012819062425257873	100371	100.371	0.050359646215297156
X	0.01275701990469019	30791	30.791	0.07454899844477844
X	0.012564995893809891	3572	3.572	0.15208403750431293
X	0.013209529914457984	175644	175.644	0.042210258770745995
X	0.014729483962926665	16108	16.108	0.09706186615296415
X	0.012881310375754145	16049	16.049	0.0929331566895322
X	0.013267653733612607	52750	52.75	0.0631234249605725
X	0.014146000491241956	4884	4.884	0.14254522413137768
X	0.012838478115280574	42047	42.047	0.06733789096487337
X	0.013878339063500921	103642	103.642	0.051160396639246285
X	0.014391368503412695	168836	168.836	0.04400942352132042
X	0.01255717199827332	11958	11.958	0.10164306782827162
X	0.012586013583169525	14267	14.267	0.09590733977945382
X	0.012179925107330925	19048	19.048	0.0861519433734409
X	0.012522952669795471	4754	4.754	0.13810705165317666
X	0.012585908142942625	12629	12.629	0.09988613254697733
X	0.014101417875291023	8716	8.716	0.11739470810137258
X	0.012539030350388265	14030	14.03	0.0963242037165565
X	0.013161890725594384	49556	49.556	0.06427972530724463
X	0.012876463564879464	50724	50.724	0.06331807502917255
X	0.012470265842657249	4682	4.682	0.13861644848409724
X	0.013944909946363009	6726	6.726	0.12751255571899722
X	0.012909323354363605	21415	21.415	0.08447505181305558
X	0.012808123571601474	13422	13.422	0.09845159538277369
X	0.012952567816119238	6477	6.477	0.1259874616229033
X	0.012515137248244145	7379	7.379	0.11925576119036756
X	0.012813227935818123	154948	154.948	0.04356698695697589
X	0.01322090229281485	6254	6.254	0.12834168580659605
X	0.014184164997394536	304751	304.751	0.03597103039283877
X	0.012572865939021398	4755	4.755	0.13828059975633175
X	0.01271598830392161	14103	14.103	0.09660795104992251
X	0.013027708488743996	13919	13.919	0.09781827482257069
X	0.012921301269299267	146167	146.167	0.04454705242139247
X	0.01269013785909787	34029	34.029	0.07197898492204359
X	0.012875277333622502	6835	6.835	0.12350163047330766
X	0.012609644195034792	71370	71.37	0.056112856617399405
X	0.013425756637691761	89974	89.974	0.05304045578851708
X	0.01287503306217494	4285	4.285	0.14429983756915601
X	0.012601257414039526	49440	49.44	0.06340329190282391
X	0.012628778932878717	85809	85.809	0.052797031694186945
X	0.014676986625791052	209631	209.631	0.04121548968914252
X	0.012678742832092976	57222	57.222	0.06051147231892478
X	0.012440342973076618	2327	2.327	0.17485524209498138
X	0.01254727208955697	16068	16.068	0.09208647120448776
X	0.012856252754949192	66912	66.912	0.05770367373907779
X	0.014494362393459857	91421	91.421	0.05412331423096204
X	0.012568477952826708	9782	9.782	0.10871388343610527
X	0.01252906027404289	2172	2.172	0.17934374150992585
X	0.012875478056280093	80219	80.219	0.05434530615920264
X	0.012576708958695733	126751	126.751	0.046295474407630224
X	0.012447542304391419	26361	26.361	0.07787066974119936
X	0.012529898792175372	7063	7.063	0.12105594406489352
X	0.012754749864804432	12792	12.792	0.0999028394019588
X	0.013865576251423836	7884	7.884	0.12070643847692175
X	0.012820599073034783	4457	4.457	0.14221821660084316
X	0.012525710180193399	10141	10.141	0.10729361021779499
X	0.01259473726674128	4879	4.879	0.1371785152805574
X	0.013425991199733273	34783	34.783	0.07281034805983222
X	0.012871002625057313	29488	29.488	0.07585583972089278
X	0.012854855645468592	15345	15.345	0.09426853772265825
X	0.014422055906880716	7747	7.747	0.12301683227417466
X	0.012533924929411097	5372	5.372	0.13263262398729245
X	0.012849613029091557	455793	455.793	0.03043505792258911
X	0.012505015362481817	7793	7.793	0.11707383669901796
X	0.01282324169769072	84613	84.613	0.05331551126676432
X	0.012591655283337909	2012	2.012	0.18428286256492699
X	0.012490975661781872	4361	4.361	0.14201583070258073
X	0.012593678203866102	2686	2.686	0.16737176445081978
X	0.01288438115335176	14918	14.918	0.09523231031763069
X	0.013292094538589483	3443	3.443	0.15687441992227139
X	0.012439470565390093	20652	20.652	0.0844526458188067
X	0.012519826876624182	3397	3.397	0.15446648319260795
X	0.014048200453883698	3128	3.128	0.16498760454262937
X	0.014290450356825303	212350	212.35	0.040675342333873314
X	0.014825415685647948	48472	48.472	0.06737601383583455
X	0.012986335176422977	13587	13.587	0.09850410834137102
X	0.012990277801231156	65047	65.047	0.05845168776646876
X	0.014931097538483505	77917	77.917	0.05765272463012229
X	0.012847337462765156	2092	2.092	0.1831262001428246
X	0.01256864842538687	6973	6.973	0.12169973642771688
X	0.012807631629744517	6181	6.181	0.12748821137745064
X	0.012306597442711745	942	0.942	0.23552067137302501
X	0.012767920256897354	5087	5.087	0.13590000560798374
X	0.012529929685185888	10334	10.334	0.10663343322792813
X	0.012419755416604821	3473	3.473	0.15292180657310364
X	0.01255502502255016	4031	4.031	0.1460388019467481
X	0.012519850268925931	6525	6.525	0.12426234344551108
X	0.012555453843060959	4514	4.514	0.14063398627340865
X	0.012967974661875468	220057	220.057	0.038914577691035
X	0.01246943119545825	8792	8.792	0.11235342634465623
X	0.014093553039039227	32397	32.397	0.07577138624412466
X	0.012589470183222906	7734	7.734	0.11763442354691832
X	0.01245211921187883	29372	29.372	0.07512248000057783
X	0.012844245133384365	30000	30.0	0.07536952477406661
X	0.013547157477089702	531375	531.375	0.02943172297660711
X	0.013438338057375753	387705	387.705	0.032604788451503705
X	0.012884303081493126	22823	22.823	0.08264743901371761
X	0.012592419604177086	37164	37.164	0.06971543022282725
X	0.012565721828656746	2555	2.555	0.17005869640135796
X	0.012430952791767088	16580	16.58	0.09084617162870008
X	0.012582702261111661	24815	24.815	0.07974189404417822
X	0.012864504392913988	45598	45.598	0.06558669625924458
X	0.012291133229698135	3567	3.567	0.1510414938526101
X	0.012547830826218983	82431	82.431	0.053394029295610594
X	0.014814982006333308	154096	154.096	0.04581101570931704
X	0.012396736522097569	28750	28.75	0.07554803519254284
X	0.012533735878425318	9691	9.691	0.10895252887547653
X	0.014932845056650294	15732	15.732	0.09827722243530322
X	0.012603426436591292	9089	9.089	0.11151266007662662
X	0.012535174178245621	3769	3.769	0.1492684690255799
X	0.014046694467676911	89204	89.204	0.05400035344081732
X	0.012543417424697487	26074	26.074	0.07835548527442332
X	0.012292683938797426	2871	2.871	0.16238198311641458
X	0.01319142856571513	101431	101.431	0.05066488263463758
X	0.013249279870558356	9086	9.086	0.11339829214496107
X	0.012477586363764543	1127	1.127	0.22287886604217755
X	0.012772237013408199	63821	63.821	0.05849262629407516
X	0.013127867316147762	84934	84.934	0.05366661463727974
X	0.013505438338179576	56238	56.238	0.06215741417675592
X	0.01261531981691063	137284	137.284	0.04512592460637128
X	0.012594163043925138	89190	89.19	0.05207361355606741
X	0.012574615393802923	7190	7.19	0.12048193671585519
X	0.012505085743517358	33103	33.103	0.0722891480735236
X	0.013267594235825995	97361	97.361	0.051459913479857886
X	0.012605609445702938	100800	100.8	0.05000741880089689
X	0.01258655576433175	6480	6.48	0.12477013149954544
X	0.01325342825675543	84516	84.516	0.05392577959777837
X	0.012908453728214794	104688	104.688	0.04977284204317379
X	0.012847424394974114	40435	40.435	0.0682369396622286
X	0.012826278497584625	28955	28.955	0.07622996120643899
X	0.013742258595288244	21041	21.041	0.08676220486398956
X	0.012559379685797812	12590	12.59	0.09991886372298378
X	0.012581118911368304	5344	5.344	0.13303041295367524
X	0.01248655166023615	3582	3.582	0.15162552313240943
X	0.01413485790086213	131341	131.341	0.04756604503870403
X	0.012898036188052124	67786	67.786	0.05751677513711055
X	0.012605059395075639	138906	138.906	0.04493740231800448
X	0.012835535149918286	117783	117.783	0.047765074138047835
X	0.012558986670438024	35141	35.141	0.07096542823812878
X	0.01301198620949423	10108	10.108	0.10878260477679205
X	0.012813377413239709	50255	50.255	0.06341053932419091
X	0.012189521321999817	2232	2.232	0.1761019454210388
X	0.012593287027015827	8727	8.727	0.11300336481322155
X	0.012868494341777887	131854	131.854	0.04604100646938342
X	0.014285622984040404	18244	18.244	0.0921707327818576
X	0.012779376926432504	2838	2.838	0.16513245196544923
X	0.012463072941516619	7357	7.357	0.11920874803520697
X	0.014488508570570355	124964	124.964	0.04876178417974679
X	0.012573276364755153	21600	21.6	0.08349585254349005
X	0.014358110459705729	50211	50.211	0.06588190367869247
X	0.012475732363796375	19926	19.926	0.08554907297832641
X	0.012547030863411406	18455	18.455	0.087931059396052
X	0.013208370501912451	3180	3.18	0.1607461972020194
X	0.014164738764414618	1808	1.808	0.1986110379614755
X	0.013856243172500584	32723	32.723	0.07509265529693399
X	0.012469438439971177	27725	27.725	0.0766169948362733
X	0.012560329911323592	2690	2.69	0.16714097154033572
X	0.013099560913754844	138324	138.324	0.04558126772985716
X	0.012399326848931405	2975	2.975	0.16093001288708197
X	0.013620115871630622	13205	13.205	0.1010370828905227
X	0.012792357663021663	4294	4.294	0.14388963726436677
X	0.012770976184214971	75072	75.072	0.05540921996151249
X	0.012661615749221023	4413	4.413	0.14209705179951154
X	0.01257837270511295	22108	22.108	0.08286255768561836
X	0.012796926698469711	12487	12.487	0.10082057969254722
X	0.013487104890114876	15873	15.873	0.09471524920386397
X	0.013484385731580494	24726	24.726	0.08170069811200209
X	0.012876663044446991	377162	377.162	0.03244081829373802
X	0.012533788947276461	9221	9.221	0.11077322127772686
X	0.012255876144291104	4117	4.117	0.14385374266177703
X	0.01282553093912795	90175	90.175	0.05219910626485053
X	0.012774260746993763	10874	10.874	0.10551531083991716
X	0.013152246447580798	240654	240.654	0.037949170241529065
X	0.01251897516645126	2535	2.535	0.17029305556429364
X	0.01279783348163379	81014	81.014	0.05405785302320955
X	0.013174424225056656	232923	232.923	0.038386020384992225
X	0.012262208389814738	3481	3.481	0.1521556988376869
X	0.012734164988241177	249265	249.265	0.03710533783204304
X	0.012543431892412433	9278	9.278	0.11057425007422972
X	0.012798418626278361	275211	275.211	0.03596086870142311
X	0.012604298283043111	7806	7.806	0.11731765543162431
X	0.01337869884827927	130189	130.189	0.046839601938677734
X	0.012565932271083697	2167	2.167	0.1796574651421363
X	0.014285246831893824	3889	3.889	0.1542943187432737
X	0.012394529470140024	5556	5.556	0.1306639960070327
X	0.012601235867682076	25183	25.183	0.07939050388838505
X	0.013436351016298696	35149	35.149	0.07257540294599985
X	0.012909127083143719	70154	70.154	0.056878501476338966
X	0.01258119793778103	11705	11.705	0.10243542941167469
X	0.012483176648604726	3595	3.595	0.1514288905566252
X	0.01255673132163229	6767	6.767	0.12288347241641373
X	0.014582153288809758	121696	121.696	0.04930017397335576
X	0.014700355665133061	30281	30.281	0.07859336368746415
X	0.013597795854933014	189141	189.141	0.04158093766908687
X	0.012606820888079347	30068	30.068	0.07484572743099228
X	0.012514108578626015	9315	9.315	0.11034153463736705
X	0.012516850395409883	3016	3.016	0.1607019850724174
X	0.012545999242309123	14126	14.126	0.09612330285702742
X	0.01497629646979816	675716	675.716	0.028089676789490894
X	0.01405384511162984	6499	6.499	0.12931518991325597
X	0.012577272307018482	11016	11.016	0.10451713879027828
X	0.012535316696048822	90365	90.365	0.05176604901550961
X	0.012452474042471476	35586	35.586	0.07046803427387258
X	0.0128293163729979	3673	3.673	0.15172629825025377
X	0.01253932262593535	2594	2.594	0.16908355419192173
X	0.0128925514836117	32323	32.323	0.07361094684806911
X	0.012871546863640322	19670	19.67	0.08681780353270856
X	0.012468520358637469	8655	8.655	0.1129403897630193
X	0.012889307965475204	35630	35.63	0.07125323598021525
X	0.014636664937985133	96851	96.851	0.05326549990376582
X	0.013241030776198752	45114	45.114	0.06645640129345705
X	0.013787310743039773	227206	227.206	0.03929644435147064
X	0.013251600744053398	54730	54.73	0.06232768046032784
X	0.014413092971429812	7329	7.329	0.1252864649446709
X	0.012896057051548867	46435	46.435	0.06524348715390019
X	0.01250432301274313	11632	11.632	0.10243977049086816
X	0.012489410846158566	6812	6.812	0.12239277200018198
X	0.012408992225026836	13990	13.99	0.09608146123433665
X	0.012587897558963435	6406	6.406	0.12525317838683084
X	0.012599996940711844	62841	62.841	0.058529630726028195
X	0.01228661574479821	13436	13.436	0.09706308569535081
X	0.01239918769239495	2436	2.436	0.17201724326018916
X	0.012523131129105702	14867	14.867	0.09444156942545687
X	0.012892725147620037	9953	9.953	0.10900930566400066
X	0.012545185816542582	16383	16.383	0.09148738670966579
X	0.012795552246405459	43666	43.666	0.06642096913394246
X	0.012449837186500577	5057	5.057	0.1350279184558251
X	0.012490577320524335	2299	2.299	0.1757982362730382
X	0.012797042025476367	13567	13.567	0.09807129808627217
X	0.013215261891086572	3064	3.064	0.16277798609628116
X	0.012548101810678607	5927	5.927	0.12840473399771066
X	0.012891348580686307	15883	15.883	0.09328001458352901
X	0.013248953853306664	113715	113.715	0.0488413819784162
X	0.012450462400795103	6827	6.827	0.12217580036724321
X	0.012448526635709861	1563	1.563	0.19970379824052495
X	0.012494353560978302	3020	3.02	0.1605347105937978
X	0.012596576338121711	85444	85.444	0.05282712597963759
X	0.014932363401785593	311304	311.304	0.0363341175174055
X	0.01242749091055566	31463	31.463	0.07337156365622147
X	0.013006593090996532	5230	5.23	0.13548377478657164
X	0.012872003770612381	62402	62.402	0.0590857224006482
X	0.013117325280528102	138570	138.57	0.045574861453403843
X	0.014371287010590698	1793	1.793	0.20012674161505303
X	0.014158195308666278	7681	7.681	0.12261117225549759
X	0.012888022976473375	25619	25.619	0.07953190333500083
X	0.01310525192822202	5828	5.828	0.13101125602290153
X	0.013554089378833607	45618	45.618	0.06672850087651427
X	0.01315540105015913	53928	53.928	0.06248319944040743
X	0.012865422019948627	17298	17.298	0.0906030285022639
X	0.012565412780880874	4636	4.636	0.13942621708135589
X	0.014115543255183438	3607	3.607	0.1575858257009325
X	0.012510569671648368	22712	22.712	0.081973579968816
X	0.013396445502365666	27715	27.715	0.07847986240042076
X	0.012600691105370786	38469	38.469	0.06893310208551595
X	0.012562348156783715	22687	22.687	0.08211665523898412
X	0.013895977932929708	177924	177.924	0.0427449546565176
X	0.01256620845842879	4384	4.384	0.14205109030162316
X	0.012323561877461787	2369	2.369	0.17327014138663568
X	0.012561187453099521	8246	8.246	0.11506125908505617
X	0.012883995375521661	49151	49.151	0.06399893308955666
X	0.012601303727635092	25254	25.254	0.07931617599684906
X	0.012550565841661374	4428	4.428	0.1415202427121816
X	0.012488375633780536	2916	2.916	0.16239505894553255
X	0.012595421144934879	28978	28.978	0.07574978916564364
X	0.012530806755808773	2771	2.771	0.16536648768162782
X	0.012520863495592614	11066	11.066	0.10420323574230046
X	0.012489617204561048	3250	3.25	0.15663491976631042
X	0.012619137564809028	11052	11.052	0.10451924554114973
X	0.012836240787318867	7156	7.156	0.12150360802838782
X	0.012591917099611308	5340	5.34	0.13310167870622422
X	0.012396939738676338	3834	3.834	0.14787251539205665
X	0.012870423770421992	85125	85.125	0.05327358303882896
X	0.012888141125489031	145762	145.762	0.0445500970830513
X	0.012890684079971746	136278	136.278	0.04556346438271516
X	0.013311811442762424	136677	136.677	0.046009470996266844
X	0.014481471686637142	22346	22.346	0.08653749144539873
X	0.012486255764309766	7232	7.232	0.11996593311834952
X	0.014818196179681786	13025	13.025	0.10439327828904621
X	0.013181855063561693	106240	106.24	0.04987652144594474
X	0.014879551117243093	8194	8.194	0.12200134634809935
X	0.012626542349425292	18133	18.133	0.08863493408176093
X	0.012526948931252727	6683	6.683	0.12329854412675043
X	0.014823140130007344	269362	269.362	0.03803656548718356
X	0.013294370089101842	50030	50.03	0.0642903446390922
X	0.014089148484774367	16280	16.28	0.09529646302145026
X	0.013297008474922274	164546	164.546	0.043233680164897995
X	0.013069586931860949	31737	31.737	0.0743986970842813
X	0.01261657252676876	21091	21.091	0.08425869703435905
X	0.01478381187523137	159969	159.969	0.04521163358001193
X	0.014025282028621112	88103	88.103	0.05419679883027027
X	0.013467883758247692	70850	70.85	0.05749805689164247
X	0.012462283902333875	11505	11.505	0.10269997531702467
X	0.012938422019459281	78354	78.354	0.05486222035092676
X	0.012879513924735863	7091	7.091	0.12201053436763612
X	0.012944822878902428	50664	50.664	0.0634549557821595
X	0.01465005628236305	408169	408.169	0.03298626107219162
X	0.012419929756023946	32704	32.704	0.07241681530747185
X	0.012993193657714766	42056	42.056	0.06760248323838473
X	0.012616012262796928	7327	7.327	0.11985752449146685
X	0.012465620588859113	34351	34.351	0.07132769229820889
X	0.01464863865911089	19266	19.266	0.09127151025399238
X	0.012483970417210583	6048	6.048	0.1273249896240506
X	0.012567852904546926	21490	21.49	0.08362604500913849
X	0.012899833899291382	225971	225.971	0.038504409745935525
X	0.01369518910752781	11229	11.229	0.10684207580658645
X	0.014461041494391643	295183	295.183	0.03659058981002804
X	0.012848877357863256	24091	24.091	0.08109675039384749
X	0.012625866717632786	32461	32.461	0.07299611868020658
X	0.012454979419970128	10678	10.678	0.10526508995181079
X	0.014330015871199671	41274	41.274	0.07028403783378061
X	0.013514163137548099	152157	152.157	0.04461686726894886
X	0.012500423243304828	2045	2.045	0.18284249342304304
X	0.012865073962643738	421105	421.105	0.031261318877599156
X	0.012715004749513354	27467	27.467	0.07735738251307123
X	0.013109065571823746	30277	30.277	0.07565186374005953
X	0.01343711152877971	4999	4.999	0.1390403743409283
X	0.012536064972618045	13596	13.596	0.09733074018897178
X	0.012496864799817944	2737	2.737	0.16589835943134157
X	0.012381744154633518	1480	1.48	0.20300498542057063
X	0.012546067342643416	10293	10.293	0.10682064939648933
X	0.014418461329862362	55825	55.825	0.06368396012549173
X	0.014753063955939756	1283100	1283.1	0.022570468472410187
X	0.012524432849988505	44948	44.948	0.06531544631221556
X	0.012609352970174263	73421	73.421	0.05558498620518426
X	0.012517928235322029	5612	5.612	0.13065867960259467
X	0.012555032397983442	27666	27.666	0.07684645213894614
X	0.013551267696744862	10083	10.083	0.11035619530933774
X	0.012556967131158975	73861	73.861	0.05539746070087979
X	0.012544509440353879	6263	6.263	0.12605413318741862
X	0.0138694876901314	627704	627.704	0.028060915830374407
X	0.012535134639346798	5062	5.062	0.13529101470680338
X	0.012624689096811862	74029	74.029	0.05545485868846928
X	0.012890462491130792	10526	10.526	0.1069879939822707
X	0.014452074028375232	52616	52.616	0.06500360603812878
X	0.012554075086448211	226082	226.082	0.03815102803433209
X	0.012478170964567003	19659	19.659	0.08594022828766971
X	0.012553732283056042	6113	6.113	0.12710798157004322
X	0.012614675434977723	47941	47.941	0.06408007293910445
X	0.012586372409117957	6940	6.94	0.12194959688611239
X	0.012079840428883621	6110	6.11	0.12550855487168858
X	0.012533472360099966	98462	98.462	0.05030381445957656
X	0.014519452700596323	287589	287.589	0.036959491877219613
X	0.014836212058545703	26183	26.183	0.08274997581003607
X	0.012850219302739323	29059	29.059	0.07618625479681712
X	0.014411658370102644	135954	135.954	0.04732681986282439
X	0.012938223660983577	122814	122.814	0.0472290191236995
X	0.012857756557069644	35949	35.949	0.07098383542021412
X	0.01309161418277224	4256	4.256	0.14543330027400028
X	0.012583548489615596	83218	83.218	0.05327563288983851
X	0.013429478057733169	47341	47.341	0.06570633377910057
X	0.012969211222900243	66717	66.717	0.05792850910570786
X	0.012548412928189414	19898	19.898	0.08575506721253176
X	0.012442858925979102	36869	36.869	0.06962303316268792
X	0.013730286500464118	34080	34.08	0.07385728480960022
X	0.012497853432703016	306158	306.158	0.03443208102961212
X	0.01256654632242876	41831	41.831	0.06697394737318074
X	0.014350694647022393	13129	13.129	0.10301024618792871
X	0.012707454754400478	3001	3.001	0.16178224544250877
X	0.01290575980271116	11563	11.563	0.10372999351819731
X	0.012592501518414076	63302	63.302	0.05837562276704597
X	0.012933214638145561	84810	84.81	0.053426059150991904
X	0.013233644682408247	157421	157.421	0.043806538179251404
X	0.012548829806354305	5169	5.169	0.1343998855894545
X	0.012565191479020995	16353	16.353	0.09159193302512697
X	0.01277315936087388	36718	36.718	0.07032986293481562
X	0.01289061526363728	32694	32.694	0.0733277787331907
X	0.012724847954502537	36977	36.977	0.07007670010729053
X	0.013231173267289804	58829	58.829	0.060813821754866326
X	0.012433518426765672	5577	5.577	0.1306364788340411
X	0.012976649795840465	24403	24.403	0.08101643363058805
X	0.013561434164615551	1750	1.75	0.1978893963716571
X	0.014164007647319956	4720	4.72	0.14423856221532566
X	0.012988477346487308	118760	118.76	0.047822177906763214
X	0.012615612914869987	30357	30.357	0.07462479792869822
X	0.012990442638329089	58602	58.602	0.06052068531070178
X	0.012536764652149987	1736	1.736	0.19329112662405437
X	0.012530851389865252	3212	3.212	0.15742305914373406
X	0.012880503919126802	74035	74.035	0.05582541039883994
X	0.012822645912077706	66076	66.076	0.05789547681888203
X	0.013122051481256539	30353	30.353	0.07561362140345096
X	0.012549507720030797	3399	3.399	0.15455812546495526
X	0.012797546935745774	4134	4.134	0.14574223308875175
X	0.013237550300329208	17033	17.033	0.09194021019677523
X	0.012731973675684159	20979	20.979	0.08466494872319691
X	0.01288041026832866	48972	48.972	0.06407086960878682
X	0.01344845191345697	7649	7.649	0.1206949811314513
X	0.012582204386745361	3348	3.348	0.15547375774725436
X	0.012361949423428432	10448	10.448	0.10576726303730055
X	0.012583672767235083	8530	8.53	0.11383770412302363
X	0.013665474478046169	142503	142.503	0.04577203654737967
X	0.01289655205330868	4846	4.846	0.13857878049786562
X	0.012564975316456242	23195	23.195	0.08151842014527767
X	0.014731815228261409	81783	81.783	0.05647606228880598
X	0.012866085795959242	31165	31.165	0.07446058386205129
X	0.01327376332315724	70315	70.315	0.05736522432333278
X	0.013275385433930862	112227	112.227	0.04908889563766578
X	0.012864895617732106	3070	3.07	0.16122137858936034
X	0.01278993131576518	64067	64.067	0.058444629254187516
X	0.012887618929539213	22156	22.156	0.08347577181038578
X	0.014042944160943741	231494	231.494	0.03929218165977278
X	0.012629320844662494	109682	109.682	0.048649854436921484
X	0.01252644690659306	25762	25.762	0.07863503939471957
X	0.01290349379157659	62242	62.242	0.05918449223376471
X	0.012589554552732887	8477	8.477	0.1140922277775303
X	0.012407009083835125	8307	8.307	0.11430738685025893
X	0.012567446659823142	3943	3.943	0.1471657533149876
X	0.012474945939943851	14430	14.43	0.0952629692121075
X	0.012579084778097563	70231	70.231	0.056368965416540864
X	0.014944197687490978	75890	75.89	0.0581785219616722
X	0.013606338865699256	8712	8.712	0.11602221134063047
X	0.012869981968132618	19987	19.987	0.08635286981807623
X	0.01314428990963405	8363	8.363	0.1162675026417418
X	0.014097976846445722	238676	238.676	0.03894478829810901
X	0.012575682553913989	23355	23.355	0.08135493419441855
X	0.012527055655705117	8329	8.329	0.11457381696337837
X	0.013448029165783701	51816	51.816	0.06378691319596315
X	0.013126375191496365	76521	76.521	0.0555633035258744
X	0.013097391364485398	6663	6.663	0.1252674770337885
X	0.01260234659483267	48097	48.097	0.06398985759485852
X	0.012903332763337941	19523	19.523	0.08710674413639316
time for making epsilon is 1.584195852279663
epsilons are
[0.20875260631708786, 0.09296625616711836, 0.19643268004067746, 0.1856026892087744, 0.0711116665546523, 0.07509248608613082, 0.06981438537510609, 0.06692997912021796, 0.12862408698969544, 0.07258259945483746, 0.1252469809992739, 0.04899506542604503, 0.1245534726917733, 0.049546776775394145, 0.0931786907200177, 0.06292985271040258, 0.07673668858546916, 0.0812885943463118, 0.03399070284837628, 0.05802768666547725, 0.09184875851735447, 0.03374519277627555, 0.09090807408509767, 0.09912096942824786, 0.09363497064395686, 0.06509929665530907, 0.08398538397580195, 0.12648131824644204, 0.08329272186032256, 0.02149161132527429, 0.126763740921516, 0.03903865109107597, 0.11172637493612801, 0.12657066866752453, 0.11161415749247657, 0.061159840889196786, 0.06543726293255342, 0.22234881536005394, 0.1583298322569709, 0.17105081003250808, 0.13876632700447117, 0.21199303143583723, 0.22477313718125616, 0.16609778371283904, 0.22894908709642708, 0.2157459802656522, 0.12476425596210235, 0.14157183017597375, 0.19519836995208, 0.11767847179550657, 0.10769985008192438, 0.20233557761860468, 0.1393125982352926, 0.1838484757197043, 0.13265412031462936, 0.14564781728665788, 0.15643543273387125, 0.176151703607428, 0.17062378516721374, 0.18754385813135724, 0.13441355108679723, 0.1940947397638672, 0.12782583592482402, 0.12445972804419987, 0.16944167758162246, 0.18095911390086794, 0.15875182519510922, 0.1174679775516784, 0.14113579285824931, 0.20399743647968077, 0.13184644918837066, 0.16984888774468634, 0.1607141358399081, 0.189498907122671, 0.17063400781777197, 0.15194420397126518, 0.19248466159819536, 0.2621369441954021, 0.15503390483936055, 0.1071034235790337, 0.15145362579182486, 0.19513982703073304, 0.15227550232936973, 0.22657894004437376, 0.1699535971007169, 0.22127993183409284, 0.18155673178968795, 0.20940898475749806, 0.17064773838657668, 0.17167497241408797, 0.1473451465027851, 0.19867116974067975, 0.21152650173273851, 0.15425845330916482, 0.16993458187565125, 0.14934692222050924, 0.17475051574872613, 0.10904865865233561, 0.14773368339865664, 0.15939146210100064, 0.11759358855328905, 0.152656070946242, 0.14232336704289295, 0.19917783207114853, 0.15791480080891376, 0.2085985883553591, 0.1396419486258113, 0.2397259497672192, 0.18052665327270812, 0.2265344441537483, 0.20821553354863412, 0.21748424488920398, 0.213535080053833, 0.15876496923901479, 0.15725182664949722, 0.1834169556161397, 0.17622652886165668, 0.20682400869289594, 0.13739327516867206, 0.20564202919976943, 0.10518106766928437, 0.16429612941114394, 0.20347340331422334, 0.18047242739523278, 0.18662388646803724, 0.17648818884797554, 0.19716796517289392, 0.18803261073040872, 0.10912037333995757, 0.17554770065613434, 0.1376057300073249, 0.2352268454504918, 0.15900749460392596, 0.21310682630113562, 0.171772759750368, 0.1970779523991238, 0.21143679823499417, 0.20171929919356574, 0.20843793406766448, 0.17666892582597377, 0.1677816532908677, 0.21916405779589332, 0.14149306821859436, 0.10884947684093729, 0.14769708122788552, 0.18492843233219092, 0.20753058880354272, 0.18006635394575002, 0.20754559743608217, 0.19913710721741706, 0.21547415184501614, 0.16090477407859693, 0.1516395215699329, 0.11525470371204356, 0.15572729230417667, 0.11490218217951463, 0.15902488379323818, 0.18961165715272518, 0.1351751092739552, 0.25834207656063407, 0.1739946119255403, 0.183596845442519, 0.1729253963850213, 0.22388841421804279, 0.18508607921475553, 0.2048359344655637, 0.1566089462810416, 0.16729720865726472, 0.1492493589852163, 0.24065441533188142, 0.20796885662677564, 0.21015520245394315, 0.12420468841539206, 0.17586857003328576, 0.15305437813285774, 0.18369484736335784, 0.1333968170768759, 0.16744705069902527, 0.1393822685585509, 0.15940349583561514, 0.16213376767275522, 0.19614678697606056, 0.15774853015481088, 0.1870465281543601, 0.13820445631639564, 0.15051123626651006, 0.1932890789387151, 0.167190878061576, 0.15481568458630177, 0.1154773051416621, 0.19880295588514088, 0.23566833254818384, 0.15472674016951776, 0.19673954110764524, 0.16490217654306602, 0.18175168739212388, 0.17928281687902076, 0.2264483047657358, 0.17346122143757764, 0.19995967697820252, 0.14531950437627364, 0.1756011982415924, 0.11572383842748281, 0.15764842521858474, 0.16790477021645556, 0.20765650120237453, 0.1738180069000882, 0.17155274971267204, 0.1673089268956818, 0.13817361664500244, 0.1598136870766534, 0.17108806112529074, 0.16794057507792307, 0.14876668855360395, 0.24981498513744568, 0.2148034072999101, 0.21888592196684345, 0.25721507692789086, 0.12848416445016925, 0.1714680385001229, 0.1542349513686234, 0.22436062570555163, 0.14803116331576255, 0.1639385278889939, 0.16287986788220263, 0.15410454470794105, 0.20002824420391008, 0.24106308412228095, 0.15765202769295986, 0.24288627894283418, 0.21063568557917908, 0.19388955189194793, 0.17053263621179002, 0.15231114480779362, 0.20726643262353484, 0.18755430324194275, 0.16207851808415732, 0.12912740920722057, 0.20968760712006523, 0.12908970781572382, 0.07779095643722848, 0.1121175762348957, 0.12861151187747025, 0.24588801876625382, 0.15519953290065702, 0.08697989457614133, 0.04986616092654664, 0.15440258844700236, 0.06551726079883662, 0.08857291351170864, 0.09102584202294553, 0.03768485382243085, 0.1846490816000829, 0.13794066109541805, 0.04162200108540832, 0.02929590473227789, 0.14444204195483273, 0.0954856800337263, 0.1227655302461195, 0.05879320169101504, 0.08599257501049008, 0.05733926813137914, 0.0767629065931677, 0.10969103743979348, 0.14481520039132284, 0.03925630364082683, 0.13856525213414608, 0.17937732794649075, 0.06675090096057429, 0.08894699925065944, 0.04596994996810564, 0.050761086527372615, 0.2001520528100437, 0.10572624925425142, 0.1333332327292711, 0.09880207505344746, 0.05830584707086215, 0.11234084232662063, 0.06078709042425506, 0.24978337765838796, 0.14148336600709963, 0.07999928795674915, 0.050359646215297156, 0.07454899844477844, 0.15208403750431293, 0.042210258770745995, 0.09706186615296415, 0.0929331566895322, 0.0631234249605725, 0.14254522413137768, 0.06733789096487337, 0.051160396639246285, 0.04400942352132042, 0.10164306782827162, 0.09590733977945382, 0.0861519433734409, 0.13810705165317666, 0.09988613254697733, 0.11739470810137258, 0.0963242037165565, 0.06427972530724463, 0.06331807502917255, 0.13861644848409724, 0.12751255571899722, 0.08447505181305558, 0.09845159538277369, 0.1259874616229033, 0.11925576119036756, 0.04356698695697589, 0.12834168580659605, 0.03597103039283877, 0.13828059975633175, 0.09660795104992251, 0.09781827482257069, 0.04454705242139247, 0.07197898492204359, 0.12350163047330766, 0.056112856617399405, 0.05304045578851708, 0.14429983756915601, 0.06340329190282391, 0.052797031694186945, 0.04121548968914252, 0.06051147231892478, 0.17485524209498138, 0.09208647120448776, 0.05770367373907779, 0.05412331423096204, 0.10871388343610527, 0.17934374150992585, 0.05434530615920264, 0.046295474407630224, 0.07787066974119936, 0.12105594406489352, 0.0999028394019588, 0.12070643847692175, 0.14221821660084316, 0.10729361021779499, 0.1371785152805574, 0.07281034805983222, 0.07585583972089278, 0.09426853772265825, 0.12301683227417466, 0.13263262398729245, 0.03043505792258911, 0.11707383669901796, 0.05331551126676432, 0.18428286256492699, 0.14201583070258073, 0.16737176445081978, 0.09523231031763069, 0.15687441992227139, 0.0844526458188067, 0.15446648319260795, 0.16498760454262937, 0.040675342333873314, 0.06737601383583455, 0.09850410834137102, 0.05845168776646876, 0.05765272463012229, 0.1831262001428246, 0.12169973642771688, 0.12748821137745064, 0.23552067137302501, 0.13590000560798374, 0.10663343322792813, 0.15292180657310364, 0.1460388019467481, 0.12426234344551108, 0.14063398627340865, 0.038914577691035, 0.11235342634465623, 0.07577138624412466, 0.11763442354691832, 0.07512248000057783, 0.07536952477406661, 0.02943172297660711, 0.032604788451503705, 0.08264743901371761, 0.06971543022282725, 0.17005869640135796, 0.09084617162870008, 0.07974189404417822, 0.06558669625924458, 0.1510414938526101, 0.053394029295610594, 0.04581101570931704, 0.07554803519254284, 0.10895252887547653, 0.09827722243530322, 0.11151266007662662, 0.1492684690255799, 0.05400035344081732, 0.07835548527442332, 0.16238198311641458, 0.05066488263463758, 0.11339829214496107, 0.22287886604217755, 0.05849262629407516, 0.05366661463727974, 0.06215741417675592, 0.04512592460637128, 0.05207361355606741, 0.12048193671585519, 0.0722891480735236, 0.051459913479857886, 0.05000741880089689, 0.12477013149954544, 0.05392577959777837, 0.04977284204317379, 0.0682369396622286, 0.07622996120643899, 0.08676220486398956, 0.09991886372298378, 0.13303041295367524, 0.15162552313240943, 0.04756604503870403, 0.05751677513711055, 0.04493740231800448, 0.047765074138047835, 0.07096542823812878, 0.10878260477679205, 0.06341053932419091, 0.1761019454210388, 0.11300336481322155, 0.04604100646938342, 0.0921707327818576, 0.16513245196544923, 0.11920874803520697, 0.04876178417974679, 0.08349585254349005, 0.06588190367869247, 0.08554907297832641, 0.087931059396052, 0.1607461972020194, 0.1986110379614755, 0.07509265529693399, 0.0766169948362733, 0.16714097154033572, 0.04558126772985716, 0.16093001288708197, 0.1010370828905227, 0.14388963726436677, 0.05540921996151249, 0.14209705179951154, 0.08286255768561836, 0.10082057969254722, 0.09471524920386397, 0.08170069811200209, 0.03244081829373802, 0.11077322127772686, 0.14385374266177703, 0.05219910626485053, 0.10551531083991716, 0.037949170241529065, 0.17029305556429364, 0.05405785302320955, 0.038386020384992225, 0.1521556988376869, 0.03710533783204304, 0.11057425007422972, 0.03596086870142311, 0.11731765543162431, 0.046839601938677734, 0.1796574651421363, 0.1542943187432737, 0.1306639960070327, 0.07939050388838505, 0.07257540294599985, 0.056878501476338966, 0.10243542941167469, 0.1514288905566252, 0.12288347241641373, 0.04930017397335576, 0.07859336368746415, 0.04158093766908687, 0.07484572743099228, 0.11034153463736705, 0.1607019850724174, 0.09612330285702742, 0.028089676789490894, 0.12931518991325597, 0.10451713879027828, 0.05176604901550961, 0.07046803427387258, 0.15172629825025377, 0.16908355419192173, 0.07361094684806911, 0.08681780353270856, 0.1129403897630193, 0.07125323598021525, 0.05326549990376582, 0.06645640129345705, 0.03929644435147064, 0.06232768046032784, 0.1252864649446709, 0.06524348715390019, 0.10243977049086816, 0.12239277200018198, 0.09608146123433665, 0.12525317838683084, 0.058529630726028195, 0.09706308569535081, 0.17201724326018916, 0.09444156942545687, 0.10900930566400066, 0.09148738670966579, 0.06642096913394246, 0.1350279184558251, 0.1757982362730382, 0.09807129808627217, 0.16277798609628116, 0.12840473399771066, 0.09328001458352901, 0.0488413819784162, 0.12217580036724321, 0.19970379824052495, 0.1605347105937978, 0.05282712597963759, 0.0363341175174055, 0.07337156365622147, 0.13548377478657164, 0.0590857224006482, 0.045574861453403843, 0.20012674161505303, 0.12261117225549759, 0.07953190333500083, 0.13101125602290153, 0.06672850087651427, 0.06248319944040743, 0.0906030285022639, 0.13942621708135589, 0.1575858257009325, 0.081973579968816, 0.07847986240042076, 0.06893310208551595, 0.08211665523898412, 0.0427449546565176, 0.14205109030162316, 0.17327014138663568, 0.11506125908505617, 0.06399893308955666, 0.07931617599684906, 0.1415202427121816, 0.16239505894553255, 0.07574978916564364, 0.16536648768162782, 0.10420323574230046, 0.15663491976631042, 0.10451924554114973, 0.12150360802838782, 0.13310167870622422, 0.14787251539205665, 0.05327358303882896, 0.0445500970830513, 0.04556346438271516, 0.046009470996266844, 0.08653749144539873, 0.11996593311834952, 0.10439327828904621, 0.04987652144594474, 0.12200134634809935, 0.08863493408176093, 0.12329854412675043, 0.03803656548718356, 0.0642903446390922, 0.09529646302145026, 0.043233680164897995, 0.0743986970842813, 0.08425869703435905, 0.04521163358001193, 0.05419679883027027, 0.05749805689164247, 0.10269997531702467, 0.05486222035092676, 0.12201053436763612, 0.0634549557821595, 0.03298626107219162, 0.07241681530747185, 0.06760248323838473, 0.11985752449146685, 0.07132769229820889, 0.09127151025399238, 0.1273249896240506, 0.08362604500913849, 0.038504409745935525, 0.10684207580658645, 0.03659058981002804, 0.08109675039384749, 0.07299611868020658, 0.10526508995181079, 0.07028403783378061, 0.04461686726894886, 0.18284249342304304, 0.031261318877599156, 0.07735738251307123, 0.07565186374005953, 0.1390403743409283, 0.09733074018897178, 0.16589835943134157, 0.20300498542057063, 0.10682064939648933, 0.06368396012549173, 0.022570468472410187, 0.06531544631221556, 0.05558498620518426, 0.13065867960259467, 0.07684645213894614, 0.11035619530933774, 0.05539746070087979, 0.12605413318741862, 0.028060915830374407, 0.13529101470680338, 0.05545485868846928, 0.1069879939822707, 0.06500360603812878, 0.03815102803433209, 0.08594022828766971, 0.12710798157004322, 0.06408007293910445, 0.12194959688611239, 0.12550855487168858, 0.05030381445957656, 0.036959491877219613, 0.08274997581003607, 0.07618625479681712, 0.04732681986282439, 0.0472290191236995, 0.07098383542021412, 0.14543330027400028, 0.05327563288983851, 0.06570633377910057, 0.05792850910570786, 0.08575506721253176, 0.06962303316268792, 0.07385728480960022, 0.03443208102961212, 0.06697394737318074, 0.10301024618792871, 0.16178224544250877, 0.10372999351819731, 0.05837562276704597, 0.053426059150991904, 0.043806538179251404, 0.1343998855894545, 0.09159193302512697, 0.07032986293481562, 0.0733277787331907, 0.07007670010729053, 0.060813821754866326, 0.1306364788340411, 0.08101643363058805, 0.1978893963716571, 0.14423856221532566, 0.047822177906763214, 0.07462479792869822, 0.06052068531070178, 0.19329112662405437, 0.15742305914373406, 0.05582541039883994, 0.05789547681888203, 0.07561362140345096, 0.15455812546495526, 0.14574223308875175, 0.09194021019677523, 0.08466494872319691, 0.06407086960878682, 0.1206949811314513, 0.15547375774725436, 0.10576726303730055, 0.11383770412302363, 0.04577203654737967, 0.13857878049786562, 0.08151842014527767, 0.05647606228880598, 0.07446058386205129, 0.05736522432333278, 0.04908889563766578, 0.16122137858936034, 0.058444629254187516, 0.08347577181038578, 0.03929218165977278, 0.048649854436921484, 0.07863503939471957, 0.05918449223376471, 0.1140922277775303, 0.11430738685025893, 0.1471657533149876, 0.0952629692121075, 0.056368965416540864, 0.0581785219616722, 0.11602221134063047, 0.08635286981807623, 0.1162675026417418, 0.03894478829810901, 0.08135493419441855, 0.11457381696337837, 0.06378691319596315, 0.0555633035258744, 0.1252674770337885, 0.06398985759485852, 0.08710674413639316]
0.10123792589645873
Making ranges
torch.Size([21937, 2])
We keep 4.20e+06/2.09e+08 =  2% of the original kernel matrix.

torch.Size([2376, 2])
We keep 1.61e+05/1.88e+06 =  8% of the original kernel matrix.

torch.Size([6907, 2])
We keep 8.29e+05/1.98e+07 =  4% of the original kernel matrix.

torch.Size([19019, 2])
We keep 1.18e+07/3.00e+08 =  3% of the original kernel matrix.

torch.Size([18449, 2])
We keep 5.84e+06/2.50e+08 =  2% of the original kernel matrix.

torch.Size([2987, 2])
We keep 3.71e+05/3.71e+06 =  9% of the original kernel matrix.

torch.Size([7877, 2])
We keep 1.12e+06/2.79e+07 =  4% of the original kernel matrix.

torch.Size([3298, 2])
We keep 3.43e+05/3.80e+06 =  9% of the original kernel matrix.

torch.Size([7783, 2])
We keep 1.07e+06/2.82e+07 =  3% of the original kernel matrix.

torch.Size([41332, 2])
We keep 5.63e+07/1.27e+09 =  4% of the original kernel matrix.

torch.Size([26340, 2])
We keep 1.03e+07/5.16e+08 =  2% of the original kernel matrix.

torch.Size([34996, 2])
We keep 2.81e+07/9.20e+08 =  3% of the original kernel matrix.

torch.Size([24375, 2])
We keep 9.03e+06/4.39e+08 =  2% of the original kernel matrix.

torch.Size([43831, 2])
We keep 3.86e+07/1.42e+09 =  2% of the original kernel matrix.

torch.Size([26927, 2])
We keep 1.10e+07/5.46e+08 =  2% of the original kernel matrix.

torch.Size([52515, 2])
We keep 4.11e+07/1.84e+09 =  2% of the original kernel matrix.

torch.Size([29440, 2])
We keep 1.22e+07/6.21e+08 =  1% of the original kernel matrix.

torch.Size([8063, 2])
We keep 3.78e+06/3.46e+07 = 10% of the original kernel matrix.

torch.Size([11874, 2])
We keep 2.46e+06/8.51e+07 =  2% of the original kernel matrix.

torch.Size([32723, 2])
We keep 8.34e+07/1.07e+09 =  7% of the original kernel matrix.

torch.Size([23141, 2])
We keep 9.46e+06/4.73e+08 =  2% of the original kernel matrix.

torch.Size([9273, 2])
We keep 2.32e+06/4.11e+07 =  5% of the original kernel matrix.

torch.Size([12589, 2])
We keep 2.57e+06/9.27e+07 =  2% of the original kernel matrix.

torch.Size([113279, 2])
We keep 2.36e+08/1.27e+10 =  1% of the original kernel matrix.

torch.Size([43784, 2])
We keep 2.82e+07/1.63e+09 =  1% of the original kernel matrix.

torch.Size([9384, 2])
We keep 2.11e+06/4.49e+07 =  4% of the original kernel matrix.

torch.Size([12828, 2])
We keep 2.73e+06/9.69e+07 =  2% of the original kernel matrix.

torch.Size([118899, 2])
We keep 2.38e+08/1.08e+10 =  2% of the original kernel matrix.

torch.Size([45831, 2])
We keep 2.57e+07/1.50e+09 =  1% of the original kernel matrix.

torch.Size([18800, 2])
We keep 7.29e+06/2.53e+08 =  2% of the original kernel matrix.

torch.Size([17900, 2])
We keep 5.34e+06/2.30e+08 =  2% of the original kernel matrix.

torch.Size([61749, 2])
We keep 7.00e+07/2.55e+09 =  2% of the original kernel matrix.

torch.Size([31922, 2])
We keep 1.40e+07/7.30e+08 =  1% of the original kernel matrix.

torch.Size([33178, 2])
We keep 2.05e+07/8.13e+08 =  2% of the original kernel matrix.

torch.Size([24051, 2])
We keep 8.59e+06/4.12e+08 =  2% of the original kernel matrix.

torch.Size([26453, 2])
We keep 1.61e+07/5.63e+08 =  2% of the original kernel matrix.

torch.Size([21283, 2])
We keep 7.37e+06/3.43e+08 =  2% of the original kernel matrix.

torch.Size([377032, 2])
We keep 2.85e+09/1.08e+11 =  2% of the original kernel matrix.

torch.Size([83199, 2])
We keep 7.48e+07/4.75e+09 =  1% of the original kernel matrix.

torch.Size([77073, 2])
We keep 1.11e+08/4.94e+09 =  2% of the original kernel matrix.

torch.Size([36341, 2])
We keep 1.87e+07/1.02e+09 =  1% of the original kernel matrix.

torch.Size([17936, 2])
We keep 1.53e+07/2.72e+08 =  5% of the original kernel matrix.

torch.Size([17562, 2])
We keep 5.50e+06/2.39e+08 =  2% of the original kernel matrix.

torch.Size([408777, 2])
We keep 1.51e+09/1.18e+11 =  1% of the original kernel matrix.

torch.Size([89581, 2])
We keep 7.67e+07/4.97e+09 =  1% of the original kernel matrix.

torch.Size([19971, 2])
We keep 1.02e+07/3.18e+08 =  3% of the original kernel matrix.

torch.Size([18698, 2])
We keep 5.92e+06/2.58e+08 =  2% of the original kernel matrix.

torch.Size([15862, 2])
We keep 1.52e+07/1.68e+08 =  9% of the original kernel matrix.

torch.Size([16893, 2])
We keep 4.42e+06/1.87e+08 =  2% of the original kernel matrix.

torch.Size([15277, 2])
We keep 1.53e+07/2.26e+08 =  6% of the original kernel matrix.

torch.Size([15536, 2])
We keep 5.07e+06/2.17e+08 =  2% of the original kernel matrix.

torch.Size([55950, 2])
We keep 4.29e+07/2.18e+09 =  1% of the original kernel matrix.

torch.Size([30627, 2])
We keep 1.30e+07/6.75e+08 =  1% of the original kernel matrix.

torch.Size([18866, 2])
We keep 5.52e+07/5.52e+08 =  9% of the original kernel matrix.

torch.Size([17786, 2])
We keep 7.31e+06/3.40e+08 =  2% of the original kernel matrix.

torch.Size([9214, 2])
We keep 1.73e+06/3.89e+07 =  4% of the original kernel matrix.

torch.Size([12453, 2])
We keep 2.51e+06/9.02e+07 =  2% of the original kernel matrix.

torch.Size([24510, 2])
We keep 1.94e+07/4.71e+08 =  4% of the original kernel matrix.

torch.Size([20242, 2])
We keep 6.79e+06/3.14e+08 =  2% of the original kernel matrix.

torch.Size([1871459, 2])
We keep 1.79e+10/1.91e+12 =  0% of the original kernel matrix.

torch.Size([194213, 2])
We keep 2.80e+08/2.00e+10 =  1% of the original kernel matrix.

torch.Size([7984, 2])
We keep 5.33e+06/3.77e+07 = 14% of the original kernel matrix.

torch.Size([11664, 2])
We keep 2.46e+06/8.88e+07 =  2% of the original kernel matrix.

torch.Size([243731, 2])
We keep 1.75e+09/5.45e+10 =  3% of the original kernel matrix.

torch.Size([66705, 2])
We keep 5.44e+07/3.38e+09 =  1% of the original kernel matrix.

torch.Size([12139, 2])
We keep 5.85e+06/1.01e+08 =  5% of the original kernel matrix.

torch.Size([14637, 2])
We keep 3.80e+06/1.45e+08 =  2% of the original kernel matrix.

torch.Size([9187, 2])
We keep 2.02e+06/4.42e+07 =  4% of the original kernel matrix.

torch.Size([12585, 2])
We keep 2.69e+06/9.61e+07 =  2% of the original kernel matrix.

torch.Size([10855, 2])
We keep 5.27e+06/8.07e+07 =  6% of the original kernel matrix.

torch.Size([13640, 2])
We keep 3.38e+06/1.30e+08 =  2% of the original kernel matrix.

torch.Size([61092, 2])
We keep 2.16e+08/3.56e+09 =  6% of the original kernel matrix.

torch.Size([32066, 2])
We keep 1.61e+07/8.62e+08 =  1% of the original kernel matrix.

torch.Size([55357, 2])
We keep 5.92e+07/2.21e+09 =  2% of the original kernel matrix.

torch.Size([30566, 2])
We keep 1.33e+07/6.80e+08 =  1% of the original kernel matrix.

torch.Size([2184, 2])
We keep 1.31e+05/1.27e+06 = 10% of the original kernel matrix.

torch.Size([6734, 2])
We keep 7.41e+05/1.63e+07 =  4% of the original kernel matrix.

torch.Size([5299, 2])
We keep 6.47e+05/1.03e+07 =  6% of the original kernel matrix.

torch.Size([9617, 2])
We keep 1.56e+06/4.64e+07 =  3% of the original kernel matrix.

torch.Size([3919, 2])
We keep 6.63e+05/6.26e+06 = 10% of the original kernel matrix.

torch.Size([8265, 2])
We keep 1.29e+06/3.62e+07 =  3% of the original kernel matrix.

torch.Size([6885, 2])
We keep 1.49e+06/2.64e+07 =  5% of the original kernel matrix.

torch.Size([11011, 2])
We keep 2.19e+06/7.43e+07 =  2% of the original kernel matrix.

torch.Size([1705, 2])
We keep 3.10e+05/1.52e+06 = 20% of the original kernel matrix.

torch.Size([5743, 2])
We keep 7.68e+05/1.78e+07 =  4% of the original kernel matrix.

torch.Size([1917, 2])
We keep 1.15e+05/1.14e+06 = 10% of the original kernel matrix.

torch.Size([6382, 2])
We keep 7.00e+05/1.54e+07 =  4% of the original kernel matrix.

torch.Size([4661, 2])
We keep 5.78e+05/7.95e+06 =  7% of the original kernel matrix.

torch.Size([9010, 2])
We keep 1.43e+06/4.08e+07 =  3% of the original kernel matrix.

torch.Size([1910, 2])
We keep 1.14e+05/1.10e+06 = 10% of the original kernel matrix.

torch.Size([6411, 2])
We keep 6.85e+05/1.51e+07 =  4% of the original kernel matrix.

torch.Size([2270, 2])
We keep 1.40e+05/1.56e+06 =  8% of the original kernel matrix.

torch.Size([6762, 2])
We keep 7.85e+05/1.80e+07 =  4% of the original kernel matrix.

torch.Size([8453, 2])
We keep 5.21e+06/4.37e+07 = 11% of the original kernel matrix.

torch.Size([12046, 2])
We keep 2.63e+06/9.56e+07 =  2% of the original kernel matrix.

torch.Size([6483, 2])
We keep 1.07e+06/1.92e+07 =  5% of the original kernel matrix.

torch.Size([10380, 2])
We keep 1.95e+06/6.34e+07 =  3% of the original kernel matrix.

torch.Size([2923, 2])
We keep 2.33e+05/2.85e+06 =  8% of the original kernel matrix.

torch.Size([7402, 2])
We keep 9.62e+05/2.44e+07 =  3% of the original kernel matrix.

torch.Size([9741, 2])
We keep 4.31e+06/5.79e+07 =  7% of the original kernel matrix.

torch.Size([12796, 2])
We keep 2.94e+06/1.10e+08 =  2% of the original kernel matrix.

torch.Size([12810, 2])
We keep 6.47e+06/1.06e+08 =  6% of the original kernel matrix.

torch.Size([15013, 2])
We keep 3.84e+06/1.49e+08 =  2% of the original kernel matrix.

torch.Size([2898, 2])
We keep 2.39e+05/2.74e+06 =  8% of the original kernel matrix.

torch.Size([7506, 2])
We keep 9.81e+05/2.39e+07 =  4% of the original kernel matrix.

torch.Size([6204, 2])
We keep 1.97e+06/2.15e+07 =  9% of the original kernel matrix.

torch.Size([10306, 2])
We keep 2.04e+06/6.70e+07 =  3% of the original kernel matrix.

torch.Size([3196, 2])
We keep 4.81e+05/4.22e+06 = 11% of the original kernel matrix.

torch.Size([7713, 2])
We keep 1.13e+06/2.97e+07 =  3% of the original kernel matrix.

torch.Size([7888, 2])
We keep 1.43e+06/2.89e+07 =  4% of the original kernel matrix.

torch.Size([11609, 2])
We keep 2.28e+06/7.77e+07 =  2% of the original kernel matrix.

torch.Size([5978, 2])
We keep 2.06e+06/2.07e+07 =  9% of the original kernel matrix.

torch.Size([10063, 2])
We keep 2.09e+06/6.58e+07 =  3% of the original kernel matrix.

torch.Size([4834, 2])
We keep 1.02e+06/1.07e+07 =  9% of the original kernel matrix.

torch.Size([9233, 2])
We keep 1.57e+06/4.72e+07 =  3% of the original kernel matrix.

torch.Size([4124, 2])
We keep 3.92e+05/5.20e+06 =  7% of the original kernel matrix.

torch.Size([8670, 2])
We keep 1.20e+06/3.30e+07 =  3% of the original kernel matrix.

torch.Size([4127, 2])
We keep 4.87e+05/6.40e+06 =  7% of the original kernel matrix.

torch.Size([8536, 2])
We keep 1.29e+06/3.66e+07 =  3% of the original kernel matrix.

torch.Size([3298, 2])
We keep 3.16e+05/3.46e+06 =  9% of the original kernel matrix.

torch.Size([7871, 2])
We keep 1.04e+06/2.69e+07 =  3% of the original kernel matrix.

torch.Size([7600, 2])
We keep 1.86e+06/2.68e+07 =  6% of the original kernel matrix.

torch.Size([11517, 2])
We keep 2.20e+06/7.49e+07 =  2% of the original kernel matrix.

torch.Size([2744, 2])
We keep 2.50e+05/2.93e+06 =  8% of the original kernel matrix.

torch.Size([7159, 2])
We keep 9.77e+05/2.48e+07 =  3% of the original kernel matrix.

torch.Size([8453, 2])
We keep 2.74e+06/4.03e+07 =  6% of the original kernel matrix.

torch.Size([12475, 2])
We keep 2.60e+06/9.18e+07 =  2% of the original kernel matrix.

torch.Size([9287, 2])
We keep 2.12e+06/4.70e+07 =  4% of the original kernel matrix.

torch.Size([12579, 2])
We keep 2.77e+06/9.91e+07 =  2% of the original kernel matrix.

torch.Size([4521, 2])
We keep 4.89e+05/6.62e+06 =  7% of the original kernel matrix.

torch.Size([8967, 2])
We keep 1.31e+06/3.72e+07 =  3% of the original kernel matrix.

torch.Size([3672, 2])
We keep 4.65e+05/4.46e+06 = 10% of the original kernel matrix.

torch.Size([8250, 2])
We keep 1.14e+06/3.05e+07 =  3% of the original kernel matrix.

torch.Size([5446, 2])
We keep 6.08e+05/9.55e+06 =  6% of the original kernel matrix.

torch.Size([9796, 2])
We keep 1.49e+06/4.47e+07 =  3% of the original kernel matrix.

torch.Size([10173, 2])
We keep 2.49e+06/5.96e+07 =  4% of the original kernel matrix.

torch.Size([13111, 2])
We keep 2.99e+06/1.12e+08 =  2% of the original kernel matrix.

torch.Size([7090, 2])
We keep 1.01e+06/1.98e+07 =  5% of the original kernel matrix.

torch.Size([10978, 2])
We keep 1.95e+06/6.44e+07 =  3% of the original kernel matrix.

torch.Size([2758, 2])
We keep 2.02e+05/2.16e+06 =  9% of the original kernel matrix.

torch.Size([7299, 2])
We keep 8.82e+05/2.12e+07 =  4% of the original kernel matrix.

torch.Size([7607, 2])
We keep 1.74e+06/3.00e+07 =  5% of the original kernel matrix.

torch.Size([11180, 2])
We keep 2.31e+06/7.93e+07 =  2% of the original kernel matrix.

torch.Size([4553, 2])
We keep 4.47e+05/6.47e+06 =  6% of the original kernel matrix.

torch.Size([8959, 2])
We keep 1.30e+06/3.68e+07 =  3% of the original kernel matrix.

torch.Size([5080, 2])
We keep 6.55e+05/9.84e+06 =  6% of the original kernel matrix.

torch.Size([9375, 2])
We keep 1.54e+06/4.54e+07 =  3% of the original kernel matrix.

torch.Size([3051, 2])
We keep 3.44e+05/3.25e+06 = 10% of the original kernel matrix.

torch.Size([7550, 2])
We keep 1.03e+06/2.61e+07 =  3% of the original kernel matrix.

torch.Size([4183, 2])
We keep 5.68e+05/6.62e+06 =  8% of the original kernel matrix.

torch.Size([8563, 2])
We keep 1.31e+06/3.72e+07 =  3% of the original kernel matrix.

torch.Size([5758, 2])
We keep 9.41e+05/1.33e+07 =  7% of the original kernel matrix.

torch.Size([9882, 2])
We keep 1.70e+06/5.27e+07 =  3% of the original kernel matrix.

torch.Size([3254, 2])
We keep 3.17e+05/3.13e+06 = 10% of the original kernel matrix.

torch.Size([7852, 2])
We keep 9.80e+05/2.56e+07 =  3% of the original kernel matrix.

torch.Size([1353, 2])
We keep 6.13e+04/4.36e+05 = 14% of the original kernel matrix.

torch.Size([5524, 2])
We keep 5.04e+05/9.54e+06 =  5% of the original kernel matrix.

torch.Size([5329, 2])
We keep 7.09e+05/1.14e+07 =  6% of the original kernel matrix.

torch.Size([9573, 2])
We keep 1.60e+06/4.88e+07 =  3% of the original kernel matrix.

torch.Size([13191, 2])
We keep 4.03e+06/1.16e+08 =  3% of the original kernel matrix.

torch.Size([15564, 2])
We keep 3.97e+06/1.56e+08 =  2% of the original kernel matrix.

torch.Size([5550, 2])
We keep 1.08e+06/1.29e+07 =  8% of the original kernel matrix.

torch.Size([9632, 2])
We keep 1.70e+06/5.18e+07 =  3% of the original kernel matrix.

torch.Size([2545, 2])
We keep 2.87e+05/2.76e+06 = 10% of the original kernel matrix.

torch.Size([6810, 2])
We keep 9.66e+05/2.40e+07 =  4% of the original kernel matrix.

torch.Size([5762, 2])
We keep 1.37e+06/1.66e+07 =  8% of the original kernel matrix.

torch.Size([10021, 2])
We keep 1.87e+06/5.89e+07 =  3% of the original kernel matrix.

torch.Size([1887, 2])
We keep 1.35e+05/1.22e+06 = 11% of the original kernel matrix.

torch.Size([6341, 2])
We keep 7.30e+05/1.60e+07 =  4% of the original kernel matrix.

torch.Size([4191, 2])
We keep 5.94e+05/6.41e+06 =  9% of the original kernel matrix.

torch.Size([8517, 2])
We keep 1.31e+06/3.66e+07 =  3% of the original kernel matrix.

torch.Size([2267, 2])
We keep 1.21e+05/1.25e+06 =  9% of the original kernel matrix.

torch.Size([6901, 2])
We keep 7.27e+05/1.62e+07 =  4% of the original kernel matrix.

torch.Size([3753, 2])
We keep 4.84e+05/4.44e+06 = 10% of the original kernel matrix.

torch.Size([8376, 2])
We keep 1.14e+06/3.05e+07 =  3% of the original kernel matrix.

torch.Size([2344, 2])
We keep 2.40e+05/1.79e+06 = 13% of the original kernel matrix.

torch.Size([6799, 2])
We keep 8.21e+05/1.93e+07 =  4% of the original kernel matrix.

torch.Size([4226, 2])
We keep 4.78e+05/6.31e+06 =  7% of the original kernel matrix.

torch.Size([8810, 2])
We keep 1.28e+06/3.63e+07 =  3% of the original kernel matrix.

torch.Size([3932, 2])
We keep 5.07e+05/6.13e+06 =  8% of the original kernel matrix.

torch.Size([8419, 2])
We keep 1.28e+06/3.58e+07 =  3% of the original kernel matrix.

torch.Size([6091, 2])
We keep 8.65e+05/1.55e+07 =  5% of the original kernel matrix.

torch.Size([10119, 2])
We keep 1.79e+06/5.70e+07 =  3% of the original kernel matrix.

torch.Size([2789, 2])
We keep 1.94e+05/2.43e+06 =  7% of the original kernel matrix.

torch.Size([7405, 2])
We keep 9.12e+05/2.25e+07 =  4% of the original kernel matrix.

torch.Size([2345, 2])
We keep 1.94e+05/1.79e+06 = 10% of the original kernel matrix.

torch.Size([6878, 2])
We keep 8.37e+05/1.94e+07 =  4% of the original kernel matrix.

torch.Size([5270, 2])
We keep 8.09e+05/1.18e+07 =  6% of the original kernel matrix.

torch.Size([9536, 2])
We keep 1.62e+06/4.97e+07 =  3% of the original kernel matrix.

torch.Size([4322, 2])
We keep 6.42e+05/6.08e+06 = 10% of the original kernel matrix.

torch.Size([8691, 2])
We keep 1.28e+06/3.56e+07 =  3% of the original kernel matrix.

torch.Size([6133, 2])
We keep 8.56e+05/1.40e+07 =  6% of the original kernel matrix.

torch.Size([10307, 2])
We keep 1.73e+06/5.41e+07 =  3% of the original kernel matrix.

torch.Size([3851, 2])
We keep 4.17e+05/5.40e+06 =  7% of the original kernel matrix.

torch.Size([8248, 2])
We keep 1.23e+06/3.36e+07 =  3% of the original kernel matrix.

torch.Size([10907, 2])
We keep 7.03e+06/9.10e+07 =  7% of the original kernel matrix.

torch.Size([13495, 2])
We keep 3.56e+06/1.38e+08 =  2% of the original kernel matrix.

torch.Size([5985, 2])
We keep 9.96e+05/1.50e+07 =  6% of the original kernel matrix.

torch.Size([10092, 2])
We keep 1.79e+06/5.61e+07 =  3% of the original kernel matrix.

torch.Size([5228, 2])
We keep 6.13e+05/9.50e+06 =  6% of the original kernel matrix.

torch.Size([9491, 2])
We keep 1.50e+06/4.46e+07 =  3% of the original kernel matrix.

torch.Size([10445, 2])
We keep 3.04e+06/6.05e+07 =  5% of the original kernel matrix.

torch.Size([13406, 2])
We keep 2.99e+06/1.12e+08 =  2% of the original kernel matrix.

torch.Size([5785, 2])
We keep 9.68e+05/1.23e+07 =  7% of the original kernel matrix.

torch.Size([9901, 2])
We keep 1.63e+06/5.06e+07 =  3% of the original kernel matrix.

torch.Size([6789, 2])
We keep 1.08e+06/1.91e+07 =  5% of the original kernel matrix.

torch.Size([10687, 2])
We keep 1.93e+06/6.32e+07 =  3% of the original kernel matrix.

torch.Size([2579, 2])
We keep 2.52e+05/2.24e+06 = 11% of the original kernel matrix.

torch.Size([6926, 2])
We keep 8.91e+05/2.16e+07 =  4% of the original kernel matrix.

torch.Size([5123, 2])
We keep 6.95e+05/9.96e+06 =  6% of the original kernel matrix.

torch.Size([9348, 2])
We keep 1.54e+06/4.56e+07 =  3% of the original kernel matrix.

torch.Size([2400, 2])
We keep 2.42e+05/1.90e+06 = 12% of the original kernel matrix.

torch.Size([6851, 2])
We keep 8.39e+05/1.99e+07 =  4% of the original kernel matrix.

torch.Size([6927, 2])
We keep 1.77e+06/2.78e+07 =  6% of the original kernel matrix.

torch.Size([10841, 2])
We keep 2.31e+06/7.63e+07 =  3% of the original kernel matrix.

torch.Size([1749, 2])
We keep 8.38e+04/8.21e+05 = 10% of the original kernel matrix.

torch.Size([6258, 2])
We keep 6.30e+05/1.31e+07 =  4% of the original kernel matrix.

torch.Size([3931, 2])
We keep 4.33e+05/5.27e+06 =  8% of the original kernel matrix.

torch.Size([8494, 2])
We keep 1.23e+06/3.32e+07 =  3% of the original kernel matrix.

torch.Size([1954, 2])
We keep 1.48e+05/1.32e+06 = 11% of the original kernel matrix.

torch.Size([6440, 2])
We keep 7.48e+05/1.66e+07 =  4% of the original kernel matrix.

torch.Size([2320, 2])
We keep 2.05e+05/1.95e+06 = 10% of the original kernel matrix.

torch.Size([6714, 2])
We keep 8.73e+05/2.02e+07 =  4% of the original kernel matrix.

torch.Size([2199, 2])
We keep 1.51e+05/1.44e+06 = 10% of the original kernel matrix.

torch.Size([6735, 2])
We keep 7.69e+05/1.74e+07 =  4% of the original kernel matrix.

torch.Size([2261, 2])
We keep 1.76e+05/1.60e+06 = 11% of the original kernel matrix.

torch.Size([6519, 2])
We keep 7.89e+05/1.83e+07 =  4% of the original kernel matrix.

torch.Size([5371, 2])
We keep 8.29e+05/1.01e+07 =  8% of the original kernel matrix.

torch.Size([9748, 2])
We keep 1.51e+06/4.59e+07 =  3% of the original kernel matrix.

torch.Size([5148, 2])
We keep 8.37e+05/1.04e+07 =  8% of the original kernel matrix.

torch.Size([9475, 2])
We keep 1.56e+06/4.65e+07 =  3% of the original kernel matrix.

torch.Size([2983, 2])
We keep 4.19e+05/4.01e+06 = 10% of the original kernel matrix.

torch.Size([7310, 2])
We keep 1.12e+06/2.89e+07 =  3% of the original kernel matrix.

torch.Size([3913, 2])
We keep 4.34e+05/5.45e+06 =  7% of the original kernel matrix.

torch.Size([8467, 2])
We keep 1.24e+06/3.38e+07 =  3% of the original kernel matrix.

torch.Size([2489, 2])
We keep 1.68e+05/1.97e+06 =  8% of the original kernel matrix.

torch.Size([6995, 2])
We keep 8.47e+05/2.03e+07 =  4% of the original kernel matrix.

torch.Size([7446, 2])
We keep 1.77e+06/3.12e+07 =  5% of the original kernel matrix.

torch.Size([11328, 2])
We keep 2.42e+06/8.08e+07 =  3% of the original kernel matrix.

torch.Size([2579, 2])
We keep 2.81e+05/2.04e+06 = 13% of the original kernel matrix.

torch.Size([7008, 2])
We keep 8.67e+05/2.06e+07 =  4% of the original kernel matrix.

torch.Size([13472, 2])
We keep 3.93e+06/1.16e+08 =  3% of the original kernel matrix.

torch.Size([15466, 2])
We keep 3.94e+06/1.56e+08 =  2% of the original kernel matrix.

torch.Size([4460, 2])
We keep 6.06e+05/7.89e+06 =  7% of the original kernel matrix.

torch.Size([8827, 2])
We keep 1.41e+06/4.06e+07 =  3% of the original kernel matrix.

torch.Size([2773, 2])
We keep 2.52e+05/2.16e+06 = 11% of the original kernel matrix.

torch.Size([7380, 2])
We keep 8.82e+05/2.12e+07 =  4% of the original kernel matrix.

torch.Size([3805, 2])
We keep 3.21e+05/4.56e+06 =  7% of the original kernel matrix.

torch.Size([8418, 2])
We keep 1.14e+06/3.09e+07 =  3% of the original kernel matrix.

torch.Size([3364, 2])
We keep 2.71e+05/3.71e+06 =  7% of the original kernel matrix.

torch.Size([7925, 2])
We keep 1.06e+06/2.79e+07 =  3% of the original kernel matrix.

torch.Size([3894, 2])
We keep 4.37e+05/5.00e+06 =  8% of the original kernel matrix.

torch.Size([8509, 2])
We keep 1.18e+06/3.23e+07 =  3% of the original kernel matrix.

torch.Size([2982, 2])
We keep 2.16e+05/2.68e+06 =  8% of the original kernel matrix.

torch.Size([7535, 2])
We keep 9.62e+05/2.37e+07 =  4% of the original kernel matrix.

torch.Size([3458, 2])
We keep 2.73e+05/3.42e+06 =  7% of the original kernel matrix.

torch.Size([7994, 2])
We keep 1.03e+06/2.68e+07 =  3% of the original kernel matrix.

torch.Size([12428, 2])
We keep 5.45e+06/1.25e+08 =  4% of the original kernel matrix.

torch.Size([15064, 2])
We keep 4.10e+06/1.62e+08 =  2% of the original kernel matrix.

torch.Size([4050, 2])
We keep 3.80e+05/5.30e+06 =  7% of the original kernel matrix.

torch.Size([8479, 2])
We keep 1.21e+06/3.33e+07 =  3% of the original kernel matrix.

torch.Size([7360, 2])
We keep 1.45e+06/2.41e+07 =  6% of the original kernel matrix.

torch.Size([11187, 2])
We keep 2.11e+06/7.10e+07 =  2% of the original kernel matrix.

torch.Size([1904, 2])
We keep 1.24e+05/9.18e+05 = 13% of the original kernel matrix.

torch.Size([6523, 2])
We keep 6.37e+05/1.39e+07 =  4% of the original kernel matrix.

torch.Size([5125, 2])
We keep 7.34e+05/9.55e+06 =  7% of the original kernel matrix.

torch.Size([9331, 2])
We keep 1.53e+06/4.47e+07 =  3% of the original kernel matrix.

torch.Size([2147, 2])
We keep 2.04e+05/1.64e+06 = 12% of the original kernel matrix.

torch.Size([6535, 2])
We keep 7.95e+05/1.85e+07 =  4% of the original kernel matrix.

torch.Size([3951, 2])
We keep 4.55e+05/6.06e+06 =  7% of the original kernel matrix.

torch.Size([8412, 2])
We keep 1.27e+06/3.56e+07 =  3% of the original kernel matrix.

torch.Size([2782, 2])
We keep 2.45e+05/2.68e+06 =  9% of the original kernel matrix.

torch.Size([7212, 2])
We keep 9.51e+05/2.37e+07 =  4% of the original kernel matrix.

torch.Size([2260, 2])
We keep 1.84e+05/1.71e+06 = 10% of the original kernel matrix.

torch.Size([6673, 2])
We keep 8.14e+05/1.89e+07 =  4% of the original kernel matrix.

torch.Size([2695, 2])
We keep 2.79e+05/2.33e+06 = 11% of the original kernel matrix.

torch.Size([7106, 2])
We keep 9.16e+05/2.21e+07 =  4% of the original kernel matrix.

torch.Size([2346, 2])
We keep 1.79e+05/1.87e+06 =  9% of the original kernel matrix.

torch.Size([6808, 2])
We keep 8.40e+05/1.98e+07 =  4% of the original kernel matrix.

torch.Size([3940, 2])
We keep 4.66e+05/5.08e+06 =  9% of the original kernel matrix.

torch.Size([8495, 2])
We keep 1.15e+06/3.26e+07 =  3% of the original kernel matrix.

torch.Size([4415, 2])
We keep 5.20e+05/6.99e+06 =  7% of the original kernel matrix.

torch.Size([8762, 2])
We keep 1.35e+06/3.82e+07 =  3% of the original kernel matrix.

torch.Size([2234, 2])
We keep 1.53e+05/1.43e+06 = 10% of the original kernel matrix.

torch.Size([6774, 2])
We keep 7.69e+05/1.73e+07 =  4% of the original kernel matrix.

torch.Size([6560, 2])
We keep 1.62e+06/2.44e+07 =  6% of the original kernel matrix.

torch.Size([10921, 2])
We keep 2.19e+06/7.14e+07 =  3% of the original kernel matrix.

torch.Size([12002, 2])
We keep 5.23e+06/9.56e+07 =  5% of the original kernel matrix.

torch.Size([14469, 2])
We keep 3.66e+06/1.41e+08 =  2% of the original kernel matrix.

torch.Size([6273, 2])
We keep 1.35e+06/2.03e+07 =  6% of the original kernel matrix.

torch.Size([10589, 2])
We keep 2.07e+06/6.52e+07 =  3% of the original kernel matrix.

torch.Size([3466, 2])
We keep 3.13e+05/3.94e+06 =  7% of the original kernel matrix.

torch.Size([7890, 2])
We keep 1.08e+06/2.87e+07 =  3% of the original kernel matrix.

torch.Size([2547, 2])
We keep 1.94e+05/1.96e+06 =  9% of the original kernel matrix.

torch.Size([7087, 2])
We keep 8.50e+05/2.03e+07 =  4% of the original kernel matrix.

torch.Size([3815, 2])
We keep 6.25e+05/5.76e+06 = 10% of the original kernel matrix.

torch.Size([8769, 2])
We keep 1.27e+06/3.47e+07 =  3% of the original kernel matrix.

torch.Size([2395, 2])
We keep 1.61e+05/1.96e+06 =  8% of the original kernel matrix.

torch.Size([6948, 2])
We keep 8.43e+05/2.03e+07 =  4% of the original kernel matrix.

torch.Size([2708, 2])
We keep 2.60e+05/2.52e+06 = 10% of the original kernel matrix.

torch.Size([7231, 2])
We keep 9.32e+05/2.30e+07 =  4% of the original kernel matrix.

torch.Size([2208, 2])
We keep 1.59e+05/1.64e+06 =  9% of the original kernel matrix.

torch.Size([6463, 2])
We keep 8.12e+05/1.85e+07 =  4% of the original kernel matrix.

torch.Size([4850, 2])
We keep 8.85e+05/9.16e+06 =  9% of the original kernel matrix.

torch.Size([9160, 2])
We keep 1.49e+06/4.38e+07 =  3% of the original kernel matrix.

torch.Size([5821, 2])
We keep 1.01e+06/1.29e+07 =  7% of the original kernel matrix.

torch.Size([9979, 2])
We keep 1.64e+06/5.20e+07 =  3% of the original kernel matrix.

torch.Size([9775, 2])
We keep 5.58e+06/6.66e+07 =  8% of the original kernel matrix.

torch.Size([12840, 2])
We keep 3.09e+06/1.18e+08 =  2% of the original kernel matrix.

torch.Size([4966, 2])
We keep 8.69e+05/1.10e+07 =  7% of the original kernel matrix.

torch.Size([9157, 2])
We keep 1.61e+06/4.81e+07 =  3% of the original kernel matrix.

torch.Size([10742, 2])
We keep 5.01e+06/6.85e+07 =  7% of the original kernel matrix.

torch.Size([13640, 2])
We keep 3.14e+06/1.20e+08 =  2% of the original kernel matrix.

torch.Size([5368, 2])
We keep 6.08e+05/9.70e+06 =  6% of the original kernel matrix.

torch.Size([9622, 2])
We keep 1.51e+06/4.50e+07 =  3% of the original kernel matrix.

torch.Size([3399, 2])
We keep 3.97e+05/3.55e+06 = 11% of the original kernel matrix.

torch.Size([7996, 2])
We keep 1.07e+06/2.72e+07 =  3% of the original kernel matrix.

torch.Size([7566, 2])
We keep 1.66e+06/2.72e+07 =  6% of the original kernel matrix.

torch.Size([11419, 2])
We keep 2.23e+06/7.54e+07 =  2% of the original kernel matrix.

torch.Size([1634, 2])
We keep 5.71e+04/4.68e+05 = 12% of the original kernel matrix.

torch.Size([6033, 2])
We keep 5.16e+05/9.89e+06 =  5% of the original kernel matrix.

torch.Size([4388, 2])
We keep 5.83e+05/8.05e+06 =  7% of the original kernel matrix.

torch.Size([9625, 2])
We keep 1.48e+06/4.10e+07 =  3% of the original kernel matrix.

torch.Size([3166, 2])
We keep 3.70e+05/4.04e+06 =  9% of the original kernel matrix.

torch.Size([7583, 2])
We keep 1.11e+06/2.91e+07 =  3% of the original kernel matrix.

torch.Size([3687, 2])
We keep 5.09e+05/5.73e+06 =  8% of the original kernel matrix.

torch.Size([7984, 2])
We keep 1.25e+06/3.46e+07 =  3% of the original kernel matrix.

torch.Size([2022, 2])
We keep 2.09e+05/1.56e+06 = 13% of the original kernel matrix.

torch.Size([6497, 2])
We keep 8.28e+05/1.81e+07 =  4% of the original kernel matrix.

torch.Size([3230, 2])
We keep 3.65e+05/3.77e+06 =  9% of the original kernel matrix.

torch.Size([7539, 2])
We keep 1.06e+06/2.81e+07 =  3% of the original kernel matrix.

torch.Size([2714, 2])
We keep 1.94e+05/2.09e+06 =  9% of the original kernel matrix.

torch.Size([7233, 2])
We keep 8.84e+05/2.09e+07 =  4% of the original kernel matrix.

torch.Size([5160, 2])
We keep 6.34e+05/1.06e+07 =  5% of the original kernel matrix.

torch.Size([9691, 2])
We keep 1.52e+06/4.72e+07 =  3% of the original kernel matrix.

torch.Size([4534, 2])
We keep 4.87e+05/6.84e+06 =  7% of the original kernel matrix.

torch.Size([8942, 2])
We keep 1.32e+06/3.78e+07 =  3% of the original kernel matrix.

torch.Size([6251, 2])
We keep 1.03e+06/1.65e+07 =  6% of the original kernel matrix.

torch.Size([10389, 2])
We keep 1.89e+06/5.87e+07 =  3% of the original kernel matrix.

torch.Size([1676, 2])
We keep 9.37e+04/7.76e+05 = 12% of the original kernel matrix.

torch.Size([5969, 2])
We keep 6.21e+05/1.27e+07 =  4% of the original kernel matrix.

torch.Size([2224, 2])
We keep 2.17e+05/2.04e+06 = 10% of the original kernel matrix.

torch.Size([6640, 2])
We keep 8.75e+05/2.06e+07 =  4% of the original kernel matrix.

torch.Size([2195, 2])
We keep 1.77e+05/1.81e+06 =  9% of the original kernel matrix.

torch.Size([6439, 2])
We keep 8.39e+05/1.94e+07 =  4% of the original kernel matrix.

torch.Size([9116, 2])
We keep 1.88e+06/4.33e+07 =  4% of the original kernel matrix.

torch.Size([12468, 2])
We keep 2.68e+06/9.52e+07 =  2% of the original kernel matrix.

torch.Size([3838, 2])
We keep 4.66e+05/5.26e+06 =  8% of the original kernel matrix.

torch.Size([8392, 2])
We keep 1.20e+06/3.32e+07 =  3% of the original kernel matrix.

torch.Size([5584, 2])
We keep 2.15e+06/1.59e+07 = 13% of the original kernel matrix.

torch.Size([10050, 2])
We keep 1.83e+06/5.77e+07 =  3% of the original kernel matrix.

torch.Size([3569, 2])
We keep 5.58e+05/5.65e+06 =  9% of the original kernel matrix.

torch.Size([8575, 2])
We keep 1.26e+06/3.44e+07 =  3% of the original kernel matrix.

torch.Size([7503, 2])
We keep 1.79e+06/2.72e+07 =  6% of the original kernel matrix.

torch.Size([11185, 2])
We keep 2.24e+06/7.54e+07 =  2% of the original kernel matrix.

torch.Size([4504, 2])
We keep 4.77e+05/6.90e+06 =  6% of the original kernel matrix.

torch.Size([8908, 2])
We keep 1.34e+06/3.80e+07 =  3% of the original kernel matrix.

torch.Size([6266, 2])
We keep 1.62e+06/2.12e+07 =  7% of the original kernel matrix.

torch.Size([10174, 2])
We keep 2.04e+06/6.66e+07 =  3% of the original kernel matrix.

torch.Size([5090, 2])
We keep 6.31e+05/9.57e+06 =  6% of the original kernel matrix.

torch.Size([9370, 2])
We keep 1.51e+06/4.47e+07 =  3% of the original kernel matrix.

torch.Size([4681, 2])
We keep 6.05e+05/8.62e+06 =  7% of the original kernel matrix.

torch.Size([9006, 2])
We keep 1.46e+06/4.25e+07 =  3% of the original kernel matrix.

torch.Size([2873, 2])
We keep 2.62e+05/2.70e+06 =  9% of the original kernel matrix.

torch.Size([7392, 2])
We keep 9.71e+05/2.37e+07 =  4% of the original kernel matrix.

torch.Size([4717, 2])
We keep 1.17e+06/1.01e+07 = 11% of the original kernel matrix.

torch.Size([9037, 2])
We keep 1.54e+06/4.60e+07 =  3% of the original kernel matrix.

torch.Size([3376, 2])
We keep 3.55e+05/3.59e+06 =  9% of the original kernel matrix.

torch.Size([8005, 2])
We keep 1.06e+06/2.74e+07 =  3% of the original kernel matrix.

torch.Size([7165, 2])
We keep 1.55e+06/2.38e+07 =  6% of the original kernel matrix.

torch.Size([11008, 2])
We keep 2.13e+06/7.05e+07 =  3% of the original kernel matrix.

torch.Size([5932, 2])
We keep 8.38e+05/1.35e+07 =  6% of the original kernel matrix.

torch.Size([10125, 2])
We keep 1.71e+06/5.30e+07 =  3% of the original kernel matrix.

torch.Size([3039, 2])
We keep 2.35e+05/2.92e+06 =  8% of the original kernel matrix.

torch.Size([7536, 2])
We keep 9.76e+05/2.47e+07 =  3% of the original kernel matrix.

torch.Size([4546, 2])
We keep 7.71e+05/9.21e+06 =  8% of the original kernel matrix.

torch.Size([8975, 2])
We keep 1.53e+06/4.39e+07 =  3% of the original kernel matrix.

torch.Size([5449, 2])
We keep 8.70e+05/1.38e+07 =  6% of the original kernel matrix.

torch.Size([9767, 2])
We keep 1.76e+06/5.37e+07 =  3% of the original kernel matrix.

torch.Size([10403, 2])
We keep 5.41e+06/6.61e+07 =  8% of the original kernel matrix.

torch.Size([13282, 2])
We keep 3.11e+06/1.18e+08 =  2% of the original kernel matrix.

torch.Size([2822, 2])
We keep 2.38e+05/2.48e+06 =  9% of the original kernel matrix.

torch.Size([7356, 2])
We keep 9.21e+05/2.28e+07 =  4% of the original kernel matrix.

torch.Size([1783, 2])
We keep 1.11e+05/8.41e+05 = 13% of the original kernel matrix.

torch.Size([6101, 2])
We keep 6.36e+05/1.33e+07 =  4% of the original kernel matrix.

torch.Size([5267, 2])
We keep 9.25e+05/1.15e+07 =  8% of the original kernel matrix.

torch.Size([9503, 2])
We keep 1.59e+06/4.91e+07 =  3% of the original kernel matrix.

torch.Size([2722, 2])
We keep 2.54e+05/2.71e+06 =  9% of the original kernel matrix.

torch.Size([7219, 2])
We keep 9.50e+05/2.38e+07 =  3% of the original kernel matrix.

torch.Size([4548, 2])
We keep 6.56e+05/7.71e+06 =  8% of the original kernel matrix.

torch.Size([8855, 2])
We keep 1.39e+06/4.02e+07 =  3% of the original kernel matrix.

torch.Size([3672, 2])
We keep 3.60e+05/4.52e+06 =  7% of the original kernel matrix.

torch.Size([8205, 2])
We keep 1.15e+06/3.07e+07 =  3% of the original kernel matrix.

torch.Size([3938, 2])
We keep 3.70e+05/4.63e+06 =  8% of the original kernel matrix.

torch.Size([8480, 2])
We keep 1.15e+06/3.11e+07 =  3% of the original kernel matrix.

torch.Size([1866, 2])
We keep 1.64e+05/1.14e+06 = 14% of the original kernel matrix.

torch.Size([6357, 2])
We keep 7.04e+05/1.54e+07 =  4% of the original kernel matrix.

torch.Size([3993, 2])
We keep 3.97e+05/5.60e+06 =  7% of the original kernel matrix.

torch.Size([8322, 2])
We keep 1.24e+06/3.42e+07 =  3% of the original kernel matrix.

torch.Size([2725, 2])
We keep 3.20e+05/2.45e+06 = 13% of the original kernel matrix.

torch.Size([7436, 2])
We keep 9.21e+05/2.26e+07 =  4% of the original kernel matrix.

torch.Size([6513, 2])
We keep 9.36e+05/1.69e+07 =  5% of the original kernel matrix.

torch.Size([10446, 2])
We keep 1.84e+06/5.94e+07 =  3% of the original kernel matrix.

torch.Size([4155, 2])
We keep 3.86e+05/5.25e+06 =  7% of the original kernel matrix.

torch.Size([8672, 2])
We keep 1.21e+06/3.31e+07 =  3% of the original kernel matrix.

torch.Size([11098, 2])
We keep 2.91e+06/6.60e+07 =  4% of the original kernel matrix.

torch.Size([13930, 2])
We keep 3.11e+06/1.17e+08 =  2% of the original kernel matrix.

torch.Size([5387, 2])
We keep 7.68e+05/1.03e+07 =  7% of the original kernel matrix.

torch.Size([9785, 2])
We keep 1.55e+06/4.64e+07 =  3% of the original kernel matrix.

torch.Size([4772, 2])
We keep 5.32e+05/7.91e+06 =  6% of the original kernel matrix.

torch.Size([9261, 2])
We keep 1.43e+06/4.07e+07 =  3% of the original kernel matrix.

torch.Size([2433, 2])
We keep 1.62e+05/1.90e+06 =  8% of the original kernel matrix.

torch.Size([6966, 2])
We keep 8.39e+05/1.99e+07 =  4% of the original kernel matrix.

torch.Size([4077, 2])
We keep 9.12e+05/7.35e+06 = 12% of the original kernel matrix.

torch.Size([9109, 2])
We keep 1.39e+06/3.92e+07 =  3% of the original kernel matrix.

torch.Size([3858, 2])
We keep 7.29e+05/7.19e+06 = 10% of the original kernel matrix.

torch.Size([8298, 2])
We keep 1.39e+06/3.88e+07 =  3% of the original kernel matrix.

torch.Size([4611, 2])
We keep 7.11e+05/7.13e+06 =  9% of the original kernel matrix.

torch.Size([9058, 2])
We keep 1.33e+06/3.86e+07 =  3% of the original kernel matrix.

torch.Size([7485, 2])
We keep 1.66e+06/2.79e+07 =  5% of the original kernel matrix.

torch.Size([11609, 2])
We keep 2.30e+06/7.64e+07 =  3% of the original kernel matrix.

torch.Size([4979, 2])
We keep 6.65e+05/9.40e+06 =  7% of the original kernel matrix.

torch.Size([9267, 2])
We keep 1.49e+06/4.43e+07 =  3% of the original kernel matrix.

torch.Size([3879, 2])
We keep 6.70e+05/6.06e+06 = 11% of the original kernel matrix.

torch.Size([8188, 2])
We keep 1.28e+06/3.56e+07 =  3% of the original kernel matrix.

torch.Size([4651, 2])
We keep 4.67e+05/6.96e+06 =  6% of the original kernel matrix.

torch.Size([9082, 2])
We keep 1.35e+06/3.82e+07 =  3% of the original kernel matrix.

torch.Size([6144, 2])
We keep 8.82e+05/1.46e+07 =  6% of the original kernel matrix.

torch.Size([10174, 2])
We keep 1.76e+06/5.52e+07 =  3% of the original kernel matrix.

torch.Size([1558, 2])
We keep 8.57e+04/6.34e+05 = 13% of the original kernel matrix.

torch.Size([6037, 2])
We keep 5.74e+05/1.15e+07 =  4% of the original kernel matrix.

torch.Size([2206, 2])
We keep 1.62e+05/1.56e+06 = 10% of the original kernel matrix.

torch.Size([6613, 2])
We keep 7.98e+05/1.81e+07 =  4% of the original kernel matrix.

torch.Size([2348, 2])
We keep 1.29e+05/1.41e+06 =  9% of the original kernel matrix.

torch.Size([7084, 2])
We keep 7.50e+05/1.72e+07 =  4% of the original kernel matrix.

torch.Size([1565, 2])
We keep 6.19e+04/5.33e+05 = 11% of the original kernel matrix.

torch.Size([5998, 2])
We keep 5.34e+05/1.06e+07 =  5% of the original kernel matrix.

torch.Size([8010, 2])
We keep 3.03e+06/3.53e+07 =  8% of the original kernel matrix.

torch.Size([11632, 2])
We keep 2.46e+06/8.59e+07 =  2% of the original kernel matrix.

torch.Size([3993, 2])
We keep 4.61e+05/6.48e+06 =  7% of the original kernel matrix.

torch.Size([8403, 2])
We keep 1.31e+06/3.68e+07 =  3% of the original kernel matrix.

torch.Size([4710, 2])
We keep 1.18e+06/1.11e+07 = 10% of the original kernel matrix.

torch.Size([8968, 2])
We keep 1.58e+06/4.83e+07 =  3% of the original kernel matrix.

torch.Size([2045, 2])
We keep 1.14e+05/1.28e+06 =  8% of the original kernel matrix.

torch.Size([6639, 2])
We keep 7.17e+05/1.64e+07 =  4% of the original kernel matrix.

torch.Size([5958, 2])
We keep 1.11e+06/1.46e+07 =  7% of the original kernel matrix.

torch.Size([10034, 2])
We keep 1.76e+06/5.52e+07 =  3% of the original kernel matrix.

torch.Size([4566, 2])
We keep 7.31e+05/8.33e+06 =  8% of the original kernel matrix.

torch.Size([8968, 2])
We keep 1.44e+06/4.17e+07 =  3% of the original kernel matrix.

torch.Size([4463, 2])
We keep 7.06e+05/8.21e+06 =  8% of the original kernel matrix.

torch.Size([8938, 2])
We keep 1.42e+06/4.14e+07 =  3% of the original kernel matrix.

torch.Size([5562, 2])
We keep 8.63e+05/1.26e+07 =  6% of the original kernel matrix.

torch.Size([10117, 2])
We keep 1.69e+06/5.13e+07 =  3% of the original kernel matrix.

torch.Size([2717, 2])
We keep 2.05e+05/2.41e+06 =  8% of the original kernel matrix.

torch.Size([7131, 2])
We keep 9.21e+05/2.24e+07 =  4% of the original kernel matrix.

torch.Size([1789, 2])
We keep 9.65e+04/7.92e+05 = 12% of the original kernel matrix.

torch.Size([6296, 2])
We keep 6.16e+05/1.29e+07 =  4% of the original kernel matrix.

torch.Size([5349, 2])
We keep 8.77e+05/1.23e+07 =  7% of the original kernel matrix.

torch.Size([9720, 2])
We keep 1.67e+06/5.07e+07 =  3% of the original kernel matrix.

torch.Size([1521, 2])
We keep 7.62e+04/6.64e+05 = 11% of the original kernel matrix.

torch.Size([5612, 2])
We keep 5.85e+05/1.18e+07 =  4% of the original kernel matrix.

torch.Size([2359, 2])
We keep 1.51e+05/1.71e+06 =  8% of the original kernel matrix.

torch.Size([6801, 2])
We keep 8.11e+05/1.89e+07 =  4% of the original kernel matrix.

torch.Size([2817, 2])
We keep 2.83e+05/2.92e+06 =  9% of the original kernel matrix.

torch.Size([7310, 2])
We keep 9.95e+05/2.47e+07 =  4% of the original kernel matrix.

torch.Size([4551, 2])
We keep 7.53e+05/8.29e+06 =  9% of the original kernel matrix.

torch.Size([9212, 2])
We keep 1.45e+06/4.16e+07 =  3% of the original kernel matrix.

torch.Size([5740, 2])
We keep 1.10e+06/1.61e+07 =  6% of the original kernel matrix.

torch.Size([9991, 2])
We keep 1.89e+06/5.81e+07 =  3% of the original kernel matrix.

torch.Size([2424, 2])
We keep 2.30e+05/1.95e+06 = 11% of the original kernel matrix.

torch.Size([6941, 2])
We keep 8.50e+05/2.02e+07 =  4% of the original kernel matrix.

torch.Size([3246, 2])
We keep 3.38e+05/3.45e+06 =  9% of the original kernel matrix.

torch.Size([7693, 2])
We keep 1.03e+06/2.69e+07 =  3% of the original kernel matrix.

torch.Size([4838, 2])
We keep 6.29e+05/8.71e+06 =  7% of the original kernel matrix.

torch.Size([9330, 2])
We keep 1.45e+06/4.27e+07 =  3% of the original kernel matrix.

torch.Size([8480, 2])
We keep 1.65e+06/3.42e+07 =  4% of the original kernel matrix.

torch.Size([12084, 2])
We keep 2.36e+06/8.46e+07 =  2% of the original kernel matrix.

torch.Size([2387, 2])
We keep 1.64e+05/1.82e+06 =  8% of the original kernel matrix.

torch.Size([6910, 2])
We keep 8.33e+05/1.95e+07 =  4% of the original kernel matrix.

torch.Size([8096, 2])
We keep 1.86e+06/3.39e+07 =  5% of the original kernel matrix.

torch.Size([11716, 2])
We keep 2.38e+06/8.42e+07 =  2% of the original kernel matrix.

torch.Size([27871, 2])
We keep 5.51e+07/7.44e+08 =  7% of the original kernel matrix.

torch.Size([21672, 2])
We keep 8.26e+06/3.94e+08 =  2% of the original kernel matrix.

torch.Size([11385, 2])
We keep 5.05e+06/8.57e+07 =  5% of the original kernel matrix.

torch.Size([14347, 2])
We keep 3.33e+06/1.34e+08 =  2% of the original kernel matrix.

torch.Size([8251, 2])
We keep 1.93e+06/3.44e+07 =  5% of the original kernel matrix.

torch.Size([11883, 2])
We keep 2.43e+06/8.48e+07 =  2% of the original kernel matrix.

torch.Size([1490, 2])
We keep 7.18e+04/6.76e+05 = 10% of the original kernel matrix.

torch.Size([5695, 2])
We keep 5.93e+05/1.19e+07 =  4% of the original kernel matrix.

torch.Size([5335, 2])
We keep 7.30e+05/1.11e+07 =  6% of the original kernel matrix.

torch.Size([9610, 2])
We keep 1.59e+06/4.82e+07 =  3% of the original kernel matrix.

torch.Size([20444, 2])
We keep 4.50e+07/3.65e+08 = 12% of the original kernel matrix.

torch.Size([18493, 2])
We keep 5.95e+06/2.76e+08 =  2% of the original kernel matrix.

torch.Size([92677, 2])
We keep 8.04e+08/1.02e+10 =  7% of the original kernel matrix.

torch.Size([39886, 2])
We keep 2.58e+07/1.46e+09 =  1% of the original kernel matrix.

torch.Size([5237, 2])
We keep 9.34e+05/1.17e+07 =  8% of the original kernel matrix.

torch.Size([9500, 2])
We keep 1.61e+06/4.94e+07 =  3% of the original kernel matrix.

torch.Size([47809, 2])
We keep 1.08e+08/2.00e+09 =  5% of the original kernel matrix.

torch.Size([27807, 2])
We keep 1.25e+07/6.47e+08 =  1% of the original kernel matrix.

torch.Size([18709, 2])
We keep 1.85e+07/3.16e+08 =  5% of the original kernel matrix.

torch.Size([17541, 2])
We keep 5.86e+06/2.57e+08 =  2% of the original kernel matrix.

torch.Size([17662, 2])
We keep 2.20e+07/2.78e+08 =  7% of the original kernel matrix.

torch.Size([17135, 2])
We keep 5.61e+06/2.41e+08 =  2% of the original kernel matrix.

torch.Size([261069, 2])
We keep 2.11e+09/7.33e+10 =  2% of the original kernel matrix.

torch.Size([68251, 2])
We keep 6.32e+07/3.92e+09 =  1% of the original kernel matrix.

torch.Size([3484, 2])
We keep 3.15e+05/3.90e+06 =  8% of the original kernel matrix.

torch.Size([8067, 2])
We keep 1.09e+06/2.86e+07 =  3% of the original kernel matrix.

torch.Size([7356, 2])
We keep 1.17e+06/2.30e+07 =  5% of the original kernel matrix.

torch.Size([11152, 2])
We keep 2.05e+06/6.93e+07 =  2% of the original kernel matrix.

torch.Size([199560, 2])
We keep 7.33e+08/3.04e+10 =  2% of the original kernel matrix.

torch.Size([59221, 2])
We keep 4.18e+07/2.52e+09 =  1% of the original kernel matrix.

torch.Size([659998, 2])
We keep 3.18e+09/2.76e+11 =  1% of the original kernel matrix.

torch.Size([112296, 2])
We keep 1.14e+08/7.60e+09 =  1% of the original kernel matrix.

torch.Size([6590, 2])
We keep 9.77e+05/1.74e+07 =  5% of the original kernel matrix.

torch.Size([10606, 2])
We keep 1.82e+06/6.03e+07 =  3% of the original kernel matrix.

torch.Size([16912, 2])
We keep 1.18e+07/2.77e+08 =  4% of the original kernel matrix.

torch.Size([17408, 2])
We keep 5.72e+06/2.41e+08 =  2% of the original kernel matrix.

torch.Size([8815, 2])
We keep 3.56e+06/4.57e+07 =  7% of the original kernel matrix.

torch.Size([12148, 2])
We keep 2.72e+06/9.77e+07 =  2% of the original kernel matrix.

torch.Size([67211, 2])
We keep 3.00e+08/3.83e+09 =  7% of the original kernel matrix.

torch.Size([33580, 2])
We keep 1.68e+07/8.95e+08 =  1% of the original kernel matrix.

torch.Size([20845, 2])
We keep 2.45e+07/3.96e+08 =  6% of the original kernel matrix.

torch.Size([18630, 2])
We keep 6.42e+06/2.88e+08 =  2% of the original kernel matrix.

torch.Size([69134, 2])
We keep 2.70e+08/5.58e+09 =  4% of the original kernel matrix.

torch.Size([33812, 2])
We keep 2.00e+07/1.08e+09 =  1% of the original kernel matrix.

torch.Size([31877, 2])
We keep 4.34e+07/8.56e+08 =  5% of the original kernel matrix.

torch.Size([23535, 2])
We keep 8.68e+06/4.23e+08 =  2% of the original kernel matrix.

torch.Size([12332, 2])
We keep 4.41e+06/9.02e+07 =  4% of the original kernel matrix.

torch.Size([14697, 2])
We keep 3.60e+06/1.37e+08 =  2% of the original kernel matrix.

torch.Size([6015, 2])
We keep 2.06e+06/1.70e+07 = 12% of the original kernel matrix.

torch.Size([10029, 2])
We keep 1.90e+06/5.97e+07 =  3% of the original kernel matrix.

torch.Size([111071, 2])
We keep 2.60e+09/4.69e+10 =  5% of the original kernel matrix.

torch.Size([41307, 2])
We keep 4.95e+07/3.13e+09 =  1% of the original kernel matrix.

torch.Size([7140, 2])
We keep 1.43e+06/2.21e+07 =  6% of the original kernel matrix.

torch.Size([11187, 2])
We keep 2.03e+06/6.79e+07 =  2% of the original kernel matrix.

torch.Size([3734, 2])
We keep 3.80e+05/4.58e+06 =  8% of the original kernel matrix.

torch.Size([8163, 2])
We keep 1.16e+06/3.09e+07 =  3% of the original kernel matrix.

torch.Size([49311, 2])
We keep 5.26e+07/1.75e+09 =  3% of the original kernel matrix.

torch.Size([28306, 2])
We keep 1.19e+07/6.05e+08 =  1% of the original kernel matrix.

torch.Size([16402, 2])
We keep 1.94e+07/3.10e+08 =  6% of the original kernel matrix.

torch.Size([16087, 2])
We keep 5.81e+06/2.55e+08 =  2% of the original kernel matrix.

torch.Size([143457, 2])
We keep 4.64e+08/1.68e+10 =  2% of the original kernel matrix.

torch.Size([50687, 2])
We keep 3.19e+07/1.88e+09 =  1% of the original kernel matrix.

torch.Size([112715, 2])
We keep 1.66e+08/9.21e+09 =  1% of the original kernel matrix.

torch.Size([44357, 2])
We keep 2.44e+07/1.39e+09 =  1% of the original kernel matrix.

torch.Size([2817, 2])
We keep 2.03e+05/2.43e+06 =  8% of the original kernel matrix.

torch.Size([7381, 2])
We keep 9.18e+05/2.25e+07 =  4% of the original kernel matrix.

torch.Size([13082, 2])
We keep 9.24e+06/1.36e+08 =  6% of the original kernel matrix.

torch.Size([15333, 2])
We keep 4.30e+06/1.69e+08 =  2% of the original kernel matrix.

torch.Size([7422, 2])
We keep 2.67e+06/2.79e+07 =  9% of the original kernel matrix.

torch.Size([11256, 2])
We keep 2.18e+06/7.64e+07 =  2% of the original kernel matrix.

torch.Size([15932, 2])
We keep 1.03e+07/1.70e+08 =  6% of the original kernel matrix.

torch.Size([16691, 2])
We keep 4.55e+06/1.89e+08 =  2% of the original kernel matrix.

torch.Size([75018, 2])
We keep 1.07e+08/4.23e+09 =  2% of the original kernel matrix.

torch.Size([36043, 2])
We keep 1.73e+07/9.40e+08 =  1% of the original kernel matrix.

torch.Size([10395, 2])
We keep 2.98e+07/8.21e+07 = 36% of the original kernel matrix.

torch.Size([13459, 2])
We keep 3.39e+06/1.31e+08 =  2% of the original kernel matrix.

torch.Size([65223, 2])
We keep 6.66e+07/3.13e+09 =  2% of the original kernel matrix.

torch.Size([32973, 2])
We keep 1.53e+07/8.09e+08 =  1% of the original kernel matrix.

torch.Size([1465, 2])
We keep 7.08e+04/6.07e+05 = 11% of the original kernel matrix.

torch.Size([5638, 2])
We keep 5.73e+05/1.13e+07 =  5% of the original kernel matrix.

torch.Size([6572, 2])
We keep 1.40e+06/2.12e+07 =  6% of the original kernel matrix.

torch.Size([10550, 2])
We keep 2.04e+06/6.66e+07 =  3% of the original kernel matrix.

torch.Size([25144, 2])
We keep 5.82e+07/6.44e+08 =  9% of the original kernel matrix.

torch.Size([20628, 2])
We keep 7.83e+06/3.67e+08 =  2% of the original kernel matrix.

torch.Size([97420, 2])
We keep 3.95e+08/1.01e+10 =  3% of the original kernel matrix.

torch.Size([40476, 2])
We keep 2.57e+07/1.45e+09 =  1% of the original kernel matrix.

torch.Size([32579, 2])
We keep 3.09e+07/9.48e+08 =  3% of the original kernel matrix.

torch.Size([22721, 2])
We keep 9.20e+06/4.45e+08 =  2% of the original kernel matrix.

torch.Size([5313, 2])
We keep 1.15e+06/1.28e+07 =  9% of the original kernel matrix.

torch.Size([9449, 2])
We keep 1.66e+06/5.17e+07 =  3% of the original kernel matrix.

torch.Size([186391, 2])
We keep 4.16e+08/3.09e+10 =  1% of the original kernel matrix.

torch.Size([57708, 2])
We keep 4.17e+07/2.54e+09 =  1% of the original kernel matrix.

torch.Size([17240, 2])
We keep 9.84e+06/2.59e+08 =  3% of the original kernel matrix.

torch.Size([17681, 2])
We keep 5.55e+06/2.33e+08 =  2% of the original kernel matrix.

torch.Size([19188, 2])
We keep 7.58e+06/2.58e+08 =  2% of the original kernel matrix.

torch.Size([18240, 2])
We keep 5.41e+06/2.32e+08 =  2% of the original kernel matrix.

torch.Size([57918, 2])
We keep 7.31e+07/2.78e+09 =  2% of the original kernel matrix.

torch.Size([31238, 2])
We keep 1.46e+07/7.63e+08 =  1% of the original kernel matrix.

torch.Size([6498, 2])
We keep 1.84e+06/2.39e+07 =  7% of the original kernel matrix.

torch.Size([10527, 2])
We keep 2.20e+06/7.06e+07 =  3% of the original kernel matrix.

torch.Size([48812, 2])
We keep 6.03e+07/1.77e+09 =  3% of the original kernel matrix.

torch.Size([28150, 2])
We keep 1.19e+07/6.08e+08 =  1% of the original kernel matrix.

torch.Size([84855, 2])
We keep 6.90e+08/1.07e+10 =  6% of the original kernel matrix.

torch.Size([38432, 2])
We keep 2.64e+07/1.50e+09 =  1% of the original kernel matrix.

torch.Size([166049, 2])
We keep 9.23e+08/2.85e+10 =  3% of the original kernel matrix.

torch.Size([54552, 2])
We keep 4.08e+07/2.44e+09 =  1% of the original kernel matrix.

torch.Size([14159, 2])
We keep 5.21e+06/1.43e+08 =  3% of the original kernel matrix.

torch.Size([15241, 2])
We keep 4.05e+06/1.73e+08 =  2% of the original kernel matrix.

torch.Size([16731, 2])
We keep 7.32e+06/2.04e+08 =  3% of the original kernel matrix.

torch.Size([17044, 2])
We keep 4.96e+06/2.06e+08 =  2% of the original kernel matrix.

torch.Size([20482, 2])
We keep 1.54e+07/3.63e+08 =  4% of the original kernel matrix.

torch.Size([18151, 2])
We keep 6.13e+06/2.75e+08 =  2% of the original kernel matrix.

torch.Size([7303, 2])
We keep 1.28e+06/2.26e+07 =  5% of the original kernel matrix.

torch.Size([11072, 2])
We keep 2.04e+06/6.87e+07 =  2% of the original kernel matrix.

torch.Size([14677, 2])
We keep 9.53e+06/1.59e+08 =  5% of the original kernel matrix.

torch.Size([15883, 2])
We keep 4.43e+06/1.83e+08 =  2% of the original kernel matrix.

torch.Size([10510, 2])
We keep 4.37e+06/7.60e+07 =  5% of the original kernel matrix.

torch.Size([14024, 2])
We keep 3.37e+06/1.26e+08 =  2% of the original kernel matrix.

torch.Size([16748, 2])
We keep 7.63e+06/1.97e+08 =  3% of the original kernel matrix.

torch.Size([16911, 2])
We keep 4.78e+06/2.03e+08 =  2% of the original kernel matrix.

torch.Size([53920, 2])
We keep 9.55e+07/2.46e+09 =  3% of the original kernel matrix.

torch.Size([30819, 2])
We keep 1.26e+07/7.17e+08 =  1% of the original kernel matrix.

torch.Size([51080, 2])
We keep 2.62e+08/2.57e+09 = 10% of the original kernel matrix.

torch.Size([29495, 2])
We keep 1.41e+07/7.33e+08 =  1% of the original kernel matrix.

torch.Size([6007, 2])
We keep 1.68e+06/2.19e+07 =  7% of the original kernel matrix.

torch.Size([10032, 2])
We keep 2.05e+06/6.77e+07 =  3% of the original kernel matrix.

torch.Size([8493, 2])
We keep 2.22e+06/4.52e+07 =  4% of the original kernel matrix.

torch.Size([12367, 2])
We keep 2.68e+06/9.73e+07 =  2% of the original kernel matrix.

torch.Size([15575, 2])
We keep 2.40e+07/4.59e+08 =  5% of the original kernel matrix.

torch.Size([16245, 2])
We keep 6.22e+06/3.10e+08 =  2% of the original kernel matrix.

torch.Size([15738, 2])
We keep 7.54e+06/1.80e+08 =  4% of the original kernel matrix.

torch.Size([16683, 2])
We keep 4.66e+06/1.94e+08 =  2% of the original kernel matrix.

torch.Size([7346, 2])
We keep 8.25e+06/4.20e+07 = 19% of the original kernel matrix.

torch.Size([11065, 2])
We keep 2.52e+06/9.37e+07 =  2% of the original kernel matrix.

torch.Size([10176, 2])
We keep 3.91e+06/5.44e+07 =  7% of the original kernel matrix.

torch.Size([13169, 2])
We keep 2.89e+06/1.07e+08 =  2% of the original kernel matrix.

torch.Size([114914, 2])
We keep 1.33e+09/2.40e+10 =  5% of the original kernel matrix.

torch.Size([44221, 2])
We keep 3.76e+07/2.24e+09 =  1% of the original kernel matrix.

torch.Size([8547, 2])
We keep 1.97e+06/3.91e+07 =  5% of the original kernel matrix.

torch.Size([12086, 2])
We keep 2.55e+06/9.04e+07 =  2% of the original kernel matrix.

torch.Size([228671, 2])
We keep 4.49e+09/9.29e+10 =  4% of the original kernel matrix.

torch.Size([60204, 2])
We keep 7.07e+07/4.41e+09 =  1% of the original kernel matrix.

torch.Size([6764, 2])
We keep 1.53e+06/2.26e+07 =  6% of the original kernel matrix.

torch.Size([10754, 2])
We keep 2.05e+06/6.88e+07 =  2% of the original kernel matrix.

torch.Size([11824, 2])
We keep 1.90e+07/1.99e+08 =  9% of the original kernel matrix.

torch.Size([13386, 2])
We keep 4.80e+06/2.04e+08 =  2% of the original kernel matrix.

torch.Size([16030, 2])
We keep 8.33e+06/1.94e+08 =  4% of the original kernel matrix.

torch.Size([16964, 2])
We keep 4.67e+06/2.01e+08 =  2% of the original kernel matrix.

torch.Size([55547, 2])
We keep 1.50e+09/2.14e+10 =  7% of the original kernel matrix.

torch.Size([27514, 2])
We keep 3.57e+07/2.11e+09 =  1% of the original kernel matrix.

torch.Size([36465, 2])
We keep 2.48e+07/1.16e+09 =  2% of the original kernel matrix.

torch.Size([24656, 2])
We keep 9.87e+06/4.92e+08 =  2% of the original kernel matrix.

torch.Size([9741, 2])
We keep 1.94e+06/4.67e+07 =  4% of the original kernel matrix.

torch.Size([13077, 2])
We keep 2.75e+06/9.88e+07 =  2% of the original kernel matrix.

torch.Size([83162, 2])
We keep 8.48e+07/5.09e+09 =  1% of the original kernel matrix.

torch.Size([37816, 2])
We keep 1.87e+07/1.03e+09 =  1% of the original kernel matrix.

torch.Size([98921, 2])
We keep 1.46e+08/8.10e+09 =  1% of the original kernel matrix.

torch.Size([41894, 2])
We keep 2.30e+07/1.30e+09 =  1% of the original kernel matrix.

torch.Size([6801, 2])
We keep 1.18e+06/1.84e+07 =  6% of the original kernel matrix.

torch.Size([10939, 2])
We keep 1.87e+06/6.20e+07 =  3% of the original kernel matrix.

torch.Size([59886, 2])
We keep 7.34e+07/2.44e+09 =  3% of the original kernel matrix.

torch.Size([31217, 2])
We keep 1.36e+07/7.15e+08 =  1% of the original kernel matrix.

torch.Size([96746, 2])
We keep 1.29e+08/7.36e+09 =  1% of the original kernel matrix.

torch.Size([41094, 2])
We keep 2.22e+07/1.24e+09 =  1% of the original kernel matrix.

torch.Size([216607, 2])
We keep 6.19e+08/4.39e+10 =  1% of the original kernel matrix.

torch.Size([63414, 2])
We keep 4.97e+07/3.03e+09 =  1% of the original kernel matrix.

torch.Size([62559, 2])
We keep 8.99e+07/3.27e+09 =  2% of the original kernel matrix.

torch.Size([32453, 2])
We keep 1.55e+07/8.27e+08 =  1% of the original kernel matrix.

torch.Size([3618, 2])
We keep 5.99e+05/5.41e+06 = 11% of the original kernel matrix.

torch.Size([7968, 2])
We keep 1.19e+06/3.36e+07 =  3% of the original kernel matrix.

torch.Size([18720, 2])
We keep 7.48e+06/2.58e+08 =  2% of the original kernel matrix.

torch.Size([18289, 2])
We keep 5.42e+06/2.32e+08 =  2% of the original kernel matrix.

torch.Size([47478, 2])
We keep 4.83e+08/4.48e+09 = 10% of the original kernel matrix.

torch.Size([28308, 2])
We keep 1.77e+07/9.68e+08 =  1% of the original kernel matrix.

torch.Size([85593, 2])
We keep 7.66e+08/8.36e+09 =  9% of the original kernel matrix.

torch.Size([38288, 2])
We keep 2.37e+07/1.32e+09 =  1% of the original kernel matrix.

torch.Size([12323, 2])
We keep 3.29e+06/9.57e+07 =  3% of the original kernel matrix.

torch.Size([14710, 2])
We keep 3.55e+06/1.41e+08 =  2% of the original kernel matrix.

torch.Size([3999, 2])
We keep 3.54e+05/4.72e+06 =  7% of the original kernel matrix.

torch.Size([8597, 2])
We keep 1.17e+06/3.14e+07 =  3% of the original kernel matrix.

torch.Size([92093, 2])
We keep 2.41e+08/6.44e+09 =  3% of the original kernel matrix.

torch.Size([40295, 2])
We keep 2.11e+07/1.16e+09 =  1% of the original kernel matrix.

torch.Size([141757, 2])
We keep 2.81e+08/1.61e+10 =  1% of the original kernel matrix.

torch.Size([50260, 2])
We keep 3.13e+07/1.83e+09 =  1% of the original kernel matrix.

torch.Size([21923, 2])
We keep 4.55e+07/6.95e+08 =  6% of the original kernel matrix.

torch.Size([18064, 2])
We keep 7.74e+06/3.81e+08 =  2% of the original kernel matrix.

torch.Size([9054, 2])
We keep 3.35e+06/4.99e+07 =  6% of the original kernel matrix.

torch.Size([12445, 2])
We keep 2.84e+06/1.02e+08 =  2% of the original kernel matrix.

torch.Size([14912, 2])
We keep 9.23e+06/1.64e+08 =  5% of the original kernel matrix.

torch.Size([16097, 2])
We keep 4.43e+06/1.85e+08 =  2% of the original kernel matrix.

torch.Size([10227, 2])
We keep 2.73e+06/6.22e+07 =  4% of the original kernel matrix.

torch.Size([13248, 2])
We keep 3.09e+06/1.14e+08 =  2% of the original kernel matrix.

torch.Size([6669, 2])
We keep 1.14e+06/1.99e+07 =  5% of the original kernel matrix.

torch.Size([10810, 2])
We keep 1.99e+06/6.44e+07 =  3% of the original kernel matrix.

torch.Size([13096, 2])
We keep 3.91e+06/1.03e+08 =  3% of the original kernel matrix.

torch.Size([15028, 2])
We keep 3.76e+06/1.47e+08 =  2% of the original kernel matrix.

torch.Size([7629, 2])
We keep 1.23e+06/2.38e+07 =  5% of the original kernel matrix.

torch.Size([11330, 2])
We keep 2.11e+06/7.06e+07 =  2% of the original kernel matrix.

torch.Size([35717, 2])
We keep 7.12e+07/1.21e+09 =  5% of the original kernel matrix.

torch.Size([24199, 2])
We keep 1.03e+07/5.03e+08 =  2% of the original kernel matrix.

torch.Size([35333, 2])
We keep 1.97e+07/8.70e+08 =  2% of the original kernel matrix.

torch.Size([24748, 2])
We keep 8.78e+06/4.26e+08 =  2% of the original kernel matrix.

torch.Size([17729, 2])
We keep 8.01e+06/2.35e+08 =  3% of the original kernel matrix.

torch.Size([17953, 2])
We keep 5.17e+06/2.22e+08 =  2% of the original kernel matrix.

torch.Size([9661, 2])
We keep 3.23e+06/6.00e+07 =  5% of the original kernel matrix.

torch.Size([13579, 2])
We keep 3.07e+06/1.12e+08 =  2% of the original kernel matrix.

torch.Size([7377, 2])
We keep 1.91e+06/2.89e+07 =  6% of the original kernel matrix.

torch.Size([11173, 2])
We keep 2.27e+06/7.77e+07 =  2% of the original kernel matrix.

torch.Size([534927, 2])
We keep 2.57e+09/2.08e+11 =  1% of the original kernel matrix.

torch.Size([100917, 2])
We keep 9.91e+07/6.59e+09 =  1% of the original kernel matrix.

torch.Size([10126, 2])
We keep 2.94e+06/6.07e+07 =  4% of the original kernel matrix.

torch.Size([13124, 2])
We keep 2.98e+06/1.13e+08 =  2% of the original kernel matrix.

torch.Size([95623, 2])
We keep 1.38e+08/7.16e+09 =  1% of the original kernel matrix.

torch.Size([40720, 2])
We keep 2.20e+07/1.22e+09 =  1% of the original kernel matrix.

torch.Size([3250, 2])
We keep 3.40e+05/4.05e+06 =  8% of the original kernel matrix.

torch.Size([7777, 2])
We keep 1.05e+06/2.91e+07 =  3% of the original kernel matrix.

torch.Size([6572, 2])
We keep 2.27e+06/1.90e+07 = 11% of the original kernel matrix.

torch.Size([10594, 2])
We keep 1.92e+06/6.31e+07 =  3% of the original kernel matrix.

torch.Size([4712, 2])
We keep 5.00e+05/7.21e+06 =  6% of the original kernel matrix.

torch.Size([9304, 2])
We keep 1.36e+06/3.88e+07 =  3% of the original kernel matrix.

torch.Size([17803, 2])
We keep 9.05e+06/2.23e+08 =  4% of the original kernel matrix.

torch.Size([18164, 2])
We keep 4.75e+06/2.16e+08 =  2% of the original kernel matrix.

torch.Size([5891, 2])
We keep 8.65e+05/1.19e+07 =  7% of the original kernel matrix.

torch.Size([10949, 2])
We keep 1.66e+06/4.98e+07 =  3% of the original kernel matrix.

torch.Size([17265, 2])
We keep 1.10e+08/4.27e+08 = 25% of the original kernel matrix.

torch.Size([16769, 2])
We keep 6.21e+06/2.99e+08 =  2% of the original kernel matrix.

torch.Size([5525, 2])
We keep 7.49e+05/1.15e+07 =  6% of the original kernel matrix.

torch.Size([9822, 2])
We keep 1.60e+06/4.91e+07 =  3% of the original kernel matrix.

torch.Size([4725, 2])
We keep 7.91e+05/9.78e+06 =  8% of the original kernel matrix.

torch.Size([9176, 2])
We keep 1.56e+06/4.52e+07 =  3% of the original kernel matrix.

torch.Size([211762, 2])
We keep 9.08e+08/4.51e+10 =  2% of the original kernel matrix.

torch.Size([61317, 2])
We keep 5.06e+07/3.07e+09 =  1% of the original kernel matrix.

torch.Size([47542, 2])
We keep 1.23e+08/2.35e+09 =  5% of the original kernel matrix.

torch.Size([28228, 2])
We keep 1.40e+07/7.01e+08 =  2% of the original kernel matrix.

torch.Size([15594, 2])
We keep 1.15e+07/1.85e+08 =  6% of the original kernel matrix.

torch.Size([16694, 2])
We keep 4.69e+06/1.96e+08 =  2% of the original kernel matrix.

torch.Size([70878, 2])
We keep 1.90e+08/4.23e+09 =  4% of the original kernel matrix.

torch.Size([35087, 2])
We keep 1.77e+07/9.41e+08 =  1% of the original kernel matrix.

torch.Size([37194, 2])
We keep 4.40e+08/6.07e+09 =  7% of the original kernel matrix.

torch.Size([24405, 2])
We keep 2.05e+07/1.13e+09 =  1% of the original kernel matrix.

torch.Size([3477, 2])
We keep 3.45e+05/4.38e+06 =  7% of the original kernel matrix.

torch.Size([8150, 2])
We keep 1.11e+06/3.03e+07 =  3% of the original kernel matrix.

torch.Size([9343, 2])
We keep 2.30e+06/4.86e+07 =  4% of the original kernel matrix.

torch.Size([12687, 2])
We keep 2.78e+06/1.01e+08 =  2% of the original kernel matrix.

torch.Size([8242, 2])
We keep 2.32e+06/3.82e+07 =  6% of the original kernel matrix.

torch.Size([11900, 2])
We keep 2.52e+06/8.94e+07 =  2% of the original kernel matrix.

torch.Size([1819, 2])
We keep 1.22e+05/8.87e+05 = 13% of the original kernel matrix.

torch.Size([6212, 2])
We keep 6.60e+05/1.36e+07 =  4% of the original kernel matrix.

torch.Size([7574, 2])
We keep 1.42e+06/2.59e+07 =  5% of the original kernel matrix.

torch.Size([11242, 2])
We keep 2.18e+06/7.36e+07 =  2% of the original kernel matrix.

torch.Size([12748, 2])
We keep 9.27e+06/1.07e+08 =  8% of the original kernel matrix.

torch.Size([14799, 2])
We keep 3.72e+06/1.49e+08 =  2% of the original kernel matrix.

torch.Size([5235, 2])
We keep 1.02e+06/1.21e+07 =  8% of the original kernel matrix.

torch.Size([9368, 2])
We keep 1.65e+06/5.02e+07 =  3% of the original kernel matrix.

torch.Size([6073, 2])
We keep 9.96e+05/1.62e+07 =  6% of the original kernel matrix.

torch.Size([10226, 2])
We keep 1.83e+06/5.83e+07 =  3% of the original kernel matrix.

torch.Size([8856, 2])
We keep 1.90e+06/4.26e+07 =  4% of the original kernel matrix.

torch.Size([12099, 2])
We keep 2.65e+06/9.44e+07 =  2% of the original kernel matrix.

torch.Size([6818, 2])
We keep 1.07e+06/2.04e+07 =  5% of the original kernel matrix.

torch.Size([10785, 2])
We keep 1.97e+06/6.53e+07 =  3% of the original kernel matrix.

torch.Size([229738, 2])
We keep 1.06e+09/4.84e+10 =  2% of the original kernel matrix.

torch.Size([63305, 2])
We keep 5.03e+07/3.18e+09 =  1% of the original kernel matrix.

torch.Size([10274, 2])
We keep 4.67e+06/7.73e+07 =  6% of the original kernel matrix.

torch.Size([13166, 2])
We keep 3.33e+06/1.27e+08 =  2% of the original kernel matrix.

torch.Size([32715, 2])
We keep 8.76e+07/1.05e+09 =  8% of the original kernel matrix.

torch.Size([23443, 2])
We keep 9.57e+06/4.68e+08 =  2% of the original kernel matrix.

torch.Size([8703, 2])
We keep 8.66e+06/5.98e+07 = 14% of the original kernel matrix.

torch.Size([12177, 2])
We keep 2.95e+06/1.12e+08 =  2% of the original kernel matrix.

torch.Size([29165, 2])
We keep 6.45e+07/8.63e+08 =  7% of the original kernel matrix.

torch.Size([21617, 2])
We keep 9.07e+06/4.25e+08 =  2% of the original kernel matrix.

torch.Size([24582, 2])
We keep 8.86e+07/9.00e+08 =  9% of the original kernel matrix.

torch.Size([19975, 2])
We keep 9.01e+06/4.34e+08 =  2% of the original kernel matrix.

torch.Size([555216, 2])
We keep 7.14e+09/2.82e+11 =  2% of the original kernel matrix.

torch.Size([102257, 2])
We keep 1.09e+08/7.68e+09 =  1% of the original kernel matrix.

torch.Size([454561, 2])
We keep 1.63e+09/1.50e+11 =  1% of the original kernel matrix.

torch.Size([93599, 2])
We keep 8.49e+07/5.61e+09 =  1% of the original kernel matrix.

torch.Size([26798, 2])
We keep 1.72e+07/5.21e+08 =  3% of the original kernel matrix.

torch.Size([21817, 2])
We keep 7.03e+06/3.30e+08 =  2% of the original kernel matrix.

torch.Size([43302, 2])
We keep 9.21e+07/1.38e+09 =  6% of the original kernel matrix.

torch.Size([27025, 2])
We keep 1.09e+07/5.37e+08 =  2% of the original kernel matrix.

torch.Size([4608, 2])
We keep 4.44e+05/6.53e+06 =  6% of the original kernel matrix.

torch.Size([9097, 2])
We keep 1.29e+06/3.69e+07 =  3% of the original kernel matrix.

torch.Size([17156, 2])
We keep 2.05e+07/2.75e+08 =  7% of the original kernel matrix.

torch.Size([16842, 2])
We keep 5.47e+06/2.40e+08 =  2% of the original kernel matrix.

torch.Size([23928, 2])
We keep 4.89e+07/6.16e+08 =  7% of the original kernel matrix.

torch.Size([19879, 2])
We keep 7.45e+06/3.59e+08 =  2% of the original kernel matrix.

torch.Size([53893, 2])
We keep 6.47e+07/2.08e+09 =  3% of the original kernel matrix.

torch.Size([30455, 2])
We keep 1.28e+07/6.59e+08 =  1% of the original kernel matrix.

torch.Size([5703, 2])
We keep 7.79e+05/1.27e+07 =  6% of the original kernel matrix.

torch.Size([9731, 2])
We keep 1.66e+06/5.16e+07 =  3% of the original kernel matrix.

torch.Size([93454, 2])
We keep 1.88e+08/6.79e+09 =  2% of the original kernel matrix.

torch.Size([40193, 2])
We keep 2.16e+07/1.19e+09 =  1% of the original kernel matrix.

torch.Size([140127, 2])
We keep 6.24e+08/2.37e+10 =  2% of the original kernel matrix.

torch.Size([49612, 2])
We keep 3.78e+07/2.23e+09 =  1% of the original kernel matrix.

torch.Size([30331, 2])
We keep 2.89e+07/8.27e+08 =  3% of the original kernel matrix.

torch.Size([21897, 2])
We keep 8.63e+06/4.16e+08 =  2% of the original kernel matrix.

torch.Size([11331, 2])
We keep 4.61e+06/9.39e+07 =  4% of the original kernel matrix.

torch.Size([13724, 2])
We keep 3.63e+06/1.40e+08 =  2% of the original kernel matrix.

torch.Size([17632, 2])
We keep 2.69e+07/2.47e+08 = 10% of the original kernel matrix.

torch.Size([19834, 2])
We keep 5.46e+06/2.27e+08 =  2% of the original kernel matrix.

torch.Size([12113, 2])
We keep 3.30e+06/8.26e+07 =  3% of the original kernel matrix.

torch.Size([14480, 2])
We keep 3.42e+06/1.31e+08 =  2% of the original kernel matrix.

torch.Size([6276, 2])
We keep 7.86e+05/1.42e+07 =  5% of the original kernel matrix.

torch.Size([10455, 2])
We keep 1.72e+06/5.45e+07 =  3% of the original kernel matrix.

torch.Size([92210, 2])
We keep 1.56e+08/7.96e+09 =  1% of the original kernel matrix.

torch.Size([40061, 2])
We keep 2.33e+07/1.29e+09 =  1% of the original kernel matrix.

torch.Size([29229, 2])
We keep 3.53e+07/6.80e+08 =  5% of the original kernel matrix.

torch.Size([21800, 2])
We keep 7.99e+06/3.77e+08 =  2% of the original kernel matrix.

torch.Size([4216, 2])
We keep 7.95e+05/8.24e+06 =  9% of the original kernel matrix.

torch.Size([8505, 2])
We keep 1.44e+06/4.15e+07 =  3% of the original kernel matrix.

torch.Size([114138, 2])
We keep 2.00e+08/1.03e+10 =  1% of the original kernel matrix.

torch.Size([44948, 2])
We keep 2.53e+07/1.47e+09 =  1% of the original kernel matrix.

torch.Size([11166, 2])
We keep 3.60e+06/8.26e+07 =  4% of the original kernel matrix.

torch.Size([14120, 2])
We keep 3.41e+06/1.31e+08 =  2% of the original kernel matrix.

torch.Size([2088, 2])
We keep 1.20e+05/1.27e+06 =  9% of the original kernel matrix.

torch.Size([6619, 2])
We keep 7.35e+05/1.63e+07 =  4% of the original kernel matrix.

torch.Size([60447, 2])
We keep 2.91e+08/4.07e+09 =  7% of the original kernel matrix.

torch.Size([31940, 2])
We keep 1.73e+07/9.23e+08 =  1% of the original kernel matrix.

torch.Size([95850, 2])
We keep 1.41e+08/7.21e+09 =  1% of the original kernel matrix.

torch.Size([40955, 2])
We keep 2.21e+07/1.23e+09 =  1% of the original kernel matrix.

torch.Size([60584, 2])
We keep 1.16e+08/3.16e+09 =  3% of the original kernel matrix.

torch.Size([31832, 2])
We keep 1.55e+07/8.13e+08 =  1% of the original kernel matrix.

torch.Size([146030, 2])
We keep 6.07e+08/1.88e+10 =  3% of the original kernel matrix.

torch.Size([50957, 2])
We keep 3.33e+07/1.99e+09 =  1% of the original kernel matrix.

torch.Size([93306, 2])
We keep 2.31e+08/7.95e+09 =  2% of the original kernel matrix.

torch.Size([39473, 2])
We keep 2.31e+07/1.29e+09 =  1% of the original kernel matrix.

torch.Size([9676, 2])
We keep 6.64e+06/5.17e+07 = 12% of the original kernel matrix.

torch.Size([12681, 2])
We keep 2.69e+06/1.04e+08 =  2% of the original kernel matrix.

torch.Size([37725, 2])
We keep 3.51e+07/1.10e+09 =  3% of the original kernel matrix.

torch.Size([24805, 2])
We keep 9.67e+06/4.79e+08 =  2% of the original kernel matrix.

torch.Size([109833, 2])
We keep 1.50e+08/9.48e+09 =  1% of the original kernel matrix.

torch.Size([44252, 2])
We keep 2.47e+07/1.41e+09 =  1% of the original kernel matrix.

torch.Size([120318, 2])
We keep 1.80e+08/1.02e+10 =  1% of the original kernel matrix.

torch.Size([46240, 2])
We keep 2.48e+07/1.46e+09 =  1% of the original kernel matrix.

torch.Size([9233, 2])
We keep 2.03e+06/4.20e+07 =  4% of the original kernel matrix.

torch.Size([12644, 2])
We keep 2.62e+06/9.37e+07 =  2% of the original kernel matrix.

torch.Size([82805, 2])
We keep 2.73e+08/7.14e+09 =  3% of the original kernel matrix.

torch.Size([38629, 2])
We keep 1.98e+07/1.22e+09 =  1% of the original kernel matrix.

torch.Size([113872, 2])
We keep 2.33e+08/1.10e+10 =  2% of the original kernel matrix.

torch.Size([45547, 2])
We keep 2.52e+07/1.51e+09 =  1% of the original kernel matrix.

torch.Size([47358, 2])
We keep 4.51e+07/1.63e+09 =  2% of the original kernel matrix.

torch.Size([28411, 2])
We keep 1.13e+07/5.85e+08 =  1% of the original kernel matrix.

torch.Size([28359, 2])
We keep 3.73e+07/8.38e+08 =  4% of the original kernel matrix.

torch.Size([22071, 2])
We keep 8.23e+06/4.19e+08 =  1% of the original kernel matrix.

torch.Size([13835, 2])
We keep 8.63e+07/4.43e+08 = 19% of the original kernel matrix.

torch.Size([14527, 2])
We keep 7.05e+06/3.04e+08 =  2% of the original kernel matrix.

torch.Size([14979, 2])
We keep 8.88e+06/1.59e+08 =  5% of the original kernel matrix.

torch.Size([16205, 2])
We keep 4.48e+06/1.82e+08 =  2% of the original kernel matrix.

torch.Size([7644, 2])
We keep 1.44e+06/2.86e+07 =  5% of the original kernel matrix.

torch.Size([11316, 2])
We keep 2.29e+06/7.73e+07 =  2% of the original kernel matrix.

torch.Size([5492, 2])
We keep 1.32e+06/1.28e+07 = 10% of the original kernel matrix.

torch.Size([9681, 2])
We keep 1.68e+06/5.18e+07 =  3% of the original kernel matrix.

torch.Size([138792, 2])
We keep 2.68e+08/1.73e+10 =  1% of the original kernel matrix.

torch.Size([50351, 2])
We keep 3.26e+07/1.90e+09 =  1% of the original kernel matrix.

torch.Size([78208, 2])
We keep 8.27e+07/4.59e+09 =  1% of the original kernel matrix.

torch.Size([36820, 2])
We keep 1.80e+07/9.80e+08 =  1% of the original kernel matrix.

torch.Size([150428, 2])
We keep 4.70e+08/1.93e+10 =  2% of the original kernel matrix.

torch.Size([51671, 2])
We keep 3.38e+07/2.01e+09 =  1% of the original kernel matrix.

torch.Size([110536, 2])
We keep 1.01e+09/1.39e+10 =  7% of the original kernel matrix.

torch.Size([44060, 2])
We keep 2.94e+07/1.70e+09 =  1% of the original kernel matrix.

torch.Size([40655, 2])
We keep 4.12e+07/1.23e+09 =  3% of the original kernel matrix.

torch.Size([25821, 2])
We keep 1.03e+07/5.08e+08 =  2% of the original kernel matrix.

torch.Size([11548, 2])
We keep 3.77e+06/1.02e+08 =  3% of the original kernel matrix.

torch.Size([13893, 2])
We keep 3.72e+06/1.46e+08 =  2% of the original kernel matrix.

torch.Size([59195, 2])
We keep 8.74e+07/2.53e+09 =  3% of the original kernel matrix.

torch.Size([31516, 2])
We keep 1.38e+07/7.27e+08 =  1% of the original kernel matrix.

torch.Size([3622, 2])
We keep 6.39e+05/4.98e+06 = 12% of the original kernel matrix.

torch.Size([8019, 2])
We keep 1.19e+06/3.23e+07 =  3% of the original kernel matrix.

torch.Size([10762, 2])
We keep 3.27e+06/7.62e+07 =  4% of the original kernel matrix.

torch.Size([13562, 2])
We keep 3.31e+06/1.26e+08 =  2% of the original kernel matrix.

torch.Size([147637, 2])
We keep 3.61e+08/1.74e+10 =  2% of the original kernel matrix.

torch.Size([51494, 2])
We keep 3.27e+07/1.91e+09 =  1% of the original kernel matrix.

torch.Size([18889, 2])
We keep 1.30e+07/3.33e+08 =  3% of the original kernel matrix.

torch.Size([18389, 2])
We keep 6.15e+06/2.64e+08 =  2% of the original kernel matrix.

torch.Size([4720, 2])
We keep 5.64e+05/8.05e+06 =  7% of the original kernel matrix.

torch.Size([9085, 2])
We keep 1.43e+06/4.10e+07 =  3% of the original kernel matrix.

torch.Size([9748, 2])
We keep 2.25e+06/5.41e+07 =  4% of the original kernel matrix.

torch.Size([12824, 2])
We keep 2.83e+06/1.06e+08 =  2% of the original kernel matrix.

torch.Size([106130, 2])
We keep 1.01e+09/1.56e+10 =  6% of the original kernel matrix.

torch.Size([43157, 2])
We keep 3.23e+07/1.81e+09 =  1% of the original kernel matrix.

torch.Size([23918, 2])
We keep 1.64e+07/4.67e+08 =  3% of the original kernel matrix.

torch.Size([19969, 2])
We keep 6.81e+06/3.12e+08 =  2% of the original kernel matrix.

torch.Size([54449, 2])
We keep 1.28e+08/2.52e+09 =  5% of the original kernel matrix.

torch.Size([30473, 2])
We keep 1.40e+07/7.26e+08 =  1% of the original kernel matrix.

torch.Size([20542, 2])
We keep 1.63e+07/3.97e+08 =  4% of the original kernel matrix.

torch.Size([18444, 2])
We keep 6.42e+06/2.88e+08 =  2% of the original kernel matrix.

torch.Size([17373, 2])
We keep 1.81e+07/3.41e+08 =  5% of the original kernel matrix.

torch.Size([16510, 2])
We keep 5.92e+06/2.67e+08 =  2% of the original kernel matrix.

torch.Size([4548, 2])
We keep 1.09e+06/1.01e+07 = 10% of the original kernel matrix.

torch.Size([8843, 2])
We keep 1.56e+06/4.60e+07 =  3% of the original kernel matrix.

torch.Size([2784, 2])
We keep 2.61e+05/3.27e+06 =  7% of the original kernel matrix.

torch.Size([7502, 2])
We keep 1.05e+06/2.61e+07 =  4% of the original kernel matrix.

torch.Size([35046, 2])
We keep 3.17e+07/1.07e+09 =  2% of the original kernel matrix.

torch.Size([24504, 2])
We keep 9.83e+06/4.73e+08 =  2% of the original kernel matrix.

torch.Size([31524, 2])
We keep 5.68e+07/7.69e+08 =  7% of the original kernel matrix.

torch.Size([22804, 2])
We keep 8.25e+06/4.01e+08 =  2% of the original kernel matrix.

torch.Size([4471, 2])
We keep 6.11e+05/7.24e+06 =  8% of the original kernel matrix.

torch.Size([8893, 2])
We keep 1.35e+06/3.89e+07 =  3% of the original kernel matrix.

torch.Size([138456, 2])
We keep 3.15e+08/1.91e+10 =  1% of the original kernel matrix.

torch.Size([50242, 2])
We keep 3.37e+07/2.00e+09 =  1% of the original kernel matrix.

torch.Size([4997, 2])
We keep 6.79e+05/8.85e+06 =  7% of the original kernel matrix.

torch.Size([9309, 2])
We keep 1.44e+06/4.30e+07 =  3% of the original kernel matrix.

torch.Size([15055, 2])
We keep 5.42e+06/1.74e+08 =  3% of the original kernel matrix.

torch.Size([15772, 2])
We keep 4.34e+06/1.91e+08 =  2% of the original kernel matrix.

torch.Size([6304, 2])
We keep 1.17e+06/1.84e+07 =  6% of the original kernel matrix.

torch.Size([10376, 2])
We keep 1.90e+06/6.21e+07 =  3% of the original kernel matrix.

torch.Size([86543, 2])
We keep 9.72e+07/5.64e+09 =  1% of the original kernel matrix.

torch.Size([38840, 2])
We keep 1.96e+07/1.09e+09 =  1% of the original kernel matrix.

torch.Size([6554, 2])
We keep 1.08e+06/1.95e+07 =  5% of the original kernel matrix.

torch.Size([10778, 2])
We keep 1.96e+06/6.38e+07 =  3% of the original kernel matrix.

torch.Size([25413, 2])
We keep 2.53e+07/4.89e+08 =  5% of the original kernel matrix.

torch.Size([20773, 2])
We keep 7.05e+06/3.20e+08 =  2% of the original kernel matrix.

torch.Size([14791, 2])
We keep 7.47e+06/1.56e+08 =  4% of the original kernel matrix.

torch.Size([15470, 2])
We keep 4.20e+06/1.81e+08 =  2% of the original kernel matrix.

torch.Size([17524, 2])
We keep 1.09e+07/2.52e+08 =  4% of the original kernel matrix.

torch.Size([17693, 2])
We keep 5.40e+06/2.30e+08 =  2% of the original kernel matrix.

torch.Size([27073, 2])
We keep 2.31e+07/6.11e+08 =  3% of the original kernel matrix.

torch.Size([21773, 2])
We keep 7.62e+06/3.58e+08 =  2% of the original kernel matrix.

torch.Size([398462, 2])
We keep 1.94e+09/1.42e+11 =  1% of the original kernel matrix.

torch.Size([85683, 2])
We keep 8.32e+07/5.45e+09 =  1% of the original kernel matrix.

torch.Size([12172, 2])
We keep 3.93e+06/8.50e+07 =  4% of the original kernel matrix.

torch.Size([14704, 2])
We keep 3.41e+06/1.33e+08 =  2% of the original kernel matrix.

torch.Size([5897, 2])
We keep 1.48e+06/1.69e+07 =  8% of the original kernel matrix.

torch.Size([9862, 2])
We keep 1.83e+06/5.95e+07 =  3% of the original kernel matrix.

torch.Size([94528, 2])
We keep 3.16e+08/8.13e+09 =  3% of the original kernel matrix.

torch.Size([40435, 2])
We keep 2.30e+07/1.30e+09 =  1% of the original kernel matrix.

torch.Size([12956, 2])
We keep 5.23e+06/1.18e+08 =  4% of the original kernel matrix.

torch.Size([15087, 2])
We keep 4.03e+06/1.57e+08 =  2% of the original kernel matrix.

torch.Size([278197, 2])
We keep 1.37e+09/5.79e+10 =  2% of the original kernel matrix.

torch.Size([71023, 2])
We keep 5.47e+07/3.48e+09 =  1% of the original kernel matrix.

torch.Size([4279, 2])
We keep 5.94e+05/6.43e+06 =  9% of the original kernel matrix.

torch.Size([8627, 2])
We keep 1.29e+06/3.67e+07 =  3% of the original kernel matrix.

torch.Size([85498, 2])
We keep 1.33e+08/6.56e+09 =  2% of the original kernel matrix.

torch.Size([37963, 2])
We keep 2.12e+07/1.17e+09 =  1% of the original kernel matrix.

torch.Size([277655, 2])
We keep 7.04e+08/5.43e+10 =  1% of the original kernel matrix.

torch.Size([70869, 2])
We keep 5.36e+07/3.37e+09 =  1% of the original kernel matrix.

torch.Size([5864, 2])
We keep 8.28e+05/1.21e+07 =  6% of the original kernel matrix.

torch.Size([9892, 2])
We keep 1.63e+06/5.03e+07 =  3% of the original kernel matrix.

torch.Size([244120, 2])
We keep 9.78e+08/6.21e+10 =  1% of the original kernel matrix.

torch.Size([63509, 2])
We keep 5.71e+07/3.60e+09 =  1% of the original kernel matrix.

torch.Size([8555, 2])
We keep 1.07e+07/8.61e+07 = 12% of the original kernel matrix.

torch.Size([12081, 2])
We keep 3.39e+06/1.34e+08 =  2% of the original kernel matrix.

torch.Size([278009, 2])
We keep 3.78e+09/7.57e+10 =  4% of the original kernel matrix.

torch.Size([70006, 2])
We keep 6.19e+07/3.98e+09 =  1% of the original kernel matrix.

torch.Size([10441, 2])
We keep 2.56e+06/6.09e+07 =  4% of the original kernel matrix.

torch.Size([13407, 2])
We keep 3.00e+06/1.13e+08 =  2% of the original kernel matrix.

torch.Size([128671, 2])
We keep 6.22e+08/1.69e+10 =  3% of the original kernel matrix.

torch.Size([47108, 2])
We keep 3.26e+07/1.88e+09 =  1% of the original kernel matrix.

torch.Size([3228, 2])
We keep 4.16e+05/4.70e+06 =  8% of the original kernel matrix.

torch.Size([7664, 2])
We keep 1.14e+06/3.13e+07 =  3% of the original kernel matrix.

torch.Size([5383, 2])
We keep 1.10e+06/1.51e+07 =  7% of the original kernel matrix.

torch.Size([9685, 2])
We keep 1.84e+06/5.62e+07 =  3% of the original kernel matrix.

torch.Size([7602, 2])
We keep 2.25e+06/3.09e+07 =  7% of the original kernel matrix.

torch.Size([11300, 2])
We keep 2.36e+06/8.03e+07 =  2% of the original kernel matrix.

torch.Size([29716, 2])
We keep 1.93e+07/6.34e+08 =  3% of the original kernel matrix.

torch.Size([22550, 2])
We keep 7.59e+06/3.64e+08 =  2% of the original kernel matrix.

torch.Size([38239, 2])
We keep 5.84e+07/1.24e+09 =  4% of the original kernel matrix.

torch.Size([25135, 2])
We keep 1.05e+07/5.08e+08 =  2% of the original kernel matrix.

torch.Size([78956, 2])
We keep 3.03e+08/4.92e+09 =  6% of the original kernel matrix.

torch.Size([37043, 2])
We keep 1.85e+07/1.01e+09 =  1% of the original kernel matrix.

torch.Size([14728, 2])
We keep 4.79e+06/1.37e+08 =  3% of the original kernel matrix.

torch.Size([16320, 2])
We keep 4.20e+06/1.69e+08 =  2% of the original kernel matrix.

torch.Size([5891, 2])
We keep 8.35e+05/1.29e+07 =  6% of the original kernel matrix.

torch.Size([9922, 2])
We keep 1.67e+06/5.20e+07 =  3% of the original kernel matrix.

torch.Size([8739, 2])
We keep 4.07e+06/4.58e+07 =  8% of the original kernel matrix.

torch.Size([12187, 2])
We keep 2.76e+06/9.79e+07 =  2% of the original kernel matrix.

torch.Size([98824, 2])
We keep 4.83e+08/1.48e+10 =  3% of the original kernel matrix.

torch.Size([41585, 2])
We keep 3.09e+07/1.76e+09 =  1% of the original kernel matrix.

torch.Size([25234, 2])
We keep 6.97e+07/9.17e+08 =  7% of the original kernel matrix.

torch.Size([20609, 2])
We keep 9.15e+06/4.38e+08 =  2% of the original kernel matrix.

torch.Size([214998, 2])
We keep 6.27e+08/3.58e+10 =  1% of the original kernel matrix.

torch.Size([63192, 2])
We keep 4.49e+07/2.73e+09 =  1% of the original kernel matrix.

torch.Size([35056, 2])
We keep 5.78e+07/9.04e+08 =  6% of the original kernel matrix.

torch.Size([24457, 2])
We keep 9.01e+06/4.35e+08 =  2% of the original kernel matrix.

torch.Size([10498, 2])
We keep 6.16e+06/8.68e+07 =  7% of the original kernel matrix.

torch.Size([13344, 2])
We keep 3.52e+06/1.35e+08 =  2% of the original kernel matrix.

torch.Size([4966, 2])
We keep 9.15e+05/9.10e+06 = 10% of the original kernel matrix.

torch.Size([9291, 2])
We keep 1.47e+06/4.36e+07 =  3% of the original kernel matrix.

torch.Size([13286, 2])
We keep 5.64e+07/2.00e+08 = 28% of the original kernel matrix.

torch.Size([14679, 2])
We keep 4.61e+06/2.04e+08 =  2% of the original kernel matrix.

torch.Size([766943, 2])
We keep 7.59e+09/4.57e+11 =  1% of the original kernel matrix.

torch.Size([122968, 2])
We keep 1.40e+08/9.77e+09 =  1% of the original kernel matrix.

torch.Size([8526, 2])
We keep 3.30e+06/4.22e+07 =  7% of the original kernel matrix.

torch.Size([12071, 2])
We keep 2.66e+06/9.40e+07 =  2% of the original kernel matrix.

torch.Size([13485, 2])
We keep 5.33e+06/1.21e+08 =  4% of the original kernel matrix.

torch.Size([15533, 2])
We keep 3.97e+06/1.59e+08 =  2% of the original kernel matrix.

torch.Size([100578, 2])
We keep 2.28e+08/8.17e+09 =  2% of the original kernel matrix.

torch.Size([41620, 2])
We keep 2.34e+07/1.31e+09 =  1% of the original kernel matrix.

torch.Size([32018, 2])
We keep 7.42e+07/1.27e+09 =  5% of the original kernel matrix.

torch.Size([21988, 2])
We keep 1.04e+07/5.15e+08 =  2% of the original kernel matrix.

torch.Size([5793, 2])
We keep 1.01e+06/1.35e+07 =  7% of the original kernel matrix.

torch.Size([9905, 2])
We keep 1.66e+06/5.31e+07 =  3% of the original kernel matrix.

torch.Size([4313, 2])
We keep 5.51e+05/6.73e+06 =  8% of the original kernel matrix.

torch.Size([8513, 2])
We keep 1.34e+06/3.75e+07 =  3% of the original kernel matrix.

torch.Size([31270, 2])
We keep 8.83e+07/1.04e+09 =  8% of the original kernel matrix.

torch.Size([22630, 2])
We keep 9.77e+06/4.67e+08 =  2% of the original kernel matrix.

torch.Size([22441, 2])
We keep 1.60e+07/3.87e+08 =  4% of the original kernel matrix.

torch.Size([19896, 2])
We keep 6.33e+06/2.84e+08 =  2% of the original kernel matrix.

torch.Size([9854, 2])
We keep 5.05e+06/7.49e+07 =  6% of the original kernel matrix.

torch.Size([12868, 2])
We keep 3.29e+06/1.25e+08 =  2% of the original kernel matrix.

torch.Size([41245, 2])
We keep 4.57e+07/1.27e+09 =  3% of the original kernel matrix.

torch.Size([26385, 2])
We keep 1.04e+07/5.15e+08 =  2% of the original kernel matrix.

torch.Size([77079, 2])
We keep 1.28e+09/9.38e+09 = 13% of the original kernel matrix.

torch.Size([37866, 2])
We keep 2.41e+07/1.40e+09 =  1% of the original kernel matrix.

torch.Size([46049, 2])
We keep 1.20e+08/2.04e+09 =  5% of the original kernel matrix.

torch.Size([27548, 2])
We keep 1.26e+07/6.52e+08 =  1% of the original kernel matrix.

torch.Size([229438, 2])
We keep 1.39e+09/5.16e+10 =  2% of the original kernel matrix.

torch.Size([63748, 2])
We keep 5.34e+07/3.29e+09 =  1% of the original kernel matrix.

torch.Size([62413, 2])
We keep 6.98e+07/3.00e+09 =  2% of the original kernel matrix.

torch.Size([32467, 2])
We keep 1.49e+07/7.91e+08 =  1% of the original kernel matrix.

torch.Size([8817, 2])
We keep 5.53e+06/5.37e+07 = 10% of the original kernel matrix.

torch.Size([12414, 2])
We keep 2.97e+06/1.06e+08 =  2% of the original kernel matrix.

torch.Size([52506, 2])
We keep 5.70e+07/2.16e+09 =  2% of the original kernel matrix.

torch.Size([29654, 2])
We keep 1.28e+07/6.71e+08 =  1% of the original kernel matrix.

torch.Size([13055, 2])
We keep 1.70e+07/1.35e+08 = 12% of the original kernel matrix.

torch.Size([15013, 2])
We keep 4.00e+06/1.68e+08 =  2% of the original kernel matrix.

torch.Size([9288, 2])
We keep 2.85e+06/4.64e+07 =  6% of the original kernel matrix.

torch.Size([12551, 2])
We keep 2.71e+06/9.85e+07 =  2% of the original kernel matrix.

torch.Size([16527, 2])
We keep 6.34e+06/1.96e+08 =  3% of the original kernel matrix.

torch.Size([16837, 2])
We keep 4.83e+06/2.02e+08 =  2% of the original kernel matrix.

torch.Size([9371, 2])
We keep 1.87e+06/4.10e+07 =  4% of the original kernel matrix.

torch.Size([12708, 2])
We keep 2.56e+06/9.26e+07 =  2% of the original kernel matrix.

torch.Size([64689, 2])
We keep 1.36e+08/3.95e+09 =  3% of the original kernel matrix.

torch.Size([32866, 2])
We keep 1.71e+07/9.09e+08 =  1% of the original kernel matrix.

torch.Size([15664, 2])
We keep 7.23e+06/1.81e+08 =  4% of the original kernel matrix.

torch.Size([16169, 2])
We keep 4.65e+06/1.94e+08 =  2% of the original kernel matrix.

torch.Size([3824, 2])
We keep 4.55e+05/5.93e+06 =  7% of the original kernel matrix.

torch.Size([8104, 2])
We keep 1.25e+06/3.52e+07 =  3% of the original kernel matrix.

torch.Size([17248, 2])
We keep 7.29e+06/2.21e+08 =  3% of the original kernel matrix.

torch.Size([17448, 2])
We keep 4.97e+06/2.15e+08 =  2% of the original kernel matrix.

torch.Size([9329, 2])
We keep 6.42e+06/9.91e+07 =  6% of the original kernel matrix.

torch.Size([12732, 2])
We keep 3.37e+06/1.44e+08 =  2% of the original kernel matrix.

torch.Size([18597, 2])
We keep 1.12e+07/2.68e+08 =  4% of the original kernel matrix.

torch.Size([17884, 2])
We keep 5.43e+06/2.37e+08 =  2% of the original kernel matrix.

torch.Size([41895, 2])
We keep 1.19e+08/1.91e+09 =  6% of the original kernel matrix.

torch.Size([25485, 2])
We keep 1.22e+07/6.31e+08 =  1% of the original kernel matrix.

torch.Size([6067, 2])
We keep 2.30e+06/2.56e+07 =  8% of the original kernel matrix.

torch.Size([10111, 2])
We keep 2.11e+06/7.31e+07 =  2% of the original kernel matrix.

torch.Size([3998, 2])
We keep 4.92e+05/5.29e+06 =  9% of the original kernel matrix.

torch.Size([8482, 2])
We keep 1.21e+06/3.32e+07 =  3% of the original kernel matrix.

torch.Size([15246, 2])
We keep 1.27e+07/1.84e+08 =  6% of the original kernel matrix.

torch.Size([16273, 2])
We keep 4.75e+06/1.96e+08 =  2% of the original kernel matrix.

torch.Size([4508, 2])
We keep 7.03e+05/9.39e+06 =  7% of the original kernel matrix.

torch.Size([9001, 2])
We keep 1.52e+06/4.43e+07 =  3% of the original kernel matrix.

torch.Size([8363, 2])
We keep 1.68e+06/3.51e+07 =  4% of the original kernel matrix.

torch.Size([11901, 2])
We keep 2.46e+06/8.57e+07 =  2% of the original kernel matrix.

torch.Size([19390, 2])
We keep 6.81e+06/2.52e+08 =  2% of the original kernel matrix.

torch.Size([18668, 2])
We keep 5.26e+06/2.30e+08 =  2% of the original kernel matrix.

torch.Size([122709, 2])
We keep 3.60e+08/1.29e+10 =  2% of the original kernel matrix.

torch.Size([46878, 2])
We keep 2.83e+07/1.64e+09 =  1% of the original kernel matrix.

torch.Size([9054, 2])
We keep 2.96e+06/4.66e+07 =  6% of the original kernel matrix.

torch.Size([12480, 2])
We keep 2.72e+06/9.87e+07 =  2% of the original kernel matrix.

torch.Size([2843, 2])
We keep 2.06e+05/2.44e+06 =  8% of the original kernel matrix.

torch.Size([7353, 2])
We keep 9.25e+05/2.26e+07 =  4% of the original kernel matrix.

torch.Size([4697, 2])
We keep 8.66e+05/9.12e+06 =  9% of the original kernel matrix.

torch.Size([8945, 2])
We keep 1.48e+06/4.37e+07 =  3% of the original kernel matrix.

torch.Size([96050, 2])
We keep 2.84e+08/7.30e+09 =  3% of the original kernel matrix.

torch.Size([40769, 2])
We keep 2.22e+07/1.24e+09 =  1% of the original kernel matrix.

torch.Size([323546, 2])
We keep 2.55e+09/9.69e+10 =  2% of the original kernel matrix.

torch.Size([77741, 2])
We keep 7.20e+07/4.50e+09 =  1% of the original kernel matrix.

torch.Size([32952, 2])
We keep 4.10e+07/9.90e+08 =  4% of the original kernel matrix.

torch.Size([22850, 2])
We keep 9.30e+06/4.55e+08 =  2% of the original kernel matrix.

torch.Size([7366, 2])
We keep 2.52e+06/2.74e+07 =  9% of the original kernel matrix.

torch.Size([11197, 2])
We keep 2.23e+06/7.56e+07 =  2% of the original kernel matrix.

torch.Size([56396, 2])
We keep 2.89e+08/3.89e+09 =  7% of the original kernel matrix.

torch.Size([30751, 2])
We keep 1.66e+07/9.02e+08 =  1% of the original kernel matrix.

torch.Size([151272, 2])
We keep 2.67e+08/1.92e+10 =  1% of the original kernel matrix.

torch.Size([53230, 2])
We keep 3.39e+07/2.00e+09 =  1% of the original kernel matrix.

torch.Size([2835, 2])
We keep 2.78e+05/3.21e+06 =  8% of the original kernel matrix.

torch.Size([7472, 2])
We keep 1.06e+06/2.59e+07 =  4% of the original kernel matrix.

torch.Size([9069, 2])
We keep 3.21e+06/5.90e+07 =  5% of the original kernel matrix.

torch.Size([12526, 2])
We keep 3.07e+06/1.11e+08 =  2% of the original kernel matrix.

torch.Size([30159, 2])
We keep 1.47e+07/6.56e+08 =  2% of the original kernel matrix.

torch.Size([23049, 2])
We keep 7.81e+06/3.70e+08 =  2% of the original kernel matrix.

torch.Size([7185, 2])
We keep 3.16e+06/3.40e+07 =  9% of the original kernel matrix.

torch.Size([11383, 2])
We keep 2.38e+06/8.43e+07 =  2% of the original kernel matrix.

torch.Size([46999, 2])
We keep 9.43e+07/2.08e+09 =  4% of the original kernel matrix.

torch.Size([27928, 2])
We keep 1.31e+07/6.60e+08 =  1% of the original kernel matrix.

torch.Size([61945, 2])
We keep 7.99e+07/2.91e+09 =  2% of the original kernel matrix.

torch.Size([32278, 2])
We keep 1.48e+07/7.80e+08 =  1% of the original kernel matrix.

torch.Size([20669, 2])
We keep 8.37e+06/2.99e+08 =  2% of the original kernel matrix.

torch.Size([19201, 2])
We keep 5.68e+06/2.50e+08 =  2% of the original kernel matrix.

torch.Size([6805, 2])
We keep 1.97e+06/2.15e+07 =  9% of the original kernel matrix.

torch.Size([10732, 2])
We keep 2.04e+06/6.70e+07 =  3% of the original kernel matrix.

torch.Size([5396, 2])
We keep 9.84e+05/1.30e+07 =  7% of the original kernel matrix.

torch.Size([9653, 2])
We keep 1.77e+06/5.22e+07 =  3% of the original kernel matrix.

torch.Size([26305, 2])
We keep 1.72e+07/5.16e+08 =  3% of the original kernel matrix.

torch.Size([21133, 2])
We keep 7.22e+06/3.28e+08 =  2% of the original kernel matrix.

torch.Size([31052, 2])
We keep 3.39e+07/7.68e+08 =  4% of the original kernel matrix.

torch.Size([22957, 2])
We keep 8.59e+06/4.01e+08 =  2% of the original kernel matrix.

torch.Size([40277, 2])
We keep 4.60e+07/1.48e+09 =  3% of the original kernel matrix.

torch.Size([25552, 2])
We keep 1.09e+07/5.56e+08 =  1% of the original kernel matrix.

torch.Size([26753, 2])
We keep 1.49e+07/5.15e+08 =  2% of the original kernel matrix.

torch.Size([21370, 2])
We keep 7.12e+06/3.28e+08 =  2% of the original kernel matrix.

torch.Size([191273, 2])
We keep 4.25e+08/3.17e+10 =  1% of the original kernel matrix.

torch.Size([59491, 2])
We keep 4.22e+07/2.57e+09 =  1% of the original kernel matrix.

torch.Size([6513, 2])
We keep 1.19e+06/1.92e+07 =  6% of the original kernel matrix.

torch.Size([10491, 2])
We keep 1.91e+06/6.34e+07 =  3% of the original kernel matrix.

torch.Size([4011, 2])
We keep 4.61e+05/5.61e+06 =  8% of the original kernel matrix.

torch.Size([8491, 2])
We keep 1.25e+06/3.43e+07 =  3% of the original kernel matrix.

torch.Size([11402, 2])
We keep 2.83e+06/6.80e+07 =  4% of the original kernel matrix.

torch.Size([14210, 2])
We keep 3.19e+06/1.19e+08 =  2% of the original kernel matrix.

torch.Size([57531, 2])
We keep 8.53e+07/2.42e+09 =  3% of the original kernel matrix.

torch.Size([31265, 2])
We keep 1.38e+07/7.11e+08 =  1% of the original kernel matrix.

torch.Size([29475, 2])
We keep 1.69e+07/6.38e+08 =  2% of the original kernel matrix.

torch.Size([22441, 2])
We keep 7.76e+06/3.65e+08 =  2% of the original kernel matrix.

torch.Size([6516, 2])
We keep 1.28e+06/1.96e+07 =  6% of the original kernel matrix.

torch.Size([10568, 2])
We keep 1.95e+06/6.40e+07 =  3% of the original kernel matrix.

torch.Size([4399, 2])
We keep 7.76e+05/8.50e+06 =  9% of the original kernel matrix.

torch.Size([8710, 2])
We keep 1.43e+06/4.22e+07 =  3% of the original kernel matrix.

torch.Size([35139, 2])
We keep 2.10e+07/8.40e+08 =  2% of the original kernel matrix.

torch.Size([24397, 2])
We keep 8.71e+06/4.19e+08 =  2% of the original kernel matrix.

torch.Size([4601, 2])
We keep 5.31e+05/7.68e+06 =  6% of the original kernel matrix.

torch.Size([9107, 2])
We keep 1.37e+06/4.01e+07 =  3% of the original kernel matrix.

torch.Size([11099, 2])
We keep 1.79e+07/1.22e+08 = 14% of the original kernel matrix.

torch.Size([13644, 2])
We keep 3.88e+06/1.60e+08 =  2% of the original kernel matrix.

torch.Size([5293, 2])
We keep 7.15e+05/1.06e+07 =  6% of the original kernel matrix.

torch.Size([9532, 2])
We keep 1.57e+06/4.70e+07 =  3% of the original kernel matrix.

torch.Size([13904, 2])
We keep 4.07e+06/1.22e+08 =  3% of the original kernel matrix.

torch.Size([15921, 2])
We keep 3.97e+06/1.60e+08 =  2% of the original kernel matrix.

torch.Size([9079, 2])
We keep 2.85e+06/5.12e+07 =  5% of the original kernel matrix.

torch.Size([12561, 2])
We keep 2.84e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([7886, 2])
We keep 1.37e+06/2.85e+07 =  4% of the original kernel matrix.

torch.Size([11607, 2])
We keep 2.25e+06/7.72e+07 =  2% of the original kernel matrix.

torch.Size([6154, 2])
We keep 8.80e+05/1.47e+07 =  5% of the original kernel matrix.

torch.Size([10125, 2])
We keep 1.78e+06/5.54e+07 =  3% of the original kernel matrix.

torch.Size([98008, 2])
We keep 1.22e+08/7.25e+09 =  1% of the original kernel matrix.

torch.Size([41477, 2])
We keep 2.19e+07/1.23e+09 =  1% of the original kernel matrix.

torch.Size([164954, 2])
We keep 5.04e+08/2.12e+10 =  2% of the original kernel matrix.

torch.Size([54832, 2])
We keep 3.54e+07/2.11e+09 =  1% of the original kernel matrix.

torch.Size([143983, 2])
We keep 5.17e+08/1.86e+10 =  2% of the original kernel matrix.

torch.Size([50889, 2])
We keep 3.32e+07/1.97e+09 =  1% of the original kernel matrix.

torch.Size([133256, 2])
We keep 6.99e+08/1.87e+10 =  3% of the original kernel matrix.

torch.Size([48145, 2])
We keep 3.42e+07/1.98e+09 =  1% of the original kernel matrix.

torch.Size([23215, 2])
We keep 1.50e+07/4.99e+08 =  3% of the original kernel matrix.

torch.Size([20297, 2])
We keep 7.25e+06/3.23e+08 =  2% of the original kernel matrix.

torch.Size([10218, 2])
We keep 2.75e+06/5.23e+07 =  5% of the original kernel matrix.

torch.Size([13049, 2])
We keep 2.86e+06/1.05e+08 =  2% of the original kernel matrix.

torch.Size([13538, 2])
We keep 1.44e+07/1.70e+08 =  8% of the original kernel matrix.

torch.Size([16301, 2])
We keep 4.76e+06/1.88e+08 =  2% of the original kernel matrix.

torch.Size([121927, 2])
We keep 1.82e+08/1.13e+10 =  1% of the original kernel matrix.

torch.Size([46408, 2])
We keep 2.68e+07/1.54e+09 =  1% of the original kernel matrix.

torch.Size([9867, 2])
We keep 3.82e+06/6.71e+07 =  5% of the original kernel matrix.

torch.Size([14288, 2])
We keep 3.23e+06/1.18e+08 =  2% of the original kernel matrix.

torch.Size([20485, 2])
We keep 1.21e+07/3.29e+08 =  3% of the original kernel matrix.

torch.Size([18861, 2])
We keep 5.99e+06/2.62e+08 =  2% of the original kernel matrix.

torch.Size([9302, 2])
We keep 2.22e+06/4.47e+07 =  4% of the original kernel matrix.

torch.Size([12502, 2])
We keep 2.72e+06/9.66e+07 =  2% of the original kernel matrix.

torch.Size([202243, 2])
We keep 3.31e+09/7.26e+10 =  4% of the original kernel matrix.

torch.Size([60073, 2])
We keep 6.16e+07/3.89e+09 =  1% of the original kernel matrix.

torch.Size([56087, 2])
We keep 1.01e+08/2.50e+09 =  4% of the original kernel matrix.

torch.Size([30433, 2])
We keep 1.42e+07/7.23e+08 =  1% of the original kernel matrix.

torch.Size([17335, 2])
We keep 1.44e+07/2.65e+08 =  5% of the original kernel matrix.

torch.Size([17916, 2])
We keep 5.51e+06/2.35e+08 =  2% of the original kernel matrix.

torch.Size([181257, 2])
We keep 1.47e+09/2.71e+10 =  5% of the original kernel matrix.

torch.Size([57665, 2])
We keep 4.00e+07/2.38e+09 =  1% of the original kernel matrix.

torch.Size([37512, 2])
We keep 2.53e+07/1.01e+09 =  2% of the original kernel matrix.

torch.Size([25412, 2])
We keep 9.49e+06/4.59e+08 =  2% of the original kernel matrix.

torch.Size([24181, 2])
We keep 1.79e+07/4.45e+08 =  4% of the original kernel matrix.

torch.Size([20554, 2])
We keep 6.71e+06/3.05e+08 =  2% of the original kernel matrix.

torch.Size([149592, 2])
We keep 7.14e+08/2.56e+10 =  2% of the original kernel matrix.

torch.Size([51888, 2])
We keep 3.94e+07/2.31e+09 =  1% of the original kernel matrix.

torch.Size([91719, 2])
We keep 1.33e+08/7.76e+09 =  1% of the original kernel matrix.

torch.Size([40181, 2])
We keep 2.28e+07/1.27e+09 =  1% of the original kernel matrix.

torch.Size([77164, 2])
We keep 1.85e+08/5.02e+09 =  3% of the original kernel matrix.

torch.Size([36181, 2])
We keep 1.91e+07/1.02e+09 =  1% of the original kernel matrix.

torch.Size([14236, 2])
We keep 5.58e+06/1.32e+08 =  4% of the original kernel matrix.

torch.Size([15740, 2])
We keep 4.14e+06/1.66e+08 =  2% of the original kernel matrix.

torch.Size([87309, 2])
We keep 2.39e+08/6.14e+09 =  3% of the original kernel matrix.

torch.Size([38718, 2])
We keep 2.06e+07/1.13e+09 =  1% of the original kernel matrix.

torch.Size([9465, 2])
We keep 2.45e+06/5.03e+07 =  4% of the original kernel matrix.

torch.Size([12926, 2])
We keep 2.80e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([60072, 2])
We keep 6.15e+07/2.57e+09 =  2% of the original kernel matrix.

torch.Size([31507, 2])
We keep 1.39e+07/7.33e+08 =  1% of the original kernel matrix.

torch.Size([411388, 2])
We keep 2.65e+09/1.67e+11 =  1% of the original kernel matrix.

torch.Size([88625, 2])
We keep 9.04e+07/5.90e+09 =  1% of the original kernel matrix.

torch.Size([38778, 2])
We keep 9.10e+07/1.07e+09 =  8% of the original kernel matrix.

torch.Size([25309, 2])
We keep 9.39e+06/4.73e+08 =  1% of the original kernel matrix.

torch.Size([25821, 2])
We keep 1.93e+08/1.77e+09 = 10% of the original kernel matrix.

torch.Size([19648, 2])
We keep 1.18e+07/6.08e+08 =  1% of the original kernel matrix.

torch.Size([9972, 2])
We keep 2.42e+06/5.37e+07 =  4% of the original kernel matrix.

torch.Size([12970, 2])
We keep 2.82e+06/1.06e+08 =  2% of the original kernel matrix.

torch.Size([35779, 2])
We keep 1.13e+08/1.18e+09 =  9% of the original kernel matrix.

torch.Size([24365, 2])
We keep 1.02e+07/4.97e+08 =  2% of the original kernel matrix.

torch.Size([19683, 2])
We keep 2.19e+07/3.71e+08 =  5% of the original kernel matrix.

torch.Size([19281, 2])
We keep 6.29e+06/2.79e+08 =  2% of the original kernel matrix.

torch.Size([7799, 2])
We keep 1.89e+06/3.66e+07 =  5% of the original kernel matrix.

torch.Size([11390, 2])
We keep 2.45e+06/8.75e+07 =  2% of the original kernel matrix.

torch.Size([24161, 2])
We keep 2.61e+07/4.62e+08 =  5% of the original kernel matrix.

torch.Size([20559, 2])
We keep 6.85e+06/3.11e+08 =  2% of the original kernel matrix.

torch.Size([271165, 2])
We keep 6.45e+08/5.11e+10 =  1% of the original kernel matrix.

torch.Size([70199, 2])
We keep 5.21e+07/3.27e+09 =  1% of the original kernel matrix.

torch.Size([10406, 2])
We keep 1.72e+07/1.26e+08 = 13% of the original kernel matrix.

torch.Size([13045, 2])
We keep 4.07e+06/1.62e+08 =  2% of the original kernel matrix.

torch.Size([216357, 2])
We keep 3.48e+09/8.71e+10 =  3% of the original kernel matrix.

torch.Size([59302, 2])
We keep 6.88e+07/4.27e+09 =  1% of the original kernel matrix.

torch.Size([25889, 2])
We keep 3.54e+07/5.80e+08 =  6% of the original kernel matrix.

torch.Size([21131, 2])
We keep 7.46e+06/3.48e+08 =  2% of the original kernel matrix.

torch.Size([36220, 2])
We keep 5.89e+07/1.05e+09 =  5% of the original kernel matrix.

torch.Size([24484, 2])
We keep 9.43e+06/4.69e+08 =  2% of the original kernel matrix.

torch.Size([11892, 2])
We keep 9.06e+06/1.14e+08 =  7% of the original kernel matrix.

torch.Size([14165, 2])
We keep 3.97e+06/1.54e+08 =  2% of the original kernel matrix.

torch.Size([44765, 2])
We keep 4.43e+07/1.70e+09 =  2% of the original kernel matrix.

torch.Size([28125, 2])
We keep 1.19e+07/5.97e+08 =  1% of the original kernel matrix.

torch.Size([166132, 2])
We keep 3.60e+08/2.32e+10 =  1% of the original kernel matrix.

torch.Size([55466, 2])
We keep 3.70e+07/2.20e+09 =  1% of the original kernel matrix.

torch.Size([3410, 2])
We keep 3.33e+05/4.18e+06 =  7% of the original kernel matrix.

torch.Size([7881, 2])
We keep 1.12e+06/2.96e+07 =  3% of the original kernel matrix.

torch.Size([508828, 2])
We keep 1.84e+09/1.77e+11 =  1% of the original kernel matrix.

torch.Size([100723, 2])
We keep 9.24e+07/6.09e+09 =  1% of the original kernel matrix.

torch.Size([30863, 2])
We keep 2.87e+07/7.54e+08 =  3% of the original kernel matrix.

torch.Size([23201, 2])
We keep 8.18e+06/3.97e+08 =  2% of the original kernel matrix.

torch.Size([32313, 2])
We keep 7.10e+07/9.17e+08 =  7% of the original kernel matrix.

torch.Size([23329, 2])
We keep 8.93e+06/4.38e+08 =  2% of the original kernel matrix.

torch.Size([6833, 2])
We keep 1.28e+06/2.50e+07 =  5% of the original kernel matrix.

torch.Size([11138, 2])
We keep 2.09e+06/7.23e+07 =  2% of the original kernel matrix.

torch.Size([15449, 2])
We keep 6.64e+06/1.85e+08 =  3% of the original kernel matrix.

torch.Size([16543, 2])
We keep 4.72e+06/1.97e+08 =  2% of the original kernel matrix.

torch.Size([4397, 2])
We keep 6.24e+05/7.49e+06 =  8% of the original kernel matrix.

torch.Size([8735, 2])
We keep 1.37e+06/3.96e+07 =  3% of the original kernel matrix.

torch.Size([2826, 2])
We keep 2.11e+05/2.19e+06 =  9% of the original kernel matrix.

torch.Size([7396, 2])
We keep 8.74e+05/2.14e+07 =  4% of the original kernel matrix.

torch.Size([13090, 2])
We keep 8.43e+06/1.06e+08 =  7% of the original kernel matrix.

torch.Size([15101, 2])
We keep 3.74e+06/1.49e+08 =  2% of the original kernel matrix.

torch.Size([52859, 2])
We keep 1.22e+08/3.12e+09 =  3% of the original kernel matrix.

torch.Size([29361, 2])
We keep 1.57e+07/8.07e+08 =  1% of the original kernel matrix.

torch.Size([904457, 2])
We keep 3.45e+10/1.65e+12 =  2% of the original kernel matrix.

torch.Size([121756, 2])
We keep 2.64e+08/1.86e+10 =  1% of the original kernel matrix.

torch.Size([36335, 2])
We keep 4.08e+08/2.02e+09 = 20% of the original kernel matrix.

torch.Size([23755, 2])
We keep 1.28e+07/6.50e+08 =  1% of the original kernel matrix.

torch.Size([87280, 2])
We keep 1.42e+08/5.39e+09 =  2% of the original kernel matrix.

torch.Size([38787, 2])
We keep 1.90e+07/1.06e+09 =  1% of the original kernel matrix.

torch.Size([7786, 2])
We keep 2.20e+06/3.15e+07 =  7% of the original kernel matrix.

torch.Size([11556, 2])
We keep 2.36e+06/8.11e+07 =  2% of the original kernel matrix.

torch.Size([31984, 2])
We keep 2.20e+07/7.65e+08 =  2% of the original kernel matrix.

torch.Size([23284, 2])
We keep 8.43e+06/4.00e+08 =  2% of the original kernel matrix.

torch.Size([11045, 2])
We keep 1.31e+07/1.02e+08 = 12% of the original kernel matrix.

torch.Size([13620, 2])
We keep 3.77e+06/1.46e+08 =  2% of the original kernel matrix.

torch.Size([75556, 2])
We keep 2.37e+08/5.46e+09 =  4% of the original kernel matrix.

torch.Size([35679, 2])
We keep 1.93e+07/1.07e+09 =  1% of the original kernel matrix.

torch.Size([9166, 2])
We keep 2.07e+06/3.92e+07 =  5% of the original kernel matrix.

torch.Size([12449, 2])
We keep 2.54e+06/9.06e+07 =  2% of the original kernel matrix.

torch.Size([754854, 2])
We keep 4.46e+09/3.94e+11 =  1% of the original kernel matrix.

torch.Size([120378, 2])
We keep 1.33e+08/9.08e+09 =  1% of the original kernel matrix.

torch.Size([7496, 2])
We keep 1.67e+06/2.56e+07 =  6% of the original kernel matrix.

torch.Size([11293, 2])
We keep 2.19e+06/7.32e+07 =  2% of the original kernel matrix.

torch.Size([87233, 2])
We keep 2.01e+08/5.48e+09 =  3% of the original kernel matrix.

torch.Size([39049, 2])
We keep 1.93e+07/1.07e+09 =  1% of the original kernel matrix.

torch.Size([13240, 2])
We keep 5.02e+06/1.11e+08 =  4% of the original kernel matrix.

torch.Size([15416, 2])
We keep 3.79e+06/1.52e+08 =  2% of the original kernel matrix.

torch.Size([53124, 2])
We keep 1.09e+08/2.77e+09 =  3% of the original kernel matrix.

torch.Size([29604, 2])
We keep 1.46e+07/7.61e+08 =  1% of the original kernel matrix.

torch.Size([256485, 2])
We keep 7.40e+08/5.11e+10 =  1% of the original kernel matrix.

torch.Size([68098, 2])
We keep 5.23e+07/3.27e+09 =  1% of the original kernel matrix.

torch.Size([22957, 2])
We keep 1.06e+07/3.86e+08 =  2% of the original kernel matrix.

torch.Size([19980, 2])
We keep 6.33e+06/2.84e+08 =  2% of the original kernel matrix.

torch.Size([8203, 2])
We keep 2.41e+06/3.74e+07 =  6% of the original kernel matrix.

torch.Size([11820, 2])
We keep 2.50e+06/8.84e+07 =  2% of the original kernel matrix.

torch.Size([59114, 2])
We keep 5.12e+07/2.30e+09 =  2% of the original kernel matrix.

torch.Size([31421, 2])
We keep 1.33e+07/6.93e+08 =  1% of the original kernel matrix.

torch.Size([9340, 2])
We keep 2.48e+06/4.82e+07 =  5% of the original kernel matrix.

torch.Size([12703, 2])
We keep 2.75e+06/1.00e+08 =  2% of the original kernel matrix.

torch.Size([7922, 2])
We keep 3.44e+06/3.73e+07 =  9% of the original kernel matrix.

torch.Size([11424, 2])
We keep 2.41e+06/8.84e+07 =  2% of the original kernel matrix.

torch.Size([110060, 2])
We keep 1.86e+08/9.69e+09 =  1% of the original kernel matrix.

torch.Size([43728, 2])
We keep 2.48e+07/1.42e+09 =  1% of the original kernel matrix.

torch.Size([208945, 2])
We keep 3.83e+09/8.27e+10 =  4% of the original kernel matrix.

torch.Size([60601, 2])
We keep 6.61e+07/4.16e+09 =  1% of the original kernel matrix.

torch.Size([27136, 2])
We keep 3.99e+07/6.86e+08 =  5% of the original kernel matrix.

torch.Size([23197, 2])
We keep 8.10e+06/3.79e+08 =  2% of the original kernel matrix.

torch.Size([31539, 2])
We keep 2.39e+07/8.44e+08 =  2% of the original kernel matrix.

torch.Size([22738, 2])
We keep 8.82e+06/4.20e+08 =  2% of the original kernel matrix.

torch.Size([119192, 2])
We keep 6.51e+08/1.85e+10 =  3% of the original kernel matrix.

torch.Size([45313, 2])
We keep 3.39e+07/1.97e+09 =  1% of the original kernel matrix.

torch.Size([86977, 2])
We keep 9.51e+08/1.51e+10 =  6% of the original kernel matrix.

torch.Size([37701, 2])
We keep 3.04e+07/1.78e+09 =  1% of the original kernel matrix.

torch.Size([43305, 2])
We keep 3.52e+07/1.29e+09 =  2% of the original kernel matrix.

torch.Size([26790, 2])
We keep 1.06e+07/5.20e+08 =  2% of the original kernel matrix.

torch.Size([6704, 2])
We keep 1.28e+06/1.81e+07 =  7% of the original kernel matrix.

torch.Size([10933, 2])
We keep 1.90e+06/6.15e+07 =  3% of the original kernel matrix.

torch.Size([84050, 2])
We keep 2.47e+08/6.93e+09 =  3% of the original kernel matrix.

torch.Size([37390, 2])
We keep 2.18e+07/1.20e+09 =  1% of the original kernel matrix.

torch.Size([49270, 2])
We keep 8.32e+07/2.24e+09 =  3% of the original kernel matrix.

torch.Size([28029, 2])
We keep 1.35e+07/6.85e+08 =  1% of the original kernel matrix.

torch.Size([74137, 2])
We keep 1.41e+08/4.45e+09 =  3% of the original kernel matrix.

torch.Size([35365, 2])
We keep 1.80e+07/9.65e+08 =  1% of the original kernel matrix.

torch.Size([22324, 2])
We keep 1.87e+07/3.96e+08 =  4% of the original kernel matrix.

torch.Size([19563, 2])
We keep 6.37e+06/2.88e+08 =  2% of the original kernel matrix.

torch.Size([33800, 2])
We keep 1.22e+08/1.36e+09 =  8% of the original kernel matrix.

torch.Size([23161, 2])
We keep 1.08e+07/5.33e+08 =  2% of the original kernel matrix.

torch.Size([32487, 2])
We keep 7.79e+07/1.16e+09 =  6% of the original kernel matrix.

torch.Size([22875, 2])
We keep 1.02e+07/4.93e+08 =  2% of the original kernel matrix.

torch.Size([255556, 2])
We keep 2.81e+09/9.37e+10 =  2% of the original kernel matrix.

torch.Size([63058, 2])
We keep 7.01e+07/4.43e+09 =  1% of the original kernel matrix.

torch.Size([46984, 2])
We keep 7.78e+07/1.75e+09 =  4% of the original kernel matrix.

torch.Size([27823, 2])
We keep 1.20e+07/6.05e+08 =  1% of the original kernel matrix.

torch.Size([14638, 2])
We keep 6.22e+06/1.72e+08 =  3% of the original kernel matrix.

torch.Size([16200, 2])
We keep 4.71e+06/1.90e+08 =  2% of the original kernel matrix.

torch.Size([4693, 2])
We keep 6.94e+05/9.01e+06 =  7% of the original kernel matrix.

torch.Size([8937, 2])
We keep 1.47e+06/4.34e+07 =  3% of the original kernel matrix.

torch.Size([14065, 2])
We keep 7.28e+06/1.34e+08 =  5% of the original kernel matrix.

torch.Size([15758, 2])
We keep 4.19e+06/1.67e+08 =  2% of the original kernel matrix.

torch.Size([65016, 2])
We keep 1.58e+08/4.01e+09 =  3% of the original kernel matrix.

torch.Size([32637, 2])
We keep 1.74e+07/9.15e+08 =  1% of the original kernel matrix.

torch.Size([89378, 2])
We keep 2.52e+08/7.19e+09 =  3% of the original kernel matrix.

torch.Size([38752, 2])
We keep 2.23e+07/1.23e+09 =  1% of the original kernel matrix.

torch.Size([175191, 2])
We keep 6.62e+08/2.48e+10 =  2% of the original kernel matrix.

torch.Size([56628, 2])
We keep 3.86e+07/2.28e+09 =  1% of the original kernel matrix.

torch.Size([7587, 2])
We keep 1.59e+06/2.67e+07 =  5% of the original kernel matrix.

torch.Size([11424, 2])
We keep 2.21e+06/7.47e+07 =  2% of the original kernel matrix.

torch.Size([18919, 2])
We keep 7.48e+06/2.67e+08 =  2% of the original kernel matrix.

torch.Size([18082, 2])
We keep 5.43e+06/2.36e+08 =  2% of the original kernel matrix.

torch.Size([42989, 2])
We keep 4.84e+07/1.35e+09 =  3% of the original kernel matrix.

torch.Size([26670, 2])
We keep 1.08e+07/5.31e+08 =  2% of the original kernel matrix.

torch.Size([38764, 2])
We keep 2.29e+07/1.07e+09 =  2% of the original kernel matrix.

torch.Size([25742, 2])
We keep 9.59e+06/4.73e+08 =  2% of the original kernel matrix.

torch.Size([43133, 2])
We keep 4.34e+07/1.37e+09 =  3% of the original kernel matrix.

torch.Size([26738, 2])
We keep 1.08e+07/5.35e+08 =  2% of the original kernel matrix.

torch.Size([66991, 2])
We keep 7.22e+07/3.46e+09 =  2% of the original kernel matrix.

torch.Size([33782, 2])
We keep 1.59e+07/8.51e+08 =  1% of the original kernel matrix.

torch.Size([7452, 2])
We keep 4.80e+06/3.11e+07 = 15% of the original kernel matrix.

torch.Size([11353, 2])
We keep 1.97e+06/8.06e+07 =  2% of the original kernel matrix.

torch.Size([27939, 2])
We keep 1.50e+07/5.96e+08 =  2% of the original kernel matrix.

torch.Size([22350, 2])
We keep 7.51e+06/3.53e+08 =  2% of the original kernel matrix.

torch.Size([2670, 2])
We keep 3.03e+05/3.06e+06 =  9% of the original kernel matrix.

torch.Size([7387, 2])
We keep 1.01e+06/2.53e+07 =  3% of the original kernel matrix.

torch.Size([6620, 2])
We keep 1.58e+06/2.23e+07 =  7% of the original kernel matrix.

torch.Size([10695, 2])
We keep 2.11e+06/6.83e+07 =  3% of the original kernel matrix.

torch.Size([132206, 2])
We keep 2.02e+08/1.41e+10 =  1% of the original kernel matrix.

torch.Size([49272, 2])
We keep 2.91e+07/1.72e+09 =  1% of the original kernel matrix.

torch.Size([33146, 2])
We keep 2.96e+07/9.22e+08 =  3% of the original kernel matrix.

torch.Size([23277, 2])
We keep 9.13e+06/4.39e+08 =  2% of the original kernel matrix.

torch.Size([70121, 2])
We keep 6.35e+07/3.43e+09 =  1% of the original kernel matrix.

torch.Size([34640, 2])
We keep 1.57e+07/8.47e+08 =  1% of the original kernel matrix.

torch.Size([3088, 2])
We keep 2.25e+05/3.01e+06 =  7% of the original kernel matrix.

torch.Size([7658, 2])
We keep 9.66e+05/2.51e+07 =  3% of the original kernel matrix.

torch.Size([5188, 2])
We keep 1.11e+06/1.03e+07 = 10% of the original kernel matrix.

torch.Size([9648, 2])
We keep 1.58e+06/4.64e+07 =  3% of the original kernel matrix.

torch.Size([80319, 2])
We keep 1.92e+08/5.48e+09 =  3% of the original kernel matrix.

torch.Size([36845, 2])
We keep 1.94e+07/1.07e+09 =  1% of the original kernel matrix.

torch.Size([74279, 2])
We keep 1.04e+08/4.37e+09 =  2% of the original kernel matrix.

torch.Size([35287, 2])
We keep 1.77e+07/9.55e+08 =  1% of the original kernel matrix.

torch.Size([35260, 2])
We keep 2.47e+07/9.21e+08 =  2% of the original kernel matrix.

torch.Size([24613, 2])
We keep 9.14e+06/4.39e+08 =  2% of the original kernel matrix.

torch.Size([5285, 2])
We keep 7.84e+05/1.16e+07 =  6% of the original kernel matrix.

torch.Size([9735, 2])
We keep 1.61e+06/4.91e+07 =  3% of the original kernel matrix.

torch.Size([6259, 2])
We keep 1.23e+06/1.71e+07 =  7% of the original kernel matrix.

torch.Size([10306, 2])
We keep 1.86e+06/5.98e+07 =  3% of the original kernel matrix.

torch.Size([18786, 2])
We keep 1.09e+07/2.90e+08 =  3% of the original kernel matrix.

torch.Size([18420, 2])
We keep 5.70e+06/2.46e+08 =  2% of the original kernel matrix.

torch.Size([24180, 2])
We keep 1.92e+07/4.40e+08 =  4% of the original kernel matrix.

torch.Size([20274, 2])
We keep 6.77e+06/3.03e+08 =  2% of the original kernel matrix.

torch.Size([53464, 2])
We keep 1.58e+08/2.40e+09 =  6% of the original kernel matrix.

torch.Size([29862, 2])
We keep 1.37e+07/7.08e+08 =  1% of the original kernel matrix.

torch.Size([9476, 2])
We keep 4.04e+06/5.85e+07 =  6% of the original kernel matrix.

torch.Size([12731, 2])
We keep 3.04e+06/1.11e+08 =  2% of the original kernel matrix.

torch.Size([4758, 2])
We keep 9.39e+05/1.12e+07 =  8% of the original kernel matrix.

torch.Size([9020, 2])
We keep 1.51e+06/4.84e+07 =  3% of the original kernel matrix.

torch.Size([11820, 2])
We keep 7.90e+06/1.09e+08 =  7% of the original kernel matrix.

torch.Size([14107, 2])
We keep 3.82e+06/1.51e+08 =  2% of the original kernel matrix.

torch.Size([10320, 2])
We keep 7.14e+06/7.28e+07 =  9% of the original kernel matrix.

torch.Size([13150, 2])
We keep 3.25e+06/1.23e+08 =  2% of the original kernel matrix.

torch.Size([145831, 2])
We keep 4.28e+08/2.03e+10 =  2% of the original kernel matrix.

torch.Size([52001, 2])
We keep 3.36e+07/2.06e+09 =  1% of the original kernel matrix.

torch.Size([7107, 2])
We keep 1.33e+06/2.35e+07 =  5% of the original kernel matrix.

torch.Size([11078, 2])
We keep 2.04e+06/7.01e+07 =  2% of the original kernel matrix.

torch.Size([26724, 2])
We keep 1.80e+07/5.38e+08 =  3% of the original kernel matrix.

torch.Size([21378, 2])
We keep 7.28e+06/3.35e+08 =  2% of the original kernel matrix.

torch.Size([81871, 2])
We keep 1.48e+08/6.69e+09 =  2% of the original kernel matrix.

torch.Size([37887, 2])
We keep 2.19e+07/1.18e+09 =  1% of the original kernel matrix.

torch.Size([36074, 2])
We keep 7.82e+07/9.71e+08 =  8% of the original kernel matrix.

torch.Size([24667, 2])
We keep 9.02e+06/4.51e+08 =  2% of the original kernel matrix.

torch.Size([76116, 2])
We keep 1.53e+08/4.94e+09 =  3% of the original kernel matrix.

torch.Size([36173, 2])
We keep 1.86e+07/1.02e+09 =  1% of the original kernel matrix.

torch.Size([124522, 2])
We keep 2.14e+08/1.26e+10 =  1% of the original kernel matrix.

torch.Size([47236, 2])
We keep 2.82e+07/1.62e+09 =  1% of the original kernel matrix.

torch.Size([4990, 2])
We keep 6.22e+05/9.42e+06 =  6% of the original kernel matrix.

torch.Size([9329, 2])
We keep 1.49e+06/4.44e+07 =  3% of the original kernel matrix.

torch.Size([73384, 2])
We keep 1.32e+08/4.10e+09 =  3% of the original kernel matrix.

torch.Size([35469, 2])
We keep 1.74e+07/9.26e+08 =  1% of the original kernel matrix.

torch.Size([25438, 2])
We keep 1.32e+07/4.91e+08 =  2% of the original kernel matrix.

torch.Size([21347, 2])
We keep 6.97e+06/3.20e+08 =  2% of the original kernel matrix.

torch.Size([259534, 2])
We keep 1.65e+09/5.36e+10 =  3% of the original kernel matrix.

torch.Size([67993, 2])
We keep 5.40e+07/3.35e+09 =  1% of the original kernel matrix.

torch.Size([117182, 2])
We keep 2.07e+08/1.20e+10 =  1% of the original kernel matrix.

torch.Size([45376, 2])
We keep 2.74e+07/1.59e+09 =  1% of the original kernel matrix.

torch.Size([25496, 2])
We keep 3.69e+07/6.64e+08 =  5% of the original kernel matrix.

torch.Size([20214, 2])
We keep 8.03e+06/3.73e+08 =  2% of the original kernel matrix.

torch.Size([72512, 2])
We keep 1.10e+08/3.87e+09 =  2% of the original kernel matrix.

torch.Size([35385, 2])
We keep 1.69e+07/9.00e+08 =  1% of the original kernel matrix.

torch.Size([11162, 2])
We keep 2.56e+06/7.19e+07 =  3% of the original kernel matrix.

torch.Size([13828, 2])
We keep 3.22e+06/1.23e+08 =  2% of the original kernel matrix.

torch.Size([8807, 2])
We keep 8.20e+06/6.90e+07 = 11% of the original kernel matrix.

torch.Size([11926, 2])
We keep 3.07e+06/1.20e+08 =  2% of the original kernel matrix.

torch.Size([5998, 2])
We keep 1.01e+06/1.55e+07 =  6% of the original kernel matrix.

torch.Size([10182, 2])
We keep 1.81e+06/5.70e+07 =  3% of the original kernel matrix.

torch.Size([16798, 2])
We keep 7.88e+06/2.08e+08 =  3% of the original kernel matrix.

torch.Size([17023, 2])
We keep 4.95e+06/2.09e+08 =  2% of the original kernel matrix.

torch.Size([81190, 2])
We keep 9.85e+07/4.93e+09 =  1% of the original kernel matrix.

torch.Size([37555, 2])
We keep 1.85e+07/1.02e+09 =  1% of the original kernel matrix.

torch.Size([70769, 2])
We keep 2.74e+08/5.76e+09 =  4% of the original kernel matrix.

torch.Size([34949, 2])
We keep 2.05e+07/1.10e+09 =  1% of the original kernel matrix.

torch.Size([9153, 2])
We keep 1.08e+07/7.59e+07 = 14% of the original kernel matrix.

torch.Size([12364, 2])
We keep 3.40e+06/1.26e+08 =  2% of the original kernel matrix.

torch.Size([22941, 2])
We keep 1.11e+07/3.99e+08 =  2% of the original kernel matrix.

torch.Size([20213, 2])
We keep 6.28e+06/2.89e+08 =  2% of the original kernel matrix.

torch.Size([11127, 2])
We keep 2.97e+06/6.99e+07 =  4% of the original kernel matrix.

torch.Size([14000, 2])
We keep 3.20e+06/1.21e+08 =  2% of the original kernel matrix.

torch.Size([246901, 2])
We keep 9.63e+08/5.70e+10 =  1% of the original kernel matrix.

torch.Size([66460, 2])
We keep 5.58e+07/3.45e+09 =  1% of the original kernel matrix.

torch.Size([26606, 2])
We keep 1.29e+07/5.45e+08 =  2% of the original kernel matrix.

torch.Size([21647, 2])
We keep 7.24e+06/3.38e+08 =  2% of the original kernel matrix.

torch.Size([11341, 2])
We keep 3.31e+06/6.94e+07 =  4% of the original kernel matrix.

torch.Size([13888, 2])
We keep 3.11e+06/1.20e+08 =  2% of the original kernel matrix.

torch.Size([56979, 2])
We keep 1.04e+08/2.68e+09 =  3% of the original kernel matrix.

torch.Size([30984, 2])
We keep 1.42e+07/7.49e+08 =  1% of the original kernel matrix.

torch.Size([87393, 2])
We keep 1.15e+08/5.86e+09 =  1% of the original kernel matrix.

torch.Size([39101, 2])
We keep 2.01e+07/1.11e+09 =  1% of the original kernel matrix.

torch.Size([8899, 2])
We keep 2.63e+06/4.44e+07 =  5% of the original kernel matrix.

torch.Size([12148, 2])
We keep 2.73e+06/9.63e+07 =  2% of the original kernel matrix.

torch.Size([55897, 2])
We keep 9.74e+07/2.31e+09 =  4% of the original kernel matrix.

torch.Size([30541, 2])
We keep 1.36e+07/6.95e+08 =  1% of the original kernel matrix.

torch.Size([21624, 2])
We keep 2.66e+07/3.81e+08 =  6% of the original kernel matrix.

torch.Size([19546, 2])
We keep 6.43e+06/2.82e+08 =  2% of the original kernel matrix.

time for making ranges is 2.4012439250946045
Sorting X and nu_X
time for sorting X is 0.08286237716674805
Sorting Z and nu_Z
time for sorting Z is 0.0002880096435546875
Starting Optim
sum tnu_Z before tensor(29741060., device='cuda:0')
c= tensor(2138.3501, device='cuda:0')
c= tensor(193322.8125, device='cuda:0')
c= tensor(197760.5000, device='cuda:0')
c= tensor(201870.3750, device='cuda:0')
c= tensor(1571905.3750, device='cuda:0')
c= tensor(2063262.6250, device='cuda:0')
c= tensor(2804988., device='cuda:0')
c= tensor(3558676.5000, device='cuda:0')
c= tensor(3677221.2500, device='cuda:0')
c= tensor(13978417., device='cuda:0')
c= tensor(14006426., device='cuda:0')
c= tensor(21031244., device='cuda:0')
c= tensor(21056934., device='cuda:0')
c= tensor(27351940., device='cuda:0')
c= tensor(27515694., device='cuda:0')
c= tensor(28811938., device='cuda:0')
c= tensor(29229446., device='cuda:0')
c= tensor(29656542., device='cuda:0')
c= tensor(1.2930e+08, device='cuda:0')
c= tensor(1.3187e+08, device='cuda:0')
c= tensor(1.3212e+08, device='cuda:0')
c= tensor(1.9101e+08, device='cuda:0')
c= tensor(1.9118e+08, device='cuda:0')
c= tensor(1.9142e+08, device='cuda:0')
c= tensor(1.9216e+08, device='cuda:0')
c= tensor(1.9307e+08, device='cuda:0')
c= tensor(1.9415e+08, device='cuda:0')
c= tensor(1.9418e+08, device='cuda:0')
c= tensor(1.9477e+08, device='cuda:0')
c= tensor(9.2778e+08, device='cuda:0')
c= tensor(9.2785e+08, device='cuda:0')
c= tensor(9.8105e+08, device='cuda:0')
c= tensor(9.8114e+08, device='cuda:0')
c= tensor(9.8116e+08, device='cuda:0')
c= tensor(9.8123e+08, device='cuda:0')
c= tensor(9.8577e+08, device='cuda:0')
c= tensor(9.8695e+08, device='cuda:0')
c= tensor(9.8695e+08, device='cuda:0')
c= tensor(9.8696e+08, device='cuda:0')
c= tensor(9.8697e+08, device='cuda:0')
c= tensor(9.8699e+08, device='cuda:0')
c= tensor(9.8699e+08, device='cuda:0')
c= tensor(9.8699e+08, device='cuda:0')
c= tensor(9.8700e+08, device='cuda:0')
c= tensor(9.8700e+08, device='cuda:0')
c= tensor(9.8700e+08, device='cuda:0')
c= tensor(9.8706e+08, device='cuda:0')
c= tensor(9.8707e+08, device='cuda:0')
c= tensor(9.8707e+08, device='cuda:0')
c= tensor(9.8713e+08, device='cuda:0')
c= tensor(9.8721e+08, device='cuda:0')
c= tensor(9.8721e+08, device='cuda:0')
c= tensor(9.8724e+08, device='cuda:0')
c= tensor(9.8724e+08, device='cuda:0')
c= tensor(9.8726e+08, device='cuda:0')
c= tensor(9.8728e+08, device='cuda:0')
c= tensor(9.8730e+08, device='cuda:0')
c= tensor(9.8730e+08, device='cuda:0')
c= tensor(9.8731e+08, device='cuda:0')
c= tensor(9.8731e+08, device='cuda:0')
c= tensor(9.8733e+08, device='cuda:0')
c= tensor(9.8734e+08, device='cuda:0')
c= tensor(9.8737e+08, device='cuda:0')
c= tensor(9.8739e+08, device='cuda:0')
c= tensor(9.8740e+08, device='cuda:0')
c= tensor(9.8740e+08, device='cuda:0')
c= tensor(9.8741e+08, device='cuda:0')
c= tensor(9.8745e+08, device='cuda:0')
c= tensor(9.8746e+08, device='cuda:0')
c= tensor(9.8746e+08, device='cuda:0')
c= tensor(9.8748e+08, device='cuda:0')
c= tensor(9.8749e+08, device='cuda:0')
c= tensor(9.8749e+08, device='cuda:0')
c= tensor(9.8750e+08, device='cuda:0')
c= tensor(9.8750e+08, device='cuda:0')
c= tensor(9.8751e+08, device='cuda:0')
c= tensor(9.8752e+08, device='cuda:0')
c= tensor(9.8752e+08, device='cuda:0')
c= tensor(9.8753e+08, device='cuda:0')
c= tensor(9.8759e+08, device='cuda:0')
c= tensor(9.8760e+08, device='cuda:0')
c= tensor(9.8760e+08, device='cuda:0')
c= tensor(9.8762e+08, device='cuda:0')
c= tensor(9.8762e+08, device='cuda:0')
c= tensor(9.8762e+08, device='cuda:0')
c= tensor(9.8762e+08, device='cuda:0')
c= tensor(9.8763e+08, device='cuda:0')
c= tensor(9.8763e+08, device='cuda:0')
c= tensor(9.8764e+08, device='cuda:0')
c= tensor(9.8764e+08, device='cuda:0')
c= tensor(9.8765e+08, device='cuda:0')
c= tensor(9.8766e+08, device='cuda:0')
c= tensor(9.8766e+08, device='cuda:0')
c= tensor(9.8767e+08, device='cuda:0')
c= tensor(9.8767e+08, device='cuda:0')
c= tensor(9.8768e+08, device='cuda:0')
c= tensor(9.8769e+08, device='cuda:0')
c= tensor(9.8780e+08, device='cuda:0')
c= tensor(9.8781e+08, device='cuda:0')
c= tensor(9.8782e+08, device='cuda:0')
c= tensor(9.8786e+08, device='cuda:0')
c= tensor(9.8787e+08, device='cuda:0')
c= tensor(9.8788e+08, device='cuda:0')
c= tensor(9.8788e+08, device='cuda:0')
c= tensor(9.8789e+08, device='cuda:0')
c= tensor(9.8789e+08, device='cuda:0')
c= tensor(9.8791e+08, device='cuda:0')
c= tensor(9.8791e+08, device='cuda:0')
c= tensor(9.8792e+08, device='cuda:0')
c= tensor(9.8792e+08, device='cuda:0')
c= tensor(9.8792e+08, device='cuda:0')
c= tensor(9.8793e+08, device='cuda:0')
c= tensor(9.8793e+08, device='cuda:0')
c= tensor(9.8794e+08, device='cuda:0')
c= tensor(9.8794e+08, device='cuda:0')
c= tensor(9.8795e+08, device='cuda:0')
c= tensor(9.8796e+08, device='cuda:0')
c= tensor(9.8796e+08, device='cuda:0')
c= tensor(9.8798e+08, device='cuda:0')
c= tensor(9.8798e+08, device='cuda:0')
c= tensor(9.8804e+08, device='cuda:0')
c= tensor(9.8805e+08, device='cuda:0')
c= tensor(9.8805e+08, device='cuda:0')
c= tensor(9.8805e+08, device='cuda:0')
c= tensor(9.8806e+08, device='cuda:0')
c= tensor(9.8806e+08, device='cuda:0')
c= tensor(9.8806e+08, device='cuda:0')
c= tensor(9.8807e+08, device='cuda:0')
c= tensor(9.8814e+08, device='cuda:0')
c= tensor(9.8814e+08, device='cuda:0')
c= tensor(9.8816e+08, device='cuda:0')
c= tensor(9.8816e+08, device='cuda:0')
c= tensor(9.8817e+08, device='cuda:0')
c= tensor(9.8817e+08, device='cuda:0')
c= tensor(9.8818e+08, device='cuda:0')
c= tensor(9.8818e+08, device='cuda:0')
c= tensor(9.8818e+08, device='cuda:0')
c= tensor(9.8818e+08, device='cuda:0')
c= tensor(9.8819e+08, device='cuda:0')
c= tensor(9.8819e+08, device='cuda:0')
c= tensor(9.8820e+08, device='cuda:0')
c= tensor(9.8820e+08, device='cuda:0')
c= tensor(9.8822e+08, device='cuda:0')
c= tensor(9.8831e+08, device='cuda:0')
c= tensor(9.8832e+08, device='cuda:0')
c= tensor(9.8832e+08, device='cuda:0')
c= tensor(9.8832e+08, device='cuda:0')
c= tensor(9.8833e+08, device='cuda:0')
c= tensor(9.8833e+08, device='cuda:0')
c= tensor(9.8834e+08, device='cuda:0')
c= tensor(9.8834e+08, device='cuda:0')
c= tensor(9.8835e+08, device='cuda:0')
c= tensor(9.8836e+08, device='cuda:0')
c= tensor(9.8843e+08, device='cuda:0')
c= tensor(9.8844e+08, device='cuda:0')
c= tensor(9.8851e+08, device='cuda:0')
c= tensor(9.8852e+08, device='cuda:0')
c= tensor(9.8852e+08, device='cuda:0')
c= tensor(9.8854e+08, device='cuda:0')
c= tensor(9.8854e+08, device='cuda:0')
c= tensor(9.8855e+08, device='cuda:0')
c= tensor(9.8855e+08, device='cuda:0')
c= tensor(9.8856e+08, device='cuda:0')
c= tensor(9.8856e+08, device='cuda:0')
c= tensor(9.8856e+08, device='cuda:0')
c= tensor(9.8857e+08, device='cuda:0')
c= tensor(9.8858e+08, device='cuda:0')
c= tensor(9.8858e+08, device='cuda:0')
c= tensor(9.8859e+08, device='cuda:0')
c= tensor(9.8859e+08, device='cuda:0')
c= tensor(9.8860e+08, device='cuda:0')
c= tensor(9.8860e+08, device='cuda:0')
c= tensor(9.8862e+08, device='cuda:0')
c= tensor(9.8863e+08, device='cuda:0')
c= tensor(9.8865e+08, device='cuda:0')
c= tensor(9.8865e+08, device='cuda:0')
c= tensor(9.8867e+08, device='cuda:0')
c= tensor(9.8868e+08, device='cuda:0')
c= tensor(9.8870e+08, device='cuda:0')
c= tensor(9.8870e+08, device='cuda:0')
c= tensor(9.8871e+08, device='cuda:0')
c= tensor(9.8871e+08, device='cuda:0')
c= tensor(9.8873e+08, device='cuda:0')
c= tensor(9.8873e+08, device='cuda:0')
c= tensor(9.8875e+08, device='cuda:0')
c= tensor(9.8876e+08, device='cuda:0')
c= tensor(9.8876e+08, device='cuda:0')
c= tensor(9.8877e+08, device='cuda:0')
c= tensor(9.8878e+08, device='cuda:0')
c= tensor(9.8885e+08, device='cuda:0')
c= tensor(9.8885e+08, device='cuda:0')
c= tensor(9.8886e+08, device='cuda:0')
c= tensor(9.8886e+08, device='cuda:0')
c= tensor(9.8887e+08, device='cuda:0')
c= tensor(9.8887e+08, device='cuda:0')
c= tensor(9.8888e+08, device='cuda:0')
c= tensor(9.8888e+08, device='cuda:0')
c= tensor(9.8889e+08, device='cuda:0')
c= tensor(9.8889e+08, device='cuda:0')
c= tensor(9.8889e+08, device='cuda:0')
c= tensor(9.8890e+08, device='cuda:0')
c= tensor(9.8891e+08, device='cuda:0')
c= tensor(9.8894e+08, device='cuda:0')
c= tensor(9.8895e+08, device='cuda:0')
c= tensor(9.8896e+08, device='cuda:0')
c= tensor(9.8896e+08, device='cuda:0')
c= tensor(9.8897e+08, device='cuda:0')
c= tensor(9.8898e+08, device='cuda:0')
c= tensor(9.8898e+08, device='cuda:0')
c= tensor(9.8900e+08, device='cuda:0')
c= tensor(9.8901e+08, device='cuda:0')
c= tensor(9.8902e+08, device='cuda:0')
c= tensor(9.8902e+08, device='cuda:0')
c= tensor(9.8903e+08, device='cuda:0')
c= tensor(9.8904e+08, device='cuda:0')
c= tensor(9.8904e+08, device='cuda:0')
c= tensor(9.8904e+08, device='cuda:0')
c= tensor(9.8904e+08, device='cuda:0')
c= tensor(9.8909e+08, device='cuda:0')
c= tensor(9.8909e+08, device='cuda:0')
c= tensor(9.8910e+08, device='cuda:0')
c= tensor(9.8911e+08, device='cuda:0')
c= tensor(9.8912e+08, device='cuda:0')
c= tensor(9.8913e+08, device='cuda:0')
c= tensor(9.8913e+08, device='cuda:0')
c= tensor(9.8914e+08, device='cuda:0')
c= tensor(9.8915e+08, device='cuda:0')
c= tensor(9.8915e+08, device='cuda:0')
c= tensor(9.8916e+08, device='cuda:0')
c= tensor(9.8916e+08, device='cuda:0')
c= tensor(9.8916e+08, device='cuda:0')
c= tensor(9.8916e+08, device='cuda:0')
c= tensor(9.8917e+08, device='cuda:0')
c= tensor(9.8918e+08, device='cuda:0')
c= tensor(9.8919e+08, device='cuda:0')
c= tensor(9.8919e+08, device='cuda:0')
c= tensor(9.8920e+08, device='cuda:0')
c= tensor(9.8923e+08, device='cuda:0')
c= tensor(9.8923e+08, device='cuda:0')
c= tensor(9.8926e+08, device='cuda:0')
c= tensor(9.9028e+08, device='cuda:0')
c= tensor(9.9047e+08, device='cuda:0')
c= tensor(9.9050e+08, device='cuda:0')
c= tensor(9.9050e+08, device='cuda:0')
c= tensor(9.9051e+08, device='cuda:0')
c= tensor(9.9181e+08, device='cuda:0')
c= tensor(1.0162e+09, device='cuda:0')
c= tensor(1.0162e+09, device='cuda:0')
c= tensor(1.0184e+09, device='cuda:0')
c= tensor(1.0189e+09, device='cuda:0')
c= tensor(1.0194e+09, device='cuda:0')
c= tensor(1.0937e+09, device='cuda:0')
c= tensor(1.0937e+09, device='cuda:0')
c= tensor(1.0937e+09, device='cuda:0')
c= tensor(1.1136e+09, device='cuda:0')
c= tensor(1.2352e+09, device='cuda:0')
c= tensor(1.2352e+09, device='cuda:0')
c= tensor(1.2354e+09, device='cuda:0')
c= tensor(1.2355e+09, device='cuda:0')
c= tensor(1.2440e+09, device='cuda:0')
c= tensor(1.2447e+09, device='cuda:0')
c= tensor(1.2521e+09, device='cuda:0')
c= tensor(1.2532e+09, device='cuda:0')
c= tensor(1.2533e+09, device='cuda:0')
c= tensor(1.2534e+09, device='cuda:0')
c= tensor(1.3363e+09, device='cuda:0')
c= tensor(1.3363e+09, device='cuda:0')
c= tensor(1.3363e+09, device='cuda:0')
c= tensor(1.3374e+09, device='cuda:0')
c= tensor(1.3378e+09, device='cuda:0')
c= tensor(1.3526e+09, device='cuda:0')
c= tensor(1.3566e+09, device='cuda:0')
c= tensor(1.3566e+09, device='cuda:0')
c= tensor(1.3567e+09, device='cuda:0')
c= tensor(1.3567e+09, device='cuda:0')
c= tensor(1.3570e+09, device='cuda:0')
c= tensor(1.3600e+09, device='cuda:0')
c= tensor(1.3620e+09, device='cuda:0')
c= tensor(1.3635e+09, device='cuda:0')
c= tensor(1.3635e+09, device='cuda:0')
c= tensor(1.3635e+09, device='cuda:0')
c= tensor(1.3645e+09, device='cuda:0')
c= tensor(1.3755e+09, device='cuda:0')
c= tensor(1.3762e+09, device='cuda:0')
c= tensor(1.3762e+09, device='cuda:0')
c= tensor(1.3995e+09, device='cuda:0')
c= tensor(1.3997e+09, device='cuda:0')
c= tensor(1.3999e+09, device='cuda:0')
c= tensor(1.4019e+09, device='cuda:0')
c= tensor(1.4019e+09, device='cuda:0')
c= tensor(1.4032e+09, device='cuda:0')
c= tensor(1.4202e+09, device='cuda:0')
c= tensor(1.4529e+09, device='cuda:0')
c= tensor(1.4530e+09, device='cuda:0')
c= tensor(1.4531e+09, device='cuda:0')
c= tensor(1.4534e+09, device='cuda:0')
c= tensor(1.4534e+09, device='cuda:0')
c= tensor(1.4539e+09, device='cuda:0')
c= tensor(1.4540e+09, device='cuda:0')
c= tensor(1.4543e+09, device='cuda:0')
c= tensor(1.4632e+09, device='cuda:0')
c= tensor(1.4697e+09, device='cuda:0')
c= tensor(1.4697e+09, device='cuda:0')
c= tensor(1.4697e+09, device='cuda:0')
c= tensor(1.4717e+09, device='cuda:0')
c= tensor(1.4719e+09, device='cuda:0')
c= tensor(1.4721e+09, device='cuda:0')
c= tensor(1.4721e+09, device='cuda:0')
c= tensor(1.5144e+09, device='cuda:0')
c= tensor(1.5144e+09, device='cuda:0')
c= tensor(1.6614e+09, device='cuda:0')
c= tensor(1.6614e+09, device='cuda:0')
c= tensor(1.6622e+09, device='cuda:0')
c= tensor(1.6627e+09, device='cuda:0')
c= tensor(1.7056e+09, device='cuda:0')
c= tensor(1.7069e+09, device='cuda:0')
c= tensor(1.7069e+09, device='cuda:0')
c= tensor(1.7100e+09, device='cuda:0')
c= tensor(1.7136e+09, device='cuda:0')
c= tensor(1.7136e+09, device='cuda:0')
c= tensor(1.7150e+09, device='cuda:0')
c= tensor(1.7210e+09, device='cuda:0')
c= tensor(1.7379e+09, device='cuda:0')
c= tensor(1.7401e+09, device='cuda:0')
c= tensor(1.7401e+09, device='cuda:0')
c= tensor(1.7403e+09, device='cuda:0')
c= tensor(1.7538e+09, device='cuda:0')
c= tensor(1.7962e+09, device='cuda:0')
c= tensor(1.7963e+09, device='cuda:0')
c= tensor(1.7963e+09, device='cuda:0')
c= tensor(1.8018e+09, device='cuda:0')
c= tensor(1.8097e+09, device='cuda:0')
c= tensor(1.8119e+09, device='cuda:0')
c= tensor(1.8119e+09, device='cuda:0')
c= tensor(1.8122e+09, device='cuda:0')
c= tensor(1.8122e+09, device='cuda:0')
c= tensor(1.8123e+09, device='cuda:0')
c= tensor(1.8123e+09, device='cuda:0')
c= tensor(1.8123e+09, device='cuda:0')
c= tensor(1.8146e+09, device='cuda:0')
c= tensor(1.8150e+09, device='cuda:0')
c= tensor(1.8152e+09, device='cuda:0')
c= tensor(1.8152e+09, device='cuda:0')
c= tensor(1.8153e+09, device='cuda:0')
c= tensor(1.9219e+09, device='cuda:0')
c= tensor(1.9220e+09, device='cuda:0')
c= tensor(1.9257e+09, device='cuda:0')
c= tensor(1.9257e+09, device='cuda:0')
c= tensor(1.9257e+09, device='cuda:0')
c= tensor(1.9257e+09, device='cuda:0')
c= tensor(1.9262e+09, device='cuda:0')
c= tensor(1.9262e+09, device='cuda:0')
c= tensor(1.9280e+09, device='cuda:0')
c= tensor(1.9280e+09, device='cuda:0')
c= tensor(1.9280e+09, device='cuda:0')
c= tensor(1.9547e+09, device='cuda:0')
c= tensor(1.9576e+09, device='cuda:0')
c= tensor(1.9579e+09, device='cuda:0')
c= tensor(1.9621e+09, device='cuda:0')
c= tensor(1.9706e+09, device='cuda:0')
c= tensor(1.9706e+09, device='cuda:0')
c= tensor(1.9706e+09, device='cuda:0')
c= tensor(1.9706e+09, device='cuda:0')
c= tensor(1.9706e+09, device='cuda:0')
c= tensor(1.9707e+09, device='cuda:0')
c= tensor(1.9708e+09, device='cuda:0')
c= tensor(1.9708e+09, device='cuda:0')
c= tensor(1.9708e+09, device='cuda:0')
c= tensor(1.9708e+09, device='cuda:0')
c= tensor(1.9708e+09, device='cuda:0')
c= tensor(2.0108e+09, device='cuda:0')
c= tensor(2.0108e+09, device='cuda:0')
c= tensor(2.0123e+09, device='cuda:0')
c= tensor(2.0125e+09, device='cuda:0')
c= tensor(2.0143e+09, device='cuda:0')
c= tensor(2.0157e+09, device='cuda:0')
c= tensor(2.3813e+09, device='cuda:0')
c= tensor(2.4438e+09, device='cuda:0')
c= tensor(2.4444e+09, device='cuda:0')
c= tensor(2.4463e+09, device='cuda:0')
c= tensor(2.4463e+09, device='cuda:0')
c= tensor(2.4471e+09, device='cuda:0')
c= tensor(2.4481e+09, device='cuda:0')
c= tensor(2.4501e+09, device='cuda:0')
c= tensor(2.4501e+09, device='cuda:0')
c= tensor(2.4561e+09, device='cuda:0')
c= tensor(2.4739e+09, device='cuda:0')
c= tensor(2.4745e+09, device='cuda:0')
c= tensor(2.4746e+09, device='cuda:0')
c= tensor(2.4751e+09, device='cuda:0')
c= tensor(2.4751e+09, device='cuda:0')
c= tensor(2.4751e+09, device='cuda:0')
c= tensor(2.4793e+09, device='cuda:0')
c= tensor(2.4805e+09, device='cuda:0')
c= tensor(2.4805e+09, device='cuda:0')
c= tensor(2.4853e+09, device='cuda:0')
c= tensor(2.4854e+09, device='cuda:0')
c= tensor(2.4854e+09, device='cuda:0')
c= tensor(2.4911e+09, device='cuda:0')
c= tensor(2.4949e+09, device='cuda:0')
c= tensor(2.4975e+09, device='cuda:0')
c= tensor(2.5141e+09, device='cuda:0')
c= tensor(2.5203e+09, device='cuda:0')
c= tensor(2.5204e+09, device='cuda:0')
c= tensor(2.5213e+09, device='cuda:0')
c= tensor(2.5250e+09, device='cuda:0')
c= tensor(2.5299e+09, device='cuda:0')
c= tensor(2.5300e+09, device='cuda:0')
c= tensor(2.5574e+09, device='cuda:0')
c= tensor(2.5744e+09, device='cuda:0')
c= tensor(2.5758e+09, device='cuda:0')
c= tensor(2.5776e+09, device='cuda:0')
c= tensor(2.5801e+09, device='cuda:0')
c= tensor(2.5802e+09, device='cuda:0')
c= tensor(2.5803e+09, device='cuda:0')
c= tensor(2.5803e+09, device='cuda:0')
c= tensor(2.5865e+09, device='cuda:0')
c= tensor(2.5890e+09, device='cuda:0')
c= tensor(2.6067e+09, device='cuda:0')
c= tensor(2.6326e+09, device='cuda:0')
c= tensor(2.6336e+09, device='cuda:0')
c= tensor(2.6338e+09, device='cuda:0')
c= tensor(2.6356e+09, device='cuda:0')
c= tensor(2.6356e+09, device='cuda:0')
c= tensor(2.6357e+09, device='cuda:0')
c= tensor(2.6465e+09, device='cuda:0')
c= tensor(2.6468e+09, device='cuda:0')
c= tensor(2.6468e+09, device='cuda:0')
c= tensor(2.6468e+09, device='cuda:0')
c= tensor(2.6856e+09, device='cuda:0')
c= tensor(2.6860e+09, device='cuda:0')
c= tensor(2.6884e+09, device='cuda:0')
c= tensor(2.6887e+09, device='cuda:0')
c= tensor(2.6891e+09, device='cuda:0')
c= tensor(2.6891e+09, device='cuda:0')
c= tensor(2.6891e+09, device='cuda:0')
c= tensor(2.6897e+09, device='cuda:0')
c= tensor(2.6908e+09, device='cuda:0')
c= tensor(2.6908e+09, device='cuda:0')
c= tensor(2.6991e+09, device='cuda:0')
c= tensor(2.6991e+09, device='cuda:0')
c= tensor(2.6992e+09, device='cuda:0')
c= tensor(2.6993e+09, device='cuda:0')
c= tensor(2.7012e+09, device='cuda:0')
c= tensor(2.7012e+09, device='cuda:0')
c= tensor(2.7018e+09, device='cuda:0')
c= tensor(2.7019e+09, device='cuda:0')
c= tensor(2.7021e+09, device='cuda:0')
c= tensor(2.7027e+09, device='cuda:0')
c= tensor(2.7813e+09, device='cuda:0')
c= tensor(2.7814e+09, device='cuda:0')
c= tensor(2.7814e+09, device='cuda:0')
c= tensor(2.7898e+09, device='cuda:0')
c= tensor(2.7898e+09, device='cuda:0')
c= tensor(2.8347e+09, device='cuda:0')
c= tensor(2.8347e+09, device='cuda:0')
c= tensor(2.8382e+09, device='cuda:0')
c= tensor(2.8568e+09, device='cuda:0')
c= tensor(2.8569e+09, device='cuda:0')
c= tensor(2.8940e+09, device='cuda:0')
c= tensor(2.8942e+09, device='cuda:0')
c= tensor(2.9960e+09, device='cuda:0')
c= tensor(2.9960e+09, device='cuda:0')
c= tensor(3.0149e+09, device='cuda:0')
c= tensor(3.0149e+09, device='cuda:0')
c= tensor(3.0149e+09, device='cuda:0')
c= tensor(3.0149e+09, device='cuda:0')
c= tensor(3.0158e+09, device='cuda:0')
c= tensor(3.0170e+09, device='cuda:0')
c= tensor(3.0242e+09, device='cuda:0')
c= tensor(3.0243e+09, device='cuda:0')
c= tensor(3.0243e+09, device='cuda:0')
c= tensor(3.0243e+09, device='cuda:0')
c= tensor(3.0363e+09, device='cuda:0')
c= tensor(3.0376e+09, device='cuda:0')
c= tensor(3.0541e+09, device='cuda:0')
c= tensor(3.0566e+09, device='cuda:0')
c= tensor(3.0567e+09, device='cuda:0')
c= tensor(3.0567e+09, device='cuda:0')
c= tensor(3.0577e+09, device='cuda:0')
c= tensor(3.3635e+09, device='cuda:0')
c= tensor(3.3635e+09, device='cuda:0')
c= tensor(3.3636e+09, device='cuda:0')
c= tensor(3.3713e+09, device='cuda:0')
c= tensor(3.3737e+09, device='cuda:0')
c= tensor(3.3737e+09, device='cuda:0')
c= tensor(3.3737e+09, device='cuda:0')
c= tensor(3.3777e+09, device='cuda:0')
c= tensor(3.3780e+09, device='cuda:0')
c= tensor(3.3782e+09, device='cuda:0')
c= tensor(3.3789e+09, device='cuda:0')
c= tensor(3.4086e+09, device='cuda:0')
c= tensor(3.4112e+09, device='cuda:0')
c= tensor(3.4574e+09, device='cuda:0')
c= tensor(3.4590e+09, device='cuda:0')
c= tensor(3.4590e+09, device='cuda:0')
c= tensor(3.4605e+09, device='cuda:0')
c= tensor(3.4608e+09, device='cuda:0')
c= tensor(3.4609e+09, device='cuda:0')
c= tensor(3.4610e+09, device='cuda:0')
c= tensor(3.4610e+09, device='cuda:0')
c= tensor(3.4663e+09, device='cuda:0')
c= tensor(3.4664e+09, device='cuda:0')
c= tensor(3.4664e+09, device='cuda:0')
c= tensor(3.4666e+09, device='cuda:0')
c= tensor(3.4670e+09, device='cuda:0')
c= tensor(3.4673e+09, device='cuda:0')
c= tensor(3.4702e+09, device='cuda:0')
c= tensor(3.4702e+09, device='cuda:0')
c= tensor(3.4702e+09, device='cuda:0')
c= tensor(3.4705e+09, device='cuda:0')
c= tensor(3.4705e+09, device='cuda:0')
c= tensor(3.4705e+09, device='cuda:0')
c= tensor(3.4706e+09, device='cuda:0')
c= tensor(3.4802e+09, device='cuda:0')
c= tensor(3.4802e+09, device='cuda:0')
c= tensor(3.4802e+09, device='cuda:0')
c= tensor(3.4802e+09, device='cuda:0')
c= tensor(3.4876e+09, device='cuda:0')
c= tensor(3.5677e+09, device='cuda:0')
c= tensor(3.5689e+09, device='cuda:0')
c= tensor(3.5689e+09, device='cuda:0')
c= tensor(3.5747e+09, device='cuda:0')
c= tensor(3.5813e+09, device='cuda:0')
c= tensor(3.5813e+09, device='cuda:0')
c= tensor(3.5814e+09, device='cuda:0')
c= tensor(3.5817e+09, device='cuda:0')
c= tensor(3.5819e+09, device='cuda:0')
c= tensor(3.5855e+09, device='cuda:0')
c= tensor(3.5872e+09, device='cuda:0')
c= tensor(3.5873e+09, device='cuda:0')
c= tensor(3.5873e+09, device='cuda:0')
c= tensor(3.5873e+09, device='cuda:0')
c= tensor(3.5877e+09, device='cuda:0')
c= tensor(3.5884e+09, device='cuda:0')
c= tensor(3.5906e+09, device='cuda:0')
c= tensor(3.5909e+09, device='cuda:0')
c= tensor(3.6019e+09, device='cuda:0')
c= tensor(3.6020e+09, device='cuda:0')
c= tensor(3.6020e+09, device='cuda:0')
c= tensor(3.6020e+09, device='cuda:0')
c= tensor(3.6042e+09, device='cuda:0')
c= tensor(3.6046e+09, device='cuda:0')
c= tensor(3.6046e+09, device='cuda:0')
c= tensor(3.6046e+09, device='cuda:0')
c= tensor(3.6050e+09, device='cuda:0')
c= tensor(3.6050e+09, device='cuda:0')
c= tensor(3.6053e+09, device='cuda:0')
c= tensor(3.6053e+09, device='cuda:0')
c= tensor(3.6054e+09, device='cuda:0')
c= tensor(3.6055e+09, device='cuda:0')
c= tensor(3.6055e+09, device='cuda:0')
c= tensor(3.6055e+09, device='cuda:0')
c= tensor(3.6087e+09, device='cuda:0')
c= tensor(3.6240e+09, device='cuda:0')
c= tensor(3.6395e+09, device='cuda:0')
c= tensor(3.6597e+09, device='cuda:0')
c= tensor(3.6601e+09, device='cuda:0')
c= tensor(3.6601e+09, device='cuda:0')
c= tensor(3.6603e+09, device='cuda:0')
c= tensor(3.6642e+09, device='cuda:0')
c= tensor(3.6642e+09, device='cuda:0')
c= tensor(3.6645e+09, device='cuda:0')
c= tensor(3.6645e+09, device='cuda:0')
c= tensor(3.7590e+09, device='cuda:0')
c= tensor(3.7611e+09, device='cuda:0')
c= tensor(3.7614e+09, device='cuda:0')
c= tensor(3.8195e+09, device='cuda:0')
c= tensor(3.8200e+09, device='cuda:0')
c= tensor(3.8204e+09, device='cuda:0')
c= tensor(3.8414e+09, device='cuda:0')
c= tensor(3.8458e+09, device='cuda:0')
c= tensor(3.8500e+09, device='cuda:0')
c= tensor(3.8501e+09, device='cuda:0')
c= tensor(3.8557e+09, device='cuda:0')
c= tensor(3.8557e+09, device='cuda:0')
c= tensor(3.8570e+09, device='cuda:0')
c= tensor(3.9448e+09, device='cuda:0')
c= tensor(3.9468e+09, device='cuda:0')
c= tensor(3.9503e+09, device='cuda:0')
c= tensor(3.9503e+09, device='cuda:0')
c= tensor(3.9525e+09, device='cuda:0')
c= tensor(3.9530e+09, device='cuda:0')
c= tensor(3.9530e+09, device='cuda:0')
c= tensor(3.9534e+09, device='cuda:0')
c= tensor(3.9733e+09, device='cuda:0')
c= tensor(3.9735e+09, device='cuda:0')
c= tensor(4.1191e+09, device='cuda:0')
c= tensor(4.1199e+09, device='cuda:0')
c= tensor(4.1210e+09, device='cuda:0')
c= tensor(4.1212e+09, device='cuda:0')
c= tensor(4.1222e+09, device='cuda:0')
c= tensor(4.1316e+09, device='cuda:0')
c= tensor(4.1316e+09, device='cuda:0')
c= tensor(4.1902e+09, device='cuda:0')
c= tensor(4.1915e+09, device='cuda:0')
c= tensor(4.1928e+09, device='cuda:0')
c= tensor(4.1929e+09, device='cuda:0')
c= tensor(4.1930e+09, device='cuda:0')
c= tensor(4.1930e+09, device='cuda:0')
c= tensor(4.1930e+09, device='cuda:0')
c= tensor(4.1931e+09, device='cuda:0')
c= tensor(4.1960e+09, device='cuda:0')
c= tensor(5.6454e+09, device='cuda:0')
c= tensor(5.6606e+09, device='cuda:0')
c= tensor(5.6637e+09, device='cuda:0')
c= tensor(5.6637e+09, device='cuda:0')
c= tensor(5.6641e+09, device='cuda:0')
c= tensor(5.6643e+09, device='cuda:0')
c= tensor(5.6714e+09, device='cuda:0')
c= tensor(5.6714e+09, device='cuda:0')
c= tensor(5.8351e+09, device='cuda:0')
c= tensor(5.8351e+09, device='cuda:0')
c= tensor(5.8397e+09, device='cuda:0')
c= tensor(5.8398e+09, device='cuda:0')
c= tensor(5.8424e+09, device='cuda:0')
c= tensor(5.8694e+09, device='cuda:0')
c= tensor(5.8696e+09, device='cuda:0')
c= tensor(5.8697e+09, device='cuda:0')
c= tensor(5.8707e+09, device='cuda:0')
c= tensor(5.8707e+09, device='cuda:0')
c= tensor(5.8708e+09, device='cuda:0')
c= tensor(5.8759e+09, device='cuda:0')
c= tensor(6.0322e+09, device='cuda:0')
c= tensor(6.0330e+09, device='cuda:0')
c= tensor(6.0334e+09, device='cuda:0')
c= tensor(6.0509e+09, device='cuda:0')
c= tensor(6.0765e+09, device='cuda:0')
c= tensor(6.0771e+09, device='cuda:0')
c= tensor(6.0772e+09, device='cuda:0')
c= tensor(6.0888e+09, device='cuda:0')
c= tensor(6.0905e+09, device='cuda:0')
c= tensor(6.0941e+09, device='cuda:0')
c= tensor(6.0944e+09, device='cuda:0')
c= tensor(6.0971e+09, device='cuda:0')
c= tensor(6.0986e+09, device='cuda:0')
c= tensor(6.2240e+09, device='cuda:0')
c= tensor(6.2258e+09, device='cuda:0')
c= tensor(6.2259e+09, device='cuda:0')
c= tensor(6.2259e+09, device='cuda:0')
c= tensor(6.2261e+09, device='cuda:0')
c= tensor(6.2304e+09, device='cuda:0')
c= tensor(6.2373e+09, device='cuda:0')
c= tensor(6.2562e+09, device='cuda:0')
c= tensor(6.2563e+09, device='cuda:0')
c= tensor(6.2565e+09, device='cuda:0')
c= tensor(6.2577e+09, device='cuda:0')
c= tensor(6.2584e+09, device='cuda:0')
c= tensor(6.2593e+09, device='cuda:0')
c= tensor(6.2610e+09, device='cuda:0')
c= tensor(6.2632e+09, device='cuda:0')
c= tensor(6.2636e+09, device='cuda:0')
c= tensor(6.2636e+09, device='cuda:0')
c= tensor(6.2636e+09, device='cuda:0')
c= tensor(6.2700e+09, device='cuda:0')
c= tensor(6.2709e+09, device='cuda:0')
c= tensor(6.2722e+09, device='cuda:0')
c= tensor(6.2722e+09, device='cuda:0')
c= tensor(6.2722e+09, device='cuda:0')
c= tensor(6.2779e+09, device='cuda:0')
c= tensor(6.2803e+09, device='cuda:0')
c= tensor(6.2808e+09, device='cuda:0')
c= tensor(6.2808e+09, device='cuda:0')
c= tensor(6.2808e+09, device='cuda:0')
c= tensor(6.2810e+09, device='cuda:0')
c= tensor(6.2814e+09, device='cuda:0')
c= tensor(6.2885e+09, device='cuda:0')
c= tensor(6.2885e+09, device='cuda:0')
c= tensor(6.2886e+09, device='cuda:0')
c= tensor(6.2887e+09, device='cuda:0')
c= tensor(6.2888e+09, device='cuda:0')
c= tensor(6.3086e+09, device='cuda:0')
c= tensor(6.3086e+09, device='cuda:0')
c= tensor(6.3091e+09, device='cuda:0')
c= tensor(6.3122e+09, device='cuda:0')
c= tensor(6.3137e+09, device='cuda:0')
c= tensor(6.3170e+09, device='cuda:0')
c= tensor(6.3227e+09, device='cuda:0')
c= tensor(6.3227e+09, device='cuda:0')
c= tensor(6.3256e+09, device='cuda:0')
c= tensor(6.3258e+09, device='cuda:0')
c= tensor(6.3683e+09, device='cuda:0')
c= tensor(6.3744e+09, device='cuda:0')
c= tensor(6.3751e+09, device='cuda:0')
c= tensor(6.3778e+09, device='cuda:0')
c= tensor(6.3778e+09, device='cuda:0')
c= tensor(6.3784e+09, device='cuda:0')
c= tensor(6.3784e+09, device='cuda:0')
c= tensor(6.3786e+09, device='cuda:0')
c= tensor(6.3811e+09, device='cuda:0')
c= tensor(6.3881e+09, device='cuda:0')
c= tensor(6.3883e+09, device='cuda:0')
c= tensor(6.3886e+09, device='cuda:0')
c= tensor(6.3886e+09, device='cuda:0')
c= tensor(6.4230e+09, device='cuda:0')
c= tensor(6.4232e+09, device='cuda:0')
c= tensor(6.4232e+09, device='cuda:0')
c= tensor(6.4254e+09, device='cuda:0')
c= tensor(6.4280e+09, device='cuda:0')
c= tensor(6.4280e+09, device='cuda:0')
c= tensor(6.4301e+09, device='cuda:0')
c= tensor(6.4306e+09, device='cuda:0')
memory (bytes)
4445474816
time for making loss 2 is 15.63489294052124
p0 True
it  0 : 1357647872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 24% |
shape of L is 
torch.Size([])
memory (bytes)
4445671424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
4446298112
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  67933200000.0
relative error loss 10.564122
shape of L is 
torch.Size([])
memory (bytes)
4632272896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  4% | 12% |
memory (bytes)
4632276992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  67932815000.0
relative error loss 10.564062
shape of L is 
torch.Size([])
memory (bytes)
4637986816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 12% |
memory (bytes)
4638076928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  67930726000.0
relative error loss 10.563737
shape of L is 
torch.Size([])
memory (bytes)
4640018432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 12% |
memory (bytes)
4640116736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  67919593000.0
relative error loss 10.562006
shape of L is 
torch.Size([])
memory (bytes)
4642205696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 12% |
memory (bytes)
4642320384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  67858260000.0
relative error loss 10.552468
shape of L is 
torch.Size([])
memory (bytes)
4644327424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 12% |
memory (bytes)
4644429824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  67186870000.0
relative error loss 10.448062
shape of L is 
torch.Size([])
memory (bytes)
4646510592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 12% |
memory (bytes)
4646551552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  60786370000.0
relative error loss 9.452736
shape of L is 
torch.Size([])
memory (bytes)
4648521728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 12% |
memory (bytes)
4648521728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  36254204000.0
relative error loss 5.6378
shape of L is 
torch.Size([])
memory (bytes)
4650725376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 12% |
memory (bytes)
4650827776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  14321172000.0
relative error loss 2.2270496
shape of L is 
torch.Size([])
memory (bytes)
4652818432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 12% |
memory (bytes)
4652818432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  9002205000.0
relative error loss 1.3999103
time to take a step is 255.0027871131897
it  1 : 1641879552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
4655038464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 12% |
memory (bytes)
4655038464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  9002205000.0
relative error loss 1.3999103
shape of L is 
torch.Size([])
memory (bytes)
4657131520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 12% |
memory (bytes)
4657233920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  7243958000.0
relative error loss 1.1264898
shape of L is 
torch.Size([])
memory (bytes)
4659109888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 12% |
memory (bytes)
4659372032
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  6591659500.0
relative error loss 1.0250524
shape of L is 
torch.Size([])
memory (bytes)
4661305344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 12% |
memory (bytes)
4661501952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  6380880400.0
relative error loss 0.9922747
shape of L is 
torch.Size([])
memory (bytes)
4663611392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 12% |
memory (bytes)
4663611392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  6272839000.0
relative error loss 0.9754735
shape of L is 
torch.Size([])
memory (bytes)
4665663488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 12% |
memory (bytes)
4665663488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  21103686000.0
relative error loss 3.2817812
shape of L is 
torch.Size([])
memory (bytes)
4667817984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 12% |
memory (bytes)
4667916288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  5906881500.0
relative error loss 0.9185644
shape of L is 
torch.Size([])
memory (bytes)
4670046208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 12% |
memory (bytes)
4670046208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  5852087300.0
relative error loss 0.9100434
shape of L is 
torch.Size([])
memory (bytes)
4672147456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 12% |
memory (bytes)
4672147456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  5811942400.0
relative error loss 0.9038006
shape of L is 
torch.Size([])
memory (bytes)
4674166784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 12% |
memory (bytes)
4674265088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  5682516000.0
relative error loss 0.88367385
time to take a step is 246.85463118553162
it  2 : 1723089408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
4676173824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 12% |
memory (bytes)
4676173824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  5682516000.0
relative error loss 0.88367385
shape of L is 
torch.Size([])
memory (bytes)
4678389760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 12% |
memory (bytes)
4678389760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  5509091300.0
relative error loss 0.856705
shape of L is 
torch.Size([])
memory (bytes)
4680536064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 12% |
memory (bytes)
4680634368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  4973527000.0
relative error loss 0.77342075
shape of L is 
torch.Size([])
memory (bytes)
4682686464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 12% |
memory (bytes)
4682756096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  4693607000.0
relative error loss 0.72989106
shape of L is 
torch.Size([])
memory (bytes)
4684865536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 12% |
memory (bytes)
4684865536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  5377139700.0
relative error loss 0.8361855
shape of L is 
torch.Size([])
memory (bytes)
4686913536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 12% |
memory (bytes)
4686999552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  4373390000.0
relative error loss 0.6800949
shape of L is 
torch.Size([])
memory (bytes)
4689096704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 12% |
memory (bytes)
4689096704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  4113425700.0
relative error loss 0.6396685
shape of L is 
torch.Size([])
memory (bytes)
4691226624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 12% |
memory (bytes)
4691226624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  3662993700.0
relative error loss 0.569623
shape of L is 
torch.Size([])
memory (bytes)
4693282816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
4693381120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  3310273500.0
relative error loss 0.51477236
shape of L is 
torch.Size([])
memory (bytes)
4695298048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
4695298048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  2994215200.0
relative error loss 0.4656229
time to take a step is 247.68600225448608
it  3 : 1723088896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
4697460736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 12% |
memory (bytes)
4697665536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  2994215200.0
relative error loss 0.4656229
shape of L is 
torch.Size([])
memory (bytes)
4699713536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 12% |
memory (bytes)
4699811840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  2797086200.0
relative error loss 0.43496788
shape of L is 
torch.Size([])
memory (bytes)
4701818880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 12% |
memory (bytes)
4701818880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% | 12% |
error is  2511186000.0
relative error loss 0.39050823
shape of L is 
torch.Size([])
memory (bytes)
4704014336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 12% |
memory (bytes)
4704108544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  2231489800.0
relative error loss 0.34701338
shape of L is 
torch.Size([])
memory (bytes)
4706156544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 12% |
memory (bytes)
4706254848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  2045452300.0
relative error loss 0.31808317
shape of L is 
torch.Size([])
memory (bytes)
4708294656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
4708294656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1910214700.0
relative error loss 0.2970527
shape of L is 
torch.Size([])
memory (bytes)
4710342656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 12% |
memory (bytes)
4710539264
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1873268700.0
relative error loss 0.29130733
shape of L is 
torch.Size([])
memory (bytes)
4712583168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 12% |
memory (bytes)
4712583168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1717590000.0
relative error loss 0.26709813
shape of L is 
torch.Size([])
memory (bytes)
4714676224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 12% |
memory (bytes)
4714676224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1537819100.0
relative error loss 0.2391424
shape of L is 
torch.Size([])
memory (bytes)
4716974080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 12% |
memory (bytes)
4716974080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1429743100.0
relative error loss 0.22233577
time to take a step is 247.37814331054688
c= tensor(2138.3501, device='cuda:0')
c= tensor(193322.8125, device='cuda:0')
c= tensor(197760.5000, device='cuda:0')
c= tensor(201870.3750, device='cuda:0')
c= tensor(1571905.3750, device='cuda:0')
c= tensor(2063262.6250, device='cuda:0')
c= tensor(2804988., device='cuda:0')
c= tensor(3558676.5000, device='cuda:0')
c= tensor(3677221.2500, device='cuda:0')
c= tensor(13978417., device='cuda:0')
c= tensor(14006426., device='cuda:0')
c= tensor(21031244., device='cuda:0')
c= tensor(21056934., device='cuda:0')
c= tensor(27351940., device='cuda:0')
c= tensor(27515694., device='cuda:0')
c= tensor(28811938., device='cuda:0')
c= tensor(29229446., device='cuda:0')
c= tensor(29656542., device='cuda:0')
c= tensor(1.2930e+08, device='cuda:0')
c= tensor(1.3187e+08, device='cuda:0')
c= tensor(1.3212e+08, device='cuda:0')
c= tensor(1.9101e+08, device='cuda:0')
c= tensor(1.9118e+08, device='cuda:0')
c= tensor(1.9142e+08, device='cuda:0')
c= tensor(1.9216e+08, device='cuda:0')
c= tensor(1.9307e+08, device='cuda:0')
c= tensor(1.9415e+08, device='cuda:0')
c= tensor(1.9418e+08, device='cuda:0')
c= tensor(1.9477e+08, device='cuda:0')
c= tensor(9.2778e+08, device='cuda:0')
c= tensor(9.2785e+08, device='cuda:0')
c= tensor(9.8105e+08, device='cuda:0')
c= tensor(9.8114e+08, device='cuda:0')
c= tensor(9.8116e+08, device='cuda:0')
c= tensor(9.8123e+08, device='cuda:0')
c= tensor(9.8577e+08, device='cuda:0')
c= tensor(9.8695e+08, device='cuda:0')
c= tensor(9.8695e+08, device='cuda:0')
c= tensor(9.8696e+08, device='cuda:0')
c= tensor(9.8697e+08, device='cuda:0')
c= tensor(9.8699e+08, device='cuda:0')
c= tensor(9.8699e+08, device='cuda:0')
c= tensor(9.8699e+08, device='cuda:0')
c= tensor(9.8700e+08, device='cuda:0')
c= tensor(9.8700e+08, device='cuda:0')
c= tensor(9.8700e+08, device='cuda:0')
c= tensor(9.8706e+08, device='cuda:0')
c= tensor(9.8707e+08, device='cuda:0')
c= tensor(9.8707e+08, device='cuda:0')
c= tensor(9.8713e+08, device='cuda:0')
c= tensor(9.8721e+08, device='cuda:0')
c= tensor(9.8721e+08, device='cuda:0')
c= tensor(9.8724e+08, device='cuda:0')
c= tensor(9.8724e+08, device='cuda:0')
c= tensor(9.8726e+08, device='cuda:0')
c= tensor(9.8728e+08, device='cuda:0')
c= tensor(9.8730e+08, device='cuda:0')
c= tensor(9.8730e+08, device='cuda:0')
c= tensor(9.8731e+08, device='cuda:0')
c= tensor(9.8731e+08, device='cuda:0')
c= tensor(9.8733e+08, device='cuda:0')
c= tensor(9.8734e+08, device='cuda:0')
c= tensor(9.8737e+08, device='cuda:0')
c= tensor(9.8739e+08, device='cuda:0')
c= tensor(9.8740e+08, device='cuda:0')
c= tensor(9.8740e+08, device='cuda:0')
c= tensor(9.8741e+08, device='cuda:0')
c= tensor(9.8745e+08, device='cuda:0')
c= tensor(9.8746e+08, device='cuda:0')
c= tensor(9.8746e+08, device='cuda:0')
c= tensor(9.8748e+08, device='cuda:0')
c= tensor(9.8749e+08, device='cuda:0')
c= tensor(9.8749e+08, device='cuda:0')
c= tensor(9.8750e+08, device='cuda:0')
c= tensor(9.8750e+08, device='cuda:0')
c= tensor(9.8751e+08, device='cuda:0')
c= tensor(9.8752e+08, device='cuda:0')
c= tensor(9.8752e+08, device='cuda:0')
c= tensor(9.8753e+08, device='cuda:0')
c= tensor(9.8759e+08, device='cuda:0')
c= tensor(9.8760e+08, device='cuda:0')
c= tensor(9.8760e+08, device='cuda:0')
c= tensor(9.8762e+08, device='cuda:0')
c= tensor(9.8762e+08, device='cuda:0')
c= tensor(9.8762e+08, device='cuda:0')
c= tensor(9.8762e+08, device='cuda:0')
c= tensor(9.8763e+08, device='cuda:0')
c= tensor(9.8763e+08, device='cuda:0')
c= tensor(9.8764e+08, device='cuda:0')
c= tensor(9.8764e+08, device='cuda:0')
c= tensor(9.8765e+08, device='cuda:0')
c= tensor(9.8766e+08, device='cuda:0')
c= tensor(9.8766e+08, device='cuda:0')
c= tensor(9.8767e+08, device='cuda:0')
c= tensor(9.8767e+08, device='cuda:0')
c= tensor(9.8768e+08, device='cuda:0')
c= tensor(9.8769e+08, device='cuda:0')
c= tensor(9.8780e+08, device='cuda:0')
c= tensor(9.8781e+08, device='cuda:0')
c= tensor(9.8782e+08, device='cuda:0')
c= tensor(9.8786e+08, device='cuda:0')
c= tensor(9.8787e+08, device='cuda:0')
c= tensor(9.8788e+08, device='cuda:0')
c= tensor(9.8788e+08, device='cuda:0')
c= tensor(9.8789e+08, device='cuda:0')
c= tensor(9.8789e+08, device='cuda:0')
c= tensor(9.8791e+08, device='cuda:0')
c= tensor(9.8791e+08, device='cuda:0')
c= tensor(9.8792e+08, device='cuda:0')
c= tensor(9.8792e+08, device='cuda:0')
c= tensor(9.8792e+08, device='cuda:0')
c= tensor(9.8793e+08, device='cuda:0')
c= tensor(9.8793e+08, device='cuda:0')
c= tensor(9.8794e+08, device='cuda:0')
c= tensor(9.8794e+08, device='cuda:0')
c= tensor(9.8795e+08, device='cuda:0')
c= tensor(9.8796e+08, device='cuda:0')
c= tensor(9.8796e+08, device='cuda:0')
c= tensor(9.8798e+08, device='cuda:0')
c= tensor(9.8798e+08, device='cuda:0')
c= tensor(9.8804e+08, device='cuda:0')
c= tensor(9.8805e+08, device='cuda:0')
c= tensor(9.8805e+08, device='cuda:0')
c= tensor(9.8805e+08, device='cuda:0')
c= tensor(9.8806e+08, device='cuda:0')
c= tensor(9.8806e+08, device='cuda:0')
c= tensor(9.8806e+08, device='cuda:0')
c= tensor(9.8807e+08, device='cuda:0')
c= tensor(9.8814e+08, device='cuda:0')
c= tensor(9.8814e+08, device='cuda:0')
c= tensor(9.8816e+08, device='cuda:0')
c= tensor(9.8816e+08, device='cuda:0')
c= tensor(9.8817e+08, device='cuda:0')
c= tensor(9.8817e+08, device='cuda:0')
c= tensor(9.8818e+08, device='cuda:0')
c= tensor(9.8818e+08, device='cuda:0')
c= tensor(9.8818e+08, device='cuda:0')
c= tensor(9.8818e+08, device='cuda:0')
c= tensor(9.8819e+08, device='cuda:0')
c= tensor(9.8819e+08, device='cuda:0')
c= tensor(9.8820e+08, device='cuda:0')
c= tensor(9.8820e+08, device='cuda:0')
c= tensor(9.8822e+08, device='cuda:0')
c= tensor(9.8831e+08, device='cuda:0')
c= tensor(9.8832e+08, device='cuda:0')
c= tensor(9.8832e+08, device='cuda:0')
c= tensor(9.8832e+08, device='cuda:0')
c= tensor(9.8833e+08, device='cuda:0')
c= tensor(9.8833e+08, device='cuda:0')
c= tensor(9.8834e+08, device='cuda:0')
c= tensor(9.8834e+08, device='cuda:0')
c= tensor(9.8835e+08, device='cuda:0')
c= tensor(9.8836e+08, device='cuda:0')
c= tensor(9.8843e+08, device='cuda:0')
c= tensor(9.8844e+08, device='cuda:0')
c= tensor(9.8851e+08, device='cuda:0')
c= tensor(9.8852e+08, device='cuda:0')
c= tensor(9.8852e+08, device='cuda:0')
c= tensor(9.8854e+08, device='cuda:0')
c= tensor(9.8854e+08, device='cuda:0')
c= tensor(9.8855e+08, device='cuda:0')
c= tensor(9.8855e+08, device='cuda:0')
c= tensor(9.8856e+08, device='cuda:0')
c= tensor(9.8856e+08, device='cuda:0')
c= tensor(9.8856e+08, device='cuda:0')
c= tensor(9.8857e+08, device='cuda:0')
c= tensor(9.8858e+08, device='cuda:0')
c= tensor(9.8858e+08, device='cuda:0')
c= tensor(9.8859e+08, device='cuda:0')
c= tensor(9.8859e+08, device='cuda:0')
c= tensor(9.8860e+08, device='cuda:0')
c= tensor(9.8860e+08, device='cuda:0')
c= tensor(9.8862e+08, device='cuda:0')
c= tensor(9.8863e+08, device='cuda:0')
c= tensor(9.8865e+08, device='cuda:0')
c= tensor(9.8865e+08, device='cuda:0')
c= tensor(9.8867e+08, device='cuda:0')
c= tensor(9.8868e+08, device='cuda:0')
c= tensor(9.8870e+08, device='cuda:0')
c= tensor(9.8870e+08, device='cuda:0')
c= tensor(9.8871e+08, device='cuda:0')
c= tensor(9.8871e+08, device='cuda:0')
c= tensor(9.8873e+08, device='cuda:0')
c= tensor(9.8873e+08, device='cuda:0')
c= tensor(9.8875e+08, device='cuda:0')
c= tensor(9.8876e+08, device='cuda:0')
c= tensor(9.8876e+08, device='cuda:0')
c= tensor(9.8877e+08, device='cuda:0')
c= tensor(9.8878e+08, device='cuda:0')
c= tensor(9.8885e+08, device='cuda:0')
c= tensor(9.8885e+08, device='cuda:0')
c= tensor(9.8886e+08, device='cuda:0')
c= tensor(9.8886e+08, device='cuda:0')
c= tensor(9.8887e+08, device='cuda:0')
c= tensor(9.8887e+08, device='cuda:0')
c= tensor(9.8888e+08, device='cuda:0')
c= tensor(9.8888e+08, device='cuda:0')
c= tensor(9.8889e+08, device='cuda:0')
c= tensor(9.8889e+08, device='cuda:0')
c= tensor(9.8889e+08, device='cuda:0')
c= tensor(9.8890e+08, device='cuda:0')
c= tensor(9.8891e+08, device='cuda:0')
c= tensor(9.8894e+08, device='cuda:0')
c= tensor(9.8895e+08, device='cuda:0')
c= tensor(9.8896e+08, device='cuda:0')
c= tensor(9.8896e+08, device='cuda:0')
c= tensor(9.8897e+08, device='cuda:0')
c= tensor(9.8898e+08, device='cuda:0')
c= tensor(9.8898e+08, device='cuda:0')
c= tensor(9.8900e+08, device='cuda:0')
c= tensor(9.8901e+08, device='cuda:0')
c= tensor(9.8902e+08, device='cuda:0')
c= tensor(9.8902e+08, device='cuda:0')
c= tensor(9.8903e+08, device='cuda:0')
c= tensor(9.8904e+08, device='cuda:0')
c= tensor(9.8904e+08, device='cuda:0')
c= tensor(9.8904e+08, device='cuda:0')
c= tensor(9.8904e+08, device='cuda:0')
c= tensor(9.8909e+08, device='cuda:0')
c= tensor(9.8909e+08, device='cuda:0')
c= tensor(9.8910e+08, device='cuda:0')
c= tensor(9.8911e+08, device='cuda:0')
c= tensor(9.8912e+08, device='cuda:0')
c= tensor(9.8913e+08, device='cuda:0')
c= tensor(9.8913e+08, device='cuda:0')
c= tensor(9.8914e+08, device='cuda:0')
c= tensor(9.8915e+08, device='cuda:0')
c= tensor(9.8915e+08, device='cuda:0')
c= tensor(9.8916e+08, device='cuda:0')
c= tensor(9.8916e+08, device='cuda:0')
c= tensor(9.8916e+08, device='cuda:0')
c= tensor(9.8916e+08, device='cuda:0')
c= tensor(9.8917e+08, device='cuda:0')
c= tensor(9.8918e+08, device='cuda:0')
c= tensor(9.8919e+08, device='cuda:0')
c= tensor(9.8919e+08, device='cuda:0')
c= tensor(9.8920e+08, device='cuda:0')
c= tensor(9.8923e+08, device='cuda:0')
c= tensor(9.8923e+08, device='cuda:0')
c= tensor(9.8926e+08, device='cuda:0')
c= tensor(9.9028e+08, device='cuda:0')
c= tensor(9.9047e+08, device='cuda:0')
c= tensor(9.9050e+08, device='cuda:0')
c= tensor(9.9050e+08, device='cuda:0')
c= tensor(9.9051e+08, device='cuda:0')
c= tensor(9.9181e+08, device='cuda:0')
c= tensor(1.0162e+09, device='cuda:0')
c= tensor(1.0162e+09, device='cuda:0')
c= tensor(1.0184e+09, device='cuda:0')
c= tensor(1.0189e+09, device='cuda:0')
c= tensor(1.0194e+09, device='cuda:0')
c= tensor(1.0937e+09, device='cuda:0')
c= tensor(1.0937e+09, device='cuda:0')
c= tensor(1.0937e+09, device='cuda:0')
c= tensor(1.1136e+09, device='cuda:0')
c= tensor(1.2352e+09, device='cuda:0')
c= tensor(1.2352e+09, device='cuda:0')
c= tensor(1.2354e+09, device='cuda:0')
c= tensor(1.2355e+09, device='cuda:0')
c= tensor(1.2440e+09, device='cuda:0')
c= tensor(1.2447e+09, device='cuda:0')
c= tensor(1.2521e+09, device='cuda:0')
c= tensor(1.2532e+09, device='cuda:0')
c= tensor(1.2533e+09, device='cuda:0')
c= tensor(1.2534e+09, device='cuda:0')
c= tensor(1.3363e+09, device='cuda:0')
c= tensor(1.3363e+09, device='cuda:0')
c= tensor(1.3363e+09, device='cuda:0')
c= tensor(1.3374e+09, device='cuda:0')
c= tensor(1.3378e+09, device='cuda:0')
c= tensor(1.3526e+09, device='cuda:0')
c= tensor(1.3566e+09, device='cuda:0')
c= tensor(1.3566e+09, device='cuda:0')
c= tensor(1.3567e+09, device='cuda:0')
c= tensor(1.3567e+09, device='cuda:0')
c= tensor(1.3570e+09, device='cuda:0')
c= tensor(1.3600e+09, device='cuda:0')
c= tensor(1.3620e+09, device='cuda:0')
c= tensor(1.3635e+09, device='cuda:0')
c= tensor(1.3635e+09, device='cuda:0')
c= tensor(1.3635e+09, device='cuda:0')
c= tensor(1.3645e+09, device='cuda:0')
c= tensor(1.3755e+09, device='cuda:0')
c= tensor(1.3762e+09, device='cuda:0')
c= tensor(1.3762e+09, device='cuda:0')
c= tensor(1.3995e+09, device='cuda:0')
c= tensor(1.3997e+09, device='cuda:0')
c= tensor(1.3999e+09, device='cuda:0')
c= tensor(1.4019e+09, device='cuda:0')
c= tensor(1.4019e+09, device='cuda:0')
c= tensor(1.4032e+09, device='cuda:0')
c= tensor(1.4202e+09, device='cuda:0')
c= tensor(1.4529e+09, device='cuda:0')
c= tensor(1.4530e+09, device='cuda:0')
c= tensor(1.4531e+09, device='cuda:0')
c= tensor(1.4534e+09, device='cuda:0')
c= tensor(1.4534e+09, device='cuda:0')
c= tensor(1.4539e+09, device='cuda:0')
c= tensor(1.4540e+09, device='cuda:0')
c= tensor(1.4543e+09, device='cuda:0')
c= tensor(1.4632e+09, device='cuda:0')
c= tensor(1.4697e+09, device='cuda:0')
c= tensor(1.4697e+09, device='cuda:0')
c= tensor(1.4697e+09, device='cuda:0')
c= tensor(1.4717e+09, device='cuda:0')
c= tensor(1.4719e+09, device='cuda:0')
c= tensor(1.4721e+09, device='cuda:0')
c= tensor(1.4721e+09, device='cuda:0')
c= tensor(1.5144e+09, device='cuda:0')
c= tensor(1.5144e+09, device='cuda:0')
c= tensor(1.6614e+09, device='cuda:0')
c= tensor(1.6614e+09, device='cuda:0')
c= tensor(1.6622e+09, device='cuda:0')
c= tensor(1.6627e+09, device='cuda:0')
c= tensor(1.7056e+09, device='cuda:0')
c= tensor(1.7069e+09, device='cuda:0')
c= tensor(1.7069e+09, device='cuda:0')
c= tensor(1.7100e+09, device='cuda:0')
c= tensor(1.7136e+09, device='cuda:0')
c= tensor(1.7136e+09, device='cuda:0')
c= tensor(1.7150e+09, device='cuda:0')
c= tensor(1.7210e+09, device='cuda:0')
c= tensor(1.7379e+09, device='cuda:0')
c= tensor(1.7401e+09, device='cuda:0')
c= tensor(1.7401e+09, device='cuda:0')
c= tensor(1.7403e+09, device='cuda:0')
c= tensor(1.7538e+09, device='cuda:0')
c= tensor(1.7962e+09, device='cuda:0')
c= tensor(1.7963e+09, device='cuda:0')
c= tensor(1.7963e+09, device='cuda:0')
c= tensor(1.8018e+09, device='cuda:0')
c= tensor(1.8097e+09, device='cuda:0')
c= tensor(1.8119e+09, device='cuda:0')
c= tensor(1.8119e+09, device='cuda:0')
c= tensor(1.8122e+09, device='cuda:0')
c= tensor(1.8122e+09, device='cuda:0')
c= tensor(1.8123e+09, device='cuda:0')
c= tensor(1.8123e+09, device='cuda:0')
c= tensor(1.8123e+09, device='cuda:0')
c= tensor(1.8146e+09, device='cuda:0')
c= tensor(1.8150e+09, device='cuda:0')
c= tensor(1.8152e+09, device='cuda:0')
c= tensor(1.8152e+09, device='cuda:0')
c= tensor(1.8153e+09, device='cuda:0')
c= tensor(1.9219e+09, device='cuda:0')
c= tensor(1.9220e+09, device='cuda:0')
c= tensor(1.9257e+09, device='cuda:0')
c= tensor(1.9257e+09, device='cuda:0')
c= tensor(1.9257e+09, device='cuda:0')
c= tensor(1.9257e+09, device='cuda:0')
c= tensor(1.9262e+09, device='cuda:0')
c= tensor(1.9262e+09, device='cuda:0')
c= tensor(1.9280e+09, device='cuda:0')
c= tensor(1.9280e+09, device='cuda:0')
c= tensor(1.9280e+09, device='cuda:0')
c= tensor(1.9547e+09, device='cuda:0')
c= tensor(1.9576e+09, device='cuda:0')
c= tensor(1.9579e+09, device='cuda:0')
c= tensor(1.9621e+09, device='cuda:0')
c= tensor(1.9706e+09, device='cuda:0')
c= tensor(1.9706e+09, device='cuda:0')
c= tensor(1.9706e+09, device='cuda:0')
c= tensor(1.9706e+09, device='cuda:0')
c= tensor(1.9706e+09, device='cuda:0')
c= tensor(1.9707e+09, device='cuda:0')
c= tensor(1.9708e+09, device='cuda:0')
c= tensor(1.9708e+09, device='cuda:0')
c= tensor(1.9708e+09, device='cuda:0')
c= tensor(1.9708e+09, device='cuda:0')
c= tensor(1.9708e+09, device='cuda:0')
c= tensor(2.0108e+09, device='cuda:0')
c= tensor(2.0108e+09, device='cuda:0')
c= tensor(2.0123e+09, device='cuda:0')
c= tensor(2.0125e+09, device='cuda:0')
c= tensor(2.0143e+09, device='cuda:0')
c= tensor(2.0157e+09, device='cuda:0')
c= tensor(2.3813e+09, device='cuda:0')
c= tensor(2.4438e+09, device='cuda:0')
c= tensor(2.4444e+09, device='cuda:0')
c= tensor(2.4463e+09, device='cuda:0')
c= tensor(2.4463e+09, device='cuda:0')
c= tensor(2.4471e+09, device='cuda:0')
c= tensor(2.4481e+09, device='cuda:0')
c= tensor(2.4501e+09, device='cuda:0')
c= tensor(2.4501e+09, device='cuda:0')
c= tensor(2.4561e+09, device='cuda:0')
c= tensor(2.4739e+09, device='cuda:0')
c= tensor(2.4745e+09, device='cuda:0')
c= tensor(2.4746e+09, device='cuda:0')
c= tensor(2.4751e+09, device='cuda:0')
c= tensor(2.4751e+09, device='cuda:0')
c= tensor(2.4751e+09, device='cuda:0')
c= tensor(2.4793e+09, device='cuda:0')
c= tensor(2.4805e+09, device='cuda:0')
c= tensor(2.4805e+09, device='cuda:0')
c= tensor(2.4853e+09, device='cuda:0')
c= tensor(2.4854e+09, device='cuda:0')
c= tensor(2.4854e+09, device='cuda:0')
c= tensor(2.4911e+09, device='cuda:0')
c= tensor(2.4949e+09, device='cuda:0')
c= tensor(2.4975e+09, device='cuda:0')
c= tensor(2.5141e+09, device='cuda:0')
c= tensor(2.5203e+09, device='cuda:0')
c= tensor(2.5204e+09, device='cuda:0')
c= tensor(2.5213e+09, device='cuda:0')
c= tensor(2.5250e+09, device='cuda:0')
c= tensor(2.5299e+09, device='cuda:0')
c= tensor(2.5300e+09, device='cuda:0')
c= tensor(2.5574e+09, device='cuda:0')
c= tensor(2.5744e+09, device='cuda:0')
c= tensor(2.5758e+09, device='cuda:0')
c= tensor(2.5776e+09, device='cuda:0')
c= tensor(2.5801e+09, device='cuda:0')
c= tensor(2.5802e+09, device='cuda:0')
c= tensor(2.5803e+09, device='cuda:0')
c= tensor(2.5803e+09, device='cuda:0')
c= tensor(2.5865e+09, device='cuda:0')
c= tensor(2.5890e+09, device='cuda:0')
c= tensor(2.6067e+09, device='cuda:0')
c= tensor(2.6326e+09, device='cuda:0')
c= tensor(2.6336e+09, device='cuda:0')
c= tensor(2.6338e+09, device='cuda:0')
c= tensor(2.6356e+09, device='cuda:0')
c= tensor(2.6356e+09, device='cuda:0')
c= tensor(2.6357e+09, device='cuda:0')
c= tensor(2.6465e+09, device='cuda:0')
c= tensor(2.6468e+09, device='cuda:0')
c= tensor(2.6468e+09, device='cuda:0')
c= tensor(2.6468e+09, device='cuda:0')
c= tensor(2.6856e+09, device='cuda:0')
c= tensor(2.6860e+09, device='cuda:0')
c= tensor(2.6884e+09, device='cuda:0')
c= tensor(2.6887e+09, device='cuda:0')
c= tensor(2.6891e+09, device='cuda:0')
c= tensor(2.6891e+09, device='cuda:0')
c= tensor(2.6891e+09, device='cuda:0')
c= tensor(2.6897e+09, device='cuda:0')
c= tensor(2.6908e+09, device='cuda:0')
c= tensor(2.6908e+09, device='cuda:0')
c= tensor(2.6991e+09, device='cuda:0')
c= tensor(2.6991e+09, device='cuda:0')
c= tensor(2.6992e+09, device='cuda:0')
c= tensor(2.6993e+09, device='cuda:0')
c= tensor(2.7012e+09, device='cuda:0')
c= tensor(2.7012e+09, device='cuda:0')
c= tensor(2.7018e+09, device='cuda:0')
c= tensor(2.7019e+09, device='cuda:0')
c= tensor(2.7021e+09, device='cuda:0')
c= tensor(2.7027e+09, device='cuda:0')
c= tensor(2.7813e+09, device='cuda:0')
c= tensor(2.7814e+09, device='cuda:0')
c= tensor(2.7814e+09, device='cuda:0')
c= tensor(2.7898e+09, device='cuda:0')
c= tensor(2.7898e+09, device='cuda:0')
c= tensor(2.8347e+09, device='cuda:0')
c= tensor(2.8347e+09, device='cuda:0')
c= tensor(2.8382e+09, device='cuda:0')
c= tensor(2.8568e+09, device='cuda:0')
c= tensor(2.8569e+09, device='cuda:0')
c= tensor(2.8940e+09, device='cuda:0')
c= tensor(2.8942e+09, device='cuda:0')
c= tensor(2.9960e+09, device='cuda:0')
c= tensor(2.9960e+09, device='cuda:0')
c= tensor(3.0149e+09, device='cuda:0')
c= tensor(3.0149e+09, device='cuda:0')
c= tensor(3.0149e+09, device='cuda:0')
c= tensor(3.0149e+09, device='cuda:0')
c= tensor(3.0158e+09, device='cuda:0')
c= tensor(3.0170e+09, device='cuda:0')
c= tensor(3.0242e+09, device='cuda:0')
c= tensor(3.0243e+09, device='cuda:0')
c= tensor(3.0243e+09, device='cuda:0')
c= tensor(3.0243e+09, device='cuda:0')
c= tensor(3.0363e+09, device='cuda:0')
c= tensor(3.0376e+09, device='cuda:0')
c= tensor(3.0541e+09, device='cuda:0')
c= tensor(3.0566e+09, device='cuda:0')
c= tensor(3.0567e+09, device='cuda:0')
c= tensor(3.0567e+09, device='cuda:0')
c= tensor(3.0577e+09, device='cuda:0')
c= tensor(3.3635e+09, device='cuda:0')
c= tensor(3.3635e+09, device='cuda:0')
c= tensor(3.3636e+09, device='cuda:0')
c= tensor(3.3713e+09, device='cuda:0')
c= tensor(3.3737e+09, device='cuda:0')
c= tensor(3.3737e+09, device='cuda:0')
c= tensor(3.3737e+09, device='cuda:0')
c= tensor(3.3777e+09, device='cuda:0')
c= tensor(3.3780e+09, device='cuda:0')
c= tensor(3.3782e+09, device='cuda:0')
c= tensor(3.3789e+09, device='cuda:0')
c= tensor(3.4086e+09, device='cuda:0')
c= tensor(3.4112e+09, device='cuda:0')
c= tensor(3.4574e+09, device='cuda:0')
c= tensor(3.4590e+09, device='cuda:0')
c= tensor(3.4590e+09, device='cuda:0')
c= tensor(3.4605e+09, device='cuda:0')
c= tensor(3.4608e+09, device='cuda:0')
c= tensor(3.4609e+09, device='cuda:0')
c= tensor(3.4610e+09, device='cuda:0')
c= tensor(3.4610e+09, device='cuda:0')
c= tensor(3.4663e+09, device='cuda:0')
c= tensor(3.4664e+09, device='cuda:0')
c= tensor(3.4664e+09, device='cuda:0')
c= tensor(3.4666e+09, device='cuda:0')
c= tensor(3.4670e+09, device='cuda:0')
c= tensor(3.4673e+09, device='cuda:0')
c= tensor(3.4702e+09, device='cuda:0')
c= tensor(3.4702e+09, device='cuda:0')
c= tensor(3.4702e+09, device='cuda:0')
c= tensor(3.4705e+09, device='cuda:0')
c= tensor(3.4705e+09, device='cuda:0')
c= tensor(3.4705e+09, device='cuda:0')
c= tensor(3.4706e+09, device='cuda:0')
c= tensor(3.4802e+09, device='cuda:0')
c= tensor(3.4802e+09, device='cuda:0')
c= tensor(3.4802e+09, device='cuda:0')
c= tensor(3.4802e+09, device='cuda:0')
c= tensor(3.4876e+09, device='cuda:0')
c= tensor(3.5677e+09, device='cuda:0')
c= tensor(3.5689e+09, device='cuda:0')
c= tensor(3.5689e+09, device='cuda:0')
c= tensor(3.5747e+09, device='cuda:0')
c= tensor(3.5813e+09, device='cuda:0')
c= tensor(3.5813e+09, device='cuda:0')
c= tensor(3.5814e+09, device='cuda:0')
c= tensor(3.5817e+09, device='cuda:0')
c= tensor(3.5819e+09, device='cuda:0')
c= tensor(3.5855e+09, device='cuda:0')
c= tensor(3.5872e+09, device='cuda:0')
c= tensor(3.5873e+09, device='cuda:0')
c= tensor(3.5873e+09, device='cuda:0')
c= tensor(3.5873e+09, device='cuda:0')
c= tensor(3.5877e+09, device='cuda:0')
c= tensor(3.5884e+09, device='cuda:0')
c= tensor(3.5906e+09, device='cuda:0')
c= tensor(3.5909e+09, device='cuda:0')
c= tensor(3.6019e+09, device='cuda:0')
c= tensor(3.6020e+09, device='cuda:0')
c= tensor(3.6020e+09, device='cuda:0')
c= tensor(3.6020e+09, device='cuda:0')
c= tensor(3.6042e+09, device='cuda:0')
c= tensor(3.6046e+09, device='cuda:0')
c= tensor(3.6046e+09, device='cuda:0')
c= tensor(3.6046e+09, device='cuda:0')
c= tensor(3.6050e+09, device='cuda:0')
c= tensor(3.6050e+09, device='cuda:0')
c= tensor(3.6053e+09, device='cuda:0')
c= tensor(3.6053e+09, device='cuda:0')
c= tensor(3.6054e+09, device='cuda:0')
c= tensor(3.6055e+09, device='cuda:0')
c= tensor(3.6055e+09, device='cuda:0')
c= tensor(3.6055e+09, device='cuda:0')
c= tensor(3.6087e+09, device='cuda:0')
c= tensor(3.6240e+09, device='cuda:0')
c= tensor(3.6395e+09, device='cuda:0')
c= tensor(3.6597e+09, device='cuda:0')
c= tensor(3.6601e+09, device='cuda:0')
c= tensor(3.6601e+09, device='cuda:0')
c= tensor(3.6603e+09, device='cuda:0')
c= tensor(3.6642e+09, device='cuda:0')
c= tensor(3.6642e+09, device='cuda:0')
c= tensor(3.6645e+09, device='cuda:0')
c= tensor(3.6645e+09, device='cuda:0')
c= tensor(3.7590e+09, device='cuda:0')
c= tensor(3.7611e+09, device='cuda:0')
c= tensor(3.7614e+09, device='cuda:0')
c= tensor(3.8195e+09, device='cuda:0')
c= tensor(3.8200e+09, device='cuda:0')
c= tensor(3.8204e+09, device='cuda:0')
c= tensor(3.8414e+09, device='cuda:0')
c= tensor(3.8458e+09, device='cuda:0')
c= tensor(3.8500e+09, device='cuda:0')
c= tensor(3.8501e+09, device='cuda:0')
c= tensor(3.8557e+09, device='cuda:0')
c= tensor(3.8557e+09, device='cuda:0')
c= tensor(3.8570e+09, device='cuda:0')
c= tensor(3.9448e+09, device='cuda:0')
c= tensor(3.9468e+09, device='cuda:0')
c= tensor(3.9503e+09, device='cuda:0')
c= tensor(3.9503e+09, device='cuda:0')
c= tensor(3.9525e+09, device='cuda:0')
c= tensor(3.9530e+09, device='cuda:0')
c= tensor(3.9530e+09, device='cuda:0')
c= tensor(3.9534e+09, device='cuda:0')
c= tensor(3.9733e+09, device='cuda:0')
c= tensor(3.9735e+09, device='cuda:0')
c= tensor(4.1191e+09, device='cuda:0')
c= tensor(4.1199e+09, device='cuda:0')
c= tensor(4.1210e+09, device='cuda:0')
c= tensor(4.1212e+09, device='cuda:0')
c= tensor(4.1222e+09, device='cuda:0')
c= tensor(4.1316e+09, device='cuda:0')
c= tensor(4.1316e+09, device='cuda:0')
c= tensor(4.1902e+09, device='cuda:0')
c= tensor(4.1915e+09, device='cuda:0')
c= tensor(4.1928e+09, device='cuda:0')
c= tensor(4.1929e+09, device='cuda:0')
c= tensor(4.1930e+09, device='cuda:0')
c= tensor(4.1930e+09, device='cuda:0')
c= tensor(4.1930e+09, device='cuda:0')
c= tensor(4.1931e+09, device='cuda:0')
c= tensor(4.1960e+09, device='cuda:0')
c= tensor(5.6454e+09, device='cuda:0')
c= tensor(5.6606e+09, device='cuda:0')
c= tensor(5.6637e+09, device='cuda:0')
c= tensor(5.6637e+09, device='cuda:0')
c= tensor(5.6641e+09, device='cuda:0')
c= tensor(5.6643e+09, device='cuda:0')
c= tensor(5.6714e+09, device='cuda:0')
c= tensor(5.6714e+09, device='cuda:0')
c= tensor(5.8351e+09, device='cuda:0')
c= tensor(5.8351e+09, device='cuda:0')
c= tensor(5.8397e+09, device='cuda:0')
c= tensor(5.8398e+09, device='cuda:0')
c= tensor(5.8424e+09, device='cuda:0')
c= tensor(5.8694e+09, device='cuda:0')
c= tensor(5.8696e+09, device='cuda:0')
c= tensor(5.8697e+09, device='cuda:0')
c= tensor(5.8707e+09, device='cuda:0')
c= tensor(5.8707e+09, device='cuda:0')
c= tensor(5.8708e+09, device='cuda:0')
c= tensor(5.8759e+09, device='cuda:0')
c= tensor(6.0322e+09, device='cuda:0')
c= tensor(6.0330e+09, device='cuda:0')
c= tensor(6.0334e+09, device='cuda:0')
c= tensor(6.0509e+09, device='cuda:0')
c= tensor(6.0765e+09, device='cuda:0')
c= tensor(6.0771e+09, device='cuda:0')
c= tensor(6.0772e+09, device='cuda:0')
c= tensor(6.0888e+09, device='cuda:0')
c= tensor(6.0905e+09, device='cuda:0')
c= tensor(6.0941e+09, device='cuda:0')
c= tensor(6.0944e+09, device='cuda:0')
c= tensor(6.0971e+09, device='cuda:0')
c= tensor(6.0986e+09, device='cuda:0')
c= tensor(6.2240e+09, device='cuda:0')
c= tensor(6.2258e+09, device='cuda:0')
c= tensor(6.2259e+09, device='cuda:0')
c= tensor(6.2259e+09, device='cuda:0')
c= tensor(6.2261e+09, device='cuda:0')
c= tensor(6.2304e+09, device='cuda:0')
c= tensor(6.2373e+09, device='cuda:0')
c= tensor(6.2562e+09, device='cuda:0')
c= tensor(6.2563e+09, device='cuda:0')
c= tensor(6.2565e+09, device='cuda:0')
c= tensor(6.2577e+09, device='cuda:0')
c= tensor(6.2584e+09, device='cuda:0')
c= tensor(6.2593e+09, device='cuda:0')
c= tensor(6.2610e+09, device='cuda:0')
c= tensor(6.2632e+09, device='cuda:0')
c= tensor(6.2636e+09, device='cuda:0')
c= tensor(6.2636e+09, device='cuda:0')
c= tensor(6.2636e+09, device='cuda:0')
c= tensor(6.2700e+09, device='cuda:0')
c= tensor(6.2709e+09, device='cuda:0')
c= tensor(6.2722e+09, device='cuda:0')
c= tensor(6.2722e+09, device='cuda:0')
c= tensor(6.2722e+09, device='cuda:0')
c= tensor(6.2779e+09, device='cuda:0')
c= tensor(6.2803e+09, device='cuda:0')
c= tensor(6.2808e+09, device='cuda:0')
c= tensor(6.2808e+09, device='cuda:0')
c= tensor(6.2808e+09, device='cuda:0')
c= tensor(6.2810e+09, device='cuda:0')
c= tensor(6.2814e+09, device='cuda:0')
c= tensor(6.2885e+09, device='cuda:0')
c= tensor(6.2885e+09, device='cuda:0')
c= tensor(6.2886e+09, device='cuda:0')
c= tensor(6.2887e+09, device='cuda:0')
c= tensor(6.2888e+09, device='cuda:0')
c= tensor(6.3086e+09, device='cuda:0')
c= tensor(6.3086e+09, device='cuda:0')
c= tensor(6.3091e+09, device='cuda:0')
c= tensor(6.3122e+09, device='cuda:0')
c= tensor(6.3137e+09, device='cuda:0')
c= tensor(6.3170e+09, device='cuda:0')
c= tensor(6.3227e+09, device='cuda:0')
c= tensor(6.3227e+09, device='cuda:0')
c= tensor(6.3256e+09, device='cuda:0')
c= tensor(6.3258e+09, device='cuda:0')
c= tensor(6.3683e+09, device='cuda:0')
c= tensor(6.3744e+09, device='cuda:0')
c= tensor(6.3751e+09, device='cuda:0')
c= tensor(6.3778e+09, device='cuda:0')
c= tensor(6.3778e+09, device='cuda:0')
c= tensor(6.3784e+09, device='cuda:0')
c= tensor(6.3784e+09, device='cuda:0')
c= tensor(6.3786e+09, device='cuda:0')
c= tensor(6.3811e+09, device='cuda:0')
c= tensor(6.3881e+09, device='cuda:0')
c= tensor(6.3883e+09, device='cuda:0')
c= tensor(6.3886e+09, device='cuda:0')
c= tensor(6.3886e+09, device='cuda:0')
c= tensor(6.4230e+09, device='cuda:0')
c= tensor(6.4232e+09, device='cuda:0')
c= tensor(6.4232e+09, device='cuda:0')
c= tensor(6.4254e+09, device='cuda:0')
c= tensor(6.4280e+09, device='cuda:0')
c= tensor(6.4280e+09, device='cuda:0')
c= tensor(6.4301e+09, device='cuda:0')
c= tensor(6.4306e+09, device='cuda:0')
time to make c is 15.450354814529419
time for making loss is 15.450371026992798
p0 True
it  0 : 1357821440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
4719038464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
memory (bytes)
4719292416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1429743100.0
relative error loss 0.22233577
shape of L is 
torch.Size([])
memory (bytes)
4746510336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 12% |
memory (bytes)
4746690560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1393030100.0
relative error loss 0.21662663
shape of L is 
torch.Size([])
memory (bytes)
4749914112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
4750090240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 12% |
error is  1373649900.0
relative error loss 0.21361285
shape of L is 
torch.Size([])
memory (bytes)
4753293312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
4753297408
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1335592000.0
relative error loss 0.20769456
shape of L is 
torch.Size([])
memory (bytes)
4756447232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
4756447232
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1292993500.0
relative error loss 0.20107019
shape of L is 
torch.Size([])
memory (bytes)
4759629824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 12% |
memory (bytes)
4759711744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 12% |
error is  1278893000.0
relative error loss 0.19887745
shape of L is 
torch.Size([])
memory (bytes)
4762832896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
4762832896
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1264175600.0
relative error loss 0.19658878
shape of L is 
torch.Size([])
memory (bytes)
4766126080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
4766142464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1251518000.0
relative error loss 0.19462042
shape of L is 
torch.Size([])
memory (bytes)
4769214464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
4769214464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1241625600.0
relative error loss 0.1930821
shape of L is 
torch.Size([])
memory (bytes)
4772581376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
4772585472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1235338800.0
relative error loss 0.19210443
time to take a step is 305.5809516906738
it  1 : 1724819968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
4775698432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
4775698432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1235338800.0
relative error loss 0.19210443
shape of L is 
torch.Size([])
memory (bytes)
4778778624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
4779020288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1228441100.0
relative error loss 0.1910318
shape of L is 
torch.Size([])
memory (bytes)
4782247936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
4782247936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1221242900.0
relative error loss 0.18991242
shape of L is 
torch.Size([])
memory (bytes)
4785385472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 12% |
memory (bytes)
4785487872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1215662100.0
relative error loss 0.18904456
shape of L is 
torch.Size([])
memory (bytes)
4788711424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
4788711424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% | 12% |
error is  1211019300.0
relative error loss 0.18832257
shape of L is 
torch.Size([])
memory (bytes)
4791689216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
4791930880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1209402900.0
relative error loss 0.1880712
shape of L is 
torch.Size([])
memory (bytes)
4795150336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 12% |
memory (bytes)
4795150336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1203491300.0
relative error loss 0.18715192
shape of L is 
torch.Size([])
memory (bytes)
4798160896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
4798357504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 12% |
error is  1202093600.0
relative error loss 0.18693456
shape of L is 
torch.Size([])
memory (bytes)
4801576960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
4801576960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 12% |
error is  1198482400.0
relative error loss 0.186373
shape of L is 
torch.Size([])
memory (bytes)
4804632576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
4804792320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 12% |
error is  1194426400.0
relative error loss 0.18574226
time to take a step is 292.0951588153839
it  2 : 1724819968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
4807913472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
4808019968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1194426400.0
relative error loss 0.18574226
shape of L is 
torch.Size([])
memory (bytes)
4811096064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
4811096064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1191145500.0
relative error loss 0.18523204
shape of L is 
torch.Size([])
memory (bytes)
4814434304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
4814454784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1188303400.0
relative error loss 0.18479007
shape of L is 
torch.Size([])
memory (bytes)
4817670144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 12% |
memory (bytes)
4817674240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1186299400.0
relative error loss 0.18447845
shape of L is 
torch.Size([])
memory (bytes)
4820889600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 12% |
memory (bytes)
4820893696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1184426500.0
relative error loss 0.1841872
shape of L is 
torch.Size([])
memory (bytes)
4824109056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
4824121344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1182739500.0
relative error loss 0.18392485
shape of L is 
torch.Size([])
memory (bytes)
4827340800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 12% |
memory (bytes)
4827340800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1180575200.0
relative error loss 0.1835883
shape of L is 
torch.Size([])
memory (bytes)
4830433280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 12% |
memory (bytes)
4830556160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1178502700.0
relative error loss 0.183266
shape of L is 
torch.Size([])
memory (bytes)
4833771520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 12% |
memory (bytes)
4833771520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1177287700.0
relative error loss 0.18307705
shape of L is 
torch.Size([])
memory (bytes)
4836986880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 12% |
memory (bytes)
4836990976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1175888400.0
relative error loss 0.18285945
time to take a step is 393.3339169025421
it  3 : 1724819968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
4840128512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 12% |
memory (bytes)
4840128512
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1175888400.0
relative error loss 0.18285945
shape of L is 
torch.Size([])
memory (bytes)
4843352064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 12% |
memory (bytes)
4843421696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1174479400.0
relative error loss 0.18264034
shape of L is 
torch.Size([])
memory (bytes)
4846645248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 12% |
memory (bytes)
4846649344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1173084200.0
relative error loss 0.18242338
shape of L is 
torch.Size([])
memory (bytes)
4849713152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 12% |
memory (bytes)
4849713152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1172169700.0
relative error loss 0.18228118
shape of L is 
torch.Size([])
memory (bytes)
4853075968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 12% |
memory (bytes)
4853100544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1170966000.0
relative error loss 0.182094
shape of L is 
torch.Size([])
memory (bytes)
4856197120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 12% |
memory (bytes)
4856320000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1171200500.0
relative error loss 0.18213046
shape of L is 
torch.Size([])
memory (bytes)
4859539456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 12% |
memory (bytes)
4859539456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1170262000.0
relative error loss 0.18198451
shape of L is 
torch.Size([])
memory (bytes)
4862746624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 12% |
memory (bytes)
4862750720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1169507300.0
relative error loss 0.18186715
shape of L is 
torch.Size([])
memory (bytes)
4865802240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 12% |
memory (bytes)
4865966080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1168819200.0
relative error loss 0.18176015
shape of L is 
torch.Size([])
memory (bytes)
4869062656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 12% |
memory (bytes)
4869185536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1168106500.0
relative error loss 0.18164931
time to take a step is 400.0962743759155
it  4 : 1724819968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
4872388608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
memory (bytes)
4872409088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1168106500.0
relative error loss 0.18164931
shape of L is 
torch.Size([])
memory (bytes)
4875620352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 12% |
memory (bytes)
4875624448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 12% |
error is  1167308300.0
relative error loss 0.18152519
shape of L is 
torch.Size([])
memory (bytes)
4878712832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 12% |
memory (bytes)
4878712832
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1166725100.0
relative error loss 0.1814345
shape of L is 
torch.Size([])
memory (bytes)
4882067456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 12% |
memory (bytes)
4882071552
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1166181400.0
relative error loss 0.18134995
shape of L is 
torch.Size([])
memory (bytes)
4885286912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 12% |
memory (bytes)
4885291008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1165270500.0
relative error loss 0.1812083
shape of L is 
torch.Size([])
memory (bytes)
4888420352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 12% |
memory (bytes)
4888420352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1164624400.0
relative error loss 0.18110782
shape of L is 
torch.Size([])
memory (bytes)
4891648000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 12% |
memory (bytes)
4891750400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% | 12% |
error is  1163857900.0
relative error loss 0.18098862
shape of L is 
torch.Size([])
memory (bytes)
4894969856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 12% |
memory (bytes)
4894973952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1162906600.0
relative error loss 0.1808407
shape of L is 
torch.Size([])
memory (bytes)
4898033664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 12% |
memory (bytes)
4898033664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1162216000.0
relative error loss 0.1807333
shape of L is 
torch.Size([])
memory (bytes)
4901416960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 12% |
memory (bytes)
4901416960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% | 12% |
error is  1161470000.0
relative error loss 0.18061729
time to take a step is 414.19880652427673
it  5 : 1724819968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
4904636416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 12% |
memory (bytes)
4904640512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1161470000.0
relative error loss 0.18061729
shape of L is 
torch.Size([])
memory (bytes)
4907786240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 12% |
memory (bytes)
4907786240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1160435200.0
relative error loss 0.18045637
shape of L is 
torch.Size([])
memory (bytes)
4911034368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
4911071232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1159879200.0
relative error loss 0.1803699
shape of L is 
torch.Size([])
memory (bytes)
4914130944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 12% |
memory (bytes)
4914290688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 12% |
error is  1159246300.0
relative error loss 0.18027149
shape of L is 
torch.Size([])
memory (bytes)
4917448704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 12% |
memory (bytes)
4917518336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1158975500.0
relative error loss 0.18022938
shape of L is 
torch.Size([])
memory (bytes)
4920733696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 12% |
memory (bytes)
4920737792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1157999600.0
relative error loss 0.18007761
shape of L is 
torch.Size([])
memory (bytes)
4923928576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 12% |
memory (bytes)
4923928576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1157631000.0
relative error loss 0.18002029
shape of L is 
torch.Size([])
memory (bytes)
4927168512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 12% |
memory (bytes)
4927176704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1157164500.0
relative error loss 0.17994776
shape of L is 
torch.Size([])
memory (bytes)
4930396160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 12% |
memory (bytes)
4930400256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1156575700.0
relative error loss 0.1798562
shape of L is 
torch.Size([])
memory (bytes)
4933447680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 12% |
memory (bytes)
4933607424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 12% |
error is  1155943400.0
relative error loss 0.17975786
time to take a step is 418.96785712242126
it  6 : 1724819968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
4936818688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 12% |
memory (bytes)
4936818688
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1155943400.0
relative error loss 0.17975786
shape of L is 
torch.Size([])
memory (bytes)
4940050432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 12% |
memory (bytes)
4940050432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1155412000.0
relative error loss 0.17967522
shape of L is 
torch.Size([])
memory (bytes)
4943155200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 12% |
memory (bytes)
4943155200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1154848800.0
relative error loss 0.17958763
shape of L is 
torch.Size([])
memory (bytes)
4946427904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 12% |
memory (bytes)
4946505728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1154403800.0
relative error loss 0.17951845
shape of L is 
torch.Size([])
memory (bytes)
4949716992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 12% |
memory (bytes)
4949721088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1153800700.0
relative error loss 0.17942466
shape of L is 
torch.Size([])
memory (bytes)
4952936448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
4952936448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1153382400.0
relative error loss 0.17935961
shape of L is 
torch.Size([])
memory (bytes)
4956160000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 12% |
memory (bytes)
4956164096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1153095700.0
relative error loss 0.17931502
shape of L is 
torch.Size([])
memory (bytes)
4959391744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 12% |
memory (bytes)
4959391744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1152770600.0
relative error loss 0.17926446
shape of L is 
torch.Size([])
memory (bytes)
4962598912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
4962607104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1152166900.0
relative error loss 0.1791706
shape of L is 
torch.Size([])
memory (bytes)
4965818368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
4965818368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 12% |
error is  1151950300.0
relative error loss 0.17913692
time to take a step is 371.7835340499878
it  7 : 1724819968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
4968906752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
4968906752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1151950300.0
relative error loss 0.17913692
shape of L is 
torch.Size([])
memory (bytes)
4972249088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
4972253184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1151594000.0
relative error loss 0.1790815
shape of L is 
torch.Size([])
memory (bytes)
4975398912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
4975398912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 12% |
error is  1151175200.0
relative error loss 0.17901637
shape of L is 
torch.Size([])
memory (bytes)
4978700288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
4978708480
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1150751200.0
relative error loss 0.17895044
shape of L is 
torch.Size([])
memory (bytes)
4981829632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
4981829632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1150420500.0
relative error loss 0.178899
shape of L is 
torch.Size([])
memory (bytes)
4985131008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
4985135104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1150183400.0
relative error loss 0.17886214
shape of L is 
torch.Size([])
memory (bytes)
4988334080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
4988334080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 12% |
error is  1149622800.0
relative error loss 0.17877495
shape of L is 
torch.Size([])
memory (bytes)
4991582208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
4991582208
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1149282300.0
relative error loss 0.17872201
shape of L is 
torch.Size([])
memory (bytes)
4994605056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
4994809856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 12% |
error is  1148799000.0
relative error loss 0.17864685
shape of L is 
torch.Size([])
memory (bytes)
4998025216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
4998029312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1148536300.0
relative error loss 0.178606
time to take a step is 294.15979957580566
it  8 : 1724819968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5001146368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 12% |
memory (bytes)
5001252864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1148536300.0
relative error loss 0.178606
shape of L is 
torch.Size([])
memory (bytes)
5004460032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5004460032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% | 12% |
error is  1148291600.0
relative error loss 0.17856795
shape of L is 
torch.Size([])
memory (bytes)
5007581184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 12% |
memory (bytes)
5007687680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1147890700.0
relative error loss 0.1785056
shape of L is 
torch.Size([])
memory (bytes)
5010841600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 12% |
memory (bytes)
5010841600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1147506700.0
relative error loss 0.17844589
shape of L is 
torch.Size([])
memory (bytes)
5014130688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 12% |
memory (bytes)
5014134784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1147295200.0
relative error loss 0.178413
shape of L is 
torch.Size([])
memory (bytes)
5017190400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5017190400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1147014700.0
relative error loss 0.17836937
shape of L is 
torch.Size([])
memory (bytes)
5020565504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5020569600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1146815000.0
relative error loss 0.17833832
shape of L is 
torch.Size([])
memory (bytes)
5023780864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 12% |
memory (bytes)
5023780864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1146412000.0
relative error loss 0.17827566
shape of L is 
torch.Size([])
memory (bytes)
5027016704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5027020800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1146191400.0
relative error loss 0.17824134
shape of L is 
torch.Size([])
memory (bytes)
5030162432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 12% |
memory (bytes)
5030162432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1145990700.0
relative error loss 0.17821014
time to take a step is 292.5453932285309
it  9 : 1724819968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5033332736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5033455616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1145990700.0
relative error loss 0.17821014
shape of L is 
torch.Size([])
memory (bytes)
5036670976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 12% |
memory (bytes)
5036670976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1145994200.0
relative error loss 0.17821069
shape of L is 
torch.Size([])
memory (bytes)
5039742976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5039898624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1145829400.0
relative error loss 0.17818506
shape of L is 
torch.Size([])
memory (bytes)
5043122176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5043122176
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1145586200.0
relative error loss 0.17814724
shape of L is 
torch.Size([])
memory (bytes)
5046267904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5046267904
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1145353200.0
relative error loss 0.17811102
shape of L is 
torch.Size([])
memory (bytes)
5049556992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5049561088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1145104900.0
relative error loss 0.1780724
shape of L is 
torch.Size([])
memory (bytes)
5052575744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 12% |
memory (bytes)
5052776448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1144790500.0
relative error loss 0.1780235
shape of L is 
torch.Size([])
memory (bytes)
5055995904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5055995904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1144497700.0
relative error loss 0.17797796
shape of L is 
torch.Size([])
memory (bytes)
5059067904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5059067904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1144216000.0
relative error loss 0.17793417
shape of L is 
torch.Size([])
memory (bytes)
5062451200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5062451200
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1143919600.0
relative error loss 0.17788808
time to take a step is 293.5091133117676
it  10 : 1724819968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5065519104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5065519104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1143919600.0
relative error loss 0.17788808
shape of L is 
torch.Size([])
memory (bytes)
5068881920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5068894208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1143836700.0
relative error loss 0.17787518
shape of L is 
torch.Size([])
memory (bytes)
5072113664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5072113664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1143621600.0
relative error loss 0.17784174
shape of L is 
torch.Size([])
memory (bytes)
5075333120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5075333120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1143461400.0
relative error loss 0.17781681
shape of L is 
torch.Size([])
memory (bytes)
5078552576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5078552576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1143316000.0
relative error loss 0.1777942
shape of L is 
torch.Size([])
memory (bytes)
5081735168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5081735168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1143102500.0
relative error loss 0.177761
shape of L is 
torch.Size([])
memory (bytes)
5084987392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5084991488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1142760000.0
relative error loss 0.17770773
shape of L is 
torch.Size([])
memory (bytes)
5088088064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 12% |
memory (bytes)
5088088064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1142490100.0
relative error loss 0.17766577
shape of L is 
torch.Size([])
memory (bytes)
5091430400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5091434496
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1142297100.0
relative error loss 0.17763576
shape of L is 
torch.Size([])
memory (bytes)
5094477824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5094637568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1142087700.0
relative error loss 0.1776032
time to take a step is 294.8006179332733
it  11 : 1724819968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5097709568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5097869312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1142087700.0
relative error loss 0.1776032
shape of L is 
torch.Size([])
memory (bytes)
5101039616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5101084672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% | 12% |
error is  1141868500.0
relative error loss 0.17756912
shape of L is 
torch.Size([])
memory (bytes)
5104128000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5104300032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1141760500.0
relative error loss 0.17755231
shape of L is 
torch.Size([])
memory (bytes)
5107515392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5107519488
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1141395500.0
relative error loss 0.17749555
shape of L is 
torch.Size([])
memory (bytes)
5110571008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5110571008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1141356000.0
relative error loss 0.17748941
shape of L is 
torch.Size([])
memory (bytes)
5113954304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5113954304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1141139000.0
relative error loss 0.17745566
shape of L is 
torch.Size([])
memory (bytes)
5117050880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5117050880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1141009400.0
relative error loss 0.17743552
shape of L is 
torch.Size([])
memory (bytes)
5120385024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 12% |
memory (bytes)
5120385024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1140879900.0
relative error loss 0.17741537
shape of L is 
torch.Size([])
memory (bytes)
5123579904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 12% |
memory (bytes)
5123600384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1140728300.0
relative error loss 0.1773918
shape of L is 
torch.Size([])
memory (bytes)
5126680576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5126840320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1140527600.0
relative error loss 0.1773606
time to take a step is 295.0316035747528
it  12 : 1724819968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5130047488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5130051584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1140527600.0
relative error loss 0.1773606
shape of L is 
torch.Size([])
memory (bytes)
5133238272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5133238272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1140305400.0
relative error loss 0.17732604
shape of L is 
torch.Size([])
memory (bytes)
5136486400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 12% |
memory (bytes)
5136486400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1140072000.0
relative error loss 0.17728972
shape of L is 
torch.Size([])
memory (bytes)
5139566592
| ID | GPU | MEM |
------------------
|  0 |  1% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5139566592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1139820000.0
relative error loss 0.17725056
shape of L is 
torch.Size([])
memory (bytes)
5142917120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5142921216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1139562000.0
relative error loss 0.17721044
shape of L is 
torch.Size([])
memory (bytes)
5146030080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 12% |
memory (bytes)
5146152960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1139395100.0
relative error loss 0.17718448
shape of L is 
torch.Size([])
memory (bytes)
5149360128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5149360128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 12% |
error is  1139243000.0
relative error loss 0.17716083
shape of L is 
torch.Size([])
memory (bytes)
5152587776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5152587776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1139155500.0
relative error loss 0.17714721
shape of L is 
torch.Size([])
memory (bytes)
5155762176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5155762176
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1138993700.0
relative error loss 0.17712206
shape of L is 
torch.Size([])
memory (bytes)
5159026688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5159030784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1138899500.0
relative error loss 0.17710741
time to take a step is 295.3313047885895
it  13 : 1724819968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5162156032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 12% |
memory (bytes)
5162156032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1138899500.0
relative error loss 0.17710741
shape of L is 
torch.Size([])
memory (bytes)
5165481984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 12% |
memory (bytes)
5165481984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1138794500.0
relative error loss 0.17709108
shape of L is 
torch.Size([])
memory (bytes)
5168533504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5168533504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1138657800.0
relative error loss 0.17706983
shape of L is 
torch.Size([])
memory (bytes)
5171916800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5171920896
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1138647600.0
relative error loss 0.17706823
shape of L is 
torch.Size([])
memory (bytes)
5175136256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5175136256
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1138575400.0
relative error loss 0.177057
shape of L is 
torch.Size([])
memory (bytes)
5178191872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5178351616
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1138486300.0
relative error loss 0.17704315
shape of L is 
torch.Size([])
memory (bytes)
5181575168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5181579264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1138406400.0
relative error loss 0.17703073
shape of L is 
torch.Size([])
memory (bytes)
5184630784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5184790528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1138295800.0
relative error loss 0.17701353
shape of L is 
torch.Size([])
memory (bytes)
5188014080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 12% |
memory (bytes)
5188018176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1138160600.0
relative error loss 0.1769925
shape of L is 
torch.Size([])
memory (bytes)
5191090176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5191090176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1138077200.0
relative error loss 0.17697953
time to take a step is 294.5347123146057
it  14 : 1724819968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5194465280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5194465280
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1138077200.0
relative error loss 0.17697953
shape of L is 
torch.Size([])
memory (bytes)
5197553664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 12% |
memory (bytes)
5197553664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1137992700.0
relative error loss 0.1769664
shape of L is 
torch.Size([])
memory (bytes)
5200916480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5200916480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1137923100.0
relative error loss 0.17695557
shape of L is 
torch.Size([])
memory (bytes)
5204131840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5204131840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1137837000.0
relative error loss 0.17694218
shape of L is 
torch.Size([])
memory (bytes)
5207236608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 12% |
memory (bytes)
5207236608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 12% |
error is  1137791500.0
relative error loss 0.1769351
shape of L is 
torch.Size([])
memory (bytes)
5210525696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5210574848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1137666000.0
relative error loss 0.1769156
shape of L is 
torch.Size([])
memory (bytes)
5213691904
| ID | GPU | MEM |
------------------
|  0 | 21% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5213691904
| ID | GPU | MEM |
------------------
|  0 | 20% |  0% |
|  1 | 99% | 12% |
error is  1137614800.0
relative error loss 0.17690763
shape of L is 
torch.Size([])
memory (bytes)
5217009664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5217009664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% | 12% |
error is  1137555500.0
relative error loss 0.1768984
shape of L is 
torch.Size([])
memory (bytes)
5220204544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5220204544
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1137492500.0
relative error loss 0.1768886
shape of L is 
torch.Size([])
memory (bytes)
5223432192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5223432192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1137382900.0
relative error loss 0.17687157
time to take a step is 293.64005422592163
sum tnnu_Z after tensor(11722646., device='cuda:0')
shape of features
(3615,)
shape of features
(3615,)
number of orig particles 14460
number of new particles after remove low mass 11261
tnuZ shape should be parts x labs
torch.Size([14460, 702])
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
error without small mass is  1429510300.0
relative error without small mass is  0.22229956
nnu_Z shape should be number of particles by maxV
(14460, 702)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
shape of features
(14460,)
Fri Feb 3 02:38:31 EST 2023
