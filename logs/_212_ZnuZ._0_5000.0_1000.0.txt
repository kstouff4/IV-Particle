Tue Jan 31 20:44:26 EST 2023
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  0% |  0% |
numbers of X: 74505670
numbers of Z: 28488
shape of features
(28488,)
shape of features
(28488,)
ZX	Vol	Parts	Cubes	Eps
Z	0.02000779413123896	28488	28.488	0.0888885352657029
X	0.019526809212284585	4089	4.089	0.16839843747476624
X	0.019914770567978884	38415	38.415	0.080332538336665
X	0.019702579922578428	4076	4.076	0.1690815839045804
X	0.01919238412182607	24242	24.242	0.09250960503210975
X	0.019862632816511898	27221	27.221	0.09002799529428605
X	0.019591238996899435	117595	117.595	0.05502469983073915
X	0.019328798913489032	84548	84.548	0.06114587672428782
X	0.01956957595067844	72171	72.171	0.06472512493508355
X	0.01961849059996212	41411	41.411	0.07795607370837479
X	0.019765841852515464	58476	58.476	0.06965931822785654
X	0.019298063087388427	24839	24.839	0.09193055275957746
X	0.01954136736114242	98485	98.485	0.058325922973812394
X	0.01938882269713153	10167	10.167	0.12400890477026436
X	0.019506611457359043	543836	543.836	0.032979036261027696
X	0.019054820284335795	43432	43.432	0.07598566631711057
X	0.019634423778924347	88035	88.035	0.06064396623337839
X	0.019388032662323314	94064	94.064	0.059070425600797165
X	0.019388159230870432	42291	42.291	0.07710742636250595
X	0.019771561514133288	189323	189.323	0.04709186216668897
X	0.019661868903774353	152083	152.083	0.050564770652632436
X	0.01897854279956403	166615	166.615	0.04847482561218274
X	0.01981211868632214	330992	330.992	0.03911750450917114
X	0.019096555693308353	21767	21.767	0.09573090300156471
X	0.019639047070080864	69286	69.286	0.0656888472286108
X	0.01937631716942788	6647	6.647	0.14285052867651327
X	0.019638801807161765	87644	87.644	0.060738528438449985
X	0.019493589245923475	99012	99.012	0.05817476788971935
X	0.0196092077471285	19891	19.891	0.09952552490027973
X	0.019520806170977317	211703	211.703	0.04517764401000803
X	0.019843685885513305	1825016	1825.016	0.022153986624157962
X	0.019035373877417182	42350	42.35	0.07660127509025709
X	0.019922698692896732	1413894	1413.894	0.024153335008705007
X	0.01956913328087187	39987	39.987	0.07880448275371471
X	0.01952878581115207	29591	29.591	0.08706398340702058
X	0.019583960010756313	71784	71.784	0.06485711400445099
X	0.019642394313189298	531329	531.329	0.033312735882836814
X	0.019949383261499577	124981	124.981	0.05424526388519711
X	0.018936985416987483	2081	2.081	0.20877552678417818
X	0.0191945793509344	6160	6.16	0.14606044818790018
X	0.019171435521114125	4686	4.686	0.15993767668321787
X	0.018993758005798007	4854	4.854	0.15758094823454413
X	0.0186929802187507	4365	4.365	0.16239205068763946
X	0.018873236798309344	2379	2.379	0.19944231917560393
X	0.018977552484711037	5449	5.449	0.15157973068021777
X	0.01896294516709839	1648	1.648	0.22576131563862728
X	0.01898652095218364	1734	1.734	0.2220574934426693
X	0.01947958529528824	3723	3.723	0.17360499776362986
X	0.019365590208796805	10382	10.382	0.12309769032141628
X	0.018897533879296824	4189	4.189	0.16523355903667106
X	0.019215016399083067	18254	18.254	0.10172497014964815
X	0.019597836258681237	26112	26.112	0.09087742145112351
X	0.018647848004089893	2849	2.849	0.18705906410005146
X	0.018966430789206453	9451	9.451	0.12613509805140966
X	0.018967164585319347	4886	4.886	0.15716276108861837
X	0.01918639604407813	7041	7.041	0.13967532380994252
X	0.018958419295229467	14756	14.756	0.1087120851658057
X	0.018918546030387277	2784	2.784	0.18941142657006585
X	0.01905522534155793	8727	8.727	0.1297327739657931
X	0.019027802743883677	7261	7.261	0.13786818807418752
X	0.01923888617646101	5267	5.267	0.15400643542217027
X	0.019584173840675168	6371	6.371	0.14540049448155415
X	0.019023267511925172	3232	3.232	0.18055294960769955
X	0.019591431033548136	8308	8.308	0.13310362721171307
X	0.01948061655644509	26484	26.484	0.09026922342336477
X	0.01898711643804135	5419	5.419	0.15188444114983152
X	0.019027970600993132	5232	5.232	0.15378297047017594
X	0.019022288340458503	5904	5.904	0.1476971575044858
X	0.019396707417520226	10812	10.812	0.12150868643163816
X	0.019042286853781176	14751	14.751	0.10888445458439198
X	0.019512011949529142	6489	6.489	0.1443360313168412
X	0.019228362587896168	5822	5.822	0.1489212595094931
X	0.01924351996342689	5390	5.39	0.15283820078287533
X	0.018999572422555802	7876	7.876	0.134115623611989
X	0.018874843056180243	6496	6.496	0.14269621192907214
X	0.01891900935941203	6886	6.886	0.1400588183271519
X	0.019570111001014103	11899	11.899	0.11803956711496882
X	0.018931330882945234	1256	1.256	0.24701908134500036
X	0.018941024096301565	2257	2.257	0.2032157194736213
X	0.01899625334119622	6764	6.764	0.14108736353250043
X	0.019483510705869983	19986	19.986	0.09915480782220837
X	0.019241636229505994	3848	3.848	0.17100244191955435
X	0.019031814543194435	3725	3.725	0.17223363731039792
X	0.019011857598209683	6789	6.789	0.1409525426945767
X	0.01880294509584164	2838	2.838	0.18781826697395396
X	0.018679377468148996	2380	2.38	0.19872925925448032
X	0.01900661181335054	3101	3.101	0.1830069765889091
X	0.018948520901282545	2835	2.835	0.18836812605317219
X	0.01872350574724945	1875	1.875	0.21534194537013546
X	0.019064277946279788	6241	6.241	0.14509598425216894
X	0.019015560617709332	6523	6.523	0.14285230480738695
X	0.019005437555901974	6693	6.693	0.14160731314172167
X	0.018868030369180493	4211	4.211	0.16485942283219995
X	0.01895688563751692	3502	3.502	0.17558346535813169
X	0.019350044272307403	6896	6.896	0.14104625844949698
X	0.01872571146671237	5534	5.534	0.15012962190109533
X	0.01919410012306493	9167	9.167	0.12793229028957392
X	0.019498166936695763	4038	4.038	0.16902170951238377
X	0.01935469458849641	10582	10.582	0.12229429238587816
X	0.01920622422378581	8670	8.67	0.13035898363419837
X	0.019015996504362614	9615	9.615	0.12552298740656936
X	0.01907663100637809	17743	17.743	0.10244518292807431
X	0.01974622083437364	6304	6.304	0.14631513816176214
X	0.01900052755183056	7777	7.777	0.1346845729143454
X	0.018881708697112375	3072	3.072	0.18317802010365836
X	0.01947115695839469	24230	24.23	0.09297070291691797
X	0.018694567993015804	4144	4.144	0.1652336750327776
X	0.019345487566868932	6355	6.355	0.1449288040049538
X	0.01889431554140851	2386	2.386	0.1993212375066995
X	0.019315652181328996	3654	3.654	0.17419951617977017
X	0.018718340562735133	2032	2.032	0.20962731914906352
X	0.018935093660342385	5547	5.547	0.15056929973275485
X	0.018851998342929287	2601	2.601	0.1935258633419783
X	0.018948029774606228	3691	3.691	0.17250697954568403
X	0.01903670642883302	4101	4.101	0.16681458309974434
X	0.01945614437235095	7675	7.675	0.13635135727221243
X	0.018928916558870554	5705	5.705	0.14915003834505416
X	0.01931500368817135	5317	5.317	0.15372436639759804
X	0.019096364856115308	1402	1.402	0.23881827845678497
X	0.019852378700684407	14019	14.019	0.11229622880794221
X	0.01870695170804961	2347	2.347	0.19975453377911664
X	0.019195218801914892	18857	18.857	0.10059432680660413
X	0.018995215531905408	3617	3.617	0.17381953041501583
X	0.01895292570230106	3250	3.25	0.17999659918329736
X	0.01902244233273185	3737	3.737	0.1720208381766862
X	0.018615513096970546	4816	4.816	0.15693850491478398
X	0.019042783131193723	3533	3.533	0.17533243381230376
X	0.019007045591242177	2362	2.362	0.20039101316853095
X	0.019043515479285636	6122	6.122	0.14597705552588233
X	0.01957137070393254	13184	13.184	0.11407522907772918
X	0.018924231794890372	4259	4.259	0.16440065352290878
X	0.019485150227157907	32379	32.379	0.08442675727644287
X	0.01893216693554001	2773	2.773	0.18970705634849944
X	0.019086762074138647	5857	5.857	0.14825829861780174
X	0.018841471034333812	4362	4.362	0.1628582318139671
X	0.019016941928272635	5621	5.621	0.1501213167300562
X	0.018977698667519555	2932	2.932	0.18636307176801897
X	0.018931799716165612	3747	3.747	0.17159425206041196
X	0.018765696520244905	4306	4.306	0.1633416309679407
X	0.019017145228041772	4434	4.434	0.1624737526364543
X	0.019007777818125363	4021	4.021	0.16782851735938084
X	0.01899411277353991	5738	5.738	0.14903427392626353
X	0.018968513860198988	2488	2.488	0.19681626084728107
X	0.019553895575627856	19644	19.644	0.09984687004989469
X	0.019279201059449188	7520	7.52	0.1368643683182886
X	0.019329152201948377	6585	6.585	0.14318109370551252
X	0.018822066695556264	2880	2.88	0.1869641215813886
X	0.019181087127851694	4960	4.96	0.15696296018414072
X	0.019864946696038638	3470	3.47	0.17888996389694156
X	0.018968493349691383	3291	3.291	0.17929505761422274
X	0.018856164133707474	3881	3.881	0.1693700317880509
X	0.018771301143932963	3980	3.98	0.16770155966072256
X	0.019316581565072273	4924	4.924	0.15771418014039343
X	0.01892093167904247	5313	5.313	0.152710031666452
X	0.019124129388856333	11575	11.575	0.1182188937941715
X	0.0189364898727377	5942	5.942	0.14715971706102005
X	0.019356424382169463	22386	22.386	0.09526854781453659
X	0.018990951358320147	7977	7.977	0.13352698507685062
X	0.01892620206624377	8439	8.439	0.13089533287503727
X	0.01886637377524372	4348	4.348	0.1631046338436317
X	0.01893900324606861	1439	1.439	0.23610151554919676
X	0.019945063971783947	12642	12.642	0.11641435846793571
X	0.018749292318165838	2771	2.771	0.1891397387344352
X	0.01940961646370779	13044	13.044	0.11416563270928097
X	0.018538022976281845	1702	1.702	0.2216671240924194
X	0.018830312563028223	10707	10.707	0.12070629194172819
X	0.018999832097305616	4698	4.698	0.15932316194552282
X	0.019009324292169238	3199	3.199	0.18112739727048902
X	0.019163936147647352	8642	8.642	0.13040377118347732
X	0.018956416032596242	7592	7.592	0.13566464332536488
X	0.018981418493086905	1817	1.817	0.2186039033500856
X	0.018710564358273098	1418	1.418	0.23630353511882388
X	0.018304327269828016	4112	4.112	0.16450037363046466
X	0.019419440804249715	11682	11.682	0.11846039397533725
X	0.019302114976299013	2794	2.794	0.19045519888153337
X	0.019542623425300925	6448	6.448	0.14471690911910756
X	0.019027782053117743	5389	5.389	0.15227431747151765
X	0.019510812516424423	12074	12.074	0.11734773949380824
X	0.018825843461201103	3873	3.873	0.1693956740781704
X	0.018975720414395762	7769	7.769	0.13467212630679054
X	0.018993524600957164	6594	6.594	0.14228273572331843
X	0.01965417360905935	7802	7.802	0.13606599880442288
X	0.018711046279775396	2920	2.92	0.18573984287943618
X	0.018946081777076372	10160	10.16	0.1230859739649033
X	0.018889565066077477	4050	4.05	0.1670791709871911
X	0.019534795881469523	7123	7.123	0.139974409626497
X	0.019476540541904584	6434	6.434	0.1446583767829168
X	0.018945120542570006	4816	4.816	0.15785934563648635
X	0.018996992744051	7248	7.248	0.13787606837513602
X	0.01962538454409876	10261	10.261	0.12412983054532435
X	0.01971273566743762	9165	9.165	0.1290837211102308
X	0.01897999400967248	4893	4.893	0.1571231890136209
X	0.018769207151443968	1431	1.431	0.23583165454779825
X	0.01896160111371669	4862	4.862	0.15740554132822546
X	0.01893963487817842	6681	6.681	0.1415283282830127
X	0.019063177532787662	12619	12.619	0.11474227348898962
X	0.019245094135572894	4094	4.094	0.16751642669760192
X	0.01921407188147199	5215	5.215	0.1544501612714764
X	0.018942630447488534	3161	3.161	0.18163739453451938
X	0.018804888475377368	9304	9.304	0.12643491477088908
X	0.018949466372071157	2063	2.063	0.20942695848909185
X	0.01964306836945484	7749	7.749	0.1363498158665045
X	0.018997442832321174	5138	5.138	0.15463234741207132
X	0.01944081828090192	16490	16.49	0.10564070602586785
X	0.019477018503186627	4108	4.108	0.16799538341992531
X	0.0190284775942685	12710	12.71	0.11439828072325199
X	0.01895316786422277	5124	5.124	0.15465271999315341
X	0.01938044395095803	5243	5.243	0.15461844023137103
X	0.01954226577235766	4424	4.424	0.16407910018450814
X	0.01899519670130184	3149	3.149	0.1820358998021706
X	0.019587508137038027	9729	9.729	0.12627101229891335
X	0.019271265318680578	11412	11.412	0.11908292380953622
X	0.0189369004899062	6917	6.917	0.13989334039628057
X	0.01895646623973714	6568	6.568	0.14237751627291256
X	0.019008883809821363	20119	20.119	0.09812584473813159
X	0.018845884719493937	1530	1.53	0.23094489488521153
X	0.01865257084915301	3018	3.018	0.1835156702762432
X	0.0189886447047472	1644	1.644	0.22604629081321131
X	0.018889528141897826	2847	2.847	0.18790768729392604
X	0.019504560210318298	16153	16.153	0.1064864436525976
X	0.018969002329023765	8361	8.361	0.13140001722084385
X	0.01893574820447548	5944	5.944	0.14714128904882462
X	0.019001043640235592	1762	1.762	0.22093126731355775
X	0.019557533531319404	10777	10.777	0.12197534829274133
X	0.018812867826889124	7470	7.47	0.13605404789734868
X	0.01949197695119739	4617	4.617	0.16162132822712125
X	0.019390202654677145	6842	6.842	0.14151411489801757
X	0.018633006562142753	3602	3.602	0.17294701718501687
X	0.018982660303844847	2615	2.615	0.19362516207248487
X	0.018915156737753507	7852	7.852	0.13405300418268565
X	0.018744620355024028	1747	1.747	0.2205605898896573
X	0.01898204381615352	6091	6.091	0.1460667806296747
X	0.018991199031337273	5727	5.727	0.14912200491775093
X	0.018936966771298094	3290	3.29	0.17921382385072207
X	0.019693437761389602	6829	6.829	0.14233820150105256
X	0.01925936296599716	4518	4.518	0.16214313428131197
X	0.019012725333217503	7520	7.52	0.13623086406556237
X	0.019360666057303633	10044	10.044	0.12445275930575248
X	0.01948892412489342	7438	7.438	0.13786178487090797
X	0.018955337667075852	4933	4.933	0.15662946695734334
X	0.019041952058017134	32624	32.624	0.08357147865206938
X	0.019212073935989258	71910	71.91	0.06440629548801116
X	0.019350243540120157	10368	10.368	0.12312053168303996
X	0.01964449921050288	15989	15.989	0.1071042127017094
X	0.01863925236523006	2887	2.887	0.18620609889754552
X	0.019495893367058044	11390	11.39	0.11962073656185769
X	0.01934985052664571	11213	11.213	0.11994586041113858
X	0.019666068817361087	198223	198.223	0.04629362293496194
X	0.01943195747399878	6871	6.871	0.14141610377392555
X	0.0196536033158756	334218	334.218	0.03888697104254756
X	0.019476419313872378	99811	99.811	0.05800208023636068
X	0.01957760737778142	151434	151.434	0.05056446387595912
X	0.019881568915978493	38204	38.204	0.08043540857582823
X	0.01897850654687908	3934	3.934	0.1689698529591499
X	0.01946163712031231	7973	7.973	0.13464364769261963
X	0.0196133619575263	533891	533.891	0.03324296975234024
X	0.01993315572688452	641899	641.899	0.031431838680709416
X	0.019110724521294722	7411	7.411	0.13713030638055332
X	0.019399604422880346	64476	64.476	0.06700876224208144
X	0.01970026320717905	35632	35.632	0.08207494675280749
X	0.01936876512511875	26480	26.48	0.09010066243293416
X	0.019603466464631486	143997	143.997	0.05144302241494383
X	0.018755418715769114	60841	60.841	0.06755281846486587
X	0.019051843892842542	37530	37.53	0.07977242909230255
X	0.019202067988348705	24701	24.701	0.09194846619879936
X	0.019061808555176384	5525	5.525	0.15110445670576797
X	0.019492570278634557	211822	211.822	0.045147393424870515
X	0.01922617044209956	19132	19.132	0.10016380297655393
X	0.019016524601030052	6328	6.328	0.14430727517303144
X	0.01922730484507516	30528	30.528	0.08571826124453225
X	0.019431506903444386	141475	141.475	0.051595155561202935
X	0.019911031466682112	327102	327.102	0.03933720687281096
X	0.01971371652385813	119903	119.903	0.05478304306960749
X	0.018764082732562363	16704	16.704	0.10395266298777583
X	0.019591012723289357	92238	92.238	0.059664453866722485
X	0.019239818199173416	7877	7.877	0.13467284645752142
X	0.01905028744514851	56361	56.361	0.06965852609452511
X	0.019777029094904485	81616	81.616	0.06234408405321959
X	0.019564758936462118	32546	32.546	0.08439673137351789
X	0.019480959587223307	132434	132.434	0.052788211689224575
X	0.01841292374992932	1980	1.98	0.21029027461457458
X	0.019233279993828418	21575	21.575	0.09624263757524662
X	0.019637819034769032	169539	169.539	0.04874625007377747
X	0.019568495143717023	79519	79.519	0.06266554768047201
X	0.019537250516138985	145280	145.28	0.05123332451540432
X	0.019333046214481953	4271	4.271	0.16542084662760584
X	0.019645941674431327	346817	346.817	0.03840527015095189
X	0.019166695171985024	30594	30.594	0.08556647828912879
X	0.019384389687213212	44681	44.681	0.0757024175980176
X	0.01961413100128734	138314	138.314	0.05214759814418835
X	0.019006367587623522	3939	3.939	0.1689809372227364
X	0.019065236490127747	194460	194.46	0.046110939586467496
X	0.019753824202593855	317048	317.048	0.03964381025729638
X	0.01987322615000181	1043310	1043.31	0.026706583586094578
X	0.019275959182393474	154849	154.849	0.04993087739650181
X	0.01965143603004543	78576	78.576	0.06300400032349157
X	0.01945409300018553	21321	21.321	0.09699167791527065
X	0.01901658649814536	10623	10.623	0.12142136958455724
X	0.019662542850658066	280655	280.655	0.041224522387832425
X	0.019593207443634762	12689	12.689	0.11558266746513128
X	0.019063138214137455	43398	43.398	0.07601656255667692
X	0.019888905396951654	51960	51.96	0.07260734892427488
X	0.01925774040621588	49715	49.715	0.07289637991054448
X	0.019530869993781643	27926	27.926	0.08876415405573944
X	0.019437174863166373	7033	7.033	0.14033441054072887
X	0.01955583404270118	179371	179.371	0.047772140901373564
X	0.019054981179506367	44050	44.05	0.07562885705304515
X	0.019380233010168495	34865	34.865	0.08222232912006504
X	0.01957987410333043	8708	8.708	0.13100780137877516
X	0.019575964132437344	113185	113.185	0.05571576464217109
X	0.01962030999194492	54329	54.329	0.07121279821468486
X	0.019847226698954543	246777	246.777	0.04316499715910217
X	0.018989173788870897	11069	11.069	0.11971058705742808
X	0.01951422708887815	52786	52.786	0.07177020725882377
X	0.019834799062568065	19376	19.376	0.1007831418608836
X	0.019665908983884012	296481	296.481	0.04047986474087691
X	0.019033332061455695	49493	49.493	0.07272052659393198
X	0.019520317176871638	31001	31.001	0.08571109740559837
X	0.019056952532456817	190022	190.022	0.04646042616916233
X	0.019795381525717023	240807	240.807	0.04348087312543985
X	0.0191669593063174	5445	5.445	0.15211957809949211
X	0.019322855705945705	252243	252.243	0.042470176181724405
X	0.019864003404820228	226749	226.749	0.04441269563294957
X	0.019895286538438375	614317	614.317	0.03187517331841337
X	0.01902954600479915	25189	25.189	0.09107639534662063
X	0.018973579296735016	5230	5.23	0.1536558829749218
X	0.019004737921816985	19561	19.561	0.09904295987105799
X	0.01905710569270096	23438	23.438	0.09333520038860496
X	0.019535060418478287	35812	35.812	0.08170752603125309
X	0.019785470092982916	119831	119.831	0.05486041180348491
X	0.018789281457544594	2788	2.788	0.18888862311937127
X	0.019649469365175556	104552	104.552	0.057280415631799
X	0.019440736613641738	197585	197.585	0.046165716651817054
X	0.019420557622490113	22552	22.552	0.09513906469646427
X	0.01963913584579893	13874	13.874	0.112281161010159
X	0.019376831247233103	144183	144.183	0.05122196477414314
X	0.01953984084839751	22139	22.139	0.09592261444178872
X	0.019587102517892606	27594	27.594	0.0892041745463502
X	0.019182884943300893	23225	23.225	0.09382517859575493
X	0.019112777814276415	6921	6.921	0.14029805618811203
X	0.01981743045938862	80790	80.79	0.06259840173445712
X	0.019658986500596925	61305	61.305	0.06844710369361773
X	0.019655630966356804	32970	32.97	0.08416328551201982
X	0.019783278297219056	69609	69.609	0.06574725446675497
X	0.019484849663335867	24401	24.401	0.09277475400159245
X	0.019814087288801373	891895	891.895	0.028111678727917838
X	0.019128818139216477	42229	42.229	0.07679962751423496
X	0.019370929714628206	186207	186.207	0.04703106580431858
X	0.019029305345170625	3857	3.857	0.17023849268901284
X	0.018983206457469985	13065	13.065	0.11326265011104879
X	0.019485714345730657	7948	7.948	0.13484023199741726
X	0.01961703512276887	23317	23.317	0.09440323979794718
X	0.01906630935465163	7054	7.054	0.13929762974683166
X	0.01946915609110683	99459	99.459	0.05806320611554898
X	0.019359304236526334	10218	10.218	0.12373938376618053
X	0.018991266279811268	11248	11.248	0.11907653952418173
X	0.01979192922250842	372437	372.437	0.03759629971987806
X	0.01934067006363946	84046	84.046	0.06127991522136292
X	0.019596293690033576	46700	46.7	0.07486618211341847
X	0.019875302302152037	181102	181.102	0.04787735674648502
X	0.01983344430072984	542218	542.218	0.03319515773989557
X	0.019231966519376118	9128	9.128	0.1281984244066852
X	0.019085154481958647	15559	15.559	0.10704621920729455
X	0.019117333906342248	35638	35.638	0.08125273730133378
X	0.018736544715658724	4291	4.291	0.1634470160887645
X	0.018964743205142867	14918	14.918	0.1083291796975191
X	0.01934128362251354	22370	22.37	0.09526640345104752
X	0.018955551476415755	7700	7.7	0.13502532550939783
X	0.018957430234369178	13949	13.949	0.11076744307932565
X	0.01902694661974091	39254	39.254	0.07855281034800454
X	0.01901911863329812	21036	21.036	0.09669612601643515
X	0.019677444961894296	566060	566.06	0.0326363874094188
X	0.019073619251816387	17409	17.409	0.10309076472746384
X	0.019648457512829107	111104	111.104	0.05613058765916848
X	0.019611289772187172	11287	11.287	0.12021976570546933
X	0.019285723884263157	5344	5.344	0.15338745147369687
X	0.0196188129241815	262440	262.44	0.042125742198799644
X	0.019945213800560253	454524	454.524	0.03527177309712986
X	0.01966476204073846	648035	648.035	0.031191055945212567
X	0.019307718717902683	49074	49.074	0.07327567620243848
X	0.019305353236808147	61611	61.611	0.06792137954081326
X	0.019030255596551315	10225	10.225	0.12300622815521052
X	0.01954387356139637	19663	19.663	0.09979764392731746
X	0.019638891400530628	1547558	1547.558	0.023325090885520202
X	0.019688070250166303	26491	26.491	0.09058054566064863
X	0.01934692385502056	12974	12.974	0.11424731693834143
X	0.019292955088523533	73754	73.754	0.06395449266250096
X	0.019785288735922	833421	833.421	0.028740389599835405
X	0.018907183261522666	105484	105.484	0.056382814087459376
X	0.01912732203052931	21801	21.801	0.09573246720100376
X	0.01985996242431404	16456	16.456	0.10646774308069774
X	0.019583733345905123	13683	13.683	0.1126950187231611
X	0.01901201420376552	8683	8.683	0.12985323583331232
X	0.019664818391194452	265582	265.582	0.04199173002877575
X	0.019519541214530704	21668	21.668	0.09657919762272005
X	0.018847846914793444	4691	4.691	0.1589761972187824
X	0.019404477389754166	124506	124.506	0.05381506757424248
X	0.019826564179597245	19134	19.134	0.10119224479825725
X	0.01878845334398985	1789	1.789	0.21899131764742225
X	0.01964957717042632	131403	131.403	0.05307817430536744
X	0.019654984738478064	132730	132.73	0.05290554657603694
X	0.019902935962863834	295207	295.207	0.04070022699954773
X	0.01964963261852042	332062	332.062	0.038968326187739225
X	0.01962114735938995	331679	331.679	0.03896447311686059
X	0.019647876786760943	45033	45.033	0.0758452258262998
X	0.018995823355921326	63374	63.374	0.06692406838103844
X	0.019687054470540852	253390	253.39	0.042670764494194005
X	0.01966198726203787	351843	351.843	0.038231924961635425
X	0.01934854073611069	24639	24.639	0.09225892279779865
X	0.019724192420039827	181123	181.123	0.0477538666753329
X	0.019666046673518074	265503	265.503	0.04199676882246659
X	0.01938922234422909	211585	211.585	0.045084283462787395
X	0.019658227893378146	140493	140.493	0.051915442459834066
X	0.019566959954428823	68817	68.817	0.06575708307137244
X	0.019598360226737204	30795	30.795	0.0860161187961728
X	0.019029476202792397	16502	16.502	0.10486488473599767
X	0.019614302181729663	35364	35.364	0.08216190369230998
X	0.019775178963385077	188458	188.458	0.04716667713644573
X	0.019611431390294105	197891	197.891	0.04627656163708066
X	0.01992379937173254	335884	335.884	0.038999671397757674
X	0.019814221325569217	168723	168.723	0.04897046640098481
X	0.019903594947861796	107221	107.221	0.05704495263187892
X	0.019036429588037845	24243	24.243	0.09225708148711033
X	0.01963119251903002	374962	374.962	0.03740989377630144
X	0.018961550920491732	10766	10.766	0.12076465000948058
X	0.018966039827907066	24394	24.394	0.09195271277389705
X	0.019648092312519475	220038	220.038	0.04469657379684168
X	0.01956921728993046	126625	126.625	0.053664200769500826
X	0.018968879415352972	7539	7.539	0.13601160874453946
X	0.01946390978776914	32271	32.271	0.08449011214640208
X	0.019863574261512217	667121	667.121	0.030994473410312747
X	0.019508240329935707	28740	28.74	0.08788412729598997
X	0.019666260381057836	139551	139.551	0.05203908007308279
X	0.019031496292292335	8999	8.999	0.1283589712192965
X	0.018870434185970758	5866	5.866	0.1476204794126945
X	0.019032507759944303	8646	8.646	0.13008491218407758
X	0.019594076518039766	12347	12.347	0.11664186379337965
X	0.019679824782303658	44236	44.236	0.07633935027419934
X	0.019562157643555803	156296	156.296	0.05002145233219018
X	0.018845300876570468	6287	6.287	0.14418493027529666
X	0.0194764376936577	170634	170.634	0.048508143249767786
X	0.019237158051715192	34903	34.903	0.08198971660637498
X	0.019632301570274763	76528	76.528	0.06354044972737279
X	0.019025143878397	15056	15.056	0.10811172110770277
X	0.01956675783552894	299066	299.066	0.040294948897491176
X	0.018966050061289354	8374	8.374	0.13132517211611847
X	0.019571329907086238	51769	51.769	0.07230752902631235
X	0.0192649343729276	64191	64.191	0.06695214018308016
X	0.01957214298802413	45986	45.986	0.07522073654887523
X	0.0196129572483617	88933	88.933	0.06041712376962964
X	0.019874680395848963	582262	582.262	0.032438485429771714
X	0.019546603117413828	33677	33.677	0.08341534800861296
X	0.019655673385690197	12088	12.088	0.11759201308156082
X	0.019564889460547616	64595	64.595	0.06715723885649144
X	0.019430749609905505	44676	44.676	0.07576554607043927
X	0.019939803406535627	681801	681.801	0.030809719619637667
X	0.019031811312101674	14534	14.534	0.1094036107888401
X	0.019556760421666656	187922	187.922	0.04703701607066102
X	0.019949120696427947	522276	522.276	0.033677607379831796
X	0.019146444199938724	26990	26.99	0.08918567054957341
X	0.019606764029497414	229956	229.956	0.04401361688262032
X	0.019377736189969266	25764	25.764	0.0909417379426203
X	0.019888243766084394	660731	660.731	0.03110693704985769
X	0.019014936743430286	45120	45.12	0.07497364025516169
X	0.019272761316539343	27982	27.982	0.08831240870303314
X	0.019444176393161535	13366	13.366	0.11330855431320898
X	0.019848193971294975	7242	7.242	0.1399439597911556
X	0.019007337965086274	11439	11.439	0.11844345818329124
X	0.019642240828612375	93221	93.221	0.059505771654407076
X	0.01933238502905816	22403	22.403	0.09520499891066538
X	0.019889283607194056	251192	251.192	0.0429408969993258
X	0.018996914364084735	26613	26.613	0.08937095738617913
X	0.019163726783616306	15730	15.73	0.10680307316023237
X	0.019386353974780484	37827	37.827	0.08002605586323173
X	0.01975987421390741	235373	235.373	0.04378672977206603
X	0.019847840858094563	192543	192.543	0.04688801474409889
X	0.019882222854457964	437749	437.749	0.035679048030536675
X	0.019863785923945634	35303	35.303	0.08255628426886415
X	0.01904115367643271	10804	10.804	0.12079145915038435
X	0.0192245350907402	5839	5.839	0.148766720833838
X	0.018889864039753304	6863	6.863	0.14014306485916211
X	0.019945429975250922	285146	285.146	0.04120267125110888
X	0.019428476988076565	11843	11.843	0.11793942588005042
X	0.019025984130899815	35229	35.229	0.08143584458158938
X	0.019515131550976606	61805	61.805	0.06809511497147064
X	0.01947510376573962	25283	25.283	0.09166785277190712
X	0.01923279644034561	40235	40.235	0.07818908720744464
X	0.019277890187864348	9584	9.584	0.12623241734453308
X	0.019804815282263293	302320	302.32	0.04031202726429367
X	0.01958354644599532	87387	87.387	0.06074093936305586
X	0.018997541196531135	17590	17.59	0.10259917103256068
X	0.019502286530428344	21611	21.611	0.09663554267138502
X	0.019680029861880336	100746	100.746	0.058022884305342186
X	0.019763439629914295	111699	111.699	0.05613982857097981
X	0.019660748524374706	562594	562.594	0.03269401950427603
X	0.019417279301683195	154763	154.763	0.050061871290338796
X	0.01936764174849677	11337	11.337	0.11954356382431824
X	0.019641788213152753	50142	50.142	0.07316888839419905
X	0.01904500098290119	49697	49.697	0.07263572495955929
X	0.01954622432528231	16760	16.76	0.10525990041606662
X	0.01915521693759546	19145	19.145	0.10001778553264408
X	0.01938254038281964	9067	9.067	0.1288197228879092
X	0.019708699896507337	34809	34.809	0.0827285622118489
X	0.01890248074678944	96912	96.912	0.05799364637035966
X	0.019108675433590832	18613	18.613	0.10087992147472792
X	0.019253274753450688	31464	31.464	0.08489788902200895
X	0.019054867799502218	12834	12.834	0.11408134558541538
X	0.019465517112911	43992	43.992	0.0762015981259424
X	0.019036835940849938	9122	9.122	0.12779138187697184
X	0.019330096978357574	29061	29.061	0.0872920098115123
X	0.019172851747601444	16720	16.72	0.10466869601743448
X	0.019671743920121852	26831	26.831	0.09017136991283127
X	0.01920997925040311	9296	9.296	0.1273728677544172
X	0.018967530542218097	19723	19.723	0.09870654237171933
X	0.01942818588104911	30448	30.448	0.0860910146491446
X	0.019484623074365645	152446	152.446	0.05037232442624007
X	0.019622540490686877	10115	10.115	0.12471818507094012
X	0.019076110285716645	5033	5.033	0.15591490390966292
X	0.01901109846364198	17151	17.151	0.10349179543835227
X	0.019387045854034782	163806	163.806	0.04909763926864487
X	0.019819353033827942	917205	917.205	0.027853152267224554
X	0.019007019977984686	119075	119.075	0.05424559539472205
X	0.019624339776754936	10117	10.117	0.12471377777654238
X	0.019641682472876995	325107	325.107	0.03923895701257784
X	0.019809662757198053	246074	246.074	0.04317878832427601
X	0.019520984242213237	4413	4.413	0.16415568503931618
X	0.019556517542316056	13444	13.444	0.11330637341741609
X	0.019308199898189878	75189	75.189	0.06356174708015651
X	0.01963914053106223	183363	183.363	0.04749015539471054
X	0.019629221150895142	713511	713.511	0.0301879854289704
X	0.01966657133647203	266066	266.066	0.041967499159854135
X	0.01939478155949713	59946	59.946	0.06865016115490635
X	0.01900453864040601	9400	9.4	0.12644737727680677
X	0.01898248383332315	5209	5.209	0.15388615922112772
X	0.01961583626821744	119883	119.883	0.05469526638278266
X	0.019614299378486022	22856	22.856	0.09502929867262945
X	0.019523679433397795	39566	39.566	0.0790217252412617
X	0.01964432506592617	214758	214.758	0.045057032687181316
X	0.01992737406069084	749870	749.87	0.029841549146107742
X	0.01901773579735202	15846	15.846	0.10627058044997993
X	0.018976692885064386	3243	3.243	0.1802012752812757
X	0.0195139156638022	37283	37.283	0.0805893879781806
X	0.01960407303182588	188391	188.391	0.04703582004333395
X	0.01949305700089826	49808	49.808	0.07314651114028592
X	0.018945284797137682	14411	14.411	0.10954746698425356
X	0.018831498563279814	5707	5.707	0.14887633525112673
X	0.019107223420938887	41325	41.325	0.0773264798001773
X	0.019214044547750452	10743	10.743	0.12138482760300644
X	0.019798261757965816	62366	62.366	0.06821706923519769
X	0.019204449839644228	11527	11.527	0.11854826228603377
X	0.019275805294806866	19611	19.611	0.09942698352197836
X	0.01956323594503428	16407	16.407	0.10604019187797187
X	0.019023012288867325	14736	14.736	0.10888462469665879
X	0.01934881904723536	29819	29.819	0.08657392934305466
X	0.019593645454790334	227251	227.251	0.04417770465354354
X	0.019807685050021066	518323	518.323	0.033683024442864216
X	0.01930985330867935	109928	109.928	0.05600458060448354
X	0.01956351526947342	176482	176.482	0.04803769483784011
X	0.019226666458744632	27836	27.836	0.08839595418798417
X	0.019493594895480495	19569	19.569	0.0998713915460516
X	0.019586626303549837	30718	30.718	0.08607074599710034
X	0.019370746244414157	221190	221.19	0.04440790525697564
X	0.01978970869384858	32429	32.429	0.08482071874700424
X	0.019637188603235083	76524	76.524	0.06354682881002965
X	0.01969443638369207	20707	20.707	0.0983426992550162
X	0.019945438941387648	1656477	1656.477	0.022920252413109618
X	0.0190075873606272	23058	23.058	0.09376378278884057
X	0.019575504664283448	101762	101.762	0.05772657324631505
X	0.019606330707787854	326695	326.695	0.039151758998180894
X	0.019396258553970952	63053	63.053	0.06750522720421148
X	0.019696291416539852	38869	38.869	0.07972484763448001
X	0.019945965226516368	1519140	1519.14	0.0235913296646647
X	0.01969875934994778	299348	299.348	0.04037267303684175
X	0.019449655118277218	122604	122.604	0.054133871594347724
X	0.01898609406207982	10826	10.826	0.1205931210292379
X	0.019647512884910238	53685	53.685	0.07152944822636843
X	0.01922713962212476	21691	21.691	0.09606054255546838
X	0.01962734651909889	287637	287.637	0.04086381848674809
X	0.019966202744846874	1340038	1340.038	0.024607044478513588
X	0.019625175647616847	145635	145.635	0.051268340979420075
X	0.019951417021909085	251332	251.332	0.04297758286583327
X	0.019625376861815806	92380	92.38	0.059668714622471065
X	0.019032019051689292	45093	45.093	0.07501105000320808
X	0.019931972699343624	44499	44.499	0.0765126676429371
X	0.019140777495310766	18755	18.755	0.10068099574081105
X	0.019667162336154058	450698	450.698	0.03520616260986054
X	0.019748095239815962	608221	608.221	0.03190224657250092
X	0.01909108602593101	105252	105.252	0.05660655944239785
X	0.01966456981355196	1798429	1798.429	0.022195437896839813
X	0.019842257155627565	221152	221.152	0.04476790118096438
X	0.01903163937508113	75866	75.866	0.06306808888630613
X	0.01892922729280444	38743	38.743	0.0787613611823444
X	0.01991523956543454	101916	101.916	0.05802935209342046
X	0.01959807459654976	212878	212.878	0.04515378994855277
X	0.01899918424845707	18673	18.673	0.10057891640549316
X	0.019958501105679507	911361	911.361	0.027977731257260746
X	0.01909979630027151	28872	28.872	0.08713325849007884
X	0.01983455683092233	57050	57.05	0.0703162363975886
X	0.019041756226638992	9860	9.86	0.12453077628317552
X	0.01920528209695712	34763	34.763	0.082054262690237
X	0.018847704604320697	3714	3.714	0.17184577368599857
X	0.019170286974796478	15140	15.14	0.10818513209313248
X	0.019796045217179987	39086	39.086	0.0797111490885474
X	0.019593727791901102	56923	56.923	0.07008252865116195
X	0.02000253644679312	3571036	3571.036	0.0177594814414481
X	0.01960906642481404	29505	29.505	0.08726774747129877
X	0.019502415493203088	509859	509.859	0.03369349830582236
X	0.01960370154011601	10604	10.604	0.1227315222788282
X	0.019639270441008796	28606	28.606	0.0882177693428446
X	0.018947379688719453	15083	15.083	0.10789975925205665
X	0.019613708530606875	273164	273.164	0.04156349739953737
X	0.019439414588949298	91788	91.788	0.05960725079024546
X	0.019905330111695086	1276179	1276.179	0.024985378342834444
X	0.019636419960316297	11029	11.029	0.12120170379979173
X	0.019825508742077825	256325	256.325	0.0426066895262994
X	0.019047864392179033	39643	39.643	0.07832371155422344
X	0.019614636263226765	172013	172.013	0.04849232377636563
X	0.019730086491232528	212567	212.567	0.04527700750232141
X	0.01902351180542525	36303	36.303	0.08062122489573742
X	0.01922054708942395	20673	20.673	0.09760095792889487
X	0.01990068751989417	110993	110.993	0.056388537414965606
X	0.019632063169886204	43649	43.649	0.07661795359126763
X	0.01957196784484056	79979	79.979	0.06254887572639456
X	0.019587004235197034	673588	673.588	0.030750904311978175
X	0.019202460419295804	15295	15.295	0.10787872354293922
X	0.019863130238356723	95645	95.645	0.05921910047004964
X	0.01959711896646606	37664	37.664	0.08043071497396836
X	0.01959760460581118	180664	180.664	0.047691808332534426
X	0.01977496410053373	469274	469.274	0.03479870402892124
X	0.019643850922776148	98273	98.273	0.058469691874830344
X	0.019444311450845363	16521	16.521	0.10558091283886015
X	0.01961045915081584	268455	268.455	0.041802804752663444
X	0.019591680476406113	154957	154.957	0.05019034294146507
X	0.019801740720217332	786474	786.474	0.029309359863829567
X	0.019639153886921792	190649	190.649	0.046877317008837675
X	0.019385965961714536	60211	60.211	0.06853891134214757
X	0.01977723872684362	69193	69.193	0.06587204783723226
X	0.019553546088322157	188177	188.177	0.04701318376858328
X	0.01959523717411846	244185	244.185	0.04313308428831057
X	0.019583163081853507	68920	68.92	0.06574244584132444
X	0.019297827792990448	9327	9.327	0.12742524676010655
X	0.019563259038871902	13725	13.725	0.1125407013818526
X	0.01963083755212362	104475	104.475	0.05727636963219921
X	0.019726001349058536	180160	180.16	0.047840263360044294
X	0.019617192643895394	108815	108.815	0.05649144670555119
X	0.019346078346343883	13576	13.576	0.112531395427262
X	0.01927437079721827	36342	36.342	0.08094508097496059
X	0.01962201843990254	74268	74.268	0.06416733220116257
X	0.019547819107456864	60433	60.433	0.0686448645672279
X	0.0196094096632592	34354	34.354	0.08295242172556372
X	0.019687415914508063	87611	87.611	0.060796236105655835
X	0.019638585820982074	25736	25.736	0.0913811012853275
X	0.019145639309364988	44314	44.314	0.07559788402558737
X	0.019352740725628422	9430	9.43	0.1270799187589755
X	0.019409811032214414	11956	11.956	0.11752902507908602
X	0.019662483039229916	295578	295.578	0.040518692092320494
X	0.019174699462697335	27889	27.889	0.08826026108307505
X	0.019465368113084268	119402	119.402	0.054628214573746496
X	0.01900313362635712	9990	9.99	0.12390435692382863
X	0.018927481113477786	12474	12.474	0.11491120017271317
X	0.01941966546951442	74521	74.521	0.06387354545862121
X	0.019460076978945244	160088	160.088	0.04953687336908581
X	0.019196774256298507	47920	47.92	0.07371748037176615
X	0.01933095977830241	9233	9.233	0.127929350178682
X	0.018984629113029512	17294	17.294	0.10315783545389959
X	0.019606652368831426	54150	54.15	0.07127463401201578
X	0.019619942692302265	45200	45.2	0.07571578567721778
X	0.019602573684803323	132454	132.454	0.05289516858325704
X	0.019048620069756692	12620	12.62	0.11471002866364541
X	0.01881415884304528	17205	17.205	0.10302517869239028
X	0.019177409033307245	112889	112.889	0.05538337723287429
X	0.01902227827530192	8604	8.604	0.13027288857443825
X	0.01984797703756696	229964	229.964	0.044192862353793265
X	0.019474453690261424	12640	12.64	0.11549754060288908
X	0.019600879187158395	134384	134.384	0.05263920593593974
X	0.019632686218278985	192693	192.693	0.04670584808566774
X	0.01960812452228461	46712	46.712	0.07487483264127308
X	0.019658677243042502	168679	168.679	0.04884623467056685
X	0.019699017701919227	157482	157.482	0.05001165381312851
X	0.018922574933493745	2498	2.498	0.19639447526151324
X	0.019649480635010774	96193	96.193	0.058893749141561015
X	0.01905729577730097	31453	31.453	0.08461870991233579
X	0.019549582821183437	259535	259.535	0.0422325385214565
X	0.019516678762898	119469	119.469	0.054665949411346365
X	0.019418482853052026	77278	77.278	0.06310345537846461
X	0.01964037528058381	122973	122.973	0.054255862433180736
X	0.019470759844014567	48308	48.308	0.07386770637093036
X	0.01893214678860682	43720	43.72	0.07565539503171413
X	0.019016255071361347	7838	7.838	0.13437131840037378
X	0.019293017234588282	15866	15.866	0.10673599446080262
X	0.019767973928277192	321658	321.658	0.03946292457681495
X	0.019945394783511147	805602	805.602	0.02914566935387672
X	0.019751068247919647	42328	42.328	0.07756294318963268
X	0.019049704716882252	21817	21.817	0.0955794232443274
X	0.019645097685580596	51909	51.909	0.07233311641429357
X	0.019653867153714154	237103	237.103	0.043601723421245284
X	0.019040603481341883	93180	93.18	0.0589005478493644
X	0.01904510481268373	15931	15.931	0.10613210462795422
X	0.019894411255106492	76756	76.756	0.06375871941549247
X	0.019669474378333666	193892	193.892	0.04663846928272674
X	0.019128098737692823	11734	11.734	0.11769063524839908
X	0.019268235974673538	50582	50.582	0.07249064873792611
X	0.019513557929078003	72704	72.704	0.06450490186999944
time for making epsilon is 4.013752698898315
epsilons are
[0.16839843747476624, 0.080332538336665, 0.1690815839045804, 0.09250960503210975, 0.09002799529428605, 0.05502469983073915, 0.06114587672428782, 0.06472512493508355, 0.07795607370837479, 0.06965931822785654, 0.09193055275957746, 0.058325922973812394, 0.12400890477026436, 0.032979036261027696, 0.07598566631711057, 0.06064396623337839, 0.059070425600797165, 0.07710742636250595, 0.04709186216668897, 0.050564770652632436, 0.04847482561218274, 0.03911750450917114, 0.09573090300156471, 0.0656888472286108, 0.14285052867651327, 0.060738528438449985, 0.05817476788971935, 0.09952552490027973, 0.04517764401000803, 0.022153986624157962, 0.07660127509025709, 0.024153335008705007, 0.07880448275371471, 0.08706398340702058, 0.06485711400445099, 0.033312735882836814, 0.05424526388519711, 0.20877552678417818, 0.14606044818790018, 0.15993767668321787, 0.15758094823454413, 0.16239205068763946, 0.19944231917560393, 0.15157973068021777, 0.22576131563862728, 0.2220574934426693, 0.17360499776362986, 0.12309769032141628, 0.16523355903667106, 0.10172497014964815, 0.09087742145112351, 0.18705906410005146, 0.12613509805140966, 0.15716276108861837, 0.13967532380994252, 0.1087120851658057, 0.18941142657006585, 0.1297327739657931, 0.13786818807418752, 0.15400643542217027, 0.14540049448155415, 0.18055294960769955, 0.13310362721171307, 0.09026922342336477, 0.15188444114983152, 0.15378297047017594, 0.1476971575044858, 0.12150868643163816, 0.10888445458439198, 0.1443360313168412, 0.1489212595094931, 0.15283820078287533, 0.134115623611989, 0.14269621192907214, 0.1400588183271519, 0.11803956711496882, 0.24701908134500036, 0.2032157194736213, 0.14108736353250043, 0.09915480782220837, 0.17100244191955435, 0.17223363731039792, 0.1409525426945767, 0.18781826697395396, 0.19872925925448032, 0.1830069765889091, 0.18836812605317219, 0.21534194537013546, 0.14509598425216894, 0.14285230480738695, 0.14160731314172167, 0.16485942283219995, 0.17558346535813169, 0.14104625844949698, 0.15012962190109533, 0.12793229028957392, 0.16902170951238377, 0.12229429238587816, 0.13035898363419837, 0.12552298740656936, 0.10244518292807431, 0.14631513816176214, 0.1346845729143454, 0.18317802010365836, 0.09297070291691797, 0.1652336750327776, 0.1449288040049538, 0.1993212375066995, 0.17419951617977017, 0.20962731914906352, 0.15056929973275485, 0.1935258633419783, 0.17250697954568403, 0.16681458309974434, 0.13635135727221243, 0.14915003834505416, 0.15372436639759804, 0.23881827845678497, 0.11229622880794221, 0.19975453377911664, 0.10059432680660413, 0.17381953041501583, 0.17999659918329736, 0.1720208381766862, 0.15693850491478398, 0.17533243381230376, 0.20039101316853095, 0.14597705552588233, 0.11407522907772918, 0.16440065352290878, 0.08442675727644287, 0.18970705634849944, 0.14825829861780174, 0.1628582318139671, 0.1501213167300562, 0.18636307176801897, 0.17159425206041196, 0.1633416309679407, 0.1624737526364543, 0.16782851735938084, 0.14903427392626353, 0.19681626084728107, 0.09984687004989469, 0.1368643683182886, 0.14318109370551252, 0.1869641215813886, 0.15696296018414072, 0.17888996389694156, 0.17929505761422274, 0.1693700317880509, 0.16770155966072256, 0.15771418014039343, 0.152710031666452, 0.1182188937941715, 0.14715971706102005, 0.09526854781453659, 0.13352698507685062, 0.13089533287503727, 0.1631046338436317, 0.23610151554919676, 0.11641435846793571, 0.1891397387344352, 0.11416563270928097, 0.2216671240924194, 0.12070629194172819, 0.15932316194552282, 0.18112739727048902, 0.13040377118347732, 0.13566464332536488, 0.2186039033500856, 0.23630353511882388, 0.16450037363046466, 0.11846039397533725, 0.19045519888153337, 0.14471690911910756, 0.15227431747151765, 0.11734773949380824, 0.1693956740781704, 0.13467212630679054, 0.14228273572331843, 0.13606599880442288, 0.18573984287943618, 0.1230859739649033, 0.1670791709871911, 0.139974409626497, 0.1446583767829168, 0.15785934563648635, 0.13787606837513602, 0.12412983054532435, 0.1290837211102308, 0.1571231890136209, 0.23583165454779825, 0.15740554132822546, 0.1415283282830127, 0.11474227348898962, 0.16751642669760192, 0.1544501612714764, 0.18163739453451938, 0.12643491477088908, 0.20942695848909185, 0.1363498158665045, 0.15463234741207132, 0.10564070602586785, 0.16799538341992531, 0.11439828072325199, 0.15465271999315341, 0.15461844023137103, 0.16407910018450814, 0.1820358998021706, 0.12627101229891335, 0.11908292380953622, 0.13989334039628057, 0.14237751627291256, 0.09812584473813159, 0.23094489488521153, 0.1835156702762432, 0.22604629081321131, 0.18790768729392604, 0.1064864436525976, 0.13140001722084385, 0.14714128904882462, 0.22093126731355775, 0.12197534829274133, 0.13605404789734868, 0.16162132822712125, 0.14151411489801757, 0.17294701718501687, 0.19362516207248487, 0.13405300418268565, 0.2205605898896573, 0.1460667806296747, 0.14912200491775093, 0.17921382385072207, 0.14233820150105256, 0.16214313428131197, 0.13623086406556237, 0.12445275930575248, 0.13786178487090797, 0.15662946695734334, 0.08357147865206938, 0.06440629548801116, 0.12312053168303996, 0.1071042127017094, 0.18620609889754552, 0.11962073656185769, 0.11994586041113858, 0.04629362293496194, 0.14141610377392555, 0.03888697104254756, 0.05800208023636068, 0.05056446387595912, 0.08043540857582823, 0.1689698529591499, 0.13464364769261963, 0.03324296975234024, 0.031431838680709416, 0.13713030638055332, 0.06700876224208144, 0.08207494675280749, 0.09010066243293416, 0.05144302241494383, 0.06755281846486587, 0.07977242909230255, 0.09194846619879936, 0.15110445670576797, 0.045147393424870515, 0.10016380297655393, 0.14430727517303144, 0.08571826124453225, 0.051595155561202935, 0.03933720687281096, 0.05478304306960749, 0.10395266298777583, 0.059664453866722485, 0.13467284645752142, 0.06965852609452511, 0.06234408405321959, 0.08439673137351789, 0.052788211689224575, 0.21029027461457458, 0.09624263757524662, 0.04874625007377747, 0.06266554768047201, 0.05123332451540432, 0.16542084662760584, 0.03840527015095189, 0.08556647828912879, 0.0757024175980176, 0.05214759814418835, 0.1689809372227364, 0.046110939586467496, 0.03964381025729638, 0.026706583586094578, 0.04993087739650181, 0.06300400032349157, 0.09699167791527065, 0.12142136958455724, 0.041224522387832425, 0.11558266746513128, 0.07601656255667692, 0.07260734892427488, 0.07289637991054448, 0.08876415405573944, 0.14033441054072887, 0.047772140901373564, 0.07562885705304515, 0.08222232912006504, 0.13100780137877516, 0.05571576464217109, 0.07121279821468486, 0.04316499715910217, 0.11971058705742808, 0.07177020725882377, 0.1007831418608836, 0.04047986474087691, 0.07272052659393198, 0.08571109740559837, 0.04646042616916233, 0.04348087312543985, 0.15211957809949211, 0.042470176181724405, 0.04441269563294957, 0.03187517331841337, 0.09107639534662063, 0.1536558829749218, 0.09904295987105799, 0.09333520038860496, 0.08170752603125309, 0.05486041180348491, 0.18888862311937127, 0.057280415631799, 0.046165716651817054, 0.09513906469646427, 0.112281161010159, 0.05122196477414314, 0.09592261444178872, 0.0892041745463502, 0.09382517859575493, 0.14029805618811203, 0.06259840173445712, 0.06844710369361773, 0.08416328551201982, 0.06574725446675497, 0.09277475400159245, 0.028111678727917838, 0.07679962751423496, 0.04703106580431858, 0.17023849268901284, 0.11326265011104879, 0.13484023199741726, 0.09440323979794718, 0.13929762974683166, 0.05806320611554898, 0.12373938376618053, 0.11907653952418173, 0.03759629971987806, 0.06127991522136292, 0.07486618211341847, 0.04787735674648502, 0.03319515773989557, 0.1281984244066852, 0.10704621920729455, 0.08125273730133378, 0.1634470160887645, 0.1083291796975191, 0.09526640345104752, 0.13502532550939783, 0.11076744307932565, 0.07855281034800454, 0.09669612601643515, 0.0326363874094188, 0.10309076472746384, 0.05613058765916848, 0.12021976570546933, 0.15338745147369687, 0.042125742198799644, 0.03527177309712986, 0.031191055945212567, 0.07327567620243848, 0.06792137954081326, 0.12300622815521052, 0.09979764392731746, 0.023325090885520202, 0.09058054566064863, 0.11424731693834143, 0.06395449266250096, 0.028740389599835405, 0.056382814087459376, 0.09573246720100376, 0.10646774308069774, 0.1126950187231611, 0.12985323583331232, 0.04199173002877575, 0.09657919762272005, 0.1589761972187824, 0.05381506757424248, 0.10119224479825725, 0.21899131764742225, 0.05307817430536744, 0.05290554657603694, 0.04070022699954773, 0.038968326187739225, 0.03896447311686059, 0.0758452258262998, 0.06692406838103844, 0.042670764494194005, 0.038231924961635425, 0.09225892279779865, 0.0477538666753329, 0.04199676882246659, 0.045084283462787395, 0.051915442459834066, 0.06575708307137244, 0.0860161187961728, 0.10486488473599767, 0.08216190369230998, 0.04716667713644573, 0.04627656163708066, 0.038999671397757674, 0.04897046640098481, 0.05704495263187892, 0.09225708148711033, 0.03740989377630144, 0.12076465000948058, 0.09195271277389705, 0.04469657379684168, 0.053664200769500826, 0.13601160874453946, 0.08449011214640208, 0.030994473410312747, 0.08788412729598997, 0.05203908007308279, 0.1283589712192965, 0.1476204794126945, 0.13008491218407758, 0.11664186379337965, 0.07633935027419934, 0.05002145233219018, 0.14418493027529666, 0.048508143249767786, 0.08198971660637498, 0.06354044972737279, 0.10811172110770277, 0.040294948897491176, 0.13132517211611847, 0.07230752902631235, 0.06695214018308016, 0.07522073654887523, 0.06041712376962964, 0.032438485429771714, 0.08341534800861296, 0.11759201308156082, 0.06715723885649144, 0.07576554607043927, 0.030809719619637667, 0.1094036107888401, 0.04703701607066102, 0.033677607379831796, 0.08918567054957341, 0.04401361688262032, 0.0909417379426203, 0.03110693704985769, 0.07497364025516169, 0.08831240870303314, 0.11330855431320898, 0.1399439597911556, 0.11844345818329124, 0.059505771654407076, 0.09520499891066538, 0.0429408969993258, 0.08937095738617913, 0.10680307316023237, 0.08002605586323173, 0.04378672977206603, 0.04688801474409889, 0.035679048030536675, 0.08255628426886415, 0.12079145915038435, 0.148766720833838, 0.14014306485916211, 0.04120267125110888, 0.11793942588005042, 0.08143584458158938, 0.06809511497147064, 0.09166785277190712, 0.07818908720744464, 0.12623241734453308, 0.04031202726429367, 0.06074093936305586, 0.10259917103256068, 0.09663554267138502, 0.058022884305342186, 0.05613982857097981, 0.03269401950427603, 0.050061871290338796, 0.11954356382431824, 0.07316888839419905, 0.07263572495955929, 0.10525990041606662, 0.10001778553264408, 0.1288197228879092, 0.0827285622118489, 0.05799364637035966, 0.10087992147472792, 0.08489788902200895, 0.11408134558541538, 0.0762015981259424, 0.12779138187697184, 0.0872920098115123, 0.10466869601743448, 0.09017136991283127, 0.1273728677544172, 0.09870654237171933, 0.0860910146491446, 0.05037232442624007, 0.12471818507094012, 0.15591490390966292, 0.10349179543835227, 0.04909763926864487, 0.027853152267224554, 0.05424559539472205, 0.12471377777654238, 0.03923895701257784, 0.04317878832427601, 0.16415568503931618, 0.11330637341741609, 0.06356174708015651, 0.04749015539471054, 0.0301879854289704, 0.041967499159854135, 0.06865016115490635, 0.12644737727680677, 0.15388615922112772, 0.05469526638278266, 0.09502929867262945, 0.0790217252412617, 0.045057032687181316, 0.029841549146107742, 0.10627058044997993, 0.1802012752812757, 0.0805893879781806, 0.04703582004333395, 0.07314651114028592, 0.10954746698425356, 0.14887633525112673, 0.0773264798001773, 0.12138482760300644, 0.06821706923519769, 0.11854826228603377, 0.09942698352197836, 0.10604019187797187, 0.10888462469665879, 0.08657392934305466, 0.04417770465354354, 0.033683024442864216, 0.05600458060448354, 0.04803769483784011, 0.08839595418798417, 0.0998713915460516, 0.08607074599710034, 0.04440790525697564, 0.08482071874700424, 0.06354682881002965, 0.0983426992550162, 0.022920252413109618, 0.09376378278884057, 0.05772657324631505, 0.039151758998180894, 0.06750522720421148, 0.07972484763448001, 0.0235913296646647, 0.04037267303684175, 0.054133871594347724, 0.1205931210292379, 0.07152944822636843, 0.09606054255546838, 0.04086381848674809, 0.024607044478513588, 0.051268340979420075, 0.04297758286583327, 0.059668714622471065, 0.07501105000320808, 0.0765126676429371, 0.10068099574081105, 0.03520616260986054, 0.03190224657250092, 0.05660655944239785, 0.022195437896839813, 0.04476790118096438, 0.06306808888630613, 0.0787613611823444, 0.05802935209342046, 0.04515378994855277, 0.10057891640549316, 0.027977731257260746, 0.08713325849007884, 0.0703162363975886, 0.12453077628317552, 0.082054262690237, 0.17184577368599857, 0.10818513209313248, 0.0797111490885474, 0.07008252865116195, 0.0177594814414481, 0.08726774747129877, 0.03369349830582236, 0.1227315222788282, 0.0882177693428446, 0.10789975925205665, 0.04156349739953737, 0.05960725079024546, 0.024985378342834444, 0.12120170379979173, 0.0426066895262994, 0.07832371155422344, 0.04849232377636563, 0.04527700750232141, 0.08062122489573742, 0.09760095792889487, 0.056388537414965606, 0.07661795359126763, 0.06254887572639456, 0.030750904311978175, 0.10787872354293922, 0.05921910047004964, 0.08043071497396836, 0.047691808332534426, 0.03479870402892124, 0.058469691874830344, 0.10558091283886015, 0.041802804752663444, 0.05019034294146507, 0.029309359863829567, 0.046877317008837675, 0.06853891134214757, 0.06587204783723226, 0.04701318376858328, 0.04313308428831057, 0.06574244584132444, 0.12742524676010655, 0.1125407013818526, 0.05727636963219921, 0.047840263360044294, 0.05649144670555119, 0.112531395427262, 0.08094508097496059, 0.06416733220116257, 0.0686448645672279, 0.08295242172556372, 0.060796236105655835, 0.0913811012853275, 0.07559788402558737, 0.1270799187589755, 0.11752902507908602, 0.040518692092320494, 0.08826026108307505, 0.054628214573746496, 0.12390435692382863, 0.11491120017271317, 0.06387354545862121, 0.04953687336908581, 0.07371748037176615, 0.127929350178682, 0.10315783545389959, 0.07127463401201578, 0.07571578567721778, 0.05289516858325704, 0.11471002866364541, 0.10302517869239028, 0.05538337723287429, 0.13027288857443825, 0.044192862353793265, 0.11549754060288908, 0.05263920593593974, 0.04670584808566774, 0.07487483264127308, 0.04884623467056685, 0.05001165381312851, 0.19639447526151324, 0.058893749141561015, 0.08461870991233579, 0.0422325385214565, 0.054665949411346365, 0.06310345537846461, 0.054255862433180736, 0.07386770637093036, 0.07565539503171413, 0.13437131840037378, 0.10673599446080262, 0.03946292457681495, 0.02914566935387672, 0.07756294318963268, 0.0955794232443274, 0.07233311641429357, 0.043601723421245284, 0.0589005478493644, 0.10613210462795422, 0.06375871941549247, 0.04663846928272674, 0.11769063524839908, 0.07249064873792611, 0.06450490186999944]
0.0888885352657029
Making ranges
torch.Size([53170, 2])
We keep 8.11e+06/8.12e+08 =  0% of the original kernel matrix.

torch.Size([10267, 2])
We keep 5.57e+05/1.67e+07 =  3% of the original kernel matrix.

torch.Size([24350, 2])
We keep 2.06e+06/1.16e+08 =  1% of the original kernel matrix.

torch.Size([68199, 2])
We keep 1.99e+07/1.48e+09 =  1% of the original kernel matrix.

torch.Size([62465, 2])
We keep 1.15e+07/1.09e+09 =  1% of the original kernel matrix.

torch.Size([10303, 2])
We keep 5.23e+05/1.66e+07 =  3% of the original kernel matrix.

torch.Size([24512, 2])
We keep 2.06e+06/1.16e+08 =  1% of the original kernel matrix.

torch.Size([37770, 2])
We keep 1.75e+07/5.88e+08 =  2% of the original kernel matrix.

torch.Size([46268, 2])
We keep 7.91e+06/6.91e+08 =  1% of the original kernel matrix.

torch.Size([48983, 2])
We keep 1.10e+07/7.41e+08 =  1% of the original kernel matrix.

torch.Size([54421, 2])
We keep 8.71e+06/7.75e+08 =  1% of the original kernel matrix.

torch.Size([215387, 2])
We keep 1.08e+08/1.38e+10 =  0% of the original kernel matrix.

torch.Size([110715, 2])
We keep 2.91e+07/3.35e+09 =  0% of the original kernel matrix.

torch.Size([159680, 2])
We keep 6.29e+07/7.15e+09 =  0% of the original kernel matrix.

torch.Size([93459, 2])
We keep 2.19e+07/2.41e+09 =  0% of the original kernel matrix.

torch.Size([137223, 2])
We keep 4.59e+07/5.21e+09 =  0% of the original kernel matrix.

torch.Size([86033, 2])
We keep 1.92e+07/2.06e+09 =  0% of the original kernel matrix.

torch.Size([71349, 2])
We keep 4.97e+07/1.71e+09 =  2% of the original kernel matrix.

torch.Size([62859, 2])
We keep 1.22e+07/1.18e+09 =  1% of the original kernel matrix.

torch.Size([95505, 2])
We keep 8.40e+07/3.42e+09 =  2% of the original kernel matrix.

torch.Size([72621, 2])
We keep 1.57e+07/1.67e+09 =  0% of the original kernel matrix.

torch.Size([45444, 2])
We keep 8.97e+06/6.17e+08 =  1% of the original kernel matrix.

torch.Size([51553, 2])
We keep 8.00e+06/7.08e+08 =  1% of the original kernel matrix.

torch.Size([164890, 2])
We keep 1.32e+08/9.70e+09 =  1% of the original kernel matrix.

torch.Size([95054, 2])
We keep 2.49e+07/2.81e+09 =  0% of the original kernel matrix.

torch.Size([22585, 2])
We keep 2.03e+06/1.03e+08 =  1% of the original kernel matrix.

torch.Size([35433, 2])
We keep 3.98e+06/2.90e+08 =  1% of the original kernel matrix.

torch.Size([1041286, 2])
We keep 2.02e+09/2.96e+11 =  0% of the original kernel matrix.

torch.Size([255139, 2])
We keep 1.13e+08/1.55e+10 =  0% of the original kernel matrix.

torch.Size([80668, 2])
We keep 1.94e+07/1.89e+09 =  1% of the original kernel matrix.

torch.Size([66974, 2])
We keep 1.25e+07/1.24e+09 =  1% of the original kernel matrix.

torch.Size([164108, 2])
We keep 8.08e+07/7.75e+09 =  1% of the original kernel matrix.

torch.Size([95209, 2])
We keep 2.27e+07/2.51e+09 =  0% of the original kernel matrix.

torch.Size([175918, 2])
We keep 8.83e+07/8.85e+09 =  0% of the original kernel matrix.

torch.Size([99007, 2])
We keep 2.40e+07/2.68e+09 =  0% of the original kernel matrix.

torch.Size([76286, 2])
We keep 2.07e+07/1.79e+09 =  1% of the original kernel matrix.

torch.Size([65216, 2])
We keep 1.22e+07/1.20e+09 =  1% of the original kernel matrix.

torch.Size([327603, 2])
We keep 3.84e+08/3.58e+10 =  1% of the original kernel matrix.

torch.Size([139151, 2])
We keep 4.41e+07/5.39e+09 =  0% of the original kernel matrix.

torch.Size([276795, 2])
We keep 2.03e+08/2.31e+10 =  0% of the original kernel matrix.

torch.Size([126937, 2])
We keep 3.62e+07/4.33e+09 =  0% of the original kernel matrix.

torch.Size([284588, 2])
We keep 2.50e+08/2.78e+10 =  0% of the original kernel matrix.

torch.Size([127207, 2])
We keep 3.94e+07/4.75e+09 =  0% of the original kernel matrix.

torch.Size([634091, 2])
We keep 6.25e+08/1.10e+11 =  0% of the original kernel matrix.

torch.Size([193624, 2])
We keep 7.12e+07/9.43e+09 =  0% of the original kernel matrix.

torch.Size([41165, 2])
We keep 6.79e+06/4.74e+08 =  1% of the original kernel matrix.

torch.Size([49320, 2])
We keep 7.19e+06/6.20e+08 =  1% of the original kernel matrix.

torch.Size([117888, 2])
We keep 1.08e+08/4.80e+09 =  2% of the original kernel matrix.

torch.Size([80421, 2])
We keep 1.85e+07/1.97e+09 =  0% of the original kernel matrix.

torch.Size([12105, 2])
We keep 9.28e+06/4.42e+07 = 20% of the original kernel matrix.

torch.Size([26021, 2])
We keep 2.60e+06/1.89e+08 =  1% of the original kernel matrix.

torch.Size([162150, 2])
We keep 6.34e+07/7.68e+09 =  0% of the original kernel matrix.

torch.Size([94809, 2])
We keep 2.25e+07/2.50e+09 =  0% of the original kernel matrix.

torch.Size([181858, 2])
We keep 9.64e+07/9.80e+09 =  0% of the original kernel matrix.

torch.Size([100894, 2])
We keep 2.51e+07/2.82e+09 =  0% of the original kernel matrix.

torch.Size([37223, 2])
We keep 8.40e+06/3.96e+08 =  2% of the original kernel matrix.

torch.Size([46602, 2])
We keep 6.66e+06/5.67e+08 =  1% of the original kernel matrix.

torch.Size([254861, 2])
We keep 6.14e+08/4.48e+10 =  1% of the original kernel matrix.

torch.Size([118455, 2])
We keep 4.91e+07/6.03e+09 =  0% of the original kernel matrix.

torch.Size([4092948, 2])
We keep 1.37e+10/3.33e+12 =  0% of the original kernel matrix.

torch.Size([516698, 2])
We keep 3.44e+08/5.20e+10 =  0% of the original kernel matrix.

torch.Size([76604, 2])
We keep 2.35e+07/1.79e+09 =  1% of the original kernel matrix.

torch.Size([64815, 2])
We keep 1.24e+07/1.21e+09 =  1% of the original kernel matrix.

torch.Size([2966830, 2])
We keep 9.10e+09/2.00e+12 =  0% of the original kernel matrix.

torch.Size([442656, 2])
We keep 2.74e+08/4.03e+10 =  0% of the original kernel matrix.

torch.Size([68122, 2])
We keep 3.27e+07/1.60e+09 =  2% of the original kernel matrix.

torch.Size([61787, 2])
We keep 1.19e+07/1.14e+09 =  1% of the original kernel matrix.

torch.Size([53497, 2])
We keep 1.27e+07/8.76e+08 =  1% of the original kernel matrix.

torch.Size([56022, 2])
We keep 9.34e+06/8.43e+08 =  1% of the original kernel matrix.

torch.Size([130186, 2])
We keep 5.70e+07/5.15e+09 =  1% of the original kernel matrix.

torch.Size([83869, 2])
We keep 1.92e+07/2.04e+09 =  0% of the original kernel matrix.

torch.Size([956932, 2])
We keep 2.15e+09/2.82e+11 =  0% of the original kernel matrix.

torch.Size([243406, 2])
We keep 1.11e+08/1.51e+10 =  0% of the original kernel matrix.

torch.Size([225643, 2])
We keep 1.69e+08/1.56e+10 =  1% of the original kernel matrix.

torch.Size([114062, 2])
We keep 3.08e+07/3.56e+09 =  0% of the original kernel matrix.

torch.Size([5830, 2])
We keep 1.89e+05/4.33e+06 =  4% of the original kernel matrix.

torch.Size([19306, 2])
We keep 1.27e+06/5.93e+07 =  2% of the original kernel matrix.

torch.Size([14992, 2])
We keep 1.02e+06/3.79e+07 =  2% of the original kernel matrix.

torch.Size([28875, 2])
We keep 2.74e+06/1.75e+08 =  1% of the original kernel matrix.

torch.Size([11469, 2])
We keep 6.60e+05/2.20e+07 =  3% of the original kernel matrix.

torch.Size([25424, 2])
We keep 2.25e+06/1.33e+08 =  1% of the original kernel matrix.

torch.Size([12677, 2])
We keep 6.51e+05/2.36e+07 =  2% of the original kernel matrix.

torch.Size([26793, 2])
We keep 2.30e+06/1.38e+08 =  1% of the original kernel matrix.

torch.Size([8144, 2])
We keep 9.39e+05/1.91e+07 =  4% of the original kernel matrix.

torch.Size([20878, 2])
We keep 2.15e+06/1.24e+08 =  1% of the original kernel matrix.

torch.Size([6282, 2])
We keep 2.20e+05/5.66e+06 =  3% of the original kernel matrix.

torch.Size([19750, 2])
We keep 1.40e+06/6.78e+07 =  2% of the original kernel matrix.

torch.Size([13339, 2])
We keep 8.77e+05/2.97e+07 =  2% of the original kernel matrix.

torch.Size([27056, 2])
We keep 2.50e+06/1.55e+08 =  1% of the original kernel matrix.

torch.Size([4570, 2])
We keep 1.28e+05/2.72e+06 =  4% of the original kernel matrix.

torch.Size([17689, 2])
We keep 1.08e+06/4.69e+07 =  2% of the original kernel matrix.

torch.Size([4727, 2])
We keep 1.30e+05/3.01e+06 =  4% of the original kernel matrix.

torch.Size([18000, 2])
We keep 1.12e+06/4.94e+07 =  2% of the original kernel matrix.

torch.Size([9047, 2])
We keep 5.11e+05/1.39e+07 =  3% of the original kernel matrix.

torch.Size([22975, 2])
We keep 1.92e+06/1.06e+08 =  1% of the original kernel matrix.

torch.Size([22426, 2])
We keep 2.08e+06/1.08e+08 =  1% of the original kernel matrix.

torch.Size([35219, 2])
We keep 4.02e+06/2.96e+08 =  1% of the original kernel matrix.

torch.Size([10792, 2])
We keep 5.69e+05/1.75e+07 =  3% of the original kernel matrix.

torch.Size([24945, 2])
We keep 2.07e+06/1.19e+08 =  1% of the original kernel matrix.

torch.Size([33490, 2])
We keep 6.60e+06/3.33e+08 =  1% of the original kernel matrix.

torch.Size([43611, 2])
We keep 6.26e+06/5.20e+08 =  1% of the original kernel matrix.

torch.Size([47394, 2])
We keep 8.46e+06/6.82e+08 =  1% of the original kernel matrix.

torch.Size([53029, 2])
We keep 8.41e+06/7.44e+08 =  1% of the original kernel matrix.

torch.Size([7755, 2])
We keep 2.93e+05/8.12e+06 =  3% of the original kernel matrix.

torch.Size([21573, 2])
We keep 1.57e+06/8.12e+07 =  1% of the original kernel matrix.

torch.Size([20041, 2])
We keep 2.39e+06/8.93e+07 =  2% of the original kernel matrix.

torch.Size([33243, 2])
We keep 3.76e+06/2.69e+08 =  1% of the original kernel matrix.

torch.Size([12096, 2])
We keep 7.38e+05/2.39e+07 =  3% of the original kernel matrix.

torch.Size([25939, 2])
We keep 2.34e+06/1.39e+08 =  1% of the original kernel matrix.

torch.Size([16942, 2])
We keep 1.19e+06/4.96e+07 =  2% of the original kernel matrix.

torch.Size([30574, 2])
We keep 3.01e+06/2.01e+08 =  1% of the original kernel matrix.

torch.Size([29066, 2])
We keep 3.96e+06/2.18e+08 =  1% of the original kernel matrix.

torch.Size([40673, 2])
We keep 5.32e+06/4.20e+08 =  1% of the original kernel matrix.

torch.Size([7280, 2])
We keep 2.84e+05/7.75e+06 =  3% of the original kernel matrix.

torch.Size([21118, 2])
We keep 1.56e+06/7.93e+07 =  1% of the original kernel matrix.

torch.Size([19525, 2])
We keep 1.62e+06/7.62e+07 =  2% of the original kernel matrix.

torch.Size([32684, 2])
We keep 3.55e+06/2.49e+08 =  1% of the original kernel matrix.

torch.Size([16805, 2])
We keep 1.29e+06/5.27e+07 =  2% of the original kernel matrix.

torch.Size([30438, 2])
We keep 3.11e+06/2.07e+08 =  1% of the original kernel matrix.

torch.Size([11987, 2])
We keep 8.58e+05/2.77e+07 =  3% of the original kernel matrix.

torch.Size([25604, 2])
We keep 2.48e+06/1.50e+08 =  1% of the original kernel matrix.

torch.Size([15155, 2])
We keep 1.04e+06/4.06e+07 =  2% of the original kernel matrix.

torch.Size([29005, 2])
We keep 2.83e+06/1.81e+08 =  1% of the original kernel matrix.

torch.Size([8443, 2])
We keep 3.77e+05/1.04e+07 =  3% of the original kernel matrix.

torch.Size([22461, 2])
We keep 1.72e+06/9.21e+07 =  1% of the original kernel matrix.

torch.Size([18286, 2])
We keep 1.66e+06/6.90e+07 =  2% of the original kernel matrix.

torch.Size([31838, 2])
We keep 3.44e+06/2.37e+08 =  1% of the original kernel matrix.

torch.Size([48781, 2])
We keep 8.33e+06/7.01e+08 =  1% of the original kernel matrix.

torch.Size([54490, 2])
We keep 8.60e+06/7.54e+08 =  1% of the original kernel matrix.

torch.Size([13689, 2])
We keep 7.73e+05/2.94e+07 =  2% of the original kernel matrix.

torch.Size([27676, 2])
We keep 2.48e+06/1.54e+08 =  1% of the original kernel matrix.

torch.Size([12663, 2])
We keep 7.59e+05/2.74e+07 =  2% of the original kernel matrix.

torch.Size([26627, 2])
We keep 2.43e+06/1.49e+08 =  1% of the original kernel matrix.

torch.Size([14348, 2])
We keep 9.24e+05/3.49e+07 =  2% of the original kernel matrix.

torch.Size([28110, 2])
We keep 2.65e+06/1.68e+08 =  1% of the original kernel matrix.

torch.Size([22413, 2])
We keep 2.46e+06/1.17e+08 =  2% of the original kernel matrix.

torch.Size([35317, 2])
We keep 4.22e+06/3.08e+08 =  1% of the original kernel matrix.

torch.Size([30383, 2])
We keep 5.09e+06/2.18e+08 =  2% of the original kernel matrix.

torch.Size([41717, 2])
We keep 5.29e+06/4.20e+08 =  1% of the original kernel matrix.

torch.Size([15288, 2])
We keep 1.13e+06/4.21e+07 =  2% of the original kernel matrix.

torch.Size([28902, 2])
We keep 2.86e+06/1.85e+08 =  1% of the original kernel matrix.

torch.Size([12961, 2])
We keep 1.21e+06/3.39e+07 =  3% of the original kernel matrix.

torch.Size([26845, 2])
We keep 2.65e+06/1.66e+08 =  1% of the original kernel matrix.

torch.Size([13312, 2])
We keep 8.63e+05/2.91e+07 =  2% of the original kernel matrix.

torch.Size([27234, 2])
We keep 2.49e+06/1.54e+08 =  1% of the original kernel matrix.

torch.Size([18083, 2])
We keep 1.46e+06/6.20e+07 =  2% of the original kernel matrix.

torch.Size([31496, 2])
We keep 3.29e+06/2.24e+08 =  1% of the original kernel matrix.

torch.Size([14988, 2])
We keep 1.12e+06/4.22e+07 =  2% of the original kernel matrix.

torch.Size([28744, 2])
We keep 2.88e+06/1.85e+08 =  1% of the original kernel matrix.

torch.Size([16263, 2])
We keep 1.19e+06/4.74e+07 =  2% of the original kernel matrix.

torch.Size([29768, 2])
We keep 2.98e+06/1.96e+08 =  1% of the original kernel matrix.

torch.Size([23081, 2])
We keep 3.35e+06/1.42e+08 =  2% of the original kernel matrix.

torch.Size([35710, 2])
We keep 4.54e+06/3.39e+08 =  1% of the original kernel matrix.

torch.Size([3670, 2])
We keep 8.04e+04/1.58e+06 =  5% of the original kernel matrix.

torch.Size([16364, 2])
We keep 9.19e+05/3.58e+07 =  2% of the original kernel matrix.

torch.Size([6365, 2])
We keep 2.15e+05/5.09e+06 =  4% of the original kernel matrix.

torch.Size([20244, 2])
We keep 1.34e+06/6.43e+07 =  2% of the original kernel matrix.

torch.Size([16238, 2])
We keep 1.16e+06/4.58e+07 =  2% of the original kernel matrix.

torch.Size([29991, 2])
We keep 2.93e+06/1.93e+08 =  1% of the original kernel matrix.

torch.Size([36854, 2])
We keep 5.55e+06/3.99e+08 =  1% of the original kernel matrix.

torch.Size([46042, 2])
We keep 6.72e+06/5.69e+08 =  1% of the original kernel matrix.

torch.Size([9929, 2])
We keep 4.88e+05/1.48e+07 =  3% of the original kernel matrix.

torch.Size([23931, 2])
We keep 1.96e+06/1.10e+08 =  1% of the original kernel matrix.

torch.Size([9267, 2])
We keep 5.34e+05/1.39e+07 =  3% of the original kernel matrix.

torch.Size([23152, 2])
We keep 1.92e+06/1.06e+08 =  1% of the original kernel matrix.

torch.Size([15502, 2])
We keep 1.33e+06/4.61e+07 =  2% of the original kernel matrix.

torch.Size([29384, 2])
We keep 2.94e+06/1.93e+08 =  1% of the original kernel matrix.

torch.Size([7707, 2])
We keep 3.14e+05/8.05e+06 =  3% of the original kernel matrix.

torch.Size([21661, 2])
We keep 1.58e+06/8.08e+07 =  1% of the original kernel matrix.

torch.Size([6303, 2])
We keep 2.35e+05/5.66e+06 =  4% of the original kernel matrix.

torch.Size([19711, 2])
We keep 1.40e+06/6.78e+07 =  2% of the original kernel matrix.

torch.Size([8044, 2])
We keep 3.29e+05/9.62e+06 =  3% of the original kernel matrix.

torch.Size([21863, 2])
We keep 1.67e+06/8.83e+07 =  1% of the original kernel matrix.

torch.Size([7502, 2])
We keep 3.27e+05/8.04e+06 =  4% of the original kernel matrix.

torch.Size([21238, 2])
We keep 1.57e+06/8.08e+07 =  1% of the original kernel matrix.

torch.Size([5445, 2])
We keep 1.54e+05/3.52e+06 =  4% of the original kernel matrix.

torch.Size([18915, 2])
We keep 1.18e+06/5.34e+07 =  2% of the original kernel matrix.

torch.Size([14802, 2])
We keep 1.01e+06/3.90e+07 =  2% of the original kernel matrix.

torch.Size([28548, 2])
We keep 2.76e+06/1.78e+08 =  1% of the original kernel matrix.

torch.Size([15416, 2])
We keep 1.15e+06/4.25e+07 =  2% of the original kernel matrix.

torch.Size([29236, 2])
We keep 2.86e+06/1.86e+08 =  1% of the original kernel matrix.

torch.Size([16281, 2])
We keep 1.07e+06/4.48e+07 =  2% of the original kernel matrix.

torch.Size([29827, 2])
We keep 2.91e+06/1.91e+08 =  1% of the original kernel matrix.

torch.Size([10992, 2])
We keep 5.50e+05/1.77e+07 =  3% of the original kernel matrix.

torch.Size([25082, 2])
We keep 2.08e+06/1.20e+08 =  1% of the original kernel matrix.

torch.Size([9196, 2])
We keep 4.44e+05/1.23e+07 =  3% of the original kernel matrix.

torch.Size([23178, 2])
We keep 1.82e+06/9.98e+07 =  1% of the original kernel matrix.

torch.Size([15479, 2])
We keep 1.33e+06/4.76e+07 =  2% of the original kernel matrix.

torch.Size([29019, 2])
We keep 3.00e+06/1.96e+08 =  1% of the original kernel matrix.

torch.Size([13116, 2])
We keep 9.32e+05/3.06e+07 =  3% of the original kernel matrix.

torch.Size([26918, 2])
We keep 2.53e+06/1.58e+08 =  1% of the original kernel matrix.

torch.Size([20744, 2])
We keep 1.76e+06/8.40e+07 =  2% of the original kernel matrix.

torch.Size([33899, 2])
We keep 3.68e+06/2.61e+08 =  1% of the original kernel matrix.

torch.Size([10390, 2])
We keep 5.08e+05/1.63e+07 =  3% of the original kernel matrix.

torch.Size([24378, 2])
We keep 2.03e+06/1.15e+08 =  1% of the original kernel matrix.

torch.Size([21918, 2])
We keep 2.90e+06/1.12e+08 =  2% of the original kernel matrix.

torch.Size([34953, 2])
We keep 4.15e+06/3.01e+08 =  1% of the original kernel matrix.

torch.Size([19384, 2])
We keep 1.67e+06/7.52e+07 =  2% of the original kernel matrix.

torch.Size([32706, 2])
We keep 3.56e+06/2.47e+08 =  1% of the original kernel matrix.

torch.Size([21701, 2])
We keep 1.82e+06/9.24e+07 =  1% of the original kernel matrix.

torch.Size([34703, 2])
We keep 3.79e+06/2.74e+08 =  1% of the original kernel matrix.

torch.Size([33156, 2])
We keep 4.77e+06/3.15e+08 =  1% of the original kernel matrix.

torch.Size([43598, 2])
We keep 6.11e+06/5.05e+08 =  1% of the original kernel matrix.

torch.Size([14797, 2])
We keep 1.08e+06/3.97e+07 =  2% of the original kernel matrix.

torch.Size([28606, 2])
We keep 2.84e+06/1.80e+08 =  1% of the original kernel matrix.

torch.Size([18177, 2])
We keep 1.42e+06/6.05e+07 =  2% of the original kernel matrix.

torch.Size([31758, 2])
We keep 3.24e+06/2.22e+08 =  1% of the original kernel matrix.

torch.Size([7725, 2])
We keep 3.47e+05/9.44e+06 =  3% of the original kernel matrix.

torch.Size([21320, 2])
We keep 1.66e+06/8.75e+07 =  1% of the original kernel matrix.

torch.Size([41901, 2])
We keep 1.02e+07/5.87e+08 =  1% of the original kernel matrix.

torch.Size([49193, 2])
We keep 7.95e+06/6.90e+08 =  1% of the original kernel matrix.

torch.Size([10532, 2])
We keep 5.84e+05/1.72e+07 =  3% of the original kernel matrix.

torch.Size([24371, 2])
We keep 2.07e+06/1.18e+08 =  1% of the original kernel matrix.

torch.Size([15324, 2])
We keep 1.01e+06/4.04e+07 =  2% of the original kernel matrix.

torch.Size([29104, 2])
We keep 2.81e+06/1.81e+08 =  1% of the original kernel matrix.

torch.Size([6287, 2])
We keep 2.34e+05/5.69e+06 =  4% of the original kernel matrix.

torch.Size([19900, 2])
We keep 1.40e+06/6.80e+07 =  2% of the original kernel matrix.

torch.Size([9258, 2])
We keep 4.57e+05/1.34e+07 =  3% of the original kernel matrix.

torch.Size([23311, 2])
We keep 1.89e+06/1.04e+08 =  1% of the original kernel matrix.

torch.Size([5472, 2])
We keep 1.99e+05/4.13e+06 =  4% of the original kernel matrix.

torch.Size([18817, 2])
We keep 1.25e+06/5.79e+07 =  2% of the original kernel matrix.

torch.Size([13110, 2])
We keep 9.11e+05/3.08e+07 =  2% of the original kernel matrix.

torch.Size([26935, 2])
We keep 2.55e+06/1.58e+08 =  1% of the original kernel matrix.

torch.Size([6821, 2])
We keep 3.57e+05/6.77e+06 =  5% of the original kernel matrix.

torch.Size([20524, 2])
We keep 1.50e+06/7.41e+07 =  2% of the original kernel matrix.

torch.Size([9460, 2])
We keep 4.53e+05/1.36e+07 =  3% of the original kernel matrix.

torch.Size([23386, 2])
We keep 1.90e+06/1.05e+08 =  1% of the original kernel matrix.

torch.Size([9761, 2])
We keep 6.10e+05/1.68e+07 =  3% of the original kernel matrix.

torch.Size([23911, 2])
We keep 2.04e+06/1.17e+08 =  1% of the original kernel matrix.

torch.Size([17346, 2])
We keep 1.56e+06/5.89e+07 =  2% of the original kernel matrix.

torch.Size([30910, 2])
We keep 3.24e+06/2.19e+08 =  1% of the original kernel matrix.

torch.Size([12654, 2])
We keep 1.03e+06/3.25e+07 =  3% of the original kernel matrix.

torch.Size([26246, 2])
We keep 2.61e+06/1.63e+08 =  1% of the original kernel matrix.

torch.Size([12804, 2])
We keep 8.24e+05/2.83e+07 =  2% of the original kernel matrix.

torch.Size([26795, 2])
We keep 2.48e+06/1.51e+08 =  1% of the original kernel matrix.

torch.Size([4068, 2])
We keep 9.28e+04/1.97e+06 =  4% of the original kernel matrix.

torch.Size([17010, 2])
We keep 9.75e+05/3.99e+07 =  2% of the original kernel matrix.

torch.Size([28129, 2])
We keep 3.33e+06/1.97e+08 =  1% of the original kernel matrix.

torch.Size([40166, 2])
We keep 5.12e+06/3.99e+08 =  1% of the original kernel matrix.

torch.Size([6148, 2])
We keep 2.32e+05/5.51e+06 =  4% of the original kernel matrix.

torch.Size([19568, 2])
We keep 1.39e+06/6.69e+07 =  2% of the original kernel matrix.

torch.Size([35833, 2])
We keep 5.05e+06/3.56e+08 =  1% of the original kernel matrix.

torch.Size([45717, 2])
We keep 6.42e+06/5.37e+08 =  1% of the original kernel matrix.

torch.Size([8700, 2])
We keep 5.17e+05/1.31e+07 =  3% of the original kernel matrix.

torch.Size([22638, 2])
We keep 1.87e+06/1.03e+08 =  1% of the original kernel matrix.

torch.Size([8512, 2])
We keep 3.64e+05/1.06e+07 =  3% of the original kernel matrix.

torch.Size([22356, 2])
We keep 1.74e+06/9.26e+07 =  1% of the original kernel matrix.

torch.Size([9851, 2])
We keep 4.51e+05/1.40e+07 =  3% of the original kernel matrix.

torch.Size([23950, 2])
We keep 1.91e+06/1.06e+08 =  1% of the original kernel matrix.

torch.Size([11930, 2])
We keep 7.89e+05/2.32e+07 =  3% of the original kernel matrix.

torch.Size([25782, 2])
We keep 2.30e+06/1.37e+08 =  1% of the original kernel matrix.

torch.Size([8897, 2])
We keep 4.51e+05/1.25e+07 =  3% of the original kernel matrix.

torch.Size([22624, 2])
We keep 1.84e+06/1.01e+08 =  1% of the original kernel matrix.

torch.Size([6202, 2])
We keep 2.24e+05/5.58e+06 =  4% of the original kernel matrix.

torch.Size([19820, 2])
We keep 1.38e+06/6.73e+07 =  2% of the original kernel matrix.

torch.Size([14635, 2])
We keep 1.42e+06/3.75e+07 =  3% of the original kernel matrix.

torch.Size([28478, 2])
We keep 2.73e+06/1.74e+08 =  1% of the original kernel matrix.

torch.Size([26464, 2])
We keep 3.23e+06/1.74e+08 =  1% of the original kernel matrix.

torch.Size([38713, 2])
We keep 4.87e+06/3.76e+08 =  1% of the original kernel matrix.

torch.Size([10799, 2])
We keep 5.75e+05/1.81e+07 =  3% of the original kernel matrix.

torch.Size([24896, 2])
We keep 2.09e+06/1.21e+08 =  1% of the original kernel matrix.

torch.Size([59418, 2])
We keep 1.17e+07/1.05e+09 =  1% of the original kernel matrix.

torch.Size([58583, 2])
We keep 9.88e+06/9.22e+08 =  1% of the original kernel matrix.

torch.Size([7016, 2])
We keep 3.21e+05/7.69e+06 =  4% of the original kernel matrix.

torch.Size([20776, 2])
We keep 1.55e+06/7.90e+07 =  1% of the original kernel matrix.

torch.Size([13184, 2])
We keep 1.09e+06/3.43e+07 =  3% of the original kernel matrix.

torch.Size([27164, 2])
We keep 2.63e+06/1.67e+08 =  1% of the original kernel matrix.

torch.Size([10316, 2])
We keep 6.76e+05/1.90e+07 =  3% of the original kernel matrix.

torch.Size([23989, 2])
We keep 2.14e+06/1.24e+08 =  1% of the original kernel matrix.

torch.Size([13583, 2])
We keep 9.39e+05/3.16e+07 =  2% of the original kernel matrix.

torch.Size([27520, 2])
We keep 2.59e+06/1.60e+08 =  1% of the original kernel matrix.

torch.Size([7567, 2])
We keep 3.33e+05/8.60e+06 =  3% of the original kernel matrix.

torch.Size([21376, 2])
We keep 1.62e+06/8.35e+07 =  1% of the original kernel matrix.

torch.Size([9814, 2])
We keep 4.97e+05/1.40e+07 =  3% of the original kernel matrix.

torch.Size([23819, 2])
We keep 1.91e+06/1.07e+08 =  1% of the original kernel matrix.

torch.Size([11070, 2])
We keep 5.71e+05/1.85e+07 =  3% of the original kernel matrix.

torch.Size([24886, 2])
We keep 2.11e+06/1.23e+08 =  1% of the original kernel matrix.

torch.Size([11384, 2])
We keep 5.72e+05/1.97e+07 =  2% of the original kernel matrix.

torch.Size([25305, 2])
We keep 2.17e+06/1.26e+08 =  1% of the original kernel matrix.

torch.Size([10192, 2])
We keep 5.02e+05/1.62e+07 =  3% of the original kernel matrix.

torch.Size([24256, 2])
We keep 2.01e+06/1.15e+08 =  1% of the original kernel matrix.

torch.Size([13662, 2])
We keep 9.42e+05/3.29e+07 =  2% of the original kernel matrix.

torch.Size([27532, 2])
We keep 2.60e+06/1.63e+08 =  1% of the original kernel matrix.

torch.Size([6224, 2])
We keep 2.60e+05/6.19e+06 =  4% of the original kernel matrix.

torch.Size([19514, 2])
We keep 1.45e+06/7.09e+07 =  2% of the original kernel matrix.

torch.Size([35913, 2])
We keep 5.88e+06/3.86e+08 =  1% of the original kernel matrix.

torch.Size([45433, 2])
We keep 6.68e+06/5.60e+08 =  1% of the original kernel matrix.

torch.Size([16987, 2])
We keep 1.36e+06/5.66e+07 =  2% of the original kernel matrix.

torch.Size([30337, 2])
We keep 3.19e+06/2.14e+08 =  1% of the original kernel matrix.

torch.Size([15203, 2])
We keep 1.21e+06/4.34e+07 =  2% of the original kernel matrix.

torch.Size([29111, 2])
We keep 2.89e+06/1.88e+08 =  1% of the original kernel matrix.

torch.Size([7897, 2])
We keep 2.95e+05/8.29e+06 =  3% of the original kernel matrix.

torch.Size([21959, 2])
We keep 1.59e+06/8.20e+07 =  1% of the original kernel matrix.

torch.Size([12206, 2])
We keep 8.99e+05/2.46e+07 =  3% of the original kernel matrix.

torch.Size([26214, 2])
We keep 2.37e+06/1.41e+08 =  1% of the original kernel matrix.

torch.Size([8831, 2])
We keep 4.27e+05/1.20e+07 =  3% of the original kernel matrix.

torch.Size([23138, 2])
We keep 1.82e+06/9.89e+07 =  1% of the original kernel matrix.

torch.Size([8957, 2])
We keep 3.76e+05/1.08e+07 =  3% of the original kernel matrix.

torch.Size([23049, 2])
We keep 1.74e+06/9.38e+07 =  1% of the original kernel matrix.

torch.Size([9536, 2])
We keep 5.82e+05/1.51e+07 =  3% of the original kernel matrix.

torch.Size([23538, 2])
We keep 1.95e+06/1.11e+08 =  1% of the original kernel matrix.

torch.Size([9881, 2])
We keep 5.84e+05/1.58e+07 =  3% of the original kernel matrix.

torch.Size([23735, 2])
We keep 1.99e+06/1.13e+08 =  1% of the original kernel matrix.

torch.Size([12458, 2])
We keep 6.97e+05/2.42e+07 =  2% of the original kernel matrix.

torch.Size([26630, 2])
We keep 2.35e+06/1.40e+08 =  1% of the original kernel matrix.

torch.Size([13341, 2])
We keep 8.16e+05/2.82e+07 =  2% of the original kernel matrix.

torch.Size([27295, 2])
We keep 2.46e+06/1.51e+08 =  1% of the original kernel matrix.

torch.Size([19597, 2])
We keep 3.99e+06/1.34e+08 =  2% of the original kernel matrix.

torch.Size([32802, 2])
We keep 4.42e+06/3.30e+08 =  1% of the original kernel matrix.

torch.Size([14239, 2])
We keep 9.69e+05/3.53e+07 =  2% of the original kernel matrix.

torch.Size([27959, 2])
We keep 2.68e+06/1.69e+08 =  1% of the original kernel matrix.

torch.Size([39068, 2])
We keep 9.03e+06/5.01e+08 =  1% of the original kernel matrix.

torch.Size([47232, 2])
We keep 7.37e+06/6.38e+08 =  1% of the original kernel matrix.

torch.Size([18368, 2])
We keep 1.41e+06/6.36e+07 =  2% of the original kernel matrix.

torch.Size([31751, 2])
We keep 3.29e+06/2.27e+08 =  1% of the original kernel matrix.

torch.Size([18497, 2])
We keep 1.97e+06/7.12e+07 =  2% of the original kernel matrix.

torch.Size([31938, 2])
We keep 3.46e+06/2.40e+08 =  1% of the original kernel matrix.

torch.Size([11212, 2])
We keep 5.75e+05/1.89e+07 =  3% of the original kernel matrix.

torch.Size([25270, 2])
We keep 2.13e+06/1.24e+08 =  1% of the original kernel matrix.

torch.Size([4017, 2])
We keep 1.01e+05/2.07e+06 =  4% of the original kernel matrix.

torch.Size([16847, 2])
We keep 9.95e+05/4.10e+07 =  2% of the original kernel matrix.

torch.Size([25915, 2])
We keep 3.50e+06/1.60e+08 =  2% of the original kernel matrix.

torch.Size([38356, 2])
We keep 4.70e+06/3.60e+08 =  1% of the original kernel matrix.

torch.Size([6673, 2])
We keep 3.48e+05/7.68e+06 =  4% of the original kernel matrix.

torch.Size([20152, 2])
We keep 1.55e+06/7.89e+07 =  1% of the original kernel matrix.

torch.Size([24482, 2])
We keep 4.12e+06/1.70e+08 =  2% of the original kernel matrix.

torch.Size([36681, 2])
We keep 4.86e+06/3.72e+08 =  1% of the original kernel matrix.

torch.Size([4660, 2])
We keep 1.50e+05/2.90e+06 =  5% of the original kernel matrix.

torch.Size([17627, 2])
We keep 1.12e+06/4.85e+07 =  2% of the original kernel matrix.

torch.Size([21626, 2])
We keep 2.72e+06/1.15e+08 =  2% of the original kernel matrix.

torch.Size([34465, 2])
We keep 4.14e+06/3.05e+08 =  1% of the original kernel matrix.

torch.Size([11591, 2])
We keep 6.95e+05/2.21e+07 =  3% of the original kernel matrix.

torch.Size([25419, 2])
We keep 2.26e+06/1.34e+08 =  1% of the original kernel matrix.

torch.Size([8556, 2])
We keep 3.42e+05/1.02e+07 =  3% of the original kernel matrix.

torch.Size([22578, 2])
We keep 1.69e+06/9.11e+07 =  1% of the original kernel matrix.

torch.Size([19348, 2])
We keep 1.76e+06/7.47e+07 =  2% of the original kernel matrix.

torch.Size([32636, 2])
We keep 3.53e+06/2.46e+08 =  1% of the original kernel matrix.

torch.Size([17626, 2])
We keep 1.38e+06/5.76e+07 =  2% of the original kernel matrix.

torch.Size([31135, 2])
We keep 3.23e+06/2.16e+08 =  1% of the original kernel matrix.

torch.Size([5111, 2])
We keep 1.45e+05/3.30e+06 =  4% of the original kernel matrix.

torch.Size([18482, 2])
We keep 1.17e+06/5.18e+07 =  2% of the original kernel matrix.

torch.Size([3955, 2])
We keep 9.66e+04/2.01e+06 =  4% of the original kernel matrix.

torch.Size([16658, 2])
We keep 9.83e+05/4.04e+07 =  2% of the original kernel matrix.

torch.Size([9362, 2])
We keep 8.12e+05/1.69e+07 =  4% of the original kernel matrix.

torch.Size([22909, 2])
We keep 2.06e+06/1.17e+08 =  1% of the original kernel matrix.

torch.Size([24448, 2])
We keep 2.82e+06/1.36e+08 =  2% of the original kernel matrix.

torch.Size([37106, 2])
We keep 4.44e+06/3.33e+08 =  1% of the original kernel matrix.

torch.Size([6997, 2])
We keep 3.17e+05/7.81e+06 =  4% of the original kernel matrix.

torch.Size([20699, 2])
We keep 1.56e+06/7.96e+07 =  1% of the original kernel matrix.

torch.Size([14836, 2])
We keep 1.11e+06/4.16e+07 =  2% of the original kernel matrix.

torch.Size([28623, 2])
We keep 2.86e+06/1.84e+08 =  1% of the original kernel matrix.

torch.Size([13411, 2])
We keep 1.52e+06/2.90e+07 =  5% of the original kernel matrix.

torch.Size([27491, 2])
We keep 2.50e+06/1.54e+08 =  1% of the original kernel matrix.

torch.Size([24500, 2])
We keep 2.97e+06/1.46e+08 =  2% of the original kernel matrix.

torch.Size([36954, 2])
We keep 4.58e+06/3.44e+08 =  1% of the original kernel matrix.

torch.Size([9913, 2])
We keep 4.79e+05/1.50e+07 =  3% of the original kernel matrix.

torch.Size([23919, 2])
We keep 1.94e+06/1.10e+08 =  1% of the original kernel matrix.

torch.Size([17067, 2])
We keep 1.82e+06/6.04e+07 =  3% of the original kernel matrix.

torch.Size([30854, 2])
We keep 3.23e+06/2.21e+08 =  1% of the original kernel matrix.

torch.Size([15798, 2])
We keep 1.08e+06/4.35e+07 =  2% of the original kernel matrix.

torch.Size([29474, 2])
We keep 2.88e+06/1.88e+08 =  1% of the original kernel matrix.

torch.Size([17602, 2])
We keep 1.42e+06/6.09e+07 =  2% of the original kernel matrix.

torch.Size([31187, 2])
We keep 3.30e+06/2.22e+08 =  1% of the original kernel matrix.

torch.Size([7580, 2])
We keep 3.84e+05/8.53e+06 =  4% of the original kernel matrix.

torch.Size([21266, 2])
We keep 1.60e+06/8.32e+07 =  1% of the original kernel matrix.

torch.Size([20265, 2])
We keep 2.84e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([33497, 2])
We keep 3.99e+06/2.89e+08 =  1% of the original kernel matrix.

torch.Size([10468, 2])
We keep 5.01e+05/1.64e+07 =  3% of the original kernel matrix.

torch.Size([24548, 2])
We keep 2.02e+06/1.15e+08 =  1% of the original kernel matrix.

torch.Size([16781, 2])
We keep 1.21e+06/5.07e+07 =  2% of the original kernel matrix.

torch.Size([30409, 2])
We keep 3.06e+06/2.03e+08 =  1% of the original kernel matrix.

torch.Size([15318, 2])
We keep 1.01e+06/4.14e+07 =  2% of the original kernel matrix.

torch.Size([29178, 2])
We keep 2.83e+06/1.83e+08 =  1% of the original kernel matrix.

torch.Size([12403, 2])
We keep 6.77e+05/2.32e+07 =  2% of the original kernel matrix.

torch.Size([26487, 2])
We keep 2.28e+06/1.37e+08 =  1% of the original kernel matrix.

torch.Size([16383, 2])
We keep 1.41e+06/5.25e+07 =  2% of the original kernel matrix.

torch.Size([30013, 2])
We keep 3.09e+06/2.06e+08 =  1% of the original kernel matrix.

torch.Size([21983, 2])
We keep 2.18e+06/1.05e+08 =  2% of the original kernel matrix.

torch.Size([35031, 2])
We keep 4.06e+06/2.92e+08 =  1% of the original kernel matrix.

torch.Size([19525, 2])
We keep 1.78e+06/8.40e+07 =  2% of the original kernel matrix.

torch.Size([32683, 2])
We keep 3.71e+06/2.61e+08 =  1% of the original kernel matrix.

torch.Size([12192, 2])
We keep 6.80e+05/2.39e+07 =  2% of the original kernel matrix.

torch.Size([26056, 2])
We keep 2.33e+06/1.39e+08 =  1% of the original kernel matrix.

torch.Size([3995, 2])
We keep 9.63e+04/2.05e+06 =  4% of the original kernel matrix.

torch.Size([16820, 2])
We keep 9.91e+05/4.08e+07 =  2% of the original kernel matrix.

torch.Size([12119, 2])
We keep 7.03e+05/2.36e+07 =  2% of the original kernel matrix.

torch.Size([26145, 2])
We keep 2.31e+06/1.39e+08 =  1% of the original kernel matrix.

torch.Size([15647, 2])
We keep 1.67e+06/4.46e+07 =  3% of the original kernel matrix.

torch.Size([29387, 2])
We keep 2.91e+06/1.90e+08 =  1% of the original kernel matrix.

torch.Size([25050, 2])
We keep 3.37e+06/1.59e+08 =  2% of the original kernel matrix.

torch.Size([37479, 2])
We keep 4.69e+06/3.59e+08 =  1% of the original kernel matrix.

torch.Size([10099, 2])
We keep 5.53e+05/1.68e+07 =  3% of the original kernel matrix.

torch.Size([24114, 2])
We keep 2.05e+06/1.17e+08 =  1% of the original kernel matrix.

torch.Size([12735, 2])
We keep 7.99e+05/2.72e+07 =  2% of the original kernel matrix.

torch.Size([26665, 2])
We keep 2.44e+06/1.49e+08 =  1% of the original kernel matrix.

torch.Size([8115, 2])
We keep 3.32e+05/9.99e+06 =  3% of the original kernel matrix.

torch.Size([22057, 2])
We keep 1.67e+06/9.01e+07 =  1% of the original kernel matrix.

torch.Size([19862, 2])
We keep 2.49e+06/8.66e+07 =  2% of the original kernel matrix.

torch.Size([32925, 2])
We keep 3.74e+06/2.65e+08 =  1% of the original kernel matrix.

torch.Size([5855, 2])
We keep 1.80e+05/4.26e+06 =  4% of the original kernel matrix.

torch.Size([19569, 2])
We keep 1.26e+06/5.88e+07 =  2% of the original kernel matrix.

torch.Size([18559, 2])
We keep 1.32e+06/6.00e+07 =  2% of the original kernel matrix.

torch.Size([32204, 2])
We keep 3.27e+06/2.21e+08 =  1% of the original kernel matrix.

torch.Size([12889, 2])
We keep 7.42e+05/2.64e+07 =  2% of the original kernel matrix.

torch.Size([26901, 2])
We keep 2.40e+06/1.46e+08 =  1% of the original kernel matrix.

torch.Size([32400, 2])
We keep 3.89e+06/2.72e+08 =  1% of the original kernel matrix.

torch.Size([43586, 2])
We keep 5.78e+06/4.70e+08 =  1% of the original kernel matrix.

torch.Size([10304, 2])
We keep 5.39e+05/1.69e+07 =  3% of the original kernel matrix.

torch.Size([24376, 2])
We keep 2.07e+06/1.17e+08 =  1% of the original kernel matrix.

torch.Size([26038, 2])
We keep 2.83e+06/1.62e+08 =  1% of the original kernel matrix.

torch.Size([38334, 2])
We keep 4.70e+06/3.62e+08 =  1% of the original kernel matrix.

torch.Size([12701, 2])
We keep 7.47e+05/2.63e+07 =  2% of the original kernel matrix.

torch.Size([26570, 2])
We keep 2.40e+06/1.46e+08 =  1% of the original kernel matrix.

torch.Size([11918, 2])
We keep 9.39e+05/2.75e+07 =  3% of the original kernel matrix.

torch.Size([25756, 2])
We keep 2.46e+06/1.49e+08 =  1% of the original kernel matrix.

torch.Size([10913, 2])
We keep 5.84e+05/1.96e+07 =  2% of the original kernel matrix.

torch.Size([25019, 2])
We keep 2.19e+06/1.26e+08 =  1% of the original kernel matrix.

torch.Size([8111, 2])
We keep 3.55e+05/9.92e+06 =  3% of the original kernel matrix.

torch.Size([22000, 2])
We keep 1.69e+06/8.97e+07 =  1% of the original kernel matrix.

torch.Size([21001, 2])
We keep 1.84e+06/9.47e+07 =  1% of the original kernel matrix.

torch.Size([34082, 2])
We keep 3.85e+06/2.77e+08 =  1% of the original kernel matrix.

torch.Size([23630, 2])
We keep 2.53e+06/1.30e+08 =  1% of the original kernel matrix.

torch.Size([36318, 2])
We keep 4.38e+06/3.25e+08 =  1% of the original kernel matrix.

torch.Size([15960, 2])
We keep 1.57e+06/4.78e+07 =  3% of the original kernel matrix.

torch.Size([29798, 2])
We keep 2.98e+06/1.97e+08 =  1% of the original kernel matrix.

torch.Size([15914, 2])
We keep 1.07e+06/4.31e+07 =  2% of the original kernel matrix.

torch.Size([29641, 2])
We keep 2.85e+06/1.87e+08 =  1% of the original kernel matrix.

torch.Size([38372, 2])
We keep 5.42e+06/4.05e+08 =  1% of the original kernel matrix.

torch.Size([47102, 2])
We keep 6.73e+06/5.73e+08 =  1% of the original kernel matrix.

torch.Size([4363, 2])
We keep 1.16e+05/2.34e+06 =  4% of the original kernel matrix.

torch.Size([17135, 2])
We keep 1.04e+06/4.36e+07 =  2% of the original kernel matrix.

torch.Size([7318, 2])
We keep 4.07e+05/9.11e+06 =  4% of the original kernel matrix.

torch.Size([20862, 2])
We keep 1.63e+06/8.60e+07 =  1% of the original kernel matrix.

torch.Size([4610, 2])
We keep 1.23e+05/2.70e+06 =  4% of the original kernel matrix.

torch.Size([17762, 2])
We keep 1.09e+06/4.68e+07 =  2% of the original kernel matrix.

torch.Size([7601, 2])
We keep 3.07e+05/8.11e+06 =  3% of the original kernel matrix.

torch.Size([21490, 2])
We keep 1.59e+06/8.11e+07 =  1% of the original kernel matrix.

torch.Size([29715, 2])
We keep 4.89e+06/2.61e+08 =  1% of the original kernel matrix.

torch.Size([40856, 2])
We keep 5.74e+06/4.60e+08 =  1% of the original kernel matrix.

torch.Size([19669, 2])
We keep 1.52e+06/6.99e+07 =  2% of the original kernel matrix.

torch.Size([33028, 2])
We keep 3.41e+06/2.38e+08 =  1% of the original kernel matrix.

torch.Size([14439, 2])
We keep 1.01e+06/3.53e+07 =  2% of the original kernel matrix.

torch.Size([28203, 2])
We keep 2.67e+06/1.69e+08 =  1% of the original kernel matrix.

torch.Size([4951, 2])
We keep 1.39e+05/3.10e+06 =  4% of the original kernel matrix.

torch.Size([18259, 2])
We keep 1.14e+06/5.02e+07 =  2% of the original kernel matrix.

torch.Size([22278, 2])
We keep 2.31e+06/1.16e+08 =  1% of the original kernel matrix.

torch.Size([35150, 2])
We keep 4.20e+06/3.07e+08 =  1% of the original kernel matrix.

torch.Size([16649, 2])
We keep 1.65e+06/5.58e+07 =  2% of the original kernel matrix.

torch.Size([30366, 2])
We keep 3.16e+06/2.13e+08 =  1% of the original kernel matrix.

torch.Size([10953, 2])
We keep 6.99e+05/2.13e+07 =  3% of the original kernel matrix.

torch.Size([24725, 2])
We keep 2.25e+06/1.32e+08 =  1% of the original kernel matrix.

torch.Size([15939, 2])
We keep 1.30e+06/4.68e+07 =  2% of the original kernel matrix.

torch.Size([29587, 2])
We keep 2.97e+06/1.95e+08 =  1% of the original kernel matrix.

torch.Size([9607, 2])
We keep 4.17e+05/1.30e+07 =  3% of the original kernel matrix.

torch.Size([23500, 2])
We keep 1.85e+06/1.03e+08 =  1% of the original kernel matrix.

torch.Size([6823, 2])
We keep 2.65e+05/6.84e+06 =  3% of the original kernel matrix.

torch.Size([20478, 2])
We keep 1.50e+06/7.45e+07 =  2% of the original kernel matrix.

torch.Size([17859, 2])
We keep 1.44e+06/6.17e+07 =  2% of the original kernel matrix.

torch.Size([31399, 2])
We keep 3.28e+06/2.24e+08 =  1% of the original kernel matrix.

torch.Size([5074, 2])
We keep 1.47e+05/3.05e+06 =  4% of the original kernel matrix.

torch.Size([18358, 2])
We keep 1.14e+06/4.98e+07 =  2% of the original kernel matrix.

torch.Size([14838, 2])
We keep 1.11e+06/3.71e+07 =  2% of the original kernel matrix.

torch.Size([28657, 2])
We keep 2.73e+06/1.74e+08 =  1% of the original kernel matrix.

torch.Size([13467, 2])
We keep 9.59e+05/3.28e+07 =  2% of the original kernel matrix.

torch.Size([27261, 2])
We keep 2.61e+06/1.63e+08 =  1% of the original kernel matrix.

torch.Size([8608, 2])
We keep 3.93e+05/1.08e+07 =  3% of the original kernel matrix.

torch.Size([22596, 2])
We keep 1.76e+06/9.37e+07 =  1% of the original kernel matrix.

torch.Size([15945, 2])
We keep 1.38e+06/4.66e+07 =  2% of the original kernel matrix.

torch.Size([29942, 2])
We keep 2.98e+06/1.95e+08 =  1% of the original kernel matrix.

torch.Size([11252, 2])
We keep 6.35e+05/2.04e+07 =  3% of the original kernel matrix.

torch.Size([25155, 2])
We keep 2.20e+06/1.29e+08 =  1% of the original kernel matrix.

torch.Size([17534, 2])
We keep 1.43e+06/5.66e+07 =  2% of the original kernel matrix.

torch.Size([31060, 2])
We keep 3.17e+06/2.14e+08 =  1% of the original kernel matrix.

torch.Size([21788, 2])
We keep 2.15e+06/1.01e+08 =  2% of the original kernel matrix.

torch.Size([34823, 2])
We keep 3.96e+06/2.86e+08 =  1% of the original kernel matrix.

torch.Size([16946, 2])
We keep 1.47e+06/5.53e+07 =  2% of the original kernel matrix.

torch.Size([30684, 2])
We keep 3.11e+06/2.12e+08 =  1% of the original kernel matrix.

torch.Size([12798, 2])
We keep 7.32e+05/2.43e+07 =  3% of the original kernel matrix.

torch.Size([26765, 2])
We keep 2.33e+06/1.41e+08 =  1% of the original kernel matrix.

torch.Size([54612, 2])
We keep 2.29e+07/1.06e+09 =  2% of the original kernel matrix.

torch.Size([55262, 2])
We keep 9.85e+06/9.29e+08 =  1% of the original kernel matrix.

torch.Size([96098, 2])
We keep 1.25e+08/5.17e+09 =  2% of the original kernel matrix.

torch.Size([71811, 2])
We keep 1.92e+07/2.05e+09 =  0% of the original kernel matrix.

torch.Size([22193, 2])
We keep 2.78e+06/1.07e+08 =  2% of the original kernel matrix.

torch.Size([35318, 2])
We keep 3.86e+06/2.95e+08 =  1% of the original kernel matrix.

torch.Size([28591, 2])
We keep 6.99e+06/2.56e+08 =  2% of the original kernel matrix.

torch.Size([40070, 2])
We keep 5.72e+06/4.55e+08 =  1% of the original kernel matrix.

torch.Size([7720, 2])
We keep 3.59e+05/8.33e+06 =  4% of the original kernel matrix.

torch.Size([21473, 2])
We keep 1.61e+06/8.22e+07 =  1% of the original kernel matrix.

torch.Size([23988, 2])
We keep 2.52e+06/1.30e+08 =  1% of the original kernel matrix.

torch.Size([36527, 2])
We keep 4.32e+06/3.24e+08 =  1% of the original kernel matrix.

torch.Size([23932, 2])
We keep 2.97e+06/1.26e+08 =  2% of the original kernel matrix.

torch.Size([36467, 2])
We keep 4.26e+06/3.19e+08 =  1% of the original kernel matrix.

torch.Size([264283, 2])
We keep 6.50e+08/3.93e+10 =  1% of the original kernel matrix.

torch.Size([122207, 2])
We keep 4.59e+07/5.65e+09 =  0% of the original kernel matrix.

torch.Size([16451, 2])
We keep 1.12e+06/4.72e+07 =  2% of the original kernel matrix.

torch.Size([30252, 2])
We keep 2.96e+06/1.96e+08 =  1% of the original kernel matrix.

torch.Size([606485, 2])
We keep 7.78e+08/1.12e+11 =  0% of the original kernel matrix.

torch.Size([187782, 2])
We keep 7.33e+07/9.52e+09 =  0% of the original kernel matrix.

torch.Size([138339, 2])
We keep 3.87e+08/9.96e+09 =  3% of the original kernel matrix.

torch.Size([86949, 2])
We keep 2.47e+07/2.84e+09 =  0% of the original kernel matrix.

torch.Size([237868, 2])
We keep 2.39e+08/2.29e+10 =  1% of the original kernel matrix.

torch.Size([115664, 2])
We keep 3.63e+07/4.31e+09 =  0% of the original kernel matrix.

torch.Size([67564, 2])
We keep 5.95e+07/1.46e+09 =  4% of the original kernel matrix.

torch.Size([61807, 2])
We keep 1.10e+07/1.09e+09 =  1% of the original kernel matrix.

torch.Size([10207, 2])
We keep 5.77e+05/1.55e+07 =  3% of the original kernel matrix.

torch.Size([24271, 2])
We keep 1.99e+06/1.12e+08 =  1% of the original kernel matrix.

torch.Size([17299, 2])
We keep 3.06e+06/6.36e+07 =  4% of the original kernel matrix.

torch.Size([31034, 2])
We keep 3.28e+06/2.27e+08 =  1% of the original kernel matrix.

torch.Size([979685, 2])
We keep 1.87e+09/2.85e+11 =  0% of the original kernel matrix.

torch.Size([245412, 2])
We keep 1.12e+08/1.52e+10 =  0% of the original kernel matrix.

torch.Size([1224127, 2])
We keep 2.48e+09/4.12e+11 =  0% of the original kernel matrix.

torch.Size([279645, 2])
We keep 1.32e+08/1.83e+10 =  0% of the original kernel matrix.

torch.Size([17041, 2])
We keep 1.67e+06/5.49e+07 =  3% of the original kernel matrix.

torch.Size([30730, 2])
We keep 3.16e+06/2.11e+08 =  1% of the original kernel matrix.

torch.Size([123526, 2])
We keep 5.51e+07/4.16e+09 =  1% of the original kernel matrix.

torch.Size([81466, 2])
We keep 1.75e+07/1.84e+09 =  0% of the original kernel matrix.

torch.Size([59029, 2])
We keep 4.59e+07/1.27e+09 =  3% of the original kernel matrix.

torch.Size([57735, 2])
We keep 1.09e+07/1.02e+09 =  1% of the original kernel matrix.

torch.Size([47451, 2])
We keep 1.95e+07/7.01e+08 =  2% of the original kernel matrix.

torch.Size([52788, 2])
We keep 8.52e+06/7.54e+08 =  1% of the original kernel matrix.

torch.Size([178876, 2])
We keep 3.50e+08/2.07e+10 =  1% of the original kernel matrix.

torch.Size([97681, 2])
We keep 3.48e+07/4.10e+09 =  0% of the original kernel matrix.

torch.Size([63946, 2])
We keep 1.53e+08/3.70e+09 =  4% of the original kernel matrix.

torch.Size([54129, 2])
We keep 1.67e+07/1.73e+09 =  0% of the original kernel matrix.

torch.Size([64467, 2])
We keep 3.00e+07/1.41e+09 =  2% of the original kernel matrix.

torch.Size([60013, 2])
We keep 1.11e+07/1.07e+09 =  1% of the original kernel matrix.

torch.Size([45483, 2])
We keep 1.25e+07/6.10e+08 =  2% of the original kernel matrix.

torch.Size([51139, 2])
We keep 7.97e+06/7.04e+08 =  1% of the original kernel matrix.

torch.Size([13774, 2])
We keep 9.06e+05/3.05e+07 =  2% of the original kernel matrix.

torch.Size([27848, 2])
We keep 2.51e+06/1.57e+08 =  1% of the original kernel matrix.

torch.Size([236108, 2])
We keep 7.43e+08/4.49e+10 =  1% of the original kernel matrix.

torch.Size([108822, 2])
We keep 4.84e+07/6.03e+09 =  0% of the original kernel matrix.

torch.Size([34551, 2])
We keep 1.58e+07/3.66e+08 =  4% of the original kernel matrix.

torch.Size([44101, 2])
We keep 6.52e+06/5.45e+08 =  1% of the original kernel matrix.

torch.Size([14599, 2])
We keep 1.15e+06/4.00e+07 =  2% of the original kernel matrix.

torch.Size([28540, 2])
We keep 2.80e+06/1.80e+08 =  1% of the original kernel matrix.

torch.Size([55144, 2])
We keep 1.62e+07/9.32e+08 =  1% of the original kernel matrix.

torch.Size([56279, 2])
We keep 9.44e+06/8.70e+08 =  1% of the original kernel matrix.

torch.Size([214852, 2])
We keep 3.19e+08/2.00e+10 =  1% of the original kernel matrix.

torch.Size([108553, 2])
We keep 3.45e+07/4.03e+09 =  0% of the original kernel matrix.

torch.Size([586889, 2])
We keep 8.73e+08/1.07e+11 =  0% of the original kernel matrix.

torch.Size([186065, 2])
We keep 7.17e+07/9.32e+09 =  0% of the original kernel matrix.

torch.Size([214170, 2])
We keep 1.28e+08/1.44e+10 =  0% of the original kernel matrix.

torch.Size([110180, 2])
We keep 2.96e+07/3.42e+09 =  0% of the original kernel matrix.

torch.Size([31102, 2])
We keep 4.96e+06/2.79e+08 =  1% of the original kernel matrix.

torch.Size([41693, 2])
We keep 5.81e+06/4.76e+08 =  1% of the original kernel matrix.

torch.Size([148234, 2])
We keep 1.10e+08/8.51e+09 =  1% of the original kernel matrix.

torch.Size([89374, 2])
We keep 2.39e+07/2.63e+09 =  0% of the original kernel matrix.

torch.Size([17318, 2])
We keep 1.75e+06/6.20e+07 =  2% of the original kernel matrix.

torch.Size([30820, 2])
We keep 3.31e+06/2.24e+08 =  1% of the original kernel matrix.

torch.Size([97510, 2])
We keep 4.89e+07/3.18e+09 =  1% of the original kernel matrix.

torch.Size([73254, 2])
We keep 1.55e+07/1.61e+09 =  0% of the original kernel matrix.

torch.Size([146082, 2])
We keep 7.03e+07/6.66e+09 =  1% of the original kernel matrix.

torch.Size([89921, 2])
We keep 2.11e+07/2.33e+09 =  0% of the original kernel matrix.

torch.Size([55028, 2])
We keep 2.77e+07/1.06e+09 =  2% of the original kernel matrix.

torch.Size([55464, 2])
We keep 1.00e+07/9.27e+08 =  1% of the original kernel matrix.

torch.Size([236897, 2])
We keep 1.58e+08/1.75e+10 =  0% of the original kernel matrix.

torch.Size([116092, 2])
We keep 3.19e+07/3.77e+09 =  0% of the original kernel matrix.

torch.Size([5310, 2])
We keep 1.88e+05/3.92e+06 =  4% of the original kernel matrix.

torch.Size([18485, 2])
We keep 1.22e+06/5.64e+07 =  2% of the original kernel matrix.

torch.Size([40377, 2])
We keep 7.24e+06/4.65e+08 =  1% of the original kernel matrix.

torch.Size([48525, 2])
We keep 7.16e+06/6.15e+08 =  1% of the original kernel matrix.

torch.Size([281795, 2])
We keep 2.79e+08/2.87e+10 =  0% of the original kernel matrix.

torch.Size([127425, 2])
We keep 4.03e+07/4.83e+09 =  0% of the original kernel matrix.

torch.Size([120185, 2])
We keep 9.24e+07/6.32e+09 =  1% of the original kernel matrix.

torch.Size([80469, 2])
We keep 2.08e+07/2.27e+09 =  0% of the original kernel matrix.

torch.Size([232242, 2])
We keep 2.41e+08/2.11e+10 =  1% of the original kernel matrix.

torch.Size([113786, 2])
We keep 3.51e+07/4.14e+09 =  0% of the original kernel matrix.

torch.Size([10441, 2])
We keep 6.17e+05/1.82e+07 =  3% of the original kernel matrix.

torch.Size([24455, 2])
We keep 2.12e+06/1.22e+08 =  1% of the original kernel matrix.

torch.Size([586797, 2])
We keep 6.73e+08/1.20e+11 =  0% of the original kernel matrix.

torch.Size([186332, 2])
We keep 7.46e+07/9.88e+09 =  0% of the original kernel matrix.

torch.Size([57188, 2])
We keep 1.07e+07/9.36e+08 =  1% of the original kernel matrix.

torch.Size([57886, 2])
We keep 9.39e+06/8.72e+08 =  1% of the original kernel matrix.

torch.Size([80033, 2])
We keep 2.53e+07/2.00e+09 =  1% of the original kernel matrix.

torch.Size([66713, 2])
We keep 1.29e+07/1.27e+09 =  1% of the original kernel matrix.

torch.Size([240141, 2])
We keep 1.84e+08/1.91e+10 =  0% of the original kernel matrix.

torch.Size([117213, 2])
We keep 3.36e+07/3.94e+09 =  0% of the original kernel matrix.

torch.Size([10034, 2])
We keep 4.85e+05/1.55e+07 =  3% of the original kernel matrix.

torch.Size([24156, 2])
We keep 2.00e+06/1.12e+08 =  1% of the original kernel matrix.

torch.Size([328619, 2])
We keep 4.10e+08/3.78e+10 =  1% of the original kernel matrix.

torch.Size([138161, 2])
We keep 4.54e+07/5.54e+09 =  0% of the original kernel matrix.

torch.Size([528143, 2])
We keep 1.37e+09/1.01e+11 =  1% of the original kernel matrix.

torch.Size([175829, 2])
We keep 6.99e+07/9.03e+09 =  0% of the original kernel matrix.

torch.Size([2014841, 2])
We keep 9.86e+09/1.09e+12 =  0% of the original kernel matrix.

torch.Size([354989, 2])
We keep 2.08e+08/2.97e+10 =  0% of the original kernel matrix.

torch.Size([264037, 2])
We keep 2.05e+08/2.40e+10 =  0% of the original kernel matrix.

torch.Size([122688, 2])
We keep 3.72e+07/4.41e+09 =  0% of the original kernel matrix.

torch.Size([139324, 2])
We keep 7.35e+07/6.17e+09 =  1% of the original kernel matrix.

torch.Size([86891, 2])
We keep 2.08e+07/2.24e+09 =  0% of the original kernel matrix.

torch.Size([35505, 2])
We keep 1.03e+07/4.55e+08 =  2% of the original kernel matrix.

torch.Size([44356, 2])
We keep 7.09e+06/6.07e+08 =  1% of the original kernel matrix.

torch.Size([23138, 2])
We keep 2.09e+06/1.13e+08 =  1% of the original kernel matrix.

torch.Size([36007, 2])
We keep 4.10e+06/3.03e+08 =  1% of the original kernel matrix.

torch.Size([453582, 2])
We keep 7.49e+08/7.88e+10 =  0% of the original kernel matrix.

torch.Size([162350, 2])
We keep 6.30e+07/8.00e+09 =  0% of the original kernel matrix.

torch.Size([26198, 2])
We keep 2.81e+06/1.61e+08 =  1% of the original kernel matrix.

torch.Size([38520, 2])
We keep 4.71e+06/3.61e+08 =  1% of the original kernel matrix.

torch.Size([71777, 2])
We keep 4.47e+07/1.88e+09 =  2% of the original kernel matrix.

torch.Size([62936, 2])
We keep 1.23e+07/1.24e+09 =  0% of the original kernel matrix.

torch.Size([81409, 2])
We keep 6.50e+07/2.70e+09 =  2% of the original kernel matrix.

torch.Size([67336, 2])
We keep 1.32e+07/1.48e+09 =  0% of the original kernel matrix.

torch.Size([89003, 2])
We keep 8.07e+07/2.47e+09 =  3% of the original kernel matrix.

torch.Size([70428, 2])
We keep 1.37e+07/1.42e+09 =  0% of the original kernel matrix.

torch.Size([42383, 2])
We keep 1.99e+07/7.80e+08 =  2% of the original kernel matrix.

torch.Size([46231, 2])
We keep 8.44e+06/7.96e+08 =  1% of the original kernel matrix.

torch.Size([16867, 2])
We keep 1.16e+06/4.95e+07 =  2% of the original kernel matrix.

torch.Size([30626, 2])
We keep 3.02e+06/2.00e+08 =  1% of the original kernel matrix.

torch.Size([186120, 2])
We keep 4.68e+08/3.22e+10 =  1% of the original kernel matrix.

torch.Size([99112, 2])
We keep 4.15e+07/5.11e+09 =  0% of the original kernel matrix.

torch.Size([70906, 2])
We keep 3.55e+08/1.94e+09 = 18% of the original kernel matrix.

torch.Size([62352, 2])
We keep 1.28e+07/1.25e+09 =  1% of the original kernel matrix.

torch.Size([43758, 2])
We keep 7.04e+07/1.22e+09 =  5% of the original kernel matrix.

torch.Size([47726, 2])
We keep 1.07e+07/9.93e+08 =  1% of the original kernel matrix.

torch.Size([19670, 2])
We keep 1.67e+06/7.58e+07 =  2% of the original kernel matrix.

torch.Size([33089, 2])
We keep 3.55e+06/2.48e+08 =  1% of the original kernel matrix.

torch.Size([176795, 2])
We keep 2.28e+08/1.28e+10 =  1% of the original kernel matrix.

torch.Size([99693, 2])
We keep 2.75e+07/3.22e+09 =  0% of the original kernel matrix.

torch.Size([101203, 2])
We keep 3.61e+07/2.95e+09 =  1% of the original kernel matrix.

torch.Size([74581, 2])
We keep 1.51e+07/1.55e+09 =  0% of the original kernel matrix.

torch.Size([340000, 2])
We keep 9.30e+08/6.09e+10 =  1% of the original kernel matrix.

torch.Size([137963, 2])
We keep 5.61e+07/7.03e+09 =  0% of the original kernel matrix.

torch.Size([20570, 2])
We keep 3.90e+06/1.23e+08 =  3% of the original kernel matrix.

torch.Size([33789, 2])
We keep 4.23e+06/3.15e+08 =  1% of the original kernel matrix.

torch.Size([70417, 2])
We keep 5.90e+07/2.79e+09 =  2% of the original kernel matrix.

torch.Size([60577, 2])
We keep 1.50e+07/1.50e+09 =  0% of the original kernel matrix.

torch.Size([35833, 2])
We keep 5.37e+06/3.75e+08 =  1% of the original kernel matrix.

torch.Size([45583, 2])
We keep 6.50e+06/5.52e+08 =  1% of the original kernel matrix.

torch.Size([411142, 2])
We keep 1.17e+09/8.79e+10 =  1% of the original kernel matrix.

torch.Size([149218, 2])
We keep 6.53e+07/8.45e+09 =  0% of the original kernel matrix.

torch.Size([73480, 2])
We keep 2.40e+07/2.45e+09 =  0% of the original kernel matrix.

torch.Size([63814, 2])
We keep 1.38e+07/1.41e+09 =  0% of the original kernel matrix.

torch.Size([56728, 2])
We keep 2.03e+07/9.61e+08 =  2% of the original kernel matrix.

torch.Size([57233, 2])
We keep 9.56e+06/8.83e+08 =  1% of the original kernel matrix.

torch.Size([348261, 2])
We keep 2.23e+08/3.61e+10 =  0% of the original kernel matrix.

torch.Size([143378, 2])
We keep 4.39e+07/5.41e+09 =  0% of the original kernel matrix.

torch.Size([434490, 2])
We keep 3.91e+08/5.80e+10 =  0% of the original kernel matrix.

torch.Size([161758, 2])
We keep 5.44e+07/6.86e+09 =  0% of the original kernel matrix.

torch.Size([13131, 2])
We keep 9.98e+05/2.96e+07 =  3% of the original kernel matrix.

torch.Size([27523, 2])
We keep 2.37e+06/1.55e+08 =  1% of the original kernel matrix.

torch.Size([444484, 2])
We keep 1.05e+09/6.36e+10 =  1% of the original kernel matrix.

torch.Size([162231, 2])
We keep 5.68e+07/7.19e+09 =  0% of the original kernel matrix.

torch.Size([391106, 2])
We keep 3.19e+08/5.14e+10 =  0% of the original kernel matrix.

torch.Size([153328, 2])
We keep 5.21e+07/6.46e+09 =  0% of the original kernel matrix.

torch.Size([1175504, 2])
We keep 1.91e+09/3.77e+11 =  0% of the original kernel matrix.

torch.Size([276521, 2])
We keep 1.26e+08/1.75e+10 =  0% of the original kernel matrix.

torch.Size([45769, 2])
We keep 1.83e+07/6.34e+08 =  2% of the original kernel matrix.

torch.Size([51276, 2])
We keep 8.13e+06/7.18e+08 =  1% of the original kernel matrix.

torch.Size([12146, 2])
We keep 8.84e+05/2.74e+07 =  3% of the original kernel matrix.

torch.Size([26142, 2])
We keep 2.44e+06/1.49e+08 =  1% of the original kernel matrix.

torch.Size([37629, 2])
We keep 5.31e+06/3.83e+08 =  1% of the original kernel matrix.

torch.Size([46903, 2])
We keep 6.58e+06/5.57e+08 =  1% of the original kernel matrix.

torch.Size([38194, 2])
We keep 1.15e+07/5.49e+08 =  2% of the original kernel matrix.

torch.Size([46360, 2])
We keep 7.35e+06/6.68e+08 =  1% of the original kernel matrix.

torch.Size([64567, 2])
We keep 1.53e+07/1.28e+09 =  1% of the original kernel matrix.

torch.Size([60476, 2])
We keep 1.07e+07/1.02e+09 =  1% of the original kernel matrix.

torch.Size([204094, 2])
We keep 2.48e+08/1.44e+10 =  1% of the original kernel matrix.

torch.Size([107616, 2])
We keep 2.99e+07/3.41e+09 =  0% of the original kernel matrix.

torch.Size([7573, 2])
We keep 2.78e+05/7.77e+06 =  3% of the original kernel matrix.

torch.Size([21382, 2])
We keep 1.55e+06/7.94e+07 =  1% of the original kernel matrix.

torch.Size([189712, 2])
We keep 1.28e+08/1.09e+10 =  1% of the original kernel matrix.

torch.Size([103516, 2])
We keep 2.63e+07/2.98e+09 =  0% of the original kernel matrix.

torch.Size([353538, 2])
We keep 2.71e+08/3.90e+10 =  0% of the original kernel matrix.

torch.Size([145366, 2])
We keep 4.56e+07/5.63e+09 =  0% of the original kernel matrix.

torch.Size([19646, 2])
We keep 6.36e+07/5.09e+08 = 12% of the original kernel matrix.

torch.Size([31768, 2])
We keep 6.69e+06/6.42e+08 =  1% of the original kernel matrix.

torch.Size([26160, 2])
We keep 4.53e+06/1.92e+08 =  2% of the original kernel matrix.

torch.Size([38517, 2])
We keep 5.08e+06/3.95e+08 =  1% of the original kernel matrix.

torch.Size([257288, 2])
We keep 2.05e+08/2.08e+10 =  0% of the original kernel matrix.

torch.Size([121730, 2])
We keep 3.46e+07/4.11e+09 =  0% of the original kernel matrix.

torch.Size([40838, 2])
We keep 7.05e+06/4.90e+08 =  1% of the original kernel matrix.

torch.Size([48688, 2])
We keep 7.29e+06/6.31e+08 =  1% of the original kernel matrix.

torch.Size([47873, 2])
We keep 1.40e+07/7.61e+08 =  1% of the original kernel matrix.

torch.Size([49940, 2])
We keep 8.28e+06/7.86e+08 =  1% of the original kernel matrix.

torch.Size([43281, 2])
We keep 7.24e+06/5.39e+08 =  1% of the original kernel matrix.

torch.Size([49830, 2])
We keep 7.52e+06/6.62e+08 =  1% of the original kernel matrix.

torch.Size([16476, 2])
We keep 1.23e+06/4.79e+07 =  2% of the original kernel matrix.

torch.Size([30374, 2])
We keep 2.94e+06/1.97e+08 =  1% of the original kernel matrix.

torch.Size([136893, 2])
We keep 1.30e+08/6.53e+09 =  1% of the original kernel matrix.

torch.Size([86001, 2])
We keep 2.07e+07/2.30e+09 =  0% of the original kernel matrix.

torch.Size([114632, 2])
We keep 3.70e+07/3.76e+09 =  0% of the original kernel matrix.

torch.Size([79212, 2])
We keep 1.66e+07/1.75e+09 =  0% of the original kernel matrix.

torch.Size([59089, 2])
We keep 1.86e+07/1.09e+09 =  1% of the original kernel matrix.

torch.Size([58262, 2])
We keep 9.99e+06/9.39e+08 =  1% of the original kernel matrix.

torch.Size([118057, 2])
We keep 6.46e+07/4.85e+09 =  1% of the original kernel matrix.

torch.Size([80337, 2])
We keep 1.88e+07/1.98e+09 =  0% of the original kernel matrix.

torch.Size([44408, 2])
We keep 9.06e+06/5.95e+08 =  1% of the original kernel matrix.

torch.Size([51200, 2])
We keep 8.00e+06/6.95e+08 =  1% of the original kernel matrix.

torch.Size([1726345, 2])
We keep 6.10e+09/7.95e+11 =  0% of the original kernel matrix.

torch.Size([328730, 2])
We keep 1.78e+08/2.54e+10 =  0% of the original kernel matrix.

torch.Size([76295, 2])
We keep 2.64e+07/1.78e+09 =  1% of the original kernel matrix.

torch.Size([65182, 2])
We keep 1.23e+07/1.20e+09 =  1% of the original kernel matrix.

torch.Size([331711, 2])
We keep 2.56e+08/3.47e+10 =  0% of the original kernel matrix.

torch.Size([139344, 2])
We keep 4.32e+07/5.30e+09 =  0% of the original kernel matrix.

torch.Size([9993, 2])
We keep 4.62e+05/1.49e+07 =  3% of the original kernel matrix.

torch.Size([24098, 2])
We keep 1.95e+06/1.10e+08 =  1% of the original kernel matrix.

torch.Size([27352, 2])
We keep 3.39e+06/1.71e+08 =  1% of the original kernel matrix.

torch.Size([39321, 2])
We keep 4.83e+06/3.72e+08 =  1% of the original kernel matrix.

torch.Size([18072, 2])
We keep 1.53e+06/6.32e+07 =  2% of the original kernel matrix.

torch.Size([31570, 2])
We keep 3.34e+06/2.26e+08 =  1% of the original kernel matrix.

torch.Size([44230, 2])
We keep 7.20e+06/5.44e+08 =  1% of the original kernel matrix.

torch.Size([51075, 2])
We keep 7.39e+06/6.64e+08 =  1% of the original kernel matrix.

torch.Size([16826, 2])
We keep 1.27e+06/4.98e+07 =  2% of the original kernel matrix.

torch.Size([30595, 2])
We keep 3.03e+06/2.01e+08 =  1% of the original kernel matrix.

torch.Size([161844, 2])
We keep 2.42e+08/9.89e+09 =  2% of the original kernel matrix.

torch.Size([94748, 2])
We keep 2.52e+07/2.83e+09 =  0% of the original kernel matrix.

torch.Size([22356, 2])
We keep 2.10e+06/1.04e+08 =  2% of the original kernel matrix.

torch.Size([35206, 2])
We keep 4.01e+06/2.91e+08 =  1% of the original kernel matrix.

torch.Size([23951, 2])
We keep 2.43e+06/1.27e+08 =  1% of the original kernel matrix.

torch.Size([36649, 2])
We keep 4.31e+06/3.20e+08 =  1% of the original kernel matrix.

torch.Size([697567, 2])
We keep 1.00e+09/1.39e+11 =  0% of the original kernel matrix.

torch.Size([202507, 2])
We keep 8.08e+07/1.06e+10 =  0% of the original kernel matrix.

torch.Size([149214, 2])
We keep 9.41e+07/7.06e+09 =  1% of the original kernel matrix.

torch.Size([90898, 2])
We keep 2.20e+07/2.39e+09 =  0% of the original kernel matrix.

torch.Size([83309, 2])
We keep 2.80e+07/2.18e+09 =  1% of the original kernel matrix.

torch.Size([67936, 2])
We keep 1.33e+07/1.33e+09 =  1% of the original kernel matrix.

torch.Size([322793, 2])
We keep 2.37e+08/3.28e+10 =  0% of the original kernel matrix.

torch.Size([138753, 2])
We keep 4.24e+07/5.16e+09 =  0% of the original kernel matrix.

torch.Size([1048805, 2])
We keep 1.49e+09/2.94e+11 =  0% of the original kernel matrix.

torch.Size([258140, 2])
We keep 1.13e+08/1.54e+10 =  0% of the original kernel matrix.

torch.Size([20065, 2])
We keep 1.84e+06/8.33e+07 =  2% of the original kernel matrix.

torch.Size([33313, 2])
We keep 3.69e+06/2.60e+08 =  1% of the original kernel matrix.

torch.Size([31142, 2])
We keep 3.65e+06/2.42e+08 =  1% of the original kernel matrix.

torch.Size([42399, 2])
We keep 5.52e+06/4.43e+08 =  1% of the original kernel matrix.

torch.Size([55074, 2])
We keep 2.84e+07/1.27e+09 =  2% of the original kernel matrix.

torch.Size([55033, 2])
We keep 1.07e+07/1.02e+09 =  1% of the original kernel matrix.

torch.Size([10429, 2])
We keep 5.99e+05/1.84e+07 =  3% of the original kernel matrix.

torch.Size([24336, 2])
We keep 2.11e+06/1.22e+08 =  1% of the original kernel matrix.

torch.Size([29607, 2])
We keep 3.74e+06/2.23e+08 =  1% of the original kernel matrix.

torch.Size([41057, 2])
We keep 5.33e+06/4.25e+08 =  1% of the original kernel matrix.

torch.Size([39359, 2])
We keep 9.24e+06/5.00e+08 =  1% of the original kernel matrix.

torch.Size([47714, 2])
We keep 7.39e+06/6.37e+08 =  1% of the original kernel matrix.

torch.Size([17395, 2])
We keep 1.69e+06/5.93e+07 =  2% of the original kernel matrix.

torch.Size([31089, 2])
We keep 3.22e+06/2.19e+08 =  1% of the original kernel matrix.

torch.Size([28051, 2])
We keep 3.43e+06/1.95e+08 =  1% of the original kernel matrix.

torch.Size([39845, 2])
We keep 5.08e+06/3.97e+08 =  1% of the original kernel matrix.

torch.Size([72986, 2])
We keep 2.04e+07/1.54e+09 =  1% of the original kernel matrix.

torch.Size([64250, 2])
We keep 1.14e+07/1.12e+09 =  1% of the original kernel matrix.

torch.Size([39100, 2])
We keep 8.56e+06/4.43e+08 =  1% of the original kernel matrix.

torch.Size([47141, 2])
We keep 6.97e+06/5.99e+08 =  1% of the original kernel matrix.

torch.Size([1041377, 2])
We keep 2.01e+09/3.20e+11 =  0% of the original kernel matrix.

torch.Size([254927, 2])
We keep 1.16e+08/1.61e+10 =  0% of the original kernel matrix.

torch.Size([27130, 2])
We keep 7.24e+06/3.03e+08 =  2% of the original kernel matrix.

torch.Size([38737, 2])
We keep 6.04e+06/4.96e+08 =  1% of the original kernel matrix.

torch.Size([191884, 2])
We keep 1.57e+08/1.23e+10 =  1% of the original kernel matrix.

torch.Size([103565, 2])
We keep 2.80e+07/3.17e+09 =  0% of the original kernel matrix.

torch.Size([21178, 2])
We keep 4.89e+06/1.27e+08 =  3% of the original kernel matrix.

torch.Size([34259, 2])
We keep 4.26e+06/3.22e+08 =  1% of the original kernel matrix.

torch.Size([12284, 2])
We keep 9.32e+05/2.86e+07 =  3% of the original kernel matrix.

torch.Size([26168, 2])
We keep 2.49e+06/1.52e+08 =  1% of the original kernel matrix.

torch.Size([455867, 2])
We keep 5.58e+08/6.89e+10 =  0% of the original kernel matrix.

torch.Size([163753, 2])
We keep 5.88e+07/7.48e+09 =  0% of the original kernel matrix.

torch.Size([771217, 2])
We keep 3.28e+09/2.07e+11 =  1% of the original kernel matrix.

torch.Size([214644, 2])
We keep 9.28e+07/1.29e+10 =  0% of the original kernel matrix.

torch.Size([1256344, 2])
We keep 1.99e+09/4.20e+11 =  0% of the original kernel matrix.

torch.Size([281038, 2])
We keep 1.31e+08/1.85e+10 =  0% of the original kernel matrix.

torch.Size([92141, 2])
We keep 9.41e+07/2.41e+09 =  3% of the original kernel matrix.

torch.Size([71517, 2])
We keep 1.35e+07/1.40e+09 =  0% of the original kernel matrix.

torch.Size([114294, 2])
We keep 6.02e+07/3.80e+09 =  1% of the original kernel matrix.

torch.Size([79085, 2])
We keep 1.68e+07/1.76e+09 =  0% of the original kernel matrix.

torch.Size([22597, 2])
We keep 2.00e+06/1.05e+08 =  1% of the original kernel matrix.

torch.Size([35535, 2])
We keep 3.98e+06/2.91e+08 =  1% of the original kernel matrix.

torch.Size([32511, 2])
We keep 1.97e+07/3.87e+08 =  5% of the original kernel matrix.

torch.Size([42659, 2])
We keep 6.18e+06/5.60e+08 =  1% of the original kernel matrix.

torch.Size([2602470, 2])
We keep 2.18e+10/2.39e+12 =  0% of the original kernel matrix.

torch.Size([394889, 2])
We keep 2.99e+08/4.41e+10 =  0% of the original kernel matrix.

torch.Size([48333, 2])
We keep 1.55e+07/7.02e+08 =  2% of the original kernel matrix.

torch.Size([53575, 2])
We keep 8.29e+06/7.55e+08 =  1% of the original kernel matrix.

torch.Size([25708, 2])
We keep 3.18e+06/1.68e+08 =  1% of the original kernel matrix.

torch.Size([37960, 2])
We keep 4.82e+06/3.70e+08 =  1% of the original kernel matrix.

torch.Size([121963, 2])
We keep 9.14e+07/5.44e+09 =  1% of the original kernel matrix.

torch.Size([81924, 2])
We keep 1.93e+07/2.10e+09 =  0% of the original kernel matrix.

torch.Size([1540303, 2])
We keep 4.64e+09/6.95e+11 =  0% of the original kernel matrix.

torch.Size([306812, 2])
We keep 1.68e+08/2.37e+10 =  0% of the original kernel matrix.

torch.Size([179353, 2])
We keep 1.11e+08/1.11e+10 =  0% of the original kernel matrix.

torch.Size([98348, 2])
We keep 2.64e+07/3.01e+09 =  0% of the original kernel matrix.

torch.Size([37780, 2])
We keep 8.66e+06/4.75e+08 =  1% of the original kernel matrix.

torch.Size([45812, 2])
We keep 7.18e+06/6.21e+08 =  1% of the original kernel matrix.

torch.Size([31379, 2])
We keep 6.10e+06/2.71e+08 =  2% of the original kernel matrix.

torch.Size([42560, 2])
We keep 5.80e+06/4.69e+08 =  1% of the original kernel matrix.

torch.Size([27222, 2])
We keep 3.07e+06/1.87e+08 =  1% of the original kernel matrix.

torch.Size([39155, 2])
We keep 5.02e+06/3.90e+08 =  1% of the original kernel matrix.

torch.Size([19667, 2])
We keep 1.75e+06/7.54e+07 =  2% of the original kernel matrix.

torch.Size([33051, 2])
We keep 3.54e+06/2.47e+08 =  1% of the original kernel matrix.

torch.Size([479610, 2])
We keep 5.58e+08/7.05e+10 =  0% of the original kernel matrix.

torch.Size([169210, 2])
We keep 5.92e+07/7.57e+09 =  0% of the original kernel matrix.

torch.Size([38214, 2])
We keep 9.11e+06/4.70e+08 =  1% of the original kernel matrix.

torch.Size([46627, 2])
We keep 7.13e+06/6.17e+08 =  1% of the original kernel matrix.

torch.Size([10709, 2])
We keep 1.01e+06/2.20e+07 =  4% of the original kernel matrix.

torch.Size([24628, 2])
We keep 2.25e+06/1.34e+08 =  1% of the original kernel matrix.

torch.Size([231173, 2])
We keep 1.54e+08/1.55e+10 =  0% of the original kernel matrix.

torch.Size([115081, 2])
We keep 3.01e+07/3.55e+09 =  0% of the original kernel matrix.

torch.Size([36717, 2])
We keep 5.72e+06/3.66e+08 =  1% of the original kernel matrix.

torch.Size([46402, 2])
We keep 6.57e+06/5.45e+08 =  1% of the original kernel matrix.

torch.Size([4953, 2])
We keep 1.60e+05/3.20e+06 =  5% of the original kernel matrix.

torch.Size([18019, 2])
We keep 1.16e+06/5.10e+07 =  2% of the original kernel matrix.

torch.Size([223200, 2])
We keep 2.04e+08/1.73e+10 =  1% of the original kernel matrix.

torch.Size([112092, 2])
We keep 3.21e+07/3.74e+09 =  0% of the original kernel matrix.

torch.Size([232793, 2])
We keep 2.07e+08/1.76e+10 =  1% of the original kernel matrix.

torch.Size([115485, 2])
We keep 3.24e+07/3.78e+09 =  0% of the original kernel matrix.

torch.Size([453333, 2])
We keep 1.15e+09/8.71e+10 =  1% of the original kernel matrix.

torch.Size([162163, 2])
We keep 6.56e+07/8.41e+09 =  0% of the original kernel matrix.

torch.Size([586275, 2])
We keep 1.14e+09/1.10e+11 =  1% of the original kernel matrix.

torch.Size([183522, 2])
We keep 7.30e+07/9.46e+09 =  0% of the original kernel matrix.

torch.Size([583504, 2])
We keep 9.23e+08/1.10e+11 =  0% of the original kernel matrix.

torch.Size([183091, 2])
We keep 7.32e+07/9.45e+09 =  0% of the original kernel matrix.

torch.Size([74785, 2])
We keep 3.38e+07/2.03e+09 =  1% of the original kernel matrix.

torch.Size([63541, 2])
We keep 1.30e+07/1.28e+09 =  1% of the original kernel matrix.

torch.Size([118391, 2])
We keep 6.63e+07/4.02e+09 =  1% of the original kernel matrix.

torch.Size([80019, 2])
We keep 1.70e+07/1.81e+09 =  0% of the original kernel matrix.

torch.Size([462994, 2])
We keep 4.49e+08/6.42e+10 =  0% of the original kernel matrix.

torch.Size([166581, 2])
We keep 5.69e+07/7.22e+09 =  0% of the original kernel matrix.

torch.Size([684838, 2])
We keep 8.93e+08/1.24e+11 =  0% of the original kernel matrix.

torch.Size([200553, 2])
We keep 7.55e+07/1.00e+10 =  0% of the original kernel matrix.

torch.Size([44009, 2])
We keep 1.03e+07/6.07e+08 =  1% of the original kernel matrix.

torch.Size([50805, 2])
We keep 8.00e+06/7.02e+08 =  1% of the original kernel matrix.

torch.Size([237806, 2])
We keep 2.56e+09/3.28e+10 =  7% of the original kernel matrix.

torch.Size([117279, 2])
We keep 4.06e+07/5.16e+09 =  0% of the original kernel matrix.

torch.Size([400050, 2])
We keep 3.95e+09/7.05e+10 =  5% of the original kernel matrix.

torch.Size([152622, 2])
We keep 5.88e+07/7.56e+09 =  0% of the original kernel matrix.

torch.Size([344022, 2])
We keep 5.80e+08/4.48e+10 =  1% of the original kernel matrix.

torch.Size([142093, 2])
We keep 4.82e+07/6.03e+09 =  0% of the original kernel matrix.

torch.Size([231330, 2])
We keep 2.52e+08/1.97e+10 =  1% of the original kernel matrix.

torch.Size([114461, 2])
We keep 3.38e+07/4.00e+09 =  0% of the original kernel matrix.

torch.Size([96003, 2])
We keep 1.72e+08/4.74e+09 =  3% of the original kernel matrix.

torch.Size([71685, 2])
We keep 1.85e+07/1.96e+09 =  0% of the original kernel matrix.

torch.Size([50664, 2])
We keep 1.63e+07/9.48e+08 =  1% of the original kernel matrix.

torch.Size([53033, 2])
We keep 9.67e+06/8.77e+08 =  1% of the original kernel matrix.

torch.Size([32576, 2])
We keep 4.30e+06/2.72e+08 =  1% of the original kernel matrix.

torch.Size([43119, 2])
We keep 5.75e+06/4.70e+08 =  1% of the original kernel matrix.

torch.Size([59900, 2])
We keep 2.38e+07/1.25e+09 =  1% of the original kernel matrix.

torch.Size([57889, 2])
We keep 1.06e+07/1.01e+09 =  1% of the original kernel matrix.

torch.Size([338267, 2])
We keep 2.28e+08/3.55e+10 =  0% of the original kernel matrix.

torch.Size([142154, 2])
We keep 4.37e+07/5.37e+09 =  0% of the original kernel matrix.

torch.Size([357099, 2])
We keep 2.54e+08/3.92e+10 =  0% of the original kernel matrix.

torch.Size([145345, 2])
We keep 4.55e+07/5.64e+09 =  0% of the original kernel matrix.

torch.Size([552452, 2])
We keep 1.17e+09/1.13e+11 =  1% of the original kernel matrix.

torch.Size([179716, 2])
We keep 7.38e+07/9.57e+09 =  0% of the original kernel matrix.

torch.Size([283068, 2])
We keep 2.84e+08/2.85e+10 =  0% of the original kernel matrix.

torch.Size([128058, 2])
We keep 3.99e+07/4.81e+09 =  0% of the original kernel matrix.

torch.Size([180901, 2])
We keep 1.35e+08/1.15e+10 =  1% of the original kernel matrix.

torch.Size([100937, 2])
We keep 2.68e+07/3.05e+09 =  0% of the original kernel matrix.

torch.Size([43426, 2])
We keep 7.62e+06/5.88e+08 =  1% of the original kernel matrix.

torch.Size([49443, 2])
We keep 7.78e+06/6.91e+08 =  1% of the original kernel matrix.

torch.Size([719902, 2])
We keep 1.14e+09/1.41e+11 =  0% of the original kernel matrix.

torch.Size([205721, 2])
We keep 8.15e+07/1.07e+10 =  0% of the original kernel matrix.

torch.Size([19445, 2])
We keep 4.18e+06/1.16e+08 =  3% of the original kernel matrix.

torch.Size([32431, 2])
We keep 4.20e+06/3.07e+08 =  1% of the original kernel matrix.

torch.Size([44187, 2])
We keep 9.72e+06/5.95e+08 =  1% of the original kernel matrix.

torch.Size([50490, 2])
We keep 7.94e+06/6.95e+08 =  1% of the original kernel matrix.

torch.Size([394728, 2])
We keep 3.45e+08/4.84e+10 =  0% of the original kernel matrix.

torch.Size([152775, 2])
We keep 4.98e+07/6.27e+09 =  0% of the original kernel matrix.

torch.Size([221401, 2])
We keep 1.68e+08/1.60e+10 =  1% of the original kernel matrix.

torch.Size([112130, 2])
We keep 3.12e+07/3.61e+09 =  0% of the original kernel matrix.

torch.Size([17400, 2])
We keep 1.42e+06/5.68e+07 =  2% of the original kernel matrix.

torch.Size([30977, 2])
We keep 3.21e+06/2.15e+08 =  1% of the original kernel matrix.

torch.Size([58463, 2])
We keep 1.45e+07/1.04e+09 =  1% of the original kernel matrix.

torch.Size([57383, 2])
We keep 9.68e+06/9.19e+08 =  1% of the original kernel matrix.

torch.Size([1104813, 2])
We keep 5.08e+09/4.45e+11 =  1% of the original kernel matrix.

torch.Size([258773, 2])
We keep 1.37e+08/1.90e+10 =  0% of the original kernel matrix.

torch.Size([51565, 2])
We keep 1.16e+07/8.26e+08 =  1% of the original kernel matrix.

torch.Size([54792, 2])
We keep 9.04e+06/8.19e+08 =  1% of the original kernel matrix.

torch.Size([246259, 2])
We keep 2.32e+08/1.95e+10 =  1% of the original kernel matrix.

torch.Size([118910, 2])
We keep 3.35e+07/3.98e+09 =  0% of the original kernel matrix.

torch.Size([20326, 2])
We keep 1.69e+06/8.10e+07 =  2% of the original kernel matrix.

torch.Size([33352, 2])
We keep 3.61e+06/2.56e+08 =  1% of the original kernel matrix.

torch.Size([14212, 2])
We keep 8.95e+05/3.44e+07 =  2% of the original kernel matrix.

torch.Size([28040, 2])
We keep 2.62e+06/1.67e+08 =  1% of the original kernel matrix.

torch.Size([18976, 2])
We keep 1.69e+06/7.48e+07 =  2% of the original kernel matrix.

torch.Size([32453, 2])
We keep 3.54e+06/2.46e+08 =  1% of the original kernel matrix.

torch.Size([24071, 2])
We keep 4.23e+06/1.52e+08 =  2% of the original kernel matrix.

torch.Size([36699, 2])
We keep 4.61e+06/3.52e+08 =  1% of the original kernel matrix.

torch.Size([82304, 2])
We keep 3.23e+07/1.96e+09 =  1% of the original kernel matrix.

torch.Size([68074, 2])
We keep 1.26e+07/1.26e+09 =  1% of the original kernel matrix.

torch.Size([267571, 2])
We keep 3.41e+08/2.44e+10 =  1% of the original kernel matrix.

torch.Size([123381, 2])
We keep 3.76e+07/4.45e+09 =  0% of the original kernel matrix.

torch.Size([14452, 2])
We keep 3.04e+06/3.95e+07 =  7% of the original kernel matrix.

torch.Size([28242, 2])
We keep 2.81e+06/1.79e+08 =  1% of the original kernel matrix.

torch.Size([300686, 2])
We keep 2.94e+08/2.91e+10 =  1% of the original kernel matrix.

torch.Size([132478, 2])
We keep 4.01e+07/4.86e+09 =  0% of the original kernel matrix.

torch.Size([53680, 2])
We keep 2.38e+07/1.22e+09 =  1% of the original kernel matrix.

torch.Size([53960, 2])
We keep 1.06e+07/9.94e+08 =  1% of the original kernel matrix.

torch.Size([141821, 2])
We keep 9.90e+07/5.86e+09 =  1% of the original kernel matrix.

torch.Size([88050, 2])
We keep 2.03e+07/2.18e+09 =  0% of the original kernel matrix.

torch.Size([27168, 2])
We keep 6.08e+06/2.27e+08 =  2% of the original kernel matrix.

torch.Size([38874, 2])
We keep 5.42e+06/4.29e+08 =  1% of the original kernel matrix.

torch.Size([566743, 2])
We keep 6.07e+08/8.94e+10 =  0% of the original kernel matrix.

torch.Size([181441, 2])
We keep 6.61e+07/8.52e+09 =  0% of the original kernel matrix.

torch.Size([17625, 2])
We keep 2.37e+06/7.01e+07 =  3% of the original kernel matrix.

torch.Size([31210, 2])
We keep 3.48e+06/2.39e+08 =  1% of the original kernel matrix.

torch.Size([92735, 2])
We keep 6.35e+07/2.68e+09 =  2% of the original kernel matrix.

torch.Size([71188, 2])
We keep 1.43e+07/1.47e+09 =  0% of the original kernel matrix.

torch.Size([123532, 2])
We keep 4.91e+07/4.12e+09 =  1% of the original kernel matrix.

torch.Size([81525, 2])
We keep 1.74e+07/1.83e+09 =  0% of the original kernel matrix.

torch.Size([72888, 2])
We keep 4.95e+07/2.11e+09 =  2% of the original kernel matrix.

torch.Size([62910, 2])
We keep 1.33e+07/1.31e+09 =  1% of the original kernel matrix.

torch.Size([157531, 2])
We keep 8.88e+07/7.91e+09 =  1% of the original kernel matrix.

torch.Size([93785, 2])
We keep 2.30e+07/2.53e+09 =  0% of the original kernel matrix.

torch.Size([1009844, 2])
We keep 3.11e+09/3.39e+11 =  0% of the original kernel matrix.

torch.Size([248247, 2])
We keep 1.20e+08/1.66e+10 =  0% of the original kernel matrix.

torch.Size([60459, 2])
We keep 1.24e+07/1.13e+09 =  1% of the original kernel matrix.

torch.Size([59459, 2])
We keep 1.03e+07/9.59e+08 =  1% of the original kernel matrix.

torch.Size([22925, 2])
We keep 4.50e+06/1.46e+08 =  3% of the original kernel matrix.

torch.Size([35725, 2])
We keep 4.51e+06/3.44e+08 =  1% of the original kernel matrix.

torch.Size([116906, 2])
We keep 6.54e+07/4.17e+09 =  1% of the original kernel matrix.

torch.Size([79937, 2])
We keep 1.73e+07/1.84e+09 =  0% of the original kernel matrix.

torch.Size([78879, 2])
We keep 2.48e+07/2.00e+09 =  1% of the original kernel matrix.

torch.Size([65905, 2])
We keep 1.29e+07/1.27e+09 =  1% of the original kernel matrix.

torch.Size([1319810, 2])
We keep 2.17e+09/4.65e+11 =  0% of the original kernel matrix.

torch.Size([288249, 2])
We keep 1.38e+08/1.94e+10 =  0% of the original kernel matrix.

torch.Size([28358, 2])
We keep 4.59e+06/2.11e+08 =  2% of the original kernel matrix.

torch.Size([39964, 2])
We keep 5.22e+06/4.14e+08 =  1% of the original kernel matrix.

torch.Size([322481, 2])
We keep 2.69e+08/3.53e+10 =  0% of the original kernel matrix.

torch.Size([137234, 2])
We keep 4.39e+07/5.35e+09 =  0% of the original kernel matrix.

torch.Size([1006124, 2])
We keep 1.28e+09/2.73e+11 =  0% of the original kernel matrix.

torch.Size([251258, 2])
We keep 1.09e+08/1.49e+10 =  0% of the original kernel matrix.

torch.Size([44101, 2])
We keep 1.79e+07/7.28e+08 =  2% of the original kernel matrix.

torch.Size([47339, 2])
We keep 8.15e+06/7.69e+08 =  1% of the original kernel matrix.

torch.Size([335808, 2])
We keep 6.33e+08/5.29e+10 =  1% of the original kernel matrix.

torch.Size([137479, 2])
We keep 5.21e+07/6.55e+09 =  0% of the original kernel matrix.

torch.Size([35796, 2])
We keep 1.61e+07/6.64e+08 =  2% of the original kernel matrix.

torch.Size([44070, 2])
We keep 8.31e+06/7.34e+08 =  1% of the original kernel matrix.

torch.Size([1235592, 2])
We keep 3.21e+09/4.37e+11 =  0% of the original kernel matrix.

torch.Size([280549, 2])
We keep 1.36e+08/1.88e+10 =  0% of the original kernel matrix.

torch.Size([85827, 2])
We keep 2.32e+07/2.04e+09 =  1% of the original kernel matrix.

torch.Size([68997, 2])
We keep 1.30e+07/1.29e+09 =  1% of the original kernel matrix.

torch.Size([50374, 2])
We keep 1.59e+07/7.83e+08 =  2% of the original kernel matrix.

torch.Size([52253, 2])
We keep 8.58e+06/7.97e+08 =  1% of the original kernel matrix.

torch.Size([19561, 2])
We keep 6.93e+06/1.79e+08 =  3% of the original kernel matrix.

torch.Size([32233, 2])
We keep 4.98e+06/3.81e+08 =  1% of the original kernel matrix.

torch.Size([16650, 2])
We keep 1.90e+06/5.24e+07 =  3% of the original kernel matrix.

torch.Size([30519, 2])
We keep 3.10e+06/2.06e+08 =  1% of the original kernel matrix.

torch.Size([22980, 2])
We keep 3.08e+06/1.31e+08 =  2% of the original kernel matrix.

torch.Size([35859, 2])
We keep 4.37e+06/3.26e+08 =  1% of the original kernel matrix.

torch.Size([167773, 2])
We keep 2.15e+08/8.69e+09 =  2% of the original kernel matrix.

torch.Size([96689, 2])
We keep 2.34e+07/2.66e+09 =  0% of the original kernel matrix.

torch.Size([38905, 2])
We keep 5.51e+07/5.02e+08 = 10% of the original kernel matrix.

torch.Size([46965, 2])
We keep 7.52e+06/6.38e+08 =  1% of the original kernel matrix.

torch.Size([445853, 2])
We keep 4.60e+08/6.31e+10 =  0% of the original kernel matrix.

torch.Size([163676, 2])
We keep 5.66e+07/7.16e+09 =  0% of the original kernel matrix.

torch.Size([49135, 2])
We keep 8.49e+06/7.08e+08 =  1% of the original kernel matrix.

torch.Size([51453, 2])
We keep 8.04e+06/7.58e+08 =  1% of the original kernel matrix.

torch.Size([30187, 2])
We keep 6.16e+06/2.47e+08 =  2% of the original kernel matrix.

torch.Size([41284, 2])
We keep 5.60e+06/4.48e+08 =  1% of the original kernel matrix.

torch.Size([66430, 2])
We keep 1.79e+07/1.43e+09 =  1% of the original kernel matrix.

torch.Size([61167, 2])
We keep 1.13e+07/1.08e+09 =  1% of the original kernel matrix.

torch.Size([301478, 2])
We keep 6.37e+08/5.54e+10 =  1% of the original kernel matrix.

torch.Size([130864, 2])
We keep 5.32e+07/6.71e+09 =  0% of the original kernel matrix.

torch.Size([201713, 2])
We keep 5.76e+08/3.71e+10 =  1% of the original kernel matrix.

torch.Size([102079, 2])
We keep 4.51e+07/5.49e+09 =  0% of the original kernel matrix.

torch.Size([850509, 2])
We keep 1.26e+09/1.92e+11 =  0% of the original kernel matrix.

torch.Size([225997, 2])
We keep 9.29e+07/1.25e+10 =  0% of the original kernel matrix.

torch.Size([63903, 2])
We keep 6.69e+07/1.25e+09 =  5% of the original kernel matrix.

torch.Size([60400, 2])
We keep 1.04e+07/1.01e+09 =  1% of the original kernel matrix.

torch.Size([19784, 2])
We keep 2.87e+06/1.17e+08 =  2% of the original kernel matrix.

torch.Size([33287, 2])
We keep 4.17e+06/3.08e+08 =  1% of the original kernel matrix.

torch.Size([14096, 2])
We keep 9.89e+05/3.41e+07 =  2% of the original kernel matrix.

torch.Size([28040, 2])
We keep 2.64e+06/1.66e+08 =  1% of the original kernel matrix.

torch.Size([14882, 2])
We keep 1.70e+06/4.71e+07 =  3% of the original kernel matrix.

torch.Size([28877, 2])
We keep 2.96e+06/1.96e+08 =  1% of the original kernel matrix.

torch.Size([519412, 2])
We keep 1.29e+09/8.13e+10 =  1% of the original kernel matrix.

torch.Size([176712, 2])
We keep 6.14e+07/8.12e+09 =  0% of the original kernel matrix.

torch.Size([24269, 2])
We keep 2.73e+06/1.40e+08 =  1% of the original kernel matrix.

torch.Size([36928, 2])
We keep 4.47e+06/3.37e+08 =  1% of the original kernel matrix.

torch.Size([65417, 2])
We keep 1.86e+07/1.24e+09 =  1% of the original kernel matrix.

torch.Size([61063, 2])
We keep 1.05e+07/1.00e+09 =  1% of the original kernel matrix.

torch.Size([108525, 2])
We keep 5.33e+07/3.82e+09 =  1% of the original kernel matrix.

torch.Size([76945, 2])
We keep 1.68e+07/1.76e+09 =  0% of the original kernel matrix.

torch.Size([34389, 2])
We keep 1.68e+07/6.39e+08 =  2% of the original kernel matrix.

torch.Size([43298, 2])
We keep 8.15e+06/7.20e+08 =  1% of the original kernel matrix.

torch.Size([63927, 2])
We keep 3.62e+07/1.62e+09 =  2% of the original kernel matrix.

torch.Size([59074, 2])
We keep 1.18e+07/1.15e+09 =  1% of the original kernel matrix.

torch.Size([19963, 2])
We keep 2.62e+06/9.19e+07 =  2% of the original kernel matrix.

torch.Size([33095, 2])
We keep 3.84e+06/2.73e+08 =  1% of the original kernel matrix.

torch.Size([504662, 2])
We keep 9.76e+08/9.14e+10 =  1% of the original kernel matrix.

torch.Size([171572, 2])
We keep 6.70e+07/8.61e+09 =  0% of the original kernel matrix.

torch.Size([151517, 2])
We keep 1.00e+08/7.64e+09 =  1% of the original kernel matrix.

torch.Size([91418, 2])
We keep 2.26e+07/2.49e+09 =  0% of the original kernel matrix.

torch.Size([31499, 2])
We keep 7.56e+06/3.09e+08 =  2% of the original kernel matrix.

torch.Size([42003, 2])
We keep 5.92e+06/5.01e+08 =  1% of the original kernel matrix.

torch.Size([40052, 2])
We keep 8.80e+06/4.67e+08 =  1% of the original kernel matrix.

torch.Size([47821, 2])
We keep 7.04e+06/6.16e+08 =  1% of the original kernel matrix.

torch.Size([137179, 2])
We keep 5.20e+08/1.01e+10 =  5% of the original kernel matrix.

torch.Size([85919, 2])
We keep 2.59e+07/2.87e+09 =  0% of the original kernel matrix.

torch.Size([199310, 2])
We keep 1.37e+08/1.25e+10 =  1% of the original kernel matrix.

torch.Size([106255, 2])
We keep 2.78e+07/3.18e+09 =  0% of the original kernel matrix.

torch.Size([683369, 2])
We keep 3.32e+09/3.17e+11 =  1% of the original kernel matrix.

torch.Size([192926, 2])
We keep 1.17e+08/1.60e+10 =  0% of the original kernel matrix.

torch.Size([280896, 2])
We keep 2.26e+08/2.40e+10 =  0% of the original kernel matrix.

torch.Size([127788, 2])
We keep 3.66e+07/4.41e+09 =  0% of the original kernel matrix.

torch.Size([23011, 2])
We keep 6.06e+06/1.29e+08 =  4% of the original kernel matrix.

torch.Size([35934, 2])
We keep 4.22e+06/3.23e+08 =  1% of the original kernel matrix.

torch.Size([90139, 2])
We keep 2.55e+07/2.51e+09 =  1% of the original kernel matrix.

torch.Size([70948, 2])
We keep 1.38e+07/1.43e+09 =  0% of the original kernel matrix.

torch.Size([64901, 2])
We keep 6.79e+07/2.47e+09 =  2% of the original kernel matrix.

torch.Size([58002, 2])
We keep 1.41e+07/1.42e+09 =  0% of the original kernel matrix.

torch.Size([31066, 2])
We keep 9.18e+06/2.81e+08 =  3% of the original kernel matrix.

torch.Size([42097, 2])
We keep 5.76e+06/4.77e+08 =  1% of the original kernel matrix.

torch.Size([32148, 2])
We keep 8.16e+06/3.67e+08 =  2% of the original kernel matrix.

torch.Size([42409, 2])
We keep 6.41e+06/5.45e+08 =  1% of the original kernel matrix.

torch.Size([19810, 2])
We keep 1.79e+06/8.22e+07 =  2% of the original kernel matrix.

torch.Size([33253, 2])
We keep 3.65e+06/2.58e+08 =  1% of the original kernel matrix.

torch.Size([61728, 2])
We keep 3.63e+07/1.21e+09 =  2% of the original kernel matrix.

torch.Size([59013, 2])
We keep 1.07e+07/9.92e+08 =  1% of the original kernel matrix.

torch.Size([160808, 2])
We keep 1.42e+08/9.39e+09 =  1% of the original kernel matrix.

torch.Size([93303, 2])
We keep 2.47e+07/2.76e+09 =  0% of the original kernel matrix.

torch.Size([29834, 2])
We keep 1.38e+07/3.46e+08 =  3% of the original kernel matrix.

torch.Size([40037, 2])
We keep 6.36e+06/5.30e+08 =  1% of the original kernel matrix.

torch.Size([56605, 2])
We keep 1.28e+07/9.90e+08 =  1% of the original kernel matrix.

torch.Size([56963, 2])
We keep 9.61e+06/8.96e+08 =  1% of the original kernel matrix.

torch.Size([23660, 2])
We keep 3.71e+06/1.65e+08 =  2% of the original kernel matrix.

torch.Size([36292, 2])
We keep 4.57e+06/3.66e+08 =  1% of the original kernel matrix.

torch.Size([62715, 2])
We keep 4.31e+07/1.94e+09 =  2% of the original kernel matrix.

torch.Size([57829, 2])
We keep 1.23e+07/1.25e+09 =  0% of the original kernel matrix.

torch.Size([18214, 2])
We keep 2.55e+06/8.32e+07 =  3% of the original kernel matrix.

torch.Size([31753, 2])
We keep 3.68e+06/2.60e+08 =  1% of the original kernel matrix.

torch.Size([45067, 2])
We keep 2.06e+07/8.45e+08 =  2% of the original kernel matrix.

torch.Size([49811, 2])
We keep 9.24e+06/8.28e+08 =  1% of the original kernel matrix.

torch.Size([24413, 2])
We keep 5.46e+07/2.80e+08 = 19% of the original kernel matrix.

torch.Size([36347, 2])
We keep 5.82e+06/4.76e+08 =  1% of the original kernel matrix.

torch.Size([45512, 2])
We keep 1.70e+07/7.20e+08 =  2% of the original kernel matrix.

torch.Size([51217, 2])
We keep 8.57e+06/7.64e+08 =  1% of the original kernel matrix.

torch.Size([20401, 2])
We keep 2.25e+06/8.64e+07 =  2% of the original kernel matrix.

torch.Size([33654, 2])
We keep 3.73e+06/2.65e+08 =  1% of the original kernel matrix.

torch.Size([37373, 2])
We keep 5.29e+06/3.89e+08 =  1% of the original kernel matrix.

torch.Size([46588, 2])
We keep 6.64e+06/5.62e+08 =  1% of the original kernel matrix.

torch.Size([57020, 2])
We keep 1.21e+07/9.27e+08 =  1% of the original kernel matrix.

torch.Size([57669, 2])
We keep 9.35e+06/8.67e+08 =  1% of the original kernel matrix.

torch.Size([259285, 2])
We keep 2.95e+08/2.32e+10 =  1% of the original kernel matrix.

torch.Size([122095, 2])
We keep 3.64e+07/4.34e+09 =  0% of the original kernel matrix.

torch.Size([21381, 2])
We keep 2.45e+06/1.02e+08 =  2% of the original kernel matrix.

torch.Size([34643, 2])
We keep 3.97e+06/2.88e+08 =  1% of the original kernel matrix.

torch.Size([11967, 2])
We keep 1.16e+06/2.53e+07 =  4% of the original kernel matrix.

torch.Size([26151, 2])
We keep 2.38e+06/1.43e+08 =  1% of the original kernel matrix.

torch.Size([33024, 2])
We keep 5.76e+06/2.94e+08 =  1% of the original kernel matrix.

torch.Size([43507, 2])
We keep 5.98e+06/4.89e+08 =  1% of the original kernel matrix.

torch.Size([267566, 2])
We keep 4.40e+08/2.68e+10 =  1% of the original kernel matrix.

torch.Size([124513, 2])
We keep 3.80e+07/4.67e+09 =  0% of the original kernel matrix.

torch.Size([1900208, 2])
We keep 6.21e+09/8.41e+11 =  0% of the original kernel matrix.

torch.Size([346458, 2])
We keep 1.83e+08/2.61e+10 =  0% of the original kernel matrix.

torch.Size([186881, 2])
We keep 1.75e+08/1.42e+10 =  1% of the original kernel matrix.

torch.Size([100815, 2])
We keep 2.95e+07/3.39e+09 =  0% of the original kernel matrix.

torch.Size([20937, 2])
We keep 2.33e+06/1.02e+08 =  2% of the original kernel matrix.

torch.Size([34062, 2])
We keep 3.99e+06/2.88e+08 =  1% of the original kernel matrix.

torch.Size([464252, 2])
We keep 1.32e+09/1.06e+11 =  1% of the original kernel matrix.

torch.Size([161402, 2])
We keep 7.14e+07/9.26e+09 =  0% of the original kernel matrix.

torch.Size([441307, 2])
We keep 3.71e+08/6.06e+10 =  0% of the original kernel matrix.

torch.Size([163762, 2])
We keep 5.54e+07/7.01e+09 =  0% of the original kernel matrix.

torch.Size([10906, 2])
We keep 6.51e+05/1.95e+07 =  3% of the original kernel matrix.

torch.Size([24924, 2])
We keep 2.18e+06/1.26e+08 =  1% of the original kernel matrix.

torch.Size([25898, 2])
We keep 4.24e+06/1.81e+08 =  2% of the original kernel matrix.

torch.Size([38086, 2])
We keep 4.92e+06/3.83e+08 =  1% of the original kernel matrix.

torch.Size([145678, 2])
We keep 4.82e+07/5.65e+09 =  0% of the original kernel matrix.

torch.Size([88908, 2])
We keep 1.97e+07/2.14e+09 =  0% of the original kernel matrix.

torch.Size([245580, 2])
We keep 5.82e+08/3.36e+10 =  1% of the original kernel matrix.

torch.Size([117314, 2])
We keep 4.21e+07/5.22e+09 =  0% of the original kernel matrix.

torch.Size([946329, 2])
We keep 5.99e+09/5.09e+11 =  1% of the original kernel matrix.

torch.Size([232159, 2])
We keep 1.46e+08/2.03e+10 =  0% of the original kernel matrix.

torch.Size([483863, 2])
We keep 4.48e+08/7.08e+10 =  0% of the original kernel matrix.

torch.Size([169501, 2])
We keep 5.96e+07/7.58e+09 =  0% of the original kernel matrix.

torch.Size([115357, 2])
We keep 7.71e+07/3.59e+09 =  2% of the original kernel matrix.

torch.Size([79205, 2])
We keep 1.64e+07/1.71e+09 =  0% of the original kernel matrix.

torch.Size([20445, 2])
We keep 1.91e+06/8.84e+07 =  2% of the original kernel matrix.

torch.Size([33688, 2])
We keep 3.72e+06/2.68e+08 =  1% of the original kernel matrix.

torch.Size([12708, 2])
We keep 8.14e+05/2.71e+07 =  3% of the original kernel matrix.

torch.Size([26498, 2])
We keep 2.42e+06/1.48e+08 =  1% of the original kernel matrix.

torch.Size([200709, 2])
We keep 1.99e+08/1.44e+10 =  1% of the original kernel matrix.

torch.Size([105660, 2])
We keep 2.96e+07/3.42e+09 =  0% of the original kernel matrix.

torch.Size([41934, 2])
We keep 8.76e+06/5.22e+08 =  1% of the original kernel matrix.

torch.Size([49461, 2])
We keep 7.48e+06/6.51e+08 =  1% of the original kernel matrix.

torch.Size([55636, 2])
We keep 6.72e+07/1.57e+09 =  4% of the original kernel matrix.

torch.Size([54402, 2])
We keep 1.13e+07/1.13e+09 =  1% of the original kernel matrix.

torch.Size([356779, 2])
We keep 4.08e+08/4.61e+10 =  0% of the original kernel matrix.

torch.Size([145392, 2])
We keep 4.95e+07/6.12e+09 =  0% of the original kernel matrix.

torch.Size([1498681, 2])
We keep 2.80e+09/5.62e+11 =  0% of the original kernel matrix.

torch.Size([306340, 2])
We keep 1.52e+08/2.14e+10 =  0% of the original kernel matrix.

torch.Size([30438, 2])
We keep 5.29e+06/2.51e+08 =  2% of the original kernel matrix.

torch.Size([41500, 2])
We keep 5.62e+06/4.51e+08 =  1% of the original kernel matrix.

torch.Size([8517, 2])
We keep 3.82e+05/1.05e+07 =  3% of the original kernel matrix.

torch.Size([22428, 2])
We keep 1.73e+06/9.24e+07 =  1% of the original kernel matrix.

torch.Size([65708, 2])
We keep 4.19e+07/1.39e+09 =  3% of the original kernel matrix.

torch.Size([60850, 2])
We keep 1.08e+07/1.06e+09 =  1% of the original kernel matrix.

torch.Size([311827, 2])
We keep 1.27e+09/3.55e+10 =  3% of the original kernel matrix.

torch.Size([135400, 2])
We keep 4.25e+07/5.37e+09 =  0% of the original kernel matrix.

torch.Size([90155, 2])
We keep 5.11e+07/2.48e+09 =  2% of the original kernel matrix.

torch.Size([70416, 2])
We keep 1.41e+07/1.42e+09 =  0% of the original kernel matrix.

torch.Size([24332, 2])
We keep 2.68e+07/2.08e+08 = 12% of the original kernel matrix.

torch.Size([36650, 2])
We keep 5.24e+06/4.11e+08 =  1% of the original kernel matrix.

torch.Size([12910, 2])
We keep 1.16e+06/3.26e+07 =  3% of the original kernel matrix.

torch.Size([26760, 2])
We keep 2.61e+06/1.63e+08 =  1% of the original kernel matrix.

torch.Size([76944, 2])
We keep 2.14e+07/1.71e+09 =  1% of the original kernel matrix.

torch.Size([65596, 2])
We keep 1.20e+07/1.18e+09 =  1% of the original kernel matrix.

torch.Size([21122, 2])
We keep 4.66e+06/1.15e+08 =  4% of the original kernel matrix.

torch.Size([34226, 2])
We keep 4.19e+06/3.06e+08 =  1% of the original kernel matrix.

torch.Size([85344, 2])
We keep 1.14e+08/3.89e+09 =  2% of the original kernel matrix.

torch.Size([66840, 2])
We keep 1.71e+07/1.78e+09 =  0% of the original kernel matrix.

torch.Size([23278, 2])
We keep 3.28e+06/1.33e+08 =  2% of the original kernel matrix.

torch.Size([36154, 2])
We keep 4.39e+06/3.28e+08 =  1% of the original kernel matrix.

torch.Size([38001, 2])
We keep 4.97e+06/3.85e+08 =  1% of the original kernel matrix.

torch.Size([47282, 2])
We keep 6.59e+06/5.59e+08 =  1% of the original kernel matrix.

torch.Size([31446, 2])
We keep 5.10e+06/2.69e+08 =  1% of the original kernel matrix.

torch.Size([42568, 2])
We keep 5.79e+06/4.67e+08 =  1% of the original kernel matrix.

torch.Size([30059, 2])
We keep 3.30e+06/2.17e+08 =  1% of the original kernel matrix.

torch.Size([41336, 2])
We keep 5.26e+06/4.20e+08 =  1% of the original kernel matrix.

torch.Size([49627, 2])
We keep 1.78e+07/8.89e+08 =  2% of the original kernel matrix.

torch.Size([53128, 2])
We keep 9.39e+06/8.49e+08 =  1% of the original kernel matrix.

torch.Size([411606, 2])
We keep 3.64e+08/5.16e+10 =  0% of the original kernel matrix.

torch.Size([155845, 2])
We keep 5.13e+07/6.47e+09 =  0% of the original kernel matrix.

torch.Size([983682, 2])
We keep 1.74e+09/2.69e+11 =  0% of the original kernel matrix.

torch.Size([246408, 2])
We keep 1.08e+08/1.48e+10 =  0% of the original kernel matrix.

torch.Size([185504, 2])
We keep 2.38e+08/1.21e+10 =  1% of the original kernel matrix.

torch.Size([102401, 2])
We keep 2.73e+07/3.13e+09 =  0% of the original kernel matrix.

torch.Size([257131, 2])
We keep 5.65e+08/3.11e+10 =  1% of the original kernel matrix.

torch.Size([120125, 2])
We keep 4.19e+07/5.03e+09 =  0% of the original kernel matrix.

torch.Size([52713, 2])
We keep 9.38e+06/7.75e+08 =  1% of the original kernel matrix.

torch.Size([53543, 2])
We keep 8.37e+06/7.93e+08 =  1% of the original kernel matrix.

torch.Size([32691, 2])
We keep 1.36e+07/3.83e+08 =  3% of the original kernel matrix.

torch.Size([42886, 2])
We keep 6.69e+06/5.57e+08 =  1% of the original kernel matrix.

torch.Size([54851, 2])
We keep 1.36e+07/9.44e+08 =  1% of the original kernel matrix.

torch.Size([56221, 2])
We keep 9.61e+06/8.75e+08 =  1% of the original kernel matrix.

torch.Size([392500, 2])
We keep 3.23e+08/4.89e+10 =  0% of the original kernel matrix.

torch.Size([157143, 2])
We keep 5.14e+07/6.30e+09 =  0% of the original kernel matrix.

torch.Size([52970, 2])
We keep 2.36e+07/1.05e+09 =  2% of the original kernel matrix.

torch.Size([54721, 2])
We keep 9.94e+06/9.24e+08 =  1% of the original kernel matrix.

torch.Size([131168, 2])
We keep 2.72e+08/5.86e+09 =  4% of the original kernel matrix.

torch.Size([85116, 2])
We keep 1.98e+07/2.18e+09 =  0% of the original kernel matrix.

torch.Size([37660, 2])
We keep 6.87e+06/4.29e+08 =  1% of the original kernel matrix.

torch.Size([46815, 2])
We keep 6.93e+06/5.90e+08 =  1% of the original kernel matrix.

torch.Size([3593013, 2])
We keep 1.26e+10/2.74e+12 =  0% of the original kernel matrix.

torch.Size([487979, 2])
We keep 3.18e+08/4.72e+10 =  0% of the original kernel matrix.

torch.Size([43714, 2])
We keep 8.09e+06/5.32e+08 =  1% of the original kernel matrix.

torch.Size([50335, 2])
We keep 7.57e+06/6.57e+08 =  1% of the original kernel matrix.

torch.Size([180876, 2])
We keep 8.99e+07/1.04e+10 =  0% of the original kernel matrix.

torch.Size([100473, 2])
We keep 2.57e+07/2.90e+09 =  0% of the original kernel matrix.

torch.Size([549147, 2])
We keep 9.04e+08/1.07e+11 =  0% of the original kernel matrix.

torch.Size([179295, 2])
We keep 7.16e+07/9.31e+09 =  0% of the original kernel matrix.

torch.Size([121895, 2])
We keep 4.49e+07/3.98e+09 =  1% of the original kernel matrix.

torch.Size([81611, 2])
We keep 1.70e+07/1.80e+09 =  0% of the original kernel matrix.

torch.Size([68187, 2])
We keep 2.51e+07/1.51e+09 =  1% of the original kernel matrix.

torch.Size([61946, 2])
We keep 1.13e+07/1.11e+09 =  1% of the original kernel matrix.

torch.Size([2936274, 2])
We keep 1.42e+10/2.31e+12 =  0% of the original kernel matrix.

torch.Size([434189, 2])
We keep 2.94e+08/4.33e+10 =  0% of the original kernel matrix.

torch.Size([548448, 2])
We keep 5.18e+08/8.96e+10 =  0% of the original kernel matrix.

torch.Size([180188, 2])
We keep 6.58e+07/8.53e+09 =  0% of the original kernel matrix.

torch.Size([215727, 2])
We keep 1.94e+08/1.50e+10 =  1% of the original kernel matrix.

torch.Size([110758, 2])
We keep 3.02e+07/3.49e+09 =  0% of the original kernel matrix.

torch.Size([22978, 2])
We keep 3.81e+06/1.17e+08 =  3% of the original kernel matrix.

torch.Size([35903, 2])
We keep 4.12e+06/3.08e+08 =  1% of the original kernel matrix.

torch.Size([94689, 2])
We keep 5.14e+07/2.88e+09 =  1% of the original kernel matrix.

torch.Size([72279, 2])
We keep 1.50e+07/1.53e+09 =  0% of the original kernel matrix.

torch.Size([37795, 2])
We keep 1.08e+07/4.70e+08 =  2% of the original kernel matrix.

torch.Size([46481, 2])
We keep 7.14e+06/6.18e+08 =  1% of the original kernel matrix.

torch.Size([531210, 2])
We keep 1.43e+09/8.27e+10 =  1% of the original kernel matrix.

torch.Size([176881, 2])
We keep 6.43e+07/8.19e+09 =  0% of the original kernel matrix.

torch.Size([2805188, 2])
We keep 9.30e+09/1.80e+12 =  0% of the original kernel matrix.

torch.Size([431300, 2])
We keep 2.60e+08/3.82e+10 =  0% of the original kernel matrix.

torch.Size([231240, 2])
We keep 5.52e+08/2.12e+10 =  2% of the original kernel matrix.

torch.Size([115433, 2])
We keep 3.52e+07/4.15e+09 =  0% of the original kernel matrix.

torch.Size([419919, 2])
We keep 4.96e+08/6.32e+10 =  0% of the original kernel matrix.

torch.Size([157404, 2])
We keep 5.68e+07/7.16e+09 =  0% of the original kernel matrix.

torch.Size([153263, 2])
We keep 3.07e+08/8.53e+09 =  3% of the original kernel matrix.

torch.Size([91891, 2])
We keep 2.39e+07/2.63e+09 =  0% of the original kernel matrix.

torch.Size([77164, 2])
We keep 4.97e+07/2.03e+09 =  2% of the original kernel matrix.

torch.Size([64971, 2])
We keep 1.30e+07/1.28e+09 =  1% of the original kernel matrix.

torch.Size([78572, 2])
We keep 2.85e+07/1.98e+09 =  1% of the original kernel matrix.

torch.Size([66183, 2])
We keep 1.29e+07/1.27e+09 =  1% of the original kernel matrix.

torch.Size([33956, 2])
We keep 7.83e+06/3.52e+08 =  2% of the original kernel matrix.

torch.Size([43614, 2])
We keep 6.36e+06/5.34e+08 =  1% of the original kernel matrix.

torch.Size([542071, 2])
We keep 3.61e+09/2.03e+11 =  1% of the original kernel matrix.

torch.Size([171186, 2])
We keep 9.68e+07/1.28e+10 =  0% of the original kernel matrix.

torch.Size([1178901, 2])
We keep 1.69e+09/3.70e+11 =  0% of the original kernel matrix.

torch.Size([273538, 2])
We keep 1.25e+08/1.73e+10 =  0% of the original kernel matrix.

torch.Size([112460, 2])
We keep 1.87e+08/1.11e+10 =  1% of the original kernel matrix.

torch.Size([73081, 2])
We keep 2.66e+07/3.00e+09 =  0% of the original kernel matrix.

torch.Size([3077918, 2])
We keep 2.79e+10/3.23e+12 =  0% of the original kernel matrix.

torch.Size([430685, 2])
We keep 3.46e+08/5.12e+10 =  0% of the original kernel matrix.

torch.Size([319375, 2])
We keep 8.53e+08/4.89e+10 =  1% of the original kernel matrix.

torch.Size([135996, 2])
We keep 5.10e+07/6.30e+09 =  0% of the original kernel matrix.

torch.Size([139133, 2])
We keep 5.62e+07/5.76e+09 =  0% of the original kernel matrix.

torch.Size([87626, 2])
We keep 1.99e+07/2.16e+09 =  0% of the original kernel matrix.

torch.Size([64909, 2])
We keep 2.53e+07/1.50e+09 =  1% of the original kernel matrix.

torch.Size([59621, 2])
We keep 1.15e+07/1.10e+09 =  1% of the original kernel matrix.

torch.Size([177186, 2])
We keep 1.44e+08/1.04e+10 =  1% of the original kernel matrix.

torch.Size([100183, 2])
We keep 2.56e+07/2.90e+09 =  0% of the original kernel matrix.

torch.Size([384022, 2])
We keep 2.89e+08/4.53e+10 =  0% of the original kernel matrix.

torch.Size([151788, 2])
We keep 4.84e+07/6.06e+09 =  0% of the original kernel matrix.

torch.Size([29842, 2])
We keep 9.33e+06/3.49e+08 =  2% of the original kernel matrix.

torch.Size([40838, 2])
We keep 6.39e+06/5.32e+08 =  1% of the original kernel matrix.

torch.Size([1881626, 2])
We keep 3.73e+09/8.31e+11 =  0% of the original kernel matrix.

torch.Size([341647, 2])
We keep 1.81e+08/2.60e+10 =  0% of the original kernel matrix.

torch.Size([50848, 2])
We keep 1.30e+07/8.34e+08 =  1% of the original kernel matrix.

torch.Size([53833, 2])
We keep 8.33e+06/8.23e+08 =  1% of the original kernel matrix.

torch.Size([86719, 2])
We keep 5.31e+07/3.25e+09 =  1% of the original kernel matrix.

torch.Size([68240, 2])
We keep 1.58e+07/1.63e+09 =  0% of the original kernel matrix.

torch.Size([20767, 2])
We keep 2.33e+06/9.72e+07 =  2% of the original kernel matrix.

torch.Size([34084, 2])
We keep 3.82e+06/2.81e+08 =  1% of the original kernel matrix.

torch.Size([61918, 2])
We keep 1.49e+07/1.21e+09 =  1% of the original kernel matrix.

torch.Size([59460, 2])
We keep 1.05e+07/9.90e+08 =  1% of the original kernel matrix.

torch.Size([9473, 2])
We keep 4.86e+05/1.38e+07 =  3% of the original kernel matrix.

torch.Size([23330, 2])
We keep 1.91e+06/1.06e+08 =  1% of the original kernel matrix.

torch.Size([27115, 2])
We keep 5.66e+06/2.29e+08 =  2% of the original kernel matrix.

torch.Size([38954, 2])
We keep 5.40e+06/4.31e+08 =  1% of the original kernel matrix.

torch.Size([68772, 2])
We keep 2.52e+07/1.53e+09 =  1% of the original kernel matrix.

torch.Size([62208, 2])
We keep 1.16e+07/1.11e+09 =  1% of the original kernel matrix.

torch.Size([98296, 2])
We keep 6.95e+07/3.24e+09 =  2% of the original kernel matrix.

torch.Size([73008, 2])
We keep 1.58e+07/1.62e+09 =  0% of the original kernel matrix.

torch.Size([5199950, 2])
We keep 8.90e+10/1.28e+13 =  0% of the original kernel matrix.

torch.Size([512418, 2])
We keep 6.54e+08/1.02e+11 =  0% of the original kernel matrix.

torch.Size([45279, 2])
We keep 2.08e+07/8.71e+08 =  2% of the original kernel matrix.

torch.Size([49884, 2])
We keep 9.29e+06/8.41e+08 =  1% of the original kernel matrix.

torch.Size([983772, 2])
We keep 1.31e+09/2.60e+11 =  0% of the original kernel matrix.

torch.Size([246168, 2])
We keep 1.06e+08/1.45e+10 =  0% of the original kernel matrix.

torch.Size([22366, 2])
We keep 2.34e+06/1.12e+08 =  2% of the original kernel matrix.

torch.Size([35407, 2])
We keep 4.14e+06/3.02e+08 =  1% of the original kernel matrix.

torch.Size([51514, 2])
We keep 1.07e+07/8.18e+08 =  1% of the original kernel matrix.

torch.Size([53935, 2])
We keep 8.87e+06/8.15e+08 =  1% of the original kernel matrix.

torch.Size([28796, 2])
We keep 4.63e+06/2.27e+08 =  2% of the original kernel matrix.

torch.Size([40094, 2])
We keep 5.36e+06/4.30e+08 =  1% of the original kernel matrix.

torch.Size([322475, 2])
We keep 1.11e+09/7.46e+10 =  1% of the original kernel matrix.

torch.Size([131290, 2])
We keep 6.12e+07/7.78e+09 =  0% of the original kernel matrix.

torch.Size([138042, 2])
We keep 1.66e+08/8.43e+09 =  1% of the original kernel matrix.

torch.Size([86163, 2])
We keep 2.35e+07/2.61e+09 =  0% of the original kernel matrix.

torch.Size([2652993, 2])
We keep 7.26e+09/1.63e+12 =  0% of the original kernel matrix.

torch.Size([415668, 2])
We keep 2.47e+08/3.64e+10 =  0% of the original kernel matrix.

torch.Size([22134, 2])
We keep 2.63e+06/1.22e+08 =  2% of the original kernel matrix.

torch.Size([35096, 2])
We keep 4.28e+06/3.14e+08 =  1% of the original kernel matrix.

torch.Size([465334, 2])
We keep 4.67e+08/6.57e+10 =  0% of the original kernel matrix.

torch.Size([167359, 2])
We keep 5.75e+07/7.30e+09 =  0% of the original kernel matrix.

torch.Size([63688, 2])
We keep 5.46e+07/1.57e+09 =  3% of the original kernel matrix.

torch.Size([59359, 2])
We keep 1.16e+07/1.13e+09 =  1% of the original kernel matrix.

torch.Size([292615, 2])
We keep 2.82e+08/2.96e+10 =  0% of the original kernel matrix.

torch.Size([130136, 2])
We keep 4.05e+07/4.90e+09 =  0% of the original kernel matrix.

torch.Size([340021, 2])
We keep 4.28e+08/4.52e+10 =  0% of the original kernel matrix.

torch.Size([140488, 2])
We keep 4.85e+07/6.06e+09 =  0% of the original kernel matrix.

torch.Size([66796, 2])
We keep 1.92e+07/1.32e+09 =  1% of the original kernel matrix.

torch.Size([61112, 2])
We keep 1.07e+07/1.03e+09 =  1% of the original kernel matrix.

torch.Size([36730, 2])
We keep 7.66e+06/4.27e+08 =  1% of the original kernel matrix.

torch.Size([46163, 2])
We keep 6.96e+06/5.89e+08 =  1% of the original kernel matrix.

torch.Size([201465, 2])
We keep 1.02e+08/1.23e+10 =  0% of the original kernel matrix.

torch.Size([107063, 2])
We keep 2.77e+07/3.16e+09 =  0% of the original kernel matrix.

torch.Size([66910, 2])
We keep 1.22e+08/1.91e+09 =  6% of the original kernel matrix.

torch.Size([60388, 2])
We keep 1.23e+07/1.24e+09 =  0% of the original kernel matrix.

torch.Size([133068, 2])
We keep 1.20e+08/6.40e+09 =  1% of the original kernel matrix.

torch.Size([84696, 2])
We keep 2.07e+07/2.28e+09 =  0% of the original kernel matrix.

torch.Size([1223862, 2])
We keep 3.59e+09/4.54e+11 =  0% of the original kernel matrix.

torch.Size([273562, 2])
We keep 1.38e+08/1.92e+10 =  0% of the original kernel matrix.

torch.Size([27525, 2])
We keep 4.82e+06/2.34e+08 =  2% of the original kernel matrix.

torch.Size([39430, 2])
We keep 5.30e+06/4.36e+08 =  1% of the original kernel matrix.

torch.Size([161748, 2])
We keep 1.77e+08/9.15e+09 =  1% of the original kernel matrix.

torch.Size([94956, 2])
We keep 2.46e+07/2.72e+09 =  0% of the original kernel matrix.

torch.Size([64353, 2])
We keep 2.31e+07/1.42e+09 =  1% of the original kernel matrix.

torch.Size([59455, 2])
We keep 1.12e+07/1.07e+09 =  1% of the original kernel matrix.

torch.Size([316310, 2])
We keep 2.62e+08/3.26e+10 =  0% of the original kernel matrix.

torch.Size([136262, 2])
We keep 4.18e+07/5.15e+09 =  0% of the original kernel matrix.

torch.Size([860651, 2])
We keep 1.67e+09/2.20e+11 =  0% of the original kernel matrix.

torch.Size([227099, 2])
We keep 9.84e+07/1.34e+10 =  0% of the original kernel matrix.

torch.Size([178708, 2])
We keep 1.17e+08/9.66e+09 =  1% of the original kernel matrix.

torch.Size([99930, 2])
We keep 2.50e+07/2.80e+09 =  0% of the original kernel matrix.

torch.Size([31804, 2])
We keep 6.88e+06/2.73e+08 =  2% of the original kernel matrix.

torch.Size([42874, 2])
We keep 5.82e+06/4.71e+08 =  1% of the original kernel matrix.

torch.Size([439678, 2])
We keep 6.54e+08/7.21e+10 =  0% of the original kernel matrix.

torch.Size([159163, 2])
We keep 5.98e+07/7.65e+09 =  0% of the original kernel matrix.

torch.Size([260213, 2])
We keep 2.34e+08/2.40e+10 =  0% of the original kernel matrix.

torch.Size([121639, 2])
We keep 3.71e+07/4.41e+09 =  0% of the original kernel matrix.

torch.Size([939207, 2])
We keep 8.45e+09/6.19e+11 =  1% of the original kernel matrix.

torch.Size([221685, 2])
We keep 1.60e+08/2.24e+10 =  0% of the original kernel matrix.

torch.Size([316977, 2])
We keep 4.10e+08/3.63e+10 =  1% of the original kernel matrix.

torch.Size([135458, 2])
We keep 4.43e+07/5.43e+09 =  0% of the original kernel matrix.

torch.Size([98949, 2])
We keep 6.38e+07/3.63e+09 =  1% of the original kernel matrix.

torch.Size([72883, 2])
We keep 1.63e+07/1.72e+09 =  0% of the original kernel matrix.

torch.Size([108716, 2])
We keep 1.20e+08/4.79e+09 =  2% of the original kernel matrix.

torch.Size([76947, 2])
We keep 1.86e+07/1.97e+09 =  0% of the original kernel matrix.

torch.Size([250360, 2])
We keep 5.19e+08/3.54e+10 =  1% of the original kernel matrix.

torch.Size([115505, 2])
We keep 4.42e+07/5.36e+09 =  0% of the original kernel matrix.

torch.Size([346267, 2])
We keep 6.52e+08/5.96e+10 =  1% of the original kernel matrix.

torch.Size([139028, 2])
We keep 5.54e+07/6.96e+09 =  0% of the original kernel matrix.

torch.Size([128023, 2])
We keep 5.86e+07/4.75e+09 =  1% of the original kernel matrix.

torch.Size([82951, 2])
We keep 1.84e+07/1.96e+09 =  0% of the original kernel matrix.

torch.Size([19992, 2])
We keep 1.89e+06/8.70e+07 =  2% of the original kernel matrix.

torch.Size([33103, 2])
We keep 3.74e+06/2.66e+08 =  1% of the original kernel matrix.

torch.Size([27126, 2])
We keep 3.72e+06/1.88e+08 =  1% of the original kernel matrix.

torch.Size([39227, 2])
We keep 5.04e+06/3.91e+08 =  1% of the original kernel matrix.

torch.Size([151824, 2])
We keep 2.23e+08/1.09e+10 =  2% of the original kernel matrix.

torch.Size([91173, 2])
We keep 2.65e+07/2.98e+09 =  0% of the original kernel matrix.

torch.Size([272238, 2])
We keep 6.29e+08/3.25e+10 =  1% of the original kernel matrix.

torch.Size([125106, 2])
We keep 4.26e+07/5.13e+09 =  0% of the original kernel matrix.

torch.Size([200580, 2])
We keep 1.11e+08/1.18e+10 =  0% of the original kernel matrix.

torch.Size([106491, 2])
We keep 2.71e+07/3.10e+09 =  0% of the original kernel matrix.

torch.Size([27234, 2])
We keep 3.63e+06/1.84e+08 =  1% of the original kernel matrix.

torch.Size([39225, 2])
We keep 4.99e+06/3.87e+08 =  1% of the original kernel matrix.

torch.Size([66062, 2])
We keep 1.41e+07/1.32e+09 =  1% of the original kernel matrix.

torch.Size([60855, 2])
We keep 1.07e+07/1.04e+09 =  1% of the original kernel matrix.

torch.Size([126230, 2])
We keep 1.84e+08/5.52e+09 =  3% of the original kernel matrix.

torch.Size([82849, 2])
We keep 1.93e+07/2.12e+09 =  0% of the original kernel matrix.

torch.Size([115353, 2])
We keep 3.47e+07/3.65e+09 =  0% of the original kernel matrix.

torch.Size([79471, 2])
We keep 1.63e+07/1.72e+09 =  0% of the original kernel matrix.

torch.Size([62524, 2])
We keep 2.36e+07/1.18e+09 =  1% of the original kernel matrix.

torch.Size([59720, 2])
We keep 1.04e+07/9.79e+08 =  1% of the original kernel matrix.

torch.Size([161434, 2])
We keep 6.97e+07/7.68e+09 =  0% of the original kernel matrix.

torch.Size([94277, 2])
We keep 2.25e+07/2.50e+09 =  0% of the original kernel matrix.

torch.Size([39885, 2])
We keep 1.61e+07/6.62e+08 =  2% of the original kernel matrix.

torch.Size([46904, 2])
We keep 8.21e+06/7.33e+08 =  1% of the original kernel matrix.

torch.Size([83230, 2])
We keep 2.54e+07/1.96e+09 =  1% of the original kernel matrix.

torch.Size([68237, 2])
We keep 1.26e+07/1.26e+09 =  0% of the original kernel matrix.

torch.Size([19057, 2])
We keep 2.32e+06/8.89e+07 =  2% of the original kernel matrix.

torch.Size([32352, 2])
We keep 3.81e+06/2.69e+08 =  1% of the original kernel matrix.

torch.Size([24540, 2])
We keep 2.62e+06/1.43e+08 =  1% of the original kernel matrix.

torch.Size([37203, 2])
We keep 4.53e+06/3.41e+08 =  1% of the original kernel matrix.

torch.Size([558734, 2])
We keep 6.11e+08/8.74e+10 =  0% of the original kernel matrix.

torch.Size([181796, 2])
We keep 6.48e+07/8.42e+09 =  0% of the original kernel matrix.

torch.Size([48988, 2])
We keep 1.38e+07/7.78e+08 =  1% of the original kernel matrix.

torch.Size([51754, 2])
We keep 8.59e+06/7.95e+08 =  1% of the original kernel matrix.

torch.Size([222036, 2])
We keep 1.39e+08/1.43e+10 =  0% of the original kernel matrix.

torch.Size([112607, 2])
We keep 2.92e+07/3.40e+09 =  0% of the original kernel matrix.

torch.Size([20948, 2])
We keep 2.99e+06/9.98e+07 =  2% of the original kernel matrix.

torch.Size([34090, 2])
We keep 3.96e+06/2.85e+08 =  1% of the original kernel matrix.

torch.Size([24129, 2])
We keep 6.52e+06/1.56e+08 =  4% of the original kernel matrix.

torch.Size([36841, 2])
We keep 4.69e+06/3.55e+08 =  1% of the original kernel matrix.

torch.Size([127604, 2])
We keep 1.51e+08/5.55e+09 =  2% of the original kernel matrix.

torch.Size([83379, 2])
We keep 1.92e+07/2.12e+09 =  0% of the original kernel matrix.

torch.Size([285018, 2])
We keep 2.37e+08/2.56e+10 =  0% of the original kernel matrix.

torch.Size([128380, 2])
We keep 3.80e+07/4.56e+09 =  0% of the original kernel matrix.

torch.Size([90427, 2])
We keep 2.90e+07/2.30e+09 =  1% of the original kernel matrix.

torch.Size([70786, 2])
We keep 1.36e+07/1.37e+09 =  0% of the original kernel matrix.

torch.Size([20464, 2])
We keep 2.24e+06/8.52e+07 =  2% of the original kernel matrix.

torch.Size([33624, 2])
We keep 3.72e+06/2.63e+08 =  1% of the original kernel matrix.

torch.Size([33274, 2])
We keep 5.09e+06/2.99e+08 =  1% of the original kernel matrix.

torch.Size([43658, 2])
We keep 5.97e+06/4.93e+08 =  1% of the original kernel matrix.

torch.Size([97958, 2])
We keep 5.50e+07/2.93e+09 =  1% of the original kernel matrix.

torch.Size([73389, 2])
We keep 1.50e+07/1.54e+09 =  0% of the original kernel matrix.

torch.Size([80306, 2])
We keep 3.74e+07/2.04e+09 =  1% of the original kernel matrix.

torch.Size([66927, 2])
We keep 1.31e+07/1.29e+09 =  1% of the original kernel matrix.

torch.Size([228378, 2])
We keep 2.68e+08/1.75e+10 =  1% of the original kernel matrix.

torch.Size([114015, 2])
We keep 3.26e+07/3.77e+09 =  0% of the original kernel matrix.

torch.Size([25155, 2])
We keep 3.24e+06/1.59e+08 =  2% of the original kernel matrix.

torch.Size([37674, 2])
We keep 4.71e+06/3.60e+08 =  1% of the original kernel matrix.

torch.Size([30769, 2])
We keep 6.76e+06/2.96e+08 =  2% of the original kernel matrix.

torch.Size([41219, 2])
We keep 5.92e+06/4.90e+08 =  1% of the original kernel matrix.

torch.Size([154713, 2])
We keep 1.76e+08/1.27e+10 =  1% of the original kernel matrix.

torch.Size([90794, 2])
We keep 2.82e+07/3.22e+09 =  0% of the original kernel matrix.

torch.Size([18237, 2])
We keep 2.74e+06/7.40e+07 =  3% of the original kernel matrix.

torch.Size([31652, 2])
We keep 3.50e+06/2.45e+08 =  1% of the original kernel matrix.

torch.Size([355776, 2])
We keep 6.57e+08/5.29e+10 =  1% of the original kernel matrix.

torch.Size([144543, 2])
We keep 5.15e+07/6.55e+09 =  0% of the original kernel matrix.

torch.Size([25760, 2])
We keep 1.02e+07/1.60e+08 =  6% of the original kernel matrix.

torch.Size([38180, 2])
We keep 4.70e+06/3.60e+08 =  1% of the original kernel matrix.

torch.Size([218339, 2])
We keep 2.08e+08/1.81e+10 =  1% of the original kernel matrix.

torch.Size([111124, 2])
We keep 3.26e+07/3.83e+09 =  0% of the original kernel matrix.

torch.Size([347645, 2])
We keep 3.05e+08/3.71e+10 =  0% of the original kernel matrix.

torch.Size([144654, 2])
We keep 4.47e+07/5.49e+09 =  0% of the original kernel matrix.

torch.Size([86113, 2])
We keep 5.46e+07/2.18e+09 =  2% of the original kernel matrix.

torch.Size([69335, 2])
We keep 1.30e+07/1.33e+09 =  0% of the original kernel matrix.

torch.Size([284123, 2])
We keep 3.49e+08/2.85e+10 =  1% of the original kernel matrix.

torch.Size([129057, 2])
We keep 4.01e+07/4.81e+09 =  0% of the original kernel matrix.

torch.Size([285257, 2])
We keep 2.06e+08/2.48e+10 =  0% of the original kernel matrix.

torch.Size([129188, 2])
We keep 3.74e+07/4.49e+09 =  0% of the original kernel matrix.

torch.Size([6806, 2])
We keep 2.32e+05/6.24e+06 =  3% of the original kernel matrix.

torch.Size([20522, 2])
We keep 1.44e+06/7.12e+07 =  2% of the original kernel matrix.

torch.Size([174213, 2])
We keep 9.44e+07/9.25e+09 =  1% of the original kernel matrix.

torch.Size([98543, 2])
We keep 2.46e+07/2.74e+09 =  0% of the original kernel matrix.

torch.Size([59969, 2])
We keep 1.28e+07/9.89e+08 =  1% of the original kernel matrix.

torch.Size([59005, 2])
We keep 9.62e+06/8.96e+08 =  1% of the original kernel matrix.

torch.Size([464939, 2])
We keep 7.92e+08/6.74e+10 =  1% of the original kernel matrix.

torch.Size([166143, 2])
We keep 5.81e+07/7.39e+09 =  0% of the original kernel matrix.

torch.Size([198536, 2])
We keep 1.50e+08/1.43e+10 =  1% of the original kernel matrix.

torch.Size([105724, 2])
We keep 2.93e+07/3.40e+09 =  0% of the original kernel matrix.

torch.Size([116443, 2])
We keep 1.17e+08/5.97e+09 =  1% of the original kernel matrix.

torch.Size([78399, 2])
We keep 2.04e+07/2.20e+09 =  0% of the original kernel matrix.

torch.Size([222729, 2])
We keep 1.25e+08/1.51e+10 =  0% of the original kernel matrix.

torch.Size([112876, 2])
We keep 3.00e+07/3.50e+09 =  0% of the original kernel matrix.

torch.Size([85446, 2])
We keep 4.09e+07/2.33e+09 =  1% of the original kernel matrix.

torch.Size([68786, 2])
We keep 1.38e+07/1.38e+09 =  1% of the original kernel matrix.

torch.Size([49313, 2])
We keep 5.63e+07/1.91e+09 =  2% of the original kernel matrix.

torch.Size([50111, 2])
We keep 1.26e+07/1.25e+09 =  1% of the original kernel matrix.

torch.Size([18092, 2])
We keep 1.49e+06/6.14e+07 =  2% of the original kernel matrix.

torch.Size([31696, 2])
We keep 3.26e+06/2.23e+08 =  1% of the original kernel matrix.

torch.Size([28542, 2])
We keep 6.62e+06/2.52e+08 =  2% of the original kernel matrix.

torch.Size([39608, 2])
We keep 5.61e+06/4.52e+08 =  1% of the original kernel matrix.

torch.Size([582974, 2])
We keep 2.01e+09/1.03e+11 =  1% of the original kernel matrix.

torch.Size([185581, 2])
We keep 6.98e+07/9.16e+09 =  0% of the original kernel matrix.

torch.Size([1604684, 2])
We keep 4.50e+09/6.49e+11 =  0% of the original kernel matrix.

torch.Size([317423, 2])
We keep 1.62e+08/2.29e+10 =  0% of the original kernel matrix.

torch.Size([68962, 2])
We keep 2.58e+07/1.79e+09 =  1% of the original kernel matrix.

torch.Size([61285, 2])
We keep 1.24e+07/1.21e+09 =  1% of the original kernel matrix.

torch.Size([40979, 2])
We keep 6.85e+06/4.76e+08 =  1% of the original kernel matrix.

torch.Size([48937, 2])
We keep 7.01e+06/6.22e+08 =  1% of the original kernel matrix.

torch.Size([95288, 2])
We keep 3.10e+07/2.69e+09 =  1% of the original kernel matrix.

torch.Size([72428, 2])
We keep 1.47e+07/1.48e+09 =  0% of the original kernel matrix.

torch.Size([400092, 2])
We keep 7.82e+08/5.62e+10 =  1% of the original kernel matrix.

torch.Size([154132, 2])
We keep 5.21e+07/6.75e+09 =  0% of the original kernel matrix.

torch.Size([176329, 2])
We keep 8.29e+07/8.68e+09 =  0% of the original kernel matrix.

torch.Size([99060, 2])
We keep 2.38e+07/2.65e+09 =  0% of the original kernel matrix.

torch.Size([31081, 2])
We keep 4.26e+06/2.54e+08 =  1% of the original kernel matrix.

torch.Size([42017, 2])
We keep 5.57e+06/4.54e+08 =  1% of the original kernel matrix.

torch.Size([141905, 2])
We keep 6.56e+07/5.89e+09 =  1% of the original kernel matrix.

torch.Size([88325, 2])
We keep 2.02e+07/2.19e+09 =  0% of the original kernel matrix.

torch.Size([345908, 2])
We keep 3.29e+08/3.76e+10 =  0% of the original kernel matrix.

torch.Size([143893, 2])
We keep 4.50e+07/5.52e+09 =  0% of the original kernel matrix.

torch.Size([22847, 2])
We keep 4.29e+06/1.38e+08 =  3% of the original kernel matrix.

torch.Size([35762, 2])
We keep 4.36e+06/3.34e+08 =  1% of the original kernel matrix.

torch.Size([90119, 2])
We keep 7.19e+07/2.56e+09 =  2% of the original kernel matrix.

torch.Size([70611, 2])
We keep 1.41e+07/1.44e+09 =  0% of the original kernel matrix.

torch.Size([104674, 2])
We keep 3.17e+08/5.29e+09 =  5% of the original kernel matrix.

torch.Size([76015, 2])
We keep 1.87e+07/2.07e+09 =  0% of the original kernel matrix.

time for making ranges is 11.18163776397705
Sorting X and nu_X
time for sorting X is 0.11082673072814941
Sorting Z and nu_Z
time for sorting Z is 0.00026798248291015625
Starting Optim
sum tnu_Z before tensor(65360708., device='cuda:0')
c= tensor(6979.7393, device='cuda:0')
c= tensor(364769.5312, device='cuda:0')
c= tensor(371762.0625, device='cuda:0')
c= tensor(633204.3750, device='cuda:0')
c= tensor(849530.6250, device='cuda:0')
c= tensor(3200810., device='cuda:0')
c= tensor(4452292., device='cuda:0')
c= tensor(5270818., device='cuda:0')
c= tensor(6951913.5000, device='cuda:0')
c= tensor(25692742., device='cuda:0')
c= tensor(25838604., device='cuda:0')
c= tensor(30943392., device='cuda:0')
c= tensor(30970752., device='cuda:0')
c= tensor(95888168., device='cuda:0')
c= tensor(96402008., device='cuda:0')
c= tensor(97896024., device='cuda:0')
c= tensor(99831360., device='cuda:0')
c= tensor(1.0051e+08, device='cuda:0')
c= tensor(1.1234e+08, device='cuda:0')
c= tensor(1.1758e+08, device='cuda:0')
c= tensor(1.2374e+08, device='cuda:0')
c= tensor(1.4521e+08, device='cuda:0')
c= tensor(1.4531e+08, device='cuda:0')
c= tensor(1.4747e+08, device='cuda:0')
c= tensor(1.4771e+08, device='cuda:0')
c= tensor(1.4925e+08, device='cuda:0')
c= tensor(1.5245e+08, device='cuda:0')
c= tensor(1.5259e+08, device='cuda:0')
c= tensor(1.7086e+08, device='cuda:0')
c= tensor(6.9521e+08, device='cuda:0')
c= tensor(6.9568e+08, device='cuda:0')
c= tensor(1.0596e+09, device='cuda:0')
c= tensor(1.0602e+09, device='cuda:0')
c= tensor(1.0605e+09, device='cuda:0')
c= tensor(1.0616e+09, device='cuda:0')
c= tensor(1.1280e+09, device='cuda:0')
c= tensor(1.1327e+09, device='cuda:0')
c= tensor(1.1327e+09, device='cuda:0')
c= tensor(1.1327e+09, device='cuda:0')
c= tensor(1.1327e+09, device='cuda:0')
c= tensor(1.1327e+09, device='cuda:0')
c= tensor(1.1327e+09, device='cuda:0')
c= tensor(1.1327e+09, device='cuda:0')
c= tensor(1.1327e+09, device='cuda:0')
c= tensor(1.1327e+09, device='cuda:0')
c= tensor(1.1327e+09, device='cuda:0')
c= tensor(1.1327e+09, device='cuda:0')
c= tensor(1.1328e+09, device='cuda:0')
c= tensor(1.1328e+09, device='cuda:0')
c= tensor(1.1329e+09, device='cuda:0')
c= tensor(1.1330e+09, device='cuda:0')
c= tensor(1.1330e+09, device='cuda:0')
c= tensor(1.1330e+09, device='cuda:0')
c= tensor(1.1331e+09, device='cuda:0')
c= tensor(1.1331e+09, device='cuda:0')
c= tensor(1.1331e+09, device='cuda:0')
c= tensor(1.1331e+09, device='cuda:0')
c= tensor(1.1331e+09, device='cuda:0')
c= tensor(1.1332e+09, device='cuda:0')
c= tensor(1.1332e+09, device='cuda:0')
c= tensor(1.1332e+09, device='cuda:0')
c= tensor(1.1332e+09, device='cuda:0')
c= tensor(1.1332e+09, device='cuda:0')
c= tensor(1.1333e+09, device='cuda:0')
c= tensor(1.1334e+09, device='cuda:0')
c= tensor(1.1334e+09, device='cuda:0')
c= tensor(1.1334e+09, device='cuda:0')
c= tensor(1.1334e+09, device='cuda:0')
c= tensor(1.1335e+09, device='cuda:0')
c= tensor(1.1335e+09, device='cuda:0')
c= tensor(1.1335e+09, device='cuda:0')
c= tensor(1.1336e+09, device='cuda:0')
c= tensor(1.1336e+09, device='cuda:0')
c= tensor(1.1336e+09, device='cuda:0')
c= tensor(1.1336e+09, device='cuda:0')
c= tensor(1.1336e+09, device='cuda:0')
c= tensor(1.1336e+09, device='cuda:0')
c= tensor(1.1336e+09, device='cuda:0')
c= tensor(1.1337e+09, device='cuda:0')
c= tensor(1.1338e+09, device='cuda:0')
c= tensor(1.1338e+09, device='cuda:0')
c= tensor(1.1338e+09, device='cuda:0')
c= tensor(1.1338e+09, device='cuda:0')
c= tensor(1.1338e+09, device='cuda:0')
c= tensor(1.1338e+09, device='cuda:0')
c= tensor(1.1338e+09, device='cuda:0')
c= tensor(1.1338e+09, device='cuda:0')
c= tensor(1.1338e+09, device='cuda:0')
c= tensor(1.1338e+09, device='cuda:0')
c= tensor(1.1338e+09, device='cuda:0')
c= tensor(1.1338e+09, device='cuda:0')
c= tensor(1.1339e+09, device='cuda:0')
c= tensor(1.1339e+09, device='cuda:0')
c= tensor(1.1339e+09, device='cuda:0')
c= tensor(1.1339e+09, device='cuda:0')
c= tensor(1.1339e+09, device='cuda:0')
c= tensor(1.1339e+09, device='cuda:0')
c= tensor(1.1340e+09, device='cuda:0')
c= tensor(1.1340e+09, device='cuda:0')
c= tensor(1.1340e+09, device='cuda:0')
c= tensor(1.1341e+09, device='cuda:0')
c= tensor(1.1341e+09, device='cuda:0')
c= tensor(1.1341e+09, device='cuda:0')
c= tensor(1.1341e+09, device='cuda:0')
c= tensor(1.1343e+09, device='cuda:0')
c= tensor(1.1343e+09, device='cuda:0')
c= tensor(1.1343e+09, device='cuda:0')
c= tensor(1.1343e+09, device='cuda:0')
c= tensor(1.1343e+09, device='cuda:0')
c= tensor(1.1343e+09, device='cuda:0')
c= tensor(1.1344e+09, device='cuda:0')
c= tensor(1.1344e+09, device='cuda:0')
c= tensor(1.1344e+09, device='cuda:0')
c= tensor(1.1344e+09, device='cuda:0')
c= tensor(1.1344e+09, device='cuda:0')
c= tensor(1.1344e+09, device='cuda:0')
c= tensor(1.1344e+09, device='cuda:0')
c= tensor(1.1344e+09, device='cuda:0')
c= tensor(1.1345e+09, device='cuda:0')
c= tensor(1.1345e+09, device='cuda:0')
c= tensor(1.1345e+09, device='cuda:0')
c= tensor(1.1345e+09, device='cuda:0')
c= tensor(1.1346e+09, device='cuda:0')
c= tensor(1.1346e+09, device='cuda:0')
c= tensor(1.1346e+09, device='cuda:0')
c= tensor(1.1346e+09, device='cuda:0')
c= tensor(1.1346e+09, device='cuda:0')
c= tensor(1.1346e+09, device='cuda:0')
c= tensor(1.1346e+09, device='cuda:0')
c= tensor(1.1347e+09, device='cuda:0')
c= tensor(1.1349e+09, device='cuda:0')
c= tensor(1.1349e+09, device='cuda:0')
c= tensor(1.1349e+09, device='cuda:0')
c= tensor(1.1349e+09, device='cuda:0')
c= tensor(1.1349e+09, device='cuda:0')
c= tensor(1.1349e+09, device='cuda:0')
c= tensor(1.1349e+09, device='cuda:0')
c= tensor(1.1349e+09, device='cuda:0')
c= tensor(1.1349e+09, device='cuda:0')
c= tensor(1.1349e+09, device='cuda:0')
c= tensor(1.1349e+09, device='cuda:0')
c= tensor(1.1349e+09, device='cuda:0')
c= tensor(1.1350e+09, device='cuda:0')
c= tensor(1.1351e+09, device='cuda:0')
c= tensor(1.1351e+09, device='cuda:0')
c= tensor(1.1351e+09, device='cuda:0')
c= tensor(1.1351e+09, device='cuda:0')
c= tensor(1.1351e+09, device='cuda:0')
c= tensor(1.1351e+09, device='cuda:0')
c= tensor(1.1351e+09, device='cuda:0')
c= tensor(1.1351e+09, device='cuda:0')
c= tensor(1.1351e+09, device='cuda:0')
c= tensor(1.1351e+09, device='cuda:0')
c= tensor(1.1352e+09, device='cuda:0')
c= tensor(1.1352e+09, device='cuda:0')
c= tensor(1.1354e+09, device='cuda:0')
c= tensor(1.1354e+09, device='cuda:0')
c= tensor(1.1354e+09, device='cuda:0')
c= tensor(1.1354e+09, device='cuda:0')
c= tensor(1.1354e+09, device='cuda:0')
c= tensor(1.1355e+09, device='cuda:0')
c= tensor(1.1355e+09, device='cuda:0')
c= tensor(1.1356e+09, device='cuda:0')
c= tensor(1.1356e+09, device='cuda:0')
c= tensor(1.1356e+09, device='cuda:0')
c= tensor(1.1356e+09, device='cuda:0')
c= tensor(1.1356e+09, device='cuda:0')
c= tensor(1.1357e+09, device='cuda:0')
c= tensor(1.1357e+09, device='cuda:0')
c= tensor(1.1357e+09, device='cuda:0')
c= tensor(1.1357e+09, device='cuda:0')
c= tensor(1.1357e+09, device='cuda:0')
c= tensor(1.1357e+09, device='cuda:0')
c= tensor(1.1357e+09, device='cuda:0')
c= tensor(1.1357e+09, device='cuda:0')
c= tensor(1.1358e+09, device='cuda:0')
c= tensor(1.1358e+09, device='cuda:0')
c= tensor(1.1358e+09, device='cuda:0')
c= tensor(1.1358e+09, device='cuda:0')
c= tensor(1.1359e+09, device='cuda:0')
c= tensor(1.1359e+09, device='cuda:0')
c= tensor(1.1359e+09, device='cuda:0')
c= tensor(1.1359e+09, device='cuda:0')
c= tensor(1.1359e+09, device='cuda:0')
c= tensor(1.1359e+09, device='cuda:0')
c= tensor(1.1360e+09, device='cuda:0')
c= tensor(1.1360e+09, device='cuda:0')
c= tensor(1.1360e+09, device='cuda:0')
c= tensor(1.1360e+09, device='cuda:0')
c= tensor(1.1360e+09, device='cuda:0')
c= tensor(1.1360e+09, device='cuda:0')
c= tensor(1.1360e+09, device='cuda:0')
c= tensor(1.1360e+09, device='cuda:0')
c= tensor(1.1361e+09, device='cuda:0')
c= tensor(1.1361e+09, device='cuda:0')
c= tensor(1.1361e+09, device='cuda:0')
c= tensor(1.1361e+09, device='cuda:0')
c= tensor(1.1362e+09, device='cuda:0')
c= tensor(1.1362e+09, device='cuda:0')
c= tensor(1.1362e+09, device='cuda:0')
c= tensor(1.1362e+09, device='cuda:0')
c= tensor(1.1362e+09, device='cuda:0')
c= tensor(1.1363e+09, device='cuda:0')
c= tensor(1.1363e+09, device='cuda:0')
c= tensor(1.1363e+09, device='cuda:0')
c= tensor(1.1363e+09, device='cuda:0')
c= tensor(1.1363e+09, device='cuda:0')
c= tensor(1.1363e+09, device='cuda:0')
c= tensor(1.1363e+09, device='cuda:0')
c= tensor(1.1364e+09, device='cuda:0')
c= tensor(1.1364e+09, device='cuda:0')
c= tensor(1.1364e+09, device='cuda:0')
c= tensor(1.1364e+09, device='cuda:0')
c= tensor(1.1365e+09, device='cuda:0')
c= tensor(1.1365e+09, device='cuda:0')
c= tensor(1.1365e+09, device='cuda:0')
c= tensor(1.1365e+09, device='cuda:0')
c= tensor(1.1365e+09, device='cuda:0')
c= tensor(1.1366e+09, device='cuda:0')
c= tensor(1.1366e+09, device='cuda:0')
c= tensor(1.1366e+09, device='cuda:0')
c= tensor(1.1366e+09, device='cuda:0')
c= tensor(1.1367e+09, device='cuda:0')
c= tensor(1.1367e+09, device='cuda:0')
c= tensor(1.1367e+09, device='cuda:0')
c= tensor(1.1367e+09, device='cuda:0')
c= tensor(1.1367e+09, device='cuda:0')
c= tensor(1.1367e+09, device='cuda:0')
c= tensor(1.1368e+09, device='cuda:0')
c= tensor(1.1368e+09, device='cuda:0')
c= tensor(1.1368e+09, device='cuda:0')
c= tensor(1.1368e+09, device='cuda:0')
c= tensor(1.1368e+09, device='cuda:0')
c= tensor(1.1368e+09, device='cuda:0')
c= tensor(1.1368e+09, device='cuda:0')
c= tensor(1.1368e+09, device='cuda:0')
c= tensor(1.1369e+09, device='cuda:0')
c= tensor(1.1369e+09, device='cuda:0')
c= tensor(1.1369e+09, device='cuda:0')
c= tensor(1.1374e+09, device='cuda:0')
c= tensor(1.1408e+09, device='cuda:0')
c= tensor(1.1409e+09, device='cuda:0')
c= tensor(1.1410e+09, device='cuda:0')
c= tensor(1.1410e+09, device='cuda:0')
c= tensor(1.1411e+09, device='cuda:0')
c= tensor(1.1411e+09, device='cuda:0')
c= tensor(1.1615e+09, device='cuda:0')
c= tensor(1.1616e+09, device='cuda:0')
c= tensor(1.1841e+09, device='cuda:0')
c= tensor(1.1931e+09, device='cuda:0')
c= tensor(1.1992e+09, device='cuda:0')
c= tensor(1.2034e+09, device='cuda:0')
c= tensor(1.2034e+09, device='cuda:0')
c= tensor(1.2035e+09, device='cuda:0')
c= tensor(1.2606e+09, device='cuda:0')
c= tensor(1.3709e+09, device='cuda:0')
c= tensor(1.3709e+09, device='cuda:0')
c= tensor(1.3717e+09, device='cuda:0')
c= tensor(1.3733e+09, device='cuda:0')
c= tensor(1.3738e+09, device='cuda:0')
c= tensor(1.3831e+09, device='cuda:0')
c= tensor(1.3862e+09, device='cuda:0')
c= tensor(1.3872e+09, device='cuda:0')
c= tensor(1.3875e+09, device='cuda:0')
c= tensor(1.3875e+09, device='cuda:0')
c= tensor(1.4158e+09, device='cuda:0')
c= tensor(1.4164e+09, device='cuda:0')
c= tensor(1.4165e+09, device='cuda:0')
c= tensor(1.4168e+09, device='cuda:0')
c= tensor(1.4239e+09, device='cuda:0')
c= tensor(1.4546e+09, device='cuda:0')
c= tensor(1.4577e+09, device='cuda:0')
c= tensor(1.4578e+09, device='cuda:0')
c= tensor(1.4603e+09, device='cuda:0')
c= tensor(1.4603e+09, device='cuda:0')
c= tensor(1.4618e+09, device='cuda:0')
c= tensor(1.4645e+09, device='cuda:0')
c= tensor(1.4680e+09, device='cuda:0')
c= tensor(1.4726e+09, device='cuda:0')
c= tensor(1.4726e+09, device='cuda:0')
c= tensor(1.4727e+09, device='cuda:0')
c= tensor(1.4801e+09, device='cuda:0')
c= tensor(1.4825e+09, device='cuda:0')
c= tensor(1.4884e+09, device='cuda:0')
c= tensor(1.4885e+09, device='cuda:0')
c= tensor(1.5315e+09, device='cuda:0')
c= tensor(1.5317e+09, device='cuda:0')
c= tensor(1.5325e+09, device='cuda:0')
c= tensor(1.5374e+09, device='cuda:0')
c= tensor(1.5374e+09, device='cuda:0')
c= tensor(1.5484e+09, device='cuda:0')
c= tensor(1.5872e+09, device='cuda:0')
c= tensor(1.9679e+09, device='cuda:0')
c= tensor(1.9730e+09, device='cuda:0')
c= tensor(1.9748e+09, device='cuda:0')
c= tensor(1.9750e+09, device='cuda:0')
c= tensor(1.9750e+09, device='cuda:0')
c= tensor(1.9988e+09, device='cuda:0')
c= tensor(1.9989e+09, device='cuda:0')
c= tensor(2.0003e+09, device='cuda:0')
c= tensor(2.0068e+09, device='cuda:0')
c= tensor(2.0089e+09, device='cuda:0')
c= tensor(2.0094e+09, device='cuda:0')
c= tensor(2.0095e+09, device='cuda:0')
c= tensor(2.0260e+09, device='cuda:0')
c= tensor(2.0366e+09, device='cuda:0')
c= tensor(2.0396e+09, device='cuda:0')
c= tensor(2.0396e+09, device='cuda:0')
c= tensor(2.0477e+09, device='cuda:0')
c= tensor(2.0484e+09, device='cuda:0')
c= tensor(2.0729e+09, device='cuda:0')
c= tensor(2.0730e+09, device='cuda:0')
c= tensor(2.0781e+09, device='cuda:0')
c= tensor(2.0783e+09, device='cuda:0')
c= tensor(2.1125e+09, device='cuda:0')
c= tensor(2.1139e+09, device='cuda:0')
c= tensor(2.1145e+09, device='cuda:0')
c= tensor(2.1230e+09, device='cuda:0')
c= tensor(2.1345e+09, device='cuda:0')
c= tensor(2.1345e+09, device='cuda:0')
c= tensor(2.1609e+09, device='cuda:0')
c= tensor(2.1789e+09, device='cuda:0')
c= tensor(2.2479e+09, device='cuda:0')
c= tensor(2.2485e+09, device='cuda:0')
c= tensor(2.2485e+09, device='cuda:0')
c= tensor(2.2486e+09, device='cuda:0')
c= tensor(2.2495e+09, device='cuda:0')
c= tensor(2.2500e+09, device='cuda:0')
c= tensor(2.2556e+09, device='cuda:0')
c= tensor(2.2556e+09, device='cuda:0')
c= tensor(2.2588e+09, device='cuda:0')
c= tensor(2.2655e+09, device='cuda:0')
c= tensor(2.2674e+09, device='cuda:0')
c= tensor(2.2675e+09, device='cuda:0')
c= tensor(2.2781e+09, device='cuda:0')
c= tensor(2.2783e+09, device='cuda:0')
c= tensor(2.2785e+09, device='cuda:0')
c= tensor(2.2787e+09, device='cuda:0')
c= tensor(2.2787e+09, device='cuda:0')
c= tensor(2.2907e+09, device='cuda:0')
c= tensor(2.2918e+09, device='cuda:0')
c= tensor(2.2922e+09, device='cuda:0')
c= tensor(2.2940e+09, device='cuda:0')
c= tensor(2.2942e+09, device='cuda:0')
c= tensor(2.5656e+09, device='cuda:0')
c= tensor(2.5661e+09, device='cuda:0')
c= tensor(2.5787e+09, device='cuda:0')
c= tensor(2.5787e+09, device='cuda:0')
c= tensor(2.5787e+09, device='cuda:0')
c= tensor(2.5788e+09, device='cuda:0')
c= tensor(2.5790e+09, device='cuda:0')
c= tensor(2.5790e+09, device='cuda:0')
c= tensor(2.5848e+09, device='cuda:0')
c= tensor(2.5848e+09, device='cuda:0')
c= tensor(2.5849e+09, device='cuda:0')
c= tensor(2.6116e+09, device='cuda:0')
c= tensor(2.6137e+09, device='cuda:0')
c= tensor(2.6144e+09, device='cuda:0')
c= tensor(2.6207e+09, device='cuda:0')
c= tensor(2.6660e+09, device='cuda:0')
c= tensor(2.6661e+09, device='cuda:0')
c= tensor(2.6661e+09, device='cuda:0')
c= tensor(2.6667e+09, device='cuda:0')
c= tensor(2.6667e+09, device='cuda:0')
c= tensor(2.6667e+09, device='cuda:0')
c= tensor(2.6669e+09, device='cuda:0')
c= tensor(2.6670e+09, device='cuda:0')
c= tensor(2.6670e+09, device='cuda:0')
c= tensor(2.6674e+09, device='cuda:0')
c= tensor(2.6676e+09, device='cuda:0')
c= tensor(2.7438e+09, device='cuda:0')
c= tensor(2.7439e+09, device='cuda:0')
c= tensor(2.7482e+09, device='cuda:0')
c= tensor(2.7484e+09, device='cuda:0')
c= tensor(2.7484e+09, device='cuda:0')
c= tensor(2.7638e+09, device='cuda:0')
c= tensor(2.9145e+09, device='cuda:0')
c= tensor(2.9961e+09, device='cuda:0')
c= tensor(2.9981e+09, device='cuda:0')
c= tensor(2.9995e+09, device='cuda:0')
c= tensor(2.9996e+09, device='cuda:0')
c= tensor(3.0001e+09, device='cuda:0')
c= tensor(3.8417e+09, device='cuda:0')
c= tensor(3.8421e+09, device='cuda:0')
c= tensor(3.8421e+09, device='cuda:0')
c= tensor(3.8451e+09, device='cuda:0')
c= tensor(4.0017e+09, device='cuda:0')
c= tensor(4.0044e+09, device='cuda:0')
c= tensor(4.0045e+09, device='cuda:0')
c= tensor(4.0046e+09, device='cuda:0')
c= tensor(4.0047e+09, device='cuda:0')
c= tensor(4.0047e+09, device='cuda:0')
c= tensor(4.0215e+09, device='cuda:0')
c= tensor(4.0217e+09, device='cuda:0')
c= tensor(4.0217e+09, device='cuda:0')
c= tensor(4.0249e+09, device='cuda:0')
c= tensor(4.0250e+09, device='cuda:0')
c= tensor(4.0251e+09, device='cuda:0')
c= tensor(4.0296e+09, device='cuda:0')
c= tensor(4.0353e+09, device='cuda:0')
c= tensor(4.0680e+09, device='cuda:0')
c= tensor(4.1001e+09, device='cuda:0')
c= tensor(4.1254e+09, device='cuda:0')
c= tensor(4.1260e+09, device='cuda:0')
c= tensor(4.1277e+09, device='cuda:0')
c= tensor(4.1394e+09, device='cuda:0')
c= tensor(4.1644e+09, device='cuda:0')
c= tensor(4.1646e+09, device='cuda:0')
c= tensor(4.2928e+09, device='cuda:0')
c= tensor(4.4704e+09, device='cuda:0')
c= tensor(4.4898e+09, device='cuda:0')
c= tensor(4.4956e+09, device='cuda:0')
c= tensor(4.5012e+09, device='cuda:0')
c= tensor(4.5017e+09, device='cuda:0')
c= tensor(4.5018e+09, device='cuda:0')
c= tensor(4.5023e+09, device='cuda:0')
c= tensor(4.5080e+09, device='cuda:0')
c= tensor(4.5183e+09, device='cuda:0')
c= tensor(4.5614e+09, device='cuda:0')
c= tensor(4.5704e+09, device='cuda:0')
c= tensor(4.5747e+09, device='cuda:0')
c= tensor(4.5752e+09, device='cuda:0')
c= tensor(4.6058e+09, device='cuda:0')
c= tensor(4.6058e+09, device='cuda:0')
c= tensor(4.6060e+09, device='cuda:0')
c= tensor(4.6163e+09, device='cuda:0')
c= tensor(4.6201e+09, device='cuda:0')
c= tensor(4.6201e+09, device='cuda:0')
c= tensor(4.6206e+09, device='cuda:0')
c= tensor(4.8032e+09, device='cuda:0')
c= tensor(4.8035e+09, device='cuda:0')
c= tensor(4.8091e+09, device='cuda:0')
c= tensor(4.8091e+09, device='cuda:0')
c= tensor(4.8091e+09, device='cuda:0')
c= tensor(4.8091e+09, device='cuda:0')
c= tensor(4.8094e+09, device='cuda:0')
c= tensor(4.8100e+09, device='cuda:0')
c= tensor(4.8196e+09, device='cuda:0')
c= tensor(4.8196e+09, device='cuda:0')
c= tensor(4.8285e+09, device='cuda:0')
c= tensor(4.8289e+09, device='cuda:0')
c= tensor(4.8313e+09, device='cuda:0')
c= tensor(4.8315e+09, device='cuda:0')
c= tensor(4.8460e+09, device='cuda:0')
c= tensor(4.8461e+09, device='cuda:0')
c= tensor(4.8475e+09, device='cuda:0')
c= tensor(4.8484e+09, device='cuda:0')
c= tensor(4.8495e+09, device='cuda:0')
c= tensor(4.8522e+09, device='cuda:0')
c= tensor(5.0112e+09, device='cuda:0')
c= tensor(5.0114e+09, device='cuda:0')
c= tensor(5.0115e+09, device='cuda:0')
c= tensor(5.0134e+09, device='cuda:0')
c= tensor(5.0139e+09, device='cuda:0')
c= tensor(5.0995e+09, device='cuda:0')
c= tensor(5.0996e+09, device='cuda:0')
c= tensor(5.1068e+09, device='cuda:0')
c= tensor(5.1442e+09, device='cuda:0')
c= tensor(5.1445e+09, device='cuda:0')
c= tensor(5.1733e+09, device='cuda:0')
c= tensor(5.1742e+09, device='cuda:0')
c= tensor(5.2786e+09, device='cuda:0')
c= tensor(5.2790e+09, device='cuda:0')
c= tensor(5.2794e+09, device='cuda:0')
c= tensor(5.2796e+09, device='cuda:0')
c= tensor(5.2797e+09, device='cuda:0')
c= tensor(5.2797e+09, device='cuda:0')
c= tensor(5.2855e+09, device='cuda:0')
c= tensor(5.2870e+09, device='cuda:0')
c= tensor(5.3016e+09, device='cuda:0')
c= tensor(5.3017e+09, device='cuda:0')
c= tensor(5.3018e+09, device='cuda:0')
c= tensor(5.3021e+09, device='cuda:0')
c= tensor(5.3186e+09, device='cuda:0')
c= tensor(5.3336e+09, device='cuda:0')
c= tensor(5.3723e+09, device='cuda:0')
c= tensor(5.3736e+09, device='cuda:0')
c= tensor(5.3737e+09, device='cuda:0')
c= tensor(5.3737e+09, device='cuda:0')
c= tensor(5.3737e+09, device='cuda:0')
c= tensor(5.4327e+09, device='cuda:0')
c= tensor(5.4327e+09, device='cuda:0')
c= tensor(5.4331e+09, device='cuda:0')
c= tensor(5.4387e+09, device='cuda:0')
c= tensor(5.4396e+09, device='cuda:0')
c= tensor(5.4404e+09, device='cuda:0')
c= tensor(5.4404e+09, device='cuda:0')
c= tensor(5.4825e+09, device='cuda:0')
c= tensor(5.4846e+09, device='cuda:0')
c= tensor(5.4849e+09, device='cuda:0')
c= tensor(5.4852e+09, device='cuda:0')
c= tensor(5.5010e+09, device='cuda:0')
c= tensor(5.5043e+09, device='cuda:0')
c= tensor(5.6137e+09, device='cuda:0')
c= tensor(5.6199e+09, device='cuda:0')
c= tensor(5.6200e+09, device='cuda:0')
c= tensor(5.6209e+09, device='cuda:0')
c= tensor(5.6222e+09, device='cuda:0')
c= tensor(5.6225e+09, device='cuda:0')
c= tensor(5.6227e+09, device='cuda:0')
c= tensor(5.6227e+09, device='cuda:0')
c= tensor(5.6246e+09, device='cuda:0')
c= tensor(5.6281e+09, device='cuda:0')
c= tensor(5.6284e+09, device='cuda:0')
c= tensor(5.6286e+09, device='cuda:0')
c= tensor(5.6287e+09, device='cuda:0')
c= tensor(5.6306e+09, device='cuda:0')
c= tensor(5.6306e+09, device='cuda:0')
c= tensor(5.6310e+09, device='cuda:0')
c= tensor(5.6321e+09, device='cuda:0')
c= tensor(5.6325e+09, device='cuda:0')
c= tensor(5.6325e+09, device='cuda:0')
c= tensor(5.6326e+09, device='cuda:0')
c= tensor(5.6328e+09, device='cuda:0')
c= tensor(5.6447e+09, device='cuda:0')
c= tensor(5.6448e+09, device='cuda:0')
c= tensor(5.6448e+09, device='cuda:0')
c= tensor(5.6449e+09, device='cuda:0')
c= tensor(5.6578e+09, device='cuda:0')
c= tensor(5.8624e+09, device='cuda:0')
c= tensor(5.8666e+09, device='cuda:0')
c= tensor(5.8667e+09, device='cuda:0')
c= tensor(5.9055e+09, device='cuda:0')
c= tensor(5.9147e+09, device='cuda:0')
c= tensor(5.9147e+09, device='cuda:0')
c= tensor(5.9150e+09, device='cuda:0')
c= tensor(5.9159e+09, device='cuda:0')
c= tensor(5.9431e+09, device='cuda:0')
c= tensor(6.1980e+09, device='cuda:0')
c= tensor(6.2104e+09, device='cuda:0')
c= tensor(6.2123e+09, device='cuda:0')
c= tensor(6.2123e+09, device='cuda:0')
c= tensor(6.2124e+09, device='cuda:0')
c= tensor(6.2174e+09, device='cuda:0')
c= tensor(6.2176e+09, device='cuda:0')
c= tensor(6.2191e+09, device='cuda:0')
c= tensor(6.2300e+09, device='cuda:0')
c= tensor(6.3134e+09, device='cuda:0')
c= tensor(6.3135e+09, device='cuda:0')
c= tensor(6.3135e+09, device='cuda:0')
c= tensor(6.3142e+09, device='cuda:0')
c= tensor(6.3471e+09, device='cuda:0')
c= tensor(6.3485e+09, device='cuda:0')
c= tensor(6.3491e+09, device='cuda:0')
c= tensor(6.3491e+09, device='cuda:0')
c= tensor(6.3496e+09, device='cuda:0')
c= tensor(6.3497e+09, device='cuda:0')
c= tensor(6.3524e+09, device='cuda:0')
c= tensor(6.3525e+09, device='cuda:0')
c= tensor(6.3525e+09, device='cuda:0')
c= tensor(6.3526e+09, device='cuda:0')
c= tensor(6.3527e+09, device='cuda:0')
c= tensor(6.3530e+09, device='cuda:0')
c= tensor(6.3640e+09, device='cuda:0')
c= tensor(6.4246e+09, device='cuda:0')
c= tensor(6.4311e+09, device='cuda:0')
c= tensor(6.4513e+09, device='cuda:0')
c= tensor(6.4516e+09, device='cuda:0')
c= tensor(6.4519e+09, device='cuda:0')
c= tensor(6.4522e+09, device='cuda:0')
c= tensor(6.4599e+09, device='cuda:0')
c= tensor(6.4604e+09, device='cuda:0')
c= tensor(6.4654e+09, device='cuda:0')
c= tensor(6.4655e+09, device='cuda:0')
c= tensor(6.9539e+09, device='cuda:0')
c= tensor(6.9541e+09, device='cuda:0')
c= tensor(6.9561e+09, device='cuda:0')
c= tensor(6.9898e+09, device='cuda:0')
c= tensor(6.9909e+09, device='cuda:0')
c= tensor(6.9916e+09, device='cuda:0')
c= tensor(7.5327e+09, device='cuda:0')
c= tensor(7.5578e+09, device='cuda:0')
c= tensor(7.5620e+09, device='cuda:0')
c= tensor(7.5621e+09, device='cuda:0')
c= tensor(7.5631e+09, device='cuda:0')
c= tensor(7.5633e+09, device='cuda:0')
c= tensor(7.6006e+09, device='cuda:0')
c= tensor(7.9226e+09, device='cuda:0')
c= tensor(7.9413e+09, device='cuda:0')
c= tensor(7.9546e+09, device='cuda:0')
c= tensor(7.9610e+09, device='cuda:0')
c= tensor(7.9620e+09, device='cuda:0')
c= tensor(7.9628e+09, device='cuda:0')
c= tensor(7.9630e+09, device='cuda:0')
c= tensor(8.0608e+09, device='cuda:0')
c= tensor(8.1230e+09, device='cuda:0')
c= tensor(8.1268e+09, device='cuda:0')
c= tensor(9.2391e+09, device='cuda:0')
c= tensor(9.2695e+09, device='cuda:0')
c= tensor(9.2711e+09, device='cuda:0')
c= tensor(9.2716e+09, device='cuda:0')
c= tensor(9.2755e+09, device='cuda:0')
c= tensor(9.2835e+09, device='cuda:0')
c= tensor(9.2836e+09, device='cuda:0')
c= tensor(9.4043e+09, device='cuda:0')
c= tensor(9.4052e+09, device='cuda:0')
c= tensor(9.4064e+09, device='cuda:0')
c= tensor(9.4065e+09, device='cuda:0')
c= tensor(9.4068e+09, device='cuda:0')
c= tensor(9.4068e+09, device='cuda:0')
c= tensor(9.4068e+09, device='cuda:0')
c= tensor(9.4075e+09, device='cuda:0')
c= tensor(9.4090e+09, device='cuda:0')
c= tensor(1.3154e+10, device='cuda:0')
c= tensor(1.3158e+10, device='cuda:0')
c= tensor(1.3197e+10, device='cuda:0')
c= tensor(1.3198e+10, device='cuda:0')
c= tensor(1.3198e+10, device='cuda:0')
c= tensor(1.3198e+10, device='cuda:0')
c= tensor(1.3236e+10, device='cuda:0')
c= tensor(1.3240e+10, device='cuda:0')
c= tensor(1.3541e+10, device='cuda:0')
c= tensor(1.3541e+10, device='cuda:0')
c= tensor(1.3554e+10, device='cuda:0')
c= tensor(1.3555e+10, device='cuda:0')
c= tensor(1.3564e+10, device='cuda:0')
c= tensor(1.3582e+10, device='cuda:0')
c= tensor(1.3583e+10, device='cuda:0')
c= tensor(1.3583e+10, device='cuda:0')
c= tensor(1.3585e+10, device='cuda:0')
c= tensor(1.3588e+10, device='cuda:0')
c= tensor(1.3590e+10, device='cuda:0')
c= tensor(1.3711e+10, device='cuda:0')
c= tensor(1.3712e+10, device='cuda:0')
c= tensor(1.3716e+10, device='cuda:0')
c= tensor(1.3717e+10, device='cuda:0')
c= tensor(1.3724e+10, device='cuda:0')
c= tensor(1.3776e+10, device='cuda:0')
c= tensor(1.3778e+10, device='cuda:0')
c= tensor(1.3779e+10, device='cuda:0')
c= tensor(1.3801e+10, device='cuda:0')
c= tensor(1.3807e+10, device='cuda:0')
c= tensor(1.4050e+10, device='cuda:0')
c= tensor(1.4064e+10, device='cuda:0')
c= tensor(1.4065e+10, device='cuda:0')
c= tensor(1.4068e+10, device='cuda:0')
c= tensor(1.4096e+10, device='cuda:0')
c= tensor(1.4114e+10, device='cuda:0')
c= tensor(1.4115e+10, device='cuda:0')
c= tensor(1.4115e+10, device='cuda:0')
c= tensor(1.4116e+10, device='cuda:0')
c= tensor(1.4122e+10, device='cuda:0')
c= tensor(1.4140e+10, device='cuda:0')
c= tensor(1.4142e+10, device='cuda:0')
c= tensor(1.4142e+10, device='cuda:0')
c= tensor(1.4143e+10, device='cuda:0')
c= tensor(1.4146e+10, device='cuda:0')
c= tensor(1.4148e+10, device='cuda:0')
c= tensor(1.4148e+10, device='cuda:0')
c= tensor(1.4150e+10, device='cuda:0')
c= tensor(1.4152e+10, device='cuda:0')
c= tensor(1.4153e+10, device='cuda:0')
c= tensor(1.4153e+10, device='cuda:0')
c= tensor(1.4153e+10, device='cuda:0')
c= tensor(1.4175e+10, device='cuda:0')
c= tensor(1.4176e+10, device='cuda:0')
c= tensor(1.4180e+10, device='cuda:0')
c= tensor(1.4180e+10, device='cuda:0')
c= tensor(1.4180e+10, device='cuda:0')
c= tensor(1.4183e+10, device='cuda:0')
c= tensor(1.4190e+10, device='cuda:0')
c= tensor(1.4190e+10, device='cuda:0')
c= tensor(1.4190e+10, device='cuda:0')
c= tensor(1.4190e+10, device='cuda:0')
c= tensor(1.4191e+10, device='cuda:0')
c= tensor(1.4192e+10, device='cuda:0')
c= tensor(1.4200e+10, device='cuda:0')
c= tensor(1.4200e+10, device='cuda:0')
c= tensor(1.4200e+10, device='cuda:0')
c= tensor(1.4204e+10, device='cuda:0')
c= tensor(1.4204e+10, device='cuda:0')
c= tensor(1.4229e+10, device='cuda:0')
c= tensor(1.4229e+10, device='cuda:0')
c= tensor(1.4242e+10, device='cuda:0')
c= tensor(1.4250e+10, device='cuda:0')
c= tensor(1.4251e+10, device='cuda:0')
c= tensor(1.4259e+10, device='cuda:0')
c= tensor(1.4264e+10, device='cuda:0')
c= tensor(1.4264e+10, device='cuda:0')
c= tensor(1.4267e+10, device='cuda:0')
c= tensor(1.4267e+10, device='cuda:0')
c= tensor(1.4287e+10, device='cuda:0')
c= tensor(1.4292e+10, device='cuda:0')
c= tensor(1.4294e+10, device='cuda:0')
c= tensor(1.4297e+10, device='cuda:0')
c= tensor(1.4299e+10, device='cuda:0')
c= tensor(1.4305e+10, device='cuda:0')
c= tensor(1.4306e+10, device='cuda:0')
c= tensor(1.4306e+10, device='cuda:0')
c= tensor(1.4362e+10, device='cuda:0')
c= tensor(1.4515e+10, device='cuda:0')
c= tensor(1.4515e+10, device='cuda:0')
c= tensor(1.4515e+10, device='cuda:0')
c= tensor(1.4516e+10, device='cuda:0')
c= tensor(1.4539e+10, device='cuda:0')
c= tensor(1.4540e+10, device='cuda:0')
c= tensor(1.4540e+10, device='cuda:0')
c= tensor(1.4542e+10, device='cuda:0')
c= tensor(1.4550e+10, device='cuda:0')
c= tensor(1.4550e+10, device='cuda:0')
c= tensor(1.4552e+10, device='cuda:0')
c= tensor(1.4557e+10, device='cuda:0')
memory (bytes)
6856036352
time for making loss 2 is 9.490374565124512
p0 True
it  0 : 4495350784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 74% |
shape of L is 
torch.Size([])
memory (bytes)
6856429568
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 15% |
memory (bytes)
6857011200
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 16% |
error is  164495640000.0
relative error loss 11.300215
shape of L is 
torch.Size([])
memory (bytes)
6926761984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  4% | 17% |
memory (bytes)
6926766080
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  164495200000.0
relative error loss 11.300184
shape of L is 
torch.Size([])
memory (bytes)
6929043456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
6929043456
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  164493180000.0
relative error loss 11.300046
shape of L is 
torch.Size([])
memory (bytes)
6930120704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
6930120704
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  164483000000.0
relative error loss 11.299347
shape of L is 
torch.Size([])
memory (bytes)
6931177472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% | 17% |
memory (bytes)
6931177472
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  164426040000.0
relative error loss 11.295434
shape of L is 
torch.Size([])
memory (bytes)
6932279296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% | 17% |
memory (bytes)
6932279296
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  163801140000.0
relative error loss 11.252505
shape of L is 
torch.Size([])
memory (bytes)
6933381120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
6933381120
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  160703250000.0
relative error loss 11.039693
shape of L is 
torch.Size([])
memory (bytes)
6934433792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 17% |
memory (bytes)
6934433792
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  129812110000.0
relative error loss 8.91759
shape of L is 
torch.Size([])
memory (bytes)
6935502848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 32% | 17% |
memory (bytes)
6935502848
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  39774540000.0
relative error loss 2.7323573
shape of L is 
torch.Size([])
memory (bytes)
6936559616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 17% |
memory (bytes)
6936559616
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  23931884000.0
relative error loss 1.644028
time to take a step is 176.37168860435486
it  1 : 5055313920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
6937608192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 17% |
memory (bytes)
6937608192
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  23931884000.0
relative error loss 1.644028
shape of L is 
torch.Size([])
memory (bytes)
6938673152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
6938673152
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  30739653000.0
relative error loss 2.1116953
shape of L is 
torch.Size([])
memory (bytes)
6939734016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
6939734016
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  18669080000.0
relative error loss 1.2824937
shape of L is 
torch.Size([])
memory (bytes)
6940790784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 17% |
memory (bytes)
6940790784
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  16104479000.0
relative error loss 1.1063155
shape of L is 
torch.Size([])
memory (bytes)
6941851648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
6941851648
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  14849613000.0
relative error loss 1.0201111
shape of L is 
torch.Size([])
memory (bytes)
6942916608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 17% |
memory (bytes)
6942916608
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  14322332000.0
relative error loss 0.98388886
shape of L is 
torch.Size([])
memory (bytes)
6943977472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 17% |
memory (bytes)
6943985664
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  15859299000.0
relative error loss 1.0894725
shape of L is 
torch.Size([])
memory (bytes)
6945042432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 17% |
memory (bytes)
6945042432
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  13559839000.0
relative error loss 0.93150854
shape of L is 
torch.Size([])
memory (bytes)
6946099200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 17% |
memory (bytes)
6946099200
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  13882012000.0
relative error loss 0.9536406
shape of L is 
torch.Size([])
memory (bytes)
6947160064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 17% |
memory (bytes)
6947160064
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  13301226000.0
relative error loss 0.9137429
time to take a step is 163.93203997612
it  2 : 5215304192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
6948220928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% | 17% |
memory (bytes)
6948220928
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  13301226000.0
relative error loss 0.9137429
shape of L is 
torch.Size([])
memory (bytes)
6949281792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
6949281792
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  12555752000.0
relative error loss 0.86253166
shape of L is 
torch.Size([])
memory (bytes)
6950342656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 17% |
memory (bytes)
6950342656
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  12016027000.0
relative error loss 0.8254546
shape of L is 
torch.Size([])
memory (bytes)
6951403520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
6951403520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 17% |
error is  11215052000.0
relative error loss 0.77043074
shape of L is 
torch.Size([])
memory (bytes)
6952460288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
6952460288
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  10782515000.0
relative error loss 0.7407171
shape of L is 
torch.Size([])
memory (bytes)
6953418752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 17% |
memory (bytes)
6953533440
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  10211482000.0
relative error loss 0.7014893
shape of L is 
torch.Size([])
memory (bytes)
6954594304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 33% | 17% |
memory (bytes)
6954594304
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  9566519000.0
relative error loss 0.65718293
shape of L is 
torch.Size([])
memory (bytes)
6955655168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 17% |
memory (bytes)
6955655168
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  9310017000.0
relative error loss 0.6395622
shape of L is 
torch.Size([])
memory (bytes)
6956711936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 17% |
memory (bytes)
6956711936
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  8620415000.0
relative error loss 0.5921892
shape of L is 
torch.Size([])
memory (bytes)
6957772800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
6957772800
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  8087979500.0
relative error loss 0.5556129
time to take a step is 165.604505777359
it  3 : 5215303680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
6958837760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
6958837760
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  8087979500.0
relative error loss 0.5556129
shape of L is 
torch.Size([])
memory (bytes)
6959894528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
6959894528
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  7723623000.0
relative error loss 0.5305831
shape of L is 
torch.Size([])
memory (bytes)
6960955392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
6960955392
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  7304854000.0
relative error loss 0.50181526
shape of L is 
torch.Size([])
memory (bytes)
6962020352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 17% |
memory (bytes)
6962020352
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  6650351600.0
relative error loss 0.45685345
shape of L is 
torch.Size([])
memory (bytes)
6963085312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
6963085312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% | 17% |
error is  6090728000.0
relative error loss 0.41840947
shape of L is 
torch.Size([])
memory (bytes)
6964146176
| ID | GPU | MEM |
------------------
|  0 |  4% |  0% |
|  1 | 14% | 17% |
memory (bytes)
6964146176
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  5548393500.0
relative error loss 0.3811532
shape of L is 
torch.Size([])
memory (bytes)
6965207040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 17% |
memory (bytes)
6965207040
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  5086585000.0
relative error loss 0.3494287
shape of L is 
torch.Size([])
memory (bytes)
6966272000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 34% | 17% |
memory (bytes)
6966272000
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  4632652000.0
relative error loss 0.3182453
shape of L is 
torch.Size([])
memory (bytes)
6967341056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 17% |
memory (bytes)
6967341056
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  4212977700.0
relative error loss 0.2894153
shape of L is 
torch.Size([])
memory (bytes)
6968397824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
6968397824
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3864723500.0
relative error loss 0.26549158
time to take a step is 164.67577505111694
c= tensor(6979.7393, device='cuda:0')
c= tensor(364769.5312, device='cuda:0')
c= tensor(371762.0625, device='cuda:0')
c= tensor(633204.3750, device='cuda:0')
c= tensor(849530.6250, device='cuda:0')
c= tensor(3200810., device='cuda:0')
c= tensor(4452292., device='cuda:0')
c= tensor(5270818., device='cuda:0')
c= tensor(6951913.5000, device='cuda:0')
c= tensor(25692742., device='cuda:0')
c= tensor(25838604., device='cuda:0')
c= tensor(30943392., device='cuda:0')
c= tensor(30970752., device='cuda:0')
c= tensor(95888168., device='cuda:0')
c= tensor(96402008., device='cuda:0')
c= tensor(97896024., device='cuda:0')
c= tensor(99831360., device='cuda:0')
c= tensor(1.0051e+08, device='cuda:0')
c= tensor(1.1234e+08, device='cuda:0')
c= tensor(1.1758e+08, device='cuda:0')
c= tensor(1.2374e+08, device='cuda:0')
c= tensor(1.4521e+08, device='cuda:0')
c= tensor(1.4531e+08, device='cuda:0')
c= tensor(1.4747e+08, device='cuda:0')
c= tensor(1.4771e+08, device='cuda:0')
c= tensor(1.4925e+08, device='cuda:0')
c= tensor(1.5245e+08, device='cuda:0')
c= tensor(1.5259e+08, device='cuda:0')
c= tensor(1.7086e+08, device='cuda:0')
c= tensor(6.9521e+08, device='cuda:0')
c= tensor(6.9568e+08, device='cuda:0')
c= tensor(1.0596e+09, device='cuda:0')
c= tensor(1.0602e+09, device='cuda:0')
c= tensor(1.0605e+09, device='cuda:0')
c= tensor(1.0616e+09, device='cuda:0')
c= tensor(1.1280e+09, device='cuda:0')
c= tensor(1.1327e+09, device='cuda:0')
c= tensor(1.1327e+09, device='cuda:0')
c= tensor(1.1327e+09, device='cuda:0')
c= tensor(1.1327e+09, device='cuda:0')
c= tensor(1.1327e+09, device='cuda:0')
c= tensor(1.1327e+09, device='cuda:0')
c= tensor(1.1327e+09, device='cuda:0')
c= tensor(1.1327e+09, device='cuda:0')
c= tensor(1.1327e+09, device='cuda:0')
c= tensor(1.1327e+09, device='cuda:0')
c= tensor(1.1327e+09, device='cuda:0')
c= tensor(1.1328e+09, device='cuda:0')
c= tensor(1.1328e+09, device='cuda:0')
c= tensor(1.1329e+09, device='cuda:0')
c= tensor(1.1330e+09, device='cuda:0')
c= tensor(1.1330e+09, device='cuda:0')
c= tensor(1.1330e+09, device='cuda:0')
c= tensor(1.1331e+09, device='cuda:0')
c= tensor(1.1331e+09, device='cuda:0')
c= tensor(1.1331e+09, device='cuda:0')
c= tensor(1.1331e+09, device='cuda:0')
c= tensor(1.1331e+09, device='cuda:0')
c= tensor(1.1332e+09, device='cuda:0')
c= tensor(1.1332e+09, device='cuda:0')
c= tensor(1.1332e+09, device='cuda:0')
c= tensor(1.1332e+09, device='cuda:0')
c= tensor(1.1332e+09, device='cuda:0')
c= tensor(1.1333e+09, device='cuda:0')
c= tensor(1.1334e+09, device='cuda:0')
c= tensor(1.1334e+09, device='cuda:0')
c= tensor(1.1334e+09, device='cuda:0')
c= tensor(1.1334e+09, device='cuda:0')
c= tensor(1.1335e+09, device='cuda:0')
c= tensor(1.1335e+09, device='cuda:0')
c= tensor(1.1335e+09, device='cuda:0')
c= tensor(1.1336e+09, device='cuda:0')
c= tensor(1.1336e+09, device='cuda:0')
c= tensor(1.1336e+09, device='cuda:0')
c= tensor(1.1336e+09, device='cuda:0')
c= tensor(1.1336e+09, device='cuda:0')
c= tensor(1.1336e+09, device='cuda:0')
c= tensor(1.1336e+09, device='cuda:0')
c= tensor(1.1337e+09, device='cuda:0')
c= tensor(1.1338e+09, device='cuda:0')
c= tensor(1.1338e+09, device='cuda:0')
c= tensor(1.1338e+09, device='cuda:0')
c= tensor(1.1338e+09, device='cuda:0')
c= tensor(1.1338e+09, device='cuda:0')
c= tensor(1.1338e+09, device='cuda:0')
c= tensor(1.1338e+09, device='cuda:0')
c= tensor(1.1338e+09, device='cuda:0')
c= tensor(1.1338e+09, device='cuda:0')
c= tensor(1.1338e+09, device='cuda:0')
c= tensor(1.1338e+09, device='cuda:0')
c= tensor(1.1338e+09, device='cuda:0')
c= tensor(1.1339e+09, device='cuda:0')
c= tensor(1.1339e+09, device='cuda:0')
c= tensor(1.1339e+09, device='cuda:0')
c= tensor(1.1339e+09, device='cuda:0')
c= tensor(1.1339e+09, device='cuda:0')
c= tensor(1.1339e+09, device='cuda:0')
c= tensor(1.1340e+09, device='cuda:0')
c= tensor(1.1340e+09, device='cuda:0')
c= tensor(1.1340e+09, device='cuda:0')
c= tensor(1.1341e+09, device='cuda:0')
c= tensor(1.1341e+09, device='cuda:0')
c= tensor(1.1341e+09, device='cuda:0')
c= tensor(1.1341e+09, device='cuda:0')
c= tensor(1.1343e+09, device='cuda:0')
c= tensor(1.1343e+09, device='cuda:0')
c= tensor(1.1343e+09, device='cuda:0')
c= tensor(1.1343e+09, device='cuda:0')
c= tensor(1.1343e+09, device='cuda:0')
c= tensor(1.1343e+09, device='cuda:0')
c= tensor(1.1344e+09, device='cuda:0')
c= tensor(1.1344e+09, device='cuda:0')
c= tensor(1.1344e+09, device='cuda:0')
c= tensor(1.1344e+09, device='cuda:0')
c= tensor(1.1344e+09, device='cuda:0')
c= tensor(1.1344e+09, device='cuda:0')
c= tensor(1.1344e+09, device='cuda:0')
c= tensor(1.1344e+09, device='cuda:0')
c= tensor(1.1345e+09, device='cuda:0')
c= tensor(1.1345e+09, device='cuda:0')
c= tensor(1.1345e+09, device='cuda:0')
c= tensor(1.1345e+09, device='cuda:0')
c= tensor(1.1346e+09, device='cuda:0')
c= tensor(1.1346e+09, device='cuda:0')
c= tensor(1.1346e+09, device='cuda:0')
c= tensor(1.1346e+09, device='cuda:0')
c= tensor(1.1346e+09, device='cuda:0')
c= tensor(1.1346e+09, device='cuda:0')
c= tensor(1.1346e+09, device='cuda:0')
c= tensor(1.1347e+09, device='cuda:0')
c= tensor(1.1349e+09, device='cuda:0')
c= tensor(1.1349e+09, device='cuda:0')
c= tensor(1.1349e+09, device='cuda:0')
c= tensor(1.1349e+09, device='cuda:0')
c= tensor(1.1349e+09, device='cuda:0')
c= tensor(1.1349e+09, device='cuda:0')
c= tensor(1.1349e+09, device='cuda:0')
c= tensor(1.1349e+09, device='cuda:0')
c= tensor(1.1349e+09, device='cuda:0')
c= tensor(1.1349e+09, device='cuda:0')
c= tensor(1.1349e+09, device='cuda:0')
c= tensor(1.1349e+09, device='cuda:0')
c= tensor(1.1350e+09, device='cuda:0')
c= tensor(1.1351e+09, device='cuda:0')
c= tensor(1.1351e+09, device='cuda:0')
c= tensor(1.1351e+09, device='cuda:0')
c= tensor(1.1351e+09, device='cuda:0')
c= tensor(1.1351e+09, device='cuda:0')
c= tensor(1.1351e+09, device='cuda:0')
c= tensor(1.1351e+09, device='cuda:0')
c= tensor(1.1351e+09, device='cuda:0')
c= tensor(1.1351e+09, device='cuda:0')
c= tensor(1.1351e+09, device='cuda:0')
c= tensor(1.1352e+09, device='cuda:0')
c= tensor(1.1352e+09, device='cuda:0')
c= tensor(1.1354e+09, device='cuda:0')
c= tensor(1.1354e+09, device='cuda:0')
c= tensor(1.1354e+09, device='cuda:0')
c= tensor(1.1354e+09, device='cuda:0')
c= tensor(1.1354e+09, device='cuda:0')
c= tensor(1.1355e+09, device='cuda:0')
c= tensor(1.1355e+09, device='cuda:0')
c= tensor(1.1356e+09, device='cuda:0')
c= tensor(1.1356e+09, device='cuda:0')
c= tensor(1.1356e+09, device='cuda:0')
c= tensor(1.1356e+09, device='cuda:0')
c= tensor(1.1356e+09, device='cuda:0')
c= tensor(1.1357e+09, device='cuda:0')
c= tensor(1.1357e+09, device='cuda:0')
c= tensor(1.1357e+09, device='cuda:0')
c= tensor(1.1357e+09, device='cuda:0')
c= tensor(1.1357e+09, device='cuda:0')
c= tensor(1.1357e+09, device='cuda:0')
c= tensor(1.1357e+09, device='cuda:0')
c= tensor(1.1357e+09, device='cuda:0')
c= tensor(1.1358e+09, device='cuda:0')
c= tensor(1.1358e+09, device='cuda:0')
c= tensor(1.1358e+09, device='cuda:0')
c= tensor(1.1358e+09, device='cuda:0')
c= tensor(1.1359e+09, device='cuda:0')
c= tensor(1.1359e+09, device='cuda:0')
c= tensor(1.1359e+09, device='cuda:0')
c= tensor(1.1359e+09, device='cuda:0')
c= tensor(1.1359e+09, device='cuda:0')
c= tensor(1.1359e+09, device='cuda:0')
c= tensor(1.1360e+09, device='cuda:0')
c= tensor(1.1360e+09, device='cuda:0')
c= tensor(1.1360e+09, device='cuda:0')
c= tensor(1.1360e+09, device='cuda:0')
c= tensor(1.1360e+09, device='cuda:0')
c= tensor(1.1360e+09, device='cuda:0')
c= tensor(1.1360e+09, device='cuda:0')
c= tensor(1.1360e+09, device='cuda:0')
c= tensor(1.1361e+09, device='cuda:0')
c= tensor(1.1361e+09, device='cuda:0')
c= tensor(1.1361e+09, device='cuda:0')
c= tensor(1.1361e+09, device='cuda:0')
c= tensor(1.1362e+09, device='cuda:0')
c= tensor(1.1362e+09, device='cuda:0')
c= tensor(1.1362e+09, device='cuda:0')
c= tensor(1.1362e+09, device='cuda:0')
c= tensor(1.1362e+09, device='cuda:0')
c= tensor(1.1363e+09, device='cuda:0')
c= tensor(1.1363e+09, device='cuda:0')
c= tensor(1.1363e+09, device='cuda:0')
c= tensor(1.1363e+09, device='cuda:0')
c= tensor(1.1363e+09, device='cuda:0')
c= tensor(1.1363e+09, device='cuda:0')
c= tensor(1.1363e+09, device='cuda:0')
c= tensor(1.1364e+09, device='cuda:0')
c= tensor(1.1364e+09, device='cuda:0')
c= tensor(1.1364e+09, device='cuda:0')
c= tensor(1.1364e+09, device='cuda:0')
c= tensor(1.1365e+09, device='cuda:0')
c= tensor(1.1365e+09, device='cuda:0')
c= tensor(1.1365e+09, device='cuda:0')
c= tensor(1.1365e+09, device='cuda:0')
c= tensor(1.1365e+09, device='cuda:0')
c= tensor(1.1366e+09, device='cuda:0')
c= tensor(1.1366e+09, device='cuda:0')
c= tensor(1.1366e+09, device='cuda:0')
c= tensor(1.1366e+09, device='cuda:0')
c= tensor(1.1367e+09, device='cuda:0')
c= tensor(1.1367e+09, device='cuda:0')
c= tensor(1.1367e+09, device='cuda:0')
c= tensor(1.1367e+09, device='cuda:0')
c= tensor(1.1367e+09, device='cuda:0')
c= tensor(1.1367e+09, device='cuda:0')
c= tensor(1.1368e+09, device='cuda:0')
c= tensor(1.1368e+09, device='cuda:0')
c= tensor(1.1368e+09, device='cuda:0')
c= tensor(1.1368e+09, device='cuda:0')
c= tensor(1.1368e+09, device='cuda:0')
c= tensor(1.1368e+09, device='cuda:0')
c= tensor(1.1368e+09, device='cuda:0')
c= tensor(1.1368e+09, device='cuda:0')
c= tensor(1.1369e+09, device='cuda:0')
c= tensor(1.1369e+09, device='cuda:0')
c= tensor(1.1369e+09, device='cuda:0')
c= tensor(1.1374e+09, device='cuda:0')
c= tensor(1.1408e+09, device='cuda:0')
c= tensor(1.1409e+09, device='cuda:0')
c= tensor(1.1410e+09, device='cuda:0')
c= tensor(1.1410e+09, device='cuda:0')
c= tensor(1.1411e+09, device='cuda:0')
c= tensor(1.1411e+09, device='cuda:0')
c= tensor(1.1615e+09, device='cuda:0')
c= tensor(1.1616e+09, device='cuda:0')
c= tensor(1.1841e+09, device='cuda:0')
c= tensor(1.1931e+09, device='cuda:0')
c= tensor(1.1992e+09, device='cuda:0')
c= tensor(1.2034e+09, device='cuda:0')
c= tensor(1.2034e+09, device='cuda:0')
c= tensor(1.2035e+09, device='cuda:0')
c= tensor(1.2606e+09, device='cuda:0')
c= tensor(1.3709e+09, device='cuda:0')
c= tensor(1.3709e+09, device='cuda:0')
c= tensor(1.3717e+09, device='cuda:0')
c= tensor(1.3733e+09, device='cuda:0')
c= tensor(1.3738e+09, device='cuda:0')
c= tensor(1.3831e+09, device='cuda:0')
c= tensor(1.3862e+09, device='cuda:0')
c= tensor(1.3872e+09, device='cuda:0')
c= tensor(1.3875e+09, device='cuda:0')
c= tensor(1.3875e+09, device='cuda:0')
c= tensor(1.4158e+09, device='cuda:0')
c= tensor(1.4164e+09, device='cuda:0')
c= tensor(1.4165e+09, device='cuda:0')
c= tensor(1.4168e+09, device='cuda:0')
c= tensor(1.4239e+09, device='cuda:0')
c= tensor(1.4546e+09, device='cuda:0')
c= tensor(1.4577e+09, device='cuda:0')
c= tensor(1.4578e+09, device='cuda:0')
c= tensor(1.4603e+09, device='cuda:0')
c= tensor(1.4603e+09, device='cuda:0')
c= tensor(1.4618e+09, device='cuda:0')
c= tensor(1.4645e+09, device='cuda:0')
c= tensor(1.4680e+09, device='cuda:0')
c= tensor(1.4726e+09, device='cuda:0')
c= tensor(1.4726e+09, device='cuda:0')
c= tensor(1.4727e+09, device='cuda:0')
c= tensor(1.4801e+09, device='cuda:0')
c= tensor(1.4825e+09, device='cuda:0')
c= tensor(1.4884e+09, device='cuda:0')
c= tensor(1.4885e+09, device='cuda:0')
c= tensor(1.5315e+09, device='cuda:0')
c= tensor(1.5317e+09, device='cuda:0')
c= tensor(1.5325e+09, device='cuda:0')
c= tensor(1.5374e+09, device='cuda:0')
c= tensor(1.5374e+09, device='cuda:0')
c= tensor(1.5484e+09, device='cuda:0')
c= tensor(1.5872e+09, device='cuda:0')
c= tensor(1.9679e+09, device='cuda:0')
c= tensor(1.9730e+09, device='cuda:0')
c= tensor(1.9748e+09, device='cuda:0')
c= tensor(1.9750e+09, device='cuda:0')
c= tensor(1.9750e+09, device='cuda:0')
c= tensor(1.9988e+09, device='cuda:0')
c= tensor(1.9989e+09, device='cuda:0')
c= tensor(2.0003e+09, device='cuda:0')
c= tensor(2.0068e+09, device='cuda:0')
c= tensor(2.0089e+09, device='cuda:0')
c= tensor(2.0094e+09, device='cuda:0')
c= tensor(2.0095e+09, device='cuda:0')
c= tensor(2.0260e+09, device='cuda:0')
c= tensor(2.0366e+09, device='cuda:0')
c= tensor(2.0396e+09, device='cuda:0')
c= tensor(2.0396e+09, device='cuda:0')
c= tensor(2.0477e+09, device='cuda:0')
c= tensor(2.0484e+09, device='cuda:0')
c= tensor(2.0729e+09, device='cuda:0')
c= tensor(2.0730e+09, device='cuda:0')
c= tensor(2.0781e+09, device='cuda:0')
c= tensor(2.0783e+09, device='cuda:0')
c= tensor(2.1125e+09, device='cuda:0')
c= tensor(2.1139e+09, device='cuda:0')
c= tensor(2.1145e+09, device='cuda:0')
c= tensor(2.1230e+09, device='cuda:0')
c= tensor(2.1345e+09, device='cuda:0')
c= tensor(2.1345e+09, device='cuda:0')
c= tensor(2.1609e+09, device='cuda:0')
c= tensor(2.1789e+09, device='cuda:0')
c= tensor(2.2479e+09, device='cuda:0')
c= tensor(2.2485e+09, device='cuda:0')
c= tensor(2.2485e+09, device='cuda:0')
c= tensor(2.2486e+09, device='cuda:0')
c= tensor(2.2495e+09, device='cuda:0')
c= tensor(2.2500e+09, device='cuda:0')
c= tensor(2.2556e+09, device='cuda:0')
c= tensor(2.2556e+09, device='cuda:0')
c= tensor(2.2588e+09, device='cuda:0')
c= tensor(2.2655e+09, device='cuda:0')
c= tensor(2.2674e+09, device='cuda:0')
c= tensor(2.2675e+09, device='cuda:0')
c= tensor(2.2781e+09, device='cuda:0')
c= tensor(2.2783e+09, device='cuda:0')
c= tensor(2.2785e+09, device='cuda:0')
c= tensor(2.2787e+09, device='cuda:0')
c= tensor(2.2787e+09, device='cuda:0')
c= tensor(2.2907e+09, device='cuda:0')
c= tensor(2.2918e+09, device='cuda:0')
c= tensor(2.2922e+09, device='cuda:0')
c= tensor(2.2940e+09, device='cuda:0')
c= tensor(2.2942e+09, device='cuda:0')
c= tensor(2.5656e+09, device='cuda:0')
c= tensor(2.5661e+09, device='cuda:0')
c= tensor(2.5787e+09, device='cuda:0')
c= tensor(2.5787e+09, device='cuda:0')
c= tensor(2.5787e+09, device='cuda:0')
c= tensor(2.5788e+09, device='cuda:0')
c= tensor(2.5790e+09, device='cuda:0')
c= tensor(2.5790e+09, device='cuda:0')
c= tensor(2.5848e+09, device='cuda:0')
c= tensor(2.5848e+09, device='cuda:0')
c= tensor(2.5849e+09, device='cuda:0')
c= tensor(2.6116e+09, device='cuda:0')
c= tensor(2.6137e+09, device='cuda:0')
c= tensor(2.6144e+09, device='cuda:0')
c= tensor(2.6207e+09, device='cuda:0')
c= tensor(2.6660e+09, device='cuda:0')
c= tensor(2.6661e+09, device='cuda:0')
c= tensor(2.6661e+09, device='cuda:0')
c= tensor(2.6667e+09, device='cuda:0')
c= tensor(2.6667e+09, device='cuda:0')
c= tensor(2.6667e+09, device='cuda:0')
c= tensor(2.6669e+09, device='cuda:0')
c= tensor(2.6670e+09, device='cuda:0')
c= tensor(2.6670e+09, device='cuda:0')
c= tensor(2.6674e+09, device='cuda:0')
c= tensor(2.6676e+09, device='cuda:0')
c= tensor(2.7438e+09, device='cuda:0')
c= tensor(2.7439e+09, device='cuda:0')
c= tensor(2.7482e+09, device='cuda:0')
c= tensor(2.7484e+09, device='cuda:0')
c= tensor(2.7484e+09, device='cuda:0')
c= tensor(2.7638e+09, device='cuda:0')
c= tensor(2.9145e+09, device='cuda:0')
c= tensor(2.9961e+09, device='cuda:0')
c= tensor(2.9981e+09, device='cuda:0')
c= tensor(2.9995e+09, device='cuda:0')
c= tensor(2.9996e+09, device='cuda:0')
c= tensor(3.0001e+09, device='cuda:0')
c= tensor(3.8417e+09, device='cuda:0')
c= tensor(3.8421e+09, device='cuda:0')
c= tensor(3.8421e+09, device='cuda:0')
c= tensor(3.8451e+09, device='cuda:0')
c= tensor(4.0017e+09, device='cuda:0')
c= tensor(4.0044e+09, device='cuda:0')
c= tensor(4.0045e+09, device='cuda:0')
c= tensor(4.0046e+09, device='cuda:0')
c= tensor(4.0047e+09, device='cuda:0')
c= tensor(4.0047e+09, device='cuda:0')
c= tensor(4.0215e+09, device='cuda:0')
c= tensor(4.0217e+09, device='cuda:0')
c= tensor(4.0217e+09, device='cuda:0')
c= tensor(4.0249e+09, device='cuda:0')
c= tensor(4.0250e+09, device='cuda:0')
c= tensor(4.0251e+09, device='cuda:0')
c= tensor(4.0296e+09, device='cuda:0')
c= tensor(4.0353e+09, device='cuda:0')
c= tensor(4.0680e+09, device='cuda:0')
c= tensor(4.1001e+09, device='cuda:0')
c= tensor(4.1254e+09, device='cuda:0')
c= tensor(4.1260e+09, device='cuda:0')
c= tensor(4.1277e+09, device='cuda:0')
c= tensor(4.1394e+09, device='cuda:0')
c= tensor(4.1644e+09, device='cuda:0')
c= tensor(4.1646e+09, device='cuda:0')
c= tensor(4.2928e+09, device='cuda:0')
c= tensor(4.4704e+09, device='cuda:0')
c= tensor(4.4898e+09, device='cuda:0')
c= tensor(4.4956e+09, device='cuda:0')
c= tensor(4.5012e+09, device='cuda:0')
c= tensor(4.5017e+09, device='cuda:0')
c= tensor(4.5018e+09, device='cuda:0')
c= tensor(4.5023e+09, device='cuda:0')
c= tensor(4.5080e+09, device='cuda:0')
c= tensor(4.5183e+09, device='cuda:0')
c= tensor(4.5614e+09, device='cuda:0')
c= tensor(4.5704e+09, device='cuda:0')
c= tensor(4.5747e+09, device='cuda:0')
c= tensor(4.5752e+09, device='cuda:0')
c= tensor(4.6058e+09, device='cuda:0')
c= tensor(4.6058e+09, device='cuda:0')
c= tensor(4.6060e+09, device='cuda:0')
c= tensor(4.6163e+09, device='cuda:0')
c= tensor(4.6201e+09, device='cuda:0')
c= tensor(4.6201e+09, device='cuda:0')
c= tensor(4.6206e+09, device='cuda:0')
c= tensor(4.8032e+09, device='cuda:0')
c= tensor(4.8035e+09, device='cuda:0')
c= tensor(4.8091e+09, device='cuda:0')
c= tensor(4.8091e+09, device='cuda:0')
c= tensor(4.8091e+09, device='cuda:0')
c= tensor(4.8091e+09, device='cuda:0')
c= tensor(4.8094e+09, device='cuda:0')
c= tensor(4.8100e+09, device='cuda:0')
c= tensor(4.8196e+09, device='cuda:0')
c= tensor(4.8196e+09, device='cuda:0')
c= tensor(4.8285e+09, device='cuda:0')
c= tensor(4.8289e+09, device='cuda:0')
c= tensor(4.8313e+09, device='cuda:0')
c= tensor(4.8315e+09, device='cuda:0')
c= tensor(4.8460e+09, device='cuda:0')
c= tensor(4.8461e+09, device='cuda:0')
c= tensor(4.8475e+09, device='cuda:0')
c= tensor(4.8484e+09, device='cuda:0')
c= tensor(4.8495e+09, device='cuda:0')
c= tensor(4.8522e+09, device='cuda:0')
c= tensor(5.0112e+09, device='cuda:0')
c= tensor(5.0114e+09, device='cuda:0')
c= tensor(5.0115e+09, device='cuda:0')
c= tensor(5.0134e+09, device='cuda:0')
c= tensor(5.0139e+09, device='cuda:0')
c= tensor(5.0995e+09, device='cuda:0')
c= tensor(5.0996e+09, device='cuda:0')
c= tensor(5.1068e+09, device='cuda:0')
c= tensor(5.1442e+09, device='cuda:0')
c= tensor(5.1445e+09, device='cuda:0')
c= tensor(5.1733e+09, device='cuda:0')
c= tensor(5.1742e+09, device='cuda:0')
c= tensor(5.2786e+09, device='cuda:0')
c= tensor(5.2790e+09, device='cuda:0')
c= tensor(5.2794e+09, device='cuda:0')
c= tensor(5.2796e+09, device='cuda:0')
c= tensor(5.2797e+09, device='cuda:0')
c= tensor(5.2797e+09, device='cuda:0')
c= tensor(5.2855e+09, device='cuda:0')
c= tensor(5.2870e+09, device='cuda:0')
c= tensor(5.3016e+09, device='cuda:0')
c= tensor(5.3017e+09, device='cuda:0')
c= tensor(5.3018e+09, device='cuda:0')
c= tensor(5.3021e+09, device='cuda:0')
c= tensor(5.3186e+09, device='cuda:0')
c= tensor(5.3336e+09, device='cuda:0')
c= tensor(5.3723e+09, device='cuda:0')
c= tensor(5.3736e+09, device='cuda:0')
c= tensor(5.3737e+09, device='cuda:0')
c= tensor(5.3737e+09, device='cuda:0')
c= tensor(5.3737e+09, device='cuda:0')
c= tensor(5.4327e+09, device='cuda:0')
c= tensor(5.4327e+09, device='cuda:0')
c= tensor(5.4331e+09, device='cuda:0')
c= tensor(5.4387e+09, device='cuda:0')
c= tensor(5.4396e+09, device='cuda:0')
c= tensor(5.4404e+09, device='cuda:0')
c= tensor(5.4404e+09, device='cuda:0')
c= tensor(5.4825e+09, device='cuda:0')
c= tensor(5.4846e+09, device='cuda:0')
c= tensor(5.4849e+09, device='cuda:0')
c= tensor(5.4852e+09, device='cuda:0')
c= tensor(5.5010e+09, device='cuda:0')
c= tensor(5.5043e+09, device='cuda:0')
c= tensor(5.6137e+09, device='cuda:0')
c= tensor(5.6199e+09, device='cuda:0')
c= tensor(5.6200e+09, device='cuda:0')
c= tensor(5.6209e+09, device='cuda:0')
c= tensor(5.6222e+09, device='cuda:0')
c= tensor(5.6225e+09, device='cuda:0')
c= tensor(5.6227e+09, device='cuda:0')
c= tensor(5.6227e+09, device='cuda:0')
c= tensor(5.6246e+09, device='cuda:0')
c= tensor(5.6281e+09, device='cuda:0')
c= tensor(5.6284e+09, device='cuda:0')
c= tensor(5.6286e+09, device='cuda:0')
c= tensor(5.6287e+09, device='cuda:0')
c= tensor(5.6306e+09, device='cuda:0')
c= tensor(5.6306e+09, device='cuda:0')
c= tensor(5.6310e+09, device='cuda:0')
c= tensor(5.6321e+09, device='cuda:0')
c= tensor(5.6325e+09, device='cuda:0')
c= tensor(5.6325e+09, device='cuda:0')
c= tensor(5.6326e+09, device='cuda:0')
c= tensor(5.6328e+09, device='cuda:0')
c= tensor(5.6447e+09, device='cuda:0')
c= tensor(5.6448e+09, device='cuda:0')
c= tensor(5.6448e+09, device='cuda:0')
c= tensor(5.6449e+09, device='cuda:0')
c= tensor(5.6578e+09, device='cuda:0')
c= tensor(5.8624e+09, device='cuda:0')
c= tensor(5.8666e+09, device='cuda:0')
c= tensor(5.8667e+09, device='cuda:0')
c= tensor(5.9055e+09, device='cuda:0')
c= tensor(5.9147e+09, device='cuda:0')
c= tensor(5.9147e+09, device='cuda:0')
c= tensor(5.9150e+09, device='cuda:0')
c= tensor(5.9159e+09, device='cuda:0')
c= tensor(5.9431e+09, device='cuda:0')
c= tensor(6.1980e+09, device='cuda:0')
c= tensor(6.2104e+09, device='cuda:0')
c= tensor(6.2123e+09, device='cuda:0')
c= tensor(6.2123e+09, device='cuda:0')
c= tensor(6.2124e+09, device='cuda:0')
c= tensor(6.2174e+09, device='cuda:0')
c= tensor(6.2176e+09, device='cuda:0')
c= tensor(6.2191e+09, device='cuda:0')
c= tensor(6.2300e+09, device='cuda:0')
c= tensor(6.3134e+09, device='cuda:0')
c= tensor(6.3135e+09, device='cuda:0')
c= tensor(6.3135e+09, device='cuda:0')
c= tensor(6.3142e+09, device='cuda:0')
c= tensor(6.3471e+09, device='cuda:0')
c= tensor(6.3485e+09, device='cuda:0')
c= tensor(6.3491e+09, device='cuda:0')
c= tensor(6.3491e+09, device='cuda:0')
c= tensor(6.3496e+09, device='cuda:0')
c= tensor(6.3497e+09, device='cuda:0')
c= tensor(6.3524e+09, device='cuda:0')
c= tensor(6.3525e+09, device='cuda:0')
c= tensor(6.3525e+09, device='cuda:0')
c= tensor(6.3526e+09, device='cuda:0')
c= tensor(6.3527e+09, device='cuda:0')
c= tensor(6.3530e+09, device='cuda:0')
c= tensor(6.3640e+09, device='cuda:0')
c= tensor(6.4246e+09, device='cuda:0')
c= tensor(6.4311e+09, device='cuda:0')
c= tensor(6.4513e+09, device='cuda:0')
c= tensor(6.4516e+09, device='cuda:0')
c= tensor(6.4519e+09, device='cuda:0')
c= tensor(6.4522e+09, device='cuda:0')
c= tensor(6.4599e+09, device='cuda:0')
c= tensor(6.4604e+09, device='cuda:0')
c= tensor(6.4654e+09, device='cuda:0')
c= tensor(6.4655e+09, device='cuda:0')
c= tensor(6.9539e+09, device='cuda:0')
c= tensor(6.9541e+09, device='cuda:0')
c= tensor(6.9561e+09, device='cuda:0')
c= tensor(6.9898e+09, device='cuda:0')
c= tensor(6.9909e+09, device='cuda:0')
c= tensor(6.9916e+09, device='cuda:0')
c= tensor(7.5327e+09, device='cuda:0')
c= tensor(7.5578e+09, device='cuda:0')
c= tensor(7.5620e+09, device='cuda:0')
c= tensor(7.5621e+09, device='cuda:0')
c= tensor(7.5631e+09, device='cuda:0')
c= tensor(7.5633e+09, device='cuda:0')
c= tensor(7.6006e+09, device='cuda:0')
c= tensor(7.9226e+09, device='cuda:0')
c= tensor(7.9413e+09, device='cuda:0')
c= tensor(7.9546e+09, device='cuda:0')
c= tensor(7.9610e+09, device='cuda:0')
c= tensor(7.9620e+09, device='cuda:0')
c= tensor(7.9628e+09, device='cuda:0')
c= tensor(7.9630e+09, device='cuda:0')
c= tensor(8.0608e+09, device='cuda:0')
c= tensor(8.1230e+09, device='cuda:0')
c= tensor(8.1268e+09, device='cuda:0')
c= tensor(9.2391e+09, device='cuda:0')
c= tensor(9.2695e+09, device='cuda:0')
c= tensor(9.2711e+09, device='cuda:0')
c= tensor(9.2716e+09, device='cuda:0')
c= tensor(9.2755e+09, device='cuda:0')
c= tensor(9.2835e+09, device='cuda:0')
c= tensor(9.2836e+09, device='cuda:0')
c= tensor(9.4043e+09, device='cuda:0')
c= tensor(9.4052e+09, device='cuda:0')
c= tensor(9.4064e+09, device='cuda:0')
c= tensor(9.4065e+09, device='cuda:0')
c= tensor(9.4068e+09, device='cuda:0')
c= tensor(9.4068e+09, device='cuda:0')
c= tensor(9.4068e+09, device='cuda:0')
c= tensor(9.4075e+09, device='cuda:0')
c= tensor(9.4090e+09, device='cuda:0')
c= tensor(1.3154e+10, device='cuda:0')
c= tensor(1.3158e+10, device='cuda:0')
c= tensor(1.3197e+10, device='cuda:0')
c= tensor(1.3198e+10, device='cuda:0')
c= tensor(1.3198e+10, device='cuda:0')
c= tensor(1.3198e+10, device='cuda:0')
c= tensor(1.3236e+10, device='cuda:0')
c= tensor(1.3240e+10, device='cuda:0')
c= tensor(1.3541e+10, device='cuda:0')
c= tensor(1.3541e+10, device='cuda:0')
c= tensor(1.3554e+10, device='cuda:0')
c= tensor(1.3555e+10, device='cuda:0')
c= tensor(1.3564e+10, device='cuda:0')
c= tensor(1.3582e+10, device='cuda:0')
c= tensor(1.3583e+10, device='cuda:0')
c= tensor(1.3583e+10, device='cuda:0')
c= tensor(1.3585e+10, device='cuda:0')
c= tensor(1.3588e+10, device='cuda:0')
c= tensor(1.3590e+10, device='cuda:0')
c= tensor(1.3711e+10, device='cuda:0')
c= tensor(1.3712e+10, device='cuda:0')
c= tensor(1.3716e+10, device='cuda:0')
c= tensor(1.3717e+10, device='cuda:0')
c= tensor(1.3724e+10, device='cuda:0')
c= tensor(1.3776e+10, device='cuda:0')
c= tensor(1.3778e+10, device='cuda:0')
c= tensor(1.3779e+10, device='cuda:0')
c= tensor(1.3801e+10, device='cuda:0')
c= tensor(1.3807e+10, device='cuda:0')
c= tensor(1.4050e+10, device='cuda:0')
c= tensor(1.4064e+10, device='cuda:0')
c= tensor(1.4065e+10, device='cuda:0')
c= tensor(1.4068e+10, device='cuda:0')
c= tensor(1.4096e+10, device='cuda:0')
c= tensor(1.4114e+10, device='cuda:0')
c= tensor(1.4115e+10, device='cuda:0')
c= tensor(1.4115e+10, device='cuda:0')
c= tensor(1.4116e+10, device='cuda:0')
c= tensor(1.4122e+10, device='cuda:0')
c= tensor(1.4140e+10, device='cuda:0')
c= tensor(1.4142e+10, device='cuda:0')
c= tensor(1.4142e+10, device='cuda:0')
c= tensor(1.4143e+10, device='cuda:0')
c= tensor(1.4146e+10, device='cuda:0')
c= tensor(1.4148e+10, device='cuda:0')
c= tensor(1.4148e+10, device='cuda:0')
c= tensor(1.4150e+10, device='cuda:0')
c= tensor(1.4152e+10, device='cuda:0')
c= tensor(1.4153e+10, device='cuda:0')
c= tensor(1.4153e+10, device='cuda:0')
c= tensor(1.4153e+10, device='cuda:0')
c= tensor(1.4175e+10, device='cuda:0')
c= tensor(1.4176e+10, device='cuda:0')
c= tensor(1.4180e+10, device='cuda:0')
c= tensor(1.4180e+10, device='cuda:0')
c= tensor(1.4180e+10, device='cuda:0')
c= tensor(1.4183e+10, device='cuda:0')
c= tensor(1.4190e+10, device='cuda:0')
c= tensor(1.4190e+10, device='cuda:0')
c= tensor(1.4190e+10, device='cuda:0')
c= tensor(1.4190e+10, device='cuda:0')
c= tensor(1.4191e+10, device='cuda:0')
c= tensor(1.4192e+10, device='cuda:0')
c= tensor(1.4200e+10, device='cuda:0')
c= tensor(1.4200e+10, device='cuda:0')
c= tensor(1.4200e+10, device='cuda:0')
c= tensor(1.4204e+10, device='cuda:0')
c= tensor(1.4204e+10, device='cuda:0')
c= tensor(1.4229e+10, device='cuda:0')
c= tensor(1.4229e+10, device='cuda:0')
c= tensor(1.4242e+10, device='cuda:0')
c= tensor(1.4250e+10, device='cuda:0')
c= tensor(1.4251e+10, device='cuda:0')
c= tensor(1.4259e+10, device='cuda:0')
c= tensor(1.4264e+10, device='cuda:0')
c= tensor(1.4264e+10, device='cuda:0')
c= tensor(1.4267e+10, device='cuda:0')
c= tensor(1.4267e+10, device='cuda:0')
c= tensor(1.4287e+10, device='cuda:0')
c= tensor(1.4292e+10, device='cuda:0')
c= tensor(1.4294e+10, device='cuda:0')
c= tensor(1.4297e+10, device='cuda:0')
c= tensor(1.4299e+10, device='cuda:0')
c= tensor(1.4305e+10, device='cuda:0')
c= tensor(1.4306e+10, device='cuda:0')
c= tensor(1.4306e+10, device='cuda:0')
c= tensor(1.4362e+10, device='cuda:0')
c= tensor(1.4515e+10, device='cuda:0')
c= tensor(1.4515e+10, device='cuda:0')
c= tensor(1.4515e+10, device='cuda:0')
c= tensor(1.4516e+10, device='cuda:0')
c= tensor(1.4539e+10, device='cuda:0')
c= tensor(1.4540e+10, device='cuda:0')
c= tensor(1.4540e+10, device='cuda:0')
c= tensor(1.4542e+10, device='cuda:0')
c= tensor(1.4550e+10, device='cuda:0')
c= tensor(1.4550e+10, device='cuda:0')
c= tensor(1.4552e+10, device='cuda:0')
c= tensor(1.4557e+10, device='cuda:0')
time to make c is 6.749393463134766
time for making loss is 6.749516010284424
p0 True
it  0 : 4495693312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
6969487360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 17% |
memory (bytes)
6969499648
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3864723500.0
relative error loss 0.26549158
shape of L is 
torch.Size([])
memory (bytes)
6991769600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 17% |
memory (bytes)
6991769600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% | 17% |
error is  3853401000.0
relative error loss 0.26471376
shape of L is 
torch.Size([])
memory (bytes)
6994694144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 17% |
memory (bytes)
6994694144
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3808048000.0
relative error loss 0.2615982
shape of L is 
torch.Size([])
memory (bytes)
6996815872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 17% |
memory (bytes)
6996815872
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3666342000.0
relative error loss 0.2518635
shape of L is 
torch.Size([])
memory (bytes)
6998933504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 17% |
memory (bytes)
6998933504
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3623902200.0
relative error loss 0.24894808
shape of L is 
torch.Size([])
memory (bytes)
7001075712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 17% |
memory (bytes)
7001075712
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3597199400.0
relative error loss 0.2471137
shape of L is 
torch.Size([])
memory (bytes)
7003201536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 17% |
memory (bytes)
7003201536
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3578350600.0
relative error loss 0.24581885
shape of L is 
torch.Size([])
memory (bytes)
7005343744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 36% | 17% |
memory (bytes)
7005343744
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3561780200.0
relative error loss 0.24468054
shape of L is 
torch.Size([])
memory (bytes)
7007465472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 17% |
memory (bytes)
7007465472
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3542870000.0
relative error loss 0.24338149
shape of L is 
torch.Size([])
memory (bytes)
7009587200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 17% |
memory (bytes)
7009587200
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3528934400.0
relative error loss 0.24242416
time to take a step is 221.28116631507874
it  1 : 5218724352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
7011708928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 17% |
memory (bytes)
7011708928
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3528934400.0
relative error loss 0.24242416
shape of L is 
torch.Size([])
memory (bytes)
7013838848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 17% |
memory (bytes)
7013838848
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3516745700.0
relative error loss 0.24158685
shape of L is 
torch.Size([])
memory (bytes)
7015960576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 17% |
memory (bytes)
7015960576
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3505581000.0
relative error loss 0.24081987
shape of L is 
torch.Size([])
memory (bytes)
7018082304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 17% |
memory (bytes)
7018082304
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3499687000.0
relative error loss 0.24041498
shape of L is 
torch.Size([])
memory (bytes)
7020208128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 17% |
memory (bytes)
7020208128
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3494413300.0
relative error loss 0.2400527
shape of L is 
torch.Size([])
memory (bytes)
7022342144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 17% |
memory (bytes)
7022342144
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3488220200.0
relative error loss 0.23962724
shape of L is 
torch.Size([])
memory (bytes)
7024459776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 17% |
memory (bytes)
7024459776
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3483417600.0
relative error loss 0.23929733
shape of L is 
torch.Size([])
memory (bytes)
7026585600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 17% |
memory (bytes)
7026585600
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3478125600.0
relative error loss 0.23893379
shape of L is 
torch.Size([])
memory (bytes)
7028719616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 17% |
memory (bytes)
7028719616
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3472291800.0
relative error loss 0.23853303
shape of L is 
torch.Size([])
memory (bytes)
7030837248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 17% |
memory (bytes)
7030837248
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3469780000.0
relative error loss 0.23836048
time to take a step is 215.58993244171143
it  2 : 5218724352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
7032958976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 17% |
memory (bytes)
7032967168
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3469780000.0
relative error loss 0.23836048
shape of L is 
torch.Size([])
memory (bytes)
7035084800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 17% |
memory (bytes)
7035084800
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3466074000.0
relative error loss 0.2381059
shape of L is 
torch.Size([])
memory (bytes)
7037214720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 17% |
memory (bytes)
7037214720
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3463416800.0
relative error loss 0.23792335
shape of L is 
torch.Size([])
memory (bytes)
7039336448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 36% | 17% |
memory (bytes)
7039336448
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3460796400.0
relative error loss 0.23774335
shape of L is 
torch.Size([])
memory (bytes)
7041449984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 17% |
memory (bytes)
7041449984
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3457386500.0
relative error loss 0.2375091
shape of L is 
torch.Size([])
memory (bytes)
7043571712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 17% |
memory (bytes)
7043571712
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3455052800.0
relative error loss 0.23734878
shape of L is 
torch.Size([])
memory (bytes)
7045705728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 17% |
memory (bytes)
7045705728
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3452765200.0
relative error loss 0.23719163
shape of L is 
torch.Size([])
memory (bytes)
7047831552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 17% |
memory (bytes)
7047831552
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3450128400.0
relative error loss 0.2370105
shape of L is 
torch.Size([])
memory (bytes)
7049957376
| ID | GPU | MEM |
------------------
|  0 | 17% |  0% |
|  1 | 36% | 17% |
memory (bytes)
7049957376
| ID | GPU | MEM |
------------------
|  0 | 15% |  0% |
|  1 | 90% | 17% |
error is  3448508400.0
relative error loss 0.23689921
shape of L is 
torch.Size([])
memory (bytes)
7052218368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 17% |
memory (bytes)
7052218368
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3446626300.0
relative error loss 0.23676991
time to take a step is 212.19872450828552
it  3 : 5218724352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
7055413248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 17% |
memory (bytes)
7055413248
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3446626300.0
relative error loss 0.23676991
shape of L is 
torch.Size([])
memory (bytes)
7058423808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 17% |
memory (bytes)
7058612224
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3444447200.0
relative error loss 0.23662022
shape of L is 
torch.Size([])
memory (bytes)
7061819392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 17% |
memory (bytes)
7061819392
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3442591700.0
relative error loss 0.23649275
shape of L is 
torch.Size([])
memory (bytes)
7065018368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 17% |
memory (bytes)
7065018368
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3441260500.0
relative error loss 0.2364013
shape of L is 
torch.Size([])
memory (bytes)
7068217344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 17% |
memory (bytes)
7068217344
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3439696000.0
relative error loss 0.23629382
shape of L is 
torch.Size([])
memory (bytes)
7071428608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 17% |
memory (bytes)
7071428608
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3437850600.0
relative error loss 0.23616706
shape of L is 
torch.Size([])
memory (bytes)
7074631680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 36% | 17% |
memory (bytes)
7074631680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 17% |
error is  3436513300.0
relative error loss 0.2360752
shape of L is 
torch.Size([])
memory (bytes)
7077830656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 17% |
memory (bytes)
7077830656
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3435327500.0
relative error loss 0.23599373
shape of L is 
torch.Size([])
memory (bytes)
7081033728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 17% |
memory (bytes)
7081033728
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3434127400.0
relative error loss 0.23591128
shape of L is 
torch.Size([])
memory (bytes)
7084236800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 17% |
memory (bytes)
7084236800
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3432426500.0
relative error loss 0.23579444
time to take a step is 217.74656105041504
it  4 : 5218724352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
7087431680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 17% |
memory (bytes)
7087431680
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3432426500.0
relative error loss 0.23579444
shape of L is 
torch.Size([])
memory (bytes)
7090630656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 17% |
memory (bytes)
7090630656
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3431775200.0
relative error loss 0.2357497
shape of L is 
torch.Size([])
memory (bytes)
7093788672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 17% |
memory (bytes)
7093788672
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3430109200.0
relative error loss 0.23563525
shape of L is 
torch.Size([])
memory (bytes)
7097040896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 36% | 17% |
memory (bytes)
7097040896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% | 17% |
error is  3429325800.0
relative error loss 0.23558143
shape of L is 
torch.Size([])
memory (bytes)
7100166144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 17% |
memory (bytes)
7100260352
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3428347000.0
relative error loss 0.23551418
shape of L is 
torch.Size([])
memory (bytes)
7103463424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 17% |
memory (bytes)
7103463424
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3426998300.0
relative error loss 0.23542154
shape of L is 
torch.Size([])
memory (bytes)
7106539520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 17% |
memory (bytes)
7106654208
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3425845200.0
relative error loss 0.23534234
shape of L is 
torch.Size([])
memory (bytes)
7109849088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% | 17% |
memory (bytes)
7109857280
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3425014800.0
relative error loss 0.23528528
shape of L is 
torch.Size([])
memory (bytes)
7113048064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 17% |
memory (bytes)
7113048064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% | 17% |
error is  3424177200.0
relative error loss 0.23522773
shape of L is 
torch.Size([])
memory (bytes)
7116251136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 34% | 17% |
memory (bytes)
7116251136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  3423233000.0
relative error loss 0.23516288
time to take a step is 215.35309886932373
it  5 : 5218724352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
7119413248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 17% |
memory (bytes)
7119413248
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3423233000.0
relative error loss 0.23516288
shape of L is 
torch.Size([])
memory (bytes)
7122661376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% | 17% |
memory (bytes)
7122661376
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3422365700.0
relative error loss 0.2351033
shape of L is 
torch.Size([])
memory (bytes)
7125745664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 17% |
memory (bytes)
7125860352
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3421706200.0
relative error loss 0.235058
shape of L is 
torch.Size([])
memory (bytes)
7129055232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 36% | 17% |
memory (bytes)
7129055232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% | 17% |
error is  3420460000.0
relative error loss 0.23497239
shape of L is 
torch.Size([])
memory (bytes)
7132250112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 17% |
memory (bytes)
7132250112
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3420153900.0
relative error loss 0.23495136
shape of L is 
torch.Size([])
memory (bytes)
7135457280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 36% | 17% |
memory (bytes)
7135457280
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3419243500.0
relative error loss 0.23488882
shape of L is 
torch.Size([])
memory (bytes)
7138656256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 37% | 17% |
memory (bytes)
7138656256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% | 17% |
error is  3418753000.0
relative error loss 0.23485513
shape of L is 
torch.Size([])
memory (bytes)
7141855232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 17% |
memory (bytes)
7141859328
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3418114000.0
relative error loss 0.23481123
shape of L is 
torch.Size([])
memory (bytes)
7145050112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 17% |
memory (bytes)
7145050112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  3417179100.0
relative error loss 0.23474701
shape of L is 
torch.Size([])
memory (bytes)
7148068864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 17% |
memory (bytes)
7148249088
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3417640000.0
relative error loss 0.23477866
shape of L is 
torch.Size([])
memory (bytes)
7151452160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 17% |
memory (bytes)
7151452160
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3416694800.0
relative error loss 0.23471373
time to take a step is 234.2043890953064
it  6 : 5218724864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
7154544640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 17% |
memory (bytes)
7154659328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% | 17% |
error is  3416694800.0
relative error loss 0.23471373
shape of L is 
torch.Size([])
memory (bytes)
7157862400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 34% | 17% |
memory (bytes)
7157862400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% | 17% |
error is  3415967700.0
relative error loss 0.23466378
shape of L is 
torch.Size([])
memory (bytes)
7161077760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 17% |
memory (bytes)
7161077760
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3415378000.0
relative error loss 0.23462327
shape of L is 
torch.Size([])
memory (bytes)
7164276736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 17% |
memory (bytes)
7164276736
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3414556700.0
relative error loss 0.23456685
shape of L is 
torch.Size([])
memory (bytes)
7167467520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 17% |
memory (bytes)
7167467520
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3413928000.0
relative error loss 0.23452365
shape of L is 
torch.Size([])
memory (bytes)
7170674688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 34% | 17% |
memory (bytes)
7170674688
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3413468200.0
relative error loss 0.23449208
shape of L is 
torch.Size([])
memory (bytes)
7173873664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 17% |
memory (bytes)
7173873664
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3412850700.0
relative error loss 0.23444965
shape of L is 
torch.Size([])
memory (bytes)
7177089024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 33% | 17% |
memory (bytes)
7177089024
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3412751400.0
relative error loss 0.23444283
shape of L is 
torch.Size([])
memory (bytes)
7180292096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 17% |
memory (bytes)
7180292096
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3412223000.0
relative error loss 0.23440653
shape of L is 
torch.Size([])
memory (bytes)
7183491072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 17% |
memory (bytes)
7183491072
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3412015000.0
relative error loss 0.23439226
time to take a step is 215.93344736099243
it  7 : 5218724352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
7186698240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 17% |
memory (bytes)
7186698240
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3412015000.0
relative error loss 0.23439226
shape of L is 
torch.Size([])
memory (bytes)
7189889024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 17% |
memory (bytes)
7189889024
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3411774500.0
relative error loss 0.23437573
shape of L is 
torch.Size([])
memory (bytes)
7193088000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 17% |
memory (bytes)
7193088000
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3411434500.0
relative error loss 0.23435237
shape of L is 
torch.Size([])
memory (bytes)
7196291072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 17% |
memory (bytes)
7196291072
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3411823600.0
relative error loss 0.2343791
shape of L is 
torch.Size([])
memory (bytes)
7199490048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 30% | 17% |
memory (bytes)
7199490048
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3411224600.0
relative error loss 0.23433796
shape of L is 
torch.Size([])
memory (bytes)
7202693120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 36% | 17% |
memory (bytes)
7202693120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 17% |
error is  3410918400.0
relative error loss 0.23431692
shape of L is 
torch.Size([])
memory (bytes)
7205896192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 34% | 17% |
memory (bytes)
7205896192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% | 17% |
error is  3410586600.0
relative error loss 0.23429413
shape of L is 
torch.Size([])
memory (bytes)
7209091072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 17% |
memory (bytes)
7209091072
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3410047000.0
relative error loss 0.23425706
shape of L is 
torch.Size([])
memory (bytes)
7212298240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 36% | 17% |
memory (bytes)
7212298240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 17% |
error is  3409837000.0
relative error loss 0.23424263
shape of L is 
torch.Size([])
memory (bytes)
7215497216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 17% |
memory (bytes)
7215497216
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3409297400.0
relative error loss 0.23420556
time to take a step is 215.1229109764099
it  8 : 5218724352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
7218704384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 17% |
memory (bytes)
7218704384
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3409297400.0
relative error loss 0.23420556
shape of L is 
torch.Size([])
memory (bytes)
7221903360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 32% | 17% |
memory (bytes)
7221903360
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3408952300.0
relative error loss 0.23418185
shape of L is 
torch.Size([])
memory (bytes)
7225106432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 17% |
memory (bytes)
7225106432
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3408566300.0
relative error loss 0.23415533
shape of L is 
torch.Size([])
memory (bytes)
7228321792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 17% |
memory (bytes)
7228321792
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3408136200.0
relative error loss 0.2341258
shape of L is 
torch.Size([])
memory (bytes)
7231426560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 17% |
memory (bytes)
7231520768
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3407559700.0
relative error loss 0.23408619
shape of L is 
torch.Size([])
memory (bytes)
7234723840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 17% |
memory (bytes)
7234723840
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3407218700.0
relative error loss 0.23406276
shape of L is 
torch.Size([])
memory (bytes)
7237922816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 17% |
memory (bytes)
7237922816
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3406858200.0
relative error loss 0.234038
shape of L is 
torch.Size([])
memory (bytes)
7241121792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 17% |
memory (bytes)
7241121792
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3406293000.0
relative error loss 0.23399916
shape of L is 
torch.Size([])
memory (bytes)
7244324864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 17% |
memory (bytes)
7244324864
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3405789200.0
relative error loss 0.23396456
shape of L is 
torch.Size([])
memory (bytes)
7247536128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 17% |
memory (bytes)
7247536128
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3405470700.0
relative error loss 0.23394269
time to take a step is 217.73099088668823
it  9 : 5218724352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
7250731008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 17% |
memory (bytes)
7250731008
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3405470700.0
relative error loss 0.23394269
shape of L is 
torch.Size([])
memory (bytes)
7253938176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 17% |
memory (bytes)
7253938176
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3405264000.0
relative error loss 0.23392847
shape of L is 
torch.Size([])
memory (bytes)
7257137152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 34% | 17% |
memory (bytes)
7257137152
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3404458000.0
relative error loss 0.23387311
shape of L is 
torch.Size([])
memory (bytes)
7260348416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 17% |
memory (bytes)
7260348416
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3404609500.0
relative error loss 0.23388352
shape of L is 
torch.Size([])
memory (bytes)
7263559680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 17% |
memory (bytes)
7263559680
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3404157000.0
relative error loss 0.23385243
shape of L is 
torch.Size([])
memory (bytes)
7266750464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 17% |
memory (bytes)
7266750464
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3403901000.0
relative error loss 0.23383485
shape of L is 
torch.Size([])
memory (bytes)
7269941248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 17% |
memory (bytes)
7269941248
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3403380700.0
relative error loss 0.23379911
shape of L is 
torch.Size([])
memory (bytes)
7273136128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 17% |
memory (bytes)
7273136128
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3403028500.0
relative error loss 0.23377492
shape of L is 
torch.Size([])
memory (bytes)
7276343296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 17% |
memory (bytes)
7276343296
| ID | GPU  | MEM |
-------------------
|  0 |   2% |  0% |
|  1 | 100% | 17% |
error is  3402846200.0
relative error loss 0.23376238
shape of L is 
torch.Size([])
memory (bytes)
7279554560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 17% |
memory (bytes)
7279554560
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3402395600.0
relative error loss 0.23373143
time to take a step is 217.06537532806396
it  10 : 5218724352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
7282765824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 17% |
memory (bytes)
7282765824
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3402395600.0
relative error loss 0.23373143
shape of L is 
torch.Size([])
memory (bytes)
7285956608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 33% | 17% |
memory (bytes)
7285956608
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3402072000.0
relative error loss 0.2337092
shape of L is 
torch.Size([])
memory (bytes)
7289163776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 17% |
memory (bytes)
7289163776
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3401794600.0
relative error loss 0.23369014
shape of L is 
torch.Size([])
memory (bytes)
7292366848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 17% |
memory (bytes)
7292366848
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3401408500.0
relative error loss 0.23366362
shape of L is 
torch.Size([])
memory (bytes)
7295574016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 34% | 17% |
memory (bytes)
7295574016
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3401030700.0
relative error loss 0.23363766
shape of L is 
torch.Size([])
memory (bytes)
7298785280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 17% |
memory (bytes)
7298785280
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3400788000.0
relative error loss 0.233621
shape of L is 
torch.Size([])
memory (bytes)
7301992448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 17% |
memory (bytes)
7301992448
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3400375300.0
relative error loss 0.23359264
shape of L is 
torch.Size([])
memory (bytes)
7305183232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 17% |
memory (bytes)
7305183232
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3399955500.0
relative error loss 0.23356381
shape of L is 
torch.Size([])
memory (bytes)
7308386304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 17% |
memory (bytes)
7308386304
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3399688200.0
relative error loss 0.23354544
shape of L is 
torch.Size([])
memory (bytes)
7311589376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 17% |
memory (bytes)
7311589376
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3399323600.0
relative error loss 0.2335204
time to take a step is 219.7302122116089
it  11 : 5218724352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
7314792448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 17% |
memory (bytes)
7314792448
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3399323600.0
relative error loss 0.2335204
shape of L is 
torch.Size([])
memory (bytes)
7317852160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 17% |
memory (bytes)
7317987328
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3399050200.0
relative error loss 0.23350161
shape of L is 
torch.Size([])
memory (bytes)
7321186304
| ID | GPU | MEM |
------------------
|  0 |  5% |  0% |
|  1 | 15% | 17% |
memory (bytes)
7321186304
| ID | GPU  | MEM |
-------------------
|  0 |   3% |  0% |
|  1 | 100% | 17% |
error is  3398768600.0
relative error loss 0.23348227
shape of L is 
torch.Size([])
memory (bytes)
7324397568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 37% | 17% |
memory (bytes)
7324397568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  3398245400.0
relative error loss 0.23344633
shape of L is 
torch.Size([])
memory (bytes)
7327596544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 17% |
memory (bytes)
7327596544
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3398229000.0
relative error loss 0.2334452
shape of L is 
torch.Size([])
memory (bytes)
7330799616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 17% |
memory (bytes)
7330799616
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3398041600.0
relative error loss 0.23343232
shape of L is 
torch.Size([])
memory (bytes)
7333998592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 17% |
memory (bytes)
7333998592
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3397906400.0
relative error loss 0.23342304
shape of L is 
torch.Size([])
memory (bytes)
7337193472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 17% |
memory (bytes)
7337193472
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3397437400.0
relative error loss 0.23339082
shape of L is 
torch.Size([])
memory (bytes)
7340388352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 17% |
memory (bytes)
7340388352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% | 17% |
error is  3397416000.0
relative error loss 0.23338935
shape of L is 
torch.Size([])
memory (bytes)
7343472640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 17% |
memory (bytes)
7343587328
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3397247000.0
relative error loss 0.23337774
time to take a step is 220.87170219421387
it  12 : 5218724864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
7346790400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 17% |
memory (bytes)
7346790400
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3397247000.0
relative error loss 0.23337774
shape of L is 
torch.Size([])
memory (bytes)
7349837824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 17% |
memory (bytes)
7350009856
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3397024800.0
relative error loss 0.23336248
shape of L is 
torch.Size([])
memory (bytes)
7353225216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 17% |
memory (bytes)
7353225216
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3396761600.0
relative error loss 0.2333444
shape of L is 
torch.Size([])
memory (bytes)
7356276736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 17% |
memory (bytes)
7356432384
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3396603000.0
relative error loss 0.2333335
shape of L is 
torch.Size([])
memory (bytes)
7359639552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 17% |
memory (bytes)
7359639552
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3396383700.0
relative error loss 0.23331845
shape of L is 
torch.Size([])
memory (bytes)
7362846720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 17% |
memory (bytes)
7362846720
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3396211700.0
relative error loss 0.23330662
shape of L is 
torch.Size([])
memory (bytes)
7366053888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 17% |
memory (bytes)
7366053888
| ID | GPU  | MEM |
-------------------
|  0 |   2% |  0% |
|  1 | 100% | 17% |
error is  3396032500.0
relative error loss 0.23329431
shape of L is 
torch.Size([])
memory (bytes)
7369265152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 17% |
memory (bytes)
7369265152
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3396351000.0
relative error loss 0.23331618
shape of L is 
torch.Size([])
memory (bytes)
7372464128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 17% |
memory (bytes)
7372464128
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3395857400.0
relative error loss 0.23328228
shape of L is 
torch.Size([])
memory (bytes)
7375654912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 17% |
memory (bytes)
7375654912
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3395560400.0
relative error loss 0.23326188
time to take a step is 220.21059131622314
it  13 : 5218724352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
7378853888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 17% |
memory (bytes)
7378853888
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3395560400.0
relative error loss 0.23326188
shape of L is 
torch.Size([])
memory (bytes)
7382061056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 17% |
memory (bytes)
7382061056
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3395309600.0
relative error loss 0.23324464
shape of L is 
torch.Size([])
memory (bytes)
7385264128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 17% |
memory (bytes)
7385264128
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3394915300.0
relative error loss 0.23321757
shape of L is 
torch.Size([])
memory (bytes)
7388471296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 36% | 17% |
memory (bytes)
7388471296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% | 17% |
error is  3394715600.0
relative error loss 0.23320384
shape of L is 
torch.Size([])
memory (bytes)
7391641600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 17% |
memory (bytes)
7391682560
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3394354200.0
relative error loss 0.23317902
shape of L is 
torch.Size([])
memory (bytes)
7394889728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 17% |
memory (bytes)
7394889728
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3394131000.0
relative error loss 0.23316368
shape of L is 
torch.Size([])
memory (bytes)
7397957632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 17% |
memory (bytes)
7398105088
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3393989600.0
relative error loss 0.23315397
shape of L is 
torch.Size([])
memory (bytes)
7401295872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 36% | 17% |
memory (bytes)
7401295872
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3393732600.0
relative error loss 0.23313631
shape of L is 
torch.Size([])
memory (bytes)
7404425216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 17% |
memory (bytes)
7404507136
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3393568800.0
relative error loss 0.23312506
shape of L is 
torch.Size([])
memory (bytes)
7407722496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 17% |
memory (bytes)
7407722496
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3393438700.0
relative error loss 0.23311614
time to take a step is 214.81141448020935
it  14 : 5218724352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
7410913280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 17% |
memory (bytes)
7410913280
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3393438700.0
relative error loss 0.23311614
shape of L is 
torch.Size([])
memory (bytes)
7414112256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 17% |
memory (bytes)
7414112256
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3393312800.0
relative error loss 0.23310748
shape of L is 
torch.Size([])
memory (bytes)
7417327616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 17% |
memory (bytes)
7417327616
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3393135600.0
relative error loss 0.2330953
shape of L is 
torch.Size([])
memory (bytes)
7420522496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 17% |
memory (bytes)
7420522496
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3393019000.0
relative error loss 0.23308729
shape of L is 
torch.Size([])
memory (bytes)
7423729664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 30% | 17% |
memory (bytes)
7423729664
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3392680000.0
relative error loss 0.23306401
shape of L is 
torch.Size([])
memory (bytes)
7426936832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 17% |
memory (bytes)
7426936832
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3392484400.0
relative error loss 0.23305057
shape of L is 
torch.Size([])
memory (bytes)
7430152192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 17% |
memory (bytes)
7430152192
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3392308200.0
relative error loss 0.23303847
shape of L is 
torch.Size([])
memory (bytes)
7433338880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 17% |
memory (bytes)
7433338880
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3391992800.0
relative error loss 0.2330168
shape of L is 
torch.Size([])
memory (bytes)
7436546048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 17% |
memory (bytes)
7436546048
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3392435200.0
relative error loss 0.23304719
shape of L is 
torch.Size([])
memory (bytes)
7439740928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 17% |
memory (bytes)
7439740928
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3391850500.0
relative error loss 0.23300703
time to take a step is 220.81129932403564
sum tnnu_Z after tensor(23118812., device='cuda:0')
shape of features
(7122,)
shape of features
(7122,)
number of orig particles 28488
number of new particles after remove low mass 26709
tnuZ shape should be parts x labs
torch.Size([28488, 702])
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
error without small mass is  3864402000.0
relative error without small mass is  0.2654695
nnu_Z shape should be number of particles by maxV
(28488, 702)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
shape of features
(28488,)
Tue Jan 31 21:54:04 EST 2023
