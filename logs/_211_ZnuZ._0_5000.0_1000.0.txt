Tue Jan 31 19:20:55 EST 2023
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  0% |  0% |
numbers of X: 41932615
numbers of Z: 20167
shape of features
(20167,)
shape of features
(20167,)
ZX	Vol	Parts	Cubes	Eps
Z	0.018942403939806004	20167	20.167	0.09793349757601257
X	0.01621685558750758	2944	2.944	0.17660769182032943
X	0.01614700426303062	22776	22.776	0.0891671396346092
X	0.016148864362141756	2600	2.6	0.18381863901459752
X	0.01833008999426837	21609	21.609	0.09466221069857728
X	0.017284641861606005	15783	15.783	0.10307585430017922
X	0.01636949469702037	65982	65.982	0.06283523823915033
X	0.01633690590747405	48310	48.31	0.06966975074036164
X	0.01638732381608566	37985	37.985	0.07556116320541305
X	0.01618446097260625	15183	15.183	0.10215200202218824
X	0.016294779040293014	65667	65.667	0.06283965131920836
X	0.016020369040012573	10127	10.127	0.11651913056676906
X	0.016089995220276418	68455	68.455	0.06171398350540724
X	0.017406244318361096	8468	8.468	0.12714757314506153
X	0.018282913032031157	394832	394.832	0.03590963712943701
X	0.01634920099758842	25491	25.491	0.0862389023239368
X	0.016945771305063903	58278	58.278	0.06624995190010692
X	0.016388983805816028	60682	60.682	0.0646394094423437
X	0.0162334817850786	22030	22.03	0.09032314692030832
X	0.016282427540594932	125396	125.396	0.05063822688801141
X	0.018269461516076432	88227	88.227	0.05916180444908263
X	0.016180407404425775	100278	100.278	0.05444123011291996
X	0.017785445535031787	196075	196.075	0.04493114731529528
X	0.016372870346810082	12147	12.147	0.110463448494767
X	0.016302521846277312	47415	47.415	0.07005616270081245
X	0.016126623678701796	2602	2.602	0.18368712626991443
X	0.016341381842523723	58416	58.416	0.06540119054379512
X	0.016297987191749724	58711	58.711	0.0652336228172665
X	0.017266462108735628	10426	10.426	0.1183119320324448
X	0.015927791332573576	54429	54.429	0.06639087625915173
X	0.01731336422064213	1179614	1179.614	0.024483895415027126
X	0.016027854093882223	18028	18.028	0.09615590143741892
X	0.01833725886823519	710847	710.847	0.029547410581598002
X	0.016166908375140687	26634	26.634	0.08467012362181692
X	0.017319958070514275	12937	12.937	0.11021425077802181
X	0.01822091289449878	44134	44.134	0.0744615856854084
X	0.016571647989959033	185302	185.302	0.04471932067287162
X	0.016378653274757293	78062	78.062	0.0594218504172528
X	0.015095598883181854	984	0.984	0.2484763637984366
X	0.016076806135927102	4054	4.054	0.15828460501348507
X	0.016176144106414176	2393	2.393	0.18907935096239822
X	0.017871042462077828	3354	3.354	0.17466086613822845
X	0.015442791279830946	3574	3.574	0.16287599601200775
X	0.015208874488950998	1098	1.098	0.24015865644129508
X	0.016267258261457068	3672	3.672	0.16423683343601772
X	0.01605494543035284	1327	1.327	0.2295686799432603
X	0.016253861810345986	1009	1.009	0.2525545122888265
X	0.016020977655947937	1966	1.966	0.2012342195553035
X	0.016316851153915946	5502	5.502	0.1436719287019357
X	0.015625896128414227	4250	4.25	0.15434243026376906
X	0.016193125876547403	10090	10.09	0.11707922578003348
X	0.016287109084732437	13122	13.122	0.10746853382215288
X	0.015810034944621604	1718	1.718	0.20955750710912166
X	0.015860852198074468	5738	5.738	0.14034232209585373
X	0.015936666724721583	4285	4.285	0.15493476868981804
X	0.01613773992749232	3827	3.827	0.16155785682367962
X	0.015969162981838973	8705	8.705	0.12241588950825014
X	0.01560228538217533	2297	2.297	0.18938361725404484
X	0.01593536335660913	6387	6.387	0.1356297677787261
X	0.01568328417969093	6198	6.198	0.13626839686281128
X	0.01568709697200487	2867	2.867	0.1762131424312007
X	0.01621753014211285	3830	3.83	0.1617814213364124
X	0.01613561728574889	2374	2.374	0.18942397967494562
X	0.01576077657569907	4873	4.873	0.14788600159601759
X	0.01630351970302148	12816	12.816	0.10835350340561081
X	0.016136649138069146	2847	2.847	0.1782961490043799
X	0.016672918391160283	2410	2.41	0.19054527939805602
X	0.015725151464435257	2959	2.959	0.17450851547257792
X	0.016171156085640563	4659	4.659	0.15140821967176396
X	0.016142469356621954	9406	9.406	0.11972596111496145
X	0.01585740443540147	4141	4.141	0.15645002039604125
X	0.016057783915976342	4303	4.303	0.15510938925030462
X	0.0161032302975073	3827	3.827	0.16144261390516212
X	0.017179915052577852	4131	4.131	0.16081329073922815
X	0.016981712646413544	4158	4.158	0.15984497974322084
X	0.0159826891317015	6135	6.135	0.1375978860475701
X	0.016174774191240724	4949	4.949	0.14840217208378134
X	0.01546363865473225	672	0.672	0.2844335400750696
X	0.01618893865089564	1658	1.658	0.21373613430896582
X	0.016107328233404102	4507	4.507	0.15288984847318757
X	0.01810118379480612	12997	12.997	0.11167465506665959
X	0.015277683353151017	2464	2.464	0.18371259581374078
X	0.015396232820460043	2518	2.518	0.18286036459760055
X	0.016722769459817804	3026	3.026	0.17679898183981718
X	0.016592302187736583	2664	2.664	0.18398855534975397
X	0.01522108564579181	1568	1.568	0.21332079347637656
X	0.01608109232119392	1540	1.54	0.21857388659665133
X	0.01567044159923593	2067	2.067	0.19644768690728825
X	0.015824496607734193	1264	1.264	0.23219931981248362
X	0.01779227858098695	5169	5.169	0.15098773813546668
X	0.016249626356270263	4243	4.243	0.1564552409776771
X	0.015568736604297117	4936	4.936	0.14665360395318663
X	0.015912166238607108	2824	2.824	0.1779459897245588
X	0.01609464333244014	1879	1.879	0.20460591857292154
X	0.015767361503229146	3553	3.553	0.16433167141861255
X	0.017054205728009864	5796	5.796	0.14329610098325332
X	0.016111817234158717	6414	6.414	0.1359372637088994
X	0.016186227919011743	2220	2.22	0.19390881796797171
X	0.016227541826588057	7241	7.241	0.13086335889947318
X	0.01586781737799674	7793	7.793	0.12674694660611863
X	0.01605783530035277	3626	3.626	0.16421761225591036
X	0.016267806392392028	9129	9.129	0.12123702273193966
X	0.016483476162600732	3605	3.605	0.16597697499511344
X	0.015946447013089532	4840	4.84	0.14880111683029804
X	0.015861534810021	1934	1.934	0.20166455837060918
X	0.016232533937066158	8451	8.451	0.12430618536069794
X	0.01581580396480375	2061	2.061	0.19724428429922627
X	0.0161310571889014	3735	3.735	0.1628511169366925
X	0.015481100472726089	1718	1.718	0.20809399916284588
X	0.016044399275670758	2415	2.415	0.1879903004691936
X	0.01601564903019462	1442	1.442	0.22311379718492227
X	0.015579033429798116	2659	2.659	0.18027716671705124
X	0.015563737338386308	2284	2.284	0.18958585481672502
X	0.01552347327556428	2522	2.522	0.18326573460397688
X	0.017804169309238706	3098	3.098	0.17912110814868154
X	0.016141645550188984	4972	4.972	0.14807175671197087
X	0.01598753224623551	3934	3.934	0.15958143251488624
X	0.016023842247902063	2947	2.947	0.1758445212455856
X	0.015470955710369	968	0.968	0.25189174122236235
X	0.016838298244528066	8477	8.477	0.1257048415081076
X	0.015420794158265097	1430	1.43	0.22093112339844798
X	0.016311887446866555	9458	9.458	0.11992276561043975
X	0.01610442067629822	1683	1.683	0.2123017937371311
X	0.015523222432700759	2153	2.153	0.19318761617531985
X	0.016364082156286156	3549	3.549	0.16644160074776004
X	0.015465136443702662	2151	2.151	0.19300613555491355
X	0.01597879411599287	3233	3.233	0.17033847918258685
X	0.01670193353203272	1515	1.515	0.22256218314325224
X	0.01576775957159379	4232	4.232	0.15502727384922863
X	0.016757313297329813	7165	7.165	0.13273824303310128
X	0.01613346941857794	3209	3.209	0.17131130274122344
X	0.01662561978029404	16805	16.805	0.09964291921145195
X	0.016030575122783622	2735	2.735	0.18030063557925186
X	0.0161585733544377	3210	3.21	0.17138231066201268
X	0.01548749461874441	1894	1.894	0.2014653663718331
X	0.015580925024386388	2439	2.439	0.185549795118554
X	0.015793183491096135	2212	2.212	0.19255797917094872
X	0.016023084593630898	2181	2.181	0.19440025039968645
X	0.0156961330900297	1832	1.832	0.20462360628052823
X	0.015909857308994003	2928	2.928	0.17580520843255476
X	0.016137449458704434	2452	2.452	0.18740081548718734
X	0.015560851168207589	2677	2.677	0.17980219638396433
X	0.015553833679002234	1528	1.528	0.21672284778969522
X	0.017142104948089047	11424	11.424	0.11448501667535163
X	0.01593760400640301	6035	6.035	0.13822351065816496
X	0.016269573856396068	3702	3.702	0.16379975749201367
X	0.01595056410422016	1414	1.414	0.2242722529317859
X	0.015632600671620218	2211	2.211	0.19193204788910154
X	0.01688772593506048	2784	2.784	0.18237584360755388
X	0.015590838579470155	1491	1.491	0.21867406447111595
X	0.015709505327777647	3518	3.518	0.16467293455543655
X	0.016109922451039524	3436	3.436	0.16737095656285333
X	0.015693667000629376	2444	2.444	0.18586933446357207
X	0.016286757718011583	3900	3.9	0.1610361366175669
X	0.0162212500410031	8534	8.534	0.1238731628996828
X	0.015830037538036017	4327	4.327	0.15408665114977485
X	0.016746595046705268	18657	18.657	0.09646317873299648
X	0.016095746785852367	4467	4.467	0.15330808526845818
X	0.016144695857849126	3528	3.528	0.16602255653998643
X	0.016199897816807432	2627	2.627	0.18337947883536576
X	0.01554582130523744	894	0.894	0.2590749268305471
X	0.018434810802619796	11957	11.957	0.11552398243898149
X	0.01621204093857055	1406	1.406	0.2259179769425834
X	0.016732903890821042	5343	5.343	0.14630596987942512
X	0.015062588801699311	1407	1.407	0.22039505242653748
X	0.01603509148654572	5538	5.538	0.14252996009977778
X	0.015949277721915296	3149	3.149	0.17173393450365335
X	0.016276856838464242	1897	1.897	0.204723518791761
X	0.01573795795228834	5353	5.353	0.14325739371228047
X	0.01583572331088813	3988	3.988	0.15835347021719626
X	0.016136737401287567	1210	1.21	0.23714289668369173
X	0.015665221782092703	937	0.937	0.25570093671012023
X	0.016247388699985803	1466	1.466	0.22295464942970009
X	0.016268567706966892	5415	5.415	0.14429466892372203
X	0.01592569016577185	1662	1.662	0.21240060409880152
X	0.015708168486228753	3444	3.444	0.16583930512167505
X	0.01751089441776462	4869	4.869	0.15321084869237808
X	0.016015467905126115	5960	5.96	0.13902656436352545
X	0.016154842955276163	2701	2.701	0.18152065588415636
X	0.015831031790937748	4136	4.136	0.15642622462165956
X	0.015890608332408056	4176	4.176	0.15612052790716072
X	0.016028479718784572	5151	5.151	0.14599352478217084
X	0.015198135036817537	1878	1.878	0.20076973095232653
X	0.01615176714504834	5779	5.779	0.14086046470286104
X	0.015813679333122877	2196	2.196	0.19310796953353845
X	0.01597735795713761	2842	2.842	0.1778116848885927
X	0.016336573824545744	3853	3.853	0.1618529639532263
X	0.016436511732461388	3257	3.257	0.17152626536345217
X	0.01578497524025758	4553	4.553	0.15134988137472782
X	0.0180250715791274	4884	4.884	0.15453746394035664
X	0.015907722930631817	5317	5.317	0.14409440540863372
X	0.01567350498190635	3346	3.346	0.16731956617846003
X	0.015353734046021155	1477	1.477	0.21824510216362458
X	0.01568260457252584	2855	2.855	0.1764428347204943
X	0.017193952117742727	4483	4.483	0.156531708222011
X	0.016191448203013416	11103	11.103	0.1134005388842931
X	0.01805501742799656	2465	2.465	0.19420496529783543
X	0.015914309450203442	2866	2.866	0.17708041760755916
X	0.016084168831560313	1571	1.571	0.21714048958781745
X	0.016123523326461404	3971	3.971	0.1595340367296641
X	0.016006936301791036	1157	1.157	0.240062847178153
X	0.016447758461062688	3526	3.526	0.16708654182595936
X	0.015870674630464276	3508	3.508	0.16539102338555403
X	0.01625931201409792	7242	7.242	0.13094267684471053
X	0.01529675126845066	2367	2.367	0.18626602601877973
X	0.016698047438468047	4946	4.946	0.15001586580457604
X	0.016236232305203845	2745	2.745	0.18084824665053997
X	0.01603296290908986	3304	3.304	0.16930037357112307
X	0.01590928241599594	2737	2.737	0.17980092899440359
X	0.01568936078495559	1809	1.809	0.20545760803617563
X	0.01605863646649824	5366	5.366	0.1441073060263362
X	0.01781510417670173	11411	11.411	0.11600807499964133
X	0.015729846384632946	4630	4.63	0.15033075206337465
X	0.016208682041569035	5362	5.362	0.1445906791987707
X	0.016169243875756648	10712	10.712	0.11471134778393337
X	0.015684296723633563	1287	1.287	0.23012413425730718
X	0.0158982826696271	1236	1.236	0.23430267553234072
X	0.01612645817822118	1174	1.174	0.23949160760847357
X	0.015786762798092276	1562	1.562	0.21620746495467905
X	0.015919819053265272	8865	8.865	0.12154948667527381
X	0.016127616109998015	5352	5.352	0.1444390724484294
X	0.016267724728893967	3959	3.959	0.16016972788124584
X	0.01585502626881915	1354	1.354	0.22708202960254778
X	0.016037105443748832	5146	5.146	0.14606698587913958
X	0.015626408633002103	3685	3.685	0.16186038221619198
X	0.016116061253022614	2450	2.45	0.18736894365193102
X	0.016333227287733443	5380	5.38	0.14479822673066237
X	0.016218763484652378	1612	1.612	0.21588256412402557
X	0.016149315526016016	2530	2.53	0.1855002643724917
X	0.015721818538660597	4733	4.733	0.14920684879339527
X	0.015718625929965334	1656	1.656	0.21173114630530407
X	0.015981770698199095	4101	4.101	0.15736629088851406
X	0.01565186596385612	2421	2.421	0.18629038642171963
X	0.015993540730634952	2148	2.148	0.19527056426305528
X	0.017570613339783877	3249	3.249	0.17552773016699974
X	0.01586671618136883	2166	2.166	0.19421206246368442
X	0.016020263945947653	4262	4.262	0.15548389346836025
X	0.016127885925941863	5301	5.301	0.14490161065931248
X	0.01627575440637537	4172	4.172	0.15742206659171015
X	0.01577381310245589	2984	2.984	0.17419912022354098
X	0.01623049927813249	23912	23.912	0.08788309150160493
X	0.01622579979581391	48625	48.625	0.06936102454782417
X	0.01637136237430189	5654	5.654	0.14253111168392205
X	0.01624847727815713	10057	10.057	0.11734053727896351
X	0.01576522200098736	1699	1.699	0.2101368594307297
X	0.016074997054393896	6819	6.819	0.13308938060114
X	0.01620575994031514	9180	9.18	0.12085804737936287
X	0.018320500285684466	165517	165.517	0.04801367048566641
X	0.016233781606165173	3428	3.428	0.16792922947679256
X	0.01821018058546398	188613	188.613	0.04587550696109274
X	0.0158712627173607	42649	42.649	0.0719284788645871
X	0.016249620504840474	65685	65.685	0.0627758120807868
X	0.016639824717808938	22107	22.107	0.09096471823989842
X	0.016065040032458767	2589	2.589	0.18375955080059928
X	0.016118320565014038	5452	5.452	0.14352293728826382
X	0.01627670085552522	198937	198.937	0.043412714197120277
X	0.01729557463515444	365563	365.563	0.03616798003978674
X	0.016250685828192658	3614	3.614	0.16505468218245464
X	0.016273496550952917	48533	48.533	0.06947276428752534
X	0.016221044504912543	20592	20.592	0.09235493254211924
X	0.016252473590941507	14838	14.838	0.10308165138976413
X	0.01618298537037119	127658	127.658	0.05023466785821042
X	0.015704107376132075	45130	45.13	0.07033680563866106
X	0.016375219684763267	22032	22.032	0.09058252114485878
X	0.016351932283314136	14903	14.903	0.10314110807180993
X	0.01608258270973175	3783	3.783	0.16199681801356083
X	0.016717040951819546	140654	140.654	0.049166478819834394
X	0.015637465359070273	10836	10.836	0.11300538036498163
X	0.016525111934890535	3247	3.247	0.1720101364758327
X	0.01822791214216972	21845	21.845	0.09414450128913684
X	0.016756298727720803	104061	104.061	0.05440389992623298
X	0.01729912930559848	153922	153.922	0.048258577442586485
X	0.01624103306512859	83036	83.036	0.05804735162450451
X	0.0158372928826935	7011	7.011	0.13121001759512047
X	0.016409916954237543	35278	35.278	0.0774819976297701
X	0.015898898482428286	5581	5.581	0.14175933630860899
X	0.016380178849030695	42657	42.657	0.07268466226645828
X	0.01731426065962192	51193	51.193	0.06967299133212423
X	0.01624385713461951	13571	13.571	0.10617585581250187
X	0.016268391518167433	85001	85.001	0.057628881893066454
X	0.015240750203824132	897	0.897	0.2570817814332926
X	0.016289348038141416	11818	11.818	0.11128925598002869
X	0.016321972768413105	104047	104.047	0.053932145242036995
X	0.01817442335161317	49316	49.316	0.07169534057151461
X	0.01626781836062636	103725	103.725	0.05392812122371607
X	0.016067006387256867	3635	3.635	0.16411320124316503
X	0.01630460181059643	225750	225.75	0.04164480306322877
X	0.016380447952368985	17552	17.552	0.09772365938392406
X	0.01629612872665236	24911	24.911	0.08680893501962805
X	0.016396833291066087	76903	76.903	0.059740964934715514
X	0.0160055440262967	2254	2.254	0.19220831709701935
X	0.016378125294859085	130825	130.825	0.05002546665719134
X	0.018234678862624573	196272	196.272	0.04529114050761911
X	0.01773611059304773	398739	398.739	0.03543152873860022
X	0.01612903823142896	90487	90.487	0.056278272822050585
X	0.016136063721895723	39597	39.597	0.07413872373233549
X	0.01609420447225507	12399	12.399	0.10908397408776661
X	0.016355119585684356	6598	6.598	0.13533614582351858
X	0.01645242740696078	140704	140.704	0.04889988597481376
X	0.01684013790380764	8587	8.587	0.12517032895123908
X	0.016357053522178658	31056	31.056	0.08075817924867948
X	0.016766635479446117	40277	40.277	0.07466715474781796
X	0.0167364603956726	21408	21.408	0.0921218192641179
X	0.016022206183973948	17352	17.352	0.09737726946055793
X	0.017193915697567133	3588	3.588	0.16859348428611004
X	0.016409119262611356	92754	92.754	0.056137216475818985
X	0.016359244226168638	26781	26.781	0.08484875655276161
X	0.016248149870175983	19993	19.993	0.09332012605567563
X	0.01618487173161702	5459	5.459	0.14365874094207673
X	0.016160466461247	59995	59.995	0.06458221630329689
X	0.017410731118389214	38353	38.353	0.0768550759426154
X	0.016208698380812972	178301	178.301	0.04496398427779635
X	0.01622440742378757	6656	6.656	0.13458144183823606
X	0.016117510712934133	38228	38.228	0.07498460880025097
X	0.017347819783666302	12382	12.382	0.11189689461393565
X	0.016299387430575383	118641	118.641	0.05159950017054796
X	0.016363318807206435	33441	33.441	0.07880081550474846
X	0.016395717162568248	17262	17.262	0.09829839689762963
X	0.01635432337352714	112727	112.727	0.05254542114062882
X	0.016393860147649354	140121	140.121	0.04890943725713249
X	0.016382280373059218	4947	4.947	0.1490541759921549
X	0.016295612138302638	179320	179.32	0.04495872464563818
X	0.017298643172612024	140619	140.619	0.04973430023129418
X	0.018305925138223492	356585	356.585	0.037165747374764704
X	0.016335977404670618	15525	15.525	0.10171176056420687
X	0.016060916751967396	3115	3.115	0.17275770525992493
X	0.01618980171470344	13718	13.718	0.10567775462209142
X	0.01638743041115186	19123	19.123	0.09498425305084264
X	0.016190510447927242	18905	18.905	0.09496450011868339
X	0.01728073801723865	77349	77.349	0.060678643162449614
X	0.016107768587792393	1745	1.745	0.2097715803902883
X	0.01630886285279492	63472	63.472	0.06357414076025435
X	0.018181074393176438	118272	118.272	0.05356887604612322
X	0.015470959483477301	7605	7.605	0.1267085759854825
X	0.015980058416055327	7359	7.359	0.12949505563354147
X	0.016270009025712177	59251	59.251	0.06499760732348968
X	0.017040862454521855	9423	9.423	0.12183370739566833
X	0.016154225474191205	7882	7.882	0.1270232547506454
X	0.016359956158945123	13400	13.4	0.10687901574812013
X	0.016651257131075615	3959	3.959	0.16141869869616257
X	0.016150753060728284	27631	27.631	0.08361137997722755
X	0.016351254377826988	38023	38.023	0.07548052261107883
X	0.01674882753456741	21639	21.639	0.09181544453092844
X	0.018258775860343128	45055	45.055	0.0740019029633939
X	0.016148994630474183	13497	13.497	0.10616207881409173
X	0.018680049591301248	634275	634.275	0.030881624904457963
X	0.016257977283411478	21355	21.355	0.0913108345727542
X	0.016267797293746198	116116	116.116	0.05193724336388202
X	0.016046516523935822	2168	2.168	0.19488294321917465
X	0.016064289878079254	7795	7.795	0.1272570361739827
X	0.016279487372779004	3834	3.834	0.16193082886807253
X	0.016343390313771612	20033	20.033	0.09343983306577848
X	0.017485417144424215	4950	4.95	0.1522966204590922
X	0.017272305923809454	96155	96.155	0.05642342994488241
X	0.018183186717863974	8276	8.276	0.13000194165609838
X	0.01705105754327106	4886	4.886	0.1516815015086273
X	0.0183108201188769	203212	203.212	0.044831797163580565
X	0.017142303551960295	58988	58.988	0.06623718032804103
X	0.016195101159951777	35092	35.092	0.07727846994735758
X	0.018343784529857108	117886	117.886	0.053786782965055166
X	0.01829824486564512	304218	304.218	0.03918093924740211
X	0.018657097775312975	8896	8.896	0.12800190636728165
X	0.016223428849472164	10050	10.05	0.11730743233290496
X	0.016227516706227733	17979	17.979	0.09664116963090144
X	0.015830430298135908	2323	2.323	0.18958959697661923
X	0.015986994266495413	7309	7.309	0.12980844649075116
X	0.01722356238216029	15416	15.416	0.10376488925600565
X	0.016070782421941418	4035	4.035	0.1585128571545988
X	0.016826775058323532	8132	8.132	0.12742887228046368
X	0.01628376508312242	20201	20.201	0.09306662967171839
X	0.016264312835627404	8614	8.614	0.12359767004883819
X	0.016219789751408097	253247	253.247	0.0400098543451691
X	0.01618054420298165	11641	11.641	0.111600878080503
X	0.01619722112302032	63527	63.527	0.06341043339526918
X	0.017201138166648117	5374	5.374	0.14737365031193672
X	0.016205423851861734	3186	3.186	0.17197746419664445
X	0.016282924048275055	143218	143.218	0.04844455052604144
X	0.017239321801250724	315377	315.377	0.03795159676249428
X	0.017700580801071036	388862	388.862	0.03570513425972457
X	0.01638439273945499	17054	17.054	0.09867368869958323
X	0.016394473660660783	37828	37.828	0.07567655716079724
X	0.016257951939973255	6198	6.198	0.13791285671457434
X	0.01616648848034496	4413	4.413	0.15415605575066224
X	0.0170567980336962	386873	386.873	0.0353272389032057
X	0.01673392031583734	17250	17.25	0.09899263071868238
X	0.016097772913304356	8570	8.57	0.12338479908452991
X	0.017803570598198034	43149	43.149	0.07444670121518043
X	0.01845474666557959	588831	588.831	0.03152868134894274
X	0.01614740440021269	45408	45.408	0.0708473237408158
X	0.016219069152177464	16427	16.427	0.0995762780981661
X	0.017287126328402087	13187	13.187	0.1094440778884778
X	0.016078639741529212	14600	14.6	0.10326793266490303
X	0.016196849487675392	6580	6.58	0.1350210717498914
X	0.016333345775149177	159236	159.236	0.0468106457439496
X	0.0159919099686288	14072	14.072	0.10435537824597635
X	0.015938902421837837	2654	2.654	0.18176872648277625
X	0.0171316927666962	101926	101.926	0.0551872091510659
X	0.018279382531832	10908	10.908	0.11877876280691507
X	0.01528815210341053	1269	1.269	0.22924390141317313
X	0.0162193096369983	56102	56.102	0.06612281593259697
X	0.018344170411647914	83980	83.98	0.06022462003150987
X	0.018247539402473815	214718	214.718	0.043965493493250296
X	0.01617322897034329	157822	157.822	0.04679610758936469
X	0.01618253227225661	171922	171.922	0.04548885538499116
X	0.016225609836140853	25678	25.678	0.08581172086314957
X	0.016141066982535552	22412	22.412	0.08963629047725506
X	0.01692582117452971	154745	154.745	0.04782383224218135
X	0.01724829168922205	158471	158.471	0.04774546160956159
X	0.016628662470859194	14137	14.137	0.1055601551892436
X	0.016403122599828845	114326	114.326	0.05235126424420878
X	0.016394079815674326	168908	168.908	0.045956358485826035
X	0.01637774363347477	160079	160.079	0.046770631664197426
X	0.016409139467012192	106270	106.27	0.05364859777772149
X	0.01815368828388899	45849	45.849	0.0734308127928393
X	0.0160249183931516	16727	16.727	0.09858085712808803
X	0.016286439728928948	8270	8.27	0.12534496179261487
X	0.01587354085628547	10632	10.632	0.11429299553499093
X	0.016957388001981318	109298	109.298	0.05373396821761646
X	0.016361736643162914	115302	115.302	0.05215919168716305
X	0.01816804121492085	239455	239.455	0.042334528987224854
X	0.016437983891970247	84399	84.399	0.05796561080372789
X	0.0172859260317291	64050	64.05	0.06462360362763277
X	0.016166413234430372	13983	13.983	0.10495531571348689
X	0.016373402256167128	140053	140.053	0.04889699519942389
X	0.015818500382981843	8296	8.296	0.12400299900062456
X	0.016330768562004862	11746	11.746	0.11161062561886859
X	0.01633298802301857	111774	111.774	0.052671410692035854
X	0.016340507309619504	65496	65.496	0.0629530707857436
X	0.0158695261246789	5460	5.46	0.1427108869522787
X	0.016300395193799354	19946	19.946	0.0934933611010859
X	0.018915795061959544	564766	564.766	0.03223433280925881
X	0.01669714882553055	15597	15.597	0.10229798562570916
X	0.018077034620884717	120382	120.382	0.053152277539475844
X	0.016241890567766204	4827	4.827	0.14984873937854798
X	0.016102023892865344	3688	3.688	0.16344181814190176
X	0.016179436728474807	5123	5.123	0.14671674410810254
X	0.016080266240690602	4530	4.53	0.1525451289453405
X	0.016229762293325328	20388	20.388	0.0926785388212377
X	0.016276279053998494	57480	57.48	0.06566684071985228
X	0.015553301559885168	4187	4.187	0.15487210831104936
X	0.016382612833044145	99950	99.95	0.054726808620211106
X	0.016137794267633388	11589	11.589	0.11166902833620963
X	0.016801647683628133	48769	48.769	0.07010290380650229
X	0.016354469909890677	11375	11.375	0.11286562449822243
X	0.017227486076342613	161024	161.024	0.04747268172096756
X	0.015405136447415153	5301	5.301	0.14270392391702386
X	0.01602148541641309	29475	29.475	0.08161117957393864
X	0.016304012871799667	33668	33.668	0.07852821582971138
X	0.016171274194353016	24847	24.847	0.08666094628911752
X	0.01631227257672621	59346	59.346	0.06501910781642627
X	0.016311545053740883	362057	362.057	0.03558273971637176
X	0.016208613126470355	19646	19.646	0.09379015183275459
X	0.0159767230631385	6193	6.193	0.1371499148884858
X	0.017236210984512883	35225	35.225	0.0788007346870196
X	0.016316575547185543	24061	24.061	0.08785607174214934
X	0.01729912578732784	412158	412.158	0.03475256045874253
X	0.016187888514317007	7170	7.17	0.13118686377130795
X	0.018220403942486495	117828	117.828	0.053674725445201514
X	0.018295735533294794	287813	287.813	0.03990982491276684
X	0.016023117849090163	7622	7.622	0.1281030178067452
X	0.01632699970334679	145435	145.435	0.04824057583276574
X	0.01628573594723941	14559	14.559	0.10380667511201493
X	0.01826652497559365	485320	485.32	0.03351268687224856
X	0.017695598091243978	19329	19.329	0.09709986362275182
X	0.016074138269009396	14050	14.05	0.1045884656643876
X	0.01601322609984174	10461	10.461	0.11524848132160302
X	0.01598001202740478	3480	3.48	0.16621336910642986
X	0.015816501105158153	7566	7.566	0.12786392342451677
X	0.018262804745349987	54567	54.567	0.06942978164591947
X	0.016189711507716042	4955	4.955	0.14838789829281399
X	0.01836736436966276	136272	136.272	0.05127197780244148
X	0.016323808211525275	14450	14.45	0.10414806660839192
X	0.015976944765353652	8788	8.788	0.12204909454815276
X	0.018195459128464687	18845	18.845	0.09883762300568016
X	0.018189290846149935	162658	162.658	0.04817777951814285
X	0.01824041859192603	160737	160.737	0.048414224613282854
X	0.018167116898536038	282313	282.313	0.0400729866034549
X	0.016265802494551437	16458	16.458	0.099609206136511
X	0.016233982437606993	7327	7.327	0.13036658627464975
X	0.016132416144310928	3629	3.629	0.1644261227552973
X	0.0159738553520448	3945	3.945	0.15938749555201215
X	0.016761323374274578	118368	118.368	0.052122431242052826
X	0.016306145416833426	6915	6.915	0.13310254990279224
X	0.016340131056850157	22771	22.771	0.08952777901190385
X	0.01631431537673242	27587	27.587	0.08393726089595487
X	0.016154512669340385	20527	20.527	0.09232573909231374
X	0.016292754397656758	40189	40.189	0.07401091446090491
X	0.015859666061988204	6196	6.196	0.13679205582006954
X	0.018125429755508727	172624	172.624	0.04717686150718636
X	0.01637840295899577	60587	60.587	0.0646592556961924
X	0.017060140670409972	7912	7.912	0.1291909553370791
X	0.016324137152783906	15259	15.259	0.1022746622553318
X	0.018168186732972064	40457	40.457	0.07657834387349369
X	0.01817550684315864	82123	82.123	0.06048865225913814
X	0.01829679836946427	434709	434.709	0.03478501194489743
X	0.016767491250530713	97770	97.77	0.055558966407834205
X	0.0163076631377849	7365	7.365	0.1303385825091631
X	0.016312479796511074	28076	28.076	0.08344396220277119
X	0.016155369964901706	35074	35.074	0.07722842954240566
X	0.01608991953660983	6845	6.845	0.132961775739727
X	0.016185516301636758	12092	12.092	0.11020707425041708
X	0.016153383923204402	5027	5.027	0.147565514599141
X	0.016333622095479745	16860	16.86	0.0989482940456654
X	0.015961303071596284	54326	54.326	0.06647936639094072
X	0.015477186184505779	5270	5.27	0.14320575257619442
X	0.016322718290466	16448	16.448	0.0997454581189406
X	0.016359696920408392	8004	8.004	0.12690811434851623
X	0.01627366408238994	17394	17.394	0.09780520337123208
X	0.01617778584176442	5566	5.566	0.14271138688205262
X	0.016294180346001073	20418	20.418	0.0927555218447984
X	0.016168746801909856	7411	7.411	0.12969797108355682
X	0.01616178873003113	11130	11.13	0.11323953751567412
X	0.016203565549255887	7011	7.011	0.13221382642551271
X	0.016260047582087926	11339	11.339	0.1127670845078589
X	0.0173051373988445	20338	20.338	0.09475939289403011
X	0.016385582011583664	120414	120.414	0.05143534172756909
X	0.01620119283986881	5614	5.614	0.14237209018566108
X	0.016225667541572494	2796	2.796	0.17970292055281503
X	0.01623526796809243	10045	10.045	0.11735542575593119
X	0.01636101687289999	93270	93.27	0.055978695275383517
X	0.01817585849887385	326814	326.814	0.03817081447280307
X	0.016165713278506324	101441	101.441	0.054215955558281216
X	0.016118415440540355	6068	6.068	0.1384922882627607
X	0.01719076827706449	254545	254.545	0.04072335703056472
X	0.01820713936146102	162231	162.231	0.04823577830014564
X	0.01697990152546837	2609	2.609	0.18670403161785407
X	0.01627188986955436	8903	8.903	0.12226454639431755
X	0.0163830369347994	44993	44.993	0.07140848506389089
X	0.018294149544177653	83508	83.508	0.06028298017083905
X	0.017475952135859393	189162	189.162	0.045206649091183694
X	0.01874212890526845	163213	163.213	0.048605795759231074
X	0.01631450947134507	29461	29.461	0.08211872140706605
X	0.01620574372812581	6017	6.017	0.139132840538776
X	0.016041328386161884	3153	3.153	0.17199089358999692
X	0.016204534250225992	50710	50.71	0.0683671770027626
X	0.016047953387505243	12242	12.242	0.10944327816235458
X	0.016135948143356036	23068	23.068	0.08876903844541863
X	0.01632208436572287	100822	100.822	0.054501288523009936
X	0.01731297868843536	449981	449.981	0.03375923053474636
X	0.0158866108949916	9513	9.513	0.11864185288063359
X	0.015661898469874622	1950	1.95	0.20026417428164212
X	0.016220336173489027	18659	18.659	0.09543854918034851
X	0.016374856052214174	97899	97.899	0.055097650315857324
X	0.016221953212538574	29029	29.029	0.08236771591791539
X	0.01620082562332325	7161	7.161	0.13127675247650217
X	0.015691841554754702	2757	2.757	0.17854418739333394
X	0.016337281237275764	25444	25.444	0.08627099350607005
X	0.01603663814659769	7197	7.197	0.13061326195074066
X	0.016195651356679486	47874	47.874	0.06967862434939122
X	0.01714296858583882	6565	6.565	0.13770522937392596
X	0.01665553223498327	13324	13.324	0.10772288195811072
X	0.01629961129152622	9261	9.261	0.12073683520677286
X	0.01623884176221517	9083	9.083	0.12136922494250989
X	0.01652417995108661	20438	20.438	0.09315950530578623
X	0.016379537524010047	145361	145.361	0.04830045750446304
X	0.01730655112266525	266790	266.79	0.040180344948902484
X	0.01639818308460245	74506	74.506	0.06037653161563172
X	0.01662904593434888	122814	122.814	0.05134992561986681
X	0.016323568564116025	17328	17.328	0.0980292220544743
X	0.016194026378054772	14985	14.985	0.10262016257740518
X	0.018305523708069635	18179	18.179	0.10023145994047576
X	0.018241209513869847	123598	123.598	0.05284623634847219
X	0.016736503797088634	26650	26.65	0.0856358952651505
X	0.016158539378529856	45979	45.979	0.07056903734778483
X	0.016217287610111337	13375	13.375	0.10663378119426524
X	0.01875352938818217	906081	906.081	0.02745608832395639
X	0.01637008843525439	15534	15.534	0.10176284534556285
X	0.016319609609652893	62868	62.868	0.0637910925462421
X	0.016313578175713522	138725	138.725	0.04899271400298533
X	0.018112048503060623	35772	35.772	0.07970302111393675
X	0.01765555055589386	24989	24.989	0.08906576365310709
X	0.018345946341625154	844063	844.063	0.027907552597943478
X	0.017272019248993513	168179	168.179	0.04682995953382891
X	0.016298426346043504	88924	88.924	0.05680356087346819
X	0.016227191110238504	6865	6.865	0.13320921523741316
X	0.016343371566969467	32153	32.153	0.07980682830103678
X	0.016356270984502666	6774	6.774	0.13415690636352182
X	0.018032275250403785	171594	171.594	0.04718994559113822
X	0.018343891192699562	895121	895.121	0.027365490691272674
X	0.016193967275888482	69466	69.466	0.06154512457844825
X	0.018350503003622787	145174	145.174	0.050186438472222865
X	0.016292349039480968	76670	76.67	0.05967412790111106
X	0.01632260942192322	29973	29.973	0.08166195854807912
X	0.016161025305080255	27572	27.572	0.08368871126714783
X	0.015793436558340902	13759	13.759	0.10470399594883516
X	0.016293085903812548	107457	107.457	0.053324019495387884
X	0.018348251016802285	365820	365.82	0.03687871180746099
X	0.016201403753273343	91931	91.931	0.05606565328231254
X	0.016222686135506773	525650	525.65	0.03136717202464817
X	0.016369645752720992	133693	133.693	0.0496565875172705
X	0.01625363940745434	47312	47.312	0.0700368230538368
X	0.016065265537122158	19983	19.983	0.0929841816659167
X	0.01831333389170399	64650	64.65	0.06567493144672945
X	0.016308362968472927	123625	123.625	0.050905888438023156
X	0.01633679298383311	12824	12.824	0.10840461352954284
X	0.01836529420989901	467692	467.692	0.03398959446230568
X	0.01692782192133163	22537	22.537	0.09090092982776087
X	0.01819517633599976	39987	39.987	0.07691525461783724
X	0.016323417199092714	5674	5.674	0.1422243364974747
X	0.016240204764786336	15568	15.568	0.10141905493035366
X	0.01570543502810851	2135	2.135	0.19448406816495434
X	0.01552630775544625	4805	4.805	0.1478398793423703
X	0.01626539721601997	18232	18.232	0.0962668557383127
X	0.01715097474811477	35933	35.933	0.07815050283584508
X	0.018373855410704916	2520000	2520.0	0.01939098712012875
X	0.01585774272250407	11776	11.776	0.11042850437157947
X	0.017303896029439717	284888	284.888	0.03930881153582168
X	0.01822948504426864	7780	7.78	0.13282044609970023
X	0.01617135063122806	15647	15.647	0.10110479129791701
X	0.01596194946530559	9030	9.03	0.12091106886880126
X	0.016254693581053845	197682	197.682	0.043484774294250336
X	0.016331445048966737	30418	30.418	0.08127643626267465
X	0.017701210945744332	735735	735.735	0.028868697315362686
X	0.016072926029806568	5506	5.506	0.1429177750013537
X	0.018362583453150273	144400	144.4	0.050286977112073396
X	0.01633473185564231	18137	18.137	0.09657147222036823
X	0.016217519273253805	124021	124.021	0.0507570529235518
X	0.017553150511464402	140654	140.654	0.04997287432540643
X	0.01630291828145274	21403	21.403	0.0913265180856112
X	0.016194352749125777	12674	12.674	0.10851338154614237
X	0.01737453174942723	71970	71.97	0.062266415991515604
X	0.016245809239013018	20879	20.879	0.09197657181640198
X	0.01602106730760922	32689	32.689	0.0788430234170329
X	0.016277081056878952	195532	195.532	0.04366360215595821
X	0.016379328035952344	9846	9.846	0.11848919522969724
X	0.018444850297583318	57793	57.793	0.06833868723968639
X	0.016212227653909058	20844	20.844	0.09196456967363398
X	0.01628078485633982	112603	112.603	0.05248579647339783
X	0.016329916072907763	278010	278.01	0.03887238418571307
X	0.018368853626835074	58636	58.636	0.06791607393382548
X	0.0178348843577918	10072	10.072	0.12098128513590725
X	0.01618729107823485	107924	107.924	0.053131496407749064
X	0.0162557336051408	87636	87.636	0.05703060896608805
X	0.017931548052253	178324	178.324	0.046501753429160034
X	0.01630382299720921	49676	49.676	0.06897858446148311
X	0.015941279065292236	21903	21.903	0.08995110866053628
X	0.01825095351042911	48381	48.381	0.07225538832006324
X	0.016056485934694137	127825	127.825	0.05008160446619975
X	0.015772775631860726	97432	97.432	0.05450071117064641
X	0.0161688490144158	24389	24.389	0.08719569373402894
X	0.015626477223702598	5646	5.646	0.1404020351983213
X	0.0177480921891607	7009	7.009	0.13630084198143047
X	0.01818783581522257	68885	68.885	0.06415326844260627
X	0.0181808231619008	87672	87.672	0.059190345667688604
X	0.016750169499418	62146	62.146	0.06459546196029682
X	0.01655466393331978	11452	11.452	0.11306972688612274
X	0.016357654514785436	22390	22.39	0.09006491550576105
X	0.016322610089693285	43086	43.086	0.07235765380704128
X	0.017124865697475237	39343	39.343	0.07578557797986732
X	0.016213021843596423	20031	20.031	0.09319381968426116
X	0.016701397243030415	56514	56.514	0.06660889865250806
X	0.016119629783318157	8244	8.244	0.12504673596601287
X	0.01639676791348127	31237	31.237	0.08066707518600591
X	0.016004390563358374	4155	4.155	0.15675546993495848
X	0.016285326530343966	5518	5.518	0.14344043432712777
X	0.01826704885438951	158993	158.993	0.048614235678354635
X	0.01633103594886312	15508	15.508	0.10173865240596203
X	0.01635706375398521	70819	70.819	0.061355267249178794
X	0.016115091204757764	6821	6.821	0.13318691906338218
X	0.01675771221348165	7345	7.345	0.1316459923485529
X	0.01626485707028372	31529	31.529	0.0802010433671498
X	0.016051758892985896	81022	81.022	0.05829615207594864
X	0.016282251090580183	33804	33.804	0.07838785577729904
X	0.016678238343434505	5435	5.435	0.14531715135789136
X	0.01621988417843018	12195	12.195	0.10997365217628147
X	0.01694534848470564	41533	41.533	0.07416842685199049
X	0.016334225927732152	22321	22.321	0.09011456077572695
X	0.01626858957764741	70349	70.349	0.061380533405082593
X	0.016237844667387622	6662	6.662	0.1345781595553905
X	0.015173447504424427	6183	6.183	0.1348843841300956
X	0.016366076906154648	75266	75.266	0.060133329458876235
X	0.01583145130202864	6709	6.709	0.1331335908407476
X	0.016410591892852038	128409	128.409	0.05037049983943684
X	0.01632834377017888	10746	10.746	0.11496485599992282
X	0.016200743892659263	73479	73.479	0.060412096361789
X	0.0171544434868229	115260	115.26	0.05299472864884503
X	0.01827796803341803	58136	58.136	0.06799770506274963
X	0.01827757303259547	125080	125.08	0.05267164536330781
X	0.017308351984168063	93644	93.644	0.056962877631364955
X	0.0161807470541926	1871	1.871	0.20526186021427842
X	0.016757491680266263	48389	48.389	0.07022430586675968
X	0.01634870327566838	20023	20.023	0.09346551172234539
X	0.018064664643446773	74956	74.956	0.06223115101779058
X	0.017898264002389037	71758	71.758	0.06294774758118951
X	0.01614480866879083	45635	45.635	0.07072586741844428
X	0.01638985339047568	57609	57.609	0.06577006090709732
X	0.016222027676253543	25084	25.084	0.08647743149656122
X	0.016156883459423253	42126	42.126	0.07265560491941217
X	0.016180734764879127	3350	3.35	0.16903805124621396
X	0.0158843703869698	9494	9.494	0.11871536331544898
X	0.01630735481761436	157952	157.952	0.04691223568052127
X	0.018255945898907642	375975	375.975	0.03648226475081801
X	0.015589763921891191	17747	17.747	0.09577192052211145
X	0.01638759988287963	15167	15.167	0.10261367026831282
X	0.016257067853716212	25103	25.103	0.08651781259614881
X	0.01758815134378157	88769	88.769	0.058297905982687706
X	0.016336069000847366	54957	54.957	0.06673825503851871
X	0.016288074540078492	7324	7.324	0.13052903807087277
X	0.016892791977258004	36186	36.186	0.07757473221131114
X	0.017196690465339323	125234	125.234	0.051591031043332736
X	0.015953303169042612	10815	10.815	0.11383470809004595
X	0.018097138098983	45228	45.228	0.07368869052182349
X	0.016386017214495556	40876	40.876	0.07373409238637633
time for making epsilon is 2.6711816787719727
epsilons are
[0.17660769182032943, 0.0891671396346092, 0.18381863901459752, 0.09466221069857728, 0.10307585430017922, 0.06283523823915033, 0.06966975074036164, 0.07556116320541305, 0.10215200202218824, 0.06283965131920836, 0.11651913056676906, 0.06171398350540724, 0.12714757314506153, 0.03590963712943701, 0.0862389023239368, 0.06624995190010692, 0.0646394094423437, 0.09032314692030832, 0.05063822688801141, 0.05916180444908263, 0.05444123011291996, 0.04493114731529528, 0.110463448494767, 0.07005616270081245, 0.18368712626991443, 0.06540119054379512, 0.0652336228172665, 0.1183119320324448, 0.06639087625915173, 0.024483895415027126, 0.09615590143741892, 0.029547410581598002, 0.08467012362181692, 0.11021425077802181, 0.0744615856854084, 0.04471932067287162, 0.0594218504172528, 0.2484763637984366, 0.15828460501348507, 0.18907935096239822, 0.17466086613822845, 0.16287599601200775, 0.24015865644129508, 0.16423683343601772, 0.2295686799432603, 0.2525545122888265, 0.2012342195553035, 0.1436719287019357, 0.15434243026376906, 0.11707922578003348, 0.10746853382215288, 0.20955750710912166, 0.14034232209585373, 0.15493476868981804, 0.16155785682367962, 0.12241588950825014, 0.18938361725404484, 0.1356297677787261, 0.13626839686281128, 0.1762131424312007, 0.1617814213364124, 0.18942397967494562, 0.14788600159601759, 0.10835350340561081, 0.1782961490043799, 0.19054527939805602, 0.17450851547257792, 0.15140821967176396, 0.11972596111496145, 0.15645002039604125, 0.15510938925030462, 0.16144261390516212, 0.16081329073922815, 0.15984497974322084, 0.1375978860475701, 0.14840217208378134, 0.2844335400750696, 0.21373613430896582, 0.15288984847318757, 0.11167465506665959, 0.18371259581374078, 0.18286036459760055, 0.17679898183981718, 0.18398855534975397, 0.21332079347637656, 0.21857388659665133, 0.19644768690728825, 0.23219931981248362, 0.15098773813546668, 0.1564552409776771, 0.14665360395318663, 0.1779459897245588, 0.20460591857292154, 0.16433167141861255, 0.14329610098325332, 0.1359372637088994, 0.19390881796797171, 0.13086335889947318, 0.12674694660611863, 0.16421761225591036, 0.12123702273193966, 0.16597697499511344, 0.14880111683029804, 0.20166455837060918, 0.12430618536069794, 0.19724428429922627, 0.1628511169366925, 0.20809399916284588, 0.1879903004691936, 0.22311379718492227, 0.18027716671705124, 0.18958585481672502, 0.18326573460397688, 0.17912110814868154, 0.14807175671197087, 0.15958143251488624, 0.1758445212455856, 0.25189174122236235, 0.1257048415081076, 0.22093112339844798, 0.11992276561043975, 0.2123017937371311, 0.19318761617531985, 0.16644160074776004, 0.19300613555491355, 0.17033847918258685, 0.22256218314325224, 0.15502727384922863, 0.13273824303310128, 0.17131130274122344, 0.09964291921145195, 0.18030063557925186, 0.17138231066201268, 0.2014653663718331, 0.185549795118554, 0.19255797917094872, 0.19440025039968645, 0.20462360628052823, 0.17580520843255476, 0.18740081548718734, 0.17980219638396433, 0.21672284778969522, 0.11448501667535163, 0.13822351065816496, 0.16379975749201367, 0.2242722529317859, 0.19193204788910154, 0.18237584360755388, 0.21867406447111595, 0.16467293455543655, 0.16737095656285333, 0.18586933446357207, 0.1610361366175669, 0.1238731628996828, 0.15408665114977485, 0.09646317873299648, 0.15330808526845818, 0.16602255653998643, 0.18337947883536576, 0.2590749268305471, 0.11552398243898149, 0.2259179769425834, 0.14630596987942512, 0.22039505242653748, 0.14252996009977778, 0.17173393450365335, 0.204723518791761, 0.14325739371228047, 0.15835347021719626, 0.23714289668369173, 0.25570093671012023, 0.22295464942970009, 0.14429466892372203, 0.21240060409880152, 0.16583930512167505, 0.15321084869237808, 0.13902656436352545, 0.18152065588415636, 0.15642622462165956, 0.15612052790716072, 0.14599352478217084, 0.20076973095232653, 0.14086046470286104, 0.19310796953353845, 0.1778116848885927, 0.1618529639532263, 0.17152626536345217, 0.15134988137472782, 0.15453746394035664, 0.14409440540863372, 0.16731956617846003, 0.21824510216362458, 0.1764428347204943, 0.156531708222011, 0.1134005388842931, 0.19420496529783543, 0.17708041760755916, 0.21714048958781745, 0.1595340367296641, 0.240062847178153, 0.16708654182595936, 0.16539102338555403, 0.13094267684471053, 0.18626602601877973, 0.15001586580457604, 0.18084824665053997, 0.16930037357112307, 0.17980092899440359, 0.20545760803617563, 0.1441073060263362, 0.11600807499964133, 0.15033075206337465, 0.1445906791987707, 0.11471134778393337, 0.23012413425730718, 0.23430267553234072, 0.23949160760847357, 0.21620746495467905, 0.12154948667527381, 0.1444390724484294, 0.16016972788124584, 0.22708202960254778, 0.14606698587913958, 0.16186038221619198, 0.18736894365193102, 0.14479822673066237, 0.21588256412402557, 0.1855002643724917, 0.14920684879339527, 0.21173114630530407, 0.15736629088851406, 0.18629038642171963, 0.19527056426305528, 0.17552773016699974, 0.19421206246368442, 0.15548389346836025, 0.14490161065931248, 0.15742206659171015, 0.17419912022354098, 0.08788309150160493, 0.06936102454782417, 0.14253111168392205, 0.11734053727896351, 0.2101368594307297, 0.13308938060114, 0.12085804737936287, 0.04801367048566641, 0.16792922947679256, 0.04587550696109274, 0.0719284788645871, 0.0627758120807868, 0.09096471823989842, 0.18375955080059928, 0.14352293728826382, 0.043412714197120277, 0.03616798003978674, 0.16505468218245464, 0.06947276428752534, 0.09235493254211924, 0.10308165138976413, 0.05023466785821042, 0.07033680563866106, 0.09058252114485878, 0.10314110807180993, 0.16199681801356083, 0.049166478819834394, 0.11300538036498163, 0.1720101364758327, 0.09414450128913684, 0.05440389992623298, 0.048258577442586485, 0.05804735162450451, 0.13121001759512047, 0.0774819976297701, 0.14175933630860899, 0.07268466226645828, 0.06967299133212423, 0.10617585581250187, 0.057628881893066454, 0.2570817814332926, 0.11128925598002869, 0.053932145242036995, 0.07169534057151461, 0.05392812122371607, 0.16411320124316503, 0.04164480306322877, 0.09772365938392406, 0.08680893501962805, 0.059740964934715514, 0.19220831709701935, 0.05002546665719134, 0.04529114050761911, 0.03543152873860022, 0.056278272822050585, 0.07413872373233549, 0.10908397408776661, 0.13533614582351858, 0.04889988597481376, 0.12517032895123908, 0.08075817924867948, 0.07466715474781796, 0.0921218192641179, 0.09737726946055793, 0.16859348428611004, 0.056137216475818985, 0.08484875655276161, 0.09332012605567563, 0.14365874094207673, 0.06458221630329689, 0.0768550759426154, 0.04496398427779635, 0.13458144183823606, 0.07498460880025097, 0.11189689461393565, 0.05159950017054796, 0.07880081550474846, 0.09829839689762963, 0.05254542114062882, 0.04890943725713249, 0.1490541759921549, 0.04495872464563818, 0.04973430023129418, 0.037165747374764704, 0.10171176056420687, 0.17275770525992493, 0.10567775462209142, 0.09498425305084264, 0.09496450011868339, 0.060678643162449614, 0.2097715803902883, 0.06357414076025435, 0.05356887604612322, 0.1267085759854825, 0.12949505563354147, 0.06499760732348968, 0.12183370739566833, 0.1270232547506454, 0.10687901574812013, 0.16141869869616257, 0.08361137997722755, 0.07548052261107883, 0.09181544453092844, 0.0740019029633939, 0.10616207881409173, 0.030881624904457963, 0.0913108345727542, 0.05193724336388202, 0.19488294321917465, 0.1272570361739827, 0.16193082886807253, 0.09343983306577848, 0.1522966204590922, 0.05642342994488241, 0.13000194165609838, 0.1516815015086273, 0.044831797163580565, 0.06623718032804103, 0.07727846994735758, 0.053786782965055166, 0.03918093924740211, 0.12800190636728165, 0.11730743233290496, 0.09664116963090144, 0.18958959697661923, 0.12980844649075116, 0.10376488925600565, 0.1585128571545988, 0.12742887228046368, 0.09306662967171839, 0.12359767004883819, 0.0400098543451691, 0.111600878080503, 0.06341043339526918, 0.14737365031193672, 0.17197746419664445, 0.04844455052604144, 0.03795159676249428, 0.03570513425972457, 0.09867368869958323, 0.07567655716079724, 0.13791285671457434, 0.15415605575066224, 0.0353272389032057, 0.09899263071868238, 0.12338479908452991, 0.07444670121518043, 0.03152868134894274, 0.0708473237408158, 0.0995762780981661, 0.1094440778884778, 0.10326793266490303, 0.1350210717498914, 0.0468106457439496, 0.10435537824597635, 0.18176872648277625, 0.0551872091510659, 0.11877876280691507, 0.22924390141317313, 0.06612281593259697, 0.06022462003150987, 0.043965493493250296, 0.04679610758936469, 0.04548885538499116, 0.08581172086314957, 0.08963629047725506, 0.04782383224218135, 0.04774546160956159, 0.1055601551892436, 0.05235126424420878, 0.045956358485826035, 0.046770631664197426, 0.05364859777772149, 0.0734308127928393, 0.09858085712808803, 0.12534496179261487, 0.11429299553499093, 0.05373396821761646, 0.05215919168716305, 0.042334528987224854, 0.05796561080372789, 0.06462360362763277, 0.10495531571348689, 0.04889699519942389, 0.12400299900062456, 0.11161062561886859, 0.052671410692035854, 0.0629530707857436, 0.1427108869522787, 0.0934933611010859, 0.03223433280925881, 0.10229798562570916, 0.053152277539475844, 0.14984873937854798, 0.16344181814190176, 0.14671674410810254, 0.1525451289453405, 0.0926785388212377, 0.06566684071985228, 0.15487210831104936, 0.054726808620211106, 0.11166902833620963, 0.07010290380650229, 0.11286562449822243, 0.04747268172096756, 0.14270392391702386, 0.08161117957393864, 0.07852821582971138, 0.08666094628911752, 0.06501910781642627, 0.03558273971637176, 0.09379015183275459, 0.1371499148884858, 0.0788007346870196, 0.08785607174214934, 0.03475256045874253, 0.13118686377130795, 0.053674725445201514, 0.03990982491276684, 0.1281030178067452, 0.04824057583276574, 0.10380667511201493, 0.03351268687224856, 0.09709986362275182, 0.1045884656643876, 0.11524848132160302, 0.16621336910642986, 0.12786392342451677, 0.06942978164591947, 0.14838789829281399, 0.05127197780244148, 0.10414806660839192, 0.12204909454815276, 0.09883762300568016, 0.04817777951814285, 0.048414224613282854, 0.0400729866034549, 0.099609206136511, 0.13036658627464975, 0.1644261227552973, 0.15938749555201215, 0.052122431242052826, 0.13310254990279224, 0.08952777901190385, 0.08393726089595487, 0.09232573909231374, 0.07401091446090491, 0.13679205582006954, 0.04717686150718636, 0.0646592556961924, 0.1291909553370791, 0.1022746622553318, 0.07657834387349369, 0.06048865225913814, 0.03478501194489743, 0.055558966407834205, 0.1303385825091631, 0.08344396220277119, 0.07722842954240566, 0.132961775739727, 0.11020707425041708, 0.147565514599141, 0.0989482940456654, 0.06647936639094072, 0.14320575257619442, 0.0997454581189406, 0.12690811434851623, 0.09780520337123208, 0.14271138688205262, 0.0927555218447984, 0.12969797108355682, 0.11323953751567412, 0.13221382642551271, 0.1127670845078589, 0.09475939289403011, 0.05143534172756909, 0.14237209018566108, 0.17970292055281503, 0.11735542575593119, 0.055978695275383517, 0.03817081447280307, 0.054215955558281216, 0.1384922882627607, 0.04072335703056472, 0.04823577830014564, 0.18670403161785407, 0.12226454639431755, 0.07140848506389089, 0.06028298017083905, 0.045206649091183694, 0.048605795759231074, 0.08211872140706605, 0.139132840538776, 0.17199089358999692, 0.0683671770027626, 0.10944327816235458, 0.08876903844541863, 0.054501288523009936, 0.03375923053474636, 0.11864185288063359, 0.20026417428164212, 0.09543854918034851, 0.055097650315857324, 0.08236771591791539, 0.13127675247650217, 0.17854418739333394, 0.08627099350607005, 0.13061326195074066, 0.06967862434939122, 0.13770522937392596, 0.10772288195811072, 0.12073683520677286, 0.12136922494250989, 0.09315950530578623, 0.04830045750446304, 0.040180344948902484, 0.06037653161563172, 0.05134992561986681, 0.0980292220544743, 0.10262016257740518, 0.10023145994047576, 0.05284623634847219, 0.0856358952651505, 0.07056903734778483, 0.10663378119426524, 0.02745608832395639, 0.10176284534556285, 0.0637910925462421, 0.04899271400298533, 0.07970302111393675, 0.08906576365310709, 0.027907552597943478, 0.04682995953382891, 0.05680356087346819, 0.13320921523741316, 0.07980682830103678, 0.13415690636352182, 0.04718994559113822, 0.027365490691272674, 0.06154512457844825, 0.050186438472222865, 0.05967412790111106, 0.08166195854807912, 0.08368871126714783, 0.10470399594883516, 0.053324019495387884, 0.03687871180746099, 0.05606565328231254, 0.03136717202464817, 0.0496565875172705, 0.0700368230538368, 0.0929841816659167, 0.06567493144672945, 0.050905888438023156, 0.10840461352954284, 0.03398959446230568, 0.09090092982776087, 0.07691525461783724, 0.1422243364974747, 0.10141905493035366, 0.19448406816495434, 0.1478398793423703, 0.0962668557383127, 0.07815050283584508, 0.01939098712012875, 0.11042850437157947, 0.03930881153582168, 0.13282044609970023, 0.10110479129791701, 0.12091106886880126, 0.043484774294250336, 0.08127643626267465, 0.028868697315362686, 0.1429177750013537, 0.050286977112073396, 0.09657147222036823, 0.0507570529235518, 0.04997287432540643, 0.0913265180856112, 0.10851338154614237, 0.062266415991515604, 0.09197657181640198, 0.0788430234170329, 0.04366360215595821, 0.11848919522969724, 0.06833868723968639, 0.09196456967363398, 0.05248579647339783, 0.03887238418571307, 0.06791607393382548, 0.12098128513590725, 0.053131496407749064, 0.05703060896608805, 0.046501753429160034, 0.06897858446148311, 0.08995110866053628, 0.07225538832006324, 0.05008160446619975, 0.05450071117064641, 0.08719569373402894, 0.1404020351983213, 0.13630084198143047, 0.06415326844260627, 0.059190345667688604, 0.06459546196029682, 0.11306972688612274, 0.09006491550576105, 0.07235765380704128, 0.07578557797986732, 0.09319381968426116, 0.06660889865250806, 0.12504673596601287, 0.08066707518600591, 0.15675546993495848, 0.14344043432712777, 0.048614235678354635, 0.10173865240596203, 0.061355267249178794, 0.13318691906338218, 0.1316459923485529, 0.0802010433671498, 0.05829615207594864, 0.07838785577729904, 0.14531715135789136, 0.10997365217628147, 0.07416842685199049, 0.09011456077572695, 0.061380533405082593, 0.1345781595553905, 0.1348843841300956, 0.060133329458876235, 0.1331335908407476, 0.05037049983943684, 0.11496485599992282, 0.060412096361789, 0.05299472864884503, 0.06799770506274963, 0.05267164536330781, 0.056962877631364955, 0.20526186021427842, 0.07022430586675968, 0.09346551172234539, 0.06223115101779058, 0.06294774758118951, 0.07072586741844428, 0.06577006090709732, 0.08647743149656122, 0.07265560491941217, 0.16903805124621396, 0.11871536331544898, 0.04691223568052127, 0.03648226475081801, 0.09577192052211145, 0.10261367026831282, 0.08651781259614881, 0.058297905982687706, 0.06673825503851871, 0.13052903807087277, 0.07757473221131114, 0.051591031043332736, 0.11383470809004595, 0.07368869052182349, 0.07373409238637633]
0.09793349757601257
Making ranges
torch.Size([32367, 2])
We keep 6.09e+06/4.07e+08 =  1% of the original kernel matrix.

torch.Size([6854, 2])
We keep 3.89e+05/8.67e+06 =  4% of the original kernel matrix.

torch.Size([15096, 2])
We keep 1.57e+06/5.94e+07 =  2% of the original kernel matrix.

torch.Size([34423, 2])
We keep 9.75e+06/5.19e+08 =  1% of the original kernel matrix.

torch.Size([33561, 2])
We keep 7.53e+06/4.59e+08 =  1% of the original kernel matrix.

torch.Size([5949, 2])
We keep 3.30e+05/6.76e+06 =  4% of the original kernel matrix.

torch.Size([14279, 2])
We keep 1.43e+06/5.24e+07 =  2% of the original kernel matrix.

torch.Size([25630, 2])
We keep 1.37e+07/4.67e+08 =  2% of the original kernel matrix.

torch.Size([28718, 2])
We keep 7.53e+06/4.36e+08 =  1% of the original kernel matrix.

torch.Size([25119, 2])
We keep 4.97e+06/2.49e+08 =  1% of the original kernel matrix.

torch.Size([28619, 2])
We keep 5.59e+06/3.18e+08 =  1% of the original kernel matrix.

torch.Size([106629, 2])
We keep 5.54e+07/4.35e+09 =  1% of the original kernel matrix.

torch.Size([57441, 2])
We keep 1.82e+07/1.33e+09 =  1% of the original kernel matrix.

torch.Size([77462, 2])
We keep 3.28e+07/2.33e+09 =  1% of the original kernel matrix.

torch.Size([48888, 2])
We keep 1.40e+07/9.74e+08 =  1% of the original kernel matrix.

torch.Size([60239, 2])
We keep 1.96e+07/1.44e+09 =  1% of the original kernel matrix.

torch.Size([43781, 2])
We keep 1.13e+07/7.66e+08 =  1% of the original kernel matrix.

torch.Size([23851, 2])
We keep 5.67e+06/2.31e+08 =  2% of the original kernel matrix.

torch.Size([27905, 2])
We keep 5.46e+06/3.06e+08 =  1% of the original kernel matrix.

torch.Size([82072, 2])
We keep 1.93e+08/4.31e+09 =  4% of the original kernel matrix.

torch.Size([50701, 2])
We keep 1.63e+07/1.32e+09 =  1% of the original kernel matrix.

torch.Size([17632, 2])
We keep 3.02e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([23562, 2])
We keep 3.90e+06/2.04e+08 =  1% of the original kernel matrix.

torch.Size([99226, 2])
We keep 1.05e+08/4.69e+09 =  2% of the original kernel matrix.

torch.Size([55397, 2])
We keep 1.86e+07/1.38e+09 =  1% of the original kernel matrix.

torch.Size([15117, 2])
We keep 1.97e+06/7.17e+07 =  2% of the original kernel matrix.

torch.Size([21833, 2])
We keep 3.47e+06/1.71e+08 =  2% of the original kernel matrix.

torch.Size([581583, 2])
We keep 1.41e+09/1.56e+11 =  0% of the original kernel matrix.

torch.Size([140791, 2])
We keep 8.92e+07/7.96e+09 =  1% of the original kernel matrix.

torch.Size([38762, 2])
We keep 1.05e+07/6.50e+08 =  1% of the original kernel matrix.

torch.Size([35311, 2])
We keep 8.13e+06/5.14e+08 =  1% of the original kernel matrix.

torch.Size([92563, 2])
We keep 5.04e+07/3.40e+09 =  1% of the original kernel matrix.

torch.Size([53229, 2])
We keep 1.64e+07/1.18e+09 =  1% of the original kernel matrix.

torch.Size([96198, 2])
We keep 5.40e+07/3.68e+09 =  1% of the original kernel matrix.

torch.Size([54592, 2])
We keep 1.69e+07/1.22e+09 =  1% of the original kernel matrix.

torch.Size([34289, 2])
We keep 8.45e+06/4.85e+08 =  1% of the original kernel matrix.

torch.Size([32953, 2])
We keep 7.15e+06/4.44e+08 =  1% of the original kernel matrix.

torch.Size([189011, 2])
We keep 2.19e+08/1.57e+10 =  1% of the original kernel matrix.

torch.Size([78797, 2])
We keep 3.18e+07/2.53e+09 =  1% of the original kernel matrix.

torch.Size([125634, 2])
We keep 1.29e+08/7.78e+09 =  1% of the original kernel matrix.

torch.Size([63210, 2])
We keep 2.36e+07/1.78e+09 =  1% of the original kernel matrix.

torch.Size([142543, 2])
We keep 1.38e+08/1.01e+10 =  1% of the original kernel matrix.

torch.Size([66804, 2])
We keep 2.63e+07/2.02e+09 =  1% of the original kernel matrix.

torch.Size([274177, 2])
We keep 3.24e+08/3.84e+10 =  0% of the original kernel matrix.

torch.Size([97502, 2])
We keep 4.60e+07/3.95e+09 =  1% of the original kernel matrix.

torch.Size([20702, 2])
We keep 3.49e+06/1.48e+08 =  2% of the original kernel matrix.

torch.Size([25988, 2])
We keep 4.53e+06/2.45e+08 =  1% of the original kernel matrix.

torch.Size([71563, 2])
We keep 6.12e+07/2.25e+09 =  2% of the original kernel matrix.

torch.Size([47128, 2])
We keep 1.38e+07/9.56e+08 =  1% of the original kernel matrix.

torch.Size([5721, 2])
We keep 3.53e+05/6.77e+06 =  5% of the original kernel matrix.

torch.Size([13797, 2])
We keep 1.43e+06/5.25e+07 =  2% of the original kernel matrix.

torch.Size([95594, 2])
We keep 4.13e+07/3.41e+09 =  1% of the original kernel matrix.

torch.Size([54577, 2])
We keep 1.62e+07/1.18e+09 =  1% of the original kernel matrix.

torch.Size([94414, 2])
We keep 4.89e+07/3.45e+09 =  1% of the original kernel matrix.

torch.Size([54097, 2])
We keep 1.63e+07/1.18e+09 =  1% of the original kernel matrix.

torch.Size([18140, 2])
We keep 2.69e+06/1.09e+08 =  2% of the original kernel matrix.

torch.Size([24026, 2])
We keep 4.06e+06/2.10e+08 =  1% of the original kernel matrix.

torch.Size([67631, 2])
We keep 1.36e+08/2.96e+09 =  4% of the original kernel matrix.

torch.Size([45676, 2])
We keep 1.50e+07/1.10e+09 =  1% of the original kernel matrix.

torch.Size([2020817, 2])
We keep 7.92e+09/1.39e+12 =  0% of the original kernel matrix.

torch.Size([274225, 2])
We keep 2.39e+08/2.38e+10 =  1% of the original kernel matrix.

torch.Size([26037, 2])
We keep 7.97e+06/3.25e+08 =  2% of the original kernel matrix.

torch.Size([29233, 2])
We keep 6.33e+06/3.64e+08 =  1% of the original kernel matrix.

torch.Size([1062585, 2])
We keep 3.73e+09/5.05e+11 =  0% of the original kernel matrix.

torch.Size([195380, 2])
We keep 1.53e+08/1.43e+10 =  1% of the original kernel matrix.

torch.Size([38908, 2])
We keep 2.11e+07/7.09e+08 =  2% of the original kernel matrix.

torch.Size([35314, 2])
We keep 8.61e+06/5.37e+08 =  1% of the original kernel matrix.

torch.Size([21104, 2])
We keep 4.33e+06/1.67e+08 =  2% of the original kernel matrix.

torch.Size([26114, 2])
We keep 4.76e+06/2.61e+08 =  1% of the original kernel matrix.

torch.Size([59346, 2])
We keep 3.04e+07/1.95e+09 =  1% of the original kernel matrix.

torch.Size([42928, 2])
We keep 1.33e+07/8.90e+08 =  1% of the original kernel matrix.

torch.Size([267566, 2])
We keep 5.13e+08/3.43e+10 =  1% of the original kernel matrix.

torch.Size([95281, 2])
We keep 4.42e+07/3.74e+09 =  1% of the original kernel matrix.

torch.Size([123695, 2])
We keep 8.80e+07/6.09e+09 =  1% of the original kernel matrix.

torch.Size([62793, 2])
We keep 2.10e+07/1.57e+09 =  1% of the original kernel matrix.

torch.Size([2445, 2])
We keep 6.40e+04/9.68e+05 =  6% of the original kernel matrix.

torch.Size([9803, 2])
We keep 7.16e+05/1.98e+07 =  3% of the original kernel matrix.

torch.Size([8491, 2])
We keep 6.73e+05/1.64e+07 =  4% of the original kernel matrix.

torch.Size([16280, 2])
We keep 1.99e+06/8.18e+07 =  2% of the original kernel matrix.

torch.Size([5228, 2])
We keep 2.82e+05/5.73e+06 =  4% of the original kernel matrix.

torch.Size([13201, 2])
We keep 1.36e+06/4.83e+07 =  2% of the original kernel matrix.

torch.Size([6935, 2])
We keep 4.90e+05/1.12e+07 =  4% of the original kernel matrix.

torch.Size([15223, 2])
We keep 1.77e+06/6.76e+07 =  2% of the original kernel matrix.

torch.Size([6118, 2])
We keep 6.86e+05/1.28e+07 =  5% of the original kernel matrix.

torch.Size([13373, 2])
We keep 1.81e+06/7.21e+07 =  2% of the original kernel matrix.

torch.Size([2798, 2])
We keep 8.37e+04/1.21e+06 =  6% of the original kernel matrix.

torch.Size([10340, 2])
We keep 7.93e+05/2.21e+07 =  3% of the original kernel matrix.

torch.Size([7439, 2])
We keep 5.66e+05/1.35e+07 =  4% of the original kernel matrix.

torch.Size([15282, 2])
We keep 1.86e+06/7.41e+07 =  2% of the original kernel matrix.

torch.Size([3125, 2])
We keep 1.07e+05/1.76e+06 =  6% of the original kernel matrix.

torch.Size([10914, 2])
We keep 8.99e+05/2.68e+07 =  3% of the original kernel matrix.

torch.Size([2613, 2])
We keep 6.92e+04/1.02e+06 =  6% of the original kernel matrix.

torch.Size([10316, 2])
We keep 7.43e+05/2.03e+07 =  3% of the original kernel matrix.

torch.Size([4541, 2])
We keep 2.10e+05/3.87e+06 =  5% of the original kernel matrix.

torch.Size([12578, 2])
We keep 1.18e+06/3.96e+07 =  2% of the original kernel matrix.

torch.Size([11325, 2])
We keep 1.01e+06/3.03e+07 =  3% of the original kernel matrix.

torch.Size([18734, 2])
We keep 2.47e+06/1.11e+08 =  2% of the original kernel matrix.

torch.Size([8815, 2])
We keep 7.66e+05/1.81e+07 =  4% of the original kernel matrix.

torch.Size([16690, 2])
We keep 2.07e+06/8.57e+07 =  2% of the original kernel matrix.

torch.Size([17162, 2])
We keep 2.92e+06/1.02e+08 =  2% of the original kernel matrix.

torch.Size([23196, 2])
We keep 3.97e+06/2.03e+08 =  1% of the original kernel matrix.

torch.Size([22100, 2])
We keep 3.64e+06/1.72e+08 =  2% of the original kernel matrix.

torch.Size([26884, 2])
We keep 4.84e+06/2.65e+08 =  1% of the original kernel matrix.

torch.Size([3846, 2])
We keep 1.66e+05/2.95e+06 =  5% of the original kernel matrix.

torch.Size([11603, 2])
We keep 1.08e+06/3.46e+07 =  3% of the original kernel matrix.

torch.Size([11186, 2])
We keep 1.25e+06/3.29e+07 =  3% of the original kernel matrix.

torch.Size([18667, 2])
We keep 2.58e+06/1.16e+08 =  2% of the original kernel matrix.

torch.Size([8443, 2])
We keep 9.81e+05/1.84e+07 =  5% of the original kernel matrix.

torch.Size([16322, 2])
We keep 2.11e+06/8.64e+07 =  2% of the original kernel matrix.

torch.Size([8428, 2])
We keep 6.02e+05/1.46e+07 =  4% of the original kernel matrix.

torch.Size([16294, 2])
We keep 1.89e+06/7.72e+07 =  2% of the original kernel matrix.

torch.Size([15727, 2])
We keep 2.26e+06/7.58e+07 =  2% of the original kernel matrix.

torch.Size([22115, 2])
We keep 3.53e+06/1.76e+08 =  2% of the original kernel matrix.

torch.Size([5246, 2])
We keep 2.66e+05/5.28e+06 =  5% of the original kernel matrix.

torch.Size([13401, 2])
We keep 1.31e+06/4.63e+07 =  2% of the original kernel matrix.

torch.Size([12400, 2])
We keep 1.32e+06/4.08e+07 =  3% of the original kernel matrix.

torch.Size([19503, 2])
We keep 2.78e+06/1.29e+08 =  2% of the original kernel matrix.

torch.Size([12207, 2])
We keep 1.27e+06/3.84e+07 =  3% of the original kernel matrix.

torch.Size([19381, 2])
We keep 2.70e+06/1.25e+08 =  2% of the original kernel matrix.

torch.Size([6164, 2])
We keep 4.20e+05/8.22e+06 =  5% of the original kernel matrix.

torch.Size([14028, 2])
We keep 1.55e+06/5.78e+07 =  2% of the original kernel matrix.

torch.Size([8334, 2])
We keep 5.95e+05/1.47e+07 =  4% of the original kernel matrix.

torch.Size([16419, 2])
We keep 1.90e+06/7.72e+07 =  2% of the original kernel matrix.

torch.Size([5411, 2])
We keep 2.83e+05/5.64e+06 =  5% of the original kernel matrix.

torch.Size([13521, 2])
We keep 1.35e+06/4.79e+07 =  2% of the original kernel matrix.

torch.Size([10205, 2])
We keep 9.19e+05/2.37e+07 =  3% of the original kernel matrix.

torch.Size([17777, 2])
We keep 2.28e+06/9.83e+07 =  2% of the original kernel matrix.

torch.Size([21667, 2])
We keep 3.46e+06/1.64e+08 =  2% of the original kernel matrix.

torch.Size([26665, 2])
We keep 4.74e+06/2.58e+08 =  1% of the original kernel matrix.

torch.Size([6246, 2])
We keep 3.57e+05/8.11e+06 =  4% of the original kernel matrix.

torch.Size([14402, 2])
We keep 1.53e+06/5.74e+07 =  2% of the original kernel matrix.

torch.Size([5419, 2])
We keep 2.85e+05/5.81e+06 =  4% of the original kernel matrix.

torch.Size([13660, 2])
We keep 1.36e+06/4.86e+07 =  2% of the original kernel matrix.

torch.Size([6841, 2])
We keep 3.91e+05/8.76e+06 =  4% of the original kernel matrix.

torch.Size([14943, 2])
We keep 1.57e+06/5.97e+07 =  2% of the original kernel matrix.

torch.Size([9382, 2])
We keep 7.95e+05/2.17e+07 =  3% of the original kernel matrix.

torch.Size([17052, 2])
We keep 2.19e+06/9.40e+07 =  2% of the original kernel matrix.

torch.Size([16815, 2])
We keep 2.38e+06/8.85e+07 =  2% of the original kernel matrix.

torch.Size([22907, 2])
We keep 3.73e+06/1.90e+08 =  1% of the original kernel matrix.

torch.Size([8827, 2])
We keep 6.93e+05/1.71e+07 =  4% of the original kernel matrix.

torch.Size([16610, 2])
We keep 2.01e+06/8.35e+07 =  2% of the original kernel matrix.

torch.Size([8423, 2])
We keep 9.12e+05/1.85e+07 =  4% of the original kernel matrix.

torch.Size([16371, 2])
We keep 2.09e+06/8.68e+07 =  2% of the original kernel matrix.

torch.Size([8148, 2])
We keep 6.28e+05/1.46e+07 =  4% of the original kernel matrix.

torch.Size([16148, 2])
We keep 1.91e+06/7.72e+07 =  2% of the original kernel matrix.

torch.Size([8508, 2])
We keep 6.82e+05/1.71e+07 =  3% of the original kernel matrix.

torch.Size([16520, 2])
We keep 2.05e+06/8.33e+07 =  2% of the original kernel matrix.

torch.Size([8234, 2])
We keep 7.10e+05/1.73e+07 =  4% of the original kernel matrix.

torch.Size([16012, 2])
We keep 2.07e+06/8.39e+07 =  2% of the original kernel matrix.

torch.Size([11966, 2])
We keep 1.30e+06/3.76e+07 =  3% of the original kernel matrix.

torch.Size([19096, 2])
We keep 2.72e+06/1.24e+08 =  2% of the original kernel matrix.

torch.Size([9453, 2])
We keep 9.64e+05/2.45e+07 =  3% of the original kernel matrix.

torch.Size([17092, 2])
We keep 2.29e+06/9.98e+07 =  2% of the original kernel matrix.

torch.Size([1860, 2])
We keep 3.82e+04/4.52e+05 =  8% of the original kernel matrix.

torch.Size([9086, 2])
We keep 5.73e+05/1.36e+07 =  4% of the original kernel matrix.

torch.Size([3871, 2])
We keep 1.64e+05/2.75e+06 =  5% of the original kernel matrix.

torch.Size([11919, 2])
We keep 1.06e+06/3.34e+07 =  3% of the original kernel matrix.

torch.Size([9387, 2])
We keep 7.64e+05/2.03e+07 =  3% of the original kernel matrix.

torch.Size([17122, 2])
We keep 2.13e+06/9.09e+07 =  2% of the original kernel matrix.

torch.Size([20096, 2])
We keep 3.89e+06/1.69e+08 =  2% of the original kernel matrix.

torch.Size([25282, 2])
We keep 4.91e+06/2.62e+08 =  1% of the original kernel matrix.

torch.Size([5666, 2])
We keep 3.07e+05/6.07e+06 =  5% of the original kernel matrix.

torch.Size([13688, 2])
We keep 1.37e+06/4.97e+07 =  2% of the original kernel matrix.

torch.Size([5443, 2])
We keep 3.42e+05/6.34e+06 =  5% of the original kernel matrix.

torch.Size([13374, 2])
We keep 1.41e+06/5.08e+07 =  2% of the original kernel matrix.

torch.Size([6410, 2])
We keep 4.45e+05/9.16e+06 =  4% of the original kernel matrix.

torch.Size([14583, 2])
We keep 1.63e+06/6.10e+07 =  2% of the original kernel matrix.

torch.Size([5864, 2])
We keep 3.60e+05/7.10e+06 =  5% of the original kernel matrix.

torch.Size([14045, 2])
We keep 1.50e+06/5.37e+07 =  2% of the original kernel matrix.

torch.Size([3764, 2])
We keep 1.45e+05/2.46e+06 =  5% of the original kernel matrix.

torch.Size([11591, 2])
We keep 1.01e+06/3.16e+07 =  3% of the original kernel matrix.

torch.Size([3472, 2])
We keep 1.45e+05/2.37e+06 =  6% of the original kernel matrix.

torch.Size([11294, 2])
We keep 9.96e+05/3.11e+07 =  3% of the original kernel matrix.

torch.Size([4820, 2])
We keep 2.65e+05/4.27e+06 =  6% of the original kernel matrix.

torch.Size([12823, 2])
We keep 1.23e+06/4.17e+07 =  2% of the original kernel matrix.

torch.Size([3014, 2])
We keep 1.04e+05/1.60e+06 =  6% of the original kernel matrix.

torch.Size([10849, 2])
We keep 8.75e+05/2.55e+07 =  3% of the original kernel matrix.

torch.Size([9699, 2])
We keep 1.03e+06/2.67e+07 =  3% of the original kernel matrix.

torch.Size([17594, 2])
We keep 2.45e+06/1.04e+08 =  2% of the original kernel matrix.

torch.Size([8716, 2])
We keep 7.62e+05/1.80e+07 =  4% of the original kernel matrix.

torch.Size([16545, 2])
We keep 2.02e+06/8.56e+07 =  2% of the original kernel matrix.

torch.Size([10256, 2])
We keep 8.81e+05/2.44e+07 =  3% of the original kernel matrix.

torch.Size([17673, 2])
We keep 2.28e+06/9.95e+07 =  2% of the original kernel matrix.

torch.Size([6049, 2])
We keep 3.71e+05/7.97e+06 =  4% of the original kernel matrix.

torch.Size([14072, 2])
We keep 1.54e+06/5.70e+07 =  2% of the original kernel matrix.

torch.Size([4434, 2])
We keep 1.99e+05/3.53e+06 =  5% of the original kernel matrix.

torch.Size([12581, 2])
We keep 1.15e+06/3.79e+07 =  3% of the original kernel matrix.

torch.Size([7373, 2])
We keep 6.09e+05/1.26e+07 =  4% of the original kernel matrix.

torch.Size([15228, 2])
We keep 1.79e+06/7.17e+07 =  2% of the original kernel matrix.

torch.Size([10973, 2])
We keep 1.29e+06/3.36e+07 =  3% of the original kernel matrix.

torch.Size([18521, 2])
We keep 2.65e+06/1.17e+08 =  2% of the original kernel matrix.

torch.Size([12184, 2])
We keep 1.29e+06/4.11e+07 =  3% of the original kernel matrix.

torch.Size([19355, 2])
We keep 2.77e+06/1.29e+08 =  2% of the original kernel matrix.

torch.Size([4968, 2])
We keep 2.55e+05/4.93e+06 =  5% of the original kernel matrix.

torch.Size([13081, 2])
We keep 1.30e+06/4.48e+07 =  2% of the original kernel matrix.

torch.Size([12379, 2])
We keep 1.98e+06/5.24e+07 =  3% of the original kernel matrix.

torch.Size([19305, 2])
We keep 3.12e+06/1.46e+08 =  2% of the original kernel matrix.

torch.Size([14672, 2])
We keep 1.85e+06/6.07e+07 =  3% of the original kernel matrix.

torch.Size([21192, 2])
We keep 3.24e+06/1.57e+08 =  2% of the original kernel matrix.

torch.Size([7920, 2])
We keep 5.31e+05/1.31e+07 =  4% of the original kernel matrix.

torch.Size([15784, 2])
We keep 1.79e+06/7.31e+07 =  2% of the original kernel matrix.

torch.Size([16094, 2])
We keep 2.27e+06/8.33e+07 =  2% of the original kernel matrix.

torch.Size([22461, 2])
We keep 3.63e+06/1.84e+08 =  1% of the original kernel matrix.

torch.Size([7439, 2])
We keep 5.41e+05/1.30e+07 =  4% of the original kernel matrix.

torch.Size([15459, 2])
We keep 1.84e+06/7.27e+07 =  2% of the original kernel matrix.

torch.Size([10286, 2])
We keep 8.68e+05/2.34e+07 =  3% of the original kernel matrix.

torch.Size([17895, 2])
We keep 2.26e+06/9.76e+07 =  2% of the original kernel matrix.

torch.Size([4365, 2])
We keep 2.17e+05/3.74e+06 =  5% of the original kernel matrix.

torch.Size([12213, 2])
We keep 1.17e+06/3.90e+07 =  2% of the original kernel matrix.

torch.Size([14917, 2])
We keep 2.56e+06/7.14e+07 =  3% of the original kernel matrix.

torch.Size([21464, 2])
We keep 3.44e+06/1.70e+08 =  2% of the original kernel matrix.

torch.Size([4724, 2])
We keep 2.24e+05/4.25e+06 =  5% of the original kernel matrix.

torch.Size([12772, 2])
We keep 1.21e+06/4.16e+07 =  2% of the original kernel matrix.

torch.Size([8346, 2])
We keep 5.52e+05/1.40e+07 =  3% of the original kernel matrix.

torch.Size([16369, 2])
We keep 1.86e+06/7.53e+07 =  2% of the original kernel matrix.

torch.Size([3715, 2])
We keep 1.82e+05/2.95e+06 =  6% of the original kernel matrix.

torch.Size([11572, 2])
We keep 1.08e+06/3.46e+07 =  3% of the original kernel matrix.

torch.Size([5315, 2])
We keep 2.88e+05/5.83e+06 =  4% of the original kernel matrix.

torch.Size([13376, 2])
We keep 1.38e+06/4.87e+07 =  2% of the original kernel matrix.

torch.Size([3199, 2])
We keep 1.39e+05/2.08e+06 =  6% of the original kernel matrix.

torch.Size([10917, 2])
We keep 9.53e+05/2.91e+07 =  3% of the original kernel matrix.

torch.Size([6002, 2])
We keep 3.35e+05/7.07e+06 =  4% of the original kernel matrix.

torch.Size([14041, 2])
We keep 1.44e+06/5.36e+07 =  2% of the original kernel matrix.

torch.Size([4378, 2])
We keep 3.59e+05/5.22e+06 =  6% of the original kernel matrix.

torch.Size([12094, 2])
We keep 1.34e+06/4.61e+07 =  2% of the original kernel matrix.

torch.Size([5679, 2])
We keep 3.06e+05/6.36e+06 =  4% of the original kernel matrix.

torch.Size([13731, 2])
We keep 1.40e+06/5.09e+07 =  2% of the original kernel matrix.

torch.Size([6114, 2])
We keep 4.56e+05/9.60e+06 =  4% of the original kernel matrix.

torch.Size([14265, 2])
We keep 1.68e+06/6.25e+07 =  2% of the original kernel matrix.

torch.Size([10215, 2])
We keep 9.51e+05/2.47e+07 =  3% of the original kernel matrix.

torch.Size([17845, 2])
We keep 2.31e+06/1.00e+08 =  2% of the original kernel matrix.

torch.Size([7719, 2])
We keep 6.68e+05/1.55e+07 =  4% of the original kernel matrix.

torch.Size([15590, 2])
We keep 1.98e+06/7.93e+07 =  2% of the original kernel matrix.

torch.Size([6658, 2])
We keep 3.90e+05/8.68e+06 =  4% of the original kernel matrix.

torch.Size([14707, 2])
We keep 1.58e+06/5.94e+07 =  2% of the original kernel matrix.

torch.Size([2518, 2])
We keep 6.36e+04/9.37e+05 =  6% of the original kernel matrix.

torch.Size([10174, 2])
We keep 7.19e+05/1.95e+07 =  3% of the original kernel matrix.

torch.Size([15138, 2])
We keep 2.10e+06/7.19e+07 =  2% of the original kernel matrix.

torch.Size([21830, 2])
We keep 3.50e+06/1.71e+08 =  2% of the original kernel matrix.

torch.Size([3330, 2])
We keep 1.22e+05/2.04e+06 =  5% of the original kernel matrix.

torch.Size([11084, 2])
We keep 9.42e+05/2.88e+07 =  3% of the original kernel matrix.

torch.Size([16926, 2])
We keep 2.20e+06/8.95e+07 =  2% of the original kernel matrix.

torch.Size([23061, 2])
We keep 3.71e+06/1.91e+08 =  1% of the original kernel matrix.

torch.Size([3807, 2])
We keep 1.68e+05/2.83e+06 =  5% of the original kernel matrix.

torch.Size([11660, 2])
We keep 1.07e+06/3.39e+07 =  3% of the original kernel matrix.

torch.Size([4758, 2])
We keep 2.49e+05/4.64e+06 =  5% of the original kernel matrix.

torch.Size([12706, 2])
We keep 1.25e+06/4.34e+07 =  2% of the original kernel matrix.

torch.Size([7269, 2])
We keep 5.14e+05/1.26e+07 =  4% of the original kernel matrix.

torch.Size([15495, 2])
We keep 1.77e+06/7.16e+07 =  2% of the original kernel matrix.

torch.Size([4941, 2])
We keep 2.65e+05/4.63e+06 =  5% of the original kernel matrix.

torch.Size([12967, 2])
We keep 1.25e+06/4.34e+07 =  2% of the original kernel matrix.

torch.Size([7204, 2])
We keep 4.72e+05/1.05e+07 =  4% of the original kernel matrix.

torch.Size([15290, 2])
We keep 1.70e+06/6.52e+07 =  2% of the original kernel matrix.

torch.Size([3613, 2])
We keep 1.39e+05/2.30e+06 =  6% of the original kernel matrix.

torch.Size([11686, 2])
We keep 1.00e+06/3.06e+07 =  3% of the original kernel matrix.

torch.Size([9255, 2])
We keep 7.11e+05/1.79e+07 =  3% of the original kernel matrix.

torch.Size([17028, 2])
We keep 2.03e+06/8.53e+07 =  2% of the original kernel matrix.

torch.Size([13446, 2])
We keep 1.59e+06/5.13e+07 =  3% of the original kernel matrix.

torch.Size([20453, 2])
We keep 3.04e+06/1.44e+08 =  2% of the original kernel matrix.

torch.Size([6779, 2])
We keep 4.76e+05/1.03e+07 =  4% of the original kernel matrix.

torch.Size([14656, 2])
We keep 1.65e+06/6.47e+07 =  2% of the original kernel matrix.

torch.Size([26186, 2])
We keep 5.40e+06/2.82e+08 =  1% of the original kernel matrix.

torch.Size([29860, 2])
We keep 6.04e+06/3.39e+08 =  1% of the original kernel matrix.

torch.Size([5652, 2])
We keep 3.95e+05/7.48e+06 =  5% of the original kernel matrix.

torch.Size([13690, 2])
We keep 1.49e+06/5.52e+07 =  2% of the original kernel matrix.

torch.Size([6779, 2])
We keep 4.74e+05/1.03e+07 =  4% of the original kernel matrix.

torch.Size([14863, 2])
We keep 1.67e+06/6.47e+07 =  2% of the original kernel matrix.

torch.Size([4367, 2])
We keep 2.08e+05/3.59e+06 =  5% of the original kernel matrix.

torch.Size([12217, 2])
We keep 1.14e+06/3.82e+07 =  2% of the original kernel matrix.

torch.Size([5446, 2])
We keep 2.90e+05/5.95e+06 =  4% of the original kernel matrix.

torch.Size([13427, 2])
We keep 1.37e+06/4.92e+07 =  2% of the original kernel matrix.

torch.Size([4903, 2])
We keep 2.51e+05/4.89e+06 =  5% of the original kernel matrix.

torch.Size([12913, 2])
We keep 1.28e+06/4.46e+07 =  2% of the original kernel matrix.

torch.Size([5115, 2])
We keep 2.60e+05/4.76e+06 =  5% of the original kernel matrix.

torch.Size([13210, 2])
We keep 1.28e+06/4.40e+07 =  2% of the original kernel matrix.

torch.Size([4427, 2])
We keep 1.84e+05/3.36e+06 =  5% of the original kernel matrix.

torch.Size([12432, 2])
We keep 1.13e+06/3.69e+07 =  3% of the original kernel matrix.

torch.Size([6839, 2])
We keep 3.84e+05/8.57e+06 =  4% of the original kernel matrix.

torch.Size([14960, 2])
We keep 1.57e+06/5.90e+07 =  2% of the original kernel matrix.

torch.Size([5474, 2])
We keep 2.99e+05/6.01e+06 =  4% of the original kernel matrix.

torch.Size([13483, 2])
We keep 1.40e+06/4.94e+07 =  2% of the original kernel matrix.

torch.Size([6000, 2])
We keep 3.32e+05/7.17e+06 =  4% of the original kernel matrix.

torch.Size([14051, 2])
We keep 1.46e+06/5.40e+07 =  2% of the original kernel matrix.

torch.Size([3628, 2])
We keep 1.48e+05/2.33e+06 =  6% of the original kernel matrix.

torch.Size([11623, 2])
We keep 9.92e+05/3.08e+07 =  3% of the original kernel matrix.

torch.Size([18813, 2])
We keep 3.23e+06/1.31e+08 =  2% of the original kernel matrix.

torch.Size([24465, 2])
We keep 4.41e+06/2.30e+08 =  1% of the original kernel matrix.

torch.Size([11605, 2])
We keep 1.23e+06/3.64e+07 =  3% of the original kernel matrix.

torch.Size([18891, 2])
We keep 2.67e+06/1.22e+08 =  2% of the original kernel matrix.

torch.Size([7649, 2])
We keep 6.06e+05/1.37e+07 =  4% of the original kernel matrix.

torch.Size([15491, 2])
We keep 1.87e+06/7.47e+07 =  2% of the original kernel matrix.

torch.Size([3543, 2])
We keep 1.25e+05/2.00e+06 =  6% of the original kernel matrix.

torch.Size([11527, 2])
We keep 9.52e+05/2.85e+07 =  3% of the original kernel matrix.

torch.Size([4931, 2])
We keep 3.23e+05/4.89e+06 =  6% of the original kernel matrix.

torch.Size([12960, 2])
We keep 1.29e+06/4.46e+07 =  2% of the original kernel matrix.

torch.Size([5894, 2])
We keep 3.67e+05/7.75e+06 =  4% of the original kernel matrix.

torch.Size([14205, 2])
We keep 1.54e+06/5.61e+07 =  2% of the original kernel matrix.

torch.Size([3500, 2])
We keep 1.40e+05/2.22e+06 =  6% of the original kernel matrix.

torch.Size([11342, 2])
We keep 9.67e+05/3.01e+07 =  3% of the original kernel matrix.

torch.Size([6860, 2])
We keep 5.79e+05/1.24e+07 =  4% of the original kernel matrix.

torch.Size([14684, 2])
We keep 1.78e+06/7.09e+07 =  2% of the original kernel matrix.

torch.Size([7121, 2])
We keep 5.54e+05/1.18e+07 =  4% of the original kernel matrix.

torch.Size([15073, 2])
We keep 1.78e+06/6.93e+07 =  2% of the original kernel matrix.

torch.Size([5629, 2])
We keep 2.98e+05/5.97e+06 =  4% of the original kernel matrix.

torch.Size([13762, 2])
We keep 1.37e+06/4.93e+07 =  2% of the original kernel matrix.

torch.Size([8439, 2])
We keep 6.25e+05/1.52e+07 =  4% of the original kernel matrix.

torch.Size([16391, 2])
We keep 1.94e+06/7.87e+07 =  2% of the original kernel matrix.

torch.Size([13643, 2])
We keep 2.52e+06/7.28e+07 =  3% of the original kernel matrix.

torch.Size([20376, 2])
We keep 3.50e+06/1.72e+08 =  2% of the original kernel matrix.

torch.Size([9353, 2])
We keep 7.12e+05/1.87e+07 =  3% of the original kernel matrix.

torch.Size([17145, 2])
We keep 2.08e+06/8.73e+07 =  2% of the original kernel matrix.

torch.Size([27356, 2])
We keep 7.54e+06/3.48e+08 =  2% of the original kernel matrix.

torch.Size([30439, 2])
We keep 6.64e+06/3.76e+08 =  1% of the original kernel matrix.

torch.Size([9665, 2])
We keep 7.48e+05/2.00e+07 =  3% of the original kernel matrix.

torch.Size([17329, 2])
We keep 2.13e+06/9.01e+07 =  2% of the original kernel matrix.

torch.Size([7049, 2])
We keep 6.72e+05/1.24e+07 =  5% of the original kernel matrix.

torch.Size([14997, 2])
We keep 1.81e+06/7.11e+07 =  2% of the original kernel matrix.

torch.Size([5952, 2])
We keep 3.28e+05/6.90e+06 =  4% of the original kernel matrix.

torch.Size([14169, 2])
We keep 1.47e+06/5.30e+07 =  2% of the original kernel matrix.

torch.Size([2407, 2])
We keep 5.88e+04/7.99e+05 =  7% of the original kernel matrix.

torch.Size([10032, 2])
We keep 6.88e+05/1.80e+07 =  3% of the original kernel matrix.

torch.Size([18850, 2])
We keep 3.78e+06/1.43e+08 =  2% of the original kernel matrix.

torch.Size([24961, 2])
We keep 4.63e+06/2.41e+08 =  1% of the original kernel matrix.

torch.Size([3226, 2])
We keep 1.33e+05/1.98e+06 =  6% of the original kernel matrix.

torch.Size([11053, 2])
We keep 9.54e+05/2.84e+07 =  3% of the original kernel matrix.

torch.Size([9835, 2])
We keep 1.59e+06/2.85e+07 =  5% of the original kernel matrix.

torch.Size([17367, 2])
We keep 2.40e+06/1.08e+08 =  2% of the original kernel matrix.

torch.Size([3257, 2])
We keep 1.38e+05/1.98e+06 =  6% of the original kernel matrix.

torch.Size([10937, 2])
We keep 9.40e+05/2.84e+07 =  3% of the original kernel matrix.

torch.Size([10599, 2])
We keep 1.11e+06/3.07e+07 =  3% of the original kernel matrix.

torch.Size([18087, 2])
We keep 2.48e+06/1.12e+08 =  2% of the original kernel matrix.

torch.Size([6413, 2])
We keep 4.20e+05/9.92e+06 =  4% of the original kernel matrix.

torch.Size([14551, 2])
We keep 1.63e+06/6.35e+07 =  2% of the original kernel matrix.

torch.Size([4640, 2])
We keep 1.86e+05/3.60e+06 =  5% of the original kernel matrix.

torch.Size([12927, 2])
We keep 1.14e+06/3.83e+07 =  2% of the original kernel matrix.

torch.Size([10799, 2])
We keep 1.03e+06/2.87e+07 =  3% of the original kernel matrix.

torch.Size([18220, 2])
We keep 2.43e+06/1.08e+08 =  2% of the original kernel matrix.

torch.Size([8518, 2])
We keep 6.30e+05/1.59e+07 =  3% of the original kernel matrix.

torch.Size([16308, 2])
We keep 1.96e+06/8.04e+07 =  2% of the original kernel matrix.

torch.Size([2845, 2])
We keep 9.89e+04/1.46e+06 =  6% of the original kernel matrix.

torch.Size([10543, 2])
We keep 8.64e+05/2.44e+07 =  3% of the original kernel matrix.

torch.Size([2360, 2])
We keep 7.33e+04/8.78e+05 =  8% of the original kernel matrix.

torch.Size([9707, 2])
We keep 7.18e+05/1.89e+07 =  3% of the original kernel matrix.

torch.Size([3290, 2])
We keep 1.53e+05/2.15e+06 =  7% of the original kernel matrix.

torch.Size([11001, 2])
We keep 9.63e+05/2.96e+07 =  3% of the original kernel matrix.

torch.Size([11046, 2])
We keep 1.11e+06/2.93e+07 =  3% of the original kernel matrix.

torch.Size([18609, 2])
We keep 2.45e+06/1.09e+08 =  2% of the original kernel matrix.

torch.Size([4020, 2])
We keep 1.53e+05/2.76e+06 =  5% of the original kernel matrix.

torch.Size([12110, 2])
We keep 1.04e+06/3.35e+07 =  3% of the original kernel matrix.

torch.Size([7379, 2])
We keep 5.05e+05/1.19e+07 =  4% of the original kernel matrix.

torch.Size([15475, 2])
We keep 1.76e+06/6.95e+07 =  2% of the original kernel matrix.

torch.Size([8824, 2])
We keep 4.86e+06/2.37e+07 = 20% of the original kernel matrix.

torch.Size([16795, 2])
We keep 2.31e+06/9.82e+07 =  2% of the original kernel matrix.

torch.Size([11626, 2])
We keep 1.26e+06/3.55e+07 =  3% of the original kernel matrix.

torch.Size([18848, 2])
We keep 2.66e+06/1.20e+08 =  2% of the original kernel matrix.

torch.Size([5944, 2])
We keep 3.38e+05/7.30e+06 =  4% of the original kernel matrix.

torch.Size([14019, 2])
We keep 1.45e+06/5.45e+07 =  2% of the original kernel matrix.

torch.Size([8470, 2])
We keep 7.89e+05/1.71e+07 =  4% of the original kernel matrix.

torch.Size([16353, 2])
We keep 2.02e+06/8.34e+07 =  2% of the original kernel matrix.

torch.Size([8976, 2])
We keep 6.70e+05/1.74e+07 =  3% of the original kernel matrix.

torch.Size([16724, 2])
We keep 2.01e+06/8.42e+07 =  2% of the original kernel matrix.

torch.Size([10179, 2])
We keep 9.71e+05/2.65e+07 =  3% of the original kernel matrix.

torch.Size([17788, 2])
We keep 2.38e+06/1.04e+08 =  2% of the original kernel matrix.

torch.Size([4296, 2])
We keep 1.99e+05/3.53e+06 =  5% of the original kernel matrix.

torch.Size([12178, 2])
We keep 1.13e+06/3.79e+07 =  2% of the original kernel matrix.

torch.Size([11002, 2])
We keep 1.27e+06/3.34e+07 =  3% of the original kernel matrix.

torch.Size([18392, 2])
We keep 2.61e+06/1.17e+08 =  2% of the original kernel matrix.

torch.Size([5146, 2])
We keep 2.39e+05/4.82e+06 =  4% of the original kernel matrix.

torch.Size([13333, 2])
We keep 1.26e+06/4.43e+07 =  2% of the original kernel matrix.

torch.Size([6434, 2])
We keep 3.63e+05/8.08e+06 =  4% of the original kernel matrix.

torch.Size([14496, 2])
We keep 1.53e+06/5.73e+07 =  2% of the original kernel matrix.

torch.Size([8278, 2])
We keep 5.98e+05/1.48e+07 =  4% of the original kernel matrix.

torch.Size([16238, 2])
We keep 1.90e+06/7.77e+07 =  2% of the original kernel matrix.

torch.Size([6724, 2])
We keep 4.68e+05/1.06e+07 =  4% of the original kernel matrix.

torch.Size([14552, 2])
We keep 1.71e+06/6.57e+07 =  2% of the original kernel matrix.

torch.Size([8992, 2])
We keep 8.47e+05/2.07e+07 =  4% of the original kernel matrix.

torch.Size([16723, 2])
We keep 2.17e+06/9.18e+07 =  2% of the original kernel matrix.

torch.Size([9408, 2])
We keep 9.35e+05/2.39e+07 =  3% of the original kernel matrix.

torch.Size([17221, 2])
We keep 2.35e+06/9.85e+07 =  2% of the original kernel matrix.

torch.Size([10796, 2])
We keep 9.74e+05/2.83e+07 =  3% of the original kernel matrix.

torch.Size([18299, 2])
We keep 2.42e+06/1.07e+08 =  2% of the original kernel matrix.

torch.Size([7403, 2])
We keep 4.84e+05/1.12e+07 =  4% of the original kernel matrix.

torch.Size([15431, 2])
We keep 1.73e+06/6.75e+07 =  2% of the original kernel matrix.

torch.Size([3316, 2])
We keep 1.51e+05/2.18e+06 =  6% of the original kernel matrix.

torch.Size([11013, 2])
We keep 9.70e+05/2.98e+07 =  3% of the original kernel matrix.

torch.Size([6601, 2])
We keep 3.76e+05/8.15e+06 =  4% of the original kernel matrix.

torch.Size([14675, 2])
We keep 1.53e+06/5.76e+07 =  2% of the original kernel matrix.

torch.Size([8505, 2])
We keep 8.63e+05/2.01e+07 =  4% of the original kernel matrix.

torch.Size([16506, 2])
We keep 2.18e+06/9.04e+07 =  2% of the original kernel matrix.

torch.Size([18900, 2])
We keep 3.15e+06/1.23e+08 =  2% of the original kernel matrix.

torch.Size([24573, 2])
We keep 4.27e+06/2.24e+08 =  1% of the original kernel matrix.

torch.Size([5132, 2])
We keep 3.40e+05/6.08e+06 =  5% of the original kernel matrix.

torch.Size([13323, 2])
We keep 1.43e+06/4.97e+07 =  2% of the original kernel matrix.

torch.Size([6469, 2])
We keep 3.62e+05/8.21e+06 =  4% of the original kernel matrix.

torch.Size([14659, 2])
We keep 1.53e+06/5.78e+07 =  2% of the original kernel matrix.

torch.Size([3659, 2])
We keep 1.35e+05/2.47e+06 =  5% of the original kernel matrix.

torch.Size([11710, 2])
We keep 1.00e+06/3.17e+07 =  3% of the original kernel matrix.

torch.Size([8254, 2])
We keep 8.55e+05/1.58e+07 =  5% of the original kernel matrix.

torch.Size([16123, 2])
We keep 1.95e+06/8.01e+07 =  2% of the original kernel matrix.

torch.Size([2922, 2])
We keep 8.99e+04/1.34e+06 =  6% of the original kernel matrix.

torch.Size([10824, 2])
We keep 8.13e+05/2.33e+07 =  3% of the original kernel matrix.

torch.Size([7636, 2])
We keep 4.98e+05/1.24e+07 =  4% of the original kernel matrix.

torch.Size([15703, 2])
We keep 1.79e+06/7.11e+07 =  2% of the original kernel matrix.

torch.Size([7575, 2])
We keep 5.24e+05/1.23e+07 =  4% of the original kernel matrix.

torch.Size([15596, 2])
We keep 1.78e+06/7.07e+07 =  2% of the original kernel matrix.

torch.Size([13867, 2])
We keep 1.47e+06/5.24e+07 =  2% of the original kernel matrix.

torch.Size([20791, 2])
We keep 3.06e+06/1.46e+08 =  2% of the original kernel matrix.

torch.Size([5385, 2])
We keep 2.74e+05/5.60e+06 =  4% of the original kernel matrix.

torch.Size([13452, 2])
We keep 1.32e+06/4.77e+07 =  2% of the original kernel matrix.

torch.Size([10078, 2])
We keep 8.91e+05/2.45e+07 =  3% of the original kernel matrix.

torch.Size([17770, 2])
We keep 2.29e+06/9.97e+07 =  2% of the original kernel matrix.

torch.Size([6143, 2])
We keep 3.56e+05/7.54e+06 =  4% of the original kernel matrix.

torch.Size([14286, 2])
We keep 1.50e+06/5.54e+07 =  2% of the original kernel matrix.

torch.Size([6937, 2])
We keep 5.52e+05/1.09e+07 =  5% of the original kernel matrix.

torch.Size([14930, 2])
We keep 1.72e+06/6.66e+07 =  2% of the original kernel matrix.

torch.Size([6035, 2])
We keep 3.55e+05/7.49e+06 =  4% of the original kernel matrix.

torch.Size([14069, 2])
We keep 1.51e+06/5.52e+07 =  2% of the original kernel matrix.

torch.Size([4513, 2])
We keep 1.80e+05/3.27e+06 =  5% of the original kernel matrix.

torch.Size([12601, 2])
We keep 1.12e+06/3.65e+07 =  3% of the original kernel matrix.

torch.Size([11381, 2])
We keep 9.77e+05/2.88e+07 =  3% of the original kernel matrix.

torch.Size([18818, 2])
We keep 2.41e+06/1.08e+08 =  2% of the original kernel matrix.

torch.Size([17689, 2])
We keep 3.38e+06/1.30e+08 =  2% of the original kernel matrix.

torch.Size([23736, 2])
We keep 4.47e+06/2.30e+08 =  1% of the original kernel matrix.

torch.Size([9043, 2])
We keep 9.77e+05/2.14e+07 =  4% of the original kernel matrix.

torch.Size([16792, 2])
We keep 2.19e+06/9.34e+07 =  2% of the original kernel matrix.

torch.Size([11099, 2])
We keep 9.99e+05/2.88e+07 =  3% of the original kernel matrix.

torch.Size([18470, 2])
We keep 2.44e+06/1.08e+08 =  2% of the original kernel matrix.

torch.Size([19060, 2])
We keep 2.75e+06/1.15e+08 =  2% of the original kernel matrix.

torch.Size([24626, 2])
We keep 4.14e+06/2.16e+08 =  1% of the original kernel matrix.

torch.Size([3158, 2])
We keep 1.06e+05/1.66e+06 =  6% of the original kernel matrix.

torch.Size([10981, 2])
We keep 8.84e+05/2.60e+07 =  3% of the original kernel matrix.

torch.Size([2843, 2])
We keep 1.15e+05/1.53e+06 =  7% of the original kernel matrix.

torch.Size([10478, 2])
We keep 8.75e+05/2.49e+07 =  3% of the original kernel matrix.

torch.Size([2857, 2])
We keep 9.45e+04/1.38e+06 =  6% of the original kernel matrix.

torch.Size([10592, 2])
We keep 8.28e+05/2.37e+07 =  3% of the original kernel matrix.

torch.Size([3647, 2])
We keep 1.48e+05/2.44e+06 =  6% of the original kernel matrix.

torch.Size([11609, 2])
We keep 1.01e+06/3.15e+07 =  3% of the original kernel matrix.

torch.Size([15330, 2])
We keep 2.83e+06/7.86e+07 =  3% of the original kernel matrix.

torch.Size([21876, 2])
We keep 3.60e+06/1.79e+08 =  2% of the original kernel matrix.

torch.Size([11246, 2])
We keep 9.50e+05/2.86e+07 =  3% of the original kernel matrix.

torch.Size([18707, 2])
We keep 2.43e+06/1.08e+08 =  2% of the original kernel matrix.

torch.Size([8287, 2])
We keep 6.33e+05/1.57e+07 =  4% of the original kernel matrix.

torch.Size([16078, 2])
We keep 1.95e+06/7.98e+07 =  2% of the original kernel matrix.

torch.Size([3194, 2])
We keep 1.16e+05/1.83e+06 =  6% of the original kernel matrix.

torch.Size([10954, 2])
We keep 9.17e+05/2.73e+07 =  3% of the original kernel matrix.

torch.Size([10138, 2])
We keep 9.87e+05/2.65e+07 =  3% of the original kernel matrix.

torch.Size([17672, 2])
We keep 2.36e+06/1.04e+08 =  2% of the original kernel matrix.

torch.Size([7859, 2])
We keep 6.04e+05/1.36e+07 =  4% of the original kernel matrix.

torch.Size([15807, 2])
We keep 1.83e+06/7.43e+07 =  2% of the original kernel matrix.

torch.Size([5062, 2])
We keep 3.11e+05/6.00e+06 =  5% of the original kernel matrix.

torch.Size([12925, 2])
We keep 1.39e+06/4.94e+07 =  2% of the original kernel matrix.

torch.Size([10853, 2])
We keep 1.13e+06/2.89e+07 =  3% of the original kernel matrix.

torch.Size([18281, 2])
We keep 2.48e+06/1.08e+08 =  2% of the original kernel matrix.

torch.Size([3775, 2])
We keep 1.51e+05/2.60e+06 =  5% of the original kernel matrix.

torch.Size([11657, 2])
We keep 1.04e+06/3.25e+07 =  3% of the original kernel matrix.

torch.Size([5390, 2])
We keep 3.27e+05/6.40e+06 =  5% of the original kernel matrix.

torch.Size([13522, 2])
We keep 1.43e+06/5.10e+07 =  2% of the original kernel matrix.

torch.Size([9986, 2])
We keep 8.71e+05/2.24e+07 =  3% of the original kernel matrix.

torch.Size([17605, 2])
We keep 2.23e+06/9.55e+07 =  2% of the original kernel matrix.

torch.Size([3827, 2])
We keep 1.74e+05/2.74e+06 =  6% of the original kernel matrix.

torch.Size([11784, 2])
We keep 1.06e+06/3.34e+07 =  3% of the original kernel matrix.

torch.Size([8563, 2])
We keep 7.26e+05/1.68e+07 =  4% of the original kernel matrix.

torch.Size([16283, 2])
We keep 2.01e+06/8.27e+07 =  2% of the original kernel matrix.

torch.Size([5402, 2])
We keep 2.99e+05/5.86e+06 =  5% of the original kernel matrix.

torch.Size([13387, 2])
We keep 1.38e+06/4.88e+07 =  2% of the original kernel matrix.

torch.Size([4924, 2])
We keep 2.47e+05/4.61e+06 =  5% of the original kernel matrix.

torch.Size([13005, 2])
We keep 1.26e+06/4.33e+07 =  2% of the original kernel matrix.

torch.Size([6474, 2])
We keep 5.21e+05/1.06e+07 =  4% of the original kernel matrix.

torch.Size([14582, 2])
We keep 1.73e+06/6.55e+07 =  2% of the original kernel matrix.

torch.Size([5253, 2])
We keep 2.38e+05/4.69e+06 =  5% of the original kernel matrix.

torch.Size([13408, 2])
We keep 1.25e+06/4.37e+07 =  2% of the original kernel matrix.

torch.Size([8853, 2])
We keep 7.54e+05/1.82e+07 =  4% of the original kernel matrix.

torch.Size([16643, 2])
We keep 2.06e+06/8.60e+07 =  2% of the original kernel matrix.

torch.Size([10997, 2])
We keep 1.01e+06/2.81e+07 =  3% of the original kernel matrix.

torch.Size([18535, 2])
We keep 2.42e+06/1.07e+08 =  2% of the original kernel matrix.

torch.Size([8499, 2])
We keep 7.01e+05/1.74e+07 =  4% of the original kernel matrix.

torch.Size([16456, 2])
We keep 2.00e+06/8.41e+07 =  2% of the original kernel matrix.

torch.Size([6527, 2])
We keep 4.62e+05/8.90e+06 =  5% of the original kernel matrix.

torch.Size([14654, 2])
We keep 1.60e+06/6.02e+07 =  2% of the original kernel matrix.

torch.Size([34307, 2])
We keep 1.33e+07/5.72e+08 =  2% of the original kernel matrix.

torch.Size([33032, 2])
We keep 7.80e+06/4.82e+08 =  1% of the original kernel matrix.

torch.Size([61204, 2])
We keep 5.97e+07/2.36e+09 =  2% of the original kernel matrix.

torch.Size([43023, 2])
We keep 1.41e+07/9.81e+08 =  1% of the original kernel matrix.

torch.Size([11093, 2])
We keep 1.47e+06/3.20e+07 =  4% of the original kernel matrix.

torch.Size([18752, 2])
We keep 2.38e+06/1.14e+08 =  2% of the original kernel matrix.

torch.Size([16501, 2])
We keep 3.72e+06/1.01e+08 =  3% of the original kernel matrix.

torch.Size([22640, 2])
We keep 3.98e+06/2.03e+08 =  1% of the original kernel matrix.

torch.Size([3865, 2])
We keep 1.93e+05/2.89e+06 =  6% of the original kernel matrix.

torch.Size([11687, 2])
We keep 1.08e+06/3.43e+07 =  3% of the original kernel matrix.

torch.Size([13471, 2])
We keep 1.42e+06/4.65e+07 =  3% of the original kernel matrix.

torch.Size([20328, 2])
We keep 2.89e+06/1.38e+08 =  2% of the original kernel matrix.

torch.Size([16810, 2])
We keep 2.86e+06/8.43e+07 =  3% of the original kernel matrix.

torch.Size([23011, 2])
We keep 3.59e+06/1.85e+08 =  1% of the original kernel matrix.

torch.Size([179191, 2])
We keep 4.70e+08/2.74e+10 =  1% of the original kernel matrix.

torch.Size([75366, 2])
We keep 4.13e+07/3.34e+09 =  1% of the original kernel matrix.

torch.Size([7707, 2])
We keep 4.77e+05/1.18e+07 =  4% of the original kernel matrix.

torch.Size([15979, 2])
We keep 1.72e+06/6.91e+07 =  2% of the original kernel matrix.

torch.Size([239981, 2])
We keep 4.14e+08/3.56e+10 =  1% of the original kernel matrix.

torch.Size([89679, 2])
We keep 4.61e+07/3.80e+09 =  1% of the original kernel matrix.

torch.Size([58506, 2])
We keep 1.36e+08/1.82e+09 =  7% of the original kernel matrix.

torch.Size([42281, 2])
We keep 1.21e+07/8.60e+08 =  1% of the original kernel matrix.

torch.Size([87326, 2])
We keep 1.06e+08/4.31e+09 =  2% of the original kernel matrix.

torch.Size([51613, 2])
We keep 1.82e+07/1.32e+09 =  1% of the original kernel matrix.

torch.Size([33688, 2])
We keep 1.34e+07/4.89e+08 =  2% of the original kernel matrix.

torch.Size([32906, 2])
We keep 7.30e+06/4.46e+08 =  1% of the original kernel matrix.

torch.Size([5802, 2])
We keep 3.81e+05/6.70e+06 =  5% of the original kernel matrix.

torch.Size([14074, 2])
We keep 1.44e+06/5.22e+07 =  2% of the original kernel matrix.

torch.Size([10817, 2])
We keep 1.13e+06/2.97e+07 =  3% of the original kernel matrix.

torch.Size([18177, 2])
We keep 2.47e+06/1.10e+08 =  2% of the original kernel matrix.

torch.Size([289970, 2])
We keep 4.43e+08/3.96e+10 =  1% of the original kernel matrix.

torch.Size([98855, 2])
We keep 4.71e+07/4.01e+09 =  1% of the original kernel matrix.

torch.Size([573168, 2])
We keep 1.32e+09/1.34e+11 =  0% of the original kernel matrix.

torch.Size([139922, 2])
We keep 8.19e+07/7.37e+09 =  1% of the original kernel matrix.

torch.Size([7716, 2])
We keep 5.40e+05/1.31e+07 =  4% of the original kernel matrix.

torch.Size([15650, 2])
We keep 1.82e+06/7.29e+07 =  2% of the original kernel matrix.

torch.Size([77781, 2])
We keep 4.13e+07/2.36e+09 =  1% of the original kernel matrix.

torch.Size([48753, 2])
We keep 1.40e+07/9.79e+08 =  1% of the original kernel matrix.

torch.Size([30261, 2])
We keep 2.16e+07/4.24e+08 =  5% of the original kernel matrix.

torch.Size([31031, 2])
We keep 6.98e+06/4.15e+08 =  1% of the original kernel matrix.

torch.Size([24477, 2])
We keep 5.71e+06/2.20e+08 =  2% of the original kernel matrix.

torch.Size([28064, 2])
We keep 5.25e+06/2.99e+08 =  1% of the original kernel matrix.

torch.Size([154666, 2])
We keep 2.77e+08/1.63e+10 =  1% of the original kernel matrix.

torch.Size([69204, 2])
We keep 3.26e+07/2.57e+09 =  1% of the original kernel matrix.

torch.Size([49113, 2])
We keep 8.56e+07/2.04e+09 =  4% of the original kernel matrix.

torch.Size([36593, 2])
We keep 1.34e+07/9.10e+08 =  1% of the original kernel matrix.

torch.Size([33476, 2])
We keep 1.17e+07/4.85e+08 =  2% of the original kernel matrix.

torch.Size([32837, 2])
We keep 7.17e+06/4.44e+08 =  1% of the original kernel matrix.

torch.Size([24583, 2])
We keep 5.75e+06/2.22e+08 =  2% of the original kernel matrix.

torch.Size([28130, 2])
We keep 5.30e+06/3.01e+08 =  1% of the original kernel matrix.

torch.Size([7931, 2])
We keep 5.86e+05/1.43e+07 =  4% of the original kernel matrix.

torch.Size([16038, 2])
We keep 1.87e+06/7.63e+07 =  2% of the original kernel matrix.

torch.Size([161583, 2])
We keep 3.04e+08/1.98e+10 =  1% of the original kernel matrix.

torch.Size([70156, 2])
We keep 3.51e+07/2.84e+09 =  1% of the original kernel matrix.

torch.Size([18246, 2])
We keep 7.89e+06/1.17e+08 =  6% of the original kernel matrix.

torch.Size([23865, 2])
We keep 4.15e+06/2.19e+08 =  1% of the original kernel matrix.

torch.Size([6550, 2])
We keep 4.33e+05/1.05e+07 =  4% of the original kernel matrix.

torch.Size([14688, 2])
We keep 1.67e+06/6.55e+07 =  2% of the original kernel matrix.

torch.Size([28921, 2])
We keep 1.16e+07/4.77e+08 =  2% of the original kernel matrix.

torch.Size([30784, 2])
We keep 7.42e+06/4.41e+08 =  1% of the original kernel matrix.

torch.Size([130709, 2])
We keep 1.95e+08/1.08e+10 =  1% of the original kernel matrix.

torch.Size([62836, 2])
We keep 2.74e+07/2.10e+09 =  1% of the original kernel matrix.

torch.Size([216927, 2])
We keep 2.92e+08/2.37e+10 =  1% of the original kernel matrix.

torch.Size([84917, 2])
We keep 3.77e+07/3.10e+09 =  1% of the original kernel matrix.

torch.Size([128721, 2])
We keep 8.41e+07/6.89e+09 =  1% of the original kernel matrix.

torch.Size([63997, 2])
We keep 2.21e+07/1.67e+09 =  1% of the original kernel matrix.

torch.Size([12340, 2])
We keep 1.69e+06/4.92e+07 =  3% of the original kernel matrix.

torch.Size([19449, 2])
We keep 2.95e+06/1.41e+08 =  2% of the original kernel matrix.

torch.Size([49969, 2])
We keep 3.36e+07/1.24e+09 =  2% of the original kernel matrix.

torch.Size([39350, 2])
We keep 1.07e+07/7.11e+08 =  1% of the original kernel matrix.

torch.Size([10578, 2])
We keep 1.30e+06/3.11e+07 =  4% of the original kernel matrix.

torch.Size([17867, 2])
We keep 2.54e+06/1.13e+08 =  2% of the original kernel matrix.

torch.Size([64491, 2])
We keep 3.24e+07/1.82e+09 =  1% of the original kernel matrix.

torch.Size([44908, 2])
We keep 1.26e+07/8.60e+08 =  1% of the original kernel matrix.

torch.Size([76164, 2])
We keep 4.64e+07/2.62e+09 =  1% of the original kernel matrix.

torch.Size([48985, 2])
We keep 1.44e+07/1.03e+09 =  1% of the original kernel matrix.

torch.Size([20803, 2])
We keep 7.02e+06/1.84e+08 =  3% of the original kernel matrix.

torch.Size([25623, 2])
We keep 4.89e+06/2.74e+08 =  1% of the original kernel matrix.

torch.Size([132443, 2])
We keep 9.82e+07/7.23e+09 =  1% of the original kernel matrix.

torch.Size([64980, 2])
We keep 2.24e+07/1.71e+09 =  1% of the original kernel matrix.

torch.Size([2286, 2])
We keep 5.72e+04/8.05e+05 =  7% of the original kernel matrix.

torch.Size([9540, 2])
We keep 6.78e+05/1.81e+07 =  3% of the original kernel matrix.

torch.Size([19886, 2])
We keep 3.67e+06/1.40e+08 =  2% of the original kernel matrix.

torch.Size([25347, 2])
We keep 4.41e+06/2.38e+08 =  1% of the original kernel matrix.

torch.Size([144311, 2])
We keep 1.42e+08/1.08e+10 =  1% of the original kernel matrix.

torch.Size([67553, 2])
We keep 2.72e+07/2.10e+09 =  1% of the original kernel matrix.

torch.Size([60078, 2])
We keep 4.92e+07/2.43e+09 =  2% of the original kernel matrix.

torch.Size([42484, 2])
We keep 1.44e+07/9.95e+08 =  1% of the original kernel matrix.

torch.Size([137842, 2])
We keep 1.48e+08/1.08e+10 =  1% of the original kernel matrix.

torch.Size([65312, 2])
We keep 2.71e+07/2.09e+09 =  1% of the original kernel matrix.

torch.Size([7320, 2])
We keep 6.78e+05/1.32e+07 =  5% of the original kernel matrix.

torch.Size([15358, 2])
We keep 1.85e+06/7.33e+07 =  2% of the original kernel matrix.

torch.Size([324251, 2])
We keep 4.23e+08/5.10e+10 =  0% of the original kernel matrix.

torch.Size([104449, 2])
We keep 5.20e+07/4.55e+09 =  1% of the original kernel matrix.

torch.Size([28421, 2])
We keep 5.46e+06/3.08e+08 =  1% of the original kernel matrix.

torch.Size([28982, 2])
We keep 5.61e+06/3.54e+08 =  1% of the original kernel matrix.

torch.Size([38962, 2])
We keep 1.14e+07/6.21e+08 =  1% of the original kernel matrix.

torch.Size([35441, 2])
We keep 8.00e+06/5.02e+08 =  1% of the original kernel matrix.

torch.Size([114738, 2])
We keep 7.92e+07/5.91e+09 =  1% of the original kernel matrix.

torch.Size([60135, 2])
We keep 2.07e+07/1.55e+09 =  1% of the original kernel matrix.

torch.Size([5064, 2])
We keep 2.52e+05/5.08e+06 =  4% of the original kernel matrix.

torch.Size([13171, 2])
We keep 1.30e+06/4.55e+07 =  2% of the original kernel matrix.

torch.Size([187235, 2])
We keep 2.33e+08/1.71e+10 =  1% of the original kernel matrix.

torch.Size([78116, 2])
We keep 3.32e+07/2.64e+09 =  1% of the original kernel matrix.

torch.Size([238347, 2])
We keep 6.73e+08/3.85e+10 =  1% of the original kernel matrix.

torch.Size([89494, 2])
We keep 4.80e+07/3.96e+09 =  1% of the original kernel matrix.

torch.Size([584525, 2])
We keep 1.72e+09/1.59e+11 =  1% of the original kernel matrix.

torch.Size([141755, 2])
We keep 8.85e+07/8.04e+09 =  1% of the original kernel matrix.

torch.Size([128840, 2])
We keep 1.08e+08/8.19e+09 =  1% of the original kernel matrix.

torch.Size([63511, 2])
We keep 2.40e+07/1.82e+09 =  1% of the original kernel matrix.

torch.Size([59421, 2])
We keep 2.93e+07/1.57e+09 =  1% of the original kernel matrix.

torch.Size([42777, 2])
We keep 1.19e+07/7.99e+08 =  1% of the original kernel matrix.

torch.Size([17925, 2])
We keep 5.98e+06/1.54e+08 =  3% of the original kernel matrix.

torch.Size([23400, 2])
We keep 4.62e+06/2.50e+08 =  1% of the original kernel matrix.

torch.Size([13027, 2])
We keep 1.31e+06/4.35e+07 =  3% of the original kernel matrix.

torch.Size([20070, 2])
We keep 2.83e+06/1.33e+08 =  2% of the original kernel matrix.

torch.Size([186176, 2])
We keep 2.98e+08/1.98e+10 =  1% of the original kernel matrix.

torch.Size([78494, 2])
We keep 3.50e+07/2.84e+09 =  1% of the original kernel matrix.

torch.Size([15547, 2])
We keep 1.97e+06/7.37e+07 =  2% of the original kernel matrix.

torch.Size([22178, 2])
We keep 3.49e+06/1.73e+08 =  2% of the original kernel matrix.

torch.Size([43417, 2])
We keep 2.88e+07/9.64e+08 =  2% of the original kernel matrix.

torch.Size([37043, 2])
We keep 9.68e+06/6.26e+08 =  1% of the original kernel matrix.

torch.Size([54010, 2])
We keep 7.77e+07/1.62e+09 =  4% of the original kernel matrix.

torch.Size([41252, 2])
We keep 1.03e+07/8.12e+08 =  1% of the original kernel matrix.

torch.Size([32480, 2])
We keep 2.39e+07/4.58e+08 =  5% of the original kernel matrix.

torch.Size([32387, 2])
We keep 6.79e+06/4.32e+08 =  1% of the original kernel matrix.

torch.Size([24036, 2])
We keep 1.01e+07/3.01e+08 =  3% of the original kernel matrix.

torch.Size([26474, 2])
We keep 5.88e+06/3.50e+08 =  1% of the original kernel matrix.

torch.Size([7672, 2])
We keep 5.32e+05/1.29e+07 =  4% of the original kernel matrix.

torch.Size([15846, 2])
We keep 1.82e+06/7.24e+07 =  2% of the original kernel matrix.

torch.Size([77464, 2])
We keep 2.22e+08/8.60e+09 =  2% of the original kernel matrix.

torch.Size([47368, 2])
We keep 2.45e+07/1.87e+09 =  1% of the original kernel matrix.

torch.Size([35374, 2])
We keep 1.39e+08/7.17e+08 = 19% of the original kernel matrix.

torch.Size([33201, 2])
We keep 8.52e+06/5.40e+08 =  1% of the original kernel matrix.

torch.Size([27086, 2])
We keep 2.80e+07/4.00e+08 =  7% of the original kernel matrix.

torch.Size([29148, 2])
We keep 6.78e+06/4.03e+08 =  1% of the original kernel matrix.

torch.Size([10651, 2])
We keep 1.12e+06/2.98e+07 =  3% of the original kernel matrix.

torch.Size([18111, 2])
We keep 2.47e+06/1.10e+08 =  2% of the original kernel matrix.

torch.Size([83218, 2])
We keep 9.04e+07/3.60e+09 =  2% of the original kernel matrix.

torch.Size([50833, 2])
We keep 1.62e+07/1.21e+09 =  1% of the original kernel matrix.

torch.Size([56502, 2])
We keep 2.54e+07/1.47e+09 =  1% of the original kernel matrix.

torch.Size([42432, 2])
We keep 1.16e+07/7.73e+08 =  1% of the original kernel matrix.

torch.Size([222453, 2])
We keep 5.62e+08/3.18e+10 =  1% of the original kernel matrix.

torch.Size([84380, 2])
We keep 4.38e+07/3.60e+09 =  1% of the original kernel matrix.

torch.Size([11439, 2])
We keep 1.76e+06/4.43e+07 =  3% of the original kernel matrix.

torch.Size([18952, 2])
We keep 2.86e+06/1.34e+08 =  2% of the original kernel matrix.

torch.Size([46029, 2])
We keep 3.18e+07/1.46e+09 =  2% of the original kernel matrix.

torch.Size([36747, 2])
We keep 1.16e+07/7.71e+08 =  1% of the original kernel matrix.

torch.Size([20154, 2])
We keep 4.31e+06/1.53e+08 =  2% of the original kernel matrix.

torch.Size([25465, 2])
We keep 4.44e+06/2.50e+08 =  1% of the original kernel matrix.

torch.Size([134088, 2])
We keep 3.36e+08/1.41e+10 =  2% of the original kernel matrix.

torch.Size([63188, 2])
We keep 2.99e+07/2.39e+09 =  1% of the original kernel matrix.

torch.Size([44792, 2])
We keep 1.65e+07/1.12e+09 =  1% of the original kernel matrix.

torch.Size([37527, 2])
We keep 9.94e+06/6.74e+08 =  1% of the original kernel matrix.

torch.Size([27943, 2])
We keep 8.24e+06/2.98e+08 =  2% of the original kernel matrix.

torch.Size([29072, 2])
We keep 5.64e+06/3.48e+08 =  1% of the original kernel matrix.

torch.Size([172916, 2])
We keep 1.22e+08/1.27e+10 =  0% of the original kernel matrix.

torch.Size([75313, 2])
We keep 2.85e+07/2.27e+09 =  1% of the original kernel matrix.

torch.Size([216517, 2])
We keep 1.93e+08/1.96e+10 =  0% of the original kernel matrix.

torch.Size([85820, 2])
We keep 3.47e+07/2.83e+09 =  1% of the original kernel matrix.

torch.Size([9988, 2])
We keep 1.15e+06/2.45e+07 =  4% of the original kernel matrix.

torch.Size([17805, 2])
We keep 2.21e+06/9.98e+07 =  2% of the original kernel matrix.

torch.Size([254465, 2])
We keep 6.30e+08/3.22e+10 =  1% of the original kernel matrix.

torch.Size([93214, 2])
We keep 4.37e+07/3.62e+09 =  1% of the original kernel matrix.

torch.Size([194983, 2])
We keep 1.89e+08/1.98e+10 =  0% of the original kernel matrix.

torch.Size([80361, 2])
We keep 3.50e+07/2.84e+09 =  1% of the original kernel matrix.

torch.Size([525580, 2])
We keep 1.06e+09/1.27e+11 =  0% of the original kernel matrix.

torch.Size([133279, 2])
We keep 8.00e+07/7.19e+09 =  1% of the original kernel matrix.

torch.Size([23904, 2])
We keep 8.48e+06/2.41e+08 =  3% of the original kernel matrix.

torch.Size([27831, 2])
We keep 5.41e+06/3.13e+08 =  1% of the original kernel matrix.

torch.Size([6564, 2])
We keep 4.63e+05/9.70e+06 =  4% of the original kernel matrix.

torch.Size([14608, 2])
We keep 1.65e+06/6.28e+07 =  2% of the original kernel matrix.

torch.Size([22873, 2])
We keep 3.94e+06/1.88e+08 =  2% of the original kernel matrix.

torch.Size([27429, 2])
We keep 4.99e+06/2.77e+08 =  1% of the original kernel matrix.

torch.Size([24979, 2])
We keep 1.37e+07/3.66e+08 =  3% of the original kernel matrix.

torch.Size([28096, 2])
We keep 5.98e+06/3.86e+08 =  1% of the original kernel matrix.

torch.Size([30359, 2])
We keep 7.22e+06/3.57e+08 =  2% of the original kernel matrix.

torch.Size([31268, 2])
We keep 6.38e+06/3.81e+08 =  1% of the original kernel matrix.

torch.Size([108247, 2])
We keep 1.68e+08/5.98e+09 =  2% of the original kernel matrix.

torch.Size([58367, 2])
We keep 2.13e+07/1.56e+09 =  1% of the original kernel matrix.

torch.Size([4057, 2])
We keep 1.76e+05/3.05e+06 =  5% of the original kernel matrix.

torch.Size([12016, 2])
We keep 1.09e+06/3.52e+07 =  3% of the original kernel matrix.

torch.Size([101036, 2])
We keep 7.19e+07/4.03e+09 =  1% of the original kernel matrix.

torch.Size([55955, 2])
We keep 1.76e+07/1.28e+09 =  1% of the original kernel matrix.

torch.Size([162267, 2])
We keep 1.57e+08/1.40e+10 =  1% of the original kernel matrix.

torch.Size([72660, 2])
We keep 3.05e+07/2.39e+09 =  1% of the original kernel matrix.

torch.Size([7969, 2])
We keep 1.54e+07/5.78e+07 = 26% of the original kernel matrix.

torch.Size([15238, 2])
We keep 2.92e+06/1.53e+08 =  1% of the original kernel matrix.

torch.Size([13597, 2])
We keep 2.03e+06/5.42e+07 =  3% of the original kernel matrix.

torch.Size([20449, 2])
We keep 3.07e+06/1.48e+08 =  2% of the original kernel matrix.

torch.Size([91048, 2])
We keep 5.85e+07/3.51e+09 =  1% of the original kernel matrix.

torch.Size([53134, 2])
We keep 1.62e+07/1.19e+09 =  1% of the original kernel matrix.

torch.Size([16445, 2])
We keep 2.66e+06/8.88e+07 =  2% of the original kernel matrix.

torch.Size([22748, 2])
We keep 3.74e+06/1.90e+08 =  1% of the original kernel matrix.

torch.Size([14193, 2])
We keep 2.47e+06/6.21e+07 =  3% of the original kernel matrix.

torch.Size([20918, 2])
We keep 3.23e+06/1.59e+08 =  2% of the original kernel matrix.

torch.Size([21792, 2])
We keep 3.70e+06/1.80e+08 =  2% of the original kernel matrix.

torch.Size([26404, 2])
We keep 4.79e+06/2.70e+08 =  1% of the original kernel matrix.

torch.Size([8545, 2])
We keep 6.36e+05/1.57e+07 =  4% of the original kernel matrix.

torch.Size([16541, 2])
We keep 1.93e+06/7.98e+07 =  2% of the original kernel matrix.

torch.Size([41902, 2])
We keep 2.25e+07/7.63e+08 =  2% of the original kernel matrix.

torch.Size([36475, 2])
We keep 8.42e+06/5.57e+08 =  1% of the original kernel matrix.

torch.Size([60232, 2])
We keep 2.03e+07/1.45e+09 =  1% of the original kernel matrix.

torch.Size([43790, 2])
We keep 1.13e+07/7.67e+08 =  1% of the original kernel matrix.

torch.Size([32761, 2])
We keep 9.64e+06/4.68e+08 =  2% of the original kernel matrix.

torch.Size([32765, 2])
We keep 7.12e+06/4.36e+08 =  1% of the original kernel matrix.

torch.Size([59031, 2])
We keep 4.70e+07/2.03e+09 =  2% of the original kernel matrix.

torch.Size([42666, 2])
We keep 1.33e+07/9.09e+08 =  1% of the original kernel matrix.

torch.Size([22275, 2])
We keep 4.14e+06/1.82e+08 =  2% of the original kernel matrix.

torch.Size([26800, 2])
We keep 4.97e+06/2.72e+08 =  1% of the original kernel matrix.

torch.Size([925137, 2])
We keep 3.93e+09/4.02e+11 =  0% of the original kernel matrix.

torch.Size([182895, 2])
We keep 1.38e+08/1.28e+10 =  1% of the original kernel matrix.

torch.Size([33014, 2])
We keep 9.39e+06/4.56e+08 =  2% of the original kernel matrix.

torch.Size([32892, 2])
We keep 7.12e+06/4.31e+08 =  1% of the original kernel matrix.

torch.Size([175209, 2])
We keep 1.49e+08/1.35e+10 =  1% of the original kernel matrix.

torch.Size([75777, 2])
We keep 2.95e+07/2.34e+09 =  1% of the original kernel matrix.

torch.Size([4976, 2])
We keep 2.36e+05/4.70e+06 =  5% of the original kernel matrix.

torch.Size([13117, 2])
We keep 1.27e+06/4.37e+07 =  2% of the original kernel matrix.

torch.Size([14510, 2])
We keep 2.31e+06/6.08e+07 =  3% of the original kernel matrix.

torch.Size([21244, 2])
We keep 3.21e+06/1.57e+08 =  2% of the original kernel matrix.

torch.Size([8317, 2])
We keep 5.93e+05/1.47e+07 =  4% of the original kernel matrix.

torch.Size([16268, 2])
We keep 1.90e+06/7.73e+07 =  2% of the original kernel matrix.

torch.Size([32428, 2])
We keep 7.53e+06/4.01e+08 =  1% of the original kernel matrix.

torch.Size([32856, 2])
We keep 6.63e+06/4.04e+08 =  1% of the original kernel matrix.

torch.Size([9333, 2])
We keep 1.08e+06/2.45e+07 =  4% of the original kernel matrix.

torch.Size([17427, 2])
We keep 2.36e+06/9.98e+07 =  2% of the original kernel matrix.

torch.Size([125879, 2])
We keep 2.30e+08/9.25e+09 =  2% of the original kernel matrix.

torch.Size([62798, 2])
We keep 2.54e+07/1.94e+09 =  1% of the original kernel matrix.

torch.Size([14613, 2])
We keep 2.04e+06/6.85e+07 =  2% of the original kernel matrix.

torch.Size([21358, 2])
We keep 3.48e+06/1.67e+08 =  2% of the original kernel matrix.

torch.Size([9309, 2])
We keep 8.65e+05/2.39e+07 =  3% of the original kernel matrix.

torch.Size([16919, 2])
We keep 2.30e+06/9.85e+07 =  2% of the original kernel matrix.

torch.Size([263165, 2])
We keep 4.36e+08/4.13e+10 =  1% of the original kernel matrix.

torch.Size([95042, 2])
We keep 4.92e+07/4.10e+09 =  1% of the original kernel matrix.

torch.Size([85817, 2])
We keep 5.83e+07/3.48e+09 =  1% of the original kernel matrix.

torch.Size([51435, 2])
We keep 1.68e+07/1.19e+09 =  1% of the original kernel matrix.

torch.Size([51545, 2])
We keep 2.37e+07/1.23e+09 =  1% of the original kernel matrix.

torch.Size([39876, 2])
We keep 1.07e+07/7.08e+08 =  1% of the original kernel matrix.

torch.Size([162562, 2])
We keep 1.65e+08/1.39e+10 =  1% of the original kernel matrix.

torch.Size([73163, 2])
We keep 3.05e+07/2.38e+09 =  1% of the original kernel matrix.

torch.Size([445050, 2])
We keep 7.83e+08/9.25e+10 =  0% of the original kernel matrix.

torch.Size([122056, 2])
We keep 7.04e+07/6.14e+09 =  1% of the original kernel matrix.

torch.Size([14357, 2])
We keep 2.31e+06/7.91e+07 =  2% of the original kernel matrix.

torch.Size([21120, 2])
We keep 3.72e+06/1.79e+08 =  2% of the original kernel matrix.

torch.Size([18001, 2])
We keep 2.48e+06/1.01e+08 =  2% of the original kernel matrix.

torch.Size([23904, 2])
We keep 3.90e+06/2.03e+08 =  1% of the original kernel matrix.

torch.Size([25504, 2])
We keep 1.07e+07/3.23e+08 =  3% of the original kernel matrix.

torch.Size([28813, 2])
We keep 6.34e+06/3.63e+08 =  1% of the original kernel matrix.

torch.Size([5082, 2])
We keep 2.75e+05/5.40e+06 =  5% of the original kernel matrix.

torch.Size([13251, 2])
We keep 1.32e+06/4.68e+07 =  2% of the original kernel matrix.

torch.Size([13903, 2])
We keep 1.69e+06/5.34e+07 =  3% of the original kernel matrix.

torch.Size([20703, 2])
We keep 2.98e+06/1.47e+08 =  2% of the original kernel matrix.

torch.Size([22915, 2])
We keep 5.32e+06/2.38e+08 =  2% of the original kernel matrix.

torch.Size([27184, 2])
We keep 5.52e+06/3.11e+08 =  1% of the original kernel matrix.

torch.Size([8049, 2])
We keep 7.71e+05/1.63e+07 =  4% of the original kernel matrix.

torch.Size([15882, 2])
We keep 1.94e+06/8.14e+07 =  2% of the original kernel matrix.

torch.Size([14455, 2])
We keep 1.97e+06/6.61e+07 =  2% of the original kernel matrix.

torch.Size([21028, 2])
We keep 3.37e+06/1.64e+08 =  2% of the original kernel matrix.

torch.Size([31508, 2])
We keep 9.51e+06/4.08e+08 =  2% of the original kernel matrix.

torch.Size([32212, 2])
We keep 6.68e+06/4.07e+08 =  1% of the original kernel matrix.

torch.Size([15316, 2])
We keep 2.55e+06/7.42e+07 =  3% of the original kernel matrix.

torch.Size([22008, 2])
We keep 3.42e+06/1.74e+08 =  1% of the original kernel matrix.

torch.Size([386034, 2])
We keep 7.21e+08/6.41e+10 =  1% of the original kernel matrix.

torch.Size([113858, 2])
We keep 5.76e+07/5.11e+09 =  1% of the original kernel matrix.

torch.Size([17395, 2])
We keep 4.15e+06/1.36e+08 =  3% of the original kernel matrix.

torch.Size([23282, 2])
We keep 4.47e+06/2.35e+08 =  1% of the original kernel matrix.

torch.Size([93837, 2])
We keep 7.37e+07/4.04e+09 =  1% of the original kernel matrix.

torch.Size([53808, 2])
We keep 1.75e+07/1.28e+09 =  1% of the original kernel matrix.

torch.Size([8875, 2])
We keep 2.71e+06/2.89e+07 =  9% of the original kernel matrix.

torch.Size([16840, 2])
We keep 2.28e+06/1.08e+08 =  2% of the original kernel matrix.

torch.Size([6481, 2])
We keep 4.87e+05/1.02e+07 =  4% of the original kernel matrix.

torch.Size([14433, 2])
We keep 1.68e+06/6.43e+07 =  2% of the original kernel matrix.

torch.Size([205790, 2])
We keep 2.31e+08/2.05e+10 =  1% of the original kernel matrix.

torch.Size([82271, 2])
We keep 3.57e+07/2.89e+09 =  1% of the original kernel matrix.

torch.Size([454904, 2])
We keep 1.67e+09/9.95e+10 =  1% of the original kernel matrix.

torch.Size([124224, 2])
We keep 6.74e+07/6.36e+09 =  1% of the original kernel matrix.

torch.Size([597283, 2])
We keep 1.09e+09/1.51e+11 =  0% of the original kernel matrix.

torch.Size([143716, 2])
We keep 8.59e+07/7.84e+09 =  1% of the original kernel matrix.

torch.Size([27045, 2])
We keep 1.17e+07/2.91e+08 =  4% of the original kernel matrix.

torch.Size([29385, 2])
We keep 5.71e+06/3.44e+08 =  1% of the original kernel matrix.

torch.Size([58412, 2])
We keep 2.42e+07/1.43e+09 =  1% of the original kernel matrix.

torch.Size([43095, 2])
We keep 1.12e+07/7.63e+08 =  1% of the original kernel matrix.

torch.Size([12361, 2])
We keep 1.25e+06/3.84e+07 =  3% of the original kernel matrix.

torch.Size([19479, 2])
We keep 2.67e+06/1.25e+08 =  2% of the original kernel matrix.

torch.Size([8764, 2])
We keep 1.62e+06/1.95e+07 =  8% of the original kernel matrix.

torch.Size([16767, 2])
We keep 2.05e+06/8.90e+07 =  2% of the original kernel matrix.

torch.Size([461891, 2])
We keep 4.03e+09/1.50e+11 =  2% of the original kernel matrix.

torch.Size([123138, 2])
We keep 8.49e+07/7.80e+09 =  1% of the original kernel matrix.

torch.Size([27354, 2])
We keep 5.92e+06/2.98e+08 =  1% of the original kernel matrix.

torch.Size([30358, 2])
We keep 6.00e+06/3.48e+08 =  1% of the original kernel matrix.

torch.Size([15465, 2])
We keep 2.11e+06/7.34e+07 =  2% of the original kernel matrix.

torch.Size([21877, 2])
We keep 3.48e+06/1.73e+08 =  2% of the original kernel matrix.

torch.Size([58614, 2])
We keep 3.37e+07/1.86e+09 =  1% of the original kernel matrix.

torch.Size([42899, 2])
We keep 1.27e+07/8.70e+08 =  1% of the original kernel matrix.

torch.Size([774977, 2])
We keep 2.93e+09/3.47e+11 =  0% of the original kernel matrix.

torch.Size([165503, 2])
We keep 1.30e+08/1.19e+10 =  1% of the original kernel matrix.

torch.Size([64621, 2])
We keep 4.03e+07/2.06e+09 =  1% of the original kernel matrix.

torch.Size([43923, 2])
We keep 1.30e+07/9.16e+08 =  1% of the original kernel matrix.

torch.Size([24158, 2])
We keep 6.20e+06/2.70e+08 =  2% of the original kernel matrix.

torch.Size([28106, 2])
We keep 5.87e+06/3.31e+08 =  1% of the original kernel matrix.

torch.Size([20056, 2])
We keep 5.09e+06/1.74e+08 =  2% of the original kernel matrix.

torch.Size([25436, 2])
We keep 4.90e+06/2.66e+08 =  1% of the original kernel matrix.

torch.Size([24206, 2])
We keep 4.35e+06/2.13e+08 =  2% of the original kernel matrix.

torch.Size([27927, 2])
We keep 5.32e+06/2.94e+08 =  1% of the original kernel matrix.

torch.Size([12708, 2])
We keep 1.71e+06/4.33e+07 =  3% of the original kernel matrix.

torch.Size([19825, 2])
We keep 2.86e+06/1.33e+08 =  2% of the original kernel matrix.

torch.Size([237452, 2])
We keep 3.10e+08/2.54e+10 =  1% of the original kernel matrix.

torch.Size([89622, 2])
We keep 3.91e+07/3.21e+09 =  1% of the original kernel matrix.

torch.Size([21828, 2])
We keep 5.66e+06/1.98e+08 =  2% of the original kernel matrix.

torch.Size([26225, 2])
We keep 5.10e+06/2.84e+08 =  1% of the original kernel matrix.

torch.Size([5483, 2])
We keep 4.69e+05/7.04e+06 =  6% of the original kernel matrix.

torch.Size([13452, 2])
We keep 1.48e+06/5.35e+07 =  2% of the original kernel matrix.

torch.Size([152956, 2])
We keep 1.35e+08/1.04e+10 =  1% of the original kernel matrix.

torch.Size([70919, 2])
We keep 2.61e+07/2.06e+09 =  1% of the original kernel matrix.

torch.Size([17292, 2])
We keep 3.69e+06/1.19e+08 =  3% of the original kernel matrix.

torch.Size([23457, 2])
We keep 4.37e+06/2.20e+08 =  1% of the original kernel matrix.

torch.Size([3213, 2])
We keep 1.07e+05/1.61e+06 =  6% of the original kernel matrix.

torch.Size([11120, 2])
We keep 8.73e+05/2.56e+07 =  3% of the original kernel matrix.

torch.Size([86467, 2])
We keep 6.11e+07/3.15e+09 =  1% of the original kernel matrix.

torch.Size([51341, 2])
We keep 1.57e+07/1.13e+09 =  1% of the original kernel matrix.

torch.Size([116947, 2])
We keep 9.91e+07/7.05e+09 =  1% of the original kernel matrix.

torch.Size([60771, 2])
We keep 2.28e+07/1.69e+09 =  1% of the original kernel matrix.

torch.Size([267685, 2])
We keep 7.13e+08/4.61e+10 =  1% of the original kernel matrix.

torch.Size([95014, 2])
We keep 5.20e+07/4.33e+09 =  1% of the original kernel matrix.

torch.Size([222121, 2])
We keep 4.43e+08/2.49e+10 =  1% of the original kernel matrix.

torch.Size([85733, 2])
We keep 3.92e+07/3.18e+09 =  1% of the original kernel matrix.

torch.Size([235663, 2])
We keep 4.00e+08/2.96e+10 =  1% of the original kernel matrix.

torch.Size([88168, 2])
We keep 4.20e+07/3.47e+09 =  1% of the original kernel matrix.

torch.Size([33625, 2])
We keep 1.69e+07/6.59e+08 =  2% of the original kernel matrix.

torch.Size([32007, 2])
We keep 8.34e+06/5.18e+08 =  1% of the original kernel matrix.

torch.Size([34452, 2])
We keep 1.48e+07/5.02e+08 =  2% of the original kernel matrix.

torch.Size([33304, 2])
We keep 7.21e+06/4.52e+08 =  1% of the original kernel matrix.

torch.Size([231296, 2])
We keep 2.42e+08/2.39e+10 =  1% of the original kernel matrix.

torch.Size([88576, 2])
We keep 3.81e+07/3.12e+09 =  1% of the original kernel matrix.

torch.Size([233375, 2])
We keep 2.69e+08/2.51e+10 =  1% of the original kernel matrix.

torch.Size([89035, 2])
We keep 3.82e+07/3.20e+09 =  1% of the original kernel matrix.

torch.Size([23087, 2])
We keep 4.60e+06/2.00e+08 =  2% of the original kernel matrix.

torch.Size([27351, 2])
We keep 5.10e+06/2.85e+08 =  1% of the original kernel matrix.

torch.Size([124293, 2])
We keep 1.10e+09/1.31e+10 =  8% of the original kernel matrix.

torch.Size([63567, 2])
We keep 2.70e+07/2.31e+09 =  1% of the original kernel matrix.

torch.Size([233832, 2])
We keep 1.75e+09/2.85e+10 =  6% of the original kernel matrix.

torch.Size([88604, 2])
We keep 4.03e+07/3.41e+09 =  1% of the original kernel matrix.

torch.Size([220794, 2])
We keep 3.66e+08/2.56e+10 =  1% of the original kernel matrix.

torch.Size([86035, 2])
We keep 3.93e+07/3.23e+09 =  1% of the original kernel matrix.

torch.Size([149348, 2])
We keep 1.64e+08/1.13e+10 =  1% of the original kernel matrix.

torch.Size([69223, 2])
We keep 2.75e+07/2.14e+09 =  1% of the original kernel matrix.

torch.Size([51067, 2])
We keep 8.20e+07/2.10e+09 =  3% of the original kernel matrix.

torch.Size([39001, 2])
We keep 1.38e+07/9.25e+08 =  1% of the original kernel matrix.

torch.Size([23656, 2])
We keep 8.52e+06/2.80e+08 =  3% of the original kernel matrix.

torch.Size([26481, 2])
We keep 5.67e+06/3.37e+08 =  1% of the original kernel matrix.

torch.Size([15280, 2])
We keep 1.77e+06/6.84e+07 =  2% of the original kernel matrix.

torch.Size([22002, 2])
We keep 3.33e+06/1.67e+08 =  1% of the original kernel matrix.

torch.Size([17651, 2])
We keep 5.41e+06/1.13e+08 =  4% of the original kernel matrix.

torch.Size([23485, 2])
We keep 3.95e+06/2.14e+08 =  1% of the original kernel matrix.

torch.Size([165746, 2])
We keep 1.16e+08/1.19e+10 =  0% of the original kernel matrix.

torch.Size([73784, 2])
We keep 2.81e+07/2.20e+09 =  1% of the original kernel matrix.

torch.Size([177932, 2])
We keep 1.33e+08/1.33e+10 =  1% of the original kernel matrix.

torch.Size([76605, 2])
We keep 2.91e+07/2.33e+09 =  1% of the original kernel matrix.

torch.Size([305013, 2])
We keep 6.73e+08/5.73e+10 =  1% of the original kernel matrix.

torch.Size([100985, 2])
We keep 5.73e+07/4.83e+09 =  1% of the original kernel matrix.

torch.Size([118881, 2])
We keep 1.23e+08/7.12e+09 =  1% of the original kernel matrix.

torch.Size([61197, 2])
We keep 2.22e+07/1.70e+09 =  1% of the original kernel matrix.

torch.Size([92268, 2])
We keep 6.42e+07/4.10e+09 =  1% of the original kernel matrix.

torch.Size([53543, 2])
We keep 1.77e+07/1.29e+09 =  1% of the original kernel matrix.

torch.Size([21720, 2])
We keep 4.07e+06/1.96e+08 =  2% of the original kernel matrix.

torch.Size([26176, 2])
We keep 5.00e+06/2.82e+08 =  1% of the original kernel matrix.

torch.Size([213684, 2])
We keep 2.35e+08/1.96e+10 =  1% of the original kernel matrix.

torch.Size([85078, 2])
We keep 3.48e+07/2.82e+09 =  1% of the original kernel matrix.

torch.Size([12708, 2])
We keep 2.92e+06/6.88e+07 =  4% of the original kernel matrix.

torch.Size([19461, 2])
We keep 3.44e+06/1.67e+08 =  2% of the original kernel matrix.

torch.Size([19329, 2])
We keep 3.77e+06/1.38e+08 =  2% of the original kernel matrix.

torch.Size([24672, 2])
We keep 4.44e+06/2.37e+08 =  1% of the original kernel matrix.

torch.Size([171789, 2])
We keep 1.37e+08/1.25e+10 =  1% of the original kernel matrix.

torch.Size([74961, 2])
We keep 2.83e+07/2.25e+09 =  1% of the original kernel matrix.

torch.Size([100386, 2])
We keep 7.96e+07/4.29e+09 =  1% of the original kernel matrix.

torch.Size([55759, 2])
We keep 1.81e+07/1.32e+09 =  1% of the original kernel matrix.

torch.Size([10592, 2])
We keep 1.17e+06/2.98e+07 =  3% of the original kernel matrix.

torch.Size([18041, 2])
We keep 2.50e+06/1.10e+08 =  2% of the original kernel matrix.

torch.Size([31498, 2])
We keep 7.75e+06/3.98e+08 =  1% of the original kernel matrix.

torch.Size([31893, 2])
We keep 6.53e+06/4.02e+08 =  1% of the original kernel matrix.

torch.Size([711043, 2])
We keep 3.95e+09/3.19e+11 =  1% of the original kernel matrix.

torch.Size([157749, 2])
We keep 1.26e+08/1.14e+10 =  1% of the original kernel matrix.

torch.Size([24368, 2])
We keep 5.30e+06/2.43e+08 =  2% of the original kernel matrix.

torch.Size([28047, 2])
We keep 5.55e+06/3.15e+08 =  1% of the original kernel matrix.

torch.Size([164800, 2])
We keep 2.26e+08/1.45e+10 =  1% of the original kernel matrix.

torch.Size([73305, 2])
We keep 3.09e+07/2.43e+09 =  1% of the original kernel matrix.

torch.Size([10301, 2])
We keep 8.56e+05/2.33e+07 =  3% of the original kernel matrix.

torch.Size([17899, 2])
We keep 2.25e+06/9.73e+07 =  2% of the original kernel matrix.

torch.Size([7929, 2])
We keep 5.77e+05/1.36e+07 =  4% of the original kernel matrix.

torch.Size([15965, 2])
We keep 1.86e+06/7.44e+07 =  2% of the original kernel matrix.

torch.Size([9656, 2])
We keep 9.51e+05/2.62e+07 =  3% of the original kernel matrix.

torch.Size([17253, 2])
We keep 2.35e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([8598, 2])
We keep 1.02e+06/2.05e+07 =  4% of the original kernel matrix.

torch.Size([16410, 2])
We keep 2.08e+06/9.14e+07 =  2% of the original kernel matrix.

torch.Size([31922, 2])
We keep 9.36e+06/4.16e+08 =  2% of the original kernel matrix.

torch.Size([32344, 2])
We keep 6.78e+06/4.11e+08 =  1% of the original kernel matrix.

torch.Size([85832, 2])
We keep 6.17e+07/3.30e+09 =  1% of the original kernel matrix.

torch.Size([50973, 2])
We keep 1.64e+07/1.16e+09 =  1% of the original kernel matrix.

torch.Size([8496, 2])
We keep 1.10e+06/1.75e+07 =  6% of the original kernel matrix.

torch.Size([16330, 2])
We keep 2.03e+06/8.44e+07 =  2% of the original kernel matrix.

torch.Size([149757, 2])
We keep 1.76e+08/9.99e+09 =  1% of the original kernel matrix.

torch.Size([69520, 2])
We keep 2.57e+07/2.02e+09 =  1% of the original kernel matrix.

torch.Size([18404, 2])
We keep 4.06e+06/1.34e+08 =  3% of the original kernel matrix.

torch.Size([24107, 2])
We keep 4.33e+06/2.34e+08 =  1% of the original kernel matrix.

torch.Size([77627, 2])
We keep 5.22e+07/2.38e+09 =  2% of the original kernel matrix.

torch.Size([49216, 2])
We keep 1.41e+07/9.84e+08 =  1% of the original kernel matrix.

torch.Size([17691, 2])
We keep 4.05e+06/1.29e+08 =  3% of the original kernel matrix.

torch.Size([23482, 2])
We keep 4.37e+06/2.29e+08 =  1% of the original kernel matrix.

torch.Size([226432, 2])
We keep 2.76e+08/2.59e+10 =  1% of the original kernel matrix.

torch.Size([87558, 2])
We keep 3.94e+07/3.25e+09 =  1% of the original kernel matrix.

torch.Size([9971, 2])
We keep 1.35e+06/2.81e+07 =  4% of the original kernel matrix.

torch.Size([17595, 2])
We keep 2.42e+06/1.07e+08 =  2% of the original kernel matrix.

torch.Size([45213, 2])
We keep 2.05e+07/8.69e+08 =  2% of the original kernel matrix.

torch.Size([37779, 2])
We keep 8.99e+06/5.94e+08 =  1% of the original kernel matrix.

torch.Size([53191, 2])
We keep 1.89e+07/1.13e+09 =  1% of the original kernel matrix.

torch.Size([41253, 2])
We keep 1.02e+07/6.79e+08 =  1% of the original kernel matrix.

torch.Size([35207, 2])
We keep 1.55e+07/6.17e+08 =  2% of the original kernel matrix.

torch.Size([33406, 2])
We keep 8.07e+06/5.01e+08 =  1% of the original kernel matrix.

torch.Size([91747, 2])
We keep 5.15e+07/3.52e+09 =  1% of the original kernel matrix.

torch.Size([53663, 2])
We keep 1.66e+07/1.20e+09 =  1% of the original kernel matrix.

torch.Size([521868, 2])
We keep 2.42e+09/1.31e+11 =  1% of the original kernel matrix.

torch.Size([133406, 2])
We keep 8.11e+07/7.30e+09 =  1% of the original kernel matrix.

torch.Size([30831, 2])
We keep 6.44e+06/3.86e+08 =  1% of the original kernel matrix.

torch.Size([32157, 2])
We keep 6.56e+06/3.96e+08 =  1% of the original kernel matrix.

torch.Size([11814, 2])
We keep 1.80e+06/3.84e+07 =  4% of the original kernel matrix.

torch.Size([19100, 2])
We keep 2.64e+06/1.25e+08 =  2% of the original kernel matrix.

torch.Size([50804, 2])
We keep 2.63e+07/1.24e+09 =  2% of the original kernel matrix.

torch.Size([40081, 2])
We keep 1.06e+07/7.10e+08 =  1% of the original kernel matrix.

torch.Size([34931, 2])
We keep 1.14e+07/5.79e+08 =  1% of the original kernel matrix.

torch.Size([33512, 2])
We keep 7.89e+06/4.85e+08 =  1% of the original kernel matrix.

torch.Size([646387, 2])
We keep 1.19e+09/1.70e+11 =  0% of the original kernel matrix.

torch.Size([150473, 2])
We keep 9.08e+07/8.31e+09 =  1% of the original kernel matrix.

torch.Size([12527, 2])
We keep 1.93e+06/5.14e+07 =  3% of the original kernel matrix.

torch.Size([19544, 2])
We keep 3.01e+06/1.45e+08 =  2% of the original kernel matrix.

torch.Size([157309, 2])
We keep 1.62e+08/1.39e+10 =  1% of the original kernel matrix.

torch.Size([71156, 2])
We keep 3.06e+07/2.38e+09 =  1% of the original kernel matrix.

torch.Size([417370, 2])
We keep 6.30e+08/8.28e+10 =  0% of the original kernel matrix.

torch.Size([118577, 2])
We keep 6.60e+07/5.80e+09 =  1% of the original kernel matrix.

torch.Size([13160, 2])
We keep 2.73e+06/5.81e+07 =  4% of the original kernel matrix.

torch.Size([20111, 2])
We keep 3.12e+06/1.54e+08 =  2% of the original kernel matrix.

torch.Size([179198, 2])
We keep 4.53e+08/2.12e+10 =  2% of the original kernel matrix.

torch.Size([75411, 2])
We keep 3.59e+07/2.93e+09 =  1% of the original kernel matrix.

torch.Size([19650, 2])
We keep 6.89e+06/2.12e+08 =  3% of the original kernel matrix.

torch.Size([24578, 2])
We keep 5.28e+06/2.94e+08 =  1% of the original kernel matrix.

torch.Size([692744, 2])
We keep 2.09e+09/2.36e+11 =  0% of the original kernel matrix.

torch.Size([156808, 2])
We keep 1.08e+08/9.79e+09 =  1% of the original kernel matrix.

torch.Size([28587, 2])
We keep 6.73e+06/3.74e+08 =  1% of the original kernel matrix.

torch.Size([30556, 2])
We keep 6.57e+06/3.90e+08 =  1% of the original kernel matrix.

torch.Size([23144, 2])
We keep 6.26e+06/1.97e+08 =  3% of the original kernel matrix.

torch.Size([27110, 2])
We keep 5.09e+06/2.83e+08 =  1% of the original kernel matrix.

torch.Size([14420, 2])
We keep 4.21e+06/1.09e+08 =  3% of the original kernel matrix.

torch.Size([20660, 2])
We keep 4.10e+06/2.11e+08 =  1% of the original kernel matrix.

torch.Size([7394, 2])
We keep 6.42e+05/1.21e+07 =  5% of the original kernel matrix.

torch.Size([15447, 2])
We keep 1.77e+06/7.02e+07 =  2% of the original kernel matrix.

torch.Size([13335, 2])
We keep 1.88e+06/5.72e+07 =  3% of the original kernel matrix.

torch.Size([20169, 2])
We keep 3.15e+06/1.53e+08 =  2% of the original kernel matrix.

torch.Size([76007, 2])
We keep 1.40e+08/2.98e+09 =  4% of the original kernel matrix.

torch.Size([48841, 2])
We keep 1.53e+07/1.10e+09 =  1% of the original kernel matrix.

torch.Size([10386, 2])
We keep 8.40e+05/2.46e+07 =  3% of the original kernel matrix.

torch.Size([18155, 2])
We keep 2.27e+06/9.99e+07 =  2% of the original kernel matrix.

torch.Size([183059, 2])
We keep 2.18e+08/1.86e+10 =  1% of the original kernel matrix.

torch.Size([77656, 2])
We keep 3.46e+07/2.75e+09 =  1% of the original kernel matrix.

torch.Size([22964, 2])
We keep 4.26e+06/2.09e+08 =  2% of the original kernel matrix.

torch.Size([27334, 2])
We keep 5.19e+06/2.91e+08 =  1% of the original kernel matrix.

torch.Size([15806, 2])
We keep 3.19e+06/7.72e+07 =  4% of the original kernel matrix.

torch.Size([22266, 2])
We keep 3.56e+06/1.77e+08 =  2% of the original kernel matrix.

torch.Size([25273, 2])
We keep 7.81e+06/3.55e+08 =  2% of the original kernel matrix.

torch.Size([28429, 2])
We keep 6.59e+06/3.80e+08 =  1% of the original kernel matrix.

torch.Size([177859, 2])
We keep 3.45e+08/2.65e+10 =  1% of the original kernel matrix.

torch.Size([74727, 2])
We keep 4.08e+07/3.28e+09 =  1% of the original kernel matrix.

torch.Size([171405, 2])
We keep 3.83e+08/2.58e+10 =  1% of the original kernel matrix.

torch.Size([73414, 2])
We keep 4.05e+07/3.24e+09 =  1% of the original kernel matrix.

torch.Size([420219, 2])
We keep 6.69e+08/7.97e+10 =  0% of the original kernel matrix.

torch.Size([119399, 2])
We keep 6.56e+07/5.69e+09 =  1% of the original kernel matrix.

torch.Size([26417, 2])
We keep 1.13e+07/2.71e+08 =  4% of the original kernel matrix.

torch.Size([29901, 2])
We keep 5.71e+06/3.32e+08 =  1% of the original kernel matrix.

torch.Size([12693, 2])
We keep 1.67e+06/5.37e+07 =  3% of the original kernel matrix.

torch.Size([19980, 2])
We keep 3.06e+06/1.48e+08 =  2% of the original kernel matrix.

torch.Size([7825, 2])
We keep 5.27e+05/1.32e+07 =  4% of the original kernel matrix.

torch.Size([15803, 2])
We keep 1.82e+06/7.32e+07 =  2% of the original kernel matrix.

torch.Size([7815, 2])
We keep 7.29e+05/1.56e+07 =  4% of the original kernel matrix.

torch.Size([15727, 2])
We keep 1.94e+06/7.96e+07 =  2% of the original kernel matrix.

torch.Size([178069, 2])
We keep 3.72e+08/1.40e+10 =  2% of the original kernel matrix.

torch.Size([76931, 2])
We keep 2.86e+07/2.39e+09 =  1% of the original kernel matrix.

torch.Size([13553, 2])
We keep 1.43e+06/4.78e+07 =  2% of the original kernel matrix.

torch.Size([20468, 2])
We keep 2.93e+06/1.39e+08 =  2% of the original kernel matrix.

torch.Size([34491, 2])
We keep 1.03e+07/5.19e+08 =  1% of the original kernel matrix.

torch.Size([33541, 2])
We keep 7.51e+06/4.59e+08 =  1% of the original kernel matrix.

torch.Size([40967, 2])
We keep 1.50e+07/7.61e+08 =  1% of the original kernel matrix.

torch.Size([35987, 2])
We keep 8.49e+06/5.56e+08 =  1% of the original kernel matrix.

torch.Size([26128, 2])
We keep 1.08e+07/4.21e+08 =  2% of the original kernel matrix.

torch.Size([28100, 2])
We keep 6.82e+06/4.14e+08 =  1% of the original kernel matrix.

torch.Size([56281, 2])
We keep 4.52e+07/1.62e+09 =  2% of the original kernel matrix.

torch.Size([41598, 2])
We keep 1.21e+07/8.10e+08 =  1% of the original kernel matrix.

torch.Size([11060, 2])
We keep 1.53e+06/3.84e+07 =  3% of the original kernel matrix.

torch.Size([18267, 2])
We keep 2.72e+06/1.25e+08 =  2% of the original kernel matrix.

torch.Size([213734, 2])
We keep 3.66e+08/2.98e+10 =  1% of the original kernel matrix.

torch.Size([83714, 2])
We keep 4.28e+07/3.48e+09 =  1% of the original kernel matrix.

torch.Size([87878, 2])
We keep 6.71e+07/3.67e+09 =  1% of the original kernel matrix.

torch.Size([51923, 2])
We keep 1.70e+07/1.22e+09 =  1% of the original kernel matrix.

torch.Size([13791, 2])
We keep 1.88e+06/6.26e+07 =  3% of the original kernel matrix.

torch.Size([20781, 2])
We keep 3.24e+06/1.60e+08 =  2% of the original kernel matrix.

torch.Size([24551, 2])
We keep 5.55e+06/2.33e+08 =  2% of the original kernel matrix.

torch.Size([28220, 2])
We keep 5.24e+06/3.08e+08 =  1% of the original kernel matrix.

torch.Size([53905, 2])
We keep 3.83e+07/1.64e+09 =  2% of the original kernel matrix.

torch.Size([41314, 2])
We keep 1.23e+07/8.16e+08 =  1% of the original kernel matrix.

torch.Size([114445, 2])
We keep 1.03e+08/6.74e+09 =  1% of the original kernel matrix.

torch.Size([59908, 2])
We keep 2.22e+07/1.66e+09 =  1% of the original kernel matrix.

torch.Size([508854, 2])
We keep 2.19e+09/1.89e+11 =  1% of the original kernel matrix.

torch.Size([127019, 2])
We keep 9.85e+07/8.77e+09 =  1% of the original kernel matrix.

torch.Size([146085, 2])
We keep 1.32e+08/9.56e+09 =  1% of the original kernel matrix.

torch.Size([68571, 2])
We keep 2.53e+07/1.97e+09 =  1% of the original kernel matrix.

torch.Size([12613, 2])
We keep 4.47e+06/5.42e+07 =  8% of the original kernel matrix.

torch.Size([19973, 2])
We keep 3.07e+06/1.49e+08 =  2% of the original kernel matrix.

torch.Size([43190, 2])
We keep 1.20e+07/7.88e+08 =  1% of the original kernel matrix.

torch.Size([37221, 2])
We keep 8.62e+06/5.66e+08 =  1% of the original kernel matrix.

torch.Size([44116, 2])
We keep 3.67e+07/1.23e+09 =  2% of the original kernel matrix.

torch.Size([36240, 2])
We keep 1.09e+07/7.07e+08 =  1% of the original kernel matrix.

torch.Size([13210, 2])
We keep 1.90e+06/4.69e+07 =  4% of the original kernel matrix.

torch.Size([20222, 2])
We keep 2.86e+06/1.38e+08 =  2% of the original kernel matrix.

torch.Size([19811, 2])
We keep 3.63e+06/1.46e+08 =  2% of the original kernel matrix.

torch.Size([24850, 2])
We keep 4.48e+06/2.44e+08 =  1% of the original kernel matrix.

torch.Size([10284, 2])
We keep 8.99e+05/2.53e+07 =  3% of the original kernel matrix.

torch.Size([18018, 2])
We keep 2.30e+06/1.01e+08 =  2% of the original kernel matrix.

torch.Size([26643, 2])
We keep 1.60e+07/2.84e+08 =  5% of the original kernel matrix.

torch.Size([29458, 2])
We keep 5.96e+06/3.40e+08 =  1% of the original kernel matrix.

torch.Size([75747, 2])
We keep 5.99e+07/2.95e+09 =  2% of the original kernel matrix.

torch.Size([47277, 2])
We keep 1.54e+07/1.10e+09 =  1% of the original kernel matrix.

torch.Size([9982, 2])
We keep 1.40e+06/2.78e+07 =  5% of the original kernel matrix.

torch.Size([17518, 2])
We keep 2.39e+06/1.06e+08 =  2% of the original kernel matrix.

torch.Size([25824, 2])
We keep 5.24e+06/2.71e+08 =  1% of the original kernel matrix.

torch.Size([29621, 2])
We keep 5.86e+06/3.32e+08 =  1% of the original kernel matrix.

torch.Size([13262, 2])
We keep 2.22e+06/6.41e+07 =  3% of the original kernel matrix.

torch.Size([20322, 2])
We keep 3.12e+06/1.61e+08 =  1% of the original kernel matrix.

torch.Size([20367, 2])
We keep 1.14e+07/3.03e+08 =  3% of the original kernel matrix.

torch.Size([24211, 2])
We keep 5.69e+06/3.51e+08 =  1% of the original kernel matrix.

torch.Size([10680, 2])
We keep 1.18e+06/3.10e+07 =  3% of the original kernel matrix.

torch.Size([18122, 2])
We keep 2.52e+06/1.12e+08 =  2% of the original kernel matrix.

torch.Size([31291, 2])
We keep 1.01e+07/4.17e+08 =  2% of the original kernel matrix.

torch.Size([31989, 2])
We keep 6.88e+06/4.12e+08 =  1% of the original kernel matrix.

torch.Size([11453, 2])
We keep 1.29e+07/5.49e+07 = 23% of the original kernel matrix.

torch.Size([18702, 2])
We keep 3.08e+06/1.49e+08 =  2% of the original kernel matrix.

torch.Size([17524, 2])
We keep 3.40e+06/1.24e+08 =  2% of the original kernel matrix.

torch.Size([23511, 2])
We keep 4.18e+06/2.24e+08 =  1% of the original kernel matrix.

torch.Size([13065, 2])
We keep 1.80e+06/4.92e+07 =  3% of the original kernel matrix.

torch.Size([20083, 2])
We keep 3.02e+06/1.41e+08 =  2% of the original kernel matrix.

torch.Size([19217, 2])
We keep 2.93e+06/1.29e+08 =  2% of the original kernel matrix.

torch.Size([24669, 2])
We keep 4.30e+06/2.29e+08 =  1% of the original kernel matrix.

torch.Size([30841, 2])
We keep 7.77e+06/4.14e+08 =  1% of the original kernel matrix.

torch.Size([32136, 2])
We keep 6.83e+06/4.10e+08 =  1% of the original kernel matrix.

torch.Size([173672, 2])
We keep 2.34e+08/1.45e+10 =  1% of the original kernel matrix.

torch.Size([75432, 2])
We keep 3.02e+07/2.43e+09 =  1% of the original kernel matrix.

torch.Size([11270, 2])
We keep 1.07e+06/3.15e+07 =  3% of the original kernel matrix.

torch.Size([18708, 2])
We keep 2.52e+06/1.13e+08 =  2% of the original kernel matrix.

torch.Size([5697, 2])
We keep 4.04e+05/7.82e+06 =  5% of the original kernel matrix.

torch.Size([13744, 2])
We keep 1.53e+06/5.64e+07 =  2% of the original kernel matrix.

torch.Size([17426, 2])
We keep 3.16e+06/1.01e+08 =  3% of the original kernel matrix.

torch.Size([23346, 2])
We keep 3.94e+06/2.03e+08 =  1% of the original kernel matrix.

torch.Size([133785, 2])
We keep 1.74e+08/8.70e+09 =  1% of the original kernel matrix.

torch.Size([65548, 2])
We keep 2.42e+07/1.88e+09 =  1% of the original kernel matrix.

torch.Size([482833, 2])
We keep 1.44e+09/1.07e+11 =  1% of the original kernel matrix.

torch.Size([128103, 2])
We keep 7.39e+07/6.59e+09 =  1% of the original kernel matrix.

torch.Size([127790, 2])
We keep 1.72e+08/1.03e+10 =  1% of the original kernel matrix.

torch.Size([62319, 2])
We keep 2.65e+07/2.05e+09 =  1% of the original kernel matrix.

torch.Size([11232, 2])
We keep 1.36e+06/3.68e+07 =  3% of the original kernel matrix.

torch.Size([18637, 2])
We keep 2.68e+06/1.22e+08 =  2% of the original kernel matrix.

torch.Size([326328, 2])
We keep 8.51e+08/6.48e+10 =  1% of the original kernel matrix.

torch.Size([103147, 2])
We keep 5.98e+07/5.13e+09 =  1% of the original kernel matrix.

torch.Size([226653, 2])
We keep 2.39e+08/2.63e+10 =  0% of the original kernel matrix.

torch.Size([87429, 2])
We keep 4.00e+07/3.27e+09 =  1% of the original kernel matrix.

torch.Size([5520, 2])
We keep 3.52e+05/6.81e+06 =  5% of the original kernel matrix.

torch.Size([13602, 2])
We keep 1.47e+06/5.26e+07 =  2% of the original kernel matrix.

torch.Size([14863, 2])
We keep 2.57e+06/7.93e+07 =  3% of the original kernel matrix.

torch.Size([21608, 2])
We keep 3.42e+06/1.80e+08 =  1% of the original kernel matrix.

torch.Size([73168, 2])
We keep 2.64e+07/2.02e+09 =  1% of the original kernel matrix.

torch.Size([47835, 2])
We keep 1.28e+07/9.07e+08 =  1% of the original kernel matrix.

torch.Size([90293, 2])
We keep 2.14e+08/6.97e+09 =  3% of the original kernel matrix.

torch.Size([52603, 2])
We keep 2.25e+07/1.68e+09 =  1% of the original kernel matrix.

torch.Size([196161, 2])
We keep 1.43e+09/3.58e+10 =  3% of the original kernel matrix.

torch.Size([79444, 2])
We keep 4.48e+07/3.81e+09 =  1% of the original kernel matrix.

torch.Size([214590, 2])
We keep 2.72e+08/2.66e+10 =  1% of the original kernel matrix.

torch.Size([84653, 2])
We keep 4.07e+07/3.29e+09 =  1% of the original kernel matrix.

torch.Size([45447, 2])
We keep 2.68e+07/8.68e+08 =  3% of the original kernel matrix.

torch.Size([38459, 2])
We keep 9.04e+06/5.94e+08 =  1% of the original kernel matrix.

torch.Size([12356, 2])
We keep 1.19e+06/3.62e+07 =  3% of the original kernel matrix.

torch.Size([19596, 2])
We keep 2.65e+06/1.21e+08 =  2% of the original kernel matrix.

torch.Size([6719, 2])
We keep 4.48e+05/9.94e+06 =  4% of the original kernel matrix.

torch.Size([14673, 2])
We keep 1.65e+06/6.36e+07 =  2% of the original kernel matrix.

torch.Size([76424, 2])
We keep 7.11e+07/2.57e+09 =  2% of the original kernel matrix.

torch.Size([48515, 2])
We keep 1.42e+07/1.02e+09 =  1% of the original kernel matrix.

torch.Size([20638, 2])
We keep 3.86e+06/1.50e+08 =  2% of the original kernel matrix.

torch.Size([25649, 2])
We keep 4.46e+06/2.47e+08 =  1% of the original kernel matrix.

torch.Size([32509, 2])
We keep 2.37e+07/5.32e+08 =  4% of the original kernel matrix.

torch.Size([31747, 2])
We keep 7.35e+06/4.65e+08 =  1% of the original kernel matrix.

torch.Size([149209, 2])
We keep 1.42e+08/1.02e+10 =  1% of the original kernel matrix.

torch.Size([69176, 2])
We keep 2.61e+07/2.03e+09 =  1% of the original kernel matrix.

torch.Size([700063, 2])
We keep 1.46e+09/2.02e+11 =  0% of the original kernel matrix.

torch.Size([157633, 2])
We keep 9.94e+07/9.07e+09 =  1% of the original kernel matrix.

torch.Size([15978, 2])
We keep 2.97e+06/9.05e+07 =  3% of the original kernel matrix.

torch.Size([22313, 2])
We keep 3.79e+06/1.92e+08 =  1% of the original kernel matrix.

torch.Size([4522, 2])
We keep 2.04e+05/3.80e+06 =  5% of the original kernel matrix.

torch.Size([12503, 2])
We keep 1.17e+06/3.93e+07 =  2% of the original kernel matrix.

torch.Size([28889, 2])
We keep 2.23e+07/3.48e+08 =  6% of the original kernel matrix.

torch.Size([30470, 2])
We keep 6.02e+06/3.76e+08 =  1% of the original kernel matrix.

torch.Size([141783, 2])
We keep 5.71e+08/9.58e+09 =  5% of the original kernel matrix.

torch.Size([67870, 2])
We keep 2.44e+07/1.97e+09 =  1% of the original kernel matrix.

torch.Size([45021, 2])
We keep 2.34e+07/8.43e+08 =  2% of the original kernel matrix.

torch.Size([37821, 2])
We keep 9.07e+06/5.85e+08 =  1% of the original kernel matrix.

torch.Size([11513, 2])
We keep 5.76e+06/5.13e+07 = 11% of the original kernel matrix.

torch.Size([18799, 2])
We keep 3.01e+06/1.44e+08 =  2% of the original kernel matrix.

torch.Size([6120, 2])
We keep 3.81e+05/7.60e+06 =  5% of the original kernel matrix.

torch.Size([14061, 2])
We keep 1.49e+06/5.56e+07 =  2% of the original kernel matrix.

torch.Size([39394, 2])
We keep 1.27e+07/6.47e+08 =  1% of the original kernel matrix.

torch.Size([35620, 2])
We keep 8.17e+06/5.13e+08 =  1% of the original kernel matrix.

torch.Size([13051, 2])
We keep 2.58e+06/5.18e+07 =  4% of the original kernel matrix.

torch.Size([20159, 2])
We keep 3.07e+06/1.45e+08 =  2% of the original kernel matrix.

torch.Size([61705, 2])
We keep 6.21e+07/2.29e+09 =  2% of the original kernel matrix.

torch.Size([42490, 2])
We keep 1.40e+07/9.65e+08 =  1% of the original kernel matrix.

torch.Size([11919, 2])
We keep 1.57e+06/4.31e+07 =  3% of the original kernel matrix.

torch.Size([19333, 2])
We keep 2.87e+06/1.32e+08 =  2% of the original kernel matrix.

torch.Size([22255, 2])
We keep 3.59e+06/1.78e+08 =  2% of the original kernel matrix.

torch.Size([27005, 2])
We keep 4.84e+06/2.69e+08 =  1% of the original kernel matrix.

torch.Size([15913, 2])
We keep 2.76e+06/8.58e+07 =  3% of the original kernel matrix.

torch.Size([22334, 2])
We keep 3.77e+06/1.87e+08 =  2% of the original kernel matrix.

torch.Size([16765, 2])
We keep 2.06e+06/8.25e+07 =  2% of the original kernel matrix.

torch.Size([23102, 2])
We keep 3.58e+06/1.83e+08 =  1% of the original kernel matrix.

torch.Size([30917, 2])
We keep 9.36e+06/4.18e+08 =  2% of the original kernel matrix.

torch.Size([31741, 2])
We keep 6.95e+06/4.12e+08 =  1% of the original kernel matrix.

torch.Size([223556, 2])
We keep 2.16e+08/2.11e+10 =  1% of the original kernel matrix.

torch.Size([86758, 2])
We keep 3.57e+07/2.93e+09 =  1% of the original kernel matrix.

torch.Size([401041, 2])
We keep 6.28e+08/7.12e+10 =  0% of the original kernel matrix.

torch.Size([116369, 2])
We keep 6.22e+07/5.38e+09 =  1% of the original kernel matrix.

torch.Size([107359, 2])
We keep 1.19e+08/5.55e+09 =  2% of the original kernel matrix.

torch.Size([58383, 2])
We keep 2.02e+07/1.50e+09 =  1% of the original kernel matrix.

torch.Size([160041, 2])
We keep 2.74e+08/1.51e+10 =  1% of the original kernel matrix.

torch.Size([71664, 2])
We keep 3.15e+07/2.48e+09 =  1% of the original kernel matrix.

torch.Size([28271, 2])
We keep 5.37e+06/3.00e+08 =  1% of the original kernel matrix.

torch.Size([28795, 2])
We keep 5.56e+06/3.49e+08 =  1% of the original kernel matrix.

torch.Size([22202, 2])
We keep 9.11e+06/2.25e+08 =  4% of the original kernel matrix.

torch.Size([26516, 2])
We keep 5.39e+06/3.02e+08 =  1% of the original kernel matrix.

torch.Size([26193, 2])
We keep 6.73e+06/3.30e+08 =  2% of the original kernel matrix.

torch.Size([29888, 2])
We keep 6.45e+06/3.67e+08 =  1% of the original kernel matrix.

torch.Size([173598, 2])
We keep 1.75e+08/1.53e+10 =  1% of the original kernel matrix.

torch.Size([75242, 2])
We keep 3.17e+07/2.49e+09 =  1% of the original kernel matrix.

torch.Size([36358, 2])
We keep 1.87e+07/7.10e+08 =  2% of the original kernel matrix.

torch.Size([33978, 2])
We keep 8.68e+06/5.37e+08 =  1% of the original kernel matrix.

torch.Size([68184, 2])
We keep 1.30e+08/2.11e+09 =  6% of the original kernel matrix.

torch.Size([45929, 2])
We keep 1.30e+07/9.27e+08 =  1% of the original kernel matrix.

torch.Size([21304, 2])
We keep 4.16e+06/1.79e+08 =  2% of the original kernel matrix.

torch.Size([25998, 2])
We keep 4.91e+06/2.70e+08 =  1% of the original kernel matrix.

torch.Size([1368253, 2])
We keep 5.89e+09/8.21e+11 =  0% of the original kernel matrix.

torch.Size([221649, 2])
We keep 1.92e+08/1.83e+10 =  1% of the original kernel matrix.

torch.Size([24505, 2])
We keep 5.19e+06/2.41e+08 =  2% of the original kernel matrix.

torch.Size([28375, 2])
We keep 5.52e+06/3.13e+08 =  1% of the original kernel matrix.

torch.Size([95622, 2])
We keep 5.75e+07/3.95e+09 =  1% of the original kernel matrix.

torch.Size([54261, 2])
We keep 1.75e+07/1.27e+09 =  1% of the original kernel matrix.

torch.Size([192649, 2])
We keep 2.71e+08/1.92e+10 =  1% of the original kernel matrix.

torch.Size([79921, 2])
We keep 3.39e+07/2.80e+09 =  1% of the original kernel matrix.

torch.Size([51323, 2])
We keep 2.10e+07/1.28e+09 =  1% of the original kernel matrix.

torch.Size([40868, 2])
We keep 1.09e+07/7.21e+08 =  1% of the original kernel matrix.

torch.Size([35325, 2])
We keep 1.56e+07/6.24e+08 =  2% of the original kernel matrix.

torch.Size([33845, 2])
We keep 8.21e+06/5.04e+08 =  1% of the original kernel matrix.

torch.Size([1180052, 2])
We keep 7.30e+09/7.12e+11 =  1% of the original kernel matrix.

torch.Size([202999, 2])
We keep 1.80e+08/1.70e+10 =  1% of the original kernel matrix.

torch.Size([244883, 2])
We keep 2.55e+08/2.83e+10 =  0% of the original kernel matrix.

torch.Size([91017, 2])
We keep 4.09e+07/3.39e+09 =  1% of the original kernel matrix.

torch.Size([133393, 2])
We keep 1.18e+08/7.91e+09 =  1% of the original kernel matrix.

torch.Size([65659, 2])
We keep 2.36e+07/1.79e+09 =  1% of the original kernel matrix.

torch.Size([12879, 2])
We keep 1.94e+06/4.71e+07 =  4% of the original kernel matrix.

torch.Size([20170, 2])
We keep 2.92e+06/1.38e+08 =  2% of the original kernel matrix.

torch.Size([49074, 2])
We keep 2.42e+07/1.03e+09 =  2% of the original kernel matrix.

torch.Size([39587, 2])
We keep 9.94e+06/6.48e+08 =  1% of the original kernel matrix.

torch.Size([12723, 2])
We keep 1.68e+06/4.59e+07 =  3% of the original kernel matrix.

torch.Size([20062, 2])
We keep 2.79e+06/1.37e+08 =  2% of the original kernel matrix.

torch.Size([236334, 2])
We keep 7.42e+08/2.94e+10 =  2% of the original kernel matrix.

torch.Size([89416, 2])
We keep 4.25e+07/3.46e+09 =  1% of the original kernel matrix.

torch.Size([1407505, 2])
We keep 5.95e+09/8.01e+11 =  0% of the original kernel matrix.

torch.Size([223885, 2])
We keep 1.90e+08/1.81e+10 =  1% of the original kernel matrix.

torch.Size([100841, 2])
We keep 1.81e+08/4.83e+09 =  3% of the original kernel matrix.

torch.Size([55921, 2])
We keep 1.92e+07/1.40e+09 =  1% of the original kernel matrix.

torch.Size([185402, 2])
We keep 2.48e+08/2.11e+10 =  1% of the original kernel matrix.

torch.Size([77466, 2])
We keep 3.69e+07/2.93e+09 =  1% of the original kernel matrix.

torch.Size([107769, 2])
We keep 2.14e+08/5.88e+09 =  3% of the original kernel matrix.

torch.Size([58255, 2])
We keep 2.11e+07/1.55e+09 =  1% of the original kernel matrix.

torch.Size([41145, 2])
We keep 2.62e+07/8.98e+08 =  2% of the original kernel matrix.

torch.Size([35894, 2])
We keep 9.43e+06/6.04e+08 =  1% of the original kernel matrix.

torch.Size([41547, 2])
We keep 1.88e+07/7.60e+08 =  2% of the original kernel matrix.

torch.Size([36355, 2])
We keep 8.75e+06/5.56e+08 =  1% of the original kernel matrix.

torch.Size([22228, 2])
We keep 4.85e+06/1.89e+08 =  2% of the original kernel matrix.

torch.Size([26554, 2])
We keep 5.05e+06/2.77e+08 =  1% of the original kernel matrix.

torch.Size([105332, 2])
We keep 6.77e+08/1.15e+10 =  5% of the original kernel matrix.

torch.Size([57539, 2])
We keep 2.68e+07/2.17e+09 =  1% of the original kernel matrix.

torch.Size([542450, 2])
We keep 9.77e+08/1.34e+11 =  0% of the original kernel matrix.

torch.Size([135909, 2])
We keep 8.22e+07/7.38e+09 =  1% of the original kernel matrix.

torch.Size([107559, 2])
We keep 1.36e+08/8.45e+09 =  1% of the original kernel matrix.

torch.Size([55887, 2])
We keep 2.46e+07/1.85e+09 =  1% of the original kernel matrix.

torch.Size([709844, 2])
We keep 5.09e+09/2.76e+11 =  1% of the original kernel matrix.

torch.Size([155490, 2])
We keep 1.13e+08/1.06e+10 =  1% of the original kernel matrix.

torch.Size([175816, 2])
We keep 2.95e+08/1.79e+10 =  1% of the original kernel matrix.

torch.Size([75414, 2])
We keep 3.39e+07/2.70e+09 =  1% of the original kernel matrix.

torch.Size([73573, 2])
We keep 3.33e+07/2.24e+09 =  1% of the original kernel matrix.

torch.Size([47936, 2])
We keep 1.36e+07/9.54e+08 =  1% of the original kernel matrix.

torch.Size([29507, 2])
We keep 1.14e+07/3.99e+08 =  2% of the original kernel matrix.

torch.Size([30563, 2])
We keep 6.73e+06/4.03e+08 =  1% of the original kernel matrix.

torch.Size([88592, 2])
We keep 1.00e+08/4.18e+09 =  2% of the original kernel matrix.

torch.Size([52989, 2])
We keep 1.80e+07/1.30e+09 =  1% of the original kernel matrix.

torch.Size([192932, 2])
We keep 1.51e+08/1.53e+10 =  0% of the original kernel matrix.

torch.Size([79990, 2])
We keep 3.09e+07/2.49e+09 =  1% of the original kernel matrix.

torch.Size([18067, 2])
We keep 5.30e+06/1.64e+08 =  3% of the original kernel matrix.

torch.Size([23749, 2])
We keep 4.85e+06/2.59e+08 =  1% of the original kernel matrix.

torch.Size([688139, 2])
We keep 1.56e+09/2.19e+11 =  0% of the original kernel matrix.

torch.Size([155736, 2])
We keep 1.02e+08/9.43e+09 =  1% of the original kernel matrix.

torch.Size([32515, 2])
We keep 1.73e+07/5.08e+08 =  3% of the original kernel matrix.

torch.Size([32404, 2])
We keep 6.72e+06/4.55e+08 =  1% of the original kernel matrix.

torch.Size([51426, 2])
We keep 3.31e+07/1.60e+09 =  2% of the original kernel matrix.

torch.Size([39874, 2])
We keep 1.21e+07/8.06e+08 =  1% of the original kernel matrix.

torch.Size([11016, 2])
We keep 1.12e+06/3.22e+07 =  3% of the original kernel matrix.

torch.Size([18584, 2])
We keep 2.46e+06/1.14e+08 =  2% of the original kernel matrix.

torch.Size([24647, 2])
We keep 4.91e+06/2.42e+08 =  2% of the original kernel matrix.

torch.Size([28416, 2])
We keep 5.53e+06/3.14e+08 =  1% of the original kernel matrix.

torch.Size([4794, 2])
We keep 2.44e+05/4.56e+06 =  5% of the original kernel matrix.

torch.Size([12763, 2])
We keep 1.25e+06/4.31e+07 =  2% of the original kernel matrix.

torch.Size([9344, 2])
We keep 1.55e+06/2.31e+07 =  6% of the original kernel matrix.

torch.Size([16912, 2])
We keep 2.18e+06/9.69e+07 =  2% of the original kernel matrix.

torch.Size([28048, 2])
We keep 8.01e+06/3.32e+08 =  2% of the original kernel matrix.

torch.Size([30522, 2])
We keep 6.39e+06/3.68e+08 =  1% of the original kernel matrix.

torch.Size([50797, 2])
We keep 3.44e+07/1.29e+09 =  2% of the original kernel matrix.

torch.Size([40040, 2])
We keep 1.11e+07/7.25e+08 =  1% of the original kernel matrix.

torch.Size([3438684, 2])
We keep 4.49e+10/6.35e+12 =  0% of the original kernel matrix.

torch.Size([343534, 2])
We keep 5.03e+08/5.08e+10 =  0% of the original kernel matrix.

torch.Size([17664, 2])
We keep 5.90e+06/1.39e+08 =  4% of the original kernel matrix.

torch.Size([23476, 2])
We keep 4.42e+06/2.37e+08 =  1% of the original kernel matrix.

torch.Size([441686, 2])
We keep 6.65e+08/8.12e+10 =  0% of the original kernel matrix.

torch.Size([121862, 2])
We keep 6.48e+07/5.75e+09 =  1% of the original kernel matrix.

torch.Size([13266, 2])
We keep 2.04e+06/6.05e+07 =  3% of the original kernel matrix.

torch.Size([20405, 2])
We keep 3.33e+06/1.57e+08 =  2% of the original kernel matrix.

torch.Size([25362, 2])
We keep 5.16e+06/2.45e+08 =  2% of the original kernel matrix.

torch.Size([28764, 2])
We keep 5.47e+06/3.16e+08 =  1% of the original kernel matrix.

torch.Size([15701, 2])
We keep 2.64e+06/8.15e+07 =  3% of the original kernel matrix.

torch.Size([21844, 2])
We keep 3.62e+06/1.82e+08 =  1% of the original kernel matrix.

torch.Size([240903, 2])
We keep 5.47e+08/3.91e+10 =  1% of the original kernel matrix.

torch.Size([87604, 2])
We keep 4.78e+07/3.99e+09 =  1% of the original kernel matrix.

torch.Size([38318, 2])
We keep 5.99e+07/9.25e+08 =  6% of the original kernel matrix.

torch.Size([34165, 2])
We keep 9.37e+06/6.13e+08 =  1% of the original kernel matrix.

torch.Size([1188003, 2])
We keep 3.66e+09/5.41e+11 =  0% of the original kernel matrix.

torch.Size([206603, 2])
We keep 1.56e+08/1.48e+10 =  1% of the original kernel matrix.

torch.Size([11004, 2])
We keep 1.15e+06/3.03e+07 =  3% of the original kernel matrix.

torch.Size([18513, 2])
We keep 2.45e+06/1.11e+08 =  2% of the original kernel matrix.

torch.Size([199108, 2])
We keep 2.41e+08/2.09e+10 =  1% of the original kernel matrix.

torch.Size([81362, 2])
We keep 3.65e+07/2.91e+09 =  1% of the original kernel matrix.

torch.Size([27680, 2])
We keep 1.02e+07/3.29e+08 =  3% of the original kernel matrix.

torch.Size([30363, 2])
We keep 6.35e+06/3.66e+08 =  1% of the original kernel matrix.

torch.Size([181030, 2])
We keep 1.93e+08/1.54e+10 =  1% of the original kernel matrix.

torch.Size([76776, 2])
We keep 3.16e+07/2.50e+09 =  1% of the original kernel matrix.

torch.Size([179659, 2])
We keep 3.22e+08/1.98e+10 =  1% of the original kernel matrix.

torch.Size([76821, 2])
We keep 3.51e+07/2.84e+09 =  1% of the original kernel matrix.

torch.Size([33824, 2])
We keep 9.59e+06/4.58e+08 =  2% of the original kernel matrix.

torch.Size([33122, 2])
We keep 7.06e+06/4.32e+08 =  1% of the original kernel matrix.

torch.Size([20919, 2])
We keep 4.06e+06/1.61e+08 =  2% of the original kernel matrix.

torch.Size([25960, 2])
We keep 4.74e+06/2.56e+08 =  1% of the original kernel matrix.

torch.Size([108804, 2])
We keep 6.28e+07/5.18e+09 =  1% of the original kernel matrix.

torch.Size([58525, 2])
We keep 1.95e+07/1.45e+09 =  1% of the original kernel matrix.

torch.Size([28947, 2])
We keep 3.47e+07/4.36e+08 =  7% of the original kernel matrix.

torch.Size([30072, 2])
We keep 6.63e+06/4.21e+08 =  1% of the original kernel matrix.

torch.Size([46989, 2])
We keep 4.00e+07/1.07e+09 =  3% of the original kernel matrix.

torch.Size([38091, 2])
We keep 9.73e+06/6.59e+08 =  1% of the original kernel matrix.

torch.Size([294549, 2])
We keep 5.02e+08/3.82e+10 =  1% of the original kernel matrix.

torch.Size([99565, 2])
We keep 4.61e+07/3.94e+09 =  1% of the original kernel matrix.

torch.Size([16048, 2])
We keep 2.86e+06/9.69e+07 =  2% of the original kernel matrix.

torch.Size([22536, 2])
We keep 3.61e+06/1.99e+08 =  1% of the original kernel matrix.

torch.Size([74653, 2])
We keep 7.29e+07/3.34e+09 =  2% of the original kernel matrix.

torch.Size([48018, 2])
We keep 1.66e+07/1.17e+09 =  1% of the original kernel matrix.

torch.Size([31019, 2])
We keep 1.18e+07/4.34e+08 =  2% of the original kernel matrix.

torch.Size([31060, 2])
We keep 6.89e+06/4.20e+08 =  1% of the original kernel matrix.

torch.Size([166450, 2])
We keep 1.80e+08/1.27e+10 =  1% of the original kernel matrix.

torch.Size([73446, 2])
We keep 2.84e+07/2.27e+09 =  1% of the original kernel matrix.

torch.Size([420986, 2])
We keep 9.03e+08/7.73e+10 =  1% of the original kernel matrix.

torch.Size([117899, 2])
We keep 6.37e+07/5.61e+09 =  1% of the original kernel matrix.

torch.Size([81107, 2])
We keep 6.67e+07/3.44e+09 =  1% of the original kernel matrix.

torch.Size([49932, 2])
We keep 1.68e+07/1.18e+09 =  1% of the original kernel matrix.

torch.Size([16920, 2])
We keep 3.36e+06/1.01e+08 =  3% of the original kernel matrix.

torch.Size([23184, 2])
We keep 4.04e+06/2.03e+08 =  1% of the original kernel matrix.

torch.Size([149460, 2])
We keep 2.23e+08/1.16e+10 =  1% of the original kernel matrix.

torch.Size([69139, 2])
We keep 2.75e+07/2.18e+09 =  1% of the original kernel matrix.

torch.Size([124799, 2])
We keep 1.15e+08/7.68e+09 =  1% of the original kernel matrix.

torch.Size([62544, 2])
We keep 2.34e+07/1.77e+09 =  1% of the original kernel matrix.

torch.Size([141896, 2])
We keep 1.55e+09/3.18e+10 =  4% of the original kernel matrix.

torch.Size([66277, 2])
We keep 4.25e+07/3.60e+09 =  1% of the original kernel matrix.

torch.Size([77481, 2])
We keep 4.23e+07/2.47e+09 =  1% of the original kernel matrix.

torch.Size([48902, 2])
We keep 1.41e+07/1.00e+09 =  1% of the original kernel matrix.

torch.Size([31609, 2])
We keep 1.28e+07/4.80e+08 =  2% of the original kernel matrix.

torch.Size([31459, 2])
We keep 7.23e+06/4.42e+08 =  1% of the original kernel matrix.

torch.Size([63994, 2])
We keep 6.41e+07/2.34e+09 =  2% of the original kernel matrix.

torch.Size([44568, 2])
We keep 1.43e+07/9.76e+08 =  1% of the original kernel matrix.

torch.Size([147955, 2])
We keep 3.16e+08/1.63e+10 =  1% of the original kernel matrix.

torch.Size([66677, 2])
We keep 3.23e+07/2.58e+09 =  1% of the original kernel matrix.

torch.Size([120174, 2])
We keep 2.50e+08/9.49e+09 =  2% of the original kernel matrix.

torch.Size([61067, 2])
We keep 2.52e+07/1.96e+09 =  1% of the original kernel matrix.

torch.Size([39098, 2])
We keep 1.06e+07/5.95e+08 =  1% of the original kernel matrix.

torch.Size([35880, 2])
We keep 7.84e+06/4.92e+08 =  1% of the original kernel matrix.

torch.Size([11123, 2])
We keep 1.12e+06/3.19e+07 =  3% of the original kernel matrix.

torch.Size([18452, 2])
We keep 2.54e+06/1.14e+08 =  2% of the original kernel matrix.

torch.Size([12265, 2])
We keep 1.64e+06/4.91e+07 =  3% of the original kernel matrix.

torch.Size([19483, 2])
We keep 3.03e+06/1.41e+08 =  2% of the original kernel matrix.

torch.Size([81836, 2])
We keep 1.02e+08/4.75e+09 =  2% of the original kernel matrix.

torch.Size([49494, 2])
We keep 1.92e+07/1.39e+09 =  1% of the original kernel matrix.

torch.Size([109399, 2])
We keep 2.14e+08/7.69e+09 =  2% of the original kernel matrix.

torch.Size([58200, 2])
We keep 2.39e+07/1.77e+09 =  1% of the original kernel matrix.

torch.Size([97634, 2])
We keep 5.68e+07/3.86e+09 =  1% of the original kernel matrix.

torch.Size([54966, 2])
We keep 1.72e+07/1.25e+09 =  1% of the original kernel matrix.

torch.Size([18582, 2])
We keep 3.32e+06/1.31e+08 =  2% of the original kernel matrix.

torch.Size([24361, 2])
We keep 4.39e+06/2.31e+08 =  1% of the original kernel matrix.

torch.Size([34840, 2])
We keep 8.25e+06/5.01e+08 =  1% of the original kernel matrix.

torch.Size([33460, 2])
We keep 7.26e+06/4.52e+08 =  1% of the original kernel matrix.

torch.Size([64598, 2])
We keep 6.62e+07/1.86e+09 =  3% of the original kernel matrix.

torch.Size([44676, 2])
We keep 1.25e+07/8.69e+08 =  1% of the original kernel matrix.

torch.Size([59091, 2])
We keep 2.20e+07/1.55e+09 =  1% of the original kernel matrix.

torch.Size([43425, 2])
We keep 1.16e+07/7.93e+08 =  1% of the original kernel matrix.

torch.Size([31107, 2])
We keep 1.05e+07/4.01e+08 =  2% of the original kernel matrix.

torch.Size([31654, 2])
We keep 6.69e+06/4.04e+08 =  1% of the original kernel matrix.

torch.Size([89916, 2])
We keep 4.29e+07/3.19e+09 =  1% of the original kernel matrix.

torch.Size([52711, 2])
We keep 1.57e+07/1.14e+09 =  1% of the original kernel matrix.

torch.Size([12844, 2])
We keep 3.71e+06/6.80e+07 =  5% of the original kernel matrix.

torch.Size([19975, 2])
We keep 3.35e+06/1.66e+08 =  2% of the original kernel matrix.

torch.Size([48083, 2])
We keep 1.64e+07/9.76e+08 =  1% of the original kernel matrix.

torch.Size([39341, 2])
We keep 9.53e+06/6.30e+08 =  1% of the original kernel matrix.

torch.Size([8555, 2])
We keep 8.51e+05/1.73e+07 =  4% of the original kernel matrix.

torch.Size([16449, 2])
We keep 2.00e+06/8.38e+07 =  2% of the original kernel matrix.

torch.Size([11069, 2])
We keep 1.13e+06/3.04e+07 =  3% of the original kernel matrix.

torch.Size([18444, 2])
We keep 2.53e+06/1.11e+08 =  2% of the original kernel matrix.

torch.Size([219164, 2])
We keep 2.73e+08/2.53e+10 =  1% of the original kernel matrix.

torch.Size([85919, 2])
We keep 3.84e+07/3.21e+09 =  1% of the original kernel matrix.

torch.Size([23466, 2])
We keep 6.09e+06/2.40e+08 =  2% of the original kernel matrix.

torch.Size([27345, 2])
We keep 5.48e+06/3.13e+08 =  1% of the original kernel matrix.

torch.Size([115722, 2])
We keep 6.82e+07/5.02e+09 =  1% of the original kernel matrix.

torch.Size([60377, 2])
We keep 1.89e+07/1.43e+09 =  1% of the original kernel matrix.

torch.Size([13006, 2])
We keep 1.63e+06/4.65e+07 =  3% of the original kernel matrix.

torch.Size([20188, 2])
We keep 2.93e+06/1.38e+08 =  2% of the original kernel matrix.

torch.Size([11706, 2])
We keep 3.52e+06/5.39e+07 =  6% of the original kernel matrix.

torch.Size([19114, 2])
We keep 3.14e+06/1.48e+08 =  2% of the original kernel matrix.

torch.Size([48086, 2])
We keep 2.40e+07/9.94e+08 =  2% of the original kernel matrix.

torch.Size([39000, 2])
We keep 9.56e+06/6.36e+08 =  1% of the original kernel matrix.

torch.Size([124954, 2])
We keep 8.36e+07/6.56e+09 =  1% of the original kernel matrix.

torch.Size([62807, 2])
We keep 2.15e+07/1.63e+09 =  1% of the original kernel matrix.

torch.Size([53368, 2])
We keep 1.88e+07/1.14e+09 =  1% of the original kernel matrix.

torch.Size([41289, 2])
We keep 1.03e+07/6.82e+08 =  1% of the original kernel matrix.

torch.Size([10361, 2])
We keep 1.11e+06/2.95e+07 =  3% of the original kernel matrix.

torch.Size([18040, 2])
We keep 2.50e+06/1.10e+08 =  2% of the original kernel matrix.

torch.Size([20450, 2])
We keep 3.72e+06/1.49e+08 =  2% of the original kernel matrix.

torch.Size([25577, 2])
We keep 4.63e+06/2.46e+08 =  1% of the original kernel matrix.

torch.Size([61974, 2])
We keep 4.25e+07/1.72e+09 =  2% of the original kernel matrix.

torch.Size([44088, 2])
We keep 1.23e+07/8.38e+08 =  1% of the original kernel matrix.

torch.Size([35031, 2])
We keep 9.28e+06/4.98e+08 =  1% of the original kernel matrix.

torch.Size([33572, 2])
We keep 7.30e+06/4.50e+08 =  1% of the original kernel matrix.

torch.Size([106974, 2])
We keep 1.31e+08/4.95e+09 =  2% of the original kernel matrix.

torch.Size([58030, 2])
We keep 1.94e+07/1.42e+09 =  1% of the original kernel matrix.

torch.Size([12363, 2])
We keep 1.36e+06/4.44e+07 =  3% of the original kernel matrix.

torch.Size([19715, 2])
We keep 2.73e+06/1.34e+08 =  2% of the original kernel matrix.

torch.Size([11509, 2])
We keep 1.46e+06/3.82e+07 =  3% of the original kernel matrix.

torch.Size([18664, 2])
We keep 2.68e+06/1.25e+08 =  2% of the original kernel matrix.

torch.Size([97558, 2])
We keep 9.55e+07/5.66e+09 =  1% of the original kernel matrix.

torch.Size([54021, 2])
We keep 2.06e+07/1.52e+09 =  1% of the original kernel matrix.

torch.Size([11772, 2])
We keep 1.85e+06/4.50e+07 =  4% of the original kernel matrix.

torch.Size([18939, 2])
We keep 2.86e+06/1.35e+08 =  2% of the original kernel matrix.

torch.Size([177847, 2])
We keep 2.36e+08/1.65e+10 =  1% of the original kernel matrix.

torch.Size([76539, 2])
We keep 3.16e+07/2.59e+09 =  1% of the original kernel matrix.

torch.Size([18955, 2])
We keep 8.46e+06/1.15e+08 =  7% of the original kernel matrix.

torch.Size([24551, 2])
We keep 4.14e+06/2.17e+08 =  1% of the original kernel matrix.

torch.Size([105206, 2])
We keep 9.33e+07/5.40e+09 =  1% of the original kernel matrix.

torch.Size([57290, 2])
We keep 1.98e+07/1.48e+09 =  1% of the original kernel matrix.

torch.Size([171977, 2])
We keep 1.73e+08/1.33e+10 =  1% of the original kernel matrix.

torch.Size([75206, 2])
We keep 2.96e+07/2.32e+09 =  1% of the original kernel matrix.

torch.Size([78321, 2])
We keep 1.57e+08/3.38e+09 =  4% of the original kernel matrix.

torch.Size([49250, 2])
We keep 1.66e+07/1.17e+09 =  1% of the original kernel matrix.

torch.Size([162842, 2])
We keep 2.34e+08/1.56e+10 =  1% of the original kernel matrix.

torch.Size([72750, 2])
We keep 3.25e+07/2.52e+09 =  1% of the original kernel matrix.

torch.Size([140164, 2])
We keep 1.08e+08/8.77e+09 =  1% of the original kernel matrix.

torch.Size([67456, 2])
We keep 2.47e+07/1.89e+09 =  1% of the original kernel matrix.

torch.Size([4358, 2])
We keep 1.83e+05/3.50e+06 =  5% of the original kernel matrix.

torch.Size([12447, 2])
We keep 1.14e+06/3.77e+07 =  3% of the original kernel matrix.

torch.Size([74381, 2])
We keep 3.64e+07/2.34e+09 =  1% of the original kernel matrix.

torch.Size([48072, 2])
We keep 1.40e+07/9.76e+08 =  1% of the original kernel matrix.

torch.Size([32209, 2])
We keep 7.44e+06/4.01e+08 =  1% of the original kernel matrix.

torch.Size([32633, 2])
We keep 6.66e+06/4.04e+08 =  1% of the original kernel matrix.

torch.Size([108725, 2])
We keep 1.31e+08/5.62e+09 =  2% of the original kernel matrix.

torch.Size([58200, 2])
We keep 2.02e+07/1.51e+09 =  1% of the original kernel matrix.

torch.Size([96563, 2])
We keep 9.11e+07/5.15e+09 =  1% of the original kernel matrix.

torch.Size([55124, 2])
We keep 1.96e+07/1.45e+09 =  1% of the original kernel matrix.

torch.Size([65650, 2])
We keep 5.81e+07/2.08e+09 =  2% of the original kernel matrix.

torch.Size([44539, 2])
We keep 1.34e+07/9.20e+08 =  1% of the original kernel matrix.

torch.Size([91198, 2])
We keep 4.79e+07/3.32e+09 =  1% of the original kernel matrix.

torch.Size([53208, 2])
We keep 1.60e+07/1.16e+09 =  1% of the original kernel matrix.

torch.Size([38326, 2])
We keep 1.82e+07/6.29e+08 =  2% of the original kernel matrix.

torch.Size([35045, 2])
We keep 7.96e+06/5.06e+08 =  1% of the original kernel matrix.

torch.Size([47028, 2])
We keep 4.69e+07/1.77e+09 =  2% of the original kernel matrix.

torch.Size([36797, 2])
We keep 1.26e+07/8.50e+08 =  1% of the original kernel matrix.

torch.Size([7491, 2])
We keep 4.55e+05/1.12e+07 =  4% of the original kernel matrix.

torch.Size([15682, 2])
We keep 1.71e+06/6.76e+07 =  2% of the original kernel matrix.

torch.Size([15301, 2])
We keep 3.32e+06/9.01e+07 =  3% of the original kernel matrix.

torch.Size([21673, 2])
We keep 3.73e+06/1.91e+08 =  1% of the original kernel matrix.

torch.Size([236025, 2])
We keep 6.03e+08/2.49e+10 =  2% of the original kernel matrix.

torch.Size([89298, 2])
We keep 3.84e+07/3.19e+09 =  1% of the original kernel matrix.

torch.Size([533518, 2])
We keep 1.86e+09/1.41e+11 =  1% of the original kernel matrix.

torch.Size([134055, 2])
We keep 8.44e+07/7.58e+09 =  1% of the original kernel matrix.

torch.Size([25088, 2])
We keep 6.99e+06/3.15e+08 =  2% of the original kernel matrix.

torch.Size([28220, 2])
We keep 6.20e+06/3.58e+08 =  1% of the original kernel matrix.

torch.Size([24205, 2])
We keep 4.81e+06/2.30e+08 =  2% of the original kernel matrix.

torch.Size([28193, 2])
We keep 5.25e+06/3.06e+08 =  1% of the original kernel matrix.

torch.Size([38625, 2])
We keep 1.16e+07/6.30e+08 =  1% of the original kernel matrix.

torch.Size([35434, 2])
We keep 8.14e+06/5.06e+08 =  1% of the original kernel matrix.

torch.Size([120140, 2])
We keep 2.14e+08/7.88e+09 =  2% of the original kernel matrix.

torch.Size([61860, 2])
We keep 2.29e+07/1.79e+09 =  1% of the original kernel matrix.

torch.Size([88755, 2])
We keep 4.24e+07/3.02e+09 =  1% of the original kernel matrix.

torch.Size([52102, 2])
We keep 1.56e+07/1.11e+09 =  1% of the original kernel matrix.

torch.Size([13871, 2])
We keep 1.60e+06/5.36e+07 =  2% of the original kernel matrix.

torch.Size([20900, 2])
We keep 2.99e+06/1.48e+08 =  2% of the original kernel matrix.

torch.Size([54744, 2])
We keep 2.39e+07/1.31e+09 =  1% of the original kernel matrix.

torch.Size([41879, 2])
We keep 1.08e+07/7.30e+08 =  1% of the original kernel matrix.

torch.Size([186424, 2])
We keep 1.82e+08/1.57e+10 =  1% of the original kernel matrix.

torch.Size([78336, 2])
We keep 3.18e+07/2.53e+09 =  1% of the original kernel matrix.

torch.Size([18004, 2])
We keep 3.90e+06/1.17e+08 =  3% of the original kernel matrix.

torch.Size([23752, 2])
We keep 4.14e+06/2.18e+08 =  1% of the original kernel matrix.

torch.Size([61862, 2])
We keep 5.83e+07/2.05e+09 =  2% of the original kernel matrix.

torch.Size([43959, 2])
We keep 1.35e+07/9.12e+08 =  1% of the original kernel matrix.

torch.Size([53869, 2])
We keep 1.37e+08/1.67e+09 =  8% of the original kernel matrix.

torch.Size([41033, 2])
We keep 1.14e+07/8.24e+08 =  1% of the original kernel matrix.

time for making ranges is 4.460130214691162
Sorting X and nu_X
time for sorting X is 0.09276986122131348
Sorting Z and nu_Z
time for sorting Z is 0.00024056434631347656
Starting Optim
sum tnu_Z before tensor(38040280., device='cuda:0')
c= tensor(5046.5825, device='cuda:0')
c= tensor(169335.7188, device='cuda:0')
c= tensor(174001.5000, device='cuda:0')
c= tensor(385683., device='cuda:0')
c= tensor(475477.8125, device='cuda:0')
c= tensor(1520373.5000, device='cuda:0')
c= tensor(2104994.5000, device='cuda:0')
c= tensor(2436675., device='cuda:0')
c= tensor(2543601.7500, device='cuda:0')
c= tensor(30445250., device='cuda:0')
c= tensor(30488960., device='cuda:0')
c= tensor(34104004., device='cuda:0')
c= tensor(34129804., device='cuda:0')
c= tensor(74129888., device='cuda:0')
c= tensor(74386952., device='cuda:0')
c= tensor(75262440., device='cuda:0')
c= tensor(76355704., device='cuda:0')
c= tensor(76622624., device='cuda:0')
c= tensor(82731264., device='cuda:0')
c= tensor(86028328., device='cuda:0')
c= tensor(89183120., device='cuda:0')
c= tensor(1.0057e+08, device='cuda:0')
c= tensor(1.0061e+08, device='cuda:0')
c= tensor(1.0176e+08, device='cuda:0')
c= tensor(1.0177e+08, device='cuda:0')
c= tensor(1.0271e+08, device='cuda:0')
c= tensor(1.0434e+08, device='cuda:0')
c= tensor(1.0438e+08, device='cuda:0')
c= tensor(1.0751e+08, device='cuda:0')
c= tensor(3.9800e+08, device='cuda:0')
c= tensor(3.9814e+08, device='cuda:0')
c= tensor(5.3263e+08, device='cuda:0')
c= tensor(5.3303e+08, device='cuda:0')
c= tensor(5.3310e+08, device='cuda:0')
c= tensor(5.3367e+08, device='cuda:0')
c= tensor(5.4772e+08, device='cuda:0')
c= tensor(5.4994e+08, device='cuda:0')
c= tensor(5.4994e+08, device='cuda:0')
c= tensor(5.4995e+08, device='cuda:0')
c= tensor(5.4995e+08, device='cuda:0')
c= tensor(5.4996e+08, device='cuda:0')
c= tensor(5.4997e+08, device='cuda:0')
c= tensor(5.4997e+08, device='cuda:0')
c= tensor(5.4998e+08, device='cuda:0')
c= tensor(5.4998e+08, device='cuda:0')
c= tensor(5.4998e+08, device='cuda:0')
c= tensor(5.4998e+08, device='cuda:0')
c= tensor(5.5000e+08, device='cuda:0')
c= tensor(5.5001e+08, device='cuda:0')
c= tensor(5.5005e+08, device='cuda:0')
c= tensor(5.5010e+08, device='cuda:0')
c= tensor(5.5010e+08, device='cuda:0')
c= tensor(5.5013e+08, device='cuda:0')
c= tensor(5.5015e+08, device='cuda:0')
c= tensor(5.5015e+08, device='cuda:0')
c= tensor(5.5019e+08, device='cuda:0')
c= tensor(5.5019e+08, device='cuda:0')
c= tensor(5.5021e+08, device='cuda:0')
c= tensor(5.5023e+08, device='cuda:0')
c= tensor(5.5024e+08, device='cuda:0')
c= tensor(5.5025e+08, device='cuda:0')
c= tensor(5.5025e+08, device='cuda:0')
c= tensor(5.5026e+08, device='cuda:0')
c= tensor(5.5031e+08, device='cuda:0')
c= tensor(5.5031e+08, device='cuda:0')
c= tensor(5.5032e+08, device='cuda:0')
c= tensor(5.5032e+08, device='cuda:0')
c= tensor(5.5034e+08, device='cuda:0')
c= tensor(5.5037e+08, device='cuda:0')
c= tensor(5.5038e+08, device='cuda:0')
c= tensor(5.5040e+08, device='cuda:0')
c= tensor(5.5041e+08, device='cuda:0')
c= tensor(5.5041e+08, device='cuda:0')
c= tensor(5.5042e+08, device='cuda:0')
c= tensor(5.5044e+08, device='cuda:0')
c= tensor(5.5045e+08, device='cuda:0')
c= tensor(5.5045e+08, device='cuda:0')
c= tensor(5.5046e+08, device='cuda:0')
c= tensor(5.5047e+08, device='cuda:0')
c= tensor(5.5054e+08, device='cuda:0')
c= tensor(5.5054e+08, device='cuda:0')
c= tensor(5.5055e+08, device='cuda:0')
c= tensor(5.5056e+08, device='cuda:0')
c= tensor(5.5056e+08, device='cuda:0')
c= tensor(5.5056e+08, device='cuda:0')
c= tensor(5.5056e+08, device='cuda:0')
c= tensor(5.5057e+08, device='cuda:0')
c= tensor(5.5057e+08, device='cuda:0')
c= tensor(5.5058e+08, device='cuda:0')
c= tensor(5.5059e+08, device='cuda:0')
c= tensor(5.5061e+08, device='cuda:0')
c= tensor(5.5061e+08, device='cuda:0')
c= tensor(5.5061e+08, device='cuda:0')
c= tensor(5.5062e+08, device='cuda:0')
c= tensor(5.5064e+08, device='cuda:0')
c= tensor(5.5066e+08, device='cuda:0')
c= tensor(5.5066e+08, device='cuda:0')
c= tensor(5.5069e+08, device='cuda:0')
c= tensor(5.5071e+08, device='cuda:0')
c= tensor(5.5072e+08, device='cuda:0')
c= tensor(5.5075e+08, device='cuda:0')
c= tensor(5.5076e+08, device='cuda:0')
c= tensor(5.5077e+08, device='cuda:0')
c= tensor(5.5077e+08, device='cuda:0')
c= tensor(5.5081e+08, device='cuda:0')
c= tensor(5.5081e+08, device='cuda:0')
c= tensor(5.5082e+08, device='cuda:0')
c= tensor(5.5082e+08, device='cuda:0')
c= tensor(5.5083e+08, device='cuda:0')
c= tensor(5.5083e+08, device='cuda:0')
c= tensor(5.5084e+08, device='cuda:0')
c= tensor(5.5084e+08, device='cuda:0')
c= tensor(5.5085e+08, device='cuda:0')
c= tensor(5.5085e+08, device='cuda:0')
c= tensor(5.5086e+08, device='cuda:0')
c= tensor(5.5087e+08, device='cuda:0')
c= tensor(5.5088e+08, device='cuda:0')
c= tensor(5.5088e+08, device='cuda:0')
c= tensor(5.5091e+08, device='cuda:0')
c= tensor(5.5091e+08, device='cuda:0')
c= tensor(5.5094e+08, device='cuda:0')
c= tensor(5.5094e+08, device='cuda:0')
c= tensor(5.5094e+08, device='cuda:0')
c= tensor(5.5095e+08, device='cuda:0')
c= tensor(5.5096e+08, device='cuda:0')
c= tensor(5.5096e+08, device='cuda:0')
c= tensor(5.5097e+08, device='cuda:0')
c= tensor(5.5098e+08, device='cuda:0')
c= tensor(5.5099e+08, device='cuda:0')
c= tensor(5.5100e+08, device='cuda:0')
c= tensor(5.5108e+08, device='cuda:0')
c= tensor(5.5109e+08, device='cuda:0')
c= tensor(5.5110e+08, device='cuda:0')
c= tensor(5.5110e+08, device='cuda:0')
c= tensor(5.5111e+08, device='cuda:0')
c= tensor(5.5111e+08, device='cuda:0')
c= tensor(5.5111e+08, device='cuda:0')
c= tensor(5.5112e+08, device='cuda:0')
c= tensor(5.5112e+08, device='cuda:0')
c= tensor(5.5113e+08, device='cuda:0')
c= tensor(5.5113e+08, device='cuda:0')
c= tensor(5.5113e+08, device='cuda:0')
c= tensor(5.5119e+08, device='cuda:0')
c= tensor(5.5120e+08, device='cuda:0')
c= tensor(5.5121e+08, device='cuda:0')
c= tensor(5.5121e+08, device='cuda:0')
c= tensor(5.5122e+08, device='cuda:0')
c= tensor(5.5122e+08, device='cuda:0')
c= tensor(5.5122e+08, device='cuda:0')
c= tensor(5.5123e+08, device='cuda:0')
c= tensor(5.5124e+08, device='cuda:0')
c= tensor(5.5124e+08, device='cuda:0')
c= tensor(5.5125e+08, device='cuda:0')
c= tensor(5.5128e+08, device='cuda:0')
c= tensor(5.5129e+08, device='cuda:0')
c= tensor(5.5142e+08, device='cuda:0')
c= tensor(5.5143e+08, device='cuda:0')
c= tensor(5.5144e+08, device='cuda:0')
c= tensor(5.5144e+08, device='cuda:0')
c= tensor(5.5144e+08, device='cuda:0')
c= tensor(5.5150e+08, device='cuda:0')
c= tensor(5.5150e+08, device='cuda:0')
c= tensor(5.5152e+08, device='cuda:0')
c= tensor(5.5152e+08, device='cuda:0')
c= tensor(5.5154e+08, device='cuda:0')
c= tensor(5.5154e+08, device='cuda:0')
c= tensor(5.5155e+08, device='cuda:0')
c= tensor(5.5156e+08, device='cuda:0')
c= tensor(5.5157e+08, device='cuda:0')
c= tensor(5.5157e+08, device='cuda:0')
c= tensor(5.5157e+08, device='cuda:0')
c= tensor(5.5157e+08, device='cuda:0')
c= tensor(5.5159e+08, device='cuda:0')
c= tensor(5.5159e+08, device='cuda:0')
c= tensor(5.5160e+08, device='cuda:0')
c= tensor(5.5176e+08, device='cuda:0')
c= tensor(5.5178e+08, device='cuda:0')
c= tensor(5.5178e+08, device='cuda:0')
c= tensor(5.5180e+08, device='cuda:0')
c= tensor(5.5180e+08, device='cuda:0')
c= tensor(5.5182e+08, device='cuda:0')
c= tensor(5.5182e+08, device='cuda:0')
c= tensor(5.5184e+08, device='cuda:0')
c= tensor(5.5184e+08, device='cuda:0')
c= tensor(5.5184e+08, device='cuda:0')
c= tensor(5.5185e+08, device='cuda:0')
c= tensor(5.5186e+08, device='cuda:0')
c= tensor(5.5187e+08, device='cuda:0')
c= tensor(5.5188e+08, device='cuda:0')
c= tensor(5.5189e+08, device='cuda:0')
c= tensor(5.5190e+08, device='cuda:0')
c= tensor(5.5190e+08, device='cuda:0')
c= tensor(5.5191e+08, device='cuda:0')
c= tensor(5.5193e+08, device='cuda:0')
c= tensor(5.5198e+08, device='cuda:0')
c= tensor(5.5198e+08, device='cuda:0')
c= tensor(5.5199e+08, device='cuda:0')
c= tensor(5.5199e+08, device='cuda:0')
c= tensor(5.5200e+08, device='cuda:0')
c= tensor(5.5200e+08, device='cuda:0')
c= tensor(5.5201e+08, device='cuda:0')
c= tensor(5.5202e+08, device='cuda:0')
c= tensor(5.5204e+08, device='cuda:0')
c= tensor(5.5204e+08, device='cuda:0')
c= tensor(5.5205e+08, device='cuda:0')
c= tensor(5.5206e+08, device='cuda:0')
c= tensor(5.5206e+08, device='cuda:0')
c= tensor(5.5207e+08, device='cuda:0')
c= tensor(5.5207e+08, device='cuda:0')
c= tensor(5.5209e+08, device='cuda:0')
c= tensor(5.5213e+08, device='cuda:0')
c= tensor(5.5215e+08, device='cuda:0')
c= tensor(5.5217e+08, device='cuda:0')
c= tensor(5.5220e+08, device='cuda:0')
c= tensor(5.5220e+08, device='cuda:0')
c= tensor(5.5221e+08, device='cuda:0')
c= tensor(5.5221e+08, device='cuda:0')
c= tensor(5.5221e+08, device='cuda:0')
c= tensor(5.5225e+08, device='cuda:0')
c= tensor(5.5227e+08, device='cuda:0')
c= tensor(5.5227e+08, device='cuda:0')
c= tensor(5.5228e+08, device='cuda:0')
c= tensor(5.5229e+08, device='cuda:0')
c= tensor(5.5230e+08, device='cuda:0')
c= tensor(5.5230e+08, device='cuda:0')
c= tensor(5.5231e+08, device='cuda:0')
c= tensor(5.5232e+08, device='cuda:0')
c= tensor(5.5232e+08, device='cuda:0')
c= tensor(5.5233e+08, device='cuda:0')
c= tensor(5.5234e+08, device='cuda:0')
c= tensor(5.5235e+08, device='cuda:0')
c= tensor(5.5235e+08, device='cuda:0')
c= tensor(5.5236e+08, device='cuda:0')
c= tensor(5.5236e+08, device='cuda:0')
c= tensor(5.5237e+08, device='cuda:0')
c= tensor(5.5238e+08, device='cuda:0')
c= tensor(5.5239e+08, device='cuda:0')
c= tensor(5.5240e+08, device='cuda:0')
c= tensor(5.5241e+08, device='cuda:0')
c= tensor(5.5270e+08, device='cuda:0')
c= tensor(5.5423e+08, device='cuda:0')
c= tensor(5.5427e+08, device='cuda:0')
c= tensor(5.5432e+08, device='cuda:0')
c= tensor(5.5432e+08, device='cuda:0')
c= tensor(5.5434e+08, device='cuda:0')
c= tensor(5.5441e+08, device='cuda:0')
c= tensor(5.6833e+08, device='cuda:0')
c= tensor(5.6833e+08, device='cuda:0')
c= tensor(5.8027e+08, device='cuda:0')
c= tensor(5.8298e+08, device='cuda:0')
c= tensor(5.8545e+08, device='cuda:0')
c= tensor(5.8631e+08, device='cuda:0')
c= tensor(5.8632e+08, device='cuda:0')
c= tensor(5.8633e+08, device='cuda:0')
c= tensor(5.9870e+08, device='cuda:0')
c= tensor(6.5170e+08, device='cuda:0')
c= tensor(6.5171e+08, device='cuda:0')
c= tensor(6.5234e+08, device='cuda:0')
c= tensor(6.5291e+08, device='cuda:0')
c= tensor(6.5301e+08, device='cuda:0')
c= tensor(6.6059e+08, device='cuda:0')
c= tensor(6.6230e+08, device='cuda:0')
c= tensor(6.6271e+08, device='cuda:0')
c= tensor(6.6285e+08, device='cuda:0')
c= tensor(6.6286e+08, device='cuda:0')
c= tensor(6.7406e+08, device='cuda:0')
c= tensor(6.7433e+08, device='cuda:0')
c= tensor(6.7434e+08, device='cuda:0')
c= tensor(6.7453e+08, device='cuda:0')
c= tensor(6.7878e+08, device='cuda:0')
c= tensor(6.8886e+08, device='cuda:0')
c= tensor(6.9074e+08, device='cuda:0')
c= tensor(6.9077e+08, device='cuda:0')
c= tensor(6.9139e+08, device='cuda:0')
c= tensor(6.9141e+08, device='cuda:0')
c= tensor(6.9238e+08, device='cuda:0')
c= tensor(6.9415e+08, device='cuda:0')
c= tensor(6.9472e+08, device='cuda:0')
c= tensor(6.9741e+08, device='cuda:0')
c= tensor(6.9741e+08, device='cuda:0')
c= tensor(6.9746e+08, device='cuda:0')
c= tensor(7.0079e+08, device='cuda:0')
c= tensor(7.0195e+08, device='cuda:0')
c= tensor(7.0559e+08, device='cuda:0')
c= tensor(7.0560e+08, device='cuda:0')
c= tensor(7.3232e+08, device='cuda:0')
c= tensor(7.3242e+08, device='cuda:0')
c= tensor(7.3271e+08, device='cuda:0')
c= tensor(7.3450e+08, device='cuda:0')
c= tensor(7.3450e+08, device='cuda:0')
c= tensor(7.4000e+08, device='cuda:0')
c= tensor(7.5829e+08, device='cuda:0')
c= tensor(8.2447e+08, device='cuda:0')
c= tensor(8.2693e+08, device='cuda:0')
c= tensor(8.2753e+08, device='cuda:0')
c= tensor(8.2764e+08, device='cuda:0')
c= tensor(8.2766e+08, device='cuda:0')
c= tensor(8.3784e+08, device='cuda:0')
c= tensor(8.3787e+08, device='cuda:0')
c= tensor(8.3874e+08, device='cuda:0')
c= tensor(8.4561e+08, device='cuda:0')
c= tensor(8.4625e+08, device='cuda:0')
c= tensor(8.4652e+08, device='cuda:0')
c= tensor(8.4653e+08, device='cuda:0')
c= tensor(8.5286e+08, device='cuda:0')
c= tensor(8.5753e+08, device='cuda:0')
c= tensor(8.5858e+08, device='cuda:0')
c= tensor(8.5859e+08, device='cuda:0')
c= tensor(8.6166e+08, device='cuda:0')
c= tensor(8.6211e+08, device='cuda:0')
c= tensor(8.7676e+08, device='cuda:0')
c= tensor(8.7683e+08, device='cuda:0')
c= tensor(8.7999e+08, device='cuda:0')
c= tensor(8.8021e+08, device='cuda:0')
c= tensor(8.8972e+08, device='cuda:0')
c= tensor(8.9070e+08, device='cuda:0')
c= tensor(8.9092e+08, device='cuda:0')
c= tensor(8.9532e+08, device='cuda:0')
c= tensor(9.0070e+08, device='cuda:0')
c= tensor(9.0073e+08, device='cuda:0')
c= tensor(9.1680e+08, device='cuda:0')
c= tensor(9.2645e+08, device='cuda:0')
c= tensor(9.5970e+08, device='cuda:0')
c= tensor(9.6009e+08, device='cuda:0')
c= tensor(9.6009e+08, device='cuda:0')
c= tensor(9.6015e+08, device='cuda:0')
c= tensor(9.6118e+08, device='cuda:0')
c= tensor(9.6140e+08, device='cuda:0')
c= tensor(9.6493e+08, device='cuda:0')
c= tensor(9.6493e+08, device='cuda:0')
c= tensor(9.6654e+08, device='cuda:0')
c= tensor(9.7002e+08, device='cuda:0')
c= tensor(9.7047e+08, device='cuda:0')
c= tensor(9.7053e+08, device='cuda:0')
c= tensor(9.7310e+08, device='cuda:0')
c= tensor(9.7316e+08, device='cuda:0')
c= tensor(9.7320e+08, device='cuda:0')
c= tensor(9.7328e+08, device='cuda:0')
c= tensor(9.7329e+08, device='cuda:0')
c= tensor(9.7510e+08, device='cuda:0')
c= tensor(9.7568e+08, device='cuda:0')
c= tensor(9.7591e+08, device='cuda:0')
c= tensor(9.7723e+08, device='cuda:0')
c= tensor(9.7730e+08, device='cuda:0')
c= tensor(1.1548e+09, device='cuda:0')
c= tensor(1.1550e+09, device='cuda:0')
c= tensor(1.1620e+09, device='cuda:0')
c= tensor(1.1620e+09, device='cuda:0')
c= tensor(1.1620e+09, device='cuda:0')
c= tensor(1.1620e+09, device='cuda:0')
c= tensor(1.1622e+09, device='cuda:0')
c= tensor(1.1622e+09, device='cuda:0')
c= tensor(1.1681e+09, device='cuda:0')
c= tensor(1.1681e+09, device='cuda:0')
c= tensor(1.1681e+09, device='cuda:0')
c= tensor(1.1792e+09, device='cuda:0')
c= tensor(1.1805e+09, device='cuda:0')
c= tensor(1.1810e+09, device='cuda:0')
c= tensor(1.1848e+09, device='cuda:0')
c= tensor(1.2053e+09, device='cuda:0')
c= tensor(1.2053e+09, device='cuda:0')
c= tensor(1.2054e+09, device='cuda:0')
c= tensor(1.2056e+09, device='cuda:0')
c= tensor(1.2056e+09, device='cuda:0')
c= tensor(1.2056e+09, device='cuda:0')
c= tensor(1.2057e+09, device='cuda:0')
c= tensor(1.2057e+09, device='cuda:0')
c= tensor(1.2058e+09, device='cuda:0')
c= tensor(1.2059e+09, device='cuda:0')
c= tensor(1.2060e+09, device='cuda:0')
c= tensor(1.2331e+09, device='cuda:0')
c= tensor(1.2332e+09, device='cuda:0')
c= tensor(1.2349e+09, device='cuda:0')
c= tensor(1.2350e+09, device='cuda:0')
c= tensor(1.2350e+09, device='cuda:0')
c= tensor(1.2409e+09, device='cuda:0')
c= tensor(1.3257e+09, device='cuda:0')
c= tensor(1.3655e+09, device='cuda:0')
c= tensor(1.3657e+09, device='cuda:0')
c= tensor(1.3663e+09, device='cuda:0')
c= tensor(1.3663e+09, device='cuda:0')
c= tensor(1.3663e+09, device='cuda:0')
c= tensor(1.4932e+09, device='cuda:0')
c= tensor(1.4934e+09, device='cuda:0')
c= tensor(1.4934e+09, device='cuda:0')
c= tensor(1.4946e+09, device='cuda:0')
c= tensor(1.5930e+09, device='cuda:0')
c= tensor(1.5939e+09, device='cuda:0')
c= tensor(1.5941e+09, device='cuda:0')
c= tensor(1.5941e+09, device='cuda:0')
c= tensor(1.5942e+09, device='cuda:0')
c= tensor(1.5943e+09, device='cuda:0')
c= tensor(1.6033e+09, device='cuda:0')
c= tensor(1.6034e+09, device='cuda:0')
c= tensor(1.6034e+09, device='cuda:0')
c= tensor(1.6062e+09, device='cuda:0')
c= tensor(1.6063e+09, device='cuda:0')
c= tensor(1.6063e+09, device='cuda:0')
c= tensor(1.6074e+09, device='cuda:0')
c= tensor(1.6098e+09, device='cuda:0')
c= tensor(1.6292e+09, device='cuda:0')
c= tensor(1.6406e+09, device='cuda:0')
c= tensor(1.6510e+09, device='cuda:0')
c= tensor(1.6513e+09, device='cuda:0')
c= tensor(1.6517e+09, device='cuda:0')
c= tensor(1.6577e+09, device='cuda:0')
c= tensor(1.6650e+09, device='cuda:0')
c= tensor(1.6651e+09, device='cuda:0')
c= tensor(1.7250e+09, device='cuda:0')
c= tensor(1.8055e+09, device='cuda:0')
c= tensor(1.8188e+09, device='cuda:0')
c= tensor(1.8228e+09, device='cuda:0')
c= tensor(1.8250e+09, device='cuda:0')
c= tensor(1.8252e+09, device='cuda:0')
c= tensor(1.8252e+09, device='cuda:0')
c= tensor(1.8253e+09, device='cuda:0')
c= tensor(1.8279e+09, device='cuda:0')
c= tensor(1.8327e+09, device='cuda:0')
c= tensor(1.8577e+09, device='cuda:0')
c= tensor(1.8615e+09, device='cuda:0')
c= tensor(1.8634e+09, device='cuda:0')
c= tensor(1.8636e+09, device='cuda:0')
c= tensor(1.8695e+09, device='cuda:0')
c= tensor(1.8695e+09, device='cuda:0')
c= tensor(1.8696e+09, device='cuda:0')
c= tensor(1.8733e+09, device='cuda:0')
c= tensor(1.8748e+09, device='cuda:0')
c= tensor(1.8749e+09, device='cuda:0')
c= tensor(1.8751e+09, device='cuda:0')
c= tensor(2.0250e+09, device='cuda:0')
c= tensor(2.0251e+09, device='cuda:0')
c= tensor(2.0304e+09, device='cuda:0')
c= tensor(2.0304e+09, device='cuda:0')
c= tensor(2.0305e+09, device='cuda:0')
c= tensor(2.0305e+09, device='cuda:0')
c= tensor(2.0305e+09, device='cuda:0')
c= tensor(2.0307e+09, device='cuda:0')
c= tensor(2.0318e+09, device='cuda:0')
c= tensor(2.0318e+09, device='cuda:0')
c= tensor(2.0366e+09, device='cuda:0')
c= tensor(2.0366e+09, device='cuda:0')
c= tensor(2.0379e+09, device='cuda:0')
c= tensor(2.0380e+09, device='cuda:0')
c= tensor(2.0444e+09, device='cuda:0')
c= tensor(2.0444e+09, device='cuda:0')
c= tensor(2.0449e+09, device='cuda:0')
c= tensor(2.0452e+09, device='cuda:0')
c= tensor(2.0456e+09, device='cuda:0')
c= tensor(2.0470e+09, device='cuda:0')
c= tensor(2.1586e+09, device='cuda:0')
c= tensor(2.1587e+09, device='cuda:0')
c= tensor(2.1587e+09, device='cuda:0')
c= tensor(2.1595e+09, device='cuda:0')
c= tensor(2.1597e+09, device='cuda:0')
c= tensor(2.2040e+09, device='cuda:0')
c= tensor(2.2041e+09, device='cuda:0')
c= tensor(2.2077e+09, device='cuda:0')
c= tensor(2.2237e+09, device='cuda:0')
c= tensor(2.2237e+09, device='cuda:0')
c= tensor(2.2423e+09, device='cuda:0')
c= tensor(2.2427e+09, device='cuda:0')
c= tensor(2.3055e+09, device='cuda:0')
c= tensor(2.3056e+09, device='cuda:0')
c= tensor(2.3057e+09, device='cuda:0')
c= tensor(2.3059e+09, device='cuda:0')
c= tensor(2.3059e+09, device='cuda:0')
c= tensor(2.3059e+09, device='cuda:0')
c= tensor(2.3094e+09, device='cuda:0')
c= tensor(2.3094e+09, device='cuda:0')
c= tensor(2.3152e+09, device='cuda:0')
c= tensor(2.3152e+09, device='cuda:0')
c= tensor(2.3153e+09, device='cuda:0')
c= tensor(2.3154e+09, device='cuda:0')
c= tensor(2.3236e+09, device='cuda:0')
c= tensor(2.3333e+09, device='cuda:0')
c= tensor(2.3520e+09, device='cuda:0')
c= tensor(2.3522e+09, device='cuda:0')
c= tensor(2.3523e+09, device='cuda:0')
c= tensor(2.3523e+09, device='cuda:0')
c= tensor(2.3523e+09, device='cuda:0')
c= tensor(2.3737e+09, device='cuda:0')
c= tensor(2.3737e+09, device='cuda:0')
c= tensor(2.3739e+09, device='cuda:0')
c= tensor(2.3756e+09, device='cuda:0')
c= tensor(2.3762e+09, device='cuda:0')
c= tensor(2.3771e+09, device='cuda:0')
c= tensor(2.3772e+09, device='cuda:0')
c= tensor(2.3919e+09, device='cuda:0')
c= tensor(2.3933e+09, device='cuda:0')
c= tensor(2.3933e+09, device='cuda:0')
c= tensor(2.3935e+09, device='cuda:0')
c= tensor(2.3947e+09, device='cuda:0')
c= tensor(2.3969e+09, device='cuda:0')
c= tensor(2.4636e+09, device='cuda:0')
c= tensor(2.4669e+09, device='cuda:0')
c= tensor(2.4670e+09, device='cuda:0')
c= tensor(2.4674e+09, device='cuda:0')
c= tensor(2.4681e+09, device='cuda:0')
c= tensor(2.4681e+09, device='cuda:0')
c= tensor(2.4682e+09, device='cuda:0')
c= tensor(2.4682e+09, device='cuda:0')
c= tensor(2.4690e+09, device='cuda:0')
c= tensor(2.4703e+09, device='cuda:0')
c= tensor(2.4703e+09, device='cuda:0')
c= tensor(2.4704e+09, device='cuda:0')
c= tensor(2.4705e+09, device='cuda:0')
c= tensor(2.4709e+09, device='cuda:0')
c= tensor(2.4709e+09, device='cuda:0')
c= tensor(2.4711e+09, device='cuda:0')
c= tensor(2.4714e+09, device='cuda:0')
c= tensor(2.4715e+09, device='cuda:0')
c= tensor(2.4715e+09, device='cuda:0')
c= tensor(2.4715e+09, device='cuda:0')
c= tensor(2.4717e+09, device='cuda:0')
c= tensor(2.4804e+09, device='cuda:0')
c= tensor(2.4805e+09, device='cuda:0')
c= tensor(2.4805e+09, device='cuda:0')
c= tensor(2.4805e+09, device='cuda:0')
c= tensor(2.4856e+09, device='cuda:0')
c= tensor(2.5294e+09, device='cuda:0')
c= tensor(2.5336e+09, device='cuda:0')
c= tensor(2.5336e+09, device='cuda:0')
c= tensor(2.5582e+09, device='cuda:0')
c= tensor(2.5636e+09, device='cuda:0')
c= tensor(2.5636e+09, device='cuda:0')
c= tensor(2.5637e+09, device='cuda:0')
c= tensor(2.5643e+09, device='cuda:0')
c= tensor(2.5738e+09, device='cuda:0')
c= tensor(2.6216e+09, device='cuda:0')
c= tensor(2.6285e+09, device='cuda:0')
c= tensor(2.6291e+09, device='cuda:0')
c= tensor(2.6292e+09, device='cuda:0')
c= tensor(2.6292e+09, device='cuda:0')
c= tensor(2.6308e+09, device='cuda:0')
c= tensor(2.6309e+09, device='cuda:0')
c= tensor(2.6314e+09, device='cuda:0')
c= tensor(2.6350e+09, device='cuda:0')
c= tensor(2.6760e+09, device='cuda:0')
c= tensor(2.6761e+09, device='cuda:0')
c= tensor(2.6761e+09, device='cuda:0')
c= tensor(2.6764e+09, device='cuda:0')
c= tensor(2.6900e+09, device='cuda:0')
c= tensor(2.6906e+09, device='cuda:0')
c= tensor(2.6907e+09, device='cuda:0')
c= tensor(2.6907e+09, device='cuda:0')
c= tensor(2.6910e+09, device='cuda:0')
c= tensor(2.6911e+09, device='cuda:0')
c= tensor(2.6924e+09, device='cuda:0')
c= tensor(2.6924e+09, device='cuda:0')
c= tensor(2.6925e+09, device='cuda:0')
c= tensor(2.6925e+09, device='cuda:0')
c= tensor(2.6926e+09, device='cuda:0')
c= tensor(2.6928e+09, device='cuda:0')
c= tensor(2.6991e+09, device='cuda:0')
c= tensor(2.7176e+09, device='cuda:0')
c= tensor(2.7205e+09, device='cuda:0')
c= tensor(2.7308e+09, device='cuda:0')
c= tensor(2.7309e+09, device='cuda:0')
c= tensor(2.7311e+09, device='cuda:0')
c= tensor(2.7312e+09, device='cuda:0')
c= tensor(2.7348e+09, device='cuda:0')
c= tensor(2.7351e+09, device='cuda:0')
c= tensor(2.7375e+09, device='cuda:0')
c= tensor(2.7375e+09, device='cuda:0')
c= tensor(2.9412e+09, device='cuda:0')
c= tensor(2.9413e+09, device='cuda:0')
c= tensor(2.9425e+09, device='cuda:0')
c= tensor(2.9545e+09, device='cuda:0')
c= tensor(2.9549e+09, device='cuda:0')
c= tensor(2.9554e+09, device='cuda:0')
c= tensor(3.2063e+09, device='cuda:0')
c= tensor(3.2181e+09, device='cuda:0')
c= tensor(3.2206e+09, device='cuda:0')
c= tensor(3.2207e+09, device='cuda:0')
c= tensor(3.2211e+09, device='cuda:0')
c= tensor(3.2212e+09, device='cuda:0')
c= tensor(3.2395e+09, device='cuda:0')
c= tensor(3.4246e+09, device='cuda:0')
c= tensor(3.4290e+09, device='cuda:0')
c= tensor(3.4349e+09, device='cuda:0')
c= tensor(3.4394e+09, device='cuda:0')
c= tensor(3.4399e+09, device='cuda:0')
c= tensor(3.4404e+09, device='cuda:0')
c= tensor(3.4405e+09, device='cuda:0')
c= tensor(3.4558e+09, device='cuda:0')
c= tensor(3.4866e+09, device='cuda:0')
c= tensor(3.4894e+09, device='cuda:0')
c= tensor(3.6745e+09, device='cuda:0')
c= tensor(3.6828e+09, device='cuda:0')
c= tensor(3.6837e+09, device='cuda:0')
c= tensor(3.6839e+09, device='cuda:0')
c= tensor(3.6861e+09, device='cuda:0')
c= tensor(3.6899e+09, device='cuda:0')
c= tensor(3.6900e+09, device='cuda:0')
c= tensor(3.7364e+09, device='cuda:0')
c= tensor(3.7374e+09, device='cuda:0')
c= tensor(3.7382e+09, device='cuda:0')
c= tensor(3.7382e+09, device='cuda:0')
c= tensor(3.7383e+09, device='cuda:0')
c= tensor(3.7383e+09, device='cuda:0')
c= tensor(3.7383e+09, device='cuda:0')
c= tensor(3.7385e+09, device='cuda:0')
c= tensor(3.7391e+09, device='cuda:0')
c= tensor(5.6012e+09, device='cuda:0')
c= tensor(5.6018e+09, device='cuda:0')
c= tensor(5.6196e+09, device='cuda:0')
c= tensor(5.6196e+09, device='cuda:0')
c= tensor(5.6197e+09, device='cuda:0')
c= tensor(5.6198e+09, device='cuda:0')
c= tensor(5.6358e+09, device='cuda:0')
c= tensor(5.6370e+09, device='cuda:0')
c= tensor(5.7826e+09, device='cuda:0')
c= tensor(5.7826e+09, device='cuda:0')
c= tensor(5.7887e+09, device='cuda:0')
c= tensor(5.7890e+09, device='cuda:0')
c= tensor(5.7945e+09, device='cuda:0')
c= tensor(5.8067e+09, device='cuda:0')
c= tensor(5.8070e+09, device='cuda:0')
c= tensor(5.8070e+09, device='cuda:0')
c= tensor(5.8086e+09, device='cuda:0')
c= tensor(5.8092e+09, device='cuda:0')
c= tensor(5.8101e+09, device='cuda:0')
c= tensor(5.8249e+09, device='cuda:0')
c= tensor(5.8251e+09, device='cuda:0')
c= tensor(5.8266e+09, device='cuda:0')
c= tensor(5.8268e+09, device='cuda:0')
c= tensor(5.8313e+09, device='cuda:0')
c= tensor(5.8574e+09, device='cuda:0')
c= tensor(5.8587e+09, device='cuda:0')
c= tensor(5.8588e+09, device='cuda:0')
c= tensor(5.8656e+09, device='cuda:0')
c= tensor(5.8680e+09, device='cuda:0')
c= tensor(5.9051e+09, device='cuda:0')
c= tensor(5.9062e+09, device='cuda:0')
c= tensor(5.9066e+09, device='cuda:0')
c= tensor(5.9082e+09, device='cuda:0')
c= tensor(5.9253e+09, device='cuda:0')
c= tensor(5.9315e+09, device='cuda:0')
c= tensor(5.9317e+09, device='cuda:0')
c= tensor(5.9317e+09, device='cuda:0')
c= tensor(5.9317e+09, device='cuda:0')
c= tensor(5.9347e+09, device='cuda:0')
c= tensor(5.9390e+09, device='cuda:0')
c= tensor(5.9401e+09, device='cuda:0')
c= tensor(5.9402e+09, device='cuda:0')
c= tensor(5.9404e+09, device='cuda:0')
c= tensor(5.9416e+09, device='cuda:0')
c= tensor(5.9425e+09, device='cuda:0')
c= tensor(5.9427e+09, device='cuda:0')
c= tensor(5.9442e+09, device='cuda:0')
c= tensor(5.9445e+09, device='cuda:0')
c= tensor(5.9450e+09, device='cuda:0')
c= tensor(5.9450e+09, device='cuda:0')
c= tensor(5.9450e+09, device='cuda:0')
c= tensor(5.9557e+09, device='cuda:0')
c= tensor(5.9559e+09, device='cuda:0')
c= tensor(5.9576e+09, device='cuda:0')
c= tensor(5.9576e+09, device='cuda:0')
c= tensor(5.9577e+09, device='cuda:0')
c= tensor(5.9582e+09, device='cuda:0')
c= tensor(5.9600e+09, device='cuda:0')
c= tensor(5.9604e+09, device='cuda:0')
c= tensor(5.9604e+09, device='cuda:0')
c= tensor(5.9605e+09, device='cuda:0')
c= tensor(5.9612e+09, device='cuda:0')
c= tensor(5.9615e+09, device='cuda:0')
c= tensor(5.9648e+09, device='cuda:0')
c= tensor(5.9649e+09, device='cuda:0')
c= tensor(5.9649e+09, device='cuda:0')
c= tensor(5.9670e+09, device='cuda:0')
c= tensor(5.9670e+09, device='cuda:0')
c= tensor(5.9753e+09, device='cuda:0')
c= tensor(5.9755e+09, device='cuda:0')
c= tensor(5.9783e+09, device='cuda:0')
c= tensor(5.9827e+09, device='cuda:0')
c= tensor(5.9853e+09, device='cuda:0')
c= tensor(5.9902e+09, device='cuda:0')
c= tensor(5.9927e+09, device='cuda:0')
c= tensor(5.9927e+09, device='cuda:0')
c= tensor(5.9935e+09, device='cuda:0')
c= tensor(5.9937e+09, device='cuda:0')
c= tensor(5.9959e+09, device='cuda:0')
c= tensor(5.9987e+09, device='cuda:0')
c= tensor(5.9996e+09, device='cuda:0')
c= tensor(6.0009e+09, device='cuda:0')
c= tensor(6.0015e+09, device='cuda:0')
c= tensor(6.0061e+09, device='cuda:0')
c= tensor(6.0061e+09, device='cuda:0')
c= tensor(6.0062e+09, device='cuda:0')
c= tensor(6.0228e+09, device='cuda:0')
c= tensor(6.0841e+09, device='cuda:0')
c= tensor(6.0842e+09, device='cuda:0')
c= tensor(6.0843e+09, device='cuda:0')
c= tensor(6.0845e+09, device='cuda:0')
c= tensor(6.0900e+09, device='cuda:0')
c= tensor(6.0907e+09, device='cuda:0')
c= tensor(6.0908e+09, device='cuda:0')
c= tensor(6.0912e+09, device='cuda:0')
c= tensor(6.0954e+09, device='cuda:0')
c= tensor(6.0955e+09, device='cuda:0')
c= tensor(6.0965e+09, device='cuda:0')
c= tensor(6.0988e+09, device='cuda:0')
memory (bytes)
5119164416
time for making loss 2 is 16.8134822845459
p0 True
it  0 : 2291652608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 62% |
shape of L is 
torch.Size([])
memory (bytes)
5119373312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5119893504
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  77377560000.0
relative error loss 12.687251
shape of L is 
torch.Size([])
memory (bytes)
5251805184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  4% | 13% |
memory (bytes)
5251878912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  77377230000.0
relative error loss 12.687198
shape of L is 
torch.Size([])
memory (bytes)
5253869568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
5253894144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  77375740000.0
relative error loss 12.686953
shape of L is 
torch.Size([])
memory (bytes)
5255917568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
memory (bytes)
5255925760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% | 13% |
error is  77367980000.0
relative error loss 12.685679
shape of L is 
torch.Size([])
memory (bytes)
5258035200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 13% |
memory (bytes)
5258059776
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  77318120000.0
relative error loss 12.677505
shape of L is 
torch.Size([])
memory (bytes)
5260169216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 13% |
memory (bytes)
5260193792
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  77050660000.0
relative error loss 12.633651
shape of L is 
torch.Size([])
memory (bytes)
5262295040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 13% |
memory (bytes)
5262319616
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  74161050000.0
relative error loss 12.159855
shape of L is 
torch.Size([])
memory (bytes)
5264457728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
memory (bytes)
5264457728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  61039460000.0
relative error loss 10.008367
shape of L is 
torch.Size([])
memory (bytes)
5266591744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 13% |
memory (bytes)
5266616320
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  18100486000.0
relative error loss 2.9678555
shape of L is 
torch.Size([])
memory (bytes)
5268733952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
5268758528
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  10398386000.0
relative error loss 1.7049767
time to take a step is 199.2384524345398
it  1 : 2688059904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
shape of L is 
torch.Size([])
memory (bytes)
5270880256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
5270904832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% | 13% |
error is  10398386000.0
relative error loss 1.7049767
shape of L is 
torch.Size([])
memory (bytes)
5272887296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 13% |
memory (bytes)
5272887296
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  9682423000.0
relative error loss 1.5875834
shape of L is 
torch.Size([])
memory (bytes)
5275156480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 13% |
memory (bytes)
5275181056
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  8034664400.0
relative error loss 1.3174078
shape of L is 
torch.Size([])
memory (bytes)
5277315072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 13% |
memory (bytes)
5277335552
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  6525127000.0
relative error loss 1.0698957
shape of L is 
torch.Size([])
memory (bytes)
5279461376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 13% |
memory (bytes)
5279485952
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  6190984700.0
relative error loss 1.015108
shape of L is 
torch.Size([])
memory (bytes)
5281599488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
5281624064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  5792157700.0
relative error loss 0.94971406
shape of L is 
torch.Size([])
memory (bytes)
5283721216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 13% |
memory (bytes)
5283745792
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  7732314000.0
relative error loss 1.2678328
shape of L is 
torch.Size([])
memory (bytes)
5285842944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
5285859328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  5673538600.0
relative error loss 0.93026465
shape of L is 
torch.Size([])
memory (bytes)
5287919616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
5287944192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  5253820000.0
relative error loss 0.86144525
shape of L is 
torch.Size([])
memory (bytes)
5290049536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  1% | 13% |
memory (bytes)
5290049536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  4710202400.0
relative error loss 0.77231073
time to take a step is 193.4950761795044
it  2 : 2801319424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
shape of L is 
torch.Size([])
memory (bytes)
5292093440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
5292093440
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  4710202400.0
relative error loss 0.77231073
shape of L is 
torch.Size([])
memory (bytes)
5294268416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 13% |
memory (bytes)
5294292992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% | 13% |
error is  4426978300.0
relative error loss 0.72587174
shape of L is 
torch.Size([])
memory (bytes)
5296218112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 13% |
memory (bytes)
5296218112
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  3946356000.0
relative error loss 0.64706624
shape of L is 
torch.Size([])
memory (bytes)
5298503680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
memory (bytes)
5298528256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  3712649500.0
relative error loss 0.60874647
shape of L is 
torch.Size([])
memory (bytes)
5300637696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
5300637696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  4015231000.0
relative error loss 0.6583594
shape of L is 
torch.Size([])
memory (bytes)
5302730752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
5302755328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 13% |
error is  3453287200.0
relative error loss 0.56622
shape of L is 
torch.Size([])
memory (bytes)
5304860672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
5304885248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  3166200300.0
relative error loss 0.51914763
shape of L is 
torch.Size([])
memory (bytes)
5307023360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 13% |
memory (bytes)
5307023360
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  2855645000.0
relative error loss 0.46822727
shape of L is 
torch.Size([])
memory (bytes)
5309145088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
memory (bytes)
5309145088
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  2563257000.0
relative error loss 0.42028576
shape of L is 
torch.Size([])
memory (bytes)
5311291392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
memory (bytes)
5311315968
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  2342904600.0
relative error loss 0.38415554
time to take a step is 196.79686570167542
it  3 : 2801319424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 13% |
shape of L is 
torch.Size([])
memory (bytes)
5313445888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 13% |
memory (bytes)
5313470464
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  2342904600.0
relative error loss 0.38415554
shape of L is 
torch.Size([])
memory (bytes)
5315624960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
5315624960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  2200841000.0
relative error loss 0.36086202
shape of L is 
torch.Size([])
memory (bytes)
5317742592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 13% |
memory (bytes)
5317767168
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  2109304300.0
relative error loss 0.34585315
shape of L is 
torch.Size([])
memory (bytes)
5319868416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 13% |
memory (bytes)
5319892992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 13% |
error is  1997907700.0
relative error loss 0.32758796
shape of L is 
torch.Size([])
memory (bytes)
5322031104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 13% |
memory (bytes)
5322031104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1916030700.0
relative error loss 0.31416294
shape of L is 
torch.Size([])
memory (bytes)
5324099584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
5324099584
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1750330400.0
relative error loss 0.2869938
shape of L is 
torch.Size([])
memory (bytes)
5326286848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
5326315520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1607142400.0
relative error loss 0.26351592
shape of L is 
torch.Size([])
memory (bytes)
5328416768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
5328416768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1447882200.0
relative error loss 0.23740275
shape of L is 
torch.Size([])
memory (bytes)
5330272256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 13% |
memory (bytes)
5330501632
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1474039300.0
relative error loss 0.2416916
shape of L is 
torch.Size([])
memory (bytes)
5332705280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 13% |
memory (bytes)
5332729856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1376283600.0
relative error loss 0.22566305
time to take a step is 200.51057147979736
c= tensor(5046.5825, device='cuda:0')
c= tensor(169335.7188, device='cuda:0')
c= tensor(174001.5000, device='cuda:0')
c= tensor(385683., device='cuda:0')
c= tensor(475477.8125, device='cuda:0')
c= tensor(1520373.5000, device='cuda:0')
c= tensor(2104994.5000, device='cuda:0')
c= tensor(2436675., device='cuda:0')
c= tensor(2543601.7500, device='cuda:0')
c= tensor(30445250., device='cuda:0')
c= tensor(30488960., device='cuda:0')
c= tensor(34104004., device='cuda:0')
c= tensor(34129804., device='cuda:0')
c= tensor(74129888., device='cuda:0')
c= tensor(74386952., device='cuda:0')
c= tensor(75262440., device='cuda:0')
c= tensor(76355704., device='cuda:0')
c= tensor(76622624., device='cuda:0')
c= tensor(82731264., device='cuda:0')
c= tensor(86028328., device='cuda:0')
c= tensor(89183120., device='cuda:0')
c= tensor(1.0057e+08, device='cuda:0')
c= tensor(1.0061e+08, device='cuda:0')
c= tensor(1.0176e+08, device='cuda:0')
c= tensor(1.0177e+08, device='cuda:0')
c= tensor(1.0271e+08, device='cuda:0')
c= tensor(1.0434e+08, device='cuda:0')
c= tensor(1.0438e+08, device='cuda:0')
c= tensor(1.0751e+08, device='cuda:0')
c= tensor(3.9800e+08, device='cuda:0')
c= tensor(3.9814e+08, device='cuda:0')
c= tensor(5.3263e+08, device='cuda:0')
c= tensor(5.3303e+08, device='cuda:0')
c= tensor(5.3310e+08, device='cuda:0')
c= tensor(5.3367e+08, device='cuda:0')
c= tensor(5.4772e+08, device='cuda:0')
c= tensor(5.4994e+08, device='cuda:0')
c= tensor(5.4994e+08, device='cuda:0')
c= tensor(5.4995e+08, device='cuda:0')
c= tensor(5.4995e+08, device='cuda:0')
c= tensor(5.4996e+08, device='cuda:0')
c= tensor(5.4997e+08, device='cuda:0')
c= tensor(5.4997e+08, device='cuda:0')
c= tensor(5.4998e+08, device='cuda:0')
c= tensor(5.4998e+08, device='cuda:0')
c= tensor(5.4998e+08, device='cuda:0')
c= tensor(5.4998e+08, device='cuda:0')
c= tensor(5.5000e+08, device='cuda:0')
c= tensor(5.5001e+08, device='cuda:0')
c= tensor(5.5005e+08, device='cuda:0')
c= tensor(5.5010e+08, device='cuda:0')
c= tensor(5.5010e+08, device='cuda:0')
c= tensor(5.5013e+08, device='cuda:0')
c= tensor(5.5015e+08, device='cuda:0')
c= tensor(5.5015e+08, device='cuda:0')
c= tensor(5.5019e+08, device='cuda:0')
c= tensor(5.5019e+08, device='cuda:0')
c= tensor(5.5021e+08, device='cuda:0')
c= tensor(5.5023e+08, device='cuda:0')
c= tensor(5.5024e+08, device='cuda:0')
c= tensor(5.5025e+08, device='cuda:0')
c= tensor(5.5025e+08, device='cuda:0')
c= tensor(5.5026e+08, device='cuda:0')
c= tensor(5.5031e+08, device='cuda:0')
c= tensor(5.5031e+08, device='cuda:0')
c= tensor(5.5032e+08, device='cuda:0')
c= tensor(5.5032e+08, device='cuda:0')
c= tensor(5.5034e+08, device='cuda:0')
c= tensor(5.5037e+08, device='cuda:0')
c= tensor(5.5038e+08, device='cuda:0')
c= tensor(5.5040e+08, device='cuda:0')
c= tensor(5.5041e+08, device='cuda:0')
c= tensor(5.5041e+08, device='cuda:0')
c= tensor(5.5042e+08, device='cuda:0')
c= tensor(5.5044e+08, device='cuda:0')
c= tensor(5.5045e+08, device='cuda:0')
c= tensor(5.5045e+08, device='cuda:0')
c= tensor(5.5046e+08, device='cuda:0')
c= tensor(5.5047e+08, device='cuda:0')
c= tensor(5.5054e+08, device='cuda:0')
c= tensor(5.5054e+08, device='cuda:0')
c= tensor(5.5055e+08, device='cuda:0')
c= tensor(5.5056e+08, device='cuda:0')
c= tensor(5.5056e+08, device='cuda:0')
c= tensor(5.5056e+08, device='cuda:0')
c= tensor(5.5056e+08, device='cuda:0')
c= tensor(5.5057e+08, device='cuda:0')
c= tensor(5.5057e+08, device='cuda:0')
c= tensor(5.5058e+08, device='cuda:0')
c= tensor(5.5059e+08, device='cuda:0')
c= tensor(5.5061e+08, device='cuda:0')
c= tensor(5.5061e+08, device='cuda:0')
c= tensor(5.5061e+08, device='cuda:0')
c= tensor(5.5062e+08, device='cuda:0')
c= tensor(5.5064e+08, device='cuda:0')
c= tensor(5.5066e+08, device='cuda:0')
c= tensor(5.5066e+08, device='cuda:0')
c= tensor(5.5069e+08, device='cuda:0')
c= tensor(5.5071e+08, device='cuda:0')
c= tensor(5.5072e+08, device='cuda:0')
c= tensor(5.5075e+08, device='cuda:0')
c= tensor(5.5076e+08, device='cuda:0')
c= tensor(5.5077e+08, device='cuda:0')
c= tensor(5.5077e+08, device='cuda:0')
c= tensor(5.5081e+08, device='cuda:0')
c= tensor(5.5081e+08, device='cuda:0')
c= tensor(5.5082e+08, device='cuda:0')
c= tensor(5.5082e+08, device='cuda:0')
c= tensor(5.5083e+08, device='cuda:0')
c= tensor(5.5083e+08, device='cuda:0')
c= tensor(5.5084e+08, device='cuda:0')
c= tensor(5.5084e+08, device='cuda:0')
c= tensor(5.5085e+08, device='cuda:0')
c= tensor(5.5085e+08, device='cuda:0')
c= tensor(5.5086e+08, device='cuda:0')
c= tensor(5.5087e+08, device='cuda:0')
c= tensor(5.5088e+08, device='cuda:0')
c= tensor(5.5088e+08, device='cuda:0')
c= tensor(5.5091e+08, device='cuda:0')
c= tensor(5.5091e+08, device='cuda:0')
c= tensor(5.5094e+08, device='cuda:0')
c= tensor(5.5094e+08, device='cuda:0')
c= tensor(5.5094e+08, device='cuda:0')
c= tensor(5.5095e+08, device='cuda:0')
c= tensor(5.5096e+08, device='cuda:0')
c= tensor(5.5096e+08, device='cuda:0')
c= tensor(5.5097e+08, device='cuda:0')
c= tensor(5.5098e+08, device='cuda:0')
c= tensor(5.5099e+08, device='cuda:0')
c= tensor(5.5100e+08, device='cuda:0')
c= tensor(5.5108e+08, device='cuda:0')
c= tensor(5.5109e+08, device='cuda:0')
c= tensor(5.5110e+08, device='cuda:0')
c= tensor(5.5110e+08, device='cuda:0')
c= tensor(5.5111e+08, device='cuda:0')
c= tensor(5.5111e+08, device='cuda:0')
c= tensor(5.5111e+08, device='cuda:0')
c= tensor(5.5112e+08, device='cuda:0')
c= tensor(5.5112e+08, device='cuda:0')
c= tensor(5.5113e+08, device='cuda:0')
c= tensor(5.5113e+08, device='cuda:0')
c= tensor(5.5113e+08, device='cuda:0')
c= tensor(5.5119e+08, device='cuda:0')
c= tensor(5.5120e+08, device='cuda:0')
c= tensor(5.5121e+08, device='cuda:0')
c= tensor(5.5121e+08, device='cuda:0')
c= tensor(5.5122e+08, device='cuda:0')
c= tensor(5.5122e+08, device='cuda:0')
c= tensor(5.5122e+08, device='cuda:0')
c= tensor(5.5123e+08, device='cuda:0')
c= tensor(5.5124e+08, device='cuda:0')
c= tensor(5.5124e+08, device='cuda:0')
c= tensor(5.5125e+08, device='cuda:0')
c= tensor(5.5128e+08, device='cuda:0')
c= tensor(5.5129e+08, device='cuda:0')
c= tensor(5.5142e+08, device='cuda:0')
c= tensor(5.5143e+08, device='cuda:0')
c= tensor(5.5144e+08, device='cuda:0')
c= tensor(5.5144e+08, device='cuda:0')
c= tensor(5.5144e+08, device='cuda:0')
c= tensor(5.5150e+08, device='cuda:0')
c= tensor(5.5150e+08, device='cuda:0')
c= tensor(5.5152e+08, device='cuda:0')
c= tensor(5.5152e+08, device='cuda:0')
c= tensor(5.5154e+08, device='cuda:0')
c= tensor(5.5154e+08, device='cuda:0')
c= tensor(5.5155e+08, device='cuda:0')
c= tensor(5.5156e+08, device='cuda:0')
c= tensor(5.5157e+08, device='cuda:0')
c= tensor(5.5157e+08, device='cuda:0')
c= tensor(5.5157e+08, device='cuda:0')
c= tensor(5.5157e+08, device='cuda:0')
c= tensor(5.5159e+08, device='cuda:0')
c= tensor(5.5159e+08, device='cuda:0')
c= tensor(5.5160e+08, device='cuda:0')
c= tensor(5.5176e+08, device='cuda:0')
c= tensor(5.5178e+08, device='cuda:0')
c= tensor(5.5178e+08, device='cuda:0')
c= tensor(5.5180e+08, device='cuda:0')
c= tensor(5.5180e+08, device='cuda:0')
c= tensor(5.5182e+08, device='cuda:0')
c= tensor(5.5182e+08, device='cuda:0')
c= tensor(5.5184e+08, device='cuda:0')
c= tensor(5.5184e+08, device='cuda:0')
c= tensor(5.5184e+08, device='cuda:0')
c= tensor(5.5185e+08, device='cuda:0')
c= tensor(5.5186e+08, device='cuda:0')
c= tensor(5.5187e+08, device='cuda:0')
c= tensor(5.5188e+08, device='cuda:0')
c= tensor(5.5189e+08, device='cuda:0')
c= tensor(5.5190e+08, device='cuda:0')
c= tensor(5.5190e+08, device='cuda:0')
c= tensor(5.5191e+08, device='cuda:0')
c= tensor(5.5193e+08, device='cuda:0')
c= tensor(5.5198e+08, device='cuda:0')
c= tensor(5.5198e+08, device='cuda:0')
c= tensor(5.5199e+08, device='cuda:0')
c= tensor(5.5199e+08, device='cuda:0')
c= tensor(5.5200e+08, device='cuda:0')
c= tensor(5.5200e+08, device='cuda:0')
c= tensor(5.5201e+08, device='cuda:0')
c= tensor(5.5202e+08, device='cuda:0')
c= tensor(5.5204e+08, device='cuda:0')
c= tensor(5.5204e+08, device='cuda:0')
c= tensor(5.5205e+08, device='cuda:0')
c= tensor(5.5206e+08, device='cuda:0')
c= tensor(5.5206e+08, device='cuda:0')
c= tensor(5.5207e+08, device='cuda:0')
c= tensor(5.5207e+08, device='cuda:0')
c= tensor(5.5209e+08, device='cuda:0')
c= tensor(5.5213e+08, device='cuda:0')
c= tensor(5.5215e+08, device='cuda:0')
c= tensor(5.5217e+08, device='cuda:0')
c= tensor(5.5220e+08, device='cuda:0')
c= tensor(5.5220e+08, device='cuda:0')
c= tensor(5.5221e+08, device='cuda:0')
c= tensor(5.5221e+08, device='cuda:0')
c= tensor(5.5221e+08, device='cuda:0')
c= tensor(5.5225e+08, device='cuda:0')
c= tensor(5.5227e+08, device='cuda:0')
c= tensor(5.5227e+08, device='cuda:0')
c= tensor(5.5228e+08, device='cuda:0')
c= tensor(5.5229e+08, device='cuda:0')
c= tensor(5.5230e+08, device='cuda:0')
c= tensor(5.5230e+08, device='cuda:0')
c= tensor(5.5231e+08, device='cuda:0')
c= tensor(5.5232e+08, device='cuda:0')
c= tensor(5.5232e+08, device='cuda:0')
c= tensor(5.5233e+08, device='cuda:0')
c= tensor(5.5234e+08, device='cuda:0')
c= tensor(5.5235e+08, device='cuda:0')
c= tensor(5.5235e+08, device='cuda:0')
c= tensor(5.5236e+08, device='cuda:0')
c= tensor(5.5236e+08, device='cuda:0')
c= tensor(5.5237e+08, device='cuda:0')
c= tensor(5.5238e+08, device='cuda:0')
c= tensor(5.5239e+08, device='cuda:0')
c= tensor(5.5240e+08, device='cuda:0')
c= tensor(5.5241e+08, device='cuda:0')
c= tensor(5.5270e+08, device='cuda:0')
c= tensor(5.5423e+08, device='cuda:0')
c= tensor(5.5427e+08, device='cuda:0')
c= tensor(5.5432e+08, device='cuda:0')
c= tensor(5.5432e+08, device='cuda:0')
c= tensor(5.5434e+08, device='cuda:0')
c= tensor(5.5441e+08, device='cuda:0')
c= tensor(5.6833e+08, device='cuda:0')
c= tensor(5.6833e+08, device='cuda:0')
c= tensor(5.8027e+08, device='cuda:0')
c= tensor(5.8298e+08, device='cuda:0')
c= tensor(5.8545e+08, device='cuda:0')
c= tensor(5.8631e+08, device='cuda:0')
c= tensor(5.8632e+08, device='cuda:0')
c= tensor(5.8633e+08, device='cuda:0')
c= tensor(5.9870e+08, device='cuda:0')
c= tensor(6.5170e+08, device='cuda:0')
c= tensor(6.5171e+08, device='cuda:0')
c= tensor(6.5234e+08, device='cuda:0')
c= tensor(6.5291e+08, device='cuda:0')
c= tensor(6.5301e+08, device='cuda:0')
c= tensor(6.6059e+08, device='cuda:0')
c= tensor(6.6230e+08, device='cuda:0')
c= tensor(6.6271e+08, device='cuda:0')
c= tensor(6.6285e+08, device='cuda:0')
c= tensor(6.6286e+08, device='cuda:0')
c= tensor(6.7406e+08, device='cuda:0')
c= tensor(6.7433e+08, device='cuda:0')
c= tensor(6.7434e+08, device='cuda:0')
c= tensor(6.7453e+08, device='cuda:0')
c= tensor(6.7878e+08, device='cuda:0')
c= tensor(6.8886e+08, device='cuda:0')
c= tensor(6.9074e+08, device='cuda:0')
c= tensor(6.9077e+08, device='cuda:0')
c= tensor(6.9139e+08, device='cuda:0')
c= tensor(6.9141e+08, device='cuda:0')
c= tensor(6.9238e+08, device='cuda:0')
c= tensor(6.9415e+08, device='cuda:0')
c= tensor(6.9472e+08, device='cuda:0')
c= tensor(6.9741e+08, device='cuda:0')
c= tensor(6.9741e+08, device='cuda:0')
c= tensor(6.9746e+08, device='cuda:0')
c= tensor(7.0079e+08, device='cuda:0')
c= tensor(7.0195e+08, device='cuda:0')
c= tensor(7.0559e+08, device='cuda:0')
c= tensor(7.0560e+08, device='cuda:0')
c= tensor(7.3232e+08, device='cuda:0')
c= tensor(7.3242e+08, device='cuda:0')
c= tensor(7.3271e+08, device='cuda:0')
c= tensor(7.3450e+08, device='cuda:0')
c= tensor(7.3450e+08, device='cuda:0')
c= tensor(7.4000e+08, device='cuda:0')
c= tensor(7.5829e+08, device='cuda:0')
c= tensor(8.2447e+08, device='cuda:0')
c= tensor(8.2693e+08, device='cuda:0')
c= tensor(8.2753e+08, device='cuda:0')
c= tensor(8.2764e+08, device='cuda:0')
c= tensor(8.2766e+08, device='cuda:0')
c= tensor(8.3784e+08, device='cuda:0')
c= tensor(8.3787e+08, device='cuda:0')
c= tensor(8.3874e+08, device='cuda:0')
c= tensor(8.4561e+08, device='cuda:0')
c= tensor(8.4625e+08, device='cuda:0')
c= tensor(8.4652e+08, device='cuda:0')
c= tensor(8.4653e+08, device='cuda:0')
c= tensor(8.5286e+08, device='cuda:0')
c= tensor(8.5753e+08, device='cuda:0')
c= tensor(8.5858e+08, device='cuda:0')
c= tensor(8.5859e+08, device='cuda:0')
c= tensor(8.6166e+08, device='cuda:0')
c= tensor(8.6211e+08, device='cuda:0')
c= tensor(8.7676e+08, device='cuda:0')
c= tensor(8.7683e+08, device='cuda:0')
c= tensor(8.7999e+08, device='cuda:0')
c= tensor(8.8021e+08, device='cuda:0')
c= tensor(8.8972e+08, device='cuda:0')
c= tensor(8.9070e+08, device='cuda:0')
c= tensor(8.9092e+08, device='cuda:0')
c= tensor(8.9532e+08, device='cuda:0')
c= tensor(9.0070e+08, device='cuda:0')
c= tensor(9.0073e+08, device='cuda:0')
c= tensor(9.1680e+08, device='cuda:0')
c= tensor(9.2645e+08, device='cuda:0')
c= tensor(9.5970e+08, device='cuda:0')
c= tensor(9.6009e+08, device='cuda:0')
c= tensor(9.6009e+08, device='cuda:0')
c= tensor(9.6015e+08, device='cuda:0')
c= tensor(9.6118e+08, device='cuda:0')
c= tensor(9.6140e+08, device='cuda:0')
c= tensor(9.6493e+08, device='cuda:0')
c= tensor(9.6493e+08, device='cuda:0')
c= tensor(9.6654e+08, device='cuda:0')
c= tensor(9.7002e+08, device='cuda:0')
c= tensor(9.7047e+08, device='cuda:0')
c= tensor(9.7053e+08, device='cuda:0')
c= tensor(9.7310e+08, device='cuda:0')
c= tensor(9.7316e+08, device='cuda:0')
c= tensor(9.7320e+08, device='cuda:0')
c= tensor(9.7328e+08, device='cuda:0')
c= tensor(9.7329e+08, device='cuda:0')
c= tensor(9.7510e+08, device='cuda:0')
c= tensor(9.7568e+08, device='cuda:0')
c= tensor(9.7591e+08, device='cuda:0')
c= tensor(9.7723e+08, device='cuda:0')
c= tensor(9.7730e+08, device='cuda:0')
c= tensor(1.1548e+09, device='cuda:0')
c= tensor(1.1550e+09, device='cuda:0')
c= tensor(1.1620e+09, device='cuda:0')
c= tensor(1.1620e+09, device='cuda:0')
c= tensor(1.1620e+09, device='cuda:0')
c= tensor(1.1620e+09, device='cuda:0')
c= tensor(1.1622e+09, device='cuda:0')
c= tensor(1.1622e+09, device='cuda:0')
c= tensor(1.1681e+09, device='cuda:0')
c= tensor(1.1681e+09, device='cuda:0')
c= tensor(1.1681e+09, device='cuda:0')
c= tensor(1.1792e+09, device='cuda:0')
c= tensor(1.1805e+09, device='cuda:0')
c= tensor(1.1810e+09, device='cuda:0')
c= tensor(1.1848e+09, device='cuda:0')
c= tensor(1.2053e+09, device='cuda:0')
c= tensor(1.2053e+09, device='cuda:0')
c= tensor(1.2054e+09, device='cuda:0')
c= tensor(1.2056e+09, device='cuda:0')
c= tensor(1.2056e+09, device='cuda:0')
c= tensor(1.2056e+09, device='cuda:0')
c= tensor(1.2057e+09, device='cuda:0')
c= tensor(1.2057e+09, device='cuda:0')
c= tensor(1.2058e+09, device='cuda:0')
c= tensor(1.2059e+09, device='cuda:0')
c= tensor(1.2060e+09, device='cuda:0')
c= tensor(1.2331e+09, device='cuda:0')
c= tensor(1.2332e+09, device='cuda:0')
c= tensor(1.2349e+09, device='cuda:0')
c= tensor(1.2350e+09, device='cuda:0')
c= tensor(1.2350e+09, device='cuda:0')
c= tensor(1.2409e+09, device='cuda:0')
c= tensor(1.3257e+09, device='cuda:0')
c= tensor(1.3655e+09, device='cuda:0')
c= tensor(1.3657e+09, device='cuda:0')
c= tensor(1.3663e+09, device='cuda:0')
c= tensor(1.3663e+09, device='cuda:0')
c= tensor(1.3663e+09, device='cuda:0')
c= tensor(1.4932e+09, device='cuda:0')
c= tensor(1.4934e+09, device='cuda:0')
c= tensor(1.4934e+09, device='cuda:0')
c= tensor(1.4946e+09, device='cuda:0')
c= tensor(1.5930e+09, device='cuda:0')
c= tensor(1.5939e+09, device='cuda:0')
c= tensor(1.5941e+09, device='cuda:0')
c= tensor(1.5941e+09, device='cuda:0')
c= tensor(1.5942e+09, device='cuda:0')
c= tensor(1.5943e+09, device='cuda:0')
c= tensor(1.6033e+09, device='cuda:0')
c= tensor(1.6034e+09, device='cuda:0')
c= tensor(1.6034e+09, device='cuda:0')
c= tensor(1.6062e+09, device='cuda:0')
c= tensor(1.6063e+09, device='cuda:0')
c= tensor(1.6063e+09, device='cuda:0')
c= tensor(1.6074e+09, device='cuda:0')
c= tensor(1.6098e+09, device='cuda:0')
c= tensor(1.6292e+09, device='cuda:0')
c= tensor(1.6406e+09, device='cuda:0')
c= tensor(1.6510e+09, device='cuda:0')
c= tensor(1.6513e+09, device='cuda:0')
c= tensor(1.6517e+09, device='cuda:0')
c= tensor(1.6577e+09, device='cuda:0')
c= tensor(1.6650e+09, device='cuda:0')
c= tensor(1.6651e+09, device='cuda:0')
c= tensor(1.7250e+09, device='cuda:0')
c= tensor(1.8055e+09, device='cuda:0')
c= tensor(1.8188e+09, device='cuda:0')
c= tensor(1.8228e+09, device='cuda:0')
c= tensor(1.8250e+09, device='cuda:0')
c= tensor(1.8252e+09, device='cuda:0')
c= tensor(1.8252e+09, device='cuda:0')
c= tensor(1.8253e+09, device='cuda:0')
c= tensor(1.8279e+09, device='cuda:0')
c= tensor(1.8327e+09, device='cuda:0')
c= tensor(1.8577e+09, device='cuda:0')
c= tensor(1.8615e+09, device='cuda:0')
c= tensor(1.8634e+09, device='cuda:0')
c= tensor(1.8636e+09, device='cuda:0')
c= tensor(1.8695e+09, device='cuda:0')
c= tensor(1.8695e+09, device='cuda:0')
c= tensor(1.8696e+09, device='cuda:0')
c= tensor(1.8733e+09, device='cuda:0')
c= tensor(1.8748e+09, device='cuda:0')
c= tensor(1.8749e+09, device='cuda:0')
c= tensor(1.8751e+09, device='cuda:0')
c= tensor(2.0250e+09, device='cuda:0')
c= tensor(2.0251e+09, device='cuda:0')
c= tensor(2.0304e+09, device='cuda:0')
c= tensor(2.0304e+09, device='cuda:0')
c= tensor(2.0305e+09, device='cuda:0')
c= tensor(2.0305e+09, device='cuda:0')
c= tensor(2.0305e+09, device='cuda:0')
c= tensor(2.0307e+09, device='cuda:0')
c= tensor(2.0318e+09, device='cuda:0')
c= tensor(2.0318e+09, device='cuda:0')
c= tensor(2.0366e+09, device='cuda:0')
c= tensor(2.0366e+09, device='cuda:0')
c= tensor(2.0379e+09, device='cuda:0')
c= tensor(2.0380e+09, device='cuda:0')
c= tensor(2.0444e+09, device='cuda:0')
c= tensor(2.0444e+09, device='cuda:0')
c= tensor(2.0449e+09, device='cuda:0')
c= tensor(2.0452e+09, device='cuda:0')
c= tensor(2.0456e+09, device='cuda:0')
c= tensor(2.0470e+09, device='cuda:0')
c= tensor(2.1586e+09, device='cuda:0')
c= tensor(2.1587e+09, device='cuda:0')
c= tensor(2.1587e+09, device='cuda:0')
c= tensor(2.1595e+09, device='cuda:0')
c= tensor(2.1597e+09, device='cuda:0')
c= tensor(2.2040e+09, device='cuda:0')
c= tensor(2.2041e+09, device='cuda:0')
c= tensor(2.2077e+09, device='cuda:0')
c= tensor(2.2237e+09, device='cuda:0')
c= tensor(2.2237e+09, device='cuda:0')
c= tensor(2.2423e+09, device='cuda:0')
c= tensor(2.2427e+09, device='cuda:0')
c= tensor(2.3055e+09, device='cuda:0')
c= tensor(2.3056e+09, device='cuda:0')
c= tensor(2.3057e+09, device='cuda:0')
c= tensor(2.3059e+09, device='cuda:0')
c= tensor(2.3059e+09, device='cuda:0')
c= tensor(2.3059e+09, device='cuda:0')
c= tensor(2.3094e+09, device='cuda:0')
c= tensor(2.3094e+09, device='cuda:0')
c= tensor(2.3152e+09, device='cuda:0')
c= tensor(2.3152e+09, device='cuda:0')
c= tensor(2.3153e+09, device='cuda:0')
c= tensor(2.3154e+09, device='cuda:0')
c= tensor(2.3236e+09, device='cuda:0')
c= tensor(2.3333e+09, device='cuda:0')
c= tensor(2.3520e+09, device='cuda:0')
c= tensor(2.3522e+09, device='cuda:0')
c= tensor(2.3523e+09, device='cuda:0')
c= tensor(2.3523e+09, device='cuda:0')
c= tensor(2.3523e+09, device='cuda:0')
c= tensor(2.3737e+09, device='cuda:0')
c= tensor(2.3737e+09, device='cuda:0')
c= tensor(2.3739e+09, device='cuda:0')
c= tensor(2.3756e+09, device='cuda:0')
c= tensor(2.3762e+09, device='cuda:0')
c= tensor(2.3771e+09, device='cuda:0')
c= tensor(2.3772e+09, device='cuda:0')
c= tensor(2.3919e+09, device='cuda:0')
c= tensor(2.3933e+09, device='cuda:0')
c= tensor(2.3933e+09, device='cuda:0')
c= tensor(2.3935e+09, device='cuda:0')
c= tensor(2.3947e+09, device='cuda:0')
c= tensor(2.3969e+09, device='cuda:0')
c= tensor(2.4636e+09, device='cuda:0')
c= tensor(2.4669e+09, device='cuda:0')
c= tensor(2.4670e+09, device='cuda:0')
c= tensor(2.4674e+09, device='cuda:0')
c= tensor(2.4681e+09, device='cuda:0')
c= tensor(2.4681e+09, device='cuda:0')
c= tensor(2.4682e+09, device='cuda:0')
c= tensor(2.4682e+09, device='cuda:0')
c= tensor(2.4690e+09, device='cuda:0')
c= tensor(2.4703e+09, device='cuda:0')
c= tensor(2.4703e+09, device='cuda:0')
c= tensor(2.4704e+09, device='cuda:0')
c= tensor(2.4705e+09, device='cuda:0')
c= tensor(2.4709e+09, device='cuda:0')
c= tensor(2.4709e+09, device='cuda:0')
c= tensor(2.4711e+09, device='cuda:0')
c= tensor(2.4714e+09, device='cuda:0')
c= tensor(2.4715e+09, device='cuda:0')
c= tensor(2.4715e+09, device='cuda:0')
c= tensor(2.4715e+09, device='cuda:0')
c= tensor(2.4717e+09, device='cuda:0')
c= tensor(2.4804e+09, device='cuda:0')
c= tensor(2.4805e+09, device='cuda:0')
c= tensor(2.4805e+09, device='cuda:0')
c= tensor(2.4805e+09, device='cuda:0')
c= tensor(2.4856e+09, device='cuda:0')
c= tensor(2.5294e+09, device='cuda:0')
c= tensor(2.5336e+09, device='cuda:0')
c= tensor(2.5336e+09, device='cuda:0')
c= tensor(2.5582e+09, device='cuda:0')
c= tensor(2.5636e+09, device='cuda:0')
c= tensor(2.5636e+09, device='cuda:0')
c= tensor(2.5637e+09, device='cuda:0')
c= tensor(2.5643e+09, device='cuda:0')
c= tensor(2.5738e+09, device='cuda:0')
c= tensor(2.6216e+09, device='cuda:0')
c= tensor(2.6285e+09, device='cuda:0')
c= tensor(2.6291e+09, device='cuda:0')
c= tensor(2.6292e+09, device='cuda:0')
c= tensor(2.6292e+09, device='cuda:0')
c= tensor(2.6308e+09, device='cuda:0')
c= tensor(2.6309e+09, device='cuda:0')
c= tensor(2.6314e+09, device='cuda:0')
c= tensor(2.6350e+09, device='cuda:0')
c= tensor(2.6760e+09, device='cuda:0')
c= tensor(2.6761e+09, device='cuda:0')
c= tensor(2.6761e+09, device='cuda:0')
c= tensor(2.6764e+09, device='cuda:0')
c= tensor(2.6900e+09, device='cuda:0')
c= tensor(2.6906e+09, device='cuda:0')
c= tensor(2.6907e+09, device='cuda:0')
c= tensor(2.6907e+09, device='cuda:0')
c= tensor(2.6910e+09, device='cuda:0')
c= tensor(2.6911e+09, device='cuda:0')
c= tensor(2.6924e+09, device='cuda:0')
c= tensor(2.6924e+09, device='cuda:0')
c= tensor(2.6925e+09, device='cuda:0')
c= tensor(2.6925e+09, device='cuda:0')
c= tensor(2.6926e+09, device='cuda:0')
c= tensor(2.6928e+09, device='cuda:0')
c= tensor(2.6991e+09, device='cuda:0')
c= tensor(2.7176e+09, device='cuda:0')
c= tensor(2.7205e+09, device='cuda:0')
c= tensor(2.7308e+09, device='cuda:0')
c= tensor(2.7309e+09, device='cuda:0')
c= tensor(2.7311e+09, device='cuda:0')
c= tensor(2.7312e+09, device='cuda:0')
c= tensor(2.7348e+09, device='cuda:0')
c= tensor(2.7351e+09, device='cuda:0')
c= tensor(2.7375e+09, device='cuda:0')
c= tensor(2.7375e+09, device='cuda:0')
c= tensor(2.9412e+09, device='cuda:0')
c= tensor(2.9413e+09, device='cuda:0')
c= tensor(2.9425e+09, device='cuda:0')
c= tensor(2.9545e+09, device='cuda:0')
c= tensor(2.9549e+09, device='cuda:0')
c= tensor(2.9554e+09, device='cuda:0')
c= tensor(3.2063e+09, device='cuda:0')
c= tensor(3.2181e+09, device='cuda:0')
c= tensor(3.2206e+09, device='cuda:0')
c= tensor(3.2207e+09, device='cuda:0')
c= tensor(3.2211e+09, device='cuda:0')
c= tensor(3.2212e+09, device='cuda:0')
c= tensor(3.2395e+09, device='cuda:0')
c= tensor(3.4246e+09, device='cuda:0')
c= tensor(3.4290e+09, device='cuda:0')
c= tensor(3.4349e+09, device='cuda:0')
c= tensor(3.4394e+09, device='cuda:0')
c= tensor(3.4399e+09, device='cuda:0')
c= tensor(3.4404e+09, device='cuda:0')
c= tensor(3.4405e+09, device='cuda:0')
c= tensor(3.4558e+09, device='cuda:0')
c= tensor(3.4866e+09, device='cuda:0')
c= tensor(3.4894e+09, device='cuda:0')
c= tensor(3.6745e+09, device='cuda:0')
c= tensor(3.6828e+09, device='cuda:0')
c= tensor(3.6837e+09, device='cuda:0')
c= tensor(3.6839e+09, device='cuda:0')
c= tensor(3.6861e+09, device='cuda:0')
c= tensor(3.6899e+09, device='cuda:0')
c= tensor(3.6900e+09, device='cuda:0')
c= tensor(3.7364e+09, device='cuda:0')
c= tensor(3.7374e+09, device='cuda:0')
c= tensor(3.7382e+09, device='cuda:0')
c= tensor(3.7382e+09, device='cuda:0')
c= tensor(3.7383e+09, device='cuda:0')
c= tensor(3.7383e+09, device='cuda:0')
c= tensor(3.7383e+09, device='cuda:0')
c= tensor(3.7385e+09, device='cuda:0')
c= tensor(3.7391e+09, device='cuda:0')
c= tensor(5.6012e+09, device='cuda:0')
c= tensor(5.6018e+09, device='cuda:0')
c= tensor(5.6196e+09, device='cuda:0')
c= tensor(5.6196e+09, device='cuda:0')
c= tensor(5.6197e+09, device='cuda:0')
c= tensor(5.6198e+09, device='cuda:0')
c= tensor(5.6358e+09, device='cuda:0')
c= tensor(5.6370e+09, device='cuda:0')
c= tensor(5.7826e+09, device='cuda:0')
c= tensor(5.7826e+09, device='cuda:0')
c= tensor(5.7887e+09, device='cuda:0')
c= tensor(5.7890e+09, device='cuda:0')
c= tensor(5.7945e+09, device='cuda:0')
c= tensor(5.8067e+09, device='cuda:0')
c= tensor(5.8070e+09, device='cuda:0')
c= tensor(5.8070e+09, device='cuda:0')
c= tensor(5.8086e+09, device='cuda:0')
c= tensor(5.8092e+09, device='cuda:0')
c= tensor(5.8101e+09, device='cuda:0')
c= tensor(5.8249e+09, device='cuda:0')
c= tensor(5.8251e+09, device='cuda:0')
c= tensor(5.8266e+09, device='cuda:0')
c= tensor(5.8268e+09, device='cuda:0')
c= tensor(5.8313e+09, device='cuda:0')
c= tensor(5.8574e+09, device='cuda:0')
c= tensor(5.8587e+09, device='cuda:0')
c= tensor(5.8588e+09, device='cuda:0')
c= tensor(5.8656e+09, device='cuda:0')
c= tensor(5.8680e+09, device='cuda:0')
c= tensor(5.9051e+09, device='cuda:0')
c= tensor(5.9062e+09, device='cuda:0')
c= tensor(5.9066e+09, device='cuda:0')
c= tensor(5.9082e+09, device='cuda:0')
c= tensor(5.9253e+09, device='cuda:0')
c= tensor(5.9315e+09, device='cuda:0')
c= tensor(5.9317e+09, device='cuda:0')
c= tensor(5.9317e+09, device='cuda:0')
c= tensor(5.9317e+09, device='cuda:0')
c= tensor(5.9347e+09, device='cuda:0')
c= tensor(5.9390e+09, device='cuda:0')
c= tensor(5.9401e+09, device='cuda:0')
c= tensor(5.9402e+09, device='cuda:0')
c= tensor(5.9404e+09, device='cuda:0')
c= tensor(5.9416e+09, device='cuda:0')
c= tensor(5.9425e+09, device='cuda:0')
c= tensor(5.9427e+09, device='cuda:0')
c= tensor(5.9442e+09, device='cuda:0')
c= tensor(5.9445e+09, device='cuda:0')
c= tensor(5.9450e+09, device='cuda:0')
c= tensor(5.9450e+09, device='cuda:0')
c= tensor(5.9450e+09, device='cuda:0')
c= tensor(5.9557e+09, device='cuda:0')
c= tensor(5.9559e+09, device='cuda:0')
c= tensor(5.9576e+09, device='cuda:0')
c= tensor(5.9576e+09, device='cuda:0')
c= tensor(5.9577e+09, device='cuda:0')
c= tensor(5.9582e+09, device='cuda:0')
c= tensor(5.9600e+09, device='cuda:0')
c= tensor(5.9604e+09, device='cuda:0')
c= tensor(5.9604e+09, device='cuda:0')
c= tensor(5.9605e+09, device='cuda:0')
c= tensor(5.9612e+09, device='cuda:0')
c= tensor(5.9615e+09, device='cuda:0')
c= tensor(5.9648e+09, device='cuda:0')
c= tensor(5.9649e+09, device='cuda:0')
c= tensor(5.9649e+09, device='cuda:0')
c= tensor(5.9670e+09, device='cuda:0')
c= tensor(5.9670e+09, device='cuda:0')
c= tensor(5.9753e+09, device='cuda:0')
c= tensor(5.9755e+09, device='cuda:0')
c= tensor(5.9783e+09, device='cuda:0')
c= tensor(5.9827e+09, device='cuda:0')
c= tensor(5.9853e+09, device='cuda:0')
c= tensor(5.9902e+09, device='cuda:0')
c= tensor(5.9927e+09, device='cuda:0')
c= tensor(5.9927e+09, device='cuda:0')
c= tensor(5.9935e+09, device='cuda:0')
c= tensor(5.9937e+09, device='cuda:0')
c= tensor(5.9959e+09, device='cuda:0')
c= tensor(5.9987e+09, device='cuda:0')
c= tensor(5.9996e+09, device='cuda:0')
c= tensor(6.0009e+09, device='cuda:0')
c= tensor(6.0015e+09, device='cuda:0')
c= tensor(6.0061e+09, device='cuda:0')
c= tensor(6.0061e+09, device='cuda:0')
c= tensor(6.0062e+09, device='cuda:0')
c= tensor(6.0228e+09, device='cuda:0')
c= tensor(6.0841e+09, device='cuda:0')
c= tensor(6.0842e+09, device='cuda:0')
c= tensor(6.0843e+09, device='cuda:0')
c= tensor(6.0845e+09, device='cuda:0')
c= tensor(6.0900e+09, device='cuda:0')
c= tensor(6.0907e+09, device='cuda:0')
c= tensor(6.0908e+09, device='cuda:0')
c= tensor(6.0912e+09, device='cuda:0')
c= tensor(6.0954e+09, device='cuda:0')
c= tensor(6.0955e+09, device='cuda:0')
c= tensor(6.0965e+09, device='cuda:0')
c= tensor(6.0988e+09, device='cuda:0')
time to make c is 10.254457235336304
time for making loss is 10.254496097564697
p0 True
it  0 : 2291895296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 13% |
shape of L is 
torch.Size([])
memory (bytes)
5334753280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 13% |
memory (bytes)
5335007232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1376283600.0
relative error loss 0.22566305
shape of L is 
torch.Size([])
memory (bytes)
5361590272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 13% |
memory (bytes)
5361676288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1370252300.0
relative error loss 0.22467412
shape of L is 
torch.Size([])
memory (bytes)
5365051392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 13% |
memory (bytes)
5365096448
| ID | GPU  | MEM |
-------------------
|  0 |  10% |  0% |
|  1 | 100% | 13% |
error is  1332090900.0
relative error loss 0.21841696
shape of L is 
torch.Size([])
memory (bytes)
5368078336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 13% |
memory (bytes)
5368340480
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1306741200.0
relative error loss 0.21426049
shape of L is 
torch.Size([])
memory (bytes)
5371555840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 13% |
memory (bytes)
5371555840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1286133800.0
relative error loss 0.21088158
shape of L is 
torch.Size([])
memory (bytes)
5374709760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 13% |
memory (bytes)
5374754816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1278481400.0
relative error loss 0.20962685
shape of L is 
torch.Size([])
memory (bytes)
5377966080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 13% |
memory (bytes)
5377966080
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1269936100.0
relative error loss 0.20822573
shape of L is 
torch.Size([])
memory (bytes)
5381124096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 13% |
memory (bytes)
5381169152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1262556700.0
relative error loss 0.20701575
shape of L is 
torch.Size([])
memory (bytes)
5384384512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
memory (bytes)
5384384512
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1253558800.0
relative error loss 0.2055404
shape of L is 
torch.Size([])
memory (bytes)
5387390976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 13% |
memory (bytes)
5387616256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1259044900.0
relative error loss 0.20643993
shape of L is 
torch.Size([])
memory (bytes)
5390786560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 13% |
memory (bytes)
5390835712
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1250625500.0
relative error loss 0.20505945
time to take a step is 290.18774604797363
it  1 : 2803742208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
shape of L is 
torch.Size([])
memory (bytes)
5393997824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 13% |
memory (bytes)
5393997824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 13% |
error is  1250625500.0
relative error loss 0.20505945
shape of L is 
torch.Size([])
memory (bytes)
5397245952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
5397250048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% | 13% |
error is  1248004600.0
relative error loss 0.2046297
shape of L is 
torch.Size([])
memory (bytes)
5400465408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
memory (bytes)
5400465408
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1243676700.0
relative error loss 0.20392008
shape of L is 
torch.Size([])
memory (bytes)
5403623424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
memory (bytes)
5403672576
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1242101800.0
relative error loss 0.20366184
shape of L is 
torch.Size([])
memory (bytes)
5406826496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
memory (bytes)
5406871552
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1239403000.0
relative error loss 0.20321934
shape of L is 
torch.Size([])
memory (bytes)
5410078720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 13% |
memory (bytes)
5410078720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1237042700.0
relative error loss 0.20283233
shape of L is 
torch.Size([])
memory (bytes)
5413257216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
memory (bytes)
5413306368
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1234545700.0
relative error loss 0.2024229
shape of L is 
torch.Size([])
memory (bytes)
5416464384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
5416529920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% | 13% |
error is  1231785500.0
relative error loss 0.20197032
shape of L is 
torch.Size([])
memory (bytes)
5419737088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 13% |
memory (bytes)
5419737088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1230990300.0
relative error loss 0.20183995
shape of L is 
torch.Size([])
memory (bytes)
5422956544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 13% |
memory (bytes)
5422956544
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1229547500.0
relative error loss 0.20160338
time to take a step is 264.8864572048187
it  2 : 2803741696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 13% |
shape of L is 
torch.Size([])
memory (bytes)
5426085888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 13% |
memory (bytes)
5426085888
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1229547500.0
relative error loss 0.20160338
shape of L is 
torch.Size([])
memory (bytes)
5429207040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 13% |
memory (bytes)
5429207040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1228697100.0
relative error loss 0.20146394
shape of L is 
torch.Size([])
memory (bytes)
5432557568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 13% |
memory (bytes)
5432598528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1226949100.0
relative error loss 0.20117734
shape of L is 
torch.Size([])
memory (bytes)
5435711488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
memory (bytes)
5435711488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1225176600.0
relative error loss 0.2008867
shape of L is 
torch.Size([])
memory (bytes)
5438767104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 13% |
memory (bytes)
5439016960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1225238000.0
relative error loss 0.20089677
shape of L is 
torch.Size([])
memory (bytes)
5442183168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 13% |
memory (bytes)
5442228224
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1224055300.0
relative error loss 0.20070285
shape of L is 
torch.Size([])
memory (bytes)
5445398528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 13% |
memory (bytes)
5445443584
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1222317600.0
relative error loss 0.20041792
shape of L is 
torch.Size([])
memory (bytes)
5448613888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 13% |
memory (bytes)
5448658944
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1221424600.0
relative error loss 0.20027152
shape of L is 
torch.Size([])
memory (bytes)
5451825152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
memory (bytes)
5451874304
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1220339200.0
relative error loss 0.20009354
shape of L is 
torch.Size([])
memory (bytes)
5455081472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
5455081472
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1219879400.0
relative error loss 0.20001815
time to take a step is 264.9355902671814
it  3 : 2803741696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
shape of L is 
torch.Size([])
memory (bytes)
5458296832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
memory (bytes)
5458296832
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1219879400.0
relative error loss 0.20001815
shape of L is 
torch.Size([])
memory (bytes)
5461471232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
5461516288
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1218978300.0
relative error loss 0.1998704
shape of L is 
torch.Size([])
memory (bytes)
5464678400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 13% |
memory (bytes)
5464723456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1218312200.0
relative error loss 0.19976118
shape of L is 
torch.Size([])
memory (bytes)
5467885568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
memory (bytes)
5467930624
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1217820200.0
relative error loss 0.1996805
shape of L is 
torch.Size([])
memory (bytes)
5471141888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 13% |
memory (bytes)
5471141888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1216690200.0
relative error loss 0.19949523
shape of L is 
torch.Size([])
memory (bytes)
5474344960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
5474344960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 13% |
error is  1217617400.0
relative error loss 0.19964726
shape of L is 
torch.Size([])
memory (bytes)
5477511168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 13% |
memory (bytes)
5477556224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1216313300.0
relative error loss 0.19943343
shape of L is 
torch.Size([])
memory (bytes)
5480771584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 13% |
memory (bytes)
5480771584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1215945200.0
relative error loss 0.19937308
shape of L is 
torch.Size([])
memory (bytes)
5483827200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 13% |
memory (bytes)
5483978752
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1215356400.0
relative error loss 0.19927654
shape of L is 
torch.Size([])
memory (bytes)
5487202304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 13% |
memory (bytes)
5487202304
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1214917100.0
relative error loss 0.1992045
time to take a step is 264.7929880619049
it  4 : 2803741696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
shape of L is 
torch.Size([])
memory (bytes)
5490262016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 13% |
memory (bytes)
5490409472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1214917100.0
relative error loss 0.1992045
shape of L is 
torch.Size([])
memory (bytes)
5493612544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
memory (bytes)
5493612544
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1214281200.0
relative error loss 0.19910024
shape of L is 
torch.Size([])
memory (bytes)
5496832000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 13% |
memory (bytes)
5496832000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1213938200.0
relative error loss 0.19904399
shape of L is 
torch.Size([])
memory (bytes)
5500047360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 13% |
memory (bytes)
5500047360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 13% |
error is  1213666800.0
relative error loss 0.1989995
shape of L is 
torch.Size([])
memory (bytes)
5503254528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 13% |
memory (bytes)
5503254528
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1212975600.0
relative error loss 0.19888617
shape of L is 
torch.Size([])
memory (bytes)
5506461696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
5506461696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% | 13% |
error is  1215302100.0
relative error loss 0.19926764
shape of L is 
torch.Size([])
memory (bytes)
5509689344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 13% |
memory (bytes)
5509689344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1212731400.0
relative error loss 0.19884612
shape of L is 
torch.Size([])
memory (bytes)
5512892416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
5512892416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 13% |
error is  1212322800.0
relative error loss 0.19877912
shape of L is 
torch.Size([])
memory (bytes)
5516062720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 13% |
memory (bytes)
5516062720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1211853800.0
relative error loss 0.19870223
shape of L is 
torch.Size([])
memory (bytes)
5519278080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
memory (bytes)
5519327232
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1211637200.0
relative error loss 0.19866672
time to take a step is 262.45147490501404
it  5 : 2803741696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
shape of L is 
torch.Size([])
memory (bytes)
5522395136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
5522550784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% | 13% |
error is  1211637200.0
relative error loss 0.19866672
shape of L is 
torch.Size([])
memory (bytes)
5525708800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 13% |
memory (bytes)
5525753856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1211092500.0
relative error loss 0.19857739
shape of L is 
torch.Size([])
memory (bytes)
5528915968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 13% |
memory (bytes)
5528961024
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1210851800.0
relative error loss 0.19853793
shape of L is 
torch.Size([])
memory (bytes)
5532131328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 13% |
memory (bytes)
5532168192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1210559500.0
relative error loss 0.19849001
shape of L is 
torch.Size([])
memory (bytes)
5535379456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 13% |
memory (bytes)
5535379456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1210052100.0
relative error loss 0.1984068
shape of L is 
torch.Size([])
memory (bytes)
5538582528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
memory (bytes)
5538582528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% | 13% |
error is  1209894400.0
relative error loss 0.19838095
shape of L is 
torch.Size([])
memory (bytes)
5541744640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
5541744640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 13% |
error is  1209294800.0
relative error loss 0.19828264
shape of L is 
torch.Size([])
memory (bytes)
5545029632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  5% | 13% |
memory (bytes)
5545029632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% | 13% |
error is  1209096700.0
relative error loss 0.19825016
shape of L is 
torch.Size([])
memory (bytes)
5548236800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
memory (bytes)
5548236800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% | 13% |
error is  1208773600.0
relative error loss 0.19819719
shape of L is 
torch.Size([])
memory (bytes)
5551407104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
5551448064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1208397800.0
relative error loss 0.19813557
time to take a step is 265.0725588798523
it  6 : 2803741696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
shape of L is 
torch.Size([])
memory (bytes)
5554626560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
memory (bytes)
5554663424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1208397800.0
relative error loss 0.19813557
shape of L is 
torch.Size([])
memory (bytes)
5557858304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
memory (bytes)
5557862400
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1208250400.0
relative error loss 0.19811139
shape of L is 
torch.Size([])
memory (bytes)
5561077760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 13% |
memory (bytes)
5561077760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1207883300.0
relative error loss 0.1980512
shape of L is 
torch.Size([])
memory (bytes)
5564289024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 13% |
memory (bytes)
5564289024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1207777300.0
relative error loss 0.19803381
shape of L is 
torch.Size([])
memory (bytes)
5567500288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 13% |
memory (bytes)
5567500288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1207429600.0
relative error loss 0.19797681
shape of L is 
torch.Size([])
memory (bytes)
5570707456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
memory (bytes)
5570707456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1207556100.0
relative error loss 0.19799756
shape of L is 
torch.Size([])
memory (bytes)
5573922816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
memory (bytes)
5573922816
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1207255000.0
relative error loss 0.19794819
shape of L is 
torch.Size([])
memory (bytes)
5577089024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 13% |
memory (bytes)
5577134080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1207029200.0
relative error loss 0.19791116
shape of L is 
torch.Size([])
memory (bytes)
5580300288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
5580345344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% | 13% |
error is  1206522900.0
relative error loss 0.19782814
shape of L is 
torch.Size([])
memory (bytes)
5583515648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
memory (bytes)
5583552512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1206747100.0
relative error loss 0.1978649
shape of L is 
torch.Size([])
memory (bytes)
5586776064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 13% |
memory (bytes)
5586776064
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1206335500.0
relative error loss 0.19779742
time to take a step is 288.7719087600708
it  7 : 2803742208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 13% |
shape of L is 
torch.Size([])
memory (bytes)
5589975040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 13% |
memory (bytes)
5589979136
| ID | GPU | MEM |
------------------
|  0 |  5% |  0% |
|  1 | 99% | 13% |
error is  1206335500.0
relative error loss 0.19779742
shape of L is 
torch.Size([])
memory (bytes)
5593153536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 13% |
memory (bytes)
5593198592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1206123000.0
relative error loss 0.19776258
shape of L is 
torch.Size([])
memory (bytes)
5596409856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
5596409856
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1205950500.0
relative error loss 0.19773428
shape of L is 
torch.Size([])
memory (bytes)
5599617024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 13% |
memory (bytes)
5599617024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 13% |
error is  1205651500.0
relative error loss 0.19768526
shape of L is 
torch.Size([])
memory (bytes)
5602787328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 13% |
memory (bytes)
5602832384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1205565400.0
relative error loss 0.19767115
shape of L is 
torch.Size([])
memory (bytes)
5606002688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 13% |
memory (bytes)
5606002688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1205226000.0
relative error loss 0.19761549
shape of L is 
torch.Size([])
memory (bytes)
5609181184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
memory (bytes)
5609246720
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1205004800.0
relative error loss 0.19757922
shape of L is 
torch.Size([])
memory (bytes)
5612457984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
memory (bytes)
5612457984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1204767200.0
relative error loss 0.19754027
shape of L is 
torch.Size([])
memory (bytes)
5615591424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
memory (bytes)
5615673344
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1204589000.0
relative error loss 0.19751106
shape of L is 
torch.Size([])
memory (bytes)
5618831360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 13% |
memory (bytes)
5618876416
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1204454400.0
relative error loss 0.19748898
time to take a step is 263.2377257347107
it  8 : 2803741696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 13% |
shape of L is 
torch.Size([])
memory (bytes)
5622083584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 13% |
memory (bytes)
5622083584
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1204454400.0
relative error loss 0.19748898
shape of L is 
torch.Size([])
memory (bytes)
5625286656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 13% |
memory (bytes)
5625286656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1204151800.0
relative error loss 0.19743936
shape of L is 
torch.Size([])
memory (bytes)
5628502016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
memory (bytes)
5628502016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1204086300.0
relative error loss 0.19742861
shape of L is 
torch.Size([])
memory (bytes)
5631709184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 13% |
memory (bytes)
5631709184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 13% |
error is  1203951600.0
relative error loss 0.19740655
shape of L is 
torch.Size([])
memory (bytes)
5634920448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 13% |
memory (bytes)
5634920448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1203914800.0
relative error loss 0.1974005
shape of L is 
torch.Size([])
memory (bytes)
5637984256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 13% |
memory (bytes)
5638139904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1203764200.0
relative error loss 0.19737582
shape of L is 
torch.Size([])
memory (bytes)
5641338880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
memory (bytes)
5641338880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1203667000.0
relative error loss 0.19735986
shape of L is 
torch.Size([])
memory (bytes)
5644546048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
memory (bytes)
5644546048
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1203556400.0
relative error loss 0.19734173
shape of L is 
torch.Size([])
memory (bytes)
5647720448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 13% |
memory (bytes)
5647769600
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1203393500.0
relative error loss 0.19731504
shape of L is 
torch.Size([])
memory (bytes)
5650976768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
5650976768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1203401200.0
relative error loss 0.19731629
shape of L is 
torch.Size([])
memory (bytes)
5654183936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 13% |
memory (bytes)
5654188032
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1203270100.0
relative error loss 0.1972948
time to take a step is 291.20903491973877
it  9 : 2803742208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
shape of L is 
torch.Size([])
memory (bytes)
5657354240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
5657395200
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1203270100.0
relative error loss 0.1972948
shape of L is 
torch.Size([])
memory (bytes)
5660536832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 13% |
memory (bytes)
5660602368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1203110900.0
relative error loss 0.1972687
shape of L is 
torch.Size([])
memory (bytes)
5663813632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
memory (bytes)
5663813632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1202985500.0
relative error loss 0.19724813
shape of L is 
torch.Size([])
memory (bytes)
5667028992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
memory (bytes)
5667028992
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1202805800.0
relative error loss 0.19721866
shape of L is 
torch.Size([])
memory (bytes)
5670232064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 13% |
memory (bytes)
5670236160
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1202672600.0
relative error loss 0.19719683
shape of L is 
torch.Size([])
memory (bytes)
5673406464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 13% |
memory (bytes)
5673451520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1202585100.0
relative error loss 0.19718248
shape of L is 
torch.Size([])
memory (bytes)
5676539904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
memory (bytes)
5676658688
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1202480600.0
relative error loss 0.19716536
shape of L is 
torch.Size([])
memory (bytes)
5679865856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 13% |
memory (bytes)
5679865856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1202347000.0
relative error loss 0.19714344
shape of L is 
torch.Size([])
memory (bytes)
5683068928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
memory (bytes)
5683068928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1202194400.0
relative error loss 0.19711842
shape of L is 
torch.Size([])
memory (bytes)
5686288384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 13% |
memory (bytes)
5686288384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1202065400.0
relative error loss 0.19709727
time to take a step is 263.95905232429504
it  10 : 2803741696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 13% |
shape of L is 
torch.Size([])
memory (bytes)
5689397248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 13% |
memory (bytes)
5689397248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1202065400.0
relative error loss 0.19709727
shape of L is 
torch.Size([])
memory (bytes)
5692706816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
memory (bytes)
5692710912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1202035200.0
relative error loss 0.19709231
shape of L is 
torch.Size([])
memory (bytes)
5695926272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
memory (bytes)
5695926272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1201984500.0
relative error loss 0.197084
shape of L is 
torch.Size([])
memory (bytes)
5699137536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
memory (bytes)
5699137536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1201945600.0
relative error loss 0.19707762
shape of L is 
torch.Size([])
memory (bytes)
5702299648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 13% |
memory (bytes)
5702299648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1201879600.0
relative error loss 0.1970668
shape of L is 
torch.Size([])
memory (bytes)
5705424896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 13% |
memory (bytes)
5705551872
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1201846300.0
relative error loss 0.19706133
shape of L is 
torch.Size([])
memory (bytes)
5708722176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 13% |
memory (bytes)
5708767232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1201788900.0
relative error loss 0.19705193
shape of L is 
torch.Size([])
memory (bytes)
5711937536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 13% |
memory (bytes)
5711978496
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1202048000.0
relative error loss 0.19709441
shape of L is 
torch.Size([])
memory (bytes)
5715185664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 13% |
memory (bytes)
5715185664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1201763800.0
relative error loss 0.19704781
shape of L is 
torch.Size([])
memory (bytes)
5718405120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 13% |
memory (bytes)
5718405120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1201706500.0
relative error loss 0.19703841
time to take a step is 267.49351811408997
it  11 : 2803741696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 13% |
shape of L is 
torch.Size([])
memory (bytes)
5721616384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 13% |
memory (bytes)
5721616384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1201706500.0
relative error loss 0.19703841
shape of L is 
torch.Size([])
memory (bytes)
5724827648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
memory (bytes)
5724827648
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1201599500.0
relative error loss 0.19702087
shape of L is 
torch.Size([])
memory (bytes)
5728059392
| ID | GPU | MEM |
------------------
|  0 |  1% |  0% |
|  1 | 15% | 13% |
memory (bytes)
5728059392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1201583100.0
relative error loss 0.19701819
shape of L is 
torch.Size([])
memory (bytes)
5731262464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 13% |
memory (bytes)
5731262464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 13% |
error is  1201472000.0
relative error loss 0.19699997
shape of L is 
torch.Size([])
memory (bytes)
5734461440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 13% |
memory (bytes)
5734461440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1201422800.0
relative error loss 0.1969919
shape of L is 
torch.Size([])
memory (bytes)
5737672704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
5737672704
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1201356300.0
relative error loss 0.196981
shape of L is 
torch.Size([])
memory (bytes)
5740883968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
5740883968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% | 13% |
error is  1201253400.0
relative error loss 0.19696411
shape of L is 
torch.Size([])
memory (bytes)
5744099328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 13% |
memory (bytes)
5744099328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1201025000.0
relative error loss 0.19692668
shape of L is 
torch.Size([])
memory (bytes)
5747318784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 13% |
memory (bytes)
5747318784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1201003000.0
relative error loss 0.19692306
shape of L is 
torch.Size([])
memory (bytes)
5750530048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
memory (bytes)
5750530048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 13% |
error is  1200838100.0
relative error loss 0.19689603
time to take a step is 277.8942358493805
it  12 : 2803741696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
shape of L is 
torch.Size([])
memory (bytes)
5753720832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
memory (bytes)
5753720832
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1200838100.0
relative error loss 0.19689603
shape of L is 
torch.Size([])
memory (bytes)
5756882944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 13% |
memory (bytes)
5756948480
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1200806400.0
relative error loss 0.19689083
shape of L is 
torch.Size([])
memory (bytes)
5760135168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 13% |
memory (bytes)
5760172032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1200741400.0
relative error loss 0.19688018
shape of L is 
torch.Size([])
memory (bytes)
5763297280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
memory (bytes)
5763297280
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1200662500.0
relative error loss 0.19686724
shape of L is 
torch.Size([])
memory (bytes)
5766594560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 13% |
memory (bytes)
5766594560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1200580100.0
relative error loss 0.19685373
shape of L is 
torch.Size([])
memory (bytes)
5769805824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
memory (bytes)
5769805824
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1200477700.0
relative error loss 0.19683693
shape of L is 
torch.Size([])
memory (bytes)
5773017088
| ID | GPU | MEM |
------------------
|  0 |  2% |  0% |
|  1 | 15% | 13% |
memory (bytes)
5773017088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1200372200.0
relative error loss 0.19681965
shape of L is 
torch.Size([])
memory (bytes)
5776244736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 13% |
memory (bytes)
5776244736
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1200259600.0
relative error loss 0.19680117
shape of L is 
torch.Size([])
memory (bytes)
5779439616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
5779439616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1200148000.0
relative error loss 0.19678287
shape of L is 
torch.Size([])
memory (bytes)
5782646784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
5782646784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1200034800.0
relative error loss 0.19676432
time to take a step is 267.14627027511597
it  13 : 2803741696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 13% |
shape of L is 
torch.Size([])
memory (bytes)
5785862144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 13% |
memory (bytes)
5785862144
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1200034800.0
relative error loss 0.19676432
shape of L is 
torch.Size([])
memory (bytes)
5789028352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 13% |
memory (bytes)
5789073408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1199991300.0
relative error loss 0.19675718
shape of L is 
torch.Size([])
memory (bytes)
5792288768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 13% |
memory (bytes)
5792288768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1199906800.0
relative error loss 0.19674332
shape of L is 
torch.Size([])
memory (bytes)
5795508224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 13% |
memory (bytes)
5795508224
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1199839200.0
relative error loss 0.19673225
shape of L is 
torch.Size([])
memory (bytes)
5798719488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 13% |
memory (bytes)
5798719488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1199787000.0
relative error loss 0.19672368
shape of L is 
torch.Size([])
memory (bytes)
5801930752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 13% |
memory (bytes)
5801930752
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1199711700.0
relative error loss 0.19671135
shape of L is 
torch.Size([])
memory (bytes)
5805133824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
memory (bytes)
5805133824
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1199651800.0
relative error loss 0.19670153
shape of L is 
torch.Size([])
memory (bytes)
5808295936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 13% |
memory (bytes)
5808361472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1199578600.0
relative error loss 0.19668952
shape of L is 
torch.Size([])
memory (bytes)
5811564544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
memory (bytes)
5811564544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% | 13% |
error is  1199507500.0
relative error loss 0.19667785
shape of L is 
torch.Size([])
memory (bytes)
5814779904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 13% |
memory (bytes)
5814779904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1199435300.0
relative error loss 0.19666602
time to take a step is 261.1263654232025
it  14 : 2803741696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
shape of L is 
torch.Size([])
memory (bytes)
5817987072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 13% |
memory (bytes)
5817987072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1199435300.0
relative error loss 0.19666602
shape of L is 
torch.Size([])
memory (bytes)
5821161472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 13% |
memory (bytes)
5821206528
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1199388700.0
relative error loss 0.19665837
shape of L is 
torch.Size([])
memory (bytes)
5824409600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 13% |
memory (bytes)
5824409600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1199355400.0
relative error loss 0.19665292
shape of L is 
torch.Size([])
memory (bytes)
5827616768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 13% |
memory (bytes)
5827616768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1199295000.0
relative error loss 0.19664301
shape of L is 
torch.Size([])
memory (bytes)
5830807552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 13% |
memory (bytes)
5830807552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1199233500.0
relative error loss 0.19663294
shape of L is 
torch.Size([])
memory (bytes)
5834047488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 13% |
memory (bytes)
5834047488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1199205900.0
relative error loss 0.1966284
shape of L is 
torch.Size([])
memory (bytes)
5837168640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
memory (bytes)
5837168640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1199147000.0
relative error loss 0.19661875
shape of L is 
torch.Size([])
memory (bytes)
5840470016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 13% |
memory (bytes)
5840470016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1199126500.0
relative error loss 0.1966154
shape of L is 
torch.Size([])
memory (bytes)
5843685376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
memory (bytes)
5843685376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% | 13% |
error is  1199074800.0
relative error loss 0.19660692
shape of L is 
torch.Size([])
memory (bytes)
5846904832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 13% |
memory (bytes)
5846904832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1199022100.0
relative error loss 0.19659826
time to take a step is 261.1183168888092
sum tnnu_Z after tensor(14557808., device='cuda:0')
shape of features
(5042,)
shape of features
(5042,)
number of orig particles 20167
number of new particles after remove low mass 18942
tnuZ shape should be parts x labs
torch.Size([20167, 702])
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
error without small mass is  1376029600.0
relative error without small mass is  0.22562139
nnu_Z shape should be number of particles by maxV
(20167, 702)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
shape of features
(20167,)
Tue Jan 31 20:44:26 EST 2023
