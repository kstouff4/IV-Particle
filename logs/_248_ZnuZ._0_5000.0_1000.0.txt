Thu Feb 2 17:01:17 EST 2023
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  0% |  0% |
numbers of X: 40710922
numbers of Z: 19636
shape of features
(19636,)
shape of features
(19636,)
ZX	Vol	Parts	Cubes	Eps
Z	0.01500529158276494	19636	19.636	0.09142475500385526
X	0.012582448812447827	1649	1.649	0.19687089041854938
X	0.014489796289336679	26576	26.576	0.08169413679263765
X	0.014193829445085297	2853	2.853	0.17071274154488447
X	0.012703080791739155	11979	11.979	0.1019755755375517
X	0.014821063487369184	24216	24.216	0.08490351164607478
X	0.0126612745249682	44678	44.678	0.06568424825666759
X	0.013646866097460915	55926	55.926	0.06248940516418283
X	0.014185413283769866	48207	48.207	0.06651362678833997
X	0.013879291254431173	12413	12.413	0.10379191175720108
X	0.013005517018697371	55709	55.709	0.06157448001487662
X	0.01356867926880336	9818	9.818	0.11138794277887402
X	0.012665012263546752	84378	84.378	0.05314455756345964
X	0.012699052531784175	7412	7.412	0.11965898179906512
X	0.014387369069680599	328826	328.826	0.03523749914121295
X	0.012715214124955607	21054	21.054	0.08452717472741379
X	0.012697119595176073	62812	62.812	0.05868866032687296
X	0.013619764159959163	55663	55.663	0.06254620914214892
X	0.012672539066214574	39413	39.413	0.06850801462982742
X	0.013807076344384142	229332	229.332	0.03919334730010412
X	0.013676498265495309	102331	102.331	0.051127657924947056
X	0.013254698139457087	30798	30.798	0.07550037504820663
X	0.01409904482428435	282488	282.488	0.03681825736134452
X	0.01269287009367164	13908	13.908	0.09699850731652117
X	0.013793674309205674	34810	34.81	0.07345003065225225
X	0.01217312663593962	8855	8.855	0.11119139822213234
X	0.012845028439596626	70219	70.219	0.05676667756397291
X	0.014893332546213507	55036	55.036	0.06468173711154312
X	0.014456439967313803	7291	7.291	0.12562943876369387
X	0.012851789523808414	70564	70.564	0.05668395410186004
X	0.014736173707317324	1467658	1467.658	0.021573467527878625
X	0.014410094221723393	14571	14.571	0.09963054073953415
X	0.014971869499504152	601113	601.113	0.029203840704832142
X	0.013468547267557494	17105	17.105	0.09234198913054457
X	0.013241291405411375	11099	11.099	0.10605931812805847
X	0.012708419636288685	15144	15.144	0.0943228646254848
X	0.014860675350013978	135690	135.69	0.047844303284279785
X	0.01480349508719069	95398	95.398	0.053737195596787656
X	0.012798976391857387	1380	1.38	0.2101029017520381
X	0.01459573666174069	5318	5.318	0.1400100558105804
X	0.013272782733111175	3059	3.059	0.1631025819661747
X	0.013701321156401507	4670	4.67	0.14315785698472822
X	0.012287443230165812	2343	2.343	0.17373863772604142
X	0.01213280119507922	930	0.93	0.23541061629171267
X	0.012921585644964068	4919	4.919	0.13797900091617393
X	0.012654090017080769	1383	1.383	0.20915563987842012
X	0.012428637127139276	2862	2.862	0.16314907893323793
X	0.012624718424282016	6184	6.184	0.12685786749910116
X	0.013475820897519041	4052	4.052	0.14926635619561832
X	0.014327098990717008	2712	2.712	0.1741631183845856
X	0.01264615640056127	12582	12.582	0.100169680661781
X	0.014205743403246793	13654	13.654	0.10132921754756112
X	0.013660532924287056	1981	1.981	0.19033897325728458
X	0.012342432690873349	6056	6.056	0.12678609729067253
X	0.01234894804472732	2494	2.494	0.1704424774027646
X	0.014568272483837758	6900	6.9	0.12828808481992185
X	0.012472953616939203	4824	4.824	0.1372526282176326
X	0.01331781753480287	2469	2.469	0.17537620046249258
X	0.012714379616295436	4750	4.75	0.13884615413320767
X	0.013663271599979664	4202	4.202	0.1481492820245686
X	0.012395467642462391	2802	2.802	0.16415908711931398
X	0.013997406903592547	4424	4.424	0.1468061185797866
X	0.012374300495365332	2574	2.574	0.16877341516724417
X	0.01494115501833929	8439	8.439	0.12097558426755937
X	0.012995768165416362	7392	7.392	0.12069242100392091
X	0.012475948743203952	2952	2.952	0.16167855147468993
X	0.0126805682326096	2188	2.188	0.17962376666960067
X	0.012685341970525534	4162	4.162	0.14498843690793534
X	0.013291446518514655	5892	5.892	0.1311504655769702
X	0.013476117101364565	5774	5.774	0.13264656220323426
X	0.012369480976938	2345	2.345	0.17407492229839047
X	0.012594492271248266	3051	3.051	0.1604154345906893
X	0.012467762618826898	3724	3.724	0.14959817112215068
X	0.012600229359554517	5561	5.561	0.13134347987319114
X	0.01306861458582622	2190	2.19	0.18138240043152182
X	0.013217452079167582	4565	4.565	0.14252869898629877
X	0.013458554104186327	5744	5.744	0.13281934203999649
X	0.01244332042823545	1455	1.455	0.2044989569191013
X	0.012306271606601834	792	0.792	0.24953632308257798
X	0.012566061794280303	3591	3.591	0.151819629989845
X	0.01262029228050668	13644	13.644	0.09743370877746073
X	0.013291190287798644	3144	3.144	0.16169395334297795
X	0.012760578236140206	2013	2.013	0.18507262855797338
X	0.014014053887066795	6582	6.582	0.1286476828813501
X	0.012829827067431902	2365	2.365	0.17571002984034317
X	0.013365428534370493	2824	2.824	0.16789563236696675
X	0.012547880282913681	1615	1.615	0.19806114251653834
X	0.012286356853116985	4962	4.962	0.1352868095691874
X	0.012306480851229599	1779	1.779	0.19053991949907334
X	0.013888935638338712	4850	4.85	0.14200677858536467
X	0.012533901584669454	2838	2.838	0.16406828170653204
X	0.01295490327773934	3932	3.932	0.1488013558941361
X	0.012373763224679892	2375	2.375	0.17335886800991251
X	0.01261240764137357	1323	1.323	0.2120375210789174
X	0.01478266939290395	4286	4.286	0.1510892501583487
X	0.01216242438618933	3793	3.793	0.14746165490877305
X	0.013093788842077948	4220	4.22	0.14585375931044592
X	0.01353368063344798	2314	2.314	0.1801707099006449
X	0.012517642807198201	6791	6.791	0.12261104900907319
X	0.014825936525510881	5223	5.223	0.1415904744004118
X	0.0126918288332815	2748	2.748	0.1665338386125887
X	0.013718562007320806	12216	12.216	0.10394250734540549
X	0.014062504989071783	4435	4.435	0.14691169015044644
X	0.01339429050547865	8820	8.82	0.11494331274054172
X	0.012280370795011137	1825	1.825	0.1887916883946289
X	0.013735197372591334	3812	3.812	0.15330653603524733
X	0.013647313804066603	2938	2.938	0.16685214548091826
X	0.013587410701959474	5710	5.71	0.13350581988706253
X	0.012189712278504145	843	0.843	0.2436250605489451
X	0.012954712870045538	3071	3.071	0.16157816007098122
X	0.012018596711509462	1358	1.358	0.20684800227072553
X	0.012194199611367824	1483	1.483	0.2018385184190939
X	0.013447011406836943	2301	2.301	0.18012322668257924
X	0.012354190046800904	1593	1.593	0.19793966641916175
X	0.012419258837318	3991	3.991	0.14599500039168278
X	0.01250820691212424	4324	4.324	0.14248521121033275
X	0.012724249840319881	1564	1.564	0.2011245782384733
X	0.01255340028880474	2215	2.215	0.17829091573274478
X	0.012449573211521426	1670	1.67	0.19534966198986176
X	0.014641268100967468	7199	7.199	0.12669773176072896
X	0.012221152503733197	1582	1.582	0.1976826532643883
X	0.013747720420598382	10431	10.431	0.10963980355241867
X	0.013083158261770934	2467	2.467	0.17438715069284233
X	0.0122777592101047	1796	1.796	0.18978895148647237
X	0.012462021268345077	2532	2.532	0.17010154657059345
X	0.012276911259084781	2099	2.099	0.18017404385237523
X	0.01252760779546326	2473	2.473	0.17174390920258745
X	0.012432095244731715	1783	1.783	0.19104294097659955
X	0.012767471754980735	3029	3.029	0.1615357097244453
X	0.013845306715506855	14208	14.208	0.09914174170141053
X	0.012450630430568281	2639	2.639	0.16771966412189082
X	0.01261823865341079	8491	8.491	0.11411602410594815
X	0.012356773996306926	1913	1.913	0.18623588962327756
X	0.013395578170932475	3922	3.922	0.15059756779690942
X	0.013165028135826833	1795	1.795	0.19429089382502296
X	0.012522103302130503	2991	2.991	0.16117102072790895
X	0.013180307199290779	2226	2.226	0.18091175675334586
X	0.0125988140361951	1857	1.857	0.18930957796185782
X	0.013193942266600702	1741	1.741	0.19642281740642756
X	0.012228487302841502	2095	2.095	0.18005128969879042
X	0.012650504371504391	2217	2.217	0.17869567828078792
X	0.012579469272830279	3781	3.781	0.1492858162597223
X	0.01301006318607138	1377	1.377	0.21140495155003972
X	0.01417794632473358	9792	9.792	0.11313074341704898
X	0.014692766189805367	10143	10.143	0.11314769576391549
X	0.014657922629977005	8277	8.277	0.12098545346443437
X	0.012571042415274746	2579	2.579	0.169553457348202
X	0.013419715661419388	2141	2.141	0.18437790766091997
X	0.014646026988914396	2184	2.184	0.18857714890291455
X	0.012638797886255866	1729	1.729	0.19407529658877787
X	0.012359102958527239	2758	2.758	0.16486592216127685
X	0.011901875373994265	1550	1.55	0.19728522739340987
X	0.014343312622592877	3845	3.845	0.15509020619466748
X	0.013259462600376411	4943	4.943	0.13894569776402063
X	0.013358625792068458	10407	10.407	0.10867894401870166
X	0.012301176931467369	2910	2.91	0.16169052036300327
X	0.013053441444171858	16112	16.112	0.0932234541867312
X	0.0134213655480368	2967	2.967	0.16538383293747233
X	0.014057912999786273	3419	3.419	0.16020420709289496
X	0.014130106507789416	5030	5.03	0.14109961111250274
X	0.013216335747211976	712	0.712	0.26477502827819727
X	0.014651482207204278	9602	9.602	0.11512596586548511
X	0.012460779592627601	2753	2.753	0.1654168199987564
X	0.014072312109484161	4140	4.14	0.15035632628382195
X	0.012829695759969513	1427	1.427	0.2079365312426013
X	0.012584902392018076	2897	2.897	0.16316754733004074
X	0.013462218280546607	1332	1.332	0.21620752290055553
X	0.014324384841477316	3600	3.6	0.1584617665518399
X	0.013283784968019522	6035	6.035	0.13008129569466664
X	0.014018006839597735	4828	4.828	0.14266135195588528
X	0.012514521488035796	881	0.881	0.2421843710743614
X	0.012212338866994502	1408	1.408	0.20546226144499205
X	0.01232465188703074	1107	1.107	0.22329329386349825
X	0.012477618428021605	6222	6.222	0.12610546210696208
X	0.013652864087177667	2228	2.228	0.18299372322183483
X	0.013192393651021457	4320	4.32	0.14508191687772623
X	0.013759725585422381	3601	3.601	0.15633715955247818
X	0.012926267461732754	5945	5.945	0.12955089757677363
X	0.01243853044926224	2385	2.385	0.17341775648122598
X	0.013594944680666519	6619	6.619	0.12711447883745994
X	0.012352127867339044	2788	2.788	0.16424153795696958
X	0.013490982638721179	3643	3.643	0.1547134737592438
X	0.012355025490236978	2323	2.323	0.17455467253987
X	0.01261509680125498	5685	5.685	0.13043276658802463
X	0.012790172507712945	1963	1.963	0.18677498096678732
X	0.013596793510059792	5183	5.183	0.1379171998678585
X	0.013710500982948945	5115	5.115	0.13891076255619114
X	0.013027207785986037	2948	2.948	0.1640997629493457
X	0.012665734471685756	6821	6.821	0.12291195029483525
X	0.014799436433654908	5200	5.2	0.14171438769788225
X	0.01464773731191347	12223	12.223	0.10621786726622198
X	0.012643736382313064	2527	2.527	0.17103699728416313
X	0.012555570947194703	753	0.753	0.25547426797233697
X	0.012378941120548161	3599	3.599	0.15095026630366468
X	0.012646823510184577	2570	2.57	0.17009156543653103
X	0.012387149572988549	3507	3.507	0.15229250791773255
X	0.014304373888999017	3690	3.69	0.15708962236856383
X	0.012678193001728984	1950	1.95	0.1866412276816995
X	0.013453907006533445	1637	1.637	0.201805615730833
X	0.012419181493580638	2865	2.865	0.1630507431366935
X	0.013258962526732252	1479	1.479	0.20773696852172507
X	0.014795201103889107	4377	4.377	0.15007721966540102
X	0.012357104130598078	1956	1.956	0.18486269723671903
X	0.013754295227853383	9097	9.097	0.11477484521676037
X	0.012819955259380596	3965	3.965	0.14787035174863547
X	0.012455973845724026	3114	3.114	0.15873999409278908
X	0.012359962623637989	1526	1.526	0.20082643302335237
X	0.01463737055384349	5446	5.446	0.139036388000238
X	0.013360611436666801	3274	3.274	0.15980212600919663
X	0.013805726440482929	5796	5.796	0.13354991899680824
X	0.01312695691222524	6827	6.827	0.12434969344520852
X	0.012553495959426023	4292	4.292	0.14301063157320737
X	0.012542099574892933	4872	4.872	0.13705271867479255
X	0.012375296392672033	4386	4.386	0.14130656108756082
X	0.012588992421829951	3406	3.406	0.1546139858557731
X	0.012750875093245831	1061	1.061	0.2290559279936085
X	0.012414720734502045	2076	2.076	0.1815111054809281
X	0.012578371826845727	1210	1.21	0.218246030182358
X	0.012084316542053372	1086	1.086	0.22325306118796656
X	0.01250850276247669	5276	5.276	0.13334196391711572
X	0.01245002109053887	2378	2.378	0.173641186852555
X	0.012251410193303157	2444	2.444	0.17114397961438005
X	0.012827132774917772	2065	2.065	0.18382440886720408
X	0.012629627424041045	4389	4.389	0.142235603363387
X	0.012273035020754686	2850	2.85	0.16269306381839
X	0.014158293589165899	3248	3.248	0.16335469873709102
X	0.014382175508462214	3534	3.534	0.15965628259229558
X	0.012324885742292659	1571	1.571	0.1987020002236866
X	0.012380024189276686	2765	2.765	0.16481957345399426
X	0.012582153090364269	6365	6.365	0.12550244471043698
X	0.012270306540793084	840	0.84	0.24445112605300853
X	0.012395361223968284	1641	1.641	0.19620807132659826
X	0.012193245951182223	2874	2.874	0.16188658315488744
X	0.013894410536739276	3514	3.514	0.15812946073542533
X	0.014611800894465801	4334	4.334	0.14994717206660843
X	0.012839048786134994	2018	2.018	0.1852979277152022
X	0.012612235436196238	2179	2.179	0.1795470523594941
X	0.01268462659748806	4068	4.068	0.14609395546087212
X	0.014570525984523634	7762	7.762	0.12335796522007882
X	0.013349735951062642	1835	1.835	0.19376657779523004
X	0.0135824477818883	13507	13.507	0.1001858486630703
X	0.014842079740204484	58260	58.26	0.06339301780948121
X	0.013965181183401405	19842	19.842	0.08895158883377663
X	0.014241560646697552	6397	6.397	0.13057511318610504
X	0.011943398361003688	828	0.828	0.24342538211946524
X	0.012569811526810313	4659	4.659	0.139212645039654
X	0.013134602113930211	17332	17.332	0.09117086605606628
X	0.013460497667575249	172123	172.123	0.042763581432888005
X	0.012624433693966873	3949	3.949	0.14731317527235763
X	0.01414300825257421	130478	130.478	0.047679844961818055
X	0.012677177329114614	28450	28.45	0.07638007811422366
X	0.01257476108721105	22482	22.482	0.08239261111006198
X	0.014869562143882789	109497	109.497	0.0514002604855019
X	0.012569720023559836	2136	2.136	0.18054056004955762
X	0.012872543546720695	5135	5.135	0.13584426466202223
X	0.01334863464700616	217225	217.225	0.0394616899690758
X	0.013729907118352288	368672	368.672	0.033394566852561924
X	0.013070810984075486	5248	5.248	0.13555105519211227
X	0.013326133284677737	30181	30.181	0.07614770915222571
X	0.014675601516998192	19490	19.49	0.09097624842991035
X	0.014963066466138087	28385	28.385	0.08078124994984774
X	0.013053808695752992	93823	93.823	0.05181744273430497
X	0.014345973064332142	49357	49.357	0.06624103132221706
X	0.01379134522392784	33358	33.358	0.07449644384932615
X	0.012682993687222503	19707	19.707	0.08633768903588603
X	0.012817866954149142	3004	3.004	0.16219542885944474
X	0.01464097513561463	352776	352.776	0.03462238572390941
X	0.012524746100502701	5252	5.252	0.1336025470443558
X	0.012540017377200122	2920	2.92	0.16254430664398636
X	0.012745843863418916	57162	57.162	0.06063923672865574
X	0.014609101842694044	57847	57.847	0.06320928705477485
X	0.014888661597198075	266604	266.604	0.03822343086759737
X	0.013850062737696562	103262	103.262	0.051188264195784344
X	0.013124736097635864	1946	1.946	0.18893652118096832
X	0.012611877183026228	16189	16.189	0.09201385719687927
X	0.014868513369989213	12738	12.738	0.10529039809399146
X	0.012684371942966231	35920	35.92	0.07068233370866572
X	0.014144171303728153	65859	65.859	0.059885377533876814
X	0.012605412465824693	5980	5.98	0.12821887409771865
X	0.012541874255415999	58056	58.056	0.060002835977279775
X	0.013059368227431742	767	0.767	0.2572619760802415
X	0.01351864782278205	5830	5.83	0.13235943354219273
X	0.012763497445509734	75185	75.185	0.055370634125246176
X	0.01271794760687484	81027	81.027	0.05394225412057826
X	0.013697919939695049	69185	69.185	0.058283759763087445
X	0.013504194387871421	9758	9.758	0.11143867909994032
X	0.013598793147627847	240452	240.452	0.03838462699375129
X	0.013389734509351435	14085	14.085	0.0983267545812695
X	0.012575521174332345	25453	25.453	0.07905493849096179
X	0.013416963115305472	60605	60.605	0.060494411579380944
X	0.012781587649216383	4169	4.169	0.14527279840402507
X	0.014049844302232557	111668	111.668	0.0501088294723321
X	0.014912050008394299	190024	190.024	0.04281306286575938
X	0.014893316892110282	258540	258.54	0.03862079670931114
X	0.013877410285285694	26471	26.471	0.08063297682132868
X	0.01279139246131428	29745	29.745	0.07548047367833476
X	0.012367086026898212	19043	19.043	0.08659856020566921
X	0.012655666276691435	5224	5.224	0.13430554658357063
X	0.014213903995999082	51501	51.501	0.06510773272386246
X	0.013389201197268386	7944	7.944	0.11900680910293604
X	0.012697435995885697	25446	25.446	0.07931685820451233
X	0.014060782553432264	103499	103.499	0.051407258753276903
X	0.013968264425817688	36283	36.283	0.07274666467182027
X	0.013692338242806775	14624	14.624	0.09782964559153416
X	0.013455891843716076	5248	5.248	0.13686935493674476
X	0.013116130479370313	91119	91.119	0.052408161874162855
X	0.013493017209924624	15299	15.299	0.0958992792639976
X	0.01288222280209849	28033	28.033	0.0771685624008848
X	0.014921910803749524	14172	14.172	0.10173360446205619
X	0.014502324408181933	192945	192.945	0.0422021615566784
X	0.01285529634086526	15274	15.274	0.09441545753700632
X	0.01435268859242879	97473	97.473	0.05280596413667883
X	0.01335796339637774	9423	9.423	0.11233549913280115
X	0.01277639103082282	32550	32.55	0.07321820328496569
X	0.013523174247934203	18320	18.32	0.09037557183429884
X	0.014851133873657593	163544	163.544	0.04494778889482521
X	0.013762663903398678	47828	47.828	0.06601968196108204
X	0.013841404070680692	7746	7.746	0.1213484457426247
X	0.01288561650324023	110464	110.464	0.04886090186618242
X	0.013858133345508842	114285	114.285	0.04949619732643648
X	0.013555016856711044	8359	8.359	0.11748486231526727
X	0.013972882681153266	152925	152.925	0.045040427901464106
X	0.01481272037368748	121882	121.882	0.04953343173775056
X	0.014986460243817275	313672	313.672	0.036286167777361523
X	0.013295331413715063	48558	48.558	0.06493509180985495
X	0.012567319247521818	2195	2.195	0.17889685579105735
X	0.01286692314116392	17273	17.273	0.09065024410584074
X	0.0130179532064887	39935	39.935	0.06882237765982231
X	0.013370294178740449	18800	18.8	0.08926085405252389
X	0.014279377157618466	30174	30.174	0.07792774805106241
X	0.012349794177950459	2781	2.781	0.16436887282230367
X	0.01376493346439368	56549	56.549	0.062438107201415974
X	0.013721520165675447	165838	165.838	0.0435752003135058
X	0.012440871700302888	24469	24.469	0.07981383829554481
X	0.01402975115146147	8319	8.319	0.1190308718262167
X	0.013657637120780919	21818	21.818	0.08554347281177331
X	0.013964862219472221	9488	9.488	0.11375067777883852
X	0.014734221681776804	5532	5.532	0.1386165256918828
X	0.013761047760742535	12858	12.858	0.10228831556512961
X	0.012575805177455887	7737	7.737	0.11757664617345394
X	0.012471066078885578	24500	24.5	0.07984465193147289
X	0.012981255086177921	45598	45.598	0.06578450784349618
X	0.014510112152705083	16542	16.542	0.09572548694823332
X	0.014683910721760745	39205	39.205	0.07208309503820738
X	0.01262214328323085	7035	7.035	0.1215130722409028
X	0.014743736028121844	398733	398.733	0.033315151211411945
X	0.013168516776829628	10172	10.172	0.10898753662729806
X	0.012760694322516183	106825	106.825	0.049249346030114396
X	0.012465118636555922	1825	1.825	0.1897337174040129
X	0.01427270625884353	7623	7.623	0.12325185642307544
X	0.013415353719519641	3314	3.314	0.1593736581858238
X	0.013318308311308877	20192	20.192	0.08704755469405072
X	0.013820374900369727	3702	3.702	0.1551295275747594
X	0.014455257904438261	148553	148.553	0.04599544298680459
X	0.012658785345460626	4262	4.262	0.14374503846295755
X	0.012552796552785048	4247	4.247	0.14351129268315366
X	0.013777298590570359	216152	216.152	0.039945537639418605
X	0.013700131223155198	59327	59.327	0.061351348513598886
X	0.014654029004525426	53757	53.757	0.06483965183827399
X	0.014900861219709575	169597	169.597	0.044456070044497115
X	0.01494117948760104	243270	243.27	0.03945470276436591
X	0.012376835052343665	5159	5.159	0.13386941279515732
X	0.012644564372715389	6012	6.012	0.12812335426064886
X	0.013534532715606744	12180	12.18	0.1035774765116792
X	0.012327036113438722	2141	2.141	0.179231360050933
X	0.013641608690340305	6908	6.908	0.12545975051507544
X	0.012693831846000106	19130	19.13	0.0872219764663793
X	0.0124352415870126	4484	4.484	0.14049564885197788
X	0.012988897878792396	7313	7.313	0.12110411760169655
X	0.013096376931835779	6457	6.457	0.12658243914179537
X	0.012955451499752755	5654	5.654	0.1318354632441809
X	0.014708780014838425	283049	283.049	0.0373168527865882
X	0.013214338500091901	10303	10.303	0.10864937754414626
X	0.01466373532499998	84742	84.742	0.05572481489474404
X	0.012827432643748514	8112	8.112	0.11650304243693584
X	0.01473007988987046	21609	21.609	0.08800826869334681
X	0.013395153302821365	41182	41.182	0.06877227925881155
X	0.014239704152917513	491609	491.609	0.030710982730650243
X	0.014688792827359626	556735	556.735	0.029770024604916994
X	0.013553145323818525	20088	20.088	0.08770704217123665
X	0.01464086939323535	44405	44.405	0.06908413603487949
X	0.012572988980849922	4184	4.184	0.14430530144172463
X	0.01296572045889568	13596	13.596	0.09843022987300326
X	0.013646185909930589	70836	70.836	0.05775448977078233
X	0.014361440641010602	29367	29.367	0.07878547204313457
X	0.013328412425697676	6574	6.574	0.1265657628262794
X	0.01261132675783294	70376	70.376	0.056378311709923884
X	0.014341417319459798	467726	467.726	0.03129922041036496
X	0.013598314768765331	52736	52.736	0.06364915387794884
X	0.013024835229155242	13442	13.442	0.09895462916994342
X	0.014756513616086838	33467	33.467	0.0761124426330235
X	0.014396113220145411	12263	12.263	0.10549118603016852
X	0.012462088901871773	3155	3.155	0.1580752317418218
X	0.014948792954600408	135811	135.811	0.04792444144707143
X	0.01310278941577518	22085	22.085	0.08402763092343865
X	0.012352519983065947	2017	2.017	0.18295736169432875
X	0.012928159563048809	82600	82.6	0.0538913279691162
X	0.014726899433891914	17175	17.175	0.0950031907896276
X	0.01257096653122747	1871	1.871	0.18869698491878178
X	0.014572734354634692	88930	88.93	0.054722288573174196
X	0.014788860415327938	98102	98.102	0.05322131686217929
X	0.014819278459784055	131953	131.953	0.04824688327599094
X	0.014711266549967754	185128	185.128	0.04299249522508899
X	0.013809446092177522	194501	194.501	0.04140805264794432
X	0.013654636490098036	17615	17.615	0.0918612812427799
X	0.012931688782134118	49797	49.797	0.06379951170889601
X	0.012862696391849551	145407	145.407	0.044556961612392566
X	0.014377741256006656	160082	160.082	0.044783296005049364
X	0.012665091070144815	10061	10.061	0.10797479528809784
X	0.01375965621120237	140330	140.33	0.0461125372739006
X	0.014429171962766975	184180	184.18	0.04278908308411733
X	0.012754512053128145	112655	112.655	0.048376872927243755
X	0.013749531650838706	81484	81.484	0.05525927571742368
X	0.01339436812470923	45208	45.208	0.06666567981634305
X	0.013108772034928188	11404	11.404	0.10475343367765083
X	0.012758050997306248	7284	7.284	0.12054192765198275
X	0.012577282601564207	6589	6.589	0.12404781070815078
X	0.013178791785374091	127516	127.516	0.04692853230583051
X	0.013524568127011414	111261	111.261	0.04953667079237081
X	0.014349925959949392	311402	311.402	0.03585167253051052
X	0.014470977562864723	106108	106.108	0.05147344637316402
X	0.013022632137188354	78799	78.799	0.054877294093179936
X	0.012664575874677456	12849	12.849	0.09951925289285574
X	0.014451769159619684	130775	130.775	0.047987938735444484
X	0.01233478555047801	3841	3.841	0.1475352392862137
X	0.013313645294200802	12456	12.456	0.10224438355910716
X	0.014179364080741465	157551	157.551	0.044813809461272384
X	0.01289254868907559	19814	19.814	0.08665405168718077
X	0.014826253567057244	5349	5.349	0.14047087095597002
X	0.012706448873983773	13770	13.77	0.09735615595013106
X	0.014961991079104111	230385	230.385	0.040195589039290325
X	0.013652542648729832	19912	19.912	0.08817921320352284
X	0.014684204881547725	177228	177.228	0.0435952694518916
X	0.012789669452079903	10218	10.218	0.10776998125661284
X	0.012636281660526728	13628	13.628	0.09751297152078899
X	0.013547742275372305	4219	4.219	0.1475318540571904
X	0.013299382206566603	2689	2.689	0.17037801533150904
X	0.014733861876985985	43473	43.473	0.06972132966726112
X	0.012503003269335261	52993	52.993	0.061791956475072754
X	0.013567302849977522	4264	4.264	0.14708174524208475
X	0.013281167294428567	108408	108.408	0.04966589945460784
X	0.012492355047511254	3750	3.75	0.1493496985623955
X	0.013709351633902985	36366	36.366	0.07223934404990001
X	0.012543446465781222	9130	9.13	0.11116857387763193
X	0.013586815976617045	122186	122.186	0.04808748699894273
X	0.013266882586079418	9913	9.913	0.11020160671226757
X	0.014309308766326143	38008	38.008	0.07220731770952889
X	0.01269692633991106	25542	25.542	0.07921630233441966
X	0.014535869652999025	32663	32.663	0.07634752854047847
X	0.014371608387342165	63204	63.204	0.06103612260436328
X	0.013183322384318916	324782	324.782	0.034367212389529396
X	0.014279310113297822	9124	9.124	0.11610225359180766
X	0.012638531742624854	10356	10.356	0.10686484510146267
X	0.013598921255082316	72881	72.881	0.05714304495040383
X	0.013696242522267999	14177	14.177	0.09885660678628838
X	0.014939768720076565	497651	497.651	0.03107942660345345
X	0.01286580027928561	4096	4.096	0.14645098122485495
X	0.013784512517681986	131777	131.777	0.04711768364604497
X	0.014835155342496352	256609	256.609	0.038666963757690555
X	0.013156720067227403	3876	3.876	0.15028695620332486
X	0.01347703169661589	151083	151.083	0.04468134282356704
X	0.012840815191759669	16477	16.477	0.09202478637326071
X	0.014905482009902384	657163	657.163	0.028306812859187746
X	0.013643150238223616	9731	9.731	0.11192292796560592
X	0.01266278031351517	28045	28.045	0.0767169289574267
X	0.013432503333065393	7442	7.442	0.12175564259014164
X	0.013103967947682796	4047	4.047	0.14794145054422433
X	0.014159609101290974	7024	7.024	0.12632488757673874
X	0.01316940574035543	33597	33.597	0.07318496895503898
X	0.013655863547180245	19798	19.798	0.08835530211504762
X	0.014863463611947927	127137	127.137	0.048897053348075654
X	0.012658180144281954	17484	17.484	0.08979320877096487
X	0.013101258050900785	4568	4.568	0.14207869797987252
X	0.01353129879765577	10161	10.161	0.11001899605770907
X	0.014930576541149855	196367	196.367	0.04236455910609206
X	0.01484651078022537	76926	76.926	0.057789696786726345
X	0.01487297413320172	246826	246.826	0.03920447470099781
X	0.012780327258168137	33131	33.131	0.0727951552739856
X	0.01260357550408255	7359	7.359	0.11964420129625854
X	0.01444037694834483	4013	4.013	0.15323916968469894
X	0.012408040593717714	8475	8.475	0.11355018154722205
X	0.014876062209433737	406216	406.216	0.03320807135076281
X	0.013674754675337785	9197	9.197	0.11413647416404775
X	0.0130862666867307	25822	25.822	0.07972758880070746
X	0.013503038812747548	71115	71.115	0.05747647173037323
X	0.013164997847604398	43224	43.224	0.06728195661793113
X	0.012698700460722977	8351	8.351	0.11499358099434448
X	0.013199013143606898	4898	4.898	0.1391578019170936
X	0.014820462049051856	192136	192.136	0.04256810653721596
X	0.014069423113238225	53914	53.914	0.06390353927321447
X	0.012549476288566506	10526	10.526	0.10603618126729987
X	0.013715517485952896	25946	25.946	0.0808563119312361
X	0.014949819350438284	160597	160.597	0.04532100225197251
X	0.014484193472021652	119888	119.888	0.049435568633423396
X	0.014914274118363602	306779	306.779	0.03649713980706303
X	0.013042372967795809	115582	115.582	0.048323175989743755
X	0.014377661997967563	5995	5.995	0.13385451580005028
X	0.013092175048543708	39072	39.072	0.06945689857112713
X	0.012791338372193187	52430	52.43	0.0624854223950697
X	0.013246873692349314	12265	12.265	0.10260029613442345
X	0.012521888426517754	14655	14.655	0.09489165087677467
X	0.013770708345711934	6873	6.873	0.12606754980045107
X	0.012798293530461824	39414	39.414	0.06873329828362458
X	0.013279226111611083	25161	25.161	0.08081307321371084
X	0.01206536198326021	2320	2.32	0.17325434051316868
X	0.013697270696420818	20784	20.784	0.08702318382725964
X	0.012687220224638703	12857	12.857	0.09955787344310953
X	0.013146320665106674	52848	52.848	0.06289149063209569
X	0.012987091208470272	5052	5.052	0.13698810234769174
X	0.012887425600294154	19248	19.248	0.08748364179878965
X	0.012758024822978616	19213	19.213	0.08724276474372615
X	0.01467122895577313	15595	15.595	0.09798517483012799
X	0.013735411048353085	5446	5.446	0.136119812199332
X	0.012418791652944364	12567	12.567	0.09960532923753998
X	0.01262086977757149	20433	20.433	0.0851632807746293
X	0.014846135999879002	185677	185.677	0.04308093377565281
X	0.014728210965653885	7450	7.45	0.12550602488951781
X	0.012755850795329314	2860	2.86	0.16460681022777482
X	0.013323244948306653	6166	6.166	0.1292812365416497
X	0.013261833126224226	148980	148.98	0.04465038380210571
X	0.014696729883483663	224334	224.334	0.04031269852191467
X	0.012741320219477277	69821	69.821	0.056720857610087626
X	0.014429813370785697	9085	9.085	0.11667521078886853
X	0.014593639866631786	127871	127.871	0.04850619269955131
X	0.014939627710615515	156092	156.092	0.04574248032785765
X	0.013333362007711552	2203	2.203	0.18223916820200337
X	0.012874773121695507	10485	10.485	0.10708378578868186
X	0.012692945884273934	31636	31.636	0.07375536567026297
X	0.012787717937441998	34222	34.222	0.07202709300684071
X	0.01272227250561009	24734	24.734	0.08012292198448966
X	0.013759877853972608	149296	149.296	0.04517055938945654
X	0.013517399682649632	16211	16.211	0.0941227158146566
X	0.013619343415395028	13298	13.298	0.10079909062833389
X	0.0137286790357627	3015	3.015	0.16574756049123865
X	0.012925684214264084	25010	25.01	0.0802502911291121
X	0.014577298099968657	18083	18.083	0.09306838639341809
X	0.012649460323405085	29261	29.261	0.07561261841907937
X	0.01374803590314742	142241	142.241	0.04589217280731722
X	0.014355403855643017	448327	448.327	0.031754614157400586
X	0.012549026974465124	6252	6.252	0.12614315658180014
X	0.012129634127042448	2567	2.567	0.1678059197430815
X	0.014900163244340075	9779	9.779	0.11507090449270253
X	0.01351078792447765	33114	33.114	0.07416910029566381
X	0.013464149084192772	28098	28.098	0.0782530309394821
X	0.012660294690018205	3782	3.782	0.1495916767271393
X	0.012521724628337321	6197	6.197	0.12642342471801649
X	0.012772906444686892	23088	23.088	0.08209205721286608
X	0.014212202479870036	3592	3.592	0.1581642596277782
X	0.014933708388179947	63177	63.177	0.061830518593147414
X	0.014223359144531735	5389	5.389	0.1381968033903749
X	0.01267498352923292	13638	13.638	0.09758855914026157
X	0.014492707172027131	9221	9.221	0.11626718698274097
X	0.01258210170521572	7774	7.774	0.11740940442415011
X	0.013665892692276691	3073	3.073	0.1644466946469172
X	0.012756954229602858	135572	135.572	0.04548403636981615
X	0.014891941835334873	348407	348.407	0.034964030178858366
X	0.013242252515829112	131861	131.861	0.04648168180865451
X	0.013751385987824653	74146	74.146	0.05702775150700306
X	0.013177263688075196	20864	20.864	0.08579774776064421
X	0.01404273853603176	9808	9.808	0.11270861615548308
X	0.013707801799306165	14473	14.473	0.09820563724685841
X	0.01377122083080928	138324	138.324	0.04634735642696434
X	0.0142572852699538	21780	21.78	0.08682797142391555
X	0.0137295304403216	26994	26.994	0.07982326363169609
X	0.014092818535617108	9847	9.847	0.11269326819733201
X	0.015001082615141676	709433	709.433	0.027652670081649056
X	0.013275885442493631	27685	27.685	0.07827198049457826
X	0.014640376461346529	49268	49.268	0.06673122599515016
X	0.014776777528643014	160598	160.598	0.045145368341273286
X	0.013642199811489194	39398	39.398	0.07022149589066233
X	0.013481993677613593	27546	27.546	0.07880707199634206
X	0.01434658657773636	287686	287.686	0.036808090260624045
X	0.013836262293714126	95891	95.891	0.052450179588097105
X	0.01269514676810614	58747	58.747	0.06000913188670762
X	0.012793919219141664	18280	18.28	0.08878558712381847
X	0.012920586188778577	41483	41.483	0.06778558581374901
X	0.012683168825001681	8795	8.795	0.11297889164171072
X	0.013665829369438413	145060	145.06	0.045501890422717205
X	0.014934152892090987	852194	852.194	0.025974510609478543
X	0.01484678169965958	125144	125.144	0.04913686338949461
X	0.013897167432843282	150362	150.362	0.04521293831730768
X	0.013404246304688092	32680	32.68	0.0742996527218495
X	0.012684948677571025	54938	54.938	0.061348689475060274
X	0.01468713580147162	42941	42.941	0.06993399189151647
X	0.012375442200872203	12419	12.419	0.09988295133059742
X	0.013723077817761331	18892	18.892	0.08989272167232751
X	0.014772119954705823	344873	344.873	0.034988705660744315
X	0.012687621805650315	57193	57.193	0.0605358227047284
X	0.012643179923715392	138828	138.828	0.0449910798613242
X	0.012779892809764125	115362	115.362	0.048027296981085765
X	0.013358879795499452	42657	42.657	0.06790888154182508
X	0.01256933371814594	5596	5.596	0.13096186477250837
X	0.01407525595617552	48270	48.27	0.06631213347399005
X	0.014446292080244488	123693	123.693	0.04888066318038718
X	0.013529588917658236	4747	4.747	0.14178222576543248
X	0.013921384865845514	573742	573.742	0.028950446464856224
X	0.012719924358834199	38395	38.395	0.06919424138269675
X	0.01420827646502278	38946	38.946	0.07145393981453474
X	0.013645121637368291	7796	7.796	0.1205133596145466
X	0.012686118659780051	16416	16.416	0.09176712841638511
X	0.012343741561371786	2826	2.826	0.16346504096092018
X	0.01268521309023928	4697	4.697	0.13925978989795723
X	0.012787095095509399	23445	23.445	0.08170347958221404
X	0.014705331695776177	106057	106.057	0.05175812028986798
X	0.014981192436881714	2354733	2354.733	0.018529693576214427
X	0.012600745957010313	7135	7.135	0.12087433361221765
X	0.012710812660982654	108971	108.971	0.04886007476492602
X	0.01367102004111504	9583	9.583	0.11257271591194894
X	0.01360737278409742	18655	18.655	0.09001737475574907
X	0.013232822083963928	10097	10.097	0.10943427684420276
X	0.01278879603004649	193104	193.104	0.040458781313729174
X	0.012879053861515903	12191	12.191	0.10184699259668932
X	0.014915351704493061	974361	974.361	0.024829684546439926
X	0.013968782191406454	5575	5.575	0.13582234508395352
X	0.014650963808312887	171883	171.883	0.04400929621187146
X	0.013962835269931851	35270	35.27	0.0734270451286399
X	0.014832388670354458	109507	109.507	0.05135582835065037
X	0.014303194706515983	173309	173.309	0.04353821751574437
X	0.012631648030707177	16580	16.58	0.09133246166298228
X	0.01352240101832672	10325	10.325	0.1094093867080327
X	0.014803595650447861	81819	81.819	0.056559342085475015
X	0.01444243725469275	9739	9.739	0.11403601729952102
X	0.012885083780450562	12832	12.832	0.1001377045633915
X	0.014392702732767614	96296	96.296	0.05306946416972031
X	0.013768814642248673	26615	26.615	0.08027679544598391
X	0.014687551796451032	57029	57.029	0.06362354812446636
X	0.012472307849859865	21171	21.171	0.08383045369081837
X	0.014182890591294546	165264	165.264	0.04410916442569413
X	0.01493131189081525	243590	243.59	0.03942873441380373
X	0.012774357917379712	45722	45.722	0.06537393400968113
X	0.014730525555592833	8132	8.132	0.12190091270288382
X	0.012694369402762501	88190	88.19	0.052407960987784455
X	0.013845821074586439	63626	63.626	0.0601489473541454
X	0.013083711502134004	34660	34.66	0.07227156628990707
X	0.012823674453946983	41237	41.237	0.0677498711935465
X	0.01293190054750533	44239	44.239	0.06636702520675496
X	0.013579431780030309	51366	51.366	0.06418033062995743
X	0.012629675266847743	180229	180.229	0.041227705999973586
X	0.014327028373423966	54815	54.815	0.0639369969636335
X	0.0136861709955963	13674	13.674	0.10002966063652494
X	0.014394666578248368	5790	5.79	0.13546934013024184
X	0.01490596759209872	19940	19.94	0.09075662548496279
X	0.012609432370095035	70068	70.068	0.05645797171273343
X	0.012661275847148849	112565	112.565	0.04827156342349551
X	0.013645099705772611	86630	86.63	0.05400526892240731
X	0.012635384426471217	10185	10.185	0.1074506735316378
X	0.012699733534083322	21788	21.788	0.08353319268385254
X	0.013635903530366983	58830	58.83	0.06142733733714393
X	0.014515901105801777	120095	120.095	0.04944317577864036
X	0.014803916360003515	51061	51.061	0.0661854362207527
X	0.012737662800383187	64737	64.737	0.05816285630235193
X	0.014091836446743276	7367	7.367	0.12413444533977773
X	0.013064735170734825	29429	29.429	0.07628544815457591
X	0.013914505160419949	2155	2.155	0.18621164237193907
X	0.013796089226085868	6568	6.568	0.12806809519007648
X	0.01316281826154576	148258	148.258	0.04461116683150248
X	0.01306498323961354	30117	30.117	0.0757005525207235
X	0.012718102323763915	60525	60.525	0.059451457417348176
X	0.012675317211068533	2446	2.446	0.17304834946467818
X	0.013492690162220499	3732	3.732	0.15348018152472126
X	0.013623783835279955	77183	77.183	0.05609515637840484
X	0.013788361322425568	73973	73.973	0.057123280609922035
X	0.012691326141531262	39207	39.207	0.06866168609831766
X	0.012467294943843066	3900	3.9	0.14731123674116994
X	0.012603620426441673	6032	6.032	0.12784330638236036
X	0.013548018585323302	50615	50.615	0.06444642745346098
X	0.013578026444181521	31676	31.676	0.07539955313134966
X	0.013742697566241331	112883	112.883	0.04956188497675189
X	0.01261637317135437	6428	6.428	0.1252043888597397
X	0.012429621704152384	3659	3.659	0.15032521507068444
X	0.01259015240515007	26562	26.562	0.07796925867149183
X	0.013660329609817284	10888	10.888	0.10785435570091172
X	0.014272812790282328	244414	244.414	0.03879667412229465
X	0.012959977076002546	5963	5.963	0.12953281628450122
X	0.012664779351912346	54872	54.872	0.061340730832271456
X	0.014920193787806351	100792	100.792	0.052899244189830934
X	0.013326400950056288	92652	92.652	0.05239453926762557
X	0.014843420197978185	161683	161.683	0.04511177072789989
X	0.01461046771479598	139383	139.383	0.04715033366033813
X	0.012724138879293152	3269	3.269	0.15730329749390579
X	0.013425634626196488	46902	46.902	0.06590441163029788
X	0.013585952400376144	19161	19.161	0.08917106890087974
X	0.012676286475351866	129301	129.301	0.046110169700550305
X	0.012931593628381238	86254	86.254	0.05312402038804727
X	0.012855906502157953	38090	38.09	0.06962479201742362
X	0.013607383654807802	64408	64.408	0.05955866282809824
X	0.012685142420815674	10251	10.251	0.10736013230265068
X	0.012667990270098095	34822	34.822	0.07138681457986719
X	0.013581549695322	5378	5.378	0.13617880214674358
X	0.013335884163998791	17240	17.24	0.09179693574563935
X	0.013514527028651436	194558	194.558	0.04110713910666899
X	0.014954703418054717	223871	223.871	0.04057513639870598
X	0.013509165426665859	14205	14.205	0.09833974865254597
X	0.013058087793313725	20110	20.11	0.0865942731452284
X	0.012859114798378868	18239	18.239	0.08900273450493677
X	0.014763084224734631	174791	174.791	0.04387522367215224
X	0.013495162308063503	27144	27.144	0.07921998488410557
X	0.012696804359266836	12773	12.773	0.09980075770105637
X	0.014664196765881213	58490	58.49	0.06305587443238243
X	0.014504386998605593	128749	128.749	0.048296816470960964
X	0.01248351511682368	11418	11.418	0.10301859383104434
X	0.013781837844413515	55618	55.618	0.06281025973682137
X	0.012687958920076698	5955	5.955	0.1286777127433183
time for making epsilon is 2.1395955085754395
epsilons are
[0.19687089041854938, 0.08169413679263765, 0.17071274154488447, 0.1019755755375517, 0.08490351164607478, 0.06568424825666759, 0.06248940516418283, 0.06651362678833997, 0.10379191175720108, 0.06157448001487662, 0.11138794277887402, 0.05314455756345964, 0.11965898179906512, 0.03523749914121295, 0.08452717472741379, 0.05868866032687296, 0.06254620914214892, 0.06850801462982742, 0.03919334730010412, 0.051127657924947056, 0.07550037504820663, 0.03681825736134452, 0.09699850731652117, 0.07345003065225225, 0.11119139822213234, 0.05676667756397291, 0.06468173711154312, 0.12562943876369387, 0.05668395410186004, 0.021573467527878625, 0.09963054073953415, 0.029203840704832142, 0.09234198913054457, 0.10605931812805847, 0.0943228646254848, 0.047844303284279785, 0.053737195596787656, 0.2101029017520381, 0.1400100558105804, 0.1631025819661747, 0.14315785698472822, 0.17373863772604142, 0.23541061629171267, 0.13797900091617393, 0.20915563987842012, 0.16314907893323793, 0.12685786749910116, 0.14926635619561832, 0.1741631183845856, 0.100169680661781, 0.10132921754756112, 0.19033897325728458, 0.12678609729067253, 0.1704424774027646, 0.12828808481992185, 0.1372526282176326, 0.17537620046249258, 0.13884615413320767, 0.1481492820245686, 0.16415908711931398, 0.1468061185797866, 0.16877341516724417, 0.12097558426755937, 0.12069242100392091, 0.16167855147468993, 0.17962376666960067, 0.14498843690793534, 0.1311504655769702, 0.13264656220323426, 0.17407492229839047, 0.1604154345906893, 0.14959817112215068, 0.13134347987319114, 0.18138240043152182, 0.14252869898629877, 0.13281934203999649, 0.2044989569191013, 0.24953632308257798, 0.151819629989845, 0.09743370877746073, 0.16169395334297795, 0.18507262855797338, 0.1286476828813501, 0.17571002984034317, 0.16789563236696675, 0.19806114251653834, 0.1352868095691874, 0.19053991949907334, 0.14200677858536467, 0.16406828170653204, 0.1488013558941361, 0.17335886800991251, 0.2120375210789174, 0.1510892501583487, 0.14746165490877305, 0.14585375931044592, 0.1801707099006449, 0.12261104900907319, 0.1415904744004118, 0.1665338386125887, 0.10394250734540549, 0.14691169015044644, 0.11494331274054172, 0.1887916883946289, 0.15330653603524733, 0.16685214548091826, 0.13350581988706253, 0.2436250605489451, 0.16157816007098122, 0.20684800227072553, 0.2018385184190939, 0.18012322668257924, 0.19793966641916175, 0.14599500039168278, 0.14248521121033275, 0.2011245782384733, 0.17829091573274478, 0.19534966198986176, 0.12669773176072896, 0.1976826532643883, 0.10963980355241867, 0.17438715069284233, 0.18978895148647237, 0.17010154657059345, 0.18017404385237523, 0.17174390920258745, 0.19104294097659955, 0.1615357097244453, 0.09914174170141053, 0.16771966412189082, 0.11411602410594815, 0.18623588962327756, 0.15059756779690942, 0.19429089382502296, 0.16117102072790895, 0.18091175675334586, 0.18930957796185782, 0.19642281740642756, 0.18005128969879042, 0.17869567828078792, 0.1492858162597223, 0.21140495155003972, 0.11313074341704898, 0.11314769576391549, 0.12098545346443437, 0.169553457348202, 0.18437790766091997, 0.18857714890291455, 0.19407529658877787, 0.16486592216127685, 0.19728522739340987, 0.15509020619466748, 0.13894569776402063, 0.10867894401870166, 0.16169052036300327, 0.0932234541867312, 0.16538383293747233, 0.16020420709289496, 0.14109961111250274, 0.26477502827819727, 0.11512596586548511, 0.1654168199987564, 0.15035632628382195, 0.2079365312426013, 0.16316754733004074, 0.21620752290055553, 0.1584617665518399, 0.13008129569466664, 0.14266135195588528, 0.2421843710743614, 0.20546226144499205, 0.22329329386349825, 0.12610546210696208, 0.18299372322183483, 0.14508191687772623, 0.15633715955247818, 0.12955089757677363, 0.17341775648122598, 0.12711447883745994, 0.16424153795696958, 0.1547134737592438, 0.17455467253987, 0.13043276658802463, 0.18677498096678732, 0.1379171998678585, 0.13891076255619114, 0.1640997629493457, 0.12291195029483525, 0.14171438769788225, 0.10621786726622198, 0.17103699728416313, 0.25547426797233697, 0.15095026630366468, 0.17009156543653103, 0.15229250791773255, 0.15708962236856383, 0.1866412276816995, 0.201805615730833, 0.1630507431366935, 0.20773696852172507, 0.15007721966540102, 0.18486269723671903, 0.11477484521676037, 0.14787035174863547, 0.15873999409278908, 0.20082643302335237, 0.139036388000238, 0.15980212600919663, 0.13354991899680824, 0.12434969344520852, 0.14301063157320737, 0.13705271867479255, 0.14130656108756082, 0.1546139858557731, 0.2290559279936085, 0.1815111054809281, 0.218246030182358, 0.22325306118796656, 0.13334196391711572, 0.173641186852555, 0.17114397961438005, 0.18382440886720408, 0.142235603363387, 0.16269306381839, 0.16335469873709102, 0.15965628259229558, 0.1987020002236866, 0.16481957345399426, 0.12550244471043698, 0.24445112605300853, 0.19620807132659826, 0.16188658315488744, 0.15812946073542533, 0.14994717206660843, 0.1852979277152022, 0.1795470523594941, 0.14609395546087212, 0.12335796522007882, 0.19376657779523004, 0.1001858486630703, 0.06339301780948121, 0.08895158883377663, 0.13057511318610504, 0.24342538211946524, 0.139212645039654, 0.09117086605606628, 0.042763581432888005, 0.14731317527235763, 0.047679844961818055, 0.07638007811422366, 0.08239261111006198, 0.0514002604855019, 0.18054056004955762, 0.13584426466202223, 0.0394616899690758, 0.033394566852561924, 0.13555105519211227, 0.07614770915222571, 0.09097624842991035, 0.08078124994984774, 0.05181744273430497, 0.06624103132221706, 0.07449644384932615, 0.08633768903588603, 0.16219542885944474, 0.03462238572390941, 0.1336025470443558, 0.16254430664398636, 0.06063923672865574, 0.06320928705477485, 0.03822343086759737, 0.051188264195784344, 0.18893652118096832, 0.09201385719687927, 0.10529039809399146, 0.07068233370866572, 0.059885377533876814, 0.12821887409771865, 0.060002835977279775, 0.2572619760802415, 0.13235943354219273, 0.055370634125246176, 0.05394225412057826, 0.058283759763087445, 0.11143867909994032, 0.03838462699375129, 0.0983267545812695, 0.07905493849096179, 0.060494411579380944, 0.14527279840402507, 0.0501088294723321, 0.04281306286575938, 0.03862079670931114, 0.08063297682132868, 0.07548047367833476, 0.08659856020566921, 0.13430554658357063, 0.06510773272386246, 0.11900680910293604, 0.07931685820451233, 0.051407258753276903, 0.07274666467182027, 0.09782964559153416, 0.13686935493674476, 0.052408161874162855, 0.0958992792639976, 0.0771685624008848, 0.10173360446205619, 0.0422021615566784, 0.09441545753700632, 0.05280596413667883, 0.11233549913280115, 0.07321820328496569, 0.09037557183429884, 0.04494778889482521, 0.06601968196108204, 0.1213484457426247, 0.04886090186618242, 0.04949619732643648, 0.11748486231526727, 0.045040427901464106, 0.04953343173775056, 0.036286167777361523, 0.06493509180985495, 0.17889685579105735, 0.09065024410584074, 0.06882237765982231, 0.08926085405252389, 0.07792774805106241, 0.16436887282230367, 0.062438107201415974, 0.0435752003135058, 0.07981383829554481, 0.1190308718262167, 0.08554347281177331, 0.11375067777883852, 0.1386165256918828, 0.10228831556512961, 0.11757664617345394, 0.07984465193147289, 0.06578450784349618, 0.09572548694823332, 0.07208309503820738, 0.1215130722409028, 0.033315151211411945, 0.10898753662729806, 0.049249346030114396, 0.1897337174040129, 0.12325185642307544, 0.1593736581858238, 0.08704755469405072, 0.1551295275747594, 0.04599544298680459, 0.14374503846295755, 0.14351129268315366, 0.039945537639418605, 0.061351348513598886, 0.06483965183827399, 0.044456070044497115, 0.03945470276436591, 0.13386941279515732, 0.12812335426064886, 0.1035774765116792, 0.179231360050933, 0.12545975051507544, 0.0872219764663793, 0.14049564885197788, 0.12110411760169655, 0.12658243914179537, 0.1318354632441809, 0.0373168527865882, 0.10864937754414626, 0.05572481489474404, 0.11650304243693584, 0.08800826869334681, 0.06877227925881155, 0.030710982730650243, 0.029770024604916994, 0.08770704217123665, 0.06908413603487949, 0.14430530144172463, 0.09843022987300326, 0.05775448977078233, 0.07878547204313457, 0.1265657628262794, 0.056378311709923884, 0.03129922041036496, 0.06364915387794884, 0.09895462916994342, 0.0761124426330235, 0.10549118603016852, 0.1580752317418218, 0.04792444144707143, 0.08402763092343865, 0.18295736169432875, 0.0538913279691162, 0.0950031907896276, 0.18869698491878178, 0.054722288573174196, 0.05322131686217929, 0.04824688327599094, 0.04299249522508899, 0.04140805264794432, 0.0918612812427799, 0.06379951170889601, 0.044556961612392566, 0.044783296005049364, 0.10797479528809784, 0.0461125372739006, 0.04278908308411733, 0.048376872927243755, 0.05525927571742368, 0.06666567981634305, 0.10475343367765083, 0.12054192765198275, 0.12404781070815078, 0.04692853230583051, 0.04953667079237081, 0.03585167253051052, 0.05147344637316402, 0.054877294093179936, 0.09951925289285574, 0.047987938735444484, 0.1475352392862137, 0.10224438355910716, 0.044813809461272384, 0.08665405168718077, 0.14047087095597002, 0.09735615595013106, 0.040195589039290325, 0.08817921320352284, 0.0435952694518916, 0.10776998125661284, 0.09751297152078899, 0.1475318540571904, 0.17037801533150904, 0.06972132966726112, 0.061791956475072754, 0.14708174524208475, 0.04966589945460784, 0.1493496985623955, 0.07223934404990001, 0.11116857387763193, 0.04808748699894273, 0.11020160671226757, 0.07220731770952889, 0.07921630233441966, 0.07634752854047847, 0.06103612260436328, 0.034367212389529396, 0.11610225359180766, 0.10686484510146267, 0.05714304495040383, 0.09885660678628838, 0.03107942660345345, 0.14645098122485495, 0.04711768364604497, 0.038666963757690555, 0.15028695620332486, 0.04468134282356704, 0.09202478637326071, 0.028306812859187746, 0.11192292796560592, 0.0767169289574267, 0.12175564259014164, 0.14794145054422433, 0.12632488757673874, 0.07318496895503898, 0.08835530211504762, 0.048897053348075654, 0.08979320877096487, 0.14207869797987252, 0.11001899605770907, 0.04236455910609206, 0.057789696786726345, 0.03920447470099781, 0.0727951552739856, 0.11964420129625854, 0.15323916968469894, 0.11355018154722205, 0.03320807135076281, 0.11413647416404775, 0.07972758880070746, 0.05747647173037323, 0.06728195661793113, 0.11499358099434448, 0.1391578019170936, 0.04256810653721596, 0.06390353927321447, 0.10603618126729987, 0.0808563119312361, 0.04532100225197251, 0.049435568633423396, 0.03649713980706303, 0.048323175989743755, 0.13385451580005028, 0.06945689857112713, 0.0624854223950697, 0.10260029613442345, 0.09489165087677467, 0.12606754980045107, 0.06873329828362458, 0.08081307321371084, 0.17325434051316868, 0.08702318382725964, 0.09955787344310953, 0.06289149063209569, 0.13698810234769174, 0.08748364179878965, 0.08724276474372615, 0.09798517483012799, 0.136119812199332, 0.09960532923753998, 0.0851632807746293, 0.04308093377565281, 0.12550602488951781, 0.16460681022777482, 0.1292812365416497, 0.04465038380210571, 0.04031269852191467, 0.056720857610087626, 0.11667521078886853, 0.04850619269955131, 0.04574248032785765, 0.18223916820200337, 0.10708378578868186, 0.07375536567026297, 0.07202709300684071, 0.08012292198448966, 0.04517055938945654, 0.0941227158146566, 0.10079909062833389, 0.16574756049123865, 0.0802502911291121, 0.09306838639341809, 0.07561261841907937, 0.04589217280731722, 0.031754614157400586, 0.12614315658180014, 0.1678059197430815, 0.11507090449270253, 0.07416910029566381, 0.0782530309394821, 0.1495916767271393, 0.12642342471801649, 0.08209205721286608, 0.1581642596277782, 0.061830518593147414, 0.1381968033903749, 0.09758855914026157, 0.11626718698274097, 0.11740940442415011, 0.1644466946469172, 0.04548403636981615, 0.034964030178858366, 0.04648168180865451, 0.05702775150700306, 0.08579774776064421, 0.11270861615548308, 0.09820563724685841, 0.04634735642696434, 0.08682797142391555, 0.07982326363169609, 0.11269326819733201, 0.027652670081649056, 0.07827198049457826, 0.06673122599515016, 0.045145368341273286, 0.07022149589066233, 0.07880707199634206, 0.036808090260624045, 0.052450179588097105, 0.06000913188670762, 0.08878558712381847, 0.06778558581374901, 0.11297889164171072, 0.045501890422717205, 0.025974510609478543, 0.04913686338949461, 0.04521293831730768, 0.0742996527218495, 0.061348689475060274, 0.06993399189151647, 0.09988295133059742, 0.08989272167232751, 0.034988705660744315, 0.0605358227047284, 0.0449910798613242, 0.048027296981085765, 0.06790888154182508, 0.13096186477250837, 0.06631213347399005, 0.04888066318038718, 0.14178222576543248, 0.028950446464856224, 0.06919424138269675, 0.07145393981453474, 0.1205133596145466, 0.09176712841638511, 0.16346504096092018, 0.13925978989795723, 0.08170347958221404, 0.05175812028986798, 0.018529693576214427, 0.12087433361221765, 0.04886007476492602, 0.11257271591194894, 0.09001737475574907, 0.10943427684420276, 0.040458781313729174, 0.10184699259668932, 0.024829684546439926, 0.13582234508395352, 0.04400929621187146, 0.0734270451286399, 0.05135582835065037, 0.04353821751574437, 0.09133246166298228, 0.1094093867080327, 0.056559342085475015, 0.11403601729952102, 0.1001377045633915, 0.05306946416972031, 0.08027679544598391, 0.06362354812446636, 0.08383045369081837, 0.04410916442569413, 0.03942873441380373, 0.06537393400968113, 0.12190091270288382, 0.052407960987784455, 0.0601489473541454, 0.07227156628990707, 0.0677498711935465, 0.06636702520675496, 0.06418033062995743, 0.041227705999973586, 0.0639369969636335, 0.10002966063652494, 0.13546934013024184, 0.09075662548496279, 0.05645797171273343, 0.04827156342349551, 0.05400526892240731, 0.1074506735316378, 0.08353319268385254, 0.06142733733714393, 0.04944317577864036, 0.0661854362207527, 0.05816285630235193, 0.12413444533977773, 0.07628544815457591, 0.18621164237193907, 0.12806809519007648, 0.04461116683150248, 0.0757005525207235, 0.059451457417348176, 0.17304834946467818, 0.15348018152472126, 0.05609515637840484, 0.057123280609922035, 0.06866168609831766, 0.14731123674116994, 0.12784330638236036, 0.06444642745346098, 0.07539955313134966, 0.04956188497675189, 0.1252043888597397, 0.15032521507068444, 0.07796925867149183, 0.10785435570091172, 0.03879667412229465, 0.12953281628450122, 0.061340730832271456, 0.052899244189830934, 0.05239453926762557, 0.04511177072789989, 0.04715033366033813, 0.15730329749390579, 0.06590441163029788, 0.08917106890087974, 0.046110169700550305, 0.05312402038804727, 0.06962479201742362, 0.05955866282809824, 0.10736013230265068, 0.07138681457986719, 0.13617880214674358, 0.09179693574563935, 0.04110713910666899, 0.04057513639870598, 0.09833974865254597, 0.0865942731452284, 0.08900273450493677, 0.04387522367215224, 0.07921998488410557, 0.09980075770105637, 0.06305587443238243, 0.048296816470960964, 0.10301859383104434, 0.06281025973682137, 0.1286777127433183]
0.09142475500385526
Making ranges
torch.Size([35272, 2])
We keep 5.59e+06/3.86e+08 =  1% of the original kernel matrix.

torch.Size([4064, 2])
We keep 1.68e+05/2.72e+06 =  6% of the original kernel matrix.

torch.Size([12438, 2])
We keep 9.64e+05/3.24e+07 =  2% of the original kernel matrix.

torch.Size([40703, 2])
We keep 1.30e+07/7.06e+08 =  1% of the original kernel matrix.

torch.Size([37665, 2])
We keep 8.04e+06/5.22e+08 =  1% of the original kernel matrix.

torch.Size([6411, 2])
We keep 6.16e+05/8.14e+06 =  7% of the original kernel matrix.

torch.Size([15382, 2])
We keep 1.43e+06/5.60e+07 =  2% of the original kernel matrix.

torch.Size([17504, 2])
We keep 8.37e+06/1.43e+08 =  5% of the original kernel matrix.

torch.Size([24205, 2])
We keep 4.27e+06/2.35e+08 =  1% of the original kernel matrix.

torch.Size([36195, 2])
We keep 1.73e+07/5.86e+08 =  2% of the original kernel matrix.

torch.Size([35890, 2])
We keep 7.08e+06/4.76e+08 =  1% of the original kernel matrix.

torch.Size([82026, 2])
We keep 3.15e+07/2.00e+09 =  1% of the original kernel matrix.

torch.Size([51451, 2])
We keep 1.22e+07/8.77e+08 =  1% of the original kernel matrix.

torch.Size([93500, 2])
We keep 4.62e+07/3.13e+09 =  1% of the original kernel matrix.

torch.Size([55365, 2])
We keep 1.49e+07/1.10e+09 =  1% of the original kernel matrix.

torch.Size([79901, 2])
We keep 3.49e+07/2.32e+09 =  1% of the original kernel matrix.

torch.Size([50702, 2])
We keep 1.31e+07/9.47e+08 =  1% of the original kernel matrix.

torch.Size([20661, 2])
We keep 5.23e+06/1.54e+08 =  3% of the original kernel matrix.

torch.Size([26873, 2])
We keep 4.40e+06/2.44e+08 =  1% of the original kernel matrix.

torch.Size([61707, 2])
We keep 2.64e+08/3.10e+09 =  8% of the original kernel matrix.

torch.Size([44536, 2])
We keep 1.47e+07/1.09e+09 =  1% of the original kernel matrix.

torch.Size([17528, 2])
We keep 2.48e+06/9.64e+07 =  2% of the original kernel matrix.

torch.Size([24624, 2])
We keep 3.61e+06/1.93e+08 =  1% of the original kernel matrix.

torch.Size([129318, 2])
We keep 1.40e+08/7.12e+09 =  1% of the original kernel matrix.

torch.Size([65313, 2])
We keep 2.08e+07/1.66e+09 =  1% of the original kernel matrix.

torch.Size([14888, 2])
We keep 1.59e+06/5.49e+07 =  2% of the original kernel matrix.

torch.Size([22536, 2])
We keep 2.87e+06/1.46e+08 =  1% of the original kernel matrix.

torch.Size([541456, 2])
We keep 1.03e+09/1.08e+11 =  0% of the original kernel matrix.

torch.Size([139164, 2])
We keep 7.13e+07/6.46e+09 =  1% of the original kernel matrix.

torch.Size([36031, 2])
We keep 7.67e+06/4.43e+08 =  1% of the original kernel matrix.

torch.Size([35016, 2])
We keep 6.49e+06/4.13e+08 =  1% of the original kernel matrix.

torch.Size([112532, 2])
We keep 5.32e+07/3.95e+09 =  1% of the original kernel matrix.

torch.Size([61397, 2])
We keep 1.62e+07/1.23e+09 =  1% of the original kernel matrix.

torch.Size([94142, 2])
We keep 4.91e+07/3.10e+09 =  1% of the original kernel matrix.

torch.Size([55641, 2])
We keep 1.48e+07/1.09e+09 =  1% of the original kernel matrix.

torch.Size([67455, 2])
We keep 3.32e+07/1.55e+09 =  2% of the original kernel matrix.

torch.Size([46879, 2])
We keep 1.07e+07/7.74e+08 =  1% of the original kernel matrix.

torch.Size([379652, 2])
We keep 6.84e+08/5.26e+10 =  1% of the original kernel matrix.

torch.Size([115306, 2])
We keep 5.06e+07/4.50e+09 =  1% of the original kernel matrix.

torch.Size([165531, 2])
We keep 1.64e+08/1.05e+10 =  1% of the original kernel matrix.

torch.Size([75725, 2])
We keep 2.48e+07/2.01e+09 =  1% of the original kernel matrix.

torch.Size([48449, 2])
We keep 2.43e+07/9.49e+08 =  2% of the original kernel matrix.

torch.Size([39737, 2])
We keep 9.12e+06/6.05e+08 =  1% of the original kernel matrix.

torch.Size([481643, 2])
We keep 9.49e+08/7.98e+10 =  1% of the original kernel matrix.

torch.Size([130980, 2])
We keep 6.06e+07/5.55e+09 =  1% of the original kernel matrix.

torch.Size([25007, 2])
We keep 4.31e+06/1.93e+08 =  2% of the original kernel matrix.

torch.Size([29932, 2])
We keep 4.67e+06/2.73e+08 =  1% of the original kernel matrix.

torch.Size([55190, 2])
We keep 5.89e+07/1.21e+09 =  4% of the original kernel matrix.

torch.Size([43097, 2])
We keep 1.02e+07/6.84e+08 =  1% of the original kernel matrix.

torch.Size([12289, 2])
We keep 1.16e+07/7.84e+07 = 14% of the original kernel matrix.

torch.Size([19625, 2])
We keep 2.85e+06/1.74e+08 =  1% of the original kernel matrix.

torch.Size([123016, 2])
We keep 5.93e+07/4.93e+09 =  1% of the original kernel matrix.

torch.Size([64507, 2])
We keep 1.78e+07/1.38e+09 =  1% of the original kernel matrix.

torch.Size([75348, 2])
We keep 6.58e+07/3.03e+09 =  2% of the original kernel matrix.

torch.Size([49536, 2])
We keep 1.50e+07/1.08e+09 =  1% of the original kernel matrix.

torch.Size([13446, 2])
We keep 1.59e+06/5.32e+07 =  2% of the original kernel matrix.

torch.Size([21539, 2])
We keep 2.88e+06/1.43e+08 =  2% of the original kernel matrix.

torch.Size([109893, 2])
We keep 1.59e+08/4.98e+09 =  3% of the original kernel matrix.

torch.Size([60619, 2])
We keep 1.75e+07/1.39e+09 =  1% of the original kernel matrix.

torch.Size([2819754, 2])
We keep 1.49e+10/2.15e+12 =  0% of the original kernel matrix.

torch.Size([332364, 2])
We keep 2.79e+08/2.88e+10 =  0% of the original kernel matrix.

torch.Size([21671, 2])
We keep 7.86e+06/2.12e+08 =  3% of the original kernel matrix.

torch.Size([27343, 2])
We keep 5.08e+06/2.86e+08 =  1% of the original kernel matrix.

torch.Size([985804, 2])
We keep 3.84e+09/3.61e+11 =  1% of the original kernel matrix.

torch.Size([193300, 2])
We keep 1.25e+08/1.18e+10 =  1% of the original kernel matrix.

torch.Size([27216, 2])
We keep 8.48e+06/2.93e+08 =  2% of the original kernel matrix.

torch.Size([30686, 2])
We keep 5.59e+06/3.36e+08 =  1% of the original kernel matrix.

torch.Size([19611, 2])
We keep 3.29e+06/1.23e+08 =  2% of the original kernel matrix.

torch.Size([26162, 2])
We keep 3.94e+06/2.18e+08 =  1% of the original kernel matrix.

torch.Size([25513, 2])
We keep 6.20e+06/2.29e+08 =  2% of the original kernel matrix.

torch.Size([29900, 2])
We keep 5.12e+06/2.97e+08 =  1% of the original kernel matrix.

torch.Size([194113, 2])
We keep 3.46e+08/1.84e+10 =  1% of the original kernel matrix.

torch.Size([82638, 2])
We keep 3.25e+07/2.66e+09 =  1% of the original kernel matrix.

torch.Size([144477, 2])
We keep 1.98e+08/9.10e+09 =  2% of the original kernel matrix.

torch.Size([70742, 2])
We keep 2.39e+07/1.87e+09 =  1% of the original kernel matrix.

torch.Size([3602, 2])
We keep 1.32e+05/1.90e+06 =  6% of the original kernel matrix.

torch.Size([11932, 2])
We keep 8.72e+05/2.71e+07 =  3% of the original kernel matrix.

torch.Size([10594, 2])
We keep 1.10e+06/2.83e+07 =  3% of the original kernel matrix.

torch.Size([19095, 2])
We keep 2.33e+06/1.04e+08 =  2% of the original kernel matrix.

torch.Size([6682, 2])
We keep 4.94e+05/9.36e+06 =  5% of the original kernel matrix.

torch.Size([15018, 2])
We keep 1.52e+06/6.01e+07 =  2% of the original kernel matrix.

torch.Size([9989, 2])
We keep 8.50e+05/2.18e+07 =  3% of the original kernel matrix.

torch.Size([18532, 2])
We keep 2.07e+06/9.17e+07 =  2% of the original kernel matrix.

torch.Size([4869, 2])
We keep 4.15e+05/5.49e+06 =  7% of the original kernel matrix.

torch.Size([12849, 2])
We keep 1.25e+06/4.60e+07 =  2% of the original kernel matrix.

torch.Size([2640, 2])
We keep 6.70e+04/8.65e+05 =  7% of the original kernel matrix.

torch.Size([10409, 2])
We keep 6.50e+05/1.83e+07 =  3% of the original kernel matrix.

torch.Size([10562, 2])
We keep 1.02e+06/2.42e+07 =  4% of the original kernel matrix.

torch.Size([18657, 2])
We keep 2.13e+06/9.66e+07 =  2% of the original kernel matrix.

torch.Size([3794, 2])
We keep 1.21e+05/1.91e+06 =  6% of the original kernel matrix.

torch.Size([12363, 2])
We keep 8.60e+05/2.72e+07 =  3% of the original kernel matrix.

torch.Size([7124, 2])
We keep 3.83e+05/8.19e+06 =  4% of the original kernel matrix.

torch.Size([15746, 2])
We keep 1.42e+06/5.62e+07 =  2% of the original kernel matrix.

torch.Size([12739, 2])
We keep 1.71e+06/3.82e+07 =  4% of the original kernel matrix.

torch.Size([20724, 2])
We keep 2.50e+06/1.21e+08 =  2% of the original kernel matrix.

torch.Size([8638, 2])
We keep 6.81e+05/1.64e+07 =  4% of the original kernel matrix.

torch.Size([17049, 2])
We keep 1.85e+06/7.96e+07 =  2% of the original kernel matrix.

torch.Size([5950, 2])
We keep 3.88e+05/7.35e+06 =  5% of the original kernel matrix.

torch.Size([14520, 2])
We keep 1.42e+06/5.33e+07 =  2% of the original kernel matrix.

torch.Size([22079, 2])
We keep 4.27e+06/1.58e+08 =  2% of the original kernel matrix.

torch.Size([27714, 2])
We keep 4.35e+06/2.47e+08 =  1% of the original kernel matrix.

torch.Size([22364, 2])
We keep 4.59e+06/1.86e+08 =  2% of the original kernel matrix.

torch.Size([28277, 2])
We keep 4.77e+06/2.68e+08 =  1% of the original kernel matrix.

torch.Size([4595, 2])
We keep 2.30e+05/3.92e+06 =  5% of the original kernel matrix.

torch.Size([13114, 2])
We keep 1.13e+06/3.89e+07 =  2% of the original kernel matrix.

torch.Size([12585, 2])
We keep 1.40e+06/3.67e+07 =  3% of the original kernel matrix.

torch.Size([20479, 2])
We keep 2.48e+06/1.19e+08 =  2% of the original kernel matrix.

torch.Size([6258, 2])
We keep 3.53e+05/6.22e+06 =  5% of the original kernel matrix.

torch.Size([14795, 2])
We keep 1.29e+06/4.90e+07 =  2% of the original kernel matrix.

torch.Size([12823, 2])
We keep 1.54e+06/4.76e+07 =  3% of the original kernel matrix.

torch.Size([20933, 2])
We keep 2.80e+06/1.35e+08 =  2% of the original kernel matrix.

torch.Size([10167, 2])
We keep 1.01e+06/2.33e+07 =  4% of the original kernel matrix.

torch.Size([18334, 2])
We keep 2.09e+06/9.47e+07 =  2% of the original kernel matrix.

torch.Size([5787, 2])
We keep 3.22e+05/6.10e+06 =  5% of the original kernel matrix.

torch.Size([14368, 2])
We keep 1.30e+06/4.85e+07 =  2% of the original kernel matrix.

torch.Size([10741, 2])
We keep 8.55e+05/2.26e+07 =  3% of the original kernel matrix.

torch.Size([18989, 2])
We keep 2.07e+06/9.33e+07 =  2% of the original kernel matrix.

torch.Size([8644, 2])
We keep 7.14e+05/1.77e+07 =  4% of the original kernel matrix.

torch.Size([16993, 2])
We keep 1.91e+06/8.25e+07 =  2% of the original kernel matrix.

torch.Size([6563, 2])
We keep 4.09e+05/7.85e+06 =  5% of the original kernel matrix.

torch.Size([14942, 2])
We keep 1.41e+06/5.50e+07 =  2% of the original kernel matrix.

torch.Size([9645, 2])
We keep 7.90e+05/1.96e+07 =  4% of the original kernel matrix.

torch.Size([18442, 2])
We keep 1.99e+06/8.69e+07 =  2% of the original kernel matrix.

torch.Size([6268, 2])
We keep 3.34e+05/6.63e+06 =  5% of the original kernel matrix.

torch.Size([14852, 2])
We keep 1.31e+06/5.05e+07 =  2% of the original kernel matrix.

torch.Size([15170, 2])
We keep 2.17e+06/7.12e+07 =  3% of the original kernel matrix.

torch.Size([23464, 2])
We keep 3.29e+06/1.66e+08 =  1% of the original kernel matrix.

torch.Size([14726, 2])
We keep 1.63e+06/5.46e+07 =  2% of the original kernel matrix.

torch.Size([22431, 2])
We keep 2.89e+06/1.45e+08 =  1% of the original kernel matrix.

torch.Size([7230, 2])
We keep 4.00e+05/8.71e+06 =  4% of the original kernel matrix.

torch.Size([15816, 2])
We keep 1.46e+06/5.80e+07 =  2% of the original kernel matrix.

torch.Size([5499, 2])
We keep 2.57e+05/4.79e+06 =  5% of the original kernel matrix.

torch.Size([14117, 2])
We keep 1.17e+06/4.30e+07 =  2% of the original kernel matrix.

torch.Size([9768, 2])
We keep 6.59e+05/1.73e+07 =  3% of the original kernel matrix.

torch.Size([18143, 2])
We keep 1.86e+06/8.17e+07 =  2% of the original kernel matrix.

torch.Size([11846, 2])
We keep 1.26e+06/3.47e+07 =  3% of the original kernel matrix.

torch.Size([19765, 2])
We keep 2.45e+06/1.16e+08 =  2% of the original kernel matrix.

torch.Size([11884, 2])
We keep 1.16e+06/3.33e+07 =  3% of the original kernel matrix.

torch.Size([19936, 2])
We keep 2.41e+06/1.13e+08 =  2% of the original kernel matrix.

torch.Size([5616, 2])
We keep 2.83e+05/5.50e+06 =  5% of the original kernel matrix.

torch.Size([14006, 2])
We keep 1.22e+06/4.60e+07 =  2% of the original kernel matrix.

torch.Size([6950, 2])
We keep 6.09e+05/9.31e+06 =  6% of the original kernel matrix.

torch.Size([15380, 2])
We keep 1.49e+06/5.99e+07 =  2% of the original kernel matrix.

torch.Size([8471, 2])
We keep 5.83e+05/1.39e+07 =  4% of the original kernel matrix.

torch.Size([16751, 2])
We keep 1.72e+06/7.31e+07 =  2% of the original kernel matrix.

torch.Size([12175, 2])
We keep 1.01e+06/3.09e+07 =  3% of the original kernel matrix.

torch.Size([20177, 2])
We keep 2.30e+06/1.09e+08 =  2% of the original kernel matrix.

torch.Size([5248, 2])
We keep 2.86e+05/4.80e+06 =  5% of the original kernel matrix.

torch.Size([13791, 2])
We keep 1.20e+06/4.30e+07 =  2% of the original kernel matrix.

torch.Size([9384, 2])
We keep 8.77e+05/2.08e+07 =  4% of the original kernel matrix.

torch.Size([17636, 2])
We keep 2.04e+06/8.96e+07 =  2% of the original kernel matrix.

torch.Size([12015, 2])
We keep 1.20e+06/3.30e+07 =  3% of the original kernel matrix.

torch.Size([20311, 2])
We keep 2.41e+06/1.13e+08 =  2% of the original kernel matrix.

torch.Size([3830, 2])
We keep 1.38e+05/2.12e+06 =  6% of the original kernel matrix.

torch.Size([12192, 2])
We keep 8.87e+05/2.86e+07 =  3% of the original kernel matrix.

torch.Size([2211, 2])
We keep 4.81e+04/6.27e+05 =  7% of the original kernel matrix.

torch.Size([9999, 2])
We keep 5.83e+05/1.56e+07 =  3% of the original kernel matrix.

torch.Size([8693, 2])
We keep 5.35e+05/1.29e+07 =  4% of the original kernel matrix.

torch.Size([17067, 2])
We keep 1.65e+06/7.05e+07 =  2% of the original kernel matrix.

torch.Size([23664, 2])
We keep 4.43e+06/1.86e+08 =  2% of the original kernel matrix.

torch.Size([28640, 2])
We keep 4.62e+06/2.68e+08 =  1% of the original kernel matrix.

torch.Size([7195, 2])
We keep 4.79e+05/9.88e+06 =  4% of the original kernel matrix.

torch.Size([15731, 2])
We keep 1.56e+06/6.17e+07 =  2% of the original kernel matrix.

torch.Size([4563, 2])
We keep 2.42e+05/4.05e+06 =  5% of the original kernel matrix.

torch.Size([12916, 2])
We keep 1.12e+06/3.95e+07 =  2% of the original kernel matrix.

torch.Size([12044, 2])
We keep 1.62e+06/4.33e+07 =  3% of the original kernel matrix.

torch.Size([20214, 2])
We keep 2.71e+06/1.29e+08 =  2% of the original kernel matrix.

torch.Size([5496, 2])
We keep 3.15e+05/5.59e+06 =  5% of the original kernel matrix.

torch.Size([13863, 2])
We keep 1.26e+06/4.64e+07 =  2% of the original kernel matrix.

torch.Size([6319, 2])
We keep 4.15e+05/7.97e+06 =  5% of the original kernel matrix.

torch.Size([14884, 2])
We keep 1.44e+06/5.55e+07 =  2% of the original kernel matrix.

torch.Size([4179, 2])
We keep 1.52e+05/2.61e+06 =  5% of the original kernel matrix.

torch.Size([12674, 2])
We keep 9.46e+05/3.17e+07 =  2% of the original kernel matrix.

torch.Size([10845, 2])
We keep 9.70e+05/2.46e+07 =  3% of the original kernel matrix.

torch.Size([18987, 2])
We keep 2.12e+06/9.74e+07 =  2% of the original kernel matrix.

torch.Size([4610, 2])
We keep 1.90e+05/3.16e+06 =  5% of the original kernel matrix.

torch.Size([13049, 2])
We keep 1.02e+06/3.49e+07 =  2% of the original kernel matrix.

torch.Size([9937, 2])
We keep 9.21e+05/2.35e+07 =  3% of the original kernel matrix.

torch.Size([18310, 2])
We keep 2.14e+06/9.52e+07 =  2% of the original kernel matrix.

torch.Size([6916, 2])
We keep 3.83e+05/8.05e+06 =  4% of the original kernel matrix.

torch.Size([15469, 2])
We keep 1.41e+06/5.57e+07 =  2% of the original kernel matrix.

torch.Size([8646, 2])
We keep 6.12e+05/1.55e+07 =  3% of the original kernel matrix.

torch.Size([16837, 2])
We keep 1.78e+06/7.72e+07 =  2% of the original kernel matrix.

torch.Size([5897, 2])
We keep 2.97e+05/5.64e+06 =  5% of the original kernel matrix.

torch.Size([14354, 2])
We keep 1.24e+06/4.66e+07 =  2% of the original kernel matrix.

torch.Size([3222, 2])
We keep 1.13e+05/1.75e+06 =  6% of the original kernel matrix.

torch.Size([11353, 2])
We keep 8.32e+05/2.60e+07 =  3% of the original kernel matrix.

torch.Size([8666, 2])
We keep 7.97e+05/1.84e+07 =  4% of the original kernel matrix.

torch.Size([17209, 2])
We keep 1.98e+06/8.42e+07 =  2% of the original kernel matrix.

torch.Size([8307, 2])
We keep 6.58e+05/1.44e+07 =  4% of the original kernel matrix.

torch.Size([16521, 2])
We keep 1.74e+06/7.45e+07 =  2% of the original kernel matrix.

torch.Size([9659, 2])
We keep 7.00e+05/1.78e+07 =  3% of the original kernel matrix.

torch.Size([18085, 2])
We keep 1.89e+06/8.29e+07 =  2% of the original kernel matrix.

torch.Size([5603, 2])
We keep 3.04e+05/5.35e+06 =  5% of the original kernel matrix.

torch.Size([14377, 2])
We keep 1.24e+06/4.54e+07 =  2% of the original kernel matrix.

torch.Size([13270, 2])
We keep 1.78e+06/4.61e+07 =  3% of the original kernel matrix.

torch.Size([21025, 2])
We keep 2.72e+06/1.33e+08 =  2% of the original kernel matrix.

torch.Size([9940, 2])
We keep 1.05e+06/2.73e+07 =  3% of the original kernel matrix.

torch.Size([18366, 2])
We keep 2.30e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([6515, 2])
We keep 3.54e+05/7.55e+06 =  4% of the original kernel matrix.

torch.Size([15121, 2])
We keep 1.36e+06/5.40e+07 =  2% of the original kernel matrix.

torch.Size([20762, 2])
We keep 3.54e+06/1.49e+08 =  2% of the original kernel matrix.

torch.Size([27109, 2])
We keep 4.32e+06/2.40e+08 =  1% of the original kernel matrix.

torch.Size([9578, 2])
We keep 7.98e+05/1.97e+07 =  4% of the original kernel matrix.

torch.Size([18242, 2])
We keep 2.00e+06/8.71e+07 =  2% of the original kernel matrix.

torch.Size([16417, 2])
We keep 2.06e+06/7.78e+07 =  2% of the original kernel matrix.

torch.Size([23736, 2])
We keep 3.31e+06/1.73e+08 =  1% of the original kernel matrix.

torch.Size([4501, 2])
We keep 2.07e+05/3.33e+06 =  6% of the original kernel matrix.

torch.Size([12841, 2])
We keep 1.04e+06/3.58e+07 =  2% of the original kernel matrix.

torch.Size([8128, 2])
We keep 6.12e+05/1.45e+07 =  4% of the original kernel matrix.

torch.Size([16707, 2])
We keep 1.78e+06/7.49e+07 =  2% of the original kernel matrix.

torch.Size([6639, 2])
We keep 4.48e+05/8.63e+06 =  5% of the original kernel matrix.

torch.Size([15228, 2])
We keep 1.49e+06/5.77e+07 =  2% of the original kernel matrix.

torch.Size([11815, 2])
We keep 1.13e+06/3.26e+07 =  3% of the original kernel matrix.

torch.Size([20045, 2])
We keep 2.39e+06/1.12e+08 =  2% of the original kernel matrix.

torch.Size([2283, 2])
We keep 5.84e+04/7.11e+05 =  8% of the original kernel matrix.

torch.Size([9988, 2])
We keep 6.03e+05/1.66e+07 =  3% of the original kernel matrix.

torch.Size([7343, 2])
We keep 4.27e+05/9.43e+06 =  4% of the original kernel matrix.

torch.Size([16086, 2])
We keep 1.51e+06/6.03e+07 =  2% of the original kernel matrix.

torch.Size([3484, 2])
We keep 1.21e+05/1.84e+06 =  6% of the original kernel matrix.

torch.Size([11713, 2])
We keep 8.39e+05/2.67e+07 =  3% of the original kernel matrix.

torch.Size([3858, 2])
We keep 1.33e+05/2.20e+06 =  6% of the original kernel matrix.

torch.Size([12217, 2])
We keep 8.93e+05/2.91e+07 =  3% of the original kernel matrix.

torch.Size([5315, 2])
We keep 3.10e+05/5.29e+06 =  5% of the original kernel matrix.

torch.Size([13792, 2])
We keep 1.25e+06/4.52e+07 =  2% of the original kernel matrix.

torch.Size([3905, 2])
We keep 1.68e+05/2.54e+06 =  6% of the original kernel matrix.

torch.Size([12179, 2])
We keep 9.46e+05/3.13e+07 =  3% of the original kernel matrix.

torch.Size([9039, 2])
We keep 7.71e+05/1.59e+07 =  4% of the original kernel matrix.

torch.Size([17564, 2])
We keep 1.79e+06/7.84e+07 =  2% of the original kernel matrix.

torch.Size([9398, 2])
We keep 7.92e+05/1.87e+07 =  4% of the original kernel matrix.

torch.Size([17668, 2])
We keep 1.91e+06/8.49e+07 =  2% of the original kernel matrix.

torch.Size([3893, 2])
We keep 1.55e+05/2.45e+06 =  6% of the original kernel matrix.

torch.Size([12115, 2])
We keep 9.42e+05/3.07e+07 =  3% of the original kernel matrix.

torch.Size([5669, 2])
We keep 2.54e+05/4.91e+06 =  5% of the original kernel matrix.

torch.Size([14254, 2])
We keep 1.17e+06/4.35e+07 =  2% of the original kernel matrix.

torch.Size([4253, 2])
We keep 1.59e+05/2.79e+06 =  5% of the original kernel matrix.

torch.Size([12753, 2])
We keep 9.69e+05/3.28e+07 =  2% of the original kernel matrix.

torch.Size([13426, 2])
We keep 1.77e+06/5.18e+07 =  3% of the original kernel matrix.

torch.Size([21639, 2])
We keep 2.91e+06/1.41e+08 =  2% of the original kernel matrix.

torch.Size([3920, 2])
We keep 1.55e+05/2.50e+06 =  6% of the original kernel matrix.

torch.Size([12180, 2])
We keep 9.36e+05/3.11e+07 =  3% of the original kernel matrix.

torch.Size([18058, 2])
We keep 2.81e+06/1.09e+08 =  2% of the original kernel matrix.

torch.Size([25393, 2])
We keep 3.79e+06/2.05e+08 =  1% of the original kernel matrix.

torch.Size([5529, 2])
We keep 3.17e+05/6.09e+06 =  5% of the original kernel matrix.

torch.Size([13934, 2])
We keep 1.29e+06/4.84e+07 =  2% of the original kernel matrix.

torch.Size([4635, 2])
We keep 1.92e+05/3.23e+06 =  5% of the original kernel matrix.

torch.Size([13148, 2])
We keep 1.02e+06/3.53e+07 =  2% of the original kernel matrix.

torch.Size([6329, 2])
We keep 3.34e+05/6.41e+06 =  5% of the original kernel matrix.

torch.Size([15035, 2])
We keep 1.29e+06/4.97e+07 =  2% of the original kernel matrix.

torch.Size([5476, 2])
We keep 2.37e+05/4.41e+06 =  5% of the original kernel matrix.

torch.Size([14157, 2])
We keep 1.12e+06/4.12e+07 =  2% of the original kernel matrix.

torch.Size([6127, 2])
We keep 3.17e+05/6.12e+06 =  5% of the original kernel matrix.

torch.Size([14562, 2])
We keep 1.29e+06/4.86e+07 =  2% of the original kernel matrix.

torch.Size([4659, 2])
We keep 1.83e+05/3.18e+06 =  5% of the original kernel matrix.

torch.Size([13212, 2])
We keep 1.01e+06/3.50e+07 =  2% of the original kernel matrix.

torch.Size([7211, 2])
We keep 4.16e+05/9.17e+06 =  4% of the original kernel matrix.

torch.Size([15754, 2])
We keep 1.48e+06/5.95e+07 =  2% of the original kernel matrix.

torch.Size([23086, 2])
We keep 4.38e+06/2.02e+08 =  2% of the original kernel matrix.

torch.Size([28825, 2])
We keep 4.86e+06/2.79e+08 =  1% of the original kernel matrix.

torch.Size([6514, 2])
We keep 3.32e+05/6.96e+06 =  4% of the original kernel matrix.

torch.Size([15166, 2])
We keep 1.33e+06/5.18e+07 =  2% of the original kernel matrix.

torch.Size([16037, 2])
We keep 1.95e+06/7.21e+07 =  2% of the original kernel matrix.

torch.Size([23418, 2])
We keep 3.18e+06/1.67e+08 =  1% of the original kernel matrix.

torch.Size([4531, 2])
We keep 2.24e+05/3.66e+06 =  6% of the original kernel matrix.

torch.Size([12876, 2])
We keep 1.07e+06/3.76e+07 =  2% of the original kernel matrix.

torch.Size([8617, 2])
We keep 6.65e+05/1.54e+07 =  4% of the original kernel matrix.

torch.Size([16974, 2])
We keep 1.81e+06/7.70e+07 =  2% of the original kernel matrix.

torch.Size([4023, 2])
We keep 2.36e+05/3.22e+06 =  7% of the original kernel matrix.

torch.Size([12133, 2])
We keep 1.05e+06/3.52e+07 =  2% of the original kernel matrix.

torch.Size([7158, 2])
We keep 4.20e+05/8.95e+06 =  4% of the original kernel matrix.

torch.Size([15622, 2])
We keep 1.47e+06/5.87e+07 =  2% of the original kernel matrix.

torch.Size([5393, 2])
We keep 2.51e+05/4.96e+06 =  5% of the original kernel matrix.

torch.Size([14023, 2])
We keep 1.20e+06/4.37e+07 =  2% of the original kernel matrix.

torch.Size([4576, 2])
We keep 2.01e+05/3.45e+06 =  5% of the original kernel matrix.

torch.Size([13065, 2])
We keep 1.04e+06/3.65e+07 =  2% of the original kernel matrix.

torch.Size([4101, 2])
We keep 1.96e+05/3.03e+06 =  6% of the original kernel matrix.

torch.Size([12356, 2])
We keep 1.02e+06/3.42e+07 =  2% of the original kernel matrix.

torch.Size([5141, 2])
We keep 2.42e+05/4.39e+06 =  5% of the original kernel matrix.

torch.Size([13583, 2])
We keep 1.14e+06/4.11e+07 =  2% of the original kernel matrix.

torch.Size([5471, 2])
We keep 2.67e+05/4.92e+06 =  5% of the original kernel matrix.

torch.Size([14090, 2])
We keep 1.17e+06/4.35e+07 =  2% of the original kernel matrix.

torch.Size([8375, 2])
We keep 6.05e+05/1.43e+07 =  4% of the original kernel matrix.

torch.Size([16779, 2])
We keep 1.74e+06/7.42e+07 =  2% of the original kernel matrix.

torch.Size([3433, 2])
We keep 1.21e+05/1.90e+06 =  6% of the original kernel matrix.

torch.Size([11840, 2])
We keep 8.69e+05/2.70e+07 =  3% of the original kernel matrix.

torch.Size([16009, 2])
We keep 2.74e+06/9.59e+07 =  2% of the original kernel matrix.

torch.Size([23342, 2])
We keep 3.67e+06/1.92e+08 =  1% of the original kernel matrix.

torch.Size([17201, 2])
We keep 3.16e+06/1.03e+08 =  3% of the original kernel matrix.

torch.Size([24647, 2])
We keep 3.77e+06/1.99e+08 =  1% of the original kernel matrix.

torch.Size([15075, 2])
We keep 2.00e+06/6.85e+07 =  2% of the original kernel matrix.

torch.Size([23391, 2])
We keep 3.23e+06/1.63e+08 =  1% of the original kernel matrix.

torch.Size([6293, 2])
We keep 3.19e+05/6.65e+06 =  4% of the original kernel matrix.

torch.Size([14771, 2])
We keep 1.31e+06/5.06e+07 =  2% of the original kernel matrix.

torch.Size([5085, 2])
We keep 2.65e+05/4.58e+06 =  5% of the original kernel matrix.

torch.Size([13726, 2])
We keep 1.18e+06/4.20e+07 =  2% of the original kernel matrix.

torch.Size([4930, 2])
We keep 2.84e+05/4.77e+06 =  5% of the original kernel matrix.

torch.Size([13982, 2])
We keep 1.22e+06/4.29e+07 =  2% of the original kernel matrix.

torch.Size([4407, 2])
We keep 1.78e+05/2.99e+06 =  5% of the original kernel matrix.

torch.Size([12872, 2])
We keep 9.94e+05/3.40e+07 =  2% of the original kernel matrix.

torch.Size([6496, 2])
We keep 3.68e+05/7.61e+06 =  4% of the original kernel matrix.

torch.Size([15046, 2])
We keep 1.38e+06/5.42e+07 =  2% of the original kernel matrix.

torch.Size([3909, 2])
We keep 1.48e+05/2.40e+06 =  6% of the original kernel matrix.

torch.Size([12092, 2])
We keep 9.13e+05/3.04e+07 =  3% of the original kernel matrix.

torch.Size([8094, 2])
We keep 6.87e+05/1.48e+07 =  4% of the original kernel matrix.

torch.Size([16777, 2])
We keep 1.83e+06/7.55e+07 =  2% of the original kernel matrix.

torch.Size([10667, 2])
We keep 9.52e+05/2.44e+07 =  3% of the original kernel matrix.

torch.Size([18850, 2])
We keep 2.15e+06/9.71e+07 =  2% of the original kernel matrix.

torch.Size([17426, 2])
We keep 3.47e+06/1.08e+08 =  3% of the original kernel matrix.

torch.Size([24562, 2])
We keep 3.81e+06/2.04e+08 =  1% of the original kernel matrix.

torch.Size([7028, 2])
We keep 3.92e+05/8.47e+06 =  4% of the original kernel matrix.

torch.Size([15482, 2])
We keep 1.43e+06/5.71e+07 =  2% of the original kernel matrix.

torch.Size([26078, 2])
We keep 6.49e+06/2.60e+08 =  2% of the original kernel matrix.

torch.Size([30331, 2])
We keep 5.46e+06/3.16e+08 =  1% of the original kernel matrix.

torch.Size([6770, 2])
We keep 4.08e+05/8.80e+06 =  4% of the original kernel matrix.

torch.Size([15306, 2])
We keep 1.48e+06/5.83e+07 =  2% of the original kernel matrix.

torch.Size([7352, 2])
We keep 6.26e+05/1.17e+07 =  5% of the original kernel matrix.

torch.Size([15958, 2])
We keep 1.68e+06/6.71e+07 =  2% of the original kernel matrix.

torch.Size([10177, 2])
We keep 9.35e+05/2.53e+07 =  3% of the original kernel matrix.

torch.Size([18621, 2])
We keep 2.20e+06/9.88e+07 =  2% of the original kernel matrix.

torch.Size([2021, 2])
We keep 4.14e+04/5.07e+05 =  8% of the original kernel matrix.

torch.Size([9782, 2])
We keep 5.60e+05/1.40e+07 =  4% of the original kernel matrix.

torch.Size([16815, 2])
We keep 2.45e+06/9.22e+07 =  2% of the original kernel matrix.

torch.Size([24659, 2])
We keep 3.60e+06/1.89e+08 =  1% of the original kernel matrix.

torch.Size([6276, 2])
We keep 3.98e+05/7.58e+06 =  5% of the original kernel matrix.

torch.Size([14718, 2])
We keep 1.38e+06/5.41e+07 =  2% of the original kernel matrix.

torch.Size([8572, 2])
We keep 7.30e+05/1.71e+07 =  4% of the original kernel matrix.

torch.Size([17142, 2])
We keep 1.92e+06/8.13e+07 =  2% of the original kernel matrix.

torch.Size([3749, 2])
We keep 1.32e+05/2.04e+06 =  6% of the original kernel matrix.

torch.Size([12128, 2])
We keep 8.82e+05/2.80e+07 =  3% of the original kernel matrix.

torch.Size([6566, 2])
We keep 4.05e+05/8.39e+06 =  4% of the original kernel matrix.

torch.Size([14943, 2])
We keep 1.44e+06/5.69e+07 =  2% of the original kernel matrix.

torch.Size([3400, 2])
We keep 1.15e+05/1.77e+06 =  6% of the original kernel matrix.

torch.Size([11789, 2])
We keep 8.51e+05/2.62e+07 =  3% of the original kernel matrix.

torch.Size([7602, 2])
We keep 6.10e+05/1.30e+07 =  4% of the original kernel matrix.

torch.Size([16348, 2])
We keep 1.73e+06/7.07e+07 =  2% of the original kernel matrix.

torch.Size([12199, 2])
We keep 1.25e+06/3.64e+07 =  3% of the original kernel matrix.

torch.Size([20204, 2])
We keep 2.50e+06/1.19e+08 =  2% of the original kernel matrix.

torch.Size([9784, 2])
We keep 9.10e+05/2.33e+07 =  3% of the original kernel matrix.

torch.Size([18219, 2])
We keep 2.15e+06/9.48e+07 =  2% of the original kernel matrix.

torch.Size([2397, 2])
We keep 5.99e+04/7.76e+05 =  7% of the original kernel matrix.

torch.Size([10207, 2])
We keep 6.36e+05/1.73e+07 =  3% of the original kernel matrix.

torch.Size([3767, 2])
We keep 1.28e+05/1.98e+06 =  6% of the original kernel matrix.

torch.Size([12008, 2])
We keep 8.59e+05/2.76e+07 =  3% of the original kernel matrix.

torch.Size([2898, 2])
We keep 8.63e+04/1.23e+06 =  7% of the original kernel matrix.

torch.Size([10806, 2])
We keep 7.36e+05/2.17e+07 =  3% of the original kernel matrix.

torch.Size([13282, 2])
We keep 1.23e+06/3.87e+07 =  3% of the original kernel matrix.

torch.Size([21061, 2])
We keep 2.50e+06/1.22e+08 =  2% of the original kernel matrix.

torch.Size([4993, 2])
We keep 3.64e+05/4.96e+06 =  7% of the original kernel matrix.

torch.Size([13541, 2])
We keep 1.23e+06/4.37e+07 =  2% of the original kernel matrix.

torch.Size([9166, 2])
We keep 8.79e+05/1.87e+07 =  4% of the original kernel matrix.

torch.Size([17699, 2])
We keep 1.94e+06/8.48e+07 =  2% of the original kernel matrix.

torch.Size([7541, 2])
We keep 6.77e+05/1.30e+07 =  5% of the original kernel matrix.

torch.Size([16032, 2])
We keep 1.69e+06/7.07e+07 =  2% of the original kernel matrix.

torch.Size([12107, 2])
We keep 1.24e+06/3.53e+07 =  3% of the original kernel matrix.

torch.Size([20024, 2])
We keep 2.44e+06/1.17e+08 =  2% of the original kernel matrix.

torch.Size([5939, 2])
We keep 2.94e+05/5.69e+06 =  5% of the original kernel matrix.

torch.Size([14496, 2])
We keep 1.24e+06/4.68e+07 =  2% of the original kernel matrix.

torch.Size([12724, 2])
We keep 1.56e+06/4.38e+07 =  3% of the original kernel matrix.

torch.Size([20647, 2])
We keep 2.69e+06/1.30e+08 =  2% of the original kernel matrix.

torch.Size([6585, 2])
We keep 3.77e+05/7.77e+06 =  4% of the original kernel matrix.

torch.Size([14952, 2])
We keep 1.38e+06/5.47e+07 =  2% of the original kernel matrix.

torch.Size([8306, 2])
We keep 6.01e+05/1.33e+07 =  4% of the original kernel matrix.

torch.Size([17068, 2])
We keep 1.70e+06/7.15e+07 =  2% of the original kernel matrix.

torch.Size([5394, 2])
We keep 2.92e+05/5.40e+06 =  5% of the original kernel matrix.

torch.Size([13780, 2])
We keep 1.23e+06/4.56e+07 =  2% of the original kernel matrix.

torch.Size([10969, 2])
We keep 1.44e+06/3.23e+07 =  4% of the original kernel matrix.

torch.Size([19072, 2])
We keep 2.38e+06/1.12e+08 =  2% of the original kernel matrix.

torch.Size([4770, 2])
We keep 2.15e+05/3.85e+06 =  5% of the original kernel matrix.

torch.Size([13182, 2])
We keep 1.10e+06/3.85e+07 =  2% of the original kernel matrix.

torch.Size([10793, 2])
We keep 1.02e+06/2.69e+07 =  3% of the original kernel matrix.

torch.Size([19185, 2])
We keep 2.21e+06/1.02e+08 =  2% of the original kernel matrix.

torch.Size([10817, 2])
We keep 9.29e+05/2.62e+07 =  3% of the original kernel matrix.

torch.Size([19221, 2])
We keep 2.20e+06/1.00e+08 =  2% of the original kernel matrix.

torch.Size([6747, 2])
We keep 4.18e+05/8.69e+06 =  4% of the original kernel matrix.

torch.Size([15289, 2])
We keep 1.47e+06/5.79e+07 =  2% of the original kernel matrix.

torch.Size([13676, 2])
We keep 1.56e+06/4.65e+07 =  3% of the original kernel matrix.

torch.Size([21325, 2])
We keep 2.71e+06/1.34e+08 =  2% of the original kernel matrix.

torch.Size([10180, 2])
We keep 1.03e+06/2.70e+07 =  3% of the original kernel matrix.

torch.Size([18624, 2])
We keep 2.26e+06/1.02e+08 =  2% of the original kernel matrix.

torch.Size([19092, 2])
We keep 4.16e+06/1.49e+08 =  2% of the original kernel matrix.

torch.Size([26100, 2])
We keep 4.36e+06/2.40e+08 =  1% of the original kernel matrix.

torch.Size([6205, 2])
We keep 3.16e+05/6.39e+06 =  4% of the original kernel matrix.

torch.Size([14809, 2])
We keep 1.31e+06/4.96e+07 =  2% of the original kernel matrix.

torch.Size([2143, 2])
We keep 4.72e+04/5.67e+05 =  8% of the original kernel matrix.

torch.Size([9886, 2])
We keep 5.75e+05/1.48e+07 =  3% of the original kernel matrix.

torch.Size([8790, 2])
We keep 5.39e+05/1.30e+07 =  4% of the original kernel matrix.

torch.Size([17278, 2])
We keep 1.67e+06/7.07e+07 =  2% of the original kernel matrix.

torch.Size([6328, 2])
We keep 3.34e+05/6.60e+06 =  5% of the original kernel matrix.

torch.Size([14939, 2])
We keep 1.32e+06/5.05e+07 =  2% of the original kernel matrix.

torch.Size([7872, 2])
We keep 6.14e+05/1.23e+07 =  4% of the original kernel matrix.

torch.Size([16297, 2])
We keep 1.66e+06/6.89e+07 =  2% of the original kernel matrix.

torch.Size([7671, 2])
We keep 6.02e+05/1.36e+07 =  4% of the original kernel matrix.

torch.Size([16209, 2])
We keep 1.77e+06/7.25e+07 =  2% of the original kernel matrix.

torch.Size([4764, 2])
We keep 2.08e+05/3.80e+06 =  5% of the original kernel matrix.

torch.Size([13308, 2])
We keep 1.07e+06/3.83e+07 =  2% of the original kernel matrix.

torch.Size([3775, 2])
We keep 1.88e+05/2.68e+06 =  7% of the original kernel matrix.

torch.Size([12013, 2])
We keep 9.96e+05/3.21e+07 =  3% of the original kernel matrix.

torch.Size([6742, 2])
We keep 3.96e+05/8.21e+06 =  4% of the original kernel matrix.

torch.Size([15097, 2])
We keep 1.43e+06/5.63e+07 =  2% of the original kernel matrix.

torch.Size([3869, 2])
We keep 1.39e+05/2.19e+06 =  6% of the original kernel matrix.

torch.Size([12435, 2])
We keep 9.01e+05/2.90e+07 =  3% of the original kernel matrix.

torch.Size([9002, 2])
We keep 7.65e+05/1.92e+07 =  3% of the original kernel matrix.

torch.Size([17720, 2])
We keep 1.99e+06/8.59e+07 =  2% of the original kernel matrix.

torch.Size([4914, 2])
We keep 2.09e+05/3.83e+06 =  5% of the original kernel matrix.

torch.Size([13320, 2])
We keep 1.08e+06/3.84e+07 =  2% of the original kernel matrix.

torch.Size([16582, 2])
We keep 2.20e+06/8.28e+07 =  2% of the original kernel matrix.

torch.Size([24129, 2])
We keep 3.39e+06/1.79e+08 =  1% of the original kernel matrix.

torch.Size([8784, 2])
We keep 6.75e+05/1.57e+07 =  4% of the original kernel matrix.

torch.Size([17162, 2])
We keep 1.81e+06/7.79e+07 =  2% of the original kernel matrix.

torch.Size([7573, 2])
We keep 4.42e+05/9.70e+06 =  4% of the original kernel matrix.

torch.Size([16102, 2])
We keep 1.50e+06/6.11e+07 =  2% of the original kernel matrix.

torch.Size([4114, 2])
We keep 1.42e+05/2.33e+06 =  6% of the original kernel matrix.

torch.Size([12583, 2])
We keep 9.09e+05/3.00e+07 =  3% of the original kernel matrix.

torch.Size([10290, 2])
We keep 1.35e+06/2.97e+07 =  4% of the original kernel matrix.

torch.Size([18875, 2])
We keep 2.39e+06/1.07e+08 =  2% of the original kernel matrix.

torch.Size([7753, 2])
We keep 4.87e+05/1.07e+07 =  4% of the original kernel matrix.

torch.Size([16586, 2])
We keep 1.58e+06/6.43e+07 =  2% of the original kernel matrix.

torch.Size([11788, 2])
We keep 1.21e+06/3.36e+07 =  3% of the original kernel matrix.

torch.Size([19834, 2])
We keep 2.45e+06/1.14e+08 =  2% of the original kernel matrix.

torch.Size([13688, 2])
We keep 1.44e+06/4.66e+07 =  3% of the original kernel matrix.

torch.Size([21462, 2])
We keep 2.72e+06/1.34e+08 =  2% of the original kernel matrix.

torch.Size([9403, 2])
We keep 7.24e+05/1.84e+07 =  3% of the original kernel matrix.

torch.Size([17712, 2])
We keep 1.91e+06/8.43e+07 =  2% of the original kernel matrix.

torch.Size([10095, 2])
We keep 1.02e+06/2.37e+07 =  4% of the original kernel matrix.

torch.Size([18196, 2])
We keep 2.10e+06/9.57e+07 =  2% of the original kernel matrix.

torch.Size([9705, 2])
We keep 7.58e+05/1.92e+07 =  3% of the original kernel matrix.

torch.Size([17808, 2])
We keep 1.93e+06/8.61e+07 =  2% of the original kernel matrix.

torch.Size([7888, 2])
We keep 5.10e+05/1.16e+07 =  4% of the original kernel matrix.

torch.Size([16223, 2])
We keep 1.61e+06/6.69e+07 =  2% of the original kernel matrix.

torch.Size([2590, 2])
We keep 8.31e+04/1.13e+06 =  7% of the original kernel matrix.

torch.Size([10521, 2])
We keep 7.27e+05/2.08e+07 =  3% of the original kernel matrix.

torch.Size([5279, 2])
We keep 2.45e+05/4.31e+06 =  5% of the original kernel matrix.

torch.Size([13862, 2])
We keep 1.14e+06/4.08e+07 =  2% of the original kernel matrix.

torch.Size([3210, 2])
We keep 1.03e+05/1.46e+06 =  7% of the original kernel matrix.

torch.Size([11453, 2])
We keep 7.81e+05/2.38e+07 =  3% of the original kernel matrix.

torch.Size([3029, 2])
We keep 8.17e+04/1.18e+06 =  6% of the original kernel matrix.

torch.Size([11173, 2])
We keep 7.20e+05/2.13e+07 =  3% of the original kernel matrix.

torch.Size([11443, 2])
We keep 1.06e+06/2.78e+07 =  3% of the original kernel matrix.

torch.Size([19488, 2])
We keep 2.23e+06/1.04e+08 =  2% of the original kernel matrix.

torch.Size([5764, 2])
We keep 2.83e+05/5.65e+06 =  4% of the original kernel matrix.

torch.Size([14351, 2])
We keep 1.24e+06/4.67e+07 =  2% of the original kernel matrix.

torch.Size([5645, 2])
We keep 3.49e+05/5.97e+06 =  5% of the original kernel matrix.

torch.Size([14078, 2])
We keep 1.28e+06/4.80e+07 =  2% of the original kernel matrix.

torch.Size([5081, 2])
We keep 2.39e+05/4.26e+06 =  5% of the original kernel matrix.

torch.Size([13727, 2])
We keep 1.15e+06/4.05e+07 =  2% of the original kernel matrix.

torch.Size([9282, 2])
We keep 7.88e+05/1.93e+07 =  4% of the original kernel matrix.

torch.Size([17427, 2])
We keep 1.95e+06/8.62e+07 =  2% of the original kernel matrix.

torch.Size([6676, 2])
We keep 4.05e+05/8.12e+06 =  4% of the original kernel matrix.

torch.Size([15094, 2])
We keep 1.41e+06/5.60e+07 =  2% of the original kernel matrix.

torch.Size([6706, 2])
We keep 5.51e+05/1.05e+07 =  5% of the original kernel matrix.

torch.Size([15283, 2])
We keep 1.62e+06/6.38e+07 =  2% of the original kernel matrix.

torch.Size([7400, 2])
We keep 5.97e+05/1.25e+07 =  4% of the original kernel matrix.

torch.Size([16025, 2])
We keep 1.71e+06/6.94e+07 =  2% of the original kernel matrix.

torch.Size([4000, 2])
We keep 1.48e+05/2.47e+06 =  6% of the original kernel matrix.

torch.Size([12260, 2])
We keep 9.19e+05/3.08e+07 =  2% of the original kernel matrix.

torch.Size([6475, 2])
We keep 3.92e+05/7.65e+06 =  5% of the original kernel matrix.

torch.Size([14985, 2])
We keep 1.39e+06/5.43e+07 =  2% of the original kernel matrix.

torch.Size([13221, 2])
We keep 1.29e+06/4.05e+07 =  3% of the original kernel matrix.

torch.Size([21008, 2])
We keep 2.55e+06/1.25e+08 =  2% of the original kernel matrix.

torch.Size([2227, 2])
We keep 5.89e+04/7.06e+05 =  8% of the original kernel matrix.

torch.Size([9819, 2])
We keep 6.15e+05/1.65e+07 =  3% of the original kernel matrix.

torch.Size([4160, 2])
We keep 1.53e+05/2.69e+06 =  5% of the original kernel matrix.

torch.Size([12633, 2])
We keep 9.48e+05/3.22e+07 =  2% of the original kernel matrix.

torch.Size([6957, 2])
We keep 4.07e+05/8.26e+06 =  4% of the original kernel matrix.

torch.Size([15477, 2])
We keep 1.42e+06/5.64e+07 =  2% of the original kernel matrix.

torch.Size([7666, 2])
We keep 5.86e+05/1.23e+07 =  4% of the original kernel matrix.

torch.Size([16256, 2])
We keep 1.70e+06/6.90e+07 =  2% of the original kernel matrix.

torch.Size([8689, 2])
We keep 8.10e+05/1.88e+07 =  4% of the original kernel matrix.

torch.Size([17385, 2])
We keep 2.00e+06/8.51e+07 =  2% of the original kernel matrix.

torch.Size([4940, 2])
We keep 2.21e+05/4.07e+06 =  5% of the original kernel matrix.

torch.Size([13469, 2])
We keep 1.11e+06/3.96e+07 =  2% of the original kernel matrix.

torch.Size([5354, 2])
We keep 2.82e+05/4.75e+06 =  5% of the original kernel matrix.

torch.Size([13739, 2])
We keep 1.18e+06/4.28e+07 =  2% of the original kernel matrix.

torch.Size([9529, 2])
We keep 6.72e+05/1.65e+07 =  4% of the original kernel matrix.

torch.Size([18032, 2])
We keep 1.82e+06/7.99e+07 =  2% of the original kernel matrix.

torch.Size([14031, 2])
We keep 1.97e+06/6.02e+07 =  3% of the original kernel matrix.

torch.Size([22210, 2])
We keep 3.06e+06/1.52e+08 =  2% of the original kernel matrix.

torch.Size([4514, 2])
We keep 1.98e+05/3.37e+06 =  5% of the original kernel matrix.

torch.Size([13041, 2])
We keep 1.06e+06/3.60e+07 =  2% of the original kernel matrix.

torch.Size([22487, 2])
We keep 5.73e+06/1.82e+08 =  3% of the original kernel matrix.

torch.Size([27845, 2])
We keep 4.67e+06/2.65e+08 =  1% of the original kernel matrix.

torch.Size([71313, 2])
We keep 1.19e+08/3.39e+09 =  3% of the original kernel matrix.

torch.Size([47963, 2])
We keep 1.58e+07/1.14e+09 =  1% of the original kernel matrix.

torch.Size([25917, 2])
We keep 1.86e+07/3.94e+08 =  4% of the original kernel matrix.

torch.Size([30046, 2])
We keep 6.21e+06/3.90e+08 =  1% of the original kernel matrix.

torch.Size([12129, 2])
We keep 1.47e+06/4.09e+07 =  3% of the original kernel matrix.

torch.Size([20390, 2])
We keep 2.65e+06/1.26e+08 =  2% of the original kernel matrix.

torch.Size([2337, 2])
We keep 5.17e+04/6.86e+05 =  7% of the original kernel matrix.

torch.Size([10086, 2])
We keep 5.99e+05/1.63e+07 =  3% of the original kernel matrix.

torch.Size([10408, 2])
We keep 8.17e+05/2.17e+07 =  3% of the original kernel matrix.

torch.Size([18542, 2])
We keep 2.01e+06/9.15e+07 =  2% of the original kernel matrix.

torch.Size([28815, 2])
We keep 1.45e+07/3.00e+08 =  4% of the original kernel matrix.

torch.Size([29887, 2])
We keep 4.95e+06/3.40e+08 =  1% of the original kernel matrix.

torch.Size([236854, 2])
We keep 5.96e+08/2.96e+10 =  2% of the original kernel matrix.

torch.Size([90494, 2])
We keep 3.99e+07/3.38e+09 =  1% of the original kernel matrix.

torch.Size([9004, 2])
We keep 6.39e+05/1.56e+07 =  4% of the original kernel matrix.

torch.Size([17463, 2])
We keep 1.79e+06/7.75e+07 =  2% of the original kernel matrix.

torch.Size([191953, 2])
We keep 2.61e+08/1.70e+10 =  1% of the original kernel matrix.

torch.Size([81989, 2])
We keep 3.14e+07/2.56e+09 =  1% of the original kernel matrix.

torch.Size([47761, 2])
We keep 1.77e+07/8.09e+08 =  2% of the original kernel matrix.

torch.Size([39572, 2])
We keep 8.18e+06/5.59e+08 =  1% of the original kernel matrix.

torch.Size([35232, 2])
We keep 1.53e+07/5.05e+08 =  3% of the original kernel matrix.

torch.Size([34185, 2])
We keep 6.83e+06/4.41e+08 =  1% of the original kernel matrix.

torch.Size([143265, 2])
We keep 3.55e+08/1.20e+10 =  2% of the original kernel matrix.

torch.Size([70652, 2])
We keep 2.52e+07/2.15e+09 =  1% of the original kernel matrix.

torch.Size([5406, 2])
We keep 2.45e+05/4.56e+06 =  5% of the original kernel matrix.

torch.Size([14028, 2])
We keep 1.15e+06/4.19e+07 =  2% of the original kernel matrix.

torch.Size([10685, 2])
We keep 1.00e+06/2.64e+07 =  3% of the original kernel matrix.

torch.Size([18873, 2])
We keep 2.15e+06/1.01e+08 =  2% of the original kernel matrix.

torch.Size([368055, 2])
We keep 5.59e+08/4.72e+10 =  1% of the original kernel matrix.

torch.Size([112774, 2])
We keep 4.86e+07/4.27e+09 =  1% of the original kernel matrix.

torch.Size([631610, 2])
We keep 1.45e+09/1.36e+11 =  1% of the original kernel matrix.

torch.Size([153336, 2])
We keep 7.77e+07/7.24e+09 =  1% of the original kernel matrix.

torch.Size([10674, 2])
We keep 1.09e+06/2.75e+07 =  3% of the original kernel matrix.

torch.Size([18898, 2])
We keep 2.23e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([50883, 2])
We keep 1.65e+07/9.11e+08 =  1% of the original kernel matrix.

torch.Size([41289, 2])
We keep 8.77e+06/5.93e+08 =  1% of the original kernel matrix.

torch.Size([28437, 2])
We keep 1.36e+07/3.80e+08 =  3% of the original kernel matrix.

torch.Size([30154, 2])
We keep 5.84e+06/3.83e+08 =  1% of the original kernel matrix.

torch.Size([42035, 2])
We keep 2.54e+07/8.06e+08 =  3% of the original kernel matrix.

torch.Size([37953, 2])
We keep 8.45e+06/5.57e+08 =  1% of the original kernel matrix.

torch.Size([142117, 2])
We keep 1.69e+08/8.80e+09 =  1% of the original kernel matrix.

torch.Size([68944, 2])
We keep 2.30e+07/1.84e+09 =  1% of the original kernel matrix.

torch.Size([63989, 2])
We keep 6.72e+07/2.44e+09 =  2% of the original kernel matrix.

torch.Size([44867, 2])
We keep 1.34e+07/9.69e+08 =  1% of the original kernel matrix.

torch.Size([52691, 2])
We keep 3.22e+07/1.11e+09 =  2% of the original kernel matrix.

torch.Size([42152, 2])
We keep 9.63e+06/6.55e+08 =  1% of the original kernel matrix.

torch.Size([34747, 2])
We keep 9.98e+06/3.88e+08 =  2% of the original kernel matrix.

torch.Size([34674, 2])
We keep 6.25e+06/3.87e+08 =  1% of the original kernel matrix.

torch.Size([7166, 2])
We keep 4.20e+05/9.02e+06 =  4% of the original kernel matrix.

torch.Size([15791, 2])
We keep 1.43e+06/5.90e+07 =  2% of the original kernel matrix.

torch.Size([413340, 2])
We keep 2.92e+09/1.24e+11 =  2% of the original kernel matrix.

torch.Size([116130, 2])
We keep 7.60e+07/6.93e+09 =  1% of the original kernel matrix.

torch.Size([11205, 2])
We keep 1.04e+06/2.76e+07 =  3% of the original kernel matrix.

torch.Size([19273, 2])
We keep 2.19e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([7006, 2])
We keep 3.86e+05/8.53e+06 =  4% of the original kernel matrix.

torch.Size([15720, 2])
We keep 1.43e+06/5.73e+07 =  2% of the original kernel matrix.

torch.Size([94599, 2])
We keep 8.54e+07/3.27e+09 =  2% of the original kernel matrix.

torch.Size([55612, 2])
We keep 1.44e+07/1.12e+09 =  1% of the original kernel matrix.

torch.Size([78834, 2])
We keep 7.74e+07/3.35e+09 =  2% of the original kernel matrix.

torch.Size([49847, 2])
We keep 1.56e+07/1.14e+09 =  1% of the original kernel matrix.

torch.Size([410026, 2])
We keep 9.05e+08/7.11e+10 =  1% of the original kernel matrix.

torch.Size([121575, 2])
We keep 5.88e+07/5.24e+09 =  1% of the original kernel matrix.

torch.Size([164778, 2])
We keep 1.62e+08/1.07e+10 =  1% of the original kernel matrix.

torch.Size([75372, 2])
We keep 2.52e+07/2.03e+09 =  1% of the original kernel matrix.

torch.Size([4763, 2])
We keep 2.07e+05/3.79e+06 =  5% of the original kernel matrix.

torch.Size([13258, 2])
We keep 1.09e+06/3.82e+07 =  2% of the original kernel matrix.

torch.Size([28889, 2])
We keep 6.60e+06/2.62e+08 =  2% of the original kernel matrix.

torch.Size([30778, 2])
We keep 5.09e+06/3.18e+08 =  1% of the original kernel matrix.

torch.Size([20218, 2])
We keep 4.62e+06/1.62e+08 =  2% of the original kernel matrix.

torch.Size([26765, 2])
We keep 4.53e+06/2.50e+08 =  1% of the original kernel matrix.

torch.Size([57660, 2])
We keep 3.54e+07/1.29e+09 =  2% of the original kernel matrix.

torch.Size([43566, 2])
We keep 1.02e+07/7.05e+08 =  1% of the original kernel matrix.

torch.Size([105167, 2])
We keep 6.96e+07/4.34e+09 =  1% of the original kernel matrix.

torch.Size([59417, 2])
We keep 1.69e+07/1.29e+09 =  1% of the original kernel matrix.

torch.Size([12460, 2])
We keep 1.23e+06/3.58e+07 =  3% of the original kernel matrix.

torch.Size([20367, 2])
We keep 2.46e+06/1.17e+08 =  2% of the original kernel matrix.

torch.Size([103941, 2])
We keep 5.41e+07/3.37e+09 =  1% of the original kernel matrix.

torch.Size([58571, 2])
We keep 1.52e+07/1.14e+09 =  1% of the original kernel matrix.

torch.Size([2062, 2])
We keep 4.95e+04/5.88e+05 =  8% of the original kernel matrix.

torch.Size([9606, 2])
We keep 5.85e+05/1.51e+07 =  3% of the original kernel matrix.

torch.Size([11878, 2])
We keep 1.19e+06/3.40e+07 =  3% of the original kernel matrix.

torch.Size([20034, 2])
We keep 2.44e+06/1.14e+08 =  2% of the original kernel matrix.

torch.Size([102652, 2])
We keep 1.10e+08/5.65e+09 =  1% of the original kernel matrix.

torch.Size([57719, 2])
We keep 1.92e+07/1.48e+09 =  1% of the original kernel matrix.

torch.Size([125114, 2])
We keep 1.14e+08/6.57e+09 =  1% of the original kernel matrix.

torch.Size([64254, 2])
We keep 2.01e+07/1.59e+09 =  1% of the original kernel matrix.

torch.Size([101746, 2])
We keep 7.58e+07/4.79e+09 =  1% of the original kernel matrix.

torch.Size([57318, 2])
We keep 1.79e+07/1.36e+09 =  1% of the original kernel matrix.

torch.Size([17066, 2])
We keep 4.67e+06/9.52e+07 =  4% of the original kernel matrix.

torch.Size([24083, 2])
We keep 3.60e+06/1.92e+08 =  1% of the original kernel matrix.

torch.Size([391856, 2])
We keep 5.33e+08/5.78e+10 =  0% of the original kernel matrix.

torch.Size([118092, 2])
We keep 5.25e+07/4.72e+09 =  1% of the original kernel matrix.

torch.Size([24350, 2])
We keep 4.20e+06/1.98e+08 =  2% of the original kernel matrix.

torch.Size([29422, 2])
We keep 4.73e+06/2.77e+08 =  1% of the original kernel matrix.

torch.Size([45425, 2])
We keep 1.09e+07/6.48e+08 =  1% of the original kernel matrix.

torch.Size([39133, 2])
We keep 7.58e+06/5.00e+08 =  1% of the original kernel matrix.

torch.Size([94641, 2])
We keep 5.60e+07/3.67e+09 =  1% of the original kernel matrix.

torch.Size([55816, 2])
We keep 1.57e+07/1.19e+09 =  1% of the original kernel matrix.

torch.Size([9318, 2])
We keep 1.11e+06/1.74e+07 =  6% of the original kernel matrix.

torch.Size([17840, 2])
We keep 1.89e+06/8.19e+07 =  2% of the original kernel matrix.

torch.Size([172641, 2])
We keep 2.10e+08/1.25e+10 =  1% of the original kernel matrix.

torch.Size([76960, 2])
We keep 2.72e+07/2.19e+09 =  1% of the original kernel matrix.

torch.Size([268566, 2])
We keep 6.77e+08/3.61e+10 =  1% of the original kernel matrix.

torch.Size([98154, 2])
We keep 4.37e+07/3.73e+09 =  1% of the original kernel matrix.

torch.Size([395295, 2])
We keep 7.43e+08/6.68e+10 =  1% of the original kernel matrix.

torch.Size([117715, 2])
We keep 5.72e+07/5.08e+09 =  1% of the original kernel matrix.

torch.Size([40146, 2])
We keep 1.42e+07/7.01e+08 =  2% of the original kernel matrix.

torch.Size([36679, 2])
We keep 8.05e+06/5.20e+08 =  1% of the original kernel matrix.

torch.Size([49531, 2])
We keep 1.81e+07/8.85e+08 =  2% of the original kernel matrix.

torch.Size([40312, 2])
We keep 8.67e+06/5.84e+08 =  1% of the original kernel matrix.

torch.Size([31609, 2])
We keep 9.65e+06/3.63e+08 =  2% of the original kernel matrix.

torch.Size([32434, 2])
We keep 5.95e+06/3.74e+08 =  1% of the original kernel matrix.

torch.Size([11441, 2])
We keep 9.60e+05/2.73e+07 =  3% of the original kernel matrix.

torch.Size([19511, 2])
We keep 2.20e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([69357, 2])
We keep 9.48e+07/2.65e+09 =  3% of the original kernel matrix.

torch.Size([47359, 2])
We keep 1.41e+07/1.01e+09 =  1% of the original kernel matrix.

torch.Size([14733, 2])
We keep 1.87e+06/6.31e+07 =  2% of the original kernel matrix.

torch.Size([22396, 2])
We keep 3.07e+06/1.56e+08 =  1% of the original kernel matrix.

torch.Size([42687, 2])
We keep 1.50e+07/6.47e+08 =  2% of the original kernel matrix.

torch.Size([37860, 2])
We keep 7.43e+06/5.00e+08 =  1% of the original kernel matrix.

torch.Size([135906, 2])
We keep 7.49e+08/1.07e+10 =  6% of the original kernel matrix.

torch.Size([68388, 2])
We keep 2.44e+07/2.03e+09 =  1% of the original kernel matrix.

torch.Size([54880, 2])
We keep 4.35e+07/1.32e+09 =  3% of the original kernel matrix.

torch.Size([42918, 2])
We keep 1.04e+07/7.12e+08 =  1% of the original kernel matrix.

torch.Size([20578, 2])
We keep 8.50e+06/2.14e+08 =  3% of the original kernel matrix.

torch.Size([25973, 2])
We keep 5.04e+06/2.87e+08 =  1% of the original kernel matrix.

torch.Size([11148, 2])
We keep 1.04e+06/2.75e+07 =  3% of the original kernel matrix.

torch.Size([19627, 2])
We keep 2.22e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([78664, 2])
We keep 2.63e+08/8.30e+09 =  3% of the original kernel matrix.

torch.Size([48953, 2])
We keep 2.27e+07/1.79e+09 =  1% of the original kernel matrix.

torch.Size([24020, 2])
We keep 7.43e+06/2.34e+08 =  3% of the original kernel matrix.

torch.Size([28816, 2])
We keep 5.11e+06/3.00e+08 =  1% of the original kernel matrix.

torch.Size([39195, 2])
We keep 4.99e+07/7.86e+08 =  6% of the original kernel matrix.

torch.Size([35370, 2])
We keep 8.43e+06/5.50e+08 =  1% of the original kernel matrix.

torch.Size([23911, 2])
We keep 4.51e+06/2.01e+08 =  2% of the original kernel matrix.

torch.Size([30349, 2])
We keep 4.91e+06/2.78e+08 =  1% of the original kernel matrix.

torch.Size([271857, 2])
We keep 8.95e+08/3.72e+10 =  2% of the original kernel matrix.

torch.Size([98103, 2])
We keep 4.40e+07/3.79e+09 =  1% of the original kernel matrix.

torch.Size([26429, 2])
We keep 5.93e+06/2.33e+08 =  2% of the original kernel matrix.

torch.Size([30671, 2])
We keep 5.14e+06/3.00e+08 =  1% of the original kernel matrix.

torch.Size([120537, 2])
We keep 2.23e+08/9.50e+09 =  2% of the original kernel matrix.

torch.Size([62279, 2])
We keep 2.42e+07/1.91e+09 =  1% of the original kernel matrix.

torch.Size([16003, 2])
We keep 3.38e+06/8.88e+07 =  3% of the original kernel matrix.

torch.Size([23283, 2])
We keep 3.51e+06/1.85e+08 =  1% of the original kernel matrix.

torch.Size([39642, 2])
We keep 3.28e+07/1.06e+09 =  3% of the original kernel matrix.

torch.Size([34329, 2])
We keep 9.46e+06/6.39e+08 =  1% of the original kernel matrix.

torch.Size([29294, 2])
We keep 2.13e+07/3.36e+08 =  6% of the original kernel matrix.

torch.Size([31867, 2])
We keep 5.89e+06/3.60e+08 =  1% of the original kernel matrix.

torch.Size([151129, 2])
We keep 6.92e+08/2.67e+10 =  2% of the original kernel matrix.

torch.Size([67520, 2])
We keep 3.85e+07/3.21e+09 =  1% of the original kernel matrix.

torch.Size([71215, 2])
We keep 3.39e+07/2.29e+09 =  1% of the original kernel matrix.

torch.Size([48675, 2])
We keep 1.30e+07/9.39e+08 =  1% of the original kernel matrix.

torch.Size([14949, 2])
We keep 1.83e+06/6.00e+07 =  3% of the original kernel matrix.

torch.Size([22649, 2])
We keep 2.98e+06/1.52e+08 =  1% of the original kernel matrix.

torch.Size([188765, 2])
We keep 1.24e+08/1.22e+10 =  1% of the original kernel matrix.

torch.Size([81255, 2])
We keep 2.63e+07/2.17e+09 =  1% of the original kernel matrix.

torch.Size([182534, 2])
We keep 1.43e+08/1.31e+10 =  1% of the original kernel matrix.

torch.Size([79892, 2])
We keep 2.75e+07/2.24e+09 =  1% of the original kernel matrix.

torch.Size([15475, 2])
We keep 2.95e+06/6.99e+07 =  4% of the original kernel matrix.

torch.Size([23012, 2])
We keep 3.24e+06/1.64e+08 =  1% of the original kernel matrix.

torch.Size([234958, 2])
We keep 4.20e+08/2.34e+10 =  1% of the original kernel matrix.

torch.Size([91473, 2])
We keep 3.58e+07/3.00e+09 =  1% of the original kernel matrix.

torch.Size([180665, 2])
We keep 1.59e+08/1.49e+10 =  1% of the original kernel matrix.

torch.Size([80635, 2])
We keep 2.93e+07/2.39e+09 =  1% of the original kernel matrix.

torch.Size([505127, 2])
We keep 9.49e+08/9.84e+10 =  0% of the original kernel matrix.

torch.Size([135414, 2])
We keep 6.79e+07/6.16e+09 =  1% of the original kernel matrix.

torch.Size([68279, 2])
We keep 2.16e+08/2.36e+09 =  9% of the original kernel matrix.

torch.Size([47157, 2])
We keep 1.28e+07/9.53e+08 =  1% of the original kernel matrix.

torch.Size([5173, 2])
We keep 2.56e+05/4.82e+06 =  5% of the original kernel matrix.

torch.Size([13645, 2])
We keep 1.16e+06/4.31e+07 =  2% of the original kernel matrix.

torch.Size([29548, 2])
We keep 5.41e+06/2.98e+08 =  1% of the original kernel matrix.

torch.Size([31842, 2])
We keep 5.47e+06/3.39e+08 =  1% of the original kernel matrix.

torch.Size([50671, 2])
We keep 9.66e+07/1.59e+09 =  6% of the original kernel matrix.

torch.Size([40567, 2])
We keep 1.08e+07/7.84e+08 =  1% of the original kernel matrix.

torch.Size([31188, 2])
We keep 7.76e+06/3.53e+08 =  2% of the original kernel matrix.

torch.Size([33022, 2])
We keep 6.01e+06/3.69e+08 =  1% of the original kernel matrix.

torch.Size([46264, 2])
We keep 4.20e+07/9.10e+08 =  4% of the original kernel matrix.

torch.Size([39435, 2])
We keep 9.12e+06/5.92e+08 =  1% of the original kernel matrix.

torch.Size([6917, 2])
We keep 3.50e+05/7.73e+06 =  4% of the original kernel matrix.

torch.Size([15469, 2])
We keep 1.37e+06/5.46e+07 =  2% of the original kernel matrix.

torch.Size([95189, 2])
We keep 5.36e+07/3.20e+09 =  1% of the original kernel matrix.

torch.Size([55982, 2])
We keep 1.50e+07/1.11e+09 =  1% of the original kernel matrix.

torch.Size([260501, 2])
We keep 2.81e+08/2.75e+10 =  1% of the original kernel matrix.

torch.Size([96495, 2])
We keep 3.81e+07/3.26e+09 =  1% of the original kernel matrix.

torch.Size([29890, 2])
We keep 4.42e+07/5.99e+08 =  7% of the original kernel matrix.

torch.Size([30664, 2])
We keep 6.72e+06/4.80e+08 =  1% of the original kernel matrix.

torch.Size([14084, 2])
We keep 2.47e+06/6.92e+07 =  3% of the original kernel matrix.

torch.Size([21784, 2])
We keep 3.24e+06/1.63e+08 =  1% of the original kernel matrix.

torch.Size([34722, 2])
We keep 1.06e+07/4.76e+08 =  2% of the original kernel matrix.

torch.Size([34340, 2])
We keep 6.86e+06/4.28e+08 =  1% of the original kernel matrix.

torch.Size([16338, 2])
We keep 2.60e+06/9.00e+07 =  2% of the original kernel matrix.

torch.Size([24030, 2])
We keep 3.52e+06/1.86e+08 =  1% of the original kernel matrix.

torch.Size([10840, 2])
We keep 1.13e+06/3.06e+07 =  3% of the original kernel matrix.

torch.Size([19157, 2])
We keep 2.39e+06/1.09e+08 =  2% of the original kernel matrix.

torch.Size([21510, 2])
We keep 3.65e+06/1.65e+08 =  2% of the original kernel matrix.

torch.Size([27895, 2])
We keep 4.45e+06/2.52e+08 =  1% of the original kernel matrix.

torch.Size([15620, 2])
We keep 2.31e+06/5.99e+07 =  3% of the original kernel matrix.

torch.Size([23036, 2])
We keep 2.88e+06/1.52e+08 =  1% of the original kernel matrix.

torch.Size([39656, 2])
We keep 1.70e+07/6.00e+08 =  2% of the original kernel matrix.

torch.Size([35949, 2])
We keep 7.17e+06/4.81e+08 =  1% of the original kernel matrix.

torch.Size([80621, 2])
We keep 2.96e+07/2.08e+09 =  1% of the original kernel matrix.

torch.Size([51317, 2])
We keep 1.23e+07/8.95e+08 =  1% of the original kernel matrix.

torch.Size([25144, 2])
We keep 6.84e+06/2.74e+08 =  2% of the original kernel matrix.

torch.Size([29792, 2])
We keep 5.47e+06/3.25e+08 =  1% of the original kernel matrix.

torch.Size([53998, 2])
We keep 5.31e+07/1.54e+09 =  3% of the original kernel matrix.

torch.Size([42320, 2])
We keep 1.14e+07/7.70e+08 =  1% of the original kernel matrix.

torch.Size([14046, 2])
We keep 2.04e+06/4.95e+07 =  4% of the original kernel matrix.

torch.Size([21714, 2])
We keep 2.79e+06/1.38e+08 =  2% of the original kernel matrix.

torch.Size([626830, 2])
We keep 1.61e+09/1.59e+11 =  1% of the original kernel matrix.

torch.Size([152418, 2])
We keep 8.43e+07/7.83e+09 =  1% of the original kernel matrix.

torch.Size([18384, 2])
We keep 2.62e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([25236, 2])
We keep 3.70e+06/2.00e+08 =  1% of the original kernel matrix.

torch.Size([182613, 2])
We keep 1.29e+08/1.14e+10 =  1% of the original kernel matrix.

torch.Size([79451, 2])
We keep 2.55e+07/2.10e+09 =  1% of the original kernel matrix.

torch.Size([4662, 2])
We keep 2.02e+05/3.33e+06 =  6% of the original kernel matrix.

torch.Size([13274, 2])
We keep 1.03e+06/3.58e+07 =  2% of the original kernel matrix.

torch.Size([13772, 2])
We keep 2.02e+06/5.81e+07 =  3% of the original kernel matrix.

torch.Size([21691, 2])
We keep 3.02e+06/1.50e+08 =  2% of the original kernel matrix.

torch.Size([7539, 2])
We keep 4.86e+05/1.10e+07 =  4% of the original kernel matrix.

torch.Size([16296, 2])
We keep 1.60e+06/6.51e+07 =  2% of the original kernel matrix.

torch.Size([33254, 2])
We keep 2.22e+07/4.08e+08 =  5% of the original kernel matrix.

torch.Size([34144, 2])
We keep 6.18e+06/3.96e+08 =  1% of the original kernel matrix.

torch.Size([8028, 2])
We keep 6.14e+05/1.37e+07 =  4% of the original kernel matrix.

torch.Size([16607, 2])
We keep 1.76e+06/7.27e+07 =  2% of the original kernel matrix.

torch.Size([172941, 2])
We keep 6.01e+08/2.21e+10 =  2% of the original kernel matrix.

torch.Size([76294, 2])
We keep 3.54e+07/2.92e+09 =  1% of the original kernel matrix.

torch.Size([9315, 2])
We keep 7.16e+05/1.82e+07 =  3% of the original kernel matrix.

torch.Size([17565, 2])
We keep 1.87e+06/8.37e+07 =  2% of the original kernel matrix.

torch.Size([9664, 2])
We keep 7.00e+05/1.80e+07 =  3% of the original kernel matrix.

torch.Size([17811, 2])
We keep 1.88e+06/8.34e+07 =  2% of the original kernel matrix.

torch.Size([354449, 2])
We keep 6.18e+08/4.67e+10 =  1% of the original kernel matrix.

torch.Size([111955, 2])
We keep 4.82e+07/4.24e+09 =  1% of the original kernel matrix.

torch.Size([94241, 2])
We keep 6.42e+07/3.52e+09 =  1% of the original kernel matrix.

torch.Size([55755, 2])
We keep 1.58e+07/1.16e+09 =  1% of the original kernel matrix.

torch.Size([74768, 2])
We keep 6.84e+07/2.89e+09 =  2% of the original kernel matrix.

torch.Size([49701, 2])
We keep 1.48e+07/1.06e+09 =  1% of the original kernel matrix.

torch.Size([242357, 2])
We keep 5.00e+08/2.88e+10 =  1% of the original kernel matrix.

torch.Size([94081, 2])
We keep 3.96e+07/3.33e+09 =  1% of the original kernel matrix.

torch.Size([284523, 2])
We keep 8.32e+08/5.92e+10 =  1% of the original kernel matrix.

torch.Size([94300, 2])
We keep 5.50e+07/4.78e+09 =  1% of the original kernel matrix.

torch.Size([11639, 2])
We keep 9.73e+05/2.66e+07 =  3% of the original kernel matrix.

torch.Size([19630, 2])
We keep 2.18e+06/1.01e+08 =  2% of the original kernel matrix.

torch.Size([12830, 2])
We keep 1.15e+06/3.61e+07 =  3% of the original kernel matrix.

torch.Size([20596, 2])
We keep 2.43e+06/1.18e+08 =  2% of the original kernel matrix.

torch.Size([20804, 2])
We keep 4.85e+06/1.48e+08 =  3% of the original kernel matrix.

torch.Size([26868, 2])
We keep 4.30e+06/2.39e+08 =  1% of the original kernel matrix.

torch.Size([5027, 2])
We keep 2.91e+05/4.58e+06 =  6% of the original kernel matrix.

torch.Size([13381, 2])
We keep 1.16e+06/4.20e+07 =  2% of the original kernel matrix.

torch.Size([13004, 2])
We keep 1.54e+06/4.77e+07 =  3% of the original kernel matrix.

torch.Size([20814, 2])
We keep 2.76e+06/1.36e+08 =  2% of the original kernel matrix.

torch.Size([31801, 2])
We keep 8.49e+06/3.66e+08 =  2% of the original kernel matrix.

torch.Size([33123, 2])
We keep 6.11e+06/3.76e+08 =  1% of the original kernel matrix.

torch.Size([9633, 2])
We keep 9.08e+05/2.01e+07 =  4% of the original kernel matrix.

torch.Size([17877, 2])
We keep 1.97e+06/8.80e+07 =  2% of the original kernel matrix.

torch.Size([14715, 2])
We keep 1.60e+06/5.35e+07 =  2% of the original kernel matrix.

torch.Size([22303, 2])
We keep 2.86e+06/1.44e+08 =  1% of the original kernel matrix.

torch.Size([13218, 2])
We keep 1.43e+06/4.17e+07 =  3% of the original kernel matrix.

torch.Size([20980, 2])
We keep 2.56e+06/1.27e+08 =  2% of the original kernel matrix.

torch.Size([11643, 2])
We keep 1.20e+06/3.20e+07 =  3% of the original kernel matrix.

torch.Size([19861, 2])
We keep 2.33e+06/1.11e+08 =  2% of the original kernel matrix.

torch.Size([442507, 2])
We keep 1.05e+09/8.01e+10 =  1% of the original kernel matrix.

torch.Size([125275, 2])
We keep 6.10e+07/5.56e+09 =  1% of the original kernel matrix.

torch.Size([17247, 2])
We keep 3.61e+06/1.06e+08 =  3% of the original kernel matrix.

torch.Size([24078, 2])
We keep 3.76e+06/2.02e+08 =  1% of the original kernel matrix.

torch.Size([117962, 2])
We keep 1.29e+08/7.18e+09 =  1% of the original kernel matrix.

torch.Size([62849, 2])
We keep 2.15e+07/1.66e+09 =  1% of the original kernel matrix.

torch.Size([13002, 2])
We keep 3.25e+06/6.58e+07 =  4% of the original kernel matrix.

torch.Size([20958, 2])
We keep 2.98e+06/1.59e+08 =  1% of the original kernel matrix.

torch.Size([28655, 2])
We keep 2.33e+07/4.67e+08 =  4% of the original kernel matrix.

torch.Size([30797, 2])
We keep 6.60e+06/4.24e+08 =  1% of the original kernel matrix.

torch.Size([63433, 2])
We keep 3.91e+07/1.70e+09 =  2% of the original kernel matrix.

torch.Size([44911, 2])
We keep 1.16e+07/8.09e+08 =  1% of the original kernel matrix.

torch.Size([731868, 2])
We keep 5.45e+09/2.42e+11 =  2% of the original kernel matrix.

torch.Size([165295, 2])
We keep 1.01e+08/9.65e+09 =  1% of the original kernel matrix.

torch.Size([934717, 2])
We keep 2.40e+09/3.10e+11 =  0% of the original kernel matrix.

torch.Size([187099, 2])
We keep 1.13e+08/1.09e+10 =  1% of the original kernel matrix.

torch.Size([32754, 2])
We keep 1.11e+07/4.04e+08 =  2% of the original kernel matrix.

torch.Size([33584, 2])
We keep 6.11e+06/3.94e+08 =  1% of the original kernel matrix.

torch.Size([69320, 2])
We keep 4.08e+07/1.97e+09 =  2% of the original kernel matrix.

torch.Size([47833, 2])
We keep 1.24e+07/8.72e+08 =  1% of the original kernel matrix.

torch.Size([9839, 2])
We keep 7.30e+05/1.75e+07 =  4% of the original kernel matrix.

torch.Size([18164, 2])
We keep 1.86e+06/8.22e+07 =  2% of the original kernel matrix.

torch.Size([19330, 2])
We keep 2.20e+07/1.85e+08 = 11% of the original kernel matrix.

torch.Size([25171, 2])
We keep 4.54e+06/2.67e+08 =  1% of the original kernel matrix.

torch.Size([94020, 2])
We keep 1.53e+08/5.02e+09 =  3% of the original kernel matrix.

torch.Size([55695, 2])
We keep 1.84e+07/1.39e+09 =  1% of the original kernel matrix.

torch.Size([45307, 2])
We keep 2.24e+07/8.62e+08 =  2% of the original kernel matrix.

torch.Size([39446, 2])
We keep 8.29e+06/5.77e+08 =  1% of the original kernel matrix.

torch.Size([12671, 2])
We keep 1.52e+06/4.32e+07 =  3% of the original kernel matrix.

torch.Size([20483, 2])
We keep 2.64e+06/1.29e+08 =  2% of the original kernel matrix.

torch.Size([118945, 2])
We keep 7.43e+07/4.95e+09 =  1% of the original kernel matrix.

torch.Size([63061, 2])
We keep 1.77e+07/1.38e+09 =  1% of the original kernel matrix.

torch.Size([726886, 2])
We keep 2.23e+09/2.19e+11 =  1% of the original kernel matrix.

torch.Size([164664, 2])
We keep 9.86e+07/9.18e+09 =  1% of the original kernel matrix.

torch.Size([80017, 2])
We keep 4.88e+07/2.78e+09 =  1% of the original kernel matrix.

torch.Size([50438, 2])
We keep 1.40e+07/1.04e+09 =  1% of the original kernel matrix.

torch.Size([21710, 2])
We keep 4.70e+06/1.81e+08 =  2% of the original kernel matrix.

torch.Size([27121, 2])
We keep 4.56e+06/2.64e+08 =  1% of the original kernel matrix.

torch.Size([49296, 2])
We keep 3.82e+07/1.12e+09 =  3% of the original kernel matrix.

torch.Size([41465, 2])
We keep 9.94e+06/6.57e+08 =  1% of the original kernel matrix.

torch.Size([20263, 2])
We keep 3.46e+06/1.50e+08 =  2% of the original kernel matrix.

torch.Size([26741, 2])
We keep 4.34e+06/2.41e+08 =  1% of the original kernel matrix.

torch.Size([7578, 2])
We keep 4.31e+05/9.95e+06 =  4% of the original kernel matrix.

torch.Size([16231, 2])
We keep 1.51e+06/6.20e+07 =  2% of the original kernel matrix.

torch.Size([198214, 2])
We keep 2.66e+08/1.84e+10 =  1% of the original kernel matrix.

torch.Size([83816, 2])
We keep 3.25e+07/2.67e+09 =  1% of the original kernel matrix.

torch.Size([35728, 2])
We keep 1.29e+07/4.88e+08 =  2% of the original kernel matrix.

torch.Size([34696, 2])
We keep 6.69e+06/4.34e+08 =  1% of the original kernel matrix.

torch.Size([5068, 2])
We keep 2.46e+05/4.07e+06 =  6% of the original kernel matrix.

torch.Size([13453, 2])
We keep 1.11e+06/3.96e+07 =  2% of the original kernel matrix.

torch.Size([139530, 2])
We keep 1.03e+08/6.82e+09 =  1% of the original kernel matrix.

torch.Size([69091, 2])
We keep 2.05e+07/1.62e+09 =  1% of the original kernel matrix.

torch.Size([28856, 2])
We keep 5.84e+06/2.95e+08 =  1% of the original kernel matrix.

torch.Size([33482, 2])
We keep 5.69e+06/3.37e+08 =  1% of the original kernel matrix.

torch.Size([4752, 2])
We keep 2.25e+05/3.50e+06 =  6% of the original kernel matrix.

torch.Size([13307, 2])
We keep 1.06e+06/3.67e+07 =  2% of the original kernel matrix.

torch.Size([130299, 2])
We keep 1.54e+08/7.91e+09 =  1% of the original kernel matrix.

torch.Size([66513, 2])
We keep 2.24e+07/1.75e+09 =  1% of the original kernel matrix.

torch.Size([145463, 2])
We keep 1.25e+08/9.62e+09 =  1% of the original kernel matrix.

torch.Size([70546, 2])
We keep 2.43e+07/1.93e+09 =  1% of the original kernel matrix.

torch.Size([188185, 2])
We keep 3.59e+08/1.74e+10 =  2% of the original kernel matrix.

torch.Size([81725, 2])
We keep 3.18e+07/2.59e+09 =  1% of the original kernel matrix.

torch.Size([266081, 2])
We keep 7.03e+08/3.43e+10 =  2% of the original kernel matrix.

torch.Size([97336, 2])
We keep 4.28e+07/3.64e+09 =  1% of the original kernel matrix.

torch.Size([293265, 2])
We keep 6.99e+08/3.78e+10 =  1% of the original kernel matrix.

torch.Size([100878, 2])
We keep 4.46e+07/3.82e+09 =  1% of the original kernel matrix.

torch.Size([27016, 2])
We keep 1.14e+07/3.10e+08 =  3% of the original kernel matrix.

torch.Size([29191, 2])
We keep 5.50e+06/3.46e+08 =  1% of the original kernel matrix.

torch.Size([79428, 2])
We keep 6.51e+07/2.48e+09 =  2% of the original kernel matrix.

torch.Size([50429, 2])
We keep 1.30e+07/9.78e+08 =  1% of the original kernel matrix.

torch.Size([248106, 2])
We keep 2.08e+08/2.11e+10 =  0% of the original kernel matrix.

torch.Size([94826, 2])
We keep 3.37e+07/2.86e+09 =  1% of the original kernel matrix.

torch.Size([244207, 2])
We keep 2.46e+08/2.56e+10 =  0% of the original kernel matrix.

torch.Size([94193, 2])
We keep 3.68e+07/3.14e+09 =  1% of the original kernel matrix.

torch.Size([19088, 2])
We keep 2.54e+06/1.01e+08 =  2% of the original kernel matrix.

torch.Size([25775, 2])
We keep 3.64e+06/1.98e+08 =  1% of the original kernel matrix.

torch.Size([172340, 2])
We keep 1.26e+09/1.97e+10 =  6% of the original kernel matrix.

torch.Size([77542, 2])
We keep 3.17e+07/2.76e+09 =  1% of the original kernel matrix.

torch.Size([249683, 2])
We keep 1.55e+09/3.39e+10 =  4% of the original kernel matrix.

torch.Size([93534, 2])
We keep 4.11e+07/3.62e+09 =  1% of the original kernel matrix.

torch.Size([186546, 2])
We keep 2.76e+08/1.27e+10 =  2% of the original kernel matrix.

torch.Size([80897, 2])
We keep 2.72e+07/2.21e+09 =  1% of the original kernel matrix.

torch.Size([106316, 2])
We keep 1.58e+08/6.64e+09 =  2% of the original kernel matrix.

torch.Size([59493, 2])
We keep 2.09e+07/1.60e+09 =  1% of the original kernel matrix.

torch.Size([41150, 2])
We keep 1.20e+08/2.04e+09 =  5% of the original kernel matrix.

torch.Size([34402, 2])
We keep 1.26e+07/8.88e+08 =  1% of the original kernel matrix.

torch.Size([19796, 2])
We keep 4.14e+06/1.30e+08 =  3% of the original kernel matrix.

torch.Size([26086, 2])
We keep 4.05e+06/2.24e+08 =  1% of the original kernel matrix.

torch.Size([14755, 2])
We keep 1.51e+06/5.31e+07 =  2% of the original kernel matrix.

torch.Size([22286, 2])
We keep 2.82e+06/1.43e+08 =  1% of the original kernel matrix.

torch.Size([12935, 2])
We keep 1.60e+06/4.34e+07 =  3% of the original kernel matrix.

torch.Size([20829, 2])
We keep 2.65e+06/1.29e+08 =  2% of the original kernel matrix.

torch.Size([212441, 2])
We keep 1.62e+08/1.63e+10 =  0% of the original kernel matrix.

torch.Size([87221, 2])
We keep 2.98e+07/2.50e+09 =  1% of the original kernel matrix.

torch.Size([181605, 2])
We keep 1.56e+08/1.24e+10 =  1% of the original kernel matrix.

torch.Size([79713, 2])
We keep 2.64e+07/2.18e+09 =  1% of the original kernel matrix.

torch.Size([477037, 2])
We keep 9.91e+08/9.70e+10 =  1% of the original kernel matrix.

torch.Size([129619, 2])
We keep 6.71e+07/6.11e+09 =  1% of the original kernel matrix.

torch.Size([156904, 2])
We keep 2.29e+08/1.13e+10 =  2% of the original kernel matrix.

torch.Size([74225, 2])
We keep 2.60e+07/2.08e+09 =  1% of the original kernel matrix.

torch.Size([127578, 2])
We keep 9.57e+07/6.21e+09 =  1% of the original kernel matrix.

torch.Size([65467, 2])
We keep 1.95e+07/1.55e+09 =  1% of the original kernel matrix.

torch.Size([21400, 2])
We keep 3.63e+06/1.65e+08 =  2% of the original kernel matrix.

torch.Size([26760, 2])
We keep 4.34e+06/2.52e+08 =  1% of the original kernel matrix.

torch.Size([198875, 2])
We keep 2.20e+08/1.71e+10 =  1% of the original kernel matrix.

torch.Size([83955, 2])
We keep 3.12e+07/2.57e+09 =  1% of the original kernel matrix.

torch.Size([8036, 2])
We keep 8.03e+05/1.48e+07 =  5% of the original kernel matrix.

torch.Size([16364, 2])
We keep 1.77e+06/7.54e+07 =  2% of the original kernel matrix.

torch.Size([20209, 2])
We keep 4.35e+06/1.55e+08 =  2% of the original kernel matrix.

torch.Size([26107, 2])
We keep 4.29e+06/2.45e+08 =  1% of the original kernel matrix.

torch.Size([242164, 2])
We keep 3.04e+08/2.48e+10 =  1% of the original kernel matrix.

torch.Size([92916, 2])
We keep 3.63e+07/3.09e+09 =  1% of the original kernel matrix.

torch.Size([34488, 2])
We keep 8.42e+06/3.93e+08 =  2% of the original kernel matrix.

torch.Size([34749, 2])
We keep 6.28e+06/3.89e+08 =  1% of the original kernel matrix.

torch.Size([10174, 2])
We keep 1.12e+06/2.86e+07 =  3% of the original kernel matrix.

torch.Size([18360, 2])
We keep 2.34e+06/1.05e+08 =  2% of the original kernel matrix.

torch.Size([23904, 2])
We keep 4.34e+06/1.90e+08 =  2% of the original kernel matrix.

torch.Size([28636, 2])
We keep 4.64e+06/2.70e+08 =  1% of the original kernel matrix.

torch.Size([288057, 2])
We keep 1.15e+09/5.31e+10 =  2% of the original kernel matrix.

torch.Size([99521, 2])
We keep 5.24e+07/4.52e+09 =  1% of the original kernel matrix.

torch.Size([31651, 2])
We keep 8.31e+06/3.96e+08 =  2% of the original kernel matrix.

torch.Size([32992, 2])
We keep 6.34e+06/3.91e+08 =  1% of the original kernel matrix.

torch.Size([261538, 2])
We keep 4.17e+08/3.14e+10 =  1% of the original kernel matrix.

torch.Size([97412, 2])
We keep 4.11e+07/3.48e+09 =  1% of the original kernel matrix.

torch.Size([13978, 2])
We keep 8.25e+06/1.04e+08 =  7% of the original kernel matrix.

torch.Size([21316, 2])
We keep 3.55e+06/2.01e+08 =  1% of the original kernel matrix.

torch.Size([23267, 2])
We keep 9.40e+06/1.86e+08 =  5% of the original kernel matrix.

torch.Size([28149, 2])
We keep 4.41e+06/2.68e+08 =  1% of the original kernel matrix.

torch.Size([8861, 2])
We keep 7.75e+05/1.78e+07 =  4% of the original kernel matrix.

torch.Size([17440, 2])
We keep 1.92e+06/8.28e+07 =  2% of the original kernel matrix.

torch.Size([6161, 2])
We keep 3.69e+05/7.23e+06 =  5% of the original kernel matrix.

torch.Size([14907, 2])
We keep 1.38e+06/5.28e+07 =  2% of the original kernel matrix.

torch.Size([67220, 2])
We keep 4.58e+07/1.89e+09 =  2% of the original kernel matrix.

torch.Size([46934, 2])
We keep 1.20e+07/8.54e+08 =  1% of the original kernel matrix.

torch.Size([87565, 2])
We keep 6.27e+07/2.81e+09 =  2% of the original kernel matrix.

torch.Size([52905, 2])
We keep 1.40e+07/1.04e+09 =  1% of the original kernel matrix.

torch.Size([9109, 2])
We keep 8.06e+05/1.82e+07 =  4% of the original kernel matrix.

torch.Size([17548, 2])
We keep 1.95e+06/8.37e+07 =  2% of the original kernel matrix.

torch.Size([174864, 2])
We keep 1.96e+08/1.18e+10 =  1% of the original kernel matrix.

torch.Size([77928, 2])
We keep 2.59e+07/2.13e+09 =  1% of the original kernel matrix.

torch.Size([8123, 2])
We keep 6.90e+05/1.41e+07 =  4% of the original kernel matrix.

torch.Size([16531, 2])
We keep 1.74e+06/7.36e+07 =  2% of the original kernel matrix.

torch.Size([60285, 2])
We keep 3.04e+07/1.32e+09 =  2% of the original kernel matrix.

torch.Size([44845, 2])
We keep 1.04e+07/7.14e+08 =  1% of the original kernel matrix.

torch.Size([16211, 2])
We keep 2.82e+06/8.34e+07 =  3% of the original kernel matrix.

torch.Size([23391, 2])
We keep 3.38e+06/1.79e+08 =  1% of the original kernel matrix.

torch.Size([195941, 2])
We keep 1.77e+08/1.49e+10 =  1% of the original kernel matrix.

torch.Size([82826, 2])
We keep 2.89e+07/2.40e+09 =  1% of the original kernel matrix.

torch.Size([16802, 2])
We keep 8.09e+06/9.83e+07 =  8% of the original kernel matrix.

torch.Size([23932, 2])
We keep 3.68e+06/1.95e+08 =  1% of the original kernel matrix.

torch.Size([58795, 2])
We keep 5.33e+07/1.44e+09 =  3% of the original kernel matrix.

torch.Size([44204, 2])
We keep 1.09e+07/7.46e+08 =  1% of the original kernel matrix.

torch.Size([45057, 2])
We keep 1.18e+07/6.52e+08 =  1% of the original kernel matrix.

torch.Size([39263, 2])
We keep 7.61e+06/5.02e+08 =  1% of the original kernel matrix.

torch.Size([48043, 2])
We keep 2.55e+07/1.07e+09 =  2% of the original kernel matrix.

torch.Size([40176, 2])
We keep 9.50e+06/6.41e+08 =  1% of the original kernel matrix.

torch.Size([97910, 2])
We keep 6.97e+07/3.99e+09 =  1% of the original kernel matrix.

torch.Size([57220, 2])
We keep 1.67e+07/1.24e+09 =  1% of the original kernel matrix.

torch.Size([501390, 2])
We keep 1.76e+09/1.05e+11 =  1% of the original kernel matrix.

torch.Size([133681, 2])
We keep 6.92e+07/6.38e+09 =  1% of the original kernel matrix.

torch.Size([16181, 2])
We keep 2.69e+06/8.32e+07 =  3% of the original kernel matrix.

torch.Size([23646, 2])
We keep 3.44e+06/1.79e+08 =  1% of the original kernel matrix.

torch.Size([17820, 2])
We keep 3.51e+06/1.07e+08 =  3% of the original kernel matrix.

torch.Size([24466, 2])
We keep 3.64e+06/2.03e+08 =  1% of the original kernel matrix.

torch.Size([118440, 2])
We keep 8.27e+07/5.31e+09 =  1% of the original kernel matrix.

torch.Size([63156, 2])
We keep 1.84e+07/1.43e+09 =  1% of the original kernel matrix.

torch.Size([22779, 2])
We keep 5.80e+06/2.01e+08 =  2% of the original kernel matrix.

torch.Size([28236, 2])
We keep 4.87e+06/2.78e+08 =  1% of the original kernel matrix.

torch.Size([798489, 2])
We keep 1.84e+09/2.48e+11 =  0% of the original kernel matrix.

torch.Size([174111, 2])
We keep 1.03e+08/9.77e+09 =  1% of the original kernel matrix.

torch.Size([9071, 2])
We keep 7.55e+05/1.68e+07 =  4% of the original kernel matrix.

torch.Size([17419, 2])
We keep 1.83e+06/8.04e+07 =  2% of the original kernel matrix.

torch.Size([201307, 2])
We keep 2.01e+08/1.74e+10 =  1% of the original kernel matrix.

torch.Size([83894, 2])
We keep 3.10e+07/2.59e+09 =  1% of the original kernel matrix.

torch.Size([412466, 2])
We keep 5.53e+08/6.58e+10 =  0% of the original kernel matrix.

torch.Size([120522, 2])
We keep 5.62e+07/5.04e+09 =  1% of the original kernel matrix.

torch.Size([8718, 2])
We keep 6.46e+05/1.50e+07 =  4% of the original kernel matrix.

torch.Size([17301, 2])
We keep 1.80e+06/7.61e+07 =  2% of the original kernel matrix.

torch.Size([187436, 2])
We keep 4.65e+08/2.28e+10 =  2% of the original kernel matrix.

torch.Size([78101, 2])
We keep 3.49e+07/2.97e+09 =  1% of the original kernel matrix.

torch.Size([22764, 2])
We keep 1.04e+07/2.71e+08 =  3% of the original kernel matrix.

torch.Size([26663, 2])
We keep 5.33e+06/3.24e+08 =  1% of the original kernel matrix.

torch.Size([1101174, 2])
We keep 4.52e+09/4.32e+11 =  1% of the original kernel matrix.

torch.Size([202061, 2])
We keep 1.35e+08/1.29e+10 =  1% of the original kernel matrix.

torch.Size([17469, 2])
We keep 2.51e+06/9.47e+07 =  2% of the original kernel matrix.

torch.Size([24621, 2])
We keep 3.58e+06/1.91e+08 =  1% of the original kernel matrix.

torch.Size([48320, 2])
We keep 1.89e+07/7.87e+08 =  2% of the original kernel matrix.

torch.Size([40080, 2])
We keep 7.98e+06/5.51e+08 =  1% of the original kernel matrix.

torch.Size([11571, 2])
We keep 3.07e+06/5.54e+07 =  5% of the original kernel matrix.

torch.Size([19456, 2])
We keep 2.94e+06/1.46e+08 =  2% of the original kernel matrix.

torch.Size([8482, 2])
We keep 8.20e+05/1.64e+07 =  5% of the original kernel matrix.

torch.Size([16872, 2])
We keep 1.81e+06/7.95e+07 =  2% of the original kernel matrix.

torch.Size([12241, 2])
We keep 1.97e+06/4.93e+07 =  3% of the original kernel matrix.

torch.Size([20193, 2])
We keep 2.87e+06/1.38e+08 =  2% of the original kernel matrix.

torch.Size([56134, 2])
We keep 2.81e+07/1.13e+09 =  2% of the original kernel matrix.

torch.Size([43396, 2])
We keep 9.40e+06/6.60e+08 =  1% of the original kernel matrix.

torch.Size([29483, 2])
We keep 2.83e+07/3.92e+08 =  7% of the original kernel matrix.

torch.Size([31646, 2])
We keep 6.19e+06/3.89e+08 =  1% of the original kernel matrix.

torch.Size([185878, 2])
We keep 2.18e+08/1.62e+10 =  1% of the original kernel matrix.

torch.Size([81013, 2])
We keep 3.06e+07/2.50e+09 =  1% of the original kernel matrix.

torch.Size([29763, 2])
We keep 6.02e+06/3.06e+08 =  1% of the original kernel matrix.

torch.Size([32571, 2])
We keep 5.70e+06/3.43e+08 =  1% of the original kernel matrix.

torch.Size([9704, 2])
We keep 8.39e+05/2.09e+07 =  4% of the original kernel matrix.

torch.Size([17972, 2])
We keep 2.01e+06/8.97e+07 =  2% of the original kernel matrix.

torch.Size([17288, 2])
We keep 4.07e+06/1.03e+08 =  3% of the original kernel matrix.

torch.Size([24291, 2])
We keep 3.77e+06/2.00e+08 =  1% of the original kernel matrix.

torch.Size([263001, 2])
We keep 5.53e+08/3.86e+10 =  1% of the original kernel matrix.

torch.Size([97050, 2])
We keep 4.54e+07/3.86e+09 =  1% of the original kernel matrix.

torch.Size([87956, 2])
We keep 1.64e+08/5.92e+09 =  2% of the original kernel matrix.

torch.Size([53048, 2])
We keep 2.01e+07/1.51e+09 =  1% of the original kernel matrix.

torch.Size([393448, 2])
We keep 6.31e+08/6.09e+10 =  1% of the original kernel matrix.

torch.Size([118329, 2])
We keep 5.51e+07/4.85e+09 =  1% of the original kernel matrix.

torch.Size([57745, 2])
We keep 3.08e+07/1.10e+09 =  2% of the original kernel matrix.

torch.Size([43801, 2])
We keep 9.01e+06/6.51e+08 =  1% of the original kernel matrix.

torch.Size([13338, 2])
We keep 2.01e+06/5.42e+07 =  3% of the original kernel matrix.

torch.Size([21177, 2])
We keep 2.85e+06/1.45e+08 =  1% of the original kernel matrix.

torch.Size([8565, 2])
We keep 7.06e+05/1.61e+07 =  4% of the original kernel matrix.

torch.Size([17204, 2])
We keep 1.87e+06/7.88e+07 =  2% of the original kernel matrix.

torch.Size([15545, 2])
We keep 3.44e+06/7.18e+07 =  4% of the original kernel matrix.

torch.Size([22703, 2])
We keep 3.13e+06/1.66e+08 =  1% of the original kernel matrix.

torch.Size([640540, 2])
We keep 2.31e+09/1.65e+11 =  1% of the original kernel matrix.

torch.Size([155840, 2])
We keep 8.40e+07/7.98e+09 =  1% of the original kernel matrix.

torch.Size([16057, 2])
We keep 2.59e+06/8.46e+07 =  3% of the original kernel matrix.

torch.Size([23415, 2])
We keep 3.47e+06/1.81e+08 =  1% of the original kernel matrix.

torch.Size([43674, 2])
We keep 1.34e+07/6.67e+08 =  2% of the original kernel matrix.

torch.Size([38491, 2])
We keep 7.76e+06/5.07e+08 =  1% of the original kernel matrix.

torch.Size([111428, 2])
We keep 1.04e+08/5.06e+09 =  2% of the original kernel matrix.

torch.Size([60766, 2])
We keep 1.78e+07/1.40e+09 =  1% of the original kernel matrix.

torch.Size([61325, 2])
We keep 5.18e+07/1.87e+09 =  2% of the original kernel matrix.

torch.Size([44331, 2])
We keep 1.16e+07/8.49e+08 =  1% of the original kernel matrix.

torch.Size([15713, 2])
We keep 2.45e+06/6.97e+07 =  3% of the original kernel matrix.

torch.Size([23007, 2])
We keep 3.13e+06/1.64e+08 =  1% of the original kernel matrix.

torch.Size([10114, 2])
We keep 1.05e+06/2.40e+07 =  4% of the original kernel matrix.

torch.Size([18482, 2])
We keep 2.13e+06/9.62e+07 =  2% of the original kernel matrix.

torch.Size([247846, 2])
We keep 6.15e+08/3.69e+10 =  1% of the original kernel matrix.

torch.Size([92776, 2])
We keep 4.40e+07/3.77e+09 =  1% of the original kernel matrix.

torch.Size([84466, 2])
We keep 6.58e+07/2.91e+09 =  2% of the original kernel matrix.

torch.Size([52770, 2])
We keep 1.46e+07/1.06e+09 =  1% of the original kernel matrix.

torch.Size([17943, 2])
We keep 4.09e+06/1.11e+08 =  3% of the original kernel matrix.

torch.Size([24461, 2])
We keep 3.76e+06/2.07e+08 =  1% of the original kernel matrix.

torch.Size([41777, 2])
We keep 1.54e+07/6.73e+08 =  2% of the original kernel matrix.

torch.Size([38137, 2])
We keep 7.64e+06/5.09e+08 =  1% of the original kernel matrix.

torch.Size([198772, 2])
We keep 5.94e+08/2.58e+10 =  2% of the original kernel matrix.

torch.Size([83921, 2])
We keep 3.76e+07/3.15e+09 =  1% of the original kernel matrix.

torch.Size([173497, 2])
We keep 2.35e+08/1.44e+10 =  1% of the original kernel matrix.

torch.Size([77593, 2])
We keep 2.93e+07/2.35e+09 =  1% of the original kernel matrix.

torch.Size([473001, 2])
We keep 1.10e+09/9.41e+10 =  1% of the original kernel matrix.

torch.Size([129306, 2])
We keep 6.72e+07/6.02e+09 =  1% of the original kernel matrix.

torch.Size([194139, 2])
We keep 2.02e+08/1.34e+10 =  1% of the original kernel matrix.

torch.Size([82685, 2])
We keep 2.76e+07/2.27e+09 =  1% of the original kernel matrix.

torch.Size([10780, 2])
We keep 1.80e+06/3.59e+07 =  5% of the original kernel matrix.

torch.Size([19335, 2])
We keep 2.53e+06/1.18e+08 =  2% of the original kernel matrix.

torch.Size([66402, 2])
We keep 2.44e+07/1.53e+09 =  1% of the original kernel matrix.

torch.Size([47061, 2])
We keep 1.07e+07/7.67e+08 =  1% of the original kernel matrix.

torch.Size([75139, 2])
We keep 9.75e+07/2.75e+09 =  3% of the original kernel matrix.

torch.Size([49139, 2])
We keep 1.43e+07/1.03e+09 =  1% of the original kernel matrix.

torch.Size([20952, 2])
We keep 8.55e+06/1.50e+08 =  5% of the original kernel matrix.

torch.Size([27049, 2])
We keep 4.33e+06/2.41e+08 =  1% of the original kernel matrix.

torch.Size([24869, 2])
We keep 4.71e+06/2.15e+08 =  2% of the original kernel matrix.

torch.Size([29051, 2])
We keep 4.83e+06/2.88e+08 =  1% of the original kernel matrix.

torch.Size([13390, 2])
We keep 1.55e+06/4.72e+07 =  3% of the original kernel matrix.

torch.Size([21276, 2])
We keep 2.75e+06/1.35e+08 =  2% of the original kernel matrix.

torch.Size([55680, 2])
We keep 5.91e+07/1.55e+09 =  3% of the original kernel matrix.

torch.Size([42636, 2])
We keep 1.07e+07/7.74e+08 =  1% of the original kernel matrix.

torch.Size([39294, 2])
We keep 1.39e+07/6.33e+08 =  2% of the original kernel matrix.

torch.Size([36097, 2])
We keep 7.63e+06/4.94e+08 =  1% of the original kernel matrix.

torch.Size([5575, 2])
We keep 3.22e+05/5.38e+06 =  5% of the original kernel matrix.

torch.Size([13823, 2])
We keep 1.20e+06/4.56e+07 =  2% of the original kernel matrix.

torch.Size([32448, 2])
We keep 9.73e+06/4.32e+08 =  2% of the original kernel matrix.

torch.Size([33692, 2])
We keep 6.51e+06/4.08e+08 =  1% of the original kernel matrix.

torch.Size([18324, 2])
We keep 9.95e+06/1.65e+08 =  6% of the original kernel matrix.

torch.Size([24340, 2])
We keep 4.36e+06/2.52e+08 =  1% of the original kernel matrix.

torch.Size([75363, 2])
We keep 1.06e+08/2.79e+09 =  3% of the original kernel matrix.

torch.Size([49545, 2])
We keep 1.39e+07/1.04e+09 =  1% of the original kernel matrix.

torch.Size([10124, 2])
We keep 1.08e+06/2.55e+07 =  4% of the original kernel matrix.

torch.Size([18373, 2])
We keep 2.16e+06/9.92e+07 =  2% of the original kernel matrix.

torch.Size([29948, 2])
We keep 9.92e+06/3.70e+08 =  2% of the original kernel matrix.

torch.Size([31855, 2])
We keep 6.15e+06/3.78e+08 =  1% of the original kernel matrix.

torch.Size([24778, 2])
We keep 3.63e+07/3.69e+08 =  9% of the original kernel matrix.

torch.Size([28413, 2])
We keep 5.88e+06/3.77e+08 =  1% of the original kernel matrix.

torch.Size([23691, 2])
We keep 6.74e+06/2.43e+08 =  2% of the original kernel matrix.

torch.Size([28654, 2])
We keep 5.30e+06/3.06e+08 =  1% of the original kernel matrix.

torch.Size([10846, 2])
We keep 1.31e+06/2.97e+07 =  4% of the original kernel matrix.

torch.Size([19036, 2])
We keep 2.34e+06/1.07e+08 =  2% of the original kernel matrix.

torch.Size([22593, 2])
We keep 3.59e+06/1.58e+08 =  2% of the original kernel matrix.

torch.Size([28038, 2])
We keep 4.29e+06/2.47e+08 =  1% of the original kernel matrix.

torch.Size([36241, 2])
We keep 7.92e+06/4.18e+08 =  1% of the original kernel matrix.

torch.Size([35596, 2])
We keep 6.28e+06/4.01e+08 =  1% of the original kernel matrix.

torch.Size([272088, 2])
We keep 5.78e+08/3.45e+10 =  1% of the original kernel matrix.

torch.Size([98430, 2])
We keep 4.26e+07/3.65e+09 =  1% of the original kernel matrix.

torch.Size([13571, 2])
We keep 1.96e+06/5.55e+07 =  3% of the original kernel matrix.

torch.Size([21786, 2])
We keep 3.00e+06/1.46e+08 =  2% of the original kernel matrix.

torch.Size([6765, 2])
We keep 3.91e+05/8.18e+06 =  4% of the original kernel matrix.

torch.Size([15201, 2])
We keep 1.42e+06/5.62e+07 =  2% of the original kernel matrix.

torch.Size([11840, 2])
We keep 1.46e+06/3.80e+07 =  3% of the original kernel matrix.

torch.Size([19857, 2])
We keep 2.56e+06/1.21e+08 =  2% of the original kernel matrix.

torch.Size([239650, 2])
We keep 3.08e+08/2.22e+10 =  1% of the original kernel matrix.

torch.Size([92303, 2])
We keep 3.44e+07/2.93e+09 =  1% of the original kernel matrix.

torch.Size([352362, 2])
We keep 5.99e+08/5.03e+10 =  1% of the original kernel matrix.

torch.Size([112675, 2])
We keep 4.99e+07/4.41e+09 =  1% of the original kernel matrix.

torch.Size([107717, 2])
We keep 8.43e+07/4.87e+09 =  1% of the original kernel matrix.

torch.Size([59017, 2])
We keep 1.79e+07/1.37e+09 =  1% of the original kernel matrix.

torch.Size([15915, 2])
We keep 2.60e+06/8.25e+07 =  3% of the original kernel matrix.

torch.Size([23675, 2])
We keep 3.48e+06/1.78e+08 =  1% of the original kernel matrix.

torch.Size([177768, 2])
We keep 2.91e+08/1.64e+10 =  1% of the original kernel matrix.

torch.Size([78736, 2])
We keep 3.10e+07/2.51e+09 =  1% of the original kernel matrix.

torch.Size([228517, 2])
We keep 2.29e+08/2.44e+10 =  0% of the original kernel matrix.

torch.Size([94849, 2])
We keep 3.70e+07/3.07e+09 =  1% of the original kernel matrix.

torch.Size([4955, 2])
We keep 2.74e+05/4.85e+06 =  5% of the original kernel matrix.

torch.Size([13368, 2])
We keep 1.22e+06/4.33e+07 =  2% of the original kernel matrix.

torch.Size([18279, 2])
We keep 5.22e+06/1.10e+08 =  4% of the original kernel matrix.

torch.Size([24897, 2])
We keep 3.76e+06/2.06e+08 =  1% of the original kernel matrix.

torch.Size([57604, 2])
We keep 1.62e+07/1.00e+09 =  1% of the original kernel matrix.

torch.Size([43762, 2])
We keep 9.01e+06/6.21e+08 =  1% of the original kernel matrix.

torch.Size([41398, 2])
We keep 4.79e+07/1.17e+09 =  4% of the original kernel matrix.

torch.Size([36098, 2])
We keep 9.86e+06/6.72e+08 =  1% of the original kernel matrix.

torch.Size([40403, 2])
We keep 1.67e+07/6.12e+08 =  2% of the original kernel matrix.

torch.Size([36727, 2])
We keep 7.12e+06/4.86e+08 =  1% of the original kernel matrix.

torch.Size([235348, 2])
We keep 2.75e+08/2.23e+10 =  1% of the original kernel matrix.

torch.Size([91505, 2])
We keep 3.50e+07/2.93e+09 =  1% of the original kernel matrix.

torch.Size([26964, 2])
We keep 5.12e+06/2.63e+08 =  1% of the original kernel matrix.

torch.Size([31460, 2])
We keep 5.33e+06/3.18e+08 =  1% of the original kernel matrix.

torch.Size([21207, 2])
We keep 2.64e+07/1.77e+08 = 14% of the original kernel matrix.

torch.Size([26678, 2])
We keep 4.57e+06/2.61e+08 =  1% of the original kernel matrix.

torch.Size([6834, 2])
We keep 4.44e+05/9.09e+06 =  4% of the original kernel matrix.

torch.Size([15339, 2])
We keep 1.51e+06/5.92e+07 =  2% of the original kernel matrix.

torch.Size([40197, 2])
We keep 1.81e+07/6.26e+08 =  2% of the original kernel matrix.

torch.Size([36642, 2])
We keep 7.32e+06/4.91e+08 =  1% of the original kernel matrix.

torch.Size([27813, 2])
We keep 8.61e+06/3.27e+08 =  2% of the original kernel matrix.

torch.Size([31644, 2])
We keep 5.92e+06/3.55e+08 =  1% of the original kernel matrix.

torch.Size([49053, 2])
We keep 1.65e+07/8.56e+08 =  1% of the original kernel matrix.

torch.Size([40407, 2])
We keep 8.52e+06/5.75e+08 =  1% of the original kernel matrix.

torch.Size([215147, 2])
We keep 5.25e+08/2.02e+10 =  2% of the original kernel matrix.

torch.Size([87799, 2])
We keep 3.40e+07/2.79e+09 =  1% of the original kernel matrix.

torch.Size([720257, 2])
We keep 1.58e+09/2.01e+11 =  0% of the original kernel matrix.

torch.Size([167941, 2])
We keep 9.40e+07/8.80e+09 =  1% of the original kernel matrix.

torch.Size([12951, 2])
We keep 1.38e+06/3.91e+07 =  3% of the original kernel matrix.

torch.Size([20791, 2])
We keep 2.49e+06/1.23e+08 =  2% of the original kernel matrix.

torch.Size([6245, 2])
We keep 3.17e+05/6.59e+06 =  4% of the original kernel matrix.

torch.Size([14624, 2])
We keep 1.29e+06/5.04e+07 =  2% of the original kernel matrix.

torch.Size([16531, 2])
We keep 2.58e+06/9.56e+07 =  2% of the original kernel matrix.

torch.Size([24147, 2])
We keep 3.68e+06/1.92e+08 =  1% of the original kernel matrix.

torch.Size([54959, 2])
We keep 1.74e+07/1.10e+09 =  1% of the original kernel matrix.

torch.Size([42958, 2])
We keep 9.41e+06/6.50e+08 =  1% of the original kernel matrix.

torch.Size([45516, 2])
We keep 1.66e+07/7.89e+08 =  2% of the original kernel matrix.

torch.Size([39097, 2])
We keep 8.27e+06/5.52e+08 =  1% of the original kernel matrix.

torch.Size([8228, 2])
We keep 8.89e+05/1.43e+07 =  6% of the original kernel matrix.

torch.Size([16757, 2])
We keep 1.66e+06/7.43e+07 =  2% of the original kernel matrix.

torch.Size([13021, 2])
We keep 1.44e+06/3.84e+07 =  3% of the original kernel matrix.

torch.Size([20829, 2])
We keep 2.51e+06/1.22e+08 =  2% of the original kernel matrix.

torch.Size([40257, 2])
We keep 1.14e+07/5.33e+08 =  2% of the original kernel matrix.

torch.Size([36985, 2])
We keep 6.98e+06/4.53e+08 =  1% of the original kernel matrix.

torch.Size([7493, 2])
We keep 6.33e+05/1.29e+07 =  4% of the original kernel matrix.

torch.Size([16204, 2])
We keep 1.73e+06/7.05e+07 =  2% of the original kernel matrix.

torch.Size([83196, 2])
We keep 1.08e+08/3.99e+09 =  2% of the original kernel matrix.

torch.Size([51844, 2])
We keep 1.69e+07/1.24e+09 =  1% of the original kernel matrix.

torch.Size([10894, 2])
We keep 1.08e+06/2.90e+07 =  3% of the original kernel matrix.

torch.Size([19177, 2])
We keep 2.33e+06/1.06e+08 =  2% of the original kernel matrix.

torch.Size([24682, 2])
We keep 3.69e+06/1.86e+08 =  1% of the original kernel matrix.

torch.Size([29643, 2])
We keep 4.57e+06/2.68e+08 =  1% of the original kernel matrix.

torch.Size([15782, 2])
We keep 2.87e+06/8.50e+07 =  3% of the original kernel matrix.

torch.Size([23649, 2])
We keep 3.47e+06/1.81e+08 =  1% of the original kernel matrix.

torch.Size([16098, 2])
We keep 1.63e+06/6.04e+07 =  2% of the original kernel matrix.

torch.Size([23443, 2])
We keep 2.96e+06/1.53e+08 =  1% of the original kernel matrix.

torch.Size([6676, 2])
We keep 4.41e+05/9.44e+06 =  4% of the original kernel matrix.

torch.Size([15093, 2])
We keep 1.53e+06/6.03e+07 =  2% of the original kernel matrix.

torch.Size([234512, 2])
We keep 1.92e+08/1.84e+10 =  1% of the original kernel matrix.

torch.Size([91595, 2])
We keep 3.16e+07/2.66e+09 =  1% of the original kernel matrix.

torch.Size([553607, 2])
We keep 1.22e+09/1.21e+11 =  1% of the original kernel matrix.

torch.Size([141301, 2])
We keep 7.47e+07/6.84e+09 =  1% of the original kernel matrix.

torch.Size([210347, 2])
We keep 2.64e+08/1.74e+10 =  1% of the original kernel matrix.

torch.Size([85960, 2])
We keep 3.10e+07/2.59e+09 =  1% of the original kernel matrix.

torch.Size([114448, 2])
We keep 9.91e+07/5.50e+09 =  1% of the original kernel matrix.

torch.Size([61519, 2])
We keep 1.90e+07/1.46e+09 =  1% of the original kernel matrix.

torch.Size([35658, 2])
We keep 7.76e+06/4.35e+08 =  1% of the original kernel matrix.

torch.Size([35301, 2])
We keep 6.46e+06/4.10e+08 =  1% of the original kernel matrix.

torch.Size([16798, 2])
We keep 3.24e+06/9.62e+07 =  3% of the original kernel matrix.

torch.Size([23895, 2])
We keep 3.63e+06/1.93e+08 =  1% of the original kernel matrix.

torch.Size([23052, 2])
We keep 6.17e+06/2.09e+08 =  2% of the original kernel matrix.

torch.Size([28200, 2])
We keep 4.96e+06/2.84e+08 =  1% of the original kernel matrix.

torch.Size([217438, 2])
We keep 1.90e+08/1.91e+10 =  0% of the original kernel matrix.

torch.Size([88463, 2])
We keep 3.24e+07/2.72e+09 =  1% of the original kernel matrix.

torch.Size([32384, 2])
We keep 1.73e+07/4.74e+08 =  3% of the original kernel matrix.

torch.Size([33775, 2])
We keep 6.94e+06/4.28e+08 =  1% of the original kernel matrix.

torch.Size([42953, 2])
We keep 3.74e+07/7.29e+08 =  5% of the original kernel matrix.

torch.Size([38063, 2])
We keep 8.19e+06/5.30e+08 =  1% of the original kernel matrix.

torch.Size([16924, 2])
We keep 3.25e+06/9.70e+07 =  3% of the original kernel matrix.

torch.Size([24066, 2])
We keep 3.68e+06/1.93e+08 =  1% of the original kernel matrix.

torch.Size([1063653, 2])
We keep 6.13e+09/5.03e+11 =  1% of the original kernel matrix.

torch.Size([200386, 2])
We keep 1.46e+08/1.39e+10 =  1% of the original kernel matrix.

torch.Size([44708, 2])
We keep 2.65e+07/7.66e+08 =  3% of the original kernel matrix.

torch.Size([38786, 2])
We keep 8.08e+06/5.44e+08 =  1% of the original kernel matrix.

torch.Size([76656, 2])
We keep 5.06e+07/2.43e+09 =  2% of the original kernel matrix.

torch.Size([50830, 2])
We keep 1.36e+07/9.67e+08 =  1% of the original kernel matrix.

torch.Size([237470, 2])
We keep 3.87e+08/2.58e+10 =  1% of the original kernel matrix.

torch.Size([92884, 2])
We keep 3.70e+07/3.15e+09 =  1% of the original kernel matrix.

torch.Size([67951, 2])
We keep 2.35e+07/1.55e+09 =  1% of the original kernel matrix.

torch.Size([47257, 2])
We keep 1.09e+07/7.74e+08 =  1% of the original kernel matrix.

torch.Size([44452, 2])
We keep 1.58e+07/7.59e+08 =  2% of the original kernel matrix.

torch.Size([38768, 2])
We keep 8.14e+06/5.41e+08 =  1% of the original kernel matrix.

torch.Size([454600, 2])
We keep 1.37e+09/8.28e+10 =  1% of the original kernel matrix.

torch.Size([127147, 2])
We keep 6.37e+07/5.65e+09 =  1% of the original kernel matrix.

torch.Size([153249, 2])
We keep 1.04e+08/9.20e+09 =  1% of the original kernel matrix.

torch.Size([72759, 2])
We keep 2.35e+07/1.88e+09 =  1% of the original kernel matrix.

torch.Size([102043, 2])
We keep 5.94e+07/3.45e+09 =  1% of the original kernel matrix.

torch.Size([58033, 2])
We keep 1.53e+07/1.15e+09 =  1% of the original kernel matrix.

torch.Size([29114, 2])
We keep 1.02e+07/3.34e+08 =  3% of the original kernel matrix.

torch.Size([31642, 2])
We keep 5.78e+06/3.59e+08 =  1% of the original kernel matrix.

torch.Size([69872, 2])
We keep 3.63e+07/1.72e+09 =  2% of the original kernel matrix.

torch.Size([47760, 2])
We keep 1.13e+07/8.15e+08 =  1% of the original kernel matrix.

torch.Size([16365, 2])
We keep 2.93e+06/7.74e+07 =  3% of the original kernel matrix.

torch.Size([23654, 2])
We keep 3.25e+06/1.73e+08 =  1% of the original kernel matrix.

torch.Size([231579, 2])
We keep 3.19e+08/2.10e+10 =  1% of the original kernel matrix.

torch.Size([91754, 2])
We keep 3.42e+07/2.85e+09 =  1% of the original kernel matrix.

torch.Size([1456330, 2])
We keep 5.40e+09/7.26e+11 =  0% of the original kernel matrix.

torch.Size([238128, 2])
We keep 1.70e+08/1.67e+10 =  1% of the original kernel matrix.

torch.Size([171466, 2])
We keep 5.63e+08/1.57e+10 =  3% of the original kernel matrix.

torch.Size([77539, 2])
We keep 3.07e+07/2.46e+09 =  1% of the original kernel matrix.

torch.Size([190060, 2])
We keep 3.49e+08/2.26e+10 =  1% of the original kernel matrix.

torch.Size([80294, 2])
We keep 3.53e+07/2.95e+09 =  1% of the original kernel matrix.

torch.Size([51675, 2])
We keep 6.11e+07/1.07e+09 =  5% of the original kernel matrix.

torch.Size([41561, 2])
We keep 9.53e+06/6.42e+08 =  1% of the original kernel matrix.

torch.Size([77050, 2])
We keep 1.07e+08/3.02e+09 =  3% of the original kernel matrix.

torch.Size([49752, 2])
We keep 1.47e+07/1.08e+09 =  1% of the original kernel matrix.

torch.Size([62962, 2])
We keep 7.57e+07/1.84e+09 =  4% of the original kernel matrix.

torch.Size([46131, 2])
We keep 1.22e+07/8.43e+08 =  1% of the original kernel matrix.

torch.Size([20936, 2])
We keep 4.20e+06/1.54e+08 =  2% of the original kernel matrix.

torch.Size([26563, 2])
We keep 4.14e+06/2.44e+08 =  1% of the original kernel matrix.

torch.Size([30029, 2])
We keep 9.45e+06/3.57e+08 =  2% of the original kernel matrix.

torch.Size([32790, 2])
We keep 6.01e+06/3.71e+08 =  1% of the original kernel matrix.

torch.Size([557143, 2])
We keep 9.29e+08/1.19e+11 =  0% of the original kernel matrix.

torch.Size([142382, 2])
We keep 7.37e+07/6.77e+09 =  1% of the original kernel matrix.

torch.Size([67601, 2])
We keep 8.34e+07/3.27e+09 =  2% of the original kernel matrix.

torch.Size([44314, 2])
We keep 1.53e+07/1.12e+09 =  1% of the original kernel matrix.

torch.Size([181148, 2])
We keep 5.13e+08/1.93e+10 =  2% of the original kernel matrix.

torch.Size([77085, 2])
We keep 3.18e+07/2.73e+09 =  1% of the original kernel matrix.

torch.Size([173772, 2])
We keep 3.11e+08/1.33e+10 =  2% of the original kernel matrix.

torch.Size([77410, 2])
We keep 2.79e+07/2.27e+09 =  1% of the original kernel matrix.

torch.Size([72206, 2])
We keep 3.01e+07/1.82e+09 =  1% of the original kernel matrix.

torch.Size([48779, 2])
We keep 1.16e+07/8.38e+08 =  1% of the original kernel matrix.

torch.Size([10967, 2])
We keep 1.75e+06/3.13e+07 =  5% of the original kernel matrix.

torch.Size([19102, 2])
We keep 2.35e+06/1.10e+08 =  2% of the original kernel matrix.

torch.Size([78487, 2])
We keep 3.95e+07/2.33e+09 =  1% of the original kernel matrix.

torch.Size([51254, 2])
We keep 1.29e+07/9.48e+08 =  1% of the original kernel matrix.

torch.Size([189025, 2])
We keep 1.74e+08/1.53e+10 =  1% of the original kernel matrix.

torch.Size([81394, 2])
We keep 2.94e+07/2.43e+09 =  1% of the original kernel matrix.

torch.Size([8989, 2])
We keep 1.29e+06/2.25e+07 =  5% of the original kernel matrix.

torch.Size([17314, 2])
We keep 2.12e+06/9.32e+07 =  2% of the original kernel matrix.

torch.Size([1058003, 2])
We keep 2.66e+09/3.29e+11 =  0% of the original kernel matrix.

torch.Size([197254, 2])
We keep 1.16e+08/1.13e+10 =  1% of the original kernel matrix.

torch.Size([62206, 2])
We keep 4.23e+07/1.47e+09 =  2% of the original kernel matrix.

torch.Size([45621, 2])
We keep 1.04e+07/7.54e+08 =  1% of the original kernel matrix.

torch.Size([60340, 2])
We keep 3.74e+07/1.52e+09 =  2% of the original kernel matrix.

torch.Size([44443, 2])
We keep 1.11e+07/7.65e+08 =  1% of the original kernel matrix.

torch.Size([14317, 2])
We keep 3.76e+06/6.08e+07 =  6% of the original kernel matrix.

torch.Size([22140, 2])
We keep 2.95e+06/1.53e+08 =  1% of the original kernel matrix.

torch.Size([28554, 2])
We keep 5.66e+06/2.69e+08 =  2% of the original kernel matrix.

torch.Size([29820, 2])
We keep 4.94e+06/3.22e+08 =  1% of the original kernel matrix.

torch.Size([6747, 2])
We keep 3.90e+05/7.99e+06 =  4% of the original kernel matrix.

torch.Size([15272, 2])
We keep 1.40e+06/5.55e+07 =  2% of the original kernel matrix.

torch.Size([9791, 2])
We keep 1.37e+06/2.21e+07 =  6% of the original kernel matrix.

torch.Size([18073, 2])
We keep 2.01e+06/9.22e+07 =  2% of the original kernel matrix.

torch.Size([39797, 2])
We keep 2.26e+07/5.50e+08 =  4% of the original kernel matrix.

torch.Size([36641, 2])
We keep 7.17e+06/4.60e+08 =  1% of the original kernel matrix.

torch.Size([135799, 2])
We keep 2.71e+08/1.12e+10 =  2% of the original kernel matrix.

torch.Size([66944, 2])
We keep 2.52e+07/2.08e+09 =  1% of the original kernel matrix.

torch.Size([3378836, 2])
We keep 5.21e+10/5.54e+12 =  0% of the original kernel matrix.

torch.Size([331440, 2])
We keep 4.47e+08/4.62e+10 =  0% of the original kernel matrix.

torch.Size([13602, 2])
We keep 2.47e+06/5.09e+07 =  4% of the original kernel matrix.

torch.Size([21402, 2])
We keep 2.81e+06/1.40e+08 =  2% of the original kernel matrix.

torch.Size([189259, 2])
We keep 1.29e+08/1.19e+10 =  1% of the original kernel matrix.

torch.Size([81335, 2])
We keep 2.60e+07/2.14e+09 =  1% of the original kernel matrix.

torch.Size([17114, 2])
We keep 4.11e+06/9.18e+07 =  4% of the original kernel matrix.

torch.Size([24225, 2])
We keep 3.61e+06/1.88e+08 =  1% of the original kernel matrix.

torch.Size([29644, 2])
We keep 7.91e+06/3.48e+08 =  2% of the original kernel matrix.

torch.Size([32742, 2])
We keep 6.12e+06/3.66e+08 =  1% of the original kernel matrix.

torch.Size([16184, 2])
We keep 4.31e+06/1.02e+08 =  4% of the original kernel matrix.

torch.Size([23024, 2])
We keep 3.72e+06/1.98e+08 =  1% of the original kernel matrix.

torch.Size([286206, 2])
We keep 7.36e+08/3.73e+10 =  1% of the original kernel matrix.

torch.Size([99469, 2])
We keep 4.40e+07/3.79e+09 =  1% of the original kernel matrix.

torch.Size([21082, 2])
We keep 5.28e+06/1.49e+08 =  3% of the original kernel matrix.

torch.Size([26918, 2])
We keep 4.15e+06/2.39e+08 =  1% of the original kernel matrix.

torch.Size([1707231, 2])
We keep 6.61e+09/9.49e+11 =  0% of the original kernel matrix.

torch.Size([259053, 2])
We keep 1.91e+08/1.91e+10 =  0% of the original kernel matrix.

torch.Size([11006, 2])
We keep 1.24e+06/3.11e+07 =  3% of the original kernel matrix.

torch.Size([19365, 2])
We keep 2.35e+06/1.09e+08 =  2% of the original kernel matrix.

torch.Size([256929, 2])
We keep 4.25e+08/2.95e+10 =  1% of the original kernel matrix.

torch.Size([96307, 2])
We keep 4.01e+07/3.38e+09 =  1% of the original kernel matrix.

torch.Size([54914, 2])
We keep 6.13e+07/1.24e+09 =  4% of the original kernel matrix.

torch.Size([43050, 2])
We keep 1.03e+07/6.93e+08 =  1% of the original kernel matrix.

torch.Size([156199, 2])
We keep 1.74e+08/1.20e+10 =  1% of the original kernel matrix.

torch.Size([73418, 2])
We keep 2.68e+07/2.15e+09 =  1% of the original kernel matrix.

torch.Size([246681, 2])
We keep 4.51e+08/3.00e+10 =  1% of the original kernel matrix.

torch.Size([92862, 2])
We keep 3.96e+07/3.40e+09 =  1% of the original kernel matrix.

torch.Size([29629, 2])
We keep 6.74e+06/2.75e+08 =  2% of the original kernel matrix.

torch.Size([30197, 2])
We keep 5.01e+06/3.26e+08 =  1% of the original kernel matrix.

torch.Size([18171, 2])
We keep 3.31e+06/1.07e+08 =  3% of the original kernel matrix.

torch.Size([25135, 2])
We keep 3.79e+06/2.03e+08 =  1% of the original kernel matrix.

torch.Size([126611, 2])
We keep 8.08e+07/6.69e+09 =  1% of the original kernel matrix.

torch.Size([66620, 2])
We keep 2.06e+07/1.61e+09 =  1% of the original kernel matrix.

torch.Size([16409, 2])
We keep 2.62e+06/9.48e+07 =  2% of the original kernel matrix.

torch.Size([23864, 2])
We keep 3.64e+06/1.91e+08 =  1% of the original kernel matrix.

torch.Size([19095, 2])
We keep 5.32e+06/1.65e+08 =  3% of the original kernel matrix.

torch.Size([25161, 2])
We keep 4.44e+06/2.52e+08 =  1% of the original kernel matrix.

torch.Size([140383, 2])
We keep 1.32e+08/9.27e+09 =  1% of the original kernel matrix.

torch.Size([68683, 2])
We keep 2.37e+07/1.89e+09 =  1% of the original kernel matrix.

torch.Size([36973, 2])
We keep 2.89e+07/7.08e+08 =  4% of the original kernel matrix.

torch.Size([35109, 2])
We keep 7.90e+06/5.23e+08 =  1% of the original kernel matrix.

torch.Size([85669, 2])
We keep 7.49e+07/3.25e+09 =  2% of the original kernel matrix.

torch.Size([53871, 2])
We keep 1.54e+07/1.12e+09 =  1% of the original kernel matrix.

torch.Size([35073, 2])
We keep 1.33e+07/4.48e+08 =  2% of the original kernel matrix.

torch.Size([34268, 2])
We keep 6.54e+06/4.16e+08 =  1% of the original kernel matrix.

torch.Size([248492, 2])
We keep 4.91e+08/2.73e+10 =  1% of the original kernel matrix.

torch.Size([94236, 2])
We keep 3.84e+07/3.25e+09 =  1% of the original kernel matrix.

torch.Size([352082, 2])
We keep 1.09e+09/5.93e+10 =  1% of the original kernel matrix.

torch.Size([110794, 2])
We keep 5.42e+07/4.78e+09 =  1% of the original kernel matrix.

torch.Size([83078, 2])
We keep 3.66e+07/2.09e+09 =  1% of the original kernel matrix.

torch.Size([51666, 2])
We keep 1.24e+07/8.98e+08 =  1% of the original kernel matrix.

torch.Size([14317, 2])
We keep 2.04e+06/6.61e+07 =  3% of the original kernel matrix.

torch.Size([21953, 2])
We keep 3.19e+06/1.60e+08 =  1% of the original kernel matrix.

torch.Size([135321, 2])
We keep 1.91e+08/7.78e+09 =  2% of the original kernel matrix.

torch.Size([67633, 2])
We keep 2.17e+07/1.73e+09 =  1% of the original kernel matrix.

torch.Size([94440, 2])
We keep 7.35e+07/4.05e+09 =  1% of the original kernel matrix.

torch.Size([55233, 2])
We keep 1.67e+07/1.25e+09 =  1% of the original kernel matrix.

torch.Size([56328, 2])
We keep 3.38e+07/1.20e+09 =  2% of the original kernel matrix.

torch.Size([42977, 2])
We keep 9.60e+06/6.81e+08 =  1% of the original kernel matrix.

torch.Size([69357, 2])
We keep 3.70e+07/1.70e+09 =  2% of the original kernel matrix.

torch.Size([47409, 2])
We keep 1.14e+07/8.10e+08 =  1% of the original kernel matrix.

torch.Size([69302, 2])
We keep 4.23e+07/1.96e+09 =  2% of the original kernel matrix.

torch.Size([46828, 2])
We keep 1.20e+07/8.69e+08 =  1% of the original kernel matrix.

torch.Size([78099, 2])
We keep 7.51e+07/2.64e+09 =  2% of the original kernel matrix.

torch.Size([50512, 2])
We keep 1.39e+07/1.01e+09 =  1% of the original kernel matrix.

torch.Size([207090, 2])
We keep 7.08e+08/3.25e+10 =  2% of the original kernel matrix.

torch.Size([78529, 2])
We keep 4.02e+07/3.54e+09 =  1% of the original kernel matrix.

torch.Size([77997, 2])
We keep 1.06e+08/3.00e+09 =  3% of the original kernel matrix.

torch.Size([50596, 2])
We keep 1.46e+07/1.08e+09 =  1% of the original kernel matrix.

torch.Size([22487, 2])
We keep 4.20e+06/1.87e+08 =  2% of the original kernel matrix.

torch.Size([28111, 2])
We keep 4.69e+06/2.69e+08 =  1% of the original kernel matrix.

torch.Size([10944, 2])
We keep 1.25e+06/3.35e+07 =  3% of the original kernel matrix.

torch.Size([19137, 2])
We keep 2.46e+06/1.14e+08 =  2% of the original kernel matrix.

torch.Size([29792, 2])
We keep 1.58e+07/3.98e+08 =  3% of the original kernel matrix.

torch.Size([32405, 2])
We keep 6.26e+06/3.92e+08 =  1% of the original kernel matrix.

torch.Size([101716, 2])
We keep 1.00e+08/4.91e+09 =  2% of the original kernel matrix.

torch.Size([57250, 2])
We keep 1.79e+07/1.38e+09 =  1% of the original kernel matrix.

torch.Size([185597, 2])
We keep 1.81e+08/1.27e+10 =  1% of the original kernel matrix.

torch.Size([80315, 2])
We keep 2.68e+07/2.21e+09 =  1% of the original kernel matrix.

torch.Size([140794, 2])
We keep 1.32e+08/7.50e+09 =  1% of the original kernel matrix.

torch.Size([69301, 2])
We keep 2.15e+07/1.70e+09 =  1% of the original kernel matrix.

torch.Size([19113, 2])
We keep 2.85e+06/1.04e+08 =  2% of the original kernel matrix.

torch.Size([25625, 2])
We keep 3.69e+06/2.00e+08 =  1% of the original kernel matrix.

torch.Size([37895, 2])
We keep 8.27e+06/4.75e+08 =  1% of the original kernel matrix.

torch.Size([35896, 2])
We keep 6.69e+06/4.28e+08 =  1% of the original kernel matrix.

torch.Size([95292, 2])
We keep 6.39e+07/3.46e+09 =  1% of the original kernel matrix.

torch.Size([56092, 2])
We keep 1.54e+07/1.16e+09 =  1% of the original kernel matrix.

torch.Size([179403, 2])
We keep 4.02e+08/1.44e+10 =  2% of the original kernel matrix.

torch.Size([79443, 2])
We keep 2.83e+07/2.36e+09 =  1% of the original kernel matrix.

torch.Size([76363, 2])
We keep 6.40e+07/2.61e+09 =  2% of the original kernel matrix.

torch.Size([49887, 2])
We keep 1.39e+07/1.00e+09 =  1% of the original kernel matrix.

torch.Size([112902, 2])
We keep 5.89e+07/4.19e+09 =  1% of the original kernel matrix.

torch.Size([61488, 2])
We keep 1.67e+07/1.27e+09 =  1% of the original kernel matrix.

torch.Size([12481, 2])
We keep 3.40e+06/5.43e+07 =  6% of the original kernel matrix.

torch.Size([20579, 2])
We keep 2.84e+06/1.45e+08 =  1% of the original kernel matrix.

torch.Size([49139, 2])
We keep 2.05e+07/8.66e+08 =  2% of the original kernel matrix.

torch.Size([40608, 2])
We keep 8.50e+06/5.78e+08 =  1% of the original kernel matrix.

torch.Size([4860, 2])
We keep 2.63e+05/4.64e+06 =  5% of the original kernel matrix.

torch.Size([13487, 2])
We keep 1.19e+06/4.23e+07 =  2% of the original kernel matrix.

torch.Size([12510, 2])
We keep 1.54e+06/4.31e+07 =  3% of the original kernel matrix.

torch.Size([20644, 2])
We keep 2.67e+06/1.29e+08 =  2% of the original kernel matrix.

torch.Size([249136, 2])
We keep 2.51e+08/2.20e+10 =  1% of the original kernel matrix.

torch.Size([94965, 2])
We keep 3.38e+07/2.91e+09 =  1% of the original kernel matrix.

torch.Size([47314, 2])
We keep 2.74e+07/9.07e+08 =  3% of the original kernel matrix.

torch.Size([39499, 2])
We keep 8.63e+06/5.91e+08 =  1% of the original kernel matrix.

torch.Size([108112, 2])
We keep 4.60e+07/3.66e+09 =  1% of the original kernel matrix.

torch.Size([60050, 2])
We keep 1.55e+07/1.19e+09 =  1% of the original kernel matrix.

torch.Size([6187, 2])
We keep 3.08e+05/5.98e+06 =  5% of the original kernel matrix.

torch.Size([14890, 2])
We keep 1.26e+06/4.80e+07 =  2% of the original kernel matrix.

torch.Size([8434, 2])
We keep 6.15e+05/1.39e+07 =  4% of the original kernel matrix.

torch.Size([17126, 2])
We keep 1.75e+06/7.33e+07 =  2% of the original kernel matrix.

torch.Size([119915, 2])
We keep 1.21e+08/5.96e+09 =  2% of the original kernel matrix.

torch.Size([63180, 2])
We keep 1.88e+07/1.52e+09 =  1% of the original kernel matrix.

torch.Size([114405, 2])
We keep 7.39e+07/5.47e+09 =  1% of the original kernel matrix.

torch.Size([61557, 2])
We keep 1.88e+07/1.45e+09 =  1% of the original kernel matrix.

torch.Size([71752, 2])
We keep 2.92e+07/1.54e+09 =  1% of the original kernel matrix.

torch.Size([48123, 2])
We keep 1.08e+07/7.70e+08 =  1% of the original kernel matrix.

torch.Size([9062, 2])
We keep 6.12e+05/1.52e+07 =  4% of the original kernel matrix.

torch.Size([17616, 2])
We keep 1.77e+06/7.66e+07 =  2% of the original kernel matrix.

torch.Size([12502, 2])
We keep 1.28e+06/3.64e+07 =  3% of the original kernel matrix.

torch.Size([20382, 2])
We keep 2.47e+06/1.18e+08 =  2% of the original kernel matrix.

torch.Size([83946, 2])
We keep 6.46e+07/2.56e+09 =  2% of the original kernel matrix.

torch.Size([52082, 2])
We keep 1.38e+07/9.94e+08 =  1% of the original kernel matrix.

torch.Size([52778, 2])
We keep 1.76e+07/1.00e+09 =  1% of the original kernel matrix.

torch.Size([42065, 2])
We keep 9.21e+06/6.22e+08 =  1% of the original kernel matrix.

torch.Size([175003, 2])
We keep 2.42e+08/1.27e+10 =  1% of the original kernel matrix.

torch.Size([78096, 2])
We keep 2.73e+07/2.22e+09 =  1% of the original kernel matrix.

torch.Size([12414, 2])
We keep 1.73e+06/4.13e+07 =  4% of the original kernel matrix.

torch.Size([20491, 2])
We keep 2.56e+06/1.26e+08 =  2% of the original kernel matrix.

torch.Size([7908, 2])
We keep 7.16e+05/1.34e+07 =  5% of the original kernel matrix.

torch.Size([16182, 2])
We keep 1.71e+06/7.18e+07 =  2% of the original kernel matrix.

torch.Size([42251, 2])
We keep 1.79e+07/7.06e+08 =  2% of the original kernel matrix.

torch.Size([37004, 2])
We keep 7.97e+06/5.22e+08 =  1% of the original kernel matrix.

torch.Size([18896, 2])
We keep 3.81e+06/1.19e+08 =  3% of the original kernel matrix.

torch.Size([25646, 2])
We keep 3.91e+06/2.14e+08 =  1% of the original kernel matrix.

torch.Size([398366, 2])
We keep 7.29e+08/5.97e+10 =  1% of the original kernel matrix.

torch.Size([118965, 2])
We keep 5.40e+07/4.80e+09 =  1% of the original kernel matrix.

torch.Size([12296, 2])
We keep 1.56e+06/3.56e+07 =  4% of the original kernel matrix.

torch.Size([20342, 2])
We keep 2.36e+06/1.17e+08 =  2% of the original kernel matrix.

torch.Size([92391, 2])
We keep 5.38e+07/3.01e+09 =  1% of the original kernel matrix.

torch.Size([55031, 2])
We keep 1.44e+07/1.08e+09 =  1% of the original kernel matrix.

torch.Size([150925, 2])
We keep 1.47e+08/1.02e+10 =  1% of the original kernel matrix.

torch.Size([72098, 2])
We keep 2.50e+07/1.98e+09 =  1% of the original kernel matrix.

torch.Size([146710, 2])
We keep 3.43e+08/8.58e+09 =  3% of the original kernel matrix.

torch.Size([70788, 2])
We keep 2.32e+07/1.82e+09 =  1% of the original kernel matrix.

torch.Size([234473, 2])
We keep 4.43e+08/2.61e+10 =  1% of the original kernel matrix.

torch.Size([92295, 2])
We keep 3.83e+07/3.17e+09 =  1% of the original kernel matrix.

torch.Size([209024, 2])
We keep 2.32e+08/1.94e+10 =  1% of the original kernel matrix.

torch.Size([86454, 2])
We keep 3.28e+07/2.74e+09 =  1% of the original kernel matrix.

torch.Size([7720, 2])
We keep 4.60e+05/1.07e+07 =  4% of the original kernel matrix.

torch.Size([16178, 2])
We keep 1.55e+06/6.42e+07 =  2% of the original kernel matrix.

torch.Size([79436, 2])
We keep 3.52e+07/2.20e+09 =  1% of the original kernel matrix.

torch.Size([50747, 2])
We keep 1.27e+07/9.21e+08 =  1% of the original kernel matrix.

torch.Size([31974, 2])
We keep 6.76e+06/3.67e+08 =  1% of the original kernel matrix.

torch.Size([34002, 2])
We keep 6.07e+06/3.76e+08 =  1% of the original kernel matrix.

torch.Size([215831, 2])
We keep 3.72e+08/1.67e+10 =  2% of the original kernel matrix.

torch.Size([87248, 2])
We keep 2.97e+07/2.54e+09 =  1% of the original kernel matrix.

torch.Size([134689, 2])
We keep 1.36e+08/7.44e+09 =  1% of the original kernel matrix.

torch.Size([67316, 2])
We keep 2.12e+07/1.69e+09 =  1% of the original kernel matrix.

torch.Size([60831, 2])
We keep 3.18e+07/1.45e+09 =  2% of the original kernel matrix.

torch.Size([43866, 2])
We keep 1.06e+07/7.48e+08 =  1% of the original kernel matrix.

torch.Size([106583, 2])
We keep 7.19e+07/4.15e+09 =  1% of the original kernel matrix.

torch.Size([59658, 2])
We keep 1.67e+07/1.26e+09 =  1% of the original kernel matrix.

torch.Size([19455, 2])
We keep 2.53e+06/1.05e+08 =  2% of the original kernel matrix.

torch.Size([25816, 2])
We keep 3.65e+06/2.01e+08 =  1% of the original kernel matrix.

torch.Size([42445, 2])
We keep 3.69e+07/1.21e+09 =  3% of the original kernel matrix.

torch.Size([36102, 2])
We keep 1.00e+07/6.84e+08 =  1% of the original kernel matrix.

torch.Size([10820, 2])
We keep 1.16e+06/2.89e+07 =  3% of the original kernel matrix.

torch.Size([19090, 2])
We keep 2.27e+06/1.06e+08 =  2% of the original kernel matrix.

torch.Size([28483, 2])
We keep 1.22e+07/2.97e+08 =  4% of the original kernel matrix.

torch.Size([30091, 2])
We keep 5.34e+06/3.39e+08 =  1% of the original kernel matrix.

torch.Size([322867, 2])
We keep 7.26e+08/3.79e+10 =  1% of the original kernel matrix.

torch.Size([107064, 2])
We keep 4.43e+07/3.82e+09 =  1% of the original kernel matrix.

torch.Size([327815, 2])
We keep 7.84e+08/5.01e+10 =  1% of the original kernel matrix.

torch.Size([108785, 2])
We keep 5.07e+07/4.40e+09 =  1% of the original kernel matrix.

torch.Size([21511, 2])
We keep 6.65e+06/2.02e+08 =  3% of the original kernel matrix.

torch.Size([27122, 2])
We keep 4.88e+06/2.79e+08 =  1% of the original kernel matrix.

torch.Size([34722, 2])
We keep 9.24e+06/4.04e+08 =  2% of the original kernel matrix.

torch.Size([35005, 2])
We keep 6.26e+06/3.95e+08 =  1% of the original kernel matrix.

torch.Size([31018, 2])
We keep 8.32e+06/3.33e+08 =  2% of the original kernel matrix.

torch.Size([33170, 2])
We keep 5.93e+06/3.58e+08 =  1% of the original kernel matrix.

torch.Size([249344, 2])
We keep 5.00e+08/3.06e+10 =  1% of the original kernel matrix.

torch.Size([94119, 2])
We keep 3.89e+07/3.43e+09 =  1% of the original kernel matrix.

torch.Size([45118, 2])
We keep 1.32e+07/7.37e+08 =  1% of the original kernel matrix.

torch.Size([39247, 2])
We keep 8.13e+06/5.33e+08 =  1% of the original kernel matrix.

torch.Size([21985, 2])
We keep 5.25e+06/1.63e+08 =  3% of the original kernel matrix.

torch.Size([27424, 2])
We keep 4.23e+06/2.51e+08 =  1% of the original kernel matrix.

torch.Size([85196, 2])
We keep 1.03e+08/3.42e+09 =  3% of the original kernel matrix.

torch.Size([53731, 2])
We keep 1.51e+07/1.15e+09 =  1% of the original kernel matrix.

torch.Size([197182, 2])
We keep 2.17e+08/1.66e+10 =  1% of the original kernel matrix.

torch.Size([83692, 2])
We keep 3.10e+07/2.53e+09 =  1% of the original kernel matrix.

torch.Size([19971, 2])
We keep 3.72e+06/1.30e+08 =  2% of the original kernel matrix.

torch.Size([25977, 2])
We keep 3.98e+06/2.24e+08 =  1% of the original kernel matrix.

torch.Size([89654, 2])
We keep 7.32e+07/3.09e+09 =  2% of the original kernel matrix.

torch.Size([54216, 2])
We keep 1.48e+07/1.09e+09 =  1% of the original kernel matrix.

torch.Size([12288, 2])
We keep 1.39e+06/3.55e+07 =  3% of the original kernel matrix.

torch.Size([20458, 2])
We keep 2.39e+06/1.17e+08 =  2% of the original kernel matrix.

time for making ranges is 4.126160383224487
Sorting X and nu_X
time for sorting X is 0.09069705009460449
Sorting Z and nu_Z
time for sorting Z is 0.00027632713317871094
Starting Optim
sum tnu_Z before tensor(40238232., device='cuda:0')
c= tensor(2457.6555, device='cuda:0')
c= tensor(226527.1094, device='cuda:0')
c= tensor(246877.6719, device='cuda:0')
c= tensor(365074.0938, device='cuda:0')
c= tensor(687156.1250, device='cuda:0')
c= tensor(1254737.7500, device='cuda:0')
c= tensor(2158288., device='cuda:0')
c= tensor(2777790., device='cuda:0')
c= tensor(2873341.7500, device='cuda:0')
c= tensor(26352480., device='cuda:0')
c= tensor(26391016., device='cuda:0')
c= tensor(31053804., device='cuda:0')
c= tensor(31076486., device='cuda:0')
c= tensor(60979560., device='cuda:0')
c= tensor(61193204., device='cuda:0')
c= tensor(62226312., device='cuda:0')
c= tensor(63300204., device='cuda:0')
c= tensor(64154752., device='cuda:0')
c= tensor(83172976., device='cuda:0')
c= tensor(87441912., device='cuda:0')
c= tensor(87903800., device='cuda:0')
c= tensor(1.2510e+08, device='cuda:0')
c= tensor(1.2516e+08, device='cuda:0')
c= tensor(1.2641e+08, device='cuda:0')
c= tensor(1.2683e+08, device='cuda:0')
c= tensor(1.2825e+08, device='cuda:0')
c= tensor(1.2987e+08, device='cuda:0')
c= tensor(1.2990e+08, device='cuda:0')
c= tensor(1.3484e+08, device='cuda:0')
c= tensor(7.0033e+08, device='cuda:0')
c= tensor(7.0045e+08, device='cuda:0')
c= tensor(8.3512e+08, device='cuda:0')
c= tensor(8.3527e+08, device='cuda:0')
c= tensor(8.3533e+08, device='cuda:0')
c= tensor(8.3543e+08, device='cuda:0')
c= tensor(8.4425e+08, device='cuda:0')
c= tensor(8.4957e+08, device='cuda:0')
c= tensor(8.4957e+08, device='cuda:0')
c= tensor(8.4959e+08, device='cuda:0')
c= tensor(8.4959e+08, device='cuda:0')
c= tensor(8.4960e+08, device='cuda:0')
c= tensor(8.4961e+08, device='cuda:0')
c= tensor(8.4961e+08, device='cuda:0')
c= tensor(8.4962e+08, device='cuda:0')
c= tensor(8.4962e+08, device='cuda:0')
c= tensor(8.4963e+08, device='cuda:0')
c= tensor(8.4965e+08, device='cuda:0')
c= tensor(8.4966e+08, device='cuda:0')
c= tensor(8.4967e+08, device='cuda:0')
c= tensor(8.4973e+08, device='cuda:0')
c= tensor(8.4980e+08, device='cuda:0')
c= tensor(8.4980e+08, device='cuda:0')
c= tensor(8.4982e+08, device='cuda:0')
c= tensor(8.4983e+08, device='cuda:0')
c= tensor(8.4984e+08, device='cuda:0')
c= tensor(8.4986e+08, device='cuda:0')
c= tensor(8.4986e+08, device='cuda:0')
c= tensor(8.4987e+08, device='cuda:0')
c= tensor(8.4988e+08, device='cuda:0')
c= tensor(8.4989e+08, device='cuda:0')
c= tensor(8.4990e+08, device='cuda:0')
c= tensor(8.4990e+08, device='cuda:0')
c= tensor(8.4993e+08, device='cuda:0')
c= tensor(8.4995e+08, device='cuda:0')
c= tensor(8.4996e+08, device='cuda:0')
c= tensor(8.4996e+08, device='cuda:0')
c= tensor(8.4997e+08, device='cuda:0')
c= tensor(8.4998e+08, device='cuda:0')
c= tensor(8.5000e+08, device='cuda:0')
c= tensor(8.5000e+08, device='cuda:0')
c= tensor(8.5001e+08, device='cuda:0')
c= tensor(8.5002e+08, device='cuda:0')
c= tensor(8.5003e+08, device='cuda:0')
c= tensor(8.5004e+08, device='cuda:0')
c= tensor(8.5005e+08, device='cuda:0')
c= tensor(8.5006e+08, device='cuda:0')
c= tensor(8.5006e+08, device='cuda:0')
c= tensor(8.5006e+08, device='cuda:0')
c= tensor(8.5007e+08, device='cuda:0')
c= tensor(8.5014e+08, device='cuda:0')
c= tensor(8.5015e+08, device='cuda:0')
c= tensor(8.5015e+08, device='cuda:0')
c= tensor(8.5017e+08, device='cuda:0')
c= tensor(8.5018e+08, device='cuda:0')
c= tensor(8.5018e+08, device='cuda:0')
c= tensor(8.5018e+08, device='cuda:0')
c= tensor(8.5020e+08, device='cuda:0')
c= tensor(8.5020e+08, device='cuda:0')
c= tensor(8.5021e+08, device='cuda:0')
c= tensor(8.5021e+08, device='cuda:0')
c= tensor(8.5022e+08, device='cuda:0')
c= tensor(8.5023e+08, device='cuda:0')
c= tensor(8.5023e+08, device='cuda:0')
c= tensor(8.5024e+08, device='cuda:0')
c= tensor(8.5025e+08, device='cuda:0')
c= tensor(8.5026e+08, device='cuda:0')
c= tensor(8.5026e+08, device='cuda:0')
c= tensor(8.5028e+08, device='cuda:0')
c= tensor(8.5029e+08, device='cuda:0')
c= tensor(8.5030e+08, device='cuda:0')
c= tensor(8.5035e+08, device='cuda:0')
c= tensor(8.5036e+08, device='cuda:0')
c= tensor(8.5039e+08, device='cuda:0')
c= tensor(8.5039e+08, device='cuda:0')
c= tensor(8.5040e+08, device='cuda:0')
c= tensor(8.5041e+08, device='cuda:0')
c= tensor(8.5042e+08, device='cuda:0')
c= tensor(8.5042e+08, device='cuda:0')
c= tensor(8.5043e+08, device='cuda:0')
c= tensor(8.5043e+08, device='cuda:0')
c= tensor(8.5043e+08, device='cuda:0')
c= tensor(8.5044e+08, device='cuda:0')
c= tensor(8.5044e+08, device='cuda:0')
c= tensor(8.5045e+08, device='cuda:0')
c= tensor(8.5046e+08, device='cuda:0')
c= tensor(8.5046e+08, device='cuda:0')
c= tensor(8.5046e+08, device='cuda:0')
c= tensor(8.5047e+08, device='cuda:0')
c= tensor(8.5049e+08, device='cuda:0')
c= tensor(8.5049e+08, device='cuda:0')
c= tensor(8.5053e+08, device='cuda:0')
c= tensor(8.5053e+08, device='cuda:0')
c= tensor(8.5054e+08, device='cuda:0')
c= tensor(8.5054e+08, device='cuda:0')
c= tensor(8.5055e+08, device='cuda:0')
c= tensor(8.5055e+08, device='cuda:0')
c= tensor(8.5055e+08, device='cuda:0')
c= tensor(8.5056e+08, device='cuda:0')
c= tensor(8.5062e+08, device='cuda:0')
c= tensor(8.5063e+08, device='cuda:0')
c= tensor(8.5066e+08, device='cuda:0')
c= tensor(8.5066e+08, device='cuda:0')
c= tensor(8.5067e+08, device='cuda:0')
c= tensor(8.5067e+08, device='cuda:0')
c= tensor(8.5067e+08, device='cuda:0')
c= tensor(8.5068e+08, device='cuda:0')
c= tensor(8.5068e+08, device='cuda:0')
c= tensor(8.5068e+08, device='cuda:0')
c= tensor(8.5069e+08, device='cuda:0')
c= tensor(8.5069e+08, device='cuda:0')
c= tensor(8.5070e+08, device='cuda:0')
c= tensor(8.5070e+08, device='cuda:0')
c= tensor(8.5074e+08, device='cuda:0')
c= tensor(8.5078e+08, device='cuda:0')
c= tensor(8.5081e+08, device='cuda:0')
c= tensor(8.5081e+08, device='cuda:0')
c= tensor(8.5082e+08, device='cuda:0')
c= tensor(8.5082e+08, device='cuda:0')
c= tensor(8.5082e+08, device='cuda:0')
c= tensor(8.5083e+08, device='cuda:0')
c= tensor(8.5083e+08, device='cuda:0')
c= tensor(8.5084e+08, device='cuda:0')
c= tensor(8.5085e+08, device='cuda:0')
c= tensor(8.5090e+08, device='cuda:0')
c= tensor(8.5091e+08, device='cuda:0')
c= tensor(8.5102e+08, device='cuda:0')
c= tensor(8.5102e+08, device='cuda:0')
c= tensor(8.5103e+08, device='cuda:0')
c= tensor(8.5104e+08, device='cuda:0')
c= tensor(8.5104e+08, device='cuda:0')
c= tensor(8.5107e+08, device='cuda:0')
c= tensor(8.5107e+08, device='cuda:0')
c= tensor(8.5108e+08, device='cuda:0')
c= tensor(8.5109e+08, device='cuda:0')
c= tensor(8.5109e+08, device='cuda:0')
c= tensor(8.5109e+08, device='cuda:0')
c= tensor(8.5110e+08, device='cuda:0')
c= tensor(8.5112e+08, device='cuda:0')
c= tensor(8.5113e+08, device='cuda:0')
c= tensor(8.5113e+08, device='cuda:0')
c= tensor(8.5113e+08, device='cuda:0')
c= tensor(8.5114e+08, device='cuda:0')
c= tensor(8.5115e+08, device='cuda:0')
c= tensor(8.5116e+08, device='cuda:0')
c= tensor(8.5117e+08, device='cuda:0')
c= tensor(8.5118e+08, device='cuda:0')
c= tensor(8.5120e+08, device='cuda:0')
c= tensor(8.5120e+08, device='cuda:0')
c= tensor(8.5122e+08, device='cuda:0')
c= tensor(8.5123e+08, device='cuda:0')
c= tensor(8.5124e+08, device='cuda:0')
c= tensor(8.5124e+08, device='cuda:0')
c= tensor(8.5126e+08, device='cuda:0')
c= tensor(8.5126e+08, device='cuda:0')
c= tensor(8.5127e+08, device='cuda:0')
c= tensor(8.5129e+08, device='cuda:0')
c= tensor(8.5129e+08, device='cuda:0')
c= tensor(8.5131e+08, device='cuda:0')
c= tensor(8.5132e+08, device='cuda:0')
c= tensor(8.5139e+08, device='cuda:0')
c= tensor(8.5139e+08, device='cuda:0')
c= tensor(8.5139e+08, device='cuda:0')
c= tensor(8.5140e+08, device='cuda:0')
c= tensor(8.5140e+08, device='cuda:0')
c= tensor(8.5141e+08, device='cuda:0')
c= tensor(8.5142e+08, device='cuda:0')
c= tensor(8.5142e+08, device='cuda:0')
c= tensor(8.5142e+08, device='cuda:0')
c= tensor(8.5143e+08, device='cuda:0')
c= tensor(8.5143e+08, device='cuda:0')
c= tensor(8.5144e+08, device='cuda:0')
c= tensor(8.5144e+08, device='cuda:0')
c= tensor(8.5147e+08, device='cuda:0')
c= tensor(8.5148e+08, device='cuda:0')
c= tensor(8.5149e+08, device='cuda:0')
c= tensor(8.5149e+08, device='cuda:0')
c= tensor(8.5150e+08, device='cuda:0')
c= tensor(8.5151e+08, device='cuda:0')
c= tensor(8.5152e+08, device='cuda:0')
c= tensor(8.5154e+08, device='cuda:0')
c= tensor(8.5155e+08, device='cuda:0')
c= tensor(8.5157e+08, device='cuda:0')
c= tensor(8.5157e+08, device='cuda:0')
c= tensor(8.5158e+08, device='cuda:0')
c= tensor(8.5158e+08, device='cuda:0')
c= tensor(8.5159e+08, device='cuda:0')
c= tensor(8.5159e+08, device='cuda:0')
c= tensor(8.5159e+08, device='cuda:0')
c= tensor(8.5160e+08, device='cuda:0')
c= tensor(8.5161e+08, device='cuda:0')
c= tensor(8.5161e+08, device='cuda:0')
c= tensor(8.5161e+08, device='cuda:0')
c= tensor(8.5162e+08, device='cuda:0')
c= tensor(8.5163e+08, device='cuda:0')
c= tensor(8.5164e+08, device='cuda:0')
c= tensor(8.5164e+08, device='cuda:0')
c= tensor(8.5165e+08, device='cuda:0')
c= tensor(8.5165e+08, device='cuda:0')
c= tensor(8.5167e+08, device='cuda:0')
c= tensor(8.5167e+08, device='cuda:0')
c= tensor(8.5167e+08, device='cuda:0')
c= tensor(8.5168e+08, device='cuda:0')
c= tensor(8.5168e+08, device='cuda:0')
c= tensor(8.5169e+08, device='cuda:0')
c= tensor(8.5169e+08, device='cuda:0')
c= tensor(8.5170e+08, device='cuda:0')
c= tensor(8.5171e+08, device='cuda:0')
c= tensor(8.5174e+08, device='cuda:0')
c= tensor(8.5175e+08, device='cuda:0')
c= tensor(8.5186e+08, device='cuda:0')
c= tensor(8.5484e+08, device='cuda:0')
c= tensor(8.5516e+08, device='cuda:0')
c= tensor(8.5518e+08, device='cuda:0')
c= tensor(8.5518e+08, device='cuda:0')
c= tensor(8.5519e+08, device='cuda:0')
c= tensor(8.5595e+08, device='cuda:0')
c= tensor(8.7478e+08, device='cuda:0')
c= tensor(8.7479e+08, device='cuda:0')
c= tensor(8.8148e+08, device='cuda:0')
c= tensor(8.8196e+08, device='cuda:0')
c= tensor(8.8227e+08, device='cuda:0')
c= tensor(8.9493e+08, device='cuda:0')
c= tensor(8.9493e+08, device='cuda:0')
c= tensor(8.9496e+08, device='cuda:0')
c= tensor(9.0976e+08, device='cuda:0')
c= tensor(9.5865e+08, device='cuda:0')
c= tensor(9.5867e+08, device='cuda:0')
c= tensor(9.5897e+08, device='cuda:0')
c= tensor(9.5946e+08, device='cuda:0')
c= tensor(9.6015e+08, device='cuda:0')
c= tensor(9.6564e+08, device='cuda:0')
c= tensor(9.6730e+08, device='cuda:0')
c= tensor(9.6817e+08, device='cuda:0')
c= tensor(9.6839e+08, device='cuda:0')
c= tensor(9.6840e+08, device='cuda:0')
c= tensor(1.0746e+09, device='cuda:0')
c= tensor(1.0746e+09, device='cuda:0')
c= tensor(1.0746e+09, device='cuda:0')
c= tensor(1.0764e+09, device='cuda:0')
c= tensor(1.0780e+09, device='cuda:0')
c= tensor(1.1101e+09, device='cuda:0')
c= tensor(1.1139e+09, device='cuda:0')
c= tensor(1.1139e+09, device='cuda:0')
c= tensor(1.1141e+09, device='cuda:0')
c= tensor(1.1141e+09, device='cuda:0')
c= tensor(1.1153e+09, device='cuda:0')
c= tensor(1.1175e+09, device='cuda:0')
c= tensor(1.1176e+09, device='cuda:0')
c= tensor(1.1188e+09, device='cuda:0')
c= tensor(1.1188e+09, device='cuda:0')
c= tensor(1.1188e+09, device='cuda:0')
c= tensor(1.1213e+09, device='cuda:0')
c= tensor(1.1246e+09, device='cuda:0')
c= tensor(1.1263e+09, device='cuda:0')
c= tensor(1.1265e+09, device='cuda:0')
c= tensor(1.1580e+09, device='cuda:0')
c= tensor(1.1580e+09, device='cuda:0')
c= tensor(1.1583e+09, device='cuda:0')
c= tensor(1.1597e+09, device='cuda:0')
c= tensor(1.1598e+09, device='cuda:0')
c= tensor(1.1648e+09, device='cuda:0')
c= tensor(1.1839e+09, device='cuda:0')
c= tensor(1.2191e+09, device='cuda:0')
c= tensor(1.2193e+09, device='cuda:0')
c= tensor(1.2197e+09, device='cuda:0')
c= tensor(1.2199e+09, device='cuda:0')
c= tensor(1.2199e+09, device='cuda:0')
c= tensor(1.2221e+09, device='cuda:0')
c= tensor(1.2221e+09, device='cuda:0')
c= tensor(1.2227e+09, device='cuda:0')
c= tensor(1.2710e+09, device='cuda:0')
c= tensor(1.2725e+09, device='cuda:0')
c= tensor(1.2728e+09, device='cuda:0')
c= tensor(1.2728e+09, device='cuda:0')
c= tensor(1.2819e+09, device='cuda:0')
c= tensor(1.2822e+09, device='cuda:0')
c= tensor(1.2840e+09, device='cuda:0')
c= tensor(1.2840e+09, device='cuda:0')
c= tensor(1.3149e+09, device='cuda:0')
c= tensor(1.3150e+09, device='cuda:0')
c= tensor(1.3208e+09, device='cuda:0')
c= tensor(1.3209e+09, device='cuda:0')
c= tensor(1.3233e+09, device='cuda:0')
c= tensor(1.3243e+09, device='cuda:0')
c= tensor(1.3450e+09, device='cuda:0')
c= tensor(1.3468e+09, device='cuda:0')
c= tensor(1.3468e+09, device='cuda:0')
c= tensor(1.3513e+09, device='cuda:0')
c= tensor(1.3550e+09, device='cuda:0')
c= tensor(1.3551e+09, device='cuda:0')
c= tensor(1.3659e+09, device='cuda:0')
c= tensor(1.3739e+09, device='cuda:0')
c= tensor(1.4048e+09, device='cuda:0')
c= tensor(1.4219e+09, device='cuda:0')
c= tensor(1.4219e+09, device='cuda:0')
c= tensor(1.4220e+09, device='cuda:0')
c= tensor(1.4253e+09, device='cuda:0')
c= tensor(1.4255e+09, device='cuda:0')
c= tensor(1.4262e+09, device='cuda:0')
c= tensor(1.4262e+09, device='cuda:0')
c= tensor(1.4274e+09, device='cuda:0')
c= tensor(1.4349e+09, device='cuda:0')
c= tensor(1.4370e+09, device='cuda:0')
c= tensor(1.4371e+09, device='cuda:0')
c= tensor(1.4374e+09, device='cuda:0')
c= tensor(1.4375e+09, device='cuda:0')
c= tensor(1.4375e+09, device='cuda:0')
c= tensor(1.4376e+09, device='cuda:0')
c= tensor(1.4376e+09, device='cuda:0')
c= tensor(1.4383e+09, device='cuda:0')
c= tensor(1.4390e+09, device='cuda:0')
c= tensor(1.4392e+09, device='cuda:0')
c= tensor(1.4405e+09, device='cuda:0')
c= tensor(1.4405e+09, device='cuda:0')
c= tensor(1.5009e+09, device='cuda:0')
c= tensor(1.5009e+09, device='cuda:0')
c= tensor(1.5058e+09, device='cuda:0')
c= tensor(1.5058e+09, device='cuda:0')
c= tensor(1.5058e+09, device='cuda:0')
c= tensor(1.5058e+09, device='cuda:0')
c= tensor(1.5068e+09, device='cuda:0')
c= tensor(1.5068e+09, device='cuda:0')
c= tensor(1.5242e+09, device='cuda:0')
c= tensor(1.5242e+09, device='cuda:0')
c= tensor(1.5242e+09, device='cuda:0')
c= tensor(1.5408e+09, device='cuda:0')
c= tensor(1.5424e+09, device='cuda:0')
c= tensor(1.5443e+09, device='cuda:0')
c= tensor(1.5576e+09, device='cuda:0')
c= tensor(1.5784e+09, device='cuda:0')
c= tensor(1.5784e+09, device='cuda:0')
c= tensor(1.5784e+09, device='cuda:0')
c= tensor(1.5785e+09, device='cuda:0')
c= tensor(1.5785e+09, device='cuda:0')
c= tensor(1.5785e+09, device='cuda:0')
c= tensor(1.5787e+09, device='cuda:0')
c= tensor(1.5787e+09, device='cuda:0')
c= tensor(1.5787e+09, device='cuda:0')
c= tensor(1.5787e+09, device='cuda:0')
c= tensor(1.5787e+09, device='cuda:0')
c= tensor(1.6194e+09, device='cuda:0')
c= tensor(1.6194e+09, device='cuda:0')
c= tensor(1.6227e+09, device='cuda:0')
c= tensor(1.6228e+09, device='cuda:0')
c= tensor(1.6233e+09, device='cuda:0')
c= tensor(1.6240e+09, device='cuda:0')
c= tensor(1.9551e+09, device='cuda:0')
c= tensor(2.0435e+09, device='cuda:0')
c= tensor(2.0439e+09, device='cuda:0')
c= tensor(2.0447e+09, device='cuda:0')
c= tensor(2.0448e+09, device='cuda:0')
c= tensor(2.0453e+09, device='cuda:0')
c= tensor(2.0489e+09, device='cuda:0')
c= tensor(2.0495e+09, device='cuda:0')
c= tensor(2.0495e+09, device='cuda:0')
c= tensor(2.0522e+09, device='cuda:0')
c= tensor(2.1285e+09, device='cuda:0')
c= tensor(2.1295e+09, device='cuda:0')
c= tensor(2.1296e+09, device='cuda:0')
c= tensor(2.1306e+09, device='cuda:0')
c= tensor(2.1307e+09, device='cuda:0')
c= tensor(2.1307e+09, device='cuda:0')
c= tensor(2.1382e+09, device='cuda:0')
c= tensor(2.1384e+09, device='cuda:0')
c= tensor(2.1385e+09, device='cuda:0')
c= tensor(2.1406e+09, device='cuda:0')
c= tensor(2.1407e+09, device='cuda:0')
c= tensor(2.1407e+09, device='cuda:0')
c= tensor(2.1441e+09, device='cuda:0')
c= tensor(2.1473e+09, device='cuda:0')
c= tensor(2.1568e+09, device='cuda:0')
c= tensor(2.1771e+09, device='cuda:0')
c= tensor(2.1963e+09, device='cuda:0')
c= tensor(2.1965e+09, device='cuda:0')
c= tensor(2.1981e+09, device='cuda:0')
c= tensor(2.2037e+09, device='cuda:0')
c= tensor(2.2107e+09, device='cuda:0')
c= tensor(2.2107e+09, device='cuda:0')
c= tensor(2.3004e+09, device='cuda:0')
c= tensor(2.4109e+09, device='cuda:0')
c= tensor(2.4191e+09, device='cuda:0')
c= tensor(2.4230e+09, device='cuda:0')
c= tensor(2.4259e+09, device='cuda:0')
c= tensor(2.4260e+09, device='cuda:0')
c= tensor(2.4260e+09, device='cuda:0')
c= tensor(2.4260e+09, device='cuda:0')
c= tensor(2.4299e+09, device='cuda:0')
c= tensor(2.4351e+09, device='cuda:0')
c= tensor(2.4781e+09, device='cuda:0')
c= tensor(2.4849e+09, device='cuda:0')
c= tensor(2.4875e+09, device='cuda:0')
c= tensor(2.4877e+09, device='cuda:0')
c= tensor(2.4932e+09, device='cuda:0')
c= tensor(2.4932e+09, device='cuda:0')
c= tensor(2.4933e+09, device='cuda:0')
c= tensor(2.5026e+09, device='cuda:0')
c= tensor(2.5028e+09, device='cuda:0')
c= tensor(2.5028e+09, device='cuda:0')
c= tensor(2.5029e+09, device='cuda:0')
c= tensor(2.5408e+09, device='cuda:0')
c= tensor(2.5410e+09, device='cuda:0')
c= tensor(2.5526e+09, device='cuda:0')
c= tensor(2.5527e+09, device='cuda:0')
c= tensor(2.5529e+09, device='cuda:0')
c= tensor(2.5529e+09, device='cuda:0')
c= tensor(2.5529e+09, device='cuda:0')
c= tensor(2.5537e+09, device='cuda:0')
c= tensor(2.5550e+09, device='cuda:0')
c= tensor(2.5550e+09, device='cuda:0')
c= tensor(2.5600e+09, device='cuda:0')
c= tensor(2.5600e+09, device='cuda:0')
c= tensor(2.5607e+09, device='cuda:0')
c= tensor(2.5608e+09, device='cuda:0')
c= tensor(2.5647e+09, device='cuda:0')
c= tensor(2.5649e+09, device='cuda:0')
c= tensor(2.5660e+09, device='cuda:0')
c= tensor(2.5662e+09, device='cuda:0')
c= tensor(2.5667e+09, device='cuda:0')
c= tensor(2.5687e+09, device='cuda:0')
c= tensor(2.6354e+09, device='cuda:0')
c= tensor(2.6355e+09, device='cuda:0')
c= tensor(2.6356e+09, device='cuda:0')
c= tensor(2.6382e+09, device='cuda:0')
c= tensor(2.6383e+09, device='cuda:0')
c= tensor(2.7086e+09, device='cuda:0')
c= tensor(2.7086e+09, device='cuda:0')
c= tensor(2.7138e+09, device='cuda:0')
c= tensor(2.7279e+09, device='cuda:0')
c= tensor(2.7279e+09, device='cuda:0')
c= tensor(2.7433e+09, device='cuda:0')
c= tensor(2.7439e+09, device='cuda:0')
c= tensor(2.8888e+09, device='cuda:0')
c= tensor(2.8888e+09, device='cuda:0')
c= tensor(2.8892e+09, device='cuda:0')
c= tensor(2.8893e+09, device='cuda:0')
c= tensor(2.8893e+09, device='cuda:0')
c= tensor(2.8893e+09, device='cuda:0')
c= tensor(2.8906e+09, device='cuda:0')
c= tensor(2.8912e+09, device='cuda:0')
c= tensor(2.8969e+09, device='cuda:0')
c= tensor(2.8970e+09, device='cuda:0')
c= tensor(2.8970e+09, device='cuda:0')
c= tensor(2.8971e+09, device='cuda:0')
c= tensor(2.9117e+09, device='cuda:0')
c= tensor(2.9155e+09, device='cuda:0')
c= tensor(2.9320e+09, device='cuda:0')
c= tensor(2.9327e+09, device='cuda:0')
c= tensor(2.9327e+09, device='cuda:0')
c= tensor(2.9327e+09, device='cuda:0')
c= tensor(2.9329e+09, device='cuda:0')
c= tensor(3.0361e+09, device='cuda:0')
c= tensor(3.0361e+09, device='cuda:0')
c= tensor(3.0364e+09, device='cuda:0')
c= tensor(3.0399e+09, device='cuda:0')
c= tensor(3.0418e+09, device='cuda:0')
c= tensor(3.0419e+09, device='cuda:0')
c= tensor(3.0419e+09, device='cuda:0')
c= tensor(3.0674e+09, device='cuda:0')
c= tensor(3.0687e+09, device='cuda:0')
c= tensor(3.0689e+09, device='cuda:0')
c= tensor(3.0692e+09, device='cuda:0')
c= tensor(3.0882e+09, device='cuda:0')
c= tensor(3.0944e+09, device='cuda:0')
c= tensor(3.1264e+09, device='cuda:0')
c= tensor(3.1318e+09, device='cuda:0')
c= tensor(3.1318e+09, device='cuda:0')
c= tensor(3.1325e+09, device='cuda:0')
c= tensor(3.1346e+09, device='cuda:0')
c= tensor(3.1348e+09, device='cuda:0')
c= tensor(3.1349e+09, device='cuda:0')
c= tensor(3.1349e+09, device='cuda:0')
c= tensor(3.1375e+09, device='cuda:0')
c= tensor(3.1377e+09, device='cuda:0')
c= tensor(3.1377e+09, device='cuda:0')
c= tensor(3.1379e+09, device='cuda:0')
c= tensor(3.1384e+09, device='cuda:0')
c= tensor(3.1420e+09, device='cuda:0')
c= tensor(3.1421e+09, device='cuda:0')
c= tensor(3.1423e+09, device='cuda:0')
c= tensor(3.1430e+09, device='cuda:0')
c= tensor(3.1432e+09, device='cuda:0')
c= tensor(3.1432e+09, device='cuda:0')
c= tensor(3.1433e+09, device='cuda:0')
c= tensor(3.1434e+09, device='cuda:0')
c= tensor(3.1620e+09, device='cuda:0')
c= tensor(3.1620e+09, device='cuda:0')
c= tensor(3.1620e+09, device='cuda:0')
c= tensor(3.1621e+09, device='cuda:0')
c= tensor(3.1731e+09, device='cuda:0')
c= tensor(3.1920e+09, device='cuda:0')
c= tensor(3.1939e+09, device='cuda:0')
c= tensor(3.1940e+09, device='cuda:0')
c= tensor(3.2015e+09, device='cuda:0')
c= tensor(3.2069e+09, device='cuda:0')
c= tensor(3.2069e+09, device='cuda:0')
c= tensor(3.2071e+09, device='cuda:0')
c= tensor(3.2075e+09, device='cuda:0')
c= tensor(3.2104e+09, device='cuda:0')
c= tensor(3.2111e+09, device='cuda:0')
c= tensor(3.2183e+09, device='cuda:0')
c= tensor(3.2184e+09, device='cuda:0')
c= tensor(3.2189e+09, device='cuda:0')
c= tensor(3.2189e+09, device='cuda:0')
c= tensor(3.2193e+09, device='cuda:0')
c= tensor(3.2195e+09, device='cuda:0')
c= tensor(3.2203e+09, device='cuda:0')
c= tensor(3.2344e+09, device='cuda:0')
c= tensor(3.2825e+09, device='cuda:0')
c= tensor(3.2825e+09, device='cuda:0')
c= tensor(3.2825e+09, device='cuda:0')
c= tensor(3.2826e+09, device='cuda:0')
c= tensor(3.2830e+09, device='cuda:0')
c= tensor(3.2834e+09, device='cuda:0')
c= tensor(3.2834e+09, device='cuda:0')
c= tensor(3.2834e+09, device='cuda:0')
c= tensor(3.2836e+09, device='cuda:0')
c= tensor(3.2836e+09, device='cuda:0')
c= tensor(3.2862e+09, device='cuda:0')
c= tensor(3.2862e+09, device='cuda:0')
c= tensor(3.2863e+09, device='cuda:0')
c= tensor(3.2864e+09, device='cuda:0')
c= tensor(3.2864e+09, device='cuda:0')
c= tensor(3.2864e+09, device='cuda:0')
c= tensor(3.2922e+09, device='cuda:0')
c= tensor(3.3310e+09, device='cuda:0')
c= tensor(3.3403e+09, device='cuda:0')
c= tensor(3.3432e+09, device='cuda:0')
c= tensor(3.3434e+09, device='cuda:0')
c= tensor(3.3434e+09, device='cuda:0')
c= tensor(3.3435e+09, device='cuda:0')
c= tensor(3.3480e+09, device='cuda:0')
c= tensor(3.3483e+09, device='cuda:0')
c= tensor(3.3491e+09, device='cuda:0')
c= tensor(3.3492e+09, device='cuda:0')
c= tensor(3.5626e+09, device='cuda:0')
c= tensor(3.5630e+09, device='cuda:0')
c= tensor(3.5640e+09, device='cuda:0')
c= tensor(3.5845e+09, device='cuda:0')
c= tensor(3.5850e+09, device='cuda:0')
c= tensor(3.5854e+09, device='cuda:0')
c= tensor(3.6247e+09, device='cuda:0')
c= tensor(3.6285e+09, device='cuda:0')
c= tensor(3.6297e+09, device='cuda:0')
c= tensor(3.6299e+09, device='cuda:0')
c= tensor(3.6306e+09, device='cuda:0')
c= tensor(3.6306e+09, device='cuda:0')
c= tensor(3.6392e+09, device='cuda:0')
c= tensor(3.8239e+09, device='cuda:0')
c= tensor(3.8414e+09, device='cuda:0')
c= tensor(3.8507e+09, device='cuda:0')
c= tensor(3.8520e+09, device='cuda:0')
c= tensor(3.8544e+09, device='cuda:0')
c= tensor(3.8570e+09, device='cuda:0')
c= tensor(3.8571e+09, device='cuda:0')
c= tensor(3.8573e+09, device='cuda:0')
c= tensor(3.8865e+09, device='cuda:0')
c= tensor(3.8882e+09, device='cuda:0')
c= tensor(3.9128e+09, device='cuda:0')
c= tensor(3.9219e+09, device='cuda:0')
c= tensor(3.9227e+09, device='cuda:0')
c= tensor(3.9227e+09, device='cuda:0')
c= tensor(3.9237e+09, device='cuda:0')
c= tensor(3.9278e+09, device='cuda:0')
c= tensor(3.9278e+09, device='cuda:0')
c= tensor(4.0083e+09, device='cuda:0')
c= tensor(4.0102e+09, device='cuda:0')
c= tensor(4.0110e+09, device='cuda:0')
c= tensor(4.0111e+09, device='cuda:0')
c= tensor(4.0112e+09, device='cuda:0')
c= tensor(4.0112e+09, device='cuda:0')
c= tensor(4.0113e+09, device='cuda:0')
c= tensor(4.0118e+09, device='cuda:0')
c= tensor(4.0188e+09, device='cuda:0')
c= tensor(6.1885e+09, device='cuda:0')
c= tensor(6.1887e+09, device='cuda:0')
c= tensor(6.1917e+09, device='cuda:0')
c= tensor(6.1918e+09, device='cuda:0')
c= tensor(6.1919e+09, device='cuda:0')
c= tensor(6.1920e+09, device='cuda:0')
c= tensor(6.2129e+09, device='cuda:0')
c= tensor(6.2130e+09, device='cuda:0')
c= tensor(6.4806e+09, device='cuda:0')
c= tensor(6.4806e+09, device='cuda:0')
c= tensor(6.4925e+09, device='cuda:0')
c= tensor(6.4940e+09, device='cuda:0')
c= tensor(6.4992e+09, device='cuda:0')
c= tensor(6.5147e+09, device='cuda:0')
c= tensor(6.5148e+09, device='cuda:0')
c= tensor(6.5149e+09, device='cuda:0')
c= tensor(6.5167e+09, device='cuda:0')
c= tensor(6.5167e+09, device='cuda:0')
c= tensor(6.5169e+09, device='cuda:0')
c= tensor(6.5198e+09, device='cuda:0')
c= tensor(6.5216e+09, device='cuda:0')
c= tensor(6.5233e+09, device='cuda:0')
c= tensor(6.5236e+09, device='cuda:0')
c= tensor(6.5374e+09, device='cuda:0')
c= tensor(6.5687e+09, device='cuda:0')
c= tensor(6.5693e+09, device='cuda:0')
c= tensor(6.5694e+09, device='cuda:0')
c= tensor(6.5761e+09, device='cuda:0')
c= tensor(6.5776e+09, device='cuda:0')
c= tensor(6.5783e+09, device='cuda:0')
c= tensor(6.5793e+09, device='cuda:0')
c= tensor(6.5805e+09, device='cuda:0')
c= tensor(6.5821e+09, device='cuda:0')
c= tensor(6.6128e+09, device='cuda:0')
c= tensor(6.6152e+09, device='cuda:0')
c= tensor(6.6152e+09, device='cuda:0')
c= tensor(6.6153e+09, device='cuda:0')
c= tensor(6.6157e+09, device='cuda:0')
c= tensor(6.6186e+09, device='cuda:0')
c= tensor(6.6233e+09, device='cuda:0')
c= tensor(6.6264e+09, device='cuda:0')
c= tensor(6.6265e+09, device='cuda:0')
c= tensor(6.6267e+09, device='cuda:0')
c= tensor(6.6287e+09, device='cuda:0')
c= tensor(6.6399e+09, device='cuda:0')
c= tensor(6.6411e+09, device='cuda:0')
c= tensor(6.6429e+09, device='cuda:0')
c= tensor(6.6433e+09, device='cuda:0')
c= tensor(6.6439e+09, device='cuda:0')
c= tensor(6.6439e+09, device='cuda:0')
c= tensor(6.6439e+09, device='cuda:0')
c= tensor(6.6530e+09, device='cuda:0')
c= tensor(6.6541e+09, device='cuda:0')
c= tensor(6.6551e+09, device='cuda:0')
c= tensor(6.6551e+09, device='cuda:0')
c= tensor(6.6551e+09, device='cuda:0')
c= tensor(6.6580e+09, device='cuda:0')
c= tensor(6.6596e+09, device='cuda:0')
c= tensor(6.6602e+09, device='cuda:0')
c= tensor(6.6602e+09, device='cuda:0')
c= tensor(6.6602e+09, device='cuda:0')
c= tensor(6.6615e+09, device='cuda:0')
c= tensor(6.6618e+09, device='cuda:0')
c= tensor(6.6701e+09, device='cuda:0')
c= tensor(6.6702e+09, device='cuda:0')
c= tensor(6.6702e+09, device='cuda:0')
c= tensor(6.6705e+09, device='cuda:0')
c= tensor(6.6706e+09, device='cuda:0')
c= tensor(6.6992e+09, device='cuda:0')
c= tensor(6.6993e+09, device='cuda:0')
c= tensor(6.7006e+09, device='cuda:0')
c= tensor(6.7036e+09, device='cuda:0')
c= tensor(6.7108e+09, device='cuda:0')
c= tensor(6.7222e+09, device='cuda:0')
c= tensor(6.7281e+09, device='cuda:0')
c= tensor(6.7281e+09, device='cuda:0')
c= tensor(6.7288e+09, device='cuda:0')
c= tensor(6.7289e+09, device='cuda:0')
c= tensor(6.7373e+09, device='cuda:0')
c= tensor(6.7411e+09, device='cuda:0')
c= tensor(6.7418e+09, device='cuda:0')
c= tensor(6.7435e+09, device='cuda:0')
c= tensor(6.7436e+09, device='cuda:0')
c= tensor(6.7466e+09, device='cuda:0')
c= tensor(6.7466e+09, device='cuda:0')
c= tensor(6.7469e+09, device='cuda:0')
c= tensor(6.7684e+09, device='cuda:0')
c= tensor(6.7918e+09, device='cuda:0')
c= tensor(6.7919e+09, device='cuda:0')
c= tensor(6.7922e+09, device='cuda:0')
c= tensor(6.7923e+09, device='cuda:0')
c= tensor(6.8076e+09, device='cuda:0')
c= tensor(6.8078e+09, device='cuda:0')
c= tensor(6.8079e+09, device='cuda:0')
c= tensor(6.8098e+09, device='cuda:0')
c= tensor(6.8148e+09, device='cuda:0')
c= tensor(6.8148e+09, device='cuda:0')
c= tensor(6.8165e+09, device='cuda:0')
c= tensor(6.8165e+09, device='cuda:0')
memory (bytes)
5029249024
time for making loss 2 is 14.526541948318481
p0 True
it  0 : 2347170304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 60% |
shape of L is 
torch.Size([])
memory (bytes)
5029449728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 10% |
memory (bytes)
5030174720
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  88619010000.0
relative error loss 13.000667
shape of L is 
torch.Size([])
memory (bytes)
5188632576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  5% | 12% |
memory (bytes)
5188902912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  88618640000.0
relative error loss 13.000612
shape of L is 
torch.Size([])
memory (bytes)
5190701056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 12% |
memory (bytes)
5190926336
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  88616980000.0
relative error loss 13.000369
shape of L is 
torch.Size([])
memory (bytes)
5192986624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 12% |
memory (bytes)
5193023488
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  88607710000.0
relative error loss 12.999009
shape of L is 
torch.Size([])
memory (bytes)
5195112448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 12% |
memory (bytes)
5195149312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  88554660000.0
relative error loss 12.991227
shape of L is 
torch.Size([])
memory (bytes)
5197135872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 12% |
memory (bytes)
5197238272
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  88265410000.0
relative error loss 12.948792
shape of L is 
torch.Size([])
memory (bytes)
5199323136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5199360000
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  85136105000.0
relative error loss 12.489715
shape of L is 
torch.Size([])
memory (bytes)
5201469440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 12% |
memory (bytes)
5201506304
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  70841520000.0
relative error loss 10.392657
shape of L is 
torch.Size([])
memory (bytes)
5203546112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 12% |
memory (bytes)
5203644416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 12% |
error is  20713542000.0
relative error loss 3.0387368
shape of L is 
torch.Size([])
memory (bytes)
5205749760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 12% |
memory (bytes)
5205786624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% | 12% |
error is  11784145000.0
relative error loss 1.7287683
time to take a step is 227.6659014225006
it  1 : 2733140992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5207732224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 12% |
memory (bytes)
5207932928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  11784145000.0
relative error loss 1.7287683
shape of L is 
torch.Size([])
memory (bytes)
5210058752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 12% |
memory (bytes)
5210095616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  10105510000.0
relative error loss 1.4825077
shape of L is 
torch.Size([])
memory (bytes)
5212196864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 12% |
memory (bytes)
5212196864
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  8961066000.0
relative error loss 1.3146145
shape of L is 
torch.Size([])
memory (bytes)
5214167040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5214355456
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  7245614000.0
relative error loss 1.0629528
shape of L is 
torch.Size([])
memory (bytes)
5216464896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 12% |
memory (bytes)
5216526336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  6879720000.0
relative error loss 1.009275
shape of L is 
torch.Size([])
memory (bytes)
5218615296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 12% |
memory (bytes)
5218615296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  10987352000.0
relative error loss 1.6118766
shape of L is 
torch.Size([])
memory (bytes)
5220585472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 12% |
memory (bytes)
5220585472
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  6651811000.0
relative error loss 0.97584003
shape of L is 
torch.Size([])
memory (bytes)
5222813696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 12% |
memory (bytes)
5222875136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  6405245000.0
relative error loss 0.9396681
shape of L is 
torch.Size([])
memory (bytes)
5224919040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 12% |
memory (bytes)
5224919040
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  6779585500.0
relative error loss 0.994585
shape of L is 
torch.Size([])
memory (bytes)
5226975232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 12% |
memory (bytes)
5226975232
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  6171387400.0
relative error loss 0.9053605
time to take a step is 216.96035528182983
it  2 : 2843419136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5229187072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 12% |
memory (bytes)
5229223936
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  6171387400.0
relative error loss 0.9053605
shape of L is 
torch.Size([])
memory (bytes)
5231185920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 12% |
memory (bytes)
5231337472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 12% |
error is  5781624000.0
relative error loss 0.84818107
shape of L is 
torch.Size([])
memory (bytes)
5233463296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 12% |
memory (bytes)
5233463296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  5318813700.0
relative error loss 0.7802855
shape of L is 
torch.Size([])
memory (bytes)
5235519488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 12% |
memory (bytes)
5235519488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  5031120000.0
relative error loss 0.73807997
shape of L is 
torch.Size([])
memory (bytes)
5237673984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 12% |
memory (bytes)
5237673984
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  5492652500.0
relative error loss 0.80578816
shape of L is 
torch.Size([])
memory (bytes)
5239783424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 12% |
memory (bytes)
5239791616
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  4845357600.0
relative error loss 0.71082807
shape of L is 
torch.Size([])
memory (bytes)
5241905152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5241905152
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  4476312600.0
relative error loss 0.6566881
shape of L is 
torch.Size([])
memory (bytes)
5243899904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 12% |
memory (bytes)
5244030976
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  4115488000.0
relative error loss 0.6037541
shape of L is 
torch.Size([])
memory (bytes)
5246177280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 12% |
memory (bytes)
5246214144
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  3800855600.0
relative error loss 0.55759656
shape of L is 
torch.Size([])
memory (bytes)
5248278528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 12% |
memory (bytes)
5248278528
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  3482646500.0
relative error loss 0.5109144
time to take a step is 217.4115903377533
it  3 : 2843418624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5250277376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 12% |
memory (bytes)
5250498560
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  3482646500.0
relative error loss 0.5109144
shape of L is 
torch.Size([])
memory (bytes)
5252599808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 12% |
memory (bytes)
5252599808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% | 12% |
error is  3219782100.0
relative error loss 0.47235143
shape of L is 
torch.Size([])
memory (bytes)
5254615040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 12% |
memory (bytes)
5254774784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% | 12% |
error is  2982380800.0
relative error loss 0.43752396
shape of L is 
torch.Size([])
memory (bytes)
5256798208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 12% |
memory (bytes)
5256798208
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  2716842000.0
relative error loss 0.39856863
shape of L is 
torch.Size([])
memory (bytes)
5259014144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 12% |
memory (bytes)
5259014144
| ID | GPU | MEM |
------------------
|  0 |  4% |  0% |
|  1 | 99% | 12% |
error is  2778866700.0
relative error loss 0.40766785
shape of L is 
torch.Size([])
memory (bytes)
5261148160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 12% |
memory (bytes)
5261185024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  2540151800.0
relative error loss 0.37264767
shape of L is 
torch.Size([])
memory (bytes)
5263306752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 12% |
memory (bytes)
5263343616
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  2388727800.0
relative error loss 0.35043332
shape of L is 
torch.Size([])
memory (bytes)
5265440768
| ID | GPU | MEM |
------------------
|  0 |  5% |  0% |
|  1 |  7% | 12% |
memory (bytes)
5265477632
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  2258414600.0
relative error loss 0.331316
shape of L is 
torch.Size([])
memory (bytes)
5267603456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 12% |
memory (bytes)
5267611648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  2136971300.0
relative error loss 0.31349993
shape of L is 
torch.Size([])
memory (bytes)
5269757952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 12% |
memory (bytes)
5269757952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1915229200.0
relative error loss 0.2809697
time to take a step is 221.00931119918823
c= tensor(2457.6555, device='cuda:0')
c= tensor(226527.1094, device='cuda:0')
c= tensor(246877.6719, device='cuda:0')
c= tensor(365074.0938, device='cuda:0')
c= tensor(687156.1250, device='cuda:0')
c= tensor(1254737.7500, device='cuda:0')
c= tensor(2158288., device='cuda:0')
c= tensor(2777790., device='cuda:0')
c= tensor(2873341.7500, device='cuda:0')
c= tensor(26352480., device='cuda:0')
c= tensor(26391016., device='cuda:0')
c= tensor(31053804., device='cuda:0')
c= tensor(31076486., device='cuda:0')
c= tensor(60979560., device='cuda:0')
c= tensor(61193204., device='cuda:0')
c= tensor(62226312., device='cuda:0')
c= tensor(63300204., device='cuda:0')
c= tensor(64154752., device='cuda:0')
c= tensor(83172976., device='cuda:0')
c= tensor(87441912., device='cuda:0')
c= tensor(87903800., device='cuda:0')
c= tensor(1.2510e+08, device='cuda:0')
c= tensor(1.2516e+08, device='cuda:0')
c= tensor(1.2641e+08, device='cuda:0')
c= tensor(1.2683e+08, device='cuda:0')
c= tensor(1.2825e+08, device='cuda:0')
c= tensor(1.2987e+08, device='cuda:0')
c= tensor(1.2990e+08, device='cuda:0')
c= tensor(1.3484e+08, device='cuda:0')
c= tensor(7.0033e+08, device='cuda:0')
c= tensor(7.0045e+08, device='cuda:0')
c= tensor(8.3512e+08, device='cuda:0')
c= tensor(8.3527e+08, device='cuda:0')
c= tensor(8.3533e+08, device='cuda:0')
c= tensor(8.3543e+08, device='cuda:0')
c= tensor(8.4425e+08, device='cuda:0')
c= tensor(8.4957e+08, device='cuda:0')
c= tensor(8.4957e+08, device='cuda:0')
c= tensor(8.4959e+08, device='cuda:0')
c= tensor(8.4959e+08, device='cuda:0')
c= tensor(8.4960e+08, device='cuda:0')
c= tensor(8.4961e+08, device='cuda:0')
c= tensor(8.4961e+08, device='cuda:0')
c= tensor(8.4962e+08, device='cuda:0')
c= tensor(8.4962e+08, device='cuda:0')
c= tensor(8.4963e+08, device='cuda:0')
c= tensor(8.4965e+08, device='cuda:0')
c= tensor(8.4966e+08, device='cuda:0')
c= tensor(8.4967e+08, device='cuda:0')
c= tensor(8.4973e+08, device='cuda:0')
c= tensor(8.4980e+08, device='cuda:0')
c= tensor(8.4980e+08, device='cuda:0')
c= tensor(8.4982e+08, device='cuda:0')
c= tensor(8.4983e+08, device='cuda:0')
c= tensor(8.4984e+08, device='cuda:0')
c= tensor(8.4986e+08, device='cuda:0')
c= tensor(8.4986e+08, device='cuda:0')
c= tensor(8.4987e+08, device='cuda:0')
c= tensor(8.4988e+08, device='cuda:0')
c= tensor(8.4989e+08, device='cuda:0')
c= tensor(8.4990e+08, device='cuda:0')
c= tensor(8.4990e+08, device='cuda:0')
c= tensor(8.4993e+08, device='cuda:0')
c= tensor(8.4995e+08, device='cuda:0')
c= tensor(8.4996e+08, device='cuda:0')
c= tensor(8.4996e+08, device='cuda:0')
c= tensor(8.4997e+08, device='cuda:0')
c= tensor(8.4998e+08, device='cuda:0')
c= tensor(8.5000e+08, device='cuda:0')
c= tensor(8.5000e+08, device='cuda:0')
c= tensor(8.5001e+08, device='cuda:0')
c= tensor(8.5002e+08, device='cuda:0')
c= tensor(8.5003e+08, device='cuda:0')
c= tensor(8.5004e+08, device='cuda:0')
c= tensor(8.5005e+08, device='cuda:0')
c= tensor(8.5006e+08, device='cuda:0')
c= tensor(8.5006e+08, device='cuda:0')
c= tensor(8.5006e+08, device='cuda:0')
c= tensor(8.5007e+08, device='cuda:0')
c= tensor(8.5014e+08, device='cuda:0')
c= tensor(8.5015e+08, device='cuda:0')
c= tensor(8.5015e+08, device='cuda:0')
c= tensor(8.5017e+08, device='cuda:0')
c= tensor(8.5018e+08, device='cuda:0')
c= tensor(8.5018e+08, device='cuda:0')
c= tensor(8.5018e+08, device='cuda:0')
c= tensor(8.5020e+08, device='cuda:0')
c= tensor(8.5020e+08, device='cuda:0')
c= tensor(8.5021e+08, device='cuda:0')
c= tensor(8.5021e+08, device='cuda:0')
c= tensor(8.5022e+08, device='cuda:0')
c= tensor(8.5023e+08, device='cuda:0')
c= tensor(8.5023e+08, device='cuda:0')
c= tensor(8.5024e+08, device='cuda:0')
c= tensor(8.5025e+08, device='cuda:0')
c= tensor(8.5026e+08, device='cuda:0')
c= tensor(8.5026e+08, device='cuda:0')
c= tensor(8.5028e+08, device='cuda:0')
c= tensor(8.5029e+08, device='cuda:0')
c= tensor(8.5030e+08, device='cuda:0')
c= tensor(8.5035e+08, device='cuda:0')
c= tensor(8.5036e+08, device='cuda:0')
c= tensor(8.5039e+08, device='cuda:0')
c= tensor(8.5039e+08, device='cuda:0')
c= tensor(8.5040e+08, device='cuda:0')
c= tensor(8.5041e+08, device='cuda:0')
c= tensor(8.5042e+08, device='cuda:0')
c= tensor(8.5042e+08, device='cuda:0')
c= tensor(8.5043e+08, device='cuda:0')
c= tensor(8.5043e+08, device='cuda:0')
c= tensor(8.5043e+08, device='cuda:0')
c= tensor(8.5044e+08, device='cuda:0')
c= tensor(8.5044e+08, device='cuda:0')
c= tensor(8.5045e+08, device='cuda:0')
c= tensor(8.5046e+08, device='cuda:0')
c= tensor(8.5046e+08, device='cuda:0')
c= tensor(8.5046e+08, device='cuda:0')
c= tensor(8.5047e+08, device='cuda:0')
c= tensor(8.5049e+08, device='cuda:0')
c= tensor(8.5049e+08, device='cuda:0')
c= tensor(8.5053e+08, device='cuda:0')
c= tensor(8.5053e+08, device='cuda:0')
c= tensor(8.5054e+08, device='cuda:0')
c= tensor(8.5054e+08, device='cuda:0')
c= tensor(8.5055e+08, device='cuda:0')
c= tensor(8.5055e+08, device='cuda:0')
c= tensor(8.5055e+08, device='cuda:0')
c= tensor(8.5056e+08, device='cuda:0')
c= tensor(8.5062e+08, device='cuda:0')
c= tensor(8.5063e+08, device='cuda:0')
c= tensor(8.5066e+08, device='cuda:0')
c= tensor(8.5066e+08, device='cuda:0')
c= tensor(8.5067e+08, device='cuda:0')
c= tensor(8.5067e+08, device='cuda:0')
c= tensor(8.5067e+08, device='cuda:0')
c= tensor(8.5068e+08, device='cuda:0')
c= tensor(8.5068e+08, device='cuda:0')
c= tensor(8.5068e+08, device='cuda:0')
c= tensor(8.5069e+08, device='cuda:0')
c= tensor(8.5069e+08, device='cuda:0')
c= tensor(8.5070e+08, device='cuda:0')
c= tensor(8.5070e+08, device='cuda:0')
c= tensor(8.5074e+08, device='cuda:0')
c= tensor(8.5078e+08, device='cuda:0')
c= tensor(8.5081e+08, device='cuda:0')
c= tensor(8.5081e+08, device='cuda:0')
c= tensor(8.5082e+08, device='cuda:0')
c= tensor(8.5082e+08, device='cuda:0')
c= tensor(8.5082e+08, device='cuda:0')
c= tensor(8.5083e+08, device='cuda:0')
c= tensor(8.5083e+08, device='cuda:0')
c= tensor(8.5084e+08, device='cuda:0')
c= tensor(8.5085e+08, device='cuda:0')
c= tensor(8.5090e+08, device='cuda:0')
c= tensor(8.5091e+08, device='cuda:0')
c= tensor(8.5102e+08, device='cuda:0')
c= tensor(8.5102e+08, device='cuda:0')
c= tensor(8.5103e+08, device='cuda:0')
c= tensor(8.5104e+08, device='cuda:0')
c= tensor(8.5104e+08, device='cuda:0')
c= tensor(8.5107e+08, device='cuda:0')
c= tensor(8.5107e+08, device='cuda:0')
c= tensor(8.5108e+08, device='cuda:0')
c= tensor(8.5109e+08, device='cuda:0')
c= tensor(8.5109e+08, device='cuda:0')
c= tensor(8.5109e+08, device='cuda:0')
c= tensor(8.5110e+08, device='cuda:0')
c= tensor(8.5112e+08, device='cuda:0')
c= tensor(8.5113e+08, device='cuda:0')
c= tensor(8.5113e+08, device='cuda:0')
c= tensor(8.5113e+08, device='cuda:0')
c= tensor(8.5114e+08, device='cuda:0')
c= tensor(8.5115e+08, device='cuda:0')
c= tensor(8.5116e+08, device='cuda:0')
c= tensor(8.5117e+08, device='cuda:0')
c= tensor(8.5118e+08, device='cuda:0')
c= tensor(8.5120e+08, device='cuda:0')
c= tensor(8.5120e+08, device='cuda:0')
c= tensor(8.5122e+08, device='cuda:0')
c= tensor(8.5123e+08, device='cuda:0')
c= tensor(8.5124e+08, device='cuda:0')
c= tensor(8.5124e+08, device='cuda:0')
c= tensor(8.5126e+08, device='cuda:0')
c= tensor(8.5126e+08, device='cuda:0')
c= tensor(8.5127e+08, device='cuda:0')
c= tensor(8.5129e+08, device='cuda:0')
c= tensor(8.5129e+08, device='cuda:0')
c= tensor(8.5131e+08, device='cuda:0')
c= tensor(8.5132e+08, device='cuda:0')
c= tensor(8.5139e+08, device='cuda:0')
c= tensor(8.5139e+08, device='cuda:0')
c= tensor(8.5139e+08, device='cuda:0')
c= tensor(8.5140e+08, device='cuda:0')
c= tensor(8.5140e+08, device='cuda:0')
c= tensor(8.5141e+08, device='cuda:0')
c= tensor(8.5142e+08, device='cuda:0')
c= tensor(8.5142e+08, device='cuda:0')
c= tensor(8.5142e+08, device='cuda:0')
c= tensor(8.5143e+08, device='cuda:0')
c= tensor(8.5143e+08, device='cuda:0')
c= tensor(8.5144e+08, device='cuda:0')
c= tensor(8.5144e+08, device='cuda:0')
c= tensor(8.5147e+08, device='cuda:0')
c= tensor(8.5148e+08, device='cuda:0')
c= tensor(8.5149e+08, device='cuda:0')
c= tensor(8.5149e+08, device='cuda:0')
c= tensor(8.5150e+08, device='cuda:0')
c= tensor(8.5151e+08, device='cuda:0')
c= tensor(8.5152e+08, device='cuda:0')
c= tensor(8.5154e+08, device='cuda:0')
c= tensor(8.5155e+08, device='cuda:0')
c= tensor(8.5157e+08, device='cuda:0')
c= tensor(8.5157e+08, device='cuda:0')
c= tensor(8.5158e+08, device='cuda:0')
c= tensor(8.5158e+08, device='cuda:0')
c= tensor(8.5159e+08, device='cuda:0')
c= tensor(8.5159e+08, device='cuda:0')
c= tensor(8.5159e+08, device='cuda:0')
c= tensor(8.5160e+08, device='cuda:0')
c= tensor(8.5161e+08, device='cuda:0')
c= tensor(8.5161e+08, device='cuda:0')
c= tensor(8.5161e+08, device='cuda:0')
c= tensor(8.5162e+08, device='cuda:0')
c= tensor(8.5163e+08, device='cuda:0')
c= tensor(8.5164e+08, device='cuda:0')
c= tensor(8.5164e+08, device='cuda:0')
c= tensor(8.5165e+08, device='cuda:0')
c= tensor(8.5165e+08, device='cuda:0')
c= tensor(8.5167e+08, device='cuda:0')
c= tensor(8.5167e+08, device='cuda:0')
c= tensor(8.5167e+08, device='cuda:0')
c= tensor(8.5168e+08, device='cuda:0')
c= tensor(8.5168e+08, device='cuda:0')
c= tensor(8.5169e+08, device='cuda:0')
c= tensor(8.5169e+08, device='cuda:0')
c= tensor(8.5170e+08, device='cuda:0')
c= tensor(8.5171e+08, device='cuda:0')
c= tensor(8.5174e+08, device='cuda:0')
c= tensor(8.5175e+08, device='cuda:0')
c= tensor(8.5186e+08, device='cuda:0')
c= tensor(8.5484e+08, device='cuda:0')
c= tensor(8.5516e+08, device='cuda:0')
c= tensor(8.5518e+08, device='cuda:0')
c= tensor(8.5518e+08, device='cuda:0')
c= tensor(8.5519e+08, device='cuda:0')
c= tensor(8.5595e+08, device='cuda:0')
c= tensor(8.7478e+08, device='cuda:0')
c= tensor(8.7479e+08, device='cuda:0')
c= tensor(8.8148e+08, device='cuda:0')
c= tensor(8.8196e+08, device='cuda:0')
c= tensor(8.8227e+08, device='cuda:0')
c= tensor(8.9493e+08, device='cuda:0')
c= tensor(8.9493e+08, device='cuda:0')
c= tensor(8.9496e+08, device='cuda:0')
c= tensor(9.0976e+08, device='cuda:0')
c= tensor(9.5865e+08, device='cuda:0')
c= tensor(9.5867e+08, device='cuda:0')
c= tensor(9.5897e+08, device='cuda:0')
c= tensor(9.5946e+08, device='cuda:0')
c= tensor(9.6015e+08, device='cuda:0')
c= tensor(9.6564e+08, device='cuda:0')
c= tensor(9.6730e+08, device='cuda:0')
c= tensor(9.6817e+08, device='cuda:0')
c= tensor(9.6839e+08, device='cuda:0')
c= tensor(9.6840e+08, device='cuda:0')
c= tensor(1.0746e+09, device='cuda:0')
c= tensor(1.0746e+09, device='cuda:0')
c= tensor(1.0746e+09, device='cuda:0')
c= tensor(1.0764e+09, device='cuda:0')
c= tensor(1.0780e+09, device='cuda:0')
c= tensor(1.1101e+09, device='cuda:0')
c= tensor(1.1139e+09, device='cuda:0')
c= tensor(1.1139e+09, device='cuda:0')
c= tensor(1.1141e+09, device='cuda:0')
c= tensor(1.1141e+09, device='cuda:0')
c= tensor(1.1153e+09, device='cuda:0')
c= tensor(1.1175e+09, device='cuda:0')
c= tensor(1.1176e+09, device='cuda:0')
c= tensor(1.1188e+09, device='cuda:0')
c= tensor(1.1188e+09, device='cuda:0')
c= tensor(1.1188e+09, device='cuda:0')
c= tensor(1.1213e+09, device='cuda:0')
c= tensor(1.1246e+09, device='cuda:0')
c= tensor(1.1263e+09, device='cuda:0')
c= tensor(1.1265e+09, device='cuda:0')
c= tensor(1.1580e+09, device='cuda:0')
c= tensor(1.1580e+09, device='cuda:0')
c= tensor(1.1583e+09, device='cuda:0')
c= tensor(1.1597e+09, device='cuda:0')
c= tensor(1.1598e+09, device='cuda:0')
c= tensor(1.1648e+09, device='cuda:0')
c= tensor(1.1839e+09, device='cuda:0')
c= tensor(1.2191e+09, device='cuda:0')
c= tensor(1.2193e+09, device='cuda:0')
c= tensor(1.2197e+09, device='cuda:0')
c= tensor(1.2199e+09, device='cuda:0')
c= tensor(1.2199e+09, device='cuda:0')
c= tensor(1.2221e+09, device='cuda:0')
c= tensor(1.2221e+09, device='cuda:0')
c= tensor(1.2227e+09, device='cuda:0')
c= tensor(1.2710e+09, device='cuda:0')
c= tensor(1.2725e+09, device='cuda:0')
c= tensor(1.2728e+09, device='cuda:0')
c= tensor(1.2728e+09, device='cuda:0')
c= tensor(1.2819e+09, device='cuda:0')
c= tensor(1.2822e+09, device='cuda:0')
c= tensor(1.2840e+09, device='cuda:0')
c= tensor(1.2840e+09, device='cuda:0')
c= tensor(1.3149e+09, device='cuda:0')
c= tensor(1.3150e+09, device='cuda:0')
c= tensor(1.3208e+09, device='cuda:0')
c= tensor(1.3209e+09, device='cuda:0')
c= tensor(1.3233e+09, device='cuda:0')
c= tensor(1.3243e+09, device='cuda:0')
c= tensor(1.3450e+09, device='cuda:0')
c= tensor(1.3468e+09, device='cuda:0')
c= tensor(1.3468e+09, device='cuda:0')
c= tensor(1.3513e+09, device='cuda:0')
c= tensor(1.3550e+09, device='cuda:0')
c= tensor(1.3551e+09, device='cuda:0')
c= tensor(1.3659e+09, device='cuda:0')
c= tensor(1.3739e+09, device='cuda:0')
c= tensor(1.4048e+09, device='cuda:0')
c= tensor(1.4219e+09, device='cuda:0')
c= tensor(1.4219e+09, device='cuda:0')
c= tensor(1.4220e+09, device='cuda:0')
c= tensor(1.4253e+09, device='cuda:0')
c= tensor(1.4255e+09, device='cuda:0')
c= tensor(1.4262e+09, device='cuda:0')
c= tensor(1.4262e+09, device='cuda:0')
c= tensor(1.4274e+09, device='cuda:0')
c= tensor(1.4349e+09, device='cuda:0')
c= tensor(1.4370e+09, device='cuda:0')
c= tensor(1.4371e+09, device='cuda:0')
c= tensor(1.4374e+09, device='cuda:0')
c= tensor(1.4375e+09, device='cuda:0')
c= tensor(1.4375e+09, device='cuda:0')
c= tensor(1.4376e+09, device='cuda:0')
c= tensor(1.4376e+09, device='cuda:0')
c= tensor(1.4383e+09, device='cuda:0')
c= tensor(1.4390e+09, device='cuda:0')
c= tensor(1.4392e+09, device='cuda:0')
c= tensor(1.4405e+09, device='cuda:0')
c= tensor(1.4405e+09, device='cuda:0')
c= tensor(1.5009e+09, device='cuda:0')
c= tensor(1.5009e+09, device='cuda:0')
c= tensor(1.5058e+09, device='cuda:0')
c= tensor(1.5058e+09, device='cuda:0')
c= tensor(1.5058e+09, device='cuda:0')
c= tensor(1.5058e+09, device='cuda:0')
c= tensor(1.5068e+09, device='cuda:0')
c= tensor(1.5068e+09, device='cuda:0')
c= tensor(1.5242e+09, device='cuda:0')
c= tensor(1.5242e+09, device='cuda:0')
c= tensor(1.5242e+09, device='cuda:0')
c= tensor(1.5408e+09, device='cuda:0')
c= tensor(1.5424e+09, device='cuda:0')
c= tensor(1.5443e+09, device='cuda:0')
c= tensor(1.5576e+09, device='cuda:0')
c= tensor(1.5784e+09, device='cuda:0')
c= tensor(1.5784e+09, device='cuda:0')
c= tensor(1.5784e+09, device='cuda:0')
c= tensor(1.5785e+09, device='cuda:0')
c= tensor(1.5785e+09, device='cuda:0')
c= tensor(1.5785e+09, device='cuda:0')
c= tensor(1.5787e+09, device='cuda:0')
c= tensor(1.5787e+09, device='cuda:0')
c= tensor(1.5787e+09, device='cuda:0')
c= tensor(1.5787e+09, device='cuda:0')
c= tensor(1.5787e+09, device='cuda:0')
c= tensor(1.6194e+09, device='cuda:0')
c= tensor(1.6194e+09, device='cuda:0')
c= tensor(1.6227e+09, device='cuda:0')
c= tensor(1.6228e+09, device='cuda:0')
c= tensor(1.6233e+09, device='cuda:0')
c= tensor(1.6240e+09, device='cuda:0')
c= tensor(1.9551e+09, device='cuda:0')
c= tensor(2.0435e+09, device='cuda:0')
c= tensor(2.0439e+09, device='cuda:0')
c= tensor(2.0447e+09, device='cuda:0')
c= tensor(2.0448e+09, device='cuda:0')
c= tensor(2.0453e+09, device='cuda:0')
c= tensor(2.0489e+09, device='cuda:0')
c= tensor(2.0495e+09, device='cuda:0')
c= tensor(2.0495e+09, device='cuda:0')
c= tensor(2.0522e+09, device='cuda:0')
c= tensor(2.1285e+09, device='cuda:0')
c= tensor(2.1295e+09, device='cuda:0')
c= tensor(2.1296e+09, device='cuda:0')
c= tensor(2.1306e+09, device='cuda:0')
c= tensor(2.1307e+09, device='cuda:0')
c= tensor(2.1307e+09, device='cuda:0')
c= tensor(2.1382e+09, device='cuda:0')
c= tensor(2.1384e+09, device='cuda:0')
c= tensor(2.1385e+09, device='cuda:0')
c= tensor(2.1406e+09, device='cuda:0')
c= tensor(2.1407e+09, device='cuda:0')
c= tensor(2.1407e+09, device='cuda:0')
c= tensor(2.1441e+09, device='cuda:0')
c= tensor(2.1473e+09, device='cuda:0')
c= tensor(2.1568e+09, device='cuda:0')
c= tensor(2.1771e+09, device='cuda:0')
c= tensor(2.1963e+09, device='cuda:0')
c= tensor(2.1965e+09, device='cuda:0')
c= tensor(2.1981e+09, device='cuda:0')
c= tensor(2.2037e+09, device='cuda:0')
c= tensor(2.2107e+09, device='cuda:0')
c= tensor(2.2107e+09, device='cuda:0')
c= tensor(2.3004e+09, device='cuda:0')
c= tensor(2.4109e+09, device='cuda:0')
c= tensor(2.4191e+09, device='cuda:0')
c= tensor(2.4230e+09, device='cuda:0')
c= tensor(2.4259e+09, device='cuda:0')
c= tensor(2.4260e+09, device='cuda:0')
c= tensor(2.4260e+09, device='cuda:0')
c= tensor(2.4260e+09, device='cuda:0')
c= tensor(2.4299e+09, device='cuda:0')
c= tensor(2.4351e+09, device='cuda:0')
c= tensor(2.4781e+09, device='cuda:0')
c= tensor(2.4849e+09, device='cuda:0')
c= tensor(2.4875e+09, device='cuda:0')
c= tensor(2.4877e+09, device='cuda:0')
c= tensor(2.4932e+09, device='cuda:0')
c= tensor(2.4932e+09, device='cuda:0')
c= tensor(2.4933e+09, device='cuda:0')
c= tensor(2.5026e+09, device='cuda:0')
c= tensor(2.5028e+09, device='cuda:0')
c= tensor(2.5028e+09, device='cuda:0')
c= tensor(2.5029e+09, device='cuda:0')
c= tensor(2.5408e+09, device='cuda:0')
c= tensor(2.5410e+09, device='cuda:0')
c= tensor(2.5526e+09, device='cuda:0')
c= tensor(2.5527e+09, device='cuda:0')
c= tensor(2.5529e+09, device='cuda:0')
c= tensor(2.5529e+09, device='cuda:0')
c= tensor(2.5529e+09, device='cuda:0')
c= tensor(2.5537e+09, device='cuda:0')
c= tensor(2.5550e+09, device='cuda:0')
c= tensor(2.5550e+09, device='cuda:0')
c= tensor(2.5600e+09, device='cuda:0')
c= tensor(2.5600e+09, device='cuda:0')
c= tensor(2.5607e+09, device='cuda:0')
c= tensor(2.5608e+09, device='cuda:0')
c= tensor(2.5647e+09, device='cuda:0')
c= tensor(2.5649e+09, device='cuda:0')
c= tensor(2.5660e+09, device='cuda:0')
c= tensor(2.5662e+09, device='cuda:0')
c= tensor(2.5667e+09, device='cuda:0')
c= tensor(2.5687e+09, device='cuda:0')
c= tensor(2.6354e+09, device='cuda:0')
c= tensor(2.6355e+09, device='cuda:0')
c= tensor(2.6356e+09, device='cuda:0')
c= tensor(2.6382e+09, device='cuda:0')
c= tensor(2.6383e+09, device='cuda:0')
c= tensor(2.7086e+09, device='cuda:0')
c= tensor(2.7086e+09, device='cuda:0')
c= tensor(2.7138e+09, device='cuda:0')
c= tensor(2.7279e+09, device='cuda:0')
c= tensor(2.7279e+09, device='cuda:0')
c= tensor(2.7433e+09, device='cuda:0')
c= tensor(2.7439e+09, device='cuda:0')
c= tensor(2.8888e+09, device='cuda:0')
c= tensor(2.8888e+09, device='cuda:0')
c= tensor(2.8892e+09, device='cuda:0')
c= tensor(2.8893e+09, device='cuda:0')
c= tensor(2.8893e+09, device='cuda:0')
c= tensor(2.8893e+09, device='cuda:0')
c= tensor(2.8906e+09, device='cuda:0')
c= tensor(2.8912e+09, device='cuda:0')
c= tensor(2.8969e+09, device='cuda:0')
c= tensor(2.8970e+09, device='cuda:0')
c= tensor(2.8970e+09, device='cuda:0')
c= tensor(2.8971e+09, device='cuda:0')
c= tensor(2.9117e+09, device='cuda:0')
c= tensor(2.9155e+09, device='cuda:0')
c= tensor(2.9320e+09, device='cuda:0')
c= tensor(2.9327e+09, device='cuda:0')
c= tensor(2.9327e+09, device='cuda:0')
c= tensor(2.9327e+09, device='cuda:0')
c= tensor(2.9329e+09, device='cuda:0')
c= tensor(3.0361e+09, device='cuda:0')
c= tensor(3.0361e+09, device='cuda:0')
c= tensor(3.0364e+09, device='cuda:0')
c= tensor(3.0399e+09, device='cuda:0')
c= tensor(3.0418e+09, device='cuda:0')
c= tensor(3.0419e+09, device='cuda:0')
c= tensor(3.0419e+09, device='cuda:0')
c= tensor(3.0674e+09, device='cuda:0')
c= tensor(3.0687e+09, device='cuda:0')
c= tensor(3.0689e+09, device='cuda:0')
c= tensor(3.0692e+09, device='cuda:0')
c= tensor(3.0882e+09, device='cuda:0')
c= tensor(3.0944e+09, device='cuda:0')
c= tensor(3.1264e+09, device='cuda:0')
c= tensor(3.1318e+09, device='cuda:0')
c= tensor(3.1318e+09, device='cuda:0')
c= tensor(3.1325e+09, device='cuda:0')
c= tensor(3.1346e+09, device='cuda:0')
c= tensor(3.1348e+09, device='cuda:0')
c= tensor(3.1349e+09, device='cuda:0')
c= tensor(3.1349e+09, device='cuda:0')
c= tensor(3.1375e+09, device='cuda:0')
c= tensor(3.1377e+09, device='cuda:0')
c= tensor(3.1377e+09, device='cuda:0')
c= tensor(3.1379e+09, device='cuda:0')
c= tensor(3.1384e+09, device='cuda:0')
c= tensor(3.1420e+09, device='cuda:0')
c= tensor(3.1421e+09, device='cuda:0')
c= tensor(3.1423e+09, device='cuda:0')
c= tensor(3.1430e+09, device='cuda:0')
c= tensor(3.1432e+09, device='cuda:0')
c= tensor(3.1432e+09, device='cuda:0')
c= tensor(3.1433e+09, device='cuda:0')
c= tensor(3.1434e+09, device='cuda:0')
c= tensor(3.1620e+09, device='cuda:0')
c= tensor(3.1620e+09, device='cuda:0')
c= tensor(3.1620e+09, device='cuda:0')
c= tensor(3.1621e+09, device='cuda:0')
c= tensor(3.1731e+09, device='cuda:0')
c= tensor(3.1920e+09, device='cuda:0')
c= tensor(3.1939e+09, device='cuda:0')
c= tensor(3.1940e+09, device='cuda:0')
c= tensor(3.2015e+09, device='cuda:0')
c= tensor(3.2069e+09, device='cuda:0')
c= tensor(3.2069e+09, device='cuda:0')
c= tensor(3.2071e+09, device='cuda:0')
c= tensor(3.2075e+09, device='cuda:0')
c= tensor(3.2104e+09, device='cuda:0')
c= tensor(3.2111e+09, device='cuda:0')
c= tensor(3.2183e+09, device='cuda:0')
c= tensor(3.2184e+09, device='cuda:0')
c= tensor(3.2189e+09, device='cuda:0')
c= tensor(3.2189e+09, device='cuda:0')
c= tensor(3.2193e+09, device='cuda:0')
c= tensor(3.2195e+09, device='cuda:0')
c= tensor(3.2203e+09, device='cuda:0')
c= tensor(3.2344e+09, device='cuda:0')
c= tensor(3.2825e+09, device='cuda:0')
c= tensor(3.2825e+09, device='cuda:0')
c= tensor(3.2825e+09, device='cuda:0')
c= tensor(3.2826e+09, device='cuda:0')
c= tensor(3.2830e+09, device='cuda:0')
c= tensor(3.2834e+09, device='cuda:0')
c= tensor(3.2834e+09, device='cuda:0')
c= tensor(3.2834e+09, device='cuda:0')
c= tensor(3.2836e+09, device='cuda:0')
c= tensor(3.2836e+09, device='cuda:0')
c= tensor(3.2862e+09, device='cuda:0')
c= tensor(3.2862e+09, device='cuda:0')
c= tensor(3.2863e+09, device='cuda:0')
c= tensor(3.2864e+09, device='cuda:0')
c= tensor(3.2864e+09, device='cuda:0')
c= tensor(3.2864e+09, device='cuda:0')
c= tensor(3.2922e+09, device='cuda:0')
c= tensor(3.3310e+09, device='cuda:0')
c= tensor(3.3403e+09, device='cuda:0')
c= tensor(3.3432e+09, device='cuda:0')
c= tensor(3.3434e+09, device='cuda:0')
c= tensor(3.3434e+09, device='cuda:0')
c= tensor(3.3435e+09, device='cuda:0')
c= tensor(3.3480e+09, device='cuda:0')
c= tensor(3.3483e+09, device='cuda:0')
c= tensor(3.3491e+09, device='cuda:0')
c= tensor(3.3492e+09, device='cuda:0')
c= tensor(3.5626e+09, device='cuda:0')
c= tensor(3.5630e+09, device='cuda:0')
c= tensor(3.5640e+09, device='cuda:0')
c= tensor(3.5845e+09, device='cuda:0')
c= tensor(3.5850e+09, device='cuda:0')
c= tensor(3.5854e+09, device='cuda:0')
c= tensor(3.6247e+09, device='cuda:0')
c= tensor(3.6285e+09, device='cuda:0')
c= tensor(3.6297e+09, device='cuda:0')
c= tensor(3.6299e+09, device='cuda:0')
c= tensor(3.6306e+09, device='cuda:0')
c= tensor(3.6306e+09, device='cuda:0')
c= tensor(3.6392e+09, device='cuda:0')
c= tensor(3.8239e+09, device='cuda:0')
c= tensor(3.8414e+09, device='cuda:0')
c= tensor(3.8507e+09, device='cuda:0')
c= tensor(3.8520e+09, device='cuda:0')
c= tensor(3.8544e+09, device='cuda:0')
c= tensor(3.8570e+09, device='cuda:0')
c= tensor(3.8571e+09, device='cuda:0')
c= tensor(3.8573e+09, device='cuda:0')
c= tensor(3.8865e+09, device='cuda:0')
c= tensor(3.8882e+09, device='cuda:0')
c= tensor(3.9128e+09, device='cuda:0')
c= tensor(3.9219e+09, device='cuda:0')
c= tensor(3.9227e+09, device='cuda:0')
c= tensor(3.9227e+09, device='cuda:0')
c= tensor(3.9237e+09, device='cuda:0')
c= tensor(3.9278e+09, device='cuda:0')
c= tensor(3.9278e+09, device='cuda:0')
c= tensor(4.0083e+09, device='cuda:0')
c= tensor(4.0102e+09, device='cuda:0')
c= tensor(4.0110e+09, device='cuda:0')
c= tensor(4.0111e+09, device='cuda:0')
c= tensor(4.0112e+09, device='cuda:0')
c= tensor(4.0112e+09, device='cuda:0')
c= tensor(4.0113e+09, device='cuda:0')
c= tensor(4.0118e+09, device='cuda:0')
c= tensor(4.0188e+09, device='cuda:0')
c= tensor(6.1885e+09, device='cuda:0')
c= tensor(6.1887e+09, device='cuda:0')
c= tensor(6.1917e+09, device='cuda:0')
c= tensor(6.1918e+09, device='cuda:0')
c= tensor(6.1919e+09, device='cuda:0')
c= tensor(6.1920e+09, device='cuda:0')
c= tensor(6.2129e+09, device='cuda:0')
c= tensor(6.2130e+09, device='cuda:0')
c= tensor(6.4806e+09, device='cuda:0')
c= tensor(6.4806e+09, device='cuda:0')
c= tensor(6.4925e+09, device='cuda:0')
c= tensor(6.4940e+09, device='cuda:0')
c= tensor(6.4992e+09, device='cuda:0')
c= tensor(6.5147e+09, device='cuda:0')
c= tensor(6.5148e+09, device='cuda:0')
c= tensor(6.5149e+09, device='cuda:0')
c= tensor(6.5167e+09, device='cuda:0')
c= tensor(6.5167e+09, device='cuda:0')
c= tensor(6.5169e+09, device='cuda:0')
c= tensor(6.5198e+09, device='cuda:0')
c= tensor(6.5216e+09, device='cuda:0')
c= tensor(6.5233e+09, device='cuda:0')
c= tensor(6.5236e+09, device='cuda:0')
c= tensor(6.5374e+09, device='cuda:0')
c= tensor(6.5687e+09, device='cuda:0')
c= tensor(6.5693e+09, device='cuda:0')
c= tensor(6.5694e+09, device='cuda:0')
c= tensor(6.5761e+09, device='cuda:0')
c= tensor(6.5776e+09, device='cuda:0')
c= tensor(6.5783e+09, device='cuda:0')
c= tensor(6.5793e+09, device='cuda:0')
c= tensor(6.5805e+09, device='cuda:0')
c= tensor(6.5821e+09, device='cuda:0')
c= tensor(6.6128e+09, device='cuda:0')
c= tensor(6.6152e+09, device='cuda:0')
c= tensor(6.6152e+09, device='cuda:0')
c= tensor(6.6153e+09, device='cuda:0')
c= tensor(6.6157e+09, device='cuda:0')
c= tensor(6.6186e+09, device='cuda:0')
c= tensor(6.6233e+09, device='cuda:0')
c= tensor(6.6264e+09, device='cuda:0')
c= tensor(6.6265e+09, device='cuda:0')
c= tensor(6.6267e+09, device='cuda:0')
c= tensor(6.6287e+09, device='cuda:0')
c= tensor(6.6399e+09, device='cuda:0')
c= tensor(6.6411e+09, device='cuda:0')
c= tensor(6.6429e+09, device='cuda:0')
c= tensor(6.6433e+09, device='cuda:0')
c= tensor(6.6439e+09, device='cuda:0')
c= tensor(6.6439e+09, device='cuda:0')
c= tensor(6.6439e+09, device='cuda:0')
c= tensor(6.6530e+09, device='cuda:0')
c= tensor(6.6541e+09, device='cuda:0')
c= tensor(6.6551e+09, device='cuda:0')
c= tensor(6.6551e+09, device='cuda:0')
c= tensor(6.6551e+09, device='cuda:0')
c= tensor(6.6580e+09, device='cuda:0')
c= tensor(6.6596e+09, device='cuda:0')
c= tensor(6.6602e+09, device='cuda:0')
c= tensor(6.6602e+09, device='cuda:0')
c= tensor(6.6602e+09, device='cuda:0')
c= tensor(6.6615e+09, device='cuda:0')
c= tensor(6.6618e+09, device='cuda:0')
c= tensor(6.6701e+09, device='cuda:0')
c= tensor(6.6702e+09, device='cuda:0')
c= tensor(6.6702e+09, device='cuda:0')
c= tensor(6.6705e+09, device='cuda:0')
c= tensor(6.6706e+09, device='cuda:0')
c= tensor(6.6992e+09, device='cuda:0')
c= tensor(6.6993e+09, device='cuda:0')
c= tensor(6.7006e+09, device='cuda:0')
c= tensor(6.7036e+09, device='cuda:0')
c= tensor(6.7108e+09, device='cuda:0')
c= tensor(6.7222e+09, device='cuda:0')
c= tensor(6.7281e+09, device='cuda:0')
c= tensor(6.7281e+09, device='cuda:0')
c= tensor(6.7288e+09, device='cuda:0')
c= tensor(6.7289e+09, device='cuda:0')
c= tensor(6.7373e+09, device='cuda:0')
c= tensor(6.7411e+09, device='cuda:0')
c= tensor(6.7418e+09, device='cuda:0')
c= tensor(6.7435e+09, device='cuda:0')
c= tensor(6.7436e+09, device='cuda:0')
c= tensor(6.7466e+09, device='cuda:0')
c= tensor(6.7466e+09, device='cuda:0')
c= tensor(6.7469e+09, device='cuda:0')
c= tensor(6.7684e+09, device='cuda:0')
c= tensor(6.7918e+09, device='cuda:0')
c= tensor(6.7919e+09, device='cuda:0')
c= tensor(6.7922e+09, device='cuda:0')
c= tensor(6.7923e+09, device='cuda:0')
c= tensor(6.8076e+09, device='cuda:0')
c= tensor(6.8078e+09, device='cuda:0')
c= tensor(6.8079e+09, device='cuda:0')
c= tensor(6.8098e+09, device='cuda:0')
c= tensor(6.8148e+09, device='cuda:0')
c= tensor(6.8148e+09, device='cuda:0')
c= tensor(6.8165e+09, device='cuda:0')
c= tensor(6.8165e+09, device='cuda:0')
time to make c is 10.04008173942566
time for making loss is 10.040107011795044
p0 True
it  0 : 2347406336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5271707648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 12% |
memory (bytes)
5272084480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1915229200.0
relative error loss 0.2809697
shape of L is 
torch.Size([])
memory (bytes)
5298585600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5298733056
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1901411300.0
relative error loss 0.27894258
shape of L is 
torch.Size([])
memory (bytes)
5302042624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5302185984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1846518300.0
relative error loss 0.2708896
shape of L is 
torch.Size([])
memory (bytes)
5305454592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5305454592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1809726500.0
relative error loss 0.26549214
shape of L is 
torch.Size([])
memory (bytes)
5308469248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5308645376
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1783278600.0
relative error loss 0.26161218
shape of L is 
torch.Size([])
memory (bytes)
5311782912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5311840256
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1765222400.0
relative error loss 0.25896326
shape of L is 
torch.Size([])
memory (bytes)
5314961408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 12% |
memory (bytes)
5315051520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1752152000.0
relative error loss 0.2570458
shape of L is 
torch.Size([])
memory (bytes)
5318258688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 12% |
memory (bytes)
5318258688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1739331600.0
relative error loss 0.255165
shape of L is 
torch.Size([])
memory (bytes)
5321428992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 12% |
memory (bytes)
5321428992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1730451000.0
relative error loss 0.2538622
shape of L is 
torch.Size([])
memory (bytes)
5324533760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5324673024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1722431500.0
relative error loss 0.25268573
time to take a step is 265.6801743507385
it  1 : 2845774336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5327900672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 12% |
memory (bytes)
5327900672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1722431500.0
relative error loss 0.25268573
shape of L is 
torch.Size([])
memory (bytes)
5331054592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5331128320
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1714972200.0
relative error loss 0.2515914
shape of L is 
torch.Size([])
memory (bytes)
5334282240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5334339584
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1710056000.0
relative error loss 0.2508702
shape of L is 
torch.Size([])
memory (bytes)
5337530368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 12% |
memory (bytes)
5337530368
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1705159200.0
relative error loss 0.2501518
shape of L is 
torch.Size([])
memory (bytes)
5340749824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5340753920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1698235400.0
relative error loss 0.24913608
shape of L is 
torch.Size([])
memory (bytes)
5343834112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 12% |
memory (bytes)
5343969280
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1694816300.0
relative error loss 0.24863449
shape of L is 
torch.Size([])
memory (bytes)
5347184640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5347184640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1690858000.0
relative error loss 0.24805379
shape of L is 
torch.Size([])
memory (bytes)
5350203392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 12% |
memory (bytes)
5350412288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1687495700.0
relative error loss 0.24756053
shape of L is 
torch.Size([])
memory (bytes)
5353566208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5353615360
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1684475900.0
relative error loss 0.24711752
shape of L is 
torch.Size([])
memory (bytes)
5356744704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 12% |
memory (bytes)
5356744704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1683657700.0
relative error loss 0.24699749
time to take a step is 258.44668221473694
it  2 : 2845774336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5360046080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5360050176
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1683657700.0
relative error loss 0.24699749
shape of L is 
torch.Size([])
memory (bytes)
5363048448
| ID | GPU | MEM |
------------------
|  0 | 25% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5363249152
| ID | GPU  | MEM |
-------------------
|  0 |  25% |  0% |
|  1 | 100% | 12% |
error is  1678322200.0
relative error loss 0.24621476
shape of L is 
torch.Size([])
memory (bytes)
5366480896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 12% |
memory (bytes)
5366480896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1676801500.0
relative error loss 0.24599168
shape of L is 
torch.Size([])
memory (bytes)
5369659392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
memory (bytes)
5369659392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1674487800.0
relative error loss 0.24565224
shape of L is 
torch.Size([])
memory (bytes)
5372841984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5372841984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1671251000.0
relative error loss 0.24517739
shape of L is 
torch.Size([])
memory (bytes)
5376110592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5376110592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1669133800.0
relative error loss 0.2448668
shape of L is 
torch.Size([])
memory (bytes)
5379235840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5379235840
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1666252300.0
relative error loss 0.24444407
shape of L is 
torch.Size([])
memory (bytes)
5382537216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5382537216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1664867800.0
relative error loss 0.24424097
shape of L is 
torch.Size([])
memory (bytes)
5385674752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
memory (bytes)
5385674752
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1662896100.0
relative error loss 0.24395171
shape of L is 
torch.Size([])
memory (bytes)
5388963840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 12% |
memory (bytes)
5388963840
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1661867000.0
relative error loss 0.24380073
time to take a step is 257.5262596607208
it  3 : 2845774336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5392072704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 12% |
memory (bytes)
5392101376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1661867000.0
relative error loss 0.24380073
shape of L is 
torch.Size([])
memory (bytes)
5395378176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 12% |
memory (bytes)
5395382272
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1660729900.0
relative error loss 0.24363391
shape of L is 
torch.Size([])
memory (bytes)
5398413312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
memory (bytes)
5398597632
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1659322400.0
relative error loss 0.24342743
shape of L is 
torch.Size([])
memory (bytes)
5401747456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5401804800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1658075100.0
relative error loss 0.24324445
shape of L is 
torch.Size([])
memory (bytes)
5404958720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 12% |
memory (bytes)
5405020160
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1656379400.0
relative error loss 0.24299568
shape of L is 
torch.Size([])
memory (bytes)
5408227328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
memory (bytes)
5408227328
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1654854100.0
relative error loss 0.24277192
shape of L is 
torch.Size([])
memory (bytes)
5411459072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 12% |
memory (bytes)
5411459072
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1653756900.0
relative error loss 0.24261096
shape of L is 
torch.Size([])
memory (bytes)
5414469632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5414658048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% | 12% |
error is  1652576800.0
relative error loss 0.24243782
shape of L is 
torch.Size([])
memory (bytes)
5417832448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5417873408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% | 12% |
error is  1651561000.0
relative error loss 0.24228881
shape of L is 
torch.Size([])
memory (bytes)
5420941312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5420941312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1650316300.0
relative error loss 0.24210621
time to take a step is 258.61304330825806
it  4 : 2845774336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5424300032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5424300032
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1650316300.0
relative error loss 0.24210621
shape of L is 
torch.Size([])
memory (bytes)
5427400704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5427503104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1648903200.0
relative error loss 0.2418989
shape of L is 
torch.Size([])
memory (bytes)
5430722560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5430722560
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1648150000.0
relative error loss 0.24178842
shape of L is 
torch.Size([])
memory (bytes)
5433872384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5433929728
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1647244800.0
relative error loss 0.24165562
shape of L is 
torch.Size([])
memory (bytes)
5437145088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5437145088
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1646574100.0
relative error loss 0.24155721
shape of L is 
torch.Size([])
memory (bytes)
5440331776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5440331776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1645888000.0
relative error loss 0.24145657
shape of L is 
torch.Size([])
memory (bytes)
5443387392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5443571712
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1644936200.0
relative error loss 0.24131693
shape of L is 
torch.Size([])
memory (bytes)
5446787072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5446787072
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1644043800.0
relative error loss 0.24118601
shape of L is 
torch.Size([])
memory (bytes)
5449875456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
memory (bytes)
5449981952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1643357700.0
relative error loss 0.24108537
shape of L is 
torch.Size([])
memory (bytes)
5453213696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5453213696
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1642640400.0
relative error loss 0.24098013
time to take a step is 258.8562970161438
it  5 : 2845774336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5456302080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5456433152
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1642640400.0
relative error loss 0.24098013
shape of L is 
torch.Size([])
memory (bytes)
5459582976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 12% |
memory (bytes)
5459628032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1642510800.0
relative error loss 0.24096113
shape of L is 
torch.Size([])
memory (bytes)
5462765568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 12% |
memory (bytes)
5462863872
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1640978400.0
relative error loss 0.24073632
shape of L is 
torch.Size([])
memory (bytes)
5466066944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 12% |
memory (bytes)
5466066944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1640564700.0
relative error loss 0.24067563
shape of L is 
torch.Size([])
memory (bytes)
5469241344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5469241344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% | 12% |
error is  1639927800.0
relative error loss 0.24058218
shape of L is 
torch.Size([])
memory (bytes)
5472509952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5472509952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1639208000.0
relative error loss 0.24047658
shape of L is 
torch.Size([])
memory (bytes)
5475663872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5475721216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1638552600.0
relative error loss 0.24038044
shape of L is 
torch.Size([])
memory (bytes)
5478920192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
memory (bytes)
5478920192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1638000100.0
relative error loss 0.24029939
shape of L is 
torch.Size([])
memory (bytes)
5482135552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 12% |
memory (bytes)
5482135552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1637262300.0
relative error loss 0.24019116
shape of L is 
torch.Size([])
memory (bytes)
5485334528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 12% |
memory (bytes)
5485334528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1636579300.0
relative error loss 0.24009095
time to take a step is 258.63875126838684
it  6 : 2845774336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5488545792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5488562176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% | 12% |
error is  1636579300.0
relative error loss 0.24009095
shape of L is 
torch.Size([])
memory (bytes)
5491736576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 12% |
memory (bytes)
5491777536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1636044800.0
relative error loss 0.24001254
shape of L is 
torch.Size([])
memory (bytes)
5494988800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 12% |
memory (bytes)
5494988800
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1635612200.0
relative error loss 0.23994908
shape of L is 
torch.Size([])
memory (bytes)
5498068992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 12% |
memory (bytes)
5498212352
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1635170800.0
relative error loss 0.23988433
shape of L is 
torch.Size([])
memory (bytes)
5501333504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5501423616
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1634657300.0
relative error loss 0.23980899
shape of L is 
torch.Size([])
memory (bytes)
5504638976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 12% |
memory (bytes)
5504638976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1634260500.0
relative error loss 0.23975077
shape of L is 
torch.Size([])
memory (bytes)
5507751936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 12% |
memory (bytes)
5507858432
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1633674200.0
relative error loss 0.23966478
shape of L is 
torch.Size([])
memory (bytes)
5511061504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5511061504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1633343000.0
relative error loss 0.23961617
shape of L is 
torch.Size([])
memory (bytes)
5514170368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5514272768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1632917500.0
relative error loss 0.23955376
shape of L is 
torch.Size([])
memory (bytes)
5517418496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  5% | 12% |
memory (bytes)
5517479936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1632413700.0
relative error loss 0.23947984
time to take a step is 259.71027541160583
it  7 : 2845774336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5520642048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5520642048
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1632413700.0
relative error loss 0.23947984
shape of L is 
torch.Size([])
memory (bytes)
5523906560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 12% |
memory (bytes)
5523906560
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1631871500.0
relative error loss 0.2394003
shape of L is 
torch.Size([])
memory (bytes)
5527019520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 12% |
memory (bytes)
5527126016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1631395800.0
relative error loss 0.23933053
shape of L is 
torch.Size([])
memory (bytes)
5530333184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
memory (bytes)
5530333184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1630989300.0
relative error loss 0.23927088
shape of L is 
torch.Size([])
memory (bytes)
5533499392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 12% |
memory (bytes)
5533548544
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1630590000.0
relative error loss 0.2392123
shape of L is 
torch.Size([])
memory (bytes)
5536722944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5536755712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 12% |
error is  1629847600.0
relative error loss 0.23910339
shape of L is 
torch.Size([])
memory (bytes)
5539962880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5539962880
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1629623800.0
relative error loss 0.23907056
shape of L is 
torch.Size([])
memory (bytes)
5543108608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5543174144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1629302300.0
relative error loss 0.23902339
shape of L is 
torch.Size([])
memory (bytes)
5546381312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5546381312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1628939300.0
relative error loss 0.23897013
shape of L is 
torch.Size([])
memory (bytes)
5549518848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  0% | 12% |
memory (bytes)
5549596672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1628525600.0
relative error loss 0.23890945
time to take a step is 262.0206141471863
it  8 : 2845774336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5552820224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5552820224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1628525600.0
relative error loss 0.23890945
shape of L is 
torch.Size([])
memory (bytes)
5555974144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5556027392
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1628026900.0
relative error loss 0.23883629
shape of L is 
torch.Size([])
memory (bytes)
5559238656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5559238656
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1627686900.0
relative error loss 0.23878641
shape of L is 
torch.Size([])
memory (bytes)
5562462208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 12% |
memory (bytes)
5562462208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1627404800.0
relative error loss 0.23874503
shape of L is 
torch.Size([])
memory (bytes)
5565530112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5565673472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1626835000.0
relative error loss 0.23866142
shape of L is 
torch.Size([])
memory (bytes)
5568888832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5568888832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1626664400.0
relative error loss 0.23863642
shape of L is 
torch.Size([])
memory (bytes)
5571985408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5572096000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1626333700.0
relative error loss 0.23858789
shape of L is 
torch.Size([])
memory (bytes)
5575258112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 12% |
memory (bytes)
5575311360
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1626337800.0
relative error loss 0.2385885
shape of L is 
torch.Size([])
memory (bytes)
5578522624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5578522624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1626091500.0
relative error loss 0.23855236
shape of L is 
torch.Size([])
memory (bytes)
5581733888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 12% |
memory (bytes)
5581733888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1625762300.0
relative error loss 0.23850407
time to take a step is 259.86587405204773
it  9 : 2845774336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5584863232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5584953344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1625762300.0
relative error loss 0.23850407
shape of L is 
torch.Size([])
memory (bytes)
5588168704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 12% |
memory (bytes)
5588168704
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1625220100.0
relative error loss 0.23842452
shape of L is 
torch.Size([])
memory (bytes)
5591281664
| ID | GPU | MEM |
------------------
|  0 |  5% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5591388160
| ID | GPU | MEM |
------------------
|  0 |  3% |  0% |
|  1 | 99% | 12% |
error is  1625067000.0
relative error loss 0.23840207
shape of L is 
torch.Size([])
memory (bytes)
5594562560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 12% |
memory (bytes)
5594562560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1624750600.0
relative error loss 0.23835565
shape of L is 
torch.Size([])
memory (bytes)
5597810688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 12% |
memory (bytes)
5597810688
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1624575000.0
relative error loss 0.23832989
shape of L is 
torch.Size([])
memory (bytes)
5600890880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5601021952
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1624441900.0
relative error loss 0.23831035
shape of L is 
torch.Size([])
memory (bytes)
5604233216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 12% |
memory (bytes)
5604233216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1624104400.0
relative error loss 0.23826085
shape of L is 
torch.Size([])
memory (bytes)
5607391232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5607391232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1623724000.0
relative error loss 0.23820505
shape of L is 
torch.Size([])
memory (bytes)
5610659840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
memory (bytes)
5610659840
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1623400000.0
relative error loss 0.2381575
shape of L is 
torch.Size([])
memory (bytes)
5613694976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5613871104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1622942700.0
relative error loss 0.23809043
time to take a step is 259.7375831604004
it  10 : 2845774336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5617086464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5617086464
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1622942700.0
relative error loss 0.23809043
shape of L is 
torch.Size([])
memory (bytes)
5620191232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
memory (bytes)
5620293632
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1622768100.0
relative error loss 0.23806481
shape of L is 
torch.Size([])
memory (bytes)
5623488512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 12% |
memory (bytes)
5623488512
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1622178300.0
relative error loss 0.23797828
shape of L is 
torch.Size([])
memory (bytes)
5626724352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 12% |
memory (bytes)
5626724352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1622006300.0
relative error loss 0.23795305
shape of L is 
torch.Size([])
memory (bytes)
5629804544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5629943808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1621845000.0
relative error loss 0.23792939
shape of L is 
torch.Size([])
memory (bytes)
5633159168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5633159168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 12% |
error is  1621528000.0
relative error loss 0.2378829
shape of L is 
torch.Size([])
memory (bytes)
5636308992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 12% |
memory (bytes)
5636366336
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1621589000.0
relative error loss 0.23789184
shape of L is 
torch.Size([])
memory (bytes)
5639573504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5639573504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1621307400.0
relative error loss 0.23785052
shape of L is 
torch.Size([])
memory (bytes)
5642665984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5642780672
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1620946400.0
relative error loss 0.23779757
shape of L is 
torch.Size([])
memory (bytes)
5645987840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
memory (bytes)
5645987840
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1620660700.0
relative error loss 0.23775566
time to take a step is 259.81377816200256
it  11 : 2845774336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5649096704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5649199104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1620660700.0
relative error loss 0.23775566
shape of L is 
torch.Size([])
memory (bytes)
5652410368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5652410368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1620207100.0
relative error loss 0.23768911
shape of L is 
torch.Size([])
memory (bytes)
5655568384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5655625728
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1619935700.0
relative error loss 0.23764929
shape of L is 
torch.Size([])
memory (bytes)
5658832896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5658832896
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1619660300.0
relative error loss 0.23760888
shape of L is 
torch.Size([])
memory (bytes)
5662040064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
memory (bytes)
5662056448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1619393000.0
relative error loss 0.23756967
shape of L is 
torch.Size([])
memory (bytes)
5665214464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5665271808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1619199500.0
relative error loss 0.23754129
shape of L is 
torch.Size([])
memory (bytes)
5668483072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5668483072
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1618909200.0
relative error loss 0.2374987
shape of L is 
torch.Size([])
memory (bytes)
5671587840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5671698432
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1618671100.0
relative error loss 0.23746377
shape of L is 
torch.Size([])
memory (bytes)
5674848256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
memory (bytes)
5674905600
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1618465800.0
relative error loss 0.23743364
shape of L is 
torch.Size([])
memory (bytes)
5678084096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5678084096
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1618288600.0
relative error loss 0.23740765
time to take a step is 259.37010955810547
it  12 : 2845774336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5681324032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5681324032
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1618288600.0
relative error loss 0.23740765
shape of L is 
torch.Size([])
memory (bytes)
5684412416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 12% |
memory (bytes)
5684543488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1618069000.0
relative error loss 0.23737544
shape of L is 
torch.Size([])
memory (bytes)
5687754752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 12% |
memory (bytes)
5687754752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1617912800.0
relative error loss 0.23735254
shape of L is 
torch.Size([])
memory (bytes)
5690970112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5690970112
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1617781800.0
relative error loss 0.2373333
shape of L is 
torch.Size([])
memory (bytes)
5694013440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5694173184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1617624000.0
relative error loss 0.23731017
shape of L is 
torch.Size([])
memory (bytes)
5697347584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 12% |
memory (bytes)
5697404928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1617466400.0
relative error loss 0.23728703
shape of L is 
torch.Size([])
memory (bytes)
5700464640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5700612096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1617196000.0
relative error loss 0.23724738
shape of L is 
torch.Size([])
memory (bytes)
5703827456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5703827456
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1617093600.0
relative error loss 0.23723234
shape of L is 
torch.Size([])
memory (bytes)
5706936320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 12% |
memory (bytes)
5707042816
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1616849400.0
relative error loss 0.23719652
shape of L is 
torch.Size([])
memory (bytes)
5710258176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5710258176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1616768000.0
relative error loss 0.23718458
time to take a step is 259.6876323223114
it  13 : 2845774336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5713424384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5713469440
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1616768000.0
relative error loss 0.23718458
shape of L is 
torch.Size([])
memory (bytes)
5716615168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5716672512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1616540700.0
relative error loss 0.23715124
shape of L is 
torch.Size([])
memory (bytes)
5719887872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5719887872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1616329700.0
relative error loss 0.23712029
shape of L is 
torch.Size([])
memory (bytes)
5723045888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5723103232
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1616112100.0
relative error loss 0.23708835
shape of L is 
torch.Size([])
memory (bytes)
5726314496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 12% |
memory (bytes)
5726314496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1615838200.0
relative error loss 0.23704818
shape of L is 
torch.Size([])
memory (bytes)
5729480704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 12% |
memory (bytes)
5729538048
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1616073700.0
relative error loss 0.23708272
shape of L is 
torch.Size([])
memory (bytes)
5732757504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 12% |
memory (bytes)
5732757504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1615712800.0
relative error loss 0.23702978
shape of L is 
torch.Size([])
memory (bytes)
5735964672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5735964672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1615510500.0
relative error loss 0.23700011
shape of L is 
torch.Size([])
memory (bytes)
5739065344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 12% |
memory (bytes)
5739180032
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1615330300.0
relative error loss 0.23697366
shape of L is 
torch.Size([])
memory (bytes)
5742333952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 12% |
memory (bytes)
5742395392
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1615156700.0
relative error loss 0.2369482
time to take a step is 259.81749415397644
it  14 : 2845774336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5745504256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5745504256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1615156700.0
relative error loss 0.2369482
shape of L is 
torch.Size([])
memory (bytes)
5748826112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5748826112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1615007700.0
relative error loss 0.23692635
shape of L is 
torch.Size([])
memory (bytes)
5751914496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 12% |
memory (bytes)
5752045568
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1614896100.0
relative error loss 0.23690997
shape of L is 
torch.Size([])
memory (bytes)
5755265024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5755265024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 12% |
error is  1614729200.0
relative error loss 0.23688549
shape of L is 
torch.Size([])
memory (bytes)
5758484480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5758484480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1614460400.0
relative error loss 0.23684604
shape of L is 
torch.Size([])
memory (bytes)
5761581056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5761691648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1614522400.0
relative error loss 0.23685513
shape of L is 
torch.Size([])
memory (bytes)
5764907008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 12% |
memory (bytes)
5764907008
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1614283300.0
relative error loss 0.23682006
shape of L is 
torch.Size([])
memory (bytes)
5767946240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5768118272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1614081000.0
relative error loss 0.23679039
shape of L is 
torch.Size([])
memory (bytes)
5771325440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 12% |
memory (bytes)
5771325440
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1613922800.0
relative error loss 0.23676719
shape of L is 
torch.Size([])
memory (bytes)
5774528512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5774528512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1613759500.0
relative error loss 0.23674323
time to take a step is 260.336590051651
sum tnnu_Z after tensor(12493186., device='cuda:0')
shape of features
(4909,)
shape of features
(4909,)
number of orig particles 19636
number of new particles after remove low mass 16511
tnuZ shape should be parts x labs
torch.Size([19636, 702])
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
error without small mass is  1915014900.0
relative error without small mass is  0.28093827
nnu_Z shape should be number of particles by maxV
(19636, 702)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
shape of features
(19636,)
Thu Feb 2 18:23:28 EST 2023
