Wed Feb 1 08:59:36 EST 2023
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  0% |  0% |
numbers of X: 33355747
numbers of Z: 34068
shape of features
(34068,)
shape of features
(34068,)
ZX	Vol	Parts	Cubes	Eps
Z	0.025007523792316893	34068	34.068	0.09020723074476737
X	0.022245675565947077	714	0.714	0.3146654998125162
X	0.02384883290416985	12419	12.419	0.12429686294581763
X	0.023642325899899057	1761	1.761	0.23767141285641524
X	0.023265456406694668	4670	4.67	0.1707909674844374
X	0.023839827754877405	5297	5.297	0.16510404498566011
X	0.023111947819774457	4182	4.182	0.17680077861696297
X	0.023291930765341612	66135	66.135	0.07061951179364197
X	0.023263677076434817	30755	30.755	0.0911145098304816
X	0.02320961709070209	6017	6.017	0.15683006460546456
X	0.023103025989617394	5793	5.793	0.15858244619397938
X	0.02384480800023718	11412	11.412	0.12784311840865595
X	0.023322978646682053	137194	137.194	0.055396581496080774
X	0.023124518360944816	5286	5.286	0.1635492332453664
X	0.023325537480366806	161924	161.924	0.05242118233765628
X	0.023216071406793277	15406	15.406	0.11464794617893319
X	0.023258475494194245	26639	26.639	0.0955772305801391
X	0.023292047328293583	46618	46.618	0.07935080594432808
X	0.023274212751778052	36592	36.592	0.08599957088244119
X	0.024168255209709697	177547	177.547	0.0514411481437787
X	0.02329901311932594	128191	128.191	0.056644793360449676
X	0.02327065268559278	3764	3.764	0.18353531852697288
X	0.023289659410659913	206361	206.361	0.048325739061796
X	0.023188544691216822	11772	11.772	0.1253548322187092
X	0.023848767265017463	32342	32.342	0.09034407747126542
X	0.022990938676907977	2868	2.868	0.20013629330630459
X	0.023526916473916897	88431	88.431	0.06431616758593513
X	0.02327391304854487	70404	70.404	0.06914445498214093
X	0.023221881345408955	4383	4.383	0.17433127655802094
X	0.023199980970151638	84744	84.744	0.06493211609298166
X	0.023367631449475568	939561	939.561	0.029189718335740945
X	0.023156086860340538	4358	4.358	0.1744988816806908
X	0.02495039999846374	828641	828.641	0.031110264123317157
X	0.023213755231605886	28615	28.615	0.09326468266654575
X	0.023254232995276142	3963	3.963	0.18036792777008998
X	0.023149042260703977	3018	3.018	0.19721408582603747
X	0.023263828477117204	85734	85.734	0.06474049766239981
X	0.023614399441776752	64426	64.426	0.07156575307099061
X	0.023191404308994554	1213	1.213	0.26739631347920556
X	0.023743169566912817	5791	5.791	0.16005222270484923
X	0.02290486789019335	2191	2.191	0.21865620416285061
X	0.0231756386485001	3275	3.275	0.19198772784913168
X	0.02263340325047744	2323	2.323	0.21358309906545647
X	0.021508322462116043	1000	1.0	0.27810076252703686
X	0.02308556384350476	3592	3.592	0.18592368539632753
X	0.023166367253866307	865	0.865	0.29919013965139807
X	0.023156210616989824	1239	1.239	0.2653782921734292
X	0.023190765921507067	4308	4.308	0.17525878607185205
X	0.02322750949180043	3568	3.568	0.18672075732602803
X	0.02314346477146384	1024	1.024	0.2827327648155367
X	0.023218340606249645	11718	11.718	0.1256008450593056
X	0.023890078534840473	10445	10.445	0.13175579051232755
X	0.023163872832920172	3503	3.503	0.1876968618362489
X	0.023182677726004786	5240	5.24	0.164163815327031
X	0.022600969739253865	2476	2.476	0.2089899864522704
X	0.023233596461037235	6364	6.364	0.15397921113232385
X	0.02288724939582621	3072	3.072	0.195309842689752
X	0.023205198509272933	1419	1.419	0.2538249430984972
X	0.022813447220206978	2977	2.977	0.19715328897718967
X	0.023099406164381654	2387	2.387	0.21309968058618048
X	0.02276575730580784	2323	2.323	0.21399861567967113
X	0.023805870102011892	4830	4.83	0.17018148523786908
X	0.023223572039936004	1696	1.696	0.23924032880581847
X	0.023932661513467946	8812	8.812	0.1395207610867612
X	0.023136110359183074	5870	5.87	0.15796131796339313
X	0.023056432383744958	1689	1.689	0.23899426861431433
X	0.023123786972506045	3747	3.747	0.18342495543581533
X	0.02311374204015986	1579	1.579	0.24462229933078863
X	0.022941984376009902	2489	2.489	0.2096695489964673
X	0.02320390471916088	3788	3.788	0.1829716064276421
X	0.022351167592622862	1902	1.902	0.22735084225780958
X	0.022183458726087523	937	0.937	0.28714189114207406
X	0.023261680903375343	4190	4.19	0.1770689275783563
X	0.02316505726767054	2370	2.37	0.21381016273090675
X	0.023218227074768744	2564	2.564	0.2084348549839247
X	0.02257067783635004	2647	2.647	0.20429773584092276
X	0.023220401849412108	4490	4.49	0.17293164131482827
X	0.023179087265401025	1168	1.168	0.27073921327296624
X	0.02313016783529581	784	0.784	0.30899698399894
X	0.02313784238625426	2047	2.047	0.22442350384132842
X	0.02302502401470526	5352	5.352	0.16264023595301588
X	0.023150748103002727	3637	3.637	0.1853278024938343
X	0.023100344937681472	1179	1.179	0.2695886185440574
X	0.0234245887254914	6804	6.804	0.15099731443251385
X	0.022802542522086248	1493	1.493	0.24810822144110858
X	0.023299772428678762	2479	2.479	0.21103689291743924
X	0.023106672755173994	1056	1.056	0.27969916199501516
X	0.02313147417634202	3017	3.017	0.1971859651102317
X	0.023108747680782745	1071	1.071	0.27839556553547756
X	0.023254288202045812	3453	3.453	0.18884355062486746
X	0.023598261499324168	3804	3.804	0.18374400023400436
X	0.02309680740303843	1347	1.347	0.2578664169176465
X	0.02299525889893164	1105	1.105	0.2750586719939287
X	0.02313071228922905	1869	1.869	0.23130937497835674
X	0.022927000616564334	2685	2.685	0.20439376843281148
X	0.023113194179518393	4350	4.35	0.1744979129347357
X	0.023209578951224295	4321	4.321	0.17513018181117299
X	0.023847497967824066	2290	2.29	0.2183747626889021
X	0.023238673150298397	10338	10.338	0.13099606999695074
X	0.02320924738345495	3467	3.467	0.1884671827429086
X	0.023084227476144366	3178	3.178	0.19366619388663567
X	0.023375837495290243	9079	9.079	0.13705968240082686
X	0.023207583498480587	2312	2.312	0.21571517482482466
X	0.02324469681654912	7309	7.309	0.14705799530477925
X	0.022615889214642318	1045	1.045	0.2786757161548671
X	0.022961277659609074	4767	4.767	0.1688821047206479
X	0.022942662992251648	1575	1.575	0.24422367151209654
X	0.0232361288440753	4646	4.646	0.17101263162334793
X	0.023213044369553612	987	0.987	0.2865090312929192
X	0.023199711869175233	3660	3.66	0.18506906688219268
X	0.023140618106538956	1307	1.307	0.2606352322233969
X	0.023191615087157197	1921	1.921	0.22940411813246084
X	0.022877913091142472	893	0.893	0.29479604247654667
X	0.02267281462554967	1348	1.348	0.25621537163250346
X	0.023243271336399134	4927	4.927	0.16771486744156885
X	0.02331883154825257	5249	5.249	0.16439050797804283
X	0.023063244301163993	1540	1.54	0.24649033374064797
X	0.02350969701716458	1221	1.221	0.2680261220153147
X	0.02313951388428089	1080	1.08	0.27774323642730914
X	0.023847325196474672	6773	6.773	0.15213163501379742
X	0.02325640804014702	2455	2.455	0.2115909299924135
X	0.023226956477381686	5103	5.103	0.16572533768147032
X	0.022970670580448622	2685	2.685	0.2045234584210874
X	0.023039040406678972	1956	1.956	0.22752642619988753
X	0.023232645699066603	2642	2.642	0.20640582626014303
X	0.02257591068255638	1032	1.032	0.2796759961582477
X	0.02316808487563172	2320	2.32	0.21534463192746855
X	0.0230446950225673	1694	1.694	0.23871837693052045
X	0.023125760299370333	1963	1.963	0.22754042944452568
X	0.02357426116148361	14347	14.347	0.11800280495483843
X	0.022715502236440904	1714	1.714	0.23664854681899528
X	0.023207290699322484	4722	4.722	0.17001979435068704
X	0.02322799952335936	1341	1.341	0.2587384726782589
X	0.023182852875142327	3059	3.059	0.19642456529909127
X	0.02281809287731251	1032	1.032	0.280672512331351
X	0.022904290195758265	3229	3.229	0.1921393265536598
X	0.023303621052099373	1757	1.757	0.23671033278471104
X	0.022625407129323965	705	0.705	0.3177867327537116
X	0.023034482623622517	1047	1.047	0.28020589005799523
X	0.023064649909469378	2396	2.396	0.21272572758930902
X	0.02317869519080108	2531	2.531	0.20921795193311027
X	0.02320671227430001	5188	5.188	0.1647673727246763
X	0.022468072279321037	1461	1.461	0.24867866779553507
X	0.023744309928764556	5977	5.977	0.15837699511670097
X	0.023490111506778353	8150	8.15	0.14231231712326423
X	0.023593968089763308	5730	5.73	0.1602810289944923
X	0.02300422780642565	1287	1.287	0.26146267750848556
X	0.02305551562420544	1688	1.688	0.23903828579676242
X	0.024517729876248007	3766	3.766	0.1867239302709228
X	0.023117784098484417	2174	2.174	0.21990185246760138
X	0.022962187647986194	1789	1.789	0.23413570123149657
X	0.02318111776185041	2941	2.941	0.1990122376774818
X	0.023152148466196756	2879	2.879	0.20034716958398155
X	0.023249116105146923	4971	4.971	0.16723258171669153
X	0.023167766499819217	7291	7.291	0.14701636810640847
X	0.022693915937692075	1868	1.868	0.22988511760234367
X	0.02364858314725836	21519	21.519	0.10319556032644131
X	0.022981093707629848	1930	1.93	0.22835181168540453
X	0.023223688214824983	2508	2.508	0.2099912603000391
X	0.02316549625344227	2777	2.777	0.20280963158472914
X	0.022607238399463457	384	0.384	0.3890201496409396
X	0.023786759273861608	14872	14.872	0.11694687830663177
X	0.02304192404404494	1576	1.576	0.2445236345921922
X	0.023251975915530382	3873	3.873	0.18174848071128583
X	0.02255908674801995	865	0.865	0.2965526452321949
X	0.02285946657393295	3000	3.0	0.19678029730883578
X	0.023192375243146626	1351	1.351	0.2579664893148665
X	0.023234690788878114	2158	2.158	0.22081495620161196
X	0.023070327695583523	2366	2.366	0.21363857591479024
X	0.023219040376696178	2467	2.467	0.21113409567223004
X	0.022038394938561277	550	0.55	0.342194026500495
X	0.023145596599222603	1627	1.627	0.24230382812818163
X	0.022560284291756837	1223	1.223	0.26422426816015854
X	0.023202027117840647	4496	4.496	0.17280907378693025
X	0.023180433392962708	1938	1.938	0.22869460853693693
X	0.023280643372052677	5568	5.568	0.16110112894556897
X	0.023260771117194375	2224	2.224	0.21869042823102952
X	0.023194771645110315	2339	2.339	0.21484238247428306
X	0.023182309751977877	3401	3.401	0.18960510407535483
X	0.02322747577038208	5280	5.28	0.16385361638422677
X	0.022632575036233088	1958	1.958	0.22610340040268534
X	0.023279849428855706	1506	1.506	0.24910650298916345
X	0.023202455508172	1929	1.929	0.22912223673113355
X	0.02314353814216923	2387	2.387	0.2132353049606793
X	0.02319998975417175	3054	3.054	0.19658011590321048
X	0.023453250444867933	8977	8.977	0.13772853838990948
X	0.023247242384902403	3757	3.757	0.18358763034903866
X	0.023192621319971147	1964	1.964	0.22772084387989677
X	0.02290195093420458	3773	3.773	0.18241556452931132
X	0.023395794005075236	6385	6.385	0.15416731038895842
X	0.023859663710374603	11101	11.101	0.1290527921304511
X	0.022687495014928734	1175	1.175	0.2682766616312778
X	0.02240743520568369	360	0.36	0.39630530220147325
X	0.023193581015287696	2289	2.289	0.216391736281118
X	0.023127876511652518	1076	1.076	0.27804035189539844
X	0.02296463036971418	2075	2.075	0.2228505889470637
X	0.023067106995546073	2596	2.596	0.20712354978886266
X	0.02324632474269087	2672	2.672	0.20567078967564134
X	0.023226071901875898	1025	1.025	0.2829766712917552
X	0.023087820508971745	1466	1.466	0.2506588189545286
X	0.02305187614514514	796	0.796	0.30708910116069016
X	0.023526268158062483	2264	2.264	0.21821882189058744
X	0.02300052154031058	1196	1.196	0.2679181699622331
X	0.02368213240677235	3536	3.536	0.18849630854311678
X	0.023186423307514303	2784	2.784	0.20270051082021853
X	0.023148563522996216	2987	2.987	0.19789262431666355
X	0.02149466560355905	481	0.481	0.35486394171697516
X	0.023469139714111558	4342	4.342	0.17549676569807532
X	0.023089300151874322	3576	3.576	0.18621060870301082
X	0.023249541231501015	3328	3.328	0.19116586957734824
X	0.02310710078582775	4701	4.701	0.17002719981780884
X	0.02331020667686327	8315	8.315	0.14100287143029586
X	0.023195005072474933	2924	2.924	0.1994369855086423
X	0.023035731695098993	1181	1.181	0.2691849063624506
X	0.023133326243449267	1139	1.139	0.2728379552996955
X	0.02291603073942393	1086	1.086	0.27633539985311134
X	0.02266020640214556	1377	1.377	0.25435677331417866
X	0.023064287129362857	747	0.747	0.3137182699731141
X	0.02280500327123578	644	0.644	0.32838357293590154
X	0.023241013415288145	7688	7.688	0.1445929868129343
X	0.023085964699529556	1841	1.841	0.232326141453403
X	0.022395190599448472	1692	1.692	0.23654737596347247
X	0.02325606994616184	1391	1.391	0.2557033385300202
X	0.023036347000328104	3886	3.886	0.18098264812864784
X	0.022908108957637417	1793	1.793	0.23377764722159644
X	0.022963546573022776	6279	6.279	0.15406931793566858
X	0.023081009777535143	5372	5.372	0.1625696985110043
X	0.023120216538776057	1190	1.19	0.26883242232804316
X	0.02307052131732931	1236	1.236	0.26526481335059
X	0.023502337698191475	5426	5.426	0.16300856500724606
X	0.022040428719255822	469	0.469	0.36086857534161115
X	0.023232949656656458	2149	2.149	0.221117260595732
X	0.023104112961712515	2592	2.592	0.207340798796762
X	0.023214008672896566	2779	2.779	0.2029024067641106
X	0.02403692895075344	7051	7.051	0.15050216765177746
X	0.023166307710405995	2328	2.328	0.2150921768507121
X	0.023123118418945255	1433	1.433	0.25269699123329015
X	0.02311800373685995	1856	1.856	0.23180570446716198
X	0.02353243484719906	3876	3.876	0.1824292049924749
X	0.022337913853970368	946	0.946	0.2868911546808268
X	0.023273246594257543	8967	8.967	0.13742632527736073
X	0.023381951459740216	55641	55.641	0.07490232109795747
X	0.023263210411687076	3920	3.92	0.18104833168936688
X	0.02293798119577312	3114	3.114	0.1945713361984762
X	0.023053003045528216	415	0.415	0.38155733224507965
X	0.023089135473133297	5143	5.143	0.16496698958119202
X	0.023432537505100416	18374	18.374	0.10844391874611632
X	0.023120763021544335	128705	128.705	0.056424655279827075
X	0.02301901448474316	2798	2.798	0.20187366701740245
X	0.023169659026055965	49966	49.966	0.07740131746776972
X	0.023193864694184118	56092	56.092	0.07450019302490467
X	0.023224435086468546	7635	7.635	0.1448923220590408
X	0.023595701893520633	108836	108.836	0.06007403126520641
X	0.02271910223727194	1308	1.308	0.25897696717850105
X	0.023229662192113254	2869	2.869	0.2008032705438025
X	0.02326873375743336	104361	104.361	0.0606379959239785
X	0.02327244006171347	117239	117.239	0.05833420340891805
X	0.023210454387682555	2975	2.975	0.19833478551325787
X	0.02323177456187708	18140	18.14	0.10859630134960102
X	0.023232798529390846	20484	20.484	0.10428669871673842
X	0.023254992318231377	48361	48.361	0.0783442226873365
X	0.023225159867785738	36176	36.176	0.0862672702094803
X	0.023232657770842248	38392	38.392	0.08458357805371489
X	0.0233741373795155	31865	31.865	0.09018629722361132
X	0.023248812361624183	17314	17.314	0.1103234377800846
X	0.022793532220908047	4644	4.644	0.1699442493800573
X	0.023317466994351672	131934	131.934	0.056118779044981616
X	0.023111273882947144	2612	2.612	0.20683160586667648
X	0.022876573050699137	1544	1.544	0.2456110435363285
X	0.023110495374947974	15899	15.899	0.11327823713304484
X	0.02257645845964466	24727	24.727	0.09701259661457326
X	0.023863269631843577	197870	197.87	0.049406413808957085
X	0.023278004428917306	63406	63.406	0.07160419446653175
X	0.02297586020476213	1085	1.085	0.27666062092500104
X	0.023228518253116785	5727	5.727	0.1594770185940638
X	0.023501606771782434	10948	10.948	0.12899936987011903
X	0.023283091314701337	9046	9.046	0.13704441857616254
X	0.02392750649793155	74839	74.839	0.06837906462516724
X	0.023247119045065667	17500	17.5	0.10992851560976617
X	0.023262041715151404	19418	19.418	0.10620568828743474
X	0.021403149840306755	706	0.706	0.3118106876039851
X	0.02355052566488539	4485	4.485	0.17381185155954337
X	0.023100658195685377	28935	28.935	0.09276844761169775
X	0.023185129276742877	39736	39.736	0.08356192924999445
X	0.023255377805446665	19937	19.937	0.1052659377112158
X	0.023870186660100184	4299	4.299	0.1770772586481142
X	0.023890746629009432	264648	264.648	0.0448594249462908
X	0.02328114906620943	7948	7.948	0.14308107168568765
X	0.02326995684662219	21874	21.874	0.10208355602248848
X	0.023303164142926654	100370	100.37	0.06146157096858776
X	0.022936857848547352	1814	1.814	0.23296939942406747
X	0.023258613993925554	29990	29.99	0.09187607954935302
X	0.024626947303490368	222311	222.311	0.04802675121263652
X	0.02319989345151308	179074	179.074	0.05060007399253569
X	0.02310041982977461	7269	7.269	0.1470217997041856
X	0.022918537362658923	2537	2.537	0.20826780676960205
X	0.022806447328523546	2797	2.797	0.2012743304758942
X	0.02313466584200439	2385	2.385	0.2132676331583567
X	0.02328770551968566	20265	20.265	0.10474340733632659
X	0.023438506490664094	11697	11.697	0.1260719533006975
X	0.0232597482682568	21072	21.072	0.10334744656636898
X	0.02339198117230384	53243	53.243	0.07602121900114112
X	0.02329501593872304	26609	26.609	0.09566318133022222
X	0.023089207645120232	6656	6.656	0.15137886292084204
X	0.023517185530694824	5370	5.37	0.16360768010944568
X	0.023652700111723778	141267	141.267	0.055116293104955846
X	0.02328407587017553	23271	23.271	0.10001872634301476
X	0.02324529262573565	14143	14.143	0.1180133554589444
X	0.023869410558937283	7975	7.975	0.14411316877288305
X	0.023256176894833856	303365	303.365	0.04248063006533555
X	0.022842396654066843	3774	3.774	0.18224121029137177
X	0.02395913210040651	119601	119.601	0.05851206476880801
X	0.023232997750821948	3814	3.814	0.18263115003433214
X	0.023091665090208605	14404	14.404	0.11703723304533488
X	0.02336901274571861	10926	10.926	0.12884267207253475
X	0.023231047847471303	112291	112.291	0.0591436264164908
X	0.023264667617657005	42866	42.866	0.081569530797358
X	0.02322324690031895	3161	3.161	0.1944014416239709
X	0.023273687313592008	105981	105.981	0.06033172285694358
X	0.023420674608447146	86882	86.882	0.06459861352609889
X	0.023204183045861474	2599	2.599	0.20745313446101488
X	0.023323694054988027	36300	36.3	0.08629061620572716
X	0.023880936861150618	99280	99.28	0.06219133277742544
X	0.024186395722583548	297064	297.064	0.04334180992635034
X	0.023248055803233917	26475	26.475	0.09575987199763837
X	0.022993062844116847	1257	1.257	0.26348375830548565
X	0.02304441513982633	4440	4.44	0.17313873498370944
X	0.023168924587006236	2291	2.291	0.2162520645051803
X	0.02324846733447232	16876	16.876	0.11126919262617214
X	0.0232380278617861	10010	10.01	0.13241028800606858
X	0.023031753399376104	1036	1.036	0.2811830142240019
X	0.023280101804166214	67786	67.786	0.07002959627583177
X	0.023717637316669194	143395	143.395	0.054892431903543024
X	0.02275969443814825	3719	3.719	0.1829139168179582
X	0.023182604828545988	4265	4.265	0.1758251779351728
X	0.023170911205957812	12378	12.378	0.12324355339339728
X	0.024332405212874143	7711	7.711	0.1466756806955085
X	0.023541328723585407	7609	7.609	0.14571394934257118
X	0.023596188529027864	5748	5.748	0.16011856871269833
X	0.023242368628951855	2933	2.933	0.1993683007465407
X	0.023184694463832776	56812	56.812	0.07417435289984005
X	0.023308629515324305	14376	14.376	0.11747881529691208
X	0.02376854700582188	15750	15.75	0.11470280148856546
X	0.023178589702932993	39269	39.269	0.08388398526907913
X	0.02307827187324044	9328	9.328	0.13525030379453795
X	0.023731830053421325	224346	224.346	0.047293809714730344
X	0.023330798134636187	3226	3.226	0.19338453669648395
X	0.023189927886913535	49378	49.378	0.07772999401396775
X	0.023211869996269105	1351	1.351	0.258038748474165
X	0.02318304687905	3006	3.006	0.19757281107484964
X	0.023240467575769454	2959	2.959	0.198777228969611
X	0.023302843747339673	10958	10.958	0.12859553087631342
X	0.023610172813197997	4159	4.159	0.17838982597922828
X	0.023206663030856462	47525	47.525	0.07874630601645659
X	0.02301935091104261	3376	3.376	0.1896255609356344
X	0.02324641523650654	4586	4.586	0.1717805465638399
X	0.024346669331088963	360617	360.617	0.04071906406135879
X	0.0239162073850945	42886	42.886	0.08231119808674374
X	0.023265621922463144	38697	38.697	0.084400649673005
X	0.024294069124863368	113582	113.582	0.05980400242210615
X	0.024056829820879137	158098	158.098	0.053387224094460055
X	0.0232765466126747	5012	5.012	0.16684088889384535
X	0.02318902968842374	2343	2.343	0.21470233253968932
X	0.02328218391204389	23980	23.98	0.09902043858310058
X	0.022389993422589	782	0.782	0.3059255416540466
X	0.023205791075119893	4359	4.359	0.17461029107140788
X	0.023582545053566176	8317	8.317	0.14153852228958283
X	0.022888670382270698	2501	2.501	0.20917139416965153
X	0.023145168769349894	2455	2.455	0.2112530324655329
X	0.023052597676919213	1317	1.317	0.2596438450614922
X	0.023199435556110783	3882	3.882	0.1814710265033815
X	0.02357494757927747	255191	255.191	0.045205900763046626
X	0.023140651326943883	10240	10.24	0.13122760639667014
X	0.02322427383102094	40880	40.88	0.0828216259170304
X	0.023198038224018173	12847	12.847	0.12177267467273653
X	0.02319784732181346	2769	2.769	0.20309921431125277
X	0.023196359466840233	6953	6.953	0.1494224547930711
X	0.023937898552497762	385801	385.801	0.03958897197507009
X	0.02331157340345761	392133	392.133	0.039028278766451624
X	0.02325798937531813	9837	9.837	0.1332201235629804
X	0.0233582585057733	13925	13.925	0.11881786713212279
X	0.02325252617777325	1879	1.879	0.23130292278380804
X	0.02325990431127263	11265	11.265	0.12733825942353805
X	0.02327594355494523	193348	193.348	0.049376754094417025
X	0.02328541210742421	22355	22.355	0.10136851510192132
X	0.02318905949039422	6424	6.424	0.15340017938999526
X	0.023143612685804976	37345	37.345	0.08525755940222879
X	0.023538768174852013	252981	252.981	0.045313952205347174
X	0.02309023185294267	21516	21.516	0.10238168024198725
X	0.022861414077739995	2869	2.869	0.1997365360818613
X	0.024898629843604206	15473	15.473	0.11718363739014058
X	0.023107106781843034	6722	6.722	0.15092077303831514
X	0.023261382027252524	2399	2.399	0.2132398763240599
X	0.023386089419826957	105926	105.926	0.06043914951443502
X	0.02299204470960054	12577	12.577	0.12227382875497915
X	0.022808255948485283	963	0.963	0.28718070914117505
X	0.023280011632577424	26256	26.256	0.09606935533710506
X	0.023882878851201248	13447	13.447	0.12110268076432497
X	0.023050617580089732	810	0.81	0.30530401494565484
X	0.023254424374481275	87101	87.101	0.06439134314801101
X	0.023265187970504576	130132	130.132	0.05633446234139028
X	0.02439269426797304	99851	99.851	0.06251282392214084
X	0.023303688015808836	127482	127.482	0.05675340591002822
X	0.0233387293557168	148819	148.819	0.05392700179546252
X	0.0231665966244306	9786	9.786	0.13327612029050498
X	0.023217252658500137	10023	10.023	0.13231356338271763
X	0.02339011729988361	81113	81.113	0.0660664004100985
X	0.02366688307302155	162174	162.174	0.05264857393412333
X	0.023177726247694676	2982	2.982	0.19808628023303437
X	0.02386210017218013	30488	30.488	0.09215662814152552
X	0.0246191902655468	231026	231.026	0.047410110620959275
X	0.02328529815836265	94259	94.259	0.06274604044379252
X	0.02374157945884682	86777	86.777	0.06491847740456
X	0.023221329145578097	72245	72.245	0.06850039770661313
X	0.02342267851217442	4638	4.638	0.17156759801143495
X	0.022492751565226853	2173	2.173	0.21793531976661443
X	0.023124128989127524	18699	18.699	0.10733688820760966
X	0.02357542804936738	57340	57.34	0.07435907284125651
X	0.02326673187811845	44647	44.647	0.08047253842743782
X	0.024077296694497922	559671	559.671	0.03503953279490754
X	0.023268864843368507	119561	119.561	0.05795112697177594
X	0.023271381129954766	71139	71.139	0.06890300048140635
X	0.02324515218714182	17190	17.19	0.11058227126475008
X	0.023305990793696215	99913	99.913	0.0615576252019013
X	0.023004646380791867	1687	1.687	0.2389095405415813
X	0.023068860593773596	4160	4.16	0.17700176811192556
X	0.02327706333004052	187753	187.753	0.0498632330896386
X	0.02320879866064637	2864	2.864	0.20085988781024966
X	0.022989260086980424	4219	4.219	0.17596994140574487
X	0.023259335224250784	14415	14.415	0.11728997230951488
X	0.024363170513017705	187822	187.822	0.050620812855663846
X	0.023232360588908266	29984	29.984	0.09184762346292898
X	0.02385751189733763	54828	54.828	0.07577761197784577
X	0.02305794222485781	1721	1.721	0.23750890210021322
X	0.023131571347387213	3551	3.551	0.18676041106540583
X	0.02328424062732086	3273	3.273	0.1923263064178863
X	0.02374193361632645	8525	8.525	0.14069345657116783
X	0.023266479622221652	27712	27.712	0.09433820359244245
X	0.023106678432750154	20763	20.763	0.1036292779513203
X	0.023217976427521356	3365	3.365	0.19037639519174948
X	0.023946300440218782	90703	90.703	0.06415131923518774
X	0.02323574957169369	1235	1.235	0.2659683198690495
X	0.023268112798688973	33725	33.725	0.08836285389898281
X	0.02311756546144037	6817	6.817	0.15023908641100533
X	0.02325766019087183	19799	19.799	0.10551339288590611
X	0.022939046682669276	3597	3.597	0.1854435115451791
X	0.02316863681525765	8846	8.846	0.13784293623146085
X	0.02350026952266268	4957	4.957	0.16799046855798566
X	0.023634690155714472	12331	12.331	0.12421781102639957
X	0.024233396927308002	80210	80.21	0.06710093830513622
X	0.02326302491211716	151686	151.686	0.053527081204829505
X	0.024257856159616828	5445	5.445	0.1645450584004417
X	0.023024649409906412	3574	3.574	0.18607134316707216
X	0.023262604937180152	100961	100.961	0.061305800448560245
X	0.023154105399067815	5297	5.297	0.16350561731623325
X	0.024270829483909632	449117	449.117	0.03780724586719477
X	0.023089941181511198	713	0.713	0.31874577160951306
X	0.02306553543938532	45105	45.105	0.07996739544436075
X	0.02391938180811698	44244	44.244	0.08146389841234737
X	0.023258757745266583	2724	2.724	0.20439007546352905
X	0.023158830873877694	60155	60.155	0.0727469796367379
X	0.02319271133814216	16240	16.24	0.11261299249871962
X	0.024128762345928874	475172	475.172	0.03703065564247911
X	0.023191500022461904	7329	7.329	0.14681193834344544
X	0.023129410494941265	10628	10.628	0.1295898556386893
X	0.023160270555361608	4353	4.353	0.17457618035472622
X	0.023807502073941132	3408	3.408	0.19116340089831826
X	0.02271614807050389	6245	6.245	0.15379211966939163
X	0.023270384833800912	14364	14.364	0.11744721502841597
X	0.022779406916144874	4397	4.397	0.17303288504277242
X	0.02326166875109586	28013	28.013	0.09399261849842692
X	0.02327642573824889	15248	15.248	0.11514218579263578
X	0.02264921679068801	1652	1.652	0.23933950137416224
X	0.023104894836254237	13761	13.761	0.11885514889226437
X	0.02496626984788607	221128	221.128	0.04833220855109308
X	0.023919313091291715	35588	35.588	0.08759548210286992
X	0.023966047382801905	117398	117.398	0.058881460233692895
X	0.023276791922141552	13911	13.911	0.1187193721693535
X	0.023265015836876377	11347	11.347	0.12704008213867551
X	0.023893384816768056	1811	1.811	0.23629420766487083
X	0.022844812383753074	4524	4.524	0.17156222361341425
X	0.02333494018952984	156914	156.914	0.05298037153488555
X	0.023255622161015444	7791	7.791	0.1439831219843578
X	0.02325859883024164	12374	12.374	0.12341211980214664
X	0.023494360077110372	20535	20.535	0.10458987351188712
X	0.023194794211123467	30069	30.069	0.09171151046338888
X	0.02307535015325105	1872	1.872	0.23100115524847017
X	0.023100769864236915	2061	2.061	0.22379454030528304
X	0.024516631564450022	312975	312.975	0.042787513661457686
X	0.02326654692943317	8295	8.295	0.141027946053894
X	0.023224015859354144	11110	11.11	0.1278618803189586
X	0.02333421742327991	9195	9.195	0.13639983204225847
X	0.02487404035376203	80538	80.538	0.0675950921847462
X	0.02329245513186907	136438	136.438	0.05547448864440075
X	0.02366690591478144	123428	123.428	0.057664597388595695
X	0.02328726774535385	68195	68.195	0.06989648468652129
X	0.02324271307183901	4675	4.675	0.1706744067321042
X	0.023272047395177558	43182	43.182	0.0813786747715595
X	0.023267080270953883	24079	24.079	0.09886315831833065
X	0.02326468674160137	5316	5.316	0.16357016017411358
X	0.023158782384329293	14373	14.373	0.11723467903136002
X	0.02330139944281645	3255	3.255	0.192727492333196
X	0.023267501394005987	20441	20.441	0.10441170879605675
X	0.023223595877482846	5658	5.658	0.16011137020274788
X	0.02302276267441911	619	0.619	0.33380199597334775
X	0.023151965297450523	5759	5.759	0.15900603012395967
X	0.02327893289191701	4299	4.299	0.1756029770705857
X	0.02429684938518863	59856	59.856	0.07404244593335584
X	0.023184487955025958	5205	5.205	0.16453523845084309
X	0.0232676441244703	23013	23.013	0.10036748884561238
X	0.022694740467999536	6098	6.098	0.15496939356209025
X	0.02332369351513461	16548	16.548	0.11212021913603087
X	0.02319366814017949	2621	2.621	0.20683981435898666
X	0.02318175803333253	6508	6.508	0.15272129853639524
X	0.023282328589774023	11445	11.445	0.12670785504264998
X	0.023275328156481492	106511	106.511	0.060232901404338256
X	0.02345011134310665	7616	7.616	0.14548090406919933
X	0.023229386964513928	1725	1.725	0.23791192798407493
X	0.02290610481969753	4222	4.222	0.17571587720385856
X	0.0232831467515279	188447	188.447	0.04980628497555288
X	0.02437960210086822	444108	444.108	0.03800546106697175
X	0.023165152576263656	3841	3.841	0.18202468407190311
X	0.02365293264527769	6868	6.868	0.1510143564361026
X	0.023229853793017428	10108	10.108	0.13196550206520002
X	0.023881765802384713	103933	103.933	0.061249750499399444
X	0.022922879290697556	3510	3.51	0.18691924449891079
X	0.02303338049459045	7666	7.666	0.14429887847069736
X	0.023251250649505808	6424	6.424	0.1535371925510566
X	0.023256611862013162	91972	91.972	0.0632358855456682
X	0.02317997212133901	54134	54.134	0.07537273310019357
X	0.023291847473160168	169932	169.932	0.05155960594924095
X	0.02321425546300716	5363	5.363	0.16297299776192983
X	0.023261928159050902	4013	4.013	0.17963550675642215
X	0.022999963185869346	1293	1.293	0.2610414893795286
X	0.023220950263770287	28235	28.235	0.09369089739048385
X	0.023717651980685817	9498	9.498	0.13566875584693683
X	0.023213702062580895	29347	29.347	0.09248264272379168
X	0.02431869746252983	132755	132.755	0.05679321761339749
X	0.02459275053622667	451190	451.19	0.03791542073935403
X	0.023176972202185456	5838	5.838	0.1583425141715883
X	0.023196141611400812	1672	1.672	0.24028490723598553
X	0.023223916121463444	6057	6.057	0.15651619937203012
X	0.023287985169155048	308944	308.944	0.04224260739612053
X	0.023248381209263192	21402	21.402	0.1027967662197827
X	0.02286321414393503	981	0.981	0.2856424562330685
X	0.02311174065699162	2956	2.956	0.19847664430635
X	0.02325010881548396	14750	14.75	0.11637981339456278
X	0.023030049647358553	910	0.91	0.29359669377077724
X	0.02325687616935609	44698	44.698	0.0804305607292148
X	0.02422924127455016	3452	3.452	0.19146512335011265
X	0.02326099947282283	14710	14.71	0.1165033909798171
X	0.02319240675748159	3832	3.832	0.1822384891712077
X	0.023097291191360603	2807	2.807	0.20188600416042762
X	0.022835637860024703	1591	1.591	0.24302315562308988
X	0.02328783537800945	144724	144.724	0.054391310358468496
X	0.023466395507085156	365106	365.106	0.04005675879913557
X	0.02331556048603156	94098	94.098	0.06280899182122074
X	0.0230799483529275	65710	65.71	0.07055608181449803
X	0.023296303955257777	24761	24.761	0.0979880122850811
X	0.023042651278207885	4468	4.468	0.17277189404432228
X	0.023291987019350684	9837	9.837	0.13328500399610727
X	0.02327821286012751	73637	73.637	0.06812154921434022
X	0.024875566516391112	60909	60.909	0.07419315456713961
X	0.02400608791355523	76130	76.13	0.06806468737814014
X	0.023207122642061095	10443	10.443	0.13049644271272712
X	0.02358724979464641	558166	558.166	0.03483143036685416
X	0.02315663627380965	11292	11.292	0.12704810789324694
X	0.024485381700249442	44022	44.022	0.08223923018410231
X	0.02384272293394179	93789	93.789	0.06334825392953033
X	0.023278361363090562	35159	35.159	0.08715760687886534
X	0.023285519431762436	59407	59.407	0.0731839932586616
X	0.023284243038457154	128420	128.42	0.05659913831549542
X	0.02321859297221914	20156	20.156	0.10482797172246736
X	0.023247831448052796	13534	13.534	0.11976192242099247
X	0.023112304414599275	3702	3.702	0.18413468853314086
X	0.023254329699202998	37768	37.768	0.0850732999881283
X	0.02329572523294075	9073	9.073	0.13693310040811232
X	0.02327411786241204	101794	101.794	0.061148200791229845
X	0.024280337416406134	936371	936.371	0.029598446746117245
X	0.023284354566103976	104540	104.54	0.06061692516829918
X	0.024179680422649216	178269	178.269	0.051379701413544525
X	0.02326504267697108	29313	29.313	0.09258654153903259
X	0.023238393306436633	20437	20.437	0.1043749589348365
X	0.02480931737050931	27590	27.59	0.09652082783955342
X	0.022999410196852872	7324	7.324	0.14643878642654082
X	0.02352400096286171	65900	65.9	0.07093739858208584
X	0.023368870599812235	163763	163.763	0.05225653941330669
X	0.022977139330771668	14164	14.164	0.11749967893022845
X	0.023293055012899463	524584	524.584	0.03541091406285073
X	0.0234226515584767	186144	186.144	0.05011053104820763
X	0.023256614220293764	19667	19.667	0.10574734127072084
X	0.023157377528467523	1451	1.451	0.25177193700438727
X	0.02437197682056987	24515	24.515	0.09980515028615948
X	0.02326741717305489	51081	51.081	0.07694190390864551
X	0.023210381942266337	3202	3.202	0.19353238700339087
X	0.023331441077157684	375491	375.491	0.03960779607625564
X	0.023386156013379383	27036	27.036	0.09528081609291404
X	0.023240023225097436	22326	22.326	0.10134645119119745
X	0.023271714441993288	6200	6.2	0.1554099771440722
X	0.02318337320251093	2465	2.465	0.21108298851230986
X	0.023229697455782745	2450	2.45	0.2116536795916822
X	0.023215760156761717	6902	6.902	0.14983133570374263
X	0.023238208434804355	8805	8.805	0.13819460667993805
X	0.02320559914199922	15196	15.196	0.11515633479530543
X	0.024898972563255916	2748076	2748.076	0.020847351828237302
X	0.023204746324872978	9098	9.098	0.13662923316545522
X	0.023515024155254083	11265	11.265	0.12780212579490036
X	0.023157599219251812	3172	3.172	0.19399332416612544
X	0.02329889315693831	8935	8.935	0.1376407121518577
X	0.02339793478989135	5747	5.747	0.159678131881381
X	0.023293593248278767	171077	171.077	0.05144560598807739
X	0.023214090491720858	47275	47.275	0.07889328597342475
X	0.02418795989393232	689118	689.118	0.03274174953481078
X	0.023239856856479467	3127	3.127	0.19514998511745124
X	0.023593107233675085	160901	160.901	0.05273214846293393
X	0.02325932408120942	27003	27.003	0.09514698357668726
X	0.023279748491231645	122101	122.101	0.05755543682368546
X	0.023766815207358988	358093	358.093	0.04048788144752563
X	0.023166557140502732	12647	12.647	0.12235582773224793
X	0.023038009613207417	2969	2.969	0.19797556262420832
X	0.02325214360599097	53638	53.638	0.07568273304368446
X	0.023202725580033806	9123	9.123	0.13650035355346266
X	0.02305743269040708	29024	29.024	0.09261568887987624
X	0.023207503509654403	18328	18.328	0.10818601306409059
X	0.02327720133024379	14998	14.998	0.1157797136762585
X	0.0249860290836192	45275	45.275	0.08202500409192295
X	0.0231989421428268	10752	10.752	0.1292189800332704
X	0.023843698517252668	86452	86.452	0.06509278586606473
X	0.02334119285615914	144074	144.074	0.05451455580226997
X	0.023115772275780048	8985	8.985	0.13702404228863768
X	0.023248321141027584	8363	8.363	0.1406079359632206
X	0.023398943123533956	126410	126.41	0.05699082407373648
X	0.02323201805364577	20491	20.491	0.10427365444773655
X	0.02368446127483614	213160	213.16	0.04807499706442508
X	0.023023594656705507	9123	9.123	0.13614817324145345
X	0.02325282754536484	46411	46.411	0.07942397428277263
X	0.023898734457688725	27050	27.05	0.09595535388062926
X	0.022799173733518297	56548	56.548	0.07387553168832717
X	0.023266139211601155	58692	58.692	0.07345958510641927
X	0.023264329912817336	9980	9.98	0.13259281844126378
X	0.02322466695551497	3954	3.954	0.18042814188964854
X	0.024668091308134826	15453	15.453	0.11687122088809092
X	0.023114771789701675	27447	27.447	0.09443469367614776
X	0.02435703359758951	76906	76.906	0.06816394602055481
X	0.02327541232507732	116041	116.041	0.05853675454667089
X	0.023626973769678504	7372	7.372	0.14743737528510736
X	0.0232646763448141	24831	24.831	0.09785152573438687
X	0.023272242034620118	25190	25.19	0.09739500536006857
X	0.023306615912922136	173237	173.237	0.051240440541664496
X	0.023160211946911227	29824	29.824	0.09191623205960651
X	0.023301736401449235	67650	67.65	0.07009819384274715
X	0.023069804093474484	13591	13.591	0.11928820348341458
X	0.023288482291493474	17656	17.656	0.10966876566356995
X	0.02367855849377173	2604	2.604	0.2087235374775522
X	0.024496999429393346	11162	11.162	0.12995419139329364
X	0.023788750554440158	37316	37.316	0.0860647905137725
X	0.02326650150120841	40292	40.292	0.08327297616222526
X	0.02327136104457388	21618	21.618	0.10248699446612237
X	0.023247265375206067	1563	1.563	0.24592591641445982
X	0.023241819804161786	5853	5.853	0.1583545458950002
X	0.023154470226430218	31368	31.368	0.09037521424210032
X	0.023268578703894847	17378	17.378	0.11021905619591874
X	0.023260142069749593	5886	5.886	0.1580995747367831
X	0.023094814528580945	2629	2.629	0.20633582126156289
X	0.02294887606578803	3205	3.205	0.19274263660237526
X	0.023257034051903696	20141	20.141	0.10491182287561032
X	0.023241481848724346	22513	22.513	0.10106717992930783
X	0.02356054936105967	100543	100.543	0.06165162446782275
X	0.023172183749575234	3004	3.004	0.19758577668000282
X	0.023189174265338158	20155	20.155	0.10478541247137312
X	0.0231463140398952	2042	2.042	0.22463393651103544
X	0.023182475966265093	2760	2.76	0.2032748169798344
X	0.023351593598454425	198073	198.073	0.049033972759062634
X	0.02325077555810708	10688	10.688	0.12957274704540173
X	0.023256123146502355	65045	65.045	0.07097539205121314
X	0.023285102416275877	137487	137.487	0.055327218720374105
X	0.023268893356139683	20095	20.095	0.10500965691246003
X	0.023593806874802913	135187	135.187	0.055884027355112664
X	0.023703282979986525	153977	153.977	0.05359416995982335
X	0.023133625936885924	2224	2.224	0.21829124039645387
X	0.023288925370990303	30843	30.843	0.09106069174378936
X	0.02327031036961388	12095	12.095	0.12437471569767444
X	0.02329851959054853	96902	96.902	0.06218207285087586
X	0.023283572757513898	175414	175.414	0.05101075950885625
X	0.02311832816743896	9395	9.395	0.13500604452974607
X	0.023270330572729925	32627	32.627	0.0893460046515028
X	0.02318627506409932	2925	2.925	0.19938923377042522
X	0.023233612460085668	10757	10.757	0.12926328566293352
X	0.02323884337605054	5927	5.927	0.1576860207945047
X	0.023219898937021314	5609	5.609	0.1605677401324619
X	0.02437347553774305	226779	226.779	0.04754500258173748
X	0.02485384970113632	477224	477.224	0.03734425475428786
X	0.02253244964266449	15425	15.425	0.11346478089091336
X	0.023273313165809623	12424	12.424	0.1232723258755888
X	0.02347725498774008	21600	21.6	0.10281690389611448
X	0.023608623725111744	64835	64.835	0.07140912612895219
X	0.023203615941541147	6636	6.636	0.15178065770390048
X	0.023985335686532414	7440	7.44	0.14772623242391483
X	0.024179365399703816	17658	17.658	0.11104554211542247
X	0.02365845546043415	125231	125.231	0.05737968766714225
X	0.022873724622497888	3020	3.02	0.19638575459132226
X	0.02325621957220934	29487	29.487	0.09239238465994946
X	0.023232181627252543	2342	2.342	0.2148660010994582
time for making epsilon is 1.7639288902282715
epsilons are
[0.3146654998125162, 0.12429686294581763, 0.23767141285641524, 0.1707909674844374, 0.16510404498566011, 0.17680077861696297, 0.07061951179364197, 0.0911145098304816, 0.15683006460546456, 0.15858244619397938, 0.12784311840865595, 0.055396581496080774, 0.1635492332453664, 0.05242118233765628, 0.11464794617893319, 0.0955772305801391, 0.07935080594432808, 0.08599957088244119, 0.0514411481437787, 0.056644793360449676, 0.18353531852697288, 0.048325739061796, 0.1253548322187092, 0.09034407747126542, 0.20013629330630459, 0.06431616758593513, 0.06914445498214093, 0.17433127655802094, 0.06493211609298166, 0.029189718335740945, 0.1744988816806908, 0.031110264123317157, 0.09326468266654575, 0.18036792777008998, 0.19721408582603747, 0.06474049766239981, 0.07156575307099061, 0.26739631347920556, 0.16005222270484923, 0.21865620416285061, 0.19198772784913168, 0.21358309906545647, 0.27810076252703686, 0.18592368539632753, 0.29919013965139807, 0.2653782921734292, 0.17525878607185205, 0.18672075732602803, 0.2827327648155367, 0.1256008450593056, 0.13175579051232755, 0.1876968618362489, 0.164163815327031, 0.2089899864522704, 0.15397921113232385, 0.195309842689752, 0.2538249430984972, 0.19715328897718967, 0.21309968058618048, 0.21399861567967113, 0.17018148523786908, 0.23924032880581847, 0.1395207610867612, 0.15796131796339313, 0.23899426861431433, 0.18342495543581533, 0.24462229933078863, 0.2096695489964673, 0.1829716064276421, 0.22735084225780958, 0.28714189114207406, 0.1770689275783563, 0.21381016273090675, 0.2084348549839247, 0.20429773584092276, 0.17293164131482827, 0.27073921327296624, 0.30899698399894, 0.22442350384132842, 0.16264023595301588, 0.1853278024938343, 0.2695886185440574, 0.15099731443251385, 0.24810822144110858, 0.21103689291743924, 0.27969916199501516, 0.1971859651102317, 0.27839556553547756, 0.18884355062486746, 0.18374400023400436, 0.2578664169176465, 0.2750586719939287, 0.23130937497835674, 0.20439376843281148, 0.1744979129347357, 0.17513018181117299, 0.2183747626889021, 0.13099606999695074, 0.1884671827429086, 0.19366619388663567, 0.13705968240082686, 0.21571517482482466, 0.14705799530477925, 0.2786757161548671, 0.1688821047206479, 0.24422367151209654, 0.17101263162334793, 0.2865090312929192, 0.18506906688219268, 0.2606352322233969, 0.22940411813246084, 0.29479604247654667, 0.25621537163250346, 0.16771486744156885, 0.16439050797804283, 0.24649033374064797, 0.2680261220153147, 0.27774323642730914, 0.15213163501379742, 0.2115909299924135, 0.16572533768147032, 0.2045234584210874, 0.22752642619988753, 0.20640582626014303, 0.2796759961582477, 0.21534463192746855, 0.23871837693052045, 0.22754042944452568, 0.11800280495483843, 0.23664854681899528, 0.17001979435068704, 0.2587384726782589, 0.19642456529909127, 0.280672512331351, 0.1921393265536598, 0.23671033278471104, 0.3177867327537116, 0.28020589005799523, 0.21272572758930902, 0.20921795193311027, 0.1647673727246763, 0.24867866779553507, 0.15837699511670097, 0.14231231712326423, 0.1602810289944923, 0.26146267750848556, 0.23903828579676242, 0.1867239302709228, 0.21990185246760138, 0.23413570123149657, 0.1990122376774818, 0.20034716958398155, 0.16723258171669153, 0.14701636810640847, 0.22988511760234367, 0.10319556032644131, 0.22835181168540453, 0.2099912603000391, 0.20280963158472914, 0.3890201496409396, 0.11694687830663177, 0.2445236345921922, 0.18174848071128583, 0.2965526452321949, 0.19678029730883578, 0.2579664893148665, 0.22081495620161196, 0.21363857591479024, 0.21113409567223004, 0.342194026500495, 0.24230382812818163, 0.26422426816015854, 0.17280907378693025, 0.22869460853693693, 0.16110112894556897, 0.21869042823102952, 0.21484238247428306, 0.18960510407535483, 0.16385361638422677, 0.22610340040268534, 0.24910650298916345, 0.22912223673113355, 0.2132353049606793, 0.19658011590321048, 0.13772853838990948, 0.18358763034903866, 0.22772084387989677, 0.18241556452931132, 0.15416731038895842, 0.1290527921304511, 0.2682766616312778, 0.39630530220147325, 0.216391736281118, 0.27804035189539844, 0.2228505889470637, 0.20712354978886266, 0.20567078967564134, 0.2829766712917552, 0.2506588189545286, 0.30708910116069016, 0.21821882189058744, 0.2679181699622331, 0.18849630854311678, 0.20270051082021853, 0.19789262431666355, 0.35486394171697516, 0.17549676569807532, 0.18621060870301082, 0.19116586957734824, 0.17002719981780884, 0.14100287143029586, 0.1994369855086423, 0.2691849063624506, 0.2728379552996955, 0.27633539985311134, 0.25435677331417866, 0.3137182699731141, 0.32838357293590154, 0.1445929868129343, 0.232326141453403, 0.23654737596347247, 0.2557033385300202, 0.18098264812864784, 0.23377764722159644, 0.15406931793566858, 0.1625696985110043, 0.26883242232804316, 0.26526481335059, 0.16300856500724606, 0.36086857534161115, 0.221117260595732, 0.207340798796762, 0.2029024067641106, 0.15050216765177746, 0.2150921768507121, 0.25269699123329015, 0.23180570446716198, 0.1824292049924749, 0.2868911546808268, 0.13742632527736073, 0.07490232109795747, 0.18104833168936688, 0.1945713361984762, 0.38155733224507965, 0.16496698958119202, 0.10844391874611632, 0.056424655279827075, 0.20187366701740245, 0.07740131746776972, 0.07450019302490467, 0.1448923220590408, 0.06007403126520641, 0.25897696717850105, 0.2008032705438025, 0.0606379959239785, 0.05833420340891805, 0.19833478551325787, 0.10859630134960102, 0.10428669871673842, 0.0783442226873365, 0.0862672702094803, 0.08458357805371489, 0.09018629722361132, 0.1103234377800846, 0.1699442493800573, 0.056118779044981616, 0.20683160586667648, 0.2456110435363285, 0.11327823713304484, 0.09701259661457326, 0.049406413808957085, 0.07160419446653175, 0.27666062092500104, 0.1594770185940638, 0.12899936987011903, 0.13704441857616254, 0.06837906462516724, 0.10992851560976617, 0.10620568828743474, 0.3118106876039851, 0.17381185155954337, 0.09276844761169775, 0.08356192924999445, 0.1052659377112158, 0.1770772586481142, 0.0448594249462908, 0.14308107168568765, 0.10208355602248848, 0.06146157096858776, 0.23296939942406747, 0.09187607954935302, 0.04802675121263652, 0.05060007399253569, 0.1470217997041856, 0.20826780676960205, 0.2012743304758942, 0.2132676331583567, 0.10474340733632659, 0.1260719533006975, 0.10334744656636898, 0.07602121900114112, 0.09566318133022222, 0.15137886292084204, 0.16360768010944568, 0.055116293104955846, 0.10001872634301476, 0.1180133554589444, 0.14411316877288305, 0.04248063006533555, 0.18224121029137177, 0.05851206476880801, 0.18263115003433214, 0.11703723304533488, 0.12884267207253475, 0.0591436264164908, 0.081569530797358, 0.1944014416239709, 0.06033172285694358, 0.06459861352609889, 0.20745313446101488, 0.08629061620572716, 0.06219133277742544, 0.04334180992635034, 0.09575987199763837, 0.26348375830548565, 0.17313873498370944, 0.2162520645051803, 0.11126919262617214, 0.13241028800606858, 0.2811830142240019, 0.07002959627583177, 0.054892431903543024, 0.1829139168179582, 0.1758251779351728, 0.12324355339339728, 0.1466756806955085, 0.14571394934257118, 0.16011856871269833, 0.1993683007465407, 0.07417435289984005, 0.11747881529691208, 0.11470280148856546, 0.08388398526907913, 0.13525030379453795, 0.047293809714730344, 0.19338453669648395, 0.07772999401396775, 0.258038748474165, 0.19757281107484964, 0.198777228969611, 0.12859553087631342, 0.17838982597922828, 0.07874630601645659, 0.1896255609356344, 0.1717805465638399, 0.04071906406135879, 0.08231119808674374, 0.084400649673005, 0.05980400242210615, 0.053387224094460055, 0.16684088889384535, 0.21470233253968932, 0.09902043858310058, 0.3059255416540466, 0.17461029107140788, 0.14153852228958283, 0.20917139416965153, 0.2112530324655329, 0.2596438450614922, 0.1814710265033815, 0.045205900763046626, 0.13122760639667014, 0.0828216259170304, 0.12177267467273653, 0.20309921431125277, 0.1494224547930711, 0.03958897197507009, 0.039028278766451624, 0.1332201235629804, 0.11881786713212279, 0.23130292278380804, 0.12733825942353805, 0.049376754094417025, 0.10136851510192132, 0.15340017938999526, 0.08525755940222879, 0.045313952205347174, 0.10238168024198725, 0.1997365360818613, 0.11718363739014058, 0.15092077303831514, 0.2132398763240599, 0.06043914951443502, 0.12227382875497915, 0.28718070914117505, 0.09606935533710506, 0.12110268076432497, 0.30530401494565484, 0.06439134314801101, 0.05633446234139028, 0.06251282392214084, 0.05675340591002822, 0.05392700179546252, 0.13327612029050498, 0.13231356338271763, 0.0660664004100985, 0.05264857393412333, 0.19808628023303437, 0.09215662814152552, 0.047410110620959275, 0.06274604044379252, 0.06491847740456, 0.06850039770661313, 0.17156759801143495, 0.21793531976661443, 0.10733688820760966, 0.07435907284125651, 0.08047253842743782, 0.03503953279490754, 0.05795112697177594, 0.06890300048140635, 0.11058227126475008, 0.0615576252019013, 0.2389095405415813, 0.17700176811192556, 0.0498632330896386, 0.20085988781024966, 0.17596994140574487, 0.11728997230951488, 0.050620812855663846, 0.09184762346292898, 0.07577761197784577, 0.23750890210021322, 0.18676041106540583, 0.1923263064178863, 0.14069345657116783, 0.09433820359244245, 0.1036292779513203, 0.19037639519174948, 0.06415131923518774, 0.2659683198690495, 0.08836285389898281, 0.15023908641100533, 0.10551339288590611, 0.1854435115451791, 0.13784293623146085, 0.16799046855798566, 0.12421781102639957, 0.06710093830513622, 0.053527081204829505, 0.1645450584004417, 0.18607134316707216, 0.061305800448560245, 0.16350561731623325, 0.03780724586719477, 0.31874577160951306, 0.07996739544436075, 0.08146389841234737, 0.20439007546352905, 0.0727469796367379, 0.11261299249871962, 0.03703065564247911, 0.14681193834344544, 0.1295898556386893, 0.17457618035472622, 0.19116340089831826, 0.15379211966939163, 0.11744721502841597, 0.17303288504277242, 0.09399261849842692, 0.11514218579263578, 0.23933950137416224, 0.11885514889226437, 0.04833220855109308, 0.08759548210286992, 0.058881460233692895, 0.1187193721693535, 0.12704008213867551, 0.23629420766487083, 0.17156222361341425, 0.05298037153488555, 0.1439831219843578, 0.12341211980214664, 0.10458987351188712, 0.09171151046338888, 0.23100115524847017, 0.22379454030528304, 0.042787513661457686, 0.141027946053894, 0.1278618803189586, 0.13639983204225847, 0.0675950921847462, 0.05547448864440075, 0.057664597388595695, 0.06989648468652129, 0.1706744067321042, 0.0813786747715595, 0.09886315831833065, 0.16357016017411358, 0.11723467903136002, 0.192727492333196, 0.10441170879605675, 0.16011137020274788, 0.33380199597334775, 0.15900603012395967, 0.1756029770705857, 0.07404244593335584, 0.16453523845084309, 0.10036748884561238, 0.15496939356209025, 0.11212021913603087, 0.20683981435898666, 0.15272129853639524, 0.12670785504264998, 0.060232901404338256, 0.14548090406919933, 0.23791192798407493, 0.17571587720385856, 0.04980628497555288, 0.03800546106697175, 0.18202468407190311, 0.1510143564361026, 0.13196550206520002, 0.061249750499399444, 0.18691924449891079, 0.14429887847069736, 0.1535371925510566, 0.0632358855456682, 0.07537273310019357, 0.05155960594924095, 0.16297299776192983, 0.17963550675642215, 0.2610414893795286, 0.09369089739048385, 0.13566875584693683, 0.09248264272379168, 0.05679321761339749, 0.03791542073935403, 0.1583425141715883, 0.24028490723598553, 0.15651619937203012, 0.04224260739612053, 0.1027967662197827, 0.2856424562330685, 0.19847664430635, 0.11637981339456278, 0.29359669377077724, 0.0804305607292148, 0.19146512335011265, 0.1165033909798171, 0.1822384891712077, 0.20188600416042762, 0.24302315562308988, 0.054391310358468496, 0.04005675879913557, 0.06280899182122074, 0.07055608181449803, 0.0979880122850811, 0.17277189404432228, 0.13328500399610727, 0.06812154921434022, 0.07419315456713961, 0.06806468737814014, 0.13049644271272712, 0.03483143036685416, 0.12704810789324694, 0.08223923018410231, 0.06334825392953033, 0.08715760687886534, 0.0731839932586616, 0.05659913831549542, 0.10482797172246736, 0.11976192242099247, 0.18413468853314086, 0.0850732999881283, 0.13693310040811232, 0.061148200791229845, 0.029598446746117245, 0.06061692516829918, 0.051379701413544525, 0.09258654153903259, 0.1043749589348365, 0.09652082783955342, 0.14643878642654082, 0.07093739858208584, 0.05225653941330669, 0.11749967893022845, 0.03541091406285073, 0.05011053104820763, 0.10574734127072084, 0.25177193700438727, 0.09980515028615948, 0.07694190390864551, 0.19353238700339087, 0.03960779607625564, 0.09528081609291404, 0.10134645119119745, 0.1554099771440722, 0.21108298851230986, 0.2116536795916822, 0.14983133570374263, 0.13819460667993805, 0.11515633479530543, 0.020847351828237302, 0.13662923316545522, 0.12780212579490036, 0.19399332416612544, 0.1376407121518577, 0.159678131881381, 0.05144560598807739, 0.07889328597342475, 0.03274174953481078, 0.19514998511745124, 0.05273214846293393, 0.09514698357668726, 0.05755543682368546, 0.04048788144752563, 0.12235582773224793, 0.19797556262420832, 0.07568273304368446, 0.13650035355346266, 0.09261568887987624, 0.10818601306409059, 0.1157797136762585, 0.08202500409192295, 0.1292189800332704, 0.06509278586606473, 0.05451455580226997, 0.13702404228863768, 0.1406079359632206, 0.05699082407373648, 0.10427365444773655, 0.04807499706442508, 0.13614817324145345, 0.07942397428277263, 0.09595535388062926, 0.07387553168832717, 0.07345958510641927, 0.13259281844126378, 0.18042814188964854, 0.11687122088809092, 0.09443469367614776, 0.06816394602055481, 0.05853675454667089, 0.14743737528510736, 0.09785152573438687, 0.09739500536006857, 0.051240440541664496, 0.09191623205960651, 0.07009819384274715, 0.11928820348341458, 0.10966876566356995, 0.2087235374775522, 0.12995419139329364, 0.0860647905137725, 0.08327297616222526, 0.10248699446612237, 0.24592591641445982, 0.1583545458950002, 0.09037521424210032, 0.11021905619591874, 0.1580995747367831, 0.20633582126156289, 0.19274263660237526, 0.10491182287561032, 0.10106717992930783, 0.06165162446782275, 0.19758577668000282, 0.10478541247137312, 0.22463393651103544, 0.2032748169798344, 0.049033972759062634, 0.12957274704540173, 0.07097539205121314, 0.055327218720374105, 0.10500965691246003, 0.055884027355112664, 0.05359416995982335, 0.21829124039645387, 0.09106069174378936, 0.12437471569767444, 0.06218207285087586, 0.05101075950885625, 0.13500604452974607, 0.0893460046515028, 0.19938923377042522, 0.12926328566293352, 0.1576860207945047, 0.1605677401324619, 0.04754500258173748, 0.03734425475428786, 0.11346478089091336, 0.1232723258755888, 0.10281690389611448, 0.07140912612895219, 0.15178065770390048, 0.14772623242391483, 0.11104554211542247, 0.05737968766714225, 0.19638575459132226, 0.09239238465994946, 0.2148660010994582]
0.09020723074476737
Making ranges
torch.Size([62700, 2])
We keep 9.86e+06/1.16e+09 =  0% of the original kernel matrix.

torch.Size([2236, 2])
We keep 3.07e+04/5.10e+05 =  6% of the original kernel matrix.

torch.Size([15135, 2])
We keep 6.80e+05/2.43e+07 =  2% of the original kernel matrix.

torch.Size([26348, 2])
We keep 2.84e+06/1.54e+08 =  1% of the original kernel matrix.

torch.Size([41804, 2])
We keep 4.92e+06/4.23e+08 =  1% of the original kernel matrix.

torch.Size([4867, 2])
We keep 1.17e+05/3.10e+06 =  3% of the original kernel matrix.

torch.Size([20358, 2])
We keep 1.23e+06/6.00e+07 =  2% of the original kernel matrix.

torch.Size([8102, 2])
We keep 1.79e+06/2.18e+07 =  8% of the original kernel matrix.

torch.Size([23296, 2])
We keep 2.40e+06/1.59e+08 =  1% of the original kernel matrix.

torch.Size([13215, 2])
We keep 7.35e+05/2.81e+07 =  2% of the original kernel matrix.

torch.Size([29910, 2])
We keep 2.60e+06/1.80e+08 =  1% of the original kernel matrix.

torch.Size([10804, 2])
We keep 4.84e+05/1.75e+07 =  2% of the original kernel matrix.

torch.Size([27386, 2])
We keep 2.19e+06/1.42e+08 =  1% of the original kernel matrix.

torch.Size([124527, 2])
We keep 3.82e+07/4.37e+09 =  0% of the original kernel matrix.

torch.Size([89846, 2])
We keep 1.85e+07/2.25e+09 =  0% of the original kernel matrix.

torch.Size([56958, 2])
We keep 9.42e+06/9.46e+08 =  0% of the original kernel matrix.

torch.Size([63802, 2])
We keep 1.00e+07/1.05e+09 =  0% of the original kernel matrix.

torch.Size([14174, 2])
We keep 1.09e+06/3.62e+07 =  3% of the original kernel matrix.

torch.Size([30971, 2])
We keep 2.87e+06/2.05e+08 =  1% of the original kernel matrix.

torch.Size([11795, 2])
We keep 1.14e+06/3.36e+07 =  3% of the original kernel matrix.

torch.Size([28478, 2])
We keep 2.73e+06/1.97e+08 =  1% of the original kernel matrix.

torch.Size([23770, 2])
We keep 2.78e+06/1.30e+08 =  2% of the original kernel matrix.

torch.Size([39590, 2])
We keep 4.62e+06/3.89e+08 =  1% of the original kernel matrix.

torch.Size([222447, 2])
We keep 2.16e+08/1.88e+10 =  1% of the original kernel matrix.

torch.Size([119669, 2])
We keep 3.46e+07/4.67e+09 =  0% of the original kernel matrix.

torch.Size([13847, 2])
We keep 6.99e+05/2.79e+07 =  2% of the original kernel matrix.

torch.Size([30454, 2])
We keep 2.62e+06/1.80e+08 =  1% of the original kernel matrix.

torch.Size([298935, 2])
We keep 2.21e+08/2.62e+10 =  0% of the original kernel matrix.

torch.Size([142490, 2])
We keep 3.95e+07/5.52e+09 =  0% of the original kernel matrix.

torch.Size([31906, 2])
We keep 3.27e+06/2.37e+08 =  1% of the original kernel matrix.

torch.Size([46205, 2])
We keep 5.76e+06/5.25e+08 =  1% of the original kernel matrix.

torch.Size([50750, 2])
We keep 9.55e+06/7.10e+08 =  1% of the original kernel matrix.

torch.Size([59356, 2])
We keep 8.81e+06/9.08e+08 =  0% of the original kernel matrix.

torch.Size([85744, 2])
We keep 2.57e+07/2.17e+09 =  1% of the original kernel matrix.

torch.Size([75551, 2])
We keep 1.37e+07/1.59e+09 =  0% of the original kernel matrix.

torch.Size([65012, 2])
We keep 1.57e+07/1.34e+09 =  1% of the original kernel matrix.

torch.Size([65974, 2])
We keep 1.14e+07/1.25e+09 =  0% of the original kernel matrix.

torch.Size([308157, 2])
We keep 2.69e+08/3.15e+10 =  0% of the original kernel matrix.

torch.Size([144654, 2])
We keep 4.31e+07/6.05e+09 =  0% of the original kernel matrix.

torch.Size([237277, 2])
We keep 1.53e+08/1.64e+10 =  0% of the original kernel matrix.

torch.Size([125928, 2])
We keep 3.20e+07/4.37e+09 =  0% of the original kernel matrix.

torch.Size([9555, 2])
We keep 4.66e+05/1.42e+07 =  3% of the original kernel matrix.

torch.Size([26090, 2])
We keep 2.05e+06/1.28e+08 =  1% of the original kernel matrix.

torch.Size([386207, 2])
We keep 2.69e+08/4.26e+10 =  0% of the original kernel matrix.

torch.Size([164495, 2])
We keep 4.82e+07/7.03e+09 =  0% of the original kernel matrix.

torch.Size([25520, 2])
We keep 3.51e+06/1.39e+08 =  2% of the original kernel matrix.

torch.Size([41003, 2])
We keep 4.63e+06/4.01e+08 =  1% of the original kernel matrix.

torch.Size([57343, 2])
We keep 3.36e+07/1.05e+09 =  3% of the original kernel matrix.

torch.Size([59474, 2])
We keep 9.79e+06/1.10e+09 =  0% of the original kernel matrix.

torch.Size([5978, 2])
We keep 7.61e+05/8.23e+06 =  9% of the original kernel matrix.

torch.Size([20952, 2])
We keep 1.60e+06/9.77e+07 =  1% of the original kernel matrix.

torch.Size([169331, 2])
We keep 5.87e+07/7.82e+09 =  0% of the original kernel matrix.

torch.Size([104874, 2])
We keep 2.35e+07/3.01e+09 =  0% of the original kernel matrix.

torch.Size([125164, 2])
We keep 5.10e+07/4.96e+09 =  1% of the original kernel matrix.

torch.Size([89776, 2])
We keep 1.94e+07/2.40e+09 =  0% of the original kernel matrix.

torch.Size([11636, 2])
We keep 5.29e+05/1.92e+07 =  2% of the original kernel matrix.

torch.Size([28548, 2])
We keep 2.27e+06/1.49e+08 =  1% of the original kernel matrix.

torch.Size([107053, 2])
We keep 1.84e+08/7.18e+09 =  2% of the original kernel matrix.

torch.Size([82262, 2])
We keep 2.31e+07/2.89e+09 =  0% of the original kernel matrix.

torch.Size([1983763, 2])
We keep 3.83e+09/8.83e+11 =  0% of the original kernel matrix.

torch.Size([383503, 2])
We keep 1.90e+08/3.20e+10 =  0% of the original kernel matrix.

torch.Size([10494, 2])
We keep 6.53e+05/1.90e+07 =  3% of the original kernel matrix.

torch.Size([27052, 2])
We keep 2.28e+06/1.48e+08 =  1% of the original kernel matrix.

torch.Size([1313889, 2])
We keep 4.49e+09/6.87e+11 =  0% of the original kernel matrix.

torch.Size([309002, 2])
We keep 1.72e+08/2.82e+10 =  0% of the original kernel matrix.

torch.Size([38583, 2])
We keep 3.27e+07/8.19e+08 =  3% of the original kernel matrix.

torch.Size([50128, 2])
We keep 9.60e+06/9.75e+08 =  0% of the original kernel matrix.

torch.Size([10681, 2])
We keep 4.30e+05/1.57e+07 =  2% of the original kernel matrix.

torch.Size([27755, 2])
We keep 2.13e+06/1.35e+08 =  1% of the original kernel matrix.

torch.Size([7861, 2])
We keep 2.96e+05/9.11e+06 =  3% of the original kernel matrix.

torch.Size([24082, 2])
We keep 1.76e+06/1.03e+08 =  1% of the original kernel matrix.

torch.Size([141957, 2])
We keep 1.15e+08/7.35e+09 =  1% of the original kernel matrix.

torch.Size([94297, 2])
We keep 2.30e+07/2.92e+09 =  0% of the original kernel matrix.

torch.Size([121254, 2])
We keep 4.59e+07/4.15e+09 =  1% of the original kernel matrix.

torch.Size([88976, 2])
We keep 1.79e+07/2.19e+09 =  0% of the original kernel matrix.

torch.Size([3455, 2])
We keep 6.76e+04/1.47e+06 =  4% of the original kernel matrix.

torch.Size([17785, 2])
We keep 9.64e+05/4.13e+07 =  2% of the original kernel matrix.

torch.Size([14100, 2])
We keep 8.97e+05/3.35e+07 =  2% of the original kernel matrix.

torch.Size([30693, 2])
We keep 2.80e+06/1.97e+08 =  1% of the original kernel matrix.

torch.Size([5909, 2])
We keep 1.88e+05/4.80e+06 =  3% of the original kernel matrix.

torch.Size([21504, 2])
We keep 1.40e+06/7.46e+07 =  1% of the original kernel matrix.

torch.Size([8898, 2])
We keep 3.13e+05/1.07e+07 =  2% of the original kernel matrix.

torch.Size([25557, 2])
We keep 1.83e+06/1.12e+08 =  1% of the original kernel matrix.

torch.Size([4309, 2])
We keep 3.22e+05/5.40e+06 =  5% of the original kernel matrix.

torch.Size([17063, 2])
We keep 1.47e+06/7.91e+07 =  1% of the original kernel matrix.

torch.Size([2737, 2])
We keep 8.83e+04/1.00e+06 =  8% of the original kernel matrix.

torch.Size([15594, 2])
We keep 8.46e+05/3.41e+07 =  2% of the original kernel matrix.

torch.Size([9546, 2])
We keep 3.98e+05/1.29e+07 =  3% of the original kernel matrix.

torch.Size([25901, 2])
We keep 2.00e+06/1.22e+08 =  1% of the original kernel matrix.

torch.Size([2727, 2])
We keep 4.19e+04/7.48e+05 =  5% of the original kernel matrix.

torch.Size([16362, 2])
We keep 7.69e+05/2.95e+07 =  2% of the original kernel matrix.

torch.Size([3667, 2])
We keep 7.03e+04/1.54e+06 =  4% of the original kernel matrix.

torch.Size([18276, 2])
We keep 9.71e+05/4.22e+07 =  2% of the original kernel matrix.

torch.Size([10774, 2])
We keep 5.51e+05/1.86e+07 =  2% of the original kernel matrix.

torch.Size([27572, 2])
We keep 2.23e+06/1.47e+08 =  1% of the original kernel matrix.

torch.Size([9365, 2])
We keep 3.81e+05/1.27e+07 =  2% of the original kernel matrix.

torch.Size([25869, 2])
We keep 1.95e+06/1.22e+08 =  1% of the original kernel matrix.

torch.Size([2924, 2])
We keep 6.08e+04/1.05e+06 =  5% of the original kernel matrix.

torch.Size([16481, 2])
We keep 8.78e+05/3.49e+07 =  2% of the original kernel matrix.

torch.Size([22588, 2])
We keep 3.49e+06/1.37e+08 =  2% of the original kernel matrix.

torch.Size([38290, 2])
We keep 4.68e+06/3.99e+08 =  1% of the original kernel matrix.

torch.Size([22967, 2])
We keep 2.08e+06/1.09e+08 =  1% of the original kernel matrix.

torch.Size([38862, 2])
We keep 4.32e+06/3.56e+08 =  1% of the original kernel matrix.

torch.Size([8812, 2])
We keep 4.41e+05/1.23e+07 =  3% of the original kernel matrix.

torch.Size([25025, 2])
We keep 1.97e+06/1.19e+08 =  1% of the original kernel matrix.

torch.Size([11458, 2])
We keep 9.31e+05/2.75e+07 =  3% of the original kernel matrix.

torch.Size([27730, 2])
We keep 2.61e+06/1.79e+08 =  1% of the original kernel matrix.

torch.Size([6375, 2])
We keep 2.29e+05/6.13e+06 =  3% of the original kernel matrix.

torch.Size([21926, 2])
We keep 1.55e+06/8.44e+07 =  1% of the original kernel matrix.

torch.Size([15475, 2])
We keep 1.05e+06/4.05e+07 =  2% of the original kernel matrix.

torch.Size([32118, 2])
We keep 2.94e+06/2.17e+08 =  1% of the original kernel matrix.

torch.Size([6801, 2])
We keep 4.29e+05/9.44e+06 =  4% of the original kernel matrix.

torch.Size([22107, 2])
We keep 1.79e+06/1.05e+08 =  1% of the original kernel matrix.

torch.Size([4285, 2])
We keep 8.73e+04/2.01e+06 =  4% of the original kernel matrix.

torch.Size([19435, 2])
We keep 1.04e+06/4.83e+07 =  2% of the original kernel matrix.

torch.Size([8115, 2])
We keep 2.77e+05/8.86e+06 =  3% of the original kernel matrix.

torch.Size([24329, 2])
We keep 1.73e+06/1.01e+08 =  1% of the original kernel matrix.

torch.Size([6355, 2])
We keep 1.98e+05/5.70e+06 =  3% of the original kernel matrix.

torch.Size([22113, 2])
We keep 1.51e+06/8.13e+07 =  1% of the original kernel matrix.

torch.Size([5921, 2])
We keep 2.19e+05/5.40e+06 =  4% of the original kernel matrix.

torch.Size([21183, 2])
We keep 1.48e+06/7.91e+07 =  1% of the original kernel matrix.

torch.Size([12172, 2])
We keep 5.89e+05/2.33e+07 =  2% of the original kernel matrix.

torch.Size([28983, 2])
We keep 2.45e+06/1.65e+08 =  1% of the original kernel matrix.

torch.Size([4800, 2])
We keep 1.29e+05/2.88e+06 =  4% of the original kernel matrix.

torch.Size([20041, 2])
We keep 1.18e+06/5.78e+07 =  2% of the original kernel matrix.

torch.Size([20505, 2])
We keep 1.62e+06/7.77e+07 =  2% of the original kernel matrix.

torch.Size([36777, 2])
We keep 3.77e+06/3.00e+08 =  1% of the original kernel matrix.

torch.Size([14358, 2])
We keep 1.03e+06/3.45e+07 =  2% of the original kernel matrix.

torch.Size([30879, 2])
We keep 2.84e+06/2.00e+08 =  1% of the original kernel matrix.

torch.Size([4598, 2])
We keep 1.15e+05/2.85e+06 =  4% of the original kernel matrix.

torch.Size([19534, 2])
We keep 1.20e+06/5.75e+07 =  2% of the original kernel matrix.

torch.Size([9935, 2])
We keep 4.19e+05/1.40e+07 =  2% of the original kernel matrix.

torch.Size([26464, 2])
We keep 2.03e+06/1.28e+08 =  1% of the original kernel matrix.

torch.Size([4336, 2])
We keep 1.08e+05/2.49e+06 =  4% of the original kernel matrix.

torch.Size([19071, 2])
We keep 1.14e+06/5.38e+07 =  2% of the original kernel matrix.

torch.Size([6510, 2])
We keep 2.31e+05/6.20e+06 =  3% of the original kernel matrix.

torch.Size([22096, 2])
We keep 1.55e+06/8.48e+07 =  1% of the original kernel matrix.

torch.Size([9814, 2])
We keep 4.44e+05/1.43e+07 =  3% of the original kernel matrix.

torch.Size([26444, 2])
We keep 2.06e+06/1.29e+08 =  1% of the original kernel matrix.

torch.Size([5202, 2])
We keep 1.47e+05/3.62e+06 =  4% of the original kernel matrix.

torch.Size([20330, 2])
We keep 1.28e+06/6.48e+07 =  1% of the original kernel matrix.

torch.Size([2805, 2])
We keep 5.53e+04/8.78e+05 =  6% of the original kernel matrix.

torch.Size([16008, 2])
We keep 8.23e+05/3.19e+07 =  2% of the original kernel matrix.

torch.Size([11069, 2])
We keep 4.99e+05/1.76e+07 =  2% of the original kernel matrix.

torch.Size([27571, 2])
We keep 2.22e+06/1.43e+08 =  1% of the original kernel matrix.

torch.Size([6576, 2])
We keep 2.15e+05/5.62e+06 =  3% of the original kernel matrix.

torch.Size([22658, 2])
We keep 1.51e+06/8.07e+07 =  1% of the original kernel matrix.

torch.Size([6546, 2])
We keep 2.57e+05/6.57e+06 =  3% of the original kernel matrix.

torch.Size([22329, 2])
We keep 1.59e+06/8.74e+07 =  1% of the original kernel matrix.

torch.Size([6875, 2])
We keep 2.47e+05/7.01e+06 =  3% of the original kernel matrix.

torch.Size([22682, 2])
We keep 1.61e+06/9.02e+07 =  1% of the original kernel matrix.

torch.Size([11245, 2])
We keep 6.75e+05/2.02e+07 =  3% of the original kernel matrix.

torch.Size([27885, 2])
We keep 2.34e+06/1.53e+08 =  1% of the original kernel matrix.

torch.Size([3686, 2])
We keep 6.27e+04/1.36e+06 =  4% of the original kernel matrix.

torch.Size([18433, 2])
We keep 9.39e+05/3.98e+07 =  2% of the original kernel matrix.

torch.Size([2338, 2])
We keep 3.87e+04/6.15e+05 =  6% of the original kernel matrix.

torch.Size([15369, 2])
We keep 7.28e+05/2.67e+07 =  2% of the original kernel matrix.

torch.Size([5656, 2])
We keep 1.69e+05/4.19e+06 =  4% of the original kernel matrix.

torch.Size([21294, 2])
We keep 1.35e+06/6.97e+07 =  1% of the original kernel matrix.

torch.Size([13640, 2])
We keep 7.37e+05/2.86e+07 =  2% of the original kernel matrix.

torch.Size([29981, 2])
We keep 2.64e+06/1.82e+08 =  1% of the original kernel matrix.

torch.Size([9518, 2])
We keep 3.89e+05/1.32e+07 =  2% of the original kernel matrix.

torch.Size([26160, 2])
We keep 2.01e+06/1.24e+08 =  1% of the original kernel matrix.

torch.Size([2988, 2])
We keep 8.81e+04/1.39e+06 =  6% of the original kernel matrix.

torch.Size([16324, 2])
We keep 9.68e+05/4.02e+07 =  2% of the original kernel matrix.

torch.Size([16199, 2])
We keep 1.15e+06/4.63e+07 =  2% of the original kernel matrix.

torch.Size([32752, 2])
We keep 3.11e+06/2.32e+08 =  1% of the original kernel matrix.

torch.Size([4169, 2])
We keep 1.03e+05/2.23e+06 =  4% of the original kernel matrix.

torch.Size([18892, 2])
We keep 1.10e+06/5.09e+07 =  2% of the original kernel matrix.

torch.Size([6535, 2])
We keep 2.22e+05/6.15e+06 =  3% of the original kernel matrix.

torch.Size([22302, 2])
We keep 1.55e+06/8.45e+07 =  1% of the original kernel matrix.

torch.Size([3229, 2])
We keep 5.46e+04/1.12e+06 =  4% of the original kernel matrix.

torch.Size([17364, 2])
We keep 8.84e+05/3.60e+07 =  2% of the original kernel matrix.

torch.Size([7653, 2])
We keep 3.26e+05/9.10e+06 =  3% of the original kernel matrix.

torch.Size([23745, 2])
We keep 1.76e+06/1.03e+08 =  1% of the original kernel matrix.

torch.Size([3036, 2])
We keep 5.78e+04/1.15e+06 =  5% of the original kernel matrix.

torch.Size([16707, 2])
We keep 8.86e+05/3.65e+07 =  2% of the original kernel matrix.

torch.Size([8887, 2])
We keep 3.81e+05/1.19e+07 =  3% of the original kernel matrix.

torch.Size([25246, 2])
We keep 1.93e+06/1.18e+08 =  1% of the original kernel matrix.

torch.Size([10004, 2])
We keep 4.24e+05/1.45e+07 =  2% of the original kernel matrix.

torch.Size([26739, 2])
We keep 2.07e+06/1.30e+08 =  1% of the original kernel matrix.

torch.Size([3992, 2])
We keep 7.84e+04/1.81e+06 =  4% of the original kernel matrix.

torch.Size([18826, 2])
We keep 1.04e+06/4.59e+07 =  2% of the original kernel matrix.

torch.Size([3111, 2])
We keep 6.21e+04/1.22e+06 =  5% of the original kernel matrix.

torch.Size([16921, 2])
We keep 9.03e+05/3.76e+07 =  2% of the original kernel matrix.

torch.Size([5285, 2])
We keep 1.40e+05/3.49e+06 =  4% of the original kernel matrix.

torch.Size([20821, 2])
We keep 1.28e+06/6.37e+07 =  2% of the original kernel matrix.

torch.Size([6839, 2])
We keep 2.71e+05/7.21e+06 =  3% of the original kernel matrix.

torch.Size([22514, 2])
We keep 1.64e+06/9.15e+07 =  1% of the original kernel matrix.

torch.Size([10933, 2])
We keep 5.90e+05/1.89e+07 =  3% of the original kernel matrix.

torch.Size([27456, 2])
We keep 2.28e+06/1.48e+08 =  1% of the original kernel matrix.

torch.Size([11388, 2])
We keep 5.11e+05/1.87e+07 =  2% of the original kernel matrix.

torch.Size([28134, 2])
We keep 2.25e+06/1.47e+08 =  1% of the original kernel matrix.

torch.Size([6117, 2])
We keep 1.96e+05/5.24e+06 =  3% of the original kernel matrix.

torch.Size([21986, 2])
We keep 1.49e+06/7.80e+07 =  1% of the original kernel matrix.

torch.Size([20794, 2])
We keep 2.73e+06/1.07e+08 =  2% of the original kernel matrix.

torch.Size([36861, 2])
We keep 4.27e+06/3.52e+08 =  1% of the original kernel matrix.

torch.Size([8946, 2])
We keep 3.79e+05/1.20e+07 =  3% of the original kernel matrix.

torch.Size([25372, 2])
We keep 1.94e+06/1.18e+08 =  1% of the original kernel matrix.

torch.Size([8567, 2])
We keep 3.33e+05/1.01e+07 =  3% of the original kernel matrix.

torch.Size([24888, 2])
We keep 1.83e+06/1.08e+08 =  1% of the original kernel matrix.

torch.Size([20265, 2])
We keep 1.65e+06/8.24e+07 =  2% of the original kernel matrix.

torch.Size([36434, 2])
We keep 3.86e+06/3.09e+08 =  1% of the original kernel matrix.

torch.Size([6431, 2])
We keep 2.17e+05/5.35e+06 =  4% of the original kernel matrix.

torch.Size([22420, 2])
We keep 1.48e+06/7.88e+07 =  1% of the original kernel matrix.

torch.Size([17552, 2])
We keep 1.15e+06/5.34e+07 =  2% of the original kernel matrix.

torch.Size([33973, 2])
We keep 3.30e+06/2.49e+08 =  1% of the original kernel matrix.

torch.Size([3072, 2])
We keep 5.86e+04/1.09e+06 =  5% of the original kernel matrix.

torch.Size([16771, 2])
We keep 8.81e+05/3.56e+07 =  2% of the original kernel matrix.

torch.Size([11070, 2])
We keep 1.00e+06/2.27e+07 =  4% of the original kernel matrix.

torch.Size([27358, 2])
We keep 2.45e+06/1.62e+08 =  1% of the original kernel matrix.

torch.Size([4407, 2])
We keep 1.07e+05/2.48e+06 =  4% of the original kernel matrix.

torch.Size([19252, 2])
We keep 1.13e+06/5.37e+07 =  2% of the original kernel matrix.

torch.Size([12355, 2])
We keep 5.57e+05/2.16e+07 =  2% of the original kernel matrix.

torch.Size([29201, 2])
We keep 2.38e+06/1.58e+08 =  1% of the original kernel matrix.

torch.Size([2763, 2])
We keep 6.10e+04/9.74e+05 =  6% of the original kernel matrix.

torch.Size([16106, 2])
We keep 8.57e+05/3.36e+07 =  2% of the original kernel matrix.

torch.Size([9292, 2])
We keep 4.10e+05/1.34e+07 =  3% of the original kernel matrix.

torch.Size([25907, 2])
We keep 2.01e+06/1.25e+08 =  1% of the original kernel matrix.

torch.Size([3651, 2])
We keep 8.67e+04/1.71e+06 =  5% of the original kernel matrix.

torch.Size([17937, 2])
We keep 1.02e+06/4.45e+07 =  2% of the original kernel matrix.

torch.Size([5039, 2])
We keep 1.61e+05/3.69e+06 =  4% of the original kernel matrix.

torch.Size([20014, 2])
We keep 1.30e+06/6.54e+07 =  1% of the original kernel matrix.

torch.Size([2555, 2])
We keep 4.75e+04/7.97e+05 =  5% of the original kernel matrix.

torch.Size([15659, 2])
We keep 7.93e+05/3.04e+07 =  2% of the original kernel matrix.

torch.Size([3685, 2])
We keep 9.05e+04/1.82e+06 =  4% of the original kernel matrix.

torch.Size([17847, 2])
We keep 1.03e+06/4.59e+07 =  2% of the original kernel matrix.

torch.Size([12538, 2])
We keep 6.57e+05/2.43e+07 =  2% of the original kernel matrix.

torch.Size([29302, 2])
We keep 2.47e+06/1.68e+08 =  1% of the original kernel matrix.

torch.Size([12668, 2])
We keep 7.30e+05/2.76e+07 =  2% of the original kernel matrix.

torch.Size([29326, 2])
We keep 2.58e+06/1.79e+08 =  1% of the original kernel matrix.

torch.Size([3800, 2])
We keep 1.30e+05/2.37e+06 =  5% of the original kernel matrix.

torch.Size([17561, 2])
We keep 1.12e+06/5.25e+07 =  2% of the original kernel matrix.

torch.Size([3659, 2])
We keep 6.96e+04/1.49e+06 =  4% of the original kernel matrix.

torch.Size([18201, 2])
We keep 9.72e+05/4.16e+07 =  2% of the original kernel matrix.

torch.Size([3118, 2])
We keep 5.53e+04/1.17e+06 =  4% of the original kernel matrix.

torch.Size([17204, 2])
We keep 8.92e+05/3.68e+07 =  2% of the original kernel matrix.

torch.Size([15572, 2])
We keep 1.12e+06/4.59e+07 =  2% of the original kernel matrix.

torch.Size([32094, 2])
We keep 3.14e+06/2.31e+08 =  1% of the original kernel matrix.

torch.Size([6913, 2])
We keep 2.06e+05/6.03e+06 =  3% of the original kernel matrix.

torch.Size([23078, 2])
We keep 1.54e+06/8.36e+07 =  1% of the original kernel matrix.

torch.Size([13116, 2])
We keep 6.42e+05/2.60e+07 =  2% of the original kernel matrix.

torch.Size([29918, 2])
We keep 2.53e+06/1.74e+08 =  1% of the original kernel matrix.

torch.Size([7077, 2])
We keep 2.49e+05/7.21e+06 =  3% of the original kernel matrix.

torch.Size([22976, 2])
We keep 1.62e+06/9.15e+07 =  1% of the original kernel matrix.

torch.Size([5250, 2])
We keep 1.55e+05/3.83e+06 =  4% of the original kernel matrix.

torch.Size([20553, 2])
We keep 1.32e+06/6.66e+07 =  1% of the original kernel matrix.

torch.Size([6851, 2])
We keep 2.51e+05/6.98e+06 =  3% of the original kernel matrix.

torch.Size([22834, 2])
We keep 1.61e+06/9.00e+07 =  1% of the original kernel matrix.

torch.Size([3115, 2])
We keep 6.02e+04/1.07e+06 =  5% of the original kernel matrix.

torch.Size([16945, 2])
We keep 8.77e+05/3.52e+07 =  2% of the original kernel matrix.

torch.Size([6124, 2])
We keep 2.15e+05/5.38e+06 =  3% of the original kernel matrix.

torch.Size([21746, 2])
We keep 1.47e+06/7.90e+07 =  1% of the original kernel matrix.

torch.Size([4879, 2])
We keep 1.23e+05/2.87e+06 =  4% of the original kernel matrix.

torch.Size([20172, 2])
We keep 1.19e+06/5.77e+07 =  2% of the original kernel matrix.

torch.Size([5495, 2])
We keep 1.52e+05/3.85e+06 =  3% of the original kernel matrix.

torch.Size([21123, 2])
We keep 1.32e+06/6.69e+07 =  1% of the original kernel matrix.

torch.Size([30434, 2])
We keep 3.32e+06/2.06e+08 =  1% of the original kernel matrix.

torch.Size([45167, 2])
We keep 5.41e+06/4.89e+08 =  1% of the original kernel matrix.

torch.Size([4755, 2])
We keep 1.29e+05/2.94e+06 =  4% of the original kernel matrix.

torch.Size([19644, 2])
We keep 1.20e+06/5.84e+07 =  2% of the original kernel matrix.

torch.Size([11674, 2])
We keep 6.11e+05/2.23e+07 =  2% of the original kernel matrix.

torch.Size([28393, 2])
We keep 2.40e+06/1.61e+08 =  1% of the original kernel matrix.

torch.Size([3573, 2])
We keep 9.30e+04/1.80e+06 =  5% of the original kernel matrix.

torch.Size([17546, 2])
We keep 1.02e+06/4.57e+07 =  2% of the original kernel matrix.

torch.Size([7487, 2])
We keep 3.31e+05/9.36e+06 =  3% of the original kernel matrix.

torch.Size([23563, 2])
We keep 1.77e+06/1.04e+08 =  1% of the original kernel matrix.

torch.Size([2606, 2])
We keep 6.53e+04/1.07e+06 =  6% of the original kernel matrix.

torch.Size([15350, 2])
We keep 8.67e+05/3.52e+07 =  2% of the original kernel matrix.

torch.Size([7361, 2])
We keep 1.79e+06/1.04e+07 = 17% of the original kernel matrix.

torch.Size([23001, 2])
We keep 1.86e+06/1.10e+08 =  1% of the original kernel matrix.

torch.Size([4724, 2])
We keep 1.27e+05/3.09e+06 =  4% of the original kernel matrix.

torch.Size([19736, 2])
We keep 1.22e+06/5.99e+07 =  2% of the original kernel matrix.

torch.Size([2041, 2])
We keep 3.18e+04/4.97e+05 =  6% of the original kernel matrix.

torch.Size([14332, 2])
We keep 6.84e+05/2.40e+07 =  2% of the original kernel matrix.

torch.Size([3039, 2])
We keep 5.62e+04/1.10e+06 =  5% of the original kernel matrix.

torch.Size([16906, 2])
We keep 8.82e+05/3.57e+07 =  2% of the original kernel matrix.

torch.Size([6692, 2])
We keep 2.01e+05/5.74e+06 =  3% of the original kernel matrix.

torch.Size([22715, 2])
We keep 1.50e+06/8.16e+07 =  1% of the original kernel matrix.

torch.Size([6855, 2])
We keep 2.22e+05/6.41e+06 =  3% of the original kernel matrix.

torch.Size([23093, 2])
We keep 1.55e+06/8.62e+07 =  1% of the original kernel matrix.

torch.Size([12830, 2])
We keep 6.94e+05/2.69e+07 =  2% of the original kernel matrix.

torch.Size([29446, 2])
We keep 2.58e+06/1.77e+08 =  1% of the original kernel matrix.

torch.Size([4116, 2])
We keep 1.32e+05/2.13e+06 =  6% of the original kernel matrix.

torch.Size([18697, 2])
We keep 1.09e+06/4.98e+07 =  2% of the original kernel matrix.

torch.Size([14053, 2])
We keep 9.11e+05/3.57e+07 =  2% of the original kernel matrix.

torch.Size([30652, 2])
We keep 2.87e+06/2.04e+08 =  1% of the original kernel matrix.

torch.Size([18723, 2])
We keep 1.41e+06/6.64e+07 =  2% of the original kernel matrix.

torch.Size([35078, 2])
We keep 3.57e+06/2.78e+08 =  1% of the original kernel matrix.

torch.Size([14345, 2])
We keep 8.16e+05/3.28e+07 =  2% of the original kernel matrix.

torch.Size([31049, 2])
We keep 2.77e+06/1.95e+08 =  1% of the original kernel matrix.

torch.Size([3741, 2])
We keep 7.39e+04/1.66e+06 =  4% of the original kernel matrix.

torch.Size([18132, 2])
We keep 9.98e+05/4.38e+07 =  2% of the original kernel matrix.

torch.Size([4832, 2])
We keep 1.22e+05/2.85e+06 =  4% of the original kernel matrix.

torch.Size([20091, 2])
We keep 1.20e+06/5.75e+07 =  2% of the original kernel matrix.

torch.Size([9137, 2])
We keep 6.20e+05/1.42e+07 =  4% of the original kernel matrix.

torch.Size([25503, 2])
We keep 2.09e+06/1.28e+08 =  1% of the original kernel matrix.

torch.Size([5788, 2])
We keep 1.83e+05/4.73e+06 =  3% of the original kernel matrix.

torch.Size([21309, 2])
We keep 1.42e+06/7.41e+07 =  1% of the original kernel matrix.

torch.Size([4834, 2])
We keep 1.37e+05/3.20e+06 =  4% of the original kernel matrix.

torch.Size([19733, 2])
We keep 1.22e+06/6.09e+07 =  2% of the original kernel matrix.

torch.Size([7505, 2])
We keep 3.17e+05/8.65e+06 =  3% of the original kernel matrix.

torch.Size([23352, 2])
We keep 1.72e+06/1.00e+08 =  1% of the original kernel matrix.

torch.Size([7592, 2])
We keep 2.86e+05/8.29e+06 =  3% of the original kernel matrix.

torch.Size([23674, 2])
We keep 1.70e+06/9.81e+07 =  1% of the original kernel matrix.

torch.Size([12838, 2])
We keep 6.19e+05/2.47e+07 =  2% of the original kernel matrix.

torch.Size([29625, 2])
We keep 2.49e+06/1.69e+08 =  1% of the original kernel matrix.

torch.Size([13679, 2])
We keep 1.77e+06/5.32e+07 =  3% of the original kernel matrix.

torch.Size([29809, 2])
We keep 3.29e+06/2.48e+08 =  1% of the original kernel matrix.

torch.Size([5311, 2])
We keep 1.52e+05/3.49e+06 =  4% of the original kernel matrix.

torch.Size([20683, 2])
We keep 1.29e+06/6.36e+07 =  2% of the original kernel matrix.

torch.Size([40447, 2])
We keep 7.45e+06/4.63e+08 =  1% of the original kernel matrix.

torch.Size([52407, 2])
We keep 7.48e+06/7.33e+08 =  1% of the original kernel matrix.

torch.Size([5563, 2])
We keep 1.43e+05/3.72e+06 =  3% of the original kernel matrix.

torch.Size([21245, 2])
We keep 1.31e+06/6.58e+07 =  1% of the original kernel matrix.

torch.Size([6366, 2])
We keep 2.74e+05/6.29e+06 =  4% of the original kernel matrix.

torch.Size([22070, 2])
We keep 1.56e+06/8.54e+07 =  1% of the original kernel matrix.

torch.Size([7315, 2])
We keep 2.56e+05/7.71e+06 =  3% of the original kernel matrix.

torch.Size([23378, 2])
We keep 1.66e+06/9.46e+07 =  1% of the original kernel matrix.

torch.Size([1201, 2])
We keep 1.63e+04/1.47e+05 = 11% of the original kernel matrix.

torch.Size([12068, 2])
We keep 4.80e+05/1.31e+07 =  3% of the original kernel matrix.

torch.Size([31116, 2])
We keep 3.37e+06/2.21e+08 =  1% of the original kernel matrix.

torch.Size([45716, 2])
We keep 5.64e+06/5.07e+08 =  1% of the original kernel matrix.

torch.Size([3922, 2])
We keep 1.38e+05/2.48e+06 =  5% of the original kernel matrix.

torch.Size([17987, 2])
We keep 1.14e+06/5.37e+07 =  2% of the original kernel matrix.

torch.Size([9533, 2])
We keep 5.66e+05/1.50e+07 =  3% of the original kernel matrix.

torch.Size([25975, 2])
We keep 2.12e+06/1.32e+08 =  1% of the original kernel matrix.

torch.Size([2717, 2])
We keep 4.11e+04/7.48e+05 =  5% of the original kernel matrix.

torch.Size([16123, 2])
We keep 7.80e+05/2.95e+07 =  2% of the original kernel matrix.

torch.Size([7129, 2])
We keep 3.22e+05/9.00e+06 =  3% of the original kernel matrix.

torch.Size([22630, 2])
We keep 1.74e+06/1.02e+08 =  1% of the original kernel matrix.

torch.Size([3939, 2])
We keep 8.94e+04/1.83e+06 =  4% of the original kernel matrix.

torch.Size([18479, 2])
We keep 1.04e+06/4.60e+07 =  2% of the original kernel matrix.

torch.Size([6139, 2])
We keep 1.59e+05/4.66e+06 =  3% of the original kernel matrix.

torch.Size([22124, 2])
We keep 1.37e+06/7.35e+07 =  1% of the original kernel matrix.

torch.Size([6583, 2])
We keep 2.00e+05/5.60e+06 =  3% of the original kernel matrix.

torch.Size([22628, 2])
We keep 1.50e+06/8.06e+07 =  1% of the original kernel matrix.

torch.Size([6601, 2])
We keep 2.26e+05/6.09e+06 =  3% of the original kernel matrix.

torch.Size([22342, 2])
We keep 1.55e+06/8.40e+07 =  1% of the original kernel matrix.

torch.Size([1940, 2])
We keep 2.15e+04/3.02e+05 =  7% of the original kernel matrix.

torch.Size([14322, 2])
We keep 5.85e+05/1.87e+07 =  3% of the original kernel matrix.

torch.Size([4558, 2])
We keep 1.13e+05/2.65e+06 =  4% of the original kernel matrix.

torch.Size([19335, 2])
We keep 1.17e+06/5.54e+07 =  2% of the original kernel matrix.

torch.Size([3348, 2])
We keep 8.21e+04/1.50e+06 =  5% of the original kernel matrix.

torch.Size([17107, 2])
We keep 9.77e+05/4.17e+07 =  2% of the original kernel matrix.

torch.Size([11677, 2])
We keep 5.79e+05/2.02e+07 =  2% of the original kernel matrix.

torch.Size([28380, 2])
We keep 2.32e+06/1.53e+08 =  1% of the original kernel matrix.

torch.Size([5064, 2])
We keep 2.12e+05/3.76e+06 =  5% of the original kernel matrix.

torch.Size([20224, 2])
We keep 1.32e+06/6.60e+07 =  1% of the original kernel matrix.

torch.Size([13766, 2])
We keep 7.72e+05/3.10e+07 =  2% of the original kernel matrix.

torch.Size([30668, 2])
We keep 2.68e+06/1.90e+08 =  1% of the original kernel matrix.

torch.Size([6006, 2])
We keep 2.35e+05/4.95e+06 =  4% of the original kernel matrix.

torch.Size([21742, 2])
We keep 1.44e+06/7.58e+07 =  1% of the original kernel matrix.

torch.Size([6089, 2])
We keep 2.08e+05/5.47e+06 =  3% of the original kernel matrix.

torch.Size([21648, 2])
We keep 1.50e+06/7.97e+07 =  1% of the original kernel matrix.

torch.Size([8849, 2])
We keep 3.53e+05/1.16e+07 =  3% of the original kernel matrix.

torch.Size([25330, 2])
We keep 1.91e+06/1.16e+08 =  1% of the original kernel matrix.

torch.Size([10841, 2])
We keep 9.70e+05/2.79e+07 =  3% of the original kernel matrix.

torch.Size([26837, 2])
We keep 2.61e+06/1.80e+08 =  1% of the original kernel matrix.

torch.Size([5110, 2])
We keep 1.52e+05/3.83e+06 =  3% of the original kernel matrix.

torch.Size([20193, 2])
We keep 1.31e+06/6.67e+07 =  1% of the original kernel matrix.

torch.Size([4435, 2])
We keep 9.84e+04/2.27e+06 =  4% of the original kernel matrix.

torch.Size([19510, 2])
We keep 1.11e+06/5.13e+07 =  2% of the original kernel matrix.

torch.Size([5169, 2])
We keep 1.64e+05/3.72e+06 =  4% of the original kernel matrix.

torch.Size([20364, 2])
We keep 1.30e+06/6.57e+07 =  1% of the original kernel matrix.

torch.Size([5400, 2])
We keep 2.83e+05/5.70e+06 =  4% of the original kernel matrix.

torch.Size([20069, 2])
We keep 1.51e+06/8.13e+07 =  1% of the original kernel matrix.

torch.Size([7951, 2])
We keep 3.33e+05/9.33e+06 =  3% of the original kernel matrix.

torch.Size([24066, 2])
We keep 1.78e+06/1.04e+08 =  1% of the original kernel matrix.

torch.Size([21446, 2])
We keep 1.53e+06/8.06e+07 =  1% of the original kernel matrix.

torch.Size([37355, 2])
We keep 3.82e+06/3.06e+08 =  1% of the original kernel matrix.

torch.Size([9645, 2])
We keep 4.03e+05/1.41e+07 =  2% of the original kernel matrix.

torch.Size([26257, 2])
We keep 2.03e+06/1.28e+08 =  1% of the original kernel matrix.

torch.Size([5323, 2])
We keep 1.51e+05/3.86e+06 =  3% of the original kernel matrix.

torch.Size([20690, 2])
We keep 1.32e+06/6.69e+07 =  1% of the original kernel matrix.

torch.Size([8862, 2])
We keep 5.38e+05/1.42e+07 =  3% of the original kernel matrix.

torch.Size([24957, 2])
We keep 2.06e+06/1.29e+08 =  1% of the original kernel matrix.

torch.Size([15433, 2])
We keep 9.54e+05/4.08e+07 =  2% of the original kernel matrix.

torch.Size([31837, 2])
We keep 2.97e+06/2.18e+08 =  1% of the original kernel matrix.

torch.Size([24227, 2])
We keep 2.26e+06/1.23e+08 =  1% of the original kernel matrix.

torch.Size([39906, 2])
We keep 4.48e+06/3.78e+08 =  1% of the original kernel matrix.

torch.Size([3374, 2])
We keep 6.99e+04/1.38e+06 =  5% of the original kernel matrix.

torch.Size([17361, 2])
We keep 9.42e+05/4.00e+07 =  2% of the original kernel matrix.

torch.Size([1253, 2])
We keep 1.03e+04/1.30e+05 =  7% of the original kernel matrix.

torch.Size([12384, 2])
We keep 4.42e+05/1.23e+07 =  3% of the original kernel matrix.

torch.Size([6275, 2])
We keep 1.88e+05/5.24e+06 =  3% of the original kernel matrix.

torch.Size([22222, 2])
We keep 1.46e+06/7.80e+07 =  1% of the original kernel matrix.

torch.Size([3132, 2])
We keep 7.13e+04/1.16e+06 =  6% of the original kernel matrix.

torch.Size([16916, 2])
We keep 9.08e+05/3.67e+07 =  2% of the original kernel matrix.

torch.Size([4869, 2])
We keep 2.11e+05/4.31e+06 =  4% of the original kernel matrix.

torch.Size([19223, 2])
We keep 1.36e+06/7.07e+07 =  1% of the original kernel matrix.

torch.Size([6730, 2])
We keep 2.37e+05/6.74e+06 =  3% of the original kernel matrix.

torch.Size([22618, 2])
We keep 1.58e+06/8.84e+07 =  1% of the original kernel matrix.

torch.Size([7163, 2])
We keep 2.42e+05/7.14e+06 =  3% of the original kernel matrix.

torch.Size([23420, 2])
We keep 1.63e+06/9.10e+07 =  1% of the original kernel matrix.

torch.Size([2814, 2])
We keep 5.97e+04/1.05e+06 =  5% of the original kernel matrix.

torch.Size([16147, 2])
We keep 8.60e+05/3.49e+07 =  2% of the original kernel matrix.

torch.Size([3830, 2])
We keep 1.42e+05/2.15e+06 =  6% of the original kernel matrix.

torch.Size([17963, 2])
We keep 1.10e+06/4.99e+07 =  2% of the original kernel matrix.

torch.Size([2584, 2])
We keep 3.56e+04/6.34e+05 =  5% of the original kernel matrix.

torch.Size([16027, 2])
We keep 7.36e+05/2.71e+07 =  2% of the original kernel matrix.

torch.Size([6401, 2])
We keep 1.80e+05/5.13e+06 =  3% of the original kernel matrix.

torch.Size([22519, 2])
We keep 1.45e+06/7.71e+07 =  1% of the original kernel matrix.

torch.Size([3741, 2])
We keep 6.74e+04/1.43e+06 =  4% of the original kernel matrix.

torch.Size([18412, 2])
We keep 9.65e+05/4.07e+07 =  2% of the original kernel matrix.

torch.Size([9259, 2])
We keep 3.70e+05/1.25e+07 =  2% of the original kernel matrix.

torch.Size([25933, 2])
We keep 1.98e+06/1.20e+08 =  1% of the original kernel matrix.

torch.Size([7494, 2])
We keep 2.59e+05/7.75e+06 =  3% of the original kernel matrix.

torch.Size([23646, 2])
We keep 1.67e+06/9.48e+07 =  1% of the original kernel matrix.

torch.Size([7618, 2])
We keep 2.91e+05/8.92e+06 =  3% of the original kernel matrix.

torch.Size([23694, 2])
We keep 1.75e+06/1.02e+08 =  1% of the original kernel matrix.

torch.Size([1567, 2])
We keep 1.68e+04/2.31e+05 =  7% of the original kernel matrix.

torch.Size([13161, 2])
We keep 5.41e+05/1.64e+07 =  3% of the original kernel matrix.

torch.Size([10655, 2])
We keep 6.04e+05/1.89e+07 =  3% of the original kernel matrix.

torch.Size([27264, 2])
We keep 2.27e+06/1.48e+08 =  1% of the original kernel matrix.

torch.Size([9513, 2])
We keep 3.78e+05/1.28e+07 =  2% of the original kernel matrix.

torch.Size([26150, 2])
We keep 1.97e+06/1.22e+08 =  1% of the original kernel matrix.

torch.Size([8706, 2])
We keep 3.40e+05/1.11e+07 =  3% of the original kernel matrix.

torch.Size([25184, 2])
We keep 1.86e+06/1.13e+08 =  1% of the original kernel matrix.

torch.Size([11965, 2])
We keep 5.92e+05/2.21e+07 =  2% of the original kernel matrix.

torch.Size([28621, 2])
We keep 2.41e+06/1.60e+08 =  1% of the original kernel matrix.

torch.Size([19407, 2])
We keep 1.54e+06/6.91e+07 =  2% of the original kernel matrix.

torch.Size([35682, 2])
We keep 3.62e+06/2.83e+08 =  1% of the original kernel matrix.

torch.Size([7150, 2])
We keep 4.13e+05/8.55e+06 =  4% of the original kernel matrix.

torch.Size([22994, 2])
We keep 1.73e+06/9.96e+07 =  1% of the original kernel matrix.

torch.Size([3570, 2])
We keep 6.84e+04/1.39e+06 =  4% of the original kernel matrix.

torch.Size([17919, 2])
We keep 9.45e+05/4.02e+07 =  2% of the original kernel matrix.

torch.Size([3381, 2])
We keep 6.77e+04/1.30e+06 =  5% of the original kernel matrix.

torch.Size([17342, 2])
We keep 9.43e+05/3.88e+07 =  2% of the original kernel matrix.

torch.Size([3096, 2])
We keep 5.91e+04/1.18e+06 =  5% of the original kernel matrix.

torch.Size([16914, 2])
We keep 8.99e+05/3.70e+07 =  2% of the original kernel matrix.

torch.Size([3511, 2])
We keep 9.63e+04/1.90e+06 =  5% of the original kernel matrix.

torch.Size([17379, 2])
We keep 1.04e+06/4.69e+07 =  2% of the original kernel matrix.

torch.Size([2439, 2])
We keep 3.20e+04/5.58e+05 =  5% of the original kernel matrix.

torch.Size([15799, 2])
We keep 6.99e+05/2.54e+07 =  2% of the original kernel matrix.

torch.Size([2095, 2])
We keep 2.88e+04/4.15e+05 =  6% of the original kernel matrix.

torch.Size([14802, 2])
We keep 6.47e+05/2.19e+07 =  2% of the original kernel matrix.

torch.Size([17217, 2])
We keep 1.91e+06/5.91e+07 =  3% of the original kernel matrix.

torch.Size([33669, 2])
We keep 3.49e+06/2.62e+08 =  1% of the original kernel matrix.

torch.Size([4949, 2])
We keep 1.74e+05/3.39e+06 =  5% of the original kernel matrix.

torch.Size([19995, 2])
We keep 1.26e+06/6.27e+07 =  2% of the original kernel matrix.

torch.Size([3540, 2])
We keep 1.75e+05/2.86e+06 =  6% of the original kernel matrix.

torch.Size([16575, 2])
We keep 1.19e+06/5.76e+07 =  2% of the original kernel matrix.

torch.Size([3685, 2])
We keep 9.24e+04/1.93e+06 =  4% of the original kernel matrix.

torch.Size([17902, 2])
We keep 1.04e+06/4.74e+07 =  2% of the original kernel matrix.

torch.Size([9565, 2])
We keep 4.70e+05/1.51e+07 =  3% of the original kernel matrix.

torch.Size([25895, 2])
We keep 2.14e+06/1.32e+08 =  1% of the original kernel matrix.

torch.Size([4447, 2])
We keep 1.67e+05/3.21e+06 =  5% of the original kernel matrix.

torch.Size([18982, 2])
We keep 1.25e+06/6.11e+07 =  2% of the original kernel matrix.

torch.Size([14284, 2])
We keep 9.85e+05/3.94e+07 =  2% of the original kernel matrix.

torch.Size([30612, 2])
We keep 2.94e+06/2.14e+08 =  1% of the original kernel matrix.

torch.Size([12959, 2])
We keep 8.33e+05/2.89e+07 =  2% of the original kernel matrix.

torch.Size([29447, 2])
We keep 2.65e+06/1.83e+08 =  1% of the original kernel matrix.

torch.Size([3409, 2])
We keep 6.98e+04/1.42e+06 =  4% of the original kernel matrix.

torch.Size([17391, 2])
We keep 9.50e+05/4.05e+07 =  2% of the original kernel matrix.

torch.Size([3302, 2])
We keep 8.13e+04/1.53e+06 =  5% of the original kernel matrix.

torch.Size([17051, 2])
We keep 9.86e+05/4.21e+07 =  2% of the original kernel matrix.

torch.Size([12642, 2])
We keep 7.48e+05/2.94e+07 =  2% of the original kernel matrix.

torch.Size([29079, 2])
We keep 2.67e+06/1.85e+08 =  1% of the original kernel matrix.

torch.Size([1260, 2])
We keep 2.22e+04/2.20e+05 = 10% of the original kernel matrix.

torch.Size([11814, 2])
We keep 5.30e+05/1.60e+07 =  3% of the original kernel matrix.

torch.Size([5532, 2])
We keep 1.85e+05/4.62e+06 =  4% of the original kernel matrix.

torch.Size([20894, 2])
We keep 1.39e+06/7.32e+07 =  1% of the original kernel matrix.

torch.Size([6411, 2])
We keep 2.65e+05/6.72e+06 =  3% of the original kernel matrix.

torch.Size([21952, 2])
We keep 1.58e+06/8.83e+07 =  1% of the original kernel matrix.

torch.Size([7219, 2])
We keep 2.84e+05/7.72e+06 =  3% of the original kernel matrix.

torch.Size([23305, 2])
We keep 1.66e+06/9.47e+07 =  1% of the original kernel matrix.

torch.Size([16853, 2])
We keep 1.18e+06/4.97e+07 =  2% of the original kernel matrix.

torch.Size([33550, 2])
We keep 3.19e+06/2.40e+08 =  1% of the original kernel matrix.

torch.Size([6295, 2])
We keep 2.30e+05/5.42e+06 =  4% of the original kernel matrix.

torch.Size([21950, 2])
We keep 1.49e+06/7.93e+07 =  1% of the original kernel matrix.

torch.Size([3795, 2])
We keep 1.25e+05/2.05e+06 =  6% of the original kernel matrix.

torch.Size([17753, 2])
We keep 1.08e+06/4.88e+07 =  2% of the original kernel matrix.

torch.Size([5291, 2])
We keep 1.36e+05/3.44e+06 =  3% of the original kernel matrix.

torch.Size([20710, 2])
We keep 1.27e+06/6.32e+07 =  2% of the original kernel matrix.

torch.Size([10162, 2])
We keep 4.34e+05/1.50e+07 =  2% of the original kernel matrix.

torch.Size([26944, 2])
We keep 2.07e+06/1.32e+08 =  1% of the original kernel matrix.

torch.Size([2817, 2])
We keep 5.62e+04/8.95e+05 =  6% of the original kernel matrix.

torch.Size([16165, 2])
We keep 8.20e+05/3.22e+07 =  2% of the original kernel matrix.

torch.Size([20128, 2])
We keep 2.26e+06/8.04e+07 =  2% of the original kernel matrix.

torch.Size([36620, 2])
We keep 3.84e+06/3.05e+08 =  1% of the original kernel matrix.

torch.Size([53591, 2])
We keep 7.62e+07/3.10e+09 =  2% of the original kernel matrix.

torch.Size([54697, 2])
We keep 1.62e+07/1.90e+09 =  0% of the original kernel matrix.

torch.Size([10183, 2])
We keep 4.59e+05/1.54e+07 =  2% of the original kernel matrix.

torch.Size([27075, 2])
We keep 2.08e+06/1.34e+08 =  1% of the original kernel matrix.

torch.Size([8184, 2])
We keep 4.05e+05/9.70e+06 =  4% of the original kernel matrix.

torch.Size([24308, 2])
We keep 1.80e+06/1.06e+08 =  1% of the original kernel matrix.

torch.Size([1268, 2])
We keep 1.45e+04/1.72e+05 =  8% of the original kernel matrix.

torch.Size([12202, 2])
We keep 5.04e+05/1.41e+07 =  3% of the original kernel matrix.

torch.Size([13303, 2])
We keep 6.53e+05/2.65e+07 =  2% of the original kernel matrix.

torch.Size([30128, 2])
We keep 2.55e+06/1.75e+08 =  1% of the original kernel matrix.

torch.Size([35668, 2])
We keep 3.25e+07/3.38e+08 =  9% of the original kernel matrix.

torch.Size([48935, 2])
We keep 5.91e+06/6.26e+08 =  0% of the original kernel matrix.

torch.Size([147151, 2])
We keep 2.84e+08/1.66e+10 =  1% of the original kernel matrix.

torch.Size([93380, 2])
We keep 3.14e+07/4.38e+09 =  0% of the original kernel matrix.

torch.Size([7636, 2])
We keep 2.50e+05/7.83e+06 =  3% of the original kernel matrix.

torch.Size([23854, 2])
We keep 1.67e+06/9.53e+07 =  1% of the original kernel matrix.

torch.Size([79985, 2])
We keep 3.38e+07/2.50e+09 =  1% of the original kernel matrix.

torch.Size([71322, 2])
We keep 1.47e+07/1.70e+09 =  0% of the original kernel matrix.

torch.Size([83278, 2])
We keep 5.79e+07/3.15e+09 =  1% of the original kernel matrix.

torch.Size([72109, 2])
We keep 1.61e+07/1.91e+09 =  0% of the original kernel matrix.

torch.Size([16068, 2])
We keep 1.78e+06/5.83e+07 =  3% of the original kernel matrix.

torch.Size([32451, 2])
We keep 3.33e+06/2.60e+08 =  1% of the original kernel matrix.

torch.Size([117436, 2])
We keep 9.12e+08/1.18e+10 =  7% of the original kernel matrix.

torch.Size([85813, 2])
We keep 2.58e+07/3.71e+09 =  0% of the original kernel matrix.

torch.Size([3868, 2])
We keep 7.81e+04/1.71e+06 =  4% of the original kernel matrix.

torch.Size([18488, 2])
We keep 1.00e+06/4.46e+07 =  2% of the original kernel matrix.

torch.Size([7178, 2])
We keep 3.22e+05/8.23e+06 =  3% of the original kernel matrix.

torch.Size([23151, 2])
We keep 1.71e+06/9.77e+07 =  1% of the original kernel matrix.

torch.Size([159566, 2])
We keep 1.35e+08/1.09e+10 =  1% of the original kernel matrix.

torch.Size([99010, 2])
We keep 2.75e+07/3.56e+09 =  0% of the original kernel matrix.

torch.Size([223413, 2])
We keep 1.38e+08/1.37e+10 =  1% of the original kernel matrix.

torch.Size([121715, 2])
We keep 3.02e+07/3.99e+09 =  0% of the original kernel matrix.

torch.Size([7479, 2])
We keep 5.74e+05/8.85e+06 =  6% of the original kernel matrix.

torch.Size([23710, 2])
We keep 1.77e+06/1.01e+08 =  1% of the original kernel matrix.

torch.Size([35054, 2])
We keep 6.00e+06/3.29e+08 =  1% of the original kernel matrix.

torch.Size([48602, 2])
We keep 6.49e+06/6.18e+08 =  1% of the original kernel matrix.

torch.Size([31256, 2])
We keep 1.29e+07/4.20e+08 =  3% of the original kernel matrix.

torch.Size([44992, 2])
We keep 7.21e+06/6.98e+08 =  1% of the original kernel matrix.

torch.Size([59644, 2])
We keep 1.82e+08/2.34e+09 =  7% of the original kernel matrix.

torch.Size([60880, 2])
We keep 1.37e+07/1.65e+09 =  0% of the original kernel matrix.

torch.Size([43524, 2])
We keep 3.37e+07/1.31e+09 =  2% of the original kernel matrix.

torch.Size([51027, 2])
We keep 1.13e+07/1.23e+09 =  0% of the original kernel matrix.

torch.Size([53961, 2])
We keep 4.86e+07/1.47e+09 =  3% of the original kernel matrix.

torch.Size([58426, 2])
We keep 1.19e+07/1.31e+09 =  0% of the original kernel matrix.

torch.Size([53982, 2])
We keep 2.50e+07/1.02e+09 =  2% of the original kernel matrix.

torch.Size([57405, 2])
We keep 9.58e+06/1.09e+09 =  0% of the original kernel matrix.

torch.Size([33844, 2])
We keep 7.22e+06/3.00e+08 =  2% of the original kernel matrix.

torch.Size([47676, 2])
We keep 6.36e+06/5.90e+08 =  1% of the original kernel matrix.

torch.Size([5932, 2])
We keep 3.91e+06/2.16e+07 = 18% of the original kernel matrix.

torch.Size([19720, 2])
We keep 2.40e+06/1.58e+08 =  1% of the original kernel matrix.

torch.Size([134112, 2])
We keep 3.28e+08/1.74e+10 =  1% of the original kernel matrix.

torch.Size([87469, 2])
We keep 3.32e+07/4.49e+09 =  0% of the original kernel matrix.

torch.Size([6704, 2])
We keep 2.70e+05/6.82e+06 =  3% of the original kernel matrix.

torch.Size([22598, 2])
We keep 1.61e+06/8.90e+07 =  1% of the original kernel matrix.

torch.Size([4407, 2])
We keep 1.03e+05/2.38e+06 =  4% of the original kernel matrix.

torch.Size([19376, 2])
We keep 1.13e+06/5.26e+07 =  2% of the original kernel matrix.

torch.Size([31487, 2])
We keep 5.04e+06/2.53e+08 =  1% of the original kernel matrix.

torch.Size([45714, 2])
We keep 5.75e+06/5.42e+08 =  1% of the original kernel matrix.

torch.Size([35551, 2])
We keep 1.68e+07/6.11e+08 =  2% of the original kernel matrix.

torch.Size([47210, 2])
We keep 8.38e+06/8.42e+08 =  0% of the original kernel matrix.

torch.Size([329963, 2])
We keep 3.49e+08/3.92e+10 =  0% of the original kernel matrix.

torch.Size([150163, 2])
We keep 4.74e+07/6.74e+09 =  0% of the original kernel matrix.

torch.Size([120651, 2])
We keep 3.66e+07/4.02e+09 =  0% of the original kernel matrix.

torch.Size([88622, 2])
We keep 1.79e+07/2.16e+09 =  0% of the original kernel matrix.

torch.Size([3246, 2])
We keep 5.93e+04/1.18e+06 =  5% of the original kernel matrix.

torch.Size([17421, 2])
We keep 8.93e+05/3.70e+07 =  2% of the original kernel matrix.

torch.Size([13748, 2])
We keep 1.04e+06/3.28e+07 =  3% of the original kernel matrix.

torch.Size([30594, 2])
We keep 2.74e+06/1.95e+08 =  1% of the original kernel matrix.

torch.Size([23279, 2])
We keep 2.46e+06/1.20e+08 =  2% of the original kernel matrix.

torch.Size([39099, 2])
We keep 4.44e+06/3.73e+08 =  1% of the original kernel matrix.

torch.Size([17020, 2])
We keep 2.29e+06/8.18e+07 =  2% of the original kernel matrix.

torch.Size([33694, 2])
We keep 3.77e+06/3.08e+08 =  1% of the original kernel matrix.

torch.Size([132280, 2])
We keep 8.13e+07/5.60e+09 =  1% of the original kernel matrix.

torch.Size([93246, 2])
We keep 2.05e+07/2.55e+09 =  0% of the original kernel matrix.

torch.Size([23480, 2])
We keep 2.30e+07/3.06e+08 =  7% of the original kernel matrix.

torch.Size([38480, 2])
We keep 6.26e+06/5.96e+08 =  1% of the original kernel matrix.

torch.Size([37075, 2])
We keep 6.43e+06/3.77e+08 =  1% of the original kernel matrix.

torch.Size([49788, 2])
We keep 6.95e+06/6.62e+08 =  1% of the original kernel matrix.

torch.Size([2127, 2])
We keep 2.84e+04/4.98e+05 =  5% of the original kernel matrix.

torch.Size([14726, 2])
We keep 6.70e+05/2.41e+07 =  2% of the original kernel matrix.

torch.Size([11482, 2])
We keep 5.28e+05/2.01e+07 =  2% of the original kernel matrix.

torch.Size([28168, 2])
We keep 2.33e+06/1.53e+08 =  1% of the original kernel matrix.

torch.Size([37983, 2])
We keep 1.56e+07/8.37e+08 =  1% of the original kernel matrix.

torch.Size([48278, 2])
We keep 9.48e+06/9.86e+08 =  0% of the original kernel matrix.

torch.Size([58038, 2])
We keep 2.62e+07/1.58e+09 =  1% of the original kernel matrix.

torch.Size([60344, 2])
We keep 1.21e+07/1.35e+09 =  0% of the original kernel matrix.

torch.Size([33945, 2])
We keep 7.14e+06/3.97e+08 =  1% of the original kernel matrix.

torch.Size([47145, 2])
We keep 7.02e+06/6.79e+08 =  1% of the original kernel matrix.

torch.Size([10330, 2])
We keep 5.81e+05/1.85e+07 =  3% of the original kernel matrix.

torch.Size([26870, 2])
We keep 2.26e+06/1.46e+08 =  1% of the original kernel matrix.

torch.Size([391995, 2])
We keep 3.69e+08/7.00e+10 =  0% of the original kernel matrix.

torch.Size([164361, 2])
We keep 6.06e+07/9.02e+09 =  0% of the original kernel matrix.

torch.Size([19374, 2])
We keep 1.29e+06/6.32e+07 =  2% of the original kernel matrix.

torch.Size([35660, 2])
We keep 3.47e+06/2.71e+08 =  1% of the original kernel matrix.

torch.Size([42674, 2])
We keep 6.21e+06/4.78e+08 =  1% of the original kernel matrix.

torch.Size([53627, 2])
We keep 7.49e+06/7.45e+08 =  1% of the original kernel matrix.

torch.Size([166542, 2])
We keep 1.11e+08/1.01e+10 =  1% of the original kernel matrix.

torch.Size([103690, 2])
We keep 2.63e+07/3.42e+09 =  0% of the original kernel matrix.

torch.Size([5046, 2])
We keep 1.36e+05/3.29e+06 =  4% of the original kernel matrix.

torch.Size([20175, 2])
We keep 1.25e+06/6.18e+07 =  2% of the original kernel matrix.

torch.Size([52179, 2])
We keep 1.76e+07/8.99e+08 =  1% of the original kernel matrix.

torch.Size([59541, 2])
We keep 9.74e+06/1.02e+09 =  0% of the original kernel matrix.

torch.Size([314006, 2])
We keep 6.71e+08/4.94e+10 =  1% of the original kernel matrix.

torch.Size([144885, 2])
We keep 5.27e+07/7.57e+09 =  0% of the original kernel matrix.

torch.Size([286692, 2])
We keep 3.01e+08/3.21e+10 =  0% of the original kernel matrix.

torch.Size([138183, 2])
We keep 4.32e+07/6.10e+09 =  0% of the original kernel matrix.

torch.Size([16119, 2])
We keep 1.29e+06/5.28e+07 =  2% of the original kernel matrix.

torch.Size([32496, 2])
We keep 3.28e+06/2.48e+08 =  1% of the original kernel matrix.

torch.Size([6585, 2])
We keep 2.39e+05/6.44e+06 =  3% of the original kernel matrix.

torch.Size([22272, 2])
We keep 1.56e+06/8.64e+07 =  1% of the original kernel matrix.

torch.Size([6553, 2])
We keep 3.25e+05/7.82e+06 =  4% of the original kernel matrix.

torch.Size([21716, 2])
We keep 1.66e+06/9.53e+07 =  1% of the original kernel matrix.

torch.Size([6752, 2])
We keep 1.99e+05/5.69e+06 =  3% of the original kernel matrix.

torch.Size([22801, 2])
We keep 1.51e+06/8.13e+07 =  1% of the original kernel matrix.

torch.Size([32505, 2])
We keep 8.29e+06/4.11e+08 =  2% of the original kernel matrix.

torch.Size([46025, 2])
We keep 7.14e+06/6.90e+08 =  1% of the original kernel matrix.

torch.Size([25850, 2])
We keep 2.29e+06/1.37e+08 =  1% of the original kernel matrix.

torch.Size([41313, 2])
We keep 4.66e+06/3.98e+08 =  1% of the original kernel matrix.

torch.Size([35768, 2])
We keep 8.10e+06/4.44e+08 =  1% of the original kernel matrix.

torch.Size([48629, 2])
We keep 7.30e+06/7.18e+08 =  1% of the original kernel matrix.

torch.Size([81320, 2])
We keep 5.87e+07/2.83e+09 =  2% of the original kernel matrix.

torch.Size([73367, 2])
We keep 1.38e+07/1.81e+09 =  0% of the original kernel matrix.

torch.Size([47189, 2])
We keep 2.01e+07/7.08e+08 =  2% of the original kernel matrix.

torch.Size([56334, 2])
We keep 8.70e+06/9.07e+08 =  0% of the original kernel matrix.

torch.Size([10790, 2])
We keep 2.33e+06/4.43e+07 =  5% of the original kernel matrix.

torch.Size([26108, 2])
We keep 3.11e+06/2.27e+08 =  1% of the original kernel matrix.

torch.Size([13775, 2])
We keep 6.92e+05/2.88e+07 =  2% of the original kernel matrix.

torch.Size([30597, 2])
We keep 2.61e+06/1.83e+08 =  1% of the original kernel matrix.

torch.Size([101256, 2])
We keep 3.39e+08/2.00e+10 =  1% of the original kernel matrix.

torch.Size([70169, 2])
We keep 3.55e+07/4.81e+09 =  0% of the original kernel matrix.

torch.Size([33945, 2])
We keep 5.34e+07/5.42e+08 =  9% of the original kernel matrix.

torch.Size([46812, 2])
We keep 7.92e+06/7.93e+08 =  0% of the original kernel matrix.

torch.Size([21515, 2])
We keep 1.13e+07/2.00e+08 =  5% of the original kernel matrix.

torch.Size([37004, 2])
We keep 5.42e+06/4.82e+08 =  1% of the original kernel matrix.

torch.Size([18321, 2])
We keep 1.43e+06/6.36e+07 =  2% of the original kernel matrix.

torch.Size([34847, 2])
We keep 3.48e+06/2.72e+08 =  1% of the original kernel matrix.

torch.Size([369056, 2])
We keep 1.42e+09/9.20e+10 =  1% of the original kernel matrix.

torch.Size([152164, 2])
We keep 6.72e+07/1.03e+10 =  0% of the original kernel matrix.

torch.Size([9398, 2])
We keep 4.62e+05/1.42e+07 =  3% of the original kernel matrix.

torch.Size([25731, 2])
We keep 2.04e+06/1.29e+08 =  1% of the original kernel matrix.

torch.Size([154377, 2])
We keep 2.13e+08/1.43e+10 =  1% of the original kernel matrix.

torch.Size([96074, 2])
We keep 3.05e+07/4.07e+09 =  0% of the original kernel matrix.

torch.Size([8302, 2])
We keep 5.67e+05/1.45e+07 =  3% of the original kernel matrix.

torch.Size([24533, 2])
We keep 2.05e+06/1.30e+08 =  1% of the original kernel matrix.

torch.Size([18365, 2])
We keep 6.30e+06/2.07e+08 =  3% of the original kernel matrix.

torch.Size([32383, 2])
We keep 5.52e+06/4.91e+08 =  1% of the original kernel matrix.

torch.Size([23685, 2])
We keep 2.32e+06/1.19e+08 =  1% of the original kernel matrix.

torch.Size([39602, 2])
We keep 4.40e+06/3.72e+08 =  1% of the original kernel matrix.

torch.Size([132873, 2])
We keep 3.27e+08/1.26e+10 =  2% of the original kernel matrix.

torch.Size([86995, 2])
We keep 2.91e+07/3.83e+09 =  0% of the original kernel matrix.

torch.Size([66884, 2])
We keep 1.69e+07/1.84e+09 =  0% of the original kernel matrix.

torch.Size([66341, 2])
We keep 1.29e+07/1.46e+09 =  0% of the original kernel matrix.

torch.Size([8607, 2])
We keep 3.15e+05/9.99e+06 =  3% of the original kernel matrix.

torch.Size([25144, 2])
We keep 1.80e+06/1.08e+08 =  1% of the original kernel matrix.

torch.Size([201581, 2])
We keep 7.77e+07/1.12e+10 =  0% of the original kernel matrix.

torch.Size([115218, 2])
We keep 2.75e+07/3.61e+09 =  0% of the original kernel matrix.

torch.Size([170430, 2])
We keep 6.17e+07/7.55e+09 =  0% of the original kernel matrix.

torch.Size([104926, 2])
We keep 2.32e+07/2.96e+09 =  0% of the original kernel matrix.

torch.Size([6715, 2])
We keep 2.35e+05/6.75e+06 =  3% of the original kernel matrix.

torch.Size([22705, 2])
We keep 1.59e+06/8.85e+07 =  1% of the original kernel matrix.

torch.Size([63020, 2])
We keep 2.27e+07/1.32e+09 =  1% of the original kernel matrix.

torch.Size([65340, 2])
We keep 1.13e+07/1.24e+09 =  0% of the original kernel matrix.

torch.Size([174346, 2])
We keep 7.08e+07/9.86e+09 =  0% of the original kernel matrix.

torch.Size([107658, 2])
We keep 2.61e+07/3.38e+09 =  0% of the original kernel matrix.

torch.Size([535190, 2])
We keep 5.06e+08/8.82e+10 =  0% of the original kernel matrix.

torch.Size([196359, 2])
We keep 6.74e+07/1.01e+10 =  0% of the original kernel matrix.

torch.Size([36674, 2])
We keep 3.91e+07/7.01e+08 =  5% of the original kernel matrix.

torch.Size([48328, 2])
We keep 8.65e+06/9.02e+08 =  0% of the original kernel matrix.

torch.Size([3240, 2])
We keep 8.14e+04/1.58e+06 =  5% of the original kernel matrix.

torch.Size([16862, 2])
We keep 9.80e+05/4.28e+07 =  2% of the original kernel matrix.

torch.Size([11637, 2])
We keep 5.24e+05/1.97e+07 =  2% of the original kernel matrix.

torch.Size([28396, 2])
We keep 2.29e+06/1.51e+08 =  1% of the original kernel matrix.

torch.Size([6053, 2])
We keep 2.11e+05/5.25e+06 =  4% of the original kernel matrix.

torch.Size([21992, 2])
We keep 1.44e+06/7.80e+07 =  1% of the original kernel matrix.

torch.Size([34479, 2])
We keep 4.49e+06/2.85e+08 =  1% of the original kernel matrix.

torch.Size([47921, 2])
We keep 6.18e+06/5.75e+08 =  1% of the original kernel matrix.

torch.Size([20448, 2])
We keep 4.65e+06/1.00e+08 =  4% of the original kernel matrix.

torch.Size([36744, 2])
We keep 4.19e+06/3.41e+08 =  1% of the original kernel matrix.

torch.Size([3025, 2])
We keep 5.39e+04/1.07e+06 =  5% of the original kernel matrix.

torch.Size([16846, 2])
We keep 8.73e+05/3.53e+07 =  2% of the original kernel matrix.

torch.Size([130343, 2])
We keep 5.50e+07/4.59e+09 =  1% of the original kernel matrix.

torch.Size([92007, 2])
We keep 1.88e+07/2.31e+09 =  0% of the original kernel matrix.

torch.Size([255678, 2])
We keep 1.45e+08/2.06e+10 =  0% of the original kernel matrix.

torch.Size([130994, 2])
We keep 3.57e+07/4.89e+09 =  0% of the original kernel matrix.

torch.Size([5722, 2])
We keep 1.36e+06/1.38e+07 =  9% of the original kernel matrix.

torch.Size([19771, 2])
We keep 1.92e+06/1.27e+08 =  1% of the original kernel matrix.

torch.Size([9674, 2])
We keep 6.32e+05/1.82e+07 =  3% of the original kernel matrix.

torch.Size([25938, 2])
We keep 2.26e+06/1.45e+08 =  1% of the original kernel matrix.

torch.Size([25218, 2])
We keep 2.76e+06/1.53e+08 =  1% of the original kernel matrix.

torch.Size([40691, 2])
We keep 4.90e+06/4.22e+08 =  1% of the original kernel matrix.

torch.Size([18345, 2])
We keep 1.25e+06/5.95e+07 =  2% of the original kernel matrix.

torch.Size([35050, 2])
We keep 3.47e+06/2.63e+08 =  1% of the original kernel matrix.

torch.Size([16448, 2])
We keep 2.26e+06/5.79e+07 =  3% of the original kernel matrix.

torch.Size([32931, 2])
We keep 3.47e+06/2.59e+08 =  1% of the original kernel matrix.

torch.Size([13793, 2])
We keep 8.61e+05/3.30e+07 =  2% of the original kernel matrix.

torch.Size([30543, 2])
We keep 2.79e+06/1.96e+08 =  1% of the original kernel matrix.

torch.Size([8130, 2])
We keep 2.61e+05/8.60e+06 =  3% of the original kernel matrix.

torch.Size([24668, 2])
We keep 1.71e+06/9.99e+07 =  1% of the original kernel matrix.

torch.Size([83806, 2])
We keep 9.19e+07/3.23e+09 =  2% of the original kernel matrix.

torch.Size([72102, 2])
We keep 1.61e+07/1.94e+09 =  0% of the original kernel matrix.

torch.Size([30374, 2])
We keep 3.08e+06/2.07e+08 =  1% of the original kernel matrix.

torch.Size([45076, 2])
We keep 5.44e+06/4.90e+08 =  1% of the original kernel matrix.

torch.Size([31506, 2])
We keep 4.95e+06/2.48e+08 =  1% of the original kernel matrix.

torch.Size([46062, 2])
We keep 5.87e+06/5.37e+08 =  1% of the original kernel matrix.

torch.Size([62212, 2])
We keep 2.76e+07/1.54e+09 =  1% of the original kernel matrix.

torch.Size([63652, 2])
We keep 1.22e+07/1.34e+09 =  0% of the original kernel matrix.

torch.Size([17026, 2])
We keep 2.74e+06/8.70e+07 =  3% of the original kernel matrix.

torch.Size([32587, 2])
We keep 3.97e+06/3.18e+08 =  1% of the original kernel matrix.

torch.Size([357256, 2])
We keep 1.98e+09/5.03e+10 =  3% of the original kernel matrix.

torch.Size([156201, 2])
We keep 5.34e+07/7.64e+09 =  0% of the original kernel matrix.

torch.Size([8175, 2])
We keep 3.84e+05/1.04e+07 =  3% of the original kernel matrix.

torch.Size([24287, 2])
We keep 1.86e+06/1.10e+08 =  1% of the original kernel matrix.

torch.Size([92130, 2])
We keep 2.53e+07/2.44e+09 =  1% of the original kernel matrix.

torch.Size([77812, 2])
We keep 1.44e+07/1.68e+09 =  0% of the original kernel matrix.

torch.Size([3729, 2])
We keep 8.38e+04/1.83e+06 =  4% of the original kernel matrix.

torch.Size([18059, 2])
We keep 1.02e+06/4.60e+07 =  2% of the original kernel matrix.

torch.Size([7886, 2])
We keep 3.03e+05/9.04e+06 =  3% of the original kernel matrix.

torch.Size([24220, 2])
We keep 1.74e+06/1.02e+08 =  1% of the original kernel matrix.

torch.Size([7711, 2])
We keep 2.93e+05/8.76e+06 =  3% of the original kernel matrix.

torch.Size([23891, 2])
We keep 1.74e+06/1.01e+08 =  1% of the original kernel matrix.

torch.Size([22704, 2])
We keep 7.69e+06/1.20e+08 =  6% of the original kernel matrix.

torch.Size([38995, 2])
We keep 4.13e+06/3.73e+08 =  1% of the original kernel matrix.

torch.Size([10456, 2])
We keep 4.88e+05/1.73e+07 =  2% of the original kernel matrix.

torch.Size([26799, 2])
We keep 2.20e+06/1.42e+08 =  1% of the original kernel matrix.

torch.Size([71463, 2])
We keep 6.70e+07/2.26e+09 =  2% of the original kernel matrix.

torch.Size([67653, 2])
We keep 1.42e+07/1.62e+09 =  0% of the original kernel matrix.

torch.Size([8835, 2])
We keep 3.49e+05/1.14e+07 =  3% of the original kernel matrix.

torch.Size([25225, 2])
We keep 1.89e+06/1.15e+08 =  1% of the original kernel matrix.

torch.Size([11687, 2])
We keep 5.80e+05/2.10e+07 =  2% of the original kernel matrix.

torch.Size([28375, 2])
We keep 2.34e+06/1.56e+08 =  1% of the original kernel matrix.

torch.Size([628762, 2])
We keep 9.23e+08/1.30e+11 =  0% of the original kernel matrix.

torch.Size([209014, 2])
We keep 8.08e+07/1.23e+10 =  0% of the original kernel matrix.

torch.Size([69539, 2])
We keep 2.91e+07/1.84e+09 =  1% of the original kernel matrix.

torch.Size([67622, 2])
We keep 1.31e+07/1.46e+09 =  0% of the original kernel matrix.

torch.Size([62334, 2])
We keep 2.20e+07/1.50e+09 =  1% of the original kernel matrix.

torch.Size([64079, 2])
We keep 1.20e+07/1.32e+09 =  0% of the original kernel matrix.

torch.Size([192957, 2])
We keep 1.44e+08/1.29e+10 =  1% of the original kernel matrix.

torch.Size([112618, 2])
We keep 2.95e+07/3.87e+09 =  0% of the original kernel matrix.

torch.Size([229347, 2])
We keep 2.37e+08/2.50e+10 =  0% of the original kernel matrix.

torch.Size([117418, 2])
We keep 3.91e+07/5.39e+09 =  0% of the original kernel matrix.

torch.Size([12737, 2])
We keep 6.33e+05/2.51e+07 =  2% of the original kernel matrix.

torch.Size([29382, 2])
We keep 2.51e+06/1.71e+08 =  1% of the original kernel matrix.

torch.Size([6183, 2])
We keep 2.12e+05/5.49e+06 =  3% of the original kernel matrix.

torch.Size([21911, 2])
We keep 1.51e+06/7.98e+07 =  1% of the original kernel matrix.

torch.Size([34632, 2])
We keep 1.51e+07/5.75e+08 =  2% of the original kernel matrix.

torch.Size([47345, 2])
We keep 8.24e+06/8.17e+08 =  1% of the original kernel matrix.

torch.Size([2448, 2])
We keep 3.76e+04/6.12e+05 =  6% of the original kernel matrix.

torch.Size([15559, 2])
We keep 7.34e+05/2.66e+07 =  2% of the original kernel matrix.

torch.Size([11521, 2])
We keep 5.05e+05/1.90e+07 =  2% of the original kernel matrix.

torch.Size([28148, 2])
We keep 2.26e+06/1.49e+08 =  1% of the original kernel matrix.

torch.Size([17218, 2])
We keep 1.79e+06/6.92e+07 =  2% of the original kernel matrix.

torch.Size([33788, 2])
We keep 3.59e+06/2.83e+08 =  1% of the original kernel matrix.

torch.Size([5815, 2])
We keep 2.99e+05/6.26e+06 =  4% of the original kernel matrix.

torch.Size([20726, 2])
We keep 1.56e+06/8.52e+07 =  1% of the original kernel matrix.

torch.Size([7040, 2])
We keep 2.15e+05/6.03e+06 =  3% of the original kernel matrix.

torch.Size([23185, 2])
We keep 1.53e+06/8.36e+07 =  1% of the original kernel matrix.

torch.Size([3621, 2])
We keep 8.37e+04/1.73e+06 =  4% of the original kernel matrix.

torch.Size([17776, 2])
We keep 1.02e+06/4.49e+07 =  2% of the original kernel matrix.

torch.Size([9797, 2])
We keep 4.82e+05/1.51e+07 =  3% of the original kernel matrix.

torch.Size([26478, 2])
We keep 2.13e+06/1.32e+08 =  1% of the original kernel matrix.

torch.Size([372414, 2])
We keep 7.78e+08/6.51e+10 =  1% of the original kernel matrix.

torch.Size([157216, 2])
We keep 5.94e+07/8.69e+09 =  0% of the original kernel matrix.

torch.Size([18698, 2])
We keep 3.13e+06/1.05e+08 =  2% of the original kernel matrix.

torch.Size([34448, 2])
We keep 4.20e+06/3.49e+08 =  1% of the original kernel matrix.

torch.Size([69734, 2])
We keep 2.49e+07/1.67e+09 =  1% of the original kernel matrix.

torch.Size([67904, 2])
We keep 1.25e+07/1.39e+09 =  0% of the original kernel matrix.

torch.Size([14642, 2])
We keep 2.34e+07/1.65e+08 = 14% of the original kernel matrix.

torch.Size([30142, 2])
We keep 4.62e+06/4.38e+08 =  1% of the original kernel matrix.

torch.Size([7192, 2])
We keep 2.68e+05/7.67e+06 =  3% of the original kernel matrix.

torch.Size([23281, 2])
We keep 1.66e+06/9.43e+07 =  1% of the original kernel matrix.

torch.Size([16233, 2])
We keep 1.25e+06/4.83e+07 =  2% of the original kernel matrix.

torch.Size([32775, 2])
We keep 3.18e+06/2.37e+08 =  1% of the original kernel matrix.

torch.Size([649239, 2])
We keep 1.62e+09/1.49e+11 =  1% of the original kernel matrix.

torch.Size([210093, 2])
We keep 8.33e+07/1.31e+10 =  0% of the original kernel matrix.

torch.Size([762160, 2])
We keep 7.57e+08/1.54e+11 =  0% of the original kernel matrix.

torch.Size([231455, 2])
We keep 8.55e+07/1.34e+10 =  0% of the original kernel matrix.

torch.Size([21436, 2])
We keep 2.32e+06/9.68e+07 =  2% of the original kernel matrix.

torch.Size([37742, 2])
We keep 4.02e+06/3.35e+08 =  1% of the original kernel matrix.

torch.Size([28835, 2])
We keep 4.96e+06/1.94e+08 =  2% of the original kernel matrix.

torch.Size([43762, 2])
We keep 5.18e+06/4.74e+08 =  1% of the original kernel matrix.

torch.Size([5426, 2])
We keep 1.43e+05/3.53e+06 =  4% of the original kernel matrix.

torch.Size([20939, 2])
We keep 1.27e+06/6.40e+07 =  1% of the original kernel matrix.

torch.Size([13285, 2])
We keep 1.12e+07/1.27e+08 =  8% of the original kernel matrix.

torch.Size([28740, 2])
We keep 4.49e+06/3.84e+08 =  1% of the original kernel matrix.

torch.Size([207871, 2])
We keep 1.05e+09/3.74e+10 =  2% of the original kernel matrix.

torch.Size([108213, 2])
We keep 4.72e+07/6.59e+09 =  0% of the original kernel matrix.

torch.Size([40690, 2])
We keep 1.43e+07/5.00e+08 =  2% of the original kernel matrix.

torch.Size([52435, 2])
We keep 7.26e+06/7.62e+08 =  0% of the original kernel matrix.

torch.Size([14556, 2])
We keep 1.36e+06/4.13e+07 =  3% of the original kernel matrix.

torch.Size([30920, 2])
We keep 3.01e+06/2.19e+08 =  1% of the original kernel matrix.

torch.Size([61881, 2])
We keep 1.83e+07/1.39e+09 =  1% of the original kernel matrix.

torch.Size([63595, 2])
We keep 1.14e+07/1.27e+09 =  0% of the original kernel matrix.

torch.Size([395772, 2])
We keep 5.64e+08/6.40e+10 =  0% of the original kernel matrix.

torch.Size([163710, 2])
We keep 5.86e+07/8.62e+09 =  0% of the original kernel matrix.

torch.Size([35581, 2])
We keep 1.04e+07/4.63e+08 =  2% of the original kernel matrix.

torch.Size([47537, 2])
We keep 7.39e+06/7.33e+08 =  1% of the original kernel matrix.

torch.Size([6660, 2])
We keep 3.13e+05/8.23e+06 =  3% of the original kernel matrix.

torch.Size([21975, 2])
We keep 1.69e+06/9.77e+07 =  1% of the original kernel matrix.

torch.Size([31609, 2])
We keep 4.10e+06/2.39e+08 =  1% of the original kernel matrix.

torch.Size([46366, 2])
We keep 5.81e+06/5.27e+08 =  1% of the original kernel matrix.

torch.Size([15815, 2])
We keep 1.03e+06/4.52e+07 =  2% of the original kernel matrix.

torch.Size([32515, 2])
We keep 3.12e+06/2.29e+08 =  1% of the original kernel matrix.

torch.Size([6478, 2])
We keep 2.92e+05/5.76e+06 =  5% of the original kernel matrix.

torch.Size([22514, 2])
We keep 1.51e+06/8.17e+07 =  1% of the original kernel matrix.

torch.Size([186735, 2])
We keep 1.24e+08/1.12e+10 =  1% of the original kernel matrix.

torch.Size([110687, 2])
We keep 2.74e+07/3.61e+09 =  0% of the original kernel matrix.

torch.Size([23188, 2])
We keep 4.14e+06/1.58e+08 =  2% of the original kernel matrix.

torch.Size([38593, 2])
We keep 4.90e+06/4.28e+08 =  1% of the original kernel matrix.

torch.Size([3190, 2])
We keep 5.01e+04/9.27e+05 =  5% of the original kernel matrix.

torch.Size([17294, 2])
We keep 8.35e+05/3.28e+07 =  2% of the original kernel matrix.

torch.Size([48658, 2])
We keep 9.47e+06/6.89e+08 =  1% of the original kernel matrix.

torch.Size([58025, 2])
We keep 8.53e+06/8.94e+08 =  0% of the original kernel matrix.

torch.Size([28845, 2])
We keep 2.90e+06/1.81e+08 =  1% of the original kernel matrix.

torch.Size([44020, 2])
We keep 5.20e+06/4.58e+08 =  1% of the original kernel matrix.

torch.Size([2461, 2])
We keep 3.90e+04/6.56e+05 =  5% of the original kernel matrix.

torch.Size([15598, 2])
We keep 7.46e+05/2.76e+07 =  2% of the original kernel matrix.

torch.Size([143517, 2])
We keep 1.37e+08/7.59e+09 =  1% of the original kernel matrix.

torch.Size([95454, 2])
We keep 2.33e+07/2.97e+09 =  0% of the original kernel matrix.

torch.Size([234810, 2])
We keep 1.41e+08/1.69e+10 =  0% of the original kernel matrix.

torch.Size([125138, 2])
We keep 3.30e+07/4.43e+09 =  0% of the original kernel matrix.

torch.Size([133949, 2])
We keep 2.66e+08/9.97e+09 =  2% of the original kernel matrix.

torch.Size([92269, 2])
We keep 2.67e+07/3.40e+09 =  0% of the original kernel matrix.

torch.Size([209669, 2])
We keep 2.62e+08/1.63e+10 =  1% of the original kernel matrix.

torch.Size([117097, 2])
We keep 3.25e+07/4.34e+09 =  0% of the original kernel matrix.

torch.Size([232637, 2])
We keep 2.52e+08/2.21e+10 =  1% of the original kernel matrix.

torch.Size([122285, 2])
We keep 3.71e+07/5.07e+09 =  0% of the original kernel matrix.

torch.Size([18405, 2])
We keep 2.94e+06/9.58e+07 =  3% of the original kernel matrix.

torch.Size([34121, 2])
We keep 4.13e+06/3.33e+08 =  1% of the original kernel matrix.

torch.Size([20713, 2])
We keep 3.18e+06/1.00e+08 =  3% of the original kernel matrix.

torch.Size([36550, 2])
We keep 4.07e+06/3.41e+08 =  1% of the original kernel matrix.

torch.Size([158445, 2])
We keep 6.08e+07/6.58e+09 =  0% of the original kernel matrix.

torch.Size([100843, 2])
We keep 2.19e+07/2.76e+09 =  0% of the original kernel matrix.

torch.Size([300245, 2])
We keep 1.97e+08/2.63e+10 =  0% of the original kernel matrix.

torch.Size([143074, 2])
We keep 3.93e+07/5.52e+09 =  0% of the original kernel matrix.

torch.Size([8005, 2])
We keep 2.95e+05/8.89e+06 =  3% of the original kernel matrix.

torch.Size([24293, 2])
We keep 1.74e+06/1.02e+08 =  1% of the original kernel matrix.

torch.Size([53249, 2])
We keep 1.98e+07/9.30e+08 =  2% of the original kernel matrix.

torch.Size([59702, 2])
We keep 9.60e+06/1.04e+09 =  0% of the original kernel matrix.

torch.Size([301318, 2])
We keep 1.35e+09/5.34e+10 =  2% of the original kernel matrix.

torch.Size([140349, 2])
We keep 5.38e+07/7.87e+09 =  0% of the original kernel matrix.

torch.Size([147174, 2])
We keep 1.78e+08/8.88e+09 =  2% of the original kernel matrix.

torch.Size([97431, 2])
We keep 2.49e+07/3.21e+09 =  0% of the original kernel matrix.

torch.Size([106996, 2])
We keep 1.58e+08/7.53e+09 =  2% of the original kernel matrix.

torch.Size([78095, 2])
We keep 2.33e+07/2.96e+09 =  0% of the original kernel matrix.

torch.Size([54708, 2])
We keep 2.29e+08/5.22e+09 =  4% of the original kernel matrix.

torch.Size([50674, 2])
We keep 2.00e+07/2.46e+09 =  0% of the original kernel matrix.

torch.Size([11875, 2])
We keep 6.04e+05/2.15e+07 =  2% of the original kernel matrix.

torch.Size([28703, 2])
We keep 2.37e+06/1.58e+08 =  1% of the original kernel matrix.

torch.Size([5389, 2])
We keep 2.22e+05/4.72e+06 =  4% of the original kernel matrix.

torch.Size([20448, 2])
We keep 1.41e+06/7.40e+07 =  1% of the original kernel matrix.

torch.Size([24622, 2])
We keep 1.74e+07/3.50e+08 =  4% of the original kernel matrix.

torch.Size([38408, 2])
We keep 6.78e+06/6.37e+08 =  1% of the original kernel matrix.

torch.Size([110388, 2])
We keep 2.81e+07/3.29e+09 =  0% of the original kernel matrix.

torch.Size([85166, 2])
We keep 1.64e+07/1.95e+09 =  0% of the original kernel matrix.

torch.Size([84286, 2])
We keep 1.79e+07/1.99e+09 =  0% of the original kernel matrix.

torch.Size([75309, 2])
We keep 1.33e+07/1.52e+09 =  0% of the original kernel matrix.

torch.Size([840468, 2])
We keep 2.55e+09/3.13e+11 =  0% of the original kernel matrix.

torch.Size([237334, 2])
We keep 1.19e+08/1.91e+10 =  0% of the original kernel matrix.

torch.Size([182758, 2])
We keep 2.06e+08/1.43e+10 =  1% of the original kernel matrix.

torch.Size([108853, 2])
We keep 3.05e+07/4.07e+09 =  0% of the original kernel matrix.

torch.Size([123395, 2])
We keep 1.04e+08/5.06e+09 =  2% of the original kernel matrix.

torch.Size([89136, 2])
We keep 1.94e+07/2.42e+09 =  0% of the original kernel matrix.

torch.Size([28417, 2])
We keep 4.17e+06/2.95e+08 =  1% of the original kernel matrix.

torch.Size([43169, 2])
We keep 6.23e+06/5.86e+08 =  1% of the original kernel matrix.

torch.Size([184561, 2])
We keep 1.09e+08/9.98e+09 =  1% of the original kernel matrix.

torch.Size([109656, 2])
We keep 2.63e+07/3.40e+09 =  0% of the original kernel matrix.

torch.Size([4451, 2])
We keep 1.28e+05/2.85e+06 =  4% of the original kernel matrix.

torch.Size([19115, 2])
We keep 1.21e+06/5.75e+07 =  2% of the original kernel matrix.

torch.Size([10212, 2])
We keep 5.29e+05/1.73e+07 =  3% of the original kernel matrix.

torch.Size([26550, 2])
We keep 2.17e+06/1.42e+08 =  1% of the original kernel matrix.

torch.Size([341569, 2])
We keep 2.56e+08/3.53e+10 =  0% of the original kernel matrix.

torch.Size([153266, 2])
We keep 4.49e+07/6.40e+09 =  0% of the original kernel matrix.

torch.Size([7715, 2])
We keep 2.60e+05/8.20e+06 =  3% of the original kernel matrix.

torch.Size([24080, 2])
We keep 1.69e+06/9.76e+07 =  1% of the original kernel matrix.

torch.Size([10794, 2])
We keep 5.10e+05/1.78e+07 =  2% of the original kernel matrix.

torch.Size([27235, 2])
We keep 2.23e+06/1.44e+08 =  1% of the original kernel matrix.

torch.Size([26514, 2])
We keep 3.53e+06/2.08e+08 =  1% of the original kernel matrix.

torch.Size([41827, 2])
We keep 5.37e+06/4.91e+08 =  1% of the original kernel matrix.

torch.Size([218569, 2])
We keep 7.36e+08/3.53e+10 =  2% of the original kernel matrix.

torch.Size([116291, 2])
We keep 4.56e+07/6.40e+09 =  0% of the original kernel matrix.

torch.Size([49536, 2])
We keep 3.31e+07/8.99e+08 =  3% of the original kernel matrix.

torch.Size([57520, 2])
We keep 9.87e+06/1.02e+09 =  0% of the original kernel matrix.

torch.Size([92713, 2])
We keep 4.06e+07/3.01e+09 =  1% of the original kernel matrix.

torch.Size([77945, 2])
We keep 1.58e+07/1.87e+09 =  0% of the original kernel matrix.

torch.Size([3881, 2])
We keep 2.02e+05/2.96e+06 =  6% of the original kernel matrix.

torch.Size([17563, 2])
We keep 1.20e+06/5.86e+07 =  2% of the original kernel matrix.

torch.Size([8738, 2])
We keep 5.09e+05/1.26e+07 =  4% of the original kernel matrix.

torch.Size([24939, 2])
We keep 1.87e+06/1.21e+08 =  1% of the original kernel matrix.

torch.Size([8170, 2])
We keep 3.57e+05/1.07e+07 =  3% of the original kernel matrix.

torch.Size([24341, 2])
We keep 1.87e+06/1.12e+08 =  1% of the original kernel matrix.

torch.Size([16774, 2])
We keep 4.80e+06/7.27e+07 =  6% of the original kernel matrix.

torch.Size([33165, 2])
We keep 3.73e+06/2.90e+08 =  1% of the original kernel matrix.

torch.Size([51693, 2])
We keep 1.26e+07/7.68e+08 =  1% of the original kernel matrix.

torch.Size([59617, 2])
We keep 8.95e+06/9.44e+08 =  0% of the original kernel matrix.

torch.Size([34137, 2])
We keep 1.20e+07/4.31e+08 =  2% of the original kernel matrix.

torch.Size([47064, 2])
We keep 7.26e+06/7.07e+08 =  1% of the original kernel matrix.

torch.Size([7995, 2])
We keep 1.37e+06/1.13e+07 = 12% of the original kernel matrix.

torch.Size([24233, 2])
We keep 1.92e+06/1.15e+08 =  1% of the original kernel matrix.

torch.Size([157221, 2])
We keep 1.09e+08/8.23e+09 =  1% of the original kernel matrix.

torch.Size([100311, 2])
We keep 2.45e+07/3.09e+09 =  0% of the original kernel matrix.

torch.Size([3730, 2])
We keep 7.17e+04/1.53e+06 =  4% of the original kernel matrix.

torch.Size([18350, 2])
We keep 9.74e+05/4.21e+07 =  2% of the original kernel matrix.

torch.Size([62180, 2])
We keep 2.29e+07/1.14e+09 =  2% of the original kernel matrix.

torch.Size([65127, 2])
We keep 1.05e+07/1.15e+09 =  0% of the original kernel matrix.

torch.Size([11109, 2])
We keep 2.98e+06/4.65e+07 =  6% of the original kernel matrix.

torch.Size([26497, 2])
We keep 3.15e+06/2.32e+08 =  1% of the original kernel matrix.

torch.Size([38826, 2])
We keep 5.39e+06/3.92e+08 =  1% of the original kernel matrix.

torch.Size([51584, 2])
We keep 6.99e+06/6.75e+08 =  1% of the original kernel matrix.

torch.Size([8843, 2])
We keep 4.57e+05/1.29e+07 =  3% of the original kernel matrix.

torch.Size([25045, 2])
We keep 2.00e+06/1.23e+08 =  1% of the original kernel matrix.

torch.Size([19263, 2])
We keep 2.08e+06/7.83e+07 =  2% of the original kernel matrix.

torch.Size([35435, 2])
We keep 3.75e+06/3.01e+08 =  1% of the original kernel matrix.

torch.Size([12196, 2])
We keep 8.03e+05/2.46e+07 =  3% of the original kernel matrix.

torch.Size([28969, 2])
We keep 2.50e+06/1.69e+08 =  1% of the original kernel matrix.

torch.Size([26065, 2])
We keep 3.41e+06/1.52e+08 =  2% of the original kernel matrix.

torch.Size([41594, 2])
We keep 4.89e+06/4.20e+08 =  1% of the original kernel matrix.

torch.Size([134467, 2])
We keep 7.15e+07/6.43e+09 =  1% of the original kernel matrix.

torch.Size([93820, 2])
We keep 2.19e+07/2.73e+09 =  0% of the original kernel matrix.

torch.Size([236185, 2])
We keep 2.96e+08/2.30e+10 =  1% of the original kernel matrix.

torch.Size([123322, 2])
We keep 3.79e+07/5.17e+09 =  0% of the original kernel matrix.

torch.Size([12851, 2])
We keep 8.68e+05/2.96e+07 =  2% of the original kernel matrix.

torch.Size([29665, 2])
We keep 2.70e+06/1.86e+08 =  1% of the original kernel matrix.

torch.Size([7071, 2])
We keep 5.50e+05/1.28e+07 =  4% of the original kernel matrix.

torch.Size([22168, 2])
We keep 1.91e+06/1.22e+08 =  1% of the original kernel matrix.

torch.Size([162707, 2])
We keep 2.03e+08/1.02e+10 =  1% of the original kernel matrix.

torch.Size([102077, 2])
We keep 2.61e+07/3.44e+09 =  0% of the original kernel matrix.

torch.Size([12764, 2])
We keep 8.07e+05/2.81e+07 =  2% of the original kernel matrix.

torch.Size([29231, 2])
We keep 2.63e+06/1.80e+08 =  1% of the original kernel matrix.

torch.Size([853132, 2])
We keep 9.92e+08/2.02e+11 =  0% of the original kernel matrix.

torch.Size([245382, 2])
We keep 9.73e+07/1.53e+10 =  0% of the original kernel matrix.

torch.Size([2318, 2])
We keep 3.20e+04/5.08e+05 =  6% of the original kernel matrix.

torch.Size([15526, 2])
We keep 6.88e+05/2.43e+07 =  2% of the original kernel matrix.

torch.Size([78621, 2])
We keep 2.24e+07/2.03e+09 =  1% of the original kernel matrix.

torch.Size([71771, 2])
We keep 1.34e+07/1.54e+09 =  0% of the original kernel matrix.

torch.Size([81911, 2])
We keep 1.66e+07/1.96e+09 =  0% of the original kernel matrix.

torch.Size([74991, 2])
We keep 1.33e+07/1.51e+09 =  0% of the original kernel matrix.

torch.Size([7301, 2])
We keep 2.60e+05/7.42e+06 =  3% of the original kernel matrix.

torch.Size([23485, 2])
We keep 1.64e+06/9.28e+07 =  1% of the original kernel matrix.

torch.Size([84540, 2])
We keep 6.31e+07/3.62e+09 =  1% of the original kernel matrix.

torch.Size([71415, 2])
We keep 1.73e+07/2.05e+09 =  0% of the original kernel matrix.

torch.Size([20107, 2])
We keep 7.51e+06/2.64e+08 =  2% of the original kernel matrix.

torch.Size([34227, 2])
We keep 6.08e+06/5.53e+08 =  1% of the original kernel matrix.

torch.Size([771986, 2])
We keep 2.05e+09/2.26e+11 =  0% of the original kernel matrix.

torch.Size([229913, 2])
We keep 1.03e+08/1.62e+10 =  0% of the original kernel matrix.

torch.Size([17946, 2])
We keep 1.15e+06/5.37e+07 =  2% of the original kernel matrix.

torch.Size([34408, 2])
We keep 3.30e+06/2.50e+08 =  1% of the original kernel matrix.

torch.Size([21146, 2])
We keep 2.51e+06/1.13e+08 =  2% of the original kernel matrix.

torch.Size([37015, 2])
We keep 4.28e+06/3.62e+08 =  1% of the original kernel matrix.

torch.Size([9047, 2])
We keep 1.00e+06/1.89e+07 =  5% of the original kernel matrix.

torch.Size([25007, 2])
We keep 2.29e+06/1.48e+08 =  1% of the original kernel matrix.

torch.Size([8969, 2])
We keep 3.51e+05/1.16e+07 =  3% of the original kernel matrix.

torch.Size([25484, 2])
We keep 1.94e+06/1.16e+08 =  1% of the original kernel matrix.

torch.Size([13288, 2])
We keep 1.20e+06/3.90e+07 =  3% of the original kernel matrix.

torch.Size([29441, 2])
We keep 2.94e+06/2.13e+08 =  1% of the original kernel matrix.

torch.Size([28553, 2])
We keep 3.19e+06/2.06e+08 =  1% of the original kernel matrix.

torch.Size([43643, 2])
We keep 5.31e+06/4.89e+08 =  1% of the original kernel matrix.

torch.Size([9551, 2])
We keep 1.02e+06/1.93e+07 =  5% of the original kernel matrix.

torch.Size([25680, 2])
We keep 2.21e+06/1.50e+08 =  1% of the original kernel matrix.

torch.Size([47384, 2])
We keep 2.13e+07/7.85e+08 =  2% of the original kernel matrix.

torch.Size([56086, 2])
We keep 8.79e+06/9.54e+08 =  0% of the original kernel matrix.

torch.Size([30372, 2])
We keep 3.47e+06/2.33e+08 =  1% of the original kernel matrix.

torch.Size([44922, 2])
We keep 5.72e+06/5.19e+08 =  1% of the original kernel matrix.

torch.Size([4586, 2])
We keep 1.16e+05/2.73e+06 =  4% of the original kernel matrix.

torch.Size([19462, 2])
We keep 1.16e+06/5.63e+07 =  2% of the original kernel matrix.

torch.Size([22320, 2])
We keep 5.38e+06/1.89e+08 =  2% of the original kernel matrix.

torch.Size([37493, 2])
We keep 5.38e+06/4.69e+08 =  1% of the original kernel matrix.

torch.Size([244425, 2])
We keep 7.63e+08/4.89e+10 =  1% of the original kernel matrix.

torch.Size([124128, 2])
We keep 5.22e+07/7.53e+09 =  0% of the original kernel matrix.

torch.Size([36760, 2])
We keep 3.99e+07/1.27e+09 =  3% of the original kernel matrix.

torch.Size([46588, 2])
We keep 1.15e+07/1.21e+09 =  0% of the original kernel matrix.

torch.Size([217794, 2])
We keep 1.20e+08/1.38e+10 =  0% of the original kernel matrix.

torch.Size([119936, 2])
We keep 2.99e+07/4.00e+09 =  0% of the original kernel matrix.

torch.Size([28478, 2])
We keep 5.68e+06/1.94e+08 =  2% of the original kernel matrix.

torch.Size([43319, 2])
We keep 5.05e+06/4.74e+08 =  1% of the original kernel matrix.

torch.Size([20919, 2])
We keep 3.24e+06/1.29e+08 =  2% of the original kernel matrix.

torch.Size([36987, 2])
We keep 4.50e+06/3.87e+08 =  1% of the original kernel matrix.

torch.Size([4975, 2])
We keep 1.32e+05/3.28e+06 =  4% of the original kernel matrix.

torch.Size([20400, 2])
We keep 1.26e+06/6.17e+07 =  2% of the original kernel matrix.

torch.Size([7103, 2])
We keep 2.26e+06/2.05e+07 = 11% of the original kernel matrix.

torch.Size([21748, 2])
We keep 2.15e+06/1.54e+08 =  1% of the original kernel matrix.

torch.Size([279772, 2])
We keep 2.62e+08/2.46e+10 =  1% of the original kernel matrix.

torch.Size([137685, 2])
We keep 3.75e+07/5.35e+09 =  0% of the original kernel matrix.

torch.Size([18723, 2])
We keep 1.35e+06/6.07e+07 =  2% of the original kernel matrix.

torch.Size([35074, 2])
We keep 3.44e+06/2.65e+08 =  1% of the original kernel matrix.

torch.Size([25278, 2])
We keep 3.44e+06/1.53e+08 =  2% of the original kernel matrix.

torch.Size([40860, 2])
We keep 4.87e+06/4.22e+08 =  1% of the original kernel matrix.

torch.Size([32576, 2])
We keep 8.72e+06/4.22e+08 =  2% of the original kernel matrix.

torch.Size([45652, 2])
We keep 7.12e+06/7.00e+08 =  1% of the original kernel matrix.

torch.Size([39711, 2])
We keep 1.90e+07/9.04e+08 =  2% of the original kernel matrix.

torch.Size([49395, 2])
We keep 9.75e+06/1.02e+09 =  0% of the original kernel matrix.

torch.Size([4949, 2])
We keep 1.58e+05/3.50e+06 =  4% of the original kernel matrix.

torch.Size([20126, 2])
We keep 1.26e+06/6.38e+07 =  1% of the original kernel matrix.

torch.Size([5451, 2])
We keep 1.88e+05/4.25e+06 =  4% of the original kernel matrix.

torch.Size([20690, 2])
We keep 1.36e+06/7.02e+07 =  1% of the original kernel matrix.

torch.Size([434619, 2])
We keep 1.02e+09/9.80e+10 =  1% of the original kernel matrix.

torch.Size([169268, 2])
We keep 7.03e+07/1.07e+10 =  0% of the original kernel matrix.

torch.Size([18214, 2])
We keep 1.89e+06/6.88e+07 =  2% of the original kernel matrix.

torch.Size([34675, 2])
We keep 3.59e+06/2.83e+08 =  1% of the original kernel matrix.

torch.Size([18046, 2])
We keep 8.75e+06/1.23e+08 =  7% of the original kernel matrix.

torch.Size([33984, 2])
We keep 4.43e+06/3.78e+08 =  1% of the original kernel matrix.

torch.Size([19730, 2])
We keep 3.14e+06/8.45e+07 =  3% of the original kernel matrix.

torch.Size([36218, 2])
We keep 3.86e+06/3.13e+08 =  1% of the original kernel matrix.

torch.Size([129009, 2])
We keep 2.49e+08/6.49e+09 =  3% of the original kernel matrix.

torch.Size([91970, 2])
We keep 2.21e+07/2.74e+09 =  0% of the original kernel matrix.

torch.Size([229721, 2])
We keep 3.90e+08/1.86e+10 =  2% of the original kernel matrix.

torch.Size([123236, 2])
We keep 3.40e+07/4.65e+09 =  0% of the original kernel matrix.

torch.Size([164598, 2])
We keep 1.91e+08/1.52e+10 =  1% of the original kernel matrix.

torch.Size([100052, 2])
We keep 3.14e+07/4.20e+09 =  0% of the original kernel matrix.

torch.Size([130616, 2])
We keep 6.97e+07/4.65e+09 =  1% of the original kernel matrix.

torch.Size([92099, 2])
We keep 1.88e+07/2.32e+09 =  0% of the original kernel matrix.

torch.Size([10833, 2])
We keep 1.17e+06/2.19e+07 =  5% of the original kernel matrix.

torch.Size([27630, 2])
We keep 2.30e+06/1.59e+08 =  1% of the original kernel matrix.

torch.Size([79582, 2])
We keep 1.72e+07/1.86e+09 =  0% of the original kernel matrix.

torch.Size([72937, 2])
We keep 1.29e+07/1.47e+09 =  0% of the original kernel matrix.

torch.Size([31053, 2])
We keep 2.45e+07/5.80e+08 =  4% of the original kernel matrix.

torch.Size([43900, 2])
We keep 8.31e+06/8.20e+08 =  1% of the original kernel matrix.

torch.Size([13673, 2])
We keep 7.08e+05/2.83e+07 =  2% of the original kernel matrix.

torch.Size([30466, 2])
We keep 2.61e+06/1.81e+08 =  1% of the original kernel matrix.

torch.Size([24915, 2])
We keep 4.46e+06/2.07e+08 =  2% of the original kernel matrix.

torch.Size([40375, 2])
We keep 5.31e+06/4.90e+08 =  1% of the original kernel matrix.

torch.Size([8615, 2])
We keep 3.23e+05/1.06e+07 =  3% of the original kernel matrix.

torch.Size([25132, 2])
We keep 1.84e+06/1.11e+08 =  1% of the original kernel matrix.

torch.Size([27968, 2])
We keep 1.70e+07/4.18e+08 =  4% of the original kernel matrix.

torch.Size([42252, 2])
We keep 7.05e+06/6.96e+08 =  1% of the original kernel matrix.

torch.Size([13429, 2])
We keep 8.78e+05/3.20e+07 =  2% of the original kernel matrix.

torch.Size([29884, 2])
We keep 2.74e+06/1.93e+08 =  1% of the original kernel matrix.

torch.Size([1982, 2])
We keep 2.60e+04/3.83e+05 =  6% of the original kernel matrix.

torch.Size([14573, 2])
We keep 6.47e+05/2.11e+07 =  3% of the original kernel matrix.

torch.Size([13397, 2])
We keep 9.19e+05/3.32e+07 =  2% of the original kernel matrix.

torch.Size([29747, 2])
We keep 2.71e+06/1.96e+08 =  1% of the original kernel matrix.

torch.Size([7523, 2])
We keep 8.15e+05/1.85e+07 =  4% of the original kernel matrix.

torch.Size([22342, 2])
We keep 2.20e+06/1.46e+08 =  1% of the original kernel matrix.

torch.Size([55465, 2])
We keep 9.42e+07/3.58e+09 =  2% of the original kernel matrix.

torch.Size([55211, 2])
We keep 1.73e+07/2.04e+09 =  0% of the original kernel matrix.

torch.Size([11065, 2])
We keep 9.20e+05/2.71e+07 =  3% of the original kernel matrix.

torch.Size([27011, 2])
We keep 2.56e+06/1.77e+08 =  1% of the original kernel matrix.

torch.Size([42154, 2])
We keep 1.20e+07/5.30e+08 =  2% of the original kernel matrix.

torch.Size([53443, 2])
We keep 7.89e+06/7.84e+08 =  1% of the original kernel matrix.

torch.Size([9629, 2])
We keep 5.87e+06/3.72e+07 = 15% of the original kernel matrix.

torch.Size([24755, 2])
We keep 2.85e+06/2.08e+08 =  1% of the original kernel matrix.

torch.Size([30095, 2])
We keep 6.05e+06/2.74e+08 =  2% of the original kernel matrix.

torch.Size([44428, 2])
We keep 6.09e+06/5.64e+08 =  1% of the original kernel matrix.

torch.Size([6702, 2])
We keep 2.65e+05/6.87e+06 =  3% of the original kernel matrix.

torch.Size([22576, 2])
We keep 1.61e+06/8.93e+07 =  1% of the original kernel matrix.

torch.Size([15859, 2])
We keep 1.05e+06/4.24e+07 =  2% of the original kernel matrix.

torch.Size([32267, 2])
We keep 3.02e+06/2.22e+08 =  1% of the original kernel matrix.

torch.Size([25802, 2])
We keep 2.24e+06/1.31e+08 =  1% of the original kernel matrix.

torch.Size([41364, 2])
We keep 4.52e+06/3.90e+08 =  1% of the original kernel matrix.

torch.Size([184924, 2])
We keep 1.33e+08/1.13e+10 =  1% of the original kernel matrix.

torch.Size([109964, 2])
We keep 2.75e+07/3.63e+09 =  0% of the original kernel matrix.

torch.Size([17609, 2])
We keep 1.34e+06/5.80e+07 =  2% of the original kernel matrix.

torch.Size([34145, 2])
We keep 3.41e+06/2.59e+08 =  1% of the original kernel matrix.

torch.Size([4869, 2])
We keep 1.40e+05/2.98e+06 =  4% of the original kernel matrix.

torch.Size([20078, 2])
We keep 1.19e+06/5.88e+07 =  2% of the original kernel matrix.

torch.Size([9677, 2])
We keep 6.27e+05/1.78e+07 =  3% of the original kernel matrix.

torch.Size([25821, 2])
We keep 2.24e+06/1.44e+08 =  1% of the original kernel matrix.

torch.Size([291558, 2])
We keep 4.46e+08/3.55e+10 =  1% of the original kernel matrix.

torch.Size([140601, 2])
We keep 4.45e+07/6.42e+09 =  0% of the original kernel matrix.

torch.Size([812559, 2])
We keep 1.88e+09/1.97e+11 =  0% of the original kernel matrix.

torch.Size([238573, 2])
We keep 9.82e+07/1.51e+10 =  0% of the original kernel matrix.

torch.Size([8368, 2])
We keep 6.23e+05/1.48e+07 =  4% of the original kernel matrix.

torch.Size([23697, 2])
We keep 2.08e+06/1.31e+08 =  1% of the original kernel matrix.

torch.Size([16299, 2])
We keep 1.12e+06/4.72e+07 =  2% of the original kernel matrix.

torch.Size([32878, 2])
We keep 3.17e+06/2.34e+08 =  1% of the original kernel matrix.

torch.Size([20163, 2])
We keep 2.86e+06/1.02e+08 =  2% of the original kernel matrix.

torch.Size([36201, 2])
We keep 4.20e+06/3.44e+08 =  1% of the original kernel matrix.

torch.Size([198621, 2])
We keep 8.09e+07/1.08e+10 =  0% of the original kernel matrix.

torch.Size([114098, 2])
We keep 2.71e+07/3.54e+09 =  0% of the original kernel matrix.

torch.Size([9298, 2])
We keep 3.67e+05/1.23e+07 =  2% of the original kernel matrix.

torch.Size([25608, 2])
We keep 1.96e+06/1.20e+08 =  1% of the original kernel matrix.

torch.Size([15173, 2])
We keep 2.05e+06/5.88e+07 =  3% of the original kernel matrix.

torch.Size([31759, 2])
We keep 3.41e+06/2.61e+08 =  1% of the original kernel matrix.

torch.Size([16402, 2])
We keep 9.20e+05/4.13e+07 =  2% of the original kernel matrix.

torch.Size([32956, 2])
We keep 2.99e+06/2.19e+08 =  1% of the original kernel matrix.

torch.Size([89613, 2])
We keep 2.97e+08/8.46e+09 =  3% of the original kernel matrix.

torch.Size([71165, 2])
We keep 2.48e+07/3.13e+09 =  0% of the original kernel matrix.

torch.Size([64440, 2])
We keep 1.29e+08/2.93e+09 =  4% of the original kernel matrix.

torch.Size([62911, 2])
We keep 1.60e+07/1.84e+09 =  0% of the original kernel matrix.

torch.Size([306390, 2])
We keep 2.36e+08/2.89e+10 =  0% of the original kernel matrix.

torch.Size([144498, 2])
We keep 4.14e+07/5.79e+09 =  0% of the original kernel matrix.

torch.Size([13649, 2])
We keep 7.56e+05/2.88e+07 =  2% of the original kernel matrix.

torch.Size([30350, 2])
We keep 2.62e+06/1.83e+08 =  1% of the original kernel matrix.

torch.Size([10610, 2])
We keep 4.48e+05/1.61e+07 =  2% of the original kernel matrix.

torch.Size([27609, 2])
We keep 2.15e+06/1.37e+08 =  1% of the original kernel matrix.

torch.Size([3828, 2])
We keep 7.23e+04/1.67e+06 =  4% of the original kernel matrix.

torch.Size([18345, 2])
We keep 1.00e+06/4.40e+07 =  2% of the original kernel matrix.

torch.Size([45895, 2])
We keep 1.57e+07/7.97e+08 =  1% of the original kernel matrix.

torch.Size([55219, 2])
We keep 9.39e+06/9.62e+08 =  0% of the original kernel matrix.

torch.Size([21507, 2])
We keep 2.22e+06/9.02e+07 =  2% of the original kernel matrix.

torch.Size([37724, 2])
We keep 3.96e+06/3.24e+08 =  1% of the original kernel matrix.

torch.Size([41252, 2])
We keep 2.05e+07/8.61e+08 =  2% of the original kernel matrix.

torch.Size([50815, 2])
We keep 9.35e+06/1.00e+09 =  0% of the original kernel matrix.

torch.Size([216252, 2])
We keep 2.64e+08/1.76e+10 =  1% of the original kernel matrix.

torch.Size([119335, 2])
We keep 3.33e+07/4.52e+09 =  0% of the original kernel matrix.

torch.Size([856106, 2])
We keep 1.16e+09/2.04e+11 =  0% of the original kernel matrix.

torch.Size([244281, 2])
We keep 9.84e+07/1.54e+10 =  0% of the original kernel matrix.

torch.Size([12402, 2])
We keep 1.17e+06/3.41e+07 =  3% of the original kernel matrix.

torch.Size([28415, 2])
We keep 2.79e+06/1.99e+08 =  1% of the original kernel matrix.

torch.Size([4775, 2])
We keep 1.18e+05/2.80e+06 =  4% of the original kernel matrix.

torch.Size([19808, 2])
We keep 1.19e+06/5.70e+07 =  2% of the original kernel matrix.

torch.Size([14981, 2])
We keep 9.12e+05/3.67e+07 =  2% of the original kernel matrix.

torch.Size([31591, 2])
We keep 2.88e+06/2.06e+08 =  1% of the original kernel matrix.

torch.Size([395175, 2])
We keep 3.11e+09/9.54e+10 =  3% of the original kernel matrix.

torch.Size([165104, 2])
We keep 7.04e+07/1.05e+10 =  0% of the original kernel matrix.

torch.Size([41092, 2])
We keep 7.41e+06/4.58e+08 =  1% of the original kernel matrix.

torch.Size([52633, 2])
We keep 7.35e+06/7.29e+08 =  1% of the original kernel matrix.

torch.Size([2906, 2])
We keep 5.88e+04/9.62e+05 =  6% of the original kernel matrix.

torch.Size([16496, 2])
We keep 8.37e+05/3.34e+07 =  2% of the original kernel matrix.

torch.Size([7239, 2])
We keep 3.61e+05/8.74e+06 =  4% of the original kernel matrix.

torch.Size([23128, 2])
We keep 1.74e+06/1.01e+08 =  1% of the original kernel matrix.

torch.Size([29630, 2])
We keep 5.31e+06/2.18e+08 =  2% of the original kernel matrix.

torch.Size([44216, 2])
We keep 5.44e+06/5.03e+08 =  1% of the original kernel matrix.

torch.Size([2751, 2])
We keep 4.40e+04/8.28e+05 =  5% of the original kernel matrix.

torch.Size([16373, 2])
We keep 7.97e+05/3.10e+07 =  2% of the original kernel matrix.

torch.Size([50187, 2])
We keep 5.93e+07/2.00e+09 =  2% of the original kernel matrix.

torch.Size([53752, 2])
We keep 1.35e+07/1.52e+09 =  0% of the original kernel matrix.

torch.Size([8835, 2])
We keep 3.59e+05/1.19e+07 =  3% of the original kernel matrix.

torch.Size([25455, 2])
We keep 1.94e+06/1.18e+08 =  1% of the original kernel matrix.

torch.Size([31126, 2])
We keep 3.09e+06/2.16e+08 =  1% of the original kernel matrix.

torch.Size([45581, 2])
We keep 5.55e+06/5.01e+08 =  1% of the original kernel matrix.

torch.Size([8847, 2])
We keep 5.14e+05/1.47e+07 =  3% of the original kernel matrix.

torch.Size([24783, 2])
We keep 2.09e+06/1.31e+08 =  1% of the original kernel matrix.

torch.Size([7506, 2])
We keep 2.59e+05/7.88e+06 =  3% of the original kernel matrix.

torch.Size([23706, 2])
We keep 1.67e+06/9.56e+07 =  1% of the original kernel matrix.

torch.Size([4531, 2])
We keep 1.12e+05/2.53e+06 =  4% of the original kernel matrix.

torch.Size([19447, 2])
We keep 1.15e+06/5.42e+07 =  2% of the original kernel matrix.

torch.Size([269257, 2])
We keep 1.59e+08/2.09e+10 =  0% of the original kernel matrix.

torch.Size([134819, 2])
We keep 3.57e+07/4.93e+09 =  0% of the original kernel matrix.

torch.Size([687044, 2])
We keep 9.89e+08/1.33e+11 =  0% of the original kernel matrix.

torch.Size([219644, 2])
We keep 8.16e+07/1.24e+10 =  0% of the original kernel matrix.

torch.Size([153567, 2])
We keep 1.62e+08/8.85e+09 =  1% of the original kernel matrix.

torch.Size([99957, 2])
We keep 2.47e+07/3.21e+09 =  0% of the original kernel matrix.

torch.Size([94792, 2])
We keep 1.71e+08/4.32e+09 =  3% of the original kernel matrix.

torch.Size([75964, 2])
We keep 1.86e+07/2.24e+09 =  0% of the original kernel matrix.

torch.Size([47540, 2])
We keep 7.10e+06/6.13e+08 =  1% of the original kernel matrix.

torch.Size([56840, 2])
We keep 8.27e+06/8.44e+08 =  0% of the original kernel matrix.

torch.Size([10386, 2])
We keep 7.56e+05/2.00e+07 =  3% of the original kernel matrix.

torch.Size([26753, 2])
We keep 2.29e+06/1.52e+08 =  1% of the original kernel matrix.

torch.Size([22430, 2])
We keep 1.85e+06/9.68e+07 =  1% of the original kernel matrix.

torch.Size([38154, 2])
We keep 4.10e+06/3.35e+08 =  1% of the original kernel matrix.

torch.Size([146272, 2])
We keep 4.94e+07/5.42e+09 =  0% of the original kernel matrix.

torch.Size([96565, 2])
We keep 2.02e+07/2.51e+09 =  0% of the original kernel matrix.

torch.Size([88338, 2])
We keep 1.07e+08/3.71e+09 =  2% of the original kernel matrix.

torch.Size([75732, 2])
We keep 1.75e+07/2.08e+09 =  0% of the original kernel matrix.

torch.Size([108190, 2])
We keep 1.58e+08/5.80e+09 =  2% of the original kernel matrix.

torch.Size([82624, 2])
We keep 2.05e+07/2.59e+09 =  0% of the original kernel matrix.

torch.Size([20503, 2])
We keep 2.92e+06/1.09e+08 =  2% of the original kernel matrix.

torch.Size([36322, 2])
We keep 4.32e+06/3.56e+08 =  1% of the original kernel matrix.

torch.Size([986199, 2])
We keep 2.08e+09/3.12e+11 =  0% of the original kernel matrix.

torch.Size([263715, 2])
We keep 1.20e+08/1.90e+10 =  0% of the original kernel matrix.

torch.Size([24569, 2])
We keep 3.32e+06/1.28e+08 =  2% of the original kernel matrix.

torch.Size([40089, 2])
We keep 4.58e+06/3.85e+08 =  1% of the original kernel matrix.

torch.Size([74150, 2])
We keep 2.32e+07/1.94e+09 =  1% of the original kernel matrix.

torch.Size([70715, 2])
We keep 1.33e+07/1.50e+09 =  0% of the original kernel matrix.

torch.Size([160109, 2])
We keep 9.56e+07/8.80e+09 =  1% of the original kernel matrix.

torch.Size([102519, 2])
We keep 2.47e+07/3.20e+09 =  0% of the original kernel matrix.

torch.Size([67043, 2])
We keep 1.28e+07/1.24e+09 =  1% of the original kernel matrix.

torch.Size([68622, 2])
We keep 1.10e+07/1.20e+09 =  0% of the original kernel matrix.

torch.Size([75603, 2])
We keep 1.30e+08/3.53e+09 =  3% of the original kernel matrix.

torch.Size([68408, 2])
We keep 1.68e+07/2.02e+09 =  0% of the original kernel matrix.

torch.Size([212845, 2])
We keep 2.13e+08/1.65e+10 =  1% of the original kernel matrix.

torch.Size([117953, 2])
We keep 3.29e+07/4.38e+09 =  0% of the original kernel matrix.

torch.Size([39197, 2])
We keep 5.68e+06/4.06e+08 =  1% of the original kernel matrix.

torch.Size([51600, 2])
We keep 7.12e+06/6.87e+08 =  1% of the original kernel matrix.

torch.Size([26504, 2])
We keep 4.15e+06/1.83e+08 =  2% of the original kernel matrix.

torch.Size([41754, 2])
We keep 5.21e+06/4.61e+08 =  1% of the original kernel matrix.

torch.Size([8020, 2])
We keep 8.77e+05/1.37e+07 =  6% of the original kernel matrix.

torch.Size([23738, 2])
We keep 1.92e+06/1.26e+08 =  1% of the original kernel matrix.

torch.Size([64704, 2])
We keep 2.34e+07/1.43e+09 =  1% of the original kernel matrix.

torch.Size([65734, 2])
We keep 1.15e+07/1.29e+09 =  0% of the original kernel matrix.

torch.Size([18717, 2])
We keep 2.65e+06/8.23e+07 =  3% of the original kernel matrix.

torch.Size([35452, 2])
We keep 3.74e+06/3.09e+08 =  1% of the original kernel matrix.

torch.Size([193490, 2])
We keep 1.69e+08/1.04e+10 =  1% of the original kernel matrix.

torch.Size([112404, 2])
We keep 2.67e+07/3.47e+09 =  0% of the original kernel matrix.

torch.Size([1872313, 2])
We keep 4.59e+09/8.77e+11 =  0% of the original kernel matrix.

torch.Size([371425, 2])
We keep 1.91e+08/3.19e+10 =  0% of the original kernel matrix.

torch.Size([161596, 2])
We keep 2.96e+08/1.09e+10 =  2% of the original kernel matrix.

torch.Size([102249, 2])
We keep 2.73e+07/3.56e+09 =  0% of the original kernel matrix.

torch.Size([230682, 2])
We keep 3.38e+08/3.18e+10 =  1% of the original kernel matrix.

torch.Size([118421, 2])
We keep 4.35e+07/6.07e+09 =  0% of the original kernel matrix.

torch.Size([38514, 2])
We keep 6.21e+07/8.59e+08 =  7% of the original kernel matrix.

torch.Size([49705, 2])
We keep 9.44e+06/9.99e+08 =  0% of the original kernel matrix.

torch.Size([27824, 2])
We keep 1.43e+07/4.18e+08 =  3% of the original kernel matrix.

torch.Size([41551, 2])
We keep 7.23e+06/6.96e+08 =  1% of the original kernel matrix.

torch.Size([49902, 2])
We keep 1.03e+07/7.61e+08 =  1% of the original kernel matrix.

torch.Size([59345, 2])
We keep 9.29e+06/9.40e+08 =  0% of the original kernel matrix.

torch.Size([13948, 2])
We keep 1.60e+06/5.36e+07 =  2% of the original kernel matrix.

torch.Size([30151, 2])
We keep 3.26e+06/2.50e+08 =  1% of the original kernel matrix.

torch.Size([89030, 2])
We keep 1.32e+08/4.34e+09 =  3% of the original kernel matrix.

torch.Size([74174, 2])
We keep 1.88e+07/2.25e+09 =  0% of the original kernel matrix.

torch.Size([307889, 2])
We keep 1.57e+08/2.68e+10 =  0% of the original kernel matrix.

torch.Size([145117, 2])
We keep 3.99e+07/5.58e+09 =  0% of the original kernel matrix.

torch.Size([17361, 2])
We keep 7.63e+06/2.01e+08 =  3% of the original kernel matrix.

torch.Size([31980, 2])
We keep 5.45e+06/4.83e+08 =  1% of the original kernel matrix.

torch.Size([663699, 2])
We keep 4.42e+09/2.75e+11 =  1% of the original kernel matrix.

torch.Size([203335, 2])
We keep 1.15e+08/1.79e+10 =  0% of the original kernel matrix.

torch.Size([228493, 2])
We keep 6.27e+08/3.46e+10 =  1% of the original kernel matrix.

torch.Size([120805, 2])
We keep 4.42e+07/6.34e+09 =  0% of the original kernel matrix.

torch.Size([39068, 2])
We keep 5.30e+06/3.87e+08 =  1% of the original kernel matrix.

torch.Size([51276, 2])
We keep 6.86e+06/6.70e+08 =  1% of the original kernel matrix.

torch.Size([4263, 2])
We keep 9.11e+04/2.11e+06 =  4% of the original kernel matrix.

torch.Size([19284, 2])
We keep 1.08e+06/4.94e+07 =  2% of the original kernel matrix.

torch.Size([43945, 2])
We keep 1.25e+07/6.01e+08 =  2% of the original kernel matrix.

torch.Size([54528, 2])
We keep 8.19e+06/8.35e+08 =  0% of the original kernel matrix.

torch.Size([95310, 2])
We keep 2.57e+07/2.61e+09 =  0% of the original kernel matrix.

torch.Size([79601, 2])
We keep 1.48e+07/1.74e+09 =  0% of the original kernel matrix.

torch.Size([8021, 2])
We keep 3.57e+05/1.03e+07 =  3% of the original kernel matrix.

torch.Size([24203, 2])
We keep 1.81e+06/1.09e+08 =  1% of the original kernel matrix.

torch.Size([750691, 2])
We keep 7.11e+08/1.41e+11 =  0% of the original kernel matrix.

torch.Size([228492, 2])
We keep 8.30e+07/1.28e+10 =  0% of the original kernel matrix.

torch.Size([38550, 2])
We keep 2.69e+07/7.31e+08 =  3% of the original kernel matrix.

torch.Size([50445, 2])
We keep 8.65e+06/9.21e+08 =  0% of the original kernel matrix.

torch.Size([41113, 2])
We keep 8.41e+06/4.98e+08 =  1% of the original kernel matrix.

torch.Size([52480, 2])
We keep 7.60e+06/7.61e+08 =  0% of the original kernel matrix.

torch.Size([15635, 2])
We keep 8.62e+05/3.84e+07 =  2% of the original kernel matrix.

torch.Size([32537, 2])
We keep 2.91e+06/2.11e+08 =  1% of the original kernel matrix.

torch.Size([6665, 2])
We keep 2.17e+05/6.08e+06 =  3% of the original kernel matrix.

torch.Size([22485, 2])
We keep 1.53e+06/8.40e+07 =  1% of the original kernel matrix.

torch.Size([6774, 2])
We keep 2.34e+05/6.00e+06 =  3% of the original kernel matrix.

torch.Size([22865, 2])
We keep 1.51e+06/8.35e+07 =  1% of the original kernel matrix.

torch.Size([13353, 2])
We keep 3.38e+06/4.76e+07 =  7% of the original kernel matrix.

torch.Size([29577, 2])
We keep 3.18e+06/2.35e+08 =  1% of the original kernel matrix.

torch.Size([19256, 2])
We keep 2.51e+06/7.75e+07 =  3% of the original kernel matrix.

torch.Size([35706, 2])
We keep 3.82e+06/3.00e+08 =  1% of the original kernel matrix.

torch.Size([30440, 2])
We keep 5.25e+06/2.31e+08 =  2% of the original kernel matrix.

torch.Size([44792, 2])
We keep 5.59e+06/5.18e+08 =  1% of the original kernel matrix.

torch.Size([4141414, 2])
We keep 5.14e+10/7.55e+12 =  0% of the original kernel matrix.

torch.Size([517891, 2])
We keep 5.22e+08/9.36e+10 =  0% of the original kernel matrix.

torch.Size([18011, 2])
We keep 2.44e+06/8.28e+07 =  2% of the original kernel matrix.

torch.Size([34320, 2])
We keep 3.81e+06/3.10e+08 =  1% of the original kernel matrix.

torch.Size([25362, 2])
We keep 2.12e+06/1.27e+08 =  1% of the original kernel matrix.

torch.Size([40763, 2])
We keep 4.51e+06/3.84e+08 =  1% of the original kernel matrix.

torch.Size([8134, 2])
We keep 3.66e+05/1.01e+07 =  3% of the original kernel matrix.

torch.Size([24380, 2])
We keep 1.82e+06/1.08e+08 =  1% of the original kernel matrix.

torch.Size([20610, 2])
We keep 1.69e+06/7.98e+07 =  2% of the original kernel matrix.

torch.Size([36497, 2])
We keep 3.86e+06/3.04e+08 =  1% of the original kernel matrix.

torch.Size([13308, 2])
We keep 8.75e+05/3.30e+07 =  2% of the original kernel matrix.

torch.Size([29847, 2])
We keep 2.78e+06/1.96e+08 =  1% of the original kernel matrix.

torch.Size([181554, 2])
We keep 4.97e+08/2.93e+10 =  1% of the original kernel matrix.

torch.Size([101721, 2])
We keep 4.13e+07/5.83e+09 =  0% of the original kernel matrix.

torch.Size([57280, 2])
We keep 8.14e+07/2.23e+09 =  3% of the original kernel matrix.

torch.Size([59526, 2])
We keep 1.43e+07/1.61e+09 =  0% of the original kernel matrix.

torch.Size([1320763, 2])
We keep 2.00e+09/4.75e+11 =  0% of the original kernel matrix.

torch.Size([314026, 2])
We keep 1.44e+08/2.35e+10 =  0% of the original kernel matrix.

torch.Size([8261, 2])
We keep 3.02e+05/9.78e+06 =  3% of the original kernel matrix.

torch.Size([24652, 2])
We keep 1.82e+06/1.07e+08 =  1% of the original kernel matrix.

torch.Size([290409, 2])
We keep 2.40e+08/2.59e+10 =  0% of the original kernel matrix.

torch.Size([140597, 2])
We keep 3.94e+07/5.48e+09 =  0% of the original kernel matrix.

torch.Size([45234, 2])
We keep 2.87e+07/7.29e+08 =  3% of the original kernel matrix.

torch.Size([55227, 2])
We keep 8.89e+06/9.20e+08 =  0% of the original kernel matrix.

torch.Size([205117, 2])
We keep 1.46e+08/1.49e+10 =  0% of the original kernel matrix.

torch.Size([115877, 2])
We keep 3.10e+07/4.16e+09 =  0% of the original kernel matrix.

torch.Size([536997, 2])
We keep 1.04e+09/1.28e+11 =  0% of the original kernel matrix.

torch.Size([192168, 2])
We keep 8.04e+07/1.22e+10 =  0% of the original kernel matrix.

torch.Size([26634, 2])
We keep 3.43e+06/1.60e+08 =  2% of the original kernel matrix.

torch.Size([41680, 2])
We keep 4.98e+06/4.31e+08 =  1% of the original kernel matrix.

torch.Size([7678, 2])
We keep 3.41e+05/8.81e+06 =  3% of the original kernel matrix.

torch.Size([23754, 2])
We keep 1.75e+06/1.01e+08 =  1% of the original kernel matrix.

torch.Size([100975, 2])
We keep 2.52e+07/2.88e+09 =  0% of the original kernel matrix.

torch.Size([81779, 2])
We keep 1.54e+07/1.83e+09 =  0% of the original kernel matrix.

torch.Size([18149, 2])
We keep 3.96e+06/8.32e+07 =  4% of the original kernel matrix.

torch.Size([34879, 2])
We keep 3.79e+06/3.11e+08 =  1% of the original kernel matrix.

torch.Size([43744, 2])
We keep 1.93e+07/8.42e+08 =  2% of the original kernel matrix.

torch.Size([52440, 2])
We keep 9.66e+06/9.89e+08 =  0% of the original kernel matrix.

torch.Size([35557, 2])
We keep 5.35e+06/3.36e+08 =  1% of the original kernel matrix.

torch.Size([48581, 2])
We keep 6.61e+06/6.24e+08 =  1% of the original kernel matrix.

torch.Size([18402, 2])
We keep 1.66e+07/2.25e+08 =  7% of the original kernel matrix.

torch.Size([34079, 2])
We keep 5.75e+06/5.11e+08 =  1% of the original kernel matrix.

torch.Size([79663, 2])
We keep 3.57e+07/2.05e+09 =  1% of the original kernel matrix.

torch.Size([74263, 2])
We keep 1.36e+07/1.54e+09 =  0% of the original kernel matrix.

torch.Size([21631, 2])
We keep 3.35e+06/1.16e+08 =  2% of the original kernel matrix.

torch.Size([37121, 2])
We keep 4.40e+06/3.66e+08 =  1% of the original kernel matrix.

torch.Size([149916, 2])
We keep 1.07e+08/7.47e+09 =  1% of the original kernel matrix.

torch.Size([98234, 2])
We keep 2.26e+07/2.95e+09 =  0% of the original kernel matrix.

torch.Size([224432, 2])
We keep 2.41e+08/2.08e+10 =  1% of the original kernel matrix.

torch.Size([119759, 2])
We keep 3.55e+07/4.91e+09 =  0% of the original kernel matrix.

torch.Size([19628, 2])
We keep 1.96e+06/8.07e+07 =  2% of the original kernel matrix.

torch.Size([35692, 2])
We keep 3.76e+06/3.06e+08 =  1% of the original kernel matrix.

torch.Size([18485, 2])
We keep 1.61e+06/6.99e+07 =  2% of the original kernel matrix.

torch.Size([34847, 2])
We keep 3.66e+06/2.85e+08 =  1% of the original kernel matrix.

torch.Size([165397, 2])
We keep 2.16e+08/1.60e+10 =  1% of the original kernel matrix.

torch.Size([100028, 2])
We keep 3.23e+07/4.31e+09 =  0% of the original kernel matrix.

torch.Size([35554, 2])
We keep 7.72e+06/4.20e+08 =  1% of the original kernel matrix.

torch.Size([48296, 2])
We keep 7.21e+06/6.98e+08 =  1% of the original kernel matrix.

torch.Size([305430, 2])
We keep 1.05e+09/4.54e+10 =  2% of the original kernel matrix.

torch.Size([142401, 2])
We keep 5.10e+07/7.26e+09 =  0% of the original kernel matrix.

torch.Size([17323, 2])
We keep 2.90e+06/8.32e+07 =  3% of the original kernel matrix.

torch.Size([33242, 2])
We keep 3.90e+06/3.11e+08 =  1% of the original kernel matrix.

torch.Size([62534, 2])
We keep 4.07e+07/2.15e+09 =  1% of the original kernel matrix.

torch.Size([62036, 2])
We keep 1.35e+07/1.58e+09 =  0% of the original kernel matrix.

torch.Size([41177, 2])
We keep 1.55e+07/7.32e+08 =  2% of the original kernel matrix.

torch.Size([51808, 2])
We keep 8.73e+06/9.22e+08 =  0% of the original kernel matrix.

torch.Size([65427, 2])
We keep 7.05e+07/3.20e+09 =  2% of the original kernel matrix.

torch.Size([59743, 2])
We keep 1.64e+07/1.93e+09 =  0% of the original kernel matrix.

torch.Size([75891, 2])
We keep 8.80e+07/3.44e+09 =  2% of the original kernel matrix.

torch.Size([68588, 2])
We keep 1.67e+07/2.00e+09 =  0% of the original kernel matrix.

torch.Size([23006, 2])
We keep 1.97e+06/9.96e+07 =  1% of the original kernel matrix.

torch.Size([38562, 2])
We keep 4.17e+06/3.40e+08 =  1% of the original kernel matrix.

torch.Size([10562, 2])
We keep 4.82e+05/1.56e+07 =  3% of the original kernel matrix.

torch.Size([27353, 2])
We keep 2.16e+06/1.35e+08 =  1% of the original kernel matrix.

torch.Size([31933, 2])
We keep 3.62e+06/2.39e+08 =  1% of the original kernel matrix.

torch.Size([46609, 2])
We keep 5.75e+06/5.26e+08 =  1% of the original kernel matrix.

torch.Size([36495, 2])
We keep 1.93e+07/7.53e+08 =  2% of the original kernel matrix.

torch.Size([47811, 2])
We keep 9.07e+06/9.35e+08 =  0% of the original kernel matrix.

torch.Size([116282, 2])
We keep 1.41e+08/5.91e+09 =  2% of the original kernel matrix.

torch.Size([85103, 2])
We keep 2.11e+07/2.62e+09 =  0% of the original kernel matrix.

torch.Size([216693, 2])
We keep 1.73e+08/1.35e+10 =  1% of the original kernel matrix.

torch.Size([119822, 2])
We keep 2.93e+07/3.95e+09 =  0% of the original kernel matrix.

torch.Size([16811, 2])
We keep 1.36e+06/5.43e+07 =  2% of the original kernel matrix.

torch.Size([33271, 2])
We keep 3.31e+06/2.51e+08 =  1% of the original kernel matrix.

torch.Size([46241, 2])
We keep 6.94e+06/6.17e+08 =  1% of the original kernel matrix.

torch.Size([55677, 2])
We keep 8.29e+06/8.46e+08 =  0% of the original kernel matrix.

torch.Size([44420, 2])
We keep 1.13e+07/6.35e+08 =  1% of the original kernel matrix.

torch.Size([54431, 2])
We keep 8.39e+06/8.58e+08 =  0% of the original kernel matrix.

torch.Size([280099, 2])
We keep 7.46e+08/3.00e+10 =  2% of the original kernel matrix.

torch.Size([138061, 2])
We keep 4.09e+07/5.90e+09 =  0% of the original kernel matrix.

torch.Size([52597, 2])
We keep 1.45e+07/8.89e+08 =  1% of the original kernel matrix.

torch.Size([59553, 2])
We keep 9.66e+06/1.02e+09 =  0% of the original kernel matrix.

torch.Size([132651, 2])
We keep 4.00e+07/4.58e+09 =  0% of the original kernel matrix.

torch.Size([92536, 2])
We keep 1.89e+07/2.30e+09 =  0% of the original kernel matrix.

torch.Size([17619, 2])
We keep 7.39e+06/1.85e+08 =  4% of the original kernel matrix.

torch.Size([32449, 2])
We keep 5.11e+06/4.63e+08 =  1% of the original kernel matrix.

torch.Size([33359, 2])
We keep 5.14e+06/3.12e+08 =  1% of the original kernel matrix.

torch.Size([47379, 2])
We keep 6.26e+06/6.02e+08 =  1% of the original kernel matrix.

torch.Size([6417, 2])
We keep 2.69e+05/6.78e+06 =  3% of the original kernel matrix.

torch.Size([22105, 2])
We keep 1.61e+06/8.87e+07 =  1% of the original kernel matrix.

torch.Size([23735, 2])
We keep 2.76e+06/1.25e+08 =  2% of the original kernel matrix.

torch.Size([39315, 2])
We keep 4.60e+06/3.80e+08 =  1% of the original kernel matrix.

torch.Size([68835, 2])
We keep 1.65e+07/1.39e+09 =  1% of the original kernel matrix.

torch.Size([69108, 2])
We keep 1.15e+07/1.27e+09 =  0% of the original kernel matrix.

torch.Size([57626, 2])
We keep 3.57e+07/1.62e+09 =  2% of the original kernel matrix.

torch.Size([60402, 2])
We keep 1.21e+07/1.37e+09 =  0% of the original kernel matrix.

torch.Size([41823, 2])
We keep 5.37e+06/4.67e+08 =  1% of the original kernel matrix.

torch.Size([53815, 2])
We keep 7.44e+06/7.36e+08 =  1% of the original kernel matrix.

torch.Size([4606, 2])
We keep 9.50e+04/2.44e+06 =  3% of the original kernel matrix.

torch.Size([19931, 2])
We keep 1.12e+06/5.32e+07 =  2% of the original kernel matrix.

torch.Size([13779, 2])
We keep 1.52e+06/3.43e+07 =  4% of the original kernel matrix.

torch.Size([30481, 2])
We keep 2.84e+06/1.99e+08 =  1% of the original kernel matrix.

torch.Size([55256, 2])
We keep 1.80e+07/9.84e+08 =  1% of the original kernel matrix.

torch.Size([57852, 2])
We keep 9.20e+06/1.07e+09 =  0% of the original kernel matrix.

torch.Size([33655, 2])
We keep 4.80e+06/3.02e+08 =  1% of the original kernel matrix.

torch.Size([47130, 2])
We keep 6.30e+06/5.92e+08 =  1% of the original kernel matrix.

torch.Size([14724, 2])
We keep 8.48e+05/3.46e+07 =  2% of the original kernel matrix.

torch.Size([31411, 2])
We keep 2.79e+06/2.01e+08 =  1% of the original kernel matrix.

torch.Size([6324, 2])
We keep 2.82e+05/6.91e+06 =  4% of the original kernel matrix.

torch.Size([21717, 2])
We keep 1.62e+06/8.96e+07 =  1% of the original kernel matrix.

torch.Size([6742, 2])
We keep 4.59e+05/1.03e+07 =  4% of the original kernel matrix.

torch.Size([21951, 2])
We keep 1.85e+06/1.09e+08 =  1% of the original kernel matrix.

torch.Size([38072, 2])
We keep 9.21e+06/4.06e+08 =  2% of the original kernel matrix.

torch.Size([50711, 2])
We keep 6.94e+06/6.86e+08 =  1% of the original kernel matrix.

torch.Size([39794, 2])
We keep 1.07e+07/5.07e+08 =  2% of the original kernel matrix.

torch.Size([51730, 2])
We keep 7.64e+06/7.67e+08 =  0% of the original kernel matrix.

torch.Size([142693, 2])
We keep 1.81e+08/1.01e+10 =  1% of the original kernel matrix.

torch.Size([95181, 2])
We keep 2.68e+07/3.43e+09 =  0% of the original kernel matrix.

torch.Size([7064, 2])
We keep 4.07e+05/9.02e+06 =  4% of the original kernel matrix.

torch.Size([22972, 2])
We keep 1.77e+06/1.02e+08 =  1% of the original kernel matrix.

torch.Size([30011, 2])
We keep 4.06e+07/4.06e+08 = 10% of the original kernel matrix.

torch.Size([43193, 2])
We keep 7.11e+06/6.87e+08 =  1% of the original kernel matrix.

torch.Size([4579, 2])
We keep 2.22e+05/4.17e+06 =  5% of the original kernel matrix.

torch.Size([18847, 2])
We keep 1.36e+06/6.96e+07 =  1% of the original kernel matrix.

torch.Size([7097, 2])
We keep 2.82e+05/7.62e+06 =  3% of the original kernel matrix.

torch.Size([22988, 2])
We keep 1.64e+06/9.40e+07 =  1% of the original kernel matrix.

torch.Size([331796, 2])
We keep 3.98e+08/3.92e+10 =  1% of the original kernel matrix.

torch.Size([150864, 2])
We keep 4.63e+07/6.75e+09 =  0% of the original kernel matrix.

torch.Size([17172, 2])
We keep 8.71e+06/1.14e+08 =  7% of the original kernel matrix.

torch.Size([33514, 2])
We keep 4.33e+06/3.64e+08 =  1% of the original kernel matrix.

torch.Size([103242, 2])
We keep 6.19e+07/4.23e+09 =  1% of the original kernel matrix.

torch.Size([81007, 2])
We keep 1.83e+07/2.22e+09 =  0% of the original kernel matrix.

torch.Size([259648, 2])
We keep 1.74e+08/1.89e+10 =  0% of the original kernel matrix.

torch.Size([132289, 2])
We keep 3.44e+07/4.68e+09 =  0% of the original kernel matrix.

torch.Size([38556, 2])
We keep 6.91e+06/4.04e+08 =  1% of the original kernel matrix.

torch.Size([51166, 2])
We keep 7.00e+06/6.85e+08 =  1% of the original kernel matrix.

torch.Size([230720, 2])
We keep 2.22e+08/1.83e+10 =  1% of the original kernel matrix.

torch.Size([124255, 2])
We keep 3.41e+07/4.61e+09 =  0% of the original kernel matrix.

torch.Size([285249, 2])
We keep 1.74e+08/2.37e+10 =  0% of the original kernel matrix.

torch.Size([139105, 2])
We keep 3.82e+07/5.25e+09 =  0% of the original kernel matrix.

torch.Size([5825, 2])
We keep 1.87e+05/4.95e+06 =  3% of the original kernel matrix.

torch.Size([21286, 2])
We keep 1.41e+06/7.58e+07 =  1% of the original kernel matrix.

torch.Size([55925, 2])
We keep 2.26e+07/9.51e+08 =  2% of the original kernel matrix.

torch.Size([62108, 2])
We keep 1.00e+07/1.05e+09 =  0% of the original kernel matrix.

torch.Size([26959, 2])
We keep 3.01e+06/1.46e+08 =  2% of the original kernel matrix.

torch.Size([42234, 2])
We keep 4.75e+06/4.12e+08 =  1% of the original kernel matrix.

torch.Size([177510, 2])
We keep 1.42e+08/9.39e+09 =  1% of the original kernel matrix.

torch.Size([106590, 2])
We keep 2.58e+07/3.30e+09 =  0% of the original kernel matrix.

torch.Size([278592, 2])
We keep 2.83e+08/3.08e+10 =  0% of the original kernel matrix.

torch.Size([136917, 2])
We keep 4.27e+07/5.98e+09 =  0% of the original kernel matrix.

torch.Size([17050, 2])
We keep 2.87e+06/8.83e+07 =  3% of the original kernel matrix.

torch.Size([32805, 2])
We keep 3.95e+06/3.20e+08 =  1% of the original kernel matrix.

torch.Size([60793, 2])
We keep 1.36e+07/1.06e+09 =  1% of the original kernel matrix.

torch.Size([65129, 2])
We keep 1.04e+07/1.11e+09 =  0% of the original kernel matrix.

torch.Size([7867, 2])
We keep 2.67e+05/8.56e+06 =  3% of the original kernel matrix.

torch.Size([24294, 2])
We keep 1.73e+06/9.96e+07 =  1% of the original kernel matrix.

torch.Size([14679, 2])
We keep 5.29e+06/1.16e+08 =  4% of the original kernel matrix.

torch.Size([30532, 2])
We keep 4.40e+06/3.66e+08 =  1% of the original kernel matrix.

torch.Size([14218, 2])
We keep 1.15e+06/3.51e+07 =  3% of the original kernel matrix.

torch.Size([31023, 2])
We keep 2.81e+06/2.02e+08 =  1% of the original kernel matrix.

torch.Size([13153, 2])
We keep 8.30e+05/3.15e+07 =  2% of the original kernel matrix.

torch.Size([29778, 2])
We keep 2.66e+06/1.91e+08 =  1% of the original kernel matrix.

torch.Size([388082, 2])
We keep 6.54e+08/5.14e+10 =  1% of the original kernel matrix.

torch.Size([164720, 2])
We keep 5.39e+07/7.73e+09 =  0% of the original kernel matrix.

torch.Size([812547, 2])
We keep 3.52e+09/2.28e+11 =  1% of the original kernel matrix.

torch.Size([237406, 2])
We keep 1.03e+08/1.63e+10 =  0% of the original kernel matrix.

torch.Size([24872, 2])
We keep 5.73e+06/2.38e+08 =  2% of the original kernel matrix.

torch.Size([39609, 2])
We keep 5.78e+06/5.25e+08 =  1% of the original kernel matrix.

torch.Size([25197, 2])
We keep 3.00e+06/1.54e+08 =  1% of the original kernel matrix.

torch.Size([41037, 2])
We keep 4.77e+06/4.23e+08 =  1% of the original kernel matrix.

torch.Size([39686, 2])
We keep 8.07e+06/4.67e+08 =  1% of the original kernel matrix.

torch.Size([51939, 2])
We keep 7.51e+06/7.36e+08 =  1% of the original kernel matrix.

torch.Size([108577, 2])
We keep 7.15e+07/4.20e+09 =  1% of the original kernel matrix.

torch.Size([83767, 2])
We keep 1.75e+07/2.21e+09 =  0% of the original kernel matrix.

torch.Size([16048, 2])
We keep 1.11e+06/4.40e+07 =  2% of the original kernel matrix.

torch.Size([32537, 2])
We keep 3.08e+06/2.26e+08 =  1% of the original kernel matrix.

torch.Size([17360, 2])
We keep 1.73e+06/5.54e+07 =  3% of the original kernel matrix.

torch.Size([34105, 2])
We keep 3.33e+06/2.53e+08 =  1% of the original kernel matrix.

torch.Size([34774, 2])
We keep 5.05e+06/3.12e+08 =  1% of the original kernel matrix.

torch.Size([48521, 2])
We keep 6.36e+06/6.02e+08 =  1% of the original kernel matrix.

torch.Size([232498, 2])
We keep 1.44e+08/1.57e+10 =  0% of the original kernel matrix.

torch.Size([124569, 2])
We keep 3.19e+07/4.27e+09 =  0% of the original kernel matrix.

torch.Size([7410, 2])
We keep 4.71e+05/9.12e+06 =  5% of the original kernel matrix.

torch.Size([23205, 2])
We keep 1.73e+06/1.03e+08 =  1% of the original kernel matrix.

torch.Size([53464, 2])
We keep 1.97e+07/8.69e+08 =  2% of the original kernel matrix.

torch.Size([59969, 2])
We keep 9.46e+06/1.00e+09 =  0% of the original kernel matrix.

torch.Size([6095, 2])
We keep 2.83e+05/5.48e+06 =  5% of the original kernel matrix.

torch.Size([21858, 2])
We keep 1.45e+06/7.98e+07 =  1% of the original kernel matrix.

time for making ranges is 5.913190126419067
Sorting X and nu_X
time for sorting X is 0.08386087417602539
Sorting Z and nu_Z
time for sorting Z is 0.00028395652770996094
Starting Optim
sum tnu_Z before tensor(32016828., device='cuda:0')
c= tensor(822.7188, device='cuda:0')
c= tensor(39289.1172, device='cuda:0')
c= tensor(41507.9336, device='cuda:0')
c= tensor(56652.2969, device='cuda:0')
c= tensor(66304.0469, device='cuda:0')
c= tensor(73113.3594, device='cuda:0')
c= tensor(860911.1250, device='cuda:0')
c= tensor(1016643.6250, device='cuda:0')
c= tensor(1031459.3750, device='cuda:0')
c= tensor(1164598.2500, device='cuda:0')
c= tensor(1203540.2500, device='cuda:0')
c= tensor(7802882., device='cuda:0')
c= tensor(7811936.5000, device='cuda:0')
c= tensor(13109988., device='cuda:0')
c= tensor(13182084., device='cuda:0')
c= tensor(13349233., device='cuda:0')
c= tensor(13923797., device='cuda:0')
c= tensor(14459632., device='cuda:0')
c= tensor(22308280., device='cuda:0')
c= tensor(26230848., device='cuda:0')
c= tensor(26237076., device='cuda:0')
c= tensor(34885652., device='cuda:0')
c= tensor(34928352., device='cuda:0')
c= tensor(35504628., device='cuda:0')
c= tensor(35531344., device='cuda:0')
c= tensor(36865964., device='cuda:0')
c= tensor(38369740., device='cuda:0')
c= tensor(38377944., device='cuda:0')
c= tensor(42952016., device='cuda:0')
c= tensor(1.6875e+08, device='cuda:0')
c= tensor(1.6876e+08, device='cuda:0')
c= tensor(3.4317e+08, device='cuda:0')
c= tensor(3.4422e+08, device='cuda:0')
c= tensor(3.4422e+08, device='cuda:0')
c= tensor(3.4423e+08, device='cuda:0')
c= tensor(3.4656e+08, device='cuda:0')
c= tensor(3.4784e+08, device='cuda:0')
c= tensor(3.4784e+08, device='cuda:0')
c= tensor(3.4785e+08, device='cuda:0')
c= tensor(3.4786e+08, device='cuda:0')
c= tensor(3.4786e+08, device='cuda:0')
c= tensor(3.4787e+08, device='cuda:0')
c= tensor(3.4787e+08, device='cuda:0')
c= tensor(3.4787e+08, device='cuda:0')
c= tensor(3.4787e+08, device='cuda:0')
c= tensor(3.4788e+08, device='cuda:0')
c= tensor(3.4788e+08, device='cuda:0')
c= tensor(3.4789e+08, device='cuda:0')
c= tensor(3.4789e+08, device='cuda:0')
c= tensor(3.4794e+08, device='cuda:0')
c= tensor(3.4797e+08, device='cuda:0')
c= tensor(3.4797e+08, device='cuda:0')
c= tensor(3.4798e+08, device='cuda:0')
c= tensor(3.4799e+08, device='cuda:0')
c= tensor(3.4800e+08, device='cuda:0')
c= tensor(3.4800e+08, device='cuda:0')
c= tensor(3.4801e+08, device='cuda:0')
c= tensor(3.4801e+08, device='cuda:0')
c= tensor(3.4801e+08, device='cuda:0')
c= tensor(3.4802e+08, device='cuda:0')
c= tensor(3.4803e+08, device='cuda:0')
c= tensor(3.4803e+08, device='cuda:0')
c= tensor(3.4805e+08, device='cuda:0')
c= tensor(3.4806e+08, device='cuda:0')
c= tensor(3.4807e+08, device='cuda:0')
c= tensor(3.4807e+08, device='cuda:0')
c= tensor(3.4807e+08, device='cuda:0')
c= tensor(3.4808e+08, device='cuda:0')
c= tensor(3.4808e+08, device='cuda:0')
c= tensor(3.4809e+08, device='cuda:0')
c= tensor(3.4809e+08, device='cuda:0')
c= tensor(3.4809e+08, device='cuda:0')
c= tensor(3.4810e+08, device='cuda:0')
c= tensor(3.4810e+08, device='cuda:0')
c= tensor(3.4811e+08, device='cuda:0')
c= tensor(3.4811e+08, device='cuda:0')
c= tensor(3.4812e+08, device='cuda:0')
c= tensor(3.4812e+08, device='cuda:0')
c= tensor(3.4812e+08, device='cuda:0')
c= tensor(3.4813e+08, device='cuda:0')
c= tensor(3.4813e+08, device='cuda:0')
c= tensor(3.4814e+08, device='cuda:0')
c= tensor(3.4815e+08, device='cuda:0')
c= tensor(3.4815e+08, device='cuda:0')
c= tensor(3.4816e+08, device='cuda:0')
c= tensor(3.4816e+08, device='cuda:0')
c= tensor(3.4816e+08, device='cuda:0')
c= tensor(3.4816e+08, device='cuda:0')
c= tensor(3.4817e+08, device='cuda:0')
c= tensor(3.4817e+08, device='cuda:0')
c= tensor(3.4818e+08, device='cuda:0')
c= tensor(3.4818e+08, device='cuda:0')
c= tensor(3.4818e+08, device='cuda:0')
c= tensor(3.4818e+08, device='cuda:0')
c= tensor(3.4819e+08, device='cuda:0')
c= tensor(3.4820e+08, device='cuda:0')
c= tensor(3.4820e+08, device='cuda:0')
c= tensor(3.4824e+08, device='cuda:0')
c= tensor(3.4825e+08, device='cuda:0')
c= tensor(3.4825e+08, device='cuda:0')
c= tensor(3.4828e+08, device='cuda:0')
c= tensor(3.4828e+08, device='cuda:0')
c= tensor(3.4830e+08, device='cuda:0')
c= tensor(3.4830e+08, device='cuda:0')
c= tensor(3.4831e+08, device='cuda:0')
c= tensor(3.4831e+08, device='cuda:0')
c= tensor(3.4832e+08, device='cuda:0')
c= tensor(3.4832e+08, device='cuda:0')
c= tensor(3.4833e+08, device='cuda:0')
c= tensor(3.4833e+08, device='cuda:0')
c= tensor(3.4833e+08, device='cuda:0')
c= tensor(3.4833e+08, device='cuda:0')
c= tensor(3.4833e+08, device='cuda:0')
c= tensor(3.4834e+08, device='cuda:0')
c= tensor(3.4835e+08, device='cuda:0')
c= tensor(3.4836e+08, device='cuda:0')
c= tensor(3.4836e+08, device='cuda:0')
c= tensor(3.4836e+08, device='cuda:0')
c= tensor(3.4837e+08, device='cuda:0')
c= tensor(3.4838e+08, device='cuda:0')
c= tensor(3.4839e+08, device='cuda:0')
c= tensor(3.4839e+08, device='cuda:0')
c= tensor(3.4839e+08, device='cuda:0')
c= tensor(3.4840e+08, device='cuda:0')
c= tensor(3.4840e+08, device='cuda:0')
c= tensor(3.4840e+08, device='cuda:0')
c= tensor(3.4840e+08, device='cuda:0')
c= tensor(3.4841e+08, device='cuda:0')
c= tensor(3.4845e+08, device='cuda:0')
c= tensor(3.4845e+08, device='cuda:0')
c= tensor(3.4846e+08, device='cuda:0')
c= tensor(3.4846e+08, device='cuda:0')
c= tensor(3.4847e+08, device='cuda:0')
c= tensor(3.4847e+08, device='cuda:0')
c= tensor(3.4854e+08, device='cuda:0')
c= tensor(3.4854e+08, device='cuda:0')
c= tensor(3.4854e+08, device='cuda:0')
c= tensor(3.4854e+08, device='cuda:0')
c= tensor(3.4854e+08, device='cuda:0')
c= tensor(3.4855e+08, device='cuda:0')
c= tensor(3.4856e+08, device='cuda:0')
c= tensor(3.4856e+08, device='cuda:0')
c= tensor(3.4858e+08, device='cuda:0')
c= tensor(3.4860e+08, device='cuda:0')
c= tensor(3.4861e+08, device='cuda:0')
c= tensor(3.4861e+08, device='cuda:0')
c= tensor(3.4861e+08, device='cuda:0')
c= tensor(3.4862e+08, device='cuda:0')
c= tensor(3.4862e+08, device='cuda:0')
c= tensor(3.4862e+08, device='cuda:0')
c= tensor(3.4863e+08, device='cuda:0')
c= tensor(3.4863e+08, device='cuda:0')
c= tensor(3.4864e+08, device='cuda:0')
c= tensor(3.4866e+08, device='cuda:0')
c= tensor(3.4867e+08, device='cuda:0')
c= tensor(3.4880e+08, device='cuda:0')
c= tensor(3.4880e+08, device='cuda:0')
c= tensor(3.4881e+08, device='cuda:0')
c= tensor(3.4881e+08, device='cuda:0')
c= tensor(3.4881e+08, device='cuda:0')
c= tensor(3.4886e+08, device='cuda:0')
c= tensor(3.4886e+08, device='cuda:0')
c= tensor(3.4887e+08, device='cuda:0')
c= tensor(3.4887e+08, device='cuda:0')
c= tensor(3.4887e+08, device='cuda:0')
c= tensor(3.4888e+08, device='cuda:0')
c= tensor(3.4888e+08, device='cuda:0')
c= tensor(3.4888e+08, device='cuda:0')
c= tensor(3.4889e+08, device='cuda:0')
c= tensor(3.4889e+08, device='cuda:0')
c= tensor(3.4889e+08, device='cuda:0')
c= tensor(3.4889e+08, device='cuda:0')
c= tensor(3.4890e+08, device='cuda:0')
c= tensor(3.4890e+08, device='cuda:0')
c= tensor(3.4891e+08, device='cuda:0')
c= tensor(3.4892e+08, device='cuda:0')
c= tensor(3.4892e+08, device='cuda:0')
c= tensor(3.4893e+08, device='cuda:0')
c= tensor(3.4894e+08, device='cuda:0')
c= tensor(3.4894e+08, device='cuda:0')
c= tensor(3.4895e+08, device='cuda:0')
c= tensor(3.4895e+08, device='cuda:0')
c= tensor(3.4895e+08, device='cuda:0')
c= tensor(3.4896e+08, device='cuda:0')
c= tensor(3.4898e+08, device='cuda:0')
c= tensor(3.4898e+08, device='cuda:0')
c= tensor(3.4899e+08, device='cuda:0')
c= tensor(3.4899e+08, device='cuda:0')
c= tensor(3.4901e+08, device='cuda:0')
c= tensor(3.4904e+08, device='cuda:0')
c= tensor(3.4904e+08, device='cuda:0')
c= tensor(3.4904e+08, device='cuda:0')
c= tensor(3.4904e+08, device='cuda:0')
c= tensor(3.4905e+08, device='cuda:0')
c= tensor(3.4905e+08, device='cuda:0')
c= tensor(3.4905e+08, device='cuda:0')
c= tensor(3.4906e+08, device='cuda:0')
c= tensor(3.4906e+08, device='cuda:0')
c= tensor(3.4906e+08, device='cuda:0')
c= tensor(3.4906e+08, device='cuda:0')
c= tensor(3.4906e+08, device='cuda:0')
c= tensor(3.4907e+08, device='cuda:0')
c= tensor(3.4907e+08, device='cuda:0')
c= tensor(3.4907e+08, device='cuda:0')
c= tensor(3.4908e+08, device='cuda:0')
c= tensor(3.4908e+08, device='cuda:0')
c= tensor(3.4909e+08, device='cuda:0')
c= tensor(3.4909e+08, device='cuda:0')
c= tensor(3.4910e+08, device='cuda:0')
c= tensor(3.4911e+08, device='cuda:0')
c= tensor(3.4913e+08, device='cuda:0')
c= tensor(3.4913e+08, device='cuda:0')
c= tensor(3.4913e+08, device='cuda:0')
c= tensor(3.4914e+08, device='cuda:0')
c= tensor(3.4914e+08, device='cuda:0')
c= tensor(3.4914e+08, device='cuda:0')
c= tensor(3.4914e+08, device='cuda:0')
c= tensor(3.4914e+08, device='cuda:0')
c= tensor(3.4916e+08, device='cuda:0')
c= tensor(3.4917e+08, device='cuda:0')
c= tensor(3.4917e+08, device='cuda:0')
c= tensor(3.4917e+08, device='cuda:0')
c= tensor(3.4918e+08, device='cuda:0')
c= tensor(3.4918e+08, device='cuda:0')
c= tensor(3.4919e+08, device='cuda:0')
c= tensor(3.4921e+08, device='cuda:0')
c= tensor(3.4921e+08, device='cuda:0')
c= tensor(3.4921e+08, device='cuda:0')
c= tensor(3.4922e+08, device='cuda:0')
c= tensor(3.4922e+08, device='cuda:0')
c= tensor(3.4922e+08, device='cuda:0')
c= tensor(3.4923e+08, device='cuda:0')
c= tensor(3.4923e+08, device='cuda:0')
c= tensor(3.4925e+08, device='cuda:0')
c= tensor(3.4925e+08, device='cuda:0')
c= tensor(3.4925e+08, device='cuda:0')
c= tensor(3.4925e+08, device='cuda:0')
c= tensor(3.4926e+08, device='cuda:0')
c= tensor(3.4926e+08, device='cuda:0')
c= tensor(3.4930e+08, device='cuda:0')
c= tensor(3.5126e+08, device='cuda:0')
c= tensor(3.5126e+08, device='cuda:0')
c= tensor(3.5127e+08, device='cuda:0')
c= tensor(3.5127e+08, device='cuda:0')
c= tensor(3.5128e+08, device='cuda:0')
c= tensor(3.5301e+08, device='cuda:0')
c= tensor(3.6384e+08, device='cuda:0')
c= tensor(3.6384e+08, device='cuda:0')
c= tensor(3.6447e+08, device='cuda:0')
c= tensor(3.6597e+08, device='cuda:0')
c= tensor(3.6599e+08, device='cuda:0')
c= tensor(3.9078e+08, device='cuda:0')
c= tensor(3.9079e+08, device='cuda:0')
c= tensor(3.9079e+08, device='cuda:0')
c= tensor(3.9343e+08, device='cuda:0')
c= tensor(3.9668e+08, device='cuda:0')
c= tensor(3.9669e+08, device='cuda:0')
c= tensor(3.9678e+08, device='cuda:0')
c= tensor(3.9716e+08, device='cuda:0')
c= tensor(4.0108e+08, device='cuda:0')
c= tensor(4.0182e+08, device='cuda:0')
c= tensor(4.0283e+08, device='cuda:0')
c= tensor(4.0355e+08, device='cuda:0')
c= tensor(4.0373e+08, device='cuda:0')
c= tensor(4.0379e+08, device='cuda:0')
c= tensor(4.1415e+08, device='cuda:0')
c= tensor(4.1415e+08, device='cuda:0')
c= tensor(4.1415e+08, device='cuda:0')
c= tensor(4.1423e+08, device='cuda:0')
c= tensor(4.1448e+08, device='cuda:0')
c= tensor(4.2626e+08, device='cuda:0')
c= tensor(4.2696e+08, device='cuda:0')
c= tensor(4.2696e+08, device='cuda:0')
c= tensor(4.2697e+08, device='cuda:0')
c= tensor(4.2701e+08, device='cuda:0')
c= tensor(4.2707e+08, device='cuda:0')
c= tensor(4.3149e+08, device='cuda:0')
c= tensor(4.3334e+08, device='cuda:0')
c= tensor(4.3345e+08, device='cuda:0')
c= tensor(4.3345e+08, device='cuda:0')
c= tensor(4.3346e+08, device='cuda:0')
c= tensor(4.3377e+08, device='cuda:0')
c= tensor(4.3441e+08, device='cuda:0')
c= tensor(4.3453e+08, device='cuda:0')
c= tensor(4.3454e+08, device='cuda:0')
c= tensor(4.5699e+08, device='cuda:0')
c= tensor(4.5701e+08, device='cuda:0')
c= tensor(4.5715e+08, device='cuda:0')
c= tensor(4.6014e+08, device='cuda:0')
c= tensor(4.6014e+08, device='cuda:0')
c= tensor(4.6050e+08, device='cuda:0')
c= tensor(4.7923e+08, device='cuda:0')
c= tensor(4.9335e+08, device='cuda:0')
c= tensor(4.9336e+08, device='cuda:0')
c= tensor(4.9337e+08, device='cuda:0')
c= tensor(4.9337e+08, device='cuda:0')
c= tensor(4.9337e+08, device='cuda:0')
c= tensor(4.9354e+08, device='cuda:0')
c= tensor(4.9358e+08, device='cuda:0')
c= tensor(4.9389e+08, device='cuda:0')
c= tensor(5.0045e+08, device='cuda:0')
c= tensor(5.0104e+08, device='cuda:0')
c= tensor(5.0108e+08, device='cuda:0')
c= tensor(5.0109e+08, device='cuda:0')
c= tensor(5.1259e+08, device='cuda:0')
c= tensor(5.1568e+08, device='cuda:0')
c= tensor(5.1603e+08, device='cuda:0')
c= tensor(5.1605e+08, device='cuda:0')
c= tensor(5.6762e+08, device='cuda:0')
c= tensor(5.6763e+08, device='cuda:0')
c= tensor(5.7252e+08, device='cuda:0')
c= tensor(5.7253e+08, device='cuda:0')
c= tensor(5.7299e+08, device='cuda:0')
c= tensor(5.7309e+08, device='cuda:0')
c= tensor(5.8448e+08, device='cuda:0')
c= tensor(5.8537e+08, device='cuda:0')
c= tensor(5.8537e+08, device='cuda:0')
c= tensor(5.8776e+08, device='cuda:0')
c= tensor(5.8902e+08, device='cuda:0')
c= tensor(5.8902e+08, device='cuda:0')
c= tensor(5.8936e+08, device='cuda:0')
c= tensor(5.9248e+08, device='cuda:0')
c= tensor(6.0783e+08, device='cuda:0')
c= tensor(6.0908e+08, device='cuda:0')
c= tensor(6.0908e+08, device='cuda:0')
c= tensor(6.0909e+08, device='cuda:0')
c= tensor(6.0909e+08, device='cuda:0')
c= tensor(6.0919e+08, device='cuda:0')
c= tensor(6.0924e+08, device='cuda:0')
c= tensor(6.0924e+08, device='cuda:0')
c= tensor(6.1040e+08, device='cuda:0')
c= tensor(6.1391e+08, device='cuda:0')
c= tensor(6.1396e+08, device='cuda:0')
c= tensor(6.1397e+08, device='cuda:0')
c= tensor(6.1401e+08, device='cuda:0')
c= tensor(6.1403e+08, device='cuda:0')
c= tensor(6.1406e+08, device='cuda:0')
c= tensor(6.1407e+08, device='cuda:0')
c= tensor(6.1407e+08, device='cuda:0')
c= tensor(6.2265e+08, device='cuda:0')
c= tensor(6.2270e+08, device='cuda:0')
c= tensor(6.2279e+08, device='cuda:0')
c= tensor(6.2358e+08, device='cuda:0')
c= tensor(6.2361e+08, device='cuda:0')
c= tensor(7.4394e+08, device='cuda:0')
c= tensor(7.4395e+08, device='cuda:0')
c= tensor(7.4465e+08, device='cuda:0')
c= tensor(7.4465e+08, device='cuda:0')
c= tensor(7.4465e+08, device='cuda:0')
c= tensor(7.4466e+08, device='cuda:0')
c= tensor(7.4479e+08, device='cuda:0')
c= tensor(7.4480e+08, device='cuda:0')
c= tensor(7.4616e+08, device='cuda:0')
c= tensor(7.4617e+08, device='cuda:0')
c= tensor(7.4618e+08, device='cuda:0')
c= tensor(7.7364e+08, device='cuda:0')
c= tensor(7.7424e+08, device='cuda:0')
c= tensor(7.7488e+08, device='cuda:0')
c= tensor(7.7826e+08, device='cuda:0')
c= tensor(7.8379e+08, device='cuda:0')
c= tensor(7.8379e+08, device='cuda:0')
c= tensor(7.8380e+08, device='cuda:0')
c= tensor(7.8410e+08, device='cuda:0')
c= tensor(7.8410e+08, device='cuda:0')
c= tensor(7.8411e+08, device='cuda:0')
c= tensor(7.8414e+08, device='cuda:0')
c= tensor(7.8414e+08, device='cuda:0')
c= tensor(7.8414e+08, device='cuda:0')
c= tensor(7.8415e+08, device='cuda:0')
c= tensor(7.8415e+08, device='cuda:0')
c= tensor(8.1085e+08, device='cuda:0')
c= tensor(8.1091e+08, device='cuda:0')
c= tensor(8.1146e+08, device='cuda:0')
c= tensor(8.1201e+08, device='cuda:0')
c= tensor(8.1201e+08, device='cuda:0')
c= tensor(8.1203e+08, device='cuda:0')
c= tensor(8.7431e+08, device='cuda:0')
c= tensor(8.9958e+08, device='cuda:0')
c= tensor(8.9962e+08, device='cuda:0')
c= tensor(8.9976e+08, device='cuda:0')
c= tensor(8.9976e+08, device='cuda:0')
c= tensor(9.0006e+08, device='cuda:0')
c= tensor(9.2561e+08, device='cuda:0')
c= tensor(9.2596e+08, device='cuda:0')
c= tensor(9.2599e+08, device='cuda:0')
c= tensor(9.2658e+08, device='cuda:0')
c= tensor(9.4548e+08, device='cuda:0')
c= tensor(9.4565e+08, device='cuda:0')
c= tensor(9.4565e+08, device='cuda:0')
c= tensor(9.4572e+08, device='cuda:0')
c= tensor(9.4574e+08, device='cuda:0')
c= tensor(9.4575e+08, device='cuda:0')
c= tensor(9.4890e+08, device='cuda:0')
c= tensor(9.4897e+08, device='cuda:0')
c= tensor(9.4897e+08, device='cuda:0')
c= tensor(9.4912e+08, device='cuda:0')
c= tensor(9.4917e+08, device='cuda:0')
c= tensor(9.4917e+08, device='cuda:0')
c= tensor(9.5200e+08, device='cuda:0')
c= tensor(9.5575e+08, device='cuda:0')
c= tensor(9.6098e+08, device='cuda:0')
c= tensor(9.6749e+08, device='cuda:0')
c= tensor(9.7352e+08, device='cuda:0')
c= tensor(9.7356e+08, device='cuda:0')
c= tensor(9.7362e+08, device='cuda:0')
c= tensor(9.7469e+08, device='cuda:0')
c= tensor(9.7943e+08, device='cuda:0')
c= tensor(9.7944e+08, device='cuda:0')
c= tensor(9.8092e+08, device='cuda:0')
c= tensor(1.0770e+09, device='cuda:0')
c= tensor(1.0818e+09, device='cuda:0')
c= tensor(1.0848e+09, device='cuda:0')
c= tensor(1.0928e+09, device='cuda:0')
c= tensor(1.0928e+09, device='cuda:0')
c= tensor(1.0928e+09, device='cuda:0')
c= tensor(1.0932e+09, device='cuda:0')
c= tensor(1.0936e+09, device='cuda:0')
c= tensor(1.0941e+09, device='cuda:0')
c= tensor(1.1912e+09, device='cuda:0')
c= tensor(1.1977e+09, device='cuda:0')
c= tensor(1.2007e+09, device='cuda:0')
c= tensor(1.2009e+09, device='cuda:0')
c= tensor(1.2031e+09, device='cuda:0')
c= tensor(1.2031e+09, device='cuda:0')
c= tensor(1.2031e+09, device='cuda:0')
c= tensor(1.2110e+09, device='cuda:0')
c= tensor(1.2110e+09, device='cuda:0')
c= tensor(1.2110e+09, device='cuda:0')
c= tensor(1.2111e+09, device='cuda:0')
c= tensor(1.2300e+09, device='cuda:0')
c= tensor(1.2307e+09, device='cuda:0')
c= tensor(1.2315e+09, device='cuda:0')
c= tensor(1.2315e+09, device='cuda:0')
c= tensor(1.2315e+09, device='cuda:0')
c= tensor(1.2315e+09, device='cuda:0')
c= tensor(1.2316e+09, device='cuda:0')
c= tensor(1.2319e+09, device='cuda:0')
c= tensor(1.2321e+09, device='cuda:0')
c= tensor(1.2321e+09, device='cuda:0')
c= tensor(1.2342e+09, device='cuda:0')
c= tensor(1.2342e+09, device='cuda:0')
c= tensor(1.2349e+09, device='cuda:0')
c= tensor(1.2349e+09, device='cuda:0')
c= tensor(1.2350e+09, device='cuda:0')
c= tensor(1.2350e+09, device='cuda:0')
c= tensor(1.2350e+09, device='cuda:0')
c= tensor(1.2350e+09, device='cuda:0')
c= tensor(1.2351e+09, device='cuda:0')
c= tensor(1.2374e+09, device='cuda:0')
c= tensor(1.2457e+09, device='cuda:0')
c= tensor(1.2457e+09, device='cuda:0')
c= tensor(1.2457e+09, device='cuda:0')
c= tensor(1.2535e+09, device='cuda:0')
c= tensor(1.2535e+09, device='cuda:0')
c= tensor(1.2891e+09, device='cuda:0')
c= tensor(1.2891e+09, device='cuda:0')
c= tensor(1.2895e+09, device='cuda:0')
c= tensor(1.2898e+09, device='cuda:0')
c= tensor(1.2898e+09, device='cuda:0')
c= tensor(1.2912e+09, device='cuda:0')
c= tensor(1.2916e+09, device='cuda:0')
c= tensor(1.3522e+09, device='cuda:0')
c= tensor(1.3522e+09, device='cuda:0')
c= tensor(1.3523e+09, device='cuda:0')
c= tensor(1.3523e+09, device='cuda:0')
c= tensor(1.3523e+09, device='cuda:0')
c= tensor(1.3523e+09, device='cuda:0')
c= tensor(1.3525e+09, device='cuda:0')
c= tensor(1.3525e+09, device='cuda:0')
c= tensor(1.3528e+09, device='cuda:0')
c= tensor(1.3529e+09, device='cuda:0')
c= tensor(1.3529e+09, device='cuda:0')
c= tensor(1.3530e+09, device='cuda:0')
c= tensor(1.3791e+09, device='cuda:0')
c= tensor(1.3801e+09, device='cuda:0')
c= tensor(1.3830e+09, device='cuda:0')
c= tensor(1.3831e+09, device='cuda:0')
c= tensor(1.3832e+09, device='cuda:0')
c= tensor(1.3832e+09, device='cuda:0')
c= tensor(1.3833e+09, device='cuda:0')
c= tensor(1.3918e+09, device='cuda:0')
c= tensor(1.3918e+09, device='cuda:0')
c= tensor(1.3918e+09, device='cuda:0')
c= tensor(1.3922e+09, device='cuda:0')
c= tensor(1.3934e+09, device='cuda:0')
c= tensor(1.3934e+09, device='cuda:0')
c= tensor(1.3934e+09, device='cuda:0')
c= tensor(1.4416e+09, device='cuda:0')
c= tensor(1.4416e+09, device='cuda:0')
c= tensor(1.4418e+09, device='cuda:0')
c= tensor(1.4419e+09, device='cuda:0')
c= tensor(1.4481e+09, device='cuda:0')
c= tensor(1.4613e+09, device='cuda:0')
c= tensor(1.4657e+09, device='cuda:0')
c= tensor(1.4676e+09, device='cuda:0')
c= tensor(1.4676e+09, device='cuda:0')
c= tensor(1.4682e+09, device='cuda:0')
c= tensor(1.4686e+09, device='cuda:0')
c= tensor(1.4686e+09, device='cuda:0')
c= tensor(1.4687e+09, device='cuda:0')
c= tensor(1.4687e+09, device='cuda:0')
c= tensor(1.4695e+09, device='cuda:0')
c= tensor(1.4695e+09, device='cuda:0')
c= tensor(1.4695e+09, device='cuda:0')
c= tensor(1.4695e+09, device='cuda:0')
c= tensor(1.4695e+09, device='cuda:0')
c= tensor(1.4722e+09, device='cuda:0')
c= tensor(1.4722e+09, device='cuda:0')
c= tensor(1.4725e+09, device='cuda:0')
c= tensor(1.4726e+09, device='cuda:0')
c= tensor(1.4728e+09, device='cuda:0')
c= tensor(1.4728e+09, device='cuda:0')
c= tensor(1.4728e+09, device='cuda:0')
c= tensor(1.4728e+09, device='cuda:0')
c= tensor(1.4778e+09, device='cuda:0')
c= tensor(1.4779e+09, device='cuda:0')
c= tensor(1.4779e+09, device='cuda:0')
c= tensor(1.4779e+09, device='cuda:0')
c= tensor(1.4964e+09, device='cuda:0')
c= tensor(1.5557e+09, device='cuda:0')
c= tensor(1.5557e+09, device='cuda:0')
c= tensor(1.5557e+09, device='cuda:0')
c= tensor(1.5557e+09, device='cuda:0')
c= tensor(1.5572e+09, device='cuda:0')
c= tensor(1.5572e+09, device='cuda:0')
c= tensor(1.5573e+09, device='cuda:0')
c= tensor(1.5573e+09, device='cuda:0')
c= tensor(1.5669e+09, device='cuda:0')
c= tensor(1.5698e+09, device='cuda:0')
c= tensor(1.5757e+09, device='cuda:0')
c= tensor(1.5758e+09, device='cuda:0')
c= tensor(1.5758e+09, device='cuda:0')
c= tensor(1.5758e+09, device='cuda:0')
c= tensor(1.5761e+09, device='cuda:0')
c= tensor(1.5761e+09, device='cuda:0')
c= tensor(1.5768e+09, device='cuda:0')
c= tensor(1.5857e+09, device='cuda:0')
c= tensor(1.6154e+09, device='cuda:0')
c= tensor(1.6154e+09, device='cuda:0')
c= tensor(1.6154e+09, device='cuda:0')
c= tensor(1.6154e+09, device='cuda:0')
c= tensor(1.7195e+09, device='cuda:0')
c= tensor(1.7197e+09, device='cuda:0')
c= tensor(1.7197e+09, device='cuda:0')
c= tensor(1.7197e+09, device='cuda:0')
c= tensor(1.7198e+09, device='cuda:0')
c= tensor(1.7198e+09, device='cuda:0')
c= tensor(1.7212e+09, device='cuda:0')
c= tensor(1.7212e+09, device='cuda:0')
c= tensor(1.7213e+09, device='cuda:0')
c= tensor(1.7213e+09, device='cuda:0')
c= tensor(1.7213e+09, device='cuda:0')
c= tensor(1.7213e+09, device='cuda:0')
c= tensor(1.7253e+09, device='cuda:0')
c= tensor(1.7581e+09, device='cuda:0')
c= tensor(1.7624e+09, device='cuda:0')
c= tensor(1.7659e+09, device='cuda:0')
c= tensor(1.7661e+09, device='cuda:0')
c= tensor(1.7661e+09, device='cuda:0')
c= tensor(1.7662e+09, device='cuda:0')
c= tensor(1.7670e+09, device='cuda:0')
c= tensor(1.7691e+09, device='cuda:0')
c= tensor(1.7734e+09, device='cuda:0')
c= tensor(1.7735e+09, device='cuda:0')
c= tensor(1.8478e+09, device='cuda:0')
c= tensor(1.8478e+09, device='cuda:0')
c= tensor(1.8483e+09, device='cuda:0')
c= tensor(1.8523e+09, device='cuda:0')
c= tensor(1.8525e+09, device='cuda:0')
c= tensor(1.8549e+09, device='cuda:0')
c= tensor(1.8596e+09, device='cuda:0')
c= tensor(1.8597e+09, device='cuda:0')
c= tensor(1.8597e+09, device='cuda:0')
c= tensor(1.8597e+09, device='cuda:0')
c= tensor(1.8602e+09, device='cuda:0')
c= tensor(1.8602e+09, device='cuda:0')
c= tensor(1.8636e+09, device='cuda:0')
c= tensor(2.0080e+09, device='cuda:0')
c= tensor(2.0173e+09, device='cuda:0')
c= tensor(2.0267e+09, device='cuda:0')
c= tensor(2.0278e+09, device='cuda:0')
c= tensor(2.0281e+09, device='cuda:0')
c= tensor(2.0282e+09, device='cuda:0')
c= tensor(2.0283e+09, device='cuda:0')
c= tensor(2.0308e+09, device='cuda:0')
c= tensor(2.0348e+09, device='cuda:0')
c= tensor(2.0349e+09, device='cuda:0')
c= tensor(2.1758e+09, device='cuda:0')
c= tensor(2.2046e+09, device='cuda:0')
c= tensor(2.2047e+09, device='cuda:0')
c= tensor(2.2047e+09, device='cuda:0')
c= tensor(2.2050e+09, device='cuda:0')
c= tensor(2.2055e+09, device='cuda:0')
c= tensor(2.2055e+09, device='cuda:0')
c= tensor(2.2234e+09, device='cuda:0')
c= tensor(2.2241e+09, device='cuda:0')
c= tensor(2.2243e+09, device='cuda:0')
c= tensor(2.2243e+09, device='cuda:0')
c= tensor(2.2244e+09, device='cuda:0')
c= tensor(2.2244e+09, device='cuda:0')
c= tensor(2.2244e+09, device='cuda:0')
c= tensor(2.2244e+09, device='cuda:0')
c= tensor(2.2245e+09, device='cuda:0')
c= tensor(4.4560e+09, device='cuda:0')
c= tensor(4.4562e+09, device='cuda:0')
c= tensor(4.4562e+09, device='cuda:0')
c= tensor(4.4562e+09, device='cuda:0')
c= tensor(4.4562e+09, device='cuda:0')
c= tensor(4.4562e+09, device='cuda:0')
c= tensor(4.4719e+09, device='cuda:0')
c= tensor(4.4737e+09, device='cuda:0')
c= tensor(4.5497e+09, device='cuda:0')
c= tensor(4.5497e+09, device='cuda:0')
c= tensor(4.5561e+09, device='cuda:0')
c= tensor(4.5569e+09, device='cuda:0')
c= tensor(4.5612e+09, device='cuda:0')
c= tensor(4.5988e+09, device='cuda:0')
c= tensor(4.5988e+09, device='cuda:0')
c= tensor(4.5988e+09, device='cuda:0')
c= tensor(4.5993e+09, device='cuda:0')
c= tensor(4.5994e+09, device='cuda:0')
c= tensor(4.5998e+09, device='cuda:0')
c= tensor(4.5999e+09, device='cuda:0')
c= tensor(4.6004e+09, device='cuda:0')
c= tensor(4.6012e+09, device='cuda:0')
c= tensor(4.6012e+09, device='cuda:0')
c= tensor(4.6047e+09, device='cuda:0')
c= tensor(4.6101e+09, device='cuda:0')
c= tensor(4.6101e+09, device='cuda:0')
c= tensor(4.6102e+09, device='cuda:0')
c= tensor(4.6177e+09, device='cuda:0')
c= tensor(4.6178e+09, device='cuda:0')
c= tensor(4.6414e+09, device='cuda:0')
c= tensor(4.6414e+09, device='cuda:0')
c= tensor(4.6426e+09, device='cuda:0')
c= tensor(4.6429e+09, device='cuda:0')
c= tensor(4.6454e+09, device='cuda:0')
c= tensor(4.6476e+09, device='cuda:0')
c= tensor(4.6476e+09, device='cuda:0')
c= tensor(4.6476e+09, device='cuda:0')
c= tensor(4.6477e+09, device='cuda:0')
c= tensor(4.6481e+09, device='cuda:0')
c= tensor(4.6514e+09, device='cuda:0')
c= tensor(4.6563e+09, device='cuda:0')
c= tensor(4.6563e+09, device='cuda:0')
c= tensor(4.6565e+09, device='cuda:0')
c= tensor(4.6568e+09, device='cuda:0')
c= tensor(4.6742e+09, device='cuda:0')
c= tensor(4.6745e+09, device='cuda:0')
c= tensor(4.6753e+09, device='cuda:0')
c= tensor(4.6758e+09, device='cuda:0')
c= tensor(4.6759e+09, device='cuda:0')
c= tensor(4.6759e+09, device='cuda:0')
c= tensor(4.6760e+09, device='cuda:0')
c= tensor(4.6763e+09, device='cuda:0')
c= tensor(4.6771e+09, device='cuda:0')
c= tensor(4.6772e+09, device='cuda:0')
c= tensor(4.6772e+09, device='cuda:0')
c= tensor(4.6772e+09, device='cuda:0')
c= tensor(4.6776e+09, device='cuda:0')
c= tensor(4.6777e+09, device='cuda:0')
c= tensor(4.6777e+09, device='cuda:0')
c= tensor(4.6777e+09, device='cuda:0')
c= tensor(4.6777e+09, device='cuda:0')
c= tensor(4.6779e+09, device='cuda:0')
c= tensor(4.6781e+09, device='cuda:0')
c= tensor(4.6827e+09, device='cuda:0')
c= tensor(4.6827e+09, device='cuda:0')
c= tensor(4.6839e+09, device='cuda:0')
c= tensor(4.6839e+09, device='cuda:0')
c= tensor(4.6839e+09, device='cuda:0')
c= tensor(4.6971e+09, device='cuda:0')
c= tensor(4.6974e+09, device='cuda:0')
c= tensor(4.6990e+09, device='cuda:0')
c= tensor(4.7032e+09, device='cuda:0')
c= tensor(4.7033e+09, device='cuda:0')
c= tensor(4.7082e+09, device='cuda:0')
c= tensor(4.7124e+09, device='cuda:0')
c= tensor(4.7124e+09, device='cuda:0')
c= tensor(4.7128e+09, device='cuda:0')
c= tensor(4.7129e+09, device='cuda:0')
c= tensor(4.7153e+09, device='cuda:0')
c= tensor(4.7236e+09, device='cuda:0')
c= tensor(4.7236e+09, device='cuda:0')
c= tensor(4.7240e+09, device='cuda:0')
c= tensor(4.7240e+09, device='cuda:0')
c= tensor(4.7243e+09, device='cuda:0')
c= tensor(4.7243e+09, device='cuda:0')
c= tensor(4.7243e+09, device='cuda:0')
c= tensor(4.7438e+09, device='cuda:0')
c= tensor(4.8798e+09, device='cuda:0')
c= tensor(4.8799e+09, device='cuda:0')
c= tensor(4.8800e+09, device='cuda:0')
c= tensor(4.8801e+09, device='cuda:0')
c= tensor(4.8818e+09, device='cuda:0')
c= tensor(4.8818e+09, device='cuda:0')
c= tensor(4.8818e+09, device='cuda:0')
c= tensor(4.8819e+09, device='cuda:0')
c= tensor(4.8849e+09, device='cuda:0')
c= tensor(4.8849e+09, device='cuda:0')
c= tensor(4.8853e+09, device='cuda:0')
c= tensor(4.8853e+09, device='cuda:0')
memory (bytes)
4768550912
time for making loss 2 is 16.704152822494507
p0 True
it  0 : 2300670976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 71% |
shape of L is 
torch.Size([])
memory (bytes)
4768817152
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
memory (bytes)
4769423360
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  34310433000.0
relative error loss 7.0231433
shape of L is 
torch.Size([])
memory (bytes)
4913221632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  5% | 14% |
memory (bytes)
4913381376
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  34310320000.0
relative error loss 7.02312
shape of L is 
torch.Size([])
memory (bytes)
4918575104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 14% |
memory (bytes)
4918697984
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  34309798000.0
relative error loss 7.023013
shape of L is 
torch.Size([])
memory (bytes)
4920877056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 14% |
memory (bytes)
4920889344
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  34307553000.0
relative error loss 7.022554
shape of L is 
torch.Size([])
memory (bytes)
4922986496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 14% |
memory (bytes)
4923011072
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  34294587000.0
relative error loss 7.0199
shape of L is 
torch.Size([])
memory (bytes)
4925026304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 14% |
memory (bytes)
4925112320
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  34152310000.0
relative error loss 6.9907765
shape of L is 
torch.Size([])
memory (bytes)
4927234048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 30% | 14% |
memory (bytes)
4927234048
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  33448313000.0
relative error loss 6.8466725
shape of L is 
torch.Size([])
memory (bytes)
4929282048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 14% |
memory (bytes)
4929359872
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  29808126000.0
relative error loss 6.1015477
shape of L is 
torch.Size([])
memory (bytes)
4931493888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 14% |
memory (bytes)
4931502080
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  10076232000.0
relative error loss 2.0625453
shape of L is 
torch.Size([])
memory (bytes)
4933476352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 14% |
memory (bytes)
4933652480
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  6750526500.0
relative error loss 1.3817929
time to take a step is 230.79746747016907
it  1 : 2970717184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 14% |
shape of L is 
torch.Size([])
memory (bytes)
4935794688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 14% |
memory (bytes)
4935794688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 14% |
error is  6750526500.0
relative error loss 1.3817929
shape of L is 
torch.Size([])
memory (bytes)
4937801728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 14% |
memory (bytes)
4937940992
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  5177519000.0
relative error loss 1.0598077
shape of L is 
torch.Size([])
memory (bytes)
4939952128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 14% |
memory (bytes)
4940095488
| ID | GPU  | MEM |
-------------------
|  0 |   2% |  0% |
|  1 | 100% | 14% |
error is  21768753000.0
relative error loss 4.4559355
shape of L is 
torch.Size([])
memory (bytes)
4942209024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 14% |
memory (bytes)
4942209024
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  4595526700.0
relative error loss 0.9406772
shape of L is 
torch.Size([])
memory (bytes)
4944359424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 14% |
memory (bytes)
4944359424
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  4213394000.0
relative error loss 0.86245686
shape of L is 
torch.Size([])
memory (bytes)
4946395136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 14% |
memory (bytes)
4946526208
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  4046567700.0
relative error loss 0.8283085
shape of L is 
torch.Size([])
memory (bytes)
4948660224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 14% |
memory (bytes)
4948664320
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  3808629200.0
relative error loss 0.77960396
shape of L is 
torch.Size([])
memory (bytes)
4950761472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 14% |
memory (bytes)
4950761472
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  3530055200.0
relative error loss 0.72258145
shape of L is 
torch.Size([])
memory (bytes)
4952862720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 14% |
memory (bytes)
4952862720
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  3248097500.0
relative error loss 0.6648664
shape of L is 
torch.Size([])
memory (bytes)
4954988544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 14% |
memory (bytes)
4954988544
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  3010575400.0
relative error loss 0.616247
time to take a step is 212.9271945953369
it  2 : 3161642496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 14% |
shape of L is 
torch.Size([])
memory (bytes)
4957089792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 14% |
memory (bytes)
4957089792
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  3010575400.0
relative error loss 0.616247
shape of L is 
torch.Size([])
memory (bytes)
4959064064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 14% |
memory (bytes)
4959240192
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  2677801500.0
relative error loss 0.54813015
shape of L is 
torch.Size([])
memory (bytes)
4961349632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 14% |
memory (bytes)
4961357824
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  2448451800.0
relative error loss 0.5011836
shape of L is 
torch.Size([])
memory (bytes)
4963422208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 14% |
memory (bytes)
4963422208
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  2296153900.0
relative error loss 0.47000915
shape of L is 
torch.Size([])
memory (bytes)
4965588992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 14% |
memory (bytes)
4965588992
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  2118547200.0
relative error loss 0.43365413
shape of L is 
torch.Size([])
memory (bytes)
4967718912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 14% |
memory (bytes)
4967727104
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  2025818400.0
relative error loss 0.41467306
shape of L is 
torch.Size([])
memory (bytes)
4969848832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 14% |
memory (bytes)
4969848832
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  1858340400.0
relative error loss 0.3803913
shape of L is 
torch.Size([])
memory (bytes)
4971991040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 14% |
memory (bytes)
4971999232
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  1741807400.0
relative error loss 0.3565377
shape of L is 
torch.Size([])
memory (bytes)
4974137344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 14% |
memory (bytes)
4974137344
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  1634060800.0
relative error loss 0.3344826
shape of L is 
torch.Size([])
memory (bytes)
4976254976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 14% |
memory (bytes)
4976271360
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  1527724300.0
relative error loss 0.31271616
time to take a step is 219.3566131591797
it  3 : 3162044416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 14% |
shape of L is 
torch.Size([])
memory (bytes)
4978413568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 14% |
memory (bytes)
4978421760
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  1527724300.0
relative error loss 0.31271616
shape of L is 
torch.Size([])
memory (bytes)
4980342784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 14% |
memory (bytes)
4980342784
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  1402011100.0
relative error loss 0.2869834
shape of L is 
torch.Size([])
memory (bytes)
4982439936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 14% |
memory (bytes)
4982652928
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  1271427300.0
relative error loss 0.26025367
shape of L is 
torch.Size([])
memory (bytes)
4984832000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 14% |
memory (bytes)
4984832000
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  1171848400.0
relative error loss 0.23987046
shape of L is 
torch.Size([])
memory (bytes)
4986925056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 14% |
memory (bytes)
4986925056
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  1078641400.0
relative error loss 0.22079153
shape of L is 
torch.Size([])
memory (bytes)
4988968960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 14% |
memory (bytes)
4989128704
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  1005334300.0
relative error loss 0.20578599
shape of L is 
torch.Size([])
memory (bytes)
4991275008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 14% |
memory (bytes)
4991275008
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  939487200.0
relative error loss 0.1923075
shape of L is 
torch.Size([])
memory (bytes)
4993241088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 14% |
memory (bytes)
4993241088
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  871350300.0
relative error loss 0.17836025
shape of L is 
torch.Size([])
memory (bytes)
4995534848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 14% |
memory (bytes)
4995555328
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  824780000.0
relative error loss 0.16882761
shape of L is 
torch.Size([])
memory (bytes)
4997697536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 14% |
memory (bytes)
4997697536
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  782489100.0
relative error loss 0.1601709
time to take a step is 225.8460259437561
c= tensor(822.7188, device='cuda:0')
c= tensor(39289.1172, device='cuda:0')
c= tensor(41507.9336, device='cuda:0')
c= tensor(56652.2969, device='cuda:0')
c= tensor(66304.0469, device='cuda:0')
c= tensor(73113.3594, device='cuda:0')
c= tensor(860911.1250, device='cuda:0')
c= tensor(1016643.6250, device='cuda:0')
c= tensor(1031459.3750, device='cuda:0')
c= tensor(1164598.2500, device='cuda:0')
c= tensor(1203540.2500, device='cuda:0')
c= tensor(7802882., device='cuda:0')
c= tensor(7811936.5000, device='cuda:0')
c= tensor(13109988., device='cuda:0')
c= tensor(13182084., device='cuda:0')
c= tensor(13349233., device='cuda:0')
c= tensor(13923797., device='cuda:0')
c= tensor(14459632., device='cuda:0')
c= tensor(22308280., device='cuda:0')
c= tensor(26230848., device='cuda:0')
c= tensor(26237076., device='cuda:0')
c= tensor(34885652., device='cuda:0')
c= tensor(34928352., device='cuda:0')
c= tensor(35504628., device='cuda:0')
c= tensor(35531344., device='cuda:0')
c= tensor(36865964., device='cuda:0')
c= tensor(38369740., device='cuda:0')
c= tensor(38377944., device='cuda:0')
c= tensor(42952016., device='cuda:0')
c= tensor(1.6875e+08, device='cuda:0')
c= tensor(1.6876e+08, device='cuda:0')
c= tensor(3.4317e+08, device='cuda:0')
c= tensor(3.4422e+08, device='cuda:0')
c= tensor(3.4422e+08, device='cuda:0')
c= tensor(3.4423e+08, device='cuda:0')
c= tensor(3.4656e+08, device='cuda:0')
c= tensor(3.4784e+08, device='cuda:0')
c= tensor(3.4784e+08, device='cuda:0')
c= tensor(3.4785e+08, device='cuda:0')
c= tensor(3.4786e+08, device='cuda:0')
c= tensor(3.4786e+08, device='cuda:0')
c= tensor(3.4787e+08, device='cuda:0')
c= tensor(3.4787e+08, device='cuda:0')
c= tensor(3.4787e+08, device='cuda:0')
c= tensor(3.4787e+08, device='cuda:0')
c= tensor(3.4788e+08, device='cuda:0')
c= tensor(3.4788e+08, device='cuda:0')
c= tensor(3.4789e+08, device='cuda:0')
c= tensor(3.4789e+08, device='cuda:0')
c= tensor(3.4794e+08, device='cuda:0')
c= tensor(3.4797e+08, device='cuda:0')
c= tensor(3.4797e+08, device='cuda:0')
c= tensor(3.4798e+08, device='cuda:0')
c= tensor(3.4799e+08, device='cuda:0')
c= tensor(3.4800e+08, device='cuda:0')
c= tensor(3.4800e+08, device='cuda:0')
c= tensor(3.4801e+08, device='cuda:0')
c= tensor(3.4801e+08, device='cuda:0')
c= tensor(3.4801e+08, device='cuda:0')
c= tensor(3.4802e+08, device='cuda:0')
c= tensor(3.4803e+08, device='cuda:0')
c= tensor(3.4803e+08, device='cuda:0')
c= tensor(3.4805e+08, device='cuda:0')
c= tensor(3.4806e+08, device='cuda:0')
c= tensor(3.4807e+08, device='cuda:0')
c= tensor(3.4807e+08, device='cuda:0')
c= tensor(3.4807e+08, device='cuda:0')
c= tensor(3.4808e+08, device='cuda:0')
c= tensor(3.4808e+08, device='cuda:0')
c= tensor(3.4809e+08, device='cuda:0')
c= tensor(3.4809e+08, device='cuda:0')
c= tensor(3.4809e+08, device='cuda:0')
c= tensor(3.4810e+08, device='cuda:0')
c= tensor(3.4810e+08, device='cuda:0')
c= tensor(3.4811e+08, device='cuda:0')
c= tensor(3.4811e+08, device='cuda:0')
c= tensor(3.4812e+08, device='cuda:0')
c= tensor(3.4812e+08, device='cuda:0')
c= tensor(3.4812e+08, device='cuda:0')
c= tensor(3.4813e+08, device='cuda:0')
c= tensor(3.4813e+08, device='cuda:0')
c= tensor(3.4814e+08, device='cuda:0')
c= tensor(3.4815e+08, device='cuda:0')
c= tensor(3.4815e+08, device='cuda:0')
c= tensor(3.4816e+08, device='cuda:0')
c= tensor(3.4816e+08, device='cuda:0')
c= tensor(3.4816e+08, device='cuda:0')
c= tensor(3.4816e+08, device='cuda:0')
c= tensor(3.4817e+08, device='cuda:0')
c= tensor(3.4817e+08, device='cuda:0')
c= tensor(3.4818e+08, device='cuda:0')
c= tensor(3.4818e+08, device='cuda:0')
c= tensor(3.4818e+08, device='cuda:0')
c= tensor(3.4818e+08, device='cuda:0')
c= tensor(3.4819e+08, device='cuda:0')
c= tensor(3.4820e+08, device='cuda:0')
c= tensor(3.4820e+08, device='cuda:0')
c= tensor(3.4824e+08, device='cuda:0')
c= tensor(3.4825e+08, device='cuda:0')
c= tensor(3.4825e+08, device='cuda:0')
c= tensor(3.4828e+08, device='cuda:0')
c= tensor(3.4828e+08, device='cuda:0')
c= tensor(3.4830e+08, device='cuda:0')
c= tensor(3.4830e+08, device='cuda:0')
c= tensor(3.4831e+08, device='cuda:0')
c= tensor(3.4831e+08, device='cuda:0')
c= tensor(3.4832e+08, device='cuda:0')
c= tensor(3.4832e+08, device='cuda:0')
c= tensor(3.4833e+08, device='cuda:0')
c= tensor(3.4833e+08, device='cuda:0')
c= tensor(3.4833e+08, device='cuda:0')
c= tensor(3.4833e+08, device='cuda:0')
c= tensor(3.4833e+08, device='cuda:0')
c= tensor(3.4834e+08, device='cuda:0')
c= tensor(3.4835e+08, device='cuda:0')
c= tensor(3.4836e+08, device='cuda:0')
c= tensor(3.4836e+08, device='cuda:0')
c= tensor(3.4836e+08, device='cuda:0')
c= tensor(3.4837e+08, device='cuda:0')
c= tensor(3.4838e+08, device='cuda:0')
c= tensor(3.4839e+08, device='cuda:0')
c= tensor(3.4839e+08, device='cuda:0')
c= tensor(3.4839e+08, device='cuda:0')
c= tensor(3.4840e+08, device='cuda:0')
c= tensor(3.4840e+08, device='cuda:0')
c= tensor(3.4840e+08, device='cuda:0')
c= tensor(3.4840e+08, device='cuda:0')
c= tensor(3.4841e+08, device='cuda:0')
c= tensor(3.4845e+08, device='cuda:0')
c= tensor(3.4845e+08, device='cuda:0')
c= tensor(3.4846e+08, device='cuda:0')
c= tensor(3.4846e+08, device='cuda:0')
c= tensor(3.4847e+08, device='cuda:0')
c= tensor(3.4847e+08, device='cuda:0')
c= tensor(3.4854e+08, device='cuda:0')
c= tensor(3.4854e+08, device='cuda:0')
c= tensor(3.4854e+08, device='cuda:0')
c= tensor(3.4854e+08, device='cuda:0')
c= tensor(3.4854e+08, device='cuda:0')
c= tensor(3.4855e+08, device='cuda:0')
c= tensor(3.4856e+08, device='cuda:0')
c= tensor(3.4856e+08, device='cuda:0')
c= tensor(3.4858e+08, device='cuda:0')
c= tensor(3.4860e+08, device='cuda:0')
c= tensor(3.4861e+08, device='cuda:0')
c= tensor(3.4861e+08, device='cuda:0')
c= tensor(3.4861e+08, device='cuda:0')
c= tensor(3.4862e+08, device='cuda:0')
c= tensor(3.4862e+08, device='cuda:0')
c= tensor(3.4862e+08, device='cuda:0')
c= tensor(3.4863e+08, device='cuda:0')
c= tensor(3.4863e+08, device='cuda:0')
c= tensor(3.4864e+08, device='cuda:0')
c= tensor(3.4866e+08, device='cuda:0')
c= tensor(3.4867e+08, device='cuda:0')
c= tensor(3.4880e+08, device='cuda:0')
c= tensor(3.4880e+08, device='cuda:0')
c= tensor(3.4881e+08, device='cuda:0')
c= tensor(3.4881e+08, device='cuda:0')
c= tensor(3.4881e+08, device='cuda:0')
c= tensor(3.4886e+08, device='cuda:0')
c= tensor(3.4886e+08, device='cuda:0')
c= tensor(3.4887e+08, device='cuda:0')
c= tensor(3.4887e+08, device='cuda:0')
c= tensor(3.4887e+08, device='cuda:0')
c= tensor(3.4888e+08, device='cuda:0')
c= tensor(3.4888e+08, device='cuda:0')
c= tensor(3.4888e+08, device='cuda:0')
c= tensor(3.4889e+08, device='cuda:0')
c= tensor(3.4889e+08, device='cuda:0')
c= tensor(3.4889e+08, device='cuda:0')
c= tensor(3.4889e+08, device='cuda:0')
c= tensor(3.4890e+08, device='cuda:0')
c= tensor(3.4890e+08, device='cuda:0')
c= tensor(3.4891e+08, device='cuda:0')
c= tensor(3.4892e+08, device='cuda:0')
c= tensor(3.4892e+08, device='cuda:0')
c= tensor(3.4893e+08, device='cuda:0')
c= tensor(3.4894e+08, device='cuda:0')
c= tensor(3.4894e+08, device='cuda:0')
c= tensor(3.4895e+08, device='cuda:0')
c= tensor(3.4895e+08, device='cuda:0')
c= tensor(3.4895e+08, device='cuda:0')
c= tensor(3.4896e+08, device='cuda:0')
c= tensor(3.4898e+08, device='cuda:0')
c= tensor(3.4898e+08, device='cuda:0')
c= tensor(3.4899e+08, device='cuda:0')
c= tensor(3.4899e+08, device='cuda:0')
c= tensor(3.4901e+08, device='cuda:0')
c= tensor(3.4904e+08, device='cuda:0')
c= tensor(3.4904e+08, device='cuda:0')
c= tensor(3.4904e+08, device='cuda:0')
c= tensor(3.4904e+08, device='cuda:0')
c= tensor(3.4905e+08, device='cuda:0')
c= tensor(3.4905e+08, device='cuda:0')
c= tensor(3.4905e+08, device='cuda:0')
c= tensor(3.4906e+08, device='cuda:0')
c= tensor(3.4906e+08, device='cuda:0')
c= tensor(3.4906e+08, device='cuda:0')
c= tensor(3.4906e+08, device='cuda:0')
c= tensor(3.4906e+08, device='cuda:0')
c= tensor(3.4907e+08, device='cuda:0')
c= tensor(3.4907e+08, device='cuda:0')
c= tensor(3.4907e+08, device='cuda:0')
c= tensor(3.4908e+08, device='cuda:0')
c= tensor(3.4908e+08, device='cuda:0')
c= tensor(3.4909e+08, device='cuda:0')
c= tensor(3.4909e+08, device='cuda:0')
c= tensor(3.4910e+08, device='cuda:0')
c= tensor(3.4911e+08, device='cuda:0')
c= tensor(3.4913e+08, device='cuda:0')
c= tensor(3.4913e+08, device='cuda:0')
c= tensor(3.4913e+08, device='cuda:0')
c= tensor(3.4914e+08, device='cuda:0')
c= tensor(3.4914e+08, device='cuda:0')
c= tensor(3.4914e+08, device='cuda:0')
c= tensor(3.4914e+08, device='cuda:0')
c= tensor(3.4914e+08, device='cuda:0')
c= tensor(3.4916e+08, device='cuda:0')
c= tensor(3.4917e+08, device='cuda:0')
c= tensor(3.4917e+08, device='cuda:0')
c= tensor(3.4917e+08, device='cuda:0')
c= tensor(3.4918e+08, device='cuda:0')
c= tensor(3.4918e+08, device='cuda:0')
c= tensor(3.4919e+08, device='cuda:0')
c= tensor(3.4921e+08, device='cuda:0')
c= tensor(3.4921e+08, device='cuda:0')
c= tensor(3.4921e+08, device='cuda:0')
c= tensor(3.4922e+08, device='cuda:0')
c= tensor(3.4922e+08, device='cuda:0')
c= tensor(3.4922e+08, device='cuda:0')
c= tensor(3.4923e+08, device='cuda:0')
c= tensor(3.4923e+08, device='cuda:0')
c= tensor(3.4925e+08, device='cuda:0')
c= tensor(3.4925e+08, device='cuda:0')
c= tensor(3.4925e+08, device='cuda:0')
c= tensor(3.4925e+08, device='cuda:0')
c= tensor(3.4926e+08, device='cuda:0')
c= tensor(3.4926e+08, device='cuda:0')
c= tensor(3.4930e+08, device='cuda:0')
c= tensor(3.5126e+08, device='cuda:0')
c= tensor(3.5126e+08, device='cuda:0')
c= tensor(3.5127e+08, device='cuda:0')
c= tensor(3.5127e+08, device='cuda:0')
c= tensor(3.5128e+08, device='cuda:0')
c= tensor(3.5301e+08, device='cuda:0')
c= tensor(3.6384e+08, device='cuda:0')
c= tensor(3.6384e+08, device='cuda:0')
c= tensor(3.6447e+08, device='cuda:0')
c= tensor(3.6597e+08, device='cuda:0')
c= tensor(3.6599e+08, device='cuda:0')
c= tensor(3.9078e+08, device='cuda:0')
c= tensor(3.9079e+08, device='cuda:0')
c= tensor(3.9079e+08, device='cuda:0')
c= tensor(3.9343e+08, device='cuda:0')
c= tensor(3.9668e+08, device='cuda:0')
c= tensor(3.9669e+08, device='cuda:0')
c= tensor(3.9678e+08, device='cuda:0')
c= tensor(3.9716e+08, device='cuda:0')
c= tensor(4.0108e+08, device='cuda:0')
c= tensor(4.0182e+08, device='cuda:0')
c= tensor(4.0283e+08, device='cuda:0')
c= tensor(4.0355e+08, device='cuda:0')
c= tensor(4.0373e+08, device='cuda:0')
c= tensor(4.0379e+08, device='cuda:0')
c= tensor(4.1415e+08, device='cuda:0')
c= tensor(4.1415e+08, device='cuda:0')
c= tensor(4.1415e+08, device='cuda:0')
c= tensor(4.1423e+08, device='cuda:0')
c= tensor(4.1448e+08, device='cuda:0')
c= tensor(4.2626e+08, device='cuda:0')
c= tensor(4.2696e+08, device='cuda:0')
c= tensor(4.2696e+08, device='cuda:0')
c= tensor(4.2697e+08, device='cuda:0')
c= tensor(4.2701e+08, device='cuda:0')
c= tensor(4.2707e+08, device='cuda:0')
c= tensor(4.3149e+08, device='cuda:0')
c= tensor(4.3334e+08, device='cuda:0')
c= tensor(4.3345e+08, device='cuda:0')
c= tensor(4.3345e+08, device='cuda:0')
c= tensor(4.3346e+08, device='cuda:0')
c= tensor(4.3377e+08, device='cuda:0')
c= tensor(4.3441e+08, device='cuda:0')
c= tensor(4.3453e+08, device='cuda:0')
c= tensor(4.3454e+08, device='cuda:0')
c= tensor(4.5699e+08, device='cuda:0')
c= tensor(4.5701e+08, device='cuda:0')
c= tensor(4.5715e+08, device='cuda:0')
c= tensor(4.6014e+08, device='cuda:0')
c= tensor(4.6014e+08, device='cuda:0')
c= tensor(4.6050e+08, device='cuda:0')
c= tensor(4.7923e+08, device='cuda:0')
c= tensor(4.9335e+08, device='cuda:0')
c= tensor(4.9336e+08, device='cuda:0')
c= tensor(4.9337e+08, device='cuda:0')
c= tensor(4.9337e+08, device='cuda:0')
c= tensor(4.9337e+08, device='cuda:0')
c= tensor(4.9354e+08, device='cuda:0')
c= tensor(4.9358e+08, device='cuda:0')
c= tensor(4.9389e+08, device='cuda:0')
c= tensor(5.0045e+08, device='cuda:0')
c= tensor(5.0104e+08, device='cuda:0')
c= tensor(5.0108e+08, device='cuda:0')
c= tensor(5.0109e+08, device='cuda:0')
c= tensor(5.1259e+08, device='cuda:0')
c= tensor(5.1568e+08, device='cuda:0')
c= tensor(5.1603e+08, device='cuda:0')
c= tensor(5.1605e+08, device='cuda:0')
c= tensor(5.6762e+08, device='cuda:0')
c= tensor(5.6763e+08, device='cuda:0')
c= tensor(5.7252e+08, device='cuda:0')
c= tensor(5.7253e+08, device='cuda:0')
c= tensor(5.7299e+08, device='cuda:0')
c= tensor(5.7309e+08, device='cuda:0')
c= tensor(5.8448e+08, device='cuda:0')
c= tensor(5.8537e+08, device='cuda:0')
c= tensor(5.8537e+08, device='cuda:0')
c= tensor(5.8776e+08, device='cuda:0')
c= tensor(5.8902e+08, device='cuda:0')
c= tensor(5.8902e+08, device='cuda:0')
c= tensor(5.8936e+08, device='cuda:0')
c= tensor(5.9248e+08, device='cuda:0')
c= tensor(6.0783e+08, device='cuda:0')
c= tensor(6.0908e+08, device='cuda:0')
c= tensor(6.0908e+08, device='cuda:0')
c= tensor(6.0909e+08, device='cuda:0')
c= tensor(6.0909e+08, device='cuda:0')
c= tensor(6.0919e+08, device='cuda:0')
c= tensor(6.0924e+08, device='cuda:0')
c= tensor(6.0924e+08, device='cuda:0')
c= tensor(6.1040e+08, device='cuda:0')
c= tensor(6.1391e+08, device='cuda:0')
c= tensor(6.1396e+08, device='cuda:0')
c= tensor(6.1397e+08, device='cuda:0')
c= tensor(6.1401e+08, device='cuda:0')
c= tensor(6.1403e+08, device='cuda:0')
c= tensor(6.1406e+08, device='cuda:0')
c= tensor(6.1407e+08, device='cuda:0')
c= tensor(6.1407e+08, device='cuda:0')
c= tensor(6.2265e+08, device='cuda:0')
c= tensor(6.2270e+08, device='cuda:0')
c= tensor(6.2279e+08, device='cuda:0')
c= tensor(6.2358e+08, device='cuda:0')
c= tensor(6.2361e+08, device='cuda:0')
c= tensor(7.4394e+08, device='cuda:0')
c= tensor(7.4395e+08, device='cuda:0')
c= tensor(7.4465e+08, device='cuda:0')
c= tensor(7.4465e+08, device='cuda:0')
c= tensor(7.4465e+08, device='cuda:0')
c= tensor(7.4466e+08, device='cuda:0')
c= tensor(7.4479e+08, device='cuda:0')
c= tensor(7.4480e+08, device='cuda:0')
c= tensor(7.4616e+08, device='cuda:0')
c= tensor(7.4617e+08, device='cuda:0')
c= tensor(7.4618e+08, device='cuda:0')
c= tensor(7.7364e+08, device='cuda:0')
c= tensor(7.7424e+08, device='cuda:0')
c= tensor(7.7488e+08, device='cuda:0')
c= tensor(7.7826e+08, device='cuda:0')
c= tensor(7.8379e+08, device='cuda:0')
c= tensor(7.8379e+08, device='cuda:0')
c= tensor(7.8380e+08, device='cuda:0')
c= tensor(7.8410e+08, device='cuda:0')
c= tensor(7.8410e+08, device='cuda:0')
c= tensor(7.8411e+08, device='cuda:0')
c= tensor(7.8414e+08, device='cuda:0')
c= tensor(7.8414e+08, device='cuda:0')
c= tensor(7.8414e+08, device='cuda:0')
c= tensor(7.8415e+08, device='cuda:0')
c= tensor(7.8415e+08, device='cuda:0')
c= tensor(8.1085e+08, device='cuda:0')
c= tensor(8.1091e+08, device='cuda:0')
c= tensor(8.1146e+08, device='cuda:0')
c= tensor(8.1201e+08, device='cuda:0')
c= tensor(8.1201e+08, device='cuda:0')
c= tensor(8.1203e+08, device='cuda:0')
c= tensor(8.7431e+08, device='cuda:0')
c= tensor(8.9958e+08, device='cuda:0')
c= tensor(8.9962e+08, device='cuda:0')
c= tensor(8.9976e+08, device='cuda:0')
c= tensor(8.9976e+08, device='cuda:0')
c= tensor(9.0006e+08, device='cuda:0')
c= tensor(9.2561e+08, device='cuda:0')
c= tensor(9.2596e+08, device='cuda:0')
c= tensor(9.2599e+08, device='cuda:0')
c= tensor(9.2658e+08, device='cuda:0')
c= tensor(9.4548e+08, device='cuda:0')
c= tensor(9.4565e+08, device='cuda:0')
c= tensor(9.4565e+08, device='cuda:0')
c= tensor(9.4572e+08, device='cuda:0')
c= tensor(9.4574e+08, device='cuda:0')
c= tensor(9.4575e+08, device='cuda:0')
c= tensor(9.4890e+08, device='cuda:0')
c= tensor(9.4897e+08, device='cuda:0')
c= tensor(9.4897e+08, device='cuda:0')
c= tensor(9.4912e+08, device='cuda:0')
c= tensor(9.4917e+08, device='cuda:0')
c= tensor(9.4917e+08, device='cuda:0')
c= tensor(9.5200e+08, device='cuda:0')
c= tensor(9.5575e+08, device='cuda:0')
c= tensor(9.6098e+08, device='cuda:0')
c= tensor(9.6749e+08, device='cuda:0')
c= tensor(9.7352e+08, device='cuda:0')
c= tensor(9.7356e+08, device='cuda:0')
c= tensor(9.7362e+08, device='cuda:0')
c= tensor(9.7469e+08, device='cuda:0')
c= tensor(9.7943e+08, device='cuda:0')
c= tensor(9.7944e+08, device='cuda:0')
c= tensor(9.8092e+08, device='cuda:0')
c= tensor(1.0770e+09, device='cuda:0')
c= tensor(1.0818e+09, device='cuda:0')
c= tensor(1.0848e+09, device='cuda:0')
c= tensor(1.0928e+09, device='cuda:0')
c= tensor(1.0928e+09, device='cuda:0')
c= tensor(1.0928e+09, device='cuda:0')
c= tensor(1.0932e+09, device='cuda:0')
c= tensor(1.0936e+09, device='cuda:0')
c= tensor(1.0941e+09, device='cuda:0')
c= tensor(1.1912e+09, device='cuda:0')
c= tensor(1.1977e+09, device='cuda:0')
c= tensor(1.2007e+09, device='cuda:0')
c= tensor(1.2009e+09, device='cuda:0')
c= tensor(1.2031e+09, device='cuda:0')
c= tensor(1.2031e+09, device='cuda:0')
c= tensor(1.2031e+09, device='cuda:0')
c= tensor(1.2110e+09, device='cuda:0')
c= tensor(1.2110e+09, device='cuda:0')
c= tensor(1.2110e+09, device='cuda:0')
c= tensor(1.2111e+09, device='cuda:0')
c= tensor(1.2300e+09, device='cuda:0')
c= tensor(1.2307e+09, device='cuda:0')
c= tensor(1.2315e+09, device='cuda:0')
c= tensor(1.2315e+09, device='cuda:0')
c= tensor(1.2315e+09, device='cuda:0')
c= tensor(1.2315e+09, device='cuda:0')
c= tensor(1.2316e+09, device='cuda:0')
c= tensor(1.2319e+09, device='cuda:0')
c= tensor(1.2321e+09, device='cuda:0')
c= tensor(1.2321e+09, device='cuda:0')
c= tensor(1.2342e+09, device='cuda:0')
c= tensor(1.2342e+09, device='cuda:0')
c= tensor(1.2349e+09, device='cuda:0')
c= tensor(1.2349e+09, device='cuda:0')
c= tensor(1.2350e+09, device='cuda:0')
c= tensor(1.2350e+09, device='cuda:0')
c= tensor(1.2350e+09, device='cuda:0')
c= tensor(1.2350e+09, device='cuda:0')
c= tensor(1.2351e+09, device='cuda:0')
c= tensor(1.2374e+09, device='cuda:0')
c= tensor(1.2457e+09, device='cuda:0')
c= tensor(1.2457e+09, device='cuda:0')
c= tensor(1.2457e+09, device='cuda:0')
c= tensor(1.2535e+09, device='cuda:0')
c= tensor(1.2535e+09, device='cuda:0')
c= tensor(1.2891e+09, device='cuda:0')
c= tensor(1.2891e+09, device='cuda:0')
c= tensor(1.2895e+09, device='cuda:0')
c= tensor(1.2898e+09, device='cuda:0')
c= tensor(1.2898e+09, device='cuda:0')
c= tensor(1.2912e+09, device='cuda:0')
c= tensor(1.2916e+09, device='cuda:0')
c= tensor(1.3522e+09, device='cuda:0')
c= tensor(1.3522e+09, device='cuda:0')
c= tensor(1.3523e+09, device='cuda:0')
c= tensor(1.3523e+09, device='cuda:0')
c= tensor(1.3523e+09, device='cuda:0')
c= tensor(1.3523e+09, device='cuda:0')
c= tensor(1.3525e+09, device='cuda:0')
c= tensor(1.3525e+09, device='cuda:0')
c= tensor(1.3528e+09, device='cuda:0')
c= tensor(1.3529e+09, device='cuda:0')
c= tensor(1.3529e+09, device='cuda:0')
c= tensor(1.3530e+09, device='cuda:0')
c= tensor(1.3791e+09, device='cuda:0')
c= tensor(1.3801e+09, device='cuda:0')
c= tensor(1.3830e+09, device='cuda:0')
c= tensor(1.3831e+09, device='cuda:0')
c= tensor(1.3832e+09, device='cuda:0')
c= tensor(1.3832e+09, device='cuda:0')
c= tensor(1.3833e+09, device='cuda:0')
c= tensor(1.3918e+09, device='cuda:0')
c= tensor(1.3918e+09, device='cuda:0')
c= tensor(1.3918e+09, device='cuda:0')
c= tensor(1.3922e+09, device='cuda:0')
c= tensor(1.3934e+09, device='cuda:0')
c= tensor(1.3934e+09, device='cuda:0')
c= tensor(1.3934e+09, device='cuda:0')
c= tensor(1.4416e+09, device='cuda:0')
c= tensor(1.4416e+09, device='cuda:0')
c= tensor(1.4418e+09, device='cuda:0')
c= tensor(1.4419e+09, device='cuda:0')
c= tensor(1.4481e+09, device='cuda:0')
c= tensor(1.4613e+09, device='cuda:0')
c= tensor(1.4657e+09, device='cuda:0')
c= tensor(1.4676e+09, device='cuda:0')
c= tensor(1.4676e+09, device='cuda:0')
c= tensor(1.4682e+09, device='cuda:0')
c= tensor(1.4686e+09, device='cuda:0')
c= tensor(1.4686e+09, device='cuda:0')
c= tensor(1.4687e+09, device='cuda:0')
c= tensor(1.4687e+09, device='cuda:0')
c= tensor(1.4695e+09, device='cuda:0')
c= tensor(1.4695e+09, device='cuda:0')
c= tensor(1.4695e+09, device='cuda:0')
c= tensor(1.4695e+09, device='cuda:0')
c= tensor(1.4695e+09, device='cuda:0')
c= tensor(1.4722e+09, device='cuda:0')
c= tensor(1.4722e+09, device='cuda:0')
c= tensor(1.4725e+09, device='cuda:0')
c= tensor(1.4726e+09, device='cuda:0')
c= tensor(1.4728e+09, device='cuda:0')
c= tensor(1.4728e+09, device='cuda:0')
c= tensor(1.4728e+09, device='cuda:0')
c= tensor(1.4728e+09, device='cuda:0')
c= tensor(1.4778e+09, device='cuda:0')
c= tensor(1.4779e+09, device='cuda:0')
c= tensor(1.4779e+09, device='cuda:0')
c= tensor(1.4779e+09, device='cuda:0')
c= tensor(1.4964e+09, device='cuda:0')
c= tensor(1.5557e+09, device='cuda:0')
c= tensor(1.5557e+09, device='cuda:0')
c= tensor(1.5557e+09, device='cuda:0')
c= tensor(1.5557e+09, device='cuda:0')
c= tensor(1.5572e+09, device='cuda:0')
c= tensor(1.5572e+09, device='cuda:0')
c= tensor(1.5573e+09, device='cuda:0')
c= tensor(1.5573e+09, device='cuda:0')
c= tensor(1.5669e+09, device='cuda:0')
c= tensor(1.5698e+09, device='cuda:0')
c= tensor(1.5757e+09, device='cuda:0')
c= tensor(1.5758e+09, device='cuda:0')
c= tensor(1.5758e+09, device='cuda:0')
c= tensor(1.5758e+09, device='cuda:0')
c= tensor(1.5761e+09, device='cuda:0')
c= tensor(1.5761e+09, device='cuda:0')
c= tensor(1.5768e+09, device='cuda:0')
c= tensor(1.5857e+09, device='cuda:0')
c= tensor(1.6154e+09, device='cuda:0')
c= tensor(1.6154e+09, device='cuda:0')
c= tensor(1.6154e+09, device='cuda:0')
c= tensor(1.6154e+09, device='cuda:0')
c= tensor(1.7195e+09, device='cuda:0')
c= tensor(1.7197e+09, device='cuda:0')
c= tensor(1.7197e+09, device='cuda:0')
c= tensor(1.7197e+09, device='cuda:0')
c= tensor(1.7198e+09, device='cuda:0')
c= tensor(1.7198e+09, device='cuda:0')
c= tensor(1.7212e+09, device='cuda:0')
c= tensor(1.7212e+09, device='cuda:0')
c= tensor(1.7213e+09, device='cuda:0')
c= tensor(1.7213e+09, device='cuda:0')
c= tensor(1.7213e+09, device='cuda:0')
c= tensor(1.7213e+09, device='cuda:0')
c= tensor(1.7253e+09, device='cuda:0')
c= tensor(1.7581e+09, device='cuda:0')
c= tensor(1.7624e+09, device='cuda:0')
c= tensor(1.7659e+09, device='cuda:0')
c= tensor(1.7661e+09, device='cuda:0')
c= tensor(1.7661e+09, device='cuda:0')
c= tensor(1.7662e+09, device='cuda:0')
c= tensor(1.7670e+09, device='cuda:0')
c= tensor(1.7691e+09, device='cuda:0')
c= tensor(1.7734e+09, device='cuda:0')
c= tensor(1.7735e+09, device='cuda:0')
c= tensor(1.8478e+09, device='cuda:0')
c= tensor(1.8478e+09, device='cuda:0')
c= tensor(1.8483e+09, device='cuda:0')
c= tensor(1.8523e+09, device='cuda:0')
c= tensor(1.8525e+09, device='cuda:0')
c= tensor(1.8549e+09, device='cuda:0')
c= tensor(1.8596e+09, device='cuda:0')
c= tensor(1.8597e+09, device='cuda:0')
c= tensor(1.8597e+09, device='cuda:0')
c= tensor(1.8597e+09, device='cuda:0')
c= tensor(1.8602e+09, device='cuda:0')
c= tensor(1.8602e+09, device='cuda:0')
c= tensor(1.8636e+09, device='cuda:0')
c= tensor(2.0080e+09, device='cuda:0')
c= tensor(2.0173e+09, device='cuda:0')
c= tensor(2.0267e+09, device='cuda:0')
c= tensor(2.0278e+09, device='cuda:0')
c= tensor(2.0281e+09, device='cuda:0')
c= tensor(2.0282e+09, device='cuda:0')
c= tensor(2.0283e+09, device='cuda:0')
c= tensor(2.0308e+09, device='cuda:0')
c= tensor(2.0348e+09, device='cuda:0')
c= tensor(2.0349e+09, device='cuda:0')
c= tensor(2.1758e+09, device='cuda:0')
c= tensor(2.2046e+09, device='cuda:0')
c= tensor(2.2047e+09, device='cuda:0')
c= tensor(2.2047e+09, device='cuda:0')
c= tensor(2.2050e+09, device='cuda:0')
c= tensor(2.2055e+09, device='cuda:0')
c= tensor(2.2055e+09, device='cuda:0')
c= tensor(2.2234e+09, device='cuda:0')
c= tensor(2.2241e+09, device='cuda:0')
c= tensor(2.2243e+09, device='cuda:0')
c= tensor(2.2243e+09, device='cuda:0')
c= tensor(2.2244e+09, device='cuda:0')
c= tensor(2.2244e+09, device='cuda:0')
c= tensor(2.2244e+09, device='cuda:0')
c= tensor(2.2244e+09, device='cuda:0')
c= tensor(2.2245e+09, device='cuda:0')
c= tensor(4.4560e+09, device='cuda:0')
c= tensor(4.4562e+09, device='cuda:0')
c= tensor(4.4562e+09, device='cuda:0')
c= tensor(4.4562e+09, device='cuda:0')
c= tensor(4.4562e+09, device='cuda:0')
c= tensor(4.4562e+09, device='cuda:0')
c= tensor(4.4719e+09, device='cuda:0')
c= tensor(4.4737e+09, device='cuda:0')
c= tensor(4.5497e+09, device='cuda:0')
c= tensor(4.5497e+09, device='cuda:0')
c= tensor(4.5561e+09, device='cuda:0')
c= tensor(4.5569e+09, device='cuda:0')
c= tensor(4.5612e+09, device='cuda:0')
c= tensor(4.5988e+09, device='cuda:0')
c= tensor(4.5988e+09, device='cuda:0')
c= tensor(4.5988e+09, device='cuda:0')
c= tensor(4.5993e+09, device='cuda:0')
c= tensor(4.5994e+09, device='cuda:0')
c= tensor(4.5998e+09, device='cuda:0')
c= tensor(4.5999e+09, device='cuda:0')
c= tensor(4.6004e+09, device='cuda:0')
c= tensor(4.6012e+09, device='cuda:0')
c= tensor(4.6012e+09, device='cuda:0')
c= tensor(4.6047e+09, device='cuda:0')
c= tensor(4.6101e+09, device='cuda:0')
c= tensor(4.6101e+09, device='cuda:0')
c= tensor(4.6102e+09, device='cuda:0')
c= tensor(4.6177e+09, device='cuda:0')
c= tensor(4.6178e+09, device='cuda:0')
c= tensor(4.6414e+09, device='cuda:0')
c= tensor(4.6414e+09, device='cuda:0')
c= tensor(4.6426e+09, device='cuda:0')
c= tensor(4.6429e+09, device='cuda:0')
c= tensor(4.6454e+09, device='cuda:0')
c= tensor(4.6476e+09, device='cuda:0')
c= tensor(4.6476e+09, device='cuda:0')
c= tensor(4.6476e+09, device='cuda:0')
c= tensor(4.6477e+09, device='cuda:0')
c= tensor(4.6481e+09, device='cuda:0')
c= tensor(4.6514e+09, device='cuda:0')
c= tensor(4.6563e+09, device='cuda:0')
c= tensor(4.6563e+09, device='cuda:0')
c= tensor(4.6565e+09, device='cuda:0')
c= tensor(4.6568e+09, device='cuda:0')
c= tensor(4.6742e+09, device='cuda:0')
c= tensor(4.6745e+09, device='cuda:0')
c= tensor(4.6753e+09, device='cuda:0')
c= tensor(4.6758e+09, device='cuda:0')
c= tensor(4.6759e+09, device='cuda:0')
c= tensor(4.6759e+09, device='cuda:0')
c= tensor(4.6760e+09, device='cuda:0')
c= tensor(4.6763e+09, device='cuda:0')
c= tensor(4.6771e+09, device='cuda:0')
c= tensor(4.6772e+09, device='cuda:0')
c= tensor(4.6772e+09, device='cuda:0')
c= tensor(4.6772e+09, device='cuda:0')
c= tensor(4.6776e+09, device='cuda:0')
c= tensor(4.6777e+09, device='cuda:0')
c= tensor(4.6777e+09, device='cuda:0')
c= tensor(4.6777e+09, device='cuda:0')
c= tensor(4.6777e+09, device='cuda:0')
c= tensor(4.6779e+09, device='cuda:0')
c= tensor(4.6781e+09, device='cuda:0')
c= tensor(4.6827e+09, device='cuda:0')
c= tensor(4.6827e+09, device='cuda:0')
c= tensor(4.6839e+09, device='cuda:0')
c= tensor(4.6839e+09, device='cuda:0')
c= tensor(4.6839e+09, device='cuda:0')
c= tensor(4.6971e+09, device='cuda:0')
c= tensor(4.6974e+09, device='cuda:0')
c= tensor(4.6990e+09, device='cuda:0')
c= tensor(4.7032e+09, device='cuda:0')
c= tensor(4.7033e+09, device='cuda:0')
c= tensor(4.7082e+09, device='cuda:0')
c= tensor(4.7124e+09, device='cuda:0')
c= tensor(4.7124e+09, device='cuda:0')
c= tensor(4.7128e+09, device='cuda:0')
c= tensor(4.7129e+09, device='cuda:0')
c= tensor(4.7153e+09, device='cuda:0')
c= tensor(4.7236e+09, device='cuda:0')
c= tensor(4.7236e+09, device='cuda:0')
c= tensor(4.7240e+09, device='cuda:0')
c= tensor(4.7240e+09, device='cuda:0')
c= tensor(4.7243e+09, device='cuda:0')
c= tensor(4.7243e+09, device='cuda:0')
c= tensor(4.7243e+09, device='cuda:0')
c= tensor(4.7438e+09, device='cuda:0')
c= tensor(4.8798e+09, device='cuda:0')
c= tensor(4.8799e+09, device='cuda:0')
c= tensor(4.8800e+09, device='cuda:0')
c= tensor(4.8801e+09, device='cuda:0')
c= tensor(4.8818e+09, device='cuda:0')
c= tensor(4.8818e+09, device='cuda:0')
c= tensor(4.8818e+09, device='cuda:0')
c= tensor(4.8819e+09, device='cuda:0')
c= tensor(4.8849e+09, device='cuda:0')
c= tensor(4.8849e+09, device='cuda:0')
c= tensor(4.8853e+09, device='cuda:0')
c= tensor(4.8853e+09, device='cuda:0')
time to make c is 10.132238626480103
time for making loss is 10.132305145263672
p0 True
it  0 : 2301080576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 14% |
shape of L is 
torch.Size([])
memory (bytes)
4999692288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 14% |
memory (bytes)
5000048640
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  782489100.0
relative error loss 0.1601709
shape of L is 
torch.Size([])
memory (bytes)
5026820096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 14% |
memory (bytes)
5026897920
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  771799300.0
relative error loss 0.15798277
shape of L is 
torch.Size([])
memory (bytes)
5030223872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 14% |
memory (bytes)
5030223872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 14% |
error is  745877000.0
relative error loss 0.15267663
shape of L is 
torch.Size([])
memory (bytes)
5033488384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 36% | 14% |
memory (bytes)
5033512960
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  734973700.0
relative error loss 0.15044478
shape of L is 
torch.Size([])
memory (bytes)
5036646400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 38% | 14% |
memory (bytes)
5036646400
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  724056800.0
relative error loss 0.14821017
shape of L is 
torch.Size([])
memory (bytes)
5039960064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 14% |
memory (bytes)
5039960064
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  715088640.0
relative error loss 0.14637442
shape of L is 
torch.Size([])
memory (bytes)
5043077120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 14% |
memory (bytes)
5043171328
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  707375900.0
relative error loss 0.14479567
shape of L is 
torch.Size([])
memory (bytes)
5046394880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 14% |
memory (bytes)
5046394880
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  701861900.0
relative error loss 0.14366698
shape of L is 
torch.Size([])
memory (bytes)
5049524224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 14% |
memory (bytes)
5049524224
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  696483840.0
relative error loss 0.14256613
shape of L is 
torch.Size([])
memory (bytes)
5052829696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 14% |
memory (bytes)
5052833792
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  692976400.0
relative error loss 0.14184818
time to take a step is 286.6159563064575
it  1 : 3165733888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 14% |
shape of L is 
torch.Size([])
memory (bytes)
5056036864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 30% | 14% |
memory (bytes)
5056036864
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  692976400.0
relative error loss 0.14184818
shape of L is 
torch.Size([])
memory (bytes)
5059248128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 14% |
memory (bytes)
5059248128
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  689990660.0
relative error loss 0.14123702
shape of L is 
torch.Size([])
memory (bytes)
5062316032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 14% |
memory (bytes)
5062316032
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  687242000.0
relative error loss 0.14067438
shape of L is 
torch.Size([])
memory (bytes)
5065687040
| ID | GPU | MEM |
------------------
|  0 | 10% |  0% |
|  1 | 13% | 14% |
memory (bytes)
5065687040
| ID | GPU  | MEM |
-------------------
|  0 |   8% |  0% |
|  1 | 100% | 14% |
error is  684625150.0
relative error loss 0.14013873
shape of L is 
torch.Size([])
memory (bytes)
5068902400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 14% |
memory (bytes)
5068926976
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  682651900.0
relative error loss 0.13973482
shape of L is 
torch.Size([])
memory (bytes)
5072138240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 37% | 14% |
memory (bytes)
5072138240
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  679474700.0
relative error loss 0.13908446
shape of L is 
torch.Size([])
memory (bytes)
5075333120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% | 14% |
memory (bytes)
5075357696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 14% |
error is  678194200.0
relative error loss 0.13882235
shape of L is 
torch.Size([])
memory (bytes)
5078417408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 14% |
memory (bytes)
5078581248
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  676380900.0
relative error loss 0.13845119
shape of L is 
torch.Size([])
memory (bytes)
5081763840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 14% |
memory (bytes)
5081788416
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  675090940.0
relative error loss 0.13818714
shape of L is 
torch.Size([])
memory (bytes)
5084778496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 14% |
memory (bytes)
5085011968
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  673110000.0
relative error loss 0.13778165
time to take a step is 285.73242020606995
it  2 : 3165733888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 14% |
shape of L is 
torch.Size([])
memory (bytes)
5088215040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 14% |
memory (bytes)
5088215040
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  673110000.0
relative error loss 0.13778165
shape of L is 
torch.Size([])
memory (bytes)
5091233792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 14% |
memory (bytes)
5091233792
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  671740160.0
relative error loss 0.13750125
shape of L is 
torch.Size([])
memory (bytes)
5094637568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 14% |
memory (bytes)
5094637568
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  670765600.0
relative error loss 0.13730176
shape of L is 
torch.Size([])
memory (bytes)
5097832448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 14% |
memory (bytes)
5097832448
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  668487200.0
relative error loss 0.13683538
shape of L is 
torch.Size([])
memory (bytes)
5100916736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 14% |
memory (bytes)
5101076480
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  668260860.0
relative error loss 0.13678905
shape of L is 
torch.Size([])
memory (bytes)
5104287744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 14% |
memory (bytes)
5104287744
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  666687500.0
relative error loss 0.136467
shape of L is 
torch.Size([])
memory (bytes)
5107339264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 33% | 14% |
memory (bytes)
5107499008
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  666185700.0
relative error loss 0.1363643
shape of L is 
torch.Size([])
memory (bytes)
5110710272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 14% |
memory (bytes)
5110722560
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  665044200.0
relative error loss 0.13613063
shape of L is 
torch.Size([])
memory (bytes)
5113769984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 14% |
memory (bytes)
5113933824
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  664086800.0
relative error loss 0.13593465
shape of L is 
torch.Size([])
memory (bytes)
5117153280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 14% |
memory (bytes)
5117153280
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  663070700.0
relative error loss 0.13572666
time to take a step is 290.8968834877014
it  3 : 3165733888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 14% |
shape of L is 
torch.Size([])
memory (bytes)
5120348160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 14% |
memory (bytes)
5120348160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 14% |
error is  663070700.0
relative error loss 0.13572666
shape of L is 
torch.Size([])
memory (bytes)
5123579904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 14% |
memory (bytes)
5123579904
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  662337300.0
relative error loss 0.13557653
shape of L is 
torch.Size([])
memory (bytes)
5126795264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 14% |
memory (bytes)
5126795264
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  661379840.0
relative error loss 0.13538055
shape of L is 
torch.Size([])
memory (bytes)
5129973760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 14% |
memory (bytes)
5129973760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% | 14% |
error is  661657860.0
relative error loss 0.13543746
shape of L is 
torch.Size([])
memory (bytes)
5133234176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 14% |
memory (bytes)
5133234176
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  660973600.0
relative error loss 0.13529739
shape of L is 
torch.Size([])
memory (bytes)
5136416768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 14% |
memory (bytes)
5136416768
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  660594940.0
relative error loss 0.13521989
shape of L is 
torch.Size([])
memory (bytes)
5139628032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 14% |
memory (bytes)
5139652608
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  660006900.0
relative error loss 0.13509952
shape of L is 
torch.Size([])
memory (bytes)
5142863872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 14% |
memory (bytes)
5142863872
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  659595260.0
relative error loss 0.13501526
shape of L is 
torch.Size([])
memory (bytes)
5146042368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 38% | 14% |
memory (bytes)
5146066944
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  659268350.0
relative error loss 0.13494834
shape of L is 
torch.Size([])
memory (bytes)
5149184000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 14% |
memory (bytes)
5149290496
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  658667000.0
relative error loss 0.13482524
time to take a step is 284.9567160606384
it  4 : 3165733888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 14% |
shape of L is 
torch.Size([])
memory (bytes)
5152505856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 14% |
memory (bytes)
5152505856
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  658667000.0
relative error loss 0.13482524
shape of L is 
torch.Size([])
memory (bytes)
5155688448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 14% |
memory (bytes)
5155688448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 14% |
error is  658733600.0
relative error loss 0.13483888
shape of L is 
torch.Size([])
memory (bytes)
5158924288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 14% |
memory (bytes)
5158924288
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  658416640.0
relative error loss 0.134774
shape of L is 
torch.Size([])
memory (bytes)
5162139648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 14% |
memory (bytes)
5162139648
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  658156300.0
relative error loss 0.13472071
shape of L is 
torch.Size([])
memory (bytes)
5165309952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 32% | 14% |
memory (bytes)
5165309952
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  657538300.0
relative error loss 0.13459422
shape of L is 
torch.Size([])
memory (bytes)
5168570368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 14% |
memory (bytes)
5168570368
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  657226500.0
relative error loss 0.13453038
shape of L is 
torch.Size([])
memory (bytes)
5171703808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 36% | 14% |
memory (bytes)
5171789824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% | 14% |
error is  656976900.0
relative error loss 0.1344793
shape of L is 
torch.Size([])
memory (bytes)
5174996992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 34% | 14% |
memory (bytes)
5174996992
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  656405500.0
relative error loss 0.13436234
shape of L is 
torch.Size([])
memory (bytes)
5178130432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 14% |
memory (bytes)
5178200064
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  656144640.0
relative error loss 0.13430893
shape of L is 
torch.Size([])
memory (bytes)
5181415424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 14% |
memory (bytes)
5181415424
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  655784960.0
relative error loss 0.13423531
time to take a step is 275.33307814598083
it  5 : 3165733888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 14% |
shape of L is 
torch.Size([])
memory (bytes)
5184552960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 14% |
memory (bytes)
5184552960
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  655784960.0
relative error loss 0.13423531
shape of L is 
torch.Size([])
memory (bytes)
5187833856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 14% |
memory (bytes)
5187833856
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  655300600.0
relative error loss 0.13413617
shape of L is 
torch.Size([])
memory (bytes)
5191024640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 14% |
memory (bytes)
5191024640
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  655633150.0
relative error loss 0.13420424
shape of L is 
torch.Size([])
memory (bytes)
5194272768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 14% |
memory (bytes)
5194272768
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  655092500.0
relative error loss 0.13409357
shape of L is 
torch.Size([])
memory (bytes)
5197484032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 14% |
memory (bytes)
5197484032
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  654876700.0
relative error loss 0.13404939
shape of L is 
torch.Size([])
memory (bytes)
5200674816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 14% |
memory (bytes)
5200695296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 14% |
error is  654483200.0
relative error loss 0.13396885
shape of L is 
torch.Size([])
memory (bytes)
5203877888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 14% |
memory (bytes)
5203906560
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  654228200.0
relative error loss 0.13391666
shape of L is 
torch.Size([])
memory (bytes)
5207126016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 14% |
memory (bytes)
5207126016
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  653955840.0
relative error loss 0.1338609
shape of L is 
torch.Size([])
memory (bytes)
5210329088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 36% | 14% |
memory (bytes)
5210329088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% | 14% |
error is  653591800.0
relative error loss 0.1337864
shape of L is 
torch.Size([])
memory (bytes)
5213556736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 14% |
memory (bytes)
5213556736
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  653309200.0
relative error loss 0.13372853
time to take a step is 330.84514331817627
it  6 : 3166142976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 14% |
shape of L is 
torch.Size([])
memory (bytes)
5216763904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 14% |
memory (bytes)
5216763904
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  653309200.0
relative error loss 0.13372853
shape of L is 
torch.Size([])
memory (bytes)
5219823616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 36% | 14% |
memory (bytes)
5219975168
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  652833000.0
relative error loss 0.13363107
shape of L is 
torch.Size([])
memory (bytes)
5223190528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 14% |
memory (bytes)
5223190528
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  652494100.0
relative error loss 0.13356169
shape of L is 
torch.Size([])
memory (bytes)
5226332160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 14% |
memory (bytes)
5226405888
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  652115460.0
relative error loss 0.13348418
shape of L is 
torch.Size([])
memory (bytes)
5229588480
| ID | GPU | MEM |
------------------
|  0 |  1% |  0% |
|  1 | 35% | 14% |
memory (bytes)
5229617152
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  651933700.0
relative error loss 0.13344698
shape of L is 
torch.Size([])
memory (bytes)
5232832512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 14% |
memory (bytes)
5232832512
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  651644160.0
relative error loss 0.13338771
shape of L is 
torch.Size([])
memory (bytes)
5236035584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 14% |
memory (bytes)
5236035584
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  651491300.0
relative error loss 0.13335644
shape of L is 
torch.Size([])
memory (bytes)
5239222272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 14% |
memory (bytes)
5239246848
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  651347700.0
relative error loss 0.13332704
shape of L is 
torch.Size([])
memory (bytes)
5242400768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 14% |
memory (bytes)
5242486784
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  651179000.0
relative error loss 0.1332925
shape of L is 
torch.Size([])
memory (bytes)
5245685760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 14% |
memory (bytes)
5245714432
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  651006700.0
relative error loss 0.13325724
time to take a step is 355.6616508960724
it  7 : 3165733888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 14% |
shape of L is 
torch.Size([])
memory (bytes)
5248925696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 14% |
memory (bytes)
5248925696
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  651006700.0
relative error loss 0.13325724
shape of L is 
torch.Size([])
memory (bytes)
5252108288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 36% | 14% |
memory (bytes)
5252136960
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  650635260.0
relative error loss 0.1331812
shape of L is 
torch.Size([])
memory (bytes)
5255335936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 32% | 14% |
memory (bytes)
5255356416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% | 14% |
error is  650784500.0
relative error loss 0.13321175
shape of L is 
torch.Size([])
memory (bytes)
5258567680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 14% |
memory (bytes)
5258567680
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  650469600.0
relative error loss 0.1331473
shape of L is 
torch.Size([])
memory (bytes)
5261770752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 37% | 14% |
memory (bytes)
5261770752
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  650304260.0
relative error loss 0.13311344
shape of L is 
torch.Size([])
memory (bytes)
5264994304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 14% |
memory (bytes)
5264994304
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  650065660.0
relative error loss 0.13306461
shape of L is 
torch.Size([])
memory (bytes)
5268205568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 14% |
memory (bytes)
5268205568
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  649882100.0
relative error loss 0.13302703
shape of L is 
torch.Size([])
memory (bytes)
5271404544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 14% |
memory (bytes)
5271404544
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  649725700.0
relative error loss 0.13299502
shape of L is 
torch.Size([])
memory (bytes)
5274652672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 14% |
memory (bytes)
5274652672
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  649473300.0
relative error loss 0.13294335
shape of L is 
torch.Size([])
memory (bytes)
5277765632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 36% | 14% |
memory (bytes)
5277859840
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  649320700.0
relative error loss 0.13291211
time to take a step is 355.89342975616455
it  8 : 3165733888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 14% |
shape of L is 
torch.Size([])
memory (bytes)
5281071104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 14% |
memory (bytes)
5281071104
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  649320700.0
relative error loss 0.13291211
shape of L is 
torch.Size([])
memory (bytes)
5284282368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 14% |
memory (bytes)
5284282368
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  649141500.0
relative error loss 0.13287544
shape of L is 
torch.Size([])
memory (bytes)
5287489536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 14% |
memory (bytes)
5287489536
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  648962800.0
relative error loss 0.13283886
shape of L is 
torch.Size([])
memory (bytes)
5290717184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 14% |
memory (bytes)
5290717184
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  648799200.0
relative error loss 0.13280538
shape of L is 
torch.Size([])
memory (bytes)
5293924352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 14% |
memory (bytes)
5293936640
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  648633340.0
relative error loss 0.13277142
shape of L is 
torch.Size([])
memory (bytes)
5297115136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 33% | 14% |
memory (bytes)
5297143808
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  648508700.0
relative error loss 0.1327459
shape of L is 
torch.Size([])
memory (bytes)
5300359168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 14% |
memory (bytes)
5300359168
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  648402940.0
relative error loss 0.13272426
shape of L is 
torch.Size([])
memory (bytes)
5303451648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 14% |
memory (bytes)
5303566336
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  648303100.0
relative error loss 0.13270383
shape of L is 
torch.Size([])
memory (bytes)
5306777600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 30% | 14% |
memory (bytes)
5306777600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% | 14% |
error is  648251140.0
relative error loss 0.13269319
shape of L is 
torch.Size([])
memory (bytes)
5309988864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 14% |
memory (bytes)
5309988864
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  648192260.0
relative error loss 0.13268113
time to take a step is 360.8267123699188
it  9 : 3165733888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 14% |
shape of L is 
torch.Size([])
memory (bytes)
5313200128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 14% |
memory (bytes)
5313200128
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  648192260.0
relative error loss 0.13268113
shape of L is 
torch.Size([])
memory (bytes)
5316407296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 14% |
memory (bytes)
5316407296
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  648172800.0
relative error loss 0.13267715
shape of L is 
torch.Size([])
memory (bytes)
5319606272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 14% |
memory (bytes)
5319606272
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  648138240.0
relative error loss 0.13267007
shape of L is 
torch.Size([])
memory (bytes)
5322829824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 14% |
memory (bytes)
5322829824
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  648093440.0
relative error loss 0.13266091
shape of L is 
torch.Size([])
memory (bytes)
5326049280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 14% |
memory (bytes)
5326049280
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  648051460.0
relative error loss 0.13265231
shape of L is 
torch.Size([])
memory (bytes)
5329268736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 14% |
memory (bytes)
5329268736
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  647986200.0
relative error loss 0.13263895
shape of L is 
torch.Size([])
memory (bytes)
5332480000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 32% | 14% |
memory (bytes)
5332480000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 14% |
error is  647918340.0
relative error loss 0.13262506
shape of L is 
torch.Size([])
memory (bytes)
5335621632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 14% |
memory (bytes)
5335621632
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  647811300.0
relative error loss 0.13260315
shape of L is 
torch.Size([])
memory (bytes)
5338898432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 33% | 14% |
memory (bytes)
5338898432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% | 14% |
error is  647691500.0
relative error loss 0.13257863
shape of L is 
torch.Size([])
memory (bytes)
5342117888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 14% |
memory (bytes)
5342117888
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  647548400.0
relative error loss 0.13254935
time to take a step is 356.70221972465515
it  10 : 3165733888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 14% |
shape of L is 
torch.Size([])
memory (bytes)
5345304576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 14% |
memory (bytes)
5345333248
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  647548400.0
relative error loss 0.13254935
shape of L is 
torch.Size([])
memory (bytes)
5348536320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 14% |
memory (bytes)
5348536320
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  647342850.0
relative error loss 0.13250726
shape of L is 
torch.Size([])
memory (bytes)
5351735296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 14% |
memory (bytes)
5351735296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 14% |
error is  647226100.0
relative error loss 0.13248336
shape of L is 
torch.Size([])
memory (bytes)
5354975232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 14% |
memory (bytes)
5354975232
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  647134200.0
relative error loss 0.13246456
shape of L is 
torch.Size([])
memory (bytes)
5358116864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 34% | 14% |
memory (bytes)
5358116864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% | 14% |
error is  647061250.0
relative error loss 0.13244963
shape of L is 
torch.Size([])
memory (bytes)
5361401856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 14% |
memory (bytes)
5361401856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 14% |
error is  646985200.0
relative error loss 0.13243406
shape of L is 
torch.Size([])
memory (bytes)
5364613120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 14% |
memory (bytes)
5364613120
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  646916600.0
relative error loss 0.13242002
shape of L is 
torch.Size([])
memory (bytes)
5367832576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 14% |
memory (bytes)
5367832576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% | 14% |
error is  646823200.0
relative error loss 0.13240089
shape of L is 
torch.Size([])
memory (bytes)
5370949632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 14% |
memory (bytes)
5371047936
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  646705900.0
relative error loss 0.1323769
shape of L is 
torch.Size([])
memory (bytes)
5374234624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 14% |
memory (bytes)
5374263296
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  646594300.0
relative error loss 0.13235404
time to take a step is 353.9542067050934
it  11 : 3165733888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 14% |
shape of L is 
torch.Size([])
memory (bytes)
5377474560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 14% |
memory (bytes)
5377474560
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  646594300.0
relative error loss 0.13235404
shape of L is 
torch.Size([])
memory (bytes)
5380665344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% | 14% |
memory (bytes)
5380665344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 14% |
error is  646460400.0
relative error loss 0.13232663
shape of L is 
torch.Size([])
memory (bytes)
5383909376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 14% |
memory (bytes)
5383909376
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  646359550.0
relative error loss 0.132306
shape of L is 
torch.Size([])
memory (bytes)
5387026432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 14% |
memory (bytes)
5387124736
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  646231800.0
relative error loss 0.13227984
shape of L is 
torch.Size([])
memory (bytes)
5390331904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 14% |
memory (bytes)
5390331904
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  646173200.0
relative error loss 0.13226783
shape of L is 
torch.Size([])
memory (bytes)
5393543168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 14% |
memory (bytes)
5393543168
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  646081800.0
relative error loss 0.13224913
shape of L is 
torch.Size([])
memory (bytes)
5396721664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 14% |
memory (bytes)
5396750336
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  646026240.0
relative error loss 0.13223776
shape of L is 
torch.Size([])
memory (bytes)
5399965696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 14% |
memory (bytes)
5399965696
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  645972000.0
relative error loss 0.13222665
shape of L is 
torch.Size([])
memory (bytes)
5403160576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 14% |
memory (bytes)
5403160576
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  645880060.0
relative error loss 0.13220784
shape of L is 
torch.Size([])
memory (bytes)
5406400512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 14% |
memory (bytes)
5406400512
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  645790700.0
relative error loss 0.13218956
time to take a step is 364.9456112384796
it  12 : 3165733888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 14% |
shape of L is 
torch.Size([])
memory (bytes)
5409452032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 14% |
memory (bytes)
5409611776
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  645790700.0
relative error loss 0.13218956
shape of L is 
torch.Size([])
memory (bytes)
5412827136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 14% |
memory (bytes)
5412827136
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  645738500.0
relative error loss 0.13217886
shape of L is 
torch.Size([])
memory (bytes)
5416026112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 14% |
memory (bytes)
5416026112
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  645684740.0
relative error loss 0.13216786
shape of L is 
torch.Size([])
memory (bytes)
5419237376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 14% |
memory (bytes)
5419237376
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  645565700.0
relative error loss 0.1321435
shape of L is 
torch.Size([])
memory (bytes)
5422452736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 14% |
memory (bytes)
5422452736
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  645469200.0
relative error loss 0.13212374
shape of L is 
torch.Size([])
memory (bytes)
5425664000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 14% |
memory (bytes)
5425664000
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  645389300.0
relative error loss 0.13210739
shape of L is 
torch.Size([])
memory (bytes)
5428887552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 33% | 14% |
memory (bytes)
5428887552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% | 14% |
error is  645286660.0
relative error loss 0.13208637
shape of L is 
torch.Size([])
memory (bytes)
5432082432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 14% |
memory (bytes)
5432111104
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  645219100.0
relative error loss 0.13207254
shape of L is 
torch.Size([])
memory (bytes)
5435330560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 14% |
memory (bytes)
5435330560
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  645112300.0
relative error loss 0.1320507
shape of L is 
torch.Size([])
memory (bytes)
5438533632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 14% |
memory (bytes)
5438533632
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  645019400.0
relative error loss 0.13203166
time to take a step is 353.41949582099915
it  13 : 3165733888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 14% |
shape of L is 
torch.Size([])
memory (bytes)
5441671168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 14% |
memory (bytes)
5441740800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% | 14% |
error is  645019400.0
relative error loss 0.13203166
shape of L is 
torch.Size([])
memory (bytes)
5444947968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 34% | 14% |
memory (bytes)
5444947968
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  644787200.0
relative error loss 0.13198413
shape of L is 
torch.Size([])
memory (bytes)
5447999488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 14% |
memory (bytes)
5448159232
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  644717300.0
relative error loss 0.13196982
shape of L is 
torch.Size([])
memory (bytes)
5451374592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 30% | 14% |
memory (bytes)
5451374592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% | 14% |
error is  644629500.0
relative error loss 0.13195185
shape of L is 
torch.Size([])
memory (bytes)
5454589952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 14% |
memory (bytes)
5454589952
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  644498940.0
relative error loss 0.13192514
shape of L is 
torch.Size([])
memory (bytes)
5457809408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 14% |
memory (bytes)
5457809408
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  644358400.0
relative error loss 0.13189636
shape of L is 
torch.Size([])
memory (bytes)
5461024768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 14% |
memory (bytes)
5461024768
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  644290050.0
relative error loss 0.13188237
shape of L is 
torch.Size([])
memory (bytes)
5464109056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 34% | 14% |
memory (bytes)
5464240128
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  644221700.0
relative error loss 0.13186838
shape of L is 
torch.Size([])
memory (bytes)
5467447296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 14% |
memory (bytes)
5467447296
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  644157440.0
relative error loss 0.13185523
shape of L is 
torch.Size([])
memory (bytes)
5470670848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 14% |
memory (bytes)
5470670848
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  644102660.0
relative error loss 0.13184401
time to take a step is 362.97741317749023
it  14 : 3165733888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 14% |
shape of L is 
torch.Size([])
memory (bytes)
5473869824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 14% |
memory (bytes)
5473869824
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  644102660.0
relative error loss 0.13184401
shape of L is 
torch.Size([])
memory (bytes)
5477085184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 14% |
memory (bytes)
5477085184
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  644027140.0
relative error loss 0.13182856
shape of L is 
torch.Size([])
memory (bytes)
5480304640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 14% |
memory (bytes)
5480304640
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  643932900.0
relative error loss 0.13180928
shape of L is 
torch.Size([])
memory (bytes)
5483507712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 14% |
memory (bytes)
5483507712
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  643883500.0
relative error loss 0.13179916
shape of L is 
torch.Size([])
memory (bytes)
5486620672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 14% |
memory (bytes)
5486723072
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  643805700.0
relative error loss 0.13178323
shape of L is 
torch.Size([])
memory (bytes)
5489930240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 14% |
memory (bytes)
5489930240
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  643699460.0
relative error loss 0.13176148
shape of L is 
torch.Size([])
memory (bytes)
5493141504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 14% |
memory (bytes)
5493141504
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  643635700.0
relative error loss 0.13174844
shape of L is 
torch.Size([])
memory (bytes)
5496315904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 14% |
memory (bytes)
5496352768
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  643600900.0
relative error loss 0.1317413
shape of L is 
torch.Size([])
memory (bytes)
5499568128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 14% |
memory (bytes)
5499568128
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  643568100.0
relative error loss 0.1317346
shape of L is 
torch.Size([])
memory (bytes)
5502693376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 14% |
memory (bytes)
5502787584
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 14% |
error is  643460100.0
relative error loss 0.13171248
time to take a step is 358.0268294811249
sum tnnu_Z after tensor(12586614., device='cuda:0')
shape of features
(8517,)
shape of features
(8517,)
number of orig particles 34068
number of new particles after remove low mass 33019
tnuZ shape should be parts x labs
torch.Size([34068, 702])
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
error without small mass is  782454000.0
relative error without small mass is  0.16016372
nnu_Z shape should be number of particles by maxV
(34068, 702)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
shape of features
(34068,)
Wed Feb 1 10:39:47 EST 2023
