Wed Feb 1 22:31:04 EST 2023
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  0% |  0% |
numbers of X: 19069138
numbers of Z: 23931
shape of features
(23931,)
shape of features
(23931,)
ZX	Vol	Parts	Cubes	Eps
Z	0.01998922710548072	23931	23.931	0.09417704334780602
X	0.017774473533738927	412	0.412	0.3507248272887892
X	0.01828870647101804	4446	4.446	0.160227790442159
X	0.01836256606645322	1129	1.129	0.25336491569617214
X	0.01824880282849515	5333	5.333	0.15069109835798777
X	0.018393644930723332	3790	3.79	0.1693074997164318
X	0.01825780980573933	6004	6.004	0.1448780743895005
X	0.018454379067674425	41226	41.226	0.07649670802989104
X	0.018356766124713833	14114	14.114	0.10915627008637331
X	0.018178918299390695	3046	3.046	0.18138973776329878
X	0.018284762076977764	6403	6.403	0.14187371378191763
X	0.018339739987583696	2465	2.465	0.19522049824222246
X	0.018570754563435137	47576	47.576	0.07308258072043759
X	0.01821178368390743	3044	3.044	0.18153872392371204
X	0.018460302235256666	149773	149.773	0.04976627014730601
X	0.018423285433126494	13980	13.98	0.10963603052106698
X	0.018336470683088333	11678	11.678	0.11622933664737853
X	0.01887579585744898	34578	34.578	0.0817275476076418
X	0.018421760247916617	21940	21.94	0.09434049866955206
X	0.01843727520528845	92330	92.33	0.05845001598014794
X	0.01845886198779497	75142	75.142	0.0626287952460525
X	0.018323913768099043	3384	3.384	0.17560193410865957
X	0.018537391247227194	97005	97.005	0.05759942730721474
X	0.018346902408336	6051	6.051	0.14473665539522187
X	0.018329576900112426	18927	18.927	0.0989365782430669
X	0.017722788384381533	685	0.685	0.2957652592147589
X	0.01846845421844054	51377	51.377	0.07110289253174887
X	0.018429285686933927	30484	30.484	0.08455621152163868
X	0.018299616824848354	2131	2.131	0.20477936037087824
X	0.018405691680702572	24199	24.199	0.09128194971790272
X	0.019162397366927868	443650	443.65	0.03508620948056177
X	0.018423885958154262	3123	3.123	0.18069095676443472
X	0.01902470667730689	469469	469.469	0.034348176281637215
X	0.01839913659036945	11641	11.641	0.11648474449904822
X	0.018339845894413336	2281	2.281	0.20033498549916282
X	0.018331478918251418	2795	2.795	0.18718536781940018
X	0.01841417936612536	36895	36.895	0.07932223664573008
X	0.01883130330777068	67481	67.481	0.0653480986843665
X	0.017998180223123036	906	0.906	0.27083209728586605
X	0.018399197515422606	5437	5.437	0.150134274457091
X	0.018038015919579698	1246	1.246	0.2437194790701121
X	0.01841722296700224	1625	1.625	0.22462439834259476
X	0.017231612134028778	829	0.829	0.2749521542086418
X	0.01778859090055405	583	0.583	0.31248151452785233
X	0.018004541092625132	1510	1.51	0.22845554470956964
X	0.01752488809255624	446	0.446	0.3399696370219795
X	0.017926582456692496	460	0.46	0.3390365789448923
X	0.01829360138883271	1984	1.984	0.20969392460367292
X	0.01815528884340601	1096	1.096	0.25491625888052677
X	0.017707043879680927	508	0.508	0.3266586155320445
X	0.018033269791376236	10468	10.468	0.11987729152363656
X	0.01853965972830359	4892	4.892	0.15590920972859454
X	0.018284218044471744	3568	3.568	0.1724052614053889
X	0.018270640058755484	3208	3.208	0.178582871041179
X	0.018294130720422432	1304	1.304	0.24118190928309227
X	0.018398720268422427	2740	2.74	0.1886596471725573
X	0.018172238107474033	3740	3.74	0.16937355593940523
X	0.018236483155407966	743	0.743	0.29061459772905596
X	0.018329268259359727	2266	2.266	0.2007374520147984
X	0.018118968901134156	1359	1.359	0.23712172241091517
X	0.01763761207521563	1426	1.426	0.23126331871268008
X	0.018439766205833697	3716	3.716	0.17056630250662633
X	0.018229886235101026	536	0.536	0.3239960555008813
X	0.01875094261935737	6916	6.916	0.13944064192249284
X	0.018234740223924226	1551	1.551	0.2273854095718542
X	0.017568665595658933	700	0.7	0.29278371170322653
X	0.018268340945305165	1517	1.517	0.22921225960566916
X	0.017835294499780396	842	0.842	0.2766874098335245
X	0.018121143927588318	1412	1.412	0.2341263552664726
X	0.018382043122113237	1711	1.711	0.22065541531234878
X	0.017735227866816975	783	0.783	0.2829382546835104
X	0.016920900262923325	840	0.84	0.27209134441288263
X	0.018378141680079545	2066	2.066	0.2072003220154124
X	0.017909943962970472	878	0.878	0.2732330544163449
X	0.01793211781849799	2351	2.351	0.19684587490724223
X	0.01842254641392734	1443	1.443	0.2337192278048021
X	0.018602637271624114	2857	2.857	0.18673315106345212
X	0.01824649336453135	730	0.73	0.2923830448329813
X	0.017956129079753715	239	0.239	0.421960313113798
X	0.017838882818755904	1208	1.208	0.24533917815832007
X	0.01914253009270909	3082	3.082	0.18381836605069868
X	0.018248482702175883	1492	1.492	0.23040185908074765
X	0.016706690696839527	401	0.401	0.34666927580016843
X	0.01844196349309636	6070	6.07	0.14483476609661655
X	0.01743800947554822	436	0.436	0.3419821775676089
X	0.01822369295482532	1025	1.025	0.2609981975030216
X	0.01755075440825037	445	0.445	0.3403914106698567
X	0.018539734212131797	2405	2.405	0.1975435288917648
X	0.017643998134390975	434	0.434	0.34385004952070597
X	0.017920319411579594	1909	1.909	0.2109501989403201
X	0.018418282586401897	1405	1.405	0.23578939560485684
X	0.017925801399975634	470	0.47	0.3366099176123533
X	0.017596495155016455	599	0.599	0.3085553757924804
X	0.01804686254190952	573	0.573	0.3158025963124338
X	0.01835199173013155	1543	1.543	0.22826487750942373
X	0.017899246114967186	1803	1.803	0.21492140498239146
X	0.01823408411625575	1817	1.817	0.21569645569498558
X	0.01844430955611479	2151	2.151	0.20467933503935
X	0.018340997506999558	5233	5.233	0.15189987123001478
X	0.01832764097193548	1659	1.659	0.22271701343610412
X	0.018181767516962852	739	0.739	0.2908465303844337
X	0.018402118457838377	8214	8.214	0.1308491907854163
X	0.018231442577694493	956	0.956	0.2671700144091839
X	0.018271645295140224	3164	3.164	0.17941017107163
X	0.01727260704643857	819	0.819	0.2762854390212991
X	0.01780547987679374	1194	1.194	0.24614052279597048
X	0.017912710021306247	880	0.88	0.27303995755233895
X	0.018198969235487965	2561	2.561	0.19225563189601522
X	0.01777177172160474	288	0.288	0.39516585610891236
X	0.018090369520811434	1266	1.266	0.2426635671581191
X	0.018255429298624146	1386	1.386	0.23616177628889506
X	0.01717598034931764	650	0.65	0.2978535124992952
X	0.017747591128725183	502	0.502	0.328205029915869
X	0.0174159366007781	610	0.61	0.3056368159691488
X	0.018356132349747985	2269	2.269	0.20074692188070964
X	0.018174974849550392	1447	1.447	0.23245296805910765
X	0.017755299804550213	643	0.643	0.30225420091300675
X	0.01811995113258362	386	0.386	0.36073634926826714
X	0.018142491406582466	425	0.425	0.349491001831538
X	0.018359320144113345	2664	2.664	0.19030087777338753
X	0.017704096786006485	736	0.736	0.2886680068691375
X	0.018350298981239792	3878	3.878	0.1678849555197824
X	0.018084194672694745	996	0.996	0.26283303199024677
X	0.017776299667834165	782	0.782	0.28327714490578637
X	0.018261165822371573	929	0.929	0.269880141208236
X	0.01814708060087843	913	0.913	0.2708810745410037
X	0.018251212513578457	1286	1.286	0.24211233589341755
X	0.018233544293077712	1028	1.028	0.2607910365859107
X	0.018021293775114883	740	0.74	0.2898576379974191
X	0.018436169330422556	11328	11.328	0.11762667369342604
X	0.0180676755000946	1726	1.726	0.21875290555137086
X	0.018267988948012855	2234	2.234	0.2014663525438978
X	0.018340846995542757	852	0.852	0.27818044511665024
X	0.017589618008889307	1200	1.2	0.244732541713719
X	0.01819943217207874	1174	1.174	0.24934265549075815
X	0.01820198022900318	2377	2.377	0.19710447547342214
X	0.018335626964465127	602	0.602	0.31229600202993074
X	0.017776315913247846	267	0.267	0.40530021376609926
X	0.01805763350271297	1002	1.002	0.2621788802410817
X	0.018378775976572573	1056	1.056	0.25914980633592666
X	0.01827451364946589	758	0.758	0.288885364820056
X	0.018260900938722346	1707	1.707	0.22034146187524967
X	0.01697638055964691	803	0.803	0.2765093697522857
X	0.019274558849777745	4687	4.687	0.1602125370556758
X	0.01867826373419862	6608	6.608	0.1413911270509088
X	0.01840523000107345	4969	4.969	0.15472390485430304
X	0.017921414288415618	480	0.48	0.33422865457139306
X	0.018269387381625406	817	0.817	0.2817305713562359
X	0.018373273240637308	2275	2.275	0.20063269687785923
X	0.01766264647978875	839	0.839	0.27612004696013676
X	0.017325250416665328	909	0.909	0.267119265857182
X	0.017736195233240774	1128	1.128	0.25052464400812213
X	0.018399652967692613	3029	3.029	0.18246102890413313
X	0.01839932540882429	2937	2.937	0.18434554696193545
X	0.018282233411390843	3566	3.566	0.17243124747326474
X	0.017835633299847102	1547	1.547	0.22590858759768218
X	0.018513410341010082	11047	11.047	0.11878110406887822
X	0.017896369280011384	804	0.804	0.2812999324912246
X	0.018358836049193625	1210	1.21	0.2475634868998497
X	0.018235312836460252	1656	1.656	0.22247657770266252
X	0.01780985678289916	145	0.145	0.49708517191147084
X	0.01842319968381588	3123	3.123	0.18068871320758692
X	0.017241077186941896	795	0.795	0.2788682614536347
X	0.017990833093205906	1852	1.852	0.2133716737920156
X	0.016509195002361605	496	0.496	0.3216730969298361
X	0.01781387428704269	1437	1.437	0.2314375758232119
X	0.017383119535755928	485	0.485	0.32970737321642435
X	0.01836745290918777	811	0.811	0.2829280634315591
X	0.01783462421046148	883	0.883	0.2723335118037977
X	0.018306636443000623	1297	1.297	0.2416700657592691
X	0.017382904723443508	248	0.248	0.41230970397237104
X	0.018214638921403306	625	0.625	0.30773733113034835
X	0.018028757623287824	780	0.78	0.2848549249319315
X	0.01837228767545386	2436	2.436	0.19610800321440758
X	0.018077057784953103	771	0.771	0.2862141644461635
X	0.018411157103630806	2368	2.368	0.19810698810122293
X	0.01820088788128696	1349	1.349	0.2380638986689959
X	0.018204633425370306	1194	1.194	0.24796622883228106
X	0.01840266756978877	2086	2.086	0.20662783516845706
X	0.01820365361325593	1822	1.822	0.21537902011950158
X	0.017425740373816983	1096	1.096	0.251454972598658
X	0.01816195724388342	651	0.651	0.30329171959504353
X	0.018010028063331414	1629	1.629	0.22277396523057016
X	0.01795117516429863	1570	1.57	0.22528437807770216
X	0.017795182527215573	842	0.842	0.2764798287654605
X	0.01832567942328521	4668	4.668	0.1577525588659506
X	0.018155086684644883	1618	1.618	0.22387551772932637
X	0.018261125963331595	2566	2.566	0.1923491626557701
X	0.01832396641518167	1479	1.479	0.23139311773778676
X	0.018284306070929533	2340	2.34	0.1984361877693205
X	0.019240765786933713	7475	7.475	0.13704726892689606
X	0.01813512078631059	767	0.767	0.28701749044787844
X	0.017332515138831148	211	0.211	0.4347034073015458
X	0.017365051709162364	840	0.84	0.27445149338656744
X	0.01786748013291701	550	0.55	0.319080457538792
X	0.01809763683881924	1423	1.423	0.23342049712760174
X	0.018395955402921167	1395	1.395	0.2362559275051817
X	0.018244215671391305	716	0.716	0.2942641745444856
X	0.017331655508817767	528	0.528	0.32018578121696073
X	0.01794563975276794	1178	1.178	0.24789696988135884
X	0.01799205908980417	349	0.349	0.3721789565306834
X	0.018116063336765346	1074	1.074	0.25646012503717247
X	0.01824944027835482	508	0.508	0.32996049431007235
X	0.018389565039299997	1845	1.845	0.21520797559770205
X	0.01834664834844433	4375	4.375	0.1612598407230891
X	0.018140254545574178	1161	1.161	0.24999829821672093
X	0.017300457680014623	231	0.231	0.4215171063267678
X	0.0182141728542125	3419	3.419	0.1746506143518559
X	0.01812023462744479	1709	1.709	0.21968846346956994
X	0.01828452353065672	940	0.94	0.2689378516998963
X	0.018275325391622695	5500	5.5	0.14922244202319954
X	0.01864222957962191	6211	6.211	0.14424876632963252
X	0.017912826519196463	946	0.946	0.26653709570663786
X	0.017949466072469607	603	0.603	0.30991652999691666
X	0.017937329526995973	369	0.369	0.36495877216115824
X	0.018894712097610126	606	0.606	0.31474285859524964
X	0.017219170607875468	586	0.586	0.308582673723281
X	0.01830622034415057	373	0.373	0.3661254776062918
X	0.017778546869072674	238	0.238	0.42115287328293677
X	0.018302272928433203	2699	2.699	0.18927825100488058
X	0.01829220908389037	1053	1.053	0.2589877733985021
X	0.017555621214088536	1018	1.018	0.2583585318364312
X	0.018439650977302255	1107	1.107	0.25538870849745576
X	0.01822191687897146	2143	2.143	0.20410671675109884
X	0.017380977753127164	1082	1.082	0.25231843423501266
X	0.018226311932491787	4395	4.395	0.1606620508495542
X	0.01801213713950485	2560	2.56	0.1916204077039514
X	0.017598571529984647	545	0.545	0.3184395282105984
X	0.017918597377994037	485	0.485	0.3330586807946599
X	0.018383264507811657	3210	3.21	0.17891188903913013
X	0.016781673400133465	233	0.233	0.4160636562721336
X	0.0180178485064865	556	0.556	0.3188179119882176
X	0.01815939958163534	871	0.871	0.2752291591550945
X	0.018013929406616776	1378	1.378	0.2355698721881132
X	0.01834621972093404	2756	2.756	0.18811458078617613
X	0.018248351920172223	1428	1.428	0.23379316668228045
X	0.017636692675613474	684	0.684	0.2954293791865803
X	0.017866350129560202	511	0.511	0.3269928929146524
X	0.018290701630469267	1944	1.944	0.21111124562857927
X	0.017054811553324126	453	0.453	0.33515798778215244
X	0.018207418366964927	5264	5.264	0.15123216297090744
X	0.018398184371949646	29694	29.694	0.08525151145566838
X	0.01841873110545022	2294	2.294	0.20024211852087276
X	0.018160744932336464	2397	2.397	0.1964062068346969
X	0.01756987274441518	288	0.288	0.3936637054671267
X	0.018393967822364857	2605	2.605	0.191847391356806
X	0.018663229979744667	29385	29.385	0.08595814556096369
X	0.01836543810428418	84531	84.531	0.06011669482750936
X	0.018237216318267458	1241	1.241	0.24494143395086862
X	0.01823107076500865	21608	21.608	0.09449290524931965
X	0.0181748445854477	25145	25.145	0.08974419272300654
X	0.01828728954293412	3316	3.316	0.17667629085077782
X	0.018646094527951844	47035	47.035	0.07346078717311544
X	0.018301973925455222	586	0.586	0.31491990040446227
X	0.018095519151624464	1909	1.909	0.21163542905236069
X	0.018436398373508116	43748	43.748	0.07497318486891512
X	0.018532862213764448	145702	145.702	0.05029123814857835
X	0.018238533239640833	1327	1.327	0.2395372147229646
X	0.01842967969762137	10446	10.446	0.12083402865862812
X	0.017642736225456434	6607	6.607	0.1387353746897465
X	0.018392796904761515	18202	18.202	0.10034819269552515
X	0.01837927153657105	21677	21.677	0.09464762169761842
X	0.01815717763762125	14519	14.519	0.1077384059758585
X	0.01843128835423307	11787	11.787	0.11606932799891509
X	0.01844287537819057	11730	11.73	0.11628138783998176
X	0.017373526890135712	981	0.981	0.260659161860268
X	0.018449813279569818	75656	75.656	0.06247642919808779
X	0.018100538299366288	1141	1.141	0.2512669002371984
X	0.018280291468488235	950	0.95	0.26797020196698146
X	0.01839658938875729	9310	9.31	0.1254861739701144
X	0.01806807624582153	23971	23.971	0.09100709553881682
X	0.018380927399462555	91892	91.892	0.05848303538285333
X	0.018451111475653304	41577	41.577	0.07627633095076983
X	0.017949805478542913	510	0.51	0.3277151534320356
X	0.01841274847567729	4523	4.523	0.159672703132545
X	0.01829105835071521	5059	5.059	0.15348221280052476
X	0.01843654184461584	15415	15.415	0.10614803968159642
X	0.018477786197793344	37709	37.709	0.07883788666701325
X	0.018148713476270374	3637	3.637	0.17088378218542125
X	0.018347444394038827	8240	8.24	0.13058184169108292
X	0.017318612206545413	304	0.304	0.3847806004695149
X	0.01830936511911114	2119	2.119	0.20520161581110222
X	0.01839278744987023	26229	26.229	0.08884273907912683
X	0.018370774551147693	19340	19.34	0.09830078800002587
X	0.018820518237115114	15698	15.698	0.10623372690313919
X	0.01795100253044538	2651	2.651	0.1891877418426053
X	0.01845882436143328	167291	167.291	0.04796345800604042
X	0.018372859858371858	3753	3.753	0.16979806339691494
X	0.018394443641539825	12667	12.667	0.1132411563274543
X	0.01845787567309723	52447	52.447	0.07060254479337069
X	0.01824114877253515	1287	1.287	0.24200511548988174
X	0.01841097536775954	25302	25.302	0.08994436951453755
X	0.018375984341739204	120506	120.506	0.05342534233797168
X	0.01844264422885322	77802	77.802	0.061888621459389594
X	0.01829631160747803	3684	3.684	0.17061399104906522
X	0.017929189136001004	1891	1.891	0.21165232323208283
X	0.017050117937649146	1075	1.075	0.25125013727755746
X	0.018284953937466857	1823	1.823	0.2156597350518573
X	0.018424546721020455	17020	17.02	0.10267839831680854
X	0.01838289699639204	4675	4.675	0.15783771298686716
X	0.018420293495170617	18050	18.05	0.10067920552348583
X	0.018463224767189985	48376	48.376	0.07253693492802475
X	0.01845107916668506	7982	7.982	0.13222188660848586
X	0.018059831508228864	4201	4.201	0.16259992373117535
X	0.018430461699917168	2784	2.784	0.1877683231955866
X	0.018456776717610226	59653	59.653	0.06763547165428899
X	0.018419595478712778	10718	10.718	0.11978123278157475
X	0.01838795037566161	4427	4.427	0.16074640428061068
X	0.018695119604811367	6407	6.407	0.1428974595748838
X	0.01845188572553478	180674	180.674	0.04674283316690227
X	0.018124451138326926	3836	3.836	0.16780133888728319
X	0.018400529060301575	60349	60.349	0.06730594258523777
X	0.01839407837249231	1414	1.414	0.2351849576889455
X	0.018575798000954696	18899	18.899	0.09942666786079374
X	0.01842366160185106	6390	6.39	0.14232844241686873
X	0.018625525828251263	102957	102.957	0.056556720823966386
X	0.01832253959002537	18101	18.101	0.10040631617442501
X	0.018275051059382093	1195	1.195	0.24821626208776915
X	0.018454716047486133	63065	63.065	0.06639055854994087
X	0.01847300133863149	58186	58.186	0.06821915248685223
X	0.01838929516218928	2010	2.01	0.209149264691077
X	0.01838944901005786	21889	21.889	0.09435847904207921
X	0.0184511968770455	46667	46.667	0.07339585825509988
X	0.019611344152592934	199331	199.331	0.04616478725468426
X	0.018434598164671197	8521	8.521	0.1293344945056032
X	0.01826425286087673	758	0.758	0.28883128683538634
X	0.01834710588275894	1762	1.762	0.21836710731227668
X	0.018453867305295905	3130	3.13	0.18065404294381499
X	0.018434486594827264	7157	7.157	0.1370776582369621
X	0.0183580837677859	4367	4.367	0.161391770195307
X	0.01836890713277446	1327	1.327	0.2401066187225792
X	0.01845554801703097	43367	43.367	0.07521812820958558
X	0.018441394937507963	51353	51.353	0.07107921939151082
X	0.017330110557892298	678	0.678	0.29457137807356204
X	0.018160482770911233	1483	1.483	0.23049530281278785
X	0.01805452327102105	5151	5.151	0.15190246630350482
X	0.019191965368997976	7452	7.452	0.13707203749585686
X	0.018241904023578726	4052	4.052	0.16512021416925857
X	0.01831437986542274	5044	5.044	0.1536994735529749
X	0.01826274117480397	1234	1.234	0.24551814915844947
X	0.018330221359844547	6206	6.206	0.14347799952460813
X	0.018448520029812447	12691	12.691	0.11328051880028353
X	0.018419974323168276	6554	6.554	0.14112182835191367
X	0.018417246519621257	19895	19.895	0.09746011494769595
X	0.017921989107260738	3215	3.215	0.17731075426320927
X	0.018459309496243644	167767	167.767	0.04791847309370145
X	0.01839204801142698	2493	2.493	0.19467160275250858
X	0.018432940300915596	27525	27.525	0.08748946543198492
X	0.018322683230415217	382	0.382	0.36333611668782034
X	0.018012334718868907	1276	1.276	0.24167948016114374
X	0.01837414265357971	2860	2.86	0.18590041112772965
X	0.0184470981431306	6003	6.003	0.1453851018335983
X	0.01822590570755974	1712	1.712	0.21998603878362027
X	0.018433707269799635	42403	42.403	0.07575396475479
X	0.018221322634997265	1809	1.809	0.21596354300969314
X	0.018190202516888094	2354	2.354	0.19770167943458516
X	0.018443899776808916	164547	164.547	0.04821560215777616
X	0.018383115914764023	26562	26.562	0.08845440206528436
X	0.018878968235813226	23119	23.119	0.093469460704719
X	0.019015561581746207	81923	81.923	0.061456492387471674
X	0.018776576926907575	92931	92.931	0.058679346714204
X	0.0184671203595417	2458	2.458	0.19585700107548978
X	0.018194503814616028	1293	1.293	0.2414240681723438
X	0.018076177363924088	10016	10.016	0.12175050166548441
X	0.017514267797549235	368	0.368	0.36239432579527525
X	0.018288389176132347	1502	1.502	0.23005685268220585
X	0.01843261504162948	5967	5.967	0.14563875971578608
X	0.018098052115639196	870	0.87	0.275024168690647
X	0.01840140477802332	1700	1.7	0.22120792693914415
X	0.018119014090085362	789	0.789	0.28424043110309266
X	0.01835683143472384	2269	2.269	0.20074947030018447
X	0.018465750758764592	72084	72.084	0.06351008545949194
X	0.018409255167293217	4670	4.67	0.15796945226191383
X	0.01835539682883234	27913	27.913	0.0869599111782851
X	0.017987968868839493	1575	1.575	0.2251993795849393
X	0.01820567728985859	1289	1.289	0.2417229992779715
X	0.018313452627377602	2951	2.951	0.1837667804337539
X	0.018647289853437585	302838	302.838	0.039488301207852766
X	0.01847142431189765	207244	207.244	0.04466899069186083
X	0.018395462771999483	3951	3.951	0.1669813223736332
X	0.018438419927392098	12094	12.094	0.11509361888526018
X	0.01824204126438821	784	0.784	0.285486640731884
X	0.017357266269822574	2104	2.104	0.20205915390822848
X	0.018393879847855094	27921	27.921	0.08701232878652747
X	0.018418866493117515	8924	8.924	0.12732131421820397
X	0.018226467022041434	9673	9.673	0.12351331238112324
X	0.018446541212548927	33270	33.27	0.08215248697121286
X	0.01843845164735883	220563	220.563	0.04372507833233293
X	0.01783754490558303	10002	10.002	0.12126889205174589
X	0.01765825941472449	1406	1.406	0.23244534714115697
X	0.019819349106246414	9927	9.927	0.12591876491814513
X	0.018140356803544123	2916	2.916	0.18391596706983077
X	0.018328832904942964	1162	1.162	0.2507896222591693
X	0.01861383972324538	46441	46.441	0.07373009652193202
X	0.018265031151673442	8010	8.01	0.13162224692986807
X	0.01687623259858339	350	0.35	0.3639729869271642
X	0.018945666246159235	15808	15.808	0.10622114658791826
X	0.018606040006200746	5170	5.17	0.15324552539604053
X	0.017335924544543094	446	0.446	0.338743301854003
X	0.018435322325275935	38716	38.716	0.07808844080832512
X	0.018465430870706057	82561	82.561	0.06070085855075447
X	0.018429663506101264	41806	41.806	0.0761072909076492
X	0.01835187561150414	56329	56.329	0.0688096694958119
X	0.01837204460383175	82365	82.365	0.060646386513358554
X	0.018397432666845306	5281	5.281	0.1515934202236725
X	0.01828231146321797	3412	3.412	0.17498763448516713
X	0.018639718862338412	33673	33.673	0.08210807098944975
X	0.018468836912474876	43120	43.12	0.0753795599922102
X	0.018334293217738776	1666	1.666	0.22243155187340116
X	0.018462435095731273	15765	15.765	0.10540591974755781
X	0.019039329940435314	137728	137.728	0.0517062616679635
X	0.018465897609461473	54341	54.341	0.06978267268869252
X	0.01846366956542869	45494	45.494	0.07403799205579246
X	0.018651376775871486	41794	41.794	0.07641858391174455
X	0.018324759950732728	3444	3.444	0.17457888569510943
X	0.017848514077772357	1123	1.123	0.2514244082329215
X	0.01796902458168053	2490	2.49	0.19324505312313328
X	0.01847993033681434	44336	44.336	0.0746989688976399
X	0.01845877161524922	30508	30.508	0.08457909221724076
X	0.018484750406585142	287025	287.025	0.04008340637651873
X	0.018450922429529695	50994	50.994	0.07125789700837983
X	0.01843415494126615	25485	25.485	0.0897662050113445
X	0.018302378257560693	11799	11.799	0.11575882464489987
X	0.0184476789239975	53341	53.341	0.07019295502605656
X	0.01798995423802853	818	0.818	0.28017255135065156
X	0.0181411101241909	2655	2.655	0.18975786181165671
X	0.01842326211641469	92136	92.136	0.058476188726457407
X	0.018267655847905094	2108	2.108	0.20540171789603567
X	0.01792710078098938	1428	1.428	0.23241310750602667
X	0.01841086339937198	10175	10.175	0.12185590495174886
X	0.018475407528145523	92174	92.174	0.05852326276282989
X	0.018318384430105687	7553	7.553	0.13435563774326867
X	0.018435568988145187	48248	48.248	0.07256475670973483
X	0.0177358244867361	1475	1.475	0.22909720317823032
X	0.017848189011407434	2060	2.06	0.2053882740360203
X	0.017885380300727987	1262	1.262	0.24199864896048826
X	0.018374739471698622	3392	3.392	0.1756258539117846
X	0.018863151418524175	21007	21.007	0.09647543345632965
X	0.018113110834701796	9460	9.46	0.1241749430236214
X	0.01805485403333142	1653	1.653	0.22187432169784294
X	0.018460902701117842	52530	52.53	0.0705691972996287
X	0.017963906547243707	605	0.605	0.3096576432628036
X	0.018487464984128416	21518	21.518	0.09506598530038762
X	0.018269977322565443	3368	3.368	0.17570682930798284
X	0.018386322897109585	12309	12.309	0.11431170533231301
X	0.018387939444902038	2585	2.585	0.19231987696115574
X	0.017994004827075617	3639	3.639	0.17036560728863126
X	0.018357560419626252	4345	4.345	0.16166216668667677
X	0.018962181057496096	12252	12.252	0.115671695193316
X	0.01869344271919997	51531	51.531	0.07131927292709915
X	0.01852852911140092	64170	64.17	0.06609515645773235
X	0.018392943795235917	2339	2.339	0.19885675154112634
X	0.017824458655289758	3527	3.527	0.17160777602530247
X	0.018461364404698328	58110	58.11	0.06823454602414288
X	0.01829102624779632	2487	2.487	0.19447066348071518
X	0.018649284418486067	327860	327.86	0.038458402992629204
X	0.01815125159971977	420	0.42	0.3509288679638537
X	0.018446487299041463	27247	27.247	0.08780750999563514
X	0.01847260958240748	49940	49.94	0.07178382089866361
X	0.01822098718626544	1079	1.079	0.25655677398020826
X	0.018469975419909246	62623	62.623	0.06656472612872148
X	0.018260642735769713	9172	9.172	0.12580097427563103
X	0.018526548006849355	400791	400.791	0.035888792667197855
X	0.01839721165465397	3754	3.754	0.1698579632417845
X	0.018259507339883435	3941	3.941	0.1667097013883862
X	0.018137518319183678	2958	2.958	0.18303180592973242
X	0.01841601436410605	1785	1.785	0.21769701535128888
X	0.018408121135265357	4278	4.278	0.16265080901414486
X	0.018447051932814214	10934	10.934	0.1190463178234916
X	0.01793880267085371	3616	3.616	0.17055129064136482
X	0.01840325970815005	15948	15.948	0.10488889479739415
X	0.018384083477299282	6835	6.835	0.1390703309485698
X	0.017335469481870532	661	0.661	0.29710597289383905
X	0.01808588320913539	5565	5.565	0.14812380208524747
X	0.01875150899895315	104314	104.314	0.05643708071548167
X	0.018732793788818994	26986	26.986	0.08854309097212654
X	0.018479152861704335	99362	99.362	0.057080431101700524
X	0.018761407650343762	7806	7.806	0.13395089642603056
X	0.018424777735493614	6937	6.937	0.13848737636861053
X	0.017952261260395112	713	0.713	0.29309599001799075
X	0.018276417245550666	2228	2.228	0.20167804646607615
X	0.01846697189364163	64529	64.529	0.06589921649241827
X	0.01844435410095395	6526	6.526	0.14138569162245448
X	0.018415700177315173	4666	4.666	0.15803301789584961
X	0.018340152306988444	9412	9.412	0.12490323108540653
X	0.018252758721130965	16648	16.648	0.10311507182000618
X	0.018359315259284244	1217	1.217	0.24709007454043647
X	0.01825477012097975	738	0.738	0.29136675834603787
X	0.018935033845045728	205510	205.51	0.04516593884544855
X	0.018304804011274715	6616	6.616	0.1403858046462961
X	0.018086377398916536	11242	11.242	0.1171752807113155
X	0.01843183827454516	3439	3.439	0.17500299923701615
X	0.019119078952205926	27996	27.996	0.0880623614680355
X	0.018562537272198358	109100	109.1	0.055412047270688373
X	0.018442586439819872	88584	88.584	0.05926826039027668
X	0.018425329230037805	70845	70.845	0.0638315441667763
X	0.01839108658174252	3161	3.161	0.1798571184026692
X	0.01846681548751516	21686	21.686	0.0947845426374779
X	0.018376978896243103	12915	12.915	0.11247601008884525
X	0.018316443937193887	2179	2.179	0.20332680838167916
X	0.018268848020598433	6994	6.994	0.13771941787046724
X	0.01838893479274399	1958	1.958	0.21098324214436653
X	0.01838067506486947	5440	5.44	0.15005628352135078
X	0.018349944277002223	3225	3.225	0.17852608324724578
X	0.017427721400081073	462	0.462	0.3353766927524987
X	0.01817583756043034	3926	3.926	0.1666663965730178
X	0.018446436909125976	3025	3.025	0.18269599932239255
X	0.018387310353197486	18935	18.935	0.09902639421787031
X	0.017834426006202435	2091	2.091	0.2043156817695232
X	0.018432957525037045	16702	16.702	0.10334167925000456
X	0.017570892965340465	5129	5.129	0.15074874711896633
X	0.018374654191235604	6883	6.883	0.13872257285669712
X	0.01804079743136442	1289	1.289	0.2409910631390244
X	0.018361946946494494	3776	3.776	0.16941905210217903
X	0.01843325279835828	7639	7.639	0.1341287332611452
X	0.018444705707709817	83537	83.537	0.06044090406955368
X	0.018345363396014953	2657	2.657	0.19041957353712155
X	0.018005900293785897	614	0.614	0.30837712599940137
X	0.017762312027159617	2036	2.036	0.20586091894608494
X	0.01847184424414843	99977	99.977	0.056955637736187316
X	0.018906188729270103	202279	202.279	0.04538207999174422
X	0.018181800202390307	8733	8.733	0.1276902704946774
X	0.01834743767577462	3666	3.666	0.17105180189745134
X	0.01837803473694738	14319	14.319	0.10867478583302163
X	0.01945182523101279	31569	31.569	0.08509417744934236
X	0.01778618554506833	1226	1.226	0.24389194068472136
X	0.018393817528036125	4475	4.475	0.16018663488873017
X	0.018451560676594102	4982	4.982	0.15471881394349518
X	0.018418144153513182	30552	30.552	0.08447640185403851
X	0.01821249272378196	5487	5.487	0.14916885071885966
X	0.018454854771216715	126751	126.751	0.05260815597911878
X	0.018307949568363752	2089	2.089	0.2061739330743699
X	0.018382541666539375	1902	1.902	0.21300919387766112
X	0.01765911139311439	681	0.681	0.29598786861400106
X	0.018416020045141396	7125	7.125	0.13723671169036183
X	0.01835380323236903	3061	3.061	0.18167182568934376
X	0.018441714051569493	13337	13.337	0.11140748687216614
X	0.018854359034115715	114762	114.762	0.054769388014308085
X	0.018706120972018847	334992	334.992	0.03822226632965015
X	0.01832473095507036	2616	2.616	0.19133743857615879
X	0.017916507369489944	1247	1.247	0.2431059750334477
X	0.018430817404251506	8022	8.022	0.13195341861799034
X	0.018484194796601842	260431	260.431	0.04140339488696247
X	0.01831779525288758	12838	12.838	0.11257931703597866
X	0.018113309831484587	768	0.768	0.2867778021053209
X	0.01773417365135861	1980	1.98	0.20767390160941604
X	0.01858253700971124	19996	19.996	0.09758594965051305
X	0.01828721893240187	433	0.433	0.3482463628738158
X	0.018364776373431233	41255	41.255	0.07635480123826256
X	0.018193154022289607	1267	1.267	0.24305830247452043
X	0.018450586281887484	8622	8.622	0.12886472483669245
X	0.018153601454412693	1983	1.983	0.20919278133834962
X	0.018038002638832866	1651	1.651	0.2218947997047822
X	0.01760372233345537	627	0.627	0.30393379276749166
X	0.018457757959838325	95851	95.851	0.05774673481276358
X	0.018466214436638477	201087	201.087	0.045116073256296234
X	0.018448156666165987	59806	59.806	0.06756722362156953
X	0.018348749369170515	10969	10.969	0.1187079515281612
X	0.018449044725656727	15695	15.695	0.10553686468172643
X	0.018353209262719963	1935	1.935	0.2116786298204518
X	0.018346424269965567	3779	3.779	0.16932646698928908
X	0.018462229967995493	29368	29.368	0.08566496890914069
X	0.019960491352691033	22599	22.599	0.0959461019677896
X	0.01842294765006127	62469	62.469	0.06656279063508054
X	0.01834482996625585	4975	4.975	0.15449231059425772
X	0.018715793218083466	322766	322.766	0.03870557165207052
X	0.01840667556253282	4693	4.693	0.15770359772340029
X	0.01971939612887374	18988	18.988	0.10126782041220586
X	0.019342332877304014	45774	45.774	0.07504066302529314
X	0.018637132885138578	20358	20.358	0.09709897913784883
X	0.01842632282407863	47095	47.095	0.07313994422076017
X	0.018896591366869327	104944	104.944	0.056468807878173786
X	0.018406489642951756	13427	13.427	0.1110871932544378
X	0.01837196896995578	9002	9.002	0.1268446725147993
X	0.0179897467870473	1842	1.842	0.213752797958653
X	0.018349971952359226	19284	19.284	0.09835869560405175
X	0.018430406193914027	5726	5.726	0.14764809719646674
X	0.018443089889918954	61091	61.091	0.06708397930140333
X	0.019079285153101495	400473	400.473	0.03625180366488434
X	0.018874160525274644	50602	50.602	0.07198355991822832
X	0.019102235561074685	98281	98.281	0.05792573367658241
X	0.01846900889982134	20267	20.267	0.09695079493294542
X	0.0184312806213407	14788	14.788	0.10761728297611427
X	0.01982809284780061	23408	23.408	0.09461766183892713
X	0.018022923572834304	4930	4.93	0.1540491976197313
X	0.018390052455769076	5263	5.263	0.1517457486716754
X	0.018448147266455352	76558	76.558	0.062228222010610144
X	0.01819800945143408	10888	10.888	0.11867482264445775
X	0.018436430450410927	109300	109.3	0.0552525372218011
X	0.018887080391784373	118383	118.383	0.054236521649237
X	0.01842904396771577	10175	10.175	0.12189600230672201
X	0.01844220933417611	1312	1.312	0.24133836754203383
X	0.018462934729470318	27539	27.539	0.08752205832898204
X	0.018431566261733163	24105	24.105	0.09144326032317779
X	0.018192028614289018	1083	1.083	0.25610470547446723
X	0.019120725181984034	169364	169.364	0.048331219386918124
X	0.018469350468865553	16963	16.963	0.10287653306267805
X	0.018403133044484845	11793	11.793	0.11599051830834263
X	0.01839969694711046	3444	3.444	0.17481653546062184
X	0.018203179443902137	1138	1.138	0.25196197046460156
X	0.017969655949623103	790	0.79	0.2833376082827897
X	0.018268069834922372	1032	1.032	0.26061794856994985
X	0.01824815421037628	4670	4.67	0.15750730014940922
X	0.01841339586331641	6765	6.765	0.1396224849085885
X	0.019313150627663724	2185287	2185.287	0.020675128390531695
X	0.01833155451285938	4069	4.069	0.165159617444559
X	0.01838243309575351	5886	5.886	0.14617089699781408
X	0.017800449025372186	1943	1.943	0.20924385808541557
X	0.018399351889344013	3951	3.951	0.16699308911932478
X	0.0183602791415343	6856	6.856	0.13886820555192206
X	0.01841623806977444	101627	101.627	0.05658881569421972
X	0.0184085883186768	5096	5.096	0.1534370907262556
X	0.018859654325942568	273698	273.698	0.040997184732118705
X	0.018349106398679347	1514	1.514	0.2297010680737186
X	0.018772584028841736	123959	123.959	0.053302633224138855
X	0.018407655002501536	17349	17.349	0.10199401145351825
X	0.018401393664858472	55823	55.823	0.06907896914271883
X	0.018590547673275535	181203	181.203	0.0468139837660956
X	0.018329126027266204	5215	5.215	0.1520416176105232
X	0.018263944985451062	1921	1.921	0.21184704400681464
X	0.018457281044496683	29023	29.023	0.08599538419729716
X	0.018188802749963973	2454	2.454	0.19497391771343975
X	0.01842791463657167	10110	10.11	0.12215418357135296
X	0.01845727151528544	9902	9.902	0.12306886433101649
X	0.018446396017370078	3700	3.7	0.1708322790030478
X	0.019813036060034494	19521	19.521	0.10049620365791669
X	0.018078073676906857	6979	6.979	0.13733660834911732
X	0.018775610725242926	48333	48.333	0.0729653673600181
X	0.018402272401308963	88039	88.039	0.05934700198549685
X	0.018194003685295383	5404	5.404	0.14987791608522785
X	0.018260448010028393	3155	3.155	0.17954391394895888
X	0.018443185459453296	41811	41.811	0.07612286516868177
X	0.018261687096855713	13788	13.788	0.10981960419820708
X	0.018754743783597774	20245	20.245	0.0974835010381541
X	0.017917357768790755	7177	7.177	0.13565745747054417
X	0.018223519029615567	20170	20.17	0.09667379826417301
X	0.018434709568304675	11162	11.162	0.11820379695546826
X	0.0174223071826644	26422	26.422	0.0870388864553533
X	0.018371902409509235	20328	20.328	0.09668367709957991
X	0.01824484221294062	4148	4.148	0.16384522459459333
X	0.01832178413743347	2587	2.587	0.19203944600961895
X	0.01965182169974218	7396	7.396	0.13850601606011986
X	0.01781921466362029	21653	21.653	0.09371091130752195
X	0.018404548534036922	28112	28.112	0.08683160090826647
X	0.018424647856258407	73768	73.768	0.0629762885577799
X	0.018260790307688117	3541	3.541	0.1727684939889033
X	0.018422309205651594	13514	13.514	0.11088004263414977
X	0.018434713560255757	12879	12.879	0.11269848647396387
X	0.01845598542006988	67442	67.442	0.06492355104082481
X	0.018312443233349945	17627	17.627	0.101279751939454
X	0.018467097951709047	38781	38.781	0.0780896032665511
X	0.0183291085570357	4160	4.16	0.1639391469503415
X	0.01842953029037099	12277	12.277	0.11450048780904966
X	0.018359156629653976	1182	1.182	0.24950451951211908
X	0.017943977109406116	4021	4.021	0.16463729088978155
X	0.018437106718208675	17139	17.139	0.1024634788272676
X	0.018432159736655327	15139	15.139	0.10678076112565975
X	0.018392409041350373	12820	12.82	0.11278470222426451
X	0.018232333724311593	779	0.779	0.28604537794848883
X	0.018425535662922696	2116	2.116	0.20573182865823686
X	0.018059859745195417	10320	10.32	0.12050680125332836
X	0.018371639985236082	9810	9.81	0.12326117583273946
X	0.018409977118668795	7012	7.012	0.1379548946118957
X	0.017899866205145575	1045	1.045	0.2577771976771232
X	0.01761816385037085	1663	1.663	0.21962888923041535
X	0.018387249814865378	7535	7.535	0.1346308248574615
X	0.018401924868277644	10092	10.092	0.12216927701992757
X	0.01840698585759832	37921	37.921	0.07859006218348974
X	0.01837046244399709	1274	1.274	0.2433979336997218
X	0.018292482108626752	7469	7.469	0.13479384440233863
X	0.018300590317584093	4322	4.322	0.1617807255433357
X	0.01748270717111204	1643	1.643	0.21994986848827489
X	0.018466230250718127	119417	119.417	0.053674832345682245
X	0.0181856933105531	8627	8.627	0.12822027034416988
X	0.018316053906901372	41941	41.941	0.07586900305169342
X	0.01846290708427721	63330	63.33	0.06630763406069443
X	0.01843215693640572	29355	29.355	0.08563106753177328
X	0.01914679490096525	105139	105.139	0.05668185843167288
X	0.01846491820708638	79693	79.693	0.06141989515662409
X	0.018051314640643395	1084	1.084	0.2553641051633983
X	0.01839717980990398	12611	12.611	0.11341415005689759
X	0.018367609307837873	4734	4.734	0.15713567416709473
X	0.018447982417816262	32657	32.657	0.08266548090698615
X	0.018461861767689616	77535	77.535	0.061981093578837854
X	0.01831921215103324	6710	6.71	0.13976381843364302
X	0.01843241112616048	25643	25.643	0.08957863466265524
X	0.018231216951598222	2486	2.486	0.1942845108675317
X	0.018162631293121408	8265	8.265	0.13001056219226065
X	0.01830672170433937	4924	4.924	0.1549164412370928
X	0.01820233986050155	2158	2.158	0.20355975814028857
X	0.01855100478505467	140301	140.301	0.05094507707840823
X	0.01895528512067538	273567	273.567	0.041072916079711294
X	0.018164553207703348	8937	8.937	0.12667113269331953
X	0.018420773157399605	5802	5.802	0.1469749687994932
X	0.01833638469115926	12005	12.005	0.11516411710017077
X	0.01887053010186893	15994	15.994	0.10566771987700695
X	0.018181039120637756	6281	6.281	0.14251589437840997
X	0.018315785045192448	3498	3.498	0.17364751697441558
X	0.018420235773213767	11986	11.986	0.11540030866849635
X	0.01845681800088277	66209	66.209	0.06532507751789562
X	0.018425492167318678	2003	2.003	0.20952992103001872
X	0.01834758687021739	32216	32.216	0.082890058347605
X	0.018386040324565697	1847	1.847	0.21511652365488165
time for making epsilon is 1.0319814682006836
epsilons are
[0.3507248272887892, 0.160227790442159, 0.25336491569617214, 0.15069109835798777, 0.1693074997164318, 0.1448780743895005, 0.07649670802989104, 0.10915627008637331, 0.18138973776329878, 0.14187371378191763, 0.19522049824222246, 0.07308258072043759, 0.18153872392371204, 0.04976627014730601, 0.10963603052106698, 0.11622933664737853, 0.0817275476076418, 0.09434049866955206, 0.05845001598014794, 0.0626287952460525, 0.17560193410865957, 0.05759942730721474, 0.14473665539522187, 0.0989365782430669, 0.2957652592147589, 0.07110289253174887, 0.08455621152163868, 0.20477936037087824, 0.09128194971790272, 0.03508620948056177, 0.18069095676443472, 0.034348176281637215, 0.11648474449904822, 0.20033498549916282, 0.18718536781940018, 0.07932223664573008, 0.0653480986843665, 0.27083209728586605, 0.150134274457091, 0.2437194790701121, 0.22462439834259476, 0.2749521542086418, 0.31248151452785233, 0.22845554470956964, 0.3399696370219795, 0.3390365789448923, 0.20969392460367292, 0.25491625888052677, 0.3266586155320445, 0.11987729152363656, 0.15590920972859454, 0.1724052614053889, 0.178582871041179, 0.24118190928309227, 0.1886596471725573, 0.16937355593940523, 0.29061459772905596, 0.2007374520147984, 0.23712172241091517, 0.23126331871268008, 0.17056630250662633, 0.3239960555008813, 0.13944064192249284, 0.2273854095718542, 0.29278371170322653, 0.22921225960566916, 0.2766874098335245, 0.2341263552664726, 0.22065541531234878, 0.2829382546835104, 0.27209134441288263, 0.2072003220154124, 0.2732330544163449, 0.19684587490724223, 0.2337192278048021, 0.18673315106345212, 0.2923830448329813, 0.421960313113798, 0.24533917815832007, 0.18381836605069868, 0.23040185908074765, 0.34666927580016843, 0.14483476609661655, 0.3419821775676089, 0.2609981975030216, 0.3403914106698567, 0.1975435288917648, 0.34385004952070597, 0.2109501989403201, 0.23578939560485684, 0.3366099176123533, 0.3085553757924804, 0.3158025963124338, 0.22826487750942373, 0.21492140498239146, 0.21569645569498558, 0.20467933503935, 0.15189987123001478, 0.22271701343610412, 0.2908465303844337, 0.1308491907854163, 0.2671700144091839, 0.17941017107163, 0.2762854390212991, 0.24614052279597048, 0.27303995755233895, 0.19225563189601522, 0.39516585610891236, 0.2426635671581191, 0.23616177628889506, 0.2978535124992952, 0.328205029915869, 0.3056368159691488, 0.20074692188070964, 0.23245296805910765, 0.30225420091300675, 0.36073634926826714, 0.349491001831538, 0.19030087777338753, 0.2886680068691375, 0.1678849555197824, 0.26283303199024677, 0.28327714490578637, 0.269880141208236, 0.2708810745410037, 0.24211233589341755, 0.2607910365859107, 0.2898576379974191, 0.11762667369342604, 0.21875290555137086, 0.2014663525438978, 0.27818044511665024, 0.244732541713719, 0.24934265549075815, 0.19710447547342214, 0.31229600202993074, 0.40530021376609926, 0.2621788802410817, 0.25914980633592666, 0.288885364820056, 0.22034146187524967, 0.2765093697522857, 0.1602125370556758, 0.1413911270509088, 0.15472390485430304, 0.33422865457139306, 0.2817305713562359, 0.20063269687785923, 0.27612004696013676, 0.267119265857182, 0.25052464400812213, 0.18246102890413313, 0.18434554696193545, 0.17243124747326474, 0.22590858759768218, 0.11878110406887822, 0.2812999324912246, 0.2475634868998497, 0.22247657770266252, 0.49708517191147084, 0.18068871320758692, 0.2788682614536347, 0.2133716737920156, 0.3216730969298361, 0.2314375758232119, 0.32970737321642435, 0.2829280634315591, 0.2723335118037977, 0.2416700657592691, 0.41230970397237104, 0.30773733113034835, 0.2848549249319315, 0.19610800321440758, 0.2862141644461635, 0.19810698810122293, 0.2380638986689959, 0.24796622883228106, 0.20662783516845706, 0.21537902011950158, 0.251454972598658, 0.30329171959504353, 0.22277396523057016, 0.22528437807770216, 0.2764798287654605, 0.1577525588659506, 0.22387551772932637, 0.1923491626557701, 0.23139311773778676, 0.1984361877693205, 0.13704726892689606, 0.28701749044787844, 0.4347034073015458, 0.27445149338656744, 0.319080457538792, 0.23342049712760174, 0.2362559275051817, 0.2942641745444856, 0.32018578121696073, 0.24789696988135884, 0.3721789565306834, 0.25646012503717247, 0.32996049431007235, 0.21520797559770205, 0.1612598407230891, 0.24999829821672093, 0.4215171063267678, 0.1746506143518559, 0.21968846346956994, 0.2689378516998963, 0.14922244202319954, 0.14424876632963252, 0.26653709570663786, 0.30991652999691666, 0.36495877216115824, 0.31474285859524964, 0.308582673723281, 0.3661254776062918, 0.42115287328293677, 0.18927825100488058, 0.2589877733985021, 0.2583585318364312, 0.25538870849745576, 0.20410671675109884, 0.25231843423501266, 0.1606620508495542, 0.1916204077039514, 0.3184395282105984, 0.3330586807946599, 0.17891188903913013, 0.4160636562721336, 0.3188179119882176, 0.2752291591550945, 0.2355698721881132, 0.18811458078617613, 0.23379316668228045, 0.2954293791865803, 0.3269928929146524, 0.21111124562857927, 0.33515798778215244, 0.15123216297090744, 0.08525151145566838, 0.20024211852087276, 0.1964062068346969, 0.3936637054671267, 0.191847391356806, 0.08595814556096369, 0.06011669482750936, 0.24494143395086862, 0.09449290524931965, 0.08974419272300654, 0.17667629085077782, 0.07346078717311544, 0.31491990040446227, 0.21163542905236069, 0.07497318486891512, 0.05029123814857835, 0.2395372147229646, 0.12083402865862812, 0.1387353746897465, 0.10034819269552515, 0.09464762169761842, 0.1077384059758585, 0.11606932799891509, 0.11628138783998176, 0.260659161860268, 0.06247642919808779, 0.2512669002371984, 0.26797020196698146, 0.1254861739701144, 0.09100709553881682, 0.05848303538285333, 0.07627633095076983, 0.3277151534320356, 0.159672703132545, 0.15348221280052476, 0.10614803968159642, 0.07883788666701325, 0.17088378218542125, 0.13058184169108292, 0.3847806004695149, 0.20520161581110222, 0.08884273907912683, 0.09830078800002587, 0.10623372690313919, 0.1891877418426053, 0.04796345800604042, 0.16979806339691494, 0.1132411563274543, 0.07060254479337069, 0.24200511548988174, 0.08994436951453755, 0.05342534233797168, 0.061888621459389594, 0.17061399104906522, 0.21165232323208283, 0.25125013727755746, 0.2156597350518573, 0.10267839831680854, 0.15783771298686716, 0.10067920552348583, 0.07253693492802475, 0.13222188660848586, 0.16259992373117535, 0.1877683231955866, 0.06763547165428899, 0.11978123278157475, 0.16074640428061068, 0.1428974595748838, 0.04674283316690227, 0.16780133888728319, 0.06730594258523777, 0.2351849576889455, 0.09942666786079374, 0.14232844241686873, 0.056556720823966386, 0.10040631617442501, 0.24821626208776915, 0.06639055854994087, 0.06821915248685223, 0.209149264691077, 0.09435847904207921, 0.07339585825509988, 0.04616478725468426, 0.1293344945056032, 0.28883128683538634, 0.21836710731227668, 0.18065404294381499, 0.1370776582369621, 0.161391770195307, 0.2401066187225792, 0.07521812820958558, 0.07107921939151082, 0.29457137807356204, 0.23049530281278785, 0.15190246630350482, 0.13707203749585686, 0.16512021416925857, 0.1536994735529749, 0.24551814915844947, 0.14347799952460813, 0.11328051880028353, 0.14112182835191367, 0.09746011494769595, 0.17731075426320927, 0.04791847309370145, 0.19467160275250858, 0.08748946543198492, 0.36333611668782034, 0.24167948016114374, 0.18590041112772965, 0.1453851018335983, 0.21998603878362027, 0.07575396475479, 0.21596354300969314, 0.19770167943458516, 0.04821560215777616, 0.08845440206528436, 0.093469460704719, 0.061456492387471674, 0.058679346714204, 0.19585700107548978, 0.2414240681723438, 0.12175050166548441, 0.36239432579527525, 0.23005685268220585, 0.14563875971578608, 0.275024168690647, 0.22120792693914415, 0.28424043110309266, 0.20074947030018447, 0.06351008545949194, 0.15796945226191383, 0.0869599111782851, 0.2251993795849393, 0.2417229992779715, 0.1837667804337539, 0.039488301207852766, 0.04466899069186083, 0.1669813223736332, 0.11509361888526018, 0.285486640731884, 0.20205915390822848, 0.08701232878652747, 0.12732131421820397, 0.12351331238112324, 0.08215248697121286, 0.04372507833233293, 0.12126889205174589, 0.23244534714115697, 0.12591876491814513, 0.18391596706983077, 0.2507896222591693, 0.07373009652193202, 0.13162224692986807, 0.3639729869271642, 0.10622114658791826, 0.15324552539604053, 0.338743301854003, 0.07808844080832512, 0.06070085855075447, 0.0761072909076492, 0.0688096694958119, 0.060646386513358554, 0.1515934202236725, 0.17498763448516713, 0.08210807098944975, 0.0753795599922102, 0.22243155187340116, 0.10540591974755781, 0.0517062616679635, 0.06978267268869252, 0.07403799205579246, 0.07641858391174455, 0.17457888569510943, 0.2514244082329215, 0.19324505312313328, 0.0746989688976399, 0.08457909221724076, 0.04008340637651873, 0.07125789700837983, 0.0897662050113445, 0.11575882464489987, 0.07019295502605656, 0.28017255135065156, 0.18975786181165671, 0.058476188726457407, 0.20540171789603567, 0.23241310750602667, 0.12185590495174886, 0.05852326276282989, 0.13435563774326867, 0.07256475670973483, 0.22909720317823032, 0.2053882740360203, 0.24199864896048826, 0.1756258539117846, 0.09647543345632965, 0.1241749430236214, 0.22187432169784294, 0.0705691972996287, 0.3096576432628036, 0.09506598530038762, 0.17570682930798284, 0.11431170533231301, 0.19231987696115574, 0.17036560728863126, 0.16166216668667677, 0.115671695193316, 0.07131927292709915, 0.06609515645773235, 0.19885675154112634, 0.17160777602530247, 0.06823454602414288, 0.19447066348071518, 0.038458402992629204, 0.3509288679638537, 0.08780750999563514, 0.07178382089866361, 0.25655677398020826, 0.06656472612872148, 0.12580097427563103, 0.035888792667197855, 0.1698579632417845, 0.1667097013883862, 0.18303180592973242, 0.21769701535128888, 0.16265080901414486, 0.1190463178234916, 0.17055129064136482, 0.10488889479739415, 0.1390703309485698, 0.29710597289383905, 0.14812380208524747, 0.05643708071548167, 0.08854309097212654, 0.057080431101700524, 0.13395089642603056, 0.13848737636861053, 0.29309599001799075, 0.20167804646607615, 0.06589921649241827, 0.14138569162245448, 0.15803301789584961, 0.12490323108540653, 0.10311507182000618, 0.24709007454043647, 0.29136675834603787, 0.04516593884544855, 0.1403858046462961, 0.1171752807113155, 0.17500299923701615, 0.0880623614680355, 0.055412047270688373, 0.05926826039027668, 0.0638315441667763, 0.1798571184026692, 0.0947845426374779, 0.11247601008884525, 0.20332680838167916, 0.13771941787046724, 0.21098324214436653, 0.15005628352135078, 0.17852608324724578, 0.3353766927524987, 0.1666663965730178, 0.18269599932239255, 0.09902639421787031, 0.2043156817695232, 0.10334167925000456, 0.15074874711896633, 0.13872257285669712, 0.2409910631390244, 0.16941905210217903, 0.1341287332611452, 0.06044090406955368, 0.19041957353712155, 0.30837712599940137, 0.20586091894608494, 0.056955637736187316, 0.04538207999174422, 0.1276902704946774, 0.17105180189745134, 0.10867478583302163, 0.08509417744934236, 0.24389194068472136, 0.16018663488873017, 0.15471881394349518, 0.08447640185403851, 0.14916885071885966, 0.05260815597911878, 0.2061739330743699, 0.21300919387766112, 0.29598786861400106, 0.13723671169036183, 0.18167182568934376, 0.11140748687216614, 0.054769388014308085, 0.03822226632965015, 0.19133743857615879, 0.2431059750334477, 0.13195341861799034, 0.04140339488696247, 0.11257931703597866, 0.2867778021053209, 0.20767390160941604, 0.09758594965051305, 0.3482463628738158, 0.07635480123826256, 0.24305830247452043, 0.12886472483669245, 0.20919278133834962, 0.2218947997047822, 0.30393379276749166, 0.05774673481276358, 0.045116073256296234, 0.06756722362156953, 0.1187079515281612, 0.10553686468172643, 0.2116786298204518, 0.16932646698928908, 0.08566496890914069, 0.0959461019677896, 0.06656279063508054, 0.15449231059425772, 0.03870557165207052, 0.15770359772340029, 0.10126782041220586, 0.07504066302529314, 0.09709897913784883, 0.07313994422076017, 0.056468807878173786, 0.1110871932544378, 0.1268446725147993, 0.213752797958653, 0.09835869560405175, 0.14764809719646674, 0.06708397930140333, 0.03625180366488434, 0.07198355991822832, 0.05792573367658241, 0.09695079493294542, 0.10761728297611427, 0.09461766183892713, 0.1540491976197313, 0.1517457486716754, 0.062228222010610144, 0.11867482264445775, 0.0552525372218011, 0.054236521649237, 0.12189600230672201, 0.24133836754203383, 0.08752205832898204, 0.09144326032317779, 0.25610470547446723, 0.048331219386918124, 0.10287653306267805, 0.11599051830834263, 0.17481653546062184, 0.25196197046460156, 0.2833376082827897, 0.26061794856994985, 0.15750730014940922, 0.1396224849085885, 0.020675128390531695, 0.165159617444559, 0.14617089699781408, 0.20924385808541557, 0.16699308911932478, 0.13886820555192206, 0.05658881569421972, 0.1534370907262556, 0.040997184732118705, 0.2297010680737186, 0.053302633224138855, 0.10199401145351825, 0.06907896914271883, 0.0468139837660956, 0.1520416176105232, 0.21184704400681464, 0.08599538419729716, 0.19497391771343975, 0.12215418357135296, 0.12306886433101649, 0.1708322790030478, 0.10049620365791669, 0.13733660834911732, 0.0729653673600181, 0.05934700198549685, 0.14987791608522785, 0.17954391394895888, 0.07612286516868177, 0.10981960419820708, 0.0974835010381541, 0.13565745747054417, 0.09667379826417301, 0.11820379695546826, 0.0870388864553533, 0.09668367709957991, 0.16384522459459333, 0.19203944600961895, 0.13850601606011986, 0.09371091130752195, 0.08683160090826647, 0.0629762885577799, 0.1727684939889033, 0.11088004263414977, 0.11269848647396387, 0.06492355104082481, 0.101279751939454, 0.0780896032665511, 0.1639391469503415, 0.11450048780904966, 0.24950451951211908, 0.16463729088978155, 0.1024634788272676, 0.10678076112565975, 0.11278470222426451, 0.28604537794848883, 0.20573182865823686, 0.12050680125332836, 0.12326117583273946, 0.1379548946118957, 0.2577771976771232, 0.21962888923041535, 0.1346308248574615, 0.12216927701992757, 0.07859006218348974, 0.2433979336997218, 0.13479384440233863, 0.1617807255433357, 0.21994986848827489, 0.053674832345682245, 0.12822027034416988, 0.07586900305169342, 0.06630763406069443, 0.08563106753177328, 0.05668185843167288, 0.06141989515662409, 0.2553641051633983, 0.11341415005689759, 0.15713567416709473, 0.08266548090698615, 0.061981093578837854, 0.13976381843364302, 0.08957863466265524, 0.1942845108675317, 0.13001056219226065, 0.1549164412370928, 0.20355975814028857, 0.05094507707840823, 0.041072916079711294, 0.12667113269331953, 0.1469749687994932, 0.11516411710017077, 0.10566771987700695, 0.14251589437840997, 0.17364751697441558, 0.11540030866849635, 0.06532507751789562, 0.20952992103001872, 0.082890058347605, 0.21511652365488165]
0.09417704334780602
Making ranges
torch.Size([38212, 2])
We keep 7.12e+06/5.73e+08 =  1% of the original kernel matrix.

torch.Size([1205, 2])
We keep 1.57e+04/1.70e+05 =  9% of the original kernel matrix.

torch.Size([9032, 2])
We keep 4.41e+05/9.86e+06 =  4% of the original kernel matrix.

torch.Size([9034, 2])
We keep 7.92e+05/1.98e+07 =  4% of the original kernel matrix.

torch.Size([19315, 2])
We keep 2.16e+06/1.06e+08 =  2% of the original kernel matrix.

torch.Size([3005, 2])
We keep 7.92e+04/1.27e+06 =  6% of the original kernel matrix.

torch.Size([12713, 2])
We keep 8.29e+05/2.70e+07 =  3% of the original kernel matrix.

torch.Size([7181, 2])
We keep 2.05e+06/2.84e+07 =  7% of the original kernel matrix.

torch.Size([16428, 2])
We keep 2.47e+06/1.28e+08 =  1% of the original kernel matrix.

torch.Size([8823, 2])
We keep 5.46e+05/1.44e+07 =  3% of the original kernel matrix.

torch.Size([19369, 2])
We keep 1.92e+06/9.07e+07 =  2% of the original kernel matrix.

torch.Size([12543, 2])
We keep 1.09e+06/3.60e+07 =  3% of the original kernel matrix.

torch.Size([22624, 2])
We keep 2.70e+06/1.44e+08 =  1% of the original kernel matrix.

torch.Size([67897, 2])
We keep 2.28e+07/1.70e+09 =  1% of the original kernel matrix.

torch.Size([53187, 2])
We keep 1.23e+07/9.87e+08 =  1% of the original kernel matrix.

torch.Size([25913, 2])
We keep 3.50e+06/1.99e+08 =  1% of the original kernel matrix.

torch.Size([33288, 2])
We keep 5.17e+06/3.38e+08 =  1% of the original kernel matrix.

torch.Size([6400, 2])
We keep 5.04e+05/9.28e+06 =  5% of the original kernel matrix.

torch.Size([16777, 2])
We keep 1.66e+06/7.29e+07 =  2% of the original kernel matrix.

torch.Size([8446, 2])
We keep 4.78e+06/4.10e+07 = 11% of the original kernel matrix.

torch.Size([18733, 2])
We keep 2.74e+06/1.53e+08 =  1% of the original kernel matrix.

torch.Size([5688, 2])
We keep 2.88e+05/6.08e+06 =  4% of the original kernel matrix.

torch.Size([16122, 2])
We keep 1.41e+06/5.90e+07 =  2% of the original kernel matrix.

torch.Size([69513, 2])
We keep 4.62e+07/2.26e+09 =  2% of the original kernel matrix.

torch.Size([52836, 2])
We keep 1.39e+07/1.14e+09 =  1% of the original kernel matrix.

torch.Size([7431, 2])
We keep 3.58e+05/9.27e+06 =  3% of the original kernel matrix.

torch.Size([17984, 2])
We keep 1.64e+06/7.28e+07 =  2% of the original kernel matrix.

torch.Size([243915, 2])
We keep 2.45e+08/2.24e+10 =  1% of the original kernel matrix.

torch.Size([103173, 2])
We keep 3.72e+07/3.58e+09 =  1% of the original kernel matrix.

torch.Size([24533, 2])
We keep 4.53e+06/1.95e+08 =  2% of the original kernel matrix.

torch.Size([32078, 2])
We keep 5.04e+06/3.35e+08 =  1% of the original kernel matrix.

torch.Size([21794, 2])
We keep 3.23e+06/1.36e+08 =  2% of the original kernel matrix.

torch.Size([30244, 2])
We keep 4.49e+06/2.79e+08 =  1% of the original kernel matrix.

torch.Size([56294, 2])
We keep 1.81e+07/1.20e+09 =  1% of the original kernel matrix.

torch.Size([48777, 2])
We keep 1.06e+07/8.27e+08 =  1% of the original kernel matrix.

torch.Size([34234, 2])
We keep 8.45e+06/4.81e+08 =  1% of the original kernel matrix.

torch.Size([36209, 2])
We keep 6.94e+06/5.25e+08 =  1% of the original kernel matrix.

torch.Size([147726, 2])
We keep 1.14e+08/8.52e+09 =  1% of the original kernel matrix.

torch.Size([78411, 2])
We keep 2.40e+07/2.21e+09 =  1% of the original kernel matrix.

torch.Size([127344, 2])
We keep 9.99e+07/5.65e+09 =  1% of the original kernel matrix.

torch.Size([72179, 2])
We keep 2.01e+07/1.80e+09 =  1% of the original kernel matrix.

torch.Size([6053, 2])
We keep 5.74e+05/1.15e+07 =  5% of the original kernel matrix.

torch.Size([15959, 2])
We keep 1.81e+06/8.10e+07 =  2% of the original kernel matrix.

torch.Size([164172, 2])
We keep 1.12e+08/9.41e+09 =  1% of the original kernel matrix.

torch.Size([83362, 2])
We keep 2.45e+07/2.32e+09 =  1% of the original kernel matrix.

torch.Size([13022, 2])
We keep 1.28e+06/3.66e+07 =  3% of the original kernel matrix.

torch.Size([23071, 2])
We keep 2.65e+06/1.45e+08 =  1% of the original kernel matrix.

torch.Size([30087, 2])
We keep 1.57e+07/3.58e+08 =  4% of the original kernel matrix.

torch.Size([35576, 2])
We keep 6.69e+06/4.53e+08 =  1% of the original kernel matrix.

torch.Size([1620, 2])
We keep 4.95e+04/4.69e+05 = 10% of the original kernel matrix.

torch.Size([9722, 2])
We keep 5.98e+05/1.64e+07 =  3% of the original kernel matrix.

torch.Size([89644, 2])
We keep 2.93e+07/2.64e+09 =  1% of the original kernel matrix.

torch.Size([60687, 2])
We keep 1.46e+07/1.23e+09 =  1% of the original kernel matrix.

torch.Size([46276, 2])
We keep 1.72e+07/9.29e+08 =  1% of the original kernel matrix.

torch.Size([43852, 2])
We keep 9.68e+06/7.30e+08 =  1% of the original kernel matrix.

torch.Size([5289, 2])
We keep 2.22e+05/4.54e+06 =  4% of the original kernel matrix.

torch.Size([15729, 2])
We keep 1.27e+06/5.10e+07 =  2% of the original kernel matrix.

torch.Size([36191, 2])
We keep 1.72e+07/5.86e+08 =  2% of the original kernel matrix.

torch.Size([38828, 2])
We keep 7.75e+06/5.79e+08 =  1% of the original kernel matrix.

torch.Size([766460, 2])
We keep 1.25e+09/1.97e+11 =  0% of the original kernel matrix.

torch.Size([187042, 2])
We keep 9.61e+07/1.06e+10 =  0% of the original kernel matrix.

torch.Size([7086, 2])
We keep 4.35e+05/9.75e+06 =  4% of the original kernel matrix.

torch.Size([17611, 2])
We keep 1.67e+06/7.47e+07 =  2% of the original kernel matrix.

torch.Size([661850, 2])
We keep 2.25e+09/2.20e+11 =  1% of the original kernel matrix.

torch.Size([171669, 2])
We keep 1.04e+08/1.12e+10 =  0% of the original kernel matrix.

torch.Size([15471, 2])
We keep 7.87e+06/1.36e+08 =  5% of the original kernel matrix.

torch.Size([24925, 2])
We keep 4.54e+06/2.79e+08 =  1% of the original kernel matrix.

torch.Size([5656, 2])
We keep 2.40e+05/5.20e+06 =  4% of the original kernel matrix.

torch.Size([16172, 2])
We keep 1.33e+06/5.46e+07 =  2% of the original kernel matrix.

torch.Size([5973, 2])
We keep 3.41e+05/7.81e+06 =  4% of the original kernel matrix.

torch.Size([16169, 2])
We keep 1.55e+06/6.69e+07 =  2% of the original kernel matrix.

torch.Size([53215, 2])
We keep 3.64e+07/1.36e+09 =  2% of the original kernel matrix.

torch.Size([46585, 2])
We keep 1.15e+07/8.83e+08 =  1% of the original kernel matrix.

torch.Size([114665, 2])
We keep 8.39e+07/4.55e+09 =  1% of the original kernel matrix.

torch.Size([68429, 2])
We keep 1.88e+07/1.61e+09 =  1% of the original kernel matrix.

torch.Size([2334, 2])
We keep 5.01e+04/8.21e+05 =  6% of the original kernel matrix.

torch.Size([11375, 2])
We keep 7.23e+05/2.17e+07 =  3% of the original kernel matrix.

torch.Size([11368, 2])
We keep 1.02e+06/2.96e+07 =  3% of the original kernel matrix.

torch.Size([21557, 2])
We keep 2.51e+06/1.30e+08 =  1% of the original kernel matrix.

torch.Size([2986, 2])
We keep 9.93e+04/1.55e+06 =  6% of the original kernel matrix.

torch.Size([12414, 2])
We keep 9.17e+05/2.98e+07 =  3% of the original kernel matrix.

torch.Size([3923, 2])
We keep 1.35e+05/2.64e+06 =  5% of the original kernel matrix.

torch.Size([13929, 2])
We keep 1.06e+06/3.89e+07 =  2% of the original kernel matrix.

torch.Size([1886, 2])
We keep 5.15e+04/6.87e+05 =  7% of the original kernel matrix.

torch.Size([10091, 2])
We keep 6.70e+05/1.98e+07 =  3% of the original kernel matrix.

torch.Size([1151, 2])
We keep 7.05e+04/3.40e+05 = 20% of the original kernel matrix.

torch.Size([8349, 2])
We keep 5.42e+05/1.40e+07 =  3% of the original kernel matrix.

torch.Size([3767, 2])
We keep 1.32e+05/2.28e+06 =  5% of the original kernel matrix.

torch.Size([13593, 2])
We keep 1.04e+06/3.61e+07 =  2% of the original kernel matrix.

torch.Size([1401, 2])
We keep 1.71e+04/1.99e+05 =  8% of the original kernel matrix.

torch.Size([9671, 2])
We keep 4.56e+05/1.07e+07 =  4% of the original kernel matrix.

torch.Size([1289, 2])
We keep 2.13e+04/2.12e+05 = 10% of the original kernel matrix.

torch.Size([9201, 2])
We keep 4.85e+05/1.10e+07 =  4% of the original kernel matrix.

torch.Size([4862, 2])
We keep 2.00e+05/3.94e+06 =  5% of the original kernel matrix.

torch.Size([15250, 2])
We keep 1.22e+06/4.75e+07 =  2% of the original kernel matrix.

torch.Size([2911, 2])
We keep 7.07e+04/1.20e+06 =  5% of the original kernel matrix.

torch.Size([12498, 2])
We keep 8.21e+05/2.62e+07 =  3% of the original kernel matrix.

torch.Size([1329, 2])
We keep 2.24e+04/2.58e+05 =  8% of the original kernel matrix.

torch.Size([9116, 2])
We keep 5.01e+05/1.22e+07 =  4% of the original kernel matrix.

torch.Size([17184, 2])
We keep 2.99e+06/1.10e+08 =  2% of the original kernel matrix.

torch.Size([26248, 2])
We keep 4.15e+06/2.51e+08 =  1% of the original kernel matrix.

torch.Size([10513, 2])
We keep 8.28e+05/2.39e+07 =  3% of the original kernel matrix.

torch.Size([20883, 2])
We keep 2.35e+06/1.17e+08 =  2% of the original kernel matrix.

torch.Size([7910, 2])
We keep 4.75e+05/1.27e+07 =  3% of the original kernel matrix.

torch.Size([18389, 2])
We keep 1.84e+06/8.54e+07 =  2% of the original kernel matrix.

torch.Size([6786, 2])
We keep 4.02e+05/1.03e+07 =  3% of the original kernel matrix.

torch.Size([17148, 2])
We keep 1.72e+06/7.68e+07 =  2% of the original kernel matrix.

torch.Size([3291, 2])
We keep 9.65e+04/1.70e+06 =  5% of the original kernel matrix.

torch.Size([12908, 2])
We keep 9.26e+05/3.12e+07 =  2% of the original kernel matrix.

torch.Size([6368, 2])
We keep 3.25e+05/7.51e+06 =  4% of the original kernel matrix.

torch.Size([16860, 2])
We keep 1.55e+06/6.56e+07 =  2% of the original kernel matrix.

torch.Size([6941, 2])
We keep 6.45e+05/1.40e+07 =  4% of the original kernel matrix.

torch.Size([16864, 2])
We keep 1.96e+06/8.95e+07 =  2% of the original kernel matrix.

torch.Size([1842, 2])
We keep 4.03e+04/5.52e+05 =  7% of the original kernel matrix.

torch.Size([10459, 2])
We keep 6.37e+05/1.78e+07 =  3% of the original kernel matrix.

torch.Size([5531, 2])
We keep 2.18e+05/5.13e+06 =  4% of the original kernel matrix.

torch.Size([15936, 2])
We keep 1.35e+06/5.42e+07 =  2% of the original kernel matrix.

torch.Size([3291, 2])
We keep 1.02e+05/1.85e+06 =  5% of the original kernel matrix.

torch.Size([12906, 2])
We keep 9.52e+05/3.25e+07 =  2% of the original kernel matrix.

torch.Size([3626, 2])
We keep 1.16e+05/2.03e+06 =  5% of the original kernel matrix.

torch.Size([13339, 2])
We keep 9.84e+05/3.41e+07 =  2% of the original kernel matrix.

torch.Size([8578, 2])
We keep 4.90e+05/1.38e+07 =  3% of the original kernel matrix.

torch.Size([19186, 2])
We keep 1.90e+06/8.89e+07 =  2% of the original kernel matrix.

torch.Size([1491, 2])
We keep 2.63e+04/2.87e+05 =  9% of the original kernel matrix.

torch.Size([9540, 2])
We keep 5.14e+05/1.28e+07 =  4% of the original kernel matrix.

torch.Size([14427, 2])
We keep 1.39e+06/4.78e+07 =  2% of the original kernel matrix.

torch.Size([24266, 2])
We keep 3.03e+06/1.66e+08 =  1% of the original kernel matrix.

torch.Size([3979, 2])
We keep 1.34e+05/2.41e+06 =  5% of the original kernel matrix.

torch.Size([13947, 2])
We keep 1.04e+06/3.71e+07 =  2% of the original kernel matrix.

torch.Size([1835, 2])
We keep 3.45e+04/4.90e+05 =  7% of the original kernel matrix.

torch.Size([10473, 2])
We keep 6.14e+05/1.68e+07 =  3% of the original kernel matrix.

torch.Size([3839, 2])
We keep 1.21e+05/2.30e+06 =  5% of the original kernel matrix.

torch.Size([13855, 2])
We keep 1.03e+06/3.63e+07 =  2% of the original kernel matrix.

torch.Size([1884, 2])
We keep 5.48e+04/7.09e+05 =  7% of the original kernel matrix.

torch.Size([10322, 2])
We keep 7.03e+05/2.01e+07 =  3% of the original kernel matrix.

torch.Size([3483, 2])
We keep 1.14e+05/1.99e+06 =  5% of the original kernel matrix.

torch.Size([13169, 2])
We keep 9.92e+05/3.38e+07 =  2% of the original kernel matrix.

torch.Size([4570, 2])
We keep 1.40e+05/2.93e+06 =  4% of the original kernel matrix.

torch.Size([14957, 2])
We keep 1.11e+06/4.09e+07 =  2% of the original kernel matrix.

torch.Size([2167, 2])
We keep 4.82e+04/6.13e+05 =  7% of the original kernel matrix.

torch.Size([11070, 2])
We keep 6.71e+05/1.87e+07 =  3% of the original kernel matrix.

torch.Size([2037, 2])
We keep 6.95e+04/7.06e+05 =  9% of the original kernel matrix.

torch.Size([10312, 2])
We keep 6.96e+05/2.01e+07 =  3% of the original kernel matrix.

torch.Size([5204, 2])
We keep 2.07e+05/4.27e+06 =  4% of the original kernel matrix.

torch.Size([15455, 2])
We keep 1.27e+06/4.94e+07 =  2% of the original kernel matrix.

torch.Size([2529, 2])
We keep 4.89e+04/7.71e+05 =  6% of the original kernel matrix.

torch.Size([11895, 2])
We keep 7.12e+05/2.10e+07 =  3% of the original kernel matrix.

torch.Size([5197, 2])
We keep 2.65e+05/5.53e+06 =  4% of the original kernel matrix.

torch.Size([15312, 2])
We keep 1.39e+06/5.63e+07 =  2% of the original kernel matrix.

torch.Size([3474, 2])
We keep 1.14e+05/2.08e+06 =  5% of the original kernel matrix.

torch.Size([13248, 2])
We keep 9.97e+05/3.45e+07 =  2% of the original kernel matrix.

torch.Size([6242, 2])
We keep 3.51e+05/8.16e+06 =  4% of the original kernel matrix.

torch.Size([16622, 2])
We keep 1.57e+06/6.84e+07 =  2% of the original kernel matrix.

torch.Size([1906, 2])
We keep 4.06e+04/5.33e+05 =  7% of the original kernel matrix.

torch.Size([10657, 2])
We keep 6.28e+05/1.75e+07 =  3% of the original kernel matrix.

torch.Size([832, 2])
We keep 7.68e+03/5.71e+04 = 13% of the original kernel matrix.

torch.Size([8042, 2])
We keep 3.17e+05/5.72e+06 =  5% of the original kernel matrix.

torch.Size([3072, 2])
We keep 9.07e+04/1.46e+06 =  6% of the original kernel matrix.

torch.Size([12627, 2])
We keep 8.82e+05/2.89e+07 =  3% of the original kernel matrix.

torch.Size([6860, 2])
We keep 3.87e+05/9.50e+06 =  4% of the original kernel matrix.

torch.Size([17286, 2])
We keep 1.71e+06/7.38e+07 =  2% of the original kernel matrix.

torch.Size([3851, 2])
We keep 1.25e+05/2.23e+06 =  5% of the original kernel matrix.

torch.Size([13785, 2])
We keep 1.00e+06/3.57e+07 =  2% of the original kernel matrix.

torch.Size([1181, 2])
We keep 1.57e+04/1.61e+05 =  9% of the original kernel matrix.

torch.Size([8870, 2])
We keep 4.28e+05/9.60e+06 =  4% of the original kernel matrix.

torch.Size([12683, 2])
We keep 1.17e+06/3.68e+07 =  3% of the original kernel matrix.

torch.Size([22726, 2])
We keep 2.75e+06/1.45e+08 =  1% of the original kernel matrix.

torch.Size([1325, 2])
We keep 1.84e+04/1.90e+05 =  9% of the original kernel matrix.

torch.Size([9280, 2])
We keep 4.66e+05/1.04e+07 =  4% of the original kernel matrix.

torch.Size([2475, 2])
We keep 6.72e+04/1.05e+06 =  6% of the original kernel matrix.

torch.Size([11472, 2])
We keep 7.78e+05/2.45e+07 =  3% of the original kernel matrix.

torch.Size([1319, 2])
We keep 2.00e+04/1.98e+05 = 10% of the original kernel matrix.

torch.Size([9329, 2])
We keep 4.72e+05/1.06e+07 =  4% of the original kernel matrix.

torch.Size([4971, 2])
We keep 2.74e+05/5.78e+06 =  4% of the original kernel matrix.

torch.Size([15012, 2])
We keep 1.43e+06/5.76e+07 =  2% of the original kernel matrix.

torch.Size([1333, 2])
We keep 1.67e+04/1.88e+05 =  8% of the original kernel matrix.

torch.Size([9449, 2])
We keep 4.56e+05/1.04e+07 =  4% of the original kernel matrix.

torch.Size([4603, 2])
We keep 1.85e+05/3.64e+06 =  5% of the original kernel matrix.

torch.Size([14663, 2])
We keep 1.19e+06/4.57e+07 =  2% of the original kernel matrix.

torch.Size([3444, 2])
We keep 1.10e+05/1.97e+06 =  5% of the original kernel matrix.

torch.Size([13204, 2])
We keep 9.69e+05/3.36e+07 =  2% of the original kernel matrix.

torch.Size([1307, 2])
We keep 2.00e+04/2.21e+05 =  9% of the original kernel matrix.

torch.Size([9104, 2])
We keep 4.87e+05/1.12e+07 =  4% of the original kernel matrix.

torch.Size([1559, 2])
We keep 2.97e+04/3.59e+05 =  8% of the original kernel matrix.

torch.Size([9837, 2])
We keep 5.47e+05/1.43e+07 =  3% of the original kernel matrix.

torch.Size([1654, 2])
We keep 2.52e+04/3.28e+05 =  7% of the original kernel matrix.

torch.Size([10227, 2])
We keep 5.37e+05/1.37e+07 =  3% of the original kernel matrix.

torch.Size([3812, 2])
We keep 1.30e+05/2.38e+06 =  5% of the original kernel matrix.

torch.Size([13781, 2])
We keep 1.04e+06/3.69e+07 =  2% of the original kernel matrix.

torch.Size([3939, 2])
We keep 1.66e+05/3.25e+06 =  5% of the original kernel matrix.

torch.Size([13616, 2])
We keep 1.16e+06/4.31e+07 =  2% of the original kernel matrix.

torch.Size([4218, 2])
We keep 1.65e+05/3.30e+06 =  4% of the original kernel matrix.

torch.Size([14204, 2])
We keep 1.16e+06/4.35e+07 =  2% of the original kernel matrix.

torch.Size([5257, 2])
We keep 2.17e+05/4.63e+06 =  4% of the original kernel matrix.

torch.Size([15624, 2])
We keep 1.31e+06/5.15e+07 =  2% of the original kernel matrix.

torch.Size([10242, 2])
We keep 1.08e+06/2.74e+07 =  3% of the original kernel matrix.

torch.Size([20544, 2])
We keep 2.47e+06/1.25e+08 =  1% of the original kernel matrix.

torch.Size([4046, 2])
We keep 1.37e+05/2.75e+06 =  4% of the original kernel matrix.

torch.Size([14092, 2])
We keep 1.10e+06/3.97e+07 =  2% of the original kernel matrix.

torch.Size([2016, 2])
We keep 3.86e+04/5.46e+05 =  7% of the original kernel matrix.

torch.Size([10998, 2])
We keep 6.31e+05/1.77e+07 =  3% of the original kernel matrix.

torch.Size([15859, 2])
We keep 1.75e+06/6.75e+07 =  2% of the original kernel matrix.

torch.Size([25499, 2])
We keep 3.43e+06/1.97e+08 =  1% of the original kernel matrix.

torch.Size([2579, 2])
We keep 6.20e+04/9.14e+05 =  6% of the original kernel matrix.

torch.Size([11862, 2])
We keep 7.59e+05/2.29e+07 =  3% of the original kernel matrix.

torch.Size([7593, 2])
We keep 3.89e+05/1.00e+07 =  3% of the original kernel matrix.

torch.Size([18122, 2])
We keep 1.71e+06/7.57e+07 =  2% of the original kernel matrix.

torch.Size([2081, 2])
We keep 4.77e+04/6.71e+05 =  7% of the original kernel matrix.

torch.Size([10787, 2])
We keep 6.74e+05/1.96e+07 =  3% of the original kernel matrix.

torch.Size([2756, 2])
We keep 1.00e+05/1.43e+06 =  7% of the original kernel matrix.

torch.Size([11869, 2])
We keep 8.62e+05/2.86e+07 =  3% of the original kernel matrix.

torch.Size([2232, 2])
We keep 4.99e+04/7.74e+05 =  6% of the original kernel matrix.

torch.Size([11245, 2])
We keep 7.16e+05/2.11e+07 =  3% of the original kernel matrix.

torch.Size([6339, 2])
We keep 2.66e+05/6.56e+06 =  4% of the original kernel matrix.

torch.Size([16870, 2])
We keep 1.45e+06/6.13e+07 =  2% of the original kernel matrix.

torch.Size([1056, 2])
We keep 9.63e+03/8.29e+04 = 11% of the original kernel matrix.

torch.Size([8756, 2])
We keep 3.47e+05/6.89e+06 =  5% of the original kernel matrix.

torch.Size([3255, 2])
We keep 8.64e+04/1.60e+06 =  5% of the original kernel matrix.

torch.Size([12847, 2])
We keep 8.96e+05/3.03e+07 =  2% of the original kernel matrix.

torch.Size([3358, 2])
We keep 1.27e+05/1.92e+06 =  6% of the original kernel matrix.

torch.Size([12955, 2])
We keep 9.55e+05/3.32e+07 =  2% of the original kernel matrix.

torch.Size([1786, 2])
We keep 3.24e+04/4.22e+05 =  7% of the original kernel matrix.

torch.Size([10153, 2])
We keep 5.75e+05/1.56e+07 =  3% of the original kernel matrix.

torch.Size([1416, 2])
We keep 2.41e+04/2.52e+05 =  9% of the original kernel matrix.

torch.Size([9321, 2])
We keep 4.98e+05/1.20e+07 =  4% of the original kernel matrix.

torch.Size([1615, 2])
We keep 3.33e+04/3.72e+05 =  8% of the original kernel matrix.

torch.Size([9694, 2])
We keep 5.81e+05/1.46e+07 =  3% of the original kernel matrix.

torch.Size([5143, 2])
We keep 2.50e+05/5.15e+06 =  4% of the original kernel matrix.

torch.Size([15375, 2])
We keep 1.32e+06/5.43e+07 =  2% of the original kernel matrix.

torch.Size([3753, 2])
We keep 1.13e+05/2.09e+06 =  5% of the original kernel matrix.

torch.Size([13616, 2])
We keep 9.81e+05/3.46e+07 =  2% of the original kernel matrix.

torch.Size([1604, 2])
We keep 3.73e+04/4.13e+05 =  9% of the original kernel matrix.

torch.Size([9571, 2])
We keep 5.97e+05/1.54e+07 =  3% of the original kernel matrix.

torch.Size([1170, 2])
We keep 1.36e+04/1.49e+05 =  9% of the original kernel matrix.

torch.Size([9037, 2])
We keep 4.17e+05/9.24e+06 =  4% of the original kernel matrix.

torch.Size([1230, 2])
We keep 1.78e+04/1.81e+05 =  9% of the original kernel matrix.

torch.Size([9047, 2])
We keep 4.53e+05/1.02e+07 =  4% of the original kernel matrix.

torch.Size([5713, 2])
We keep 3.17e+05/7.10e+06 =  4% of the original kernel matrix.

torch.Size([15827, 2])
We keep 1.52e+06/6.38e+07 =  2% of the original kernel matrix.

torch.Size([1889, 2])
We keep 3.82e+04/5.42e+05 =  7% of the original kernel matrix.

torch.Size([10525, 2])
We keep 6.39e+05/1.76e+07 =  3% of the original kernel matrix.

torch.Size([9036, 2])
We keep 5.51e+05/1.50e+07 =  3% of the original kernel matrix.

torch.Size([19466, 2])
We keep 1.95e+06/9.28e+07 =  2% of the original kernel matrix.

torch.Size([2492, 2])
We keep 6.57e+04/9.92e+05 =  6% of the original kernel matrix.

torch.Size([11608, 2])
We keep 7.65e+05/2.38e+07 =  3% of the original kernel matrix.

torch.Size([2238, 2])
We keep 4.07e+04/6.12e+05 =  6% of the original kernel matrix.

torch.Size([11311, 2])
We keep 6.61e+05/1.87e+07 =  3% of the original kernel matrix.

torch.Size([2388, 2])
We keep 6.07e+04/8.63e+05 =  7% of the original kernel matrix.

torch.Size([11469, 2])
We keep 7.25e+05/2.22e+07 =  3% of the original kernel matrix.

torch.Size([2403, 2])
We keep 5.28e+04/8.34e+05 =  6% of the original kernel matrix.

torch.Size([11621, 2])
We keep 7.30e+05/2.18e+07 =  3% of the original kernel matrix.

torch.Size([3175, 2])
We keep 9.59e+04/1.65e+06 =  5% of the original kernel matrix.

torch.Size([12756, 2])
We keep 9.22e+05/3.08e+07 =  2% of the original kernel matrix.

torch.Size([2803, 2])
We keep 7.00e+04/1.06e+06 =  6% of the original kernel matrix.

torch.Size([12192, 2])
We keep 7.95e+05/2.46e+07 =  3% of the original kernel matrix.

torch.Size([1955, 2])
We keep 4.50e+04/5.48e+05 =  8% of the original kernel matrix.

torch.Size([10783, 2])
We keep 6.48e+05/1.77e+07 =  3% of the original kernel matrix.

torch.Size([21696, 2])
We keep 2.58e+06/1.28e+08 =  2% of the original kernel matrix.

torch.Size([29996, 2])
We keep 4.33e+06/2.71e+08 =  1% of the original kernel matrix.

torch.Size([3993, 2])
We keep 1.56e+05/2.98e+06 =  5% of the original kernel matrix.

torch.Size([13831, 2])
We keep 1.13e+06/4.13e+07 =  2% of the original kernel matrix.

torch.Size([4983, 2])
We keep 2.16e+05/4.99e+06 =  4% of the original kernel matrix.

torch.Size([15154, 2])
We keep 1.34e+06/5.35e+07 =  2% of the original kernel matrix.

torch.Size([2272, 2])
We keep 5.81e+04/7.26e+05 =  7% of the original kernel matrix.

torch.Size([11255, 2])
We keep 7.26e+05/2.04e+07 =  3% of the original kernel matrix.

torch.Size([2983, 2])
We keep 9.88e+04/1.44e+06 =  6% of the original kernel matrix.

torch.Size([12253, 2])
We keep 8.77e+05/2.87e+07 =  3% of the original kernel matrix.

torch.Size([2449, 2])
We keep 9.49e+04/1.38e+06 =  6% of the original kernel matrix.

torch.Size([11219, 2])
We keep 8.76e+05/2.81e+07 =  3% of the original kernel matrix.

torch.Size([5369, 2])
We keep 6.33e+05/5.65e+06 = 11% of the original kernel matrix.

torch.Size([15741, 2])
We keep 1.41e+06/5.69e+07 =  2% of the original kernel matrix.

torch.Size([1521, 2])
We keep 2.75e+04/3.62e+05 =  7% of the original kernel matrix.

torch.Size([9876, 2])
We keep 5.50e+05/1.44e+07 =  3% of the original kernel matrix.

torch.Size([770, 2])
We keep 8.36e+03/7.13e+04 = 11% of the original kernel matrix.

torch.Size([7643, 2])
We keep 3.45e+05/6.39e+06 =  5% of the original kernel matrix.

torch.Size([2546, 2])
We keep 5.98e+04/1.00e+06 =  5% of the original kernel matrix.

torch.Size([11767, 2])
We keep 7.81e+05/2.40e+07 =  3% of the original kernel matrix.

torch.Size([2775, 2])
We keep 6.69e+04/1.12e+06 =  5% of the original kernel matrix.

torch.Size([12283, 2])
We keep 8.09e+05/2.53e+07 =  3% of the original kernel matrix.

torch.Size([2078, 2])
We keep 4.20e+04/5.75e+05 =  7% of the original kernel matrix.

torch.Size([10988, 2])
We keep 6.46e+05/1.81e+07 =  3% of the original kernel matrix.

torch.Size([4526, 2])
We keep 1.47e+05/2.91e+06 =  5% of the original kernel matrix.

torch.Size([14773, 2])
We keep 1.11e+06/4.09e+07 =  2% of the original kernel matrix.

torch.Size([2168, 2])
We keep 5.62e+04/6.45e+05 =  8% of the original kernel matrix.

torch.Size([10944, 2])
We keep 6.59e+05/1.92e+07 =  3% of the original kernel matrix.

torch.Size([9758, 2])
We keep 7.87e+05/2.20e+07 =  3% of the original kernel matrix.

torch.Size([20160, 2])
We keep 2.30e+06/1.12e+08 =  2% of the original kernel matrix.

torch.Size([12810, 2])
We keep 1.26e+06/4.37e+07 =  2% of the original kernel matrix.

torch.Size([22829, 2])
We keep 2.88e+06/1.58e+08 =  1% of the original kernel matrix.

torch.Size([10649, 2])
We keep 8.10e+05/2.47e+07 =  3% of the original kernel matrix.

torch.Size([20924, 2])
We keep 2.34e+06/1.19e+08 =  1% of the original kernel matrix.

torch.Size([1281, 2])
We keep 1.89e+04/2.30e+05 =  8% of the original kernel matrix.

torch.Size([9247, 2])
We keep 4.81e+05/1.15e+07 =  4% of the original kernel matrix.

torch.Size([2213, 2])
We keep 4.83e+04/6.67e+05 =  7% of the original kernel matrix.

torch.Size([11298, 2])
We keep 6.88e+05/1.96e+07 =  3% of the original kernel matrix.

torch.Size([4867, 2])
We keep 4.67e+05/5.18e+06 =  9% of the original kernel matrix.

torch.Size([14899, 2])
We keep 1.32e+06/5.44e+07 =  2% of the original kernel matrix.

torch.Size([2146, 2])
We keep 5.42e+04/7.04e+05 =  7% of the original kernel matrix.

torch.Size([10967, 2])
We keep 6.81e+05/2.01e+07 =  3% of the original kernel matrix.

torch.Size([2159, 2])
We keep 5.94e+04/8.26e+05 =  7% of the original kernel matrix.

torch.Size([10942, 2])
We keep 7.35e+05/2.18e+07 =  3% of the original kernel matrix.

torch.Size([2652, 2])
We keep 8.32e+04/1.27e+06 =  6% of the original kernel matrix.

torch.Size([11752, 2])
We keep 8.43e+05/2.70e+07 =  3% of the original kernel matrix.

torch.Size([6712, 2])
We keep 4.01e+05/9.17e+06 =  4% of the original kernel matrix.

torch.Size([17169, 2])
We keep 1.66e+06/7.25e+07 =  2% of the original kernel matrix.

torch.Size([7162, 2])
We keep 3.31e+05/8.63e+06 =  3% of the original kernel matrix.

torch.Size([17680, 2])
We keep 1.60e+06/7.03e+07 =  2% of the original kernel matrix.

torch.Size([6178, 2])
We keep 5.84e+05/1.27e+07 =  4% of the original kernel matrix.

torch.Size([16040, 2])
We keep 1.88e+06/8.53e+07 =  2% of the original kernel matrix.

torch.Size([3871, 2])
We keep 1.28e+05/2.39e+06 =  5% of the original kernel matrix.

torch.Size([13767, 2])
We keep 1.03e+06/3.70e+07 =  2% of the original kernel matrix.

torch.Size([21019, 2])
We keep 2.72e+06/1.22e+08 =  2% of the original kernel matrix.

torch.Size([29523, 2])
We keep 4.31e+06/2.64e+08 =  1% of the original kernel matrix.

torch.Size([2253, 2])
We keep 4.31e+04/6.46e+05 =  6% of the original kernel matrix.

torch.Size([11385, 2])
We keep 6.74e+05/1.92e+07 =  3% of the original kernel matrix.

torch.Size([2920, 2])
We keep 1.05e+05/1.46e+06 =  7% of the original kernel matrix.

torch.Size([12294, 2])
We keep 8.94e+05/2.90e+07 =  3% of the original kernel matrix.

torch.Size([3726, 2])
We keep 1.46e+05/2.74e+06 =  5% of the original kernel matrix.

torch.Size([13547, 2])
We keep 1.10e+06/3.96e+07 =  2% of the original kernel matrix.

torch.Size([399, 2])
We keep 4.83e+03/2.10e+04 = 22% of the original kernel matrix.

torch.Size([5852, 2])
We keep 2.34e+05/3.47e+06 =  6% of the original kernel matrix.

torch.Size([7391, 2])
We keep 4.20e+05/9.75e+06 =  4% of the original kernel matrix.

torch.Size([18014, 2])
We keep 1.69e+06/7.47e+07 =  2% of the original kernel matrix.

torch.Size([2030, 2])
We keep 5.12e+04/6.32e+05 =  8% of the original kernel matrix.

torch.Size([10584, 2])
We keep 6.63e+05/1.90e+07 =  3% of the original kernel matrix.

torch.Size([4338, 2])
We keep 2.06e+05/3.43e+06 =  6% of the original kernel matrix.

torch.Size([14479, 2])
We keep 1.17e+06/4.43e+07 =  2% of the original kernel matrix.

torch.Size([1568, 2])
We keep 2.27e+04/2.46e+05 =  9% of the original kernel matrix.

torch.Size([9742, 2])
We keep 4.90e+05/1.19e+07 =  4% of the original kernel matrix.

torch.Size([3250, 2])
We keep 1.18e+05/2.06e+06 =  5% of the original kernel matrix.

torch.Size([12603, 2])
We keep 9.64e+05/3.44e+07 =  2% of the original kernel matrix.

torch.Size([1279, 2])
We keep 1.90e+04/2.35e+05 =  8% of the original kernel matrix.

torch.Size([9128, 2])
We keep 4.84e+05/1.16e+07 =  4% of the original kernel matrix.

torch.Size([2227, 2])
We keep 4.45e+04/6.58e+05 =  6% of the original kernel matrix.

torch.Size([11330, 2])
We keep 6.69e+05/1.94e+07 =  3% of the original kernel matrix.

torch.Size([2309, 2])
We keep 5.33e+04/7.80e+05 =  6% of the original kernel matrix.

torch.Size([11401, 2])
We keep 7.19e+05/2.11e+07 =  3% of the original kernel matrix.

torch.Size([3392, 2])
We keep 9.17e+04/1.68e+06 =  5% of the original kernel matrix.

torch.Size([13212, 2])
We keep 9.24e+05/3.10e+07 =  2% of the original kernel matrix.

torch.Size([819, 2])
We keep 7.40e+03/6.15e+04 = 12% of the original kernel matrix.

torch.Size([8127, 2])
We keep 3.23e+05/5.93e+06 =  5% of the original kernel matrix.

torch.Size([1703, 2])
We keep 3.24e+04/3.91e+05 =  8% of the original kernel matrix.

torch.Size([10085, 2])
We keep 5.85e+05/1.50e+07 =  3% of the original kernel matrix.

torch.Size([1995, 2])
We keep 4.87e+04/6.08e+05 =  8% of the original kernel matrix.

torch.Size([10623, 2])
We keep 6.57e+05/1.87e+07 =  3% of the original kernel matrix.

torch.Size([5845, 2])
We keep 2.59e+05/5.93e+06 =  4% of the original kernel matrix.

torch.Size([16308, 2])
We keep 1.40e+06/5.83e+07 =  2% of the original kernel matrix.

torch.Size([1920, 2])
We keep 4.17e+04/5.94e+05 =  7% of the original kernel matrix.

torch.Size([10568, 2])
We keep 6.42e+05/1.85e+07 =  3% of the original kernel matrix.

torch.Size([5890, 2])
We keep 2.47e+05/5.61e+06 =  4% of the original kernel matrix.

torch.Size([16505, 2])
We keep 1.38e+06/5.67e+07 =  2% of the original kernel matrix.

torch.Size([3584, 2])
We keep 1.07e+05/1.82e+06 =  5% of the original kernel matrix.

torch.Size([13522, 2])
We keep 9.50e+05/3.23e+07 =  2% of the original kernel matrix.

torch.Size([2824, 2])
We keep 8.51e+04/1.43e+06 =  5% of the original kernel matrix.

torch.Size([12181, 2])
We keep 8.79e+05/2.86e+07 =  3% of the original kernel matrix.

torch.Size([5097, 2])
We keep 2.04e+05/4.35e+06 =  4% of the original kernel matrix.

torch.Size([15490, 2])
We keep 1.27e+06/4.99e+07 =  2% of the original kernel matrix.

torch.Size([3765, 2])
We keep 1.79e+05/3.32e+06 =  5% of the original kernel matrix.

torch.Size([13220, 2])
We keep 1.17e+06/4.36e+07 =  2% of the original kernel matrix.

torch.Size([2628, 2])
We keep 9.33e+04/1.20e+06 =  7% of the original kernel matrix.

torch.Size([11727, 2])
We keep 8.15e+05/2.62e+07 =  3% of the original kernel matrix.

torch.Size([1841, 2])
We keep 3.28e+04/4.24e+05 =  7% of the original kernel matrix.

torch.Size([10394, 2])
We keep 5.87e+05/1.56e+07 =  3% of the original kernel matrix.

torch.Size([3902, 2])
We keep 1.41e+05/2.65e+06 =  5% of the original kernel matrix.

torch.Size([13789, 2])
We keep 1.08e+06/3.90e+07 =  2% of the original kernel matrix.

torch.Size([3304, 2])
We keep 1.40e+05/2.46e+06 =  5% of the original kernel matrix.

torch.Size([12602, 2])
We keep 1.06e+06/3.76e+07 =  2% of the original kernel matrix.

torch.Size([2404, 2])
We keep 4.80e+04/7.09e+05 =  6% of the original kernel matrix.

torch.Size([11638, 2])
We keep 6.84e+05/2.01e+07 =  3% of the original kernel matrix.

torch.Size([11128, 2])
We keep 6.86e+05/2.18e+07 =  3% of the original kernel matrix.

torch.Size([21433, 2])
We keep 2.22e+06/1.12e+08 =  1% of the original kernel matrix.

torch.Size([3769, 2])
We keep 1.31e+05/2.62e+06 =  5% of the original kernel matrix.

torch.Size([13594, 2])
We keep 1.06e+06/3.87e+07 =  2% of the original kernel matrix.

torch.Size([5849, 2])
We keep 2.99e+05/6.58e+06 =  4% of the original kernel matrix.

torch.Size([16269, 2])
We keep 1.48e+06/6.14e+07 =  2% of the original kernel matrix.

torch.Size([3264, 2])
We keep 1.40e+05/2.19e+06 =  6% of the original kernel matrix.

torch.Size([12734, 2])
We keep 1.02e+06/3.54e+07 =  2% of the original kernel matrix.

torch.Size([5643, 2])
We keep 2.42e+05/5.48e+06 =  4% of the original kernel matrix.

torch.Size([16047, 2])
We keep 1.36e+06/5.60e+07 =  2% of the original kernel matrix.

torch.Size([14676, 2])
We keep 1.63e+06/5.59e+07 =  2% of the original kernel matrix.

torch.Size([24514, 2])
We keep 3.24e+06/1.79e+08 =  1% of the original kernel matrix.

torch.Size([1771, 2])
We keep 4.22e+04/5.88e+05 =  7% of the original kernel matrix.

torch.Size([10063, 2])
We keep 6.53e+05/1.84e+07 =  3% of the original kernel matrix.

torch.Size([791, 2])
We keep 5.98e+03/4.45e+04 = 13% of the original kernel matrix.

torch.Size([7929, 2])
We keep 2.98e+05/5.05e+06 =  5% of the original kernel matrix.

torch.Size([2296, 2])
We keep 4.32e+04/7.06e+05 =  6% of the original kernel matrix.

torch.Size([11340, 2])
We keep 6.70e+05/2.01e+07 =  3% of the original kernel matrix.

torch.Size([1580, 2])
We keep 2.42e+04/3.02e+05 =  8% of the original kernel matrix.

torch.Size([9994, 2])
We keep 5.19e+05/1.32e+07 =  3% of the original kernel matrix.

torch.Size([3173, 2])
We keep 1.24e+05/2.02e+06 =  6% of the original kernel matrix.

torch.Size([12417, 2])
We keep 9.97e+05/3.41e+07 =  2% of the original kernel matrix.

torch.Size([3059, 2])
We keep 1.09e+05/1.95e+06 =  5% of the original kernel matrix.

torch.Size([12558, 2])
We keep 9.78e+05/3.34e+07 =  2% of the original kernel matrix.

torch.Size([1994, 2])
We keep 3.64e+04/5.13e+05 =  7% of the original kernel matrix.

torch.Size([10894, 2])
We keep 6.18e+05/1.71e+07 =  3% of the original kernel matrix.

torch.Size([1554, 2])
We keep 3.17e+04/2.79e+05 = 11% of the original kernel matrix.

torch.Size([9794, 2])
We keep 5.22e+05/1.26e+07 =  4% of the original kernel matrix.

torch.Size([2993, 2])
We keep 8.03e+04/1.39e+06 =  5% of the original kernel matrix.

torch.Size([12523, 2])
We keep 8.68e+05/2.82e+07 =  3% of the original kernel matrix.

torch.Size([1125, 2])
We keep 1.29e+04/1.22e+05 = 10% of the original kernel matrix.

torch.Size([8843, 2])
We keep 4.04e+05/8.35e+06 =  4% of the original kernel matrix.

torch.Size([2891, 2])
We keep 6.66e+04/1.15e+06 =  5% of the original kernel matrix.

torch.Size([12428, 2])
We keep 8.08e+05/2.57e+07 =  3% of the original kernel matrix.

torch.Size([1513, 2])
We keep 2.33e+04/2.58e+05 =  9% of the original kernel matrix.

torch.Size([9952, 2])
We keep 5.04e+05/1.22e+07 =  4% of the original kernel matrix.

torch.Size([4472, 2])
We keep 1.65e+05/3.40e+06 =  4% of the original kernel matrix.

torch.Size([14695, 2])
We keep 1.17e+06/4.42e+07 =  2% of the original kernel matrix.

torch.Size([9113, 2])
We keep 6.80e+05/1.91e+07 =  3% of the original kernel matrix.

torch.Size([19473, 2])
We keep 2.17e+06/1.05e+08 =  2% of the original kernel matrix.

torch.Size([2876, 2])
We keep 8.11e+04/1.35e+06 =  6% of the original kernel matrix.

torch.Size([12296, 2])
We keep 8.50e+05/2.78e+07 =  3% of the original kernel matrix.

torch.Size([876, 2])
We keep 6.77e+03/5.34e+04 = 12% of the original kernel matrix.

torch.Size([8279, 2])
We keep 3.10e+05/5.53e+06 =  5% of the original kernel matrix.

torch.Size([7516, 2])
We keep 4.85e+05/1.17e+07 =  4% of the original kernel matrix.

torch.Size([18013, 2])
We keep 1.81e+06/8.18e+07 =  2% of the original kernel matrix.

torch.Size([4397, 2])
We keep 1.49e+05/2.92e+06 =  5% of the original kernel matrix.

torch.Size([14569, 2])
We keep 1.11e+06/4.09e+07 =  2% of the original kernel matrix.

torch.Size([2425, 2])
We keep 5.49e+04/8.84e+05 =  6% of the original kernel matrix.

torch.Size([11683, 2])
We keep 7.50e+05/2.25e+07 =  3% of the original kernel matrix.

torch.Size([11871, 2])
We keep 8.92e+05/3.02e+07 =  2% of the original kernel matrix.

torch.Size([22010, 2])
We keep 2.53e+06/1.32e+08 =  1% of the original kernel matrix.

torch.Size([13183, 2])
We keep 1.20e+06/3.86e+07 =  3% of the original kernel matrix.

torch.Size([23174, 2])
We keep 2.79e+06/1.49e+08 =  1% of the original kernel matrix.

torch.Size([2257, 2])
We keep 6.59e+04/8.95e+05 =  7% of the original kernel matrix.

torch.Size([10994, 2])
We keep 7.54e+05/2.26e+07 =  3% of the original kernel matrix.

torch.Size([1602, 2])
We keep 2.95e+04/3.64e+05 =  8% of the original kernel matrix.

torch.Size([9875, 2])
We keep 5.59e+05/1.44e+07 =  3% of the original kernel matrix.

torch.Size([1060, 2])
We keep 1.35e+04/1.36e+05 =  9% of the original kernel matrix.

torch.Size([8558, 2])
We keep 4.15e+05/8.83e+06 =  4% of the original kernel matrix.

torch.Size([1592, 2])
We keep 2.89e+04/3.67e+05 =  7% of the original kernel matrix.

torch.Size([10083, 2])
We keep 5.75e+05/1.45e+07 =  3% of the original kernel matrix.

torch.Size([1291, 2])
We keep 2.82e+04/3.43e+05 =  8% of the original kernel matrix.

torch.Size([8734, 2])
We keep 5.36e+05/1.40e+07 =  3% of the original kernel matrix.

torch.Size([1206, 2])
We keep 1.34e+04/1.39e+05 =  9% of the original kernel matrix.

torch.Size([9219, 2])
We keep 4.20e+05/8.93e+06 =  4% of the original kernel matrix.

torch.Size([770, 2])
We keep 6.88e+03/5.66e+04 = 12% of the original kernel matrix.

torch.Size([7829, 2])
We keep 3.21e+05/5.70e+06 =  5% of the original kernel matrix.

torch.Size([6299, 2])
We keep 3.36e+05/7.28e+06 =  4% of the original kernel matrix.

torch.Size([16695, 2])
We keep 1.52e+06/6.46e+07 =  2% of the original kernel matrix.

torch.Size([2499, 2])
We keep 8.76e+04/1.11e+06 =  7% of the original kernel matrix.

torch.Size([11484, 2])
We keep 7.99e+05/2.52e+07 =  3% of the original kernel matrix.

torch.Size([2197, 2])
We keep 7.81e+04/1.04e+06 =  7% of the original kernel matrix.

torch.Size([10829, 2])
We keep 7.94e+05/2.44e+07 =  3% of the original kernel matrix.

torch.Size([2677, 2])
We keep 7.06e+04/1.23e+06 =  5% of the original kernel matrix.

torch.Size([12083, 2])
We keep 8.21e+05/2.65e+07 =  3% of the original kernel matrix.

torch.Size([5125, 2])
We keep 2.23e+05/4.59e+06 =  4% of the original kernel matrix.

torch.Size([15364, 2])
We keep 1.31e+06/5.13e+07 =  2% of the original kernel matrix.

torch.Size([2534, 2])
We keep 7.74e+04/1.17e+06 =  6% of the original kernel matrix.

torch.Size([11537, 2])
We keep 8.17e+05/2.59e+07 =  3% of the original kernel matrix.

torch.Size([8407, 2])
We keep 7.06e+05/1.93e+07 =  3% of the original kernel matrix.

torch.Size([18691, 2])
We keep 2.14e+06/1.05e+08 =  2% of the original kernel matrix.

torch.Size([5943, 2])
We keep 3.08e+05/6.55e+06 =  4% of the original kernel matrix.

torch.Size([16224, 2])
We keep 1.47e+06/6.13e+07 =  2% of the original kernel matrix.

torch.Size([1432, 2])
We keep 2.62e+04/2.97e+05 =  8% of the original kernel matrix.

torch.Size([9400, 2])
We keep 5.30e+05/1.30e+07 =  4% of the original kernel matrix.

torch.Size([1253, 2])
We keep 2.31e+04/2.35e+05 =  9% of the original kernel matrix.

torch.Size([8866, 2])
We keep 4.88e+05/1.16e+07 =  4% of the original kernel matrix.

torch.Size([7020, 2])
We keep 4.30e+05/1.03e+07 =  4% of the original kernel matrix.

torch.Size([17372, 2])
We keep 1.73e+06/7.68e+07 =  2% of the original kernel matrix.

torch.Size([714, 2])
We keep 8.07e+03/5.43e+04 = 14% of the original kernel matrix.

torch.Size([7358, 2])
We keep 3.20e+05/5.58e+06 =  5% of the original kernel matrix.

torch.Size([1446, 2])
We keep 2.47e+04/3.09e+05 =  8% of the original kernel matrix.

torch.Size([9494, 2])
We keep 5.22e+05/1.33e+07 =  3% of the original kernel matrix.

torch.Size([2149, 2])
We keep 5.44e+04/7.59e+05 =  7% of the original kernel matrix.

torch.Size([10920, 2])
We keep 7.12e+05/2.08e+07 =  3% of the original kernel matrix.

torch.Size([3219, 2])
We keep 1.08e+05/1.90e+06 =  5% of the original kernel matrix.

torch.Size([12746, 2])
We keep 9.65e+05/3.30e+07 =  2% of the original kernel matrix.

torch.Size([6384, 2])
We keep 3.29e+05/7.60e+06 =  4% of the original kernel matrix.

torch.Size([16802, 2])
We keep 1.54e+06/6.60e+07 =  2% of the original kernel matrix.

torch.Size([3690, 2])
We keep 1.09e+05/2.04e+06 =  5% of the original kernel matrix.

torch.Size([13656, 2])
We keep 9.82e+05/3.42e+07 =  2% of the original kernel matrix.

torch.Size([1643, 2])
We keep 4.66e+04/4.68e+05 =  9% of the original kernel matrix.

torch.Size([9722, 2])
We keep 6.24e+05/1.64e+07 =  3% of the original kernel matrix.

torch.Size([1505, 2])
We keep 2.13e+04/2.61e+05 =  8% of the original kernel matrix.

torch.Size([9769, 2])
We keep 4.96e+05/1.22e+07 =  4% of the original kernel matrix.

torch.Size([4762, 2])
We keep 1.87e+05/3.78e+06 =  4% of the original kernel matrix.

torch.Size([15139, 2])
We keep 1.21e+06/4.65e+07 =  2% of the original kernel matrix.

torch.Size([1140, 2])
We keep 2.12e+04/2.05e+05 = 10% of the original kernel matrix.

torch.Size([8529, 2])
We keep 4.72e+05/1.08e+07 =  4% of the original kernel matrix.

torch.Size([10792, 2])
We keep 1.24e+06/2.77e+07 =  4% of the original kernel matrix.

torch.Size([21328, 2])
We keep 2.49e+06/1.26e+08 =  1% of the original kernel matrix.

torch.Size([28891, 2])
We keep 2.94e+07/8.82e+08 =  3% of the original kernel matrix.

torch.Size([32734, 2])
We keep 9.49e+06/7.11e+08 =  1% of the original kernel matrix.

torch.Size([5252, 2])
We keep 2.62e+05/5.26e+06 =  4% of the original kernel matrix.

torch.Size([15659, 2])
We keep 1.25e+06/5.49e+07 =  2% of the original kernel matrix.

torch.Size([5542, 2])
We keep 2.86e+05/5.75e+06 =  4% of the original kernel matrix.

torch.Size([15799, 2])
We keep 1.38e+06/5.74e+07 =  2% of the original kernel matrix.

torch.Size([860, 2])
We keep 9.72e+03/8.29e+04 = 11% of the original kernel matrix.

torch.Size([7980, 2])
We keep 3.53e+05/6.89e+06 =  5% of the original kernel matrix.

torch.Size([6048, 2])
We keep 2.91e+05/6.79e+06 =  4% of the original kernel matrix.

torch.Size([16568, 2])
We keep 1.47e+06/6.23e+07 =  2% of the original kernel matrix.

torch.Size([41083, 2])
We keep 1.03e+08/8.63e+08 = 11% of the original kernel matrix.

torch.Size([41149, 2])
We keep 9.62e+06/7.03e+08 =  1% of the original kernel matrix.

torch.Size([93023, 2])
We keep 2.36e+08/7.15e+09 =  3% of the original kernel matrix.

torch.Size([58744, 2])
We keep 2.27e+07/2.02e+09 =  1% of the original kernel matrix.

torch.Size([3364, 2])
We keep 9.02e+04/1.54e+06 =  5% of the original kernel matrix.

torch.Size([13215, 2])
We keep 9.03e+05/2.97e+07 =  3% of the original kernel matrix.

torch.Size([27594, 2])
We keep 1.19e+07/4.67e+08 =  2% of the original kernel matrix.

torch.Size([31789, 2])
We keep 6.82e+06/5.17e+08 =  1% of the original kernel matrix.

torch.Size([34781, 2])
We keep 2.08e+07/6.32e+08 =  3% of the original kernel matrix.

torch.Size([37465, 2])
We keep 7.97e+06/6.02e+08 =  1% of the original kernel matrix.

torch.Size([6888, 2])
We keep 6.19e+05/1.10e+07 =  5% of the original kernel matrix.

torch.Size([17237, 2])
We keep 1.75e+06/7.94e+07 =  2% of the original kernel matrix.

torch.Size([54674, 2])
We keep 2.10e+08/2.21e+09 =  9% of the original kernel matrix.

torch.Size([46786, 2])
We keep 1.27e+07/1.13e+09 =  1% of the original kernel matrix.

torch.Size([1741, 2])
We keep 2.89e+04/3.43e+05 =  8% of the original kernel matrix.

torch.Size([10400, 2])
We keep 5.55e+05/1.40e+07 =  3% of the original kernel matrix.

torch.Size([4254, 2])
We keep 1.94e+05/3.64e+06 =  5% of the original kernel matrix.

torch.Size([14163, 2])
We keep 1.22e+06/4.57e+07 =  2% of the original kernel matrix.

torch.Size([58627, 2])
We keep 3.29e+07/1.91e+09 =  1% of the original kernel matrix.

torch.Size([47414, 2])
We keep 1.29e+07/1.05e+09 =  1% of the original kernel matrix.

torch.Size([242373, 2])
We keep 2.33e+08/2.12e+10 =  1% of the original kernel matrix.

torch.Size([102936, 2])
We keep 3.59e+07/3.49e+09 =  1% of the original kernel matrix.

torch.Size([3039, 2])
We keep 1.43e+05/1.76e+06 =  8% of the original kernel matrix.

torch.Size([12475, 2])
We keep 9.42e+05/3.18e+07 =  2% of the original kernel matrix.

torch.Size([19253, 2])
We keep 3.33e+06/1.09e+08 =  3% of the original kernel matrix.

torch.Size([28258, 2])
We keep 4.10e+06/2.50e+08 =  1% of the original kernel matrix.

torch.Size([11442, 2])
We keep 1.84e+06/4.37e+07 =  4% of the original kernel matrix.

torch.Size([21348, 2])
We keep 2.91e+06/1.58e+08 =  1% of the original kernel matrix.

torch.Size([23890, 2])
We keep 2.54e+07/3.31e+08 =  7% of the original kernel matrix.

torch.Size([30923, 2])
We keep 6.14e+06/4.36e+08 =  1% of the original kernel matrix.

torch.Size([24014, 2])
We keep 1.65e+07/4.70e+08 =  3% of the original kernel matrix.

torch.Size([29296, 2])
We keep 6.91e+06/5.19e+08 =  1% of the original kernel matrix.

torch.Size([20645, 2])
We keep 7.64e+06/2.11e+08 =  3% of the original kernel matrix.

torch.Size([28735, 2])
We keep 5.26e+06/3.47e+08 =  1% of the original kernel matrix.

torch.Size([19751, 2])
We keep 4.25e+06/1.39e+08 =  3% of the original kernel matrix.

torch.Size([28581, 2])
We keep 4.49e+06/2.82e+08 =  1% of the original kernel matrix.

torch.Size([21421, 2])
We keep 4.08e+06/1.38e+08 =  2% of the original kernel matrix.

torch.Size([29864, 2])
We keep 4.51e+06/2.81e+08 =  1% of the original kernel matrix.

torch.Size([1945, 2])
We keep 2.24e+05/9.62e+05 = 23% of the original kernel matrix.

torch.Size([10230, 2])
We keep 7.24e+05/2.35e+07 =  3% of the original kernel matrix.

torch.Size([83822, 2])
We keep 1.16e+08/5.72e+09 =  2% of the original kernel matrix.

torch.Size([55602, 2])
We keep 2.04e+07/1.81e+09 =  1% of the original kernel matrix.

torch.Size([2912, 2])
We keep 7.59e+04/1.30e+06 =  5% of the original kernel matrix.

torch.Size([12487, 2])
We keep 8.40e+05/2.73e+07 =  3% of the original kernel matrix.

torch.Size([2480, 2])
We keep 5.90e+04/9.02e+05 =  6% of the original kernel matrix.

torch.Size([11651, 2])
We keep 7.40e+05/2.27e+07 =  3% of the original kernel matrix.

torch.Size([16814, 2])
We keep 2.60e+06/8.67e+07 =  2% of the original kernel matrix.

torch.Size([26306, 2])
We keep 3.74e+06/2.23e+08 =  1% of the original kernel matrix.

torch.Size([27405, 2])
We keep 1.52e+07/5.75e+08 =  2% of the original kernel matrix.

torch.Size([32108, 2])
We keep 8.10e+06/5.74e+08 =  1% of the original kernel matrix.

torch.Size([139260, 2])
We keep 1.09e+08/8.44e+09 =  1% of the original kernel matrix.

torch.Size([76071, 2])
We keep 2.40e+07/2.20e+09 =  1% of the original kernel matrix.

torch.Size([70346, 2])
We keep 2.19e+07/1.73e+09 =  1% of the original kernel matrix.

torch.Size([54234, 2])
We keep 1.23e+07/9.95e+08 =  1% of the original kernel matrix.

torch.Size([1495, 2])
We keep 2.22e+04/2.60e+05 =  8% of the original kernel matrix.

torch.Size([9677, 2])
We keep 4.96e+05/1.22e+07 =  4% of the original kernel matrix.

torch.Size([9373, 2])
We keep 1.13e+06/2.05e+07 =  5% of the original kernel matrix.

torch.Size([19824, 2])
We keep 2.22e+06/1.08e+08 =  2% of the original kernel matrix.

torch.Size([9625, 2])
We keep 8.64e+05/2.56e+07 =  3% of the original kernel matrix.

torch.Size([19769, 2])
We keep 2.40e+06/1.21e+08 =  1% of the original kernel matrix.

torch.Size([21187, 2])
We keep 7.54e+06/2.38e+08 =  3% of the original kernel matrix.

torch.Size([29188, 2])
We keep 5.53e+06/3.69e+08 =  1% of the original kernel matrix.

torch.Size([61244, 2])
We keep 2.91e+07/1.42e+09 =  2% of the original kernel matrix.

torch.Size([50623, 2])
We keep 1.12e+07/9.02e+08 =  1% of the original kernel matrix.

torch.Size([7157, 2])
We keep 2.85e+06/1.32e+07 = 21% of the original kernel matrix.

torch.Size([17556, 2])
We keep 1.59e+06/8.70e+07 =  1% of the original kernel matrix.

torch.Size([15376, 2])
We keep 2.19e+06/6.79e+07 =  3% of the original kernel matrix.

torch.Size([25007, 2])
We keep 3.45e+06/1.97e+08 =  1% of the original kernel matrix.

torch.Size([944, 2])
We keep 1.14e+04/9.24e+04 = 12% of the original kernel matrix.

torch.Size([8251, 2])
We keep 3.63e+05/7.28e+06 =  4% of the original kernel matrix.

torch.Size([5127, 2])
We keep 1.99e+05/4.49e+06 =  4% of the original kernel matrix.

torch.Size([15396, 2])
We keep 1.27e+06/5.07e+07 =  2% of the original kernel matrix.

torch.Size([31859, 2])
We keep 1.49e+07/6.88e+08 =  2% of the original kernel matrix.

torch.Size([35100, 2])
We keep 8.64e+06/6.28e+08 =  1% of the original kernel matrix.

torch.Size([25204, 2])
We keep 9.62e+06/3.74e+08 =  2% of the original kernel matrix.

torch.Size([31932, 2])
We keep 6.65e+06/4.63e+08 =  1% of the original kernel matrix.

torch.Size([22078, 2])
We keep 5.65e+06/2.46e+08 =  2% of the original kernel matrix.

torch.Size([30162, 2])
We keep 5.74e+06/3.76e+08 =  1% of the original kernel matrix.

torch.Size([5332, 2])
We keep 3.16e+05/7.03e+06 =  4% of the original kernel matrix.

torch.Size([15229, 2])
We keep 1.52e+06/6.34e+07 =  2% of the original kernel matrix.

torch.Size([225749, 2])
We keep 2.22e+08/2.80e+10 =  0% of the original kernel matrix.

torch.Size([98638, 2])
We keep 3.97e+07/4.00e+09 =  0% of the original kernel matrix.

torch.Size([8852, 2])
We keep 4.82e+05/1.41e+07 =  3% of the original kernel matrix.

torch.Size([19414, 2])
We keep 1.88e+06/8.98e+07 =  2% of the original kernel matrix.

torch.Size([23395, 2])
We keep 3.16e+06/1.60e+08 =  1% of the original kernel matrix.

torch.Size([31243, 2])
We keep 4.70e+06/3.03e+08 =  1% of the original kernel matrix.

torch.Size([74941, 2])
We keep 4.26e+07/2.75e+09 =  1% of the original kernel matrix.

torch.Size([54637, 2])
We keep 1.51e+07/1.26e+09 =  1% of the original kernel matrix.

torch.Size([3393, 2])
We keep 8.93e+04/1.66e+06 =  5% of the original kernel matrix.

torch.Size([13162, 2])
We keep 9.10e+05/3.08e+07 =  2% of the original kernel matrix.

torch.Size([36482, 2])
We keep 1.35e+07/6.40e+08 =  2% of the original kernel matrix.

torch.Size([39061, 2])
We keep 8.37e+06/6.06e+08 =  1% of the original kernel matrix.

torch.Size([169197, 2])
We keep 2.60e+08/1.45e+10 =  1% of the original kernel matrix.

torch.Size([83654, 2])
We keep 3.09e+07/2.88e+09 =  1% of the original kernel matrix.

torch.Size([110909, 2])
We keep 9.26e+07/6.05e+09 =  1% of the original kernel matrix.

torch.Size([66832, 2])
We keep 2.10e+07/1.86e+09 =  1% of the original kernel matrix.

torch.Size([6843, 2])
We keep 5.57e+05/1.36e+07 =  4% of the original kernel matrix.

torch.Size([16885, 2])
We keep 1.88e+06/8.82e+07 =  2% of the original kernel matrix.

torch.Size([4077, 2])
We keep 2.07e+05/3.58e+06 =  5% of the original kernel matrix.

torch.Size([13766, 2])
We keep 1.20e+06/4.53e+07 =  2% of the original kernel matrix.

torch.Size([2351, 2])
We keep 9.78e+04/1.16e+06 =  8% of the original kernel matrix.

torch.Size([10770, 2])
We keep 8.18e+05/2.57e+07 =  3% of the original kernel matrix.

torch.Size([4694, 2])
We keep 1.59e+05/3.32e+06 =  4% of the original kernel matrix.

torch.Size([15003, 2])
We keep 1.16e+06/4.36e+07 =  2% of the original kernel matrix.

torch.Size([20807, 2])
We keep 8.77e+06/2.90e+08 =  3% of the original kernel matrix.

torch.Size([28393, 2])
We keep 5.90e+06/4.07e+08 =  1% of the original kernel matrix.

torch.Size([10904, 2])
We keep 6.82e+05/2.19e+07 =  3% of the original kernel matrix.

torch.Size([21382, 2])
We keep 2.23e+06/1.12e+08 =  1% of the original kernel matrix.

torch.Size([26272, 2])
We keep 7.61e+06/3.26e+08 =  2% of the original kernel matrix.

torch.Size([32943, 2])
We keep 6.27e+06/4.32e+08 =  1% of the original kernel matrix.

torch.Size([51117, 2])
We keep 1.43e+08/2.34e+09 =  6% of the original kernel matrix.

torch.Size([45677, 2])
We keep 1.23e+07/1.16e+09 =  1% of the original kernel matrix.

torch.Size([14948, 2])
We keep 2.38e+06/6.37e+07 =  3% of the original kernel matrix.

torch.Size([24945, 2])
We keep 3.17e+06/1.91e+08 =  1% of the original kernel matrix.

torch.Size([5742, 2])
We keep 1.35e+06/1.76e+07 =  7% of the original kernel matrix.

torch.Size([14993, 2])
We keep 2.11e+06/1.01e+08 =  2% of the original kernel matrix.

torch.Size([7102, 2])
We keep 3.00e+05/7.75e+06 =  3% of the original kernel matrix.

torch.Size([17793, 2])
We keep 1.53e+06/6.66e+07 =  2% of the original kernel matrix.

torch.Size([35648, 2])
We keep 1.15e+08/3.56e+09 =  3% of the original kernel matrix.

torch.Size([32945, 2])
We keep 1.66e+07/1.43e+09 =  1% of the original kernel matrix.

torch.Size([14059, 2])
We keep 1.37e+07/1.15e+08 = 11% of the original kernel matrix.

torch.Size([23664, 2])
We keep 4.14e+06/2.56e+08 =  1% of the original kernel matrix.

torch.Size([7381, 2])
We keep 1.65e+06/1.96e+07 =  8% of the original kernel matrix.

torch.Size([17678, 2])
We keep 2.19e+06/1.06e+08 =  2% of the original kernel matrix.

torch.Size([12750, 2])
We keep 1.24e+06/4.10e+07 =  3% of the original kernel matrix.

torch.Size([22826, 2])
We keep 2.84e+06/1.53e+08 =  1% of the original kernel matrix.

torch.Size([219409, 2])
We keep 6.27e+08/3.26e+10 =  1% of the original kernel matrix.

torch.Size([95397, 2])
We keep 4.27e+07/4.32e+09 =  0% of the original kernel matrix.

torch.Size([7414, 2])
We keep 6.95e+05/1.47e+07 =  4% of the original kernel matrix.

torch.Size([17569, 2])
We keep 1.97e+06/9.18e+07 =  2% of the original kernel matrix.

torch.Size([73117, 2])
We keep 8.05e+07/3.64e+09 =  2% of the original kernel matrix.

torch.Size([50802, 2])
We keep 1.65e+07/1.44e+09 =  1% of the original kernel matrix.

torch.Size([3275, 2])
We keep 1.11e+05/2.00e+06 =  5% of the original kernel matrix.

torch.Size([12896, 2])
We keep 9.60e+05/3.38e+07 =  2% of the original kernel matrix.

torch.Size([19555, 2])
We keep 1.14e+07/3.57e+08 =  3% of the original kernel matrix.

torch.Size([26234, 2])
We keep 6.70e+06/4.52e+08 =  1% of the original kernel matrix.

torch.Size([13017, 2])
We keep 1.98e+06/4.08e+07 =  4% of the original kernel matrix.

torch.Size([23384, 2])
We keep 2.76e+06/1.53e+08 =  1% of the original kernel matrix.

torch.Size([117239, 2])
We keep 2.57e+08/1.06e+10 =  2% of the original kernel matrix.

torch.Size([66420, 2])
We keep 2.74e+07/2.46e+09 =  1% of the original kernel matrix.

torch.Size([29072, 2])
We keep 5.21e+06/3.28e+08 =  1% of the original kernel matrix.

torch.Size([34571, 2])
We keep 6.19e+06/4.33e+08 =  1% of the original kernel matrix.

torch.Size([3033, 2])
We keep 8.52e+04/1.43e+06 =  5% of the original kernel matrix.

torch.Size([12698, 2])
We keep 8.67e+05/2.86e+07 =  3% of the original kernel matrix.

torch.Size([108755, 2])
We keep 4.08e+07/3.98e+09 =  1% of the original kernel matrix.

torch.Size([66890, 2])
We keep 1.74e+07/1.51e+09 =  1% of the original kernel matrix.

torch.Size([101459, 2])
We keep 4.05e+07/3.39e+09 =  1% of the original kernel matrix.

torch.Size([64290, 2])
We keep 1.62e+07/1.39e+09 =  1% of the original kernel matrix.

torch.Size([4123, 2])
We keep 3.36e+05/4.04e+06 =  8% of the original kernel matrix.

torch.Size([13897, 2])
We keep 1.20e+06/4.81e+07 =  2% of the original kernel matrix.

torch.Size([33876, 2])
We keep 1.37e+07/4.79e+08 =  2% of the original kernel matrix.

torch.Size([35812, 2])
We keep 6.72e+06/5.24e+08 =  1% of the original kernel matrix.

torch.Size([77396, 2])
We keep 2.65e+07/2.18e+09 =  1% of the original kernel matrix.

torch.Size([56672, 2])
We keep 1.34e+07/1.12e+09 =  1% of the original kernel matrix.

torch.Size([307606, 2])
We keep 3.58e+08/3.97e+10 =  0% of the original kernel matrix.

torch.Size([117708, 2])
We keep 4.65e+07/4.77e+09 =  0% of the original kernel matrix.

torch.Size([13783, 2])
We keep 4.12e+06/7.26e+07 =  5% of the original kernel matrix.

torch.Size([23784, 2])
We keep 3.50e+06/2.04e+08 =  1% of the original kernel matrix.

torch.Size([1771, 2])
We keep 6.32e+04/5.75e+05 = 11% of the original kernel matrix.

torch.Size([10076, 2])
We keep 6.22e+05/1.81e+07 =  3% of the original kernel matrix.

torch.Size([4577, 2])
We keep 1.59e+05/3.10e+06 =  5% of the original kernel matrix.

torch.Size([14909, 2])
We keep 1.14e+06/4.22e+07 =  2% of the original kernel matrix.

torch.Size([5149, 2])
We keep 1.36e+06/9.80e+06 = 13% of the original kernel matrix.

torch.Size([15029, 2])
We keep 1.61e+06/7.49e+07 =  2% of the original kernel matrix.

torch.Size([14559, 2])
We keep 1.47e+06/5.12e+07 =  2% of the original kernel matrix.

torch.Size([24360, 2])
We keep 3.08e+06/1.71e+08 =  1% of the original kernel matrix.

torch.Size([8307, 2])
We keep 1.18e+06/1.91e+07 =  6% of the original kernel matrix.

torch.Size([18643, 2])
We keep 2.14e+06/1.05e+08 =  2% of the original kernel matrix.

torch.Size([3057, 2])
We keep 1.08e+05/1.76e+06 =  6% of the original kernel matrix.

torch.Size([12519, 2])
We keep 9.51e+05/3.18e+07 =  2% of the original kernel matrix.

torch.Size([71968, 2])
We keep 3.42e+07/1.88e+09 =  1% of the original kernel matrix.

torch.Size([54704, 2])
We keep 1.28e+07/1.04e+09 =  1% of the original kernel matrix.

torch.Size([84814, 2])
We keep 3.67e+07/2.64e+09 =  1% of the original kernel matrix.

torch.Size([58537, 2])
We keep 1.48e+07/1.23e+09 =  1% of the original kernel matrix.

torch.Size([1390, 2])
We keep 5.62e+04/4.60e+05 = 12% of the original kernel matrix.

torch.Size([8882, 2])
We keep 6.01e+05/1.62e+07 =  3% of the original kernel matrix.

torch.Size([3319, 2])
We keep 1.41e+05/2.20e+06 =  6% of the original kernel matrix.

torch.Size([12794, 2])
We keep 1.02e+06/3.55e+07 =  2% of the original kernel matrix.

torch.Size([10050, 2])
We keep 8.72e+05/2.65e+07 =  3% of the original kernel matrix.

torch.Size([20129, 2])
We keep 2.42e+06/1.23e+08 =  1% of the original kernel matrix.

torch.Size([15004, 2])
We keep 1.51e+06/5.55e+07 =  2% of the original kernel matrix.

torch.Size([24703, 2])
We keep 3.19e+06/1.78e+08 =  1% of the original kernel matrix.

torch.Size([9081, 2])
We keep 6.32e+05/1.64e+07 =  3% of the original kernel matrix.

torch.Size([19448, 2])
We keep 2.03e+06/9.70e+07 =  2% of the original kernel matrix.

torch.Size([11095, 2])
We keep 8.46e+05/2.54e+07 =  3% of the original kernel matrix.

torch.Size([21456, 2])
We keep 2.38e+06/1.21e+08 =  1% of the original kernel matrix.

torch.Size([3268, 2])
We keep 8.38e+04/1.52e+06 =  5% of the original kernel matrix.

torch.Size([13028, 2])
We keep 8.85e+05/2.95e+07 =  2% of the original kernel matrix.

torch.Size([10856, 2])
We keep 2.10e+06/3.85e+07 =  5% of the original kernel matrix.

torch.Size([21084, 2])
We keep 2.78e+06/1.49e+08 =  1% of the original kernel matrix.

torch.Size([24025, 2])
We keep 3.11e+06/1.61e+08 =  1% of the original kernel matrix.

torch.Size([31799, 2])
We keep 4.71e+06/3.04e+08 =  1% of the original kernel matrix.

torch.Size([13633, 2])
We keep 1.30e+06/4.30e+07 =  3% of the original kernel matrix.

torch.Size([23717, 2])
We keep 2.84e+06/1.57e+08 =  1% of the original kernel matrix.

torch.Size([28321, 2])
We keep 1.32e+07/3.96e+08 =  3% of the original kernel matrix.

torch.Size([34357, 2])
We keep 6.80e+06/4.76e+08 =  1% of the original kernel matrix.

torch.Size([6183, 2])
We keep 4.68e+05/1.03e+07 =  4% of the original kernel matrix.

torch.Size([16149, 2])
We keep 1.73e+06/7.69e+07 =  2% of the original kernel matrix.

torch.Size([250949, 2])
We keep 1.52e+09/2.81e+10 =  5% of the original kernel matrix.

torch.Size([104451, 2])
We keep 4.09e+07/4.01e+09 =  1% of the original kernel matrix.

torch.Size([5555, 2])
We keep 2.75e+05/6.22e+06 =  4% of the original kernel matrix.

torch.Size([15884, 2])
We keep 1.44e+06/5.97e+07 =  2% of the original kernel matrix.

torch.Size([44982, 2])
We keep 1.21e+07/7.58e+08 =  1% of the original kernel matrix.

torch.Size([43533, 2])
We keep 8.78e+06/6.59e+08 =  1% of the original kernel matrix.

torch.Size([1147, 2])
We keep 1.41e+04/1.46e+05 =  9% of the original kernel matrix.

torch.Size([8817, 2])
We keep 4.05e+05/9.14e+06 =  4% of the original kernel matrix.

torch.Size([3040, 2])
We keep 9.15e+04/1.63e+06 =  5% of the original kernel matrix.

torch.Size([12343, 2])
We keep 9.02e+05/3.05e+07 =  2% of the original kernel matrix.

torch.Size([7027, 2])
We keep 3.33e+05/8.18e+06 =  4% of the original kernel matrix.

torch.Size([17583, 2])
We keep 1.59e+06/6.84e+07 =  2% of the original kernel matrix.

torch.Size([12341, 2])
We keep 1.77e+06/3.60e+07 =  4% of the original kernel matrix.

torch.Size([22574, 2])
We keep 2.54e+06/1.44e+08 =  1% of the original kernel matrix.

torch.Size([4467, 2])
We keep 1.47e+05/2.93e+06 =  5% of the original kernel matrix.

torch.Size([14790, 2])
We keep 1.13e+06/4.10e+07 =  2% of the original kernel matrix.

torch.Size([55571, 2])
We keep 5.79e+07/1.80e+09 =  3% of the original kernel matrix.

torch.Size([46607, 2])
We keep 1.29e+07/1.01e+09 =  1% of the original kernel matrix.

torch.Size([4443, 2])
We keep 1.68e+05/3.27e+06 =  5% of the original kernel matrix.

torch.Size([14623, 2])
We keep 1.16e+06/4.33e+07 =  2% of the original kernel matrix.

torch.Size([5749, 2])
We keep 2.41e+05/5.54e+06 =  4% of the original kernel matrix.

torch.Size([16174, 2])
We keep 1.36e+06/5.63e+07 =  2% of the original kernel matrix.

torch.Size([255070, 2])
We keep 2.72e+08/2.71e+10 =  1% of the original kernel matrix.

torch.Size([105409, 2])
We keep 4.00e+07/3.94e+09 =  1% of the original kernel matrix.

torch.Size([38402, 2])
We keep 1.53e+07/7.06e+08 =  2% of the original kernel matrix.

torch.Size([39820, 2])
We keep 8.58e+06/6.36e+08 =  1% of the original kernel matrix.

torch.Size([32403, 2])
We keep 1.35e+07/5.34e+08 =  2% of the original kernel matrix.

torch.Size([36169, 2])
We keep 7.60e+06/5.53e+08 =  1% of the original kernel matrix.

torch.Size([123046, 2])
We keep 1.09e+08/6.71e+09 =  1% of the original kernel matrix.

torch.Size([71050, 2])
We keep 2.19e+07/1.96e+09 =  1% of the original kernel matrix.

torch.Size([124913, 2])
We keep 1.17e+08/8.64e+09 =  1% of the original kernel matrix.

torch.Size([67972, 2])
We keep 2.46e+07/2.22e+09 =  1% of the original kernel matrix.

torch.Size([6039, 2])
We keep 2.55e+05/6.04e+06 =  4% of the original kernel matrix.

torch.Size([16471, 2])
We keep 1.43e+06/5.88e+07 =  2% of the original kernel matrix.

torch.Size([3530, 2])
We keep 9.93e+04/1.67e+06 =  5% of the original kernel matrix.

torch.Size([13452, 2])
We keep 9.32e+05/3.09e+07 =  3% of the original kernel matrix.

torch.Size([14492, 2])
We keep 4.28e+06/1.00e+08 =  4% of the original kernel matrix.

torch.Size([23796, 2])
We keep 4.01e+06/2.40e+08 =  1% of the original kernel matrix.

torch.Size([1087, 2])
We keep 1.36e+04/1.35e+05 = 10% of the original kernel matrix.

torch.Size([8785, 2])
We keep 4.12e+05/8.81e+06 =  4% of the original kernel matrix.

torch.Size([3847, 2])
We keep 1.16e+05/2.26e+06 =  5% of the original kernel matrix.

torch.Size([13832, 2])
We keep 1.01e+06/3.59e+07 =  2% of the original kernel matrix.

torch.Size([11043, 2])
We keep 1.32e+06/3.56e+07 =  3% of the original kernel matrix.

torch.Size([21355, 2])
We keep 2.68e+06/1.43e+08 =  1% of the original kernel matrix.

torch.Size([2317, 2])
We keep 5.24e+04/7.57e+05 =  6% of the original kernel matrix.

torch.Size([11538, 2])
We keep 7.08e+05/2.08e+07 =  3% of the original kernel matrix.

torch.Size([4196, 2])
We keep 1.59e+05/2.89e+06 =  5% of the original kernel matrix.

torch.Size([14223, 2])
We keep 1.12e+06/4.07e+07 =  2% of the original kernel matrix.

torch.Size([2059, 2])
We keep 4.62e+04/6.23e+05 =  7% of the original kernel matrix.

torch.Size([10783, 2])
We keep 6.71e+05/1.89e+07 =  3% of the original kernel matrix.

torch.Size([5586, 2])
We keep 2.29e+05/5.15e+06 =  4% of the original kernel matrix.

torch.Size([16003, 2])
We keep 1.33e+06/5.43e+07 =  2% of the original kernel matrix.

torch.Size([99429, 2])
We keep 8.35e+07/5.20e+09 =  1% of the original kernel matrix.

torch.Size([63186, 2])
We keep 1.95e+07/1.73e+09 =  1% of the original kernel matrix.

torch.Size([8390, 2])
We keep 8.91e+05/2.18e+07 =  4% of the original kernel matrix.

torch.Size([18288, 2])
We keep 2.29e+06/1.12e+08 =  2% of the original kernel matrix.

torch.Size([38833, 2])
We keep 1.69e+07/7.79e+08 =  2% of the original kernel matrix.

torch.Size([39702, 2])
We keep 8.86e+06/6.68e+08 =  1% of the original kernel matrix.

torch.Size([3217, 2])
We keep 1.87e+05/2.48e+06 =  7% of the original kernel matrix.

torch.Size([12498, 2])
We keep 9.90e+05/3.77e+07 =  2% of the original kernel matrix.

torch.Size([3055, 2])
We keep 1.22e+05/1.66e+06 =  7% of the original kernel matrix.

torch.Size([12530, 2])
We keep 9.38e+05/3.08e+07 =  3% of the original kernel matrix.

torch.Size([6850, 2])
We keep 3.58e+05/8.71e+06 =  4% of the original kernel matrix.

torch.Size([17347, 2])
We keep 1.63e+06/7.06e+07 =  2% of the original kernel matrix.

torch.Size([461871, 2])
We keep 1.16e+09/9.17e+10 =  1% of the original kernel matrix.

torch.Size([141190, 2])
We keep 6.64e+07/7.25e+09 =  0% of the original kernel matrix.

torch.Size([343894, 2])
We keep 3.11e+08/4.30e+10 =  0% of the original kernel matrix.

torch.Size([124216, 2])
We keep 4.82e+07/4.96e+09 =  0% of the original kernel matrix.

torch.Size([8381, 2])
We keep 6.86e+05/1.56e+07 =  4% of the original kernel matrix.

torch.Size([18968, 2])
We keep 1.97e+06/9.46e+07 =  2% of the original kernel matrix.

torch.Size([19770, 2])
We keep 8.55e+06/1.46e+08 =  5% of the original kernel matrix.

torch.Size([28483, 2])
We keep 4.61e+06/2.89e+08 =  1% of the original kernel matrix.

torch.Size([2102, 2])
We keep 4.47e+04/6.15e+05 =  7% of the original kernel matrix.

torch.Size([10896, 2])
We keep 6.60e+05/1.88e+07 =  3% of the original kernel matrix.

torch.Size([3215, 2])
We keep 4.33e+05/4.43e+06 =  9% of the original kernel matrix.

torch.Size([12166, 2])
We keep 1.28e+06/5.04e+07 =  2% of the original kernel matrix.

torch.Size([27906, 2])
We keep 5.63e+07/7.80e+08 =  7% of the original kernel matrix.

torch.Size([31275, 2])
We keep 8.71e+06/6.68e+08 =  1% of the original kernel matrix.

torch.Size([16756, 2])
We keep 3.26e+06/7.96e+07 =  4% of the original kernel matrix.

torch.Size([26358, 2])
We keep 3.51e+06/2.14e+08 =  1% of the original kernel matrix.

torch.Size([14795, 2])
We keep 1.06e+07/9.36e+07 = 11% of the original kernel matrix.

torch.Size([24507, 2])
We keep 3.96e+06/2.31e+08 =  1% of the original kernel matrix.

torch.Size([49695, 2])
We keep 1.98e+07/1.11e+09 =  1% of the original kernel matrix.

torch.Size([45244, 2])
We keep 1.02e+07/7.96e+08 =  1% of the original kernel matrix.

torch.Size([301016, 2])
We keep 6.01e+08/4.86e+10 =  1% of the original kernel matrix.

torch.Size([113049, 2])
We keep 5.26e+07/5.28e+09 =  0% of the original kernel matrix.

torch.Size([14952, 2])
We keep 3.44e+06/1.00e+08 =  3% of the original kernel matrix.

torch.Size([23813, 2])
We keep 3.92e+06/2.39e+08 =  1% of the original kernel matrix.

torch.Size([3086, 2])
We keep 1.45e+05/1.98e+06 =  7% of the original kernel matrix.

torch.Size([12312, 2])
We keep 9.64e+05/3.36e+07 =  2% of the original kernel matrix.

torch.Size([18871, 2])
We keep 2.35e+06/9.85e+07 =  2% of the original kernel matrix.

torch.Size([27946, 2])
We keep 4.02e+06/2.38e+08 =  1% of the original kernel matrix.

torch.Size([6623, 2])
We keep 3.39e+05/8.50e+06 =  3% of the original kernel matrix.

torch.Size([17133, 2])
We keep 1.61e+06/6.98e+07 =  2% of the original kernel matrix.

torch.Size([2890, 2])
We keep 1.14e+05/1.35e+06 =  8% of the original kernel matrix.

torch.Size([12414, 2])
We keep 8.68e+05/2.78e+07 =  3% of the original kernel matrix.

torch.Size([69368, 2])
We keep 5.08e+07/2.16e+09 =  2% of the original kernel matrix.

torch.Size([53288, 2])
We keep 1.32e+07/1.11e+09 =  1% of the original kernel matrix.

torch.Size([11782, 2])
We keep 3.68e+06/6.42e+07 =  5% of the original kernel matrix.

torch.Size([21586, 2])
We keep 3.46e+06/1.92e+08 =  1% of the original kernel matrix.

torch.Size([1132, 2])
We keep 1.33e+04/1.22e+05 = 10% of the original kernel matrix.

torch.Size([8817, 2])
We keep 4.04e+05/8.38e+06 =  4% of the original kernel matrix.

torch.Size([26880, 2])
We keep 4.85e+06/2.50e+08 =  1% of the original kernel matrix.

torch.Size([33969, 2])
We keep 5.61e+06/3.78e+08 =  1% of the original kernel matrix.

torch.Size([11111, 2])
We keep 9.03e+05/2.67e+07 =  3% of the original kernel matrix.

torch.Size([21486, 2])
We keep 2.42e+06/1.24e+08 =  1% of the original kernel matrix.

torch.Size([1319, 2])
We keep 1.93e+04/1.99e+05 =  9% of the original kernel matrix.

torch.Size([9240, 2])
We keep 4.60e+05/1.07e+07 =  4% of the original kernel matrix.

torch.Size([57107, 2])
We keep 3.54e+07/1.50e+09 =  2% of the original kernel matrix.

torch.Size([48156, 2])
We keep 1.17e+07/9.27e+08 =  1% of the original kernel matrix.

torch.Size([135176, 2])
We keep 7.90e+07/6.82e+09 =  1% of the original kernel matrix.

torch.Size([74766, 2])
We keep 2.19e+07/1.98e+09 =  1% of the original kernel matrix.

torch.Size([61150, 2])
We keep 5.26e+07/1.75e+09 =  3% of the original kernel matrix.

torch.Size([50132, 2])
We keep 1.27e+07/1.00e+09 =  1% of the original kernel matrix.

torch.Size([84514, 2])
We keep 7.74e+07/3.17e+09 =  2% of the original kernel matrix.

torch.Size([57769, 2])
We keep 1.63e+07/1.35e+09 =  1% of the original kernel matrix.

torch.Size([114796, 2])
We keep 1.30e+08/6.78e+09 =  1% of the original kernel matrix.

torch.Size([67471, 2])
We keep 2.22e+07/1.97e+09 =  1% of the original kernel matrix.

torch.Size([7842, 2])
We keep 1.50e+06/2.79e+07 =  5% of the original kernel matrix.

torch.Size([17657, 2])
We keep 2.53e+06/1.26e+08 =  2% of the original kernel matrix.

torch.Size([6774, 2])
We keep 6.77e+05/1.16e+07 =  5% of the original kernel matrix.

torch.Size([17122, 2])
We keep 1.74e+06/8.17e+07 =  2% of the original kernel matrix.

torch.Size([56030, 2])
We keep 1.62e+07/1.13e+09 =  1% of the original kernel matrix.

torch.Size([48929, 2])
We keep 1.03e+07/8.06e+08 =  1% of the original kernel matrix.

torch.Size([74640, 2])
We keep 2.23e+07/1.86e+09 =  1% of the original kernel matrix.

torch.Size([55766, 2])
We keep 1.25e+07/1.03e+09 =  1% of the original kernel matrix.

torch.Size([4068, 2])
We keep 1.47e+05/2.78e+06 =  5% of the original kernel matrix.

torch.Size([14155, 2])
We keep 1.11e+06/3.99e+07 =  2% of the original kernel matrix.

torch.Size([24416, 2])
We keep 1.06e+07/2.49e+08 =  4% of the original kernel matrix.

torch.Size([31808, 2])
We keep 5.10e+06/3.77e+08 =  1% of the original kernel matrix.

torch.Size([176259, 2])
We keep 6.16e+08/1.90e+10 =  3% of the original kernel matrix.

torch.Size([85625, 2])
We keep 3.33e+07/3.30e+09 =  1% of the original kernel matrix.

torch.Size([83153, 2])
We keep 5.78e+07/2.95e+09 =  1% of the original kernel matrix.

torch.Size([58184, 2])
We keep 1.56e+07/1.30e+09 =  1% of the original kernel matrix.

torch.Size([53166, 2])
We keep 4.88e+07/2.07e+09 =  2% of the original kernel matrix.

torch.Size([44421, 2])
We keep 1.31e+07/1.09e+09 =  1% of the original kernel matrix.

torch.Size([33858, 2])
We keep 8.03e+07/1.75e+09 =  4% of the original kernel matrix.

torch.Size([33078, 2])
We keep 1.25e+07/1.00e+09 =  1% of the original kernel matrix.

torch.Size([7783, 2])
We keep 5.65e+05/1.19e+07 =  4% of the original kernel matrix.

torch.Size([18200, 2])
We keep 1.81e+06/8.24e+07 =  2% of the original kernel matrix.

torch.Size([2552, 2])
We keep 8.13e+04/1.26e+06 =  6% of the original kernel matrix.

torch.Size([11527, 2])
We keep 8.45e+05/2.69e+07 =  3% of the original kernel matrix.

torch.Size([4625, 2])
We keep 6.89e+05/6.20e+06 = 11% of the original kernel matrix.

torch.Size([14309, 2])
We keep 1.39e+06/5.96e+07 =  2% of the original kernel matrix.

torch.Size([76260, 2])
We keep 2.31e+07/1.97e+09 =  1% of the original kernel matrix.

torch.Size([56343, 2])
We keep 1.29e+07/1.06e+09 =  1% of the original kernel matrix.

torch.Size([51683, 2])
We keep 1.26e+07/9.31e+08 =  1% of the original kernel matrix.

torch.Size([47027, 2])
We keep 9.43e+06/7.30e+08 =  1% of the original kernel matrix.

torch.Size([396730, 2])
We keep 9.05e+08/8.24e+10 =  1% of the original kernel matrix.

torch.Size([129252, 2])
We keep 6.54e+07/6.87e+09 =  0% of the original kernel matrix.

torch.Size([67037, 2])
We keep 6.82e+07/2.60e+09 =  2% of the original kernel matrix.

torch.Size([51494, 2])
We keep 1.49e+07/1.22e+09 =  1% of the original kernel matrix.

torch.Size([40222, 2])
We keep 1.80e+07/6.49e+08 =  2% of the original kernel matrix.

torch.Size([41018, 2])
We keep 8.02e+06/6.10e+08 =  1% of the original kernel matrix.

torch.Size([17111, 2])
We keep 2.87e+06/1.39e+08 =  2% of the original kernel matrix.

torch.Size([26573, 2])
We keep 4.43e+06/2.82e+08 =  1% of the original kernel matrix.

torch.Size([89442, 2])
We keep 4.69e+07/2.85e+09 =  1% of the original kernel matrix.

torch.Size([60284, 2])
We keep 1.53e+07/1.28e+09 =  1% of the original kernel matrix.

torch.Size([2250, 2])
We keep 4.79e+04/6.69e+05 =  7% of the original kernel matrix.

torch.Size([11168, 2])
We keep 6.78e+05/1.96e+07 =  3% of the original kernel matrix.

torch.Size([5130, 2])
We keep 5.82e+05/7.05e+06 =  8% of the original kernel matrix.

torch.Size([15018, 2])
We keep 1.43e+06/6.35e+07 =  2% of the original kernel matrix.

torch.Size([153814, 2])
We keep 1.11e+08/8.49e+09 =  1% of the original kernel matrix.

torch.Size([80380, 2])
We keep 2.41e+07/2.20e+09 =  1% of the original kernel matrix.

torch.Size([5075, 2])
We keep 2.03e+05/4.44e+06 =  4% of the original kernel matrix.

torch.Size([15336, 2])
We keep 1.27e+06/5.04e+07 =  2% of the original kernel matrix.

torch.Size([3600, 2])
We keep 1.17e+05/2.04e+06 =  5% of the original kernel matrix.

torch.Size([13406, 2])
We keep 9.81e+05/3.42e+07 =  2% of the original kernel matrix.

torch.Size([17661, 2])
We keep 2.40e+06/1.04e+08 =  2% of the original kernel matrix.

torch.Size([26972, 2])
We keep 3.94e+06/2.43e+08 =  1% of the original kernel matrix.

torch.Size([106048, 2])
We keep 2.07e+08/8.50e+09 =  2% of the original kernel matrix.

torch.Size([64691, 2])
We keep 2.44e+07/2.21e+09 =  1% of the original kernel matrix.

torch.Size([13653, 2])
We keep 2.18e+06/5.70e+07 =  3% of the original kernel matrix.

torch.Size([23526, 2])
We keep 3.25e+06/1.81e+08 =  1% of the original kernel matrix.

torch.Size([70618, 2])
We keep 5.43e+07/2.33e+09 =  2% of the original kernel matrix.

torch.Size([53445, 2])
We keep 1.40e+07/1.15e+09 =  1% of the original kernel matrix.

torch.Size([3299, 2])
We keep 2.34e+05/2.18e+06 = 10% of the original kernel matrix.

torch.Size([12764, 2])
We keep 1.01e+06/3.53e+07 =  2% of the original kernel matrix.

torch.Size([4448, 2])
We keep 3.52e+05/4.24e+06 =  8% of the original kernel matrix.

torch.Size([14105, 2])
We keep 1.28e+06/4.93e+07 =  2% of the original kernel matrix.

torch.Size([3108, 2])
We keep 8.94e+04/1.59e+06 =  5% of the original kernel matrix.

torch.Size([12699, 2])
We keep 9.04e+05/3.02e+07 =  2% of the original kernel matrix.

torch.Size([7854, 2])
We keep 4.46e+05/1.15e+07 =  3% of the original kernel matrix.

torch.Size([18372, 2])
We keep 1.76e+06/8.12e+07 =  2% of the original kernel matrix.

torch.Size([34048, 2])
We keep 1.28e+07/4.41e+08 =  2% of the original kernel matrix.

torch.Size([38074, 2])
We keep 7.27e+06/5.03e+08 =  1% of the original kernel matrix.

torch.Size([14776, 2])
We keep 3.29e+06/8.95e+07 =  3% of the original kernel matrix.

torch.Size([24266, 2])
We keep 3.91e+06/2.26e+08 =  1% of the original kernel matrix.

torch.Size([3880, 2])
We keep 3.25e+05/2.73e+06 = 11% of the original kernel matrix.

torch.Size([13711, 2])
We keep 1.10e+06/3.96e+07 =  2% of the original kernel matrix.

torch.Size([83842, 2])
We keep 5.41e+07/2.76e+09 =  1% of the original kernel matrix.

torch.Size([58296, 2])
We keep 1.49e+07/1.26e+09 =  1% of the original kernel matrix.

torch.Size([1595, 2])
We keep 2.89e+04/3.66e+05 =  7% of the original kernel matrix.

torch.Size([9811, 2])
We keep 5.54e+05/1.45e+07 =  3% of the original kernel matrix.

torch.Size([35596, 2])
We keep 1.46e+07/4.63e+08 =  3% of the original kernel matrix.

torch.Size([38852, 2])
We keep 7.29e+06/5.15e+08 =  1% of the original kernel matrix.

torch.Size([5894, 2])
We keep 7.09e+05/1.13e+07 =  6% of the original kernel matrix.

torch.Size([15958, 2])
We keep 1.80e+06/8.06e+07 =  2% of the original kernel matrix.

torch.Size([23335, 2])
We keep 3.46e+06/1.52e+08 =  2% of the original kernel matrix.

torch.Size([31300, 2])
We keep 4.66e+06/2.95e+08 =  1% of the original kernel matrix.

torch.Size([5900, 2])
We keep 3.24e+05/6.68e+06 =  4% of the original kernel matrix.

torch.Size([16416, 2])
We keep 1.50e+06/6.19e+07 =  2% of the original kernel matrix.

torch.Size([7288, 2])
We keep 6.20e+05/1.32e+07 =  4% of the original kernel matrix.

torch.Size([17597, 2])
We keep 1.87e+06/8.71e+07 =  2% of the original kernel matrix.

torch.Size([9575, 2])
We keep 7.32e+05/1.89e+07 =  3% of the original kernel matrix.

torch.Size([20093, 2])
We keep 2.13e+06/1.04e+08 =  2% of the original kernel matrix.

torch.Size([21521, 2])
We keep 3.90e+06/1.50e+08 =  2% of the original kernel matrix.

torch.Size([30087, 2])
We keep 4.70e+06/2.93e+08 =  1% of the original kernel matrix.

torch.Size([80284, 2])
We keep 4.12e+07/2.66e+09 =  1% of the original kernel matrix.

torch.Size([57401, 2])
We keep 1.49e+07/1.23e+09 =  1% of the original kernel matrix.

torch.Size([86930, 2])
We keep 1.04e+08/4.12e+09 =  2% of the original kernel matrix.

torch.Size([58652, 2])
We keep 1.78e+07/1.54e+09 =  1% of the original kernel matrix.

torch.Size([5462, 2])
We keep 2.53e+05/5.47e+06 =  4% of the original kernel matrix.

torch.Size([15857, 2])
We keep 1.37e+06/5.60e+07 =  2% of the original kernel matrix.

torch.Size([4933, 2])
We keep 9.06e+05/1.24e+07 =  7% of the original kernel matrix.

torch.Size([13957, 2])
We keep 1.88e+06/8.44e+07 =  2% of the original kernel matrix.

torch.Size([82110, 2])
We keep 8.72e+07/3.38e+09 =  2% of the original kernel matrix.

torch.Size([56828, 2])
We keep 1.64e+07/1.39e+09 =  1% of the original kernel matrix.

torch.Size([5637, 2])
We keep 3.03e+05/6.19e+06 =  4% of the original kernel matrix.

torch.Size([15988, 2])
We keep 1.45e+06/5.95e+07 =  2% of the original kernel matrix.

torch.Size([571582, 2])
We keep 7.48e+08/1.07e+11 =  0% of the original kernel matrix.

torch.Size([160189, 2])
We keep 7.30e+07/7.85e+09 =  0% of the original kernel matrix.

torch.Size([1172, 2])
We keep 1.97e+04/1.76e+05 = 11% of the original kernel matrix.

torch.Size([8795, 2])
We keep 4.66e+05/1.01e+07 =  4% of the original kernel matrix.

torch.Size([39433, 2])
We keep 1.30e+07/7.42e+08 =  1% of the original kernel matrix.

torch.Size([40525, 2])
We keep 8.74e+06/6.52e+08 =  1% of the original kernel matrix.

torch.Size([85631, 2])
We keep 2.92e+07/2.49e+09 =  1% of the original kernel matrix.

torch.Size([59175, 2])
We keep 1.42e+07/1.20e+09 =  1% of the original kernel matrix.

torch.Size([2617, 2])
We keep 7.55e+04/1.16e+06 =  6% of the original kernel matrix.

torch.Size([11767, 2])
We keep 8.05e+05/2.58e+07 =  3% of the original kernel matrix.

torch.Size([78827, 2])
We keep 8.42e+07/3.92e+09 =  2% of the original kernel matrix.

torch.Size([54870, 2])
We keep 1.74e+07/1.50e+09 =  1% of the original kernel matrix.

torch.Size([11080, 2])
We keep 2.82e+06/8.41e+07 =  3% of the original kernel matrix.

torch.Size([20584, 2])
We keep 3.77e+06/2.19e+08 =  1% of the original kernel matrix.

torch.Size([562435, 2])
We keep 1.75e+09/1.61e+11 =  1% of the original kernel matrix.

torch.Size([154322, 2])
We keep 8.95e+07/9.59e+09 =  0% of the original kernel matrix.

torch.Size([8908, 2])
We keep 5.05e+05/1.41e+07 =  3% of the original kernel matrix.

torch.Size([19439, 2])
We keep 1.93e+06/8.98e+07 =  2% of the original kernel matrix.

torch.Size([7528, 2])
We keep 6.88e+05/1.55e+07 =  4% of the original kernel matrix.

torch.Size([17906, 2])
We keep 2.01e+06/9.43e+07 =  2% of the original kernel matrix.

torch.Size([5131, 2])
We keep 5.69e+05/8.75e+06 =  6% of the original kernel matrix.

torch.Size([14822, 2])
We keep 1.65e+06/7.08e+07 =  2% of the original kernel matrix.

torch.Size([4418, 2])
We keep 1.55e+05/3.19e+06 =  4% of the original kernel matrix.

torch.Size([14634, 2])
We keep 1.15e+06/4.27e+07 =  2% of the original kernel matrix.

torch.Size([7705, 2])
We keep 8.21e+05/1.83e+07 =  4% of the original kernel matrix.

torch.Size([17930, 2])
We keep 2.13e+06/1.02e+08 =  2% of the original kernel matrix.

torch.Size([18602, 2])
We keep 3.59e+06/1.20e+08 =  3% of the original kernel matrix.

torch.Size([27822, 2])
We keep 4.10e+06/2.62e+08 =  1% of the original kernel matrix.

torch.Size([4646, 2])
We keep 1.67e+06/1.31e+07 = 12% of the original kernel matrix.

torch.Size([13965, 2])
We keep 1.94e+06/8.65e+07 =  2% of the original kernel matrix.

torch.Size([24205, 2])
We keep 6.85e+06/2.54e+08 =  2% of the original kernel matrix.

torch.Size([31800, 2])
We keep 5.64e+06/3.82e+08 =  1% of the original kernel matrix.

torch.Size([12423, 2])
We keep 1.43e+06/4.67e+07 =  3% of the original kernel matrix.

torch.Size([22393, 2])
We keep 2.98e+06/1.64e+08 =  1% of the original kernel matrix.

torch.Size([1913, 2])
We keep 3.30e+04/4.37e+05 =  7% of the original kernel matrix.

torch.Size([10573, 2])
We keep 5.94e+05/1.58e+07 =  3% of the original kernel matrix.

torch.Size([9125, 2])
We keep 1.50e+06/3.10e+07 =  4% of the original kernel matrix.

torch.Size([18905, 2])
We keep 2.58e+06/1.33e+08 =  1% of the original kernel matrix.

torch.Size([135127, 2])
We keep 2.07e+08/1.09e+10 =  1% of the original kernel matrix.

torch.Size([73524, 2])
We keep 2.74e+07/2.50e+09 =  1% of the original kernel matrix.

torch.Size([31337, 2])
We keep 1.94e+07/7.28e+08 =  2% of the original kernel matrix.

torch.Size([34689, 2])
We keep 8.78e+06/6.46e+08 =  1% of the original kernel matrix.

torch.Size([169614, 2])
We keep 1.33e+08/9.87e+09 =  1% of the original kernel matrix.

torch.Size([84823, 2])
We keep 2.59e+07/2.38e+09 =  1% of the original kernel matrix.

torch.Size([13911, 2])
We keep 3.32e+06/6.09e+07 =  5% of the original kernel matrix.

torch.Size([23951, 2])
We keep 3.17e+06/1.87e+08 =  1% of the original kernel matrix.

torch.Size([12649, 2])
We keep 1.56e+06/4.81e+07 =  3% of the original kernel matrix.

torch.Size([22782, 2])
We keep 3.01e+06/1.66e+08 =  1% of the original kernel matrix.

torch.Size([1936, 2])
We keep 3.78e+04/5.08e+05 =  7% of the original kernel matrix.

torch.Size([10669, 2])
We keep 6.27e+05/1.71e+07 =  3% of the original kernel matrix.

torch.Size([3963, 2])
We keep 5.14e+05/4.96e+06 = 10% of the original kernel matrix.

torch.Size([13295, 2])
We keep 1.26e+06/5.33e+07 =  2% of the original kernel matrix.

torch.Size([106245, 2])
We keep 1.01e+08/4.16e+09 =  2% of the original kernel matrix.

torch.Size([65899, 2])
We keep 1.70e+07/1.54e+09 =  1% of the original kernel matrix.

torch.Size([13546, 2])
We keep 1.27e+06/4.26e+07 =  2% of the original kernel matrix.

torch.Size([23531, 2])
We keep 2.86e+06/1.56e+08 =  1% of the original kernel matrix.

torch.Size([9398, 2])
We keep 9.32e+05/2.18e+07 =  4% of the original kernel matrix.

torch.Size([19870, 2])
We keep 2.27e+06/1.12e+08 =  2% of the original kernel matrix.

torch.Size([15041, 2])
We keep 3.13e+06/8.86e+07 =  3% of the original kernel matrix.

torch.Size([24524, 2])
We keep 3.83e+06/2.25e+08 =  1% of the original kernel matrix.

torch.Size([20802, 2])
We keep 7.99e+06/2.77e+08 =  2% of the original kernel matrix.

torch.Size([28277, 2])
We keep 5.97e+06/3.98e+08 =  1% of the original kernel matrix.

torch.Size([2756, 2])
We keep 1.12e+05/1.48e+06 =  7% of the original kernel matrix.

torch.Size([11972, 2])
We keep 8.84e+05/2.91e+07 =  3% of the original kernel matrix.

torch.Size([1804, 2])
We keep 4.38e+04/5.45e+05 =  8% of the original kernel matrix.

torch.Size([10260, 2])
We keep 6.44e+05/1.77e+07 =  3% of the original kernel matrix.

torch.Size([259100, 2])
We keep 5.33e+08/4.22e+10 =  1% of the original kernel matrix.

torch.Size([104431, 2])
We keep 4.84e+07/4.92e+09 =  0% of the original kernel matrix.

torch.Size([11875, 2])
We keep 1.72e+06/4.38e+07 =  3% of the original kernel matrix.

torch.Size([22082, 2])
We keep 2.92e+06/1.58e+08 =  1% of the original kernel matrix.

torch.Size([11944, 2])
We keep 1.11e+07/1.26e+08 =  8% of the original kernel matrix.

torch.Size([21653, 2])
We keep 4.43e+06/2.69e+08 =  1% of the original kernel matrix.

torch.Size([7481, 2])
We keep 5.14e+05/1.18e+07 =  4% of the original kernel matrix.

torch.Size([18139, 2])
We keep 1.71e+06/8.23e+07 =  2% of the original kernel matrix.

torch.Size([42643, 2])
We keep 2.05e+07/7.84e+08 =  2% of the original kernel matrix.

torch.Size([42594, 2])
We keep 8.92e+06/6.70e+08 =  1% of the original kernel matrix.

torch.Size([165685, 2])
We keep 2.01e+08/1.19e+10 =  1% of the original kernel matrix.

torch.Size([83415, 2])
We keep 2.82e+07/2.61e+09 =  1% of the original kernel matrix.

torch.Size([129326, 2])
We keep 1.14e+08/7.85e+09 =  1% of the original kernel matrix.

torch.Size([72350, 2])
We keep 2.36e+07/2.12e+09 =  1% of the original kernel matrix.

torch.Size([117953, 2])
We keep 9.43e+07/5.02e+09 =  1% of the original kernel matrix.

torch.Size([69613, 2])
We keep 1.95e+07/1.70e+09 =  1% of the original kernel matrix.

torch.Size([6117, 2])
We keep 1.48e+06/9.99e+06 = 14% of the original kernel matrix.

torch.Size([16490, 2])
We keep 1.71e+06/7.56e+07 =  2% of the original kernel matrix.

torch.Size([37256, 2])
We keep 6.81e+06/4.70e+08 =  1% of the original kernel matrix.

torch.Size([39081, 2])
We keep 6.92e+06/5.19e+08 =  1% of the original kernel matrix.

torch.Size([17320, 2])
We keep 5.80e+06/1.67e+08 =  3% of the original kernel matrix.

torch.Size([26077, 2])
We keep 4.88e+06/3.09e+08 =  1% of the original kernel matrix.

torch.Size([5328, 2])
We keep 2.12e+05/4.75e+06 =  4% of the original kernel matrix.

torch.Size([15798, 2])
We keep 1.30e+06/5.21e+07 =  2% of the original kernel matrix.

torch.Size([13274, 2])
We keep 1.58e+06/4.89e+07 =  3% of the original kernel matrix.

torch.Size([23321, 2])
We keep 3.00e+06/1.67e+08 =  1% of the original kernel matrix.

torch.Size([4815, 2])
We keep 1.75e+05/3.83e+06 =  4% of the original kernel matrix.

torch.Size([15128, 2])
We keep 1.20e+06/4.69e+07 =  2% of the original kernel matrix.

torch.Size([7270, 2])
We keep 2.81e+06/2.96e+07 =  9% of the original kernel matrix.

torch.Size([17516, 2])
We keep 2.57e+06/1.30e+08 =  1% of the original kernel matrix.

torch.Size([7427, 2])
We keep 4.08e+05/1.04e+07 =  3% of the original kernel matrix.

torch.Size([18014, 2])
We keep 1.73e+06/7.72e+07 =  2% of the original kernel matrix.

torch.Size([1331, 2])
We keep 1.88e+04/2.13e+05 =  8% of the original kernel matrix.

torch.Size([9376, 2])
We keep 4.76e+05/1.11e+07 =  4% of the original kernel matrix.

torch.Size([7997, 2])
We keep 5.92e+05/1.54e+07 =  3% of the original kernel matrix.

torch.Size([18359, 2])
We keep 1.98e+06/9.40e+07 =  2% of the original kernel matrix.

torch.Size([5345, 2])
We keep 4.39e+05/9.15e+06 =  4% of the original kernel matrix.

torch.Size([15239, 2])
We keep 1.57e+06/7.24e+07 =  2% of the original kernel matrix.

torch.Size([15279, 2])
We keep 3.02e+07/3.59e+08 =  8% of the original kernel matrix.

torch.Size([23379, 2])
We keep 6.14e+06/4.53e+08 =  1% of the original kernel matrix.

torch.Size([4177, 2])
We keep 2.34e+05/4.37e+06 =  5% of the original kernel matrix.

torch.Size([13856, 2])
We keep 1.28e+06/5.00e+07 =  2% of the original kernel matrix.

torch.Size([28385, 2])
We keep 6.77e+06/2.79e+08 =  2% of the original kernel matrix.

torch.Size([34764, 2])
We keep 5.88e+06/4.00e+08 =  1% of the original kernel matrix.

torch.Size([5574, 2])
We keep 4.61e+06/2.63e+07 = 17% of the original kernel matrix.

torch.Size([14303, 2])
We keep 2.23e+06/1.23e+08 =  1% of the original kernel matrix.

torch.Size([12625, 2])
We keep 1.76e+06/4.74e+07 =  3% of the original kernel matrix.

torch.Size([22746, 2])
We keep 2.97e+06/1.65e+08 =  1% of the original kernel matrix.

torch.Size([2907, 2])
We keep 1.20e+05/1.66e+06 =  7% of the original kernel matrix.

torch.Size([12089, 2])
We keep 9.20e+05/3.08e+07 =  2% of the original kernel matrix.

torch.Size([8112, 2])
We keep 5.67e+05/1.43e+07 =  3% of the original kernel matrix.

torch.Size([18530, 2])
We keep 1.95e+06/9.04e+07 =  2% of the original kernel matrix.

torch.Size([15726, 2])
We keep 1.77e+06/5.84e+07 =  3% of the original kernel matrix.

torch.Size([25318, 2])
We keep 3.15e+06/1.83e+08 =  1% of the original kernel matrix.

torch.Size([125943, 2])
We keep 1.45e+08/6.98e+09 =  2% of the original kernel matrix.

torch.Size([72254, 2])
We keep 2.24e+07/2.00e+09 =  1% of the original kernel matrix.

torch.Size([5959, 2])
We keep 2.98e+05/7.06e+06 =  4% of the original kernel matrix.

torch.Size([16318, 2])
We keep 1.52e+06/6.36e+07 =  2% of the original kernel matrix.

torch.Size([1650, 2])
We keep 3.09e+04/3.77e+05 =  8% of the original kernel matrix.

torch.Size([10245, 2])
We keep 5.67e+05/1.47e+07 =  3% of the original kernel matrix.

torch.Size([4692, 2])
We keep 2.05e+05/4.15e+06 =  4% of the original kernel matrix.

torch.Size([14581, 2])
We keep 1.26e+06/4.87e+07 =  2% of the original kernel matrix.

torch.Size([135151, 2])
We keep 2.60e+08/1.00e+10 =  2% of the original kernel matrix.

torch.Size([74586, 2])
We keep 2.62e+07/2.39e+09 =  1% of the original kernel matrix.

torch.Size([315515, 2])
We keep 6.00e+08/4.09e+10 =  1% of the original kernel matrix.

torch.Size([118981, 2])
We keep 4.70e+07/4.84e+09 =  0% of the original kernel matrix.

torch.Size([10943, 2])
We keep 3.85e+06/7.63e+07 =  5% of the original kernel matrix.

torch.Size([20083, 2])
We keep 3.59e+06/2.09e+08 =  1% of the original kernel matrix.

torch.Size([7972, 2])
We keep 5.70e+05/1.34e+07 =  4% of the original kernel matrix.

torch.Size([18445, 2])
We keep 1.91e+06/8.77e+07 =  2% of the original kernel matrix.

torch.Size([18564, 2])
We keep 6.57e+06/2.05e+08 =  3% of the original kernel matrix.

torch.Size([27152, 2])
We keep 5.33e+06/3.43e+08 =  1% of the original kernel matrix.

torch.Size([50795, 2])
We keep 1.39e+07/9.97e+08 =  1% of the original kernel matrix.

torch.Size([46799, 2])
We keep 9.90e+06/7.55e+08 =  1% of the original kernel matrix.

torch.Size([3268, 2])
We keep 8.60e+04/1.50e+06 =  5% of the original kernel matrix.

torch.Size([12951, 2])
We keep 8.78e+05/2.93e+07 =  2% of the original kernel matrix.

torch.Size([8044, 2])
We keep 1.04e+06/2.00e+07 =  5% of the original kernel matrix.

torch.Size([18450, 2])
We keep 2.10e+06/1.07e+08 =  1% of the original kernel matrix.

torch.Size([11159, 2])
We keep 7.61e+05/2.48e+07 =  3% of the original kernel matrix.

torch.Size([21368, 2])
We keep 2.33e+06/1.19e+08 =  1% of the original kernel matrix.

torch.Size([31696, 2])
We keep 3.37e+07/9.33e+08 =  3% of the original kernel matrix.

torch.Size([34810, 2])
We keep 9.70e+06/7.31e+08 =  1% of the original kernel matrix.

torch.Size([9134, 2])
We keep 3.85e+06/3.01e+07 = 12% of the original kernel matrix.

torch.Size([19564, 2])
We keep 2.40e+06/1.31e+08 =  1% of the original kernel matrix.

torch.Size([205037, 2])
We keep 1.86e+08/1.61e+10 =  1% of the original kernel matrix.

torch.Size([93889, 2])
We keep 3.20e+07/3.03e+09 =  1% of the original kernel matrix.

torch.Size([5148, 2])
We keep 1.97e+05/4.36e+06 =  4% of the original kernel matrix.

torch.Size([15471, 2])
We keep 1.27e+06/5.00e+07 =  2% of the original kernel matrix.

torch.Size([4666, 2])
We keep 1.72e+05/3.62e+06 =  4% of the original kernel matrix.

torch.Size([15008, 2])
We keep 1.19e+06/4.55e+07 =  2% of the original kernel matrix.

torch.Size([1989, 2])
We keep 3.57e+04/4.64e+05 =  7% of the original kernel matrix.

torch.Size([10810, 2])
We keep 6.09e+05/1.63e+07 =  3% of the original kernel matrix.

torch.Size([13508, 2])
We keep 2.09e+06/5.08e+07 =  4% of the original kernel matrix.

torch.Size([23611, 2])
We keep 3.00e+06/1.71e+08 =  1% of the original kernel matrix.

torch.Size([7335, 2])
We keep 3.77e+05/9.37e+06 =  4% of the original kernel matrix.

torch.Size([17975, 2])
We keep 1.63e+06/7.33e+07 =  2% of the original kernel matrix.

torch.Size([20577, 2])
We keep 4.34e+06/1.78e+08 =  2% of the original kernel matrix.

torch.Size([28921, 2])
We keep 4.91e+06/3.19e+08 =  1% of the original kernel matrix.

torch.Size([166078, 2])
We keep 3.20e+08/1.32e+10 =  2% of the original kernel matrix.

torch.Size([83604, 2])
We keep 2.97e+07/2.75e+09 =  1% of the original kernel matrix.

torch.Size([596678, 2])
We keep 8.89e+08/1.12e+11 =  0% of the original kernel matrix.

torch.Size([162759, 2])
We keep 7.52e+07/8.02e+09 =  0% of the original kernel matrix.

torch.Size([5727, 2])
We keep 3.16e+05/6.84e+06 =  4% of the original kernel matrix.

torch.Size([15917, 2])
We keep 1.49e+06/6.26e+07 =  2% of the original kernel matrix.

torch.Size([3361, 2])
We keep 8.85e+04/1.56e+06 =  5% of the original kernel matrix.

torch.Size([13109, 2])
We keep 8.98e+05/2.98e+07 =  3% of the original kernel matrix.

torch.Size([16618, 2])
We keep 1.64e+06/6.44e+07 =  2% of the original kernel matrix.

torch.Size([26058, 2])
We keep 3.35e+06/1.92e+08 =  1% of the original kernel matrix.

torch.Size([287484, 2])
We keep 3.00e+09/6.78e+10 =  4% of the original kernel matrix.

torch.Size([112107, 2])
We keep 5.82e+07/6.23e+09 =  0% of the original kernel matrix.

torch.Size([22861, 2])
We keep 6.03e+06/1.65e+08 =  3% of the original kernel matrix.

torch.Size([30872, 2])
We keep 4.64e+06/3.07e+08 =  1% of the original kernel matrix.

torch.Size([2083, 2])
We keep 5.67e+04/5.90e+05 =  9% of the original kernel matrix.

torch.Size([10865, 2])
We keep 6.60e+05/1.84e+07 =  3% of the original kernel matrix.

torch.Size([4251, 2])
We keep 2.35e+05/3.92e+06 =  5% of the original kernel matrix.

torch.Size([13809, 2])
We keep 1.24e+06/4.74e+07 =  2% of the original kernel matrix.

torch.Size([32872, 2])
We keep 3.01e+07/4.00e+08 =  7% of the original kernel matrix.

torch.Size([37442, 2])
We keep 6.96e+06/4.79e+08 =  1% of the original kernel matrix.

torch.Size([1171, 2])
We keep 1.87e+04/1.87e+05 =  9% of the original kernel matrix.

torch.Size([8815, 2])
We keep 4.55e+05/1.04e+07 =  4% of the original kernel matrix.

torch.Size([44176, 2])
We keep 3.77e+07/1.70e+09 =  2% of the original kernel matrix.

torch.Size([39884, 2])
We keep 1.25e+07/9.87e+08 =  1% of the original kernel matrix.

torch.Size([3208, 2])
We keep 9.29e+04/1.61e+06 =  5% of the original kernel matrix.

torch.Size([12854, 2])
We keep 9.09e+05/3.03e+07 =  2% of the original kernel matrix.

torch.Size([17884, 2])
We keep 1.71e+06/7.43e+07 =  2% of the original kernel matrix.

torch.Size([27018, 2])
We keep 3.50e+06/2.06e+08 =  1% of the original kernel matrix.

torch.Size([4013, 2])
We keep 2.40e+05/3.93e+06 =  6% of the original kernel matrix.

torch.Size([13624, 2])
We keep 1.22e+06/4.75e+07 =  2% of the original kernel matrix.

torch.Size([4034, 2])
We keep 1.38e+05/2.73e+06 =  5% of the original kernel matrix.

torch.Size([14024, 2])
We keep 1.10e+06/3.95e+07 =  2% of the original kernel matrix.

torch.Size([1780, 2])
We keep 2.86e+04/3.93e+05 =  7% of the original kernel matrix.

torch.Size([10463, 2])
We keep 5.75e+05/1.50e+07 =  3% of the original kernel matrix.

torch.Size([160667, 2])
We keep 9.96e+07/9.19e+09 =  1% of the original kernel matrix.

torch.Size([82442, 2])
We keep 2.48e+07/2.29e+09 =  1% of the original kernel matrix.

torch.Size([329461, 2])
We keep 4.11e+08/4.04e+10 =  1% of the original kernel matrix.

torch.Size([121916, 2])
We keep 4.77e+07/4.81e+09 =  0% of the original kernel matrix.

torch.Size([87401, 2])
We keep 7.86e+07/3.58e+09 =  2% of the original kernel matrix.

torch.Size([59464, 2])
We keep 1.69e+07/1.43e+09 =  1% of the original kernel matrix.

torch.Size([17251, 2])
We keep 4.96e+06/1.20e+08 =  4% of the original kernel matrix.

torch.Size([26493, 2])
We keep 4.28e+06/2.62e+08 =  1% of the original kernel matrix.

torch.Size([27988, 2])
We keep 4.53e+06/2.46e+08 =  1% of the original kernel matrix.

torch.Size([34390, 2])
We keep 5.56e+06/3.76e+08 =  1% of the original kernel matrix.

torch.Size([4138, 2])
We keep 2.25e+05/3.74e+06 =  6% of the original kernel matrix.

torch.Size([14062, 2])
We keep 1.20e+06/4.63e+07 =  2% of the original kernel matrix.

torch.Size([8543, 2])
We keep 5.88e+05/1.43e+07 =  4% of the original kernel matrix.

torch.Size([18976, 2])
We keep 1.94e+06/9.04e+07 =  2% of the original kernel matrix.

torch.Size([48525, 2])
We keep 1.36e+07/8.62e+08 =  1% of the original kernel matrix.

torch.Size([45747, 2])
We keep 9.33e+06/7.03e+08 =  1% of the original kernel matrix.

torch.Size([36666, 2])
We keep 1.06e+07/5.11e+08 =  2% of the original kernel matrix.

torch.Size([40629, 2])
We keep 7.65e+06/5.41e+08 =  1% of the original kernel matrix.

torch.Size([80290, 2])
We keep 1.20e+08/3.90e+09 =  3% of the original kernel matrix.

torch.Size([56562, 2])
We keep 1.73e+07/1.49e+09 =  1% of the original kernel matrix.

torch.Size([9063, 2])
We keep 1.17e+06/2.48e+07 =  4% of the original kernel matrix.

torch.Size([19231, 2])
We keep 2.36e+06/1.19e+08 =  1% of the original kernel matrix.

torch.Size([494717, 2])
We keep 9.85e+08/1.04e+11 =  0% of the original kernel matrix.

torch.Size([146374, 2])
We keep 7.38e+07/7.72e+09 =  0% of the original kernel matrix.

torch.Size([10749, 2])
We keep 7.79e+05/2.20e+07 =  3% of the original kernel matrix.

torch.Size([21193, 2])
We keep 2.24e+06/1.12e+08 =  1% of the original kernel matrix.

torch.Size([29409, 2])
We keep 7.51e+06/3.61e+08 =  2% of the original kernel matrix.

torch.Size([35574, 2])
We keep 6.67e+06/4.54e+08 =  1% of the original kernel matrix.

torch.Size([68477, 2])
We keep 3.39e+07/2.10e+09 =  1% of the original kernel matrix.

torch.Size([53214, 2])
We keep 1.33e+07/1.10e+09 =  1% of the original kernel matrix.

torch.Size([35072, 2])
We keep 6.48e+06/4.14e+08 =  1% of the original kernel matrix.

torch.Size([38987, 2])
We keep 6.78e+06/4.87e+08 =  1% of the original kernel matrix.

torch.Size([57518, 2])
We keep 1.05e+08/2.22e+09 =  4% of the original kernel matrix.

torch.Size([47974, 2])
We keep 1.35e+07/1.13e+09 =  1% of the original kernel matrix.

torch.Size([143885, 2])
We keep 1.74e+08/1.10e+10 =  1% of the original kernel matrix.

torch.Size([76875, 2])
We keep 2.73e+07/2.51e+09 =  1% of the original kernel matrix.

torch.Size([25193, 2])
We keep 3.34e+06/1.80e+08 =  1% of the original kernel matrix.

torch.Size([32498, 2])
We keep 4.95e+06/3.21e+08 =  1% of the original kernel matrix.

torch.Size([15266, 2])
We keep 2.88e+06/8.10e+07 =  3% of the original kernel matrix.

torch.Size([24861, 2])
We keep 3.61e+06/2.15e+08 =  1% of the original kernel matrix.

torch.Size([4167, 2])
We keep 2.38e+05/3.39e+06 =  7% of the original kernel matrix.

torch.Size([14107, 2])
We keep 1.12e+06/4.41e+07 =  2% of the original kernel matrix.

torch.Size([29304, 2])
We keep 9.72e+06/3.72e+08 =  2% of the original kernel matrix.

torch.Size([35450, 2])
We keep 6.58e+06/4.61e+08 =  1% of the original kernel matrix.

torch.Size([10059, 2])
We keep 1.94e+06/3.28e+07 =  5% of the original kernel matrix.

torch.Size([20710, 2])
We keep 2.59e+06/1.37e+08 =  1% of the original kernel matrix.

torch.Size([105141, 2])
We keep 7.98e+07/3.73e+09 =  2% of the original kernel matrix.

torch.Size([65219, 2])
We keep 1.70e+07/1.46e+09 =  1% of the original kernel matrix.

torch.Size([682019, 2])
We keep 1.39e+09/1.60e+11 =  0% of the original kernel matrix.

torch.Size([174008, 2])
We keep 8.93e+07/9.58e+09 =  0% of the original kernel matrix.

torch.Size([70326, 2])
We keep 9.70e+07/2.56e+09 =  3% of the original kernel matrix.

torch.Size([53381, 2])
We keep 1.49e+07/1.21e+09 =  1% of the original kernel matrix.

torch.Size([123056, 2])
We keep 1.51e+08/9.66e+09 =  1% of the original kernel matrix.

torch.Size([68846, 2])
We keep 2.61e+07/2.35e+09 =  1% of the original kernel matrix.

torch.Size([22129, 2])
We keep 2.20e+07/4.11e+08 =  5% of the original kernel matrix.

torch.Size([29191, 2])
We keep 6.96e+06/4.85e+08 =  1% of the original kernel matrix.

torch.Size([18825, 2])
We keep 8.08e+06/2.19e+08 =  3% of the original kernel matrix.

torch.Size([27096, 2])
We keep 5.41e+06/3.54e+08 =  1% of the original kernel matrix.

torch.Size([37900, 2])
We keep 8.43e+06/5.48e+08 =  1% of the original kernel matrix.

torch.Size([39019, 2])
We keep 7.42e+06/5.60e+08 =  1% of the original kernel matrix.

torch.Size([8940, 2])
We keep 9.32e+05/2.43e+07 =  3% of the original kernel matrix.

torch.Size([19136, 2])
We keep 2.37e+06/1.18e+08 =  2% of the original kernel matrix.

torch.Size([10924, 2])
We keep 1.65e+06/2.77e+07 =  5% of the original kernel matrix.

torch.Size([21476, 2])
We keep 2.41e+06/1.26e+08 =  1% of the original kernel matrix.

torch.Size([132863, 2])
We keep 6.24e+07/5.86e+09 =  1% of the original kernel matrix.

torch.Size([74051, 2])
We keep 2.04e+07/1.83e+09 =  1% of the original kernel matrix.

torch.Size([12280, 2])
We keep 4.55e+06/1.19e+08 =  3% of the original kernel matrix.

torch.Size([21037, 2])
We keep 4.26e+06/2.61e+08 =  1% of the original kernel matrix.

torch.Size([124986, 2])
We keep 2.47e+08/1.19e+10 =  2% of the original kernel matrix.

torch.Size([69682, 2])
We keep 2.79e+07/2.62e+09 =  1% of the original kernel matrix.

torch.Size([122823, 2])
We keep 5.06e+08/1.40e+10 =  3% of the original kernel matrix.

torch.Size([69096, 2])
We keep 2.94e+07/2.83e+09 =  1% of the original kernel matrix.

torch.Size([18849, 2])
We keep 2.47e+06/1.04e+08 =  2% of the original kernel matrix.

torch.Size([27880, 2])
We keep 4.01e+06/2.43e+08 =  1% of the original kernel matrix.

torch.Size([3456, 2])
We keep 9.99e+04/1.72e+06 =  5% of the original kernel matrix.

torch.Size([13412, 2])
We keep 9.42e+05/3.14e+07 =  3% of the original kernel matrix.

torch.Size([43070, 2])
We keep 3.09e+07/7.58e+08 =  4% of the original kernel matrix.

torch.Size([42472, 2])
We keep 8.81e+06/6.59e+08 =  1% of the original kernel matrix.

torch.Size([39849, 2])
We keep 8.72e+06/5.81e+08 =  1% of the original kernel matrix.

torch.Size([41542, 2])
We keep 7.85e+06/5.77e+08 =  1% of the original kernel matrix.

torch.Size([2652, 2])
We keep 7.60e+04/1.17e+06 =  6% of the original kernel matrix.

torch.Size([11878, 2])
We keep 8.15e+05/2.59e+07 =  3% of the original kernel matrix.

torch.Size([277680, 2])
We keep 2.34e+08/2.87e+10 =  0% of the original kernel matrix.

torch.Size([110933, 2])
We keep 4.08e+07/4.05e+09 =  1% of the original kernel matrix.

torch.Size([22734, 2])
We keep 1.04e+07/2.88e+08 =  3% of the original kernel matrix.

torch.Size([30514, 2])
We keep 5.51e+06/4.06e+08 =  1% of the original kernel matrix.

torch.Size([20458, 2])
We keep 4.87e+06/1.39e+08 =  3% of the original kernel matrix.

torch.Size([29083, 2])
We keep 4.59e+06/2.82e+08 =  1% of the original kernel matrix.

torch.Size([8267, 2])
We keep 4.30e+05/1.19e+07 =  3% of the original kernel matrix.

torch.Size([18874, 2])
We keep 1.76e+06/8.24e+07 =  2% of the original kernel matrix.

torch.Size([2733, 2])
We keep 8.18e+04/1.30e+06 =  6% of the original kernel matrix.

torch.Size([11988, 2])
We keep 8.54e+05/2.72e+07 =  3% of the original kernel matrix.

torch.Size([2176, 2])
We keep 4.24e+04/6.24e+05 =  6% of the original kernel matrix.

torch.Size([11160, 2])
We keep 6.60e+05/1.89e+07 =  3% of the original kernel matrix.

torch.Size([2379, 2])
We keep 9.30e+04/1.07e+06 =  8% of the original kernel matrix.

torch.Size([11209, 2])
We keep 7.77e+05/2.47e+07 =  3% of the original kernel matrix.

torch.Size([9586, 2])
We keep 1.28e+06/2.18e+07 =  5% of the original kernel matrix.

torch.Size([20145, 2])
We keep 2.27e+06/1.12e+08 =  2% of the original kernel matrix.

torch.Size([12664, 2])
We keep 1.64e+06/4.58e+07 =  3% of the original kernel matrix.

torch.Size([22670, 2])
We keep 2.85e+06/1.62e+08 =  1% of the original kernel matrix.

torch.Size([3957527, 2])
We keep 3.23e+10/4.78e+12 =  0% of the original kernel matrix.

torch.Size([436037, 2])
We keep 4.30e+08/5.23e+10 =  0% of the original kernel matrix.

torch.Size([7892, 2])
We keep 6.38e+05/1.66e+07 =  3% of the original kernel matrix.

torch.Size([18177, 2])
We keep 2.05e+06/9.74e+07 =  2% of the original kernel matrix.

torch.Size([12757, 2])
We keep 1.05e+06/3.46e+07 =  3% of the original kernel matrix.

torch.Size([22730, 2])
We keep 2.64e+06/1.41e+08 =  1% of the original kernel matrix.

torch.Size([4128, 2])
We keep 2.18e+05/3.78e+06 =  5% of the original kernel matrix.

torch.Size([13816, 2])
We keep 1.21e+06/4.65e+07 =  2% of the original kernel matrix.

torch.Size([8864, 2])
We keep 5.89e+05/1.56e+07 =  3% of the original kernel matrix.

torch.Size([19226, 2])
We keep 2.02e+06/9.46e+07 =  2% of the original kernel matrix.

torch.Size([13962, 2])
We keep 1.50e+06/4.70e+07 =  3% of the original kernel matrix.

torch.Size([23873, 2])
We keep 3.00e+06/1.64e+08 =  1% of the original kernel matrix.

torch.Size([112898, 2])
We keep 2.01e+08/1.03e+10 =  1% of the original kernel matrix.

torch.Size([64235, 2])
We keep 2.66e+07/2.43e+09 =  1% of the original kernel matrix.

torch.Size([9250, 2])
We keep 3.41e+06/2.60e+07 = 13% of the original kernel matrix.

torch.Size([19660, 2])
We keep 2.30e+06/1.22e+08 =  1% of the original kernel matrix.

torch.Size([462488, 2])
We keep 5.59e+08/7.49e+10 =  0% of the original kernel matrix.

torch.Size([143959, 2])
We keep 6.28e+07/6.55e+09 =  0% of the original kernel matrix.

torch.Size([3895, 2])
We keep 1.20e+05/2.29e+06 =  5% of the original kernel matrix.

torch.Size([13899, 2])
We keep 1.03e+06/3.62e+07 =  2% of the original kernel matrix.

torch.Size([200355, 2])
We keep 2.11e+08/1.54e+10 =  1% of the original kernel matrix.

torch.Size([92838, 2])
We keep 3.13e+07/2.97e+09 =  1% of the original kernel matrix.

torch.Size([25094, 2])
We keep 1.92e+07/3.01e+08 =  6% of the original kernel matrix.

torch.Size([32121, 2])
We keep 6.11e+06/4.15e+08 =  1% of the original kernel matrix.

torch.Size([79121, 2])
We keep 5.32e+07/3.12e+09 =  1% of the original kernel matrix.

torch.Size([56046, 2])
We keep 1.60e+07/1.34e+09 =  1% of the original kernel matrix.

torch.Size([249254, 2])
We keep 3.98e+08/3.28e+10 =  1% of the original kernel matrix.

torch.Size([103679, 2])
We keep 4.34e+07/4.34e+09 =  1% of the original kernel matrix.

torch.Size([11695, 2])
We keep 9.90e+05/2.72e+07 =  3% of the original kernel matrix.

torch.Size([21979, 2])
We keep 2.45e+06/1.25e+08 =  1% of the original kernel matrix.

torch.Size([4461, 2])
We keep 2.15e+05/3.69e+06 =  5% of the original kernel matrix.

torch.Size([14620, 2])
We keep 1.22e+06/4.60e+07 =  2% of the original kernel matrix.

torch.Size([49231, 2])
We keep 1.21e+07/8.42e+08 =  1% of the original kernel matrix.

torch.Size([46059, 2])
We keep 9.14e+06/6.95e+08 =  1% of the original kernel matrix.

torch.Size([5356, 2])
We keep 3.08e+05/6.02e+06 =  5% of the original kernel matrix.

torch.Size([15703, 2])
We keep 1.41e+06/5.87e+07 =  2% of the original kernel matrix.

torch.Size([15888, 2])
We keep 3.66e+06/1.02e+08 =  3% of the original kernel matrix.

torch.Size([25124, 2])
We keep 4.03e+06/2.42e+08 =  1% of the original kernel matrix.

torch.Size([18318, 2])
We keep 2.59e+06/9.80e+07 =  2% of the original kernel matrix.

torch.Size([27404, 2])
We keep 3.97e+06/2.37e+08 =  1% of the original kernel matrix.

torch.Size([5996, 2])
We keep 8.31e+05/1.37e+07 =  6% of the original kernel matrix.

torch.Size([16042, 2])
We keep 1.77e+06/8.85e+07 =  1% of the original kernel matrix.

torch.Size([31143, 2])
We keep 9.35e+06/3.81e+08 =  2% of the original kernel matrix.

torch.Size([36698, 2])
We keep 6.78e+06/4.67e+08 =  1% of the original kernel matrix.

torch.Size([13125, 2])
We keep 2.28e+06/4.87e+07 =  4% of the original kernel matrix.

torch.Size([22954, 2])
We keep 3.07e+06/1.67e+08 =  1% of the original kernel matrix.

torch.Size([72245, 2])
We keep 5.18e+07/2.34e+09 =  2% of the original kernel matrix.

torch.Size([54214, 2])
We keep 1.41e+07/1.16e+09 =  1% of the original kernel matrix.

torch.Size([126081, 2])
We keep 1.32e+08/7.75e+09 =  1% of the original kernel matrix.

torch.Size([70902, 2])
We keep 2.34e+07/2.11e+09 =  1% of the original kernel matrix.

torch.Size([10484, 2])
We keep 1.40e+06/2.92e+07 =  4% of the original kernel matrix.

torch.Size([20772, 2])
We keep 2.52e+06/1.29e+08 =  1% of the original kernel matrix.

torch.Size([6845, 2])
We keep 3.98e+05/9.95e+06 =  4% of the original kernel matrix.

torch.Size([17282, 2])
We keep 1.71e+06/7.55e+07 =  2% of the original kernel matrix.

torch.Size([48423, 2])
We keep 5.29e+07/1.75e+09 =  3% of the original kernel matrix.

torch.Size([43343, 2])
We keep 1.19e+07/1.00e+09 =  1% of the original kernel matrix.

torch.Size([20208, 2])
We keep 5.04e+06/1.90e+08 =  2% of the original kernel matrix.

torch.Size([28428, 2])
We keep 5.14e+06/3.30e+08 =  1% of the original kernel matrix.

torch.Size([29980, 2])
We keep 2.33e+07/4.10e+08 =  5% of the original kernel matrix.

torch.Size([35296, 2])
We keep 6.79e+06/4.84e+08 =  1% of the original kernel matrix.

torch.Size([11891, 2])
We keep 2.68e+06/5.15e+07 =  5% of the original kernel matrix.

torch.Size([21754, 2])
We keep 3.04e+06/1.72e+08 =  1% of the original kernel matrix.

torch.Size([25788, 2])
We keep 1.25e+07/4.07e+08 =  3% of the original kernel matrix.

torch.Size([31656, 2])
We keep 6.80e+06/4.83e+08 =  1% of the original kernel matrix.

torch.Size([16999, 2])
We keep 5.14e+06/1.25e+08 =  4% of the original kernel matrix.

torch.Size([26017, 2])
We keep 4.28e+06/2.67e+08 =  1% of the original kernel matrix.

torch.Size([26542, 2])
We keep 2.74e+07/6.98e+08 =  3% of the original kernel matrix.

torch.Size([30509, 2])
We keep 8.52e+06/6.32e+08 =  1% of the original kernel matrix.

torch.Size([21836, 2])
We keep 2.77e+07/4.13e+08 =  6% of the original kernel matrix.

torch.Size([29459, 2])
We keep 6.78e+06/4.86e+08 =  1% of the original kernel matrix.

torch.Size([9699, 2])
We keep 6.17e+05/1.72e+07 =  3% of the original kernel matrix.

torch.Size([20231, 2])
We keep 2.07e+06/9.93e+07 =  2% of the original kernel matrix.

torch.Size([5990, 2])
We keep 2.99e+05/6.69e+06 =  4% of the original kernel matrix.

torch.Size([16295, 2])
We keep 1.48e+06/6.19e+07 =  2% of the original kernel matrix.

torch.Size([14781, 2])
We keep 1.57e+06/5.47e+07 =  2% of the original kernel matrix.

torch.Size([24521, 2])
We keep 3.20e+06/1.77e+08 =  1% of the original kernel matrix.

torch.Size([22864, 2])
We keep 1.20e+07/4.69e+08 =  2% of the original kernel matrix.

torch.Size([28470, 2])
We keep 6.99e+06/5.18e+08 =  1% of the original kernel matrix.

torch.Size([38455, 2])
We keep 2.47e+07/7.90e+08 =  3% of the original kernel matrix.

torch.Size([39443, 2])
We keep 9.04e+06/6.73e+08 =  1% of the original kernel matrix.

torch.Size([125723, 2])
We keep 1.21e+08/5.44e+09 =  2% of the original kernel matrix.

torch.Size([71904, 2])
We keep 2.01e+07/1.77e+09 =  1% of the original kernel matrix.

torch.Size([7513, 2])
We keep 5.38e+05/1.25e+07 =  4% of the original kernel matrix.

torch.Size([17950, 2])
We keep 1.85e+06/8.47e+07 =  2% of the original kernel matrix.

torch.Size([24136, 2])
We keep 3.55e+06/1.83e+08 =  1% of the original kernel matrix.

torch.Size([31675, 2])
We keep 4.89e+06/3.23e+08 =  1% of the original kernel matrix.

torch.Size([21297, 2])
We keep 4.69e+06/1.66e+08 =  2% of the original kernel matrix.

torch.Size([29686, 2])
We keep 4.85e+06/3.08e+08 =  1% of the original kernel matrix.

torch.Size([105680, 2])
We keep 1.56e+08/4.55e+09 =  3% of the original kernel matrix.

torch.Size([65922, 2])
We keep 1.79e+07/1.61e+09 =  1% of the original kernel matrix.

torch.Size([26901, 2])
We keep 1.07e+07/3.11e+08 =  3% of the original kernel matrix.

torch.Size([33495, 2])
We keep 6.19e+06/4.22e+08 =  1% of the original kernel matrix.

torch.Size([66100, 2])
We keep 1.90e+07/1.50e+09 =  1% of the original kernel matrix.

torch.Size([52754, 2])
We keep 1.16e+07/9.28e+08 =  1% of the original kernel matrix.

torch.Size([8028, 2])
We keep 1.34e+06/1.73e+07 =  7% of the original kernel matrix.

torch.Size([18591, 2])
We keep 1.99e+06/9.96e+07 =  1% of the original kernel matrix.

torch.Size([21765, 2])
We keep 3.77e+06/1.51e+08 =  2% of the original kernel matrix.

torch.Size([30104, 2])
We keep 4.51e+06/2.94e+08 =  1% of the original kernel matrix.

torch.Size([2820, 2])
We keep 8.72e+04/1.40e+06 =  6% of the original kernel matrix.

torch.Size([12174, 2])
We keep 8.65e+05/2.83e+07 =  3% of the original kernel matrix.

torch.Size([8872, 2])
We keep 5.99e+05/1.62e+07 =  3% of the original kernel matrix.

torch.Size([19162, 2])
We keep 1.99e+06/9.62e+07 =  2% of the original kernel matrix.

torch.Size([30659, 2])
We keep 5.08e+06/2.94e+08 =  1% of the original kernel matrix.

torch.Size([36302, 2])
We keep 5.88e+06/4.10e+08 =  1% of the original kernel matrix.

torch.Size([21230, 2])
We keep 6.93e+06/2.29e+08 =  3% of the original kernel matrix.

torch.Size([29382, 2])
We keep 5.39e+06/3.62e+08 =  1% of the original kernel matrix.

torch.Size([24061, 2])
We keep 3.25e+06/1.64e+08 =  1% of the original kernel matrix.

torch.Size([31858, 2])
We keep 4.68e+06/3.07e+08 =  1% of the original kernel matrix.

torch.Size([2317, 2])
We keep 4.06e+04/6.07e+05 =  6% of the original kernel matrix.

torch.Size([11596, 2])
We keep 6.47e+05/1.86e+07 =  3% of the original kernel matrix.

torch.Size([5058, 2])
We keep 2.15e+05/4.48e+06 =  4% of the original kernel matrix.

torch.Size([15356, 2])
We keep 1.28e+06/5.06e+07 =  2% of the original kernel matrix.

torch.Size([19401, 2])
We keep 3.47e+06/1.07e+08 =  3% of the original kernel matrix.

torch.Size([28202, 2])
We keep 3.96e+06/2.47e+08 =  1% of the original kernel matrix.

torch.Size([17187, 2])
We keep 2.58e+06/9.62e+07 =  2% of the original kernel matrix.

torch.Size([26397, 2])
We keep 3.92e+06/2.35e+08 =  1% of the original kernel matrix.

torch.Size([14329, 2])
We keep 1.60e+06/4.92e+07 =  3% of the original kernel matrix.

torch.Size([24223, 2])
We keep 3.04e+06/1.68e+08 =  1% of the original kernel matrix.

torch.Size([2757, 2])
We keep 7.38e+04/1.09e+06 =  6% of the original kernel matrix.

torch.Size([12151, 2])
We keep 8.15e+05/2.50e+07 =  3% of the original kernel matrix.

torch.Size([3364, 2])
We keep 1.63e+05/2.77e+06 =  5% of the original kernel matrix.

torch.Size([12613, 2])
We keep 1.10e+06/3.98e+07 =  2% of the original kernel matrix.

torch.Size([14299, 2])
We keep 2.45e+06/5.68e+07 =  4% of the original kernel matrix.

torch.Size([24168, 2])
We keep 3.17e+06/1.80e+08 =  1% of the original kernel matrix.

torch.Size([19356, 2])
We keep 2.57e+06/1.02e+08 =  2% of the original kernel matrix.

torch.Size([28217, 2])
We keep 3.97e+06/2.42e+08 =  1% of the original kernel matrix.

torch.Size([54736, 2])
We keep 3.58e+07/1.44e+09 =  2% of the original kernel matrix.

torch.Size([47326, 2])
We keep 1.16e+07/9.07e+08 =  1% of the original kernel matrix.

torch.Size([3116, 2])
We keep 1.13e+05/1.62e+06 =  6% of the original kernel matrix.

torch.Size([12738, 2])
We keep 8.82e+05/3.05e+07 =  2% of the original kernel matrix.

torch.Size([12547, 2])
We keep 6.17e+06/5.58e+07 = 11% of the original kernel matrix.

torch.Size([22447, 2])
We keep 3.11e+06/1.79e+08 =  1% of the original kernel matrix.

torch.Size([7229, 2])
We keep 9.37e+05/1.87e+07 =  5% of the original kernel matrix.

torch.Size([17142, 2])
We keep 2.17e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([3605, 2])
We keep 1.53e+05/2.70e+06 =  5% of the original kernel matrix.

torch.Size([13128, 2])
We keep 1.08e+06/3.93e+07 =  2% of the original kernel matrix.

torch.Size([178824, 2])
We keep 1.98e+08/1.43e+10 =  1% of the original kernel matrix.

torch.Size([87039, 2])
We keep 2.95e+07/2.86e+09 =  1% of the original kernel matrix.

torch.Size([10401, 2])
We keep 4.82e+06/7.44e+07 =  6% of the original kernel matrix.

torch.Size([20324, 2])
We keep 3.47e+06/2.06e+08 =  1% of the original kernel matrix.

torch.Size([54881, 2])
We keep 3.56e+07/1.76e+09 =  2% of the original kernel matrix.

torch.Size([46674, 2])
We keep 1.24e+07/1.00e+09 =  1% of the original kernel matrix.

torch.Size([112264, 2])
We keep 5.68e+07/4.01e+09 =  1% of the original kernel matrix.

torch.Size([67177, 2])
We keep 1.75e+07/1.52e+09 =  1% of the original kernel matrix.

torch.Size([40884, 2])
We keep 5.85e+07/8.62e+08 =  6% of the original kernel matrix.

torch.Size([40991, 2])
We keep 8.98e+06/7.02e+08 =  1% of the original kernel matrix.

torch.Size([156392, 2])
We keep 1.67e+08/1.11e+10 =  1% of the original kernel matrix.

torch.Size([81410, 2])
We keep 2.72e+07/2.52e+09 =  1% of the original kernel matrix.

torch.Size([139051, 2])
We keep 8.06e+07/6.35e+09 =  1% of the original kernel matrix.

torch.Size([75779, 2])
We keep 2.11e+07/1.91e+09 =  1% of the original kernel matrix.

torch.Size([2731, 2])
We keep 7.09e+04/1.18e+06 =  6% of the original kernel matrix.

torch.Size([12063, 2])
We keep 8.10e+05/2.59e+07 =  3% of the original kernel matrix.

torch.Size([22614, 2])
We keep 3.87e+06/1.59e+08 =  2% of the original kernel matrix.

torch.Size([30793, 2])
We keep 4.77e+06/3.02e+08 =  1% of the original kernel matrix.

torch.Size([10788, 2])
We keep 8.10e+05/2.24e+07 =  3% of the original kernel matrix.

torch.Size([21211, 2])
We keep 2.24e+06/1.13e+08 =  1% of the original kernel matrix.

torch.Size([51279, 2])
We keep 5.17e+07/1.07e+09 =  4% of the original kernel matrix.

torch.Size([46594, 2])
We keep 9.88e+06/7.82e+08 =  1% of the original kernel matrix.

torch.Size([115401, 2])
We keep 9.51e+07/6.01e+09 =  1% of the original kernel matrix.

torch.Size([68915, 2])
We keep 2.08e+07/1.86e+09 =  1% of the original kernel matrix.

torch.Size([10252, 2])
We keep 2.17e+06/4.50e+07 =  4% of the original kernel matrix.

torch.Size([20105, 2])
We keep 2.94e+06/1.61e+08 =  1% of the original kernel matrix.

torch.Size([43067, 2])
We keep 1.31e+07/6.58e+08 =  1% of the original kernel matrix.

torch.Size([42833, 2])
We keep 8.38e+06/6.14e+08 =  1% of the original kernel matrix.

torch.Size([6162, 2])
We keep 2.54e+05/6.18e+06 =  4% of the original kernel matrix.

torch.Size([16613, 2])
We keep 1.43e+06/5.95e+07 =  2% of the original kernel matrix.

torch.Size([8737, 2])
We keep 3.31e+06/6.83e+07 =  4% of the original kernel matrix.

torch.Size([18266, 2])
We keep 3.36e+06/1.98e+08 =  1% of the original kernel matrix.

torch.Size([7885, 2])
We keep 3.09e+06/2.42e+07 = 12% of the original kernel matrix.

torch.Size([18139, 2])
We keep 2.27e+06/1.18e+08 =  1% of the original kernel matrix.

torch.Size([4652, 2])
We keep 2.37e+05/4.66e+06 =  5% of the original kernel matrix.

torch.Size([14615, 2])
We keep 1.29e+06/5.16e+07 =  2% of the original kernel matrix.

torch.Size([218179, 2])
We keep 3.73e+08/1.97e+10 =  1% of the original kernel matrix.

torch.Size([97137, 2])
We keep 3.49e+07/3.36e+09 =  1% of the original kernel matrix.

torch.Size([411410, 2])
We keep 1.68e+09/7.48e+10 =  2% of the original kernel matrix.

torch.Size([134017, 2])
We keep 6.38e+07/6.55e+09 =  0% of the original kernel matrix.

torch.Size([12614, 2])
We keep 3.80e+06/7.99e+07 =  4% of the original kernel matrix.

torch.Size([22227, 2])
We keep 3.74e+06/2.14e+08 =  1% of the original kernel matrix.

torch.Size([11565, 2])
We keep 1.26e+06/3.37e+07 =  3% of the original kernel matrix.

torch.Size([21968, 2])
We keep 2.54e+06/1.39e+08 =  1% of the original kernel matrix.

torch.Size([21217, 2])
We keep 3.67e+06/1.44e+08 =  2% of the original kernel matrix.

torch.Size([29696, 2])
We keep 4.56e+06/2.87e+08 =  1% of the original kernel matrix.

torch.Size([25564, 2])
We keep 8.99e+06/2.56e+08 =  3% of the original kernel matrix.

torch.Size([32648, 2])
We keep 5.61e+06/3.83e+08 =  1% of the original kernel matrix.

torch.Size([13709, 2])
We keep 1.21e+06/3.95e+07 =  3% of the original kernel matrix.

torch.Size([23596, 2])
We keep 2.81e+06/1.50e+08 =  1% of the original kernel matrix.

torch.Size([7524, 2])
We keep 6.96e+05/1.22e+07 =  5% of the original kernel matrix.

torch.Size([18108, 2])
We keep 1.82e+06/8.37e+07 =  2% of the original kernel matrix.

torch.Size([21920, 2])
We keep 3.60e+06/1.44e+08 =  2% of the original kernel matrix.

torch.Size([30251, 2])
We keep 4.49e+06/2.87e+08 =  1% of the original kernel matrix.

torch.Size([114955, 2])
We keep 6.46e+07/4.38e+09 =  1% of the original kernel matrix.

torch.Size([68217, 2])
We keep 1.83e+07/1.58e+09 =  1% of the original kernel matrix.

torch.Size([4670, 2])
We keep 2.50e+05/4.01e+06 =  6% of the original kernel matrix.

torch.Size([14717, 2])
We keep 1.25e+06/4.79e+07 =  2% of the original kernel matrix.

torch.Size([47768, 2])
We keep 3.22e+07/1.04e+09 =  3% of the original kernel matrix.

torch.Size([44485, 2])
We keep 1.01e+07/7.71e+08 =  1% of the original kernel matrix.

torch.Size([3987, 2])
We keep 2.97e+05/3.41e+06 =  8% of the original kernel matrix.

torch.Size([13764, 2])
We keep 1.12e+06/4.42e+07 =  2% of the original kernel matrix.

time for making ranges is 3.18697190284729
Sorting X and nu_X
time for sorting X is 0.0649716854095459
Sorting Z and nu_Z
time for sorting Z is 0.0002524852752685547
Starting Optim
sum tnu_Z before tensor(16649214., device='cuda:0')
c= tensor(479.0144, device='cuda:0')
c= tensor(10196.9160, device='cuda:0')
c= tensor(11675.0791, device='cuda:0')
c= tensor(32387.5977, device='cuda:0')
c= tensor(39332.3281, device='cuda:0')
c= tensor(55504.2422, device='cuda:0')
c= tensor(542883.0625, device='cuda:0')
c= tensor(594030.3750, device='cuda:0')
c= tensor(600582.5625, device='cuda:0')
c= tensor(920888.2500, device='cuda:0')
c= tensor(925474.8750, device='cuda:0')
c= tensor(2102881.7500, device='cuda:0')
c= tensor(2107957.5000, device='cuda:0')
c= tensor(8617114., device='cuda:0')
c= tensor(8700128., device='cuda:0')
c= tensor(8746657., device='cuda:0')
c= tensor(9224757., device='cuda:0')
c= tensor(9483037., device='cuda:0')
c= tensor(12266479., device='cuda:0')
c= tensor(14802362., device='cuda:0')
c= tensor(14810020., device='cuda:0')
c= tensor(18232890., device='cuda:0')
c= tensor(18248892., device='cuda:0')
c= tensor(18512918., device='cuda:0')
c= tensor(18514302., device='cuda:0')
c= tensor(19130092., device='cuda:0')
c= tensor(19556958., device='cuda:0')
c= tensor(19560796., device='cuda:0')
c= tensor(20238304., device='cuda:0')
c= tensor(57048988., device='cuda:0')
c= tensor(57054588., device='cuda:0')
c= tensor(1.5423e+08, device='cuda:0')
c= tensor(1.5443e+08, device='cuda:0')
c= tensor(1.5444e+08, device='cuda:0')
c= tensor(1.5444e+08, device='cuda:0')
c= tensor(1.5530e+08, device='cuda:0')
c= tensor(1.5791e+08, device='cuda:0')
c= tensor(1.5791e+08, device='cuda:0')
c= tensor(1.5792e+08, device='cuda:0')
c= tensor(1.5793e+08, device='cuda:0')
c= tensor(1.5793e+08, device='cuda:0')
c= tensor(1.5793e+08, device='cuda:0')
c= tensor(1.5793e+08, device='cuda:0')
c= tensor(1.5793e+08, device='cuda:0')
c= tensor(1.5793e+08, device='cuda:0')
c= tensor(1.5793e+08, device='cuda:0')
c= tensor(1.5794e+08, device='cuda:0')
c= tensor(1.5794e+08, device='cuda:0')
c= tensor(1.5794e+08, device='cuda:0')
c= tensor(1.5798e+08, device='cuda:0')
c= tensor(1.5799e+08, device='cuda:0')
c= tensor(1.5800e+08, device='cuda:0')
c= tensor(1.5801e+08, device='cuda:0')
c= tensor(1.5801e+08, device='cuda:0')
c= tensor(1.5801e+08, device='cuda:0')
c= tensor(1.5802e+08, device='cuda:0')
c= tensor(1.5802e+08, device='cuda:0')
c= tensor(1.5803e+08, device='cuda:0')
c= tensor(1.5803e+08, device='cuda:0')
c= tensor(1.5803e+08, device='cuda:0')
c= tensor(1.5804e+08, device='cuda:0')
c= tensor(1.5804e+08, device='cuda:0')
c= tensor(1.5806e+08, device='cuda:0')
c= tensor(1.5806e+08, device='cuda:0')
c= tensor(1.5806e+08, device='cuda:0')
c= tensor(1.5806e+08, device='cuda:0')
c= tensor(1.5806e+08, device='cuda:0')
c= tensor(1.5806e+08, device='cuda:0')
c= tensor(1.5807e+08, device='cuda:0')
c= tensor(1.5807e+08, device='cuda:0')
c= tensor(1.5807e+08, device='cuda:0')
c= tensor(1.5807e+08, device='cuda:0')
c= tensor(1.5807e+08, device='cuda:0')
c= tensor(1.5808e+08, device='cuda:0')
c= tensor(1.5808e+08, device='cuda:0')
c= tensor(1.5808e+08, device='cuda:0')
c= tensor(1.5808e+08, device='cuda:0')
c= tensor(1.5809e+08, device='cuda:0')
c= tensor(1.5809e+08, device='cuda:0')
c= tensor(1.5809e+08, device='cuda:0')
c= tensor(1.5809e+08, device='cuda:0')
c= tensor(1.5809e+08, device='cuda:0')
c= tensor(1.5811e+08, device='cuda:0')
c= tensor(1.5811e+08, device='cuda:0')
c= tensor(1.5811e+08, device='cuda:0')
c= tensor(1.5811e+08, device='cuda:0')
c= tensor(1.5812e+08, device='cuda:0')
c= tensor(1.5812e+08, device='cuda:0')
c= tensor(1.5812e+08, device='cuda:0')
c= tensor(1.5812e+08, device='cuda:0')
c= tensor(1.5812e+08, device='cuda:0')
c= tensor(1.5812e+08, device='cuda:0')
c= tensor(1.5812e+08, device='cuda:0')
c= tensor(1.5812e+08, device='cuda:0')
c= tensor(1.5813e+08, device='cuda:0')
c= tensor(1.5813e+08, device='cuda:0')
c= tensor(1.5813e+08, device='cuda:0')
c= tensor(1.5815e+08, device='cuda:0')
c= tensor(1.5815e+08, device='cuda:0')
c= tensor(1.5815e+08, device='cuda:0')
c= tensor(1.5818e+08, device='cuda:0')
c= tensor(1.5818e+08, device='cuda:0')
c= tensor(1.5819e+08, device='cuda:0')
c= tensor(1.5819e+08, device='cuda:0')
c= tensor(1.5819e+08, device='cuda:0')
c= tensor(1.5819e+08, device='cuda:0')
c= tensor(1.5819e+08, device='cuda:0')
c= tensor(1.5819e+08, device='cuda:0')
c= tensor(1.5820e+08, device='cuda:0')
c= tensor(1.5820e+08, device='cuda:0')
c= tensor(1.5820e+08, device='cuda:0')
c= tensor(1.5820e+08, device='cuda:0')
c= tensor(1.5820e+08, device='cuda:0')
c= tensor(1.5820e+08, device='cuda:0')
c= tensor(1.5821e+08, device='cuda:0')
c= tensor(1.5821e+08, device='cuda:0')
c= tensor(1.5821e+08, device='cuda:0')
c= tensor(1.5821e+08, device='cuda:0')
c= tensor(1.5821e+08, device='cuda:0')
c= tensor(1.5821e+08, device='cuda:0')
c= tensor(1.5822e+08, device='cuda:0')
c= tensor(1.5822e+08, device='cuda:0')
c= tensor(1.5822e+08, device='cuda:0')
c= tensor(1.5822e+08, device='cuda:0')
c= tensor(1.5822e+08, device='cuda:0')
c= tensor(1.5823e+08, device='cuda:0')
c= tensor(1.5823e+08, device='cuda:0')
c= tensor(1.5823e+08, device='cuda:0')
c= tensor(1.5826e+08, device='cuda:0')
c= tensor(1.5827e+08, device='cuda:0')
c= tensor(1.5827e+08, device='cuda:0')
c= tensor(1.5827e+08, device='cuda:0')
c= tensor(1.5827e+08, device='cuda:0')
c= tensor(1.5827e+08, device='cuda:0')
c= tensor(1.5830e+08, device='cuda:0')
c= tensor(1.5830e+08, device='cuda:0')
c= tensor(1.5830e+08, device='cuda:0')
c= tensor(1.5830e+08, device='cuda:0')
c= tensor(1.5830e+08, device='cuda:0')
c= tensor(1.5830e+08, device='cuda:0')
c= tensor(1.5830e+08, device='cuda:0')
c= tensor(1.5830e+08, device='cuda:0')
c= tensor(1.5832e+08, device='cuda:0')
c= tensor(1.5833e+08, device='cuda:0')
c= tensor(1.5834e+08, device='cuda:0')
c= tensor(1.5835e+08, device='cuda:0')
c= tensor(1.5835e+08, device='cuda:0')
c= tensor(1.5835e+08, device='cuda:0')
c= tensor(1.5835e+08, device='cuda:0')
c= tensor(1.5835e+08, device='cuda:0')
c= tensor(1.5836e+08, device='cuda:0')
c= tensor(1.5836e+08, device='cuda:0')
c= tensor(1.5837e+08, device='cuda:0')
c= tensor(1.5837e+08, device='cuda:0')
c= tensor(1.5838e+08, device='cuda:0')
c= tensor(1.5841e+08, device='cuda:0')
c= tensor(1.5842e+08, device='cuda:0')
c= tensor(1.5842e+08, device='cuda:0')
c= tensor(1.5842e+08, device='cuda:0')
c= tensor(1.5842e+08, device='cuda:0')
c= tensor(1.5843e+08, device='cuda:0')
c= tensor(1.5843e+08, device='cuda:0')
c= tensor(1.5843e+08, device='cuda:0')
c= tensor(1.5843e+08, device='cuda:0')
c= tensor(1.5843e+08, device='cuda:0')
c= tensor(1.5844e+08, device='cuda:0')
c= tensor(1.5844e+08, device='cuda:0')
c= tensor(1.5844e+08, device='cuda:0')
c= tensor(1.5844e+08, device='cuda:0')
c= tensor(1.5844e+08, device='cuda:0')
c= tensor(1.5844e+08, device='cuda:0')
c= tensor(1.5844e+08, device='cuda:0')
c= tensor(1.5844e+08, device='cuda:0')
c= tensor(1.5845e+08, device='cuda:0')
c= tensor(1.5845e+08, device='cuda:0')
c= tensor(1.5845e+08, device='cuda:0')
c= tensor(1.5845e+08, device='cuda:0')
c= tensor(1.5846e+08, device='cuda:0')
c= tensor(1.5846e+08, device='cuda:0')
c= tensor(1.5846e+08, device='cuda:0')
c= tensor(1.5846e+08, device='cuda:0')
c= tensor(1.5846e+08, device='cuda:0')
c= tensor(1.5847e+08, device='cuda:0')
c= tensor(1.5847e+08, device='cuda:0')
c= tensor(1.5848e+08, device='cuda:0')
c= tensor(1.5848e+08, device='cuda:0')
c= tensor(1.5848e+08, device='cuda:0')
c= tensor(1.5849e+08, device='cuda:0')
c= tensor(1.5849e+08, device='cuda:0')
c= tensor(1.5851e+08, device='cuda:0')
c= tensor(1.5851e+08, device='cuda:0')
c= tensor(1.5851e+08, device='cuda:0')
c= tensor(1.5851e+08, device='cuda:0')
c= tensor(1.5852e+08, device='cuda:0')
c= tensor(1.5852e+08, device='cuda:0')
c= tensor(1.5852e+08, device='cuda:0')
c= tensor(1.5852e+08, device='cuda:0')
c= tensor(1.5852e+08, device='cuda:0')
c= tensor(1.5852e+08, device='cuda:0')
c= tensor(1.5852e+08, device='cuda:0')
c= tensor(1.5852e+08, device='cuda:0')
c= tensor(1.5852e+08, device='cuda:0')
c= tensor(1.5853e+08, device='cuda:0')
c= tensor(1.5854e+08, device='cuda:0')
c= tensor(1.5854e+08, device='cuda:0')
c= tensor(1.5854e+08, device='cuda:0')
c= tensor(1.5855e+08, device='cuda:0')
c= tensor(1.5855e+08, device='cuda:0')
c= tensor(1.5855e+08, device='cuda:0')
c= tensor(1.5856e+08, device='cuda:0')
c= tensor(1.5858e+08, device='cuda:0')
c= tensor(1.5858e+08, device='cuda:0')
c= tensor(1.5858e+08, device='cuda:0')
c= tensor(1.5858e+08, device='cuda:0')
c= tensor(1.5858e+08, device='cuda:0')
c= tensor(1.5858e+08, device='cuda:0')
c= tensor(1.5858e+08, device='cuda:0')
c= tensor(1.5858e+08, device='cuda:0')
c= tensor(1.5859e+08, device='cuda:0')
c= tensor(1.5859e+08, device='cuda:0')
c= tensor(1.5859e+08, device='cuda:0')
c= tensor(1.5859e+08, device='cuda:0')
c= tensor(1.5859e+08, device='cuda:0')
c= tensor(1.5859e+08, device='cuda:0')
c= tensor(1.5861e+08, device='cuda:0')
c= tensor(1.5861e+08, device='cuda:0')
c= tensor(1.5861e+08, device='cuda:0')
c= tensor(1.5861e+08, device='cuda:0')
c= tensor(1.5862e+08, device='cuda:0')
c= tensor(1.5862e+08, device='cuda:0')
c= tensor(1.5862e+08, device='cuda:0')
c= tensor(1.5862e+08, device='cuda:0')
c= tensor(1.5862e+08, device='cuda:0')
c= tensor(1.5863e+08, device='cuda:0')
c= tensor(1.5863e+08, device='cuda:0')
c= tensor(1.5863e+08, device='cuda:0')
c= tensor(1.5863e+08, device='cuda:0')
c= tensor(1.5863e+08, device='cuda:0')
c= tensor(1.5863e+08, device='cuda:0')
c= tensor(1.5865e+08, device='cuda:0')
c= tensor(1.5935e+08, device='cuda:0')
c= tensor(1.5936e+08, device='cuda:0')
c= tensor(1.5937e+08, device='cuda:0')
c= tensor(1.5937e+08, device='cuda:0')
c= tensor(1.5937e+08, device='cuda:0')
c= tensor(1.6247e+08, device='cuda:0')
c= tensor(1.7056e+08, device='cuda:0')
c= tensor(1.7056e+08, device='cuda:0')
c= tensor(1.7076e+08, device='cuda:0')
c= tensor(1.7117e+08, device='cuda:0')
c= tensor(1.7117e+08, device='cuda:0')
c= tensor(1.7760e+08, device='cuda:0')
c= tensor(1.7760e+08, device='cuda:0')
c= tensor(1.7761e+08, device='cuda:0')
c= tensor(1.7825e+08, device='cuda:0')
c= tensor(1.8479e+08, device='cuda:0')
c= tensor(1.8479e+08, device='cuda:0')
c= tensor(1.8483e+08, device='cuda:0')
c= tensor(1.8486e+08, device='cuda:0')
c= tensor(1.8585e+08, device='cuda:0')
c= tensor(1.8617e+08, device='cuda:0')
c= tensor(1.8635e+08, device='cuda:0')
c= tensor(1.8645e+08, device='cuda:0')
c= tensor(1.8654e+08, device='cuda:0')
c= tensor(1.8654e+08, device='cuda:0')
c= tensor(1.9060e+08, device='cuda:0')
c= tensor(1.9060e+08, device='cuda:0')
c= tensor(1.9060e+08, device='cuda:0')
c= tensor(1.9064e+08, device='cuda:0')
c= tensor(1.9090e+08, device='cuda:0')
c= tensor(1.9456e+08, device='cuda:0')
c= tensor(1.9498e+08, device='cuda:0')
c= tensor(1.9499e+08, device='cuda:0')
c= tensor(1.9500e+08, device='cuda:0')
c= tensor(1.9501e+08, device='cuda:0')
c= tensor(1.9522e+08, device='cuda:0')
c= tensor(1.9636e+08, device='cuda:0')
c= tensor(1.9654e+08, device='cuda:0')
c= tensor(1.9657e+08, device='cuda:0')
c= tensor(1.9657e+08, device='cuda:0')
c= tensor(1.9657e+08, device='cuda:0')
c= tensor(1.9691e+08, device='cuda:0')
c= tensor(1.9718e+08, device='cuda:0')
c= tensor(1.9728e+08, device='cuda:0')
c= tensor(1.9729e+08, device='cuda:0')
c= tensor(2.0979e+08, device='cuda:0')
c= tensor(2.0980e+08, device='cuda:0')
c= tensor(2.0986e+08, device='cuda:0')
c= tensor(2.1093e+08, device='cuda:0')
c= tensor(2.1094e+08, device='cuda:0')
c= tensor(2.1122e+08, device='cuda:0')
c= tensor(2.1841e+08, device='cuda:0')
c= tensor(2.2471e+08, device='cuda:0')
c= tensor(2.2472e+08, device='cuda:0')
c= tensor(2.2472e+08, device='cuda:0')
c= tensor(2.2472e+08, device='cuda:0')
c= tensor(2.2473e+08, device='cuda:0')
c= tensor(2.2493e+08, device='cuda:0')
c= tensor(2.2494e+08, device='cuda:0')
c= tensor(2.2519e+08, device='cuda:0')
c= tensor(2.3330e+08, device='cuda:0')
c= tensor(2.3336e+08, device='cuda:0')
c= tensor(2.3339e+08, device='cuda:0')
c= tensor(2.3339e+08, device='cuda:0')
c= tensor(2.3676e+08, device='cuda:0')
c= tensor(2.3735e+08, device='cuda:0')
c= tensor(2.3740e+08, device='cuda:0')
c= tensor(2.3742e+08, device='cuda:0')
c= tensor(2.6556e+08, device='cuda:0')
c= tensor(2.6557e+08, device='cuda:0')
c= tensor(2.6754e+08, device='cuda:0')
c= tensor(2.6755e+08, device='cuda:0')
c= tensor(2.6857e+08, device='cuda:0')
c= tensor(2.6864e+08, device='cuda:0')
c= tensor(2.7871e+08, device='cuda:0')
c= tensor(2.7893e+08, device='cuda:0')
c= tensor(2.7893e+08, device='cuda:0')
c= tensor(2.8013e+08, device='cuda:0')
c= tensor(2.8092e+08, device='cuda:0')
c= tensor(2.8093e+08, device='cuda:0')
c= tensor(2.8113e+08, device='cuda:0')
c= tensor(2.8202e+08, device='cuda:0')
c= tensor(2.9367e+08, device='cuda:0')
c= tensor(2.9381e+08, device='cuda:0')
c= tensor(2.9381e+08, device='cuda:0')
c= tensor(2.9382e+08, device='cuda:0')
c= tensor(2.9386e+08, device='cuda:0')
c= tensor(2.9388e+08, device='cuda:0')
c= tensor(2.9390e+08, device='cuda:0')
c= tensor(2.9390e+08, device='cuda:0')
c= tensor(2.9456e+08, device='cuda:0')
c= tensor(2.9526e+08, device='cuda:0')
c= tensor(2.9526e+08, device='cuda:0')
c= tensor(2.9527e+08, device='cuda:0')
c= tensor(2.9528e+08, device='cuda:0')
c= tensor(2.9530e+08, device='cuda:0')
c= tensor(2.9531e+08, device='cuda:0')
c= tensor(2.9532e+08, device='cuda:0')
c= tensor(2.9533e+08, device='cuda:0')
c= tensor(2.9541e+08, device='cuda:0')
c= tensor(2.9547e+08, device='cuda:0')
c= tensor(2.9549e+08, device='cuda:0')
c= tensor(2.9580e+08, device='cuda:0')
c= tensor(2.9580e+08, device='cuda:0')
c= tensor(3.6989e+08, device='cuda:0')
c= tensor(3.6989e+08, device='cuda:0')
c= tensor(3.7024e+08, device='cuda:0')
c= tensor(3.7024e+08, device='cuda:0')
c= tensor(3.7024e+08, device='cuda:0')
c= tensor(3.7024e+08, device='cuda:0')
c= tensor(3.7029e+08, device='cuda:0')
c= tensor(3.7029e+08, device='cuda:0')
c= tensor(3.7141e+08, device='cuda:0')
c= tensor(3.7142e+08, device='cuda:0')
c= tensor(3.7142e+08, device='cuda:0')
c= tensor(3.7907e+08, device='cuda:0')
c= tensor(3.7937e+08, device='cuda:0')
c= tensor(3.7970e+08, device='cuda:0')
c= tensor(3.8216e+08, device='cuda:0')
c= tensor(3.8501e+08, device='cuda:0')
c= tensor(3.8501e+08, device='cuda:0')
c= tensor(3.8501e+08, device='cuda:0')
c= tensor(3.8510e+08, device='cuda:0')
c= tensor(3.8510e+08, device='cuda:0')
c= tensor(3.8510e+08, device='cuda:0')
c= tensor(3.8512e+08, device='cuda:0')
c= tensor(3.8512e+08, device='cuda:0')
c= tensor(3.8512e+08, device='cuda:0')
c= tensor(3.8512e+08, device='cuda:0')
c= tensor(3.8513e+08, device='cuda:0')
c= tensor(3.8851e+08, device='cuda:0')
c= tensor(3.8852e+08, device='cuda:0')
c= tensor(3.8888e+08, device='cuda:0')
c= tensor(3.8889e+08, device='cuda:0')
c= tensor(3.8889e+08, device='cuda:0')
c= tensor(3.8890e+08, device='cuda:0')
c= tensor(4.3724e+08, device='cuda:0')
c= tensor(4.4737e+08, device='cuda:0')
c= tensor(4.4738e+08, device='cuda:0')
c= tensor(4.4758e+08, device='cuda:0')
c= tensor(4.4758e+08, device='cuda:0')
c= tensor(4.4760e+08, device='cuda:0')
c= tensor(4.4873e+08, device='cuda:0')
c= tensor(4.4880e+08, device='cuda:0')
c= tensor(4.4918e+08, device='cuda:0')
c= tensor(4.4974e+08, device='cuda:0')
c= tensor(4.7682e+08, device='cuda:0')
c= tensor(4.7687e+08, device='cuda:0')
c= tensor(4.7687e+08, device='cuda:0')
c= tensor(4.7690e+08, device='cuda:0')
c= tensor(4.7691e+08, device='cuda:0')
c= tensor(4.7691e+08, device='cuda:0')
c= tensor(4.7810e+08, device='cuda:0')
c= tensor(4.7816e+08, device='cuda:0')
c= tensor(4.7816e+08, device='cuda:0')
c= tensor(4.7823e+08, device='cuda:0')
c= tensor(4.7824e+08, device='cuda:0')
c= tensor(4.7824e+08, device='cuda:0')
c= tensor(4.7897e+08, device='cuda:0')
c= tensor(4.8085e+08, device='cuda:0')
c= tensor(4.8175e+08, device='cuda:0')
c= tensor(4.8332e+08, device='cuda:0')
c= tensor(4.8658e+08, device='cuda:0')
c= tensor(4.8660e+08, device='cuda:0')
c= tensor(4.8661e+08, device='cuda:0')
c= tensor(4.8688e+08, device='cuda:0')
c= tensor(4.8733e+08, device='cuda:0')
c= tensor(4.8733e+08, device='cuda:0')
c= tensor(4.8785e+08, device='cuda:0')
c= tensor(5.1926e+08, device='cuda:0')
c= tensor(5.2082e+08, device='cuda:0')
c= tensor(5.2182e+08, device='cuda:0')
c= tensor(5.2387e+08, device='cuda:0')
c= tensor(5.2388e+08, device='cuda:0')
c= tensor(5.2388e+08, device='cuda:0')
c= tensor(5.2389e+08, device='cuda:0')
c= tensor(5.2430e+08, device='cuda:0')
c= tensor(5.2458e+08, device='cuda:0')
c= tensor(5.5681e+08, device='cuda:0')
c= tensor(5.5883e+08, device='cuda:0')
c= tensor(5.5946e+08, device='cuda:0')
c= tensor(5.5966e+08, device='cuda:0')
c= tensor(5.6052e+08, device='cuda:0')
c= tensor(5.6052e+08, device='cuda:0')
c= tensor(5.6053e+08, device='cuda:0')
c= tensor(5.6379e+08, device='cuda:0')
c= tensor(5.6380e+08, device='cuda:0')
c= tensor(5.6380e+08, device='cuda:0')
c= tensor(5.6387e+08, device='cuda:0')
c= tensor(5.6911e+08, device='cuda:0')
c= tensor(5.6915e+08, device='cuda:0')
c= tensor(5.7016e+08, device='cuda:0')
c= tensor(5.7016e+08, device='cuda:0')
c= tensor(5.7017e+08, device='cuda:0')
c= tensor(5.7017e+08, device='cuda:0')
c= tensor(5.7018e+08, device='cuda:0')
c= tensor(5.7050e+08, device='cuda:0')
c= tensor(5.7055e+08, device='cuda:0')
c= tensor(5.7056e+08, device='cuda:0')
c= tensor(5.7158e+08, device='cuda:0')
c= tensor(5.7158e+08, device='cuda:0')
c= tensor(5.7196e+08, device='cuda:0')
c= tensor(5.7198e+08, device='cuda:0')
c= tensor(5.7202e+08, device='cuda:0')
c= tensor(5.7203e+08, device='cuda:0')
c= tensor(5.7204e+08, device='cuda:0')
c= tensor(5.7205e+08, device='cuda:0')
c= tensor(5.7211e+08, device='cuda:0')
c= tensor(5.7326e+08, device='cuda:0')
c= tensor(5.7588e+08, device='cuda:0')
c= tensor(5.7588e+08, device='cuda:0')
c= tensor(5.7591e+08, device='cuda:0')
c= tensor(5.7924e+08, device='cuda:0')
c= tensor(5.7925e+08, device='cuda:0')
c= tensor(6.0566e+08, device='cuda:0')
c= tensor(6.0566e+08, device='cuda:0')
c= tensor(6.0588e+08, device='cuda:0')
c= tensor(6.0638e+08, device='cuda:0')
c= tensor(6.0638e+08, device='cuda:0')
c= tensor(6.0851e+08, device='cuda:0')
c= tensor(6.0866e+08, device='cuda:0')
c= tensor(6.6198e+08, device='cuda:0')
c= tensor(6.6198e+08, device='cuda:0')
c= tensor(6.6200e+08, device='cuda:0')
c= tensor(6.6201e+08, device='cuda:0')
c= tensor(6.6201e+08, device='cuda:0')
c= tensor(6.6202e+08, device='cuda:0')
c= tensor(6.6219e+08, device='cuda:0')
c= tensor(6.6223e+08, device='cuda:0')
c= tensor(6.6234e+08, device='cuda:0')
c= tensor(6.6236e+08, device='cuda:0')
c= tensor(6.6236e+08, device='cuda:0')
c= tensor(6.6238e+08, device='cuda:0')
c= tensor(6.6904e+08, device='cuda:0')
c= tensor(6.6965e+08, device='cuda:0')
c= tensor(6.7326e+08, device='cuda:0')
c= tensor(6.7332e+08, device='cuda:0')
c= tensor(6.7336e+08, device='cuda:0')
c= tensor(6.7337e+08, device='cuda:0')
c= tensor(6.7338e+08, device='cuda:0')
c= tensor(6.7708e+08, device='cuda:0')
c= tensor(6.7710e+08, device='cuda:0')
c= tensor(6.7711e+08, device='cuda:0')
c= tensor(6.7719e+08, device='cuda:0')
c= tensor(6.7758e+08, device='cuda:0')
c= tensor(6.7758e+08, device='cuda:0')
c= tensor(6.7758e+08, device='cuda:0')
c= tensor(7.0247e+08, device='cuda:0')
c= tensor(7.0250e+08, device='cuda:0')
c= tensor(7.0284e+08, device='cuda:0')
c= tensor(7.0285e+08, device='cuda:0')
c= tensor(7.0335e+08, device='cuda:0')
c= tensor(7.1005e+08, device='cuda:0')
c= tensor(7.1270e+08, device='cuda:0')
c= tensor(7.1554e+08, device='cuda:0')
c= tensor(7.1557e+08, device='cuda:0')
c= tensor(7.1577e+08, device='cuda:0')
c= tensor(7.1587e+08, device='cuda:0')
c= tensor(7.1587e+08, device='cuda:0')
c= tensor(7.1590e+08, device='cuda:0')
c= tensor(7.1590e+08, device='cuda:0')
c= tensor(7.1605e+08, device='cuda:0')
c= tensor(7.1605e+08, device='cuda:0')
c= tensor(7.1605e+08, device='cuda:0')
c= tensor(7.1606e+08, device='cuda:0')
c= tensor(7.1607e+08, device='cuda:0')
c= tensor(7.1679e+08, device='cuda:0')
c= tensor(7.1680e+08, device='cuda:0')
c= tensor(7.1696e+08, device='cuda:0')
c= tensor(7.1709e+08, device='cuda:0')
c= tensor(7.1713e+08, device='cuda:0')
c= tensor(7.1713e+08, device='cuda:0')
c= tensor(7.1714e+08, device='cuda:0')
c= tensor(7.1716e+08, device='cuda:0')
c= tensor(7.2176e+08, device='cuda:0')
c= tensor(7.2176e+08, device='cuda:0')
c= tensor(7.2176e+08, device='cuda:0')
c= tensor(7.2177e+08, device='cuda:0')
c= tensor(7.3177e+08, device='cuda:0')
c= tensor(7.4980e+08, device='cuda:0')
c= tensor(7.4984e+08, device='cuda:0')
c= tensor(7.4985e+08, device='cuda:0')
c= tensor(7.4995e+08, device='cuda:0')
c= tensor(7.5018e+08, device='cuda:0')
c= tensor(7.5018e+08, device='cuda:0')
c= tensor(7.5022e+08, device='cuda:0')
c= tensor(7.5023e+08, device='cuda:0')
c= tensor(7.5199e+08, device='cuda:0')
c= tensor(7.5207e+08, device='cuda:0')
c= tensor(7.5712e+08, device='cuda:0')
c= tensor(7.5713e+08, device='cuda:0')
c= tensor(7.5713e+08, device='cuda:0')
c= tensor(7.5713e+08, device='cuda:0')
c= tensor(7.5717e+08, device='cuda:0')
c= tensor(7.5718e+08, device='cuda:0')
c= tensor(7.5732e+08, device='cuda:0')
c= tensor(7.6785e+08, device='cuda:0')
c= tensor(7.9175e+08, device='cuda:0')
c= tensor(7.9176e+08, device='cuda:0')
c= tensor(7.9176e+08, device='cuda:0')
c= tensor(7.9178e+08, device='cuda:0')
c= tensor(8.7935e+08, device='cuda:0')
c= tensor(8.7952e+08, device='cuda:0')
c= tensor(8.7952e+08, device='cuda:0')
c= tensor(8.7952e+08, device='cuda:0')
c= tensor(8.8025e+08, device='cuda:0')
c= tensor(8.8025e+08, device='cuda:0')
c= tensor(8.8119e+08, device='cuda:0')
c= tensor(8.8119e+08, device='cuda:0')
c= tensor(8.8121e+08, device='cuda:0')
c= tensor(8.8122e+08, device='cuda:0')
c= tensor(8.8122e+08, device='cuda:0')
c= tensor(8.8122e+08, device='cuda:0')
c= tensor(8.8364e+08, device='cuda:0')
c= tensor(8.9569e+08, device='cuda:0')
c= tensor(8.9797e+08, device='cuda:0')
c= tensor(8.9806e+08, device='cuda:0')
c= tensor(8.9816e+08, device='cuda:0')
c= tensor(8.9817e+08, device='cuda:0')
c= tensor(8.9817e+08, device='cuda:0')
c= tensor(8.9840e+08, device='cuda:0')
c= tensor(8.9857e+08, device='cuda:0')
c= tensor(9.0270e+08, device='cuda:0')
c= tensor(9.0271e+08, device='cuda:0')
c= tensor(9.3817e+08, device='cuda:0')
c= tensor(9.3818e+08, device='cuda:0')
c= tensor(9.3833e+08, device='cuda:0')
c= tensor(9.3982e+08, device='cuda:0')
c= tensor(9.3995e+08, device='cuda:0')
c= tensor(9.4178e+08, device='cuda:0')
c= tensor(9.4569e+08, device='cuda:0')
c= tensor(9.4575e+08, device='cuda:0')
c= tensor(9.4578e+08, device='cuda:0')
c= tensor(9.4579e+08, device='cuda:0')
c= tensor(9.4597e+08, device='cuda:0')
c= tensor(9.4603e+08, device='cuda:0')
c= tensor(9.4754e+08, device='cuda:0')
c= tensor(9.8434e+08, device='cuda:0')
c= tensor(9.8663e+08, device='cuda:0')
c= tensor(9.9140e+08, device='cuda:0')
c= tensor(9.9183e+08, device='cuda:0')
c= tensor(9.9198e+08, device='cuda:0')
c= tensor(9.9212e+08, device='cuda:0')
c= tensor(9.9214e+08, device='cuda:0')
c= tensor(9.9216e+08, device='cuda:0')
c= tensor(9.9352e+08, device='cuda:0')
c= tensor(9.9358e+08, device='cuda:0')
c= tensor(1.0014e+09, device='cuda:0')
c= tensor(1.0218e+09, device='cuda:0')
c= tensor(1.0218e+09, device='cuda:0')
c= tensor(1.0218e+09, device='cuda:0')
c= tensor(1.0226e+09, device='cuda:0')
c= tensor(1.0228e+09, device='cuda:0')
c= tensor(1.0228e+09, device='cuda:0')
c= tensor(1.0280e+09, device='cuda:0')
c= tensor(1.0284e+09, device='cuda:0')
c= tensor(1.0285e+09, device='cuda:0')
c= tensor(1.0285e+09, device='cuda:0')
c= tensor(1.0285e+09, device='cuda:0')
c= tensor(1.0285e+09, device='cuda:0')
c= tensor(1.0285e+09, device='cuda:0')
c= tensor(1.0286e+09, device='cuda:0')
c= tensor(1.0286e+09, device='cuda:0')
c= tensor(2.4928e+09, device='cuda:0')
c= tensor(2.4928e+09, device='cuda:0')
c= tensor(2.4929e+09, device='cuda:0')
c= tensor(2.4929e+09, device='cuda:0')
c= tensor(2.4929e+09, device='cuda:0')
c= tensor(2.4929e+09, device='cuda:0')
c= tensor(2.4985e+09, device='cuda:0')
c= tensor(2.4986e+09, device='cuda:0')
c= tensor(2.5174e+09, device='cuda:0')
c= tensor(2.5174e+09, device='cuda:0')
c= tensor(2.5237e+09, device='cuda:0')
c= tensor(2.5242e+09, device='cuda:0')
c= tensor(2.5258e+09, device='cuda:0')
c= tensor(2.5392e+09, device='cuda:0')
c= tensor(2.5392e+09, device='cuda:0')
c= tensor(2.5392e+09, device='cuda:0')
c= tensor(2.5394e+09, device='cuda:0')
c= tensor(2.5394e+09, device='cuda:0')
c= tensor(2.5395e+09, device='cuda:0')
c= tensor(2.5396e+09, device='cuda:0')
c= tensor(2.5396e+09, device='cuda:0')
c= tensor(2.5398e+09, device='cuda:0')
c= tensor(2.5398e+09, device='cuda:0')
c= tensor(2.5415e+09, device='cuda:0')
c= tensor(2.5448e+09, device='cuda:0')
c= tensor(2.5448e+09, device='cuda:0')
c= tensor(2.5448e+09, device='cuda:0')
c= tensor(2.5468e+09, device='cuda:0')
c= tensor(2.5468e+09, device='cuda:0')
c= tensor(2.5473e+09, device='cuda:0')
c= tensor(2.5473e+09, device='cuda:0')
c= tensor(2.5477e+09, device='cuda:0')
c= tensor(2.5478e+09, device='cuda:0')
c= tensor(2.5487e+09, device='cuda:0')
c= tensor(2.5496e+09, device='cuda:0')
c= tensor(2.5496e+09, device='cuda:0')
c= tensor(2.5496e+09, device='cuda:0')
c= tensor(2.5496e+09, device='cuda:0')
c= tensor(2.5500e+09, device='cuda:0')
c= tensor(2.5505e+09, device='cuda:0')
c= tensor(2.5542e+09, device='cuda:0')
c= tensor(2.5542e+09, device='cuda:0')
c= tensor(2.5543e+09, device='cuda:0')
c= tensor(2.5543e+09, device='cuda:0')
c= tensor(2.5573e+09, device='cuda:0')
c= tensor(2.5575e+09, device='cuda:0')
c= tensor(2.5579e+09, device='cuda:0')
c= tensor(2.5580e+09, device='cuda:0')
c= tensor(2.5581e+09, device='cuda:0')
c= tensor(2.5581e+09, device='cuda:0')
c= tensor(2.5581e+09, device='cuda:0')
c= tensor(2.5582e+09, device='cuda:0')
c= tensor(2.5583e+09, device='cuda:0')
c= tensor(2.5584e+09, device='cuda:0')
c= tensor(2.5584e+09, device='cuda:0')
c= tensor(2.5584e+09, device='cuda:0')
c= tensor(2.5585e+09, device='cuda:0')
c= tensor(2.5585e+09, device='cuda:0')
c= tensor(2.5585e+09, device='cuda:0')
c= tensor(2.5585e+09, device='cuda:0')
c= tensor(2.5585e+09, device='cuda:0')
c= tensor(2.5586e+09, device='cuda:0')
c= tensor(2.5586e+09, device='cuda:0')
c= tensor(2.5595e+09, device='cuda:0')
c= tensor(2.5595e+09, device='cuda:0')
c= tensor(2.5597e+09, device='cuda:0')
c= tensor(2.5597e+09, device='cuda:0')
c= tensor(2.5597e+09, device='cuda:0')
c= tensor(2.5662e+09, device='cuda:0')
c= tensor(2.5665e+09, device='cuda:0')
c= tensor(2.5672e+09, device='cuda:0')
c= tensor(2.5681e+09, device='cuda:0')
c= tensor(2.5690e+09, device='cuda:0')
c= tensor(2.5727e+09, device='cuda:0')
c= tensor(2.5744e+09, device='cuda:0')
c= tensor(2.5744e+09, device='cuda:0')
c= tensor(2.5744e+09, device='cuda:0')
c= tensor(2.5744e+09, device='cuda:0')
c= tensor(2.5753e+09, device='cuda:0')
c= tensor(2.5778e+09, device='cuda:0')
c= tensor(2.5778e+09, device='cuda:0')
c= tensor(2.5782e+09, device='cuda:0')
c= tensor(2.5782e+09, device='cuda:0')
c= tensor(2.5784e+09, device='cuda:0')
c= tensor(2.5784e+09, device='cuda:0')
c= tensor(2.5784e+09, device='cuda:0')
c= tensor(2.5886e+09, device='cuda:0')
c= tensor(2.6597e+09, device='cuda:0')
c= tensor(2.6598e+09, device='cuda:0')
c= tensor(2.6599e+09, device='cuda:0')
c= tensor(2.6599e+09, device='cuda:0')
c= tensor(2.6601e+09, device='cuda:0')
c= tensor(2.6601e+09, device='cuda:0')
c= tensor(2.6601e+09, device='cuda:0')
c= tensor(2.6602e+09, device='cuda:0')
c= tensor(2.6613e+09, device='cuda:0')
c= tensor(2.6613e+09, device='cuda:0')
c= tensor(2.6620e+09, device='cuda:0')
c= tensor(2.6620e+09, device='cuda:0')
memory (bytes)
3963539456
time for making loss 2 is 15.50769591331482
p0 True
it  0 : 1307217408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 61% |
shape of L is 
torch.Size([])
memory (bytes)
3963744256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% |  7% |
memory (bytes)
3964342272
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  13755741000.0
relative error loss 5.167448
shape of L is 
torch.Size([])
memory (bytes)
4177342464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  5% |  9% |
memory (bytes)
4177346560
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  13755668000.0
relative error loss 5.167421
shape of L is 
torch.Size([])
memory (bytes)
4181401600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  9% |
memory (bytes)
4181401600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% |  9% |
error is  13755403000.0
relative error loss 5.167321
shape of L is 
torch.Size([])
memory (bytes)
4183384064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% |  9% |
memory (bytes)
4183506944
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  13753886000.0
relative error loss 5.1667514
shape of L is 
torch.Size([])
memory (bytes)
4185661440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  9% |
memory (bytes)
4185661440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  13745511000.0
relative error loss 5.163605
shape of L is 
torch.Size([])
memory (bytes)
4187840512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  9% |
memory (bytes)
4187840512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  9% |
error is  13653763000.0
relative error loss 5.1291394
shape of L is 
torch.Size([])
memory (bytes)
4189945856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  9% |
memory (bytes)
4189945856
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  12768990000.0
relative error loss 4.7967677
shape of L is 
torch.Size([])
memory (bytes)
4192055296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% |  9% |
memory (bytes)
4192092160
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  9148475000.0
relative error loss 3.436694
shape of L is 
torch.Size([])
memory (bytes)
4194037760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% |  9% |
memory (bytes)
4194226176
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  4246800400.0
relative error loss 1.5953426
shape of L is 
torch.Size([])
memory (bytes)
4196327424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  9% |
memory (bytes)
4196327424
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  3175591000.0
relative error loss 1.1929348
time to take a step is 241.79971146583557
it  1 : 1777609216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  9% |
shape of L is 
torch.Size([])
memory (bytes)
4198232064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  9% |
memory (bytes)
4198486016
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  3175591000.0
relative error loss 1.1929348
shape of L is 
torch.Size([])
memory (bytes)
4200665088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  9% |
memory (bytes)
4200665088
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  2455980000.0
relative error loss 0.9226075
shape of L is 
torch.Size([])
memory (bytes)
4202504192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% |  9% |
memory (bytes)
4202745856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  5728816000.0
relative error loss 2.1520731
shape of L is 
torch.Size([])
memory (bytes)
4204945408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  9% |
memory (bytes)
4204945408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  2307363600.0
relative error loss 0.86677855
shape of L is 
torch.Size([])
memory (bytes)
4207083520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  9% |
memory (bytes)
4207083520
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  2142492200.0
relative error loss 0.80484337
shape of L is 
torch.Size([])
memory (bytes)
4209217536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  9% |
memory (bytes)
4209217536
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1961072600.0
relative error loss 0.7366918
shape of L is 
torch.Size([])
memory (bytes)
4211298304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  9% |
memory (bytes)
4211335168
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1991421000.0
relative error loss 0.74809235
shape of L is 
torch.Size([])
memory (bytes)
4213436416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  9% |
memory (bytes)
4213448704
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1792627000.0
relative error loss 0.6734139
shape of L is 
torch.Size([])
memory (bytes)
4215529472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% |  9% |
memory (bytes)
4215566336
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1650180700.0
relative error loss 0.61990285
shape of L is 
torch.Size([])
memory (bytes)
4217667584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  9% |
memory (bytes)
4217704448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1445584000.0
relative error loss 0.54304457
time to take a step is 245.11508011817932
it  2 : 1912007168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  9% |
shape of L is 
torch.Size([])
memory (bytes)
4219809792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  9% |
memory (bytes)
4219830272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1445584000.0
relative error loss 0.54304457
shape of L is 
torch.Size([])
memory (bytes)
4221952000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  9% |
memory (bytes)
4221952000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  9% |
error is  1297471000.0
relative error loss 0.4874048
shape of L is 
torch.Size([])
memory (bytes)
4223995904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  9% |
memory (bytes)
4224077824
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1130601700.0
relative error loss 0.4247191
shape of L is 
torch.Size([])
memory (bytes)
4226187264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  9% |
memory (bytes)
4226187264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  1031777400.0
relative error loss 0.387595
shape of L is 
torch.Size([])
memory (bytes)
4228337664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  9% |
memory (bytes)
4228337664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  9% |
error is  954269700.0
relative error loss 0.35847864
shape of L is 
torch.Size([])
memory (bytes)
4230471680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% |  9% |
memory (bytes)
4230471680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  886123500.0
relative error loss 0.332879
shape of L is 
torch.Size([])
memory (bytes)
4232572928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  9% |
memory (bytes)
4232572928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% |  9% |
error is  834433000.0
relative error loss 0.31346107
shape of L is 
torch.Size([])
memory (bytes)
4234698752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  9% |
memory (bytes)
4234735616
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  806710300.0
relative error loss 0.30304682
shape of L is 
torch.Size([])
memory (bytes)
4236832768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% |  9% |
memory (bytes)
4236832768
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  767139600.0
relative error loss 0.28818178
shape of L is 
torch.Size([])
memory (bytes)
4239032320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  9% |
memory (bytes)
4239032320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  9% |
error is  752237950.0
relative error loss 0.28258386
time to take a step is 247.93764638900757
it  3 : 1912007168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  9% |
shape of L is 
torch.Size([])
memory (bytes)
4241162240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% |  9% |
memory (bytes)
4241162240
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  752237950.0
relative error loss 0.28258386
shape of L is 
torch.Size([])
memory (bytes)
4243230720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  9% |
memory (bytes)
4243230720
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  689608960.0
relative error loss 0.2590568
shape of L is 
torch.Size([])
memory (bytes)
4245475328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
memory (bytes)
4245475328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  634584300.0
relative error loss 0.2383864
shape of L is 
torch.Size([])
memory (bytes)
4247502848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  9% |
memory (bytes)
4247502848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  9% |
error is  582306050.0
relative error loss 0.21874768
shape of L is 
torch.Size([])
memory (bytes)
4249460736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% |  9% |
memory (bytes)
4249460736
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  571720300.0
relative error loss 0.21477106
shape of L is 
torch.Size([])
memory (bytes)
4251889664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  9% |
memory (bytes)
4251926528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  9% |
error is  553010200.0
relative error loss 0.20774245
shape of L is 
torch.Size([])
memory (bytes)
4253917184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  9% |
memory (bytes)
4253917184
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  518564350.0
relative error loss 0.19480263
shape of L is 
torch.Size([])
memory (bytes)
4255969280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  9% |
memory (bytes)
4255969280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  473417730.0
relative error loss 0.17784294
shape of L is 
torch.Size([])
memory (bytes)
4258344960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  9% |
memory (bytes)
4258377728
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  443905540.0
relative error loss 0.16675647
shape of L is 
torch.Size([])
memory (bytes)
4260446208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  9% |
memory (bytes)
4260446208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  9% |
error is  407364100.0
relative error loss 0.1530294
time to take a step is 245.52129364013672
c= tensor(479.0144, device='cuda:0')
c= tensor(10196.9160, device='cuda:0')
c= tensor(11675.0791, device='cuda:0')
c= tensor(32387.5977, device='cuda:0')
c= tensor(39332.3281, device='cuda:0')
c= tensor(55504.2422, device='cuda:0')
c= tensor(542883.0625, device='cuda:0')
c= tensor(594030.3750, device='cuda:0')
c= tensor(600582.5625, device='cuda:0')
c= tensor(920888.2500, device='cuda:0')
c= tensor(925474.8750, device='cuda:0')
c= tensor(2102881.7500, device='cuda:0')
c= tensor(2107957.5000, device='cuda:0')
c= tensor(8617114., device='cuda:0')
c= tensor(8700128., device='cuda:0')
c= tensor(8746657., device='cuda:0')
c= tensor(9224757., device='cuda:0')
c= tensor(9483037., device='cuda:0')
c= tensor(12266479., device='cuda:0')
c= tensor(14802362., device='cuda:0')
c= tensor(14810020., device='cuda:0')
c= tensor(18232890., device='cuda:0')
c= tensor(18248892., device='cuda:0')
c= tensor(18512918., device='cuda:0')
c= tensor(18514302., device='cuda:0')
c= tensor(19130092., device='cuda:0')
c= tensor(19556958., device='cuda:0')
c= tensor(19560796., device='cuda:0')
c= tensor(20238304., device='cuda:0')
c= tensor(57048988., device='cuda:0')
c= tensor(57054588., device='cuda:0')
c= tensor(1.5423e+08, device='cuda:0')
c= tensor(1.5443e+08, device='cuda:0')
c= tensor(1.5444e+08, device='cuda:0')
c= tensor(1.5444e+08, device='cuda:0')
c= tensor(1.5530e+08, device='cuda:0')
c= tensor(1.5791e+08, device='cuda:0')
c= tensor(1.5791e+08, device='cuda:0')
c= tensor(1.5792e+08, device='cuda:0')
c= tensor(1.5793e+08, device='cuda:0')
c= tensor(1.5793e+08, device='cuda:0')
c= tensor(1.5793e+08, device='cuda:0')
c= tensor(1.5793e+08, device='cuda:0')
c= tensor(1.5793e+08, device='cuda:0')
c= tensor(1.5793e+08, device='cuda:0')
c= tensor(1.5793e+08, device='cuda:0')
c= tensor(1.5794e+08, device='cuda:0')
c= tensor(1.5794e+08, device='cuda:0')
c= tensor(1.5794e+08, device='cuda:0')
c= tensor(1.5798e+08, device='cuda:0')
c= tensor(1.5799e+08, device='cuda:0')
c= tensor(1.5800e+08, device='cuda:0')
c= tensor(1.5801e+08, device='cuda:0')
c= tensor(1.5801e+08, device='cuda:0')
c= tensor(1.5801e+08, device='cuda:0')
c= tensor(1.5802e+08, device='cuda:0')
c= tensor(1.5802e+08, device='cuda:0')
c= tensor(1.5803e+08, device='cuda:0')
c= tensor(1.5803e+08, device='cuda:0')
c= tensor(1.5803e+08, device='cuda:0')
c= tensor(1.5804e+08, device='cuda:0')
c= tensor(1.5804e+08, device='cuda:0')
c= tensor(1.5806e+08, device='cuda:0')
c= tensor(1.5806e+08, device='cuda:0')
c= tensor(1.5806e+08, device='cuda:0')
c= tensor(1.5806e+08, device='cuda:0')
c= tensor(1.5806e+08, device='cuda:0')
c= tensor(1.5806e+08, device='cuda:0')
c= tensor(1.5807e+08, device='cuda:0')
c= tensor(1.5807e+08, device='cuda:0')
c= tensor(1.5807e+08, device='cuda:0')
c= tensor(1.5807e+08, device='cuda:0')
c= tensor(1.5807e+08, device='cuda:0')
c= tensor(1.5808e+08, device='cuda:0')
c= tensor(1.5808e+08, device='cuda:0')
c= tensor(1.5808e+08, device='cuda:0')
c= tensor(1.5808e+08, device='cuda:0')
c= tensor(1.5809e+08, device='cuda:0')
c= tensor(1.5809e+08, device='cuda:0')
c= tensor(1.5809e+08, device='cuda:0')
c= tensor(1.5809e+08, device='cuda:0')
c= tensor(1.5809e+08, device='cuda:0')
c= tensor(1.5811e+08, device='cuda:0')
c= tensor(1.5811e+08, device='cuda:0')
c= tensor(1.5811e+08, device='cuda:0')
c= tensor(1.5811e+08, device='cuda:0')
c= tensor(1.5812e+08, device='cuda:0')
c= tensor(1.5812e+08, device='cuda:0')
c= tensor(1.5812e+08, device='cuda:0')
c= tensor(1.5812e+08, device='cuda:0')
c= tensor(1.5812e+08, device='cuda:0')
c= tensor(1.5812e+08, device='cuda:0')
c= tensor(1.5812e+08, device='cuda:0')
c= tensor(1.5812e+08, device='cuda:0')
c= tensor(1.5813e+08, device='cuda:0')
c= tensor(1.5813e+08, device='cuda:0')
c= tensor(1.5813e+08, device='cuda:0')
c= tensor(1.5815e+08, device='cuda:0')
c= tensor(1.5815e+08, device='cuda:0')
c= tensor(1.5815e+08, device='cuda:0')
c= tensor(1.5818e+08, device='cuda:0')
c= tensor(1.5818e+08, device='cuda:0')
c= tensor(1.5819e+08, device='cuda:0')
c= tensor(1.5819e+08, device='cuda:0')
c= tensor(1.5819e+08, device='cuda:0')
c= tensor(1.5819e+08, device='cuda:0')
c= tensor(1.5819e+08, device='cuda:0')
c= tensor(1.5819e+08, device='cuda:0')
c= tensor(1.5820e+08, device='cuda:0')
c= tensor(1.5820e+08, device='cuda:0')
c= tensor(1.5820e+08, device='cuda:0')
c= tensor(1.5820e+08, device='cuda:0')
c= tensor(1.5820e+08, device='cuda:0')
c= tensor(1.5820e+08, device='cuda:0')
c= tensor(1.5821e+08, device='cuda:0')
c= tensor(1.5821e+08, device='cuda:0')
c= tensor(1.5821e+08, device='cuda:0')
c= tensor(1.5821e+08, device='cuda:0')
c= tensor(1.5821e+08, device='cuda:0')
c= tensor(1.5821e+08, device='cuda:0')
c= tensor(1.5822e+08, device='cuda:0')
c= tensor(1.5822e+08, device='cuda:0')
c= tensor(1.5822e+08, device='cuda:0')
c= tensor(1.5822e+08, device='cuda:0')
c= tensor(1.5822e+08, device='cuda:0')
c= tensor(1.5823e+08, device='cuda:0')
c= tensor(1.5823e+08, device='cuda:0')
c= tensor(1.5823e+08, device='cuda:0')
c= tensor(1.5826e+08, device='cuda:0')
c= tensor(1.5827e+08, device='cuda:0')
c= tensor(1.5827e+08, device='cuda:0')
c= tensor(1.5827e+08, device='cuda:0')
c= tensor(1.5827e+08, device='cuda:0')
c= tensor(1.5827e+08, device='cuda:0')
c= tensor(1.5830e+08, device='cuda:0')
c= tensor(1.5830e+08, device='cuda:0')
c= tensor(1.5830e+08, device='cuda:0')
c= tensor(1.5830e+08, device='cuda:0')
c= tensor(1.5830e+08, device='cuda:0')
c= tensor(1.5830e+08, device='cuda:0')
c= tensor(1.5830e+08, device='cuda:0')
c= tensor(1.5830e+08, device='cuda:0')
c= tensor(1.5832e+08, device='cuda:0')
c= tensor(1.5833e+08, device='cuda:0')
c= tensor(1.5834e+08, device='cuda:0')
c= tensor(1.5835e+08, device='cuda:0')
c= tensor(1.5835e+08, device='cuda:0')
c= tensor(1.5835e+08, device='cuda:0')
c= tensor(1.5835e+08, device='cuda:0')
c= tensor(1.5835e+08, device='cuda:0')
c= tensor(1.5836e+08, device='cuda:0')
c= tensor(1.5836e+08, device='cuda:0')
c= tensor(1.5837e+08, device='cuda:0')
c= tensor(1.5837e+08, device='cuda:0')
c= tensor(1.5838e+08, device='cuda:0')
c= tensor(1.5841e+08, device='cuda:0')
c= tensor(1.5842e+08, device='cuda:0')
c= tensor(1.5842e+08, device='cuda:0')
c= tensor(1.5842e+08, device='cuda:0')
c= tensor(1.5842e+08, device='cuda:0')
c= tensor(1.5843e+08, device='cuda:0')
c= tensor(1.5843e+08, device='cuda:0')
c= tensor(1.5843e+08, device='cuda:0')
c= tensor(1.5843e+08, device='cuda:0')
c= tensor(1.5843e+08, device='cuda:0')
c= tensor(1.5844e+08, device='cuda:0')
c= tensor(1.5844e+08, device='cuda:0')
c= tensor(1.5844e+08, device='cuda:0')
c= tensor(1.5844e+08, device='cuda:0')
c= tensor(1.5844e+08, device='cuda:0')
c= tensor(1.5844e+08, device='cuda:0')
c= tensor(1.5844e+08, device='cuda:0')
c= tensor(1.5844e+08, device='cuda:0')
c= tensor(1.5845e+08, device='cuda:0')
c= tensor(1.5845e+08, device='cuda:0')
c= tensor(1.5845e+08, device='cuda:0')
c= tensor(1.5845e+08, device='cuda:0')
c= tensor(1.5846e+08, device='cuda:0')
c= tensor(1.5846e+08, device='cuda:0')
c= tensor(1.5846e+08, device='cuda:0')
c= tensor(1.5846e+08, device='cuda:0')
c= tensor(1.5846e+08, device='cuda:0')
c= tensor(1.5847e+08, device='cuda:0')
c= tensor(1.5847e+08, device='cuda:0')
c= tensor(1.5848e+08, device='cuda:0')
c= tensor(1.5848e+08, device='cuda:0')
c= tensor(1.5848e+08, device='cuda:0')
c= tensor(1.5849e+08, device='cuda:0')
c= tensor(1.5849e+08, device='cuda:0')
c= tensor(1.5851e+08, device='cuda:0')
c= tensor(1.5851e+08, device='cuda:0')
c= tensor(1.5851e+08, device='cuda:0')
c= tensor(1.5851e+08, device='cuda:0')
c= tensor(1.5852e+08, device='cuda:0')
c= tensor(1.5852e+08, device='cuda:0')
c= tensor(1.5852e+08, device='cuda:0')
c= tensor(1.5852e+08, device='cuda:0')
c= tensor(1.5852e+08, device='cuda:0')
c= tensor(1.5852e+08, device='cuda:0')
c= tensor(1.5852e+08, device='cuda:0')
c= tensor(1.5852e+08, device='cuda:0')
c= tensor(1.5852e+08, device='cuda:0')
c= tensor(1.5853e+08, device='cuda:0')
c= tensor(1.5854e+08, device='cuda:0')
c= tensor(1.5854e+08, device='cuda:0')
c= tensor(1.5854e+08, device='cuda:0')
c= tensor(1.5855e+08, device='cuda:0')
c= tensor(1.5855e+08, device='cuda:0')
c= tensor(1.5855e+08, device='cuda:0')
c= tensor(1.5856e+08, device='cuda:0')
c= tensor(1.5858e+08, device='cuda:0')
c= tensor(1.5858e+08, device='cuda:0')
c= tensor(1.5858e+08, device='cuda:0')
c= tensor(1.5858e+08, device='cuda:0')
c= tensor(1.5858e+08, device='cuda:0')
c= tensor(1.5858e+08, device='cuda:0')
c= tensor(1.5858e+08, device='cuda:0')
c= tensor(1.5858e+08, device='cuda:0')
c= tensor(1.5859e+08, device='cuda:0')
c= tensor(1.5859e+08, device='cuda:0')
c= tensor(1.5859e+08, device='cuda:0')
c= tensor(1.5859e+08, device='cuda:0')
c= tensor(1.5859e+08, device='cuda:0')
c= tensor(1.5859e+08, device='cuda:0')
c= tensor(1.5861e+08, device='cuda:0')
c= tensor(1.5861e+08, device='cuda:0')
c= tensor(1.5861e+08, device='cuda:0')
c= tensor(1.5861e+08, device='cuda:0')
c= tensor(1.5862e+08, device='cuda:0')
c= tensor(1.5862e+08, device='cuda:0')
c= tensor(1.5862e+08, device='cuda:0')
c= tensor(1.5862e+08, device='cuda:0')
c= tensor(1.5862e+08, device='cuda:0')
c= tensor(1.5863e+08, device='cuda:0')
c= tensor(1.5863e+08, device='cuda:0')
c= tensor(1.5863e+08, device='cuda:0')
c= tensor(1.5863e+08, device='cuda:0')
c= tensor(1.5863e+08, device='cuda:0')
c= tensor(1.5863e+08, device='cuda:0')
c= tensor(1.5865e+08, device='cuda:0')
c= tensor(1.5935e+08, device='cuda:0')
c= tensor(1.5936e+08, device='cuda:0')
c= tensor(1.5937e+08, device='cuda:0')
c= tensor(1.5937e+08, device='cuda:0')
c= tensor(1.5937e+08, device='cuda:0')
c= tensor(1.6247e+08, device='cuda:0')
c= tensor(1.7056e+08, device='cuda:0')
c= tensor(1.7056e+08, device='cuda:0')
c= tensor(1.7076e+08, device='cuda:0')
c= tensor(1.7117e+08, device='cuda:0')
c= tensor(1.7117e+08, device='cuda:0')
c= tensor(1.7760e+08, device='cuda:0')
c= tensor(1.7760e+08, device='cuda:0')
c= tensor(1.7761e+08, device='cuda:0')
c= tensor(1.7825e+08, device='cuda:0')
c= tensor(1.8479e+08, device='cuda:0')
c= tensor(1.8479e+08, device='cuda:0')
c= tensor(1.8483e+08, device='cuda:0')
c= tensor(1.8486e+08, device='cuda:0')
c= tensor(1.8585e+08, device='cuda:0')
c= tensor(1.8617e+08, device='cuda:0')
c= tensor(1.8635e+08, device='cuda:0')
c= tensor(1.8645e+08, device='cuda:0')
c= tensor(1.8654e+08, device='cuda:0')
c= tensor(1.8654e+08, device='cuda:0')
c= tensor(1.9060e+08, device='cuda:0')
c= tensor(1.9060e+08, device='cuda:0')
c= tensor(1.9060e+08, device='cuda:0')
c= tensor(1.9064e+08, device='cuda:0')
c= tensor(1.9090e+08, device='cuda:0')
c= tensor(1.9456e+08, device='cuda:0')
c= tensor(1.9498e+08, device='cuda:0')
c= tensor(1.9499e+08, device='cuda:0')
c= tensor(1.9500e+08, device='cuda:0')
c= tensor(1.9501e+08, device='cuda:0')
c= tensor(1.9522e+08, device='cuda:0')
c= tensor(1.9636e+08, device='cuda:0')
c= tensor(1.9654e+08, device='cuda:0')
c= tensor(1.9657e+08, device='cuda:0')
c= tensor(1.9657e+08, device='cuda:0')
c= tensor(1.9657e+08, device='cuda:0')
c= tensor(1.9691e+08, device='cuda:0')
c= tensor(1.9718e+08, device='cuda:0')
c= tensor(1.9728e+08, device='cuda:0')
c= tensor(1.9729e+08, device='cuda:0')
c= tensor(2.0979e+08, device='cuda:0')
c= tensor(2.0980e+08, device='cuda:0')
c= tensor(2.0986e+08, device='cuda:0')
c= tensor(2.1093e+08, device='cuda:0')
c= tensor(2.1094e+08, device='cuda:0')
c= tensor(2.1122e+08, device='cuda:0')
c= tensor(2.1841e+08, device='cuda:0')
c= tensor(2.2471e+08, device='cuda:0')
c= tensor(2.2472e+08, device='cuda:0')
c= tensor(2.2472e+08, device='cuda:0')
c= tensor(2.2472e+08, device='cuda:0')
c= tensor(2.2473e+08, device='cuda:0')
c= tensor(2.2493e+08, device='cuda:0')
c= tensor(2.2494e+08, device='cuda:0')
c= tensor(2.2519e+08, device='cuda:0')
c= tensor(2.3330e+08, device='cuda:0')
c= tensor(2.3336e+08, device='cuda:0')
c= tensor(2.3339e+08, device='cuda:0')
c= tensor(2.3339e+08, device='cuda:0')
c= tensor(2.3676e+08, device='cuda:0')
c= tensor(2.3735e+08, device='cuda:0')
c= tensor(2.3740e+08, device='cuda:0')
c= tensor(2.3742e+08, device='cuda:0')
c= tensor(2.6556e+08, device='cuda:0')
c= tensor(2.6557e+08, device='cuda:0')
c= tensor(2.6754e+08, device='cuda:0')
c= tensor(2.6755e+08, device='cuda:0')
c= tensor(2.6857e+08, device='cuda:0')
c= tensor(2.6864e+08, device='cuda:0')
c= tensor(2.7871e+08, device='cuda:0')
c= tensor(2.7893e+08, device='cuda:0')
c= tensor(2.7893e+08, device='cuda:0')
c= tensor(2.8013e+08, device='cuda:0')
c= tensor(2.8092e+08, device='cuda:0')
c= tensor(2.8093e+08, device='cuda:0')
c= tensor(2.8113e+08, device='cuda:0')
c= tensor(2.8202e+08, device='cuda:0')
c= tensor(2.9367e+08, device='cuda:0')
c= tensor(2.9381e+08, device='cuda:0')
c= tensor(2.9381e+08, device='cuda:0')
c= tensor(2.9382e+08, device='cuda:0')
c= tensor(2.9386e+08, device='cuda:0')
c= tensor(2.9388e+08, device='cuda:0')
c= tensor(2.9390e+08, device='cuda:0')
c= tensor(2.9390e+08, device='cuda:0')
c= tensor(2.9456e+08, device='cuda:0')
c= tensor(2.9526e+08, device='cuda:0')
c= tensor(2.9526e+08, device='cuda:0')
c= tensor(2.9527e+08, device='cuda:0')
c= tensor(2.9528e+08, device='cuda:0')
c= tensor(2.9530e+08, device='cuda:0')
c= tensor(2.9531e+08, device='cuda:0')
c= tensor(2.9532e+08, device='cuda:0')
c= tensor(2.9533e+08, device='cuda:0')
c= tensor(2.9541e+08, device='cuda:0')
c= tensor(2.9547e+08, device='cuda:0')
c= tensor(2.9549e+08, device='cuda:0')
c= tensor(2.9580e+08, device='cuda:0')
c= tensor(2.9580e+08, device='cuda:0')
c= tensor(3.6989e+08, device='cuda:0')
c= tensor(3.6989e+08, device='cuda:0')
c= tensor(3.7024e+08, device='cuda:0')
c= tensor(3.7024e+08, device='cuda:0')
c= tensor(3.7024e+08, device='cuda:0')
c= tensor(3.7024e+08, device='cuda:0')
c= tensor(3.7029e+08, device='cuda:0')
c= tensor(3.7029e+08, device='cuda:0')
c= tensor(3.7141e+08, device='cuda:0')
c= tensor(3.7142e+08, device='cuda:0')
c= tensor(3.7142e+08, device='cuda:0')
c= tensor(3.7907e+08, device='cuda:0')
c= tensor(3.7937e+08, device='cuda:0')
c= tensor(3.7970e+08, device='cuda:0')
c= tensor(3.8216e+08, device='cuda:0')
c= tensor(3.8501e+08, device='cuda:0')
c= tensor(3.8501e+08, device='cuda:0')
c= tensor(3.8501e+08, device='cuda:0')
c= tensor(3.8510e+08, device='cuda:0')
c= tensor(3.8510e+08, device='cuda:0')
c= tensor(3.8510e+08, device='cuda:0')
c= tensor(3.8512e+08, device='cuda:0')
c= tensor(3.8512e+08, device='cuda:0')
c= tensor(3.8512e+08, device='cuda:0')
c= tensor(3.8512e+08, device='cuda:0')
c= tensor(3.8513e+08, device='cuda:0')
c= tensor(3.8851e+08, device='cuda:0')
c= tensor(3.8852e+08, device='cuda:0')
c= tensor(3.8888e+08, device='cuda:0')
c= tensor(3.8889e+08, device='cuda:0')
c= tensor(3.8889e+08, device='cuda:0')
c= tensor(3.8890e+08, device='cuda:0')
c= tensor(4.3724e+08, device='cuda:0')
c= tensor(4.4737e+08, device='cuda:0')
c= tensor(4.4738e+08, device='cuda:0')
c= tensor(4.4758e+08, device='cuda:0')
c= tensor(4.4758e+08, device='cuda:0')
c= tensor(4.4760e+08, device='cuda:0')
c= tensor(4.4873e+08, device='cuda:0')
c= tensor(4.4880e+08, device='cuda:0')
c= tensor(4.4918e+08, device='cuda:0')
c= tensor(4.4974e+08, device='cuda:0')
c= tensor(4.7682e+08, device='cuda:0')
c= tensor(4.7687e+08, device='cuda:0')
c= tensor(4.7687e+08, device='cuda:0')
c= tensor(4.7690e+08, device='cuda:0')
c= tensor(4.7691e+08, device='cuda:0')
c= tensor(4.7691e+08, device='cuda:0')
c= tensor(4.7810e+08, device='cuda:0')
c= tensor(4.7816e+08, device='cuda:0')
c= tensor(4.7816e+08, device='cuda:0')
c= tensor(4.7823e+08, device='cuda:0')
c= tensor(4.7824e+08, device='cuda:0')
c= tensor(4.7824e+08, device='cuda:0')
c= tensor(4.7897e+08, device='cuda:0')
c= tensor(4.8085e+08, device='cuda:0')
c= tensor(4.8175e+08, device='cuda:0')
c= tensor(4.8332e+08, device='cuda:0')
c= tensor(4.8658e+08, device='cuda:0')
c= tensor(4.8660e+08, device='cuda:0')
c= tensor(4.8661e+08, device='cuda:0')
c= tensor(4.8688e+08, device='cuda:0')
c= tensor(4.8733e+08, device='cuda:0')
c= tensor(4.8733e+08, device='cuda:0')
c= tensor(4.8785e+08, device='cuda:0')
c= tensor(5.1926e+08, device='cuda:0')
c= tensor(5.2082e+08, device='cuda:0')
c= tensor(5.2182e+08, device='cuda:0')
c= tensor(5.2387e+08, device='cuda:0')
c= tensor(5.2388e+08, device='cuda:0')
c= tensor(5.2388e+08, device='cuda:0')
c= tensor(5.2389e+08, device='cuda:0')
c= tensor(5.2430e+08, device='cuda:0')
c= tensor(5.2458e+08, device='cuda:0')
c= tensor(5.5681e+08, device='cuda:0')
c= tensor(5.5883e+08, device='cuda:0')
c= tensor(5.5946e+08, device='cuda:0')
c= tensor(5.5966e+08, device='cuda:0')
c= tensor(5.6052e+08, device='cuda:0')
c= tensor(5.6052e+08, device='cuda:0')
c= tensor(5.6053e+08, device='cuda:0')
c= tensor(5.6379e+08, device='cuda:0')
c= tensor(5.6380e+08, device='cuda:0')
c= tensor(5.6380e+08, device='cuda:0')
c= tensor(5.6387e+08, device='cuda:0')
c= tensor(5.6911e+08, device='cuda:0')
c= tensor(5.6915e+08, device='cuda:0')
c= tensor(5.7016e+08, device='cuda:0')
c= tensor(5.7016e+08, device='cuda:0')
c= tensor(5.7017e+08, device='cuda:0')
c= tensor(5.7017e+08, device='cuda:0')
c= tensor(5.7018e+08, device='cuda:0')
c= tensor(5.7050e+08, device='cuda:0')
c= tensor(5.7055e+08, device='cuda:0')
c= tensor(5.7056e+08, device='cuda:0')
c= tensor(5.7158e+08, device='cuda:0')
c= tensor(5.7158e+08, device='cuda:0')
c= tensor(5.7196e+08, device='cuda:0')
c= tensor(5.7198e+08, device='cuda:0')
c= tensor(5.7202e+08, device='cuda:0')
c= tensor(5.7203e+08, device='cuda:0')
c= tensor(5.7204e+08, device='cuda:0')
c= tensor(5.7205e+08, device='cuda:0')
c= tensor(5.7211e+08, device='cuda:0')
c= tensor(5.7326e+08, device='cuda:0')
c= tensor(5.7588e+08, device='cuda:0')
c= tensor(5.7588e+08, device='cuda:0')
c= tensor(5.7591e+08, device='cuda:0')
c= tensor(5.7924e+08, device='cuda:0')
c= tensor(5.7925e+08, device='cuda:0')
c= tensor(6.0566e+08, device='cuda:0')
c= tensor(6.0566e+08, device='cuda:0')
c= tensor(6.0588e+08, device='cuda:0')
c= tensor(6.0638e+08, device='cuda:0')
c= tensor(6.0638e+08, device='cuda:0')
c= tensor(6.0851e+08, device='cuda:0')
c= tensor(6.0866e+08, device='cuda:0')
c= tensor(6.6198e+08, device='cuda:0')
c= tensor(6.6198e+08, device='cuda:0')
c= tensor(6.6200e+08, device='cuda:0')
c= tensor(6.6201e+08, device='cuda:0')
c= tensor(6.6201e+08, device='cuda:0')
c= tensor(6.6202e+08, device='cuda:0')
c= tensor(6.6219e+08, device='cuda:0')
c= tensor(6.6223e+08, device='cuda:0')
c= tensor(6.6234e+08, device='cuda:0')
c= tensor(6.6236e+08, device='cuda:0')
c= tensor(6.6236e+08, device='cuda:0')
c= tensor(6.6238e+08, device='cuda:0')
c= tensor(6.6904e+08, device='cuda:0')
c= tensor(6.6965e+08, device='cuda:0')
c= tensor(6.7326e+08, device='cuda:0')
c= tensor(6.7332e+08, device='cuda:0')
c= tensor(6.7336e+08, device='cuda:0')
c= tensor(6.7337e+08, device='cuda:0')
c= tensor(6.7338e+08, device='cuda:0')
c= tensor(6.7708e+08, device='cuda:0')
c= tensor(6.7710e+08, device='cuda:0')
c= tensor(6.7711e+08, device='cuda:0')
c= tensor(6.7719e+08, device='cuda:0')
c= tensor(6.7758e+08, device='cuda:0')
c= tensor(6.7758e+08, device='cuda:0')
c= tensor(6.7758e+08, device='cuda:0')
c= tensor(7.0247e+08, device='cuda:0')
c= tensor(7.0250e+08, device='cuda:0')
c= tensor(7.0284e+08, device='cuda:0')
c= tensor(7.0285e+08, device='cuda:0')
c= tensor(7.0335e+08, device='cuda:0')
c= tensor(7.1005e+08, device='cuda:0')
c= tensor(7.1270e+08, device='cuda:0')
c= tensor(7.1554e+08, device='cuda:0')
c= tensor(7.1557e+08, device='cuda:0')
c= tensor(7.1577e+08, device='cuda:0')
c= tensor(7.1587e+08, device='cuda:0')
c= tensor(7.1587e+08, device='cuda:0')
c= tensor(7.1590e+08, device='cuda:0')
c= tensor(7.1590e+08, device='cuda:0')
c= tensor(7.1605e+08, device='cuda:0')
c= tensor(7.1605e+08, device='cuda:0')
c= tensor(7.1605e+08, device='cuda:0')
c= tensor(7.1606e+08, device='cuda:0')
c= tensor(7.1607e+08, device='cuda:0')
c= tensor(7.1679e+08, device='cuda:0')
c= tensor(7.1680e+08, device='cuda:0')
c= tensor(7.1696e+08, device='cuda:0')
c= tensor(7.1709e+08, device='cuda:0')
c= tensor(7.1713e+08, device='cuda:0')
c= tensor(7.1713e+08, device='cuda:0')
c= tensor(7.1714e+08, device='cuda:0')
c= tensor(7.1716e+08, device='cuda:0')
c= tensor(7.2176e+08, device='cuda:0')
c= tensor(7.2176e+08, device='cuda:0')
c= tensor(7.2176e+08, device='cuda:0')
c= tensor(7.2177e+08, device='cuda:0')
c= tensor(7.3177e+08, device='cuda:0')
c= tensor(7.4980e+08, device='cuda:0')
c= tensor(7.4984e+08, device='cuda:0')
c= tensor(7.4985e+08, device='cuda:0')
c= tensor(7.4995e+08, device='cuda:0')
c= tensor(7.5018e+08, device='cuda:0')
c= tensor(7.5018e+08, device='cuda:0')
c= tensor(7.5022e+08, device='cuda:0')
c= tensor(7.5023e+08, device='cuda:0')
c= tensor(7.5199e+08, device='cuda:0')
c= tensor(7.5207e+08, device='cuda:0')
c= tensor(7.5712e+08, device='cuda:0')
c= tensor(7.5713e+08, device='cuda:0')
c= tensor(7.5713e+08, device='cuda:0')
c= tensor(7.5713e+08, device='cuda:0')
c= tensor(7.5717e+08, device='cuda:0')
c= tensor(7.5718e+08, device='cuda:0')
c= tensor(7.5732e+08, device='cuda:0')
c= tensor(7.6785e+08, device='cuda:0')
c= tensor(7.9175e+08, device='cuda:0')
c= tensor(7.9176e+08, device='cuda:0')
c= tensor(7.9176e+08, device='cuda:0')
c= tensor(7.9178e+08, device='cuda:0')
c= tensor(8.7935e+08, device='cuda:0')
c= tensor(8.7952e+08, device='cuda:0')
c= tensor(8.7952e+08, device='cuda:0')
c= tensor(8.7952e+08, device='cuda:0')
c= tensor(8.8025e+08, device='cuda:0')
c= tensor(8.8025e+08, device='cuda:0')
c= tensor(8.8119e+08, device='cuda:0')
c= tensor(8.8119e+08, device='cuda:0')
c= tensor(8.8121e+08, device='cuda:0')
c= tensor(8.8122e+08, device='cuda:0')
c= tensor(8.8122e+08, device='cuda:0')
c= tensor(8.8122e+08, device='cuda:0')
c= tensor(8.8364e+08, device='cuda:0')
c= tensor(8.9569e+08, device='cuda:0')
c= tensor(8.9797e+08, device='cuda:0')
c= tensor(8.9806e+08, device='cuda:0')
c= tensor(8.9816e+08, device='cuda:0')
c= tensor(8.9817e+08, device='cuda:0')
c= tensor(8.9817e+08, device='cuda:0')
c= tensor(8.9840e+08, device='cuda:0')
c= tensor(8.9857e+08, device='cuda:0')
c= tensor(9.0270e+08, device='cuda:0')
c= tensor(9.0271e+08, device='cuda:0')
c= tensor(9.3817e+08, device='cuda:0')
c= tensor(9.3818e+08, device='cuda:0')
c= tensor(9.3833e+08, device='cuda:0')
c= tensor(9.3982e+08, device='cuda:0')
c= tensor(9.3995e+08, device='cuda:0')
c= tensor(9.4178e+08, device='cuda:0')
c= tensor(9.4569e+08, device='cuda:0')
c= tensor(9.4575e+08, device='cuda:0')
c= tensor(9.4578e+08, device='cuda:0')
c= tensor(9.4579e+08, device='cuda:0')
c= tensor(9.4597e+08, device='cuda:0')
c= tensor(9.4603e+08, device='cuda:0')
c= tensor(9.4754e+08, device='cuda:0')
c= tensor(9.8434e+08, device='cuda:0')
c= tensor(9.8663e+08, device='cuda:0')
c= tensor(9.9140e+08, device='cuda:0')
c= tensor(9.9183e+08, device='cuda:0')
c= tensor(9.9198e+08, device='cuda:0')
c= tensor(9.9212e+08, device='cuda:0')
c= tensor(9.9214e+08, device='cuda:0')
c= tensor(9.9216e+08, device='cuda:0')
c= tensor(9.9352e+08, device='cuda:0')
c= tensor(9.9358e+08, device='cuda:0')
c= tensor(1.0014e+09, device='cuda:0')
c= tensor(1.0218e+09, device='cuda:0')
c= tensor(1.0218e+09, device='cuda:0')
c= tensor(1.0218e+09, device='cuda:0')
c= tensor(1.0226e+09, device='cuda:0')
c= tensor(1.0228e+09, device='cuda:0')
c= tensor(1.0228e+09, device='cuda:0')
c= tensor(1.0280e+09, device='cuda:0')
c= tensor(1.0284e+09, device='cuda:0')
c= tensor(1.0285e+09, device='cuda:0')
c= tensor(1.0285e+09, device='cuda:0')
c= tensor(1.0285e+09, device='cuda:0')
c= tensor(1.0285e+09, device='cuda:0')
c= tensor(1.0285e+09, device='cuda:0')
c= tensor(1.0286e+09, device='cuda:0')
c= tensor(1.0286e+09, device='cuda:0')
c= tensor(2.4928e+09, device='cuda:0')
c= tensor(2.4928e+09, device='cuda:0')
c= tensor(2.4929e+09, device='cuda:0')
c= tensor(2.4929e+09, device='cuda:0')
c= tensor(2.4929e+09, device='cuda:0')
c= tensor(2.4929e+09, device='cuda:0')
c= tensor(2.4985e+09, device='cuda:0')
c= tensor(2.4986e+09, device='cuda:0')
c= tensor(2.5174e+09, device='cuda:0')
c= tensor(2.5174e+09, device='cuda:0')
c= tensor(2.5237e+09, device='cuda:0')
c= tensor(2.5242e+09, device='cuda:0')
c= tensor(2.5258e+09, device='cuda:0')
c= tensor(2.5392e+09, device='cuda:0')
c= tensor(2.5392e+09, device='cuda:0')
c= tensor(2.5392e+09, device='cuda:0')
c= tensor(2.5394e+09, device='cuda:0')
c= tensor(2.5394e+09, device='cuda:0')
c= tensor(2.5395e+09, device='cuda:0')
c= tensor(2.5396e+09, device='cuda:0')
c= tensor(2.5396e+09, device='cuda:0')
c= tensor(2.5398e+09, device='cuda:0')
c= tensor(2.5398e+09, device='cuda:0')
c= tensor(2.5415e+09, device='cuda:0')
c= tensor(2.5448e+09, device='cuda:0')
c= tensor(2.5448e+09, device='cuda:0')
c= tensor(2.5448e+09, device='cuda:0')
c= tensor(2.5468e+09, device='cuda:0')
c= tensor(2.5468e+09, device='cuda:0')
c= tensor(2.5473e+09, device='cuda:0')
c= tensor(2.5473e+09, device='cuda:0')
c= tensor(2.5477e+09, device='cuda:0')
c= tensor(2.5478e+09, device='cuda:0')
c= tensor(2.5487e+09, device='cuda:0')
c= tensor(2.5496e+09, device='cuda:0')
c= tensor(2.5496e+09, device='cuda:0')
c= tensor(2.5496e+09, device='cuda:0')
c= tensor(2.5496e+09, device='cuda:0')
c= tensor(2.5500e+09, device='cuda:0')
c= tensor(2.5505e+09, device='cuda:0')
c= tensor(2.5542e+09, device='cuda:0')
c= tensor(2.5542e+09, device='cuda:0')
c= tensor(2.5543e+09, device='cuda:0')
c= tensor(2.5543e+09, device='cuda:0')
c= tensor(2.5573e+09, device='cuda:0')
c= tensor(2.5575e+09, device='cuda:0')
c= tensor(2.5579e+09, device='cuda:0')
c= tensor(2.5580e+09, device='cuda:0')
c= tensor(2.5581e+09, device='cuda:0')
c= tensor(2.5581e+09, device='cuda:0')
c= tensor(2.5581e+09, device='cuda:0')
c= tensor(2.5582e+09, device='cuda:0')
c= tensor(2.5583e+09, device='cuda:0')
c= tensor(2.5584e+09, device='cuda:0')
c= tensor(2.5584e+09, device='cuda:0')
c= tensor(2.5584e+09, device='cuda:0')
c= tensor(2.5585e+09, device='cuda:0')
c= tensor(2.5585e+09, device='cuda:0')
c= tensor(2.5585e+09, device='cuda:0')
c= tensor(2.5585e+09, device='cuda:0')
c= tensor(2.5585e+09, device='cuda:0')
c= tensor(2.5586e+09, device='cuda:0')
c= tensor(2.5586e+09, device='cuda:0')
c= tensor(2.5595e+09, device='cuda:0')
c= tensor(2.5595e+09, device='cuda:0')
c= tensor(2.5597e+09, device='cuda:0')
c= tensor(2.5597e+09, device='cuda:0')
c= tensor(2.5597e+09, device='cuda:0')
c= tensor(2.5662e+09, device='cuda:0')
c= tensor(2.5665e+09, device='cuda:0')
c= tensor(2.5672e+09, device='cuda:0')
c= tensor(2.5681e+09, device='cuda:0')
c= tensor(2.5690e+09, device='cuda:0')
c= tensor(2.5727e+09, device='cuda:0')
c= tensor(2.5744e+09, device='cuda:0')
c= tensor(2.5744e+09, device='cuda:0')
c= tensor(2.5744e+09, device='cuda:0')
c= tensor(2.5744e+09, device='cuda:0')
c= tensor(2.5753e+09, device='cuda:0')
c= tensor(2.5778e+09, device='cuda:0')
c= tensor(2.5778e+09, device='cuda:0')
c= tensor(2.5782e+09, device='cuda:0')
c= tensor(2.5782e+09, device='cuda:0')
c= tensor(2.5784e+09, device='cuda:0')
c= tensor(2.5784e+09, device='cuda:0')
c= tensor(2.5784e+09, device='cuda:0')
c= tensor(2.5886e+09, device='cuda:0')
c= tensor(2.6597e+09, device='cuda:0')
c= tensor(2.6598e+09, device='cuda:0')
c= tensor(2.6599e+09, device='cuda:0')
c= tensor(2.6599e+09, device='cuda:0')
c= tensor(2.6601e+09, device='cuda:0')
c= tensor(2.6601e+09, device='cuda:0')
c= tensor(2.6601e+09, device='cuda:0')
c= tensor(2.6602e+09, device='cuda:0')
c= tensor(2.6613e+09, device='cuda:0')
c= tensor(2.6613e+09, device='cuda:0')
c= tensor(2.6620e+09, device='cuda:0')
c= tensor(2.6620e+09, device='cuda:0')
time to make c is 11.598073244094849
time for making loss is 11.59809136390686
p0 True
it  0 : 1307505152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
shape of L is 
torch.Size([])
memory (bytes)
4262510592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4262903808
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  407364100.0
relative error loss 0.1530294
shape of L is 
torch.Size([])
memory (bytes)
4289921024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  9% |
memory (bytes)
4289921024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  399099400.0
relative error loss 0.14992471
shape of L is 
torch.Size([])
memory (bytes)
4293488640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4293537792
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  384560900.0
relative error loss 0.14446321
shape of L is 
torch.Size([])
memory (bytes)
4296830976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4296830976
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  378347000.0
relative error loss 0.14212891
shape of L is 
torch.Size([])
memory (bytes)
4299960320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4299960320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  9% |
error is  370841600.0
relative error loss 0.13930945
shape of L is 
torch.Size([])
memory (bytes)
4303200256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
memory (bytes)
4303265792
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  364994800.0
relative error loss 0.13711306
shape of L is 
torch.Size([])
memory (bytes)
4306395136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% |  9% |
memory (bytes)
4306477056
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  360971260.0
relative error loss 0.13560158
shape of L is 
torch.Size([])
memory (bytes)
4309602304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4309688320
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  358718200.0
relative error loss 0.13475521
shape of L is 
torch.Size([])
memory (bytes)
4312907776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% |  9% |
memory (bytes)
4312907776
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  356176640.0
relative error loss 0.13380045
shape of L is 
torch.Size([])
memory (bytes)
4315979776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
memory (bytes)
4315979776
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  354262000.0
relative error loss 0.13308121
time to take a step is 301.358948469162
it  1 : 1914880000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
shape of L is 
torch.Size([])
memory (bytes)
4319186944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4319338496
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  354262000.0
relative error loss 0.13308121
shape of L is 
torch.Size([])
memory (bytes)
4322549760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% |  9% |
memory (bytes)
4322549760
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  352296450.0
relative error loss 0.13234283
shape of L is 
torch.Size([])
memory (bytes)
4325695488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% |  9% |
memory (bytes)
4325777408
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  350745100.0
relative error loss 0.13176005
shape of L is 
torch.Size([])
memory (bytes)
4328783872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
memory (bytes)
4328992768
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  349163520.0
relative error loss 0.13116592
shape of L is 
torch.Size([])
memory (bytes)
4332052480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% |  9% |
memory (bytes)
4332052480
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  347734270.0
relative error loss 0.130629
shape of L is 
torch.Size([])
memory (bytes)
4335452160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% |  9% |
memory (bytes)
4335452160
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  346817800.0
relative error loss 0.13028473
shape of L is 
torch.Size([])
memory (bytes)
4338671616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4338671616
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  345944830.0
relative error loss 0.1299568
shape of L is 
torch.Size([])
memory (bytes)
4341878784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4341886976
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  345313800.0
relative error loss 0.12971973
shape of L is 
torch.Size([])
memory (bytes)
4345053184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4345118720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  344486660.0
relative error loss 0.12940902
shape of L is 
torch.Size([])
memory (bytes)
4348342272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
memory (bytes)
4348342272
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  343939840.0
relative error loss 0.1292036
time to take a step is 296.42193627357483
it  2 : 1914880000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
shape of L is 
torch.Size([])
memory (bytes)
4351479808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
memory (bytes)
4351565824
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  343939840.0
relative error loss 0.1292036
shape of L is 
torch.Size([])
memory (bytes)
4354789376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4354789376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  343077380.0
relative error loss 0.1288796
shape of L is 
torch.Size([])
memory (bytes)
4357873664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4357873664
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  342585100.0
relative error loss 0.12869468
shape of L is 
torch.Size([])
memory (bytes)
4361154560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  9% |
memory (bytes)
4361236480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  341972220.0
relative error loss 0.12846445
shape of L is 
torch.Size([])
memory (bytes)
4364431360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
memory (bytes)
4364431360
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  341406980.0
relative error loss 0.12825212
shape of L is 
torch.Size([])
memory (bytes)
4367601664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  9% |
memory (bytes)
4367601664
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  340564740.0
relative error loss 0.12793572
shape of L is 
torch.Size([])
memory (bytes)
4370886656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  9% |
memory (bytes)
4370890752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  340149250.0
relative error loss 0.12777963
shape of L is 
torch.Size([])
memory (bytes)
4374036480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% |  9% |
memory (bytes)
4374036480
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  339685900.0
relative error loss 0.12760557
shape of L is 
torch.Size([])
memory (bytes)
4377251840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
memory (bytes)
4377333760
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  339075070.0
relative error loss 0.12737611
shape of L is 
torch.Size([])
memory (bytes)
4380483584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4380557312
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  338575870.0
relative error loss 0.1271886
time to take a step is 296.2050576210022
it  3 : 1914880000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
shape of L is 
torch.Size([])
memory (bytes)
4383657984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4383772672
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  338575870.0
relative error loss 0.1271886
shape of L is 
torch.Size([])
memory (bytes)
4386914304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  9% |
memory (bytes)
4386992128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  338282000.0
relative error loss 0.12707819
shape of L is 
torch.Size([])
memory (bytes)
4390055936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4390223872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  9% |
error is  337930240.0
relative error loss 0.12694605
shape of L is 
torch.Size([])
memory (bytes)
4393431040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% |  9% |
memory (bytes)
4393431040
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  337410050.0
relative error loss 0.12675063
shape of L is 
torch.Size([])
memory (bytes)
4396572672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4396654592
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  337487870.0
relative error loss 0.12677987
shape of L is 
torch.Size([])
memory (bytes)
4399874048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4399874048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  337113340.0
relative error loss 0.12663917
shape of L is 
torch.Size([])
memory (bytes)
4403077120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% |  9% |
memory (bytes)
4403105792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% |  9% |
error is  336848400.0
relative error loss 0.12653965
shape of L is 
torch.Size([])
memory (bytes)
4406231040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% |  9% |
memory (bytes)
4406231040
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  336543740.0
relative error loss 0.1264252
shape of L is 
torch.Size([])
memory (bytes)
4409528320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% |  9% |
memory (bytes)
4409528320
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  336220670.0
relative error loss 0.12630384
shape of L is 
torch.Size([])
memory (bytes)
4412764160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4412764160
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  335891200.0
relative error loss 0.12618007
time to take a step is 305.236266374588
it  4 : 1914880000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  9% |
shape of L is 
torch.Size([])
memory (bytes)
4415897600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% |  9% |
memory (bytes)
4415979520
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  335891200.0
relative error loss 0.12618007
shape of L is 
torch.Size([])
memory (bytes)
4419035136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4419182592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  335690000.0
relative error loss 0.12610447
shape of L is 
torch.Size([])
memory (bytes)
4422410240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% |  9% |
memory (bytes)
4422410240
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  335477000.0
relative error loss 0.12602447
shape of L is 
torch.Size([])
memory (bytes)
4425629696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  9% |
memory (bytes)
4425629696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  335262720.0
relative error loss 0.12594397
shape of L is 
torch.Size([])
memory (bytes)
4428845056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% |  9% |
memory (bytes)
4428857344
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  335003400.0
relative error loss 0.12584656
shape of L is 
torch.Size([])
memory (bytes)
4432019456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  9% |
memory (bytes)
4432019456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  334862600.0
relative error loss 0.12579367
shape of L is 
torch.Size([])
memory (bytes)
4435312640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4435312640
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  334726900.0
relative error loss 0.12574269
shape of L is 
torch.Size([])
memory (bytes)
4438454272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  9% |
memory (bytes)
4438454272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  9% |
error is  334509060.0
relative error loss 0.12566085
shape of L is 
torch.Size([])
memory (bytes)
4441755648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  9% |
memory (bytes)
4441755648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  334325500.0
relative error loss 0.1255919
shape of L is 
torch.Size([])
memory (bytes)
4444889088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% |  9% |
memory (bytes)
4444889088
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  334224130.0
relative error loss 0.12555382
time to take a step is 311.33938574790955
it  5 : 1914880000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
shape of L is 
torch.Size([])
memory (bytes)
4448206848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
memory (bytes)
4448206848
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  334224130.0
relative error loss 0.12555382
shape of L is 
torch.Size([])
memory (bytes)
4451323904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  9% |
memory (bytes)
4451323904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  334115330.0
relative error loss 0.12551294
shape of L is 
torch.Size([])
memory (bytes)
4454637568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
memory (bytes)
4454637568
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  333906180.0
relative error loss 0.12543438
shape of L is 
torch.Size([])
memory (bytes)
4457848832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% |  9% |
memory (bytes)
4457848832
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  333749760.0
relative error loss 0.12537563
shape of L is 
torch.Size([])
memory (bytes)
4460994560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  9% |
memory (bytes)
4461080576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  333578000.0
relative error loss 0.12531109
shape of L is 
torch.Size([])
memory (bytes)
4464287744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  9% |
memory (bytes)
4464291840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% |  9% |
error is  333416450.0
relative error loss 0.12525041
shape of L is 
torch.Size([])
memory (bytes)
4467503104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  9% |
memory (bytes)
4467503104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  333256450.0
relative error loss 0.1251903
shape of L is 
torch.Size([])
memory (bytes)
4470726656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% |  9% |
memory (bytes)
4470730752
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  333231600.0
relative error loss 0.12518097
shape of L is 
torch.Size([])
memory (bytes)
4473774080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  9% |
memory (bytes)
4473946112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  333119500.0
relative error loss 0.12513885
shape of L is 
torch.Size([])
memory (bytes)
4477173760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
memory (bytes)
4477173760
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  332940300.0
relative error loss 0.12507154
time to take a step is 303.03374099731445
it  6 : 1914880000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
shape of L is 
torch.Size([])
memory (bytes)
4480331776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
memory (bytes)
4480331776
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  332940300.0
relative error loss 0.12507154
shape of L is 
torch.Size([])
memory (bytes)
4483608576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  9% |
memory (bytes)
4483608576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  332769540.0
relative error loss 0.12500739
shape of L is 
torch.Size([])
memory (bytes)
4486754304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% |  9% |
memory (bytes)
4486754304
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  332580100.0
relative error loss 0.12493623
shape of L is 
torch.Size([])
memory (bytes)
4490059776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  9% |
memory (bytes)
4490059776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  332437000.0
relative error loss 0.12488247
shape of L is 
torch.Size([])
memory (bytes)
4493221888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4493275136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  332285440.0
relative error loss 0.12482554
shape of L is 
torch.Size([])
memory (bytes)
4496494592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% |  9% |
memory (bytes)
4496494592
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  332165630.0
relative error loss 0.12478053
shape of L is 
torch.Size([])
memory (bytes)
4499709952
| ID | GPU | MEM |
------------------
|  0 |  3% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4499709952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  332035600.0
relative error loss 0.124731675
shape of L is 
torch.Size([])
memory (bytes)
4502835200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
memory (bytes)
4502929408
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  332105730.0
relative error loss 0.12475803
shape of L is 
torch.Size([])
memory (bytes)
4506144768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% |  9% |
memory (bytes)
4506144768
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  331918600.0
relative error loss 0.12468773
shape of L is 
torch.Size([])
memory (bytes)
4509331456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4509331456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  331756300.0
relative error loss 0.124626756
time to take a step is 297.47731494903564
it  7 : 1914880000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
shape of L is 
torch.Size([])
memory (bytes)
4512583680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4512583680
| ID | GPU | MEM |
------------------
|  0 |  2% |  0% |
|  1 | 99% |  9% |
error is  331756300.0
relative error loss 0.124626756
shape of L is 
torch.Size([])
memory (bytes)
4515745792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  9% |
memory (bytes)
4515745792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  331656450.0
relative error loss 0.12458925
shape of L is 
torch.Size([])
memory (bytes)
4518862848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4519026688
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  331530240.0
relative error loss 0.12454184
shape of L is 
torch.Size([])
memory (bytes)
4522250240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% |  9% |
memory (bytes)
4522250240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% |  9% |
error is  331405570.0
relative error loss 0.12449501
shape of L is 
torch.Size([])
memory (bytes)
4525416448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% |  9% |
memory (bytes)
4525416448
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  331283460.0
relative error loss 0.124449134
shape of L is 
torch.Size([])
memory (bytes)
4528685056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  9% |
memory (bytes)
4528685056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  9% |
error is  331201540.0
relative error loss 0.12441836
shape of L is 
torch.Size([])
memory (bytes)
4531908608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  9% |
memory (bytes)
4531908608
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  331130880.0
relative error loss 0.12439182
shape of L is 
torch.Size([])
memory (bytes)
4535132160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4535132160
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  331057920.0
relative error loss 0.12436441
shape of L is 
torch.Size([])
memory (bytes)
4538347520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% |  9% |
memory (bytes)
4538347520
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  331010050.0
relative error loss 0.12434643
shape of L is 
torch.Size([])
memory (bytes)
4541558784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4541558784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% |  9% |
error is  330969340.0
relative error loss 0.12433114
time to take a step is 299.07466411590576
it  8 : 1914880000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  9% |
shape of L is 
torch.Size([])
memory (bytes)
4544782336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% |  9% |
memory (bytes)
4544782336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  330969340.0
relative error loss 0.12433114
shape of L is 
torch.Size([])
memory (bytes)
4547956736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  9% |
memory (bytes)
4547956736
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  330932480.0
relative error loss 0.12431729
shape of L is 
torch.Size([])
memory (bytes)
4551131136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  9% |
memory (bytes)
4551241728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% |  9% |
error is  330906370.0
relative error loss 0.12430748
shape of L is 
torch.Size([])
memory (bytes)
4554375168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  9% |
memory (bytes)
4554457088
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  330863870.0
relative error loss 0.12429152
shape of L is 
torch.Size([])
memory (bytes)
4557639680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4557639680
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  330821380.0
relative error loss 0.12427555
shape of L is 
torch.Size([])
memory (bytes)
4560891904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4560891904
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  330768400.0
relative error loss 0.12425564
shape of L is 
torch.Size([])
memory (bytes)
4563951616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% |  9% |
memory (bytes)
4564115456
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  330684930.0
relative error loss 0.1242243
shape of L is 
torch.Size([])
memory (bytes)
4567339008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
memory (bytes)
4567339008
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  330625020.0
relative error loss 0.12420179
shape of L is 
torch.Size([])
memory (bytes)
4570566656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  9% |
memory (bytes)
4570566656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  9% |
error is  330536450.0
relative error loss 0.124168515
shape of L is 
torch.Size([])
memory (bytes)
4573622272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% |  9% |
memory (bytes)
4573786112
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  330470140.0
relative error loss 0.12414361
time to take a step is 305.8315842151642
it  9 : 1914880000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
shape of L is 
torch.Size([])
memory (bytes)
4577009664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4577009664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  330470140.0
relative error loss 0.12414361
shape of L is 
torch.Size([])
memory (bytes)
4580225024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% |  9% |
memory (bytes)
4580225024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% |  9% |
error is  330407680.0
relative error loss 0.124120146
shape of L is 
torch.Size([])
memory (bytes)
4583440384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  9% |
memory (bytes)
4583440384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  330427400.0
relative error loss 0.12412755
shape of L is 
torch.Size([])
memory (bytes)
4586508288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  9% |
memory (bytes)
4586672128
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  330367230.0
relative error loss 0.12410495
shape of L is 
torch.Size([])
memory (bytes)
4589895680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% |  9% |
memory (bytes)
4589895680
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  330311940.0
relative error loss 0.124084175
shape of L is 
torch.Size([])
memory (bytes)
4593016832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  9% |
memory (bytes)
4593016832
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  330266100.0
relative error loss 0.124066964
shape of L is 
torch.Size([])
memory (bytes)
4596322304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4596322304
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  330223100.0
relative error loss 0.1240508
shape of L is 
torch.Size([])
memory (bytes)
4599451648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% |  9% |
memory (bytes)
4599451648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% |  9% |
error is  330176260.0
relative error loss 0.124033205
shape of L is 
torch.Size([])
memory (bytes)
4602683392
| ID | GPU | MEM |
------------------
|  0 | 15% |  0% |
|  1 | 28% |  9% |
memory (bytes)
4602683392
| ID | GPU  | MEM |
-------------------
|  0 |  13% |  0% |
|  1 | 100% |  9% |
error is  330120450.0
relative error loss 0.12401224
shape of L is 
torch.Size([])
memory (bytes)
4605988864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% |  9% |
memory (bytes)
4605988864
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  330078460.0
relative error loss 0.123996474
time to take a step is 297.5273997783661
it  10 : 1914880000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
shape of L is 
torch.Size([])
memory (bytes)
4609126400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4609126400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  330078460.0
relative error loss 0.123996474
shape of L is 
torch.Size([])
memory (bytes)
4612423680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  9% |
memory (bytes)
4612423680
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  330022900.0
relative error loss 0.123975605
shape of L is 
torch.Size([])
memory (bytes)
4615602176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4615602176
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  329963780.0
relative error loss 0.12395339
shape of L is 
torch.Size([])
memory (bytes)
4618866688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4618866688
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  329910530.0
relative error loss 0.12393338
shape of L is 
torch.Size([])
memory (bytes)
4622086144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% |  9% |
memory (bytes)
4622094336
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  329866750.0
relative error loss 0.12391694
shape of L is 
torch.Size([])
memory (bytes)
4625223680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% |  9% |
memory (bytes)
4625223680
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  329768450.0
relative error loss 0.123880014
shape of L is 
torch.Size([])
memory (bytes)
4628541440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4628541440
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  329733900.0
relative error loss 0.12386703
shape of L is 
torch.Size([])
memory (bytes)
4631646208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% |  9% |
memory (bytes)
4631752704
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  329681660.0
relative error loss 0.12384741
shape of L is 
torch.Size([])
memory (bytes)
4634980352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4634980352
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  329623550.0
relative error loss 0.12382558
shape of L is 
torch.Size([])
memory (bytes)
4638154752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  9% |
memory (bytes)
4638154752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  329549570.0
relative error loss 0.12379779
time to take a step is 295.7427816390991
it  11 : 1914880000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
shape of L is 
torch.Size([])
memory (bytes)
4641415168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
memory (bytes)
4641415168
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  329549570.0
relative error loss 0.12379779
shape of L is 
torch.Size([])
memory (bytes)
4644556800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
memory (bytes)
4644556800
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  329496060.0
relative error loss 0.12377769
shape of L is 
torch.Size([])
memory (bytes)
4647768064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  9% |
memory (bytes)
4647849984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 89% |  9% |
error is  329458940.0
relative error loss 0.12376375
shape of L is 
torch.Size([])
memory (bytes)
4651077632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4651077632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  329359600.0
relative error loss 0.12372643
shape of L is 
torch.Size([])
memory (bytes)
4654219264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% |  9% |
memory (bytes)
4654219264
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  329281540.0
relative error loss 0.1236971
shape of L is 
torch.Size([])
memory (bytes)
4657516544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
memory (bytes)
4657516544
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  329229060.0
relative error loss 0.12367739
shape of L is 
torch.Size([])
memory (bytes)
4660580352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% |  9% |
memory (bytes)
4660731904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% |  9% |
error is  329170940.0
relative error loss 0.12365556
shape of L is 
torch.Size([])
memory (bytes)
4663955456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  9% |
memory (bytes)
4663955456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  329102600.0
relative error loss 0.123629875
shape of L is 
torch.Size([])
memory (bytes)
4667088896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% |  9% |
memory (bytes)
4667170816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% |  9% |
error is  329096700.0
relative error loss 0.12362766
shape of L is 
torch.Size([])
memory (bytes)
4670304256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4670390272
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  329057540.0
relative error loss 0.123612955
time to take a step is 296.38075280189514
it  12 : 1914880512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
shape of L is 
torch.Size([])
memory (bytes)
4673617920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  9% |
memory (bytes)
4673617920
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  329057540.0
relative error loss 0.123612955
shape of L is 
torch.Size([])
memory (bytes)
4676747264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
memory (bytes)
4676747264
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  328999680.0
relative error loss 0.123591214
shape of L is 
torch.Size([])
memory (bytes)
4680060928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% |  9% |
memory (bytes)
4680060928
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  328957200.0
relative error loss 0.123575255
shape of L is 
torch.Size([])
memory (bytes)
4683284480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% |  9% |
memory (bytes)
4683284480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% |  9% |
error is  328891400.0
relative error loss 0.12355054
shape of L is 
torch.Size([])
memory (bytes)
4686499840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% |  9% |
memory (bytes)
4686499840
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  328828160.0
relative error loss 0.12352678
shape of L is 
torch.Size([])
memory (bytes)
4689588224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  9% |
memory (bytes)
4689723392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  328773630.0
relative error loss 0.1235063
shape of L is 
torch.Size([])
memory (bytes)
4692819968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4692959232
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  328730880.0
relative error loss 0.123490244
shape of L is 
torch.Size([])
memory (bytes)
4696186880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4696186880
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  328696830.0
relative error loss 0.12347745
shape of L is 
torch.Size([])
memory (bytes)
4699271168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
memory (bytes)
4699410432
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  328662000.0
relative error loss 0.12346437
shape of L is 
torch.Size([])
memory (bytes)
4702625792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4702625792
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  328627200.0
relative error loss 0.12345129
time to take a step is 297.81763458251953
it  13 : 1914880000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
shape of L is 
torch.Size([])
memory (bytes)
4705775616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
memory (bytes)
4705775616
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  328627200.0
relative error loss 0.12345129
shape of L is 
torch.Size([])
memory (bytes)
4709085184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  9% |
memory (bytes)
4709085184
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  328606720.0
relative error loss 0.123443596
shape of L is 
torch.Size([])
memory (bytes)
4712230912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  9% |
memory (bytes)
4712230912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  328583680.0
relative error loss 0.123434946
shape of L is 
torch.Size([])
memory (bytes)
4715524096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4715524096
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  328559360.0
relative error loss 0.12342581
shape of L is 
torch.Size([])
memory (bytes)
4718743552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4718743552
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  328537860.0
relative error loss 0.12341773
shape of L is 
torch.Size([])
memory (bytes)
4721864704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4721963008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  328491780.0
relative error loss 0.12340042
shape of L is 
torch.Size([])
memory (bytes)
4725182464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
memory (bytes)
4725182464
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  328452350.0
relative error loss 0.12338561
shape of L is 
torch.Size([])
memory (bytes)
4728352768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4728352768
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  328424700.0
relative error loss 0.12337522
shape of L is 
torch.Size([])
memory (bytes)
4731621376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4731621376
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  328382720.0
relative error loss 0.12335945
shape of L is 
torch.Size([])
memory (bytes)
4734824448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
memory (bytes)
4734824448
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  328366340.0
relative error loss 0.123353295
time to take a step is 295.53076457977295
it  14 : 1914880000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  9% |
shape of L is 
torch.Size([])
memory (bytes)
4738064384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% |  9% |
memory (bytes)
4738064384
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  328366340.0
relative error loss 0.123353295
shape of L is 
torch.Size([])
memory (bytes)
4741283840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  9% |
memory (bytes)
4741283840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  328346100.0
relative error loss 0.1233457
shape of L is 
torch.Size([])
memory (bytes)
4744445952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
memory (bytes)
4744503296
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  328318980.0
relative error loss 0.1233355
shape of L is 
torch.Size([])
memory (bytes)
4747722752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  9% |
memory (bytes)
4747722752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  328305920.0
relative error loss 0.1233306
shape of L is 
torch.Size([])
memory (bytes)
4750868480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  9% |
memory (bytes)
4750868480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  328306700.0
relative error loss 0.12333089
shape of L is 
torch.Size([])
memory (bytes)
4754157568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
memory (bytes)
4754157568
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  328287230.0
relative error loss 0.12332358
shape of L is 
torch.Size([])
memory (bytes)
4757291008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4757291008
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  328256770.0
relative error loss 0.12331214
shape of L is 
torch.Size([])
memory (bytes)
4760588288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
memory (bytes)
4760588288
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  328233470.0
relative error loss 0.12330338
shape of L is 
torch.Size([])
memory (bytes)
4763824128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  9% |
memory (bytes)
4763824128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% |  9% |
error is  328207600.0
relative error loss 0.123293675
shape of L is 
torch.Size([])
memory (bytes)
4766949376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4767043584
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  328178430.0
relative error loss 0.12328271
time to take a step is 296.98746728897095
sum tnnu_Z after tensor(7194795., device='cuda:0')
shape of features
(5983,)
shape of features
(5983,)
number of orig particles 23931
number of new particles after remove low mass 23931
tnuZ shape should be parts x labs
torch.Size([23931, 702])
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
error without small mass is  407341100.0
relative error without small mass is  0.15302075
nnu_Z shape should be number of particles by maxV
(23931, 702)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
shape of features
(23931,)
Thu Feb 2 00:04:04 EST 2023
