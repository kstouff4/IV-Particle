Wed Feb 1 12:13:33 EST 2023
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  0% |  0% |
numbers of X: 27506034
numbers of Z: 26687
shape of features
(26687,)
shape of features
(26687,)
ZX	Vol	Parts	Cubes	Eps
Z	0.019938368234707227	26687	26.687	0.09073954954433733
X	0.0184036796352299	518	0.518	0.3287443531106375
X	0.0184698982494776	9736	9.736	0.12379258900753508
X	0.01809858237325982	1201	1.201	0.24700202691234951
X	0.018515362620598864	6697	6.697	0.1403515805379688
X	0.019758123917097147	26313	26.313	0.09089188557140164
X	0.018583985351428408	20562	20.562	0.09668471763406825
X	0.018594517605653966	42594	42.594	0.07585988205760757
X	0.018582870744519836	28263	28.263	0.08695572542629976
X	0.018520028875416476	7739	7.739	0.13375774473763727
X	0.018497222658120697	13998	13.998	0.10973542375563566
X	0.018421944174651404	4877	4.877	0.1557378717230037
X	0.01857874229747763	89752	89.752	0.05915490009710619
X	0.019435550086001216	7714	7.714	0.13607313818949593
X	0.01859514287741293	182523	182.523	0.04670470576729143
X	0.018552382120370615	19841	19.841	0.09778645629088571
X	0.018585062145460296	19355	19.355	0.0986560252038982
X	0.01858729233742393	49893	49.893	0.07195464536592691
X	0.018585414350415302	32764	32.764	0.0827799405566316
X	0.018578753932903536	110085	110.085	0.05526236648616895
X	0.018603410501913758	74643	74.643	0.06293146294901496
X	0.017989799653319517	13896	13.896	0.10898803949331061
X	0.01899933726854739	141633	141.633	0.05119073116148194
X	0.018513020301750976	10039	10.039	0.1226296835970578
X	0.01855775356770808	22331	22.331	0.09401686032765644
X	0.01895800904936511	6943	6.943	0.1397703932234596
X	0.018579548013213584	65920	65.92	0.06556508803156813
X	0.01858246794923172	37592	37.592	0.07906834589263538
X	0.018514070683752906	6187	6.187	0.1441033024683703
X	0.0185348229099784	54812	54.812	0.06966868243556221
X	0.018818676894063552	1055104	1055.104	0.02612750495338368
X	0.018509750264763376	6093	6.093	0.1448293095895506
X	0.018995688293226152	478389	478.389	0.034115987752479614
X	0.01855191789203931	13970	13.97	0.10991681505510648
X	0.01852889963825981	4883	4.883	0.15597475120507695
X	0.018452692058220296	8718	8.718	0.12839485559603278
X	0.018588441489007727	103624	103.624	0.05639763278106248
X	0.01903043925737321	70552	70.552	0.06461199652449484
X	0.017730851551695435	849	0.849	0.2753852242265836
X	0.019447961892842456	4561	4.561	0.16215786119813655
X	0.018306332676406443	1603	1.603	0.22519354761834368
X	0.018558637326922187	2319	2.319	0.2000238484100066
X	0.018187299363645763	929	0.929	0.2695157608571509
X	0.01793183030402049	565	0.565	0.3166105562957106
X	0.018254975814393646	2037	2.037	0.20771287214279854
X	0.018333632477805246	657	0.657	0.30331534613284433
X	0.017688305727965023	520	0.52	0.32401190245413397
X	0.01850207388138202	1591	1.591	0.22656009097067306
X	0.018406360453606118	2325	2.325	0.1993035289019869
X	0.018443201202492758	2956	2.956	0.18409583289614084
X	0.0183847728371535	8626	8.626	0.12869142349168314
X	0.018551839673029317	10476	10.476	0.1209847127131068
X	0.018400060157500647	2790	2.79	0.18753039996031523
X	0.01849867248727956	2926	2.926	0.1849078776629621
X	0.01809690337443721	2221	2.221	0.20122652914561054
X	0.01855691995520301	3353	3.353	0.1768849081184056
X	0.01830786787532171	3462	3.462	0.1742222301481899
X	0.01797686019332053	754	0.754	0.2878154909293303
X	0.018474242998199327	4686	4.686	0.15797491214000914
X	0.01829329606856201	2168	2.168	0.2035842744101003
X	0.0187454498368359	2039	2.039	0.20948819122862433
X	0.018577288457792218	5393	5.393	0.15102562260224028
X	0.018464237382981065	759	0.759	0.28975428443597234
X	0.01970102847921525	14210	14.21	0.11150600943967155
X	0.01852535684778531	3991	3.991	0.1668123823660024
X	0.017110234795862857	1544	1.544	0.22294764593379493
X	0.018520959962814262	1898	1.898	0.21369240897808703
X	0.017094738896255162	836	0.836	0.2734547037238305
X	0.01849739749680564	1568	1.568	0.22764329178708972
X	0.018491150375343938	2714	2.714	0.18957658500121827
X	0.018378496765520565	2378	2.378	0.19771185552324025
X	0.018420646630389836	3284	3.284	0.17767810254321093
X	0.018532523170823458	4172	4.172	0.16438546608166021
X	0.018112350989462064	1113	1.113	0.2534116366213729
X	0.018211068903531427	3674	3.674	0.17050302763679373
X	0.01845288499742059	2235	2.235	0.20211362087774093
X	0.018305616384281295	4526	4.526	0.15932720433812453
X	0.01786612985127376	680	0.68	0.2972855894337573
X	0.018020481648459687	536	0.536	0.3227507065078084
X	0.0181281004284767	1552	1.552	0.22689252793396958
X	0.018843226835152813	5274	5.274	0.15287567358119733
X	0.018157990043485114	1911	1.911	0.2118047500090499
X	0.01799743078910151	1229	1.229	0.2446543130499229
X	0.019560826679762333	4606	4.606	0.16194010770616726
X	0.018342824649533636	1202	1.202	0.24803934866123703
X	0.01836963175926679	916	0.916	0.27168666676150033
X	0.018191774114356664	509	0.509	0.3293965838475761
X	0.018463930129627694	1855	1.855	0.2151097413296399
X	0.017686494171107653	480	0.48	0.33276183150862015
X	0.019831152872965083	3856	3.856	0.17261173282217215
X	0.018489727893012373	2344	2.344	0.19906319524691193
X	0.018479250128110063	1812	1.812	0.216857967750735
X	0.01821401257775647	691	0.691	0.2976065794136916
X	0.018407791104271085	841	0.841	0.27972754070392153
X	0.018531431373388408	2731	2.731	0.18931967714295764
X	0.018320197720984893	3031	3.031	0.18215792620764623
X	0.01844045300249702	2673	2.673	0.19036665358987342
X	0.01836566613136433	1948	1.948	0.21125447181779866
X	0.018502179609626357	5368	5.368	0.15105558777908093
X	0.01855586493434356	9112	9.112	0.12675229472201224
X	0.01849272970791484	1785	1.785	0.21799888242208623
X	0.018539303810639035	11060	11.06	0.1187898766854197
X	0.018440276709114134	4149	4.149	0.16441496067956918
X	0.018467552407581474	2595	2.595	0.19234932177482983
X	0.01830123561198633	1061	1.061	0.2583776889807795
X	0.01850028179009575	4001	4.001	0.16659805649773562
X	0.018141472229355606	1280	1.28	0.24200305746646014
X	0.018524338794258615	4042	4.042	0.16610478192464723
X	0.018330157327095038	674	0.674	0.300724468042699
X	0.018323089654487906	1484	1.484	0.23112926340080614
X	0.018353228928752442	1445	1.445	0.2333179820383688
X	0.018168077726530982	1519	1.519	0.22869169942842385
X	0.018369342819798157	1301	1.301	0.24169747177073025
X	0.01818342926322823	1112	1.112	0.25381873075861955
X	0.01826549341431214	3129	3.129	0.18005642172214908
X	0.018392333089514996	2504	2.504	0.19438712638409358
X	0.018142225044375473	1298	1.298	0.24088251992987997
X	0.018230020139785013	1720	1.72	0.21966097174202628
X	0.017680127360969817	593	0.593	0.3100822442327723
X	0.019360353794551633	5534	5.534	0.1518068533128621
X	0.017599072366042703	506	0.506	0.326422249549844
X	0.018530985549448906	8253	8.253	0.1309470012823892
X	0.017994428213289428	1174	1.174	0.24840289409643487
X	0.01844285280789906	1411	1.411	0.23555935806953981
X	0.018462904576555483	2633	2.633	0.19140343256220738
X	0.018469913418090796	1792	1.792	0.217625083832195
X	0.018391135548249504	1961	1.961	0.21088400968619397
X	0.018417001919593372	1410	1.41	0.2355048959818846
X	0.01851162394944259	2325	2.325	0.1996827369490622
X	0.019726171469015643	9605	9.605	0.12711061253849018
X	0.018449480979125667	1714	1.714	0.22079595449957087
X	0.01849789012559385	3588	3.588	0.17275206749240965
X	0.01840808471907662	941	0.941	0.269446777329158
X	0.01844037962172793	2606	2.606	0.1919840498117696
X	0.01804871898864271	1350	1.35	0.23733996367947074
X	0.01834458610061074	2372	2.372	0.19775664137820753
X	0.018153008471637495	1194	1.194	0.24773161155030754
X	0.01769082275583886	874	0.874	0.27252867175361745
X	0.018357936969512035	942	0.942	0.2691065842897652
X	0.017969596563506076	1374	1.374	0.23560465383855878
X	0.01776744954975976	1097	1.097	0.2530110493193352
X	0.018477743270347163	3173	3.173	0.1799117876647329
X	0.018358597297827114	875	0.875	0.27581030011657304
X	0.0199089045404008	7346	7.346	0.13942226914318814
X	0.01853344186587571	6954	6.954	0.1386459319224957
X	0.018552875454333657	6464	6.464	0.14211392266862122
X	0.017990885696005526	882	0.882	0.2732297510312587
X	0.018490826303479653	824	0.824	0.2820609964120345
X	0.018493499688444756	2642	2.642	0.19129139326695518
X	0.01848369378038218	1443	1.443	0.23397752602127525
X	0.018392435081779124	1404	1.404	0.2357349856419789
X	0.018406997742711293	1722	1.722	0.220284160628779
X	0.01855139197222926	2955	2.955	0.1844759145952064
X	0.018506108962170326	3798	3.798	0.1695326633593469
X	0.018530492957630682	4710	4.71	0.15786603038068467
X	0.018182717457805007	2217	2.217	0.2016652329526939
X	0.01944255331854982	14459	14.459	0.11037524492608322
X	0.018387223348003835	1283	1.283	0.24290129583163575
X	0.01832319003826754	2168	2.168	0.20369510973110969
X	0.018401498187083896	1677	1.677	0.22221500108154665
X	0.017424980912692685	242	0.242	0.4160245712461943
X	0.018966231080425607	6286	6.286	0.1445003459935858
X	0.017253962866852136	722	0.722	0.288038508764752
X	0.01817497446179765	3662	3.662	0.17057622142929232
X	0.017903596153915555	742	0.742	0.28896514143541513
X	0.01847325078876526	3194	3.194	0.17950207424420941
X	0.018437889561498683	1723	1.723	0.22036467500469342
X	0.018499753437735796	1096	1.096	0.256518364339806
X	0.018525840293152362	1852	1.852	0.2154661133656126
X	0.01850091048596151	2109	2.109	0.20623965525126556
X	0.018419418173001165	415	0.415	0.3540592434281021
X	0.018236000855215095	746	0.746	0.2902219520071378
X	0.01744276670244128	810	0.81	0.27821259327746506
X	0.01854561811962295	3788	3.788	0.16980238300075468
X	0.01828691015628305	1140	1.14	0.252200045132288
X	0.018541602901957745	2088	2.088	0.20708036754383113
X	0.018815656008347913	2061	2.061	0.20900040335933698
X	0.018378913438599796	2284	2.284	0.20038932278616922
X	0.018540181690353544	2895	2.895	0.18570423412366538
X	0.018452599423940928	3236	3.236	0.17865548202538276
X	0.01828822279968612	944	0.944	0.2685755682118654
X	0.018500816224181295	3391	3.391	0.17604392106058078
X	0.018133595035820634	1943	1.943	0.21054117259256933
X	0.01838458659591324	2718	2.718	0.189118844362963
X	0.018540541608837884	1300	1.3	0.24250815905503864
X	0.018505921105656385	3622	3.622	0.17223473986934576
X	0.018444231130363326	2214	2.214	0.20271893367475968
X	0.018406012836761725	1722	1.722	0.220280231632985
X	0.018517986343494944	3026	3.026	0.1829117529805428
X	0.018524312352319296	4030	4.03	0.16626940771180193
X	0.018545495739496702	10132	10.132	0.12232477396766744
X	0.01826695291766114	1610	1.61	0.22470534921434818
X	0.017593199427221366	468	0.468	0.3349908855273885
X	0.018316655329309314	1061	1.061	0.2584502340372298
X	0.018453521211731057	1297	1.297	0.24231469780029855
X	0.018342820958181613	2313	2.313	0.1994176059125811
X	0.018465017697211327	990	0.99	0.26519877350640336
X	0.01848679648860112	2000	2.0	0.20986687136071228
X	0.018220025280284435	668	0.668	0.3010168713718979
X	0.018310430508077152	1978	1.978	0.20997008435535544
X	0.018307567593237848	611	0.611	0.3105964436176154
X	0.018520511713010182	3015	3.015	0.18314225409334964
X	0.018420045225452146	1878	1.878	0.2140581173198926
X	0.01943547253124238	11596	11.596	0.1187851906634672
X	0.0184818475512053	4592	4.592	0.15906739472765
X	0.018455310962876583	3105	3.105	0.181142317995094
X	0.017775341513823158	497	0.497	0.3294735243857378
X	0.01954071428316328	2565	2.565	0.19676643707117772
X	0.018550088650038855	2888	2.888	0.18588724898445266
X	0.01834799401461682	1081	1.081	0.25699280539786634
X	0.01982379629578389	9984	9.984	0.1256880778678836
X	0.019390583040996768	9994	9.994	0.12472413697926772
X	0.018133826023297352	1151	1.151	0.2506905945775324
X	0.018398697422567264	773	0.773	0.2876531898920686
X	0.017453435112062928	1508	1.508	0.22620030772020483
X	0.018274215256279057	591	0.591	0.3138704416252245
X	0.01803991949009535	690	0.69	0.2967985909201094
X	0.018387939683850545	375	0.375	0.3660164470089271
X	0.018514337641008207	772	0.772	0.28837899661517524
X	0.018274767500414037	6634	6.634	0.14018196101520375
X	0.018024234115789954	713	0.713	0.2934871538729358
X	0.018425306967872947	1595	1.595	0.22605702867004537
X	0.018063091925985732	1009	1.009	0.26159753174311096
X	0.018060942200221905	3697	3.697	0.16967986789984033
X	0.01825657509465227	1572	1.572	0.22645865419767364
X	0.018370195555927032	3680	3.68	0.1709052170861261
X	0.019877496164750832	4892	4.892	0.159572632592767
X	0.01843111649689156	915	0.915	0.2720884980823877
X	0.01834785189376524	941	0.941	0.26915257191801883
X	0.018342133477075646	6419	6.419	0.14190384328936356
X	0.017579490754796343	540	0.54	0.3193038659757615
X	0.01846525452631395	1007	1.007	0.26369907675425996
X	0.018036241742385512	1762	1.762	0.21712677468288186
X	0.01840026243441993	1479	1.479	0.2317138257332235
X	0.018514647467127027	3567	3.567	0.17314267087917115
X	0.018483486568146447	1877	1.877	0.21434163599423017
X	0.018103625178250773	808	0.808	0.28191492120904704
X	0.018511249274132488	4064	4.064	0.16576544732954954
X	0.01854062200586583	2743	2.743	0.18907444347474894
X	0.018223379671817454	1106	1.106	0.25446297412606156
X	0.018469761933982583	9708	9.708	0.12391118491601417
X	0.01859754617767546	36811	36.811	0.07964515162448423
X	0.018569176158285385	2092	2.092	0.2070508345544612
X	0.01847630618697193	3129	3.129	0.18074648593712928
X	0.01689500190173436	511	0.511	0.3209562089659257
X	0.018339524791502047	4069	4.069	0.16518355026556447
X	0.01855474642624344	8369	8.369	0.13039486385354712
X	0.018438856624838428	51820	51.82	0.07086180212413194
X	0.018485735823004206	2279	2.279	0.20092353281703013
X	0.01857535167025633	73267	73.267	0.06329113420062264
X	0.0185721130797161	30357	30.357	0.08489214155152733
X	0.01842042439579641	13204	13.204	0.11173726547212331
X	0.01904118582238495	51752	51.752	0.07165650430990048
X	0.018501564908095715	2315	2.315	0.19993361694691542
X	0.018549265892020547	4862	4.862	0.15625619959887488
X	0.018557083632860467	103144	103.144	0.05645320267302734
X	0.018606860555283048	296188	296.188	0.03975287544127534
X	0.01844382723385324	2921	2.921	0.1848302976203133
X	0.01858447242740778	24816	24.816	0.09081121482678801
X	0.018540757905975907	14277	14.277	0.109101362574583
X	0.01851578267963041	25037	25.037	0.09043154037252354
X	0.01990216045805337	30941	30.941	0.08632209246360617
X	0.018565710188942233	25394	25.394	0.09008659050062724
X	0.01935973233682246	15499	15.499	0.10769574794210737
X	0.01857830360030472	11419	11.419	0.11761408319267891
X	0.018337353751279768	4638	4.638	0.15812552227364035
X	0.018762724501377167	98608	98.608	0.057516782532852646
X	0.01840453591277706	2790	2.79	0.18754560411587276
X	0.01835870533366879	2255	2.255	0.2011709043885109
X	0.01852169533019086	14528	14.528	0.10843220806167496
X	0.018477904014582398	37964	37.964	0.07866114202935252
X	0.01945154507875674	260192	260.192	0.04212630698107695
X	0.018550874763094208	64320	64.32	0.066070275731605
X	0.018456246904627976	709	0.709	0.2963687343088642
X	0.018540781479888697	10547	10.547	0.12068863158626153
X	0.019411201845511348	5041	5.041	0.15673954295394513
X	0.01857220220913724	15184	15.184	0.10694465147366247
X	0.01857073142190277	43589	43.589	0.07524610676621862
X	0.019886570685012955	15123	15.123	0.10955713047804629
X	0.01849489683989303	35393	35.393	0.080546160975413
X	0.01764898381179299	547	0.547	0.318354353195893
X	0.01852481690656793	3241	3.241	0.17879620549504183
X	0.018575091782694016	46635	46.635	0.0735765910818094
X	0.018495933543858003	31682	31.682	0.0835772137712148
X	0.018452640499697817	28788	28.788	0.08622152016175261
X	0.01838318516218282	3539	3.539	0.17318624645318095
X	0.018589998385657204	210023	210.023	0.04456605221555102
X	0.0185239634854613	5442	5.442	0.15042676859216694
X	0.01855127355774563	19522	19.522	0.09831425072712639
X	0.018600288145599733	72589	72.589	0.06351597262611602
X	0.018496688795894254	1825	1.825	0.21640985811531982
X	0.01850323509036923	43061	43.061	0.07546076179332345
X	0.019799009015101034	131846	131.846	0.053152720560731226
X	0.019896989852419514	167029	167.029	0.0492037835274881
X	0.01855459445986274	17165	17.165	0.10262879260772963
X	0.018339433674723824	15453	15.453	0.10587441863868237
X	0.018118435265629923	2276	2.276	0.19967152902519952
X	0.01850653593742797	3701	3.701	0.17100232553931796
X	0.018576559062070702	53161	53.161	0.07043535540322493
X	0.018528708223800667	6983	6.983	0.13844194754023878
X	0.018533950276929514	16916	16.916	0.10309163774841332
X	0.018615754368495163	26888	26.888	0.0884655224792199
X	0.01857461645777867	14457	14.457	0.1087127690629853
X	0.01849697647825487	13523	13.523	0.11100500620015163
X	0.01850546983203613	2831	2.831	0.1869763806647327
X	0.018585829142642042	74203	74.203	0.06303573563505195
X	0.018559964360825257	16161	16.161	0.10472160809039403
X	0.01848037809149902	11115	11.115	0.1184678377111793
X	0.018568750111409408	12226	12.226	0.11494747262055557
X	0.018583970178477877	139041	139.041	0.05112875359875089
X	0.018508533889455316	8614	8.614	0.12903941445761946
X	0.018548217608456454	120163	120.163	0.05364268076081678
X	0.018535548426113775	2567	2.567	0.19328279205781643
X	0.01831620661772466	19900	19.9	0.09727341264955107
X	0.018560913252228247	7443	7.443	0.1356075201151318
X	0.018495621909175194	52896	52.896	0.07045016841822732
X	0.01857004992087612	29402	29.402	0.0857983098383208
X	0.01857553976126833	4247	4.247	0.16353839345053628
X	0.018578146556435204	77112	77.112	0.062224317112680866
X	0.018599628110607568	98694	98.694	0.05733297835846406
X	0.018488121679583674	2284	2.284	0.2007854463568345
X	0.018544615675238706	62713	62.713	0.06662237135493958
X	0.019446735862878893	124579	124.579	0.05384358376624545
X	0.01903176351953187	282299	282.299	0.04069957719982896
X	0.018576673071335485	22868	22.868	0.09330679058022287
X	0.01855889231666485	1009	1.009	0.2639694263039709
X	0.018523856356830837	3400	3.4	0.17596143531469544
X	0.018658132060409702	6744	6.744	0.1403837589504918
X	0.018526968252926317	14710	14.71	0.10799340341701634
X	0.018923838543034158	17355	17.355	0.10292673709884344
X	0.018463726385552392	1266	1.266	0.24432161098953542
X	0.018553621820070032	41618	41.618	0.07639222592688058
X	0.018591284324711972	74487	74.487	0.06296167934037443
X	0.01818497948793698	7608	7.608	0.1337049349968368
X	0.018530301593545274	3997	3.997	0.16674370474900307
X	0.01850460923905152	13209	13.209	0.1118931051462082
X	0.01985769772069615	12530	12.53	0.11658945570763928
X	0.019240826096156922	10259	10.259	0.12332171664736837
X	0.01857797475017425	9894	9.894	0.12336978865700639
X	0.01848977271092771	2136	2.136	0.20532576112548664
X	0.018766325987690576	48704	48.704	0.07276762631895584
X	0.018587035389036492	23607	23.607	0.09233997981134767
X	0.018573738004962294	12685	12.685	0.11355413356172261
X	0.01857102816832409	17532	17.532	0.10193769963679375
X	0.01842663673186465	6445	6.445	0.14193005927436114
X	0.018830398141462912	426365	426.365	0.03534755009950962
X	0.019364908054192453	4870	4.87	0.15842686415336213
X	0.019866038634632766	92703	92.703	0.0598419626846667
X	0.01826697768189247	825	0.825	0.2808046206655386
X	0.018479832554209704	1493	1.493	0.23131976442308153
X	0.01854335123097549	3755	3.755	0.17029141768324071
X	0.01859078503768855	6471	6.471	0.14215935460562326
X	0.018332942118497515	4955	4.955	0.15466646853124189
X	0.01843770739491586	26798	26.798	0.08828118997848126
X	0.01828777292325723	4024	4.024	0.16564091319807842
X	0.018359880896421373	3098	3.098	0.18096565067970222
X	0.0185917765119838	215837	215.837	0.04416365393221757
X	0.018584548053359514	23453	23.453	0.09253752226492913
X	0.018555907567621707	16643	16.643	0.10369318057569463
X	0.018905433941301886	69950	69.95	0.0646546319518914
X	0.019037122635309565	98274	98.274	0.05786121621918271
X	0.019440497389950216	5769	5.769	0.14992323490006568
X	0.018519845864726336	5242	5.242	0.15230475081389677
X	0.018538333945293726	9232	9.232	0.12616095518707443
X	0.017840583973386826	513	0.513	0.32641033432010524
X	0.018157234947380117	1691	1.691	0.22061516813645426
X	0.018555060264354765	11510	11.51	0.11725436395562812
X	0.018159477173527783	1526	1.526	0.22830544481061277
X	0.01837035896219513	1547	1.547	0.22814403045455836
X	0.018433408269653277	2650	2.65	0.19089149946854395
X	0.018381022317650923	3800	3.8	0.16912015003468372
X	0.01856515875749687	207927	207.927	0.04469537580930561
X	0.01853964087114292	6237	6.237	0.14378332563222332
X	0.01855930438846591	57370	57.37	0.06864764054678707
X	0.018484404747136095	4804	4.804	0.1566994565439555
X	0.018561724876435582	2467	2.467	0.1959520120688728
X	0.018545767709430203	16606	16.606	0.1037512315885647
X	0.01861573347407567	270308	270.308	0.04098960668492726
X	0.0186133656474537	281166	281.166	0.04045330747643236
X	0.018568050277254475	11668	11.668	0.11674992967826292
X	0.01859489675995292	18784	18.784	0.09966329252472357
X	0.018481739957201652	1252	1.252	0.24530863853900042
X	0.01845377501016405	17962	17.962	0.10090441675170463
X	0.018582869852890652	183630	183.63	0.04660040777893419
X	0.019812587215524077	18047	18.047	0.10316017202353099
X	0.018515787969593578	7657	7.657	0.13422328098300065
X	0.01855265473601495	48668	48.668	0.0725082662862283
X	0.019007959856006975	293989	293.989	0.0401360665719151
X	0.018356327367571996	25202	25.202	0.08997397940011798
X	0.01802010538202972	6037	6.037	0.14398327583418088
X	0.019957090384666493	21246	21.246	0.09793547086061609
X	0.01941695563330847	17314	17.314	0.10389498268362346
X	0.018557361512072372	2580	2.58	0.19303327282912458
X	0.018575877724028765	132744	132.744	0.05191722619409534
X	0.018101257836441578	11024	11.024	0.11797496305176455
X	0.018141703461087278	651	0.651	0.3031789364226445
X	0.01857098388595119	15175	15.175	0.10696345055490189
X	0.018585339572957695	15009	15.009	0.10738400093645911
X	0.017517932895958606	672	0.672	0.2965089874729181
X	0.018587294886566987	63486	63.486	0.06640173209032865
X	0.018583526467347164	78261	78.261	0.06192427242056282
X	0.019898836778412653	95411	95.411	0.059302965881410465
X	0.018580796999638823	115263	115.263	0.054424127226929575
X	0.018521266172752345	82004	82.004	0.06089924082522328
X	0.018399408185152667	8519	8.519	0.12926226015871928
X	0.01851439039888129	7055	7.055	0.13793383135802406
X	0.01858201480786387	33205	33.205	0.08240681056829673
X	0.018962823028403956	97957	97.957	0.05784810968227482
X	0.018497512794560907	3781	3.781	0.16976007288178332
X	0.018583548428609688	32551	32.551	0.08295733073001445
X	0.018619343420900915	130961	130.961	0.05219242308550135
X	0.01860540593105468	44618	44.618	0.07470958951690308
X	0.0185979324302806	25274	25.274	0.09028111098222114
X	0.01891831855493973	42956	42.956	0.07608275555662554
X	0.018491082378498974	7141	7.141	0.13732020194765127
X	0.018455558901876543	3021	3.021	0.18280672101540504
X	0.017665544402182615	11411	11.411	0.11568252874392475
X	0.018977553696214185	86773	86.773	0.06024928150003241
X	0.01859508919098238	45789	45.789	0.07405351923806418
X	0.01869861476628364	365968	365.968	0.03710696550712661
X	0.018799292603360468	57341	57.341	0.06895388628683471
X	0.018557330694432207	49521	49.521	0.07209559001171505
X	0.0185145982029216	13600	13.6	0.1108302878193902
X	0.018593459045610168	105744	105.744	0.05602323103540586
X	0.01779930336135895	1702	1.702	0.21868272806915506
X	0.01817356415207712	6318	6.318	0.14221764918236485
X	0.019455161883694963	162136	162.136	0.04932326280989946
X	0.01854435836191507	17618	17.618	0.10172282246324671
X	0.018251738861910685	3427	3.427	0.17463450090240307
X	0.01851266745047695	12258	12.258	0.1147316204689035
X	0.018597924450382396	130941	130.941	0.05217505817182807
X	0.019905516170398093	19426	19.426	0.10081612942082702
X	0.018587622247645152	32103	32.103	0.08334753107897185
X	0.01848621900654777	2308	2.308	0.20008019244636874
X	0.018248375242473715	5486	5.486	0.14927582033830566
X	0.01832987081984072	2354	2.354	0.19820638860384682
X	0.019372365370057876	8206	8.206	0.13315282335395692
X	0.01896220959771517	17626	17.626	0.10246567192434886
X	0.018550637308489732	33176	33.176	0.08238439132012183
X	0.018376357787434	1962	1.962	0.2107916863793247
X	0.01859140831741023	93104	93.104	0.0584495694565263
X	0.018209931178867773	2819	2.819	0.1862391908902165
X	0.019769341392251248	21252	21.252	0.09761820059147126
X	0.018430120689553277	6281	6.281	0.14316377074074266
X	0.01855839381937765	16790	16.79	0.1033942919385304
X	0.0186938066522113	3257	3.257	0.1790440924185838
X	0.018472934194714404	13668	13.668	0.11056312476391623
X	0.0185820831433595	15380	15.38	0.10650729489812334
X	0.018548659175042198	15156	15.156	0.10696523301048881
X	0.019867526037548846	45195	45.195	0.07603574403977031
X	0.01859489428073543	236257	236.257	0.04285514901206987
X	0.018517532693668133	3353	3.353	0.17675967276754917
X	0.018355072054052478	3958	3.958	0.16676059445982153
X	0.018590927407591898	44353	44.353	0.07483866265780156
X	0.01854819407221628	7551	7.551	0.1349270588518237
X	0.018828004358099903	319797	319.797	0.038902421491525285
X	0.017603554508933382	848	0.848	0.27483255313276544
X	0.018577476677814654	100529	100.529	0.0569593626062393
X	0.019048096836167133	204689	204.689	0.04531608283549252
X	0.018511820430034114	1891	1.891	0.2139205632120772
X	0.01859464020734124	134151	134.151	0.05175249455914523
X	0.018557069007747284	15031	15.031	0.10727713569582102
X	0.018605651385566822	326112	326.112	0.03849692430576199
X	0.01880843627165413	5150	5.15	0.15399802589100958
X	0.01851601073304812	16328	16.328	0.10428090960688383
X	0.01851389016542785	4030	4.03	0.16623821958278712
X	0.018396814221262856	3443	3.443	0.17482432764262912
X	0.01828827943429248	4191	4.191	0.1634124204151926
X	0.018945580739470842	18490	18.49	0.10081465549107933
X	0.018393735666056163	8464	8.464	0.12952832876885872
X	0.018913206468860553	45844	45.844	0.07444363194235046
X	0.01858293059014273	10306	10.306	0.12171422988371997
X	0.01832025262677625	2394	2.394	0.19706179633933296
X	0.018523240390060307	9102	9.102	0.12672434171913194
X	0.01900617311556904	64063	64.063	0.06669538790418249
X	0.019900045833950927	29074	29.074	0.08812851054410847
X	0.018983602473344307	154237	154.237	0.04974277625253503
X	0.01855213442954943	11275	11.275	0.11805719056588929
X	0.018539592077006894	6455	6.455	0.14214599981083018
X	0.01838601218704159	2759	2.759	0.18818223159457356
X	0.017597810034714795	3001	3.001	0.18032843346836222
X	0.018585332628200866	205576	205.576	0.04488135659824503
X	0.018583274238664987	6153	6.153	0.14454789568069026
X	0.018544722337138032	9168	9.168	0.12646836680907458
X	0.01854660707713934	26260	26.26	0.08905464631427432
X	0.018884339618017387	20266	20.266	0.0976737633469084
X	0.018392746588060036	3433	3.433	0.17498101361416515
X	0.017917639662626036	4083	4.083	0.16371952381400817
X	0.018600826256302674	185133	185.133	0.04648892093735345
X	0.01857662642621921	16827	16.827	0.10335227741521323
X	0.018502314112171014	13413	13.413	0.11131833664958639
X	0.01854043955044989	25949	25.949	0.08939909725174335
X	0.019398586323775176	61089	61.089	0.0682237659221169
X	0.01858494957029515	45157	45.157	0.07438386974869604
X	0.01982508125193163	136875	136.875	0.05251664189208539
X	0.019937925693361638	51169	51.169	0.07303948143944122
X	0.018507825816855224	2829	2.829	0.18702836848365295
X	0.01857748069374899	29683	29.683	0.08553811500518901
X	0.019391631228647378	10218	10.218	0.12380822056550658
X	0.018470424789039243	7715	7.715	0.13377665045339637
X	0.018436219542769746	13517	13.517	0.11089973774229904
X	0.018500630486623437	1875	1.875	0.2144840915038573
X	0.018552577398003908	33176	33.176	0.0823872632344292
X	0.018508916127601895	9977	9.977	0.12287409676789501
X	0.017918071670990462	3028	3.028	0.1808749675416931
X	0.018544637295299385	12009	12.009	0.11558562692894668
X	0.01851687471999289	3367	3.367	0.17651225258594694
X	0.01882350142226231	49989	49.989	0.0722118803763616
X	0.018471479460006067	4878	4.878	0.15586668366073572
X	0.0185601743987384	12613	12.613	0.11374209389763987
X	0.018330308521153135	4594	4.594	0.15860842819049892
X	0.01857726845023346	9584	9.584	0.12468427488817103
X	0.018404765385442192	2668	2.668	0.19036253818671928
X	0.018523342237300063	5052	5.052	0.15420034177959896
X	0.018588585213510977	15525	15.525	0.106187054932991
X	0.018572338744147574	74361	74.361	0.06297581426980316
X	0.01848250005537539	2944	2.944	0.18447646594929892
X	0.018088362531916816	793	0.793	0.2836016079019925
X	0.01848761967689944	4434	4.434	0.1609515276881661
X	0.01858492144889111	77060	77.06	0.06224587482681665
X	0.01991467261183216	410732	410.732	0.03646460754387892
X	0.018255319615500788	20708	20.708	0.09588494605140091
X	0.01849014272131538	4607	4.607	0.15891833894392238
X	0.01858352397739834	48545	48.545	0.07260967992272299
X	0.019396666512904205	63330	63.33	0.06740713524627145
X	0.018379948161860237	1600	1.6	0.22563584816223697
X	0.018535969960910486	7333	7.333	0.13622115051720102
X	0.018574441999315943	13226	13.226	0.1119856613550308
X	0.018933107577584686	96486	96.486	0.05811022184255561
X	0.018548399494395146	83320	83.32	0.06060647984084706
X	0.018684988700689617	117355	117.355	0.05419972469839279
X	0.01848114814673395	6326	6.326	0.1429551877597203
X	0.01852113621296632	2559	2.559	0.19343383697826497
X	0.01808388753665708	1067	1.067	0.2568674805537627
X	0.018530289821481076	37865	37.865	0.07880396756548078
X	0.018552278830972577	9026	9.026	0.1271453976980728
X	0.018565802456545176	16599	16.599	0.10380316608808703
X	0.018508426917808397	55679	55.679	0.06927226062196423
X	0.018610477990211934	269851	269.851	0.04100887278518677
X	0.018571648940632412	4686	4.686	0.15825206792796467
X	0.018357865364613	1338	1.338	0.23939882931895898
X	0.019908220682492327	13503	13.503	0.11381532063393622
X	0.018592926536123624	300025	300.025	0.03957279904188364
X	0.01857829847106039	20396	20.396	0.09693642020728366
X	0.018528023714270606	1952	1.952	0.2117303399733625
X	0.018116376235171004	1755	1.755	0.21773658319851738
X	0.018550418013240016	26956	26.956	0.08828754031447135
X	0.018486534251586785	1623	1.623	0.2249981738757765
X	0.01899475136432475	26072	26.072	0.08998147098140094
X	0.018465519672812996	1690	1.69	0.22190049785460317
X	0.018565325165946812	9795	9.795	0.12375593621106344
X	0.01856534116678825	9953	9.953	0.12309761637499304
X	0.018397217423095028	2957	2.957	0.18392196807116168
X	0.018356948988736875	2624	2.624	0.19125474707910642
X	0.018955667603216573	131915	131.915	0.052377926381927356
X	0.018942279407382048	314736	314.736	0.03918879764985164
X	0.018590311916948227	73359	73.359	0.06328164469977786
X	0.01855587322756353	70097	70.097	0.06420871646369707
X	0.018551338997585695	13461	13.461	0.1112839790382618
X	0.018540513982857613	7872	7.872	0.13304918789243808
X	0.01851368280462068	8014	8.014	0.13219483695336554
X	0.018595454385663217	53689	53.689	0.07022749216932675
X	0.01980046226452904	25664	25.664	0.09171710280761834
X	0.01856220601186441	41315	41.315	0.07659033053097115
X	0.01853835080370214	6916	6.916	0.13891166084948298
X	0.01985342424233609	613738	613.738	0.031862814591079404
X	0.018572732335431188	15287	15.287	0.1067049364344567
X	0.019435799330739252	25494	25.494	0.09135271834253508
X	0.01920514971802885	110359	110.359	0.055830302403918036
X	0.018867400247183905	34143	34.143	0.08206099807720403
X	0.018568805351002218	63638	63.638	0.06632681562276829
X	0.019022608542666766	347484	347.484	0.03797039247919738
X	0.01857008153473478	46143	46.143	0.07383053144595048
X	0.01857858633685412	45582	45.582	0.07414350216794503
X	0.018501104517096773	5268	5.268	0.15200246472396603
X	0.018560536546545933	29010	29.01	0.08616831475457483
X	0.018528234609844704	5826	5.826	0.14705772857200092
X	0.018584372395534426	129570	129.57	0.052345716272587564
X	0.01988516528801551	587890	587.89	0.032340324811335884
X	0.01900670690140281	49857	49.857	0.07250927760313082
X	0.018534419220134064	79927	79.927	0.06143678626497196
X	0.019464088698081545	29142	29.142	0.0874121039869004
X	0.018556896120829346	13451	13.451	0.11132266331715852
X	0.01992376740992753	60835	60.835	0.06892963575900614
X	0.018449297725351355	4796	4.796	0.156687212694876
X	0.01902045529408182	75662	75.662	0.06311235179069702
X	0.018588751298447396	228854	228.854	0.0433075786851602
X	0.018480541898801855	31335	31.335	0.08386131488959016
X	0.018552914626125145	444066	444.066	0.03469936611180211
X	0.018570420184181608	117303	117.303	0.054096712114314834
X	0.01854706168427947	25163	25.163	0.0903311556174058
X	0.01839476447004415	2516	2.516	0.19408614389328468
X	0.019457582896278053	49281	49.281	0.07336176856246923
X	0.018574529379055862	80451	80.451	0.06134730055875648
X	0.01851229043085947	3197	3.197	0.17957222876737333
X	0.018602151260655647	331318	331.318	0.03829182335515482
X	0.018571101179922642	18869	18.869	0.09947094805763754
X	0.018556101164519407	14921	14.921	0.107538242304808
X	0.01853377296497305	5214	5.214	0.15261513456758483
X	0.018372725408991736	2487	2.487	0.19475977598525357
X	0.018163058057384246	1075	1.075	0.2566020779609197
X	0.018429334597037585	5790	5.79	0.14709921793259415
X	0.018655481623151525	11419	11.419	0.11777672240139982
X	0.018569444237509374	41263	41.263	0.07663244850333724
X	0.019028264202025653	1522698	1522.698	0.023205724493661956
X	0.01836259156593083	6546	6.546	0.14103253897807203
X	0.01860870102240495	44776	44.776	0.07462601564014434
X	0.018166529296768954	3174	3.174	0.17887721052449268
X	0.018471585079581444	10979	10.979	0.11893612727946737
X	0.018838301369821016	7345	7.345	0.13688314662596818
X	0.018563966994782244	90149	90.149	0.05905227379417967
X	0.018453778686005666	32987	32.987	0.08239752416730779
X	0.019802112158869886	478887	478.887	0.03458009293343846
X	0.01855564914668677	4558	4.558	0.15967390397854395
X	0.018680679708021935	108095	108.095	0.05570091842882865
X	0.018581249521519783	15421	15.421	0.10641122906889561
X	0.018631529531175695	97089	97.089	0.05768012068801587
X	0.018594705994198837	202223	202.223	0.045135638349888936
X	0.018524228364821215	9132	9.132	0.1265876704677063
X	0.018555151552840196	5367	5.367	0.151208998470834
X	0.019458555744938343	69839	69.839	0.06531368643198773
X	0.018388466243424162	7877	7.877	0.13265640400875164
X	0.018567628707108968	21478	21.478	0.09526225733299784
X	0.018555977329694818	70528	70.528	0.06407777477004122
X	0.018596224293035788	24266	24.266	0.0915114691031912
X	0.01856508906277774	31590	31.59	0.0837624047402744
X	0.01835008606533073	12237	12.237	0.11446016874149033
X	0.018574748603978834	45161	45.161	0.07436806213389761
X	0.01860788285706093	193540	193.54	0.04581159460165908
X	0.018527665235280977	8122	8.122	0.1316394027795529
X	0.019397418919244635	5468	5.468	0.1525123968617491
X	0.018538770611141887	76831	76.831	0.0622560384442174
X	0.018508760347529678	28160	28.16	0.08694572380490728
X	0.018951758212225042	135129	135.129	0.051955753546605844
X	0.018322959444963417	22758	22.758	0.0930294619359823
X	0.018389873271953527	23957	23.957	0.0915620345907269
X	0.01856092960474057	15354	15.354	0.10652692620128319
X	0.0185518521619213	101374	101.374	0.056774532119916954
X	0.018567409750709193	60902	60.902	0.06730385220649505
X	0.01851870357009973	20584	20.584	0.09653695638123408
X	0.018472082162446217	4396	4.396	0.16136873306493846
X	0.019454556895730453	14759	14.759	0.10964482455056938
X	0.018528698107954385	26813	26.813	0.08840968279718907
X	0.018546503766308762	59117	59.117	0.06794902379539153
X	0.01858283109593302	81430	81.43	0.06110955880458178
X	0.018983288990507392	5959	5.959	0.14714065713005456
X	0.018562567356871153	19265	19.265	0.09876953513466884
X	0.01847251053350292	19176	19.176	0.09876186912666424
X	0.01858020175813896	105398	105.398	0.056071135600519684
X	0.018711409026675396	23199	23.199	0.0930848655604146
X	0.018581042852268744	47694	47.694	0.07303574200835698
X	0.018816544554167553	11218	11.218	0.11881596852200647
X	0.018576036287319672	19980	19.98	0.09760060781864902
X	0.018430581796656683	1714	1.714	0.22072053614124112
X	0.018491417083810083	6062	6.062	0.14502785403726243
X	0.018584545851133425	76866	76.866	0.0622977780399314
X	0.0185804069056064	30151	30.151	0.08509770217351013
X	0.018577441999737807	34506	34.506	0.08135116287900308
X	0.018847761224301696	1400	1.4	0.23789058000590893
X	0.018447485866557777	2885	2.885	0.1856081845684341
X	0.018475773387352915	28357	28.357	0.08669235110213017
X	0.018552418425513983	34611	34.611	0.08123230832028427
X	0.01858587321935117	13817	13.817	0.11038830654254178
X	0.018549767033385154	2735	2.735	0.1892897261835918
X	0.018134113631435203	2501	2.501	0.19355046261157685
X	0.01855065903155841	13656	13.656	0.11075039358502746
X	0.01983636819562021	21215	21.215	0.0977851804662983
X	0.018536659258953554	54195	54.195	0.06993438317390273
X	0.01846539893431206	3570	3.57	0.17294054617639662
X	0.018521635040318674	13169	13.169	0.11204062124469957
X	0.01856743954675143	16294	16.294	0.10444991771990876
X	0.01849628475112455	5148	5.148	0.153161162797991
X	0.018622046867219508	152804	152.804	0.04957897730690202
X	0.018774101789573568	9323	9.323	0.1262799771566973
X	0.018557183781972304	56007	56.007	0.06919742274312612
X	0.018588998503003645	87234	87.234	0.059729663560732
X	0.018560225847254693	28928	28.928	0.08624917493065165
X	0.019933299249282693	89689	89.689	0.06057312805321968
X	0.018589125106342357	113433	113.433	0.05472341320808836
X	0.018446764711711854	1264	1.264	0.24437552709725122
X	0.01857066144234437	36170	36.17	0.08007426382348926
X	0.01855871807560542	7653	7.653	0.13435033501210084
X	0.018985343380813684	177260	177.26	0.04749004384032996
X	0.018578010352744707	84791	84.791	0.060286001651432254
X	0.018558206945976954	20096	20.096	0.09738127664691815
X	0.01858288059832087	28904	28.904	0.08630812779464342
X	0.01856835037326493	9518	9.518	0.12495180687140446
X	0.01856875832046398	12115	12.115	0.11529747996113851
X	0.01849259773293543	6717	6.717	0.14015465438016098
X	0.018424202448934474	7178	7.178	0.13691837865032494
X	0.018581917172280017	144465	144.465	0.050478831441519205
X	0.01906547866759002	202935	202.935	0.04546008595130644
X	0.018490849661930105	9411	9.411	0.12524883589212382
X	0.01856469256710251	7728	7.728	0.1339286679628424
X	0.018517927239516372	13216	13.216	0.11190017896830393
X	0.01895584016622926	87565	87.565	0.060044167624780265
X	0.018588512392382746	13264	13.264	0.11190685931552129
X	0.018560714519992597	5648	5.648	0.14867342085473412
X	0.018582527705737237	36792	36.792	0.0796374106460738
X	0.018602004929706314	61557	61.557	0.06710590993665932
X	0.01847833755295202	5519	5.519	0.1496008866218326
X	0.018578414243064143	22302	22.302	0.09409248609725614
X	0.018508955517824117	7060	7.06	0.13788776647113918
time for making epsilon is 1.4723148345947266
epsilons are
[0.3287443531106375, 0.12379258900753508, 0.24700202691234951, 0.1403515805379688, 0.09089188557140164, 0.09668471763406825, 0.07585988205760757, 0.08695572542629976, 0.13375774473763727, 0.10973542375563566, 0.1557378717230037, 0.05915490009710619, 0.13607313818949593, 0.04670470576729143, 0.09778645629088571, 0.0986560252038982, 0.07195464536592691, 0.0827799405566316, 0.05526236648616895, 0.06293146294901496, 0.10898803949331061, 0.05119073116148194, 0.1226296835970578, 0.09401686032765644, 0.1397703932234596, 0.06556508803156813, 0.07906834589263538, 0.1441033024683703, 0.06966868243556221, 0.02612750495338368, 0.1448293095895506, 0.034115987752479614, 0.10991681505510648, 0.15597475120507695, 0.12839485559603278, 0.05639763278106248, 0.06461199652449484, 0.2753852242265836, 0.16215786119813655, 0.22519354761834368, 0.2000238484100066, 0.2695157608571509, 0.3166105562957106, 0.20771287214279854, 0.30331534613284433, 0.32401190245413397, 0.22656009097067306, 0.1993035289019869, 0.18409583289614084, 0.12869142349168314, 0.1209847127131068, 0.18753039996031523, 0.1849078776629621, 0.20122652914561054, 0.1768849081184056, 0.1742222301481899, 0.2878154909293303, 0.15797491214000914, 0.2035842744101003, 0.20948819122862433, 0.15102562260224028, 0.28975428443597234, 0.11150600943967155, 0.1668123823660024, 0.22294764593379493, 0.21369240897808703, 0.2734547037238305, 0.22764329178708972, 0.18957658500121827, 0.19771185552324025, 0.17767810254321093, 0.16438546608166021, 0.2534116366213729, 0.17050302763679373, 0.20211362087774093, 0.15932720433812453, 0.2972855894337573, 0.3227507065078084, 0.22689252793396958, 0.15287567358119733, 0.2118047500090499, 0.2446543130499229, 0.16194010770616726, 0.24803934866123703, 0.27168666676150033, 0.3293965838475761, 0.2151097413296399, 0.33276183150862015, 0.17261173282217215, 0.19906319524691193, 0.216857967750735, 0.2976065794136916, 0.27972754070392153, 0.18931967714295764, 0.18215792620764623, 0.19036665358987342, 0.21125447181779866, 0.15105558777908093, 0.12675229472201224, 0.21799888242208623, 0.1187898766854197, 0.16441496067956918, 0.19234932177482983, 0.2583776889807795, 0.16659805649773562, 0.24200305746646014, 0.16610478192464723, 0.300724468042699, 0.23112926340080614, 0.2333179820383688, 0.22869169942842385, 0.24169747177073025, 0.25381873075861955, 0.18005642172214908, 0.19438712638409358, 0.24088251992987997, 0.21966097174202628, 0.3100822442327723, 0.1518068533128621, 0.326422249549844, 0.1309470012823892, 0.24840289409643487, 0.23555935806953981, 0.19140343256220738, 0.217625083832195, 0.21088400968619397, 0.2355048959818846, 0.1996827369490622, 0.12711061253849018, 0.22079595449957087, 0.17275206749240965, 0.269446777329158, 0.1919840498117696, 0.23733996367947074, 0.19775664137820753, 0.24773161155030754, 0.27252867175361745, 0.2691065842897652, 0.23560465383855878, 0.2530110493193352, 0.1799117876647329, 0.27581030011657304, 0.13942226914318814, 0.1386459319224957, 0.14211392266862122, 0.2732297510312587, 0.2820609964120345, 0.19129139326695518, 0.23397752602127525, 0.2357349856419789, 0.220284160628779, 0.1844759145952064, 0.1695326633593469, 0.15786603038068467, 0.2016652329526939, 0.11037524492608322, 0.24290129583163575, 0.20369510973110969, 0.22221500108154665, 0.4160245712461943, 0.1445003459935858, 0.288038508764752, 0.17057622142929232, 0.28896514143541513, 0.17950207424420941, 0.22036467500469342, 0.256518364339806, 0.2154661133656126, 0.20623965525126556, 0.3540592434281021, 0.2902219520071378, 0.27821259327746506, 0.16980238300075468, 0.252200045132288, 0.20708036754383113, 0.20900040335933698, 0.20038932278616922, 0.18570423412366538, 0.17865548202538276, 0.2685755682118654, 0.17604392106058078, 0.21054117259256933, 0.189118844362963, 0.24250815905503864, 0.17223473986934576, 0.20271893367475968, 0.220280231632985, 0.1829117529805428, 0.16626940771180193, 0.12232477396766744, 0.22470534921434818, 0.3349908855273885, 0.2584502340372298, 0.24231469780029855, 0.1994176059125811, 0.26519877350640336, 0.20986687136071228, 0.3010168713718979, 0.20997008435535544, 0.3105964436176154, 0.18314225409334964, 0.2140581173198926, 0.1187851906634672, 0.15906739472765, 0.181142317995094, 0.3294735243857378, 0.19676643707117772, 0.18588724898445266, 0.25699280539786634, 0.1256880778678836, 0.12472413697926772, 0.2506905945775324, 0.2876531898920686, 0.22620030772020483, 0.3138704416252245, 0.2967985909201094, 0.3660164470089271, 0.28837899661517524, 0.14018196101520375, 0.2934871538729358, 0.22605702867004537, 0.26159753174311096, 0.16967986789984033, 0.22645865419767364, 0.1709052170861261, 0.159572632592767, 0.2720884980823877, 0.26915257191801883, 0.14190384328936356, 0.3193038659757615, 0.26369907675425996, 0.21712677468288186, 0.2317138257332235, 0.17314267087917115, 0.21434163599423017, 0.28191492120904704, 0.16576544732954954, 0.18907444347474894, 0.25446297412606156, 0.12391118491601417, 0.07964515162448423, 0.2070508345544612, 0.18074648593712928, 0.3209562089659257, 0.16518355026556447, 0.13039486385354712, 0.07086180212413194, 0.20092353281703013, 0.06329113420062264, 0.08489214155152733, 0.11173726547212331, 0.07165650430990048, 0.19993361694691542, 0.15625619959887488, 0.05645320267302734, 0.03975287544127534, 0.1848302976203133, 0.09081121482678801, 0.109101362574583, 0.09043154037252354, 0.08632209246360617, 0.09008659050062724, 0.10769574794210737, 0.11761408319267891, 0.15812552227364035, 0.057516782532852646, 0.18754560411587276, 0.2011709043885109, 0.10843220806167496, 0.07866114202935252, 0.04212630698107695, 0.066070275731605, 0.2963687343088642, 0.12068863158626153, 0.15673954295394513, 0.10694465147366247, 0.07524610676621862, 0.10955713047804629, 0.080546160975413, 0.318354353195893, 0.17879620549504183, 0.0735765910818094, 0.0835772137712148, 0.08622152016175261, 0.17318624645318095, 0.04456605221555102, 0.15042676859216694, 0.09831425072712639, 0.06351597262611602, 0.21640985811531982, 0.07546076179332345, 0.053152720560731226, 0.0492037835274881, 0.10262879260772963, 0.10587441863868237, 0.19967152902519952, 0.17100232553931796, 0.07043535540322493, 0.13844194754023878, 0.10309163774841332, 0.0884655224792199, 0.1087127690629853, 0.11100500620015163, 0.1869763806647327, 0.06303573563505195, 0.10472160809039403, 0.1184678377111793, 0.11494747262055557, 0.05112875359875089, 0.12903941445761946, 0.05364268076081678, 0.19328279205781643, 0.09727341264955107, 0.1356075201151318, 0.07045016841822732, 0.0857983098383208, 0.16353839345053628, 0.062224317112680866, 0.05733297835846406, 0.2007854463568345, 0.06662237135493958, 0.05384358376624545, 0.04069957719982896, 0.09330679058022287, 0.2639694263039709, 0.17596143531469544, 0.1403837589504918, 0.10799340341701634, 0.10292673709884344, 0.24432161098953542, 0.07639222592688058, 0.06296167934037443, 0.1337049349968368, 0.16674370474900307, 0.1118931051462082, 0.11658945570763928, 0.12332171664736837, 0.12336978865700639, 0.20532576112548664, 0.07276762631895584, 0.09233997981134767, 0.11355413356172261, 0.10193769963679375, 0.14193005927436114, 0.03534755009950962, 0.15842686415336213, 0.0598419626846667, 0.2808046206655386, 0.23131976442308153, 0.17029141768324071, 0.14215935460562326, 0.15466646853124189, 0.08828118997848126, 0.16564091319807842, 0.18096565067970222, 0.04416365393221757, 0.09253752226492913, 0.10369318057569463, 0.0646546319518914, 0.05786121621918271, 0.14992323490006568, 0.15230475081389677, 0.12616095518707443, 0.32641033432010524, 0.22061516813645426, 0.11725436395562812, 0.22830544481061277, 0.22814403045455836, 0.19089149946854395, 0.16912015003468372, 0.04469537580930561, 0.14378332563222332, 0.06864764054678707, 0.1566994565439555, 0.1959520120688728, 0.1037512315885647, 0.04098960668492726, 0.04045330747643236, 0.11674992967826292, 0.09966329252472357, 0.24530863853900042, 0.10090441675170463, 0.04660040777893419, 0.10316017202353099, 0.13422328098300065, 0.0725082662862283, 0.0401360665719151, 0.08997397940011798, 0.14398327583418088, 0.09793547086061609, 0.10389498268362346, 0.19303327282912458, 0.05191722619409534, 0.11797496305176455, 0.3031789364226445, 0.10696345055490189, 0.10738400093645911, 0.2965089874729181, 0.06640173209032865, 0.06192427242056282, 0.059302965881410465, 0.054424127226929575, 0.06089924082522328, 0.12926226015871928, 0.13793383135802406, 0.08240681056829673, 0.05784810968227482, 0.16976007288178332, 0.08295733073001445, 0.05219242308550135, 0.07470958951690308, 0.09028111098222114, 0.07608275555662554, 0.13732020194765127, 0.18280672101540504, 0.11568252874392475, 0.06024928150003241, 0.07405351923806418, 0.03710696550712661, 0.06895388628683471, 0.07209559001171505, 0.1108302878193902, 0.05602323103540586, 0.21868272806915506, 0.14221764918236485, 0.04932326280989946, 0.10172282246324671, 0.17463450090240307, 0.1147316204689035, 0.05217505817182807, 0.10081612942082702, 0.08334753107897185, 0.20008019244636874, 0.14927582033830566, 0.19820638860384682, 0.13315282335395692, 0.10246567192434886, 0.08238439132012183, 0.2107916863793247, 0.0584495694565263, 0.1862391908902165, 0.09761820059147126, 0.14316377074074266, 0.1033942919385304, 0.1790440924185838, 0.11056312476391623, 0.10650729489812334, 0.10696523301048881, 0.07603574403977031, 0.04285514901206987, 0.17675967276754917, 0.16676059445982153, 0.07483866265780156, 0.1349270588518237, 0.038902421491525285, 0.27483255313276544, 0.0569593626062393, 0.04531608283549252, 0.2139205632120772, 0.05175249455914523, 0.10727713569582102, 0.03849692430576199, 0.15399802589100958, 0.10428090960688383, 0.16623821958278712, 0.17482432764262912, 0.1634124204151926, 0.10081465549107933, 0.12952832876885872, 0.07444363194235046, 0.12171422988371997, 0.19706179633933296, 0.12672434171913194, 0.06669538790418249, 0.08812851054410847, 0.04974277625253503, 0.11805719056588929, 0.14214599981083018, 0.18818223159457356, 0.18032843346836222, 0.04488135659824503, 0.14454789568069026, 0.12646836680907458, 0.08905464631427432, 0.0976737633469084, 0.17498101361416515, 0.16371952381400817, 0.04648892093735345, 0.10335227741521323, 0.11131833664958639, 0.08939909725174335, 0.0682237659221169, 0.07438386974869604, 0.05251664189208539, 0.07303948143944122, 0.18702836848365295, 0.08553811500518901, 0.12380822056550658, 0.13377665045339637, 0.11089973774229904, 0.2144840915038573, 0.0823872632344292, 0.12287409676789501, 0.1808749675416931, 0.11558562692894668, 0.17651225258594694, 0.0722118803763616, 0.15586668366073572, 0.11374209389763987, 0.15860842819049892, 0.12468427488817103, 0.19036253818671928, 0.15420034177959896, 0.106187054932991, 0.06297581426980316, 0.18447646594929892, 0.2836016079019925, 0.1609515276881661, 0.06224587482681665, 0.03646460754387892, 0.09588494605140091, 0.15891833894392238, 0.07260967992272299, 0.06740713524627145, 0.22563584816223697, 0.13622115051720102, 0.1119856613550308, 0.05811022184255561, 0.06060647984084706, 0.05419972469839279, 0.1429551877597203, 0.19343383697826497, 0.2568674805537627, 0.07880396756548078, 0.1271453976980728, 0.10380316608808703, 0.06927226062196423, 0.04100887278518677, 0.15825206792796467, 0.23939882931895898, 0.11381532063393622, 0.03957279904188364, 0.09693642020728366, 0.2117303399733625, 0.21773658319851738, 0.08828754031447135, 0.2249981738757765, 0.08998147098140094, 0.22190049785460317, 0.12375593621106344, 0.12309761637499304, 0.18392196807116168, 0.19125474707910642, 0.052377926381927356, 0.03918879764985164, 0.06328164469977786, 0.06420871646369707, 0.1112839790382618, 0.13304918789243808, 0.13219483695336554, 0.07022749216932675, 0.09171710280761834, 0.07659033053097115, 0.13891166084948298, 0.031862814591079404, 0.1067049364344567, 0.09135271834253508, 0.055830302403918036, 0.08206099807720403, 0.06632681562276829, 0.03797039247919738, 0.07383053144595048, 0.07414350216794503, 0.15200246472396603, 0.08616831475457483, 0.14705772857200092, 0.052345716272587564, 0.032340324811335884, 0.07250927760313082, 0.06143678626497196, 0.0874121039869004, 0.11132266331715852, 0.06892963575900614, 0.156687212694876, 0.06311235179069702, 0.0433075786851602, 0.08386131488959016, 0.03469936611180211, 0.054096712114314834, 0.0903311556174058, 0.19408614389328468, 0.07336176856246923, 0.06134730055875648, 0.17957222876737333, 0.03829182335515482, 0.09947094805763754, 0.107538242304808, 0.15261513456758483, 0.19475977598525357, 0.2566020779609197, 0.14709921793259415, 0.11777672240139982, 0.07663244850333724, 0.023205724493661956, 0.14103253897807203, 0.07462601564014434, 0.17887721052449268, 0.11893612727946737, 0.13688314662596818, 0.05905227379417967, 0.08239752416730779, 0.03458009293343846, 0.15967390397854395, 0.05570091842882865, 0.10641122906889561, 0.05768012068801587, 0.045135638349888936, 0.1265876704677063, 0.151208998470834, 0.06531368643198773, 0.13265640400875164, 0.09526225733299784, 0.06407777477004122, 0.0915114691031912, 0.0837624047402744, 0.11446016874149033, 0.07436806213389761, 0.04581159460165908, 0.1316394027795529, 0.1525123968617491, 0.0622560384442174, 0.08694572380490728, 0.051955753546605844, 0.0930294619359823, 0.0915620345907269, 0.10652692620128319, 0.056774532119916954, 0.06730385220649505, 0.09653695638123408, 0.16136873306493846, 0.10964482455056938, 0.08840968279718907, 0.06794902379539153, 0.06110955880458178, 0.14714065713005456, 0.09876953513466884, 0.09876186912666424, 0.056071135600519684, 0.0930848655604146, 0.07303574200835698, 0.11881596852200647, 0.09760060781864902, 0.22072053614124112, 0.14502785403726243, 0.0622977780399314, 0.08509770217351013, 0.08135116287900308, 0.23789058000590893, 0.1856081845684341, 0.08669235110213017, 0.08123230832028427, 0.11038830654254178, 0.1892897261835918, 0.19355046261157685, 0.11075039358502746, 0.0977851804662983, 0.06993438317390273, 0.17294054617639662, 0.11204062124469957, 0.10444991771990876, 0.153161162797991, 0.04957897730690202, 0.1262799771566973, 0.06919742274312612, 0.059729663560732, 0.08624917493065165, 0.06057312805321968, 0.05472341320808836, 0.24437552709725122, 0.08007426382348926, 0.13435033501210084, 0.04749004384032996, 0.060286001651432254, 0.09738127664691815, 0.08630812779464342, 0.12495180687140446, 0.11529747996113851, 0.14015465438016098, 0.13691837865032494, 0.050478831441519205, 0.04546008595130644, 0.12524883589212382, 0.1339286679628424, 0.11190017896830393, 0.060044167624780265, 0.11190685931552129, 0.14867342085473412, 0.0796374106460738, 0.06710590993665932, 0.1496008866218326, 0.09409248609725614, 0.13788776647113918]
0.09073954954433733
Making ranges
torch.Size([47527, 2])
We keep 7.70e+06/7.12e+08 =  1% of the original kernel matrix.

torch.Size([1482, 2])
We keep 2.43e+04/2.68e+05 =  9% of the original kernel matrix.

torch.Size([10773, 2])
We keep 5.19e+05/1.38e+07 =  3% of the original kernel matrix.

torch.Size([19013, 2])
We keep 2.38e+06/9.48e+07 =  2% of the original kernel matrix.

torch.Size([30691, 2])
We keep 3.84e+06/2.60e+08 =  1% of the original kernel matrix.

torch.Size([3349, 2])
We keep 7.72e+04/1.44e+06 =  5% of the original kernel matrix.

torch.Size([14632, 2])
We keep 8.66e+05/3.21e+07 =  2% of the original kernel matrix.

torch.Size([10633, 2])
We keep 3.92e+06/4.48e+07 =  8% of the original kernel matrix.

torch.Size([22625, 2])
We keep 2.74e+06/1.79e+08 =  1% of the original kernel matrix.

torch.Size([45795, 2])
We keep 9.47e+06/6.92e+08 =  1% of the original kernel matrix.

torch.Size([46144, 2])
We keep 7.74e+06/7.02e+08 =  1% of the original kernel matrix.

torch.Size([38431, 2])
We keep 5.99e+06/4.23e+08 =  1% of the original kernel matrix.

torch.Size([44823, 2])
We keep 6.78e+06/5.49e+08 =  1% of the original kernel matrix.

torch.Size([78900, 2])
We keep 2.02e+07/1.81e+09 =  1% of the original kernel matrix.

torch.Size([62816, 2])
We keep 1.23e+07/1.14e+09 =  1% of the original kernel matrix.

torch.Size([52706, 2])
We keep 9.34e+06/7.99e+08 =  1% of the original kernel matrix.

torch.Size([52853, 2])
We keep 8.80e+06/7.54e+08 =  1% of the original kernel matrix.

torch.Size([15405, 2])
We keep 2.09e+06/5.99e+07 =  3% of the original kernel matrix.

torch.Size([27654, 2])
We keep 3.22e+06/2.07e+08 =  1% of the original kernel matrix.

torch.Size([20252, 2])
We keep 7.45e+06/1.96e+08 =  3% of the original kernel matrix.

torch.Size([31420, 2])
We keep 4.93e+06/3.74e+08 =  1% of the original kernel matrix.

torch.Size([10691, 2])
We keep 9.21e+05/2.38e+07 =  3% of the original kernel matrix.

torch.Size([23159, 2])
We keep 2.32e+06/1.30e+08 =  1% of the original kernel matrix.

torch.Size([150552, 2])
We keep 1.13e+08/8.06e+09 =  1% of the original kernel matrix.

torch.Size([85808, 2])
We keep 2.30e+07/2.40e+09 =  0% of the original kernel matrix.

torch.Size([17384, 2])
We keep 1.48e+06/5.95e+07 =  2% of the original kernel matrix.

torch.Size([29356, 2])
We keep 3.23e+06/2.06e+08 =  1% of the original kernel matrix.

torch.Size([331670, 2])
We keep 3.28e+08/3.33e+10 =  0% of the original kernel matrix.

torch.Size([133566, 2])
We keep 4.24e+07/4.87e+09 =  0% of the original kernel matrix.

torch.Size([37415, 2])
We keep 5.39e+06/3.94e+08 =  1% of the original kernel matrix.

torch.Size([43781, 2])
We keep 6.60e+06/5.29e+08 =  1% of the original kernel matrix.

torch.Size([37091, 2])
We keep 6.07e+06/3.75e+08 =  1% of the original kernel matrix.

torch.Size([44059, 2])
We keep 6.41e+06/5.17e+08 =  1% of the original kernel matrix.

torch.Size([94631, 2])
We keep 3.10e+07/2.49e+09 =  1% of the original kernel matrix.

torch.Size([68584, 2])
We keep 1.38e+07/1.33e+09 =  1% of the original kernel matrix.

torch.Size([57865, 2])
We keep 1.60e+07/1.07e+09 =  1% of the original kernel matrix.

torch.Size([53776, 2])
We keep 9.78e+06/8.74e+08 =  1% of the original kernel matrix.

torch.Size([198146, 2])
We keep 1.33e+08/1.21e+10 =  1% of the original kernel matrix.

torch.Size([100442, 2])
We keep 2.73e+07/2.94e+09 =  0% of the original kernel matrix.

torch.Size([140580, 2])
We keep 6.83e+07/5.57e+09 =  1% of the original kernel matrix.

torch.Size([83248, 2])
We keep 1.93e+07/1.99e+09 =  0% of the original kernel matrix.

torch.Size([21192, 2])
We keep 6.10e+06/1.93e+08 =  3% of the original kernel matrix.

torch.Size([31386, 2])
We keep 4.99e+06/3.71e+08 =  1% of the original kernel matrix.

torch.Size([258075, 2])
We keep 1.57e+08/2.01e+10 =  0% of the original kernel matrix.

torch.Size([116162, 2])
We keep 3.34e+07/3.78e+09 =  0% of the original kernel matrix.

torch.Size([21799, 2])
We keep 2.40e+06/1.01e+08 =  2% of the original kernel matrix.

torch.Size([33039, 2])
We keep 3.83e+06/2.68e+08 =  1% of the original kernel matrix.

torch.Size([38520, 2])
We keep 2.39e+07/4.99e+08 =  4% of the original kernel matrix.

torch.Size([44562, 2])
We keep 7.49e+06/5.96e+08 =  1% of the original kernel matrix.

torch.Size([8466, 2])
We keep 5.23e+06/4.82e+07 = 10% of the original kernel matrix.

torch.Size([19688, 2])
We keep 2.91e+06/1.85e+08 =  1% of the original kernel matrix.

torch.Size([127658, 2])
We keep 4.07e+07/4.35e+09 =  0% of the original kernel matrix.

torch.Size([79469, 2])
We keep 1.75e+07/1.76e+09 =  0% of the original kernel matrix.

torch.Size([61963, 2])
We keep 2.07e+07/1.41e+09 =  1% of the original kernel matrix.

torch.Size([55052, 2])
We keep 1.11e+07/1.00e+09 =  1% of the original kernel matrix.

torch.Size([14401, 2])
We keep 1.09e+06/3.83e+07 =  2% of the original kernel matrix.

torch.Size([26850, 2])
We keep 2.73e+06/1.65e+08 =  1% of the original kernel matrix.

torch.Size([69196, 2])
We keep 1.00e+08/3.00e+09 =  3% of the original kernel matrix.

torch.Size([57230, 2])
We keep 1.55e+07/1.46e+09 =  1% of the original kernel matrix.

torch.Size([2241557, 2])
We keep 5.16e+09/1.11e+12 =  0% of the original kernel matrix.

torch.Size([360663, 2])
We keep 2.06e+08/2.82e+10 =  0% of the original kernel matrix.

torch.Size([11891, 2])
We keep 1.46e+06/3.71e+07 =  3% of the original kernel matrix.

torch.Size([24273, 2])
We keep 2.65e+06/1.63e+08 =  1% of the original kernel matrix.

torch.Size([740890, 2])
We keep 2.13e+09/2.29e+11 =  0% of the original kernel matrix.

torch.Size([198134, 2])
We keep 1.02e+08/1.28e+10 =  0% of the original kernel matrix.

torch.Size([21229, 2])
We keep 9.15e+06/1.95e+08 =  4% of the original kernel matrix.

torch.Size([32027, 2])
We keep 4.82e+06/3.73e+08 =  1% of the original kernel matrix.

torch.Size([11806, 2])
We keep 8.44e+05/2.38e+07 =  3% of the original kernel matrix.

torch.Size([24522, 2])
We keep 2.27e+06/1.30e+08 =  1% of the original kernel matrix.

torch.Size([16727, 2])
We keep 2.23e+06/7.60e+07 =  2% of the original kernel matrix.

torch.Size([28456, 2])
We keep 3.51e+06/2.33e+08 =  1% of the original kernel matrix.

torch.Size([155125, 2])
We keep 1.85e+08/1.07e+10 =  1% of the original kernel matrix.

torch.Size([86054, 2])
We keep 2.66e+07/2.77e+09 =  0% of the original kernel matrix.

torch.Size([134099, 2])
We keep 5.47e+07/4.98e+09 =  1% of the original kernel matrix.

torch.Size([81347, 2])
We keep 1.87e+07/1.88e+09 =  0% of the original kernel matrix.

torch.Size([2544, 2])
We keep 4.58e+04/7.21e+05 =  6% of the original kernel matrix.

torch.Size([13234, 2])
We keep 6.88e+05/2.27e+07 =  3% of the original kernel matrix.

torch.Size([10815, 2])
We keep 7.78e+05/2.08e+07 =  3% of the original kernel matrix.

torch.Size([23571, 2])
We keep 2.18e+06/1.22e+08 =  1% of the original kernel matrix.

torch.Size([4029, 2])
We keep 1.48e+05/2.57e+06 =  5% of the original kernel matrix.

torch.Size([15327, 2])
We keep 1.05e+06/4.28e+07 =  2% of the original kernel matrix.

torch.Size([6334, 2])
We keep 2.18e+05/5.38e+06 =  4% of the original kernel matrix.

torch.Size([18898, 2])
We keep 1.35e+06/6.19e+07 =  2% of the original kernel matrix.

torch.Size([1865, 2])
We keep 8.18e+04/8.63e+05 =  9% of the original kernel matrix.

torch.Size([10475, 2])
We keep 7.44e+05/2.48e+07 =  2% of the original kernel matrix.

torch.Size([1384, 2])
We keep 4.63e+04/3.19e+05 = 14% of the original kernel matrix.

torch.Size([10025, 2])
We keep 5.47e+05/1.51e+07 =  3% of the original kernel matrix.

torch.Size([5347, 2])
We keep 1.91e+05/4.15e+06 =  4% of the original kernel matrix.

torch.Size([17380, 2])
We keep 1.25e+06/5.44e+07 =  2% of the original kernel matrix.

torch.Size([1923, 2])
We keep 3.14e+04/4.32e+05 =  7% of the original kernel matrix.

torch.Size([11912, 2])
We keep 5.90e+05/1.75e+07 =  3% of the original kernel matrix.

torch.Size([1618, 2])
We keep 2.43e+04/2.70e+05 =  8% of the original kernel matrix.

torch.Size([11170, 2])
We keep 5.18e+05/1.39e+07 =  3% of the original kernel matrix.

torch.Size([4331, 2])
We keep 1.25e+05/2.53e+06 =  4% of the original kernel matrix.

torch.Size([16147, 2])
We keep 1.05e+06/4.25e+07 =  2% of the original kernel matrix.

torch.Size([6174, 2])
We keep 2.31e+05/5.41e+06 =  4% of the original kernel matrix.

torch.Size([18513, 2])
We keep 1.37e+06/6.20e+07 =  2% of the original kernel matrix.

torch.Size([7475, 2])
We keep 3.38e+05/8.74e+06 =  3% of the original kernel matrix.

torch.Size([19755, 2])
We keep 1.60e+06/7.89e+07 =  2% of the original kernel matrix.

torch.Size([13420, 2])
We keep 3.23e+06/7.44e+07 =  4% of the original kernel matrix.

torch.Size([24989, 2])
We keep 3.53e+06/2.30e+08 =  1% of the original kernel matrix.

torch.Size([21482, 2])
We keep 2.75e+06/1.10e+08 =  2% of the original kernel matrix.

torch.Size([32913, 2])
We keep 4.05e+06/2.80e+08 =  1% of the original kernel matrix.

torch.Size([6946, 2])
We keep 3.41e+05/7.78e+06 =  4% of the original kernel matrix.

torch.Size([19345, 2])
We keep 1.55e+06/7.45e+07 =  2% of the original kernel matrix.

torch.Size([6718, 2])
We keep 4.16e+05/8.56e+06 =  4% of the original kernel matrix.

torch.Size([18879, 2])
We keep 1.61e+06/7.81e+07 =  2% of the original kernel matrix.

torch.Size([6142, 2])
We keep 2.30e+05/4.93e+06 =  4% of the original kernel matrix.

torch.Size([18478, 2])
We keep 1.33e+06/5.93e+07 =  2% of the original kernel matrix.

torch.Size([8701, 2])
We keep 4.25e+05/1.12e+07 =  3% of the original kernel matrix.

torch.Size([21417, 2])
We keep 1.75e+06/8.95e+07 =  1% of the original kernel matrix.

torch.Size([6157, 2])
We keep 7.58e+05/1.20e+07 =  6% of the original kernel matrix.

torch.Size([17235, 2])
We keep 1.82e+06/9.24e+07 =  1% of the original kernel matrix.

torch.Size([2206, 2])
We keep 3.75e+04/5.69e+05 =  6% of the original kernel matrix.

torch.Size([12505, 2])
We keep 6.46e+05/2.01e+07 =  3% of the original kernel matrix.

torch.Size([11769, 2])
We keep 6.20e+05/2.20e+07 =  2% of the original kernel matrix.

torch.Size([24295, 2])
We keep 2.21e+06/1.25e+08 =  1% of the original kernel matrix.

torch.Size([5705, 2])
We keep 2.06e+05/4.70e+06 =  4% of the original kernel matrix.

torch.Size([18011, 2])
We keep 1.31e+06/5.79e+07 =  2% of the original kernel matrix.

torch.Size([4754, 2])
We keep 2.13e+05/4.16e+06 =  5% of the original kernel matrix.

torch.Size([16480, 2])
We keep 1.26e+06/5.44e+07 =  2% of the original kernel matrix.

torch.Size([13288, 2])
We keep 7.86e+05/2.91e+07 =  2% of the original kernel matrix.

torch.Size([26006, 2])
We keep 2.45e+06/1.44e+08 =  1% of the original kernel matrix.

torch.Size([2034, 2])
We keep 4.35e+04/5.76e+05 =  7% of the original kernel matrix.

torch.Size([12034, 2])
We keep 6.49e+05/2.03e+07 =  3% of the original kernel matrix.

torch.Size([27320, 2])
We keep 4.12e+06/2.02e+08 =  2% of the original kernel matrix.

torch.Size([37463, 2])
We keep 5.16e+06/3.79e+08 =  1% of the original kernel matrix.

torch.Size([9734, 2])
We keep 5.86e+05/1.59e+07 =  3% of the original kernel matrix.

torch.Size([22417, 2])
We keep 2.00e+06/1.07e+08 =  1% of the original kernel matrix.

torch.Size([4064, 2])
We keep 1.34e+05/2.38e+06 =  5% of the original kernel matrix.

torch.Size([15356, 2])
We keep 1.02e+06/4.12e+07 =  2% of the original kernel matrix.

torch.Size([5332, 2])
We keep 1.74e+05/3.60e+06 =  4% of the original kernel matrix.

torch.Size([17559, 2])
We keep 1.19e+06/5.07e+07 =  2% of the original kernel matrix.

torch.Size([2388, 2])
We keep 4.97e+04/6.99e+05 =  7% of the original kernel matrix.

torch.Size([12618, 2])
We keep 6.89e+05/2.23e+07 =  3% of the original kernel matrix.

torch.Size([4360, 2])
We keep 1.36e+05/2.46e+06 =  5% of the original kernel matrix.

torch.Size([16292, 2])
We keep 1.04e+06/4.18e+07 =  2% of the original kernel matrix.

torch.Size([7131, 2])
We keep 2.88e+05/7.37e+06 =  3% of the original kernel matrix.

torch.Size([19783, 2])
We keep 1.49e+06/7.24e+07 =  2% of the original kernel matrix.

torch.Size([6147, 2])
We keep 2.85e+05/5.65e+06 =  5% of the original kernel matrix.

torch.Size([18410, 2])
We keep 1.39e+06/6.35e+07 =  2% of the original kernel matrix.

torch.Size([7012, 2])
We keep 5.97e+05/1.08e+07 =  5% of the original kernel matrix.

torch.Size([18984, 2])
We keep 1.74e+06/8.76e+07 =  1% of the original kernel matrix.

torch.Size([10296, 2])
We keep 6.11e+05/1.74e+07 =  3% of the original kernel matrix.

torch.Size([22845, 2])
We keep 2.06e+06/1.11e+08 =  1% of the original kernel matrix.

torch.Size([3154, 2])
We keep 6.50e+04/1.24e+06 =  5% of the original kernel matrix.

torch.Size([14381, 2])
We keep 8.20e+05/2.97e+07 =  2% of the original kernel matrix.

torch.Size([7294, 2])
We keep 6.61e+05/1.35e+07 =  4% of the original kernel matrix.

torch.Size([19092, 2])
We keep 1.87e+06/9.80e+07 =  1% of the original kernel matrix.

torch.Size([5559, 2])
We keep 2.61e+05/5.00e+06 =  5% of the original kernel matrix.

torch.Size([17691, 2])
We keep 1.33e+06/5.96e+07 =  2% of the original kernel matrix.

torch.Size([9965, 2])
We keep 8.76e+05/2.05e+07 =  4% of the original kernel matrix.

torch.Size([22285, 2])
We keep 2.17e+06/1.21e+08 =  1% of the original kernel matrix.

torch.Size([1957, 2])
We keep 3.44e+04/4.62e+05 =  7% of the original kernel matrix.

torch.Size([11924, 2])
We keep 6.13e+05/1.81e+07 =  3% of the original kernel matrix.

torch.Size([1547, 2])
We keep 2.49e+04/2.87e+05 =  8% of the original kernel matrix.

torch.Size([10928, 2])
We keep 5.17e+05/1.43e+07 =  3% of the original kernel matrix.

torch.Size([4415, 2])
We keep 1.21e+05/2.41e+06 =  5% of the original kernel matrix.

torch.Size([16083, 2])
We keep 1.02e+06/4.14e+07 =  2% of the original kernel matrix.

torch.Size([12229, 2])
We keep 9.23e+05/2.78e+07 =  3% of the original kernel matrix.

torch.Size([24519, 2])
We keep 2.43e+06/1.41e+08 =  1% of the original kernel matrix.

torch.Size([5172, 2])
We keep 1.61e+05/3.65e+06 =  4% of the original kernel matrix.

torch.Size([17212, 2])
We keep 1.18e+06/5.10e+07 =  2% of the original kernel matrix.

torch.Size([3131, 2])
We keep 9.71e+04/1.51e+06 =  6% of the original kernel matrix.

torch.Size([13926, 2])
We keep 8.94e+05/3.28e+07 =  2% of the original kernel matrix.

torch.Size([10923, 2])
We keep 9.06e+05/2.12e+07 =  4% of the original kernel matrix.

torch.Size([23762, 2])
We keep 2.19e+06/1.23e+08 =  1% of the original kernel matrix.

torch.Size([3139, 2])
We keep 9.47e+04/1.44e+06 =  6% of the original kernel matrix.

torch.Size([14152, 2])
We keep 8.94e+05/3.21e+07 =  2% of the original kernel matrix.

torch.Size([2501, 2])
We keep 6.04e+04/8.39e+05 =  7% of the original kernel matrix.

torch.Size([12996, 2])
We keep 7.28e+05/2.44e+07 =  2% of the original kernel matrix.

torch.Size([1514, 2])
We keep 2.25e+04/2.59e+05 =  8% of the original kernel matrix.

torch.Size([10967, 2])
We keep 5.11e+05/1.36e+07 =  3% of the original kernel matrix.

torch.Size([4471, 2])
We keep 2.14e+05/3.44e+06 =  6% of the original kernel matrix.

torch.Size([16085, 2])
We keep 1.18e+06/4.95e+07 =  2% of the original kernel matrix.

torch.Size([1424, 2])
We keep 1.98e+04/2.30e+05 =  8% of the original kernel matrix.

torch.Size([10693, 2])
We keep 4.88e+05/1.28e+07 =  3% of the original kernel matrix.

torch.Size([9333, 2])
We keep 5.52e+05/1.49e+07 =  3% of the original kernel matrix.

torch.Size([21981, 2])
We keep 1.95e+06/1.03e+08 =  1% of the original kernel matrix.

torch.Size([5973, 2])
We keep 2.42e+05/5.49e+06 =  4% of the original kernel matrix.

torch.Size([18166, 2])
We keep 1.37e+06/6.26e+07 =  2% of the original kernel matrix.

torch.Size([5077, 2])
We keep 1.49e+05/3.28e+06 =  4% of the original kernel matrix.

torch.Size([17296, 2])
We keep 1.16e+06/4.84e+07 =  2% of the original kernel matrix.

torch.Size([2084, 2])
We keep 3.29e+04/4.77e+05 =  6% of the original kernel matrix.

torch.Size([12395, 2])
We keep 6.18e+05/1.84e+07 =  3% of the original kernel matrix.

torch.Size([2414, 2])
We keep 4.43e+04/7.07e+05 =  6% of the original kernel matrix.

torch.Size([13062, 2])
We keep 7.01e+05/2.24e+07 =  3% of the original kernel matrix.

torch.Size([6387, 2])
We keep 3.42e+05/7.46e+06 =  4% of the original kernel matrix.

torch.Size([18433, 2])
We keep 1.55e+06/7.29e+07 =  2% of the original kernel matrix.

torch.Size([7524, 2])
We keep 4.07e+05/9.19e+06 =  4% of the original kernel matrix.

torch.Size([20232, 2])
We keep 1.65e+06/8.09e+07 =  2% of the original kernel matrix.

torch.Size([6718, 2])
We keep 2.73e+05/7.14e+06 =  3% of the original kernel matrix.

torch.Size([19111, 2])
We keep 1.49e+06/7.13e+07 =  2% of the original kernel matrix.

torch.Size([5078, 2])
We keep 1.92e+05/3.79e+06 =  5% of the original kernel matrix.

torch.Size([17238, 2])
We keep 1.21e+06/5.20e+07 =  2% of the original kernel matrix.

torch.Size([10410, 2])
We keep 1.47e+06/2.88e+07 =  5% of the original kernel matrix.

torch.Size([22642, 2])
We keep 2.48e+06/1.43e+08 =  1% of the original kernel matrix.

torch.Size([19890, 2])
We keep 2.14e+06/8.30e+07 =  2% of the original kernel matrix.

torch.Size([31402, 2])
We keep 3.63e+06/2.43e+08 =  1% of the original kernel matrix.

torch.Size([4951, 2])
We keep 1.47e+05/3.19e+06 =  4% of the original kernel matrix.

torch.Size([17133, 2])
We keep 1.13e+06/4.76e+07 =  2% of the original kernel matrix.

torch.Size([22303, 2])
We keep 2.59e+06/1.22e+08 =  2% of the original kernel matrix.

torch.Size([33393, 2])
We keep 4.24e+06/2.95e+08 =  1% of the original kernel matrix.

torch.Size([9697, 2])
We keep 7.85e+05/1.72e+07 =  4% of the original kernel matrix.

torch.Size([22198, 2])
We keep 2.08e+06/1.11e+08 =  1% of the original kernel matrix.

torch.Size([6629, 2])
We keep 2.91e+05/6.73e+06 =  4% of the original kernel matrix.

torch.Size([19160, 2])
We keep 1.45e+06/6.93e+07 =  2% of the original kernel matrix.

torch.Size([3035, 2])
We keep 6.87e+04/1.13e+06 =  6% of the original kernel matrix.

torch.Size([14132, 2])
We keep 8.17e+05/2.83e+07 =  2% of the original kernel matrix.

torch.Size([8392, 2])
We keep 8.95e+05/1.60e+07 =  5% of the original kernel matrix.

torch.Size([20534, 2])
We keep 2.04e+06/1.07e+08 =  1% of the original kernel matrix.

torch.Size([3559, 2])
We keep 9.61e+04/1.64e+06 =  5% of the original kernel matrix.

torch.Size([14957, 2])
We keep 9.13e+05/3.42e+07 =  2% of the original kernel matrix.

torch.Size([10325, 2])
We keep 5.17e+05/1.63e+07 =  3% of the original kernel matrix.

torch.Size([23017, 2])
We keep 2.00e+06/1.08e+08 =  1% of the original kernel matrix.

torch.Size([1927, 2])
We keep 4.28e+04/4.54e+05 =  9% of the original kernel matrix.

torch.Size([11764, 2])
We keep 6.21e+05/1.80e+07 =  3% of the original kernel matrix.

torch.Size([3712, 2])
We keep 1.20e+05/2.20e+06 =  5% of the original kernel matrix.

torch.Size([15046, 2])
We keep 1.00e+06/3.96e+07 =  2% of the original kernel matrix.

torch.Size([3682, 2])
We keep 1.42e+05/2.09e+06 =  6% of the original kernel matrix.

torch.Size([15059, 2])
We keep 9.84e+05/3.86e+07 =  2% of the original kernel matrix.

torch.Size([3638, 2])
We keep 1.62e+05/2.31e+06 =  7% of the original kernel matrix.

torch.Size([14660, 2])
We keep 1.03e+06/4.05e+07 =  2% of the original kernel matrix.

torch.Size([3255, 2])
We keep 1.12e+05/1.69e+06 =  6% of the original kernel matrix.

torch.Size([14146, 2])
We keep 9.19e+05/3.47e+07 =  2% of the original kernel matrix.

torch.Size([2712, 2])
We keep 8.53e+04/1.24e+06 =  6% of the original kernel matrix.

torch.Size([13107, 2])
We keep 8.28e+05/2.97e+07 =  2% of the original kernel matrix.

torch.Size([7539, 2])
We keep 4.48e+05/9.79e+06 =  4% of the original kernel matrix.

torch.Size([19934, 2])
We keep 1.66e+06/8.35e+07 =  1% of the original kernel matrix.

torch.Size([6268, 2])
We keep 2.96e+05/6.27e+06 =  4% of the original kernel matrix.

torch.Size([18533, 2])
We keep 1.44e+06/6.68e+07 =  2% of the original kernel matrix.

torch.Size([2991, 2])
We keep 1.09e+05/1.68e+06 =  6% of the original kernel matrix.

torch.Size([13541, 2])
We keep 9.27e+05/3.46e+07 =  2% of the original kernel matrix.

torch.Size([4096, 2])
We keep 1.62e+05/2.96e+06 =  5% of the original kernel matrix.

torch.Size([15520, 2])
We keep 1.09e+06/4.59e+07 =  2% of the original kernel matrix.

torch.Size([1798, 2])
We keep 2.86e+04/3.52e+05 =  8% of the original kernel matrix.

torch.Size([11647, 2])
We keep 5.53e+05/1.58e+07 =  3% of the original kernel matrix.

torch.Size([11758, 2])
We keep 1.03e+06/3.06e+07 =  3% of the original kernel matrix.

torch.Size([24332, 2])
We keep 2.56e+06/1.48e+08 =  1% of the original kernel matrix.

torch.Size([1643, 2])
We keep 2.14e+04/2.56e+05 =  8% of the original kernel matrix.

torch.Size([11190, 2])
We keep 4.99e+05/1.35e+07 =  3% of the original kernel matrix.

torch.Size([19139, 2])
We keep 1.47e+06/6.81e+07 =  2% of the original kernel matrix.

torch.Size([30812, 2])
We keep 3.34e+06/2.20e+08 =  1% of the original kernel matrix.

torch.Size([3239, 2])
We keep 8.15e+04/1.38e+06 =  5% of the original kernel matrix.

torch.Size([14274, 2])
We keep 8.56e+05/3.13e+07 =  2% of the original kernel matrix.

torch.Size([3741, 2])
We keep 1.22e+05/1.99e+06 =  6% of the original kernel matrix.

torch.Size([15182, 2])
We keep 9.92e+05/3.77e+07 =  2% of the original kernel matrix.

torch.Size([6572, 2])
We keep 2.93e+05/6.93e+06 =  4% of the original kernel matrix.

torch.Size([18960, 2])
We keep 1.50e+06/7.03e+07 =  2% of the original kernel matrix.

torch.Size([4987, 2])
We keep 1.61e+05/3.21e+06 =  5% of the original kernel matrix.

torch.Size([17070, 2])
We keep 1.15e+06/4.78e+07 =  2% of the original kernel matrix.

torch.Size([5086, 2])
We keep 2.03e+05/3.85e+06 =  5% of the original kernel matrix.

torch.Size([17268, 2])
We keep 1.21e+06/5.23e+07 =  2% of the original kernel matrix.

torch.Size([3918, 2])
We keep 9.88e+04/1.99e+06 =  4% of the original kernel matrix.

torch.Size([15632, 2])
We keep 9.75e+05/3.76e+07 =  2% of the original kernel matrix.

torch.Size([6161, 2])
We keep 2.25e+05/5.41e+06 =  4% of the original kernel matrix.

torch.Size([18577, 2])
We keep 1.35e+06/6.20e+07 =  2% of the original kernel matrix.

torch.Size([20508, 2])
We keep 2.07e+06/9.23e+07 =  2% of the original kernel matrix.

torch.Size([32076, 2])
We keep 3.76e+06/2.56e+08 =  1% of the original kernel matrix.

torch.Size([4506, 2])
We keep 1.38e+05/2.94e+06 =  4% of the original kernel matrix.

torch.Size([16519, 2])
We keep 1.11e+06/4.57e+07 =  2% of the original kernel matrix.

torch.Size([8406, 2])
We keep 4.71e+05/1.29e+07 =  3% of the original kernel matrix.

torch.Size([20835, 2])
We keep 1.82e+06/9.58e+07 =  1% of the original kernel matrix.

torch.Size([2473, 2])
We keep 6.30e+04/8.85e+05 =  7% of the original kernel matrix.

torch.Size([12708, 2])
We keep 7.54e+05/2.51e+07 =  3% of the original kernel matrix.

torch.Size([6527, 2])
We keep 3.16e+05/6.79e+06 =  4% of the original kernel matrix.

torch.Size([18846, 2])
We keep 1.48e+06/6.95e+07 =  2% of the original kernel matrix.

torch.Size([2658, 2])
We keep 1.44e+05/1.82e+06 =  7% of the original kernel matrix.

torch.Size([12387, 2])
We keep 9.48e+05/3.60e+07 =  2% of the original kernel matrix.

torch.Size([5999, 2])
We keep 3.70e+05/5.63e+06 =  6% of the original kernel matrix.

torch.Size([18345, 2])
We keep 1.35e+06/6.33e+07 =  2% of the original kernel matrix.

torch.Size([3109, 2])
We keep 8.28e+04/1.43e+06 =  5% of the original kernel matrix.

torch.Size([14069, 2])
We keep 8.66e+05/3.19e+07 =  2% of the original kernel matrix.

torch.Size([2395, 2])
We keep 5.03e+04/7.64e+05 =  6% of the original kernel matrix.

torch.Size([12759, 2])
We keep 6.96e+05/2.33e+07 =  2% of the original kernel matrix.

torch.Size([2775, 2])
We keep 6.01e+04/8.87e+05 =  6% of the original kernel matrix.

torch.Size([13667, 2])
We keep 7.50e+05/2.51e+07 =  2% of the original kernel matrix.

torch.Size([3879, 2])
We keep 1.07e+05/1.89e+06 =  5% of the original kernel matrix.

torch.Size([15445, 2])
We keep 9.58e+05/3.67e+07 =  2% of the original kernel matrix.

torch.Size([3108, 2])
We keep 7.11e+04/1.20e+06 =  5% of the original kernel matrix.

torch.Size([14238, 2])
We keep 8.32e+05/2.93e+07 =  2% of the original kernel matrix.

torch.Size([7701, 2])
We keep 4.58e+05/1.01e+07 =  4% of the original kernel matrix.

torch.Size([20262, 2])
We keep 1.68e+06/8.47e+07 =  1% of the original kernel matrix.

torch.Size([2365, 2])
We keep 5.62e+04/7.66e+05 =  7% of the original kernel matrix.

torch.Size([12759, 2])
We keep 7.26e+05/2.34e+07 =  3% of the original kernel matrix.

torch.Size([15792, 2])
We keep 1.65e+06/5.40e+07 =  3% of the original kernel matrix.

torch.Size([28252, 2])
We keep 3.14e+06/1.96e+08 =  1% of the original kernel matrix.

torch.Size([15194, 2])
We keep 1.27e+06/4.84e+07 =  2% of the original kernel matrix.

torch.Size([27320, 2])
We keep 2.96e+06/1.86e+08 =  1% of the original kernel matrix.

torch.Size([14671, 2])
We keep 1.19e+06/4.18e+07 =  2% of the original kernel matrix.

torch.Size([26980, 2])
We keep 2.82e+06/1.73e+08 =  1% of the original kernel matrix.

torch.Size([2610, 2])
We keep 5.01e+04/7.78e+05 =  6% of the original kernel matrix.

torch.Size([13298, 2])
We keep 7.19e+05/2.35e+07 =  3% of the original kernel matrix.

torch.Size([2287, 2])
We keep 4.54e+04/6.79e+05 =  6% of the original kernel matrix.

torch.Size([12795, 2])
We keep 6.82e+05/2.20e+07 =  3% of the original kernel matrix.

torch.Size([5841, 2])
We keep 5.34e+05/6.98e+06 =  7% of the original kernel matrix.

torch.Size([17745, 2])
We keep 1.50e+06/7.05e+07 =  2% of the original kernel matrix.

torch.Size([3574, 2])
We keep 1.21e+05/2.08e+06 =  5% of the original kernel matrix.

torch.Size([14758, 2])
We keep 9.95e+05/3.85e+07 =  2% of the original kernel matrix.

torch.Size([3551, 2])
We keep 1.39e+05/1.97e+06 =  7% of the original kernel matrix.

torch.Size([14771, 2])
We keep 9.55e+05/3.75e+07 =  2% of the original kernel matrix.

torch.Size([4185, 2])
We keep 1.64e+05/2.97e+06 =  5% of the original kernel matrix.

torch.Size([15547, 2])
We keep 1.10e+06/4.60e+07 =  2% of the original kernel matrix.

torch.Size([7528, 2])
We keep 4.38e+05/8.73e+06 =  5% of the original kernel matrix.

torch.Size([19999, 2])
We keep 1.58e+06/7.89e+07 =  2% of the original kernel matrix.

torch.Size([9938, 2])
We keep 4.54e+05/1.44e+07 =  3% of the original kernel matrix.

torch.Size([22745, 2])
We keep 1.92e+06/1.01e+08 =  1% of the original kernel matrix.

torch.Size([8016, 2])
We keep 1.29e+06/2.22e+07 =  5% of the original kernel matrix.

torch.Size([19531, 2])
We keep 2.26e+06/1.26e+08 =  1% of the original kernel matrix.

torch.Size([5880, 2])
We keep 2.38e+05/4.92e+06 =  4% of the original kernel matrix.

torch.Size([18161, 2])
We keep 1.31e+06/5.92e+07 =  2% of the original kernel matrix.

torch.Size([27448, 2])
We keep 5.00e+06/2.09e+08 =  2% of the original kernel matrix.

torch.Size([37636, 2])
We keep 5.18e+06/3.86e+08 =  1% of the original kernel matrix.

torch.Size([3671, 2])
We keep 9.06e+04/1.65e+06 =  5% of the original kernel matrix.

torch.Size([15215, 2])
We keep 9.15e+05/3.42e+07 =  2% of the original kernel matrix.

torch.Size([4966, 2])
We keep 2.74e+05/4.70e+06 =  5% of the original kernel matrix.

torch.Size([16680, 2])
We keep 1.29e+06/5.79e+07 =  2% of the original kernel matrix.

torch.Size([4255, 2])
We keep 1.44e+05/2.81e+06 =  5% of the original kernel matrix.

torch.Size([15958, 2])
We keep 1.09e+06/4.48e+07 =  2% of the original kernel matrix.

torch.Size([633, 2])
We keep 8.77e+03/5.86e+04 = 14% of the original kernel matrix.

torch.Size([7663, 2])
We keep 3.20e+05/6.46e+06 =  4% of the original kernel matrix.

torch.Size([13504, 2])
We keep 1.39e+06/3.95e+07 =  3% of the original kernel matrix.

torch.Size([25944, 2])
We keep 2.78e+06/1.68e+08 =  1% of the original kernel matrix.

torch.Size([1970, 2])
We keep 4.39e+04/5.21e+05 =  8% of the original kernel matrix.

torch.Size([11735, 2])
We keep 6.23e+05/1.93e+07 =  3% of the original kernel matrix.

torch.Size([8717, 2])
We keep 7.05e+05/1.34e+07 =  5% of the original kernel matrix.

torch.Size([21049, 2])
We keep 1.88e+06/9.77e+07 =  1% of the original kernel matrix.

torch.Size([2225, 2])
We keep 4.22e+04/5.51e+05 =  7% of the original kernel matrix.

torch.Size([12420, 2])
We keep 6.48e+05/1.98e+07 =  3% of the original kernel matrix.

torch.Size([7361, 2])
We keep 4.06e+05/1.02e+07 =  3% of the original kernel matrix.

torch.Size([19632, 2])
We keep 1.68e+06/8.52e+07 =  1% of the original kernel matrix.

torch.Size([4451, 2])
We keep 1.44e+05/2.97e+06 =  4% of the original kernel matrix.

torch.Size([16354, 2])
We keep 1.12e+06/4.60e+07 =  2% of the original kernel matrix.

torch.Size([3092, 2])
We keep 7.80e+04/1.20e+06 =  6% of the original kernel matrix.

torch.Size([14173, 2])
We keep 8.30e+05/2.92e+07 =  2% of the original kernel matrix.

torch.Size([4923, 2])
We keep 1.67e+05/3.43e+06 =  4% of the original kernel matrix.

torch.Size([16887, 2])
We keep 1.17e+06/4.94e+07 =  2% of the original kernel matrix.

torch.Size([5524, 2])
We keep 1.95e+05/4.45e+06 =  4% of the original kernel matrix.

torch.Size([17853, 2])
We keep 1.28e+06/5.63e+07 =  2% of the original kernel matrix.

torch.Size([1360, 2])
We keep 1.50e+04/1.72e+05 =  8% of the original kernel matrix.

torch.Size([10837, 2])
We keep 4.59e+05/1.11e+07 =  4% of the original kernel matrix.

torch.Size([2099, 2])
We keep 4.36e+04/5.57e+05 =  7% of the original kernel matrix.

torch.Size([12073, 2])
We keep 6.47e+05/1.99e+07 =  3% of the original kernel matrix.

torch.Size([2355, 2])
We keep 4.77e+04/6.56e+05 =  7% of the original kernel matrix.

torch.Size([12633, 2])
We keep 6.80e+05/2.16e+07 =  3% of the original kernel matrix.

torch.Size([9610, 2])
We keep 4.76e+05/1.43e+07 =  3% of the original kernel matrix.

torch.Size([22319, 2])
We keep 1.90e+06/1.01e+08 =  1% of the original kernel matrix.

torch.Size([3189, 2])
We keep 7.44e+04/1.30e+06 =  5% of the original kernel matrix.

torch.Size([14357, 2])
We keep 8.42e+05/3.04e+07 =  2% of the original kernel matrix.

torch.Size([5615, 2])
We keep 1.82e+05/4.36e+06 =  4% of the original kernel matrix.

torch.Size([18061, 2])
We keep 1.25e+06/5.57e+07 =  2% of the original kernel matrix.

torch.Size([5056, 2])
We keep 2.69e+05/4.25e+06 =  6% of the original kernel matrix.

torch.Size([17154, 2])
We keep 1.27e+06/5.50e+07 =  2% of the original kernel matrix.

torch.Size([5874, 2])
We keep 2.26e+05/5.22e+06 =  4% of the original kernel matrix.

torch.Size([18093, 2])
We keep 1.35e+06/6.10e+07 =  2% of the original kernel matrix.

torch.Size([7579, 2])
We keep 3.34e+05/8.38e+06 =  3% of the original kernel matrix.

torch.Size([20100, 2])
We keep 1.58e+06/7.73e+07 =  2% of the original kernel matrix.

torch.Size([6986, 2])
We keep 5.26e+05/1.05e+07 =  5% of the original kernel matrix.

torch.Size([18916, 2])
We keep 1.72e+06/8.64e+07 =  1% of the original kernel matrix.

torch.Size([2536, 2])
We keep 6.40e+04/8.91e+05 =  7% of the original kernel matrix.

torch.Size([13130, 2])
We keep 7.51e+05/2.52e+07 =  2% of the original kernel matrix.

torch.Size([8406, 2])
We keep 4.08e+05/1.15e+07 =  3% of the original kernel matrix.

torch.Size([21063, 2])
We keep 1.76e+06/9.05e+07 =  1% of the original kernel matrix.

torch.Size([4944, 2])
We keep 1.94e+05/3.78e+06 =  5% of the original kernel matrix.

torch.Size([16761, 2])
We keep 1.21e+06/5.19e+07 =  2% of the original kernel matrix.

torch.Size([5013, 2])
We keep 4.63e+05/7.39e+06 =  6% of the original kernel matrix.

torch.Size([16030, 2])
We keep 1.49e+06/7.25e+07 =  2% of the original kernel matrix.

torch.Size([3598, 2])
We keep 9.30e+04/1.69e+06 =  5% of the original kernel matrix.

torch.Size([15070, 2])
We keep 9.15e+05/3.47e+07 =  2% of the original kernel matrix.

torch.Size([9178, 2])
We keep 4.45e+05/1.31e+07 =  3% of the original kernel matrix.

torch.Size([21867, 2])
We keep 1.84e+06/9.67e+07 =  1% of the original kernel matrix.

torch.Size([5879, 2])
We keep 2.05e+05/4.90e+06 =  4% of the original kernel matrix.

torch.Size([18137, 2])
We keep 1.31e+06/5.91e+07 =  2% of the original kernel matrix.

torch.Size([4410, 2])
We keep 1.46e+05/2.97e+06 =  4% of the original kernel matrix.

torch.Size([16197, 2])
We keep 1.11e+06/4.60e+07 =  2% of the original kernel matrix.

torch.Size([6128, 2])
We keep 5.71e+05/9.16e+06 =  6% of the original kernel matrix.

torch.Size([17659, 2])
We keep 1.64e+06/8.08e+07 =  2% of the original kernel matrix.

torch.Size([9843, 2])
We keep 5.67e+05/1.62e+07 =  3% of the original kernel matrix.

torch.Size([22410, 2])
We keep 2.00e+06/1.08e+08 =  1% of the original kernel matrix.

torch.Size([21136, 2])
We keep 2.31e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([32457, 2])
We keep 3.93e+06/2.70e+08 =  1% of the original kernel matrix.

torch.Size([3915, 2])
We keep 1.43e+05/2.59e+06 =  5% of the original kernel matrix.

torch.Size([15148, 2])
We keep 1.07e+06/4.30e+07 =  2% of the original kernel matrix.

torch.Size([1514, 2])
We keep 2.00e+04/2.19e+05 =  9% of the original kernel matrix.

torch.Size([10833, 2])
We keep 4.78e+05/1.25e+07 =  3% of the original kernel matrix.

torch.Size([2997, 2])
We keep 6.56e+04/1.13e+06 =  5% of the original kernel matrix.

torch.Size([14024, 2])
We keep 8.11e+05/2.83e+07 =  2% of the original kernel matrix.

torch.Size([3105, 2])
We keep 1.60e+05/1.68e+06 =  9% of the original kernel matrix.

torch.Size([13977, 2])
We keep 9.20e+05/3.46e+07 =  2% of the original kernel matrix.

torch.Size([4445, 2])
We keep 3.35e+05/5.35e+06 =  6% of the original kernel matrix.

torch.Size([15386, 2])
We keep 1.37e+06/6.17e+07 =  2% of the original kernel matrix.

torch.Size([2721, 2])
We keep 5.88e+04/9.80e+05 =  5% of the original kernel matrix.

torch.Size([13456, 2])
We keep 7.75e+05/2.64e+07 =  2% of the original kernel matrix.

torch.Size([5246, 2])
We keep 2.04e+05/4.00e+06 =  5% of the original kernel matrix.

torch.Size([17540, 2])
We keep 1.25e+06/5.34e+07 =  2% of the original kernel matrix.

torch.Size([2018, 2])
We keep 3.70e+04/4.46e+05 =  8% of the original kernel matrix.

torch.Size([12051, 2])
We keep 6.09e+05/1.78e+07 =  3% of the original kernel matrix.

torch.Size([4916, 2])
We keep 2.44e+05/3.91e+06 =  6% of the original kernel matrix.

torch.Size([16814, 2])
We keep 1.24e+06/5.28e+07 =  2% of the original kernel matrix.

torch.Size([1876, 2])
We keep 2.81e+04/3.73e+05 =  7% of the original kernel matrix.

torch.Size([11924, 2])
We keep 5.60e+05/1.63e+07 =  3% of the original kernel matrix.

torch.Size([7909, 2])
We keep 3.18e+05/9.09e+06 =  3% of the original kernel matrix.

torch.Size([20633, 2])
We keep 1.62e+06/8.05e+07 =  2% of the original kernel matrix.

torch.Size([5232, 2])
We keep 1.51e+05/3.53e+06 =  4% of the original kernel matrix.

torch.Size([17607, 2])
We keep 1.18e+06/5.01e+07 =  2% of the original kernel matrix.

torch.Size([23945, 2])
We keep 2.62e+06/1.34e+08 =  1% of the original kernel matrix.

torch.Size([34740, 2])
We keep 4.39e+06/3.09e+08 =  1% of the original kernel matrix.

torch.Size([10460, 2])
We keep 7.33e+05/2.11e+07 =  3% of the original kernel matrix.

torch.Size([22979, 2])
We keep 2.19e+06/1.23e+08 =  1% of the original kernel matrix.

torch.Size([7663, 2])
We keep 3.92e+05/9.64e+06 =  4% of the original kernel matrix.

torch.Size([20348, 2])
We keep 1.67e+06/8.29e+07 =  2% of the original kernel matrix.

torch.Size([1578, 2])
We keep 2.21e+04/2.47e+05 =  8% of the original kernel matrix.

torch.Size([11042, 2])
We keep 5.05e+05/1.33e+07 =  3% of the original kernel matrix.

torch.Size([6402, 2])
We keep 3.17e+05/6.58e+06 =  4% of the original kernel matrix.

torch.Size([18950, 2])
We keep 1.47e+06/6.85e+07 =  2% of the original kernel matrix.

torch.Size([7404, 2])
We keep 3.33e+05/8.34e+06 =  3% of the original kernel matrix.

torch.Size([19977, 2])
We keep 1.58e+06/7.71e+07 =  2% of the original kernel matrix.

torch.Size([2847, 2])
We keep 8.33e+04/1.17e+06 =  7% of the original kernel matrix.

torch.Size([13640, 2])
We keep 8.18e+05/2.88e+07 =  2% of the original kernel matrix.

torch.Size([21178, 2])
We keep 2.20e+06/9.97e+07 =  2% of the original kernel matrix.

torch.Size([32657, 2])
We keep 3.94e+06/2.66e+08 =  1% of the original kernel matrix.

torch.Size([20993, 2])
We keep 2.46e+06/9.99e+07 =  2% of the original kernel matrix.

torch.Size([32491, 2])
We keep 3.92e+06/2.67e+08 =  1% of the original kernel matrix.

torch.Size([2716, 2])
We keep 9.52e+04/1.32e+06 =  7% of the original kernel matrix.

torch.Size([13120, 2])
We keep 8.61e+05/3.07e+07 =  2% of the original kernel matrix.

torch.Size([2275, 2])
We keep 4.33e+04/5.98e+05 =  7% of the original kernel matrix.

torch.Size([12719, 2])
We keep 6.69e+05/2.06e+07 =  3% of the original kernel matrix.

torch.Size([4359, 2])
We keep 1.21e+05/2.27e+06 =  5% of the original kernel matrix.

torch.Size([16005, 2])
We keep 1.00e+06/4.02e+07 =  2% of the original kernel matrix.

torch.Size([1758, 2])
We keep 2.97e+04/3.49e+05 =  8% of the original kernel matrix.

torch.Size([11422, 2])
We keep 5.61e+05/1.58e+07 =  3% of the original kernel matrix.

torch.Size([1848, 2])
We keep 4.32e+04/4.76e+05 =  9% of the original kernel matrix.

torch.Size([11396, 2])
We keep 6.19e+05/1.84e+07 =  3% of the original kernel matrix.

torch.Size([1184, 2])
We keep 1.33e+04/1.41e+05 =  9% of the original kernel matrix.

torch.Size([10215, 2])
We keep 4.16e+05/1.00e+07 =  4% of the original kernel matrix.

torch.Size([2113, 2])
We keep 4.44e+04/5.96e+05 =  7% of the original kernel matrix.

torch.Size([12254, 2])
We keep 6.71e+05/2.06e+07 =  3% of the original kernel matrix.

torch.Size([13514, 2])
We keep 1.83e+06/4.40e+07 =  4% of the original kernel matrix.

torch.Size([25843, 2])
We keep 2.90e+06/1.77e+08 =  1% of the original kernel matrix.

torch.Size([1959, 2])
We keep 4.65e+04/5.08e+05 =  9% of the original kernel matrix.

torch.Size([11619, 2])
We keep 6.21e+05/1.90e+07 =  3% of the original kernel matrix.

torch.Size([3048, 2])
We keep 1.99e+05/2.54e+06 =  7% of the original kernel matrix.

torch.Size([13019, 2])
We keep 1.06e+06/4.26e+07 =  2% of the original kernel matrix.

torch.Size([2686, 2])
We keep 6.97e+04/1.02e+06 =  6% of the original kernel matrix.

torch.Size([13393, 2])
We keep 7.80e+05/2.69e+07 =  2% of the original kernel matrix.

torch.Size([9009, 2])
We keep 5.50e+05/1.37e+07 =  4% of the original kernel matrix.

torch.Size([21488, 2])
We keep 1.89e+06/9.87e+07 =  1% of the original kernel matrix.

torch.Size([3843, 2])
We keep 1.74e+05/2.47e+06 =  7% of the original kernel matrix.

torch.Size([15159, 2])
We keep 1.05e+06/4.20e+07 =  2% of the original kernel matrix.

torch.Size([8446, 2])
We keep 5.31e+05/1.35e+07 =  3% of the original kernel matrix.

torch.Size([20775, 2])
We keep 1.85e+06/9.82e+07 =  1% of the original kernel matrix.

torch.Size([11297, 2])
We keep 9.39e+05/2.39e+07 =  3% of the original kernel matrix.

torch.Size([24004, 2])
We keep 2.35e+06/1.31e+08 =  1% of the original kernel matrix.

torch.Size([2621, 2])
We keep 5.61e+04/8.37e+05 =  6% of the original kernel matrix.

torch.Size([13267, 2])
We keep 7.42e+05/2.44e+07 =  3% of the original kernel matrix.

torch.Size([2438, 2])
We keep 6.75e+04/8.85e+05 =  7% of the original kernel matrix.

torch.Size([12634, 2])
We keep 7.60e+05/2.51e+07 =  3% of the original kernel matrix.

torch.Size([13788, 2])
We keep 1.15e+06/4.12e+07 =  2% of the original kernel matrix.

torch.Size([25893, 2])
We keep 2.80e+06/1.71e+08 =  1% of the original kernel matrix.

torch.Size([1763, 2])
We keep 2.47e+04/2.92e+05 =  8% of the original kernel matrix.

torch.Size([11542, 2])
We keep 5.25e+05/1.44e+07 =  3% of the original kernel matrix.

torch.Size([2434, 2])
We keep 6.76e+04/1.01e+06 =  6% of the original kernel matrix.

torch.Size([12734, 2])
We keep 7.81e+05/2.69e+07 =  2% of the original kernel matrix.

torch.Size([4440, 2])
We keep 1.90e+05/3.10e+06 =  6% of the original kernel matrix.

torch.Size([16023, 2])
We keep 1.12e+06/4.70e+07 =  2% of the original kernel matrix.

torch.Size([3775, 2])
We keep 1.31e+05/2.19e+06 =  5% of the original kernel matrix.

torch.Size([15173, 2])
We keep 1.01e+06/3.95e+07 =  2% of the original kernel matrix.

torch.Size([8432, 2])
We keep 5.65e+05/1.27e+07 =  4% of the original kernel matrix.

torch.Size([20804, 2])
We keep 1.81e+06/9.52e+07 =  1% of the original kernel matrix.

torch.Size([5247, 2])
We keep 1.71e+05/3.52e+06 =  4% of the original kernel matrix.

torch.Size([17542, 2])
We keep 1.19e+06/5.01e+07 =  2% of the original kernel matrix.

torch.Size([1907, 2])
We keep 5.60e+04/6.53e+05 =  8% of the original kernel matrix.

torch.Size([11550, 2])
We keep 6.92e+05/2.16e+07 =  3% of the original kernel matrix.

torch.Size([9628, 2])
We keep 5.83e+05/1.65e+07 =  3% of the original kernel matrix.

torch.Size([22072, 2])
We keep 2.02e+06/1.08e+08 =  1% of the original kernel matrix.

torch.Size([6930, 2])
We keep 3.22e+05/7.52e+06 =  4% of the original kernel matrix.

torch.Size([19472, 2])
We keep 1.50e+06/7.32e+07 =  2% of the original kernel matrix.

torch.Size([2955, 2])
We keep 8.79e+04/1.22e+06 =  7% of the original kernel matrix.

torch.Size([13753, 2])
We keep 8.29e+05/2.95e+07 =  2% of the original kernel matrix.

torch.Size([19277, 2])
We keep 3.22e+06/9.42e+07 =  3% of the original kernel matrix.

torch.Size([30880, 2])
We keep 3.82e+06/2.59e+08 =  1% of the original kernel matrix.

torch.Size([38970, 2])
We keep 4.33e+07/1.36e+09 =  3% of the original kernel matrix.

torch.Size([42068, 2])
We keep 1.09e+07/9.82e+08 =  1% of the original kernel matrix.

torch.Size([5234, 2])
We keep 2.31e+05/4.38e+06 =  5% of the original kernel matrix.

torch.Size([17498, 2])
We keep 1.17e+06/5.58e+07 =  2% of the original kernel matrix.

torch.Size([7360, 2])
We keep 4.19e+05/9.79e+06 =  4% of the original kernel matrix.

torch.Size([19835, 2])
We keep 1.67e+06/8.35e+07 =  2% of the original kernel matrix.

torch.Size([1383, 2])
We keep 2.83e+04/2.61e+05 = 10% of the original kernel matrix.

torch.Size([10327, 2])
We keep 5.13e+05/1.36e+07 =  3% of the original kernel matrix.

torch.Size([10221, 2])
We keep 5.46e+05/1.66e+07 =  3% of the original kernel matrix.

torch.Size([22837, 2])
We keep 2.01e+06/1.09e+08 =  1% of the original kernel matrix.

torch.Size([18872, 2])
We keep 1.68e+06/7.00e+07 =  2% of the original kernel matrix.

torch.Size([30680, 2])
We keep 3.35e+06/2.23e+08 =  1% of the original kernel matrix.

torch.Size([63314, 2])
We keep 7.71e+07/2.69e+09 =  2% of the original kernel matrix.

torch.Size([54194, 2])
We keep 1.41e+07/1.38e+09 =  1% of the original kernel matrix.

torch.Size([6031, 2])
We keep 2.35e+05/5.19e+06 =  4% of the original kernel matrix.

torch.Size([18604, 2])
We keep 1.34e+06/6.08e+07 =  2% of the original kernel matrix.

torch.Size([115824, 2])
We keep 8.33e+07/5.37e+09 =  1% of the original kernel matrix.

torch.Size([74234, 2])
We keep 1.92e+07/1.96e+09 =  0% of the original kernel matrix.

torch.Size([45731, 2])
We keep 1.95e+07/9.22e+08 =  2% of the original kernel matrix.

torch.Size([47411, 2])
We keep 9.28e+06/8.10e+08 =  1% of the original kernel matrix.

torch.Size([23566, 2])
We keep 4.62e+06/1.74e+08 =  2% of the original kernel matrix.

torch.Size([33967, 2])
We keep 4.71e+06/3.52e+08 =  1% of the original kernel matrix.

torch.Size([72259, 2])
We keep 1.26e+08/2.68e+09 =  4% of the original kernel matrix.

torch.Size([59646, 2])
We keep 1.30e+07/1.38e+09 =  0% of the original kernel matrix.

torch.Size([6283, 2])
We keep 2.19e+05/5.36e+06 =  4% of the original kernel matrix.

torch.Size([18840, 2])
We keep 1.36e+06/6.18e+07 =  2% of the original kernel matrix.

torch.Size([12043, 2])
We keep 7.28e+05/2.36e+07 =  3% of the original kernel matrix.

torch.Size([24638, 2])
We keep 2.28e+06/1.30e+08 =  1% of the original kernel matrix.

torch.Size([154092, 2])
We keep 1.46e+08/1.06e+10 =  1% of the original kernel matrix.

torch.Size([85608, 2])
We keep 2.63e+07/2.75e+09 =  0% of the original kernel matrix.

torch.Size([570086, 2])
We keep 7.51e+08/8.77e+10 =  0% of the original kernel matrix.

torch.Size([174660, 2])
We keep 6.50e+07/7.90e+09 =  0% of the original kernel matrix.

torch.Size([6732, 2])
We keep 5.16e+05/8.53e+06 =  6% of the original kernel matrix.

torch.Size([18941, 2])
We keep 1.61e+06/7.80e+07 =  2% of the original kernel matrix.

torch.Size([44954, 2])
We keep 1.26e+07/6.16e+08 =  2% of the original kernel matrix.

torch.Size([45571, 2])
We keep 7.29e+06/6.62e+08 =  1% of the original kernel matrix.

torch.Size([24836, 2])
We keep 6.72e+06/2.04e+08 =  3% of the original kernel matrix.

torch.Size([35223, 2])
We keep 5.14e+06/3.81e+08 =  1% of the original kernel matrix.

torch.Size([41821, 2])
We keep 3.19e+07/6.27e+08 =  5% of the original kernel matrix.

torch.Size([44438, 2])
We keep 7.30e+06/6.68e+08 =  1% of the original kernel matrix.

torch.Size([39950, 2])
We keep 2.86e+07/9.57e+08 =  2% of the original kernel matrix.

torch.Size([43974, 2])
We keep 9.29e+06/8.26e+08 =  1% of the original kernel matrix.

torch.Size([33124, 2])
We keep 3.44e+07/6.45e+08 =  5% of the original kernel matrix.

torch.Size([38864, 2])
We keep 7.70e+06/6.78e+08 =  1% of the original kernel matrix.

torch.Size([26371, 2])
We keep 6.85e+06/2.40e+08 =  2% of the original kernel matrix.

torch.Size([36480, 2])
We keep 5.43e+06/4.14e+08 =  1% of the original kernel matrix.

torch.Size([22797, 2])
We keep 3.96e+06/1.30e+08 =  3% of the original kernel matrix.

torch.Size([33920, 2])
We keep 4.33e+06/3.05e+08 =  1% of the original kernel matrix.

torch.Size([7899, 2])
We keep 2.80e+06/2.15e+07 = 13% of the original kernel matrix.

torch.Size([20090, 2])
We keep 2.24e+06/1.24e+08 =  1% of the original kernel matrix.

torch.Size([103630, 2])
We keep 2.22e+08/9.72e+09 =  2% of the original kernel matrix.

torch.Size([66283, 2])
We keep 2.45e+07/2.63e+09 =  0% of the original kernel matrix.

torch.Size([6563, 2])
We keep 4.06e+05/7.78e+06 =  5% of the original kernel matrix.

torch.Size([18968, 2])
We keep 1.55e+06/7.45e+07 =  2% of the original kernel matrix.

torch.Size([5914, 2])
We keep 2.12e+05/5.09e+06 =  4% of the original kernel matrix.

torch.Size([18253, 2])
We keep 1.33e+06/6.02e+07 =  2% of the original kernel matrix.

torch.Size([27520, 2])
We keep 4.69e+06/2.11e+08 =  2% of the original kernel matrix.

torch.Size([37459, 2])
We keep 5.07e+06/3.88e+08 =  1% of the original kernel matrix.

torch.Size([55589, 2])
We keep 4.10e+07/1.44e+09 =  2% of the original kernel matrix.

torch.Size([51004, 2])
We keep 1.11e+07/1.01e+09 =  1% of the original kernel matrix.

torch.Size([436887, 2])
We keep 6.49e+08/6.77e+10 =  0% of the original kernel matrix.

torch.Size([152724, 2])
We keep 5.92e+07/6.94e+09 =  0% of the original kernel matrix.

torch.Size([122617, 2])
We keep 4.29e+07/4.14e+09 =  1% of the original kernel matrix.

torch.Size([77530, 2])
We keep 1.73e+07/1.72e+09 =  1% of the original kernel matrix.

torch.Size([1934, 2])
We keep 3.83e+04/5.03e+05 =  7% of the original kernel matrix.

torch.Size([11768, 2])
We keep 6.30e+05/1.89e+07 =  3% of the original kernel matrix.

torch.Size([20809, 2])
We keep 3.22e+06/1.11e+08 =  2% of the original kernel matrix.

torch.Size([32282, 2])
We keep 4.12e+06/2.81e+08 =  1% of the original kernel matrix.

torch.Size([10635, 2])
We keep 1.02e+06/2.54e+07 =  4% of the original kernel matrix.

torch.Size([22978, 2])
We keep 2.36e+06/1.35e+08 =  1% of the original kernel matrix.

torch.Size([25798, 2])
We keep 5.83e+06/2.31e+08 =  2% of the original kernel matrix.

torch.Size([35942, 2])
We keep 5.30e+06/4.05e+08 =  1% of the original kernel matrix.

torch.Size([80523, 2])
We keep 2.72e+07/1.90e+09 =  1% of the original kernel matrix.

torch.Size([63453, 2])
We keep 1.24e+07/1.16e+09 =  1% of the original kernel matrix.

torch.Size([22498, 2])
We keep 1.47e+07/2.29e+08 =  6% of the original kernel matrix.

torch.Size([33102, 2])
We keep 5.52e+06/4.04e+08 =  1% of the original kernel matrix.

torch.Size([62870, 2])
We keep 1.92e+07/1.25e+09 =  1% of the original kernel matrix.

torch.Size([56298, 2])
We keep 1.05e+07/9.45e+08 =  1% of the original kernel matrix.

torch.Size([1715, 2])
We keep 2.48e+04/2.99e+05 =  8% of the original kernel matrix.

torch.Size([11365, 2])
We keep 5.23e+05/1.46e+07 =  3% of the original kernel matrix.

torch.Size([8340, 2])
We keep 3.93e+05/1.05e+07 =  3% of the original kernel matrix.

torch.Size([20962, 2])
We keep 1.71e+06/8.65e+07 =  1% of the original kernel matrix.

torch.Size([60212, 2])
We keep 4.38e+07/2.17e+09 =  2% of the original kernel matrix.

torch.Size([51144, 2])
We keep 1.32e+07/1.24e+09 =  1% of the original kernel matrix.

torch.Size([46996, 2])
We keep 2.02e+07/1.00e+09 =  2% of the original kernel matrix.

torch.Size([47272, 2])
We keep 9.62e+06/8.45e+08 =  1% of the original kernel matrix.

torch.Size([45823, 2])
We keep 1.76e+07/8.29e+08 =  2% of the original kernel matrix.

torch.Size([47593, 2])
We keep 8.74e+06/7.68e+08 =  1% of the original kernel matrix.

torch.Size([7628, 2])
We keep 5.29e+05/1.25e+07 =  4% of the original kernel matrix.

torch.Size([19586, 2])
We keep 1.81e+06/9.44e+07 =  1% of the original kernel matrix.

torch.Size([332360, 2])
We keep 2.84e+08/4.41e+10 =  0% of the original kernel matrix.

torch.Size([132806, 2])
We keep 4.78e+07/5.60e+09 =  0% of the original kernel matrix.

torch.Size([13518, 2])
We keep 7.72e+05/2.96e+07 =  2% of the original kernel matrix.

torch.Size([26253, 2])
We keep 2.46e+06/1.45e+08 =  1% of the original kernel matrix.

torch.Size([37329, 2])
We keep 5.92e+06/3.81e+08 =  1% of the original kernel matrix.

torch.Size([43773, 2])
We keep 6.47e+06/5.21e+08 =  1% of the original kernel matrix.

torch.Size([123794, 2])
We keep 6.79e+07/5.27e+09 =  1% of the original kernel matrix.

torch.Size([77753, 2])
We keep 1.92e+07/1.94e+09 =  0% of the original kernel matrix.

torch.Size([5021, 2])
We keep 1.54e+05/3.33e+06 =  4% of the original kernel matrix.

torch.Size([17213, 2])
We keep 1.15e+06/4.87e+07 =  2% of the original kernel matrix.

torch.Size([79687, 2])
We keep 3.10e+07/1.85e+09 =  1% of the original kernel matrix.

torch.Size([63117, 2])
We keep 1.24e+07/1.15e+09 =  1% of the original kernel matrix.

torch.Size([190866, 2])
We keep 3.33e+08/1.74e+10 =  1% of the original kernel matrix.

torch.Size([98025, 2])
We keep 3.26e+07/3.52e+09 =  0% of the original kernel matrix.

torch.Size([248256, 2])
We keep 3.48e+08/2.79e+10 =  1% of the original kernel matrix.

torch.Size([112927, 2])
We keep 3.96e+07/4.46e+09 =  0% of the original kernel matrix.

torch.Size([29140, 2])
We keep 6.01e+06/2.95e+08 =  2% of the original kernel matrix.

torch.Size([38254, 2])
We keep 5.89e+06/4.58e+08 =  1% of the original kernel matrix.

torch.Size([28349, 2])
We keep 4.50e+06/2.39e+08 =  1% of the original kernel matrix.

torch.Size([37758, 2])
We keep 5.40e+06/4.12e+08 =  1% of the original kernel matrix.

torch.Size([5132, 2])
We keep 2.99e+05/5.18e+06 =  5% of the original kernel matrix.

torch.Size([16550, 2])
We keep 1.33e+06/6.07e+07 =  2% of the original kernel matrix.

torch.Size([9777, 2])
We keep 4.43e+05/1.37e+07 =  3% of the original kernel matrix.

torch.Size([22488, 2])
We keep 1.87e+06/9.88e+07 =  1% of the original kernel matrix.

torch.Size([78539, 2])
We keep 5.65e+07/2.83e+09 =  1% of the original kernel matrix.

torch.Size([61092, 2])
We keep 1.49e+07/1.42e+09 =  1% of the original kernel matrix.

torch.Size([16102, 2])
We keep 1.24e+06/4.88e+07 =  2% of the original kernel matrix.

torch.Size([28288, 2])
We keep 2.96e+06/1.86e+08 =  1% of the original kernel matrix.

torch.Size([30187, 2])
We keep 6.03e+06/2.86e+08 =  2% of the original kernel matrix.

torch.Size([39060, 2])
We keep 5.69e+06/4.51e+08 =  1% of the original kernel matrix.

torch.Size([42071, 2])
We keep 2.07e+07/7.23e+08 =  2% of the original kernel matrix.

torch.Size([45998, 2])
We keep 7.64e+06/7.18e+08 =  1% of the original kernel matrix.

torch.Size([25030, 2])
We keep 8.99e+06/2.09e+08 =  4% of the original kernel matrix.

torch.Size([35440, 2])
We keep 5.22e+06/3.86e+08 =  1% of the original kernel matrix.

torch.Size([18755, 2])
We keep 8.14e+06/1.83e+08 =  4% of the original kernel matrix.

torch.Size([29226, 2])
We keep 4.98e+06/3.61e+08 =  1% of the original kernel matrix.

torch.Size([7518, 2])
We keep 2.71e+05/8.01e+06 =  3% of the original kernel matrix.

torch.Size([20255, 2])
We keep 1.52e+06/7.56e+07 =  2% of the original kernel matrix.

torch.Size([56719, 2])
We keep 1.43e+08/5.51e+09 =  2% of the original kernel matrix.

torch.Size([46413, 2])
We keep 1.99e+07/1.98e+09 =  1% of the original kernel matrix.

torch.Size([27275, 2])
We keep 2.18e+07/2.61e+08 =  8% of the original kernel matrix.

torch.Size([36805, 2])
We keep 5.58e+06/4.31e+08 =  1% of the original kernel matrix.

torch.Size([15707, 2])
We keep 9.25e+06/1.24e+08 =  7% of the original kernel matrix.

torch.Size([27165, 2])
We keep 4.30e+06/2.97e+08 =  1% of the original kernel matrix.

torch.Size([24564, 2])
We keep 2.89e+06/1.49e+08 =  1% of the original kernel matrix.

torch.Size([35142, 2])
We keep 4.48e+06/3.26e+08 =  1% of the original kernel matrix.

torch.Size([172621, 2])
We keep 3.21e+08/1.93e+10 =  1% of the original kernel matrix.

torch.Size([91267, 2])
We keep 3.24e+07/3.71e+09 =  0% of the original kernel matrix.

torch.Size([18537, 2])
We keep 2.09e+06/7.42e+07 =  2% of the original kernel matrix.

torch.Size([30323, 2])
We keep 3.46e+06/2.30e+08 =  1% of the original kernel matrix.

torch.Size([162269, 2])
We keep 2.76e+08/1.44e+10 =  1% of the original kernel matrix.

torch.Size([88196, 2])
We keep 2.89e+07/3.21e+09 =  0% of the original kernel matrix.

torch.Size([5303, 2])
We keep 3.84e+05/6.59e+06 =  5% of the original kernel matrix.

torch.Size([17177, 2])
We keep 1.40e+06/6.85e+07 =  2% of the original kernel matrix.

torch.Size([22675, 2])
We keep 1.29e+07/3.96e+08 =  3% of the original kernel matrix.

torch.Size([31131, 2])
We keep 6.74e+06/5.31e+08 =  1% of the original kernel matrix.

torch.Size([16606, 2])
We keep 1.37e+06/5.54e+07 =  2% of the original kernel matrix.

torch.Size([28915, 2])
We keep 3.09e+06/1.99e+08 =  1% of the original kernel matrix.

torch.Size([68270, 2])
We keep 7.29e+07/2.80e+09 =  2% of the original kernel matrix.

torch.Size([55276, 2])
We keep 1.51e+07/1.41e+09 =  1% of the original kernel matrix.

torch.Size([47473, 2])
We keep 1.03e+07/8.64e+08 =  1% of the original kernel matrix.

torch.Size([48571, 2])
We keep 9.02e+06/7.85e+08 =  1% of the original kernel matrix.

torch.Size([10497, 2])
We keep 5.73e+05/1.80e+07 =  3% of the original kernel matrix.

torch.Size([23289, 2])
We keep 2.05e+06/1.13e+08 =  1% of the original kernel matrix.

torch.Size([144204, 2])
We keep 5.46e+07/5.95e+09 =  0% of the original kernel matrix.

torch.Size([84861, 2])
We keep 2.01e+07/2.06e+09 =  0% of the original kernel matrix.

torch.Size([186351, 2])
We keep 8.72e+07/9.74e+09 =  0% of the original kernel matrix.

torch.Size([97398, 2])
We keep 2.48e+07/2.63e+09 =  0% of the original kernel matrix.

torch.Size([5616, 2])
We keep 2.28e+05/5.22e+06 =  4% of the original kernel matrix.

torch.Size([17823, 2])
We keep 1.34e+06/6.10e+07 =  2% of the original kernel matrix.

torch.Size([115031, 2])
We keep 9.77e+07/3.93e+09 =  2% of the original kernel matrix.

torch.Size([74953, 2])
We keep 1.63e+07/1.67e+09 =  0% of the original kernel matrix.

torch.Size([211813, 2])
We keep 1.20e+08/1.55e+10 =  0% of the original kernel matrix.

torch.Size([104599, 2])
We keep 3.05e+07/3.32e+09 =  0% of the original kernel matrix.

torch.Size([532888, 2])
We keep 5.31e+08/7.97e+10 =  0% of the original kernel matrix.

torch.Size([169302, 2])
We keep 6.17e+07/7.53e+09 =  0% of the original kernel matrix.

torch.Size([34685, 2])
We keep 3.17e+07/5.23e+08 =  6% of the original kernel matrix.

torch.Size([41682, 2])
We keep 7.66e+06/6.10e+08 =  1% of the original kernel matrix.

torch.Size([2301, 2])
We keep 7.76e+04/1.02e+06 =  7% of the original kernel matrix.

torch.Size([12323, 2])
We keep 7.81e+05/2.69e+07 =  2% of the original kernel matrix.

torch.Size([8837, 2])
We keep 4.09e+05/1.16e+07 =  3% of the original kernel matrix.

torch.Size([21580, 2])
We keep 1.75e+06/9.07e+07 =  1% of the original kernel matrix.

torch.Size([10822, 2])
We keep 1.91e+06/4.55e+07 =  4% of the original kernel matrix.

torch.Size([23347, 2])
We keep 2.81e+06/1.80e+08 =  1% of the original kernel matrix.

torch.Size([28349, 2])
We keep 3.97e+06/2.16e+08 =  1% of the original kernel matrix.

torch.Size([37983, 2])
We keep 5.23e+06/3.93e+08 =  1% of the original kernel matrix.

torch.Size([30483, 2])
We keep 1.44e+07/3.01e+08 =  4% of the original kernel matrix.

torch.Size([39383, 2])
We keep 5.94e+06/4.63e+08 =  1% of the original kernel matrix.

torch.Size([3689, 2])
We keep 8.97e+04/1.60e+06 =  5% of the original kernel matrix.

torch.Size([15239, 2])
We keep 9.13e+05/3.38e+07 =  2% of the original kernel matrix.

torch.Size([77827, 2])
We keep 2.27e+07/1.73e+09 =  1% of the original kernel matrix.

torch.Size([62600, 2])
We keep 1.20e+07/1.11e+09 =  1% of the original kernel matrix.

torch.Size([137506, 2])
We keep 5.73e+07/5.55e+09 =  1% of the original kernel matrix.

torch.Size([82023, 2])
We keep 1.95e+07/1.99e+09 =  0% of the original kernel matrix.

torch.Size([6406, 2])
We keep 5.69e+06/5.79e+07 =  9% of the original kernel matrix.

torch.Size([16325, 2])
We keep 3.03e+06/2.03e+08 =  1% of the original kernel matrix.

torch.Size([9078, 2])
We keep 7.26e+05/1.60e+07 =  4% of the original kernel matrix.

torch.Size([21640, 2])
We keep 2.00e+06/1.07e+08 =  1% of the original kernel matrix.

torch.Size([24339, 2])
We keep 3.68e+06/1.74e+08 =  2% of the original kernel matrix.

torch.Size([34727, 2])
We keep 4.83e+06/3.53e+08 =  1% of the original kernel matrix.

torch.Size([24649, 2])
We keep 3.05e+06/1.57e+08 =  1% of the original kernel matrix.

torch.Size([35584, 2])
We keep 4.65e+06/3.34e+08 =  1% of the original kernel matrix.

torch.Size([20373, 2])
We keep 3.20e+06/1.05e+08 =  3% of the original kernel matrix.

torch.Size([31831, 2])
We keep 4.05e+06/2.74e+08 =  1% of the original kernel matrix.

torch.Size([21054, 2])
We keep 2.17e+06/9.79e+07 =  2% of the original kernel matrix.

torch.Size([32408, 2])
We keep 3.88e+06/2.64e+08 =  1% of the original kernel matrix.

torch.Size([5986, 2])
We keep 1.96e+05/4.56e+06 =  4% of the original kernel matrix.

torch.Size([18524, 2])
We keep 1.26e+06/5.70e+07 =  2% of the original kernel matrix.

torch.Size([68671, 2])
We keep 8.71e+07/2.37e+09 =  3% of the original kernel matrix.

torch.Size([56843, 2])
We keep 1.39e+07/1.30e+09 =  1% of the original kernel matrix.

torch.Size([43743, 2])
We keep 7.13e+06/5.57e+08 =  1% of the original kernel matrix.

torch.Size([48432, 2])
We keep 7.60e+06/6.30e+08 =  1% of the original kernel matrix.

torch.Size([25337, 2])
We keep 4.06e+06/1.61e+08 =  2% of the original kernel matrix.

torch.Size([35861, 2])
We keep 4.67e+06/3.39e+08 =  1% of the original kernel matrix.

torch.Size([27959, 2])
We keep 8.33e+06/3.07e+08 =  2% of the original kernel matrix.

torch.Size([37333, 2])
We keep 5.91e+06/4.68e+08 =  1% of the original kernel matrix.

torch.Size([10887, 2])
We keep 1.89e+06/4.15e+07 =  4% of the original kernel matrix.

torch.Size([22447, 2])
We keep 2.81e+06/1.72e+08 =  1% of the original kernel matrix.

torch.Size([756125, 2])
We keep 2.58e+09/1.82e+11 =  1% of the original kernel matrix.

torch.Size([203027, 2])
We keep 9.04e+07/1.14e+10 =  0% of the original kernel matrix.

torch.Size([10746, 2])
We keep 8.51e+05/2.37e+07 =  3% of the original kernel matrix.

torch.Size([23391, 2])
We keep 2.32e+06/1.30e+08 =  1% of the original kernel matrix.

torch.Size([162217, 2])
We keep 9.36e+07/8.59e+09 =  1% of the original kernel matrix.

torch.Size([90147, 2])
We keep 2.38e+07/2.47e+09 =  0% of the original kernel matrix.

torch.Size([2177, 2])
We keep 5.22e+04/6.81e+05 =  7% of the original kernel matrix.

torch.Size([12337, 2])
We keep 6.98e+05/2.20e+07 =  3% of the original kernel matrix.

torch.Size([3988, 2])
We keep 1.15e+05/2.23e+06 =  5% of the original kernel matrix.

torch.Size([15791, 2])
We keep 1.02e+06/3.98e+07 =  2% of the original kernel matrix.

torch.Size([9673, 2])
We keep 4.77e+05/1.41e+07 =  3% of the original kernel matrix.

torch.Size([22417, 2])
We keep 1.90e+06/1.00e+08 =  1% of the original kernel matrix.

torch.Size([15268, 2])
We keep 1.13e+06/4.19e+07 =  2% of the original kernel matrix.

torch.Size([27626, 2])
We keep 2.72e+06/1.73e+08 =  1% of the original kernel matrix.

torch.Size([11054, 2])
We keep 1.05e+06/2.46e+07 =  4% of the original kernel matrix.

torch.Size([23501, 2])
We keep 2.30e+06/1.32e+08 =  1% of the original kernel matrix.

torch.Size([39249, 2])
We keep 3.14e+07/7.18e+08 =  4% of the original kernel matrix.

torch.Size([43921, 2])
We keep 8.53e+06/7.15e+08 =  1% of the original kernel matrix.

torch.Size([9722, 2])
We keep 5.65e+05/1.62e+07 =  3% of the original kernel matrix.

torch.Size([22274, 2])
We keep 1.99e+06/1.07e+08 =  1% of the original kernel matrix.

torch.Size([7522, 2])
We keep 3.57e+05/9.60e+06 =  3% of the original kernel matrix.

torch.Size([20036, 2])
We keep 1.64e+06/8.27e+07 =  1% of the original kernel matrix.

torch.Size([379717, 2])
We keep 4.16e+08/4.66e+10 =  0% of the original kernel matrix.

torch.Size([143368, 2])
We keep 4.93e+07/5.76e+09 =  0% of the original kernel matrix.

torch.Size([40463, 2])
We keep 1.13e+07/5.50e+08 =  2% of the original kernel matrix.

torch.Size([45798, 2])
We keep 7.52e+06/6.26e+08 =  1% of the original kernel matrix.

torch.Size([28111, 2])
We keep 6.21e+06/2.77e+08 =  2% of the original kernel matrix.

torch.Size([37578, 2])
We keep 5.71e+06/4.44e+08 =  1% of the original kernel matrix.

torch.Size([124942, 2])
We keep 7.30e+07/4.89e+09 =  1% of the original kernel matrix.

torch.Size([78495, 2])
We keep 1.86e+07/1.87e+09 =  0% of the original kernel matrix.

torch.Size([146417, 2])
We keep 1.27e+08/9.66e+09 =  1% of the original kernel matrix.

torch.Size([82625, 2])
We keep 2.49e+07/2.62e+09 =  0% of the original kernel matrix.

torch.Size([13487, 2])
We keep 9.51e+05/3.33e+07 =  2% of the original kernel matrix.

torch.Size([25978, 2])
We keep 2.64e+06/1.54e+08 =  1% of the original kernel matrix.

torch.Size([12281, 2])
We keep 9.17e+05/2.75e+07 =  3% of the original kernel matrix.

torch.Size([24812, 2])
We keep 2.42e+06/1.40e+08 =  1% of the original kernel matrix.

torch.Size([15027, 2])
We keep 3.81e+06/8.52e+07 =  4% of the original kernel matrix.

torch.Size([26906, 2])
We keep 3.60e+06/2.46e+08 =  1% of the original kernel matrix.

torch.Size([1294, 2])
We keep 2.57e+04/2.63e+05 =  9% of the original kernel matrix.

torch.Size([9808, 2])
We keep 4.99e+05/1.37e+07 =  3% of the original kernel matrix.

torch.Size([4564, 2])
We keep 1.39e+05/2.86e+06 =  4% of the original kernel matrix.

torch.Size([16456, 2])
We keep 1.10e+06/4.51e+07 =  2% of the original kernel matrix.

torch.Size([21266, 2])
We keep 3.65e+06/1.32e+08 =  2% of the original kernel matrix.

torch.Size([32509, 2])
We keep 4.32e+06/3.07e+08 =  1% of the original kernel matrix.

torch.Size([4138, 2])
We keep 1.34e+05/2.33e+06 =  5% of the original kernel matrix.

torch.Size([15690, 2])
We keep 1.03e+06/4.07e+07 =  2% of the original kernel matrix.

torch.Size([4342, 2])
We keep 1.22e+05/2.39e+06 =  5% of the original kernel matrix.

torch.Size([16161, 2])
We keep 1.03e+06/4.13e+07 =  2% of the original kernel matrix.

torch.Size([6839, 2])
We keep 2.80e+05/7.02e+06 =  3% of the original kernel matrix.

torch.Size([19236, 2])
We keep 1.48e+06/7.07e+07 =  2% of the original kernel matrix.

torch.Size([9033, 2])
We keep 5.58e+05/1.44e+07 =  3% of the original kernel matrix.

torch.Size([21502, 2])
We keep 1.92e+06/1.01e+08 =  1% of the original kernel matrix.

torch.Size([314355, 2])
We keep 5.59e+08/4.32e+10 =  1% of the original kernel matrix.

torch.Size([125902, 2])
We keep 4.83e+07/5.55e+09 =  0% of the original kernel matrix.

torch.Size([9970, 2])
We keep 1.96e+06/3.89e+07 =  5% of the original kernel matrix.

torch.Size([21447, 2])
We keep 2.74e+06/1.66e+08 =  1% of the original kernel matrix.

torch.Size([102951, 2])
We keep 5.46e+07/3.29e+09 =  1% of the original kernel matrix.

torch.Size([71165, 2])
We keep 1.59e+07/1.53e+09 =  1% of the original kernel matrix.

torch.Size([8777, 2])
We keep 1.88e+06/2.31e+07 =  8% of the original kernel matrix.

torch.Size([21047, 2])
We keep 2.18e+06/1.28e+08 =  1% of the original kernel matrix.

torch.Size([5731, 2])
We keep 2.79e+05/6.09e+06 =  4% of the original kernel matrix.

torch.Size([17753, 2])
We keep 1.39e+06/6.58e+07 =  2% of the original kernel matrix.

torch.Size([25245, 2])
We keep 6.45e+06/2.76e+08 =  2% of the original kernel matrix.

torch.Size([35100, 2])
We keep 5.73e+06/4.43e+08 =  1% of the original kernel matrix.

torch.Size([458423, 2])
We keep 1.05e+09/7.31e+10 =  1% of the original kernel matrix.

torch.Size([154538, 2])
We keep 5.78e+07/7.21e+09 =  0% of the original kernel matrix.

torch.Size([533690, 2])
We keep 4.74e+08/7.91e+10 =  0% of the original kernel matrix.

torch.Size([169424, 2])
We keep 6.16e+07/7.50e+09 =  0% of the original kernel matrix.

torch.Size([23845, 2])
We keep 2.97e+06/1.36e+08 =  2% of the original kernel matrix.

torch.Size([34615, 2])
We keep 4.37e+06/3.11e+08 =  1% of the original kernel matrix.

torch.Size([34820, 2])
We keep 7.50e+06/3.53e+08 =  2% of the original kernel matrix.

torch.Size([42606, 2])
We keep 6.28e+06/5.01e+08 =  1% of the original kernel matrix.

torch.Size([3406, 2])
We keep 8.87e+04/1.57e+06 =  5% of the original kernel matrix.

torch.Size([14779, 2])
We keep 8.94e+05/3.34e+07 =  2% of the original kernel matrix.

torch.Size([18287, 2])
We keep 1.76e+07/3.23e+08 =  5% of the original kernel matrix.

torch.Size([28229, 2])
We keep 6.16e+06/4.79e+08 =  1% of the original kernel matrix.

torch.Size([216716, 2])
We keep 1.00e+09/3.37e+10 =  2% of the original kernel matrix.

torch.Size([100640, 2])
We keep 4.41e+07/4.90e+09 =  0% of the original kernel matrix.

torch.Size([31296, 2])
We keep 8.89e+06/3.26e+08 =  2% of the original kernel matrix.

torch.Size([40108, 2])
We keep 6.15e+06/4.82e+08 =  1% of the original kernel matrix.

torch.Size([16814, 2])
We keep 1.79e+06/5.86e+07 =  3% of the original kernel matrix.

torch.Size([28862, 2])
We keep 3.21e+06/2.04e+08 =  1% of the original kernel matrix.

torch.Size([79757, 2])
We keep 3.72e+07/2.37e+09 =  1% of the original kernel matrix.

torch.Size([62528, 2])
We keep 1.37e+07/1.30e+09 =  1% of the original kernel matrix.

torch.Size([461142, 2])
We keep 9.88e+08/8.64e+10 =  1% of the original kernel matrix.

torch.Size([154443, 2])
We keep 6.47e+07/7.85e+09 =  0% of the original kernel matrix.

torch.Size([40516, 2])
We keep 1.22e+07/6.35e+08 =  1% of the original kernel matrix.

torch.Size([43899, 2])
We keep 7.89e+06/6.73e+08 =  1% of the original kernel matrix.

torch.Size([11584, 2])
We keep 1.26e+06/3.64e+07 =  3% of the original kernel matrix.

torch.Size([23528, 2])
We keep 2.70e+06/1.61e+08 =  1% of the original kernel matrix.

torch.Size([39054, 2])
We keep 6.55e+06/4.51e+08 =  1% of the original kernel matrix.

torch.Size([46140, 2])
We keep 7.05e+06/5.67e+08 =  1% of the original kernel matrix.

torch.Size([31623, 2])
We keep 5.28e+06/3.00e+08 =  1% of the original kernel matrix.

torch.Size([40246, 2])
We keep 6.02e+06/4.62e+08 =  1% of the original kernel matrix.

torch.Size([7011, 2])
We keep 2.61e+05/6.66e+06 =  3% of the original kernel matrix.

torch.Size([19663, 2])
We keep 1.46e+06/6.89e+07 =  2% of the original kernel matrix.

torch.Size([230041, 2])
We keep 1.94e+08/1.76e+10 =  1% of the original kernel matrix.

torch.Size([109141, 2])
We keep 3.22e+07/3.54e+09 =  0% of the original kernel matrix.

torch.Size([19049, 2])
We keep 3.67e+06/1.22e+08 =  3% of the original kernel matrix.

torch.Size([30061, 2])
We keep 4.20e+06/2.94e+08 =  1% of the original kernel matrix.

torch.Size([1836, 2])
We keep 3.32e+04/4.24e+05 =  7% of the original kernel matrix.

torch.Size([11585, 2])
We keep 5.89e+05/1.74e+07 =  3% of the original kernel matrix.

torch.Size([29770, 2])
We keep 4.28e+06/2.30e+08 =  1% of the original kernel matrix.

torch.Size([39118, 2])
We keep 5.23e+06/4.05e+08 =  1% of the original kernel matrix.

torch.Size([28382, 2])
We keep 4.41e+06/2.25e+08 =  1% of the original kernel matrix.

torch.Size([37983, 2])
We keep 5.36e+06/4.01e+08 =  1% of the original kernel matrix.

torch.Size([1912, 2])
We keep 3.66e+04/4.52e+05 =  8% of the original kernel matrix.

torch.Size([11828, 2])
We keep 6.11e+05/1.79e+07 =  3% of the original kernel matrix.

torch.Size([101656, 2])
We keep 9.94e+07/4.03e+09 =  2% of the original kernel matrix.

torch.Size([69616, 2])
We keep 1.74e+07/1.69e+09 =  1% of the original kernel matrix.

torch.Size([143504, 2])
We keep 7.16e+07/6.12e+09 =  1% of the original kernel matrix.

torch.Size([84374, 2])
We keep 2.04e+07/2.09e+09 =  0% of the original kernel matrix.

torch.Size([154977, 2])
We keep 2.30e+08/9.10e+09 =  2% of the original kernel matrix.

torch.Size([88426, 2])
We keep 2.48e+07/2.55e+09 =  0% of the original kernel matrix.

torch.Size([196647, 2])
We keep 2.16e+08/1.33e+10 =  1% of the original kernel matrix.

torch.Size([100163, 2])
We keep 2.90e+07/3.08e+09 =  0% of the original kernel matrix.

torch.Size([132583, 2])
We keep 1.26e+08/6.72e+09 =  1% of the original kernel matrix.

torch.Size([79954, 2])
We keep 2.10e+07/2.19e+09 =  0% of the original kernel matrix.

torch.Size([14848, 2])
We keep 2.93e+06/7.26e+07 =  4% of the original kernel matrix.

torch.Size([26355, 2])
We keep 3.53e+06/2.27e+08 =  1% of the original kernel matrix.

torch.Size([14376, 2])
We keep 1.77e+06/4.98e+07 =  3% of the original kernel matrix.

torch.Size([26475, 2])
We keep 3.02e+06/1.88e+08 =  1% of the original kernel matrix.

torch.Size([61541, 2])
We keep 1.35e+07/1.10e+09 =  1% of the original kernel matrix.

torch.Size([56252, 2])
We keep 9.86e+06/8.86e+08 =  1% of the original kernel matrix.

torch.Size([181430, 2])
We keep 1.01e+08/9.60e+09 =  1% of the original kernel matrix.

torch.Size([95819, 2])
We keep 2.46e+07/2.61e+09 =  0% of the original kernel matrix.

torch.Size([9071, 2])
We keep 5.34e+05/1.43e+07 =  3% of the original kernel matrix.

torch.Size([21632, 2])
We keep 1.90e+06/1.01e+08 =  1% of the original kernel matrix.

torch.Size([50183, 2])
We keep 3.51e+07/1.06e+09 =  3% of the original kernel matrix.

torch.Size([50130, 2])
We keep 9.22e+06/8.69e+08 =  1% of the original kernel matrix.

torch.Size([186753, 2])
We keep 4.26e+08/1.72e+10 =  2% of the original kernel matrix.

torch.Size([96754, 2])
We keep 3.09e+07/3.49e+09 =  0% of the original kernel matrix.

torch.Size([72384, 2])
We keep 6.05e+07/1.99e+09 =  3% of the original kernel matrix.

torch.Size([59970, 2])
We keep 1.26e+07/1.19e+09 =  1% of the original kernel matrix.

torch.Size([33940, 2])
We keep 2.03e+07/6.39e+08 =  3% of the original kernel matrix.

torch.Size([39020, 2])
We keep 7.34e+06/6.74e+08 =  1% of the original kernel matrix.

torch.Size([36815, 2])
We keep 1.03e+08/1.85e+09 =  5% of the original kernel matrix.

torch.Size([39059, 2])
We keep 1.19e+07/1.15e+09 =  1% of the original kernel matrix.

torch.Size([14806, 2])
We keep 1.45e+06/5.10e+07 =  2% of the original kernel matrix.

torch.Size([27261, 2])
We keep 3.06e+06/1.91e+08 =  1% of the original kernel matrix.

torch.Size([7555, 2])
We keep 3.36e+05/9.13e+06 =  3% of the original kernel matrix.

torch.Size([20146, 2])
We keep 1.63e+06/8.06e+07 =  2% of the original kernel matrix.

torch.Size([16084, 2])
We keep 7.04e+06/1.30e+08 =  5% of the original kernel matrix.

torch.Size([27276, 2])
We keep 4.40e+06/3.05e+08 =  1% of the original kernel matrix.

torch.Size([162519, 2])
We keep 6.54e+07/7.53e+09 =  0% of the original kernel matrix.

torch.Size([90038, 2])
We keep 2.23e+07/2.32e+09 =  0% of the original kernel matrix.

torch.Size([85856, 2])
We keep 2.32e+07/2.10e+09 =  1% of the original kernel matrix.

torch.Size([65462, 2])
We keep 1.30e+07/1.22e+09 =  1% of the original kernel matrix.

torch.Size([580007, 2])
We keep 1.44e+09/1.34e+11 =  1% of the original kernel matrix.

torch.Size([172583, 2])
We keep 7.81e+07/9.77e+09 =  0% of the original kernel matrix.

torch.Size([88003, 2])
We keep 6.39e+07/3.29e+09 =  1% of the original kernel matrix.

torch.Size([65033, 2])
We keep 1.59e+07/1.53e+09 =  1% of the original kernel matrix.

torch.Size([83039, 2])
We keep 6.26e+07/2.45e+09 =  2% of the original kernel matrix.

torch.Size([63722, 2])
We keep 1.40e+07/1.32e+09 =  1% of the original kernel matrix.

torch.Size([22359, 2])
We keep 3.18e+06/1.85e+08 =  1% of the original kernel matrix.

torch.Size([33492, 2])
We keep 4.85e+06/3.63e+08 =  1% of the original kernel matrix.

torch.Size([188102, 2])
We keep 1.37e+08/1.12e+10 =  1% of the original kernel matrix.

torch.Size([97957, 2])
We keep 2.68e+07/2.82e+09 =  0% of the original kernel matrix.

torch.Size([3947, 2])
We keep 2.31e+05/2.90e+06 =  7% of the original kernel matrix.

torch.Size([15013, 2])
We keep 1.08e+06/4.54e+07 =  2% of the original kernel matrix.

torch.Size([13605, 2])
We keep 1.30e+06/3.99e+07 =  3% of the original kernel matrix.

torch.Size([25567, 2])
We keep 2.78e+06/1.69e+08 =  1% of the original kernel matrix.

torch.Size([283339, 2])
We keep 2.35e+08/2.63e+10 =  0% of the original kernel matrix.

torch.Size([122265, 2])
We keep 3.84e+07/4.33e+09 =  0% of the original kernel matrix.

torch.Size([31928, 2])
We keep 6.87e+06/3.10e+08 =  2% of the original kernel matrix.

torch.Size([40540, 2])
We keep 5.91e+06/4.70e+08 =  1% of the original kernel matrix.

torch.Size([7567, 2])
We keep 5.95e+05/1.17e+07 =  5% of the original kernel matrix.

torch.Size([19661, 2])
We keep 1.75e+06/9.15e+07 =  1% of the original kernel matrix.

torch.Size([21683, 2])
We keep 3.07e+06/1.50e+08 =  2% of the original kernel matrix.

torch.Size([32807, 2])
We keep 4.45e+06/3.27e+08 =  1% of the original kernel matrix.

torch.Size([162159, 2])
We keep 4.95e+08/1.71e+10 =  2% of the original kernel matrix.

torch.Size([87401, 2])
We keep 3.08e+07/3.49e+09 =  0% of the original kernel matrix.

torch.Size([30687, 2])
We keep 1.79e+07/3.77e+08 =  4% of the original kernel matrix.

torch.Size([38872, 2])
We keep 6.70e+06/5.18e+08 =  1% of the original kernel matrix.

torch.Size([55001, 2])
We keep 1.93e+07/1.03e+09 =  1% of the original kernel matrix.

torch.Size([53087, 2])
We keep 9.74e+06/8.57e+08 =  1% of the original kernel matrix.

torch.Size([4470, 2])
We keep 5.87e+05/5.33e+06 = 11% of the original kernel matrix.

torch.Size([15529, 2])
We keep 1.31e+06/6.16e+07 =  2% of the original kernel matrix.

torch.Size([9526, 2])
We keep 3.71e+06/3.01e+07 = 12% of the original kernel matrix.

torch.Size([21400, 2])
We keep 2.35e+06/1.46e+08 =  1% of the original kernel matrix.

torch.Size([5634, 2])
We keep 2.66e+05/5.54e+06 =  4% of the original kernel matrix.

torch.Size([17544, 2])
We keep 1.37e+06/6.28e+07 =  2% of the original kernel matrix.

torch.Size([16415, 2])
We keep 2.84e+06/6.73e+07 =  4% of the original kernel matrix.

torch.Size([28541, 2])
We keep 3.43e+06/2.19e+08 =  1% of the original kernel matrix.

torch.Size([33552, 2])
We keep 4.81e+06/3.11e+08 =  1% of the original kernel matrix.

torch.Size([41895, 2])
We keep 6.05e+06/4.70e+08 =  1% of the original kernel matrix.

torch.Size([49578, 2])
We keep 3.60e+07/1.10e+09 =  3% of the original kernel matrix.

torch.Size([48749, 2])
We keep 1.02e+07/8.85e+08 =  1% of the original kernel matrix.

torch.Size([4560, 2])
We keep 5.25e+05/3.85e+06 = 13% of the original kernel matrix.

torch.Size([16305, 2])
We keep 1.21e+06/5.24e+07 =  2% of the original kernel matrix.

torch.Size([161995, 2])
We keep 1.15e+08/8.67e+09 =  1% of the original kernel matrix.

torch.Size([89935, 2])
We keep 2.37e+07/2.48e+09 =  0% of the original kernel matrix.

torch.Size([6258, 2])
We keep 3.94e+05/7.95e+06 =  4% of the original kernel matrix.

torch.Size([18263, 2])
We keep 1.51e+06/7.52e+07 =  2% of the original kernel matrix.

torch.Size([37739, 2])
We keep 9.68e+06/4.52e+08 =  2% of the original kernel matrix.

torch.Size([44565, 2])
We keep 7.13e+06/5.67e+08 =  1% of the original kernel matrix.

torch.Size([10889, 2])
We keep 2.19e+06/3.95e+07 =  5% of the original kernel matrix.

torch.Size([23083, 2])
We keep 2.81e+06/1.68e+08 =  1% of the original kernel matrix.

torch.Size([32231, 2])
We keep 4.99e+06/2.82e+08 =  1% of the original kernel matrix.

torch.Size([40833, 2])
We keep 5.70e+06/4.48e+08 =  1% of the original kernel matrix.

torch.Size([7708, 2])
We keep 4.91e+05/1.06e+07 =  4% of the original kernel matrix.

torch.Size([20067, 2])
We keep 1.73e+06/8.69e+07 =  1% of the original kernel matrix.

torch.Size([23471, 2])
We keep 7.58e+06/1.87e+08 =  4% of the original kernel matrix.

torch.Size([34188, 2])
We keep 4.99e+06/3.65e+08 =  1% of the original kernel matrix.

torch.Size([28297, 2])
We keep 5.35e+06/2.37e+08 =  2% of the original kernel matrix.

torch.Size([37928, 2])
We keep 5.38e+06/4.10e+08 =  1% of the original kernel matrix.

torch.Size([26964, 2])
We keep 7.49e+06/2.30e+08 =  3% of the original kernel matrix.

torch.Size([36790, 2])
We keep 5.39e+06/4.04e+08 =  1% of the original kernel matrix.

torch.Size([74104, 2])
We keep 2.92e+07/2.04e+09 =  1% of the original kernel matrix.

torch.Size([60867, 2])
We keep 1.31e+07/1.21e+09 =  1% of the original kernel matrix.

torch.Size([378243, 2])
We keep 7.15e+08/5.58e+10 =  1% of the original kernel matrix.

torch.Size([140401, 2])
We keep 5.33e+07/6.30e+09 =  0% of the original kernel matrix.

torch.Size([8822, 2])
We keep 3.99e+05/1.12e+07 =  3% of the original kernel matrix.

torch.Size([21581, 2])
We keep 1.73e+06/8.95e+07 =  1% of the original kernel matrix.

torch.Size([7166, 2])
We keep 1.13e+06/1.57e+07 =  7% of the original kernel matrix.

torch.Size([18746, 2])
We keep 1.97e+06/1.06e+08 =  1% of the original kernel matrix.

torch.Size([71444, 2])
We keep 3.44e+07/1.97e+09 =  1% of the original kernel matrix.

torch.Size([58796, 2])
We keep 1.27e+07/1.18e+09 =  1% of the original kernel matrix.

torch.Size([15485, 2])
We keep 1.74e+06/5.70e+07 =  3% of the original kernel matrix.

torch.Size([27591, 2])
We keep 3.19e+06/2.02e+08 =  1% of the original kernel matrix.

torch.Size([608950, 2])
We keep 6.46e+08/1.02e+11 =  0% of the original kernel matrix.

torch.Size([180726, 2])
We keep 6.96e+07/8.53e+09 =  0% of the original kernel matrix.

torch.Size([2390, 2])
We keep 5.10e+04/7.19e+05 =  7% of the original kernel matrix.

torch.Size([12671, 2])
We keep 6.91e+05/2.26e+07 =  3% of the original kernel matrix.

torch.Size([173738, 2])
We keep 9.50e+07/1.01e+10 =  0% of the original kernel matrix.

torch.Size([93372, 2])
We keep 2.53e+07/2.68e+09 =  0% of the original kernel matrix.

torch.Size([365111, 2])
We keep 2.47e+08/4.19e+10 =  0% of the original kernel matrix.

torch.Size([143952, 2])
We keep 4.76e+07/5.46e+09 =  0% of the original kernel matrix.

torch.Size([4994, 2])
We keep 1.80e+05/3.58e+06 =  5% of the original kernel matrix.

torch.Size([17084, 2])
We keep 1.20e+06/5.05e+07 =  2% of the original kernel matrix.

torch.Size([189273, 2])
We keep 2.58e+08/1.80e+10 =  1% of the original kernel matrix.

torch.Size([96380, 2])
We keep 3.25e+07/3.58e+09 =  0% of the original kernel matrix.

torch.Size([18133, 2])
We keep 6.57e+06/2.26e+08 =  2% of the original kernel matrix.

torch.Size([28099, 2])
We keep 5.41e+06/4.01e+08 =  1% of the original kernel matrix.

torch.Size([536584, 2])
We keep 1.23e+09/1.06e+11 =  1% of the original kernel matrix.

torch.Size([165841, 2])
We keep 7.13e+07/8.70e+09 =  0% of the original kernel matrix.

torch.Size([12353, 2])
We keep 8.34e+05/2.65e+07 =  3% of the original kernel matrix.

torch.Size([24902, 2])
We keep 2.40e+06/1.37e+08 =  1% of the original kernel matrix.

torch.Size([26399, 2])
We keep 7.27e+06/2.67e+08 =  2% of the original kernel matrix.

torch.Size([35910, 2])
We keep 5.65e+06/4.36e+08 =  1% of the original kernel matrix.

torch.Size([7004, 2])
We keep 1.26e+06/1.62e+07 =  7% of the original kernel matrix.

torch.Size([18471, 2])
We keep 1.98e+06/1.08e+08 =  1% of the original kernel matrix.

torch.Size([8592, 2])
We keep 4.68e+05/1.19e+07 =  3% of the original kernel matrix.

torch.Size([20995, 2])
We keep 1.80e+06/9.19e+07 =  1% of the original kernel matrix.

torch.Size([8282, 2])
We keep 8.81e+05/1.76e+07 =  5% of the original kernel matrix.

torch.Size([20272, 2])
We keep 2.05e+06/1.12e+08 =  1% of the original kernel matrix.

torch.Size([32911, 2])
We keep 6.53e+06/3.42e+08 =  1% of the original kernel matrix.

torch.Size([40737, 2])
We keep 6.06e+06/4.93e+08 =  1% of the original kernel matrix.

torch.Size([14077, 2])
We keep 3.87e+06/7.16e+07 =  5% of the original kernel matrix.

torch.Size([26249, 2])
We keep 3.45e+06/2.26e+08 =  1% of the original kernel matrix.

torch.Size([71136, 2])
We keep 3.61e+07/2.10e+09 =  1% of the original kernel matrix.

torch.Size([58636, 2])
We keep 1.26e+07/1.22e+09 =  1% of the original kernel matrix.

torch.Size([21123, 2])
We keep 2.23e+06/1.06e+08 =  2% of the original kernel matrix.

torch.Size([32346, 2])
We keep 4.00e+06/2.75e+08 =  1% of the original kernel matrix.

torch.Size([6484, 2])
We keep 2.28e+05/5.73e+06 =  3% of the original kernel matrix.

torch.Size([19029, 2])
We keep 1.37e+06/6.39e+07 =  2% of the original kernel matrix.

torch.Size([15565, 2])
We keep 3.05e+06/8.28e+07 =  3% of the original kernel matrix.

torch.Size([27040, 2])
We keep 3.63e+06/2.43e+08 =  1% of the original kernel matrix.

torch.Size([82921, 2])
We keep 1.01e+08/4.10e+09 =  2% of the original kernel matrix.

torch.Size([61852, 2])
We keep 1.72e+07/1.71e+09 =  1% of the original kernel matrix.

torch.Size([29508, 2])
We keep 3.09e+07/8.45e+08 =  3% of the original kernel matrix.

torch.Size([37156, 2])
We keep 9.27e+06/7.76e+08 =  1% of the original kernel matrix.

torch.Size([280597, 2])
We keep 2.13e+08/2.38e+10 =  0% of the original kernel matrix.

torch.Size([121729, 2])
We keep 3.65e+07/4.12e+09 =  0% of the original kernel matrix.

torch.Size([22894, 2])
We keep 4.04e+06/1.27e+08 =  3% of the original kernel matrix.

torch.Size([34046, 2])
We keep 4.17e+06/3.01e+08 =  1% of the original kernel matrix.

torch.Size([11706, 2])
We keep 1.66e+06/4.17e+07 =  3% of the original kernel matrix.

torch.Size([23981, 2])
We keep 2.77e+06/1.72e+08 =  1% of the original kernel matrix.

torch.Size([6955, 2])
We keep 3.36e+05/7.61e+06 =  4% of the original kernel matrix.

torch.Size([19442, 2])
We keep 1.52e+06/7.36e+07 =  2% of the original kernel matrix.

torch.Size([6158, 2])
We keep 5.00e+05/9.01e+06 =  5% of the original kernel matrix.

torch.Size([17885, 2])
We keep 1.53e+06/8.01e+07 =  1% of the original kernel matrix.

torch.Size([350059, 2])
We keep 4.81e+08/4.23e+10 =  1% of the original kernel matrix.

torch.Size([137558, 2])
We keep 4.67e+07/5.49e+09 =  0% of the original kernel matrix.

torch.Size([14220, 2])
We keep 1.21e+06/3.79e+07 =  3% of the original kernel matrix.

torch.Size([26673, 2])
We keep 2.69e+06/1.64e+08 =  1% of the original kernel matrix.

torch.Size([18934, 2])
We keep 2.43e+06/8.41e+07 =  2% of the original kernel matrix.

torch.Size([30696, 2])
We keep 3.62e+06/2.45e+08 =  1% of the original kernel matrix.

torch.Size([38399, 2])
We keep 1.76e+07/6.90e+08 =  2% of the original kernel matrix.

torch.Size([43142, 2])
We keep 8.21e+06/7.01e+08 =  1% of the original kernel matrix.

torch.Size([25919, 2])
We keep 1.04e+07/4.11e+08 =  2% of the original kernel matrix.

torch.Size([34547, 2])
We keep 6.67e+06/5.41e+08 =  1% of the original kernel matrix.

torch.Size([7399, 2])
We keep 7.14e+05/1.18e+07 =  6% of the original kernel matrix.

torch.Size([19584, 2])
We keep 1.74e+06/9.16e+07 =  1% of the original kernel matrix.

torch.Size([9613, 2])
We keep 6.57e+05/1.67e+07 =  3% of the original kernel matrix.

torch.Size([21910, 2])
We keep 2.00e+06/1.09e+08 =  1% of the original kernel matrix.

torch.Size([268061, 2])
We keep 4.73e+08/3.43e+10 =  1% of the original kernel matrix.

torch.Size([116173, 2])
We keep 4.24e+07/4.94e+09 =  0% of the original kernel matrix.

torch.Size([29705, 2])
We keep 6.27e+06/2.83e+08 =  2% of the original kernel matrix.

torch.Size([38840, 2])
We keep 5.71e+06/4.49e+08 =  1% of the original kernel matrix.

torch.Size([19671, 2])
We keep 6.43e+06/1.80e+08 =  3% of the original kernel matrix.

torch.Size([30409, 2])
We keep 4.90e+06/3.58e+08 =  1% of the original kernel matrix.

torch.Size([46323, 2])
We keep 1.44e+07/6.73e+08 =  2% of the original kernel matrix.

torch.Size([49681, 2])
We keep 8.28e+06/6.93e+08 =  1% of the original kernel matrix.

torch.Size([79509, 2])
We keep 2.18e+08/3.73e+09 =  5% of the original kernel matrix.

torch.Size([61649, 2])
We keep 1.71e+07/1.63e+09 =  1% of the original kernel matrix.

torch.Size([79621, 2])
We keep 3.58e+07/2.04e+09 =  1% of the original kernel matrix.

torch.Size([62904, 2])
We keep 1.29e+07/1.21e+09 =  1% of the original kernel matrix.

torch.Size([172443, 2])
We keep 3.24e+08/1.87e+10 =  1% of the original kernel matrix.

torch.Size([90241, 2])
We keep 3.29e+07/3.65e+09 =  0% of the original kernel matrix.

torch.Size([91009, 2])
We keep 3.30e+07/2.62e+09 =  1% of the original kernel matrix.

torch.Size([67276, 2])
We keep 1.44e+07/1.37e+09 =  1% of the original kernel matrix.

torch.Size([6641, 2])
We keep 4.96e+05/8.00e+06 =  6% of the original kernel matrix.

torch.Size([19308, 2])
We keep 1.55e+06/7.55e+07 =  2% of the original kernel matrix.

torch.Size([54964, 2])
We keep 1.07e+07/8.81e+08 =  1% of the original kernel matrix.

torch.Size([52766, 2])
We keep 9.08e+06/7.92e+08 =  1% of the original kernel matrix.

torch.Size([15146, 2])
We keep 6.97e+06/1.04e+08 =  6% of the original kernel matrix.

torch.Size([26918, 2])
We keep 3.85e+06/2.73e+08 =  1% of the original kernel matrix.

torch.Size([15977, 2])
We keep 2.16e+06/5.95e+07 =  3% of the original kernel matrix.

torch.Size([28089, 2])
We keep 3.26e+06/2.06e+08 =  1% of the original kernel matrix.

torch.Size([23765, 2])
We keep 3.63e+06/1.83e+08 =  1% of the original kernel matrix.

torch.Size([34337, 2])
We keep 4.84e+06/3.61e+08 =  1% of the original kernel matrix.

torch.Size([5081, 2])
We keep 1.68e+05/3.52e+06 =  4% of the original kernel matrix.

torch.Size([17273, 2])
We keep 1.16e+06/5.00e+07 =  2% of the original kernel matrix.

torch.Size([44018, 2])
We keep 4.31e+07/1.10e+09 =  3% of the original kernel matrix.

torch.Size([45825, 2])
We keep 9.91e+06/8.85e+08 =  1% of the original kernel matrix.

torch.Size([19599, 2])
We keep 2.89e+06/9.95e+07 =  2% of the original kernel matrix.

torch.Size([30928, 2])
We keep 3.88e+06/2.66e+08 =  1% of the original kernel matrix.

torch.Size([6627, 2])
We keep 4.70e+05/9.17e+06 =  5% of the original kernel matrix.

torch.Size([18422, 2])
We keep 1.68e+06/8.08e+07 =  2% of the original kernel matrix.

torch.Size([23673, 2])
We keep 2.92e+06/1.44e+08 =  2% of the original kernel matrix.

torch.Size([34312, 2])
We keep 4.47e+06/3.20e+08 =  1% of the original kernel matrix.

torch.Size([5705, 2])
We keep 6.80e+05/1.13e+07 =  5% of the original kernel matrix.

torch.Size([16794, 2])
We keep 1.73e+06/8.99e+07 =  1% of the original kernel matrix.

torch.Size([49919, 2])
We keep 7.28e+07/2.50e+09 =  2% of the original kernel matrix.

torch.Size([45004, 2])
We keep 1.42e+07/1.33e+09 =  1% of the original kernel matrix.

torch.Size([9536, 2])
We keep 1.07e+06/2.38e+07 =  4% of the original kernel matrix.

torch.Size([21709, 2])
We keep 2.27e+06/1.30e+08 =  1% of the original kernel matrix.

torch.Size([25172, 2])
We keep 4.94e+06/1.59e+08 =  3% of the original kernel matrix.

torch.Size([35703, 2])
We keep 4.61e+06/3.37e+08 =  1% of the original kernel matrix.

torch.Size([6794, 2])
We keep 3.63e+06/2.11e+07 = 17% of the original kernel matrix.

torch.Size([17905, 2])
We keep 2.14e+06/1.23e+08 =  1% of the original kernel matrix.

torch.Size([19487, 2])
We keep 2.30e+06/9.19e+07 =  2% of the original kernel matrix.

torch.Size([31161, 2])
We keep 3.75e+06/2.56e+08 =  1% of the original kernel matrix.

torch.Size([6345, 2])
We keep 3.73e+05/7.12e+06 =  5% of the original kernel matrix.

torch.Size([18595, 2])
We keep 1.48e+06/7.12e+07 =  2% of the original kernel matrix.

torch.Size([11821, 2])
We keep 8.54e+05/2.55e+07 =  3% of the original kernel matrix.

torch.Size([24341, 2])
We keep 2.35e+06/1.35e+08 =  1% of the original kernel matrix.

torch.Size([31251, 2])
We keep 3.89e+06/2.41e+08 =  1% of the original kernel matrix.

torch.Size([40145, 2])
We keep 5.40e+06/4.14e+08 =  1% of the original kernel matrix.

torch.Size([131133, 2])
We keep 7.94e+07/5.53e+09 =  1% of the original kernel matrix.

torch.Size([80621, 2])
We keep 1.96e+07/1.98e+09 =  0% of the original kernel matrix.

torch.Size([7052, 2])
We keep 3.82e+05/8.67e+06 =  4% of the original kernel matrix.

torch.Size([19291, 2])
We keep 1.59e+06/7.86e+07 =  2% of the original kernel matrix.

torch.Size([2318, 2])
We keep 3.98e+04/6.29e+05 =  6% of the original kernel matrix.

torch.Size([12783, 2])
We keep 6.76e+05/2.12e+07 =  3% of the original kernel matrix.

torch.Size([9612, 2])
We keep 9.07e+05/1.97e+07 =  4% of the original kernel matrix.

torch.Size([21943, 2])
We keep 2.16e+06/1.18e+08 =  1% of the original kernel matrix.

torch.Size([125285, 2])
We keep 9.09e+07/5.94e+09 =  1% of the original kernel matrix.

torch.Size([78531, 2])
We keep 2.01e+07/2.06e+09 =  0% of the original kernel matrix.

torch.Size([742264, 2])
We keep 2.09e+09/1.69e+11 =  1% of the original kernel matrix.

torch.Size([200719, 2])
We keep 8.93e+07/1.10e+10 =  0% of the original kernel matrix.

torch.Size([31538, 2])
We keep 1.04e+07/4.29e+08 =  2% of the original kernel matrix.

torch.Size([39088, 2])
We keep 6.78e+06/5.53e+08 =  1% of the original kernel matrix.

torch.Size([10199, 2])
We keep 7.86e+05/2.12e+07 =  3% of the original kernel matrix.

torch.Size([22647, 2])
We keep 2.21e+06/1.23e+08 =  1% of the original kernel matrix.

torch.Size([66935, 2])
We keep 5.31e+07/2.36e+09 =  2% of the original kernel matrix.

torch.Size([55451, 2])
We keep 1.38e+07/1.30e+09 =  1% of the original kernel matrix.

torch.Size([118554, 2])
We keep 4.53e+07/4.01e+09 =  1% of the original kernel matrix.

torch.Size([75773, 2])
We keep 1.70e+07/1.69e+09 =  1% of the original kernel matrix.

torch.Size([4551, 2])
We keep 1.19e+05/2.56e+06 =  4% of the original kernel matrix.

torch.Size([16539, 2])
We keep 1.06e+06/4.27e+07 =  2% of the original kernel matrix.

torch.Size([13959, 2])
We keep 2.19e+06/5.38e+07 =  4% of the original kernel matrix.

torch.Size([26396, 2])
We keep 3.05e+06/1.96e+08 =  1% of the original kernel matrix.

torch.Size([27620, 2])
We keep 2.82e+06/1.75e+08 =  1% of the original kernel matrix.

torch.Size([37554, 2])
We keep 4.78e+06/3.53e+08 =  1% of the original kernel matrix.

torch.Size([83681, 2])
We keep 3.74e+08/9.31e+09 =  4% of the original kernel matrix.

torch.Size([58917, 2])
We keep 2.52e+07/2.57e+09 =  0% of the original kernel matrix.

torch.Size([85663, 2])
We keep 3.06e+08/6.94e+09 =  4% of the original kernel matrix.

torch.Size([62386, 2])
We keep 2.23e+07/2.22e+09 =  1% of the original kernel matrix.

torch.Size([210963, 2])
We keep 1.46e+08/1.38e+10 =  1% of the original kernel matrix.

torch.Size([104221, 2])
We keep 2.89e+07/3.13e+09 =  0% of the original kernel matrix.

torch.Size([15002, 2])
We keep 1.14e+06/4.00e+07 =  2% of the original kernel matrix.

torch.Size([27353, 2])
We keep 2.75e+06/1.69e+08 =  1% of the original kernel matrix.

torch.Size([6950, 2])
We keep 2.58e+05/6.55e+06 =  3% of the original kernel matrix.

torch.Size([19705, 2])
We keep 1.45e+06/6.83e+07 =  2% of the original kernel matrix.

torch.Size([3167, 2])
We keep 6.41e+04/1.14e+06 =  5% of the original kernel matrix.

torch.Size([14299, 2])
We keep 8.07e+05/2.85e+07 =  2% of the original kernel matrix.

torch.Size([62643, 2])
We keep 2.96e+07/1.43e+09 =  2% of the original kernel matrix.

torch.Size([55810, 2])
We keep 1.13e+07/1.01e+09 =  1% of the original kernel matrix.

torch.Size([19640, 2])
We keep 2.15e+06/8.15e+07 =  2% of the original kernel matrix.

torch.Size([31406, 2])
We keep 3.59e+06/2.41e+08 =  1% of the original kernel matrix.

torch.Size([23532, 2])
We keep 9.10e+06/2.76e+08 =  3% of the original kernel matrix.

torch.Size([33123, 2])
We keep 5.62e+06/4.43e+08 =  1% of the original kernel matrix.

torch.Size([101307, 2])
We keep 4.84e+07/3.10e+09 =  1% of the original kernel matrix.

torch.Size([70344, 2])
We keep 1.54e+07/1.49e+09 =  1% of the original kernel matrix.

torch.Size([518854, 2])
We keep 5.59e+08/7.28e+10 =  0% of the original kernel matrix.

torch.Size([166703, 2])
We keep 5.96e+07/7.20e+09 =  0% of the original kernel matrix.

torch.Size([10613, 2])
We keep 8.76e+05/2.20e+07 =  3% of the original kernel matrix.

torch.Size([23000, 2])
We keep 2.20e+06/1.25e+08 =  1% of the original kernel matrix.

torch.Size([3630, 2])
We keep 9.49e+04/1.79e+06 =  5% of the original kernel matrix.

torch.Size([15069, 2])
We keep 9.23e+05/3.57e+07 =  2% of the original kernel matrix.

torch.Size([25623, 2])
We keep 3.39e+06/1.82e+08 =  1% of the original kernel matrix.

torch.Size([36039, 2])
We keep 4.94e+06/3.60e+08 =  1% of the original kernel matrix.

torch.Size([388280, 2])
We keep 4.21e+09/9.00e+10 =  4% of the original kernel matrix.

torch.Size([143130, 2])
We keep 6.56e+07/8.01e+09 =  0% of the original kernel matrix.

torch.Size([37617, 2])
We keep 7.49e+06/4.16e+08 =  1% of the original kernel matrix.

torch.Size([44309, 2])
We keep 6.82e+06/5.44e+08 =  1% of the original kernel matrix.

torch.Size([3824, 2])
We keep 5.12e+05/3.81e+06 = 13% of the original kernel matrix.

torch.Size([14757, 2])
We keep 1.19e+06/5.21e+07 =  2% of the original kernel matrix.

torch.Size([3973, 2])
We keep 1.97e+05/3.08e+06 =  6% of the original kernel matrix.

torch.Size([15159, 2])
We keep 1.13e+06/4.68e+07 =  2% of the original kernel matrix.

torch.Size([46666, 2])
We keep 1.35e+07/7.27e+08 =  1% of the original kernel matrix.

torch.Size([48775, 2])
We keep 8.19e+06/7.19e+08 =  1% of the original kernel matrix.

torch.Size([4217, 2])
We keep 1.46e+05/2.63e+06 =  5% of the original kernel matrix.

torch.Size([15951, 2])
We keep 1.07e+06/4.33e+07 =  2% of the original kernel matrix.

torch.Size([31984, 2])
We keep 3.12e+07/6.80e+08 =  4% of the original kernel matrix.

torch.Size([38183, 2])
We keep 8.20e+06/6.96e+08 =  1% of the original kernel matrix.

torch.Size([4552, 2])
We keep 1.45e+05/2.86e+06 =  5% of the original kernel matrix.

torch.Size([16475, 2])
We keep 1.09e+06/4.51e+07 =  2% of the original kernel matrix.

torch.Size([21168, 2])
We keep 1.92e+06/9.59e+07 =  2% of the original kernel matrix.

torch.Size([32459, 2])
We keep 3.80e+06/2.61e+08 =  1% of the original kernel matrix.

torch.Size([20660, 2])
We keep 2.35e+06/9.91e+07 =  2% of the original kernel matrix.

torch.Size([31916, 2])
We keep 3.86e+06/2.66e+08 =  1% of the original kernel matrix.

torch.Size([7841, 2])
We keep 3.10e+05/8.74e+06 =  3% of the original kernel matrix.

torch.Size([20490, 2])
We keep 1.60e+06/7.89e+07 =  2% of the original kernel matrix.

torch.Size([6880, 2])
We keep 2.70e+05/6.89e+06 =  3% of the original kernel matrix.

torch.Size([19268, 2])
We keep 1.47e+06/7.00e+07 =  2% of the original kernel matrix.

torch.Size([238790, 2])
We keep 1.42e+08/1.74e+10 =  0% of the original kernel matrix.

torch.Size([111420, 2])
We keep 3.17e+07/3.52e+09 =  0% of the original kernel matrix.

torch.Size([586236, 2])
We keep 8.88e+08/9.91e+10 =  0% of the original kernel matrix.

torch.Size([176740, 2])
We keep 6.96e+07/8.40e+09 =  0% of the original kernel matrix.

torch.Size([123204, 2])
We keep 1.17e+08/5.38e+09 =  2% of the original kernel matrix.

torch.Size([78082, 2])
We keep 1.95e+07/1.96e+09 =  0% of the original kernel matrix.

torch.Size([101727, 2])
We keep 2.08e+08/4.91e+09 =  4% of the original kernel matrix.

torch.Size([69017, 2])
We keep 1.89e+07/1.87e+09 =  1% of the original kernel matrix.

torch.Size([27444, 2])
We keep 3.06e+06/1.81e+08 =  1% of the original kernel matrix.

torch.Size([37388, 2])
We keep 4.86e+06/3.59e+08 =  1% of the original kernel matrix.

torch.Size([16044, 2])
We keep 2.15e+06/6.20e+07 =  3% of the original kernel matrix.

torch.Size([28242, 2])
We keep 3.19e+06/2.10e+08 =  1% of the original kernel matrix.

torch.Size([17159, 2])
We keep 1.95e+06/6.42e+07 =  3% of the original kernel matrix.

torch.Size([29220, 2])
We keep 3.32e+06/2.14e+08 =  1% of the original kernel matrix.

torch.Size([104444, 2])
We keep 3.63e+07/2.88e+09 =  1% of the original kernel matrix.

torch.Size([71431, 2])
We keep 1.48e+07/1.43e+09 =  1% of the original kernel matrix.

torch.Size([43413, 2])
We keep 2.11e+07/6.59e+08 =  3% of the original kernel matrix.

torch.Size([48011, 2])
We keep 8.37e+06/6.85e+08 =  1% of the original kernel matrix.

torch.Size([64174, 2])
We keep 6.90e+07/1.71e+09 =  4% of the original kernel matrix.

torch.Size([56198, 2])
We keep 1.17e+07/1.10e+09 =  1% of the original kernel matrix.

torch.Size([15001, 2])
We keep 1.60e+06/4.78e+07 =  3% of the original kernel matrix.

torch.Size([27380, 2])
We keep 2.95e+06/1.85e+08 =  1% of the original kernel matrix.

torch.Size([999002, 2])
We keep 2.73e+09/3.77e+11 =  0% of the original kernel matrix.

torch.Size([236867, 2])
We keep 1.27e+08/1.64e+10 =  0% of the original kernel matrix.

torch.Size([29984, 2])
We keep 5.81e+06/2.34e+08 =  2% of the original kernel matrix.

torch.Size([39045, 2])
We keep 5.41e+06/4.08e+08 =  1% of the original kernel matrix.

torch.Size([41384, 2])
We keep 1.22e+07/6.50e+08 =  1% of the original kernel matrix.

torch.Size([44908, 2])
We keep 7.77e+06/6.80e+08 =  1% of the original kernel matrix.

torch.Size([187927, 2])
We keep 1.40e+08/1.22e+10 =  1% of the original kernel matrix.

torch.Size([97955, 2])
We keep 2.74e+07/2.95e+09 =  0% of the original kernel matrix.

torch.Size([63117, 2])
We keep 1.40e+07/1.17e+09 =  1% of the original kernel matrix.

torch.Size([57196, 2])
We keep 1.02e+07/9.11e+08 =  1% of the original kernel matrix.

torch.Size([86720, 2])
We keep 1.87e+08/4.05e+09 =  4% of the original kernel matrix.

torch.Size([64307, 2])
We keep 1.66e+07/1.70e+09 =  0% of the original kernel matrix.

torch.Size([607643, 2])
We keep 1.43e+09/1.21e+11 =  1% of the original kernel matrix.

torch.Size([179063, 2])
We keep 7.54e+07/9.27e+09 =  0% of the original kernel matrix.

torch.Size([85751, 2])
We keep 2.57e+07/2.13e+09 =  1% of the original kernel matrix.

torch.Size([65370, 2])
We keep 1.30e+07/1.23e+09 =  1% of the original kernel matrix.

torch.Size([82834, 2])
We keep 3.73e+07/2.08e+09 =  1% of the original kernel matrix.

torch.Size([64059, 2])
We keep 1.28e+07/1.22e+09 =  1% of the original kernel matrix.

torch.Size([11000, 2])
We keep 1.29e+06/2.78e+07 =  4% of the original kernel matrix.

torch.Size([23599, 2])
We keep 2.38e+06/1.41e+08 =  1% of the original kernel matrix.

torch.Size([48667, 2])
We keep 1.68e+07/8.42e+08 =  2% of the original kernel matrix.

torch.Size([49659, 2])
We keep 8.89e+06/7.74e+08 =  1% of the original kernel matrix.

torch.Size([12437, 2])
We keep 1.81e+06/3.39e+07 =  5% of the original kernel matrix.

torch.Size([25255, 2])
We keep 2.63e+06/1.55e+08 =  1% of the original kernel matrix.

torch.Size([239120, 2])
We keep 3.04e+08/1.68e+10 =  1% of the original kernel matrix.

torch.Size([111277, 2])
We keep 3.11e+07/3.46e+09 =  0% of the original kernel matrix.

torch.Size([1053835, 2])
We keep 2.12e+09/3.46e+11 =  0% of the original kernel matrix.

torch.Size([248414, 2])
We keep 1.21e+08/1.57e+10 =  0% of the original kernel matrix.

torch.Size([77487, 2])
We keep 8.21e+07/2.49e+09 =  3% of the original kernel matrix.

torch.Size([61879, 2])
We keep 1.43e+07/1.33e+09 =  1% of the original kernel matrix.

torch.Size([108755, 2])
We keep 9.45e+07/6.39e+09 =  1% of the original kernel matrix.

torch.Size([68908, 2])
We keep 2.10e+07/2.13e+09 =  0% of the original kernel matrix.

torch.Size([41330, 2])
We keep 6.49e+07/8.49e+08 =  7% of the original kernel matrix.

torch.Size([45212, 2])
We keep 8.93e+06/7.78e+08 =  1% of the original kernel matrix.

torch.Size([22090, 2])
We keep 9.01e+06/1.81e+08 =  4% of the original kernel matrix.

torch.Size([33038, 2])
We keep 5.00e+06/3.59e+08 =  1% of the original kernel matrix.

torch.Size([111393, 2])
We keep 5.18e+07/3.70e+09 =  1% of the original kernel matrix.

torch.Size([74732, 2])
We keep 1.67e+07/1.62e+09 =  1% of the original kernel matrix.

torch.Size([9403, 2])
We keep 9.49e+05/2.30e+07 =  4% of the original kernel matrix.

torch.Size([21658, 2])
We keep 2.24e+06/1.28e+08 =  1% of the original kernel matrix.

torch.Size([101802, 2])
We keep 1.70e+08/5.72e+09 =  2% of the original kernel matrix.

torch.Size([68976, 2])
We keep 2.05e+07/2.02e+09 =  1% of the original kernel matrix.

torch.Size([426374, 2])
We keep 3.16e+08/5.24e+10 =  0% of the original kernel matrix.

torch.Size([152126, 2])
We keep 5.17e+07/6.11e+09 =  0% of the original kernel matrix.

torch.Size([30047, 2])
We keep 3.29e+07/9.82e+08 =  3% of the original kernel matrix.

torch.Size([35848, 2])
We keep 9.49e+06/8.36e+08 =  1% of the original kernel matrix.

torch.Size([543393, 2])
We keep 3.96e+09/1.97e+11 =  2% of the original kernel matrix.

torch.Size([161078, 2])
We keep 9.62e+07/1.19e+10 =  0% of the original kernel matrix.

torch.Size([153421, 2])
We keep 3.42e+08/1.38e+10 =  2% of the original kernel matrix.

torch.Size([86809, 2])
We keep 2.92e+07/3.13e+09 =  0% of the original kernel matrix.

torch.Size([45347, 2])
We keep 8.88e+06/6.33e+08 =  1% of the original kernel matrix.

torch.Size([46522, 2])
We keep 7.61e+06/6.72e+08 =  1% of the original kernel matrix.

torch.Size([6191, 2])
We keep 3.27e+05/6.33e+06 =  5% of the original kernel matrix.

torch.Size([18425, 2])
We keep 1.45e+06/6.71e+07 =  2% of the original kernel matrix.

torch.Size([83882, 2])
We keep 4.04e+07/2.43e+09 =  1% of the original kernel matrix.

torch.Size([64697, 2])
We keep 1.39e+07/1.32e+09 =  1% of the original kernel matrix.

torch.Size([154299, 2])
We keep 6.06e+07/6.47e+09 =  0% of the original kernel matrix.

torch.Size([87421, 2])
We keep 2.08e+07/2.15e+09 =  0% of the original kernel matrix.

torch.Size([7021, 2])
We keep 4.54e+05/1.02e+07 =  4% of the original kernel matrix.

torch.Size([19191, 2])
We keep 1.65e+06/8.53e+07 =  1% of the original kernel matrix.

torch.Size([661110, 2])
We keep 6.55e+08/1.10e+11 =  0% of the original kernel matrix.

torch.Size([187328, 2])
We keep 7.19e+07/8.84e+09 =  0% of the original kernel matrix.

torch.Size([28135, 2])
We keep 1.73e+07/3.56e+08 =  4% of the original kernel matrix.

torch.Size([37587, 2])
We keep 6.23e+06/5.04e+08 =  1% of the original kernel matrix.

torch.Size([27742, 2])
We keep 4.51e+06/2.23e+08 =  2% of the original kernel matrix.

torch.Size([37470, 2])
We keep 5.23e+06/3.98e+08 =  1% of the original kernel matrix.

torch.Size([12571, 2])
We keep 8.21e+05/2.72e+07 =  3% of the original kernel matrix.

torch.Size([25257, 2])
We keep 2.41e+06/1.39e+08 =  1% of the original kernel matrix.

torch.Size([6569, 2])
We keep 2.59e+05/6.19e+06 =  4% of the original kernel matrix.

torch.Size([19001, 2])
We keep 1.41e+06/6.64e+07 =  2% of the original kernel matrix.

torch.Size([3026, 2])
We keep 7.48e+04/1.16e+06 =  6% of the original kernel matrix.

torch.Size([13971, 2])
We keep 8.10e+05/2.87e+07 =  2% of the original kernel matrix.

torch.Size([10494, 2])
We keep 2.27e+06/3.35e+07 =  6% of the original kernel matrix.

torch.Size([22628, 2])
We keep 2.67e+06/1.55e+08 =  1% of the original kernel matrix.

torch.Size([21729, 2])
We keep 4.93e+06/1.30e+08 =  3% of the original kernel matrix.

torch.Size([32930, 2])
We keep 4.35e+06/3.05e+08 =  1% of the original kernel matrix.

torch.Size([69743, 2])
We keep 3.47e+07/1.70e+09 =  2% of the original kernel matrix.

torch.Size([58523, 2])
We keep 1.14e+07/1.10e+09 =  1% of the original kernel matrix.

torch.Size([2008153, 2])
We keep 2.20e+10/2.32e+12 =  0% of the original kernel matrix.

torch.Size([303684, 2])
We keep 2.93e+08/4.06e+10 =  0% of the original kernel matrix.

torch.Size([11873, 2])
We keep 1.87e+06/4.29e+07 =  4% of the original kernel matrix.

torch.Size([24075, 2])
We keep 2.86e+06/1.75e+08 =  1% of the original kernel matrix.

torch.Size([83290, 2])
We keep 2.18e+07/2.00e+09 =  1% of the original kernel matrix.

torch.Size([64582, 2])
We keep 1.26e+07/1.19e+09 =  1% of the original kernel matrix.

torch.Size([7683, 2])
We keep 4.39e+05/1.01e+07 =  4% of the original kernel matrix.

torch.Size([20196, 2])
We keep 1.69e+06/8.47e+07 =  1% of the original kernel matrix.

torch.Size([22870, 2])
We keep 2.77e+06/1.21e+08 =  2% of the original kernel matrix.

torch.Size([33799, 2])
We keep 4.21e+06/2.93e+08 =  1% of the original kernel matrix.

torch.Size([15378, 2])
We keep 1.95e+06/5.39e+07 =  3% of the original kernel matrix.

torch.Size([27620, 2])
We keep 3.10e+06/1.96e+08 =  1% of the original kernel matrix.

torch.Size([96685, 2])
We keep 2.34e+08/8.13e+09 =  2% of the original kernel matrix.

torch.Size([64638, 2])
We keep 2.30e+07/2.41e+09 =  0% of the original kernel matrix.

torch.Size([41327, 2])
We keep 4.63e+07/1.09e+09 =  4% of the original kernel matrix.

torch.Size([44124, 2])
We keep 1.02e+07/8.80e+08 =  1% of the original kernel matrix.

torch.Size([890620, 2])
We keep 1.35e+09/2.29e+11 =  0% of the original kernel matrix.

torch.Size([221797, 2])
We keep 1.01e+08/1.28e+10 =  0% of the original kernel matrix.

torch.Size([10903, 2])
We keep 6.98e+05/2.08e+07 =  3% of the original kernel matrix.

torch.Size([23508, 2])
We keep 2.20e+06/1.22e+08 =  1% of the original kernel matrix.

torch.Size([196401, 2])
We keep 1.39e+08/1.17e+10 =  1% of the original kernel matrix.

torch.Size([100293, 2])
We keep 2.67e+07/2.88e+09 =  0% of the original kernel matrix.

torch.Size([26990, 2])
We keep 9.38e+06/2.38e+08 =  3% of the original kernel matrix.

torch.Size([37062, 2])
We keep 5.39e+06/4.12e+08 =  1% of the original kernel matrix.

torch.Size([163676, 2])
We keep 1.12e+08/9.43e+09 =  1% of the original kernel matrix.

torch.Size([90407, 2])
We keep 2.43e+07/2.59e+09 =  0% of the original kernel matrix.

torch.Size([318220, 2])
We keep 4.29e+08/4.09e+10 =  1% of the original kernel matrix.

torch.Size([128705, 2])
We keep 4.62e+07/5.40e+09 =  0% of the original kernel matrix.

torch.Size([19595, 2])
We keep 2.17e+06/8.34e+07 =  2% of the original kernel matrix.

torch.Size([31069, 2])
We keep 3.64e+06/2.44e+08 =  1% of the original kernel matrix.

torch.Size([12141, 2])
We keep 1.13e+06/2.88e+07 =  3% of the original kernel matrix.

torch.Size([24809, 2])
We keep 2.45e+06/1.43e+08 =  1% of the original kernel matrix.

torch.Size([128213, 2])
We keep 4.74e+07/4.88e+09 =  0% of the original kernel matrix.

torch.Size([79419, 2])
We keep 1.86e+07/1.86e+09 =  0% of the original kernel matrix.

torch.Size([14921, 2])
We keep 2.90e+06/6.20e+07 =  4% of the original kernel matrix.

torch.Size([26920, 2])
We keep 3.24e+06/2.10e+08 =  1% of the original kernel matrix.

torch.Size([32887, 2])
We keep 1.29e+07/4.61e+08 =  2% of the original kernel matrix.

torch.Size([39743, 2])
We keep 7.13e+06/5.73e+08 =  1% of the original kernel matrix.

torch.Size([128060, 2])
We keep 6.46e+07/4.97e+09 =  1% of the original kernel matrix.

torch.Size([78867, 2])
We keep 1.89e+07/1.88e+09 =  1% of the original kernel matrix.

torch.Size([25636, 2])
We keep 4.59e+07/5.89e+08 =  7% of the original kernel matrix.

torch.Size([34051, 2])
We keep 7.93e+06/6.48e+08 =  1% of the original kernel matrix.

torch.Size([52359, 2])
We keep 3.58e+07/9.98e+08 =  3% of the original kernel matrix.

torch.Size([51119, 2])
We keep 9.78e+06/8.43e+08 =  1% of the original kernel matrix.

torch.Size([23014, 2])
We keep 4.18e+06/1.50e+08 =  2% of the original kernel matrix.

torch.Size([33594, 2])
We keep 4.54e+06/3.27e+08 =  1% of the original kernel matrix.

torch.Size([79799, 2])
We keep 2.94e+07/2.04e+09 =  1% of the original kernel matrix.

torch.Size([62783, 2])
We keep 1.29e+07/1.21e+09 =  1% of the original kernel matrix.

torch.Size([303575, 2])
We keep 4.11e+08/3.75e+10 =  1% of the original kernel matrix.

torch.Size([125903, 2])
We keep 4.38e+07/5.17e+09 =  0% of the original kernel matrix.

torch.Size([17724, 2])
We keep 2.04e+06/6.60e+07 =  3% of the original kernel matrix.

torch.Size([29638, 2])
We keep 3.18e+06/2.17e+08 =  1% of the original kernel matrix.

torch.Size([11250, 2])
We keep 1.12e+06/2.99e+07 =  3% of the original kernel matrix.

torch.Size([23647, 2])
We keep 2.52e+06/1.46e+08 =  1% of the original kernel matrix.

torch.Size([115615, 2])
We keep 9.72e+07/5.90e+09 =  1% of the original kernel matrix.

torch.Size([74150, 2])
We keep 2.05e+07/2.05e+09 =  0% of the original kernel matrix.

torch.Size([46678, 2])
We keep 1.56e+07/7.93e+08 =  1% of the original kernel matrix.

torch.Size([48239, 2])
We keep 8.71e+06/7.52e+08 =  1% of the original kernel matrix.

torch.Size([199584, 2])
We keep 6.49e+08/1.83e+10 =  3% of the original kernel matrix.

torch.Size([100101, 2])
We keep 3.37e+07/3.61e+09 =  0% of the original kernel matrix.

torch.Size([33721, 2])
We keep 1.72e+07/5.18e+08 =  3% of the original kernel matrix.

torch.Size([40766, 2])
We keep 7.65e+06/6.07e+08 =  1% of the original kernel matrix.

torch.Size([33981, 2])
We keep 1.56e+07/5.74e+08 =  2% of the original kernel matrix.

torch.Size([39850, 2])
We keep 7.66e+06/6.39e+08 =  1% of the original kernel matrix.

torch.Size([24874, 2])
We keep 7.31e+06/2.36e+08 =  3% of the original kernel matrix.

torch.Size([34723, 2])
We keep 5.29e+06/4.10e+08 =  1% of the original kernel matrix.

torch.Size([122860, 2])
We keep 2.01e+08/1.03e+10 =  1% of the original kernel matrix.

torch.Size([73962, 2])
We keep 2.56e+07/2.71e+09 =  0% of the original kernel matrix.

torch.Size([82810, 2])
We keep 1.03e+08/3.71e+09 =  2% of the original kernel matrix.

torch.Size([62339, 2])
We keep 1.70e+07/1.63e+09 =  1% of the original kernel matrix.

torch.Size([37906, 2])
We keep 7.59e+06/4.24e+08 =  1% of the original kernel matrix.

torch.Size([44574, 2])
We keep 6.95e+06/5.49e+08 =  1% of the original kernel matrix.

torch.Size([10493, 2])
We keep 7.18e+05/1.93e+07 =  3% of the original kernel matrix.

torch.Size([23044, 2])
We keep 2.12e+06/1.17e+08 =  1% of the original kernel matrix.

torch.Size([28888, 2])
We keep 4.34e+06/2.18e+08 =  1% of the original kernel matrix.

torch.Size([38637, 2])
We keep 5.26e+06/3.94e+08 =  1% of the original kernel matrix.

torch.Size([37870, 2])
We keep 2.11e+07/7.19e+08 =  2% of the original kernel matrix.

torch.Size([42882, 2])
We keep 8.19e+06/7.16e+08 =  1% of the original kernel matrix.

torch.Size([94445, 2])
We keep 1.03e+08/3.49e+09 =  2% of the original kernel matrix.

torch.Size([67097, 2])
We keep 1.62e+07/1.58e+09 =  1% of the original kernel matrix.

torch.Size([153024, 2])
We keep 8.72e+07/6.63e+09 =  1% of the original kernel matrix.

torch.Size([87191, 2])
We keep 2.09e+07/2.17e+09 =  0% of the original kernel matrix.

torch.Size([13205, 2])
We keep 1.19e+06/3.55e+07 =  3% of the original kernel matrix.

torch.Size([25531, 2])
We keep 2.67e+06/1.59e+08 =  1% of the original kernel matrix.

torch.Size([35794, 2])
We keep 5.22e+06/3.71e+08 =  1% of the original kernel matrix.

torch.Size([42515, 2])
We keep 6.39e+06/5.14e+08 =  1% of the original kernel matrix.

torch.Size([35400, 2])
We keep 7.37e+06/3.68e+08 =  2% of the original kernel matrix.

torch.Size([42441, 2])
We keep 6.41e+06/5.12e+08 =  1% of the original kernel matrix.

torch.Size([186213, 2])
We keep 2.92e+08/1.11e+10 =  2% of the original kernel matrix.

torch.Size([97571, 2])
We keep 2.52e+07/2.81e+09 =  0% of the original kernel matrix.

torch.Size([42136, 2])
We keep 9.28e+06/5.38e+08 =  1% of the original kernel matrix.

torch.Size([46754, 2])
We keep 7.55e+06/6.19e+08 =  1% of the original kernel matrix.

torch.Size([91425, 2])
We keep 2.47e+07/2.27e+09 =  1% of the original kernel matrix.

torch.Size([67369, 2])
We keep 1.34e+07/1.27e+09 =  1% of the original kernel matrix.

torch.Size([19350, 2])
We keep 4.52e+06/1.26e+08 =  3% of the original kernel matrix.

torch.Size([30779, 2])
We keep 4.32e+06/2.99e+08 =  1% of the original kernel matrix.

torch.Size([37144, 2])
We keep 5.83e+06/3.99e+08 =  1% of the original kernel matrix.

torch.Size([44098, 2])
We keep 6.61e+06/5.33e+08 =  1% of the original kernel matrix.

torch.Size([3882, 2])
We keep 1.78e+05/2.94e+06 =  6% of the original kernel matrix.

torch.Size([14894, 2])
We keep 1.11e+06/4.57e+07 =  2% of the original kernel matrix.

torch.Size([13582, 2])
We keep 1.30e+06/3.67e+07 =  3% of the original kernel matrix.

torch.Size([25930, 2])
We keep 2.70e+06/1.62e+08 =  1% of the original kernel matrix.

torch.Size([148683, 2])
We keep 5.51e+07/5.91e+09 =  0% of the original kernel matrix.

torch.Size([85731, 2])
We keep 1.98e+07/2.05e+09 =  0% of the original kernel matrix.

torch.Size([45159, 2])
We keep 2.00e+07/9.09e+08 =  2% of the original kernel matrix.

torch.Size([47027, 2])
We keep 9.06e+06/8.05e+08 =  1% of the original kernel matrix.

torch.Size([64486, 2])
We keep 1.33e+07/1.19e+09 =  1% of the original kernel matrix.

torch.Size([57687, 2])
We keep 1.02e+07/9.21e+08 =  1% of the original kernel matrix.

torch.Size([4036, 2])
We keep 9.76e+04/1.96e+06 =  4% of the original kernel matrix.

torch.Size([16005, 2])
We keep 9.78e+05/3.74e+07 =  2% of the original kernel matrix.

torch.Size([7034, 2])
We keep 4.98e+05/8.32e+06 =  5% of the original kernel matrix.

torch.Size([19545, 2])
We keep 1.54e+06/7.70e+07 =  1% of the original kernel matrix.

torch.Size([49061, 2])
We keep 1.71e+07/8.04e+08 =  2% of the original kernel matrix.

torch.Size([50011, 2])
We keep 8.84e+06/7.57e+08 =  1% of the original kernel matrix.

torch.Size([61170, 2])
We keep 1.73e+07/1.20e+09 =  1% of the original kernel matrix.

torch.Size([55662, 2])
We keep 1.04e+07/9.24e+08 =  1% of the original kernel matrix.

torch.Size([27372, 2])
We keep 3.54e+06/1.91e+08 =  1% of the original kernel matrix.

torch.Size([37416, 2])
We keep 4.93e+06/3.69e+08 =  1% of the original kernel matrix.

torch.Size([6687, 2])
We keep 3.75e+05/7.48e+06 =  5% of the original kernel matrix.

torch.Size([19199, 2])
We keep 1.54e+06/7.30e+07 =  2% of the original kernel matrix.

torch.Size([5350, 2])
We keep 3.85e+05/6.26e+06 =  6% of the original kernel matrix.

torch.Size([16870, 2])
We keep 1.44e+06/6.67e+07 =  2% of the original kernel matrix.

torch.Size([26718, 2])
We keep 4.31e+06/1.86e+08 =  2% of the original kernel matrix.

torch.Size([36949, 2])
We keep 4.82e+06/3.64e+08 =  1% of the original kernel matrix.

torch.Size([35600, 2])
We keep 1.02e+07/4.50e+08 =  2% of the original kernel matrix.

torch.Size([42829, 2])
We keep 6.97e+06/5.66e+08 =  1% of the original kernel matrix.

torch.Size([74927, 2])
We keep 7.42e+07/2.94e+09 =  2% of the original kernel matrix.

torch.Size([59036, 2])
We keep 1.53e+07/1.45e+09 =  1% of the original kernel matrix.

torch.Size([8055, 2])
We keep 6.02e+05/1.27e+07 =  4% of the original kernel matrix.

torch.Size([20631, 2])
We keep 1.84e+06/9.53e+07 =  1% of the original kernel matrix.

torch.Size([21190, 2])
We keep 1.52e+07/1.73e+08 =  8% of the original kernel matrix.

torch.Size([32017, 2])
We keep 4.85e+06/3.51e+08 =  1% of the original kernel matrix.

torch.Size([22590, 2])
We keep 8.45e+06/2.65e+08 =  3% of the original kernel matrix.

torch.Size([32830, 2])
We keep 5.68e+06/4.35e+08 =  1% of the original kernel matrix.

torch.Size([10785, 2])
We keep 9.95e+05/2.65e+07 =  3% of the original kernel matrix.

torch.Size([23011, 2])
We keep 2.39e+06/1.37e+08 =  1% of the original kernel matrix.

torch.Size([256731, 2])
We keep 2.81e+08/2.33e+10 =  1% of the original kernel matrix.

torch.Size([115718, 2])
We keep 3.58e+07/4.08e+09 =  0% of the original kernel matrix.

torch.Size([15377, 2])
We keep 6.74e+06/8.69e+07 =  7% of the original kernel matrix.

torch.Size([27712, 2])
We keep 3.58e+06/2.49e+08 =  1% of the original kernel matrix.

torch.Size([92315, 2])
We keep 4.80e+07/3.14e+09 =  1% of the original kernel matrix.

torch.Size([66869, 2])
We keep 1.53e+07/1.49e+09 =  1% of the original kernel matrix.

torch.Size([166653, 2])
We keep 9.03e+07/7.61e+09 =  1% of the original kernel matrix.

torch.Size([91533, 2])
We keep 2.24e+07/2.33e+09 =  0% of the original kernel matrix.

torch.Size([51833, 2])
We keep 1.41e+07/8.37e+08 =  1% of the original kernel matrix.

torch.Size([51906, 2])
We keep 8.89e+06/7.72e+08 =  1% of the original kernel matrix.

torch.Size([151571, 2])
We keep 1.29e+08/8.04e+09 =  1% of the original kernel matrix.

torch.Size([87434, 2])
We keep 2.31e+07/2.39e+09 =  0% of the original kernel matrix.

torch.Size([212558, 2])
We keep 1.22e+08/1.29e+10 =  0% of the original kernel matrix.

torch.Size([104557, 2])
We keep 2.81e+07/3.03e+09 =  0% of the original kernel matrix.

torch.Size([3281, 2])
We keep 9.41e+04/1.60e+06 =  5% of the original kernel matrix.

torch.Size([14241, 2])
We keep 9.10e+05/3.37e+07 =  2% of the original kernel matrix.

torch.Size([65876, 2])
We keep 3.18e+07/1.31e+09 =  2% of the original kernel matrix.

torch.Size([57682, 2])
We keep 1.09e+07/9.65e+08 =  1% of the original kernel matrix.

torch.Size([17640, 2])
We keep 1.50e+06/5.86e+07 =  2% of the original kernel matrix.

torch.Size([29606, 2])
We keep 3.15e+06/2.04e+08 =  1% of the original kernel matrix.

torch.Size([308875, 2])
We keep 3.62e+08/3.14e+10 =  1% of the original kernel matrix.

torch.Size([128455, 2])
We keep 4.24e+07/4.73e+09 =  0% of the original kernel matrix.

torch.Size([141047, 2])
We keep 9.57e+07/7.19e+09 =  1% of the original kernel matrix.

torch.Size([83960, 2])
We keep 2.18e+07/2.26e+09 =  0% of the original kernel matrix.

torch.Size([29542, 2])
We keep 9.91e+06/4.04e+08 =  2% of the original kernel matrix.

torch.Size([37906, 2])
We keep 6.67e+06/5.36e+08 =  1% of the original kernel matrix.

torch.Size([53202, 2])
We keep 1.08e+07/8.35e+08 =  1% of the original kernel matrix.

torch.Size([52555, 2])
We keep 8.98e+06/7.71e+08 =  1% of the original kernel matrix.

torch.Size([19551, 2])
We keep 1.98e+06/9.06e+07 =  2% of the original kernel matrix.

torch.Size([31427, 2])
We keep 3.77e+06/2.54e+08 =  1% of the original kernel matrix.

torch.Size([12782, 2])
We keep 7.86e+06/1.47e+08 =  5% of the original kernel matrix.

torch.Size([23239, 2])
We keep 4.26e+06/3.23e+08 =  1% of the original kernel matrix.

torch.Size([14027, 2])
We keep 3.26e+06/4.51e+07 =  7% of the original kernel matrix.

torch.Size([26555, 2])
We keep 2.89e+06/1.79e+08 =  1% of the original kernel matrix.

torch.Size([14424, 2])
We keep 1.46e+06/5.15e+07 =  2% of the original kernel matrix.

torch.Size([26259, 2])
We keep 3.03e+06/1.92e+08 =  1% of the original kernel matrix.

torch.Size([260158, 2])
We keep 3.32e+08/2.09e+10 =  1% of the original kernel matrix.

torch.Size([116643, 2])
We keep 3.49e+07/3.86e+09 =  0% of the original kernel matrix.

torch.Size([342313, 2])
We keep 4.66e+08/4.12e+10 =  1% of the original kernel matrix.

torch.Size([137650, 2])
We keep 4.80e+07/5.42e+09 =  0% of the original kernel matrix.

torch.Size([14610, 2])
We keep 3.50e+06/8.86e+07 =  3% of the original kernel matrix.

torch.Size([26025, 2])
We keep 3.77e+06/2.51e+08 =  1% of the original kernel matrix.

torch.Size([16835, 2])
We keep 1.52e+06/5.97e+07 =  2% of the original kernel matrix.

torch.Size([29147, 2])
We keep 3.15e+06/2.06e+08 =  1% of the original kernel matrix.

torch.Size([26327, 2])
We keep 4.40e+06/1.75e+08 =  2% of the original kernel matrix.

torch.Size([36522, 2])
We keep 4.81e+06/3.53e+08 =  1% of the original kernel matrix.

torch.Size([145010, 2])
We keep 1.17e+08/7.67e+09 =  1% of the original kernel matrix.

torch.Size([85035, 2])
We keep 2.23e+07/2.34e+09 =  0% of the original kernel matrix.

torch.Size([27212, 2])
We keep 3.45e+06/1.76e+08 =  1% of the original kernel matrix.

torch.Size([37187, 2])
We keep 4.79e+06/3.54e+08 =  1% of the original kernel matrix.

torch.Size([13115, 2])
We keep 8.85e+05/3.19e+07 =  2% of the original kernel matrix.

torch.Size([25785, 2])
We keep 2.50e+06/1.51e+08 =  1% of the original kernel matrix.

torch.Size([65562, 2])
We keep 2.16e+07/1.35e+09 =  1% of the original kernel matrix.

torch.Size([57721, 2])
We keep 1.08e+07/9.82e+08 =  1% of the original kernel matrix.

torch.Size([117197, 2])
We keep 5.47e+07/3.79e+09 =  1% of the original kernel matrix.

torch.Size([75834, 2])
We keep 1.66e+07/1.64e+09 =  1% of the original kernel matrix.

torch.Size([11780, 2])
We keep 1.15e+06/3.05e+07 =  3% of the original kernel matrix.

torch.Size([24180, 2])
We keep 2.47e+06/1.47e+08 =  1% of the original kernel matrix.

torch.Size([39313, 2])
We keep 1.44e+07/4.97e+08 =  2% of the original kernel matrix.

torch.Size([45161, 2])
We keep 7.42e+06/5.95e+08 =  1% of the original kernel matrix.

torch.Size([15558, 2])
We keep 1.96e+06/4.98e+07 =  3% of the original kernel matrix.

torch.Size([28074, 2])
We keep 2.89e+06/1.88e+08 =  1% of the original kernel matrix.

time for making ranges is 4.018868446350098
Sorting X and nu_X
time for sorting X is 0.0780329704284668
Sorting Z and nu_Z
time for sorting Z is 0.00027632713317871094
Starting Optim
sum tnu_Z before tensor(28862710., device='cuda:0')
c= tensor(604.8281, device='cuda:0')
c= tensor(33237.4688, device='cuda:0')
c= tensor(34745.3594, device='cuda:0')
c= tensor(72863.5938, device='cuda:0')
c= tensor(269477.5625, device='cuda:0')
c= tensor(371673.9375, device='cuda:0')
c= tensor(754488.8750, device='cuda:0')
c= tensor(908841.5000, device='cuda:0')
c= tensor(944828.3750, device='cuda:0')
c= tensor(2505062., device='cuda:0')
c= tensor(2516766.7500, device='cuda:0')
c= tensor(5639022., device='cuda:0')
c= tensor(5657395., device='cuda:0')
c= tensor(14384125., device='cuda:0')
c= tensor(14522952., device='cuda:0')
c= tensor(14613977., device='cuda:0')
c= tensor(15245757., device='cuda:0')
c= tensor(15724501., device='cuda:0')
c= tensor(19278770., device='cuda:0')
c= tensor(20821552., device='cuda:0')
c= tensor(20912360., device='cuda:0')
c= tensor(25469430., device='cuda:0')
c= tensor(25502576., device='cuda:0')
c= tensor(25916432., device='cuda:0')
c= tensor(26062998., device='cuda:0')
c= tensor(26990090., device='cuda:0')
c= tensor(27539634., device='cuda:0')
c= tensor(27556042., device='cuda:0')
c= tensor(29743940., device='cuda:0')
c= tensor(2.1839e+08, device='cuda:0')
c= tensor(2.1840e+08, device='cuda:0')
c= tensor(2.8998e+08, device='cuda:0')
c= tensor(2.9016e+08, device='cuda:0')
c= tensor(2.9017e+08, device='cuda:0')
c= tensor(2.9020e+08, device='cuda:0')
c= tensor(2.9404e+08, device='cuda:0')
c= tensor(2.9532e+08, device='cuda:0')
c= tensor(2.9532e+08, device='cuda:0')
c= tensor(2.9533e+08, device='cuda:0')
c= tensor(2.9533e+08, device='cuda:0')
c= tensor(2.9533e+08, device='cuda:0')
c= tensor(2.9533e+08, device='cuda:0')
c= tensor(2.9533e+08, device='cuda:0')
c= tensor(2.9534e+08, device='cuda:0')
c= tensor(2.9534e+08, device='cuda:0')
c= tensor(2.9534e+08, device='cuda:0')
c= tensor(2.9534e+08, device='cuda:0')
c= tensor(2.9534e+08, device='cuda:0')
c= tensor(2.9535e+08, device='cuda:0')
c= tensor(2.9539e+08, device='cuda:0')
c= tensor(2.9543e+08, device='cuda:0')
c= tensor(2.9543e+08, device='cuda:0')
c= tensor(2.9544e+08, device='cuda:0')
c= tensor(2.9544e+08, device='cuda:0')
c= tensor(2.9545e+08, device='cuda:0')
c= tensor(2.9545e+08, device='cuda:0')
c= tensor(2.9546e+08, device='cuda:0')
c= tensor(2.9546e+08, device='cuda:0')
c= tensor(2.9547e+08, device='cuda:0')
c= tensor(2.9547e+08, device='cuda:0')
c= tensor(2.9548e+08, device='cuda:0')
c= tensor(2.9548e+08, device='cuda:0')
c= tensor(2.9554e+08, device='cuda:0')
c= tensor(2.9554e+08, device='cuda:0')
c= tensor(2.9555e+08, device='cuda:0')
c= tensor(2.9555e+08, device='cuda:0')
c= tensor(2.9555e+08, device='cuda:0')
c= tensor(2.9555e+08, device='cuda:0')
c= tensor(2.9556e+08, device='cuda:0')
c= tensor(2.9556e+08, device='cuda:0')
c= tensor(2.9557e+08, device='cuda:0')
c= tensor(2.9558e+08, device='cuda:0')
c= tensor(2.9558e+08, device='cuda:0')
c= tensor(2.9559e+08, device='cuda:0')
c= tensor(2.9559e+08, device='cuda:0')
c= tensor(2.9560e+08, device='cuda:0')
c= tensor(2.9560e+08, device='cuda:0')
c= tensor(2.9560e+08, device='cuda:0')
c= tensor(2.9560e+08, device='cuda:0')
c= tensor(2.9562e+08, device='cuda:0')
c= tensor(2.9562e+08, device='cuda:0')
c= tensor(2.9562e+08, device='cuda:0')
c= tensor(2.9563e+08, device='cuda:0')
c= tensor(2.9563e+08, device='cuda:0')
c= tensor(2.9563e+08, device='cuda:0')
c= tensor(2.9563e+08, device='cuda:0')
c= tensor(2.9564e+08, device='cuda:0')
c= tensor(2.9564e+08, device='cuda:0')
c= tensor(2.9564e+08, device='cuda:0')
c= tensor(2.9565e+08, device='cuda:0')
c= tensor(2.9565e+08, device='cuda:0')
c= tensor(2.9565e+08, device='cuda:0')
c= tensor(2.9565e+08, device='cuda:0')
c= tensor(2.9566e+08, device='cuda:0')
c= tensor(2.9566e+08, device='cuda:0')
c= tensor(2.9567e+08, device='cuda:0')
c= tensor(2.9567e+08, device='cuda:0')
c= tensor(2.9568e+08, device='cuda:0')
c= tensor(2.9571e+08, device='cuda:0')
c= tensor(2.9571e+08, device='cuda:0')
c= tensor(2.9576e+08, device='cuda:0')
c= tensor(2.9577e+08, device='cuda:0')
c= tensor(2.9577e+08, device='cuda:0')
c= tensor(2.9577e+08, device='cuda:0')
c= tensor(2.9579e+08, device='cuda:0')
c= tensor(2.9579e+08, device='cuda:0')
c= tensor(2.9580e+08, device='cuda:0')
c= tensor(2.9580e+08, device='cuda:0')
c= tensor(2.9580e+08, device='cuda:0')
c= tensor(2.9580e+08, device='cuda:0')
c= tensor(2.9580e+08, device='cuda:0')
c= tensor(2.9580e+08, device='cuda:0')
c= tensor(2.9581e+08, device='cuda:0')
c= tensor(2.9581e+08, device='cuda:0')
c= tensor(2.9582e+08, device='cuda:0')
c= tensor(2.9582e+08, device='cuda:0')
c= tensor(2.9582e+08, device='cuda:0')
c= tensor(2.9582e+08, device='cuda:0')
c= tensor(2.9584e+08, device='cuda:0')
c= tensor(2.9584e+08, device='cuda:0')
c= tensor(2.9586e+08, device='cuda:0')
c= tensor(2.9586e+08, device='cuda:0')
c= tensor(2.9586e+08, device='cuda:0')
c= tensor(2.9586e+08, device='cuda:0')
c= tensor(2.9587e+08, device='cuda:0')
c= tensor(2.9587e+08, device='cuda:0')
c= tensor(2.9587e+08, device='cuda:0')
c= tensor(2.9587e+08, device='cuda:0')
c= tensor(2.9590e+08, device='cuda:0')
c= tensor(2.9590e+08, device='cuda:0')
c= tensor(2.9591e+08, device='cuda:0')
c= tensor(2.9591e+08, device='cuda:0')
c= tensor(2.9591e+08, device='cuda:0')
c= tensor(2.9592e+08, device='cuda:0')
c= tensor(2.9593e+08, device='cuda:0')
c= tensor(2.9593e+08, device='cuda:0')
c= tensor(2.9593e+08, device='cuda:0')
c= tensor(2.9593e+08, device='cuda:0')
c= tensor(2.9593e+08, device='cuda:0')
c= tensor(2.9593e+08, device='cuda:0')
c= tensor(2.9594e+08, device='cuda:0')
c= tensor(2.9594e+08, device='cuda:0')
c= tensor(2.9598e+08, device='cuda:0')
c= tensor(2.9600e+08, device='cuda:0')
c= tensor(2.9601e+08, device='cuda:0')
c= tensor(2.9601e+08, device='cuda:0')
c= tensor(2.9601e+08, device='cuda:0')
c= tensor(2.9602e+08, device='cuda:0')
c= tensor(2.9602e+08, device='cuda:0')
c= tensor(2.9602e+08, device='cuda:0')
c= tensor(2.9602e+08, device='cuda:0')
c= tensor(2.9603e+08, device='cuda:0')
c= tensor(2.9604e+08, device='cuda:0')
c= tensor(2.9605e+08, device='cuda:0')
c= tensor(2.9606e+08, device='cuda:0')
c= tensor(2.9612e+08, device='cuda:0')
c= tensor(2.9612e+08, device='cuda:0')
c= tensor(2.9613e+08, device='cuda:0')
c= tensor(2.9613e+08, device='cuda:0')
c= tensor(2.9613e+08, device='cuda:0')
c= tensor(2.9615e+08, device='cuda:0')
c= tensor(2.9615e+08, device='cuda:0')
c= tensor(2.9616e+08, device='cuda:0')
c= tensor(2.9616e+08, device='cuda:0')
c= tensor(2.9617e+08, device='cuda:0')
c= tensor(2.9617e+08, device='cuda:0')
c= tensor(2.9617e+08, device='cuda:0')
c= tensor(2.9617e+08, device='cuda:0')
c= tensor(2.9618e+08, device='cuda:0')
c= tensor(2.9618e+08, device='cuda:0')
c= tensor(2.9618e+08, device='cuda:0')
c= tensor(2.9618e+08, device='cuda:0')
c= tensor(2.9618e+08, device='cuda:0')
c= tensor(2.9619e+08, device='cuda:0')
c= tensor(2.9619e+08, device='cuda:0')
c= tensor(2.9620e+08, device='cuda:0')
c= tensor(2.9620e+08, device='cuda:0')
c= tensor(2.9621e+08, device='cuda:0')
c= tensor(2.9621e+08, device='cuda:0')
c= tensor(2.9621e+08, device='cuda:0')
c= tensor(2.9622e+08, device='cuda:0')
c= tensor(2.9622e+08, device='cuda:0')
c= tensor(2.9623e+08, device='cuda:0')
c= tensor(2.9623e+08, device='cuda:0')
c= tensor(2.9624e+08, device='cuda:0')
c= tensor(2.9624e+08, device='cuda:0')
c= tensor(2.9624e+08, device='cuda:0')
c= tensor(2.9625e+08, device='cuda:0')
c= tensor(2.9625e+08, device='cuda:0')
c= tensor(2.9629e+08, device='cuda:0')
c= tensor(2.9629e+08, device='cuda:0')
c= tensor(2.9629e+08, device='cuda:0')
c= tensor(2.9629e+08, device='cuda:0')
c= tensor(2.9630e+08, device='cuda:0')
c= tensor(2.9630e+08, device='cuda:0')
c= tensor(2.9630e+08, device='cuda:0')
c= tensor(2.9631e+08, device='cuda:0')
c= tensor(2.9631e+08, device='cuda:0')
c= tensor(2.9631e+08, device='cuda:0')
c= tensor(2.9631e+08, device='cuda:0')
c= tensor(2.9632e+08, device='cuda:0')
c= tensor(2.9632e+08, device='cuda:0')
c= tensor(2.9635e+08, device='cuda:0')
c= tensor(2.9636e+08, device='cuda:0')
c= tensor(2.9637e+08, device='cuda:0')
c= tensor(2.9637e+08, device='cuda:0')
c= tensor(2.9637e+08, device='cuda:0')
c= tensor(2.9638e+08, device='cuda:0')
c= tensor(2.9638e+08, device='cuda:0')
c= tensor(2.9641e+08, device='cuda:0')
c= tensor(2.9644e+08, device='cuda:0')
c= tensor(2.9644e+08, device='cuda:0')
c= tensor(2.9644e+08, device='cuda:0')
c= tensor(2.9644e+08, device='cuda:0')
c= tensor(2.9644e+08, device='cuda:0')
c= tensor(2.9644e+08, device='cuda:0')
c= tensor(2.9644e+08, device='cuda:0')
c= tensor(2.9645e+08, device='cuda:0')
c= tensor(2.9647e+08, device='cuda:0')
c= tensor(2.9647e+08, device='cuda:0')
c= tensor(2.9647e+08, device='cuda:0')
c= tensor(2.9647e+08, device='cuda:0')
c= tensor(2.9648e+08, device='cuda:0')
c= tensor(2.9648e+08, device='cuda:0')
c= tensor(2.9649e+08, device='cuda:0')
c= tensor(2.9650e+08, device='cuda:0')
c= tensor(2.9650e+08, device='cuda:0')
c= tensor(2.9650e+08, device='cuda:0')
c= tensor(2.9652e+08, device='cuda:0')
c= tensor(2.9652e+08, device='cuda:0')
c= tensor(2.9652e+08, device='cuda:0')
c= tensor(2.9652e+08, device='cuda:0')
c= tensor(2.9653e+08, device='cuda:0')
c= tensor(2.9653e+08, device='cuda:0')
c= tensor(2.9654e+08, device='cuda:0')
c= tensor(2.9654e+08, device='cuda:0')
c= tensor(2.9654e+08, device='cuda:0')
c= tensor(2.9655e+08, device='cuda:0')
c= tensor(2.9655e+08, device='cuda:0')
c= tensor(2.9661e+08, device='cuda:0')
c= tensor(2.9770e+08, device='cuda:0')
c= tensor(2.9770e+08, device='cuda:0')
c= tensor(2.9771e+08, device='cuda:0')
c= tensor(2.9771e+08, device='cuda:0')
c= tensor(2.9772e+08, device='cuda:0')
c= tensor(2.9775e+08, device='cuda:0')
c= tensor(2.9966e+08, device='cuda:0')
c= tensor(2.9967e+08, device='cuda:0')
c= tensor(3.0140e+08, device='cuda:0')
c= tensor(3.0185e+08, device='cuda:0')
c= tensor(3.0192e+08, device='cuda:0')
c= tensor(3.0566e+08, device='cuda:0')
c= tensor(3.0566e+08, device='cuda:0')
c= tensor(3.0568e+08, device='cuda:0')
c= tensor(3.0871e+08, device='cuda:0')
c= tensor(3.3402e+08, device='cuda:0')
c= tensor(3.3403e+08, device='cuda:0')
c= tensor(3.3424e+08, device='cuda:0')
c= tensor(3.3439e+08, device='cuda:0')
c= tensor(3.3514e+08, device='cuda:0')
c= tensor(3.3572e+08, device='cuda:0')
c= tensor(3.3641e+08, device='cuda:0')
c= tensor(3.3654e+08, device='cuda:0')
c= tensor(3.3663e+08, device='cuda:0')
c= tensor(3.3669e+08, device='cuda:0')
c= tensor(3.4369e+08, device='cuda:0')
c= tensor(3.4370e+08, device='cuda:0')
c= tensor(3.4370e+08, device='cuda:0')
c= tensor(3.4378e+08, device='cuda:0')
c= tensor(3.4450e+08, device='cuda:0')
c= tensor(3.6727e+08, device='cuda:0')
c= tensor(3.6815e+08, device='cuda:0')
c= tensor(3.6815e+08, device='cuda:0')
c= tensor(3.6819e+08, device='cuda:0')
c= tensor(3.6821e+08, device='cuda:0')
c= tensor(3.6839e+08, device='cuda:0')
c= tensor(3.6964e+08, device='cuda:0')
c= tensor(3.7068e+08, device='cuda:0')
c= tensor(3.7111e+08, device='cuda:0')
c= tensor(3.7111e+08, device='cuda:0')
c= tensor(3.7112e+08, device='cuda:0')
c= tensor(3.7197e+08, device='cuda:0')
c= tensor(3.7251e+08, device='cuda:0')
c= tensor(3.7283e+08, device='cuda:0')
c= tensor(3.7284e+08, device='cuda:0')
c= tensor(3.9042e+08, device='cuda:0')
c= tensor(3.9043e+08, device='cuda:0')
c= tensor(3.9057e+08, device='cuda:0')
c= tensor(3.9212e+08, device='cuda:0')
c= tensor(3.9213e+08, device='cuda:0')
c= tensor(3.9269e+08, device='cuda:0')
c= tensor(4.0091e+08, device='cuda:0')
c= tensor(4.1495e+08, device='cuda:0')
c= tensor(4.1505e+08, device='cuda:0')
c= tensor(4.1513e+08, device='cuda:0')
c= tensor(4.1514e+08, device='cuda:0')
c= tensor(4.1514e+08, device='cuda:0')
c= tensor(4.1649e+08, device='cuda:0')
c= tensor(4.1651e+08, device='cuda:0')
c= tensor(4.1676e+08, device='cuda:0')
c= tensor(4.1892e+08, device='cuda:0')
c= tensor(4.1915e+08, device='cuda:0')
c= tensor(4.1934e+08, device='cuda:0')
c= tensor(4.1934e+08, device='cuda:0')
c= tensor(4.2355e+08, device='cuda:0')
c= tensor(4.2511e+08, device='cuda:0')
c= tensor(4.2537e+08, device='cuda:0')
c= tensor(4.2542e+08, device='cuda:0')
c= tensor(4.3598e+08, device='cuda:0')
c= tensor(4.3601e+08, device='cuda:0')
c= tensor(4.4244e+08, device='cuda:0')
c= tensor(4.4245e+08, device='cuda:0')
c= tensor(4.4336e+08, device='cuda:0')
c= tensor(4.4340e+08, device='cuda:0')
c= tensor(4.4511e+08, device='cuda:0')
c= tensor(4.4564e+08, device='cuda:0')
c= tensor(4.4565e+08, device='cuda:0')
c= tensor(4.4733e+08, device='cuda:0')
c= tensor(4.4945e+08, device='cuda:0')
c= tensor(4.4945e+08, device='cuda:0')
c= tensor(4.5115e+08, device='cuda:0')
c= tensor(4.5661e+08, device='cuda:0')
c= tensor(4.7319e+08, device='cuda:0')
c= tensor(4.7395e+08, device='cuda:0')
c= tensor(4.7395e+08, device='cuda:0')
c= tensor(4.7396e+08, device='cuda:0')
c= tensor(4.7409e+08, device='cuda:0')
c= tensor(4.7419e+08, device='cuda:0')
c= tensor(4.7441e+08, device='cuda:0')
c= tensor(4.7441e+08, device='cuda:0')
c= tensor(4.7494e+08, device='cuda:0')
c= tensor(4.7607e+08, device='cuda:0')
c= tensor(4.7623e+08, device='cuda:0')
c= tensor(4.7624e+08, device='cuda:0')
c= tensor(4.7637e+08, device='cuda:0')
c= tensor(4.7641e+08, device='cuda:0')
c= tensor(4.7645e+08, device='cuda:0')
c= tensor(4.7649e+08, device='cuda:0')
c= tensor(4.7649e+08, device='cuda:0')
c= tensor(4.8125e+08, device='cuda:0')
c= tensor(4.8141e+08, device='cuda:0')
c= tensor(4.8148e+08, device='cuda:0')
c= tensor(4.8167e+08, device='cuda:0')
c= tensor(4.8169e+08, device='cuda:0')
c= tensor(6.2061e+08, device='cuda:0')
c= tensor(6.2062e+08, device='cuda:0')
c= tensor(6.2334e+08, device='cuda:0')
c= tensor(6.2334e+08, device='cuda:0')
c= tensor(6.2335e+08, device='cuda:0')
c= tensor(6.2335e+08, device='cuda:0')
c= tensor(6.2337e+08, device='cuda:0')
c= tensor(6.2339e+08, device='cuda:0')
c= tensor(6.2396e+08, device='cuda:0')
c= tensor(6.2397e+08, device='cuda:0')
c= tensor(6.2398e+08, device='cuda:0')
c= tensor(6.3567e+08, device='cuda:0')
c= tensor(6.3591e+08, device='cuda:0')
c= tensor(6.3604e+08, device='cuda:0')
c= tensor(6.3757e+08, device='cuda:0')
c= tensor(6.4008e+08, device='cuda:0')
c= tensor(6.4009e+08, device='cuda:0')
c= tensor(6.4010e+08, device='cuda:0')
c= tensor(6.4016e+08, device='cuda:0')
c= tensor(6.4016e+08, device='cuda:0')
c= tensor(6.4016e+08, device='cuda:0')
c= tensor(6.4022e+08, device='cuda:0')
c= tensor(6.4022e+08, device='cuda:0')
c= tensor(6.4023e+08, device='cuda:0')
c= tensor(6.4023e+08, device='cuda:0')
c= tensor(6.4024e+08, device='cuda:0')
c= tensor(6.5972e+08, device='cuda:0')
c= tensor(6.5974e+08, device='cuda:0')
c= tensor(6.6096e+08, device='cuda:0')
c= tensor(6.6100e+08, device='cuda:0')
c= tensor(6.6101e+08, device='cuda:0')
c= tensor(6.6111e+08, device='cuda:0')
c= tensor(7.0273e+08, device='cuda:0')
c= tensor(7.1933e+08, device='cuda:0')
c= tensor(7.1939e+08, device='cuda:0')
c= tensor(7.1953e+08, device='cuda:0')
c= tensor(7.1953e+08, device='cuda:0')
c= tensor(7.2015e+08, device='cuda:0')
c= tensor(7.4673e+08, device='cuda:0')
c= tensor(7.4696e+08, device='cuda:0')
c= tensor(7.4699e+08, device='cuda:0')
c= tensor(7.4825e+08, device='cuda:0')
c= tensor(7.7621e+08, device='cuda:0')
c= tensor(7.7645e+08, device='cuda:0')
c= tensor(7.7647e+08, device='cuda:0')
c= tensor(7.7656e+08, device='cuda:0')
c= tensor(7.7665e+08, device='cuda:0')
c= tensor(7.7665e+08, device='cuda:0')
c= tensor(7.8188e+08, device='cuda:0')
c= tensor(7.8194e+08, device='cuda:0')
c= tensor(7.8194e+08, device='cuda:0')
c= tensor(7.8201e+08, device='cuda:0')
c= tensor(7.8208e+08, device='cuda:0')
c= tensor(7.8208e+08, device='cuda:0')
c= tensor(7.8381e+08, device='cuda:0')
c= tensor(7.8548e+08, device='cuda:0')
c= tensor(7.9032e+08, device='cuda:0')
c= tensor(7.9548e+08, device='cuda:0')
c= tensor(7.9802e+08, device='cuda:0')
c= tensor(7.9806e+08, device='cuda:0')
c= tensor(7.9810e+08, device='cuda:0')
c= tensor(7.9836e+08, device='cuda:0')
c= tensor(8.0063e+08, device='cuda:0')
c= tensor(8.0063e+08, device='cuda:0')
c= tensor(8.0384e+08, device='cuda:0')
c= tensor(8.3598e+08, device='cuda:0')
c= tensor(8.3731e+08, device='cuda:0')
c= tensor(8.3765e+08, device='cuda:0')
c= tensor(8.4126e+08, device='cuda:0')
c= tensor(8.4128e+08, device='cuda:0')
c= tensor(8.4129e+08, device='cuda:0')
c= tensor(8.4144e+08, device='cuda:0')
c= tensor(8.4274e+08, device='cuda:0')
c= tensor(8.4333e+08, device='cuda:0')
c= tensor(8.9730e+08, device='cuda:0')
c= tensor(8.9895e+08, device='cuda:0')
c= tensor(9.0063e+08, device='cuda:0')
c= tensor(9.0084e+08, device='cuda:0')
c= tensor(9.0391e+08, device='cuda:0')
c= tensor(9.0391e+08, device='cuda:0')
c= tensor(9.0393e+08, device='cuda:0')
c= tensor(9.1098e+08, device='cuda:0')
c= tensor(9.1108e+08, device='cuda:0')
c= tensor(9.1109e+08, device='cuda:0')
c= tensor(9.1119e+08, device='cuda:0')
c= tensor(9.2360e+08, device='cuda:0')
c= tensor(9.2396e+08, device='cuda:0')
c= tensor(9.2431e+08, device='cuda:0')
c= tensor(9.2432e+08, device='cuda:0')
c= tensor(9.2436e+08, device='cuda:0')
c= tensor(9.2436e+08, device='cuda:0')
c= tensor(9.2441e+08, device='cuda:0')
c= tensor(9.2449e+08, device='cuda:0')
c= tensor(9.2522e+08, device='cuda:0')
c= tensor(9.2524e+08, device='cuda:0')
c= tensor(9.2782e+08, device='cuda:0')
c= tensor(9.2782e+08, device='cuda:0')
c= tensor(9.2803e+08, device='cuda:0')
c= tensor(9.2808e+08, device='cuda:0')
c= tensor(9.2815e+08, device='cuda:0')
c= tensor(9.2815e+08, device='cuda:0')
c= tensor(9.2828e+08, device='cuda:0')
c= tensor(9.2836e+08, device='cuda:0')
c= tensor(9.2847e+08, device='cuda:0')
c= tensor(9.2930e+08, device='cuda:0')
c= tensor(9.5335e+08, device='cuda:0')
c= tensor(9.5335e+08, device='cuda:0')
c= tensor(9.5338e+08, device='cuda:0')
c= tensor(9.5430e+08, device='cuda:0')
c= tensor(9.5432e+08, device='cuda:0')
c= tensor(9.7726e+08, device='cuda:0')
c= tensor(9.7726e+08, device='cuda:0')
c= tensor(9.7960e+08, device='cuda:0')
c= tensor(9.8570e+08, device='cuda:0')
c= tensor(9.8571e+08, device='cuda:0')
c= tensor(9.9347e+08, device='cuda:0')
c= tensor(9.9382e+08, device='cuda:0')
c= tensor(1.0276e+09, device='cuda:0')
c= tensor(1.0276e+09, device='cuda:0')
c= tensor(1.0277e+09, device='cuda:0')
c= tensor(1.0278e+09, device='cuda:0')
c= tensor(1.0278e+09, device='cuda:0')
c= tensor(1.0278e+09, device='cuda:0')
c= tensor(1.0281e+09, device='cuda:0')
c= tensor(1.0281e+09, device='cuda:0')
c= tensor(1.0289e+09, device='cuda:0')
c= tensor(1.0290e+09, device='cuda:0')
c= tensor(1.0290e+09, device='cuda:0')
c= tensor(1.0290e+09, device='cuda:0')
c= tensor(1.0308e+09, device='cuda:0')
c= tensor(1.0313e+09, device='cuda:0')
c= tensor(1.0364e+09, device='cuda:0')
c= tensor(1.0364e+09, device='cuda:0')
c= tensor(1.0365e+09, device='cuda:0')
c= tensor(1.0365e+09, device='cuda:0')
c= tensor(1.0365e+09, device='cuda:0')
c= tensor(1.0513e+09, device='cuda:0')
c= tensor(1.0513e+09, device='cuda:0')
c= tensor(1.0514e+09, device='cuda:0')
c= tensor(1.0520e+09, device='cuda:0')
c= tensor(1.0526e+09, device='cuda:0')
c= tensor(1.0526e+09, device='cuda:0')
c= tensor(1.0526e+09, device='cuda:0')
c= tensor(1.0727e+09, device='cuda:0')
c= tensor(1.0728e+09, device='cuda:0')
c= tensor(1.0730e+09, device='cuda:0')
c= tensor(1.0732e+09, device='cuda:0')
c= tensor(1.0784e+09, device='cuda:0')
c= tensor(1.0791e+09, device='cuda:0')
c= tensor(1.0869e+09, device='cuda:0')
c= tensor(1.0876e+09, device='cuda:0')
c= tensor(1.0876e+09, device='cuda:0')
c= tensor(1.0879e+09, device='cuda:0')
c= tensor(1.0880e+09, device='cuda:0')
c= tensor(1.0880e+09, device='cuda:0')
c= tensor(1.0881e+09, device='cuda:0')
c= tensor(1.0881e+09, device='cuda:0')
c= tensor(1.0897e+09, device='cuda:0')
c= tensor(1.0897e+09, device='cuda:0')
c= tensor(1.0897e+09, device='cuda:0')
c= tensor(1.0898e+09, device='cuda:0')
c= tensor(1.0898e+09, device='cuda:0')
c= tensor(1.0917e+09, device='cuda:0')
c= tensor(1.0918e+09, device='cuda:0')
c= tensor(1.0918e+09, device='cuda:0')
c= tensor(1.0920e+09, device='cuda:0')
c= tensor(1.0920e+09, device='cuda:0')
c= tensor(1.0921e+09, device='cuda:0')
c= tensor(1.0921e+09, device='cuda:0')
c= tensor(1.0921e+09, device='cuda:0')
c= tensor(1.0949e+09, device='cuda:0')
c= tensor(1.0949e+09, device='cuda:0')
c= tensor(1.0949e+09, device='cuda:0')
c= tensor(1.0949e+09, device='cuda:0')
c= tensor(1.0975e+09, device='cuda:0')
c= tensor(1.1672e+09, device='cuda:0')
c= tensor(1.1674e+09, device='cuda:0')
c= tensor(1.1674e+09, device='cuda:0')
c= tensor(1.1684e+09, device='cuda:0')
c= tensor(1.1692e+09, device='cuda:0')
c= tensor(1.1692e+09, device='cuda:0')
c= tensor(1.1692e+09, device='cuda:0')
c= tensor(1.1693e+09, device='cuda:0')
c= tensor(1.1816e+09, device='cuda:0')
c= tensor(1.1894e+09, device='cuda:0')
c= tensor(1.1927e+09, device='cuda:0')
c= tensor(1.1928e+09, device='cuda:0')
c= tensor(1.1928e+09, device='cuda:0')
c= tensor(1.1928e+09, device='cuda:0')
c= tensor(1.1934e+09, device='cuda:0')
c= tensor(1.1934e+09, device='cuda:0')
c= tensor(1.1937e+09, device='cuda:0')
c= tensor(1.1947e+09, device='cuda:0')
c= tensor(1.2086e+09, device='cuda:0')
c= tensor(1.2086e+09, device='cuda:0')
c= tensor(1.2086e+09, device='cuda:0')
c= tensor(1.2087e+09, device='cuda:0')
c= tensor(1.3473e+09, device='cuda:0')
c= tensor(1.3474e+09, device='cuda:0')
c= tensor(1.3474e+09, device='cuda:0')
c= tensor(1.3474e+09, device='cuda:0')
c= tensor(1.3477e+09, device='cuda:0')
c= tensor(1.3477e+09, device='cuda:0')
c= tensor(1.3484e+09, device='cuda:0')
c= tensor(1.3484e+09, device='cuda:0')
c= tensor(1.3484e+09, device='cuda:0')
c= tensor(1.3485e+09, device='cuda:0')
c= tensor(1.3485e+09, device='cuda:0')
c= tensor(1.3485e+09, device='cuda:0')
c= tensor(1.3524e+09, device='cuda:0')
c= tensor(1.3820e+09, device='cuda:0')
c= tensor(1.3848e+09, device='cuda:0')
c= tensor(1.3897e+09, device='cuda:0')
c= tensor(1.3898e+09, device='cuda:0')
c= tensor(1.3898e+09, device='cuda:0')
c= tensor(1.3898e+09, device='cuda:0')
c= tensor(1.3904e+09, device='cuda:0')
c= tensor(1.3908e+09, device='cuda:0')
c= tensor(1.3921e+09, device='cuda:0')
c= tensor(1.3921e+09, device='cuda:0')
c= tensor(1.4878e+09, device='cuda:0')
c= tensor(1.4879e+09, device='cuda:0')
c= tensor(1.4881e+09, device='cuda:0')
c= tensor(1.4939e+09, device='cuda:0')
c= tensor(1.4942e+09, device='cuda:0')
c= tensor(1.4981e+09, device='cuda:0')
c= tensor(1.5405e+09, device='cuda:0')
c= tensor(1.5413e+09, device='cuda:0')
c= tensor(1.5421e+09, device='cuda:0')
c= tensor(1.5421e+09, device='cuda:0')
c= tensor(1.5424e+09, device='cuda:0')
c= tensor(1.5425e+09, device='cuda:0')
c= tensor(1.5495e+09, device='cuda:0')
c= tensor(1.6145e+09, device='cuda:0')
c= tensor(1.6168e+09, device='cuda:0')
c= tensor(1.6188e+09, device='cuda:0')
c= tensor(1.6200e+09, device='cuda:0')
c= tensor(1.6201e+09, device='cuda:0')
c= tensor(1.6210e+09, device='cuda:0')
c= tensor(1.6210e+09, device='cuda:0')
c= tensor(1.6245e+09, device='cuda:0')
c= tensor(1.6341e+09, device='cuda:0')
c= tensor(1.6346e+09, device='cuda:0')
c= tensor(1.7586e+09, device='cuda:0')
c= tensor(1.7696e+09, device='cuda:0')
c= tensor(1.7699e+09, device='cuda:0')
c= tensor(1.7699e+09, device='cuda:0')
c= tensor(1.7708e+09, device='cuda:0')
c= tensor(1.7720e+09, device='cuda:0')
c= tensor(1.7720e+09, device='cuda:0')
c= tensor(1.7890e+09, device='cuda:0')
c= tensor(1.7894e+09, device='cuda:0')
c= tensor(1.7895e+09, device='cuda:0')
c= tensor(1.7895e+09, device='cuda:0')
c= tensor(1.7895e+09, device='cuda:0')
c= tensor(1.7895e+09, device='cuda:0')
c= tensor(1.7895e+09, device='cuda:0')
c= tensor(1.7896e+09, device='cuda:0')
c= tensor(1.7904e+09, device='cuda:0')
c= tensor(2.6485e+09, device='cuda:0')
c= tensor(2.6487e+09, device='cuda:0')
c= tensor(2.6491e+09, device='cuda:0')
c= tensor(2.6491e+09, device='cuda:0')
c= tensor(2.6491e+09, device='cuda:0')
c= tensor(2.6491e+09, device='cuda:0')
c= tensor(2.6557e+09, device='cuda:0')
c= tensor(2.6566e+09, device='cuda:0')
c= tensor(2.7039e+09, device='cuda:0')
c= tensor(2.7039e+09, device='cuda:0')
c= tensor(2.7073e+09, device='cuda:0')
c= tensor(2.7075e+09, device='cuda:0')
c= tensor(2.7106e+09, device='cuda:0')
c= tensor(2.7251e+09, device='cuda:0')
c= tensor(2.7252e+09, device='cuda:0')
c= tensor(2.7252e+09, device='cuda:0')
c= tensor(2.7263e+09, device='cuda:0')
c= tensor(2.7263e+09, device='cuda:0')
c= tensor(2.7266e+09, device='cuda:0')
c= tensor(2.7278e+09, device='cuda:0')
c= tensor(2.7296e+09, device='cuda:0')
c= tensor(2.7303e+09, device='cuda:0')
c= tensor(2.7304e+09, device='cuda:0')
c= tensor(2.7310e+09, device='cuda:0')
c= tensor(2.7421e+09, device='cuda:0')
c= tensor(2.7421e+09, device='cuda:0')
c= tensor(2.7422e+09, device='cuda:0')
c= tensor(2.7449e+09, device='cuda:0')
c= tensor(2.7452e+09, device='cuda:0')
c= tensor(2.7580e+09, device='cuda:0')
c= tensor(2.7584e+09, device='cuda:0')
c= tensor(2.7588e+09, device='cuda:0')
c= tensor(2.7589e+09, device='cuda:0')
c= tensor(2.7678e+09, device='cuda:0')
c= tensor(2.7699e+09, device='cuda:0')
c= tensor(2.7700e+09, device='cuda:0')
c= tensor(2.7700e+09, device='cuda:0')
c= tensor(2.7701e+09, device='cuda:0')
c= tensor(2.7706e+09, device='cuda:0')
c= tensor(2.7728e+09, device='cuda:0')
c= tensor(2.7746e+09, device='cuda:0')
c= tensor(2.7746e+09, device='cuda:0')
c= tensor(2.7747e+09, device='cuda:0')
c= tensor(2.7749e+09, device='cuda:0')
c= tensor(2.7819e+09, device='cuda:0')
c= tensor(2.7821e+09, device='cuda:0')
c= tensor(2.7826e+09, device='cuda:0')
c= tensor(2.7829e+09, device='cuda:0')
c= tensor(2.7830e+09, device='cuda:0')
c= tensor(2.7830e+09, device='cuda:0')
c= tensor(2.7830e+09, device='cuda:0')
c= tensor(2.7844e+09, device='cuda:0')
c= tensor(2.7849e+09, device='cuda:0')
c= tensor(2.7852e+09, device='cuda:0')
c= tensor(2.7852e+09, device='cuda:0')
c= tensor(2.7852e+09, device='cuda:0')
c= tensor(2.7855e+09, device='cuda:0')
c= tensor(2.7859e+09, device='cuda:0')
c= tensor(2.7860e+09, device='cuda:0')
c= tensor(2.7860e+09, device='cuda:0')
c= tensor(2.7860e+09, device='cuda:0')
c= tensor(2.7860e+09, device='cuda:0')
c= tensor(2.7862e+09, device='cuda:0')
c= tensor(2.7879e+09, device='cuda:0')
c= tensor(2.7879e+09, device='cuda:0')
c= tensor(2.7885e+09, device='cuda:0')
c= tensor(2.7886e+09, device='cuda:0')
c= tensor(2.7886e+09, device='cuda:0')
c= tensor(2.7973e+09, device='cuda:0')
c= tensor(2.7975e+09, device='cuda:0')
c= tensor(2.7989e+09, device='cuda:0')
c= tensor(2.8009e+09, device='cuda:0')
c= tensor(2.8011e+09, device='cuda:0')
c= tensor(2.8037e+09, device='cuda:0')
c= tensor(2.8066e+09, device='cuda:0')
c= tensor(2.8066e+09, device='cuda:0')
c= tensor(2.8072e+09, device='cuda:0')
c= tensor(2.8073e+09, device='cuda:0')
c= tensor(2.8155e+09, device='cuda:0')
c= tensor(2.8179e+09, device='cuda:0')
c= tensor(2.8181e+09, device='cuda:0')
c= tensor(2.8184e+09, device='cuda:0')
c= tensor(2.8184e+09, device='cuda:0')
c= tensor(2.8190e+09, device='cuda:0')
c= tensor(2.8191e+09, device='cuda:0')
c= tensor(2.8191e+09, device='cuda:0')
c= tensor(2.8282e+09, device='cuda:0')
c= tensor(2.8407e+09, device='cuda:0')
c= tensor(2.8407e+09, device='cuda:0')
c= tensor(2.8408e+09, device='cuda:0')
c= tensor(2.8408e+09, device='cuda:0')
c= tensor(2.8437e+09, device='cuda:0')
c= tensor(2.8437e+09, device='cuda:0')
c= tensor(2.8438e+09, device='cuda:0')
c= tensor(2.8442e+09, device='cuda:0')
c= tensor(2.8451e+09, device='cuda:0')
c= tensor(2.8452e+09, device='cuda:0')
c= tensor(2.8454e+09, device='cuda:0')
c= tensor(2.8455e+09, device='cuda:0')
memory (bytes)
4387917824
time for making loss 2 is 12.853367328643799
p0 True
it  0 : 1879653888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 38% |
shape of L is 
torch.Size([])
memory (bytes)
4388184064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 30% |  8% |
memory (bytes)
4388597760
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  33360409000.0
relative error loss 11.724095
shape of L is 
torch.Size([])
memory (bytes)
4591796224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  4% |  9% |
memory (bytes)
4591976448
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  33360270000.0
relative error loss 11.724047
shape of L is 
torch.Size([])
memory (bytes)
4596551680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  9% |
memory (bytes)
4596551680
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  33359630000.0
relative error loss 11.723822
shape of L is 
torch.Size([])
memory (bytes)
4598611968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  9% |
memory (bytes)
4598611968
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  33356500000.0
relative error loss 11.722722
shape of L is 
torch.Size([])
memory (bytes)
4600745984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  9% |
memory (bytes)
4600745984
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  33338868000.0
relative error loss 11.716525
shape of L is 
torch.Size([])
memory (bytes)
4602880000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  9% |
memory (bytes)
4602900480
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  33145516000.0
relative error loss 11.648574
shape of L is 
torch.Size([])
memory (bytes)
4605018112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% |  9% |
memory (bytes)
4605018112
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  32192020000.0
relative error loss 11.31348
shape of L is 
torch.Size([])
memory (bytes)
4607164416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  9% |
memory (bytes)
4607164416
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  23167848000.0
relative error loss 8.142049
shape of L is 
torch.Size([])
memory (bytes)
4609265664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% |  9% |
memory (bytes)
4609265664
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  7593882600.0
relative error loss 2.6687744
shape of L is 
torch.Size([])
memory (bytes)
4611420160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  9% |
memory (bytes)
4611432448
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  4535041000.0
relative error loss 1.593783
time to take a step is 213.8030378818512
it  1 : 2404777984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
4613353472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  9% |
memory (bytes)
4613353472
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  4535041000.0
relative error loss 1.593783
shape of L is 
torch.Size([])
memory (bytes)
4615712768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  9% |
memory (bytes)
4615712768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% | 10% |
error is  8586092000.0
relative error loss 3.017474
shape of L is 
torch.Size([])
memory (bytes)
4617846784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4617846784
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  3694750000.0
relative error loss 1.2984734
shape of L is 
torch.Size([])
memory (bytes)
4619837440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
memory (bytes)
4619837440
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  3066433000.0
relative error loss 1.0776592
shape of L is 
torch.Size([])
memory (bytes)
4622086144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
memory (bytes)
4622102528
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  2838475500.0
relative error loss 0.9975465
shape of L is 
torch.Size([])
memory (bytes)
4624232448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  0% | 10% |
memory (bytes)
4624232448
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  13436940000.0
relative error loss 4.7222433
shape of L is 
torch.Size([])
memory (bytes)
4626161664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 10% |
memory (bytes)
4626370560
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  2783678500.0
relative error loss 0.97828877
shape of L is 
torch.Size([])
memory (bytes)
4628164608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 10% |
memory (bytes)
4628164608
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  2729914000.0
relative error loss 0.9593939
shape of L is 
torch.Size([])
memory (bytes)
4630577152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 10% |
memory (bytes)
4630577152
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  2641360000.0
relative error loss 0.92827266
shape of L is 
torch.Size([])
memory (bytes)
4632621056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 10% |
memory (bytes)
4632621056
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  2479192800.0
relative error loss 0.8712811
time to take a step is 209.5743589401245
it  2 : 2555213824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
4634554368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
memory (bytes)
4634816512
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  2479192800.0
relative error loss 0.8712811
shape of L is 
torch.Size([])
memory (bytes)
4636925952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 10% |
memory (bytes)
4636925952
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  2322539000.0
relative error loss 0.8162271
shape of L is 
torch.Size([])
memory (bytes)
4639014912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 10% |
memory (bytes)
4639014912
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  2172337000.0
relative error loss 0.76344043
shape of L is 
torch.Size([])
memory (bytes)
4640808960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 10% |
memory (bytes)
4640808960
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  2045950000.0
relative error loss 0.71902335
shape of L is 
torch.Size([])
memory (bytes)
4643262464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% | 10% |
memory (bytes)
4643262464
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  1959896600.0
relative error loss 0.68878096
shape of L is 
torch.Size([])
memory (bytes)
4645384192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 10% |
memory (bytes)
4645384192
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  1982917100.0
relative error loss 0.6968713
shape of L is 
torch.Size([])
memory (bytes)
4647407616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 10% |
memory (bytes)
4647407616
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  1859183100.0
relative error loss 0.6533865
shape of L is 
torch.Size([])
memory (bytes)
4649431040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 10% |
memory (bytes)
4649562112
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  1735486100.0
relative error loss 0.6099147
shape of L is 
torch.Size([])
memory (bytes)
4651806720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
memory (bytes)
4651806720
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  1614485800.0
relative error loss 0.5673907
shape of L is 
torch.Size([])
memory (bytes)
4653834240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
memory (bytes)
4653834240
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  1525685600.0
relative error loss 0.536183
time to take a step is 207.53603267669678
it  3 : 2556334080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
4655857664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 10% |
memory (bytes)
4656087040
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  1525685600.0
relative error loss 0.536183
shape of L is 
torch.Size([])
memory (bytes)
4658221056
| ID | GPU | MEM |
------------------
|  0 | 16% |  0% |
|  1 |  8% | 10% |
memory (bytes)
4658221056
| ID | GPU  | MEM |
-------------------
|  0 |  15% |  0% |
|  1 | 100% | 10% |
error is  1436030500.0
relative error loss 0.50467485
shape of L is 
torch.Size([])
memory (bytes)
4660371456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
memory (bytes)
4660371456
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  1324923100.0
relative error loss 0.46562755
shape of L is 
torch.Size([])
memory (bytes)
4662382592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 10% |
memory (bytes)
4662382592
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  1229620200.0
relative error loss 0.43213454
shape of L is 
torch.Size([])
memory (bytes)
4664541184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 10% |
memory (bytes)
4664541184
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  1135193500.0
relative error loss 0.39894944
shape of L is 
torch.Size([])
memory (bytes)
4666810368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 10% |
memory (bytes)
4666818560
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  1031814400.0
relative error loss 0.36261818
shape of L is 
torch.Size([])
memory (bytes)
4668833792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 10% |
memory (bytes)
4668833792
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  955128600.0
relative error loss 0.3356679
shape of L is 
torch.Size([])
memory (bytes)
4670955520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 10% |
memory (bytes)
4670955520
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  883442800.0
relative error loss 0.31047484
shape of L is 
torch.Size([])
memory (bytes)
4673228800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 10% |
memory (bytes)
4673228800
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  814270340.0
relative error loss 0.28616506
shape of L is 
torch.Size([])
memory (bytes)
4675186688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 10% |
memory (bytes)
4675186688
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  760861200.0
relative error loss 0.26739508
time to take a step is 210.48629641532898
c= tensor(604.8281, device='cuda:0')
c= tensor(33237.4688, device='cuda:0')
c= tensor(34745.3594, device='cuda:0')
c= tensor(72863.5938, device='cuda:0')
c= tensor(269477.5625, device='cuda:0')
c= tensor(371673.9375, device='cuda:0')
c= tensor(754488.8750, device='cuda:0')
c= tensor(908841.5000, device='cuda:0')
c= tensor(944828.3750, device='cuda:0')
c= tensor(2505062., device='cuda:0')
c= tensor(2516766.7500, device='cuda:0')
c= tensor(5639022., device='cuda:0')
c= tensor(5657395., device='cuda:0')
c= tensor(14384125., device='cuda:0')
c= tensor(14522952., device='cuda:0')
c= tensor(14613977., device='cuda:0')
c= tensor(15245757., device='cuda:0')
c= tensor(15724501., device='cuda:0')
c= tensor(19278770., device='cuda:0')
c= tensor(20821552., device='cuda:0')
c= tensor(20912360., device='cuda:0')
c= tensor(25469430., device='cuda:0')
c= tensor(25502576., device='cuda:0')
c= tensor(25916432., device='cuda:0')
c= tensor(26062998., device='cuda:0')
c= tensor(26990090., device='cuda:0')
c= tensor(27539634., device='cuda:0')
c= tensor(27556042., device='cuda:0')
c= tensor(29743940., device='cuda:0')
c= tensor(2.1839e+08, device='cuda:0')
c= tensor(2.1840e+08, device='cuda:0')
c= tensor(2.8998e+08, device='cuda:0')
c= tensor(2.9016e+08, device='cuda:0')
c= tensor(2.9017e+08, device='cuda:0')
c= tensor(2.9020e+08, device='cuda:0')
c= tensor(2.9404e+08, device='cuda:0')
c= tensor(2.9532e+08, device='cuda:0')
c= tensor(2.9532e+08, device='cuda:0')
c= tensor(2.9533e+08, device='cuda:0')
c= tensor(2.9533e+08, device='cuda:0')
c= tensor(2.9533e+08, device='cuda:0')
c= tensor(2.9533e+08, device='cuda:0')
c= tensor(2.9533e+08, device='cuda:0')
c= tensor(2.9534e+08, device='cuda:0')
c= tensor(2.9534e+08, device='cuda:0')
c= tensor(2.9534e+08, device='cuda:0')
c= tensor(2.9534e+08, device='cuda:0')
c= tensor(2.9534e+08, device='cuda:0')
c= tensor(2.9535e+08, device='cuda:0')
c= tensor(2.9539e+08, device='cuda:0')
c= tensor(2.9543e+08, device='cuda:0')
c= tensor(2.9543e+08, device='cuda:0')
c= tensor(2.9544e+08, device='cuda:0')
c= tensor(2.9544e+08, device='cuda:0')
c= tensor(2.9545e+08, device='cuda:0')
c= tensor(2.9545e+08, device='cuda:0')
c= tensor(2.9546e+08, device='cuda:0')
c= tensor(2.9546e+08, device='cuda:0')
c= tensor(2.9547e+08, device='cuda:0')
c= tensor(2.9547e+08, device='cuda:0')
c= tensor(2.9548e+08, device='cuda:0')
c= tensor(2.9548e+08, device='cuda:0')
c= tensor(2.9554e+08, device='cuda:0')
c= tensor(2.9554e+08, device='cuda:0')
c= tensor(2.9555e+08, device='cuda:0')
c= tensor(2.9555e+08, device='cuda:0')
c= tensor(2.9555e+08, device='cuda:0')
c= tensor(2.9555e+08, device='cuda:0')
c= tensor(2.9556e+08, device='cuda:0')
c= tensor(2.9556e+08, device='cuda:0')
c= tensor(2.9557e+08, device='cuda:0')
c= tensor(2.9558e+08, device='cuda:0')
c= tensor(2.9558e+08, device='cuda:0')
c= tensor(2.9559e+08, device='cuda:0')
c= tensor(2.9559e+08, device='cuda:0')
c= tensor(2.9560e+08, device='cuda:0')
c= tensor(2.9560e+08, device='cuda:0')
c= tensor(2.9560e+08, device='cuda:0')
c= tensor(2.9560e+08, device='cuda:0')
c= tensor(2.9562e+08, device='cuda:0')
c= tensor(2.9562e+08, device='cuda:0')
c= tensor(2.9562e+08, device='cuda:0')
c= tensor(2.9563e+08, device='cuda:0')
c= tensor(2.9563e+08, device='cuda:0')
c= tensor(2.9563e+08, device='cuda:0')
c= tensor(2.9563e+08, device='cuda:0')
c= tensor(2.9564e+08, device='cuda:0')
c= tensor(2.9564e+08, device='cuda:0')
c= tensor(2.9564e+08, device='cuda:0')
c= tensor(2.9565e+08, device='cuda:0')
c= tensor(2.9565e+08, device='cuda:0')
c= tensor(2.9565e+08, device='cuda:0')
c= tensor(2.9565e+08, device='cuda:0')
c= tensor(2.9566e+08, device='cuda:0')
c= tensor(2.9566e+08, device='cuda:0')
c= tensor(2.9567e+08, device='cuda:0')
c= tensor(2.9567e+08, device='cuda:0')
c= tensor(2.9568e+08, device='cuda:0')
c= tensor(2.9571e+08, device='cuda:0')
c= tensor(2.9571e+08, device='cuda:0')
c= tensor(2.9576e+08, device='cuda:0')
c= tensor(2.9577e+08, device='cuda:0')
c= tensor(2.9577e+08, device='cuda:0')
c= tensor(2.9577e+08, device='cuda:0')
c= tensor(2.9579e+08, device='cuda:0')
c= tensor(2.9579e+08, device='cuda:0')
c= tensor(2.9580e+08, device='cuda:0')
c= tensor(2.9580e+08, device='cuda:0')
c= tensor(2.9580e+08, device='cuda:0')
c= tensor(2.9580e+08, device='cuda:0')
c= tensor(2.9580e+08, device='cuda:0')
c= tensor(2.9580e+08, device='cuda:0')
c= tensor(2.9581e+08, device='cuda:0')
c= tensor(2.9581e+08, device='cuda:0')
c= tensor(2.9582e+08, device='cuda:0')
c= tensor(2.9582e+08, device='cuda:0')
c= tensor(2.9582e+08, device='cuda:0')
c= tensor(2.9582e+08, device='cuda:0')
c= tensor(2.9584e+08, device='cuda:0')
c= tensor(2.9584e+08, device='cuda:0')
c= tensor(2.9586e+08, device='cuda:0')
c= tensor(2.9586e+08, device='cuda:0')
c= tensor(2.9586e+08, device='cuda:0')
c= tensor(2.9586e+08, device='cuda:0')
c= tensor(2.9587e+08, device='cuda:0')
c= tensor(2.9587e+08, device='cuda:0')
c= tensor(2.9587e+08, device='cuda:0')
c= tensor(2.9587e+08, device='cuda:0')
c= tensor(2.9590e+08, device='cuda:0')
c= tensor(2.9590e+08, device='cuda:0')
c= tensor(2.9591e+08, device='cuda:0')
c= tensor(2.9591e+08, device='cuda:0')
c= tensor(2.9591e+08, device='cuda:0')
c= tensor(2.9592e+08, device='cuda:0')
c= tensor(2.9593e+08, device='cuda:0')
c= tensor(2.9593e+08, device='cuda:0')
c= tensor(2.9593e+08, device='cuda:0')
c= tensor(2.9593e+08, device='cuda:0')
c= tensor(2.9593e+08, device='cuda:0')
c= tensor(2.9593e+08, device='cuda:0')
c= tensor(2.9594e+08, device='cuda:0')
c= tensor(2.9594e+08, device='cuda:0')
c= tensor(2.9598e+08, device='cuda:0')
c= tensor(2.9600e+08, device='cuda:0')
c= tensor(2.9601e+08, device='cuda:0')
c= tensor(2.9601e+08, device='cuda:0')
c= tensor(2.9601e+08, device='cuda:0')
c= tensor(2.9602e+08, device='cuda:0')
c= tensor(2.9602e+08, device='cuda:0')
c= tensor(2.9602e+08, device='cuda:0')
c= tensor(2.9602e+08, device='cuda:0')
c= tensor(2.9603e+08, device='cuda:0')
c= tensor(2.9604e+08, device='cuda:0')
c= tensor(2.9605e+08, device='cuda:0')
c= tensor(2.9606e+08, device='cuda:0')
c= tensor(2.9612e+08, device='cuda:0')
c= tensor(2.9612e+08, device='cuda:0')
c= tensor(2.9613e+08, device='cuda:0')
c= tensor(2.9613e+08, device='cuda:0')
c= tensor(2.9613e+08, device='cuda:0')
c= tensor(2.9615e+08, device='cuda:0')
c= tensor(2.9615e+08, device='cuda:0')
c= tensor(2.9616e+08, device='cuda:0')
c= tensor(2.9616e+08, device='cuda:0')
c= tensor(2.9617e+08, device='cuda:0')
c= tensor(2.9617e+08, device='cuda:0')
c= tensor(2.9617e+08, device='cuda:0')
c= tensor(2.9617e+08, device='cuda:0')
c= tensor(2.9618e+08, device='cuda:0')
c= tensor(2.9618e+08, device='cuda:0')
c= tensor(2.9618e+08, device='cuda:0')
c= tensor(2.9618e+08, device='cuda:0')
c= tensor(2.9618e+08, device='cuda:0')
c= tensor(2.9619e+08, device='cuda:0')
c= tensor(2.9619e+08, device='cuda:0')
c= tensor(2.9620e+08, device='cuda:0')
c= tensor(2.9620e+08, device='cuda:0')
c= tensor(2.9621e+08, device='cuda:0')
c= tensor(2.9621e+08, device='cuda:0')
c= tensor(2.9621e+08, device='cuda:0')
c= tensor(2.9622e+08, device='cuda:0')
c= tensor(2.9622e+08, device='cuda:0')
c= tensor(2.9623e+08, device='cuda:0')
c= tensor(2.9623e+08, device='cuda:0')
c= tensor(2.9624e+08, device='cuda:0')
c= tensor(2.9624e+08, device='cuda:0')
c= tensor(2.9624e+08, device='cuda:0')
c= tensor(2.9625e+08, device='cuda:0')
c= tensor(2.9625e+08, device='cuda:0')
c= tensor(2.9629e+08, device='cuda:0')
c= tensor(2.9629e+08, device='cuda:0')
c= tensor(2.9629e+08, device='cuda:0')
c= tensor(2.9629e+08, device='cuda:0')
c= tensor(2.9630e+08, device='cuda:0')
c= tensor(2.9630e+08, device='cuda:0')
c= tensor(2.9630e+08, device='cuda:0')
c= tensor(2.9631e+08, device='cuda:0')
c= tensor(2.9631e+08, device='cuda:0')
c= tensor(2.9631e+08, device='cuda:0')
c= tensor(2.9631e+08, device='cuda:0')
c= tensor(2.9632e+08, device='cuda:0')
c= tensor(2.9632e+08, device='cuda:0')
c= tensor(2.9635e+08, device='cuda:0')
c= tensor(2.9636e+08, device='cuda:0')
c= tensor(2.9637e+08, device='cuda:0')
c= tensor(2.9637e+08, device='cuda:0')
c= tensor(2.9637e+08, device='cuda:0')
c= tensor(2.9638e+08, device='cuda:0')
c= tensor(2.9638e+08, device='cuda:0')
c= tensor(2.9641e+08, device='cuda:0')
c= tensor(2.9644e+08, device='cuda:0')
c= tensor(2.9644e+08, device='cuda:0')
c= tensor(2.9644e+08, device='cuda:0')
c= tensor(2.9644e+08, device='cuda:0')
c= tensor(2.9644e+08, device='cuda:0')
c= tensor(2.9644e+08, device='cuda:0')
c= tensor(2.9644e+08, device='cuda:0')
c= tensor(2.9645e+08, device='cuda:0')
c= tensor(2.9647e+08, device='cuda:0')
c= tensor(2.9647e+08, device='cuda:0')
c= tensor(2.9647e+08, device='cuda:0')
c= tensor(2.9647e+08, device='cuda:0')
c= tensor(2.9648e+08, device='cuda:0')
c= tensor(2.9648e+08, device='cuda:0')
c= tensor(2.9649e+08, device='cuda:0')
c= tensor(2.9650e+08, device='cuda:0')
c= tensor(2.9650e+08, device='cuda:0')
c= tensor(2.9650e+08, device='cuda:0')
c= tensor(2.9652e+08, device='cuda:0')
c= tensor(2.9652e+08, device='cuda:0')
c= tensor(2.9652e+08, device='cuda:0')
c= tensor(2.9652e+08, device='cuda:0')
c= tensor(2.9653e+08, device='cuda:0')
c= tensor(2.9653e+08, device='cuda:0')
c= tensor(2.9654e+08, device='cuda:0')
c= tensor(2.9654e+08, device='cuda:0')
c= tensor(2.9654e+08, device='cuda:0')
c= tensor(2.9655e+08, device='cuda:0')
c= tensor(2.9655e+08, device='cuda:0')
c= tensor(2.9661e+08, device='cuda:0')
c= tensor(2.9770e+08, device='cuda:0')
c= tensor(2.9770e+08, device='cuda:0')
c= tensor(2.9771e+08, device='cuda:0')
c= tensor(2.9771e+08, device='cuda:0')
c= tensor(2.9772e+08, device='cuda:0')
c= tensor(2.9775e+08, device='cuda:0')
c= tensor(2.9966e+08, device='cuda:0')
c= tensor(2.9967e+08, device='cuda:0')
c= tensor(3.0140e+08, device='cuda:0')
c= tensor(3.0185e+08, device='cuda:0')
c= tensor(3.0192e+08, device='cuda:0')
c= tensor(3.0566e+08, device='cuda:0')
c= tensor(3.0566e+08, device='cuda:0')
c= tensor(3.0568e+08, device='cuda:0')
c= tensor(3.0871e+08, device='cuda:0')
c= tensor(3.3402e+08, device='cuda:0')
c= tensor(3.3403e+08, device='cuda:0')
c= tensor(3.3424e+08, device='cuda:0')
c= tensor(3.3439e+08, device='cuda:0')
c= tensor(3.3514e+08, device='cuda:0')
c= tensor(3.3572e+08, device='cuda:0')
c= tensor(3.3641e+08, device='cuda:0')
c= tensor(3.3654e+08, device='cuda:0')
c= tensor(3.3663e+08, device='cuda:0')
c= tensor(3.3669e+08, device='cuda:0')
c= tensor(3.4369e+08, device='cuda:0')
c= tensor(3.4370e+08, device='cuda:0')
c= tensor(3.4370e+08, device='cuda:0')
c= tensor(3.4378e+08, device='cuda:0')
c= tensor(3.4450e+08, device='cuda:0')
c= tensor(3.6727e+08, device='cuda:0')
c= tensor(3.6815e+08, device='cuda:0')
c= tensor(3.6815e+08, device='cuda:0')
c= tensor(3.6819e+08, device='cuda:0')
c= tensor(3.6821e+08, device='cuda:0')
c= tensor(3.6839e+08, device='cuda:0')
c= tensor(3.6964e+08, device='cuda:0')
c= tensor(3.7068e+08, device='cuda:0')
c= tensor(3.7111e+08, device='cuda:0')
c= tensor(3.7111e+08, device='cuda:0')
c= tensor(3.7112e+08, device='cuda:0')
c= tensor(3.7197e+08, device='cuda:0')
c= tensor(3.7251e+08, device='cuda:0')
c= tensor(3.7283e+08, device='cuda:0')
c= tensor(3.7284e+08, device='cuda:0')
c= tensor(3.9042e+08, device='cuda:0')
c= tensor(3.9043e+08, device='cuda:0')
c= tensor(3.9057e+08, device='cuda:0')
c= tensor(3.9212e+08, device='cuda:0')
c= tensor(3.9213e+08, device='cuda:0')
c= tensor(3.9269e+08, device='cuda:0')
c= tensor(4.0091e+08, device='cuda:0')
c= tensor(4.1495e+08, device='cuda:0')
c= tensor(4.1505e+08, device='cuda:0')
c= tensor(4.1513e+08, device='cuda:0')
c= tensor(4.1514e+08, device='cuda:0')
c= tensor(4.1514e+08, device='cuda:0')
c= tensor(4.1649e+08, device='cuda:0')
c= tensor(4.1651e+08, device='cuda:0')
c= tensor(4.1676e+08, device='cuda:0')
c= tensor(4.1892e+08, device='cuda:0')
c= tensor(4.1915e+08, device='cuda:0')
c= tensor(4.1934e+08, device='cuda:0')
c= tensor(4.1934e+08, device='cuda:0')
c= tensor(4.2355e+08, device='cuda:0')
c= tensor(4.2511e+08, device='cuda:0')
c= tensor(4.2537e+08, device='cuda:0')
c= tensor(4.2542e+08, device='cuda:0')
c= tensor(4.3598e+08, device='cuda:0')
c= tensor(4.3601e+08, device='cuda:0')
c= tensor(4.4244e+08, device='cuda:0')
c= tensor(4.4245e+08, device='cuda:0')
c= tensor(4.4336e+08, device='cuda:0')
c= tensor(4.4340e+08, device='cuda:0')
c= tensor(4.4511e+08, device='cuda:0')
c= tensor(4.4564e+08, device='cuda:0')
c= tensor(4.4565e+08, device='cuda:0')
c= tensor(4.4733e+08, device='cuda:0')
c= tensor(4.4945e+08, device='cuda:0')
c= tensor(4.4945e+08, device='cuda:0')
c= tensor(4.5115e+08, device='cuda:0')
c= tensor(4.5661e+08, device='cuda:0')
c= tensor(4.7319e+08, device='cuda:0')
c= tensor(4.7395e+08, device='cuda:0')
c= tensor(4.7395e+08, device='cuda:0')
c= tensor(4.7396e+08, device='cuda:0')
c= tensor(4.7409e+08, device='cuda:0')
c= tensor(4.7419e+08, device='cuda:0')
c= tensor(4.7441e+08, device='cuda:0')
c= tensor(4.7441e+08, device='cuda:0')
c= tensor(4.7494e+08, device='cuda:0')
c= tensor(4.7607e+08, device='cuda:0')
c= tensor(4.7623e+08, device='cuda:0')
c= tensor(4.7624e+08, device='cuda:0')
c= tensor(4.7637e+08, device='cuda:0')
c= tensor(4.7641e+08, device='cuda:0')
c= tensor(4.7645e+08, device='cuda:0')
c= tensor(4.7649e+08, device='cuda:0')
c= tensor(4.7649e+08, device='cuda:0')
c= tensor(4.8125e+08, device='cuda:0')
c= tensor(4.8141e+08, device='cuda:0')
c= tensor(4.8148e+08, device='cuda:0')
c= tensor(4.8167e+08, device='cuda:0')
c= tensor(4.8169e+08, device='cuda:0')
c= tensor(6.2061e+08, device='cuda:0')
c= tensor(6.2062e+08, device='cuda:0')
c= tensor(6.2334e+08, device='cuda:0')
c= tensor(6.2334e+08, device='cuda:0')
c= tensor(6.2335e+08, device='cuda:0')
c= tensor(6.2335e+08, device='cuda:0')
c= tensor(6.2337e+08, device='cuda:0')
c= tensor(6.2339e+08, device='cuda:0')
c= tensor(6.2396e+08, device='cuda:0')
c= tensor(6.2397e+08, device='cuda:0')
c= tensor(6.2398e+08, device='cuda:0')
c= tensor(6.3567e+08, device='cuda:0')
c= tensor(6.3591e+08, device='cuda:0')
c= tensor(6.3604e+08, device='cuda:0')
c= tensor(6.3757e+08, device='cuda:0')
c= tensor(6.4008e+08, device='cuda:0')
c= tensor(6.4009e+08, device='cuda:0')
c= tensor(6.4010e+08, device='cuda:0')
c= tensor(6.4016e+08, device='cuda:0')
c= tensor(6.4016e+08, device='cuda:0')
c= tensor(6.4016e+08, device='cuda:0')
c= tensor(6.4022e+08, device='cuda:0')
c= tensor(6.4022e+08, device='cuda:0')
c= tensor(6.4023e+08, device='cuda:0')
c= tensor(6.4023e+08, device='cuda:0')
c= tensor(6.4024e+08, device='cuda:0')
c= tensor(6.5972e+08, device='cuda:0')
c= tensor(6.5974e+08, device='cuda:0')
c= tensor(6.6096e+08, device='cuda:0')
c= tensor(6.6100e+08, device='cuda:0')
c= tensor(6.6101e+08, device='cuda:0')
c= tensor(6.6111e+08, device='cuda:0')
c= tensor(7.0273e+08, device='cuda:0')
c= tensor(7.1933e+08, device='cuda:0')
c= tensor(7.1939e+08, device='cuda:0')
c= tensor(7.1953e+08, device='cuda:0')
c= tensor(7.1953e+08, device='cuda:0')
c= tensor(7.2015e+08, device='cuda:0')
c= tensor(7.4673e+08, device='cuda:0')
c= tensor(7.4696e+08, device='cuda:0')
c= tensor(7.4699e+08, device='cuda:0')
c= tensor(7.4825e+08, device='cuda:0')
c= tensor(7.7621e+08, device='cuda:0')
c= tensor(7.7645e+08, device='cuda:0')
c= tensor(7.7647e+08, device='cuda:0')
c= tensor(7.7656e+08, device='cuda:0')
c= tensor(7.7665e+08, device='cuda:0')
c= tensor(7.7665e+08, device='cuda:0')
c= tensor(7.8188e+08, device='cuda:0')
c= tensor(7.8194e+08, device='cuda:0')
c= tensor(7.8194e+08, device='cuda:0')
c= tensor(7.8201e+08, device='cuda:0')
c= tensor(7.8208e+08, device='cuda:0')
c= tensor(7.8208e+08, device='cuda:0')
c= tensor(7.8381e+08, device='cuda:0')
c= tensor(7.8548e+08, device='cuda:0')
c= tensor(7.9032e+08, device='cuda:0')
c= tensor(7.9548e+08, device='cuda:0')
c= tensor(7.9802e+08, device='cuda:0')
c= tensor(7.9806e+08, device='cuda:0')
c= tensor(7.9810e+08, device='cuda:0')
c= tensor(7.9836e+08, device='cuda:0')
c= tensor(8.0063e+08, device='cuda:0')
c= tensor(8.0063e+08, device='cuda:0')
c= tensor(8.0384e+08, device='cuda:0')
c= tensor(8.3598e+08, device='cuda:0')
c= tensor(8.3731e+08, device='cuda:0')
c= tensor(8.3765e+08, device='cuda:0')
c= tensor(8.4126e+08, device='cuda:0')
c= tensor(8.4128e+08, device='cuda:0')
c= tensor(8.4129e+08, device='cuda:0')
c= tensor(8.4144e+08, device='cuda:0')
c= tensor(8.4274e+08, device='cuda:0')
c= tensor(8.4333e+08, device='cuda:0')
c= tensor(8.9730e+08, device='cuda:0')
c= tensor(8.9895e+08, device='cuda:0')
c= tensor(9.0063e+08, device='cuda:0')
c= tensor(9.0084e+08, device='cuda:0')
c= tensor(9.0391e+08, device='cuda:0')
c= tensor(9.0391e+08, device='cuda:0')
c= tensor(9.0393e+08, device='cuda:0')
c= tensor(9.1098e+08, device='cuda:0')
c= tensor(9.1108e+08, device='cuda:0')
c= tensor(9.1109e+08, device='cuda:0')
c= tensor(9.1119e+08, device='cuda:0')
c= tensor(9.2360e+08, device='cuda:0')
c= tensor(9.2396e+08, device='cuda:0')
c= tensor(9.2431e+08, device='cuda:0')
c= tensor(9.2432e+08, device='cuda:0')
c= tensor(9.2436e+08, device='cuda:0')
c= tensor(9.2436e+08, device='cuda:0')
c= tensor(9.2441e+08, device='cuda:0')
c= tensor(9.2449e+08, device='cuda:0')
c= tensor(9.2522e+08, device='cuda:0')
c= tensor(9.2524e+08, device='cuda:0')
c= tensor(9.2782e+08, device='cuda:0')
c= tensor(9.2782e+08, device='cuda:0')
c= tensor(9.2803e+08, device='cuda:0')
c= tensor(9.2808e+08, device='cuda:0')
c= tensor(9.2815e+08, device='cuda:0')
c= tensor(9.2815e+08, device='cuda:0')
c= tensor(9.2828e+08, device='cuda:0')
c= tensor(9.2836e+08, device='cuda:0')
c= tensor(9.2847e+08, device='cuda:0')
c= tensor(9.2930e+08, device='cuda:0')
c= tensor(9.5335e+08, device='cuda:0')
c= tensor(9.5335e+08, device='cuda:0')
c= tensor(9.5338e+08, device='cuda:0')
c= tensor(9.5430e+08, device='cuda:0')
c= tensor(9.5432e+08, device='cuda:0')
c= tensor(9.7726e+08, device='cuda:0')
c= tensor(9.7726e+08, device='cuda:0')
c= tensor(9.7960e+08, device='cuda:0')
c= tensor(9.8570e+08, device='cuda:0')
c= tensor(9.8571e+08, device='cuda:0')
c= tensor(9.9347e+08, device='cuda:0')
c= tensor(9.9382e+08, device='cuda:0')
c= tensor(1.0276e+09, device='cuda:0')
c= tensor(1.0276e+09, device='cuda:0')
c= tensor(1.0277e+09, device='cuda:0')
c= tensor(1.0278e+09, device='cuda:0')
c= tensor(1.0278e+09, device='cuda:0')
c= tensor(1.0278e+09, device='cuda:0')
c= tensor(1.0281e+09, device='cuda:0')
c= tensor(1.0281e+09, device='cuda:0')
c= tensor(1.0289e+09, device='cuda:0')
c= tensor(1.0290e+09, device='cuda:0')
c= tensor(1.0290e+09, device='cuda:0')
c= tensor(1.0290e+09, device='cuda:0')
c= tensor(1.0308e+09, device='cuda:0')
c= tensor(1.0313e+09, device='cuda:0')
c= tensor(1.0364e+09, device='cuda:0')
c= tensor(1.0364e+09, device='cuda:0')
c= tensor(1.0365e+09, device='cuda:0')
c= tensor(1.0365e+09, device='cuda:0')
c= tensor(1.0365e+09, device='cuda:0')
c= tensor(1.0513e+09, device='cuda:0')
c= tensor(1.0513e+09, device='cuda:0')
c= tensor(1.0514e+09, device='cuda:0')
c= tensor(1.0520e+09, device='cuda:0')
c= tensor(1.0526e+09, device='cuda:0')
c= tensor(1.0526e+09, device='cuda:0')
c= tensor(1.0526e+09, device='cuda:0')
c= tensor(1.0727e+09, device='cuda:0')
c= tensor(1.0728e+09, device='cuda:0')
c= tensor(1.0730e+09, device='cuda:0')
c= tensor(1.0732e+09, device='cuda:0')
c= tensor(1.0784e+09, device='cuda:0')
c= tensor(1.0791e+09, device='cuda:0')
c= tensor(1.0869e+09, device='cuda:0')
c= tensor(1.0876e+09, device='cuda:0')
c= tensor(1.0876e+09, device='cuda:0')
c= tensor(1.0879e+09, device='cuda:0')
c= tensor(1.0880e+09, device='cuda:0')
c= tensor(1.0880e+09, device='cuda:0')
c= tensor(1.0881e+09, device='cuda:0')
c= tensor(1.0881e+09, device='cuda:0')
c= tensor(1.0897e+09, device='cuda:0')
c= tensor(1.0897e+09, device='cuda:0')
c= tensor(1.0897e+09, device='cuda:0')
c= tensor(1.0898e+09, device='cuda:0')
c= tensor(1.0898e+09, device='cuda:0')
c= tensor(1.0917e+09, device='cuda:0')
c= tensor(1.0918e+09, device='cuda:0')
c= tensor(1.0918e+09, device='cuda:0')
c= tensor(1.0920e+09, device='cuda:0')
c= tensor(1.0920e+09, device='cuda:0')
c= tensor(1.0921e+09, device='cuda:0')
c= tensor(1.0921e+09, device='cuda:0')
c= tensor(1.0921e+09, device='cuda:0')
c= tensor(1.0949e+09, device='cuda:0')
c= tensor(1.0949e+09, device='cuda:0')
c= tensor(1.0949e+09, device='cuda:0')
c= tensor(1.0949e+09, device='cuda:0')
c= tensor(1.0975e+09, device='cuda:0')
c= tensor(1.1672e+09, device='cuda:0')
c= tensor(1.1674e+09, device='cuda:0')
c= tensor(1.1674e+09, device='cuda:0')
c= tensor(1.1684e+09, device='cuda:0')
c= tensor(1.1692e+09, device='cuda:0')
c= tensor(1.1692e+09, device='cuda:0')
c= tensor(1.1692e+09, device='cuda:0')
c= tensor(1.1693e+09, device='cuda:0')
c= tensor(1.1816e+09, device='cuda:0')
c= tensor(1.1894e+09, device='cuda:0')
c= tensor(1.1927e+09, device='cuda:0')
c= tensor(1.1928e+09, device='cuda:0')
c= tensor(1.1928e+09, device='cuda:0')
c= tensor(1.1928e+09, device='cuda:0')
c= tensor(1.1934e+09, device='cuda:0')
c= tensor(1.1934e+09, device='cuda:0')
c= tensor(1.1937e+09, device='cuda:0')
c= tensor(1.1947e+09, device='cuda:0')
c= tensor(1.2086e+09, device='cuda:0')
c= tensor(1.2086e+09, device='cuda:0')
c= tensor(1.2086e+09, device='cuda:0')
c= tensor(1.2087e+09, device='cuda:0')
c= tensor(1.3473e+09, device='cuda:0')
c= tensor(1.3474e+09, device='cuda:0')
c= tensor(1.3474e+09, device='cuda:0')
c= tensor(1.3474e+09, device='cuda:0')
c= tensor(1.3477e+09, device='cuda:0')
c= tensor(1.3477e+09, device='cuda:0')
c= tensor(1.3484e+09, device='cuda:0')
c= tensor(1.3484e+09, device='cuda:0')
c= tensor(1.3484e+09, device='cuda:0')
c= tensor(1.3485e+09, device='cuda:0')
c= tensor(1.3485e+09, device='cuda:0')
c= tensor(1.3485e+09, device='cuda:0')
c= tensor(1.3524e+09, device='cuda:0')
c= tensor(1.3820e+09, device='cuda:0')
c= tensor(1.3848e+09, device='cuda:0')
c= tensor(1.3897e+09, device='cuda:0')
c= tensor(1.3898e+09, device='cuda:0')
c= tensor(1.3898e+09, device='cuda:0')
c= tensor(1.3898e+09, device='cuda:0')
c= tensor(1.3904e+09, device='cuda:0')
c= tensor(1.3908e+09, device='cuda:0')
c= tensor(1.3921e+09, device='cuda:0')
c= tensor(1.3921e+09, device='cuda:0')
c= tensor(1.4878e+09, device='cuda:0')
c= tensor(1.4879e+09, device='cuda:0')
c= tensor(1.4881e+09, device='cuda:0')
c= tensor(1.4939e+09, device='cuda:0')
c= tensor(1.4942e+09, device='cuda:0')
c= tensor(1.4981e+09, device='cuda:0')
c= tensor(1.5405e+09, device='cuda:0')
c= tensor(1.5413e+09, device='cuda:0')
c= tensor(1.5421e+09, device='cuda:0')
c= tensor(1.5421e+09, device='cuda:0')
c= tensor(1.5424e+09, device='cuda:0')
c= tensor(1.5425e+09, device='cuda:0')
c= tensor(1.5495e+09, device='cuda:0')
c= tensor(1.6145e+09, device='cuda:0')
c= tensor(1.6168e+09, device='cuda:0')
c= tensor(1.6188e+09, device='cuda:0')
c= tensor(1.6200e+09, device='cuda:0')
c= tensor(1.6201e+09, device='cuda:0')
c= tensor(1.6210e+09, device='cuda:0')
c= tensor(1.6210e+09, device='cuda:0')
c= tensor(1.6245e+09, device='cuda:0')
c= tensor(1.6341e+09, device='cuda:0')
c= tensor(1.6346e+09, device='cuda:0')
c= tensor(1.7586e+09, device='cuda:0')
c= tensor(1.7696e+09, device='cuda:0')
c= tensor(1.7699e+09, device='cuda:0')
c= tensor(1.7699e+09, device='cuda:0')
c= tensor(1.7708e+09, device='cuda:0')
c= tensor(1.7720e+09, device='cuda:0')
c= tensor(1.7720e+09, device='cuda:0')
c= tensor(1.7890e+09, device='cuda:0')
c= tensor(1.7894e+09, device='cuda:0')
c= tensor(1.7895e+09, device='cuda:0')
c= tensor(1.7895e+09, device='cuda:0')
c= tensor(1.7895e+09, device='cuda:0')
c= tensor(1.7895e+09, device='cuda:0')
c= tensor(1.7895e+09, device='cuda:0')
c= tensor(1.7896e+09, device='cuda:0')
c= tensor(1.7904e+09, device='cuda:0')
c= tensor(2.6485e+09, device='cuda:0')
c= tensor(2.6487e+09, device='cuda:0')
c= tensor(2.6491e+09, device='cuda:0')
c= tensor(2.6491e+09, device='cuda:0')
c= tensor(2.6491e+09, device='cuda:0')
c= tensor(2.6491e+09, device='cuda:0')
c= tensor(2.6557e+09, device='cuda:0')
c= tensor(2.6566e+09, device='cuda:0')
c= tensor(2.7039e+09, device='cuda:0')
c= tensor(2.7039e+09, device='cuda:0')
c= tensor(2.7073e+09, device='cuda:0')
c= tensor(2.7075e+09, device='cuda:0')
c= tensor(2.7106e+09, device='cuda:0')
c= tensor(2.7251e+09, device='cuda:0')
c= tensor(2.7252e+09, device='cuda:0')
c= tensor(2.7252e+09, device='cuda:0')
c= tensor(2.7263e+09, device='cuda:0')
c= tensor(2.7263e+09, device='cuda:0')
c= tensor(2.7266e+09, device='cuda:0')
c= tensor(2.7278e+09, device='cuda:0')
c= tensor(2.7296e+09, device='cuda:0')
c= tensor(2.7303e+09, device='cuda:0')
c= tensor(2.7304e+09, device='cuda:0')
c= tensor(2.7310e+09, device='cuda:0')
c= tensor(2.7421e+09, device='cuda:0')
c= tensor(2.7421e+09, device='cuda:0')
c= tensor(2.7422e+09, device='cuda:0')
c= tensor(2.7449e+09, device='cuda:0')
c= tensor(2.7452e+09, device='cuda:0')
c= tensor(2.7580e+09, device='cuda:0')
c= tensor(2.7584e+09, device='cuda:0')
c= tensor(2.7588e+09, device='cuda:0')
c= tensor(2.7589e+09, device='cuda:0')
c= tensor(2.7678e+09, device='cuda:0')
c= tensor(2.7699e+09, device='cuda:0')
c= tensor(2.7700e+09, device='cuda:0')
c= tensor(2.7700e+09, device='cuda:0')
c= tensor(2.7701e+09, device='cuda:0')
c= tensor(2.7706e+09, device='cuda:0')
c= tensor(2.7728e+09, device='cuda:0')
c= tensor(2.7746e+09, device='cuda:0')
c= tensor(2.7746e+09, device='cuda:0')
c= tensor(2.7747e+09, device='cuda:0')
c= tensor(2.7749e+09, device='cuda:0')
c= tensor(2.7819e+09, device='cuda:0')
c= tensor(2.7821e+09, device='cuda:0')
c= tensor(2.7826e+09, device='cuda:0')
c= tensor(2.7829e+09, device='cuda:0')
c= tensor(2.7830e+09, device='cuda:0')
c= tensor(2.7830e+09, device='cuda:0')
c= tensor(2.7830e+09, device='cuda:0')
c= tensor(2.7844e+09, device='cuda:0')
c= tensor(2.7849e+09, device='cuda:0')
c= tensor(2.7852e+09, device='cuda:0')
c= tensor(2.7852e+09, device='cuda:0')
c= tensor(2.7852e+09, device='cuda:0')
c= tensor(2.7855e+09, device='cuda:0')
c= tensor(2.7859e+09, device='cuda:0')
c= tensor(2.7860e+09, device='cuda:0')
c= tensor(2.7860e+09, device='cuda:0')
c= tensor(2.7860e+09, device='cuda:0')
c= tensor(2.7860e+09, device='cuda:0')
c= tensor(2.7862e+09, device='cuda:0')
c= tensor(2.7879e+09, device='cuda:0')
c= tensor(2.7879e+09, device='cuda:0')
c= tensor(2.7885e+09, device='cuda:0')
c= tensor(2.7886e+09, device='cuda:0')
c= tensor(2.7886e+09, device='cuda:0')
c= tensor(2.7973e+09, device='cuda:0')
c= tensor(2.7975e+09, device='cuda:0')
c= tensor(2.7989e+09, device='cuda:0')
c= tensor(2.8009e+09, device='cuda:0')
c= tensor(2.8011e+09, device='cuda:0')
c= tensor(2.8037e+09, device='cuda:0')
c= tensor(2.8066e+09, device='cuda:0')
c= tensor(2.8066e+09, device='cuda:0')
c= tensor(2.8072e+09, device='cuda:0')
c= tensor(2.8073e+09, device='cuda:0')
c= tensor(2.8155e+09, device='cuda:0')
c= tensor(2.8179e+09, device='cuda:0')
c= tensor(2.8181e+09, device='cuda:0')
c= tensor(2.8184e+09, device='cuda:0')
c= tensor(2.8184e+09, device='cuda:0')
c= tensor(2.8190e+09, device='cuda:0')
c= tensor(2.8191e+09, device='cuda:0')
c= tensor(2.8191e+09, device='cuda:0')
c= tensor(2.8282e+09, device='cuda:0')
c= tensor(2.8407e+09, device='cuda:0')
c= tensor(2.8407e+09, device='cuda:0')
c= tensor(2.8408e+09, device='cuda:0')
c= tensor(2.8408e+09, device='cuda:0')
c= tensor(2.8437e+09, device='cuda:0')
c= tensor(2.8437e+09, device='cuda:0')
c= tensor(2.8438e+09, device='cuda:0')
c= tensor(2.8442e+09, device='cuda:0')
c= tensor(2.8451e+09, device='cuda:0')
c= tensor(2.8452e+09, device='cuda:0')
c= tensor(2.8454e+09, device='cuda:0')
c= tensor(2.8455e+09, device='cuda:0')
time to make c is 9.919011354446411
time for making loss is 9.919055223464966
p0 True
it  0 : 1879974400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  9% |
shape of L is 
torch.Size([])
memory (bytes)
4677345280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% |  9% |
memory (bytes)
4677791744
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  760861200.0
relative error loss 0.26739508
shape of L is 
torch.Size([])
memory (bytes)
4704260096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  9% |
memory (bytes)
4704415744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% |  9% |
error is  755958400.0
relative error loss 0.26567206
shape of L is 
torch.Size([])
memory (bytes)
4707934208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% |  9% |
memory (bytes)
4707934208
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  741199000.0
relative error loss 0.26048505
shape of L is 
torch.Size([])
memory (bytes)
4711174144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% |  9% |
memory (bytes)
4711174144
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  730933760.0
relative error loss 0.25687745
shape of L is 
torch.Size([])
memory (bytes)
4714164224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 10% |
memory (bytes)
4714393600
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  725288600.0
relative error loss 0.25489354
shape of L is 
torch.Size([])
memory (bytes)
4717592576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 10% |
memory (bytes)
4717592576
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  715990400.0
relative error loss 0.2516258
shape of L is 
torch.Size([])
memory (bytes)
4720775168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 10% |
memory (bytes)
4720775168
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  711841800.0
relative error loss 0.25016785
shape of L is 
torch.Size([])
memory (bytes)
4723851264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 10% |
memory (bytes)
4724023296
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  707662600.0
relative error loss 0.24869911
shape of L is 
torch.Size([])
memory (bytes)
4727230464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 10% |
memory (bytes)
4727230464
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  702663940.0
relative error loss 0.24694239
shape of L is 
torch.Size([])
memory (bytes)
4730294272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
4730294272
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  699480060.0
relative error loss 0.24582346
time to take a step is 271.36894631385803
it  1 : 2558014464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
4733661184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
4733661184
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  699480060.0
relative error loss 0.24582346
shape of L is 
torch.Size([])
memory (bytes)
4736851968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
4736851968
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  697249800.0
relative error loss 0.24503966
shape of L is 
torch.Size([])
memory (bytes)
4740104192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 10% |
memory (bytes)
4740108288
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  694591500.0
relative error loss 0.24410543
shape of L is 
torch.Size([])
memory (bytes)
4743315456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 10% |
memory (bytes)
4743315456
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  692742140.0
relative error loss 0.2434555
shape of L is 
torch.Size([])
memory (bytes)
4746309632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
4746534912
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  691041540.0
relative error loss 0.24285784
shape of L is 
torch.Size([])
memory (bytes)
4749750272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4749750272
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  689036000.0
relative error loss 0.24215303
shape of L is 
torch.Size([])
memory (bytes)
4752818176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 10% |
memory (bytes)
4752818176
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  688114400.0
relative error loss 0.24182916
shape of L is 
torch.Size([])
memory (bytes)
4756164608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 10% |
memory (bytes)
4756164608
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  686612500.0
relative error loss 0.24130131
shape of L is 
torch.Size([])
memory (bytes)
4759339008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 10% |
memory (bytes)
4759339008
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  685406200.0
relative error loss 0.24087738
shape of L is 
torch.Size([])
memory (bytes)
4762550272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
4762550272
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  683791360.0
relative error loss 0.24030986
time to take a step is 266.1714017391205
it  2 : 2558014464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
4765802496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
4765802496
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  683791360.0
relative error loss 0.24030986
shape of L is 
torch.Size([])
memory (bytes)
4768935936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 10% |
memory (bytes)
4768935936
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  682826750.0
relative error loss 0.23997086
shape of L is 
torch.Size([])
memory (bytes)
4772220928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 10% |
memory (bytes)
4772220928
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  681692900.0
relative error loss 0.23957239
shape of L is 
torch.Size([])
memory (bytes)
4775333888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
4775333888
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  680614660.0
relative error loss 0.23919345
shape of L is 
torch.Size([])
memory (bytes)
4778639360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 10% |
memory (bytes)
4778639360
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  679527200.0
relative error loss 0.23881127
shape of L is 
torch.Size([])
memory (bytes)
4781826048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 32% | 10% |
memory (bytes)
4781826048
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  678720500.0
relative error loss 0.23852777
shape of L is 
torch.Size([])
memory (bytes)
4784926720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 10% |
memory (bytes)
4785065984
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  677908000.0
relative error loss 0.23824222
shape of L is 
torch.Size([])
memory (bytes)
4788281344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
4788281344
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  677320960.0
relative error loss 0.23803592
shape of L is 
torch.Size([])
memory (bytes)
4791328768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
4791508992
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  676491000.0
relative error loss 0.23774424
shape of L is 
torch.Size([])
memory (bytes)
4794720256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 30% | 10% |
memory (bytes)
4794720256
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  676107000.0
relative error loss 0.2376093
time to take a step is 267.1651840209961
it  3 : 2558014464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
4797911040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
4797911040
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  676107000.0
relative error loss 0.2376093
shape of L is 
torch.Size([])
memory (bytes)
4801146880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 30% | 10% |
memory (bytes)
4801146880
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  675555100.0
relative error loss 0.23741533
shape of L is 
torch.Size([])
memory (bytes)
4804345856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 10% |
memory (bytes)
4804345856
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  675386900.0
relative error loss 0.23735622
shape of L is 
torch.Size([])
memory (bytes)
4807544832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 10% |
memory (bytes)
4807544832
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  674735100.0
relative error loss 0.23712716
shape of L is 
torch.Size([])
memory (bytes)
4810797056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 10% |
memory (bytes)
4810797056
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  674440960.0
relative error loss 0.23702379
shape of L is 
torch.Size([])
memory (bytes)
4813979648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 30% | 10% |
memory (bytes)
4813979648
| ID | GPU  | MEM |
-------------------
|  0 |   1% |  0% |
|  1 | 100% | 10% |
error is  674197500.0
relative error loss 0.23693822
shape of L is 
torch.Size([])
memory (bytes)
4817215488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
4817215488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% | 10% |
error is  673647600.0
relative error loss 0.23674497
shape of L is 
torch.Size([])
memory (bytes)
4820328448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4820328448
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  673295900.0
relative error loss 0.23662135
shape of L is 
torch.Size([])
memory (bytes)
4823638016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
4823638016
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  672651800.0
relative error loss 0.236395
shape of L is 
torch.Size([])
memory (bytes)
4826726400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 10% |
memory (bytes)
4826726400
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  672383500.0
relative error loss 0.2363007
time to take a step is 266.71836161613464
it  4 : 2558014464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
4829970432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4829970432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% | 10% |
error is  672383500.0
relative error loss 0.2363007
shape of L is 
torch.Size([])
memory (bytes)
4833275904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 10% |
memory (bytes)
4833275904
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  671915500.0
relative error loss 0.23613624
shape of L is 
torch.Size([])
memory (bytes)
4836478976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
4836491264
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  671779300.0
relative error loss 0.23608838
shape of L is 
torch.Size([])
memory (bytes)
4839698432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 10% |
memory (bytes)
4839698432
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  671355140.0
relative error loss 0.23593931
shape of L is 
torch.Size([])
memory (bytes)
4842856448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
4842856448
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  671022850.0
relative error loss 0.23582253
shape of L is 
torch.Size([])
memory (bytes)
4846141440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
4846141440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 10% |
error is  670714900.0
relative error loss 0.2357143
shape of L is 
torch.Size([])
memory (bytes)
4849217536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 10% |
memory (bytes)
4849217536
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  670218750.0
relative error loss 0.23553994
shape of L is 
torch.Size([])
memory (bytes)
4852588544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
4852588544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% | 10% |
error is  671110140.0
relative error loss 0.23585321
shape of L is 
torch.Size([])
memory (bytes)
4855787520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
4855787520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 10% |
error is  669965800.0
relative error loss 0.23545106
shape of L is 
torch.Size([])
memory (bytes)
4859023360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 10% |
memory (bytes)
4859023360
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  669611500.0
relative error loss 0.23532654
time to take a step is 267.1504878997803
it  5 : 2558014464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
4862234624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
4862234624
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  669611500.0
relative error loss 0.23532654
shape of L is 
torch.Size([])
memory (bytes)
4865396736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
4865396736
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  669296100.0
relative error loss 0.2352157
shape of L is 
torch.Size([])
memory (bytes)
4868657152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 10% |
memory (bytes)
4868657152
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  669055740.0
relative error loss 0.23513122
shape of L is 
torch.Size([])
memory (bytes)
4871802880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 10% |
memory (bytes)
4871802880
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  668676350.0
relative error loss 0.23499788
shape of L is 
torch.Size([])
memory (bytes)
4875071488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 10% |
memory (bytes)
4875071488
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  668503300.0
relative error loss 0.23493707
shape of L is 
torch.Size([])
memory (bytes)
4878221312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 30% | 10% |
memory (bytes)
4878221312
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  668299500.0
relative error loss 0.23486546
shape of L is 
torch.Size([])
memory (bytes)
4881317888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4881494016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  667950850.0
relative error loss 0.23474291
shape of L is 
torch.Size([])
memory (bytes)
4884709376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 10% |
memory (bytes)
4884709376
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  668055300.0
relative error loss 0.23477963
shape of L is 
torch.Size([])
memory (bytes)
4887834624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 10% |
memory (bytes)
4887834624
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  667781400.0
relative error loss 0.23468335
shape of L is 
torch.Size([])
memory (bytes)
4891148288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4891148288
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  667584800.0
relative error loss 0.23461427
time to take a step is 267.5510456562042
it  6 : 2558014464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
4894326784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 10% |
memory (bytes)
4894326784
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  667584800.0
relative error loss 0.23461427
shape of L is 
torch.Size([])
memory (bytes)
4897554432
| ID | GPU | MEM |
------------------
|  0 | 21% |  0% |
|  1 | 30% | 10% |
memory (bytes)
4897554432
| ID | GPU  | MEM |
-------------------
|  0 |  17% |  0% |
|  1 | 100% | 10% |
error is  667399700.0
relative error loss 0.23454921
shape of L is 
torch.Size([])
memory (bytes)
4900773888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 10% |
memory (bytes)
4900773888
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  667260900.0
relative error loss 0.23450045
shape of L is 
torch.Size([])
memory (bytes)
4903993344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
4903993344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  667072260.0
relative error loss 0.23443414
shape of L is 
torch.Size([])
memory (bytes)
4907196416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
4907196416
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  666969860.0
relative error loss 0.23439816
shape of L is 
torch.Size([])
memory (bytes)
4910387200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
4910387200
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  666858240.0
relative error loss 0.23435894
shape of L is 
torch.Size([])
memory (bytes)
4913614848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
4913627136
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  666673400.0
relative error loss 0.23429398
shape of L is 
torch.Size([])
memory (bytes)
4916846592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
4916846592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% | 10% |
error is  666588400.0
relative error loss 0.2342641
shape of L is 
torch.Size([])
memory (bytes)
4919877632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4920053760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 10% |
error is  666474750.0
relative error loss 0.23422416
shape of L is 
torch.Size([])
memory (bytes)
4923269120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 10% |
memory (bytes)
4923269120
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  666401540.0
relative error loss 0.23419844
time to take a step is 267.5974109172821
it  7 : 2558014464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
4926418944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 10% |
memory (bytes)
4926418944
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  666401540.0
relative error loss 0.23419844
shape of L is 
torch.Size([])
memory (bytes)
4929679360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
4929679360
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  666262000.0
relative error loss 0.2341494
shape of L is 
torch.Size([])
memory (bytes)
4932890624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 10% |
memory (bytes)
4932919296
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  666104600.0
relative error loss 0.23409407
shape of L is 
torch.Size([])
memory (bytes)
4935974912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
4936130560
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  665893400.0
relative error loss 0.23401985
shape of L is 
torch.Size([])
memory (bytes)
4939333632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 30% | 10% |
memory (bytes)
4939333632
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  665656600.0
relative error loss 0.23393662
shape of L is 
torch.Size([])
memory (bytes)
4942491648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 10% |
memory (bytes)
4942491648
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  665486600.0
relative error loss 0.23387688
shape of L is 
torch.Size([])
memory (bytes)
4945772544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
4945772544
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  665367040.0
relative error loss 0.23383486
shape of L is 
torch.Size([])
memory (bytes)
4948799488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
4948979712
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  665062660.0
relative error loss 0.2337279
shape of L is 
torch.Size([])
memory (bytes)
4952076288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 10% |
memory (bytes)
4952203264
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  664930800.0
relative error loss 0.23368156
shape of L is 
torch.Size([])
memory (bytes)
4955414528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 10% |
memory (bytes)
4955414528
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  664794100.0
relative error loss 0.23363352
time to take a step is 267.9329741001129
it  8 : 2558014464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
4958445568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% | 10% |
memory (bytes)
4958621696
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  664794100.0
relative error loss 0.23363352
shape of L is 
torch.Size([])
memory (bytes)
4961828864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 10% |
memory (bytes)
4961828864
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  664594400.0
relative error loss 0.23356335
shape of L is 
torch.Size([])
memory (bytes)
4964880384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 10% |
memory (bytes)
4964880384
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  664390900.0
relative error loss 0.23349182
shape of L is 
torch.Size([])
memory (bytes)
4968255488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 10% |
memory (bytes)
4968255488
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  664254200.0
relative error loss 0.23344378
shape of L is 
torch.Size([])
memory (bytes)
4971474944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4971474944
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  664097300.0
relative error loss 0.23338863
shape of L is 
torch.Size([])
memory (bytes)
4974620672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
4974620672
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  663919100.0
relative error loss 0.233326
shape of L is 
torch.Size([])
memory (bytes)
4977897472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
4977897472
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  663711740.0
relative error loss 0.23325314
shape of L is 
torch.Size([])
memory (bytes)
4981026816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
4981026816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% | 10% |
error is  663602700.0
relative error loss 0.23321481
shape of L is 
torch.Size([])
memory (bytes)
4984324096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
4984324096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 10% |
error is  663478300.0
relative error loss 0.23317109
shape of L is 
torch.Size([])
memory (bytes)
4987531264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
4987531264
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  663348200.0
relative error loss 0.23312539
time to take a step is 268.5633006095886
it  9 : 2557534208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
4990685184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 10% |
memory (bytes)
4990685184
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  663348200.0
relative error loss 0.23312539
shape of L is 
torch.Size([])
memory (bytes)
4993949696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4993949696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% | 10% |
error is  663232500.0
relative error loss 0.23308471
shape of L is 
torch.Size([])
memory (bytes)
4997091328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
4997091328
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  663142400.0
relative error loss 0.23305304
shape of L is 
torch.Size([])
memory (bytes)
5000388608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
5000388608
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  663066900.0
relative error loss 0.2330265
shape of L is 
torch.Size([])
memory (bytes)
5003427840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 10% |
memory (bytes)
5003603968
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  662982400.0
relative error loss 0.23299682
shape of L is 
torch.Size([])
memory (bytes)
5006819328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 10% |
memory (bytes)
5006819328
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  662989300.0
relative error loss 0.23299925
shape of L is 
torch.Size([])
memory (bytes)
5010034688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 10% |
memory (bytes)
5010034688
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  662937100.0
relative error loss 0.23298089
shape of L is 
torch.Size([])
memory (bytes)
5013237760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 11% |
memory (bytes)
5013237760
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  662891800.0
relative error loss 0.23296496
shape of L is 
torch.Size([])
memory (bytes)
5016461312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 10% |
memory (bytes)
5016461312
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  662798600.0
relative error loss 0.23293222
shape of L is 
torch.Size([])
memory (bytes)
5019631616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
5019631616
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  662759700.0
relative error loss 0.23291855
time to take a step is 269.8912675380707
it  10 : 2558014464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
5022892032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
5022892032
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  662759700.0
relative error loss 0.23291855
shape of L is 
torch.Size([])
memory (bytes)
5025988608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
5025988608
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  662687740.0
relative error loss 0.23289326
shape of L is 
torch.Size([])
memory (bytes)
5029212160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 30% | 10% |
memory (bytes)
5029306368
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  662621200.0
relative error loss 0.23286988
shape of L is 
torch.Size([])
memory (bytes)
5032529920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 10% |
memory (bytes)
5032529920
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  662552800.0
relative error loss 0.23284586
shape of L is 
torch.Size([])
memory (bytes)
5035634688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 10% |
memory (bytes)
5035634688
| ID | GPU  | MEM |
-------------------
|  0 |   1% |  0% |
|  1 | 100% | 10% |
error is  662514400.0
relative error loss 0.23283236
shape of L is 
torch.Size([])
memory (bytes)
5038952448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
5038952448
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  662358300.0
relative error loss 0.23277748
shape of L is 
torch.Size([])
memory (bytes)
5042061312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 10% |
memory (bytes)
5042061312
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  662276600.0
relative error loss 0.23274878
shape of L is 
torch.Size([])
memory (bytes)
5045379072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
5045379072
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  662202600.0
relative error loss 0.23272277
shape of L is 
torch.Size([])
memory (bytes)
5048582144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 10% |
memory (bytes)
5048582144
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  662081800.0
relative error loss 0.2326803
shape of L is 
torch.Size([])
memory (bytes)
5051707392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
5051801600
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  662078460.0
relative error loss 0.23267914
shape of L is 
torch.Size([])
memory (bytes)
5055004672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
5055004672
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  662004000.0
relative error loss 0.23265296
time to take a step is 295.7313027381897
it  11 : 2557774848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5058113536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
5058113536
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  662004000.0
relative error loss 0.23265296
shape of L is 
torch.Size([])
memory (bytes)
5061439488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 10% |
memory (bytes)
5061439488
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  661889300.0
relative error loss 0.23261265
shape of L is 
torch.Size([])
memory (bytes)
5064527872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
5064527872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  661815800.0
relative error loss 0.23258683
shape of L is 
torch.Size([])
memory (bytes)
5067857920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 10% |
memory (bytes)
5067857920
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  661717760.0
relative error loss 0.23255238
shape of L is 
torch.Size([])
memory (bytes)
5071089664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 30% | 10% |
memory (bytes)
5071089664
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  661634800.0
relative error loss 0.23252323
shape of L is 
torch.Size([])
memory (bytes)
5074305024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 10% |
memory (bytes)
5074305024
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  661582850.0
relative error loss 0.23250496
shape of L is 
torch.Size([])
memory (bytes)
5077520384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% | 10% |
memory (bytes)
5077520384
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  661510900.0
relative error loss 0.23247968
shape of L is 
torch.Size([])
memory (bytes)
5080596480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
5080596480
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  661437440.0
relative error loss 0.23245387
shape of L is 
torch.Size([])
memory (bytes)
5083963392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 10% |
memory (bytes)
5083963392
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  661369100.0
relative error loss 0.23242983
shape of L is 
torch.Size([])
memory (bytes)
5087170560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
5087174656
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  661342700.0
relative error loss 0.23242058
time to take a step is 269.8022270202637
it  12 : 2558014464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
5090344960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
5090344960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  661342700.0
relative error loss 0.23242058
shape of L is 
torch.Size([])
memory (bytes)
5093588992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 10% |
memory (bytes)
5093588992
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  661313800.0
relative error loss 0.2324104
shape of L is 
torch.Size([])
memory (bytes)
5096775680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
5096775680
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  661260540.0
relative error loss 0.23239169
shape of L is 
torch.Size([])
memory (bytes)
5100015616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 32% | 10% |
memory (bytes)
5100015616
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  661259800.0
relative error loss 0.23239142
shape of L is 
torch.Size([])
memory (bytes)
5103239168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
5103239168
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  661224200.0
relative error loss 0.23237891
shape of L is 
torch.Size([])
memory (bytes)
5106352128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 10% |
memory (bytes)
5106446336
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  661174000.0
relative error loss 0.23236129
shape of L is 
torch.Size([])
memory (bytes)
5109665792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
5109665792
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  661135100.0
relative error loss 0.23234761
shape of L is 
torch.Size([])
memory (bytes)
5112852480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
5112852480
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  661075700.0
relative error loss 0.23232673
shape of L is 
torch.Size([])
memory (bytes)
5116092416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 10% |
memory (bytes)
5116092416
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  661072640.0
relative error loss 0.23232566
shape of L is 
torch.Size([])
memory (bytes)
5119303680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
5119303680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% | 11% |
error is  661042940.0
relative error loss 0.23231521
time to take a step is 269.55827617645264
it  13 : 2558014976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5122531328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
5122531328
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  661042940.0
relative error loss 0.23231521
shape of L is 
torch.Size([])
memory (bytes)
5125644288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
5125644288
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  661000200.0
relative error loss 0.23230019
shape of L is 
torch.Size([])
memory (bytes)
5128953856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
5128953856
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  660954400.0
relative error loss 0.23228408
shape of L is 
torch.Size([])
memory (bytes)
5132165120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
5132165120
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  660923400.0
relative error loss 0.2322732
shape of L is 
torch.Size([])
memory (bytes)
5135204352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 10% |
memory (bytes)
5135384576
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  660882400.0
relative error loss 0.23225881
shape of L is 
torch.Size([])
memory (bytes)
5138604032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
5138604032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 10% |
error is  660855800.0
relative error loss 0.23224945
shape of L is 
torch.Size([])
memory (bytes)
5141643264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 10% |
memory (bytes)
5141819392
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  660830700.0
relative error loss 0.23224063
shape of L is 
torch.Size([])
memory (bytes)
5145034752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 10% |
memory (bytes)
5145034752
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  660777200.0
relative error loss 0.23222183
shape of L is 
torch.Size([])
memory (bytes)
5148241920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 10% |
memory (bytes)
5148241920
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  660735000.0
relative error loss 0.23220699
shape of L is 
torch.Size([])
memory (bytes)
5151461376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
5151461376
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  660707300.0
relative error loss 0.23219727
time to take a step is 272.9976942539215
it  14 : 2557534208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
5154676736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
5154676736
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  660707300.0
relative error loss 0.23219727
shape of L is 
torch.Size([])
memory (bytes)
5157806080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 10% |
memory (bytes)
5157806080
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  660681700.0
relative error loss 0.23218827
shape of L is 
torch.Size([])
memory (bytes)
5161103360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 10% |
memory (bytes)
5161103360
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  660569860.0
relative error loss 0.23214896
shape of L is 
torch.Size([])
memory (bytes)
5164302336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
5164302336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% | 10% |
error is  660588540.0
relative error loss 0.23215553
shape of L is 
torch.Size([])
memory (bytes)
5167374336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
5167525888
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  660516600.0
relative error loss 0.23213024
shape of L is 
torch.Size([])
memory (bytes)
5170741248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 11% |
memory (bytes)
5170741248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 11% |
error is  660472060.0
relative error loss 0.2321146
shape of L is 
torch.Size([])
memory (bytes)
5173882880
| ID | GPU | MEM |
------------------
|  0 |  2% |  0% |
|  1 | 12% | 11% |
memory (bytes)
5173882880
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  660340200.0
relative error loss 0.23206826
shape of L is 
torch.Size([])
memory (bytes)
5177176064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 11% |
memory (bytes)
5177176064
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  660267260.0
relative error loss 0.23204261
shape of L is 
torch.Size([])
memory (bytes)
5180387328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 11% |
memory (bytes)
5180387328
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  660194050.0
relative error loss 0.23201689
shape of L is 
torch.Size([])
memory (bytes)
5183582208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 11% |
memory (bytes)
5183610880
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  660133900.0
relative error loss 0.23199575
time to take a step is 269.651184797287
sum tnnu_Z after tensor(8320424., device='cuda:0')
shape of features
(6672,)
shape of features
(6672,)
number of orig particles 26687
number of new particles after remove low mass 26687
tnuZ shape should be parts x labs
torch.Size([26687, 702])
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
error without small mass is  760817300.0
relative error without small mass is  0.26737964
nnu_Z shape should be number of particles by maxV
(26687, 702)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
shape of features
(26687,)
Wed Feb 1 13:37:09 EST 2023
