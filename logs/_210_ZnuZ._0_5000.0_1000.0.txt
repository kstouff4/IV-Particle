Tue Jan 31 17:59:01 EST 2023
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  0% |  0% |
numbers of X: 43133984
numbers of Z: 20602
shape of features
(20602,)
shape of features
(20602,)
ZX	Vol	Parts	Cubes	Eps
Z	0.018240461002002533	20602	20.602	0.09602304544274079
X	0.017013465730480692	2030	2.03	0.20312603173954322
X	0.017037853911073788	24826	24.826	0.08820693123754353
X	0.01716122427142142	2429	2.429	0.19188465724290046
X	0.017334400443766947	3071	3.071	0.1780504201638374
X	0.01707196892697887	81964	81.964	0.059277082764028086
X	0.01715409064024871	71626	71.626	0.06210105821517584
X	0.01709863521363567	63890	63.89	0.0644430647679444
X	0.017308752291838658	56235	56.235	0.06751801842153655
X	0.01807638530452479	18101	18.101	0.09995465101124029
X	0.017111241235051554	40304	40.304	0.07515844531676295
X	0.017003507813776422	29135	29.135	0.08356822304454019
X	0.01802870316595365	63705	63.705	0.06565437268627831
X	0.01779784098555516	9927	9.927	0.12148325881837359
X	0.017151874695879026	235844	235.844	0.04174094281790122
X	0.017096780299999795	22446	22.446	0.09132544367488156
X	0.0174524619840131	41553	41.553	0.07488901277968911
X	0.01717491419354064	50352	50.352	0.06987030142066393
X	0.0180541601468329	56498	56.498	0.06836724383666969
X	0.01709178251186442	150520	150.52	0.048424414989232015
X	0.017137154000315468	142724	142.724	0.049334102320882395
X	0.017014358522336428	60474	60.474	0.06552629732620668
X	0.01810336524066838	190944	190.944	0.04559855697970739
X	0.01705773169616405	14799	14.799	0.10484868057096576
X	0.01712666117675424	15305	15.305	0.10381970504210948
X	0.017009872286403018	39367	39.367	0.0756001941596862
X	0.0171301676326904	61702	61.702	0.06523601999450215
X	0.018027841257593054	81600	81.6	0.06045299968958537
X	0.01708522673818795	18923	18.923	0.09665186807708433
X	0.01808861578443449	163228	163.228	0.048032688037326916
X	0.018183985696463217	1173443	1173.443	0.024931152821782605
X	0.017097852775798346	14820	14.82	0.10488123432905642
X	0.01815782402661467	889113	889.113	0.027333942775964255
X	0.017819792511030785	10372	10.372	0.11976963102047908
X	0.017822800791819974	24591	24.591	0.08982555744302077
X	0.017016547367266013	49960	49.96	0.06983658684645001
X	0.018011614719600107	388900	388.9	0.03591188776760641
X	0.018107099150486014	64930	64.93	0.06533328453784176
X	0.016997834716143752	2577	2.577	0.18753963271108512
X	0.016999423079505988	5114	5.114	0.14924203181778922
X	0.01674797257855878	3657	3.657	0.1660653761773683
X	0.017247599218381596	3099	3.099	0.17721576693504504
X	0.016188372442843133	784	0.784	0.27434415276499013
X	0.016087556709800208	1446	1.446	0.22324098631130496
X	0.017037374882968394	3756	3.756	0.16553596515602298
X	0.016899503398346012	891	0.891	0.26668509657107436
X	0.016814834166720086	1883	1.883	0.20746623365801903
X	0.017047409031391708	2315	2.315	0.19455204287315273
X	0.017099349219991768	8891	8.891	0.12435874308854025
X	0.017059516790906306	3222	3.222	0.1742931811221333
X	0.017408338332240477	6561	6.561	0.13844026660019004
X	0.017071509487489187	14522	14.522	0.10553953193928545
X	0.016734210868091653	1735	1.735	0.21286385320513274
X	0.01701473735422012	4566	4.566	0.1550350097112635
X	0.016816895144350162	4909	4.909	0.1507477810009163
X	0.01747173431309902	5718	5.718	0.14510998213033804
X	0.017040568337785125	6199	6.199	0.1400836598392008
X	0.01705652347651074	1722	1.722	0.21475953572103443
X	0.017031765292747657	5779	5.779	0.1433735338263974
X	0.016925807080337026	3047	3.047	0.1771028883443682
X	0.017027310742190847	2619	2.619	0.18663948229484836
X	0.017031015364813516	4223	4.223	0.15917437493834827
X	0.017014206840465358	1781	1.781	0.21218561097042962
X	0.01694724948945391	3610	3.61	0.16744184742351934
X	0.01788054301507432	18586	18.586	0.09871843390559773
X	0.016964256675536343	3824	3.824	0.16431314834581184
X	0.01689313058035392	3741	3.741	0.16528780422349557
X	0.017016369321113726	4252	4.252	0.15876613861356137
X	0.016994033040911544	5841	5.841	0.14275886420067263
X	0.017132830214738336	6803	6.803	0.13605344616188736
X	0.017034722435313846	3771	3.771	0.16530760851527612
X	0.017027126616965167	2647	2.647	0.18597838614858336
X	0.017017281235742768	5203	5.203	0.14843812165627493
X	0.01701975154233784	3572	3.572	0.16827280022911253
X	0.01682841785792208	3160	3.16	0.17462995209552581
X	0.01704038106543707	3915	3.915	0.1632735536784455
X	0.017051949615667316	10549	10.549	0.11736022032063533
X	0.01693506082092432	868	0.868	0.26920865103094743
X	0.01670683173268585	1722	1.722	0.21328172787551367
X	0.017002325928560692	5159	5.159	0.14881530648065316
X	0.017520856357688804	16905	16.905	0.10119989253987761
X	0.017260637114209335	2217	2.217	0.19819698469896668
X	0.017275163046084396	1526	1.526	0.2245376676318722
X	0.017005366268863557	3375	3.375	0.17143680766052985
X	0.016865813045728442	1365	1.365	0.23118423233858185
X	0.01653281608559664	1594	1.594	0.21808184156104454
X	0.016963187887646433	2959	2.959	0.17897299361042693
X	0.016563146743391627	1786	1.786	0.2100972969827552
X	0.01663305432918794	1245	1.245	0.23728350647311533
X	0.01727286957558412	3744	3.744	0.16647264679670049
X	0.017055207781118586	4812	4.812	0.15246744830884065
X	0.017021887226549733	5902	5.902	0.14234301743616778
X	0.016965306396782034	2768	2.768	0.18300594883384397
X	0.01703058620250894	2005	2.005	0.20403518837729231
X	0.01675455715931003	8276	8.276	0.1265039706467502
X	0.01774145350714556	4990	4.99	0.15262636133558707
X	0.01695434363801936	7533	7.533	0.13105019494311496
X	0.016993289138282873	3268	3.268	0.17324676466177769
X	0.018127498717815983	2869	2.869	0.18487113673240718
X	0.016966946288998322	5171	5.171	0.14859688850407582
X	0.0169699361461819	6595	6.595	0.1370319389157117
X	0.017021677422626447	12343	12.343	0.11130821263350693
X	0.016838538681931664	2914	2.914	0.17944786012246666
X	0.017098358351551244	6081	6.081	0.14114314939428538
X	0.016835050008052486	2336	2.336	0.19315851480311752
X	0.016928396197826063	13598	13.598	0.10757555433429324
X	0.016833261660238148	2167	2.167	0.19804769292917113
X	0.017048261859424253	3778	3.778	0.1652492072146795
X	0.017342961292244233	1687	1.687	0.21743830673338876
X	0.017040311593569776	2365	2.365	0.19314440361938845
X	0.016908723124406753	1244	1.244	0.23865112032617922
X	0.016750257874674525	5918	5.918	0.1414540910005738
X	0.016902822992270115	1092	1.092	0.2492176912398843
X	0.01676695367650401	2040	2.04	0.20180937265207444
X	0.01695611389321367	2531	2.531	0.18851448528552528
X	0.017066034954084	6355	6.355	0.13899708483981538
X	0.016705976294261557	2222	2.222	0.19590368355751855
X	0.017034334120259113	3080	3.08	0.17684446912949786
X	0.016962731887876645	901	0.901	0.2660257454954357
X	0.016897126417132004	9518	9.518	0.12108463889722919
X	0.016351391997083818	1222	1.222	0.23740754008964143
X	0.018062196795810125	12943	12.943	0.11174940159389485
X	0.016822529855496433	2004	2.004	0.20323470128927182
X	0.01642963930046075	2368	2.368	0.190728434295629
X	0.017061372083953672	3910	3.91	0.16341016393115979
X	0.017009887868362956	4801	4.801	0.15244853254357654
X	0.017526045652642555	1515	1.515	0.22616415205901166
X	0.016979155545048447	2249	2.249	0.19617434466305442
X	0.017083534181329654	5746	5.746	0.14379287478028788
X	0.017065773941454643	9536	9.536	0.12140966162533694
X	0.016891878739486006	3502	3.502	0.16896131119192961
X	0.01735141473846719	21784	21.784	0.09269701600622891
X	0.016893239902241924	989	0.989	0.2575365573920554
X	0.017048536763844158	3015	3.015	0.17815578589371034
X	0.017043996693307202	1631	1.631	0.21862802257984268
X	0.0170626194020415	2442	2.442	0.19117599005502936
X	0.01680023422125224	3052	3.052	0.17656729718430936
X	0.017110275287280444	2127	2.127	0.2003686793367174
X	0.01676475108913338	3479	3.479	0.1689069657545495
X	0.01677081749871459	3667	3.667	0.16598968763339028
X	0.016813642809060957	2359	2.359	0.1924470649786289
X	0.016984078621220294	3754	3.754	0.16539253652404506
X	0.017062409525391703	1901	1.901	0.2078193975704875
X	0.01694912771602106	10649	10.649	0.11675608079429571
X	0.01693741974077145	5652	5.652	0.1441722575635476
X	0.016989620324421792	5629	5.629	0.14451651330799825
X	0.01699621593893997	2230	2.23	0.19679578498862335
X	0.016993725135166688	2530	2.53	0.188678619257331
X	0.01701336935049864	1874	1.874	0.2086124617058453
X	0.016970473215941764	2906	2.906	0.18008026215582668
X	0.01686795648466512	1851	1.851	0.20887446526331954
X	0.016812612566780943	2675	2.675	0.18454564725194653
X	0.016836097871956585	3912	3.912	0.1626600364207693
X	0.01704621476442514	3623	3.623	0.1675662461357761
X	0.01702659778353486	2598	2.598	0.187138399152508
X	0.01714367444831578	4246	4.246	0.15923601740585588
X	0.0180280117517216	9629	9.629	0.12325055452868841
X	0.016999834013425832	5044	5.044	0.1499304580085421
X	0.017023228337241104	2833	2.833	0.1818019135207022
X	0.01693337554632668	2696	2.696	0.1845048913614956
X	0.016686928888100656	942	0.942	0.26068047527379784
X	0.018152366785528957	15201	15.201	0.10609307400245206
X	0.01701085749922502	1212	1.212	0.24121697242384985
X	0.01799026359806057	12372	12.372	0.11329198994942792
X	0.016507960535389752	772	0.772	0.27756116853742474
X	0.01690893373941675	4027	4.027	0.1613286616092743
X	0.017137505155768465	4605	4.605	0.154967034139935
X	0.017125725013905108	1668	1.668	0.21734566040089823
X	0.017014827008271095	3353	3.353	0.17184279513818876
X	0.017044820253702974	4317	4.317	0.1580532603703193
X	0.016954678228372315	1138	1.138	0.2460645762486321
X	0.016523265423713636	1505	1.505	0.22225581191458726
X	0.016762272967978355	2370	2.37	0.19195297799617453
X	0.01699494043681864	6616	6.616	0.13695399861081448
X	0.017024728472400014	1451	1.451	0.2272327514736651
X	0.017087497628788285	4547	4.547	0.15547163687543022
X	0.01715230653571739	3368	3.368	0.1720482061759648
X	0.01702848399891981	5633	5.633	0.14459238171586655
X	0.017047028565819394	3565	3.565	0.16847277066807856
X	0.017112892577713205	2600	2.6	0.18740594285222692
X	0.016917880012571707	3670	3.67	0.16642808859929897
X	0.017029394422020422	6199	6.199	0.14005303438821598
X	0.0169160844983157	1567	1.567	0.22100916467740153
X	0.01702938535168586	2899	2.899	0.18043339435302752
X	0.017976857538779804	2870	2.87	0.18433619838867726
X	0.0170251547251001	4759	4.759	0.1529414209763699
X	0.017003457101571525	5449	5.449	0.14613025451977193
X	0.017038550727355502	3526	3.526	0.16906360079424787
X	0.01692947792949223	3150	3.15	0.17516379155386674
X	0.017042831725108395	7427	7.427	0.13189938705125998
X	0.017003982298954053	7667	7.667	0.13040924892246164
X	0.017020994666624923	3236	3.236	0.17391036806314739
X	0.01615938579392325	1075	1.075	0.246796385628933
X	0.01679035992501516	3182	3.182	0.17409512500443544
X	0.0169885554914937	2929	2.929	0.1796714311523255
X	0.016933086969085482	4415	4.415	0.15653152283292365
X	0.01687399238306875	3296	3.296	0.17234957437480708
X	0.01706564708843596	3824	3.824	0.16463984892633607
X	0.01685720985130368	2500	2.5	0.18892172173900015
X	0.016951285079957665	4378	4.378	0.15702746639647028
X	0.017068621667102768	1268	1.268	0.23788098760509782
X	0.01700584664814235	5522	5.522	0.1454902707417445
X	0.017096477348414577	3965	3.965	0.16276254844807253
X	0.017052412541388527	9473	9.473	0.12164644767441123
X	0.016757257839442517	2626	2.626	0.1854824156938516
X	0.016938020802987504	8142	8.142	0.1276567708846644
X	0.017029175819995728	3654	3.654	0.16703533873904286
X	0.01686457599661474	2327	2.327	0.19352021944609285
X	0.016890579931818396	3297	3.297	0.17238859814134055
X	0.017131216131987217	1630	1.63	0.2190450936606188
X	0.017059483552981508	8393	8.393	0.12667265856053153
X	0.017842330497629302	6010	6.01	0.14372281093086497
X	0.016862588312056923	2335	2.335	0.1932913639939733
X	0.016876090868937613	3376	3.376	0.17098439356939982
X	0.016998669950839273	10245	10.245	0.11838617609681194
X	0.015918393234306414	813	0.813	0.2695273829666294
X	0.016605119820147795	1046	1.046	0.2513256279913786
X	0.016995499656486397	723	0.723	0.2864608084536645
X	0.016900318357676886	1434	1.434	0.22757012327905837
X	0.01704916402877741	10683	10.683	0.11686109559018573
X	0.01691229161750541	4779	4.779	0.15238953458499666
X	0.01702230492996603	3181	3.181	0.17491144687368004
X	0.01698910765397451	921	0.921	0.2642228135006257
X	0.017827914926683952	7794	7.794	0.1317589273294771
X	0.01699633413077191	2407	2.407	0.191849081074006
X	0.017010065699883104	4019	4.019	0.1617568427726046
X	0.0172625612774018	4599	4.599	0.1554105895279162
X	0.016787444655749325	4028	4.028	0.16092803584562382
X	0.016949800297185257	1477	1.477	0.22555962477961028
X	0.01719470642447799	5001	5.001	0.15093129681849413
X	0.01686228371035265	1317	1.317	0.23394308126910537
X	0.01693058570748175	2955	2.955	0.17893892851819443
X	0.016708964883564	2362	2.362	0.1919655147048638
X	0.01687665091587837	2429	2.429	0.19081810745653235
X	0.017026318289288116	2979	2.979	0.17879282296960416
X	0.017086832471914	4667	4.667	0.15412553134908302
X	0.017022408972868212	5129	5.129	0.14916357165541008
X	0.017119371454808636	4602	4.602	0.15494601110124445
X	0.017081602475302948	4588	4.588	0.15498930607354514
X	0.016964875170080496	2412	2.412	0.191598066293958
X	0.017065750209124146	8152	8.152	0.12792450285123136
X	0.01714540825371604	7428	7.428	0.13215754989132691
X	0.017167225378689876	10953	10.953	0.11615975134803912
X	0.017071888592614926	6892	6.892	0.1353044560692532
X	0.01659976873892171	4056	4.056	0.15995630515941103
X	0.017022437922791348	11533	11.533	0.11385702251438902
X	0.017690655708931795	16280	16.28	0.10280869424777289
X	0.01715167378472774	15155	15.155	0.10421179003291688
X	0.017116050514855537	5024	5.024	0.15047048112156614
X	0.018154171648533	167364	167.364	0.04769118681323953
X	0.01711945924858411	91012	91.012	0.05729681239636067
X	0.017036195949138192	70574	70.574	0.06226480483661154
X	0.017759384719334362	58434	58.434	0.0672337651892848
X	0.0169983908279847	2375	2.375	0.19271465067816057
X	0.016857415823761955	11729	11.729	0.1128521934058354
X	0.017786688135925807	381680	381.68	0.03598586079378149
X	0.017185636612454314	352550	352.55	0.036529884731614004
X	0.017017873444718862	3234	3.234	0.17393557798944204
X	0.017122394001629175	29926	29.926	0.08301793725008814
X	0.017988686566641734	80191	80.191	0.0607609574205682
X	0.01696532783997501	16772	16.772	0.10038276043826708
X	0.017072156792284132	21100	21.1	0.09318270465122505
X	0.01707774848102367	31559	31.559	0.08148965138346491
X	0.017101463977003996	8956	8.956	0.12406227334288222
X	0.017747931832349295	8858	8.858	0.12606775683035945
X	0.017126813219762005	10436	10.436	0.11795440485100141
X	0.017182503327409098	17172	17.172	0.10002038432073157
X	0.017023019903757562	12328	12.328	0.11135626617097588
X	0.016956303318045474	5695	5.695	0.14386190805735374
X	0.0171314810082699	30063	30.063	0.08290629946178935
X	0.01715148192651967	20025	20.025	0.09496779950201628
X	0.017438271464393543	226919	226.919	0.042515196534975104
X	0.0179531376367035	61833	61.833	0.06621758648917872
X	0.0170863695260859	10965	10.965	0.1159347739396627
X	0.01814168389467824	39468	39.468	0.07717515172348485
X	0.01700036489604487	4640	4.64	0.15416297674050253
X	0.01717231312117325	17685	17.685	0.09902417774543947
X	0.01712705268579848	49165	49.165	0.07036265149510379
X	0.018032521029495173	63128	63.128	0.06585844500877626
X	0.01740034139478887	150920	150.92	0.048671006145034605
X	0.016378734912908748	1984	1.984	0.202106165334473
X	0.01707803656029396	6432	6.432	0.1384726385873275
X	0.01721789219116792	65029	65.029	0.06421319848924027
X	0.01808866147381702	70129	70.129	0.0636555480565484
X	0.01816016500543156	43266	43.266	0.07487284714580292
X	0.017243556657059178	3036	3.036	0.17841924353325875
X	0.01769840927026286	193391	193.391	0.045064302992315466
X	0.017157932010822264	15867	15.867	0.10264159062091417
X	0.01712881720394035	20078	20.078	0.09484235041193394
X	0.017198550036666888	79431	79.431	0.06004828403954173
X	0.017094608881819417	3073	3.073	0.1771871418915033
X	0.017126811451118027	59101	59.101	0.0661749917414433
X	0.017790579589353334	74980	74.98	0.06190820617781231
X	0.018193046883450695	323017	323.017	0.03833187116405682
X	0.01813080938176534	57113	57.113	0.0682172287891411
X	0.0171016082477545	48619	48.619	0.07059007509358346
X	0.01679377170583008	21857	21.857	0.09159095013598544
X	0.01709213254868307	6190	6.19	0.14029274119239432
X	0.018183012311959196	29825	29.825	0.0847933499716349
X	0.017040123083894063	8960	8.96	0.12389532119814683
X	0.017155700478491076	16146	16.146	0.10204251854732141
X	0.017203975815487913	44796	44.796	0.07268807972336552
X	0.017703348802019805	28252	28.252	0.08557273436646533
X	0.017022077513373514	7420	7.42	0.13188727228068556
X	0.017073939951477674	7055	7.055	0.13425967056004995
X	0.01812582261056786	298302	298.302	0.039313995784503904
X	0.01706321880790361	8334	8.334	0.12698014576172362
X	0.01687773783338221	2028	2.028	0.20265100505120467
X	0.017016981131414005	7413	7.413	0.13191560463586033
X	0.017120628942660497	143188	143.188	0.04926491040104025
X	0.01810541580295767	28573	28.573	0.08589163844749347
X	0.017246748983680362	17141	17.141	0.10020522384568305
X	0.017588790448480208	11946	11.946	0.113763904245205
X	0.017037690115851968	13602	13.602	0.10779600060138333
X	0.017112097297470246	14696	14.696	0.10520459267024324
X	0.018182919612756573	473363	473.363	0.033740882438589845
X	0.01707833611265496	40232	40.232	0.07515501710182855
X	0.017147031021691263	16232	16.232	0.10184482505756395
X	0.017143100718337478	118835	118.835	0.052446317777346554
X	0.017172020837859927	121173	121.173	0.0521360969413162
X	0.017171024687684753	6294	6.294	0.1397300532897835
X	0.017078582648120788	97072	97.072	0.056034127766695654
X	0.017598489873183906	115816	115.816	0.05336243191951015
X	0.018203353867583493	411304	411.304	0.03537226143444623
X	0.017037919851453462	9961	9.961	0.1195926649478737
X	0.0168775622062176	2461	2.461	0.18999085677079453
X	0.017096991269094624	12321	12.321	0.11153844292323228
X	0.01711063525497095	18384	18.384	0.0976357141927688
X	0.017096045602341972	35762	35.762	0.07819121403539618
X	0.01787359178673975	24825	24.825	0.08962741260569147
X	0.016892548477656266	2132	2.132	0.19935906640445114
X	0.018080286101684592	28848	28.848	0.0855782096162177
X	0.017901104479717482	133795	133.795	0.051146193161471225
X	0.016964651494526212	161899	161.899	0.04714477767426956
X	0.017985336518162226	5691	5.691	0.1467495103565867
X	0.01731524096804824	168918	168.918	0.04680054059931328
X	0.017038540371138975	16923	16.923	0.1002270645798971
X	0.017069245633942416	19337	19.337	0.0959271945915547
X	0.017066999074437938	13068	13.068	0.10930734516611554
X	0.016925786055069725	4042	4.042	0.16118236084463816
X	0.018059766345054195	178344	178.344	0.04661058367718617
X	0.017134674637838476	42423	42.423	0.07391935381769595
X	0.01813031139382186	11059	11.059	0.1179133926078067
X	0.017311255676622467	25435	25.435	0.08796277927849296
X	0.017108607413714567	8124	8.124	0.12817842042245967
X	0.018015617829468623	345268	345.268	0.03736780693490538
X	0.017132598965104186	30269	30.269	0.08271959357339462
X	0.017479647146870874	189090	189.09	0.045215572412134766
X	0.016840193262081755	2289	2.289	0.1944914106117681
X	0.017018321323068093	5398	5.398	0.14663172589489268
X	0.017033430938984986	5082	5.082	0.14965428548706702
X	0.01730866637463432	18056	18.056	0.09860085638435104
X	0.018170180340726594	5829	5.829	0.14607920415399658
X	0.017070614407176346	10540	10.54	0.11743643182777252
X	0.018069170051036348	11720	11.72	0.1155234626588018
X	0.016908838442848538	8250	8.25	0.12702424103561477
X	0.01810653976627386	214300	214.3	0.0438804524831135
X	0.017185478748268746	26720	26.72	0.08631939618959208
X	0.01702148582947858	25160	25.16	0.08778674444669535
X	0.01817812951144823	80499	80.499	0.060895645158681184
X	0.018089535769067865	304157	304.157	0.039034012954836446
X	0.017693289072597262	8200	8.2	0.129220546539214
X	0.01707697486326331	10681	10.681	0.11693190041401692
X	0.017055514890399803	7095	7.095	0.13395866527324715
X	0.016845165860694705	2790	2.79	0.18209178899425044
X	0.01720054496606299	20879	20.879	0.09374414898435292
X	0.017073021503984015	16323	16.323	0.1015087474495575
X	0.017026680220332244	11025	11.025	0.11558916518254095
X	0.017523900491498413	8300	8.3	0.12828746654985976
X	0.01708611001044893	51140	51.14	0.06938956392469252
X	0.017040333885907234	24100	24.1	0.08908823451272546
X	0.017860178188999796	1000716	1000.716	0.02613314318361763
X	0.016689593449520573	4394	4.394	0.15602537498013488
X	0.01708568936753223	41914	41.914	0.07414658142402233
X	0.016997849818093075	20235	20.235	0.09435471192397551
X	0.016798082929951653	2388	2.388	0.19160571834960238
X	0.01731261944304872	197460	197.46	0.04442504571068382
X	0.01789917442173429	180656	180.656	0.04627291616396759
X	0.017275112135300287	501650	501.65	0.032534191564900054
X	0.017159775314123372	57314	57.314	0.06689858920488713
X	0.01750054054122792	28184	28.184	0.08531320893020176
X	0.01698937515928566	5125	5.125	0.14910579143805608
X	0.017226897297711635	92455	92.455	0.05711614496711943
X	0.017976258324280125	937325	937.325	0.026767205508701167
X	0.018127540256470457	51294	51.294	0.07070076346952615
X	0.017418432257528815	13460	13.46	0.10897360912648567
X	0.017184285057902322	66420	66.42	0.0637202589935265
X	0.018019938889026105	275138	275.138	0.04030889683097882
X	0.017051359632427467	79467	79.467	0.059867446127892146
X	0.01715480615786222	13540	13.54	0.10820708978607552
X	0.01798710002303234	6399	6.399	0.14112902672904865
X	0.01699892850146996	10223	10.223	0.11847163865022062
X	0.017596192963730255	9817	9.817	0.12147306087543179
X	0.018076061064555643	318402	318.402	0.03843344705185598
X	0.01704396740990542	14246	14.246	0.10615959467682325
X	0.016895484555467955	5260	5.26	0.1475462929614791
X	0.01712907671505336	43013	43.013	0.07357180427043193
X	0.01716769838186498	19629	19.629	0.09563230711542453
X	0.01671053223777804	1750	1.75	0.21215376559951854
X	0.01708293052379581	70930	70.93	0.06221724907235823
X	0.017600164968101222	57559	57.559	0.06737019940520729
X	0.01710737164860091	53444	53.444	0.06840609171371735
X	0.01807576659872448	100452	100.452	0.05645633901269566
X	0.017227796713712887	95255	95.255	0.05655191541055994
X	0.016796706131601512	27671	27.671	0.0846706882350726
X	0.017077276011185298	86036	86.036	0.05833279628808047
X	0.017191361447355728	93902	93.902	0.05678214700233606
X	0.018158339052174434	300258	300.258	0.03925188440262509
X	0.017048719790934135	11090	11.09	0.11541264890686209
X	0.017194485692971842	97018	97.018	0.05617101749513188
X	0.017164699117256564	66952	66.952	0.06352688298342389
X	0.017189862940265102	67703	67.703	0.06332203064124571
X	0.017152873761571912	24361	24.361	0.08896392320477027
X	0.01773161898970522	8706	8.706	0.12675834958960547
X	0.017148214208490695	14708	14.708	0.10524991640498395
X	0.017072691794290284	13735	13.735	0.1075204596127638
X	0.01718500330300377	77080	77.08	0.060636757724909604
X	0.018012447217783292	179900	179.9	0.046435185621726244
X	0.017131622084037346	125643	125.643	0.05146990847407237
X	0.01716503168874665	170323	170.323	0.04653614855616593
X	0.017091935887497023	77679	77.679	0.060371116769125516
X	0.017814765233048386	98349	98.349	0.05658092611373981
X	0.016982286642691004	12515	12.515	0.11071040453906451
X	0.01788435356832496	184053	184.053	0.04597376965114791
X	0.017011889898995557	2436	2.436	0.19114301251831403
X	0.01697591069218935	20624	20.624	0.09371736312738853
X	0.01731592268105325	243403	243.403	0.04143556232634418
X	0.01801907889251114	42327	42.327	0.07522664607008693
X	0.016938358648251405	3687	3.687	0.1662389277063729
X	0.017109580017521834	29005	29.005	0.08386655937750984
X	0.018180983052773273	150332	150.332	0.04945254447795524
X	0.01720534104510091	44987	44.987	0.07258698386161622
X	0.017357655019008177	67233	67.233	0.06367508314907819
X	0.016835183504054312	6278	6.278	0.13893089956389798
X	0.017075936161957093	4977	4.977	0.15082463880121608
X	0.0169594359727284	6678	6.678	0.1364337019511324
X	0.01681528851689725	58792	58.792	0.06588633695409324
X	0.018132242733271874	37714	37.714	0.07833989332569184
X	0.017094831233298843	63615	63.615	0.06453100498265604
X	0.01706849026543872	2825	2.825	0.1821345004329109
X	0.017187858363230488	91764	91.764	0.05721586613178196
X	0.017904889698872137	17122	17.122	0.10150148256301442
X	0.01795026882272138	34046	34.046	0.08078578586424867
X	0.01706592567943178	11168	11.168	0.1151820529452536
X	0.017307317564407055	142249	142.249	0.04955188500383582
X	0.016857378867635695	2538	2.538	0.18797473300141485
X	0.01705685694275011	81340	81.34	0.05941073787667563
X	0.01725365977857103	35019	35.019	0.07898158227750435
X	0.016968305883174748	21738	21.738	0.09207455601514442
X	0.01796833790173145	31673	31.673	0.08278269645444768
X	0.017395326762602133	320935	320.935	0.037844708954806815
X	0.017089156065870553	17822	17.822	0.09861009840947405
X	0.017051203573367854	15316	15.316	0.10364218507617348
X	0.017398673719077107	113103	113.103	0.053581393775369614
X	0.017038848530439056	28496	28.496	0.08424654773974291
X	0.018119223525928048	434510	434.51	0.03467740592613536
X	0.01706356172477638	18697	18.697	0.09699870136972333
X	0.018137601999207796	125542	125.542	0.052472321023082406
X	0.01817276563279744	358193	358.193	0.03701984906745793
X	0.016970690697603964	10868	10.868	0.11601566386449222
X	0.017191608802135095	130906	130.906	0.050829868852701306
X	0.017094612646423417	19059	19.059	0.0964390783924547
X	0.017598990287500896	156130	156.13	0.048305931810383514
X	0.017172862041864776	34150	34.15	0.07952139517578309
X	0.017062123512959448	40106	40.106	0.0752098248592209
X	0.017982474904072298	1396	1.396	0.2344164123014289
X	0.01699049027851471	6287	6.287	0.13929028176623212
X	0.01709173570922625	5286	5.286	0.14787213271744487
X	0.01718247566168495	29925	29.925	0.08311585168684685
X	0.017821623726450193	22967	22.967	0.09189269739366906
X	0.018077477596803587	142821	142.821	0.05020904035694723
X	0.017060615365436904	19920	19.92	0.09496606709246493
X	0.01694053722638382	9498	9.498	0.12127324668528304
X	0.017063086128581727	14013	14.013	0.10678464797734202
X	0.017928255485855315	11297	11.297	0.11664289801499668
X	0.01819815966204814	3475	3.475	0.17365643749137596
X	0.0181839949415868	153593	153.593	0.049102766292958934
X	0.017979134417465986	54122	54.122	0.06925720818526078
X	0.017163066022127086	3958	3.958	0.1630696082020874
X	0.017004082860749236	3760	3.76	0.16536938929390455
X	0.01709293254070009	7754	7.754	0.13014583096125096
X	0.017887983204241332	537557	537.557	0.03216465905834708
X	0.016993337320531886	9521	9.521	0.1213012764758756
X	0.0171836890812428	21664	21.664	0.09256765309194834
X	0.017045708697554486	73866	73.866	0.06133713597487924
X	0.017200624319813833	18551	18.551	0.09751220155009133
X	0.017088502768789267	7396	7.396	0.1322012932247296
X	0.01669608783892111	7196	7.196	0.13238573432453646
X	0.01781702459791467	66763	66.763	0.06438229376651948
X	0.01738084722436241	58591	58.591	0.06669296271564278
X	0.016942550787187274	60953	60.953	0.06526212891869425
X	0.017123793777666076	12654	12.654	0.11060906460556459
X	0.017045672684564177	7976	7.976	0.12880804829019113
X	0.017910210844942136	46201	46.201	0.07291492626027564
X	0.018192911314395467	27750	27.75	0.08687198363970228
X	0.01810625010360339	96773	96.773	0.05719500515771464
X	0.016917665647818235	4269	4.269	0.15824795055438343
X	0.01711052396770002	100170	100.17	0.055484951624420166
X	0.017071013437019705	3959	3.959	0.1627638425357182
X	0.01741252116042583	60958	60.958	0.06585826963963716
X	0.017015592799093524	16113	16.113	0.10183339564456204
X	0.01702973607994302	5614	5.614	0.14475886514064804
X	0.017035501412859023	13832	13.832	0.10719058641761489
X	0.017036671513715403	81570	81.57	0.0593314227199829
X	0.01704744885954611	24415	24.415	0.08871578284981554
X	0.017081054120403213	49967	49.967	0.06992145620398407
X	0.017076969266970103	5920	5.92	0.14235182195528254
X	0.017066381059542533	172866	172.866	0.04621794362517178
X	0.01692982729122992	2334	2.334	0.19357557530766503
X	0.017062248461247258	3925	3.925	0.1632045266747236
X	0.01694638146725799	11417	11.417	0.11407093265643699
X	0.017049957652506244	24518	24.518	0.0885957225974648
X	0.01702460292085879	3417	3.417	0.17079585924523338
X	0.01698699165655629	14051	14.051	0.10652946514764183
X	0.017141779683701636	27671	27.671	0.08524659052486336
X	0.017398205072104392	126294	126.294	0.05164646197741214
X	0.01707536397210154	5436	5.436	0.14645251716351412
X	0.01688201962767187	2697	2.697	0.18429539550504287
X	0.01725740412798799	8488	8.488	0.12668446630200966
X	0.017804407933387566	233913	233.913	0.042379685138098706
X	0.018201439960324986	714733	714.733	0.029420765672495623
X	0.0170698185578056	26143	26.143	0.08675433010935513
X	0.01704783607914605	6666	6.666	0.1367523035124485
X	0.017106994660933718	212873	212.873	0.04315366770441342
X	0.017515831415012773	146821	146.821	0.04922822681981334
X	0.017017152137065658	2801	2.801	0.18246990988959483
X	0.017071031204857545	9564	9.564	0.1213035179879785
X	0.0171721793667941	50417	50.417	0.06983655462619386
X	0.018152644261116	369016	369.016	0.036640796005676095
X	0.01821308122102308	802518	802.518	0.028312369271013924
X	0.01753203327147195	130626	130.626	0.05119971607882063
X	0.017166510351836035	28879	28.879	0.08408140426301905
X	0.018071257405091242	8153	8.153	0.13038380268405675
X	0.017337757831655948	4611	4.611	0.155500791328068
X	0.01706640475345605	94697	94.697	0.05648527838712218
X	0.017117699166448907	16555	16.555	0.10112038974986409
X	0.017041586636735245	36070	36.07	0.0778851441721237
X	0.01820044477591494	122175	122.175	0.053011067664490194
X	0.01810990012086362	406154	406.154	0.03546024839512435
X	0.017105437615072064	7271	7.271	0.13299848122746818
X	0.01699791245344805	1995	1.995	0.20424474717027968
X	0.017032033419566023	26595	26.595	0.08619634193677941
X	0.017742546424379523	115358	115.358	0.05357835935618276
X	0.01711067042257141	36021	36.021	0.07802559460472412
X	0.01684395930216023	9325	9.325	0.12178630317228413
X	0.01699687499220616	1718	1.718	0.21467524815708783
X	0.017135458654048412	23276	23.276	0.09029474204299422
X	0.017073133028169332	6151	6.151	0.14053651045538215
X	0.018041694536941787	5000	5.0	0.15338013195522976
X	0.017021069486756423	4486	4.486	0.1559705218196074
X	0.017733320792271978	11453	11.453	0.11568852125612362
X	0.017066350085547424	12972	12.972	0.10957493878904553
X	0.017103905092952672	8852	8.852	0.12455216785713667
X	0.016977447885664677	6136	6.136	0.1403876868755924
X	0.01760706704402729	138098	138.098	0.05033072770308646
X	0.018139954163406993	286643	286.643	0.03985030181286596
X	0.017123086732661566	60030	60.03	0.06582707700504184
X	0.017190068772704722	42077	42.077	0.07420120339668042
X	0.017155074791370375	16943	16.943	0.10041550305915821
X	0.017101911480444314	8680	8.68	0.12536461646249794
X	0.018034529573492337	25855	25.855	0.08868566469868919
X	0.018079767578384625	157698	157.698	0.04857978099157679
X	0.017324301479447406	15666	15.666	0.10341079340058858
X	0.018021393943857794	44146	44.146	0.07418208012598272
X	0.01711626946341197	15091	15.091	0.10428704584413712
X	0.018221705655028526	695861	695.861	0.02969537700716148
X	0.017153470256738208	22736	22.736	0.09103589042424136
X	0.017448398513810744	77275	77.275	0.060893675143267545
X	0.017254842741618783	162744	162.744	0.047329873075031045
X	0.017928747062837296	57770	57.77	0.06770423847353721
X	0.01707303945978226	21853	21.853	0.0921014765534434
X	0.018201764148615008	340668	340.668	0.03766408147204409
X	0.017192755859614977	142123	142.123	0.049456917352888284
X	0.017080030764957503	24910	24.91	0.0881803138768757
X	0.01695702533209474	10695	10.695	0.116606553706861
X	0.01712588663406974	32706	32.706	0.08060126863186255
X	0.01708529078399926	13365	13.365	0.10853033341422147
X	0.017169592491276264	75995	75.995	0.06090575382024789
X	0.018156483382002986	385138	385.138	0.03612477259505555
X	0.018116587084155405	64627	64.627	0.06544665630778387
X	0.01771182954902095	111014	111.014	0.05423694565131044
X	0.017108040227358304	14763	14.763	0.10503689643484006
X	0.017074982754506866	8412	8.412	0.1266155375782052
X	0.016976588970432176	24588	24.588	0.088384425455926
X	0.016727746323263088	11390	11.39	0.11366791745671949
X	0.018230136481198218	172724	172.724	0.04725840694216696
X	0.018094610449352462	376942	376.942	0.03634334547269761
X	0.016795137967403506	2130	2.13	0.199037385123294
X	0.017558911062048092	826418	826.418	0.027696991883041377
X	0.01783775121042862	31089	31.089	0.08309555314683716
X	0.017086285825455204	63153	63.153	0.06467720250940086
X	0.017024421829751408	18206	18.206	0.09778808569128739
X	0.018090120579711107	54123	54.123	0.06939899801506076
X	0.018106201637881597	184443	184.443	0.046130524410523094
X	0.01705184702465082	3058	3.058	0.17732827208705026
X	0.01785441489907898	708232	708.232	0.029321759391581218
X	0.017168217523379676	22503	22.503	0.09137518354781649
X	0.017749511374374288	36315	36.315	0.07877107718970669
X	0.017069183829225926	18093	18.093	0.09807704916894071
X	0.01702996812768755	31650	31.65	0.08133548117528411
X	0.01677042128160345	2660	2.66	0.18473709580792624
X	0.017074665332463247	18806	18.806	0.0968319303139322
X	0.017269699144295332	11759	11.759	0.11366797606007017
X	0.018176290694375836	33953	33.953	0.08119744594389047
X	0.018227608366871446	87663	87.663	0.059243101495552924
X	0.017375799753724695	30075	30.075	0.08328748006154314
X	0.018118707892940306	350826	350.826	0.037240183921567055
X	0.017528292872466312	7003	7.003	0.13577458944042417
X	0.017166509440394724	31267	31.267	0.08188391416128739
X	0.01688608259614848	8111	8.111	0.1276884169176956
X	0.017064495542982845	13620	13.62	0.10780496979605968
X	0.01747544863492498	105546	105.546	0.05491135808742079
X	0.018162494610291697	1232810	1232.81	0.024514692495680415
X	0.017098581894796572	16655	16.655	0.10088001820176337
X	0.0180683226437391	160899	160.899	0.04824527994135036
X	0.0171788176859611	8554	8.554	0.126165711610008
X	0.017125356033881725	137733	137.733	0.04991148989084861
X	0.017350339259852993	141766	141.766	0.049649166475463635
X	0.017141869894544772	39099	39.099	0.07596802715525972
X	0.016929294269826645	7773	7.773	0.12962339603703618
X	0.017517442799211698	96405	96.405	0.05664006151527646
X	0.017061381710730242	30757	30.757	0.08216560844712459
X	0.017009617490668028	104129	104.129	0.054664781504858984
X	0.01814226413186047	412329	412.329	0.0353033459278351
X	0.017142069444850923	13119	13.119	0.1093253406786989
X	0.017004855918756074	32952	32.952	0.08021034651441852
X	0.017192820188919558	27364	27.364	0.08564904268243118
X	0.01814318500130322	159544	159.544	0.04844820423210847
X	0.017973485302474324	201872	201.872	0.044653146982885486
X	0.01805367208271814	73555	73.555	0.0626110686901133
X	0.017069855252259902	11613	11.613	0.11370035067199116
X	0.017479420168206177	349428	349.428	0.03684595125145753
X	0.017452071310382795	96557	96.557	0.05653981784662735
X	0.0181177707779047	329686	329.686	0.03801906380700779
X	0.017229817322631912	170727	170.727	0.046557839764962916
X	0.017590476367043292	100489	100.489	0.05593964321488133
X	0.018198949100866686	23458	23.458	0.09188651412804806
X	0.016842821535098557	125787	125.787	0.05115950838730073
X	0.017147766448087628	213592	213.592	0.043139409140594956
X	0.017708852600278456	81925	81.925	0.06001474857576348
X	0.01679293690416876	6236	6.236	0.13912553580005071
X	0.01688510826258681	21413	21.413	0.09238655595809953
X	0.01819712706399748	24086	24.086	0.09107783850098372
X	0.017123526979735862	82732	82.732	0.0591525175450228
X	0.017181932293723227	54458	54.458	0.06807747487956232
X	0.01685565411169887	7107	7.107	0.1333582123164932
X	0.017080125752810617	20235	20.235	0.09450670428979958
X	0.01712213992881254	107658	107.658	0.0541797809467109
X	0.01746473370694543	53933	53.933	0.06867032928491083
X	0.017594747473529137	32343	32.343	0.08163334248626575
X	0.01716702621268611	44232	44.232	0.07294342719066836
X	0.01704184820593794	25387	25.387	0.08755919697132877
X	0.017153783013793254	22159	22.159	0.09181985125709279
X	0.016858630825714947	5456	5.456	0.14565184141816181
X	0.016836752243295807	8023	8.023	0.12802865301929248
X	0.0178488881514694	206142	206.142	0.04423997717907967
X	0.017087107767541094	18249	18.249	0.09783100067512139
X	0.01725689152143175	80821	80.821	0.05976947619576586
X	0.0170097517073028	3965	3.965	0.16248686549350386
X	0.01707114913210037	5230	5.23	0.1483384337511592
X	0.017778794496442558	153257	153.257	0.0487708880238528
X	0.0180657182961265	139067	139.067	0.05064583269042469
X	0.017079551840396272	39884	39.884	0.07537475631350084
X	0.016841565453138353	5664	5.664	0.14379807455992802
X	0.017245372174591267	6473	6.473	0.1386294993386651
X	0.017252817209743244	32122	32.122	0.08128663541414614
X	0.0174152966457201	21367	21.367	0.09341051693679893
X	0.017442281196775947	94063	94.063	0.05702449999565523
X	0.017031066844803093	6666	6.666	0.13670744965253584
X	0.016986796712544067	15652	15.652	0.10276547092428455
X	0.01786858826888668	35224	35.224	0.07975364174738683
X	0.017024025291282253	13484	13.484	0.10808062248566658
X	0.017663621096553693	116810	116.81	0.05327619976836568
X	0.017886152793017782	7814	7.814	0.13178961112587245
X	0.01716467577673079	75810	75.81	0.06094943760108256
X	0.017184929705176235	194946	194.946	0.04450523963652462
X	0.017954282403404305	39176	39.176	0.07709910210742427
X	0.01820847970413222	40467	40.467	0.07662860021890693
X	0.018074882597098763	106087	106.087	0.055437602342615765
X	0.016868190305343936	2518	2.518	0.18851138697119738
X	0.017165465128300684	42045	42.045	0.07418459664481217
X	0.017164818281229195	21816	21.816	0.09231834885334617
X	0.017930598285859783	292391	292.391	0.039434553397129604
X	0.017188570651223086	69695	69.695	0.06271133513183395
X	0.01723948539308271	58718	58.718	0.06646367446684551
X	0.0171562271687904	95696	95.696	0.056386612042703156
X	0.01711452778306134	24729	24.729	0.08845440294201891
X	0.017282612580962697	4599	4.599	0.15547073856184265
X	0.016985975932703463	5561	5.561	0.14509280266495542
X	0.01681827737746123	22361	22.361	0.0909417756794793
X	0.01804767288277283	179665	179.665	0.04648568585692601
X	0.018232387898134212	479356	479.356	0.03363012344145762
X	0.017364727486509735	52678	52.678	0.06907903012033267
X	0.017105242851262588	13963	13.963	0.10699993211294899
X	0.017163823508272592	19515	19.515	0.09581095333822592
X	0.0181557925745059	567299	567.299	0.03174931780837998
X	0.017161022258890855	31625	31.625	0.08156506879286604
X	0.01709988535095478	13490	13.49	0.10822487253353902
X	0.01710364738006782	63634	63.634	0.06453567203331695
X	0.017164768744071177	62796	62.796	0.06489859955851403
X	0.01701496758178185	12223	12.223	0.11165660998689075
X	0.018144002368874097	13658	13.658	0.10992977232633924
X	0.017088286173836977	76414	76.414	0.06069811323075978
time for making epsilon is 2.374049186706543
epsilons are
[0.20312603173954322, 0.08820693123754353, 0.19188465724290046, 0.1780504201638374, 0.059277082764028086, 0.06210105821517584, 0.0644430647679444, 0.06751801842153655, 0.09995465101124029, 0.07515844531676295, 0.08356822304454019, 0.06565437268627831, 0.12148325881837359, 0.04174094281790122, 0.09132544367488156, 0.07488901277968911, 0.06987030142066393, 0.06836724383666969, 0.048424414989232015, 0.049334102320882395, 0.06552629732620668, 0.04559855697970739, 0.10484868057096576, 0.10381970504210948, 0.0756001941596862, 0.06523601999450215, 0.06045299968958537, 0.09665186807708433, 0.048032688037326916, 0.024931152821782605, 0.10488123432905642, 0.027333942775964255, 0.11976963102047908, 0.08982555744302077, 0.06983658684645001, 0.03591188776760641, 0.06533328453784176, 0.18753963271108512, 0.14924203181778922, 0.1660653761773683, 0.17721576693504504, 0.27434415276499013, 0.22324098631130496, 0.16553596515602298, 0.26668509657107436, 0.20746623365801903, 0.19455204287315273, 0.12435874308854025, 0.1742931811221333, 0.13844026660019004, 0.10553953193928545, 0.21286385320513274, 0.1550350097112635, 0.1507477810009163, 0.14510998213033804, 0.1400836598392008, 0.21475953572103443, 0.1433735338263974, 0.1771028883443682, 0.18663948229484836, 0.15917437493834827, 0.21218561097042962, 0.16744184742351934, 0.09871843390559773, 0.16431314834581184, 0.16528780422349557, 0.15876613861356137, 0.14275886420067263, 0.13605344616188736, 0.16530760851527612, 0.18597838614858336, 0.14843812165627493, 0.16827280022911253, 0.17462995209552581, 0.1632735536784455, 0.11736022032063533, 0.26920865103094743, 0.21328172787551367, 0.14881530648065316, 0.10119989253987761, 0.19819698469896668, 0.2245376676318722, 0.17143680766052985, 0.23118423233858185, 0.21808184156104454, 0.17897299361042693, 0.2100972969827552, 0.23728350647311533, 0.16647264679670049, 0.15246744830884065, 0.14234301743616778, 0.18300594883384397, 0.20403518837729231, 0.1265039706467502, 0.15262636133558707, 0.13105019494311496, 0.17324676466177769, 0.18487113673240718, 0.14859688850407582, 0.1370319389157117, 0.11130821263350693, 0.17944786012246666, 0.14114314939428538, 0.19315851480311752, 0.10757555433429324, 0.19804769292917113, 0.1652492072146795, 0.21743830673338876, 0.19314440361938845, 0.23865112032617922, 0.1414540910005738, 0.2492176912398843, 0.20180937265207444, 0.18851448528552528, 0.13899708483981538, 0.19590368355751855, 0.17684446912949786, 0.2660257454954357, 0.12108463889722919, 0.23740754008964143, 0.11174940159389485, 0.20323470128927182, 0.190728434295629, 0.16341016393115979, 0.15244853254357654, 0.22616415205901166, 0.19617434466305442, 0.14379287478028788, 0.12140966162533694, 0.16896131119192961, 0.09269701600622891, 0.2575365573920554, 0.17815578589371034, 0.21862802257984268, 0.19117599005502936, 0.17656729718430936, 0.2003686793367174, 0.1689069657545495, 0.16598968763339028, 0.1924470649786289, 0.16539253652404506, 0.2078193975704875, 0.11675608079429571, 0.1441722575635476, 0.14451651330799825, 0.19679578498862335, 0.188678619257331, 0.2086124617058453, 0.18008026215582668, 0.20887446526331954, 0.18454564725194653, 0.1626600364207693, 0.1675662461357761, 0.187138399152508, 0.15923601740585588, 0.12325055452868841, 0.1499304580085421, 0.1818019135207022, 0.1845048913614956, 0.26068047527379784, 0.10609307400245206, 0.24121697242384985, 0.11329198994942792, 0.27756116853742474, 0.1613286616092743, 0.154967034139935, 0.21734566040089823, 0.17184279513818876, 0.1580532603703193, 0.2460645762486321, 0.22225581191458726, 0.19195297799617453, 0.13695399861081448, 0.2272327514736651, 0.15547163687543022, 0.1720482061759648, 0.14459238171586655, 0.16847277066807856, 0.18740594285222692, 0.16642808859929897, 0.14005303438821598, 0.22100916467740153, 0.18043339435302752, 0.18433619838867726, 0.1529414209763699, 0.14613025451977193, 0.16906360079424787, 0.17516379155386674, 0.13189938705125998, 0.13040924892246164, 0.17391036806314739, 0.246796385628933, 0.17409512500443544, 0.1796714311523255, 0.15653152283292365, 0.17234957437480708, 0.16463984892633607, 0.18892172173900015, 0.15702746639647028, 0.23788098760509782, 0.1454902707417445, 0.16276254844807253, 0.12164644767441123, 0.1854824156938516, 0.1276567708846644, 0.16703533873904286, 0.19352021944609285, 0.17238859814134055, 0.2190450936606188, 0.12667265856053153, 0.14372281093086497, 0.1932913639939733, 0.17098439356939982, 0.11838617609681194, 0.2695273829666294, 0.2513256279913786, 0.2864608084536645, 0.22757012327905837, 0.11686109559018573, 0.15238953458499666, 0.17491144687368004, 0.2642228135006257, 0.1317589273294771, 0.191849081074006, 0.1617568427726046, 0.1554105895279162, 0.16092803584562382, 0.22555962477961028, 0.15093129681849413, 0.23394308126910537, 0.17893892851819443, 0.1919655147048638, 0.19081810745653235, 0.17879282296960416, 0.15412553134908302, 0.14916357165541008, 0.15494601110124445, 0.15498930607354514, 0.191598066293958, 0.12792450285123136, 0.13215754989132691, 0.11615975134803912, 0.1353044560692532, 0.15995630515941103, 0.11385702251438902, 0.10280869424777289, 0.10421179003291688, 0.15047048112156614, 0.04769118681323953, 0.05729681239636067, 0.06226480483661154, 0.0672337651892848, 0.19271465067816057, 0.1128521934058354, 0.03598586079378149, 0.036529884731614004, 0.17393557798944204, 0.08301793725008814, 0.0607609574205682, 0.10038276043826708, 0.09318270465122505, 0.08148965138346491, 0.12406227334288222, 0.12606775683035945, 0.11795440485100141, 0.10002038432073157, 0.11135626617097588, 0.14386190805735374, 0.08290629946178935, 0.09496779950201628, 0.042515196534975104, 0.06621758648917872, 0.1159347739396627, 0.07717515172348485, 0.15416297674050253, 0.09902417774543947, 0.07036265149510379, 0.06585844500877626, 0.048671006145034605, 0.202106165334473, 0.1384726385873275, 0.06421319848924027, 0.0636555480565484, 0.07487284714580292, 0.17841924353325875, 0.045064302992315466, 0.10264159062091417, 0.09484235041193394, 0.06004828403954173, 0.1771871418915033, 0.0661749917414433, 0.06190820617781231, 0.03833187116405682, 0.0682172287891411, 0.07059007509358346, 0.09159095013598544, 0.14029274119239432, 0.0847933499716349, 0.12389532119814683, 0.10204251854732141, 0.07268807972336552, 0.08557273436646533, 0.13188727228068556, 0.13425967056004995, 0.039313995784503904, 0.12698014576172362, 0.20265100505120467, 0.13191560463586033, 0.04926491040104025, 0.08589163844749347, 0.10020522384568305, 0.113763904245205, 0.10779600060138333, 0.10520459267024324, 0.033740882438589845, 0.07515501710182855, 0.10184482505756395, 0.052446317777346554, 0.0521360969413162, 0.1397300532897835, 0.056034127766695654, 0.05336243191951015, 0.03537226143444623, 0.1195926649478737, 0.18999085677079453, 0.11153844292323228, 0.0976357141927688, 0.07819121403539618, 0.08962741260569147, 0.19935906640445114, 0.0855782096162177, 0.051146193161471225, 0.04714477767426956, 0.1467495103565867, 0.04680054059931328, 0.1002270645798971, 0.0959271945915547, 0.10930734516611554, 0.16118236084463816, 0.04661058367718617, 0.07391935381769595, 0.1179133926078067, 0.08796277927849296, 0.12817842042245967, 0.03736780693490538, 0.08271959357339462, 0.045215572412134766, 0.1944914106117681, 0.14663172589489268, 0.14965428548706702, 0.09860085638435104, 0.14607920415399658, 0.11743643182777252, 0.1155234626588018, 0.12702424103561477, 0.0438804524831135, 0.08631939618959208, 0.08778674444669535, 0.060895645158681184, 0.039034012954836446, 0.129220546539214, 0.11693190041401692, 0.13395866527324715, 0.18209178899425044, 0.09374414898435292, 0.1015087474495575, 0.11558916518254095, 0.12828746654985976, 0.06938956392469252, 0.08908823451272546, 0.02613314318361763, 0.15602537498013488, 0.07414658142402233, 0.09435471192397551, 0.19160571834960238, 0.04442504571068382, 0.04627291616396759, 0.032534191564900054, 0.06689858920488713, 0.08531320893020176, 0.14910579143805608, 0.05711614496711943, 0.026767205508701167, 0.07070076346952615, 0.10897360912648567, 0.0637202589935265, 0.04030889683097882, 0.059867446127892146, 0.10820708978607552, 0.14112902672904865, 0.11847163865022062, 0.12147306087543179, 0.03843344705185598, 0.10615959467682325, 0.1475462929614791, 0.07357180427043193, 0.09563230711542453, 0.21215376559951854, 0.06221724907235823, 0.06737019940520729, 0.06840609171371735, 0.05645633901269566, 0.05655191541055994, 0.0846706882350726, 0.05833279628808047, 0.05678214700233606, 0.03925188440262509, 0.11541264890686209, 0.05617101749513188, 0.06352688298342389, 0.06332203064124571, 0.08896392320477027, 0.12675834958960547, 0.10524991640498395, 0.1075204596127638, 0.060636757724909604, 0.046435185621726244, 0.05146990847407237, 0.04653614855616593, 0.060371116769125516, 0.05658092611373981, 0.11071040453906451, 0.04597376965114791, 0.19114301251831403, 0.09371736312738853, 0.04143556232634418, 0.07522664607008693, 0.1662389277063729, 0.08386655937750984, 0.04945254447795524, 0.07258698386161622, 0.06367508314907819, 0.13893089956389798, 0.15082463880121608, 0.1364337019511324, 0.06588633695409324, 0.07833989332569184, 0.06453100498265604, 0.1821345004329109, 0.05721586613178196, 0.10150148256301442, 0.08078578586424867, 0.1151820529452536, 0.04955188500383582, 0.18797473300141485, 0.05941073787667563, 0.07898158227750435, 0.09207455601514442, 0.08278269645444768, 0.037844708954806815, 0.09861009840947405, 0.10364218507617348, 0.053581393775369614, 0.08424654773974291, 0.03467740592613536, 0.09699870136972333, 0.052472321023082406, 0.03701984906745793, 0.11601566386449222, 0.050829868852701306, 0.0964390783924547, 0.048305931810383514, 0.07952139517578309, 0.0752098248592209, 0.2344164123014289, 0.13929028176623212, 0.14787213271744487, 0.08311585168684685, 0.09189269739366906, 0.05020904035694723, 0.09496606709246493, 0.12127324668528304, 0.10678464797734202, 0.11664289801499668, 0.17365643749137596, 0.049102766292958934, 0.06925720818526078, 0.1630696082020874, 0.16536938929390455, 0.13014583096125096, 0.03216465905834708, 0.1213012764758756, 0.09256765309194834, 0.06133713597487924, 0.09751220155009133, 0.1322012932247296, 0.13238573432453646, 0.06438229376651948, 0.06669296271564278, 0.06526212891869425, 0.11060906460556459, 0.12880804829019113, 0.07291492626027564, 0.08687198363970228, 0.05719500515771464, 0.15824795055438343, 0.055484951624420166, 0.1627638425357182, 0.06585826963963716, 0.10183339564456204, 0.14475886514064804, 0.10719058641761489, 0.0593314227199829, 0.08871578284981554, 0.06992145620398407, 0.14235182195528254, 0.04621794362517178, 0.19357557530766503, 0.1632045266747236, 0.11407093265643699, 0.0885957225974648, 0.17079585924523338, 0.10652946514764183, 0.08524659052486336, 0.05164646197741214, 0.14645251716351412, 0.18429539550504287, 0.12668446630200966, 0.042379685138098706, 0.029420765672495623, 0.08675433010935513, 0.1367523035124485, 0.04315366770441342, 0.04922822681981334, 0.18246990988959483, 0.1213035179879785, 0.06983655462619386, 0.036640796005676095, 0.028312369271013924, 0.05119971607882063, 0.08408140426301905, 0.13038380268405675, 0.155500791328068, 0.05648527838712218, 0.10112038974986409, 0.0778851441721237, 0.053011067664490194, 0.03546024839512435, 0.13299848122746818, 0.20424474717027968, 0.08619634193677941, 0.05357835935618276, 0.07802559460472412, 0.12178630317228413, 0.21467524815708783, 0.09029474204299422, 0.14053651045538215, 0.15338013195522976, 0.1559705218196074, 0.11568852125612362, 0.10957493878904553, 0.12455216785713667, 0.1403876868755924, 0.05033072770308646, 0.03985030181286596, 0.06582707700504184, 0.07420120339668042, 0.10041550305915821, 0.12536461646249794, 0.08868566469868919, 0.04857978099157679, 0.10341079340058858, 0.07418208012598272, 0.10428704584413712, 0.02969537700716148, 0.09103589042424136, 0.060893675143267545, 0.047329873075031045, 0.06770423847353721, 0.0921014765534434, 0.03766408147204409, 0.049456917352888284, 0.0881803138768757, 0.116606553706861, 0.08060126863186255, 0.10853033341422147, 0.06090575382024789, 0.03612477259505555, 0.06544665630778387, 0.05423694565131044, 0.10503689643484006, 0.1266155375782052, 0.088384425455926, 0.11366791745671949, 0.04725840694216696, 0.03634334547269761, 0.199037385123294, 0.027696991883041377, 0.08309555314683716, 0.06467720250940086, 0.09778808569128739, 0.06939899801506076, 0.046130524410523094, 0.17732827208705026, 0.029321759391581218, 0.09137518354781649, 0.07877107718970669, 0.09807704916894071, 0.08133548117528411, 0.18473709580792624, 0.0968319303139322, 0.11366797606007017, 0.08119744594389047, 0.059243101495552924, 0.08328748006154314, 0.037240183921567055, 0.13577458944042417, 0.08188391416128739, 0.1276884169176956, 0.10780496979605968, 0.05491135808742079, 0.024514692495680415, 0.10088001820176337, 0.04824527994135036, 0.126165711610008, 0.04991148989084861, 0.049649166475463635, 0.07596802715525972, 0.12962339603703618, 0.05664006151527646, 0.08216560844712459, 0.054664781504858984, 0.0353033459278351, 0.1093253406786989, 0.08021034651441852, 0.08564904268243118, 0.04844820423210847, 0.044653146982885486, 0.0626110686901133, 0.11370035067199116, 0.03684595125145753, 0.05653981784662735, 0.03801906380700779, 0.046557839764962916, 0.05593964321488133, 0.09188651412804806, 0.05115950838730073, 0.043139409140594956, 0.06001474857576348, 0.13912553580005071, 0.09238655595809953, 0.09107783850098372, 0.0591525175450228, 0.06807747487956232, 0.1333582123164932, 0.09450670428979958, 0.0541797809467109, 0.06867032928491083, 0.08163334248626575, 0.07294342719066836, 0.08755919697132877, 0.09181985125709279, 0.14565184141816181, 0.12802865301929248, 0.04423997717907967, 0.09783100067512139, 0.05976947619576586, 0.16248686549350386, 0.1483384337511592, 0.0487708880238528, 0.05064583269042469, 0.07537475631350084, 0.14379807455992802, 0.1386294993386651, 0.08128663541414614, 0.09341051693679893, 0.05702449999565523, 0.13670744965253584, 0.10276547092428455, 0.07975364174738683, 0.10808062248566658, 0.05327619976836568, 0.13178961112587245, 0.06094943760108256, 0.04450523963652462, 0.07709910210742427, 0.07662860021890693, 0.055437602342615765, 0.18851138697119738, 0.07418459664481217, 0.09231834885334617, 0.039434553397129604, 0.06271133513183395, 0.06646367446684551, 0.056386612042703156, 0.08845440294201891, 0.15547073856184265, 0.14509280266495542, 0.0909417756794793, 0.04648568585692601, 0.03363012344145762, 0.06907903012033267, 0.10699993211294899, 0.09581095333822592, 0.03174931780837998, 0.08156506879286604, 0.10822487253353902, 0.06453567203331695, 0.06489859955851403, 0.11165660998689075, 0.10992977232633924, 0.06069811323075978]
0.09602304544274079
Making ranges
torch.Size([31387, 2])
We keep 6.21e+06/4.24e+08 =  1% of the original kernel matrix.

torch.Size([4584, 2])
We keep 2.14e+05/4.12e+06 =  5% of the original kernel matrix.

torch.Size([12973, 2])
We keep 1.23e+06/4.18e+07 =  2% of the original kernel matrix.

torch.Size([35501, 2])
We keep 1.11e+07/6.16e+08 =  1% of the original kernel matrix.

torch.Size([34687, 2])
We keep 8.04e+06/5.11e+08 =  1% of the original kernel matrix.

torch.Size([5377, 2])
We keep 2.87e+05/5.90e+06 =  4% of the original kernel matrix.

torch.Size([13892, 2])
We keep 1.39e+06/5.00e+07 =  2% of the original kernel matrix.

torch.Size([6318, 2])
We keep 5.19e+05/9.43e+06 =  5% of the original kernel matrix.

torch.Size([14888, 2])
We keep 1.62e+06/6.33e+07 =  2% of the original kernel matrix.

torch.Size([117153, 2])
We keep 1.82e+08/6.72e+09 =  2% of the original kernel matrix.

torch.Size([61870, 2])
We keep 2.23e+07/1.69e+09 =  1% of the original kernel matrix.

torch.Size([107316, 2])
We keep 6.30e+07/5.13e+09 =  1% of the original kernel matrix.

torch.Size([58876, 2])
We keep 1.94e+07/1.48e+09 =  1% of the original kernel matrix.

torch.Size([97073, 2])
We keep 5.12e+07/4.08e+09 =  1% of the original kernel matrix.

torch.Size([56168, 2])
We keep 1.76e+07/1.32e+09 =  1% of the original kernel matrix.

torch.Size([86290, 2])
We keep 4.08e+07/3.16e+09 =  1% of the original kernel matrix.

torch.Size([52435, 2])
We keep 1.58e+07/1.16e+09 =  1% of the original kernel matrix.

torch.Size([25837, 2])
We keep 1.59e+07/3.28e+08 =  4% of the original kernel matrix.

torch.Size([29554, 2])
We keep 6.14e+06/3.73e+08 =  1% of the original kernel matrix.

torch.Size([53379, 2])
We keep 1.15e+08/1.62e+09 =  7% of the original kernel matrix.

torch.Size([41999, 2])
We keep 1.18e+07/8.30e+08 =  1% of the original kernel matrix.

torch.Size([42328, 2])
We keep 1.51e+07/8.49e+08 =  1% of the original kernel matrix.

torch.Size([37798, 2])
We keep 9.09e+06/6.00e+08 =  1% of the original kernel matrix.

torch.Size([85383, 2])
We keep 8.23e+07/4.06e+09 =  2% of the original kernel matrix.

torch.Size([52398, 2])
We keep 1.78e+07/1.31e+09 =  1% of the original kernel matrix.

torch.Size([17763, 2])
We keep 2.38e+06/9.85e+07 =  2% of the original kernel matrix.

torch.Size([24693, 2])
We keep 3.91e+06/2.05e+08 =  1% of the original kernel matrix.

torch.Size([347683, 2])
We keep 5.94e+08/5.56e+10 =  1% of the original kernel matrix.

torch.Size([110920, 2])
We keep 5.51e+07/4.86e+09 =  1% of the original kernel matrix.

torch.Size([33677, 2])
We keep 9.01e+06/5.04e+08 =  1% of the original kernel matrix.

torch.Size([33603, 2])
We keep 7.38e+06/4.62e+08 =  1% of the original kernel matrix.

torch.Size([59893, 2])
We keep 2.77e+07/1.73e+09 =  1% of the original kernel matrix.

torch.Size([44350, 2])
We keep 1.23e+07/8.56e+08 =  1% of the original kernel matrix.

torch.Size([76008, 2])
We keep 3.54e+07/2.54e+09 =  1% of the original kernel matrix.

torch.Size([49455, 2])
We keep 1.44e+07/1.04e+09 =  1% of the original kernel matrix.

torch.Size([77607, 2])
We keep 5.01e+07/3.19e+09 =  1% of the original kernel matrix.

torch.Size([49970, 2])
We keep 1.61e+07/1.16e+09 =  1% of the original kernel matrix.

torch.Size([216874, 2])
We keep 2.98e+08/2.27e+10 =  1% of the original kernel matrix.

torch.Size([86745, 2])
We keep 3.71e+07/3.10e+09 =  1% of the original kernel matrix.

torch.Size([210646, 2])
We keep 2.09e+08/2.04e+10 =  1% of the original kernel matrix.

torch.Size([85650, 2])
We keep 3.52e+07/2.94e+09 =  1% of the original kernel matrix.

torch.Size([87619, 2])
We keep 6.32e+07/3.66e+09 =  1% of the original kernel matrix.

torch.Size([52755, 2])
We keep 1.65e+07/1.25e+09 =  1% of the original kernel matrix.

torch.Size([261430, 2])
We keep 3.24e+08/3.65e+10 =  0% of the original kernel matrix.

torch.Size([97611, 2])
We keep 4.51e+07/3.93e+09 =  1% of the original kernel matrix.

torch.Size([22907, 2])
We keep 5.06e+06/2.19e+08 =  2% of the original kernel matrix.

torch.Size([27877, 2])
We keep 5.30e+06/3.05e+08 =  1% of the original kernel matrix.

torch.Size([24424, 2])
We keep 6.33e+06/2.34e+08 =  2% of the original kernel matrix.

torch.Size([28866, 2])
We keep 5.36e+06/3.15e+08 =  1% of the original kernel matrix.

torch.Size([43130, 2])
We keep 6.30e+07/1.55e+09 =  4% of the original kernel matrix.

torch.Size([36725, 2])
We keep 1.20e+07/8.11e+08 =  1% of the original kernel matrix.

torch.Size([92747, 2])
We keep 4.91e+07/3.81e+09 =  1% of the original kernel matrix.

torch.Size([54671, 2])
We keep 1.71e+07/1.27e+09 =  1% of the original kernel matrix.

torch.Size([114488, 2])
We keep 1.04e+08/6.66e+09 =  1% of the original kernel matrix.

torch.Size([61402, 2])
We keep 2.20e+07/1.68e+09 =  1% of the original kernel matrix.

torch.Size([27824, 2])
We keep 9.68e+06/3.58e+08 =  2% of the original kernel matrix.

torch.Size([30190, 2])
We keep 6.35e+06/3.90e+08 =  1% of the original kernel matrix.

torch.Size([198582, 2])
We keep 3.29e+08/2.66e+10 =  1% of the original kernel matrix.

torch.Size([83292, 2])
We keep 3.93e+07/3.36e+09 =  1% of the original kernel matrix.

torch.Size([1895521, 2])
We keep 1.00e+10/1.38e+12 =  0% of the original kernel matrix.

torch.Size([272926, 2])
We keep 2.40e+08/2.42e+10 =  0% of the original kernel matrix.

torch.Size([21907, 2])
We keep 7.19e+06/2.20e+08 =  3% of the original kernel matrix.

torch.Size([26941, 2])
We keep 5.00e+06/3.05e+08 =  1% of the original kernel matrix.

torch.Size([1401340, 2])
We keep 6.62e+09/7.91e+11 =  0% of the original kernel matrix.

torch.Size([234715, 2])
We keep 1.82e+08/1.83e+10 =  0% of the original kernel matrix.

torch.Size([16729, 2])
We keep 2.94e+06/1.08e+08 =  2% of the original kernel matrix.

torch.Size([23467, 2])
We keep 4.07e+06/2.14e+08 =  1% of the original kernel matrix.

torch.Size([35091, 2])
We keep 1.26e+07/6.05e+08 =  2% of the original kernel matrix.

torch.Size([34816, 2])
We keep 7.94e+06/5.07e+08 =  1% of the original kernel matrix.

torch.Size([73095, 2])
We keep 3.79e+07/2.50e+09 =  1% of the original kernel matrix.

torch.Size([48436, 2])
We keep 1.44e+07/1.03e+09 =  1% of the original kernel matrix.

torch.Size([582390, 2])
We keep 1.35e+09/1.51e+11 =  0% of the original kernel matrix.

torch.Size([144885, 2])
We keep 8.55e+07/8.01e+09 =  1% of the original kernel matrix.

torch.Size([94935, 2])
We keep 5.60e+07/4.22e+09 =  1% of the original kernel matrix.

torch.Size([55835, 2])
We keep 1.80e+07/1.34e+09 =  1% of the original kernel matrix.

torch.Size([5473, 2])
We keep 3.62e+05/6.64e+06 =  5% of the original kernel matrix.

torch.Size([13818, 2])
We keep 1.47e+06/5.31e+07 =  2% of the original kernel matrix.

torch.Size([10419, 2])
We keep 9.13e+05/2.62e+07 =  3% of the original kernel matrix.

torch.Size([18295, 2])
We keep 2.35e+06/1.05e+08 =  2% of the original kernel matrix.

torch.Size([7359, 2])
We keep 5.65e+05/1.34e+07 =  4% of the original kernel matrix.

torch.Size([15485, 2])
We keep 1.84e+06/7.53e+07 =  2% of the original kernel matrix.

torch.Size([6741, 2])
We keep 3.96e+05/9.60e+06 =  4% of the original kernel matrix.

torch.Size([15303, 2])
We keep 1.63e+06/6.38e+07 =  2% of the original kernel matrix.

torch.Size([2030, 2])
We keep 5.23e+04/6.15e+05 =  8% of the original kernel matrix.

torch.Size([9458, 2])
We keep 6.33e+05/1.62e+07 =  3% of the original kernel matrix.

torch.Size([3358, 2])
We keep 1.25e+05/2.09e+06 =  5% of the original kernel matrix.

torch.Size([11359, 2])
We keep 9.51e+05/2.98e+07 =  3% of the original kernel matrix.

torch.Size([7549, 2])
We keep 5.87e+05/1.41e+07 =  4% of the original kernel matrix.

torch.Size([15813, 2])
We keep 1.89e+06/7.74e+07 =  2% of the original kernel matrix.

torch.Size([2040, 2])
We keep 5.65e+04/7.94e+05 =  7% of the original kernel matrix.

torch.Size([9492, 2])
We keep 7.08e+05/1.84e+07 =  3% of the original kernel matrix.

torch.Size([4260, 2])
We keep 1.91e+05/3.55e+06 =  5% of the original kernel matrix.

torch.Size([12564, 2])
We keep 1.16e+06/3.88e+07 =  2% of the original kernel matrix.

torch.Size([5007, 2])
We keep 2.57e+05/5.36e+06 =  4% of the original kernel matrix.

torch.Size([13358, 2])
We keep 1.33e+06/4.77e+07 =  2% of the original kernel matrix.

torch.Size([15329, 2])
We keep 2.10e+06/7.90e+07 =  2% of the original kernel matrix.

torch.Size([22189, 2])
We keep 3.59e+06/1.83e+08 =  1% of the original kernel matrix.

torch.Size([6636, 2])
We keep 4.88e+05/1.04e+07 =  4% of the original kernel matrix.

torch.Size([14916, 2])
We keep 1.72e+06/6.64e+07 =  2% of the original kernel matrix.

torch.Size([12144, 2])
We keep 1.38e+06/4.30e+07 =  3% of the original kernel matrix.

torch.Size([19673, 2])
We keep 2.83e+06/1.35e+08 =  2% of the original kernel matrix.

torch.Size([22763, 2])
We keep 4.34e+06/2.11e+08 =  2% of the original kernel matrix.

torch.Size([27757, 2])
We keep 5.20e+06/2.99e+08 =  1% of the original kernel matrix.

torch.Size([3970, 2])
We keep 1.72e+05/3.01e+06 =  5% of the original kernel matrix.

torch.Size([12245, 2])
We keep 1.10e+06/3.57e+07 =  3% of the original kernel matrix.

torch.Size([9141, 2])
We keep 8.24e+05/2.08e+07 =  3% of the original kernel matrix.

torch.Size([17334, 2])
We keep 2.14e+06/9.41e+07 =  2% of the original kernel matrix.

torch.Size([9761, 2])
We keep 9.18e+05/2.41e+07 =  3% of the original kernel matrix.

torch.Size([17763, 2])
We keep 2.25e+06/1.01e+08 =  2% of the original kernel matrix.

torch.Size([11006, 2])
We keep 1.23e+06/3.27e+07 =  3% of the original kernel matrix.

torch.Size([18827, 2])
We keep 2.62e+06/1.18e+08 =  2% of the original kernel matrix.

torch.Size([11849, 2])
We keep 1.22e+06/3.84e+07 =  3% of the original kernel matrix.

torch.Size([19479, 2])
We keep 2.70e+06/1.28e+08 =  2% of the original kernel matrix.

torch.Size([4122, 2])
We keep 1.65e+05/2.97e+06 =  5% of the original kernel matrix.

torch.Size([12515, 2])
We keep 1.09e+06/3.55e+07 =  3% of the original kernel matrix.

torch.Size([10915, 2])
We keep 1.10e+06/3.34e+07 =  3% of the original kernel matrix.

torch.Size([18614, 2])
We keep 2.56e+06/1.19e+08 =  2% of the original kernel matrix.

torch.Size([6737, 2])
We keep 4.15e+05/9.28e+06 =  4% of the original kernel matrix.

torch.Size([15234, 2])
We keep 1.63e+06/6.28e+07 =  2% of the original kernel matrix.

torch.Size([5490, 2])
We keep 3.47e+05/6.86e+06 =  5% of the original kernel matrix.

torch.Size([13783, 2])
We keep 1.45e+06/5.40e+07 =  2% of the original kernel matrix.

torch.Size([8585, 2])
We keep 6.79e+05/1.78e+07 =  3% of the original kernel matrix.

torch.Size([16814, 2])
We keep 2.06e+06/8.70e+07 =  2% of the original kernel matrix.

torch.Size([3959, 2])
We keep 1.81e+05/3.17e+06 =  5% of the original kernel matrix.

torch.Size([12089, 2])
We keep 1.12e+06/3.67e+07 =  3% of the original kernel matrix.

torch.Size([7387, 2])
We keep 5.30e+05/1.30e+07 =  4% of the original kernel matrix.

torch.Size([15571, 2])
We keep 1.82e+06/7.44e+07 =  2% of the original kernel matrix.

torch.Size([27272, 2])
We keep 6.23e+06/3.45e+08 =  1% of the original kernel matrix.

torch.Size([30498, 2])
We keep 6.39e+06/3.83e+08 =  1% of the original kernel matrix.

torch.Size([8040, 2])
We keep 5.82e+05/1.46e+07 =  3% of the original kernel matrix.

torch.Size([16325, 2])
We keep 1.89e+06/7.88e+07 =  2% of the original kernel matrix.

torch.Size([7599, 2])
We keep 5.85e+05/1.40e+07 =  4% of the original kernel matrix.

torch.Size([15833, 2])
We keep 1.89e+06/7.71e+07 =  2% of the original kernel matrix.

torch.Size([8520, 2])
We keep 6.88e+05/1.81e+07 =  3% of the original kernel matrix.

torch.Size([16606, 2])
We keep 2.05e+06/8.76e+07 =  2% of the original kernel matrix.

torch.Size([10684, 2])
We keep 1.41e+06/3.41e+07 =  4% of the original kernel matrix.

torch.Size([18422, 2])
We keep 2.64e+06/1.20e+08 =  2% of the original kernel matrix.

torch.Size([12811, 2])
We keep 1.38e+06/4.63e+07 =  2% of the original kernel matrix.

torch.Size([20266, 2])
We keep 2.93e+06/1.40e+08 =  2% of the original kernel matrix.

torch.Size([7736, 2])
We keep 5.66e+05/1.42e+07 =  3% of the original kernel matrix.

torch.Size([15864, 2])
We keep 1.88e+06/7.77e+07 =  2% of the original kernel matrix.

torch.Size([5354, 2])
We keep 3.97e+05/7.01e+06 =  5% of the original kernel matrix.

torch.Size([13652, 2])
We keep 1.49e+06/5.45e+07 =  2% of the original kernel matrix.

torch.Size([10227, 2])
We keep 9.90e+05/2.71e+07 =  3% of the original kernel matrix.

torch.Size([17988, 2])
We keep 2.41e+06/1.07e+08 =  2% of the original kernel matrix.

torch.Size([7356, 2])
We keep 5.91e+05/1.28e+07 =  4% of the original kernel matrix.

torch.Size([15608, 2])
We keep 1.82e+06/7.36e+07 =  2% of the original kernel matrix.

torch.Size([6648, 2])
We keep 4.24e+05/9.99e+06 =  4% of the original kernel matrix.

torch.Size([14977, 2])
We keep 1.64e+06/6.51e+07 =  2% of the original kernel matrix.

torch.Size([8024, 2])
We keep 6.05e+05/1.53e+07 =  3% of the original kernel matrix.

torch.Size([16198, 2])
We keep 1.93e+06/8.07e+07 =  2% of the original kernel matrix.

torch.Size([16866, 2])
We keep 3.83e+06/1.11e+08 =  3% of the original kernel matrix.

torch.Size([23508, 2])
We keep 3.96e+06/2.17e+08 =  1% of the original kernel matrix.

torch.Size([2162, 2])
We keep 5.53e+04/7.53e+05 =  7% of the original kernel matrix.

torch.Size([9820, 2])
We keep 6.95e+05/1.79e+07 =  3% of the original kernel matrix.

torch.Size([4010, 2])
We keep 1.65e+05/2.97e+06 =  5% of the original kernel matrix.

torch.Size([12389, 2])
We keep 1.10e+06/3.55e+07 =  3% of the original kernel matrix.

torch.Size([10240, 2])
We keep 9.86e+05/2.66e+07 =  3% of the original kernel matrix.

torch.Size([18191, 2])
We keep 2.39e+06/1.06e+08 =  2% of the original kernel matrix.

torch.Size([25228, 2])
We keep 5.86e+06/2.86e+08 =  2% of the original kernel matrix.

torch.Size([29273, 2])
We keep 5.97e+06/3.48e+08 =  1% of the original kernel matrix.

torch.Size([4733, 2])
We keep 2.51e+05/4.92e+06 =  5% of the original kernel matrix.

torch.Size([13098, 2])
We keep 1.30e+06/4.57e+07 =  2% of the original kernel matrix.

torch.Size([3410, 2])
We keep 1.34e+05/2.33e+06 =  5% of the original kernel matrix.

torch.Size([11588, 2])
We keep 1.01e+06/3.14e+07 =  3% of the original kernel matrix.

torch.Size([7028, 2])
We keep 4.99e+05/1.14e+07 =  4% of the original kernel matrix.

torch.Size([15410, 2])
We keep 1.74e+06/6.95e+07 =  2% of the original kernel matrix.

torch.Size([3466, 2])
We keep 1.10e+05/1.86e+06 =  5% of the original kernel matrix.

torch.Size([11793, 2])
We keep 9.28e+05/2.81e+07 =  3% of the original kernel matrix.

torch.Size([3626, 2])
We keep 1.38e+05/2.54e+06 =  5% of the original kernel matrix.

torch.Size([11855, 2])
We keep 1.02e+06/3.28e+07 =  3% of the original kernel matrix.

torch.Size([6208, 2])
We keep 3.85e+05/8.76e+06 =  4% of the original kernel matrix.

torch.Size([14599, 2])
We keep 1.58e+06/6.10e+07 =  2% of the original kernel matrix.

torch.Size([3909, 2])
We keep 1.69e+05/3.19e+06 =  5% of the original kernel matrix.

torch.Size([12117, 2])
We keep 1.11e+06/3.68e+07 =  3% of the original kernel matrix.

torch.Size([3130, 2])
We keep 1.00e+05/1.55e+06 =  6% of the original kernel matrix.

torch.Size([11238, 2])
We keep 8.68e+05/2.56e+07 =  3% of the original kernel matrix.

torch.Size([7579, 2])
We keep 5.65e+05/1.40e+07 =  4% of the original kernel matrix.

torch.Size([15854, 2])
We keep 1.88e+06/7.71e+07 =  2% of the original kernel matrix.

torch.Size([9233, 2])
We keep 8.65e+05/2.32e+07 =  3% of the original kernel matrix.

torch.Size([17253, 2])
We keep 2.27e+06/9.91e+07 =  2% of the original kernel matrix.

torch.Size([11309, 2])
We keep 1.20e+06/3.48e+07 =  3% of the original kernel matrix.

torch.Size([18978, 2])
We keep 2.66e+06/1.22e+08 =  2% of the original kernel matrix.

torch.Size([6073, 2])
We keep 3.54e+05/7.66e+06 =  4% of the original kernel matrix.

torch.Size([14479, 2])
We keep 1.52e+06/5.70e+07 =  2% of the original kernel matrix.

torch.Size([4591, 2])
We keep 2.07e+05/4.02e+06 =  5% of the original kernel matrix.

torch.Size([13039, 2])
We keep 1.20e+06/4.13e+07 =  2% of the original kernel matrix.

torch.Size([14777, 2])
We keep 1.95e+06/6.85e+07 =  2% of the original kernel matrix.

torch.Size([21839, 2])
We keep 3.38e+06/1.71e+08 =  1% of the original kernel matrix.

torch.Size([9671, 2])
We keep 8.93e+05/2.49e+07 =  3% of the original kernel matrix.

torch.Size([17849, 2])
We keep 2.34e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([14060, 2])
We keep 1.69e+06/5.67e+07 =  2% of the original kernel matrix.

torch.Size([21376, 2])
We keep 3.15e+06/1.55e+08 =  2% of the original kernel matrix.

torch.Size([6621, 2])
We keep 4.61e+05/1.07e+07 =  4% of the original kernel matrix.

torch.Size([14970, 2])
We keep 1.72e+06/6.73e+07 =  2% of the original kernel matrix.

torch.Size([5839, 2])
We keep 4.00e+05/8.23e+06 =  4% of the original kernel matrix.

torch.Size([14227, 2])
We keep 1.59e+06/5.91e+07 =  2% of the original kernel matrix.

torch.Size([10101, 2])
We keep 9.37e+05/2.67e+07 =  3% of the original kernel matrix.

torch.Size([17936, 2])
We keep 2.39e+06/1.07e+08 =  2% of the original kernel matrix.

torch.Size([12408, 2])
We keep 1.31e+06/4.35e+07 =  3% of the original kernel matrix.

torch.Size([19841, 2])
We keep 2.80e+06/1.36e+08 =  2% of the original kernel matrix.

torch.Size([19486, 2])
We keep 3.47e+06/1.52e+08 =  2% of the original kernel matrix.

torch.Size([25462, 2])
We keep 4.61e+06/2.54e+08 =  1% of the original kernel matrix.

torch.Size([6261, 2])
We keep 3.81e+05/8.49e+06 =  4% of the original kernel matrix.

torch.Size([14520, 2])
We keep 1.56e+06/6.00e+07 =  2% of the original kernel matrix.

torch.Size([11872, 2])
We keep 1.18e+06/3.70e+07 =  3% of the original kernel matrix.

torch.Size([19459, 2])
We keep 2.69e+06/1.25e+08 =  2% of the original kernel matrix.

torch.Size([4858, 2])
We keep 2.70e+05/5.46e+06 =  4% of the original kernel matrix.

torch.Size([13142, 2])
We keep 1.34e+06/4.81e+07 =  2% of the original kernel matrix.

torch.Size([20783, 2])
We keep 4.76e+06/1.85e+08 =  2% of the original kernel matrix.

torch.Size([26214, 2])
We keep 4.83e+06/2.80e+08 =  1% of the original kernel matrix.

torch.Size([4788, 2])
We keep 2.38e+05/4.70e+06 =  5% of the original kernel matrix.

torch.Size([13176, 2])
We keep 1.28e+06/4.46e+07 =  2% of the original kernel matrix.

torch.Size([7578, 2])
We keep 6.78e+05/1.43e+07 =  4% of the original kernel matrix.

torch.Size([15827, 2])
We keep 1.91e+06/7.78e+07 =  2% of the original kernel matrix.

torch.Size([3684, 2])
We keep 1.65e+05/2.85e+06 =  5% of the original kernel matrix.

torch.Size([11951, 2])
We keep 1.06e+06/3.48e+07 =  3% of the original kernel matrix.

torch.Size([5112, 2])
We keep 2.76e+05/5.59e+06 =  4% of the original kernel matrix.

torch.Size([13461, 2])
We keep 1.37e+06/4.87e+07 =  2% of the original kernel matrix.

torch.Size([2853, 2])
We keep 1.06e+05/1.55e+06 =  6% of the original kernel matrix.

torch.Size([10669, 2])
We keep 8.74e+05/2.56e+07 =  3% of the original kernel matrix.

torch.Size([10866, 2])
We keep 1.24e+06/3.50e+07 =  3% of the original kernel matrix.

torch.Size([18698, 2])
We keep 2.58e+06/1.22e+08 =  2% of the original kernel matrix.

torch.Size([2545, 2])
We keep 7.60e+04/1.19e+06 =  6% of the original kernel matrix.

torch.Size([10285, 2])
We keep 7.97e+05/2.25e+07 =  3% of the original kernel matrix.

torch.Size([4576, 2])
We keep 2.12e+05/4.16e+06 =  5% of the original kernel matrix.

torch.Size([12903, 2])
We keep 1.21e+06/4.20e+07 =  2% of the original kernel matrix.

torch.Size([5634, 2])
We keep 3.22e+05/6.41e+06 =  5% of the original kernel matrix.

torch.Size([14049, 2])
We keep 1.43e+06/5.21e+07 =  2% of the original kernel matrix.

torch.Size([12164, 2])
We keep 1.30e+06/4.04e+07 =  3% of the original kernel matrix.

torch.Size([20006, 2])
We keep 2.79e+06/1.31e+08 =  2% of the original kernel matrix.

torch.Size([4993, 2])
We keep 2.62e+05/4.94e+06 =  5% of the original kernel matrix.

torch.Size([13382, 2])
We keep 1.27e+06/4.58e+07 =  2% of the original kernel matrix.

torch.Size([6683, 2])
We keep 4.11e+05/9.49e+06 =  4% of the original kernel matrix.

torch.Size([15137, 2])
We keep 1.65e+06/6.35e+07 =  2% of the original kernel matrix.

torch.Size([2068, 2])
We keep 6.24e+04/8.12e+05 =  7% of the original kernel matrix.

torch.Size([9473, 2])
We keep 7.19e+05/1.86e+07 =  3% of the original kernel matrix.

torch.Size([16516, 2])
We keep 2.47e+06/9.06e+07 =  2% of the original kernel matrix.

torch.Size([23253, 2])
We keep 3.74e+06/1.96e+08 =  1% of the original kernel matrix.

torch.Size([2761, 2])
We keep 9.58e+04/1.49e+06 =  6% of the original kernel matrix.

torch.Size([10477, 2])
We keep 8.57e+05/2.52e+07 =  3% of the original kernel matrix.

torch.Size([19752, 2])
We keep 5.06e+06/1.68e+08 =  3% of the original kernel matrix.

torch.Size([25745, 2])
We keep 4.90e+06/2.67e+08 =  1% of the original kernel matrix.

torch.Size([4502, 2])
We keep 2.22e+05/4.02e+06 =  5% of the original kernel matrix.

torch.Size([12831, 2])
We keep 1.23e+06/4.13e+07 =  2% of the original kernel matrix.

torch.Size([5223, 2])
We keep 2.81e+05/5.61e+06 =  5% of the original kernel matrix.

torch.Size([13638, 2])
We keep 1.35e+06/4.88e+07 =  2% of the original kernel matrix.

torch.Size([8060, 2])
We keep 5.94e+05/1.53e+07 =  3% of the original kernel matrix.

torch.Size([16328, 2])
We keep 1.93e+06/8.06e+07 =  2% of the original kernel matrix.

torch.Size([9057, 2])
We keep 9.25e+05/2.30e+07 =  4% of the original kernel matrix.

torch.Size([17057, 2])
We keep 2.24e+06/9.89e+07 =  2% of the original kernel matrix.

torch.Size([3478, 2])
We keep 1.34e+05/2.30e+06 =  5% of the original kernel matrix.

torch.Size([11743, 2])
We keep 1.01e+06/3.12e+07 =  3% of the original kernel matrix.

torch.Size([5070, 2])
We keep 2.70e+05/5.06e+06 =  5% of the original kernel matrix.

torch.Size([13463, 2])
We keep 1.33e+06/4.63e+07 =  2% of the original kernel matrix.

torch.Size([11109, 2])
We keep 1.11e+06/3.30e+07 =  3% of the original kernel matrix.

torch.Size([18817, 2])
We keep 2.58e+06/1.18e+08 =  2% of the original kernel matrix.

torch.Size([16960, 2])
We keep 2.32e+06/9.09e+07 =  2% of the original kernel matrix.

torch.Size([23764, 2])
We keep 3.74e+06/1.96e+08 =  1% of the original kernel matrix.

torch.Size([7511, 2])
We keep 5.00e+05/1.23e+07 =  4% of the original kernel matrix.

torch.Size([15823, 2])
We keep 1.78e+06/7.21e+07 =  2% of the original kernel matrix.

torch.Size([31409, 2])
We keep 8.11e+06/4.75e+08 =  1% of the original kernel matrix.

torch.Size([33210, 2])
We keep 7.27e+06/4.49e+08 =  1% of the original kernel matrix.

torch.Size([2522, 2])
We keep 6.68e+04/9.78e+05 =  6% of the original kernel matrix.

torch.Size([10413, 2])
We keep 7.45e+05/2.04e+07 =  3% of the original kernel matrix.

torch.Size([6561, 2])
We keep 4.81e+05/9.09e+06 =  5% of the original kernel matrix.

torch.Size([14974, 2])
We keep 1.62e+06/6.21e+07 =  2% of the original kernel matrix.

torch.Size([3682, 2])
We keep 1.45e+05/2.66e+06 =  5% of the original kernel matrix.

torch.Size([11969, 2])
We keep 1.05e+06/3.36e+07 =  3% of the original kernel matrix.

torch.Size([5252, 2])
We keep 2.97e+05/5.96e+06 =  4% of the original kernel matrix.

torch.Size([13644, 2])
We keep 1.39e+06/5.03e+07 =  2% of the original kernel matrix.

torch.Size([6518, 2])
We keep 4.53e+05/9.31e+06 =  4% of the original kernel matrix.

torch.Size([14963, 2])
We keep 1.65e+06/6.29e+07 =  2% of the original kernel matrix.

torch.Size([4651, 2])
We keep 2.32e+05/4.52e+06 =  5% of the original kernel matrix.

torch.Size([12897, 2])
We keep 1.26e+06/4.38e+07 =  2% of the original kernel matrix.

torch.Size([7351, 2])
We keep 5.18e+05/1.21e+07 =  4% of the original kernel matrix.

torch.Size([15623, 2])
We keep 1.78e+06/7.17e+07 =  2% of the original kernel matrix.

torch.Size([7475, 2])
We keep 5.43e+05/1.34e+07 =  4% of the original kernel matrix.

torch.Size([15616, 2])
We keep 1.81e+06/7.55e+07 =  2% of the original kernel matrix.

torch.Size([5088, 2])
We keep 2.75e+05/5.56e+06 =  4% of the original kernel matrix.

torch.Size([13413, 2])
We keep 1.37e+06/4.86e+07 =  2% of the original kernel matrix.

torch.Size([7541, 2])
We keep 5.69e+05/1.41e+07 =  4% of the original kernel matrix.

torch.Size([15756, 2])
We keep 1.90e+06/7.73e+07 =  2% of the original kernel matrix.

torch.Size([3754, 2])
We keep 2.30e+05/3.61e+06 =  6% of the original kernel matrix.

torch.Size([11913, 2])
We keep 1.15e+06/3.92e+07 =  2% of the original kernel matrix.

torch.Size([17118, 2])
We keep 2.69e+06/1.13e+08 =  2% of the original kernel matrix.

torch.Size([23600, 2])
We keep 4.10e+06/2.19e+08 =  1% of the original kernel matrix.

torch.Size([10691, 2])
We keep 1.06e+06/3.19e+07 =  3% of the original kernel matrix.

torch.Size([18360, 2])
We keep 2.53e+06/1.16e+08 =  2% of the original kernel matrix.

torch.Size([10855, 2])
We keep 1.09e+06/3.17e+07 =  3% of the original kernel matrix.

torch.Size([18648, 2])
We keep 2.54e+06/1.16e+08 =  2% of the original kernel matrix.

torch.Size([4925, 2])
We keep 2.57e+05/4.97e+06 =  5% of the original kernel matrix.

torch.Size([13223, 2])
We keep 1.32e+06/4.59e+07 =  2% of the original kernel matrix.

torch.Size([5475, 2])
We keep 3.05e+05/6.40e+06 =  4% of the original kernel matrix.

torch.Size([13897, 2])
We keep 1.41e+06/5.21e+07 =  2% of the original kernel matrix.

torch.Size([3810, 2])
We keep 2.21e+05/3.51e+06 =  6% of the original kernel matrix.

torch.Size([11928, 2])
We keep 1.18e+06/3.86e+07 =  3% of the original kernel matrix.

torch.Size([5916, 2])
We keep 4.29e+05/8.44e+06 =  5% of the original kernel matrix.

torch.Size([14203, 2])
We keep 1.58e+06/5.99e+07 =  2% of the original kernel matrix.

torch.Size([3857, 2])
We keep 1.81e+05/3.43e+06 =  5% of the original kernel matrix.

torch.Size([12019, 2])
We keep 1.15e+06/3.81e+07 =  3% of the original kernel matrix.

torch.Size([5896, 2])
We keep 3.34e+05/7.16e+06 =  4% of the original kernel matrix.

torch.Size([14307, 2])
We keep 1.48e+06/5.51e+07 =  2% of the original kernel matrix.

torch.Size([8031, 2])
We keep 6.05e+05/1.53e+07 =  3% of the original kernel matrix.

torch.Size([16248, 2])
We keep 1.93e+06/8.06e+07 =  2% of the original kernel matrix.

torch.Size([7452, 2])
We keep 5.29e+05/1.31e+07 =  4% of the original kernel matrix.

torch.Size([15767, 2])
We keep 1.83e+06/7.46e+07 =  2% of the original kernel matrix.

torch.Size([5459, 2])
We keep 3.78e+05/6.75e+06 =  5% of the original kernel matrix.

torch.Size([14011, 2])
We keep 1.44e+06/5.35e+07 =  2% of the original kernel matrix.

torch.Size([8453, 2])
We keep 6.95e+05/1.80e+07 =  3% of the original kernel matrix.

torch.Size([16639, 2])
We keep 2.07e+06/8.75e+07 =  2% of the original kernel matrix.

torch.Size([16901, 2])
We keep 2.35e+06/9.27e+07 =  2% of the original kernel matrix.

torch.Size([23896, 2])
We keep 3.80e+06/1.98e+08 =  1% of the original kernel matrix.

torch.Size([10298, 2])
We keep 9.20e+05/2.54e+07 =  3% of the original kernel matrix.

torch.Size([18206, 2])
We keep 2.35e+06/1.04e+08 =  2% of the original kernel matrix.

torch.Size([6001, 2])
We keep 3.57e+05/8.03e+06 =  4% of the original kernel matrix.

torch.Size([14416, 2])
We keep 1.54e+06/5.84e+07 =  2% of the original kernel matrix.

torch.Size([5937, 2])
We keep 3.47e+05/7.27e+06 =  4% of the original kernel matrix.

torch.Size([14325, 2])
We keep 1.49e+06/5.55e+07 =  2% of the original kernel matrix.

torch.Size([2304, 2])
We keep 6.47e+04/8.87e+05 =  7% of the original kernel matrix.

torch.Size([9972, 2])
We keep 7.38e+05/1.94e+07 =  3% of the original kernel matrix.

torch.Size([24506, 2])
We keep 5.88e+06/2.31e+08 =  2% of the original kernel matrix.

torch.Size([29636, 2])
We keep 5.48e+06/3.13e+08 =  1% of the original kernel matrix.

torch.Size([2874, 2])
We keep 9.87e+04/1.47e+06 =  6% of the original kernel matrix.

torch.Size([10924, 2])
We keep 8.71e+05/2.50e+07 =  3% of the original kernel matrix.

torch.Size([19091, 2])
We keep 3.96e+06/1.53e+08 =  2% of the original kernel matrix.

torch.Size([25156, 2])
We keep 4.55e+06/2.55e+08 =  1% of the original kernel matrix.

torch.Size([1913, 2])
We keep 5.76e+04/5.96e+05 =  9% of the original kernel matrix.

torch.Size([9237, 2])
We keep 6.55e+05/1.59e+07 =  4% of the original kernel matrix.

torch.Size([7922, 2])
We keep 6.64e+05/1.62e+07 =  4% of the original kernel matrix.

torch.Size([16163, 2])
We keep 1.98e+06/8.30e+07 =  2% of the original kernel matrix.

torch.Size([9126, 2])
We keep 8.29e+05/2.12e+07 =  3% of the original kernel matrix.

torch.Size([17256, 2])
We keep 2.20e+06/9.49e+07 =  2% of the original kernel matrix.

torch.Size([3912, 2])
We keep 1.57e+05/2.78e+06 =  5% of the original kernel matrix.

torch.Size([12358, 2])
We keep 1.07e+06/3.44e+07 =  3% of the original kernel matrix.

torch.Size([6900, 2])
We keep 5.06e+05/1.12e+07 =  4% of the original kernel matrix.

torch.Size([15144, 2])
We keep 1.75e+06/6.91e+07 =  2% of the original kernel matrix.

torch.Size([8752, 2])
We keep 7.20e+05/1.86e+07 =  3% of the original kernel matrix.

torch.Size([16916, 2])
We keep 2.08e+06/8.89e+07 =  2% of the original kernel matrix.

torch.Size([2735, 2])
We keep 8.70e+04/1.30e+06 =  6% of the original kernel matrix.

torch.Size([10582, 2])
We keep 8.40e+05/2.34e+07 =  3% of the original kernel matrix.

torch.Size([3374, 2])
We keep 1.48e+05/2.27e+06 =  6% of the original kernel matrix.

torch.Size([11307, 2])
We keep 9.99e+05/3.10e+07 =  3% of the original kernel matrix.

torch.Size([4902, 2])
We keep 3.08e+05/5.62e+06 =  5% of the original kernel matrix.

torch.Size([13210, 2])
We keep 1.31e+06/4.88e+07 =  2% of the original kernel matrix.

torch.Size([12429, 2])
We keep 1.44e+06/4.38e+07 =  3% of the original kernel matrix.

torch.Size([19934, 2])
We keep 2.84e+06/1.36e+08 =  2% of the original kernel matrix.

torch.Size([3392, 2])
We keep 1.30e+05/2.11e+06 =  6% of the original kernel matrix.

torch.Size([11484, 2])
We keep 9.73e+05/2.99e+07 =  3% of the original kernel matrix.

torch.Size([9239, 2])
We keep 7.79e+05/2.07e+07 =  3% of the original kernel matrix.

torch.Size([17410, 2])
We keep 2.15e+06/9.37e+07 =  2% of the original kernel matrix.

torch.Size([6818, 2])
We keep 4.96e+05/1.13e+07 =  4% of the original kernel matrix.

torch.Size([15265, 2])
We keep 1.75e+06/6.94e+07 =  2% of the original kernel matrix.

torch.Size([10904, 2])
We keep 1.09e+06/3.17e+07 =  3% of the original kernel matrix.

torch.Size([18721, 2])
We keep 2.52e+06/1.16e+08 =  2% of the original kernel matrix.

torch.Size([7308, 2])
We keep 5.74e+05/1.27e+07 =  4% of the original kernel matrix.

torch.Size([15553, 2])
We keep 1.84e+06/7.34e+07 =  2% of the original kernel matrix.

torch.Size([5546, 2])
We keep 3.32e+05/6.76e+06 =  4% of the original kernel matrix.

torch.Size([13908, 2])
We keep 1.46e+06/5.36e+07 =  2% of the original kernel matrix.

torch.Size([7460, 2])
We keep 5.53e+05/1.35e+07 =  4% of the original kernel matrix.

torch.Size([15621, 2])
We keep 1.85e+06/7.56e+07 =  2% of the original kernel matrix.

torch.Size([11576, 2])
We keep 1.30e+06/3.84e+07 =  3% of the original kernel matrix.

torch.Size([19378, 2])
We keep 2.72e+06/1.28e+08 =  2% of the original kernel matrix.

torch.Size([3586, 2])
We keep 1.47e+05/2.46e+06 =  5% of the original kernel matrix.

torch.Size([11759, 2])
We keep 1.03e+06/3.23e+07 =  3% of the original kernel matrix.

torch.Size([5981, 2])
We keep 3.73e+05/8.40e+06 =  4% of the original kernel matrix.

torch.Size([14357, 2])
We keep 1.56e+06/5.97e+07 =  2% of the original kernel matrix.

torch.Size([5952, 2])
We keep 3.91e+05/8.24e+06 =  4% of the original kernel matrix.

torch.Size([14447, 2])
We keep 1.57e+06/5.91e+07 =  2% of the original kernel matrix.

torch.Size([9381, 2])
We keep 8.09e+05/2.26e+07 =  3% of the original kernel matrix.

torch.Size([17287, 2])
We keep 2.24e+06/9.80e+07 =  2% of the original kernel matrix.

torch.Size([10721, 2])
We keep 9.95e+05/2.97e+07 =  3% of the original kernel matrix.

torch.Size([18644, 2])
We keep 2.45e+06/1.12e+08 =  2% of the original kernel matrix.

torch.Size([7491, 2])
We keep 5.17e+05/1.24e+07 =  4% of the original kernel matrix.

torch.Size([15769, 2])
We keep 1.81e+06/7.26e+07 =  2% of the original kernel matrix.

torch.Size([6664, 2])
We keep 4.12e+05/9.92e+06 =  4% of the original kernel matrix.

torch.Size([15005, 2])
We keep 1.64e+06/6.49e+07 =  2% of the original kernel matrix.

torch.Size([13333, 2])
We keep 1.57e+06/5.52e+07 =  2% of the original kernel matrix.

torch.Size([20756, 2])
We keep 3.11e+06/1.53e+08 =  2% of the original kernel matrix.

torch.Size([14051, 2])
We keep 1.80e+06/5.88e+07 =  3% of the original kernel matrix.

torch.Size([21313, 2])
We keep 3.21e+06/1.58e+08 =  2% of the original kernel matrix.

torch.Size([6575, 2])
We keep 4.43e+05/1.05e+07 =  4% of the original kernel matrix.

torch.Size([14833, 2])
We keep 1.69e+06/6.67e+07 =  2% of the original kernel matrix.

torch.Size([2601, 2])
We keep 8.02e+04/1.16e+06 =  6% of the original kernel matrix.

torch.Size([10503, 2])
We keep 7.95e+05/2.21e+07 =  3% of the original kernel matrix.

torch.Size([6665, 2])
We keep 4.41e+05/1.01e+07 =  4% of the original kernel matrix.

torch.Size([14924, 2])
We keep 1.68e+06/6.56e+07 =  2% of the original kernel matrix.

torch.Size([5981, 2])
We keep 3.79e+05/8.58e+06 =  4% of the original kernel matrix.

torch.Size([14326, 2])
We keep 1.58e+06/6.03e+07 =  2% of the original kernel matrix.

torch.Size([8695, 2])
We keep 7.36e+05/1.95e+07 =  3% of the original kernel matrix.

torch.Size([16736, 2])
We keep 2.10e+06/9.10e+07 =  2% of the original kernel matrix.

torch.Size([6768, 2])
We keep 4.66e+05/1.09e+07 =  4% of the original kernel matrix.

torch.Size([15017, 2])
We keep 1.71e+06/6.79e+07 =  2% of the original kernel matrix.

torch.Size([7931, 2])
We keep 5.81e+05/1.46e+07 =  3% of the original kernel matrix.

torch.Size([16266, 2])
We keep 1.89e+06/7.88e+07 =  2% of the original kernel matrix.

torch.Size([5303, 2])
We keep 2.95e+05/6.25e+06 =  4% of the original kernel matrix.

torch.Size([13703, 2])
We keep 1.40e+06/5.15e+07 =  2% of the original kernel matrix.

torch.Size([8615, 2])
We keep 7.89e+05/1.92e+07 =  4% of the original kernel matrix.

torch.Size([16820, 2])
We keep 2.05e+06/9.02e+07 =  2% of the original kernel matrix.

torch.Size([2966, 2])
We keep 9.94e+04/1.61e+06 =  6% of the original kernel matrix.

torch.Size([11012, 2])
We keep 8.91e+05/2.61e+07 =  3% of the original kernel matrix.

torch.Size([10988, 2])
We keep 1.10e+06/3.05e+07 =  3% of the original kernel matrix.

torch.Size([18782, 2])
We keep 2.52e+06/1.14e+08 =  2% of the original kernel matrix.

torch.Size([8189, 2])
We keep 6.49e+05/1.57e+07 =  4% of the original kernel matrix.

torch.Size([16315, 2])
We keep 1.96e+06/8.17e+07 =  2% of the original kernel matrix.

torch.Size([16798, 2])
We keep 2.26e+06/8.97e+07 =  2% of the original kernel matrix.

torch.Size([23438, 2])
We keep 3.75e+06/1.95e+08 =  1% of the original kernel matrix.

torch.Size([5649, 2])
We keep 3.27e+05/6.90e+06 =  4% of the original kernel matrix.

torch.Size([13945, 2])
We keep 1.45e+06/5.41e+07 =  2% of the original kernel matrix.

torch.Size([14678, 2])
We keep 1.86e+06/6.63e+07 =  2% of the original kernel matrix.

torch.Size([21817, 2])
We keep 3.30e+06/1.68e+08 =  1% of the original kernel matrix.

torch.Size([7247, 2])
We keep 5.38e+05/1.34e+07 =  4% of the original kernel matrix.

torch.Size([15447, 2])
We keep 1.83e+06/7.53e+07 =  2% of the original kernel matrix.

torch.Size([5004, 2])
We keep 2.75e+05/5.41e+06 =  5% of the original kernel matrix.

torch.Size([13342, 2])
We keep 1.33e+06/4.79e+07 =  2% of the original kernel matrix.

torch.Size([6753, 2])
We keep 4.64e+05/1.09e+07 =  4% of the original kernel matrix.

torch.Size([15014, 2])
We keep 1.70e+06/6.79e+07 =  2% of the original kernel matrix.

torch.Size([3678, 2])
We keep 1.47e+05/2.66e+06 =  5% of the original kernel matrix.

torch.Size([11991, 2])
We keep 1.05e+06/3.36e+07 =  3% of the original kernel matrix.

torch.Size([15273, 2])
We keep 1.89e+06/7.04e+07 =  2% of the original kernel matrix.

torch.Size([22382, 2])
We keep 3.37e+06/1.73e+08 =  1% of the original kernel matrix.

torch.Size([11210, 2])
We keep 1.16e+06/3.61e+07 =  3% of the original kernel matrix.

torch.Size([19223, 2])
We keep 2.66e+06/1.24e+08 =  2% of the original kernel matrix.

torch.Size([5078, 2])
We keep 2.70e+05/5.45e+06 =  4% of the original kernel matrix.

torch.Size([13434, 2])
We keep 1.34e+06/4.81e+07 =  2% of the original kernel matrix.

torch.Size([7204, 2])
We keep 4.80e+05/1.14e+07 =  4% of the original kernel matrix.

torch.Size([15449, 2])
We keep 1.75e+06/6.96e+07 =  2% of the original kernel matrix.

torch.Size([17725, 2])
We keep 2.51e+06/1.05e+08 =  2% of the original kernel matrix.

torch.Size([24140, 2])
We keep 3.97e+06/2.11e+08 =  1% of the original kernel matrix.

torch.Size([2067, 2])
We keep 5.32e+04/6.61e+05 =  8% of the original kernel matrix.

torch.Size([9507, 2])
We keep 6.49e+05/1.67e+07 =  3% of the original kernel matrix.

torch.Size([2544, 2])
We keep 7.41e+04/1.09e+06 =  6% of the original kernel matrix.

torch.Size([10492, 2])
We keep 7.79e+05/2.15e+07 =  3% of the original kernel matrix.

torch.Size([1782, 2])
We keep 4.08e+04/5.23e+05 =  7% of the original kernel matrix.

torch.Size([9180, 2])
We keep 6.10e+05/1.49e+07 =  4% of the original kernel matrix.

torch.Size([3370, 2])
We keep 1.20e+05/2.06e+06 =  5% of the original kernel matrix.

torch.Size([11556, 2])
We keep 9.64e+05/2.95e+07 =  3% of the original kernel matrix.

torch.Size([17164, 2])
We keep 3.39e+06/1.14e+08 =  2% of the original kernel matrix.

torch.Size([23716, 2])
We keep 3.99e+06/2.20e+08 =  1% of the original kernel matrix.

torch.Size([9328, 2])
We keep 8.35e+05/2.28e+07 =  3% of the original kernel matrix.

torch.Size([17268, 2])
We keep 2.25e+06/9.85e+07 =  2% of the original kernel matrix.

torch.Size([6664, 2])
We keep 4.39e+05/1.01e+07 =  4% of the original kernel matrix.

torch.Size([14979, 2])
We keep 1.67e+06/6.55e+07 =  2% of the original kernel matrix.

torch.Size([2138, 2])
We keep 6.10e+04/8.48e+05 =  7% of the original kernel matrix.

torch.Size([9716, 2])
We keep 7.31e+05/1.90e+07 =  3% of the original kernel matrix.

torch.Size([13719, 2])
We keep 1.84e+06/6.07e+07 =  3% of the original kernel matrix.

torch.Size([21174, 2])
We keep 3.24e+06/1.61e+08 =  2% of the original kernel matrix.

torch.Size([5132, 2])
We keep 2.94e+05/5.79e+06 =  5% of the original kernel matrix.

torch.Size([13531, 2])
We keep 1.38e+06/4.96e+07 =  2% of the original kernel matrix.

torch.Size([7714, 2])
We keep 6.51e+05/1.62e+07 =  4% of the original kernel matrix.

torch.Size([15957, 2])
We keep 1.99e+06/8.28e+07 =  2% of the original kernel matrix.

torch.Size([9155, 2])
We keep 7.83e+05/2.12e+07 =  3% of the original kernel matrix.

torch.Size([17198, 2])
We keep 2.20e+06/9.47e+07 =  2% of the original kernel matrix.

torch.Size([8193, 2])
We keep 6.68e+05/1.62e+07 =  4% of the original kernel matrix.

torch.Size([16247, 2])
We keep 1.99e+06/8.30e+07 =  2% of the original kernel matrix.

torch.Size([3491, 2])
We keep 1.34e+05/2.18e+06 =  6% of the original kernel matrix.

torch.Size([11649, 2])
We keep 9.88e+05/3.04e+07 =  3% of the original kernel matrix.

torch.Size([9958, 2])
We keep 9.81e+05/2.50e+07 =  3% of the original kernel matrix.

torch.Size([17913, 2])
We keep 2.36e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([3074, 2])
We keep 1.08e+05/1.73e+06 =  6% of the original kernel matrix.

torch.Size([11097, 2])
We keep 9.12e+05/2.71e+07 =  3% of the original kernel matrix.

torch.Size([6175, 2])
We keep 3.80e+05/8.73e+06 =  4% of the original kernel matrix.

torch.Size([14576, 2])
We keep 1.58e+06/6.09e+07 =  2% of the original kernel matrix.

torch.Size([5104, 2])
We keep 2.68e+05/5.58e+06 =  4% of the original kernel matrix.

torch.Size([13446, 2])
We keep 1.34e+06/4.87e+07 =  2% of the original kernel matrix.

torch.Size([5305, 2])
We keep 3.17e+05/5.90e+06 =  5% of the original kernel matrix.

torch.Size([13641, 2])
We keep 1.40e+06/5.00e+07 =  2% of the original kernel matrix.

torch.Size([6356, 2])
We keep 3.90e+05/8.87e+06 =  4% of the original kernel matrix.

torch.Size([14775, 2])
We keep 1.60e+06/6.14e+07 =  2% of the original kernel matrix.

torch.Size([9271, 2])
We keep 8.37e+05/2.18e+07 =  3% of the original kernel matrix.

torch.Size([17327, 2])
We keep 2.20e+06/9.61e+07 =  2% of the original kernel matrix.

torch.Size([10139, 2])
We keep 9.44e+05/2.63e+07 =  3% of the original kernel matrix.

torch.Size([18063, 2])
We keep 2.37e+06/1.06e+08 =  2% of the original kernel matrix.

torch.Size([9323, 2])
We keep 7.73e+05/2.12e+07 =  3% of the original kernel matrix.

torch.Size([17423, 2])
We keep 2.19e+06/9.48e+07 =  2% of the original kernel matrix.

torch.Size([9459, 2])
We keep 7.84e+05/2.10e+07 =  3% of the original kernel matrix.

torch.Size([17550, 2])
We keep 2.16e+06/9.45e+07 =  2% of the original kernel matrix.

torch.Size([5311, 2])
We keep 2.73e+05/5.82e+06 =  4% of the original kernel matrix.

torch.Size([13724, 2])
We keep 1.38e+06/4.97e+07 =  2% of the original kernel matrix.

torch.Size([13944, 2])
We keep 1.85e+06/6.65e+07 =  2% of the original kernel matrix.

torch.Size([21298, 2])
We keep 3.36e+06/1.68e+08 =  1% of the original kernel matrix.

torch.Size([12564, 2])
We keep 2.12e+06/5.52e+07 =  3% of the original kernel matrix.

torch.Size([20165, 2])
We keep 3.15e+06/1.53e+08 =  2% of the original kernel matrix.

torch.Size([18188, 2])
We keep 3.94e+06/1.20e+08 =  3% of the original kernel matrix.

torch.Size([24620, 2])
We keep 4.25e+06/2.26e+08 =  1% of the original kernel matrix.

torch.Size([12507, 2])
We keep 1.65e+06/4.75e+07 =  3% of the original kernel matrix.

torch.Size([20053, 2])
We keep 2.97e+06/1.42e+08 =  2% of the original kernel matrix.

torch.Size([7538, 2])
We keep 9.58e+05/1.65e+07 =  5% of the original kernel matrix.

torch.Size([15858, 2])
We keep 2.02e+06/8.36e+07 =  2% of the original kernel matrix.

torch.Size([18587, 2])
We keep 3.42e+06/1.33e+08 =  2% of the original kernel matrix.

torch.Size([24926, 2])
We keep 4.40e+06/2.38e+08 =  1% of the original kernel matrix.

torch.Size([24798, 2])
We keep 9.38e+06/2.65e+08 =  3% of the original kernel matrix.

torch.Size([29352, 2])
We keep 5.83e+06/3.35e+08 =  1% of the original kernel matrix.

torch.Size([23172, 2])
We keep 6.51e+06/2.30e+08 =  2% of the original kernel matrix.

torch.Size([27823, 2])
We keep 5.46e+06/3.12e+08 =  1% of the original kernel matrix.

torch.Size([10212, 2])
We keep 8.56e+05/2.52e+07 =  3% of the original kernel matrix.

torch.Size([18298, 2])
We keep 2.30e+06/1.04e+08 =  2% of the original kernel matrix.

torch.Size([224925, 2])
We keep 3.01e+08/2.80e+10 =  1% of the original kernel matrix.

torch.Size([88779, 2])
We keep 4.02e+07/3.45e+09 =  1% of the original kernel matrix.

torch.Size([116822, 2])
We keep 1.71e+08/8.28e+09 =  2% of the original kernel matrix.

torch.Size([61869, 2])
We keep 2.35e+07/1.88e+09 =  1% of the original kernel matrix.

torch.Size([95640, 2])
We keep 1.11e+08/4.98e+09 =  2% of the original kernel matrix.

torch.Size([55243, 2])
We keep 1.89e+07/1.45e+09 =  1% of the original kernel matrix.

torch.Size([81857, 2])
We keep 8.97e+07/3.41e+09 =  2% of the original kernel matrix.

torch.Size([52179, 2])
We keep 1.68e+07/1.20e+09 =  1% of the original kernel matrix.

torch.Size([5090, 2])
We keep 2.65e+05/5.64e+06 =  4% of the original kernel matrix.

torch.Size([13543, 2])
We keep 1.35e+06/4.89e+07 =  2% of the original kernel matrix.

torch.Size([17426, 2])
We keep 9.83e+06/1.38e+08 =  7% of the original kernel matrix.

torch.Size([23789, 2])
We keep 4.55e+06/2.42e+08 =  1% of the original kernel matrix.

torch.Size([573415, 2])
We keep 1.36e+09/1.46e+11 =  0% of the original kernel matrix.

torch.Size([142417, 2])
We keep 8.43e+07/7.86e+09 =  1% of the original kernel matrix.

torch.Size([555520, 2])
We keep 1.29e+09/1.24e+11 =  1% of the original kernel matrix.

torch.Size([140409, 2])
We keep 7.88e+07/7.26e+09 =  1% of the original kernel matrix.

torch.Size([6730, 2])
We keep 4.33e+05/1.05e+07 =  4% of the original kernel matrix.

torch.Size([15077, 2])
We keep 1.66e+06/6.66e+07 =  2% of the original kernel matrix.

torch.Size([43699, 2])
We keep 1.43e+07/8.96e+08 =  1% of the original kernel matrix.

torch.Size([38569, 2])
We keep 9.43e+06/6.17e+08 =  1% of the original kernel matrix.

torch.Size([109198, 2])
We keep 1.61e+08/6.43e+09 =  2% of the original kernel matrix.

torch.Size([60586, 2])
We keep 2.19e+07/1.65e+09 =  1% of the original kernel matrix.

torch.Size([25257, 2])
We keep 7.42e+06/2.81e+08 =  2% of the original kernel matrix.

torch.Size([29235, 2])
We keep 6.01e+06/3.46e+08 =  1% of the original kernel matrix.

torch.Size([30180, 2])
We keep 1.27e+07/4.45e+08 =  2% of the original kernel matrix.

torch.Size([31616, 2])
We keep 7.06e+06/4.35e+08 =  1% of the original kernel matrix.

torch.Size([40731, 2])
We keep 2.37e+07/9.96e+08 =  2% of the original kernel matrix.

torch.Size([36156, 2])
We keep 1.01e+07/6.50e+08 =  1% of the original kernel matrix.

torch.Size([14954, 2])
We keep 3.45e+06/8.02e+07 =  4% of the original kernel matrix.

torch.Size([22125, 2])
We keep 3.52e+06/1.85e+08 =  1% of the original kernel matrix.

torch.Size([15152, 2])
We keep 2.79e+06/7.85e+07 =  3% of the original kernel matrix.

torch.Size([22243, 2])
We keep 3.57e+06/1.82e+08 =  1% of the original kernel matrix.

torch.Size([16550, 2])
We keep 8.16e+06/1.09e+08 =  7% of the original kernel matrix.

torch.Size([23403, 2])
We keep 4.18e+06/2.15e+08 =  1% of the original kernel matrix.

torch.Size([24917, 2])
We keep 1.98e+07/2.95e+08 =  6% of the original kernel matrix.

torch.Size([28868, 2])
We keep 5.87e+06/3.54e+08 =  1% of the original kernel matrix.

torch.Size([19072, 2])
We keep 7.41e+06/1.52e+08 =  4% of the original kernel matrix.

torch.Size([25208, 2])
We keep 4.55e+06/2.54e+08 =  1% of the original kernel matrix.

torch.Size([10554, 2])
We keep 1.06e+06/3.24e+07 =  3% of the original kernel matrix.

torch.Size([18523, 2])
We keep 2.54e+06/1.17e+08 =  2% of the original kernel matrix.

torch.Size([43097, 2])
We keep 2.29e+07/9.04e+08 =  2% of the original kernel matrix.

torch.Size([37987, 2])
We keep 9.57e+06/6.19e+08 =  1% of the original kernel matrix.

torch.Size([28068, 2])
We keep 1.46e+07/4.01e+08 =  3% of the original kernel matrix.

torch.Size([30922, 2])
We keep 6.88e+06/4.13e+08 =  1% of the original kernel matrix.

torch.Size([322828, 2])
We keep 6.03e+08/5.15e+10 =  1% of the original kernel matrix.

torch.Size([106925, 2])
We keep 5.32e+07/4.67e+09 =  1% of the original kernel matrix.

torch.Size([88551, 2])
We keep 5.89e+07/3.82e+09 =  1% of the original kernel matrix.

torch.Size([53752, 2])
We keep 1.73e+07/1.27e+09 =  1% of the original kernel matrix.

torch.Size([17933, 2])
We keep 3.25e+06/1.20e+08 =  2% of the original kernel matrix.

torch.Size([24147, 2])
We keep 4.19e+06/2.26e+08 =  1% of the original kernel matrix.

torch.Size([53480, 2])
We keep 2.46e+07/1.56e+09 =  1% of the original kernel matrix.

torch.Size([42030, 2])
We keep 1.17e+07/8.13e+08 =  1% of the original kernel matrix.

torch.Size([9198, 2])
We keep 7.91e+05/2.15e+07 =  3% of the original kernel matrix.

torch.Size([17187, 2])
We keep 2.19e+06/9.56e+07 =  2% of the original kernel matrix.

torch.Size([26717, 2])
We keep 6.28e+06/3.13e+08 =  2% of the original kernel matrix.

torch.Size([29976, 2])
We keep 6.13e+06/3.64e+08 =  1% of the original kernel matrix.

torch.Size([73756, 2])
We keep 3.75e+07/2.42e+09 =  1% of the original kernel matrix.

torch.Size([49260, 2])
We keep 1.40e+07/1.01e+09 =  1% of the original kernel matrix.

torch.Size([79889, 2])
We keep 1.16e+08/3.99e+09 =  2% of the original kernel matrix.

torch.Size([51836, 2])
We keep 1.79e+07/1.30e+09 =  1% of the original kernel matrix.

torch.Size([213879, 2])
We keep 2.24e+08/2.28e+10 =  0% of the original kernel matrix.

torch.Size([86284, 2])
We keep 3.70e+07/3.11e+09 =  1% of the original kernel matrix.

torch.Size([4310, 2])
We keep 2.61e+05/3.94e+06 =  6% of the original kernel matrix.

torch.Size([12386, 2])
We keep 1.21e+06/4.09e+07 =  2% of the original kernel matrix.

torch.Size([12322, 2])
We keep 1.28e+06/4.14e+07 =  3% of the original kernel matrix.

torch.Size([19882, 2])
We keep 2.73e+06/1.33e+08 =  2% of the original kernel matrix.

torch.Size([90965, 2])
We keep 6.83e+07/4.23e+09 =  1% of the original kernel matrix.

torch.Size([54148, 2])
We keep 1.78e+07/1.34e+09 =  1% of the original kernel matrix.

torch.Size([81073, 2])
We keep 1.77e+08/4.92e+09 =  3% of the original kernel matrix.

torch.Size([50927, 2])
We keep 1.97e+07/1.44e+09 =  1% of the original kernel matrix.

torch.Size([56621, 2])
We keep 4.57e+07/1.87e+09 =  2% of the original kernel matrix.

torch.Size([42781, 2])
We keep 1.29e+07/8.91e+08 =  1% of the original kernel matrix.

torch.Size([6227, 2])
We keep 4.19e+05/9.22e+06 =  4% of the original kernel matrix.

torch.Size([14504, 2])
We keep 1.62e+06/6.25e+07 =  2% of the original kernel matrix.

torch.Size([258973, 2])
We keep 3.28e+08/3.74e+10 =  0% of the original kernel matrix.

torch.Size([95181, 2])
We keep 4.59e+07/3.98e+09 =  1% of the original kernel matrix.

torch.Size([24277, 2])
We keep 4.71e+06/2.52e+08 =  1% of the original kernel matrix.

torch.Size([28895, 2])
We keep 5.56e+06/3.27e+08 =  1% of the original kernel matrix.

torch.Size([29939, 2])
We keep 7.01e+06/4.03e+08 =  1% of the original kernel matrix.

torch.Size([32341, 2])
We keep 6.84e+06/4.14e+08 =  1% of the original kernel matrix.

torch.Size([116490, 2])
We keep 7.64e+07/6.31e+09 =  1% of the original kernel matrix.

torch.Size([61885, 2])
We keep 2.11e+07/1.64e+09 =  1% of the original kernel matrix.

torch.Size([6625, 2])
We keep 4.19e+05/9.44e+06 =  4% of the original kernel matrix.

torch.Size([15149, 2])
We keep 1.63e+06/6.33e+07 =  2% of the original kernel matrix.

torch.Size([88370, 2])
We keep 6.34e+07/3.49e+09 =  1% of the original kernel matrix.

torch.Size([53314, 2])
We keep 1.66e+07/1.22e+09 =  1% of the original kernel matrix.

torch.Size([105034, 2])
We keep 9.21e+07/5.62e+09 =  1% of the original kernel matrix.

torch.Size([59005, 2])
We keep 2.00e+07/1.54e+09 =  1% of the original kernel matrix.

torch.Size([456680, 2])
We keep 1.41e+09/1.04e+11 =  1% of the original kernel matrix.

torch.Size([129224, 2])
We keep 7.27e+07/6.65e+09 =  1% of the original kernel matrix.

torch.Size([80641, 2])
We keep 4.92e+07/3.26e+09 =  1% of the original kernel matrix.

torch.Size([50976, 2])
We keep 1.59e+07/1.18e+09 =  1% of the original kernel matrix.

torch.Size([71383, 2])
We keep 3.48e+07/2.36e+09 =  1% of the original kernel matrix.

torch.Size([48001, 2])
We keep 1.39e+07/1.00e+09 =  1% of the original kernel matrix.

torch.Size([29938, 2])
We keep 1.41e+07/4.78e+08 =  2% of the original kernel matrix.

torch.Size([31359, 2])
We keep 7.44e+06/4.50e+08 =  1% of the original kernel matrix.

torch.Size([11924, 2])
We keep 1.20e+06/3.83e+07 =  3% of the original kernel matrix.

torch.Size([19551, 2])
We keep 2.71e+06/1.28e+08 =  2% of the original kernel matrix.

torch.Size([39108, 2])
We keep 2.62e+07/8.90e+08 =  2% of the original kernel matrix.

torch.Size([36491, 2])
We keep 8.80e+06/6.14e+08 =  1% of the original kernel matrix.

torch.Size([15626, 2])
We keep 2.08e+06/8.03e+07 =  2% of the original kernel matrix.

torch.Size([22631, 2])
We keep 3.56e+06/1.85e+08 =  1% of the original kernel matrix.

torch.Size([23508, 2])
We keep 7.48e+06/2.61e+08 =  2% of the original kernel matrix.

torch.Size([27786, 2])
We keep 5.54e+06/3.33e+08 =  1% of the original kernel matrix.

torch.Size([58669, 2])
We keep 1.19e+08/2.01e+09 =  5% of the original kernel matrix.

torch.Size([43945, 2])
We keep 1.27e+07/9.23e+08 =  1% of the original kernel matrix.

torch.Size([40392, 2])
We keep 2.22e+07/7.98e+08 =  2% of the original kernel matrix.

torch.Size([36892, 2])
We keep 8.71e+06/5.82e+08 =  1% of the original kernel matrix.

torch.Size([12127, 2])
We keep 2.45e+06/5.51e+07 =  4% of the original kernel matrix.

torch.Size([19837, 2])
We keep 3.18e+06/1.53e+08 =  2% of the original kernel matrix.

torch.Size([13173, 2])
We keep 1.51e+06/4.98e+07 =  3% of the original kernel matrix.

torch.Size([20674, 2])
We keep 3.02e+06/1.45e+08 =  2% of the original kernel matrix.

torch.Size([249791, 2])
We keep 1.42e+09/8.90e+10 =  1% of the original kernel matrix.

torch.Size([89502, 2])
We keep 6.92e+07/6.15e+09 =  1% of the original kernel matrix.

torch.Size([14834, 2])
We keep 2.51e+06/6.95e+07 =  3% of the original kernel matrix.

torch.Size([22051, 2])
We keep 3.41e+06/1.72e+08 =  1% of the original kernel matrix.

torch.Size([4535, 2])
We keep 2.35e+05/4.11e+06 =  5% of the original kernel matrix.

torch.Size([12901, 2])
We keep 1.21e+06/4.18e+07 =  2% of the original kernel matrix.

torch.Size([13119, 2])
We keep 1.71e+06/5.50e+07 =  3% of the original kernel matrix.

torch.Size([20486, 2])
We keep 3.10e+06/1.53e+08 =  2% of the original kernel matrix.

torch.Size([163435, 2])
We keep 4.07e+08/2.05e+10 =  1% of the original kernel matrix.

torch.Size([74003, 2])
We keep 3.54e+07/2.95e+09 =  1% of the original kernel matrix.

torch.Size([40432, 2])
We keep 1.78e+07/8.16e+08 =  2% of the original kernel matrix.

torch.Size([37322, 2])
We keep 9.06e+06/5.89e+08 =  1% of the original kernel matrix.

torch.Size([22649, 2])
We keep 1.43e+07/2.94e+08 =  4% of the original kernel matrix.

torch.Size([27561, 2])
We keep 6.02e+06/3.53e+08 =  1% of the original kernel matrix.

torch.Size([17013, 2])
We keep 7.86e+06/1.43e+08 =  5% of the original kernel matrix.

torch.Size([23634, 2])
We keep 4.63e+06/2.46e+08 =  1% of the original kernel matrix.

torch.Size([19626, 2])
We keep 6.23e+06/1.85e+08 =  3% of the original kernel matrix.

torch.Size([25303, 2])
We keep 5.01e+06/2.80e+08 =  1% of the original kernel matrix.

torch.Size([22639, 2])
We keep 1.13e+07/2.16e+08 =  5% of the original kernel matrix.

torch.Size([27542, 2])
We keep 5.22e+06/3.03e+08 =  1% of the original kernel matrix.

torch.Size([556753, 2])
We keep 2.91e+09/2.24e+11 =  1% of the original kernel matrix.

torch.Size([141741, 2])
We keep 1.03e+08/9.75e+09 =  1% of the original kernel matrix.

torch.Size([51409, 2])
We keep 2.40e+07/1.62e+09 =  1% of the original kernel matrix.

torch.Size([41009, 2])
We keep 1.20e+07/8.29e+08 =  1% of the original kernel matrix.

torch.Size([24977, 2])
We keep 5.65e+06/2.63e+08 =  2% of the original kernel matrix.

torch.Size([29112, 2])
We keep 5.69e+06/3.34e+08 =  1% of the original kernel matrix.

torch.Size([174645, 2])
We keep 1.38e+08/1.41e+10 =  0% of the original kernel matrix.

torch.Size([77243, 2])
We keep 2.99e+07/2.45e+09 =  1% of the original kernel matrix.

torch.Size([180870, 2])
We keep 1.38e+08/1.47e+10 =  0% of the original kernel matrix.

torch.Size([78833, 2])
We keep 3.02e+07/2.50e+09 =  1% of the original kernel matrix.

torch.Size([10733, 2])
We keep 3.62e+06/3.96e+07 =  9% of the original kernel matrix.

torch.Size([18843, 2])
We keep 2.73e+06/1.30e+08 =  2% of the original kernel matrix.

torch.Size([143380, 2])
We keep 1.34e+08/9.42e+09 =  1% of the original kernel matrix.

torch.Size([69425, 2])
We keep 2.51e+07/2.00e+09 =  1% of the original kernel matrix.

torch.Size([158750, 2])
We keep 1.36e+08/1.34e+10 =  1% of the original kernel matrix.

torch.Size([73550, 2])
We keep 2.92e+07/2.39e+09 =  1% of the original kernel matrix.

torch.Size([631804, 2])
We keep 1.87e+09/1.69e+11 =  1% of the original kernel matrix.

torch.Size([153515, 2])
We keep 8.94e+07/8.47e+09 =  1% of the original kernel matrix.

torch.Size([16764, 2])
We keep 6.18e+06/9.92e+07 =  6% of the original kernel matrix.

torch.Size([23435, 2])
We keep 3.96e+06/2.05e+08 =  1% of the original kernel matrix.

torch.Size([5231, 2])
We keep 3.09e+05/6.06e+06 =  5% of the original kernel matrix.

torch.Size([13640, 2])
We keep 1.40e+06/5.07e+07 =  2% of the original kernel matrix.

torch.Size([19602, 2])
We keep 3.51e+06/1.52e+08 =  2% of the original kernel matrix.

torch.Size([25610, 2])
We keep 4.61e+06/2.54e+08 =  1% of the original kernel matrix.

torch.Size([23433, 2])
We keep 1.76e+07/3.38e+08 =  5% of the original kernel matrix.

torch.Size([28036, 2])
We keep 6.36e+06/3.79e+08 =  1% of the original kernel matrix.

torch.Size([51569, 2])
We keep 2.81e+07/1.28e+09 =  2% of the original kernel matrix.

torch.Size([41362, 2])
We keep 1.10e+07/7.37e+08 =  1% of the original kernel matrix.

torch.Size([35977, 2])
We keep 1.37e+07/6.16e+08 =  2% of the original kernel matrix.

torch.Size([35530, 2])
We keep 7.85e+06/5.11e+08 =  1% of the original kernel matrix.

torch.Size([4673, 2])
We keep 2.29e+05/4.55e+06 =  5% of the original kernel matrix.

torch.Size([13020, 2])
We keep 1.26e+06/4.39e+07 =  2% of the original kernel matrix.

torch.Size([40904, 2])
We keep 1.67e+07/8.32e+08 =  2% of the original kernel matrix.

torch.Size([37603, 2])
We keep 9.07e+06/5.94e+08 =  1% of the original kernel matrix.

torch.Size([186161, 2])
We keep 1.85e+08/1.79e+10 =  1% of the original kernel matrix.

torch.Size([79992, 2])
We keep 3.35e+07/2.76e+09 =  1% of the original kernel matrix.

torch.Size([155129, 2])
We keep 5.94e+08/2.62e+10 =  2% of the original kernel matrix.

torch.Size([69715, 2])
We keep 4.01e+07/3.34e+09 =  1% of the original kernel matrix.

torch.Size([10122, 2])
We keep 1.56e+06/3.24e+07 =  4% of the original kernel matrix.

torch.Size([18095, 2])
We keep 2.58e+06/1.17e+08 =  2% of the original kernel matrix.

torch.Size([234976, 2])
We keep 3.22e+08/2.85e+10 =  1% of the original kernel matrix.

torch.Size([90668, 2])
We keep 4.06e+07/3.48e+09 =  1% of the original kernel matrix.

torch.Size([25430, 2])
We keep 6.11e+06/2.86e+08 =  2% of the original kernel matrix.

torch.Size([29414, 2])
We keep 5.86e+06/3.49e+08 =  1% of the original kernel matrix.

torch.Size([29368, 2])
We keep 7.75e+06/3.74e+08 =  2% of the original kernel matrix.

torch.Size([29720, 2])
We keep 5.94e+06/3.98e+08 =  1% of the original kernel matrix.

torch.Size([20904, 2])
We keep 3.87e+06/1.71e+08 =  2% of the original kernel matrix.

torch.Size([26374, 2])
We keep 4.79e+06/2.69e+08 =  1% of the original kernel matrix.

torch.Size([8439, 2])
We keep 6.41e+05/1.63e+07 =  3% of the original kernel matrix.

torch.Size([16605, 2])
We keep 1.96e+06/8.33e+07 =  2% of the original kernel matrix.

torch.Size([227573, 2])
We keep 5.00e+08/3.18e+10 =  1% of the original kernel matrix.

torch.Size([89385, 2])
We keep 4.35e+07/3.67e+09 =  1% of the original kernel matrix.

torch.Size([63651, 2])
We keep 2.61e+07/1.80e+09 =  1% of the original kernel matrix.

torch.Size([45767, 2])
We keep 1.25e+07/8.74e+08 =  1% of the original kernel matrix.

torch.Size([17780, 2])
We keep 4.55e+06/1.22e+08 =  3% of the original kernel matrix.

torch.Size([24585, 2])
We keep 4.21e+06/2.28e+08 =  1% of the original kernel matrix.

torch.Size([34668, 2])
We keep 1.73e+07/6.47e+08 =  2% of the original kernel matrix.

torch.Size([34451, 2])
We keep 7.90e+06/5.24e+08 =  1% of the original kernel matrix.

torch.Size([14364, 2])
We keep 1.87e+06/6.60e+07 =  2% of the original kernel matrix.

torch.Size([21474, 2])
We keep 3.30e+06/1.67e+08 =  1% of the original kernel matrix.

torch.Size([485197, 2])
We keep 1.35e+09/1.19e+11 =  1% of the original kernel matrix.

torch.Size([132651, 2])
We keep 7.78e+07/7.11e+09 =  1% of the original kernel matrix.

torch.Size([43033, 2])
We keep 1.50e+07/9.16e+08 =  1% of the original kernel matrix.

torch.Size([37952, 2])
We keep 9.46e+06/6.24e+08 =  1% of the original kernel matrix.

torch.Size([267584, 2])
We keep 3.57e+08/3.58e+10 =  0% of the original kernel matrix.

torch.Size([97594, 2])
We keep 4.53e+07/3.90e+09 =  1% of the original kernel matrix.

torch.Size([5094, 2])
We keep 2.60e+05/5.24e+06 =  4% of the original kernel matrix.

torch.Size([13494, 2])
We keep 1.32e+06/4.72e+07 =  2% of the original kernel matrix.

torch.Size([10603, 2])
We keep 9.91e+05/2.91e+07 =  3% of the original kernel matrix.

torch.Size([18380, 2])
We keep 2.45e+06/1.11e+08 =  2% of the original kernel matrix.

torch.Size([10132, 2])
We keep 9.57e+05/2.58e+07 =  3% of the original kernel matrix.

torch.Size([18196, 2])
We keep 2.33e+06/1.05e+08 =  2% of the original kernel matrix.

torch.Size([27307, 2])
We keep 9.71e+06/3.26e+08 =  2% of the original kernel matrix.

torch.Size([30604, 2])
We keep 6.20e+06/3.72e+08 =  1% of the original kernel matrix.

torch.Size([11011, 2])
We keep 1.24e+06/3.40e+07 =  3% of the original kernel matrix.

torch.Size([19115, 2])
We keep 2.64e+06/1.20e+08 =  2% of the original kernel matrix.

torch.Size([17538, 2])
We keep 3.58e+06/1.11e+08 =  3% of the original kernel matrix.

torch.Size([24063, 2])
We keep 4.04e+06/2.17e+08 =  1% of the original kernel matrix.

torch.Size([19758, 2])
We keep 3.37e+06/1.37e+08 =  2% of the original kernel matrix.

torch.Size([26160, 2])
We keep 4.48e+06/2.41e+08 =  1% of the original kernel matrix.

torch.Size([15060, 2])
We keep 1.95e+06/6.81e+07 =  2% of the original kernel matrix.

torch.Size([22084, 2])
We keep 3.38e+06/1.70e+08 =  1% of the original kernel matrix.

torch.Size([296555, 2])
We keep 6.50e+08/4.59e+10 =  1% of the original kernel matrix.

torch.Size([104216, 2])
We keep 5.08e+07/4.42e+09 =  1% of the original kernel matrix.

torch.Size([38811, 2])
We keep 1.26e+07/7.14e+08 =  1% of the original kernel matrix.

torch.Size([36061, 2])
We keep 8.41e+06/5.50e+08 =  1% of the original kernel matrix.

torch.Size([35010, 2])
We keep 1.25e+07/6.33e+08 =  1% of the original kernel matrix.

torch.Size([34390, 2])
We keep 8.02e+06/5.18e+08 =  1% of the original kernel matrix.

torch.Size([117614, 2])
We keep 8.12e+07/6.48e+09 =  1% of the original kernel matrix.

torch.Size([63035, 2])
We keep 2.13e+07/1.66e+09 =  1% of the original kernel matrix.

torch.Size([455675, 2])
We keep 7.88e+08/9.25e+10 =  0% of the original kernel matrix.

torch.Size([128610, 2])
We keep 6.87e+07/6.27e+09 =  1% of the original kernel matrix.

torch.Size([14625, 2])
We keep 2.08e+06/6.72e+07 =  3% of the original kernel matrix.

torch.Size([22117, 2])
We keep 3.40e+06/1.69e+08 =  2% of the original kernel matrix.

torch.Size([17541, 2])
We keep 2.88e+06/1.14e+08 =  2% of the original kernel matrix.

torch.Size([23966, 2])
We keep 4.12e+06/2.20e+08 =  1% of the original kernel matrix.

torch.Size([12352, 2])
We keep 3.26e+06/5.03e+07 =  6% of the original kernel matrix.

torch.Size([20064, 2])
We keep 2.93e+06/1.46e+08 =  2% of the original kernel matrix.

torch.Size([5373, 2])
We keep 4.59e+05/7.78e+06 =  5% of the original kernel matrix.

torch.Size([13728, 2])
We keep 1.54e+06/5.75e+07 =  2% of the original kernel matrix.

torch.Size([30936, 2])
We keep 1.13e+07/4.36e+08 =  2% of the original kernel matrix.

torch.Size([32322, 2])
We keep 7.05e+06/4.30e+08 =  1% of the original kernel matrix.

torch.Size([23960, 2])
We keep 8.26e+06/2.66e+08 =  3% of the original kernel matrix.

torch.Size([28495, 2])
We keep 5.85e+06/3.36e+08 =  1% of the original kernel matrix.

torch.Size([17668, 2])
We keep 3.74e+06/1.22e+08 =  3% of the original kernel matrix.

torch.Size([23939, 2])
We keep 4.26e+06/2.27e+08 =  1% of the original kernel matrix.

torch.Size([13962, 2])
We keep 1.95e+06/6.89e+07 =  2% of the original kernel matrix.

torch.Size([21126, 2])
We keep 3.41e+06/1.71e+08 =  1% of the original kernel matrix.

torch.Size([78502, 2])
We keep 5.27e+07/2.62e+09 =  2% of the original kernel matrix.

torch.Size([50071, 2])
We keep 1.48e+07/1.05e+09 =  1% of the original kernel matrix.

torch.Size([35322, 2])
We keep 9.86e+06/5.81e+08 =  1% of the original kernel matrix.

torch.Size([34617, 2])
We keep 7.82e+06/4.97e+08 =  1% of the original kernel matrix.

torch.Size([1608731, 2])
We keep 6.32e+09/1.00e+12 =  0% of the original kernel matrix.

torch.Size([248945, 2])
We keep 2.06e+08/2.06e+10 =  0% of the original kernel matrix.

torch.Size([8715, 2])
We keep 1.12e+06/1.93e+07 =  5% of the original kernel matrix.

torch.Size([16990, 2])
We keep 2.13e+06/9.05e+07 =  2% of the original kernel matrix.

torch.Size([62554, 2])
We keep 2.73e+07/1.76e+09 =  1% of the original kernel matrix.

torch.Size([45216, 2])
We keep 1.23e+07/8.64e+08 =  1% of the original kernel matrix.

torch.Size([27617, 2])
We keep 1.23e+07/4.09e+08 =  3% of the original kernel matrix.

torch.Size([30366, 2])
We keep 6.91e+06/4.17e+08 =  1% of the original kernel matrix.

torch.Size([5048, 2])
We keep 2.75e+05/5.70e+06 =  4% of the original kernel matrix.

torch.Size([13429, 2])
We keep 1.37e+06/4.92e+07 =  2% of the original kernel matrix.

torch.Size([265909, 2])
We keep 4.63e+08/3.90e+10 =  1% of the original kernel matrix.

torch.Size([96900, 2])
We keep 4.71e+07/4.07e+09 =  1% of the original kernel matrix.

torch.Size([215204, 2])
We keep 9.43e+08/3.26e+10 =  2% of the original kernel matrix.

torch.Size([87090, 2])
We keep 4.16e+07/3.72e+09 =  1% of the original kernel matrix.

torch.Size([768350, 2])
We keep 1.93e+09/2.52e+11 =  0% of the original kernel matrix.

torch.Size([169830, 2])
We keep 1.09e+08/1.03e+10 =  1% of the original kernel matrix.

torch.Size([82721, 2])
We keep 9.07e+07/3.28e+09 =  2% of the original kernel matrix.

torch.Size([51918, 2])
We keep 1.60e+07/1.18e+09 =  1% of the original kernel matrix.

torch.Size([40281, 2])
We keep 1.86e+07/7.94e+08 =  2% of the original kernel matrix.

torch.Size([36998, 2])
We keep 8.94e+06/5.81e+08 =  1% of the original kernel matrix.

torch.Size([10226, 2])
We keep 9.53e+05/2.63e+07 =  3% of the original kernel matrix.

torch.Size([18166, 2])
We keep 2.38e+06/1.06e+08 =  2% of the original kernel matrix.

torch.Size([115428, 2])
We keep 1.82e+08/8.55e+09 =  2% of the original kernel matrix.

torch.Size([61490, 2])
We keep 2.47e+07/1.90e+09 =  1% of the original kernel matrix.

torch.Size([1249923, 2])
We keep 8.57e+09/8.79e+11 =  0% of the original kernel matrix.

torch.Size([213576, 2])
We keep 1.89e+08/1.93e+10 =  0% of the original kernel matrix.

torch.Size([65707, 2])
We keep 8.36e+07/2.63e+09 =  3% of the original kernel matrix.

torch.Size([46741, 2])
We keep 1.50e+07/1.06e+09 =  1% of the original kernel matrix.

torch.Size([21043, 2])
We keep 6.21e+06/1.81e+08 =  3% of the original kernel matrix.

torch.Size([26579, 2])
We keep 5.04e+06/2.77e+08 =  1% of the original kernel matrix.

torch.Size([88440, 2])
We keep 9.21e+07/4.41e+09 =  2% of the original kernel matrix.

torch.Size([53802, 2])
We keep 1.85e+07/1.37e+09 =  1% of the original kernel matrix.

torch.Size([387385, 2])
We keep 7.81e+08/7.57e+10 =  1% of the original kernel matrix.

torch.Size([117246, 2])
We keep 6.25e+07/5.67e+09 =  1% of the original kernel matrix.

torch.Size([113642, 2])
We keep 8.86e+07/6.32e+09 =  1% of the original kernel matrix.

torch.Size([60441, 2])
We keep 2.11e+07/1.64e+09 =  1% of the original kernel matrix.

torch.Size([20068, 2])
We keep 6.41e+06/1.83e+08 =  3% of the original kernel matrix.

torch.Size([25660, 2])
We keep 5.01e+06/2.79e+08 =  1% of the original kernel matrix.

torch.Size([11798, 2])
We keep 1.33e+06/4.09e+07 =  3% of the original kernel matrix.

torch.Size([19514, 2])
We keep 2.83e+06/1.32e+08 =  2% of the original kernel matrix.

torch.Size([17473, 2])
We keep 2.72e+06/1.05e+08 =  2% of the original kernel matrix.

torch.Size([23871, 2])
We keep 3.98e+06/2.11e+08 =  1% of the original kernel matrix.

torch.Size([16498, 2])
We keep 3.76e+06/9.64e+07 =  3% of the original kernel matrix.

torch.Size([23297, 2])
We keep 3.87e+06/2.02e+08 =  1% of the original kernel matrix.

torch.Size([460820, 2])
We keep 1.19e+09/1.01e+11 =  1% of the original kernel matrix.

torch.Size([127976, 2])
We keep 7.15e+07/6.56e+09 =  1% of the original kernel matrix.

torch.Size([21999, 2])
We keep 6.50e+06/2.03e+08 =  3% of the original kernel matrix.

torch.Size([27185, 2])
We keep 5.20e+06/2.93e+08 =  1% of the original kernel matrix.

torch.Size([9407, 2])
We keep 2.15e+06/2.77e+07 =  7% of the original kernel matrix.

torch.Size([17420, 2])
We keep 2.50e+06/1.08e+08 =  2% of the original kernel matrix.

torch.Size([64180, 2])
We keep 5.35e+07/1.85e+09 =  2% of the original kernel matrix.

torch.Size([45880, 2])
We keep 1.22e+07/8.86e+08 =  1% of the original kernel matrix.

torch.Size([29693, 2])
We keep 8.81e+06/3.85e+08 =  2% of the original kernel matrix.

torch.Size([30529, 2])
We keep 6.33e+06/4.04e+08 =  1% of the original kernel matrix.

torch.Size([3882, 2])
We keep 1.66e+05/3.06e+06 =  5% of the original kernel matrix.

torch.Size([12070, 2])
We keep 1.10e+06/3.61e+07 =  3% of the original kernel matrix.

torch.Size([100981, 2])
We keep 8.68e+07/5.03e+09 =  1% of the original kernel matrix.

torch.Size([57215, 2])
We keep 1.90e+07/1.46e+09 =  1% of the original kernel matrix.

torch.Size([81709, 2])
We keep 6.48e+07/3.31e+09 =  1% of the original kernel matrix.

torch.Size([51744, 2])
We keep 1.62e+07/1.19e+09 =  1% of the original kernel matrix.

torch.Size([75670, 2])
We keep 1.21e+08/2.86e+09 =  4% of the original kernel matrix.

torch.Size([49486, 2])
We keep 1.46e+07/1.10e+09 =  1% of the original kernel matrix.

torch.Size([140336, 2])
We keep 1.53e+08/1.01e+10 =  1% of the original kernel matrix.

torch.Size([69461, 2])
We keep 2.56e+07/2.07e+09 =  1% of the original kernel matrix.

torch.Size([135594, 2])
We keep 1.35e+08/9.07e+09 =  1% of the original kernel matrix.

torch.Size([67320, 2])
We keep 2.40e+07/1.96e+09 =  1% of the original kernel matrix.

torch.Size([39494, 2])
We keep 1.48e+07/7.66e+08 =  1% of the original kernel matrix.

torch.Size([36175, 2])
We keep 8.76e+06/5.70e+08 =  1% of the original kernel matrix.

torch.Size([122745, 2])
We keep 9.60e+07/7.40e+09 =  1% of the original kernel matrix.

torch.Size([63468, 2])
We keep 2.29e+07/1.77e+09 =  1% of the original kernel matrix.

torch.Size([139011, 2])
We keep 9.70e+07/8.82e+09 =  1% of the original kernel matrix.

torch.Size([68354, 2])
We keep 2.43e+07/1.93e+09 =  1% of the original kernel matrix.

torch.Size([434325, 2])
We keep 8.57e+08/9.02e+10 =  0% of the original kernel matrix.

torch.Size([123478, 2])
We keep 6.75e+07/6.19e+09 =  1% of the original kernel matrix.

torch.Size([18637, 2])
We keep 3.60e+06/1.23e+08 =  2% of the original kernel matrix.

torch.Size([24909, 2])
We keep 4.28e+06/2.28e+08 =  1% of the original kernel matrix.

torch.Size([112855, 2])
We keep 5.80e+08/9.41e+09 =  6% of the original kernel matrix.

torch.Size([61606, 2])
We keep 2.46e+07/2.00e+09 =  1% of the original kernel matrix.

torch.Size([93893, 2])
We keep 1.30e+08/4.48e+09 =  2% of the original kernel matrix.

torch.Size([55490, 2])
We keep 1.77e+07/1.38e+09 =  1% of the original kernel matrix.

torch.Size([96445, 2])
We keep 8.94e+07/4.58e+09 =  1% of the original kernel matrix.

torch.Size([56359, 2])
We keep 1.84e+07/1.39e+09 =  1% of the original kernel matrix.

torch.Size([34254, 2])
We keep 1.82e+07/5.93e+08 =  3% of the original kernel matrix.

torch.Size([34130, 2])
We keep 7.66e+06/5.02e+08 =  1% of the original kernel matrix.

torch.Size([14185, 2])
We keep 1.99e+07/7.58e+07 = 26% of the original kernel matrix.

torch.Size([21553, 2])
We keep 3.46e+06/1.79e+08 =  1% of the original kernel matrix.

torch.Size([22122, 2])
We keep 4.86e+06/2.16e+08 =  2% of the original kernel matrix.

torch.Size([26940, 2])
We keep 5.27e+06/3.03e+08 =  1% of the original kernel matrix.

torch.Size([21450, 2])
We keep 4.23e+06/1.89e+08 =  2% of the original kernel matrix.

torch.Size([26754, 2])
We keep 5.05e+06/2.83e+08 =  1% of the original kernel matrix.

torch.Size([106720, 2])
We keep 1.15e+08/5.94e+09 =  1% of the original kernel matrix.

torch.Size([59477, 2])
We keep 2.07e+07/1.59e+09 =  1% of the original kernel matrix.

torch.Size([247936, 2])
We keep 3.50e+08/3.24e+10 =  1% of the original kernel matrix.

torch.Size([94459, 2])
We keep 4.32e+07/3.71e+09 =  1% of the original kernel matrix.

torch.Size([184462, 2])
We keep 1.53e+08/1.58e+10 =  0% of the original kernel matrix.

torch.Size([79666, 2])
We keep 3.14e+07/2.59e+09 =  1% of the original kernel matrix.

torch.Size([231961, 2])
We keep 5.02e+08/2.90e+10 =  1% of the original kernel matrix.

torch.Size([89836, 2])
We keep 4.18e+07/3.51e+09 =  1% of the original kernel matrix.

torch.Size([109185, 2])
We keep 1.17e+08/6.03e+09 =  1% of the original kernel matrix.

torch.Size([59753, 2])
We keep 2.03e+07/1.60e+09 =  1% of the original kernel matrix.

torch.Size([135499, 2])
We keep 1.88e+08/9.67e+09 =  1% of the original kernel matrix.

torch.Size([67338, 2])
We keep 2.58e+07/2.03e+09 =  1% of the original kernel matrix.

torch.Size([19346, 2])
We keep 3.59e+06/1.57e+08 =  2% of the original kernel matrix.

torch.Size([25169, 2])
We keep 4.57e+06/2.58e+08 =  1% of the original kernel matrix.

torch.Size([255066, 2])
We keep 3.13e+08/3.39e+10 =  0% of the original kernel matrix.

torch.Size([95887, 2])
We keep 4.35e+07/3.79e+09 =  1% of the original kernel matrix.

torch.Size([5383, 2])
We keep 2.92e+05/5.93e+06 =  4% of the original kernel matrix.

torch.Size([13862, 2])
We keep 1.37e+06/5.02e+07 =  2% of the original kernel matrix.

torch.Size([30278, 2])
We keep 8.27e+06/4.25e+08 =  1% of the original kernel matrix.

torch.Size([32044, 2])
We keep 6.96e+06/4.25e+08 =  1% of the original kernel matrix.

torch.Size([356071, 2])
We keep 5.20e+08/5.92e+10 =  0% of the original kernel matrix.

torch.Size([112082, 2])
We keep 5.66e+07/5.01e+09 =  1% of the original kernel matrix.

torch.Size([59428, 2])
We keep 2.64e+07/1.79e+09 =  1% of the original kernel matrix.

torch.Size([44291, 2])
We keep 1.25e+07/8.72e+08 =  1% of the original kernel matrix.

torch.Size([7807, 2])
We keep 5.43e+05/1.36e+07 =  3% of the original kernel matrix.

torch.Size([16031, 2])
We keep 1.84e+06/7.60e+07 =  2% of the original kernel matrix.

torch.Size([41084, 2])
We keep 2.52e+07/8.41e+08 =  2% of the original kernel matrix.

torch.Size([36756, 2])
We keep 9.07e+06/5.98e+08 =  1% of the original kernel matrix.

torch.Size([178989, 2])
We keep 3.05e+08/2.26e+10 =  1% of the original kernel matrix.

torch.Size([78983, 2])
We keep 3.65e+07/3.10e+09 =  1% of the original kernel matrix.

torch.Size([65003, 2])
We keep 5.43e+07/2.02e+09 =  2% of the original kernel matrix.

torch.Size([45821, 2])
We keep 1.33e+07/9.27e+08 =  1% of the original kernel matrix.

torch.Size([100482, 2])
We keep 6.07e+07/4.52e+09 =  1% of the original kernel matrix.

torch.Size([57222, 2])
We keep 1.85e+07/1.39e+09 =  1% of the original kernel matrix.

torch.Size([12047, 2])
We keep 1.23e+06/3.94e+07 =  3% of the original kernel matrix.

torch.Size([19608, 2])
We keep 2.74e+06/1.29e+08 =  2% of the original kernel matrix.

torch.Size([9718, 2])
We keep 1.08e+06/2.48e+07 =  4% of the original kernel matrix.

torch.Size([17889, 2])
We keep 2.36e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([11873, 2])
We keep 1.86e+06/4.46e+07 =  4% of the original kernel matrix.

torch.Size([19558, 2])
We keep 2.91e+06/1.38e+08 =  2% of the original kernel matrix.

torch.Size([70958, 2])
We keep 1.20e+08/3.46e+09 =  3% of the original kernel matrix.

torch.Size([47219, 2])
We keep 1.68e+07/1.21e+09 =  1% of the original kernel matrix.

torch.Size([52628, 2])
We keep 2.35e+07/1.42e+09 =  1% of the original kernel matrix.

torch.Size([42469, 2])
We keep 1.14e+07/7.77e+08 =  1% of the original kernel matrix.

torch.Size([95065, 2])
We keep 1.02e+08/4.05e+09 =  2% of the original kernel matrix.

torch.Size([55114, 2])
We keep 1.73e+07/1.31e+09 =  1% of the original kernel matrix.

torch.Size([5906, 2])
We keep 3.49e+05/7.98e+06 =  4% of the original kernel matrix.

torch.Size([14245, 2])
We keep 1.54e+06/5.82e+07 =  2% of the original kernel matrix.

torch.Size([133378, 2])
We keep 1.31e+08/8.42e+09 =  1% of the original kernel matrix.

torch.Size([66696, 2])
We keep 2.40e+07/1.89e+09 =  1% of the original kernel matrix.

torch.Size([22763, 2])
We keep 8.76e+06/2.93e+08 =  2% of the original kernel matrix.

torch.Size([27530, 2])
We keep 5.94e+06/3.53e+08 =  1% of the original kernel matrix.

torch.Size([50134, 2])
We keep 1.83e+07/1.16e+09 =  1% of the original kernel matrix.

torch.Size([42029, 2])
We keep 1.02e+07/7.01e+08 =  1% of the original kernel matrix.

torch.Size([15685, 2])
We keep 1.18e+07/1.25e+08 =  9% of the original kernel matrix.

torch.Size([22650, 2])
We keep 4.41e+06/2.30e+08 =  1% of the original kernel matrix.

torch.Size([207853, 2])
We keep 2.17e+08/2.02e+10 =  1% of the original kernel matrix.

torch.Size([84584, 2])
We keep 3.45e+07/2.93e+09 =  1% of the original kernel matrix.

torch.Size([5640, 2])
We keep 3.16e+05/6.44e+06 =  4% of the original kernel matrix.

torch.Size([14061, 2])
We keep 1.41e+06/5.23e+07 =  2% of the original kernel matrix.

torch.Size([116434, 2])
We keep 1.34e+08/6.62e+09 =  2% of the original kernel matrix.

torch.Size([61522, 2])
We keep 2.18e+07/1.68e+09 =  1% of the original kernel matrix.

torch.Size([52066, 2])
We keep 2.51e+07/1.23e+09 =  2% of the original kernel matrix.

torch.Size([41741, 2])
We keep 1.07e+07/7.21e+08 =  1% of the original kernel matrix.

torch.Size([30565, 2])
We keep 1.86e+07/4.73e+08 =  3% of the original kernel matrix.

torch.Size([31986, 2])
We keep 7.41e+06/4.48e+08 =  1% of the original kernel matrix.

torch.Size([44098, 2])
We keep 1.59e+07/1.00e+09 =  1% of the original kernel matrix.

torch.Size([38542, 2])
We keep 9.85e+06/6.53e+08 =  1% of the original kernel matrix.

torch.Size([465457, 2])
We keep 1.29e+09/1.03e+11 =  1% of the original kernel matrix.

torch.Size([128617, 2])
We keep 7.27e+07/6.61e+09 =  1% of the original kernel matrix.

torch.Size([27018, 2])
We keep 5.80e+06/3.18e+08 =  1% of the original kernel matrix.

torch.Size([30516, 2])
We keep 6.08e+06/3.67e+08 =  1% of the original kernel matrix.

torch.Size([23078, 2])
We keep 6.54e+06/2.35e+08 =  2% of the original kernel matrix.

torch.Size([27718, 2])
We keep 5.41e+06/3.16e+08 =  1% of the original kernel matrix.

torch.Size([157909, 2])
We keep 2.04e+08/1.28e+10 =  1% of the original kernel matrix.

torch.Size([72970, 2])
We keep 2.89e+07/2.33e+09 =  1% of the original kernel matrix.

torch.Size([41571, 2])
We keep 1.33e+07/8.12e+08 =  1% of the original kernel matrix.

torch.Size([37320, 2])
We keep 8.96e+06/5.87e+08 =  1% of the original kernel matrix.

torch.Size([649363, 2])
We keep 1.45e+09/1.89e+11 =  0% of the original kernel matrix.

torch.Size([155251, 2])
We keep 9.52e+07/8.95e+09 =  1% of the original kernel matrix.

torch.Size([27108, 2])
We keep 7.99e+06/3.50e+08 =  2% of the original kernel matrix.

torch.Size([30623, 2])
We keep 6.63e+06/3.85e+08 =  1% of the original kernel matrix.

torch.Size([170471, 2])
We keep 1.69e+08/1.58e+10 =  1% of the original kernel matrix.

torch.Size([76413, 2])
We keep 3.14e+07/2.59e+09 =  1% of the original kernel matrix.

torch.Size([548154, 2])
We keep 9.42e+08/1.28e+11 =  0% of the original kernel matrix.

torch.Size([141253, 2])
We keep 8.01e+07/7.38e+09 =  1% of the original kernel matrix.

torch.Size([18141, 2])
We keep 3.50e+06/1.18e+08 =  2% of the original kernel matrix.

torch.Size([24302, 2])
We keep 4.09e+06/2.24e+08 =  1% of the original kernel matrix.

torch.Size([165841, 2])
We keep 2.82e+08/1.71e+10 =  1% of the original kernel matrix.

torch.Size([74676, 2])
We keep 3.27e+07/2.70e+09 =  1% of the original kernel matrix.

torch.Size([22723, 2])
We keep 1.08e+07/3.63e+08 =  2% of the original kernel matrix.

torch.Size([26859, 2])
We keep 6.43e+06/3.93e+08 =  1% of the original kernel matrix.

torch.Size([221513, 2])
We keep 2.85e+08/2.44e+10 =  1% of the original kernel matrix.

torch.Size([87779, 2])
We keep 3.82e+07/3.22e+09 =  1% of the original kernel matrix.

torch.Size([48659, 2])
We keep 1.71e+07/1.17e+09 =  1% of the original kernel matrix.

torch.Size([40454, 2])
We keep 1.04e+07/7.04e+08 =  1% of the original kernel matrix.

torch.Size([57406, 2])
We keep 3.49e+07/1.61e+09 =  2% of the original kernel matrix.

torch.Size([43326, 2])
We keep 1.22e+07/8.26e+08 =  1% of the original kernel matrix.

torch.Size([3025, 2])
We keep 1.25e+05/1.95e+06 =  6% of the original kernel matrix.

torch.Size([11059, 2])
We keep 9.55e+05/2.88e+07 =  3% of the original kernel matrix.

torch.Size([11820, 2])
We keep 1.47e+06/3.95e+07 =  3% of the original kernel matrix.

torch.Size([19443, 2])
We keep 2.77e+06/1.30e+08 =  2% of the original kernel matrix.

torch.Size([9928, 2])
We keep 1.11e+06/2.79e+07 =  3% of the original kernel matrix.

torch.Size([17789, 2])
We keep 2.40e+06/1.09e+08 =  2% of the original kernel matrix.

torch.Size([44504, 2])
We keep 1.49e+07/8.96e+08 =  1% of the original kernel matrix.

torch.Size([38754, 2])
We keep 9.19e+06/6.17e+08 =  1% of the original kernel matrix.

torch.Size([32130, 2])
We keep 1.50e+07/5.27e+08 =  2% of the original kernel matrix.

torch.Size([32786, 2])
We keep 7.78e+06/4.73e+08 =  1% of the original kernel matrix.

torch.Size([194656, 2])
We keep 2.78e+08/2.04e+10 =  1% of the original kernel matrix.

torch.Size([82931, 2])
We keep 3.48e+07/2.94e+09 =  1% of the original kernel matrix.

torch.Size([29227, 2])
We keep 7.00e+06/3.97e+08 =  1% of the original kernel matrix.

torch.Size([32326, 2])
We keep 6.84e+06/4.10e+08 =  1% of the original kernel matrix.

torch.Size([16310, 2])
We keep 2.95e+06/9.02e+07 =  3% of the original kernel matrix.

torch.Size([23098, 2])
We keep 3.83e+06/1.96e+08 =  1% of the original kernel matrix.

torch.Size([21640, 2])
We keep 5.23e+06/1.96e+08 =  2% of the original kernel matrix.

torch.Size([26951, 2])
We keep 4.88e+06/2.89e+08 =  1% of the original kernel matrix.

torch.Size([17514, 2])
We keep 9.38e+06/1.28e+08 =  7% of the original kernel matrix.

torch.Size([24233, 2])
We keep 4.22e+06/2.33e+08 =  1% of the original kernel matrix.

torch.Size([7046, 2])
We keep 5.65e+05/1.21e+07 =  4% of the original kernel matrix.

torch.Size([15697, 2])
We keep 1.77e+06/7.16e+07 =  2% of the original kernel matrix.

torch.Size([211803, 2])
We keep 2.37e+08/2.36e+10 =  1% of the original kernel matrix.

torch.Size([86775, 2])
We keep 3.76e+07/3.16e+09 =  1% of the original kernel matrix.

torch.Size([75187, 2])
We keep 1.02e+08/2.93e+09 =  3% of the original kernel matrix.

torch.Size([49527, 2])
We keep 1.55e+07/1.12e+09 =  1% of the original kernel matrix.

torch.Size([7451, 2])
We keep 6.92e+05/1.57e+07 =  4% of the original kernel matrix.

torch.Size([15955, 2])
We keep 1.97e+06/8.15e+07 =  2% of the original kernel matrix.

torch.Size([7748, 2])
We keep 6.30e+05/1.41e+07 =  4% of the original kernel matrix.

torch.Size([15960, 2])
We keep 1.90e+06/7.75e+07 =  2% of the original kernel matrix.

torch.Size([12641, 2])
We keep 2.85e+06/6.01e+07 =  4% of the original kernel matrix.

torch.Size([20209, 2])
We keep 3.29e+06/1.60e+08 =  2% of the original kernel matrix.

torch.Size([796814, 2])
We keep 3.11e+09/2.89e+11 =  1% of the original kernel matrix.

torch.Size([176046, 2])
We keep 1.17e+08/1.11e+10 =  1% of the original kernel matrix.

torch.Size([16695, 2])
We keep 2.28e+06/9.06e+07 =  2% of the original kernel matrix.

torch.Size([23445, 2])
We keep 3.71e+06/1.96e+08 =  1% of the original kernel matrix.

torch.Size([31653, 2])
We keep 1.13e+07/4.69e+08 =  2% of the original kernel matrix.

torch.Size([33047, 2])
We keep 7.21e+06/4.46e+08 =  1% of the original kernel matrix.

torch.Size([105502, 2])
We keep 8.31e+07/5.46e+09 =  1% of the original kernel matrix.

torch.Size([58408, 2])
We keep 2.02e+07/1.52e+09 =  1% of the original kernel matrix.

torch.Size([21807, 2])
We keep 2.63e+07/3.44e+08 =  7% of the original kernel matrix.

torch.Size([26776, 2])
We keep 6.64e+06/3.82e+08 =  1% of the original kernel matrix.

torch.Size([12636, 2])
We keep 2.38e+06/5.47e+07 =  4% of the original kernel matrix.

torch.Size([20283, 2])
We keep 3.13e+06/1.52e+08 =  2% of the original kernel matrix.

torch.Size([12638, 2])
We keep 1.68e+06/5.18e+07 =  3% of the original kernel matrix.

torch.Size([20060, 2])
We keep 3.05e+06/1.48e+08 =  2% of the original kernel matrix.

torch.Size([89497, 2])
We keep 1.06e+08/4.46e+09 =  2% of the original kernel matrix.

torch.Size([54398, 2])
We keep 1.80e+07/1.38e+09 =  1% of the original kernel matrix.

torch.Size([85813, 2])
We keep 1.11e+08/3.43e+09 =  3% of the original kernel matrix.

torch.Size([52793, 2])
We keep 1.66e+07/1.21e+09 =  1% of the original kernel matrix.

torch.Size([77803, 2])
We keep 7.45e+07/3.72e+09 =  2% of the original kernel matrix.

torch.Size([49686, 2])
We keep 1.73e+07/1.26e+09 =  1% of the original kernel matrix.

torch.Size([19945, 2])
We keep 4.44e+06/1.60e+08 =  2% of the original kernel matrix.

torch.Size([25588, 2])
We keep 4.70e+06/2.61e+08 =  1% of the original kernel matrix.

torch.Size([14163, 2])
We keep 1.82e+06/6.36e+07 =  2% of the original kernel matrix.

torch.Size([21412, 2])
We keep 3.30e+06/1.64e+08 =  2% of the original kernel matrix.

torch.Size([66037, 2])
We keep 3.56e+07/2.13e+09 =  1% of the original kernel matrix.

torch.Size([46416, 2])
We keep 1.33e+07/9.52e+08 =  1% of the original kernel matrix.

torch.Size([33837, 2])
We keep 1.94e+08/7.70e+08 = 25% of the original kernel matrix.

torch.Size([33595, 2])
We keep 8.53e+06/5.72e+08 =  1% of the original kernel matrix.

torch.Size([136334, 2])
We keep 1.44e+08/9.37e+09 =  1% of the original kernel matrix.

torch.Size([67683, 2])
We keep 2.53e+07/1.99e+09 =  1% of the original kernel matrix.

torch.Size([7695, 2])
We keep 1.56e+06/1.82e+07 =  8% of the original kernel matrix.

torch.Size([16013, 2])
We keep 1.96e+06/8.79e+07 =  2% of the original kernel matrix.

torch.Size([146213, 2])
We keep 1.21e+08/1.00e+10 =  1% of the original kernel matrix.

torch.Size([70115, 2])
We keep 2.61e+07/2.06e+09 =  1% of the original kernel matrix.

torch.Size([7999, 2])
We keep 6.96e+05/1.57e+07 =  4% of the original kernel matrix.

torch.Size([16356, 2])
We keep 1.95e+06/8.16e+07 =  2% of the original kernel matrix.

torch.Size([87465, 2])
We keep 8.11e+07/3.72e+09 =  2% of the original kernel matrix.

torch.Size([53015, 2])
We keep 1.70e+07/1.26e+09 =  1% of the original kernel matrix.

torch.Size([21659, 2])
We keep 7.59e+06/2.60e+08 =  2% of the original kernel matrix.

torch.Size([26504, 2])
We keep 5.68e+06/3.32e+08 =  1% of the original kernel matrix.

torch.Size([10850, 2])
We keep 1.15e+06/3.15e+07 =  3% of the original kernel matrix.

torch.Size([18684, 2])
We keep 2.55e+06/1.16e+08 =  2% of the original kernel matrix.

torch.Size([21532, 2])
We keep 4.39e+06/1.91e+08 =  2% of the original kernel matrix.

torch.Size([26841, 2])
We keep 4.99e+06/2.85e+08 =  1% of the original kernel matrix.

torch.Size([117202, 2])
We keep 9.52e+07/6.65e+09 =  1% of the original kernel matrix.

torch.Size([61641, 2])
We keep 2.19e+07/1.68e+09 =  1% of the original kernel matrix.

torch.Size([34426, 2])
We keep 1.23e+07/5.96e+08 =  2% of the original kernel matrix.

torch.Size([33654, 2])
We keep 7.84e+06/5.03e+08 =  1% of the original kernel matrix.

torch.Size([72849, 2])
We keep 4.57e+07/2.50e+09 =  1% of the original kernel matrix.

torch.Size([48254, 2])
We keep 1.45e+07/1.03e+09 =  1% of the original kernel matrix.

torch.Size([9015, 2])
We keep 6.32e+06/3.50e+07 = 18% of the original kernel matrix.

torch.Size([17312, 2])
We keep 2.62e+06/1.22e+08 =  2% of the original kernel matrix.

torch.Size([198944, 2])
We keep 6.56e+08/2.99e+10 =  2% of the original kernel matrix.

torch.Size([81686, 2])
We keep 4.10e+07/3.56e+09 =  1% of the original kernel matrix.

torch.Size([5058, 2])
We keep 3.09e+05/5.45e+06 =  5% of the original kernel matrix.

torch.Size([13361, 2])
We keep 1.35e+06/4.81e+07 =  2% of the original kernel matrix.

torch.Size([7206, 2])
We keep 8.90e+05/1.54e+07 =  5% of the original kernel matrix.

torch.Size([15549, 2])
We keep 1.87e+06/8.09e+07 =  2% of the original kernel matrix.

torch.Size([14444, 2])
We keep 7.99e+06/1.30e+08 =  6% of the original kernel matrix.

torch.Size([21524, 2])
We keep 4.32e+06/2.35e+08 =  1% of the original kernel matrix.

torch.Size([35007, 2])
We keep 1.44e+07/6.01e+08 =  2% of the original kernel matrix.

torch.Size([34030, 2])
We keep 8.01e+06/5.05e+08 =  1% of the original kernel matrix.

torch.Size([7245, 2])
We keep 4.99e+05/1.17e+07 =  4% of the original kernel matrix.

torch.Size([15677, 2])
We keep 1.73e+06/7.04e+07 =  2% of the original kernel matrix.

torch.Size([22264, 2])
We keep 3.99e+06/1.97e+08 =  2% of the original kernel matrix.

torch.Size([27474, 2])
We keep 5.07e+06/2.89e+08 =  1% of the original kernel matrix.

torch.Size([40695, 2])
We keep 1.28e+07/7.66e+08 =  1% of the original kernel matrix.

torch.Size([37260, 2])
We keep 8.80e+06/5.70e+08 =  1% of the original kernel matrix.

torch.Size([160806, 2])
We keep 2.95e+08/1.60e+10 =  1% of the original kernel matrix.

torch.Size([73955, 2])
We keep 3.21e+07/2.60e+09 =  1% of the original kernel matrix.

torch.Size([10580, 2])
We keep 1.11e+06/2.96e+07 =  3% of the original kernel matrix.

torch.Size([18507, 2])
We keep 2.49e+06/1.12e+08 =  2% of the original kernel matrix.

torch.Size([5577, 2])
We keep 3.83e+05/7.27e+06 =  5% of the original kernel matrix.

torch.Size([13877, 2])
We keep 1.48e+06/5.56e+07 =  2% of the original kernel matrix.

torch.Size([15084, 2])
We keep 2.23e+06/7.20e+07 =  3% of the original kernel matrix.

torch.Size([22122, 2])
We keep 3.47e+06/1.75e+08 =  1% of the original kernel matrix.

torch.Size([297430, 2])
We keep 7.65e+08/5.47e+10 =  1% of the original kernel matrix.

torch.Size([101980, 2])
We keep 5.53e+07/4.82e+09 =  1% of the original kernel matrix.

torch.Size([1126917, 2])
We keep 4.99e+09/5.11e+11 =  0% of the original kernel matrix.

torch.Size([210477, 2])
We keep 1.47e+08/1.47e+10 =  1% of the original kernel matrix.

torch.Size([37202, 2])
We keep 1.40e+07/6.83e+08 =  2% of the original kernel matrix.

torch.Size([35102, 2])
We keep 8.37e+06/5.39e+08 =  1% of the original kernel matrix.

torch.Size([11973, 2])
We keep 1.43e+06/4.44e+07 =  3% of the original kernel matrix.

torch.Size([19542, 2])
We keep 2.88e+06/1.37e+08 =  2% of the original kernel matrix.

torch.Size([295570, 2])
We keep 5.57e+08/4.53e+10 =  1% of the original kernel matrix.

torch.Size([101978, 2])
We keep 5.12e+07/4.39e+09 =  1% of the original kernel matrix.

torch.Size([211539, 2])
We keep 2.13e+08/2.16e+10 =  0% of the original kernel matrix.

torch.Size([85562, 2])
We keep 3.61e+07/3.02e+09 =  1% of the original kernel matrix.

torch.Size([6090, 2])
We keep 3.66e+05/7.85e+06 =  4% of the original kernel matrix.

torch.Size([14491, 2])
We keep 1.52e+06/5.77e+07 =  2% of the original kernel matrix.

torch.Size([16017, 2])
We keep 3.78e+06/9.15e+07 =  4% of the original kernel matrix.

torch.Size([22891, 2])
We keep 3.78e+06/1.97e+08 =  1% of the original kernel matrix.

torch.Size([77012, 2])
We keep 3.42e+07/2.54e+09 =  1% of the original kernel matrix.

torch.Size([49739, 2])
We keep 1.44e+07/1.04e+09 =  1% of the original kernel matrix.

torch.Size([370976, 2])
We keep 1.83e+09/1.36e+11 =  1% of the original kernel matrix.

torch.Size([111706, 2])
We keep 8.24e+07/7.60e+09 =  1% of the original kernel matrix.

torch.Size([1114571, 2])
We keep 6.38e+09/6.44e+11 =  0% of the original kernel matrix.

torch.Size([203555, 2])
We keep 1.66e+08/1.65e+10 =  1% of the original kernel matrix.

torch.Size([185126, 2])
We keep 1.84e+08/1.71e+10 =  1% of the original kernel matrix.

torch.Size([79760, 2])
We keep 3.22e+07/2.69e+09 =  1% of the original kernel matrix.

torch.Size([42749, 2])
We keep 1.80e+07/8.34e+08 =  2% of the original kernel matrix.

torch.Size([38223, 2])
We keep 9.03e+06/5.95e+08 =  1% of the original kernel matrix.

torch.Size([14534, 2])
We keep 1.92e+06/6.65e+07 =  2% of the original kernel matrix.

torch.Size([21886, 2])
We keep 3.38e+06/1.68e+08 =  2% of the original kernel matrix.

torch.Size([9090, 2])
We keep 1.03e+06/2.13e+07 =  4% of the original kernel matrix.

torch.Size([17264, 2])
We keep 2.23e+06/9.50e+07 =  2% of the original kernel matrix.

torch.Size([132946, 2])
We keep 2.02e+08/8.97e+09 =  2% of the original kernel matrix.

torch.Size([66266, 2])
We keep 2.42e+07/1.95e+09 =  1% of the original kernel matrix.

torch.Size([25705, 2])
We keep 5.90e+06/2.74e+08 =  2% of the original kernel matrix.

torch.Size([29629, 2])
We keep 5.85e+06/3.41e+08 =  1% of the original kernel matrix.

torch.Size([37960, 2])
We keep 5.85e+07/1.30e+09 =  4% of the original kernel matrix.

torch.Size([34541, 2])
We keep 1.08e+07/7.43e+08 =  1% of the original kernel matrix.

torch.Size([165188, 2])
We keep 2.17e+08/1.49e+10 =  1% of the original kernel matrix.

torch.Size([75515, 2])
We keep 3.08e+07/2.52e+09 =  1% of the original kernel matrix.

torch.Size([615952, 2])
We keep 1.33e+09/1.65e+11 =  0% of the original kernel matrix.

torch.Size([149220, 2])
We keep 8.99e+07/8.37e+09 =  1% of the original kernel matrix.

torch.Size([12827, 2])
We keep 1.63e+06/5.29e+07 =  3% of the original kernel matrix.

torch.Size([20368, 2])
We keep 3.05e+06/1.50e+08 =  2% of the original kernel matrix.

torch.Size([4393, 2])
We keep 2.34e+05/3.98e+06 =  5% of the original kernel matrix.

torch.Size([12623, 2])
We keep 1.23e+06/4.11e+07 =  2% of the original kernel matrix.

torch.Size([37170, 2])
We keep 3.13e+07/7.07e+08 =  4% of the original kernel matrix.

torch.Size([35305, 2])
We keep 8.31e+06/5.48e+08 =  1% of the original kernel matrix.

torch.Size([150812, 2])
We keep 6.00e+08/1.33e+10 =  4% of the original kernel matrix.

torch.Size([71623, 2])
We keep 2.80e+07/2.38e+09 =  1% of the original kernel matrix.

torch.Size([52991, 2])
We keep 3.02e+07/1.30e+09 =  2% of the original kernel matrix.

torch.Size([41812, 2])
We keep 1.10e+07/7.42e+08 =  1% of the original kernel matrix.

torch.Size([12862, 2])
We keep 5.77e+06/8.70e+07 =  6% of the original kernel matrix.

torch.Size([20163, 2])
We keep 3.72e+06/1.92e+08 =  1% of the original kernel matrix.

torch.Size([4011, 2])
We keep 1.71e+05/2.95e+06 =  5% of the original kernel matrix.

torch.Size([12279, 2])
We keep 1.09e+06/3.54e+07 =  3% of the original kernel matrix.

torch.Size([33613, 2])
We keep 1.07e+07/5.42e+08 =  1% of the original kernel matrix.

torch.Size([33718, 2])
We keep 7.68e+06/4.80e+08 =  1% of the original kernel matrix.

torch.Size([11450, 2])
We keep 1.37e+06/3.78e+07 =  3% of the original kernel matrix.

torch.Size([19153, 2])
We keep 2.74e+06/1.27e+08 =  2% of the original kernel matrix.

torch.Size([9291, 2])
We keep 1.32e+06/2.50e+07 =  5% of the original kernel matrix.

torch.Size([17525, 2])
We keep 2.36e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([9126, 2])
We keep 8.24e+05/2.01e+07 =  4% of the original kernel matrix.

torch.Size([17193, 2])
We keep 2.14e+06/9.24e+07 =  2% of the original kernel matrix.

torch.Size([19000, 2])
We keep 2.94e+06/1.31e+08 =  2% of the original kernel matrix.

torch.Size([25321, 2])
We keep 4.39e+06/2.36e+08 =  1% of the original kernel matrix.

torch.Size([20235, 2])
We keep 5.08e+06/1.68e+08 =  3% of the original kernel matrix.

torch.Size([25800, 2])
We keep 4.80e+06/2.67e+08 =  1% of the original kernel matrix.

torch.Size([15595, 2])
We keep 2.01e+06/7.84e+07 =  2% of the original kernel matrix.

torch.Size([22530, 2])
We keep 3.55e+06/1.82e+08 =  1% of the original kernel matrix.

torch.Size([11685, 2])
We keep 1.59e+06/3.77e+07 =  4% of the original kernel matrix.

torch.Size([19418, 2])
We keep 2.74e+06/1.26e+08 =  2% of the original kernel matrix.

torch.Size([195458, 2])
We keep 1.89e+08/1.91e+10 =  0% of the original kernel matrix.

torch.Size([82619, 2])
We keep 3.45e+07/2.85e+09 =  1% of the original kernel matrix.

torch.Size([416194, 2])
We keep 8.55e+08/8.22e+10 =  1% of the original kernel matrix.

torch.Size([121285, 2])
We keep 6.57e+07/5.91e+09 =  1% of the original kernel matrix.

torch.Size([86243, 2])
We keep 7.23e+07/3.60e+09 =  2% of the original kernel matrix.

torch.Size([53021, 2])
We keep 1.70e+07/1.24e+09 =  1% of the original kernel matrix.

torch.Size([55162, 2])
We keep 6.85e+07/1.77e+09 =  3% of the original kernel matrix.

torch.Size([42314, 2])
We keep 1.25e+07/8.67e+08 =  1% of the original kernel matrix.

torch.Size([25925, 2])
We keep 5.58e+06/2.87e+08 =  1% of the original kernel matrix.

torch.Size([29869, 2])
We keep 5.95e+06/3.49e+08 =  1% of the original kernel matrix.

torch.Size([14509, 2])
We keep 2.88e+06/7.53e+07 =  3% of the original kernel matrix.

torch.Size([21599, 2])
We keep 3.58e+06/1.79e+08 =  2% of the original kernel matrix.

torch.Size([36028, 2])
We keep 2.05e+07/6.68e+08 =  3% of the original kernel matrix.

torch.Size([35379, 2])
We keep 8.44e+06/5.33e+08 =  1% of the original kernel matrix.

torch.Size([221047, 2])
We keep 2.31e+08/2.49e+10 =  0% of the original kernel matrix.

torch.Size([88083, 2])
We keep 3.87e+07/3.25e+09 =  1% of the original kernel matrix.

torch.Size([23374, 2])
We keep 9.78e+06/2.45e+08 =  3% of the original kernel matrix.

torch.Size([27958, 2])
We keep 5.72e+06/3.23e+08 =  1% of the original kernel matrix.

torch.Size([60658, 2])
We keep 1.01e+08/1.95e+09 =  5% of the original kernel matrix.

torch.Size([44775, 2])
We keep 1.25e+07/9.09e+08 =  1% of the original kernel matrix.

torch.Size([22773, 2])
We keep 5.46e+06/2.28e+08 =  2% of the original kernel matrix.

torch.Size([27680, 2])
We keep 5.37e+06/3.11e+08 =  1% of the original kernel matrix.

torch.Size([1080207, 2])
We keep 4.28e+09/4.84e+11 =  0% of the original kernel matrix.

torch.Size([206606, 2])
We keep 1.45e+08/1.43e+10 =  1% of the original kernel matrix.

torch.Size([32692, 2])
We keep 1.76e+07/5.17e+08 =  3% of the original kernel matrix.

torch.Size([33189, 2])
We keep 7.62e+06/4.68e+08 =  1% of the original kernel matrix.

torch.Size([110300, 2])
We keep 8.73e+07/5.97e+09 =  1% of the original kernel matrix.

torch.Size([60107, 2])
We keep 2.07e+07/1.59e+09 =  1% of the original kernel matrix.

torch.Size([220097, 2])
We keep 3.45e+08/2.65e+10 =  1% of the original kernel matrix.

torch.Size([87726, 2])
We keep 3.86e+07/3.35e+09 =  1% of the original kernel matrix.

torch.Size([84031, 2])
We keep 5.79e+07/3.34e+09 =  1% of the original kernel matrix.

torch.Size([52226, 2])
We keep 1.64e+07/1.19e+09 =  1% of the original kernel matrix.

torch.Size([32452, 2])
We keep 1.08e+07/4.78e+08 =  2% of the original kernel matrix.

torch.Size([33302, 2])
We keep 7.33e+06/4.50e+08 =  1% of the original kernel matrix.

torch.Size([503552, 2])
We keep 1.25e+09/1.16e+11 =  1% of the original kernel matrix.

torch.Size([136487, 2])
We keep 7.48e+07/7.02e+09 =  1% of the original kernel matrix.

torch.Size([206213, 2])
We keep 1.96e+08/2.02e+10 =  0% of the original kernel matrix.

torch.Size([84548, 2])
We keep 3.50e+07/2.93e+09 =  1% of the original kernel matrix.

torch.Size([35627, 2])
We keep 1.04e+07/6.21e+08 =  1% of the original kernel matrix.

torch.Size([35152, 2])
We keep 8.01e+06/5.13e+08 =  1% of the original kernel matrix.

torch.Size([16350, 2])
We keep 4.25e+06/1.14e+08 =  3% of the original kernel matrix.

torch.Size([23127, 2])
We keep 4.22e+06/2.20e+08 =  1% of the original kernel matrix.

torch.Size([48042, 2])
We keep 2.29e+07/1.07e+09 =  2% of the original kernel matrix.

torch.Size([40065, 2])
We keep 1.02e+07/6.74e+08 =  1% of the original kernel matrix.

torch.Size([20587, 2])
We keep 5.11e+06/1.79e+08 =  2% of the original kernel matrix.

torch.Size([26260, 2])
We keep 4.83e+06/2.75e+08 =  1% of the original kernel matrix.

torch.Size([116380, 2])
We keep 7.72e+07/5.78e+09 =  1% of the original kernel matrix.

torch.Size([61708, 2])
We keep 2.03e+07/1.57e+09 =  1% of the original kernel matrix.

torch.Size([584473, 2])
We keep 1.28e+09/1.48e+11 =  0% of the original kernel matrix.

torch.Size([146138, 2])
We keep 8.52e+07/7.93e+09 =  1% of the original kernel matrix.

torch.Size([92870, 2])
We keep 9.89e+07/4.18e+09 =  2% of the original kernel matrix.

torch.Size([55536, 2])
We keep 1.78e+07/1.33e+09 =  1% of the original kernel matrix.

torch.Size([146001, 2])
We keep 1.72e+08/1.23e+10 =  1% of the original kernel matrix.

torch.Size([70043, 2])
We keep 2.82e+07/2.29e+09 =  1% of the original kernel matrix.

torch.Size([22784, 2])
We keep 4.73e+06/2.18e+08 =  2% of the original kernel matrix.

torch.Size([27728, 2])
We keep 5.28e+06/3.04e+08 =  1% of the original kernel matrix.

torch.Size([15027, 2])
We keep 2.38e+06/7.08e+07 =  3% of the original kernel matrix.

torch.Size([22195, 2])
We keep 3.39e+06/1.73e+08 =  1% of the original kernel matrix.

torch.Size([35476, 2])
We keep 1.06e+07/6.05e+08 =  1% of the original kernel matrix.

torch.Size([34614, 2])
We keep 7.79e+06/5.07e+08 =  1% of the original kernel matrix.

torch.Size([17230, 2])
We keep 5.29e+06/1.30e+08 =  4% of the original kernel matrix.

torch.Size([23644, 2])
We keep 4.42e+06/2.35e+08 =  1% of the original kernel matrix.

torch.Size([223095, 2])
We keep 4.03e+08/2.98e+10 =  1% of the original kernel matrix.

torch.Size([90296, 2])
We keep 4.08e+07/3.56e+09 =  1% of the original kernel matrix.

torch.Size([568681, 2])
We keep 1.05e+09/1.42e+11 =  0% of the original kernel matrix.

torch.Size([144325, 2])
We keep 8.37e+07/7.77e+09 =  1% of the original kernel matrix.

torch.Size([4667, 2])
We keep 2.38e+05/4.54e+06 =  5% of the original kernel matrix.

torch.Size([12977, 2])
We keep 1.25e+06/4.39e+07 =  2% of the original kernel matrix.

torch.Size([1001497, 2])
We keep 8.68e+09/6.83e+11 =  1% of the original kernel matrix.

torch.Size([188967, 2])
We keep 1.66e+08/1.70e+10 =  0% of the original kernel matrix.

torch.Size([44541, 2])
We keep 1.41e+08/9.67e+08 = 14% of the original kernel matrix.

torch.Size([38885, 2])
We keep 9.41e+06/6.40e+08 =  1% of the original kernel matrix.

torch.Size([91704, 2])
We keep 8.77e+07/3.99e+09 =  2% of the original kernel matrix.

torch.Size([54654, 2])
We keep 1.77e+07/1.30e+09 =  1% of the original kernel matrix.

torch.Size([25880, 2])
We keep 8.51e+06/3.31e+08 =  2% of the original kernel matrix.

torch.Size([29540, 2])
We keep 6.31e+06/3.75e+08 =  1% of the original kernel matrix.

torch.Size([77239, 2])
We keep 5.96e+07/2.93e+09 =  2% of the original kernel matrix.

torch.Size([50460, 2])
We keep 1.53e+07/1.12e+09 =  1% of the original kernel matrix.

torch.Size([251461, 2])
We keep 3.66e+08/3.40e+10 =  1% of the original kernel matrix.

torch.Size([95005, 2])
We keep 4.45e+07/3.80e+09 =  1% of the original kernel matrix.

torch.Size([6681, 2])
We keep 4.25e+05/9.35e+06 =  4% of the original kernel matrix.

torch.Size([15102, 2])
We keep 1.64e+06/6.30e+07 =  2% of the original kernel matrix.

torch.Size([1139453, 2])
We keep 3.48e+09/5.02e+11 =  0% of the original kernel matrix.

torch.Size([206604, 2])
We keep 1.49e+08/1.46e+10 =  1% of the original kernel matrix.

torch.Size([31932, 2])
We keep 1.11e+07/5.06e+08 =  2% of the original kernel matrix.

torch.Size([32817, 2])
We keep 7.34e+06/4.64e+08 =  1% of the original kernel matrix.

torch.Size([47165, 2])
We keep 4.16e+07/1.32e+09 =  3% of the original kernel matrix.

torch.Size([39284, 2])
We keep 1.12e+07/7.48e+08 =  1% of the original kernel matrix.

torch.Size([26432, 2])
We keep 9.02e+06/3.27e+08 =  2% of the original kernel matrix.

torch.Size([29796, 2])
We keep 6.42e+06/3.73e+08 =  1% of the original kernel matrix.

torch.Size([46064, 2])
We keep 1.70e+07/1.00e+09 =  1% of the original kernel matrix.

torch.Size([39361, 2])
We keep 9.82e+06/6.52e+08 =  1% of the original kernel matrix.

torch.Size([5618, 2])
We keep 4.39e+05/7.08e+06 =  6% of the original kernel matrix.

torch.Size([14000, 2])
We keep 1.50e+06/5.48e+07 =  2% of the original kernel matrix.

torch.Size([27159, 2])
We keep 7.02e+06/3.54e+08 =  1% of the original kernel matrix.

torch.Size([30298, 2])
We keep 6.42e+06/3.87e+08 =  1% of the original kernel matrix.

torch.Size([18816, 2])
We keep 3.48e+06/1.38e+08 =  2% of the original kernel matrix.

torch.Size([25089, 2])
We keep 4.42e+06/2.42e+08 =  1% of the original kernel matrix.

torch.Size([47101, 2])
We keep 2.42e+07/1.15e+09 =  2% of the original kernel matrix.

torch.Size([39905, 2])
We keep 1.05e+07/6.99e+08 =  1% of the original kernel matrix.

torch.Size([77157, 2])
We keep 3.19e+09/7.68e+09 = 41% of the original kernel matrix.

torch.Size([50202, 2])
We keep 2.15e+07/1.81e+09 =  1% of the original kernel matrix.

torch.Size([36112, 2])
We keep 3.52e+07/9.05e+08 =  3% of the original kernel matrix.

torch.Size([34113, 2])
We keep 9.56e+06/6.20e+08 =  1% of the original kernel matrix.

torch.Size([525144, 2])
We keep 1.08e+09/1.23e+11 =  0% of the original kernel matrix.

torch.Size([135938, 2])
We keep 7.87e+07/7.23e+09 =  1% of the original kernel matrix.

torch.Size([12968, 2])
We keep 1.49e+06/4.90e+07 =  3% of the original kernel matrix.

torch.Size([20664, 2])
We keep 2.99e+06/1.44e+08 =  2% of the original kernel matrix.

torch.Size([46028, 2])
We keep 1.88e+07/9.78e+08 =  1% of the original kernel matrix.

torch.Size([39473, 2])
We keep 9.77e+06/6.44e+08 =  1% of the original kernel matrix.

torch.Size([14285, 2])
We keep 1.89e+06/6.58e+07 =  2% of the original kernel matrix.

torch.Size([21367, 2])
We keep 3.29e+06/1.67e+08 =  1% of the original kernel matrix.

torch.Size([20774, 2])
We keep 5.42e+07/1.86e+08 = 29% of the original kernel matrix.

torch.Size([26190, 2])
We keep 4.67e+06/2.81e+08 =  1% of the original kernel matrix.

torch.Size([135556, 2])
We keep 1.89e+08/1.11e+10 =  1% of the original kernel matrix.

torch.Size([67019, 2])
We keep 2.66e+07/2.17e+09 =  1% of the original kernel matrix.

torch.Size([2020610, 2])
We keep 1.00e+10/1.52e+12 =  0% of the original kernel matrix.

torch.Size([284952, 2])
We keep 2.51e+08/2.54e+10 =  0% of the original kernel matrix.

torch.Size([24979, 2])
We keep 8.21e+06/2.77e+08 =  2% of the original kernel matrix.

torch.Size([28995, 2])
We keep 5.79e+06/3.43e+08 =  1% of the original kernel matrix.

torch.Size([224276, 2])
We keep 2.66e+08/2.59e+10 =  1% of the original kernel matrix.

torch.Size([89234, 2])
We keep 3.91e+07/3.31e+09 =  1% of the original kernel matrix.

torch.Size([15272, 2])
We keep 2.00e+06/7.32e+07 =  2% of the original kernel matrix.

torch.Size([22377, 2])
We keep 3.45e+06/1.76e+08 =  1% of the original kernel matrix.

torch.Size([194311, 2])
We keep 2.17e+08/1.90e+10 =  1% of the original kernel matrix.

torch.Size([81583, 2])
We keep 3.44e+07/2.84e+09 =  1% of the original kernel matrix.

torch.Size([182663, 2])
We keep 2.62e+08/2.01e+10 =  1% of the original kernel matrix.

torch.Size([79111, 2])
We keep 3.52e+07/2.92e+09 =  1% of the original kernel matrix.

torch.Size([56336, 2])
We keep 2.85e+07/1.53e+09 =  1% of the original kernel matrix.

torch.Size([42960, 2])
We keep 1.17e+07/8.06e+08 =  1% of the original kernel matrix.

torch.Size([14017, 2])
We keep 2.33e+06/6.04e+07 =  3% of the original kernel matrix.

torch.Size([21314, 2])
We keep 3.23e+06/1.60e+08 =  2% of the original kernel matrix.

torch.Size([137549, 2])
We keep 1.68e+08/9.29e+09 =  1% of the original kernel matrix.

torch.Size([67727, 2])
We keep 2.54e+07/1.99e+09 =  1% of the original kernel matrix.

torch.Size([39287, 2])
We keep 6.34e+07/9.46e+08 =  6% of the original kernel matrix.

torch.Size([35780, 2])
We keep 9.27e+06/6.34e+08 =  1% of the original kernel matrix.

torch.Size([149102, 2])
We keep 1.65e+08/1.08e+10 =  1% of the original kernel matrix.

torch.Size([70683, 2])
We keep 2.65e+07/2.15e+09 =  1% of the original kernel matrix.

torch.Size([601601, 2])
We keep 1.40e+09/1.70e+11 =  0% of the original kernel matrix.

torch.Size([146893, 2])
We keep 9.00e+07/8.49e+09 =  1% of the original kernel matrix.

torch.Size([18425, 2])
We keep 9.45e+06/1.72e+08 =  5% of the original kernel matrix.

torch.Size([24721, 2])
We keep 4.86e+06/2.70e+08 =  1% of the original kernel matrix.

torch.Size([48299, 2])
We keep 2.11e+07/1.09e+09 =  1% of the original kernel matrix.

torch.Size([40144, 2])
We keep 1.01e+07/6.79e+08 =  1% of the original kernel matrix.

torch.Size([39601, 2])
We keep 1.49e+07/7.49e+08 =  1% of the original kernel matrix.

torch.Size([36271, 2])
We keep 8.73e+06/5.64e+08 =  1% of the original kernel matrix.

torch.Size([212318, 2])
We keep 2.60e+08/2.55e+10 =  1% of the original kernel matrix.

torch.Size([85572, 2])
We keep 3.87e+07/3.29e+09 =  1% of the original kernel matrix.

torch.Size([270747, 2])
We keep 4.81e+08/4.08e+10 =  1% of the original kernel matrix.

torch.Size([98717, 2])
We keep 4.68e+07/4.16e+09 =  1% of the original kernel matrix.

torch.Size([104877, 2])
We keep 1.03e+08/5.41e+09 =  1% of the original kernel matrix.

torch.Size([58214, 2])
We keep 2.03e+07/1.52e+09 =  1% of the original kernel matrix.

torch.Size([19046, 2])
We keep 3.09e+06/1.35e+08 =  2% of the original kernel matrix.

torch.Size([25339, 2])
We keep 4.38e+06/2.39e+08 =  1% of the original kernel matrix.

torch.Size([484355, 2])
We keep 1.13e+09/1.22e+11 =  0% of the original kernel matrix.

torch.Size([130834, 2])
We keep 7.78e+07/7.20e+09 =  1% of the original kernel matrix.

torch.Size([135507, 2])
We keep 1.17e+08/9.32e+09 =  1% of the original kernel matrix.

torch.Size([66993, 2])
We keep 2.49e+07/1.99e+09 =  1% of the original kernel matrix.

torch.Size([432234, 2])
We keep 1.41e+09/1.09e+11 =  1% of the original kernel matrix.

torch.Size([123840, 2])
We keep 7.23e+07/6.79e+09 =  1% of the original kernel matrix.

torch.Size([238869, 2])
We keep 3.41e+08/2.91e+10 =  1% of the original kernel matrix.

torch.Size([91578, 2])
We keep 4.08e+07/3.52e+09 =  1% of the original kernel matrix.

torch.Size([127496, 2])
We keep 1.76e+08/1.01e+10 =  1% of the original kernel matrix.

torch.Size([65133, 2])
We keep 2.61e+07/2.07e+09 =  1% of the original kernel matrix.

torch.Size([30690, 2])
We keep 2.07e+07/5.50e+08 =  3% of the original kernel matrix.

torch.Size([32345, 2])
We keep 7.97e+06/4.83e+08 =  1% of the original kernel matrix.

torch.Size([150062, 2])
We keep 4.89e+08/1.58e+10 =  3% of the original kernel matrix.

torch.Size([70157, 2])
We keep 3.18e+07/2.59e+09 =  1% of the original kernel matrix.

torch.Size([278889, 2])
We keep 7.07e+08/4.56e+10 =  1% of the original kernel matrix.

torch.Size([98621, 2])
We keep 4.97e+07/4.40e+09 =  1% of the original kernel matrix.

torch.Size([118268, 2])
We keep 8.31e+07/6.71e+09 =  1% of the original kernel matrix.

torch.Size([62262, 2])
We keep 2.19e+07/1.69e+09 =  1% of the original kernel matrix.

torch.Size([11269, 2])
We keep 1.39e+06/3.89e+07 =  3% of the original kernel matrix.

torch.Size([18932, 2])
We keep 2.72e+06/1.28e+08 =  2% of the original kernel matrix.

torch.Size([31224, 2])
We keep 1.42e+07/4.59e+08 =  3% of the original kernel matrix.

torch.Size([32316, 2])
We keep 7.27e+06/4.41e+08 =  1% of the original kernel matrix.

torch.Size([32532, 2])
We keep 3.08e+07/5.80e+08 =  5% of the original kernel matrix.

torch.Size([33058, 2])
We keep 7.81e+06/4.96e+08 =  1% of the original kernel matrix.

torch.Size([120754, 2])
We keep 1.34e+08/6.84e+09 =  1% of the original kernel matrix.

torch.Size([63054, 2])
We keep 2.19e+07/1.70e+09 =  1% of the original kernel matrix.

torch.Size([83663, 2])
We keep 4.25e+07/2.97e+09 =  1% of the original kernel matrix.

torch.Size([51869, 2])
We keep 1.53e+07/1.12e+09 =  1% of the original kernel matrix.

torch.Size([13165, 2])
We keep 1.48e+06/5.05e+07 =  2% of the original kernel matrix.

torch.Size([20621, 2])
We keep 3.00e+06/1.46e+08 =  2% of the original kernel matrix.

torch.Size([30058, 2])
We keep 7.09e+06/4.09e+08 =  1% of the original kernel matrix.

torch.Size([32141, 2])
We keep 6.83e+06/4.17e+08 =  1% of the original kernel matrix.

torch.Size([152762, 2])
We keep 1.93e+08/1.16e+10 =  1% of the original kernel matrix.

torch.Size([71651, 2])
We keep 2.77e+07/2.22e+09 =  1% of the original kernel matrix.

torch.Size([79575, 2])
We keep 4.24e+07/2.91e+09 =  1% of the original kernel matrix.

torch.Size([50728, 2])
We keep 1.54e+07/1.11e+09 =  1% of the original kernel matrix.

torch.Size([46208, 2])
We keep 2.14e+07/1.05e+09 =  2% of the original kernel matrix.

torch.Size([39604, 2])
We keep 1.01e+07/6.66e+08 =  1% of the original kernel matrix.

torch.Size([67236, 2])
We keep 2.85e+07/1.96e+09 =  1% of the original kernel matrix.

torch.Size([46889, 2])
We keep 1.28e+07/9.11e+08 =  1% of the original kernel matrix.

torch.Size([36503, 2])
We keep 1.69e+07/6.44e+08 =  2% of the original kernel matrix.

torch.Size([35138, 2])
We keep 8.20e+06/5.23e+08 =  1% of the original kernel matrix.

torch.Size([33076, 2])
We keep 9.83e+06/4.91e+08 =  2% of the original kernel matrix.

torch.Size([33718, 2])
We keep 7.30e+06/4.57e+08 =  1% of the original kernel matrix.

torch.Size([10430, 2])
We keep 1.10e+06/2.98e+07 =  3% of the original kernel matrix.

torch.Size([18426, 2])
We keep 2.45e+06/1.12e+08 =  2% of the original kernel matrix.

torch.Size([14108, 2])
We keep 1.83e+06/6.44e+07 =  2% of the original kernel matrix.

torch.Size([21260, 2])
We keep 3.29e+06/1.65e+08 =  1% of the original kernel matrix.

torch.Size([289539, 2])
We keep 3.74e+08/4.25e+10 =  0% of the original kernel matrix.

torch.Size([102361, 2])
We keep 4.87e+07/4.25e+09 =  1% of the original kernel matrix.

torch.Size([26985, 2])
We keep 1.13e+07/3.33e+08 =  3% of the original kernel matrix.

torch.Size([30330, 2])
We keep 6.43e+06/3.76e+08 =  1% of the original kernel matrix.

torch.Size([121004, 2])
We keep 7.86e+07/6.53e+09 =  1% of the original kernel matrix.

torch.Size([63194, 2])
We keep 2.16e+07/1.67e+09 =  1% of the original kernel matrix.

torch.Size([7988, 2])
We keep 7.28e+05/1.57e+07 =  4% of the original kernel matrix.

torch.Size([16203, 2])
We keep 1.99e+06/8.17e+07 =  2% of the original kernel matrix.

torch.Size([9959, 2])
We keep 1.13e+06/2.74e+07 =  4% of the original kernel matrix.

torch.Size([17955, 2])
We keep 2.42e+06/1.08e+08 =  2% of the original kernel matrix.

torch.Size([193507, 2])
We keep 4.64e+08/2.35e+10 =  1% of the original kernel matrix.

torch.Size([81711, 2])
We keep 3.82e+07/3.16e+09 =  1% of the original kernel matrix.

torch.Size([190452, 2])
We keep 2.18e+08/1.93e+10 =  1% of the original kernel matrix.

torch.Size([80958, 2])
We keep 3.47e+07/2.87e+09 =  1% of the original kernel matrix.

torch.Size([59167, 2])
We keep 3.21e+07/1.59e+09 =  2% of the original kernel matrix.

torch.Size([44174, 2])
We keep 1.20e+07/8.22e+08 =  1% of the original kernel matrix.

torch.Size([10822, 2])
We keep 1.08e+06/3.21e+07 =  3% of the original kernel matrix.

torch.Size([18675, 2])
We keep 2.52e+06/1.17e+08 =  2% of the original kernel matrix.

torch.Size([12188, 2])
We keep 1.35e+06/4.19e+07 =  3% of the original kernel matrix.

torch.Size([19756, 2])
We keep 2.82e+06/1.33e+08 =  2% of the original kernel matrix.

torch.Size([46423, 2])
We keep 2.38e+07/1.03e+09 =  2% of the original kernel matrix.

torch.Size([39333, 2])
We keep 9.95e+06/6.62e+08 =  1% of the original kernel matrix.

torch.Size([31532, 2])
We keep 1.03e+07/4.57e+08 =  2% of the original kernel matrix.

torch.Size([32734, 2])
We keep 7.02e+06/4.40e+08 =  1% of the original kernel matrix.

torch.Size([130402, 2])
We keep 1.35e+08/8.85e+09 =  1% of the original kernel matrix.

torch.Size([65778, 2])
We keep 2.46e+07/1.94e+09 =  1% of the original kernel matrix.

torch.Size([11122, 2])
We keep 2.16e+06/4.44e+07 =  4% of the original kernel matrix.

torch.Size([19145, 2])
We keep 2.88e+06/1.37e+08 =  2% of the original kernel matrix.

torch.Size([22648, 2])
We keep 6.69e+06/2.45e+08 =  2% of the original kernel matrix.

torch.Size([27301, 2])
We keep 5.54e+06/3.22e+08 =  1% of the original kernel matrix.

torch.Size([43903, 2])
We keep 3.18e+07/1.24e+09 =  2% of the original kernel matrix.

torch.Size([37648, 2])
We keep 1.08e+07/7.26e+08 =  1% of the original kernel matrix.

torch.Size([20030, 2])
We keep 6.92e+06/1.82e+08 =  3% of the original kernel matrix.

torch.Size([25603, 2])
We keep 5.10e+06/2.78e+08 =  1% of the original kernel matrix.

torch.Size([165447, 2])
We keep 1.70e+08/1.36e+10 =  1% of the original kernel matrix.

torch.Size([75405, 2])
We keep 2.96e+07/2.41e+09 =  1% of the original kernel matrix.

torch.Size([13297, 2])
We keep 2.87e+06/6.11e+07 =  4% of the original kernel matrix.

torch.Size([20869, 2])
We keep 3.27e+06/1.61e+08 =  2% of the original kernel matrix.

torch.Size([107862, 2])
We keep 8.75e+07/5.75e+09 =  1% of the original kernel matrix.

torch.Size([59341, 2])
We keep 2.02e+07/1.56e+09 =  1% of the original kernel matrix.

torch.Size([283173, 2])
We keep 4.63e+08/3.80e+10 =  1% of the original kernel matrix.

torch.Size([100959, 2])
We keep 4.62e+07/4.02e+09 =  1% of the original kernel matrix.

torch.Size([55303, 2])
We keep 2.34e+07/1.53e+09 =  1% of the original kernel matrix.

torch.Size([42931, 2])
We keep 1.18e+07/8.07e+08 =  1% of the original kernel matrix.

torch.Size([57157, 2])
We keep 4.77e+07/1.64e+09 =  2% of the original kernel matrix.

torch.Size([43941, 2])
We keep 1.23e+07/8.34e+08 =  1% of the original kernel matrix.

torch.Size([150244, 2])
We keep 1.19e+08/1.13e+10 =  1% of the original kernel matrix.

torch.Size([71839, 2])
We keep 2.75e+07/2.19e+09 =  1% of the original kernel matrix.

torch.Size([5450, 2])
We keep 3.07e+05/6.34e+06 =  4% of the original kernel matrix.

torch.Size([13764, 2])
We keep 1.43e+06/5.19e+07 =  2% of the original kernel matrix.

torch.Size([63025, 2])
We keep 3.40e+07/1.77e+09 =  1% of the original kernel matrix.

torch.Size([45608, 2])
We keep 1.24e+07/8.66e+08 =  1% of the original kernel matrix.

torch.Size([32046, 2])
We keep 1.09e+07/4.76e+08 =  2% of the original kernel matrix.

torch.Size([33328, 2])
We keep 7.34e+06/4.49e+08 =  1% of the original kernel matrix.

torch.Size([421664, 2])
We keep 9.03e+08/8.55e+10 =  1% of the original kernel matrix.

torch.Size([121629, 2])
We keep 6.65e+07/6.02e+09 =  1% of the original kernel matrix.

torch.Size([94569, 2])
We keep 8.05e+07/4.86e+09 =  1% of the original kernel matrix.

torch.Size([55643, 2])
We keep 1.90e+07/1.44e+09 =  1% of the original kernel matrix.

torch.Size([80791, 2])
We keep 7.41e+07/3.45e+09 =  2% of the original kernel matrix.

torch.Size([50644, 2])
We keep 1.67e+07/1.21e+09 =  1% of the original kernel matrix.

torch.Size([141599, 2])
We keep 1.28e+08/9.16e+09 =  1% of the original kernel matrix.

torch.Size([68917, 2])
We keep 2.43e+07/1.97e+09 =  1% of the original kernel matrix.

torch.Size([36124, 2])
We keep 1.14e+07/6.12e+08 =  1% of the original kernel matrix.

torch.Size([34784, 2])
We keep 7.90e+06/5.09e+08 =  1% of the original kernel matrix.

torch.Size([7806, 2])
We keep 1.84e+06/2.12e+07 =  8% of the original kernel matrix.

torch.Size([16080, 2])
We keep 2.18e+06/9.47e+07 =  2% of the original kernel matrix.

torch.Size([10869, 2])
We keep 1.10e+06/3.09e+07 =  3% of the original kernel matrix.

torch.Size([18701, 2])
We keep 2.53e+06/1.15e+08 =  2% of the original kernel matrix.

torch.Size([30941, 2])
We keep 1.48e+07/5.00e+08 =  2% of the original kernel matrix.

torch.Size([31755, 2])
We keep 7.56e+06/4.61e+08 =  1% of the original kernel matrix.

torch.Size([244694, 2])
We keep 7.14e+08/3.23e+10 =  2% of the original kernel matrix.

torch.Size([93502, 2])
We keep 4.29e+07/3.70e+09 =  1% of the original kernel matrix.

torch.Size([702303, 2])
We keep 2.02e+09/2.30e+11 =  0% of the original kernel matrix.

torch.Size([165049, 2])
We keep 1.02e+08/9.88e+09 =  1% of the original kernel matrix.

torch.Size([68396, 2])
We keep 6.70e+07/2.77e+09 =  2% of the original kernel matrix.

torch.Size([46780, 2])
We keep 1.47e+07/1.09e+09 =  1% of the original kernel matrix.

torch.Size([21912, 2])
We keep 4.51e+06/1.95e+08 =  2% of the original kernel matrix.

torch.Size([27048, 2])
We keep 5.02e+06/2.88e+08 =  1% of the original kernel matrix.

torch.Size([29811, 2])
We keep 7.31e+06/3.81e+08 =  1% of the original kernel matrix.

torch.Size([30523, 2])
We keep 6.06e+06/4.02e+08 =  1% of the original kernel matrix.

torch.Size([811820, 2])
We keep 2.82e+09/3.22e+11 =  0% of the original kernel matrix.

torch.Size([177065, 2])
We keep 1.23e+08/1.17e+10 =  1% of the original kernel matrix.

torch.Size([46620, 2])
We keep 1.65e+07/1.00e+09 =  1% of the original kernel matrix.

torch.Size([39804, 2])
We keep 9.77e+06/6.52e+08 =  1% of the original kernel matrix.

torch.Size([21156, 2])
We keep 4.65e+06/1.82e+08 =  2% of the original kernel matrix.

torch.Size([26565, 2])
We keep 4.93e+06/2.78e+08 =  1% of the original kernel matrix.

torch.Size([98003, 2])
We keep 5.47e+07/4.05e+09 =  1% of the original kernel matrix.

torch.Size([56115, 2])
We keep 1.75e+07/1.31e+09 =  1% of the original kernel matrix.

torch.Size([98153, 2])
We keep 5.46e+07/3.94e+09 =  1% of the original kernel matrix.

torch.Size([56121, 2])
We keep 1.71e+07/1.29e+09 =  1% of the original kernel matrix.

torch.Size([18585, 2])
We keep 5.10e+06/1.49e+08 =  3% of the original kernel matrix.

torch.Size([24717, 2])
We keep 4.68e+06/2.52e+08 =  1% of the original kernel matrix.

torch.Size([20807, 2])
We keep 4.96e+06/1.87e+08 =  2% of the original kernel matrix.

torch.Size([26455, 2])
We keep 5.10e+06/2.81e+08 =  1% of the original kernel matrix.

torch.Size([95164, 2])
We keep 2.42e+08/5.84e+09 =  4% of the original kernel matrix.

torch.Size([55656, 2])
We keep 2.01e+07/1.57e+09 =  1% of the original kernel matrix.

time for making ranges is 4.273214817047119
Sorting X and nu_X
time for sorting X is 0.09267187118530273
Sorting Z and nu_Z
time for sorting Z is 0.0002677440643310547
Starting Optim
sum tnu_Z before tensor(44679364., device='cuda:0')
c= tensor(3032.5371, device='cuda:0')
c= tensor(205429.2969, device='cuda:0')
c= tensor(210257.4531, device='cuda:0')
c= tensor(218338.1250, device='cuda:0')
c= tensor(4195295., device='cuda:0')
c= tensor(5463638., device='cuda:0')
c= tensor(6469248., device='cuda:0')
c= tensor(7189663., device='cuda:0')
c= tensor(7801846., device='cuda:0')
c= tensor(24530990., device='cuda:0')
c= tensor(24849328., device='cuda:0')
c= tensor(27845460., device='cuda:0')
c= tensor(27875880., device='cuda:0')
c= tensor(46474876., device='cuda:0')
c= tensor(46723112., device='cuda:0')
c= tensor(47153748., device='cuda:0')
c= tensor(47850952., device='cuda:0')
c= tensor(49209836., device='cuda:0')
c= tensor(56829060., device='cuda:0')
c= tensor(61802392., device='cuda:0')
c= tensor(63231256., device='cuda:0')
c= tensor(74791248., device='cuda:0')
c= tensor(74872752., device='cuda:0')
c= tensor(74989504., device='cuda:0')
c= tensor(76886568., device='cuda:0')
c= tensor(77990480., device='cuda:0')
c= tensor(80334920., device='cuda:0')
c= tensor(80519312., device='cuda:0')
c= tensor(92521480., device='cuda:0')
c= tensor(4.6098e+08, device='cuda:0')
c= tensor(4.6116e+08, device='cuda:0')
c= tensor(7.2989e+08, device='cuda:0')
c= tensor(7.2995e+08, device='cuda:0')
c= tensor(7.3028e+08, device='cuda:0')
c= tensor(7.3117e+08, device='cuda:0')
c= tensor(7.7390e+08, device='cuda:0')
c= tensor(7.7498e+08, device='cuda:0')
c= tensor(7.7498e+08, device='cuda:0')
c= tensor(7.7499e+08, device='cuda:0')
c= tensor(7.7500e+08, device='cuda:0')
c= tensor(7.7501e+08, device='cuda:0')
c= tensor(7.7501e+08, device='cuda:0')
c= tensor(7.7501e+08, device='cuda:0')
c= tensor(7.7502e+08, device='cuda:0')
c= tensor(7.7502e+08, device='cuda:0')
c= tensor(7.7502e+08, device='cuda:0')
c= tensor(7.7502e+08, device='cuda:0')
c= tensor(7.7505e+08, device='cuda:0')
c= tensor(7.7506e+08, device='cuda:0')
c= tensor(7.7508e+08, device='cuda:0')
c= tensor(7.7515e+08, device='cuda:0')
c= tensor(7.7515e+08, device='cuda:0')
c= tensor(7.7516e+08, device='cuda:0')
c= tensor(7.7517e+08, device='cuda:0')
c= tensor(7.7519e+08, device='cuda:0')
c= tensor(7.7520e+08, device='cuda:0')
c= tensor(7.7521e+08, device='cuda:0')
c= tensor(7.7522e+08, device='cuda:0')
c= tensor(7.7523e+08, device='cuda:0')
c= tensor(7.7523e+08, device='cuda:0')
c= tensor(7.7524e+08, device='cuda:0')
c= tensor(7.7524e+08, device='cuda:0')
c= tensor(7.7525e+08, device='cuda:0')
c= tensor(7.7535e+08, device='cuda:0')
c= tensor(7.7536e+08, device='cuda:0')
c= tensor(7.7537e+08, device='cuda:0')
c= tensor(7.7537e+08, device='cuda:0')
c= tensor(7.7539e+08, device='cuda:0')
c= tensor(7.7541e+08, device='cuda:0')
c= tensor(7.7542e+08, device='cuda:0')
c= tensor(7.7542e+08, device='cuda:0')
c= tensor(7.7544e+08, device='cuda:0')
c= tensor(7.7544e+08, device='cuda:0')
c= tensor(7.7545e+08, device='cuda:0')
c= tensor(7.7546e+08, device='cuda:0')
c= tensor(7.7553e+08, device='cuda:0')
c= tensor(7.7553e+08, device='cuda:0')
c= tensor(7.7553e+08, device='cuda:0')
c= tensor(7.7554e+08, device='cuda:0')
c= tensor(7.7564e+08, device='cuda:0')
c= tensor(7.7564e+08, device='cuda:0')
c= tensor(7.7564e+08, device='cuda:0')
c= tensor(7.7565e+08, device='cuda:0')
c= tensor(7.7565e+08, device='cuda:0')
c= tensor(7.7566e+08, device='cuda:0')
c= tensor(7.7566e+08, device='cuda:0')
c= tensor(7.7566e+08, device='cuda:0')
c= tensor(7.7566e+08, device='cuda:0')
c= tensor(7.7567e+08, device='cuda:0')
c= tensor(7.7568e+08, device='cuda:0')
c= tensor(7.7570e+08, device='cuda:0')
c= tensor(7.7570e+08, device='cuda:0')
c= tensor(7.7570e+08, device='cuda:0')
c= tensor(7.7573e+08, device='cuda:0')
c= tensor(7.7574e+08, device='cuda:0')
c= tensor(7.7577e+08, device='cuda:0')
c= tensor(7.7577e+08, device='cuda:0')
c= tensor(7.7578e+08, device='cuda:0')
c= tensor(7.7579e+08, device='cuda:0')
c= tensor(7.7581e+08, device='cuda:0')
c= tensor(7.7586e+08, device='cuda:0')
c= tensor(7.7586e+08, device='cuda:0')
c= tensor(7.7588e+08, device='cuda:0')
c= tensor(7.7588e+08, device='cuda:0')
c= tensor(7.7597e+08, device='cuda:0')
c= tensor(7.7598e+08, device='cuda:0')
c= tensor(7.7598e+08, device='cuda:0')
c= tensor(7.7599e+08, device='cuda:0')
c= tensor(7.7599e+08, device='cuda:0')
c= tensor(7.7599e+08, device='cuda:0')
c= tensor(7.7601e+08, device='cuda:0')
c= tensor(7.7601e+08, device='cuda:0')
c= tensor(7.7601e+08, device='cuda:0')
c= tensor(7.7602e+08, device='cuda:0')
c= tensor(7.7603e+08, device='cuda:0')
c= tensor(7.7604e+08, device='cuda:0')
c= tensor(7.7604e+08, device='cuda:0')
c= tensor(7.7604e+08, device='cuda:0')
c= tensor(7.7608e+08, device='cuda:0')
c= tensor(7.7608e+08, device='cuda:0')
c= tensor(7.7614e+08, device='cuda:0')
c= tensor(7.7615e+08, device='cuda:0')
c= tensor(7.7615e+08, device='cuda:0')
c= tensor(7.7616e+08, device='cuda:0')
c= tensor(7.7617e+08, device='cuda:0')
c= tensor(7.7617e+08, device='cuda:0')
c= tensor(7.7618e+08, device='cuda:0')
c= tensor(7.7619e+08, device='cuda:0')
c= tensor(7.7622e+08, device='cuda:0')
c= tensor(7.7623e+08, device='cuda:0')
c= tensor(7.7637e+08, device='cuda:0')
c= tensor(7.7637e+08, device='cuda:0')
c= tensor(7.7638e+08, device='cuda:0')
c= tensor(7.7638e+08, device='cuda:0')
c= tensor(7.7638e+08, device='cuda:0')
c= tensor(7.7639e+08, device='cuda:0')
c= tensor(7.7639e+08, device='cuda:0')
c= tensor(7.7640e+08, device='cuda:0')
c= tensor(7.7641e+08, device='cuda:0')
c= tensor(7.7641e+08, device='cuda:0')
c= tensor(7.7642e+08, device='cuda:0')
c= tensor(7.7642e+08, device='cuda:0')
c= tensor(7.7646e+08, device='cuda:0')
c= tensor(7.7648e+08, device='cuda:0')
c= tensor(7.7649e+08, device='cuda:0')
c= tensor(7.7650e+08, device='cuda:0')
c= tensor(7.7650e+08, device='cuda:0')
c= tensor(7.7650e+08, device='cuda:0')
c= tensor(7.7651e+08, device='cuda:0')
c= tensor(7.7651e+08, device='cuda:0')
c= tensor(7.7652e+08, device='cuda:0')
c= tensor(7.7653e+08, device='cuda:0')
c= tensor(7.7653e+08, device='cuda:0')
c= tensor(7.7654e+08, device='cuda:0')
c= tensor(7.7655e+08, device='cuda:0')
c= tensor(7.7658e+08, device='cuda:0')
c= tensor(7.7659e+08, device='cuda:0')
c= tensor(7.7660e+08, device='cuda:0')
c= tensor(7.7660e+08, device='cuda:0')
c= tensor(7.7660e+08, device='cuda:0')
c= tensor(7.7668e+08, device='cuda:0')
c= tensor(7.7668e+08, device='cuda:0')
c= tensor(7.7675e+08, device='cuda:0')
c= tensor(7.7675e+08, device='cuda:0')
c= tensor(7.7676e+08, device='cuda:0')
c= tensor(7.7677e+08, device='cuda:0')
c= tensor(7.7678e+08, device='cuda:0')
c= tensor(7.7678e+08, device='cuda:0')
c= tensor(7.7679e+08, device='cuda:0')
c= tensor(7.7679e+08, device='cuda:0')
c= tensor(7.7680e+08, device='cuda:0')
c= tensor(7.7680e+08, device='cuda:0')
c= tensor(7.7682e+08, device='cuda:0')
c= tensor(7.7682e+08, device='cuda:0')
c= tensor(7.7683e+08, device='cuda:0')
c= tensor(7.7684e+08, device='cuda:0')
c= tensor(7.7685e+08, device='cuda:0')
c= tensor(7.7686e+08, device='cuda:0')
c= tensor(7.7686e+08, device='cuda:0')
c= tensor(7.7687e+08, device='cuda:0')
c= tensor(7.7689e+08, device='cuda:0')
c= tensor(7.7689e+08, device='cuda:0')
c= tensor(7.7690e+08, device='cuda:0')
c= tensor(7.7690e+08, device='cuda:0')
c= tensor(7.7691e+08, device='cuda:0')
c= tensor(7.7693e+08, device='cuda:0')
c= tensor(7.7693e+08, device='cuda:0')
c= tensor(7.7694e+08, device='cuda:0')
c= tensor(7.7696e+08, device='cuda:0')
c= tensor(7.7698e+08, device='cuda:0')
c= tensor(7.7699e+08, device='cuda:0')
c= tensor(7.7699e+08, device='cuda:0')
c= tensor(7.7700e+08, device='cuda:0')
c= tensor(7.7700e+08, device='cuda:0')
c= tensor(7.7701e+08, device='cuda:0')
c= tensor(7.7702e+08, device='cuda:0')
c= tensor(7.7702e+08, device='cuda:0')
c= tensor(7.7703e+08, device='cuda:0')
c= tensor(7.7704e+08, device='cuda:0')
c= tensor(7.7704e+08, device='cuda:0')
c= tensor(7.7705e+08, device='cuda:0')
c= tensor(7.7706e+08, device='cuda:0')
c= tensor(7.7709e+08, device='cuda:0')
c= tensor(7.7709e+08, device='cuda:0')
c= tensor(7.7712e+08, device='cuda:0')
c= tensor(7.7713e+08, device='cuda:0')
c= tensor(7.7713e+08, device='cuda:0')
c= tensor(7.7714e+08, device='cuda:0')
c= tensor(7.7714e+08, device='cuda:0')
c= tensor(7.7717e+08, device='cuda:0')
c= tensor(7.7718e+08, device='cuda:0')
c= tensor(7.7718e+08, device='cuda:0')
c= tensor(7.7719e+08, device='cuda:0')
c= tensor(7.7723e+08, device='cuda:0')
c= tensor(7.7723e+08, device='cuda:0')
c= tensor(7.7723e+08, device='cuda:0')
c= tensor(7.7723e+08, device='cuda:0')
c= tensor(7.7723e+08, device='cuda:0')
c= tensor(7.7729e+08, device='cuda:0')
c= tensor(7.7730e+08, device='cuda:0')
c= tensor(7.7731e+08, device='cuda:0')
c= tensor(7.7731e+08, device='cuda:0')
c= tensor(7.7733e+08, device='cuda:0')
c= tensor(7.7734e+08, device='cuda:0')
c= tensor(7.7734e+08, device='cuda:0')
c= tensor(7.7735e+08, device='cuda:0')
c= tensor(7.7736e+08, device='cuda:0')
c= tensor(7.7737e+08, device='cuda:0')
c= tensor(7.7738e+08, device='cuda:0')
c= tensor(7.7738e+08, device='cuda:0')
c= tensor(7.7738e+08, device='cuda:0')
c= tensor(7.7739e+08, device='cuda:0')
c= tensor(7.7739e+08, device='cuda:0')
c= tensor(7.7740e+08, device='cuda:0')
c= tensor(7.7741e+08, device='cuda:0')
c= tensor(7.7742e+08, device='cuda:0')
c= tensor(7.7743e+08, device='cuda:0')
c= tensor(7.7744e+08, device='cuda:0')
c= tensor(7.7745e+08, device='cuda:0')
c= tensor(7.7749e+08, device='cuda:0')
c= tensor(7.7754e+08, device='cuda:0')
c= tensor(7.7766e+08, device='cuda:0')
c= tensor(7.7769e+08, device='cuda:0')
c= tensor(7.7771e+08, device='cuda:0')
c= tensor(7.7776e+08, device='cuda:0')
c= tensor(7.7818e+08, device='cuda:0')
c= tensor(7.7834e+08, device='cuda:0')
c= tensor(7.7835e+08, device='cuda:0')
c= tensor(7.8724e+08, device='cuda:0')
c= tensor(7.9203e+08, device='cuda:0')
c= tensor(7.9433e+08, device='cuda:0')
c= tensor(7.9772e+08, device='cuda:0')
c= tensor(7.9772e+08, device='cuda:0')
c= tensor(7.9796e+08, device='cuda:0')
c= tensor(8.3753e+08, device='cuda:0')
c= tensor(8.8563e+08, device='cuda:0')
c= tensor(8.8563e+08, device='cuda:0')
c= tensor(8.8587e+08, device='cuda:0')
c= tensor(8.9171e+08, device='cuda:0')
c= tensor(8.9188e+08, device='cuda:0')
c= tensor(8.9227e+08, device='cuda:0')
c= tensor(8.9290e+08, device='cuda:0')
c= tensor(8.9308e+08, device='cuda:0')
c= tensor(8.9314e+08, device='cuda:0')
c= tensor(8.9336e+08, device='cuda:0')
c= tensor(8.9483e+08, device='cuda:0')
c= tensor(8.9514e+08, device='cuda:0')
c= tensor(8.9516e+08, device='cuda:0')
c= tensor(8.9556e+08, device='cuda:0')
c= tensor(8.9590e+08, device='cuda:0')
c= tensor(9.1658e+08, device='cuda:0')
c= tensor(9.1777e+08, device='cuda:0')
c= tensor(9.1782e+08, device='cuda:0')
c= tensor(9.1837e+08, device='cuda:0')
c= tensor(9.1838e+08, device='cuda:0')
c= tensor(9.1869e+08, device='cuda:0')
c= tensor(9.2006e+08, device='cuda:0')
c= tensor(9.2850e+08, device='cuda:0')
c= tensor(9.3497e+08, device='cuda:0')
c= tensor(9.3497e+08, device='cuda:0')
c= tensor(9.3499e+08, device='cuda:0')
c= tensor(9.3656e+08, device='cuda:0')
c= tensor(9.4072e+08, device='cuda:0')
c= tensor(9.4170e+08, device='cuda:0')
c= tensor(9.4170e+08, device='cuda:0')
c= tensor(9.6324e+08, device='cuda:0')
c= tensor(9.6333e+08, device='cuda:0')
c= tensor(9.6350e+08, device='cuda:0')
c= tensor(9.6546e+08, device='cuda:0')
c= tensor(9.6546e+08, device='cuda:0')
c= tensor(9.6693e+08, device='cuda:0')
c= tensor(9.6919e+08, device='cuda:0')
c= tensor(1.0186e+09, device='cuda:0')
c= tensor(1.0197e+09, device='cuda:0')
c= tensor(1.0205e+09, device='cuda:0')
c= tensor(1.0207e+09, device='cuda:0')
c= tensor(1.0207e+09, device='cuda:0')
c= tensor(1.0215e+09, device='cuda:0')
c= tensor(1.0216e+09, device='cuda:0')
c= tensor(1.0220e+09, device='cuda:0')
c= tensor(1.0307e+09, device='cuda:0')
c= tensor(1.0313e+09, device='cuda:0')
c= tensor(1.0314e+09, device='cuda:0')
c= tensor(1.0314e+09, device='cuda:0')
c= tensor(1.0840e+09, device='cuda:0')
c= tensor(1.0841e+09, device='cuda:0')
c= tensor(1.0841e+09, device='cuda:0')
c= tensor(1.0841e+09, device='cuda:0')
c= tensor(1.0973e+09, device='cuda:0')
c= tensor(1.0976e+09, device='cuda:0')
c= tensor(1.0980e+09, device='cuda:0')
c= tensor(1.0983e+09, device='cuda:0')
c= tensor(1.0989e+09, device='cuda:0')
c= tensor(1.0996e+09, device='cuda:0')
c= tensor(1.1893e+09, device='cuda:0')
c= tensor(1.1907e+09, device='cuda:0')
c= tensor(1.1909e+09, device='cuda:0')
c= tensor(1.1955e+09, device='cuda:0')
c= tensor(1.1988e+09, device='cuda:0')
c= tensor(1.1989e+09, device='cuda:0')
c= tensor(1.2025e+09, device='cuda:0')
c= tensor(1.2106e+09, device='cuda:0')
c= tensor(1.2876e+09, device='cuda:0')
c= tensor(1.2880e+09, device='cuda:0')
c= tensor(1.2880e+09, device='cuda:0')
c= tensor(1.2881e+09, device='cuda:0')
c= tensor(1.2894e+09, device='cuda:0')
c= tensor(1.2899e+09, device='cuda:0')
c= tensor(1.2902e+09, device='cuda:0')
c= tensor(1.2902e+09, device='cuda:0')
c= tensor(1.2906e+09, device='cuda:0')
c= tensor(1.2950e+09, device='cuda:0')
c= tensor(1.3159e+09, device='cuda:0')
c= tensor(1.3159e+09, device='cuda:0')
c= tensor(1.3293e+09, device='cuda:0')
c= tensor(1.3294e+09, device='cuda:0')
c= tensor(1.3296e+09, device='cuda:0')
c= tensor(1.3297e+09, device='cuda:0')
c= tensor(1.3297e+09, device='cuda:0')
c= tensor(1.3647e+09, device='cuda:0')
c= tensor(1.3654e+09, device='cuda:0')
c= tensor(1.3655e+09, device='cuda:0')
c= tensor(1.3660e+09, device='cuda:0')
c= tensor(1.3660e+09, device='cuda:0')
c= tensor(1.4244e+09, device='cuda:0')
c= tensor(1.4246e+09, device='cuda:0')
c= tensor(1.4382e+09, device='cuda:0')
c= tensor(1.4382e+09, device='cuda:0')
c= tensor(1.4382e+09, device='cuda:0')
c= tensor(1.4383e+09, device='cuda:0')
c= tensor(1.4386e+09, device='cuda:0')
c= tensor(1.4387e+09, device='cuda:0')
c= tensor(1.4387e+09, device='cuda:0')
c= tensor(1.4388e+09, device='cuda:0')
c= tensor(1.4388e+09, device='cuda:0')
c= tensor(1.4565e+09, device='cuda:0')
c= tensor(1.4570e+09, device='cuda:0')
c= tensor(1.4574e+09, device='cuda:0')
c= tensor(1.4596e+09, device='cuda:0')
c= tensor(1.4824e+09, device='cuda:0')
c= tensor(1.4825e+09, device='cuda:0')
c= tensor(1.4825e+09, device='cuda:0')
c= tensor(1.4826e+09, device='cuda:0')
c= tensor(1.4826e+09, device='cuda:0')
c= tensor(1.4828e+09, device='cuda:0')
c= tensor(1.4829e+09, device='cuda:0')
c= tensor(1.4830e+09, device='cuda:0')
c= tensor(1.4830e+09, device='cuda:0')
c= tensor(1.4839e+09, device='cuda:0')
c= tensor(1.4841e+09, device='cuda:0')
c= tensor(1.7342e+09, device='cuda:0')
c= tensor(1.7342e+09, device='cuda:0')
c= tensor(1.7349e+09, device='cuda:0')
c= tensor(1.7353e+09, device='cuda:0')
c= tensor(1.7353e+09, device='cuda:0')
c= tensor(1.7484e+09, device='cuda:0')
c= tensor(1.8144e+09, device='cuda:0')
c= tensor(1.8909e+09, device='cuda:0')
c= tensor(1.8932e+09, device='cuda:0')
c= tensor(1.8936e+09, device='cuda:0')
c= tensor(1.8936e+09, device='cuda:0')
c= tensor(1.8987e+09, device='cuda:0')
c= tensor(2.2454e+09, device='cuda:0')
c= tensor(2.2469e+09, device='cuda:0')
c= tensor(2.2471e+09, device='cuda:0')
c= tensor(2.2503e+09, device='cuda:0')
c= tensor(2.2778e+09, device='cuda:0')
c= tensor(2.2802e+09, device='cuda:0')
c= tensor(2.2803e+09, device='cuda:0')
c= tensor(2.2803e+09, device='cuda:0')
c= tensor(2.2804e+09, device='cuda:0')
c= tensor(2.2805e+09, device='cuda:0')
c= tensor(2.3255e+09, device='cuda:0')
c= tensor(2.3256e+09, device='cuda:0')
c= tensor(2.3257e+09, device='cuda:0')
c= tensor(2.3265e+09, device='cuda:0')
c= tensor(2.3268e+09, device='cuda:0')
c= tensor(2.3268e+09, device='cuda:0')
c= tensor(2.3284e+09, device='cuda:0')
c= tensor(2.3301e+09, device='cuda:0')
c= tensor(2.3325e+09, device='cuda:0')
c= tensor(2.3359e+09, device='cuda:0')
c= tensor(2.3399e+09, device='cuda:0')
c= tensor(2.3402e+09, device='cuda:0')
c= tensor(2.3427e+09, device='cuda:0')
c= tensor(2.3449e+09, device='cuda:0')
c= tensor(2.3689e+09, device='cuda:0')
c= tensor(2.3690e+09, device='cuda:0')
c= tensor(2.4159e+09, device='cuda:0')
c= tensor(2.4243e+09, device='cuda:0')
c= tensor(2.4283e+09, device='cuda:0')
c= tensor(2.4295e+09, device='cuda:0')
c= tensor(2.4305e+09, device='cuda:0')
c= tensor(2.4307e+09, device='cuda:0')
c= tensor(2.4308e+09, device='cuda:0')
c= tensor(2.4357e+09, device='cuda:0')
c= tensor(2.4469e+09, device='cuda:0')
c= tensor(2.4522e+09, device='cuda:0')
c= tensor(2.4693e+09, device='cuda:0')
c= tensor(2.4721e+09, device='cuda:0')
c= tensor(2.4766e+09, device='cuda:0')
c= tensor(2.4768e+09, device='cuda:0')
c= tensor(2.4857e+09, device='cuda:0')
c= tensor(2.4857e+09, device='cuda:0')
c= tensor(2.4858e+09, device='cuda:0')
c= tensor(2.5013e+09, device='cuda:0')
c= tensor(2.5019e+09, device='cuda:0')
c= tensor(2.5019e+09, device='cuda:0')
c= tensor(2.5026e+09, device='cuda:0')
c= tensor(2.5117e+09, device='cuda:0')
c= tensor(2.5127e+09, device='cuda:0')
c= tensor(2.5139e+09, device='cuda:0')
c= tensor(2.5140e+09, device='cuda:0')
c= tensor(2.5140e+09, device='cuda:0')
c= tensor(2.5140e+09, device='cuda:0')
c= tensor(2.5168e+09, device='cuda:0')
c= tensor(2.5172e+09, device='cuda:0')
c= tensor(2.5201e+09, device='cuda:0')
c= tensor(2.5201e+09, device='cuda:0')
c= tensor(2.5239e+09, device='cuda:0')
c= tensor(2.5240e+09, device='cuda:0')
c= tensor(2.5244e+09, device='cuda:0')
c= tensor(2.5250e+09, device='cuda:0')
c= tensor(2.5298e+09, device='cuda:0')
c= tensor(2.5298e+09, device='cuda:0')
c= tensor(2.5336e+09, device='cuda:0')
c= tensor(2.5342e+09, device='cuda:0')
c= tensor(2.5346e+09, device='cuda:0')
c= tensor(2.5351e+09, device='cuda:0')
c= tensor(2.5939e+09, device='cuda:0')
c= tensor(2.5940e+09, device='cuda:0')
c= tensor(2.5942e+09, device='cuda:0')
c= tensor(2.6016e+09, device='cuda:0')
c= tensor(2.6018e+09, device='cuda:0')
c= tensor(2.6535e+09, device='cuda:0')
c= tensor(2.6536e+09, device='cuda:0')
c= tensor(2.6587e+09, device='cuda:0')
c= tensor(2.6832e+09, device='cuda:0')
c= tensor(2.6833e+09, device='cuda:0')
c= tensor(2.6948e+09, device='cuda:0')
c= tensor(2.6954e+09, device='cuda:0')
c= tensor(2.7022e+09, device='cuda:0')
c= tensor(2.7025e+09, device='cuda:0')
c= tensor(2.7033e+09, device='cuda:0')
c= tensor(2.7033e+09, device='cuda:0')
c= tensor(2.7033e+09, device='cuda:0')
c= tensor(2.7033e+09, device='cuda:0')
c= tensor(2.7040e+09, device='cuda:0')
c= tensor(2.7044e+09, device='cuda:0')
c= tensor(2.7156e+09, device='cuda:0')
c= tensor(2.7158e+09, device='cuda:0')
c= tensor(2.7158e+09, device='cuda:0')
c= tensor(2.7159e+09, device='cuda:0')
c= tensor(2.7161e+09, device='cuda:0')
c= tensor(2.7161e+09, device='cuda:0')
c= tensor(2.7226e+09, device='cuda:0')
c= tensor(2.7248e+09, device='cuda:0')
c= tensor(2.7249e+09, device='cuda:0')
c= tensor(2.7249e+09, device='cuda:0')
c= tensor(2.7250e+09, device='cuda:0')
c= tensor(2.8671e+09, device='cuda:0')
c= tensor(2.8671e+09, device='cuda:0')
c= tensor(2.8673e+09, device='cuda:0')
c= tensor(2.8729e+09, device='cuda:0')
c= tensor(2.8740e+09, device='cuda:0')
c= tensor(2.8741e+09, device='cuda:0')
c= tensor(2.8741e+09, device='cuda:0')
c= tensor(2.8784e+09, device='cuda:0')
c= tensor(2.8838e+09, device='cuda:0')
c= tensor(2.8860e+09, device='cuda:0')
c= tensor(2.8861e+09, device='cuda:0')
c= tensor(2.8862e+09, device='cuda:0')
c= tensor(2.8872e+09, device='cuda:0')
c= tensor(2.8947e+09, device='cuda:0')
c= tensor(2.8978e+09, device='cuda:0')
c= tensor(2.8978e+09, device='cuda:0')
c= tensor(2.9011e+09, device='cuda:0')
c= tensor(2.9011e+09, device='cuda:0')
c= tensor(2.9036e+09, device='cuda:0')
c= tensor(2.9037e+09, device='cuda:0')
c= tensor(2.9037e+09, device='cuda:0')
c= tensor(2.9039e+09, device='cuda:0')
c= tensor(2.9064e+09, device='cuda:0')
c= tensor(2.9067e+09, device='cuda:0')
c= tensor(2.9077e+09, device='cuda:0')
c= tensor(2.9080e+09, device='cuda:0')
c= tensor(2.9363e+09, device='cuda:0')
c= tensor(2.9363e+09, device='cuda:0')
c= tensor(2.9363e+09, device='cuda:0')
c= tensor(2.9365e+09, device='cuda:0')
c= tensor(2.9368e+09, device='cuda:0')
c= tensor(2.9368e+09, device='cuda:0')
c= tensor(2.9369e+09, device='cuda:0')
c= tensor(2.9372e+09, device='cuda:0')
c= tensor(2.9470e+09, device='cuda:0')
c= tensor(2.9471e+09, device='cuda:0')
c= tensor(2.9471e+09, device='cuda:0')
c= tensor(2.9471e+09, device='cuda:0')
c= tensor(2.9755e+09, device='cuda:0')
c= tensor(3.1724e+09, device='cuda:0')
c= tensor(3.1728e+09, device='cuda:0')
c= tensor(3.1728e+09, device='cuda:0')
c= tensor(3.1925e+09, device='cuda:0')
c= tensor(3.1971e+09, device='cuda:0')
c= tensor(3.1971e+09, device='cuda:0')
c= tensor(3.1973e+09, device='cuda:0')
c= tensor(3.1979e+09, device='cuda:0')
c= tensor(3.2805e+09, device='cuda:0')
c= tensor(3.5833e+09, device='cuda:0')
c= tensor(3.5881e+09, device='cuda:0')
c= tensor(3.5884e+09, device='cuda:0')
c= tensor(3.5884e+09, device='cuda:0')
c= tensor(3.5885e+09, device='cuda:0')
c= tensor(3.5938e+09, device='cuda:0')
c= tensor(3.5939e+09, device='cuda:0')
c= tensor(3.5951e+09, device='cuda:0')
c= tensor(3.6005e+09, device='cuda:0')
c= tensor(3.6380e+09, device='cuda:0')
c= tensor(3.6380e+09, device='cuda:0')
c= tensor(3.6380e+09, device='cuda:0')
c= tensor(3.6387e+09, device='cuda:0')
c= tensor(3.6537e+09, device='cuda:0')
c= tensor(3.6548e+09, device='cuda:0')
c= tensor(3.6549e+09, device='cuda:0')
c= tensor(3.6549e+09, device='cuda:0')
c= tensor(3.6551e+09, device='cuda:0')
c= tensor(3.6551e+09, device='cuda:0')
c= tensor(3.6552e+09, device='cuda:0')
c= tensor(3.6552e+09, device='cuda:0')
c= tensor(3.6552e+09, device='cuda:0')
c= tensor(3.6554e+09, device='cuda:0')
c= tensor(3.6554e+09, device='cuda:0')
c= tensor(3.6554e+09, device='cuda:0')
c= tensor(3.6607e+09, device='cuda:0')
c= tensor(3.6870e+09, device='cuda:0')
c= tensor(3.6894e+09, device='cuda:0')
c= tensor(3.6912e+09, device='cuda:0')
c= tensor(3.6913e+09, device='cuda:0')
c= tensor(3.6914e+09, device='cuda:0')
c= tensor(3.6918e+09, device='cuda:0')
c= tensor(3.6971e+09, device='cuda:0')
c= tensor(3.6973e+09, device='cuda:0')
c= tensor(3.6992e+09, device='cuda:0')
c= tensor(3.6993e+09, device='cuda:0')
c= tensor(3.8750e+09, device='cuda:0')
c= tensor(3.8754e+09, device='cuda:0')
c= tensor(3.8773e+09, device='cuda:0')
c= tensor(3.8899e+09, device='cuda:0')
c= tensor(3.8917e+09, device='cuda:0')
c= tensor(3.8920e+09, device='cuda:0')
c= tensor(3.9346e+09, device='cuda:0')
c= tensor(3.9450e+09, device='cuda:0')
c= tensor(3.9452e+09, device='cuda:0')
c= tensor(3.9453e+09, device='cuda:0')
c= tensor(3.9458e+09, device='cuda:0')
c= tensor(3.9459e+09, device='cuda:0')
c= tensor(3.9476e+09, device='cuda:0')
c= tensor(3.9853e+09, device='cuda:0')
c= tensor(3.9887e+09, device='cuda:0')
c= tensor(3.9931e+09, device='cuda:0')
c= tensor(3.9932e+09, device='cuda:0')
c= tensor(3.9932e+09, device='cuda:0')
c= tensor(3.9935e+09, device='cuda:0')
c= tensor(3.9936e+09, device='cuda:0')
c= tensor(4.0051e+09, device='cuda:0')
c= tensor(4.0393e+09, device='cuda:0')
c= tensor(4.0393e+09, device='cuda:0')
c= tensor(4.3897e+09, device='cuda:0')
c= tensor(4.3963e+09, device='cuda:0')
c= tensor(4.3984e+09, device='cuda:0')
c= tensor(4.3986e+09, device='cuda:0')
c= tensor(4.4001e+09, device='cuda:0')
c= tensor(4.4096e+09, device='cuda:0')
c= tensor(4.4096e+09, device='cuda:0')
c= tensor(4.5181e+09, device='cuda:0')
c= tensor(4.5186e+09, device='cuda:0')
c= tensor(4.5198e+09, device='cuda:0')
c= tensor(4.5201e+09, device='cuda:0')
c= tensor(4.5203e+09, device='cuda:0')
c= tensor(4.5203e+09, device='cuda:0')
c= tensor(4.5205e+09, device='cuda:0')
c= tensor(4.5205e+09, device='cuda:0')
c= tensor(4.5214e+09, device='cuda:0')
c= tensor(4.6360e+09, device='cuda:0')
c= tensor(4.6391e+09, device='cuda:0')
c= tensor(4.6688e+09, device='cuda:0')
c= tensor(4.6688e+09, device='cuda:0')
c= tensor(4.6692e+09, device='cuda:0')
c= tensor(4.6692e+09, device='cuda:0')
c= tensor(4.6713e+09, device='cuda:0')
c= tensor(4.6777e+09, device='cuda:0')
c= tensor(5.0544e+09, device='cuda:0')
c= tensor(5.0547e+09, device='cuda:0')
c= tensor(5.0620e+09, device='cuda:0')
c= tensor(5.0621e+09, device='cuda:0')
c= tensor(5.0680e+09, device='cuda:0')
c= tensor(5.0781e+09, device='cuda:0')
c= tensor(5.0788e+09, device='cuda:0')
c= tensor(5.0788e+09, device='cuda:0')
c= tensor(5.0825e+09, device='cuda:0')
c= tensor(5.0836e+09, device='cuda:0')
c= tensor(5.0875e+09, device='cuda:0')
c= tensor(5.1322e+09, device='cuda:0')
c= tensor(5.1329e+09, device='cuda:0')
c= tensor(5.1335e+09, device='cuda:0')
c= tensor(5.1339e+09, device='cuda:0')
c= tensor(5.1412e+09, device='cuda:0')
c= tensor(5.1544e+09, device='cuda:0')
c= tensor(5.1565e+09, device='cuda:0')
c= tensor(5.1566e+09, device='cuda:0')
c= tensor(5.2024e+09, device='cuda:0')
c= tensor(5.2052e+09, device='cuda:0')
c= tensor(5.2429e+09, device='cuda:0')
c= tensor(5.2532e+09, device='cuda:0')
c= tensor(5.2579e+09, device='cuda:0')
c= tensor(5.2583e+09, device='cuda:0')
c= tensor(5.2786e+09, device='cuda:0')
c= tensor(5.3105e+09, device='cuda:0')
c= tensor(5.3122e+09, device='cuda:0')
c= tensor(5.3122e+09, device='cuda:0')
c= tensor(5.3126e+09, device='cuda:0')
c= tensor(5.3134e+09, device='cuda:0')
c= tensor(5.3178e+09, device='cuda:0')
c= tensor(5.3186e+09, device='cuda:0')
c= tensor(5.3186e+09, device='cuda:0')
c= tensor(5.3189e+09, device='cuda:0')
c= tensor(5.3251e+09, device='cuda:0')
c= tensor(5.3266e+09, device='cuda:0')
c= tensor(5.3270e+09, device='cuda:0')
c= tensor(5.3279e+09, device='cuda:0')
c= tensor(5.3292e+09, device='cuda:0')
c= tensor(5.3295e+09, device='cuda:0')
c= tensor(5.3295e+09, device='cuda:0')
c= tensor(5.3295e+09, device='cuda:0')
c= tensor(5.3434e+09, device='cuda:0')
c= tensor(5.3437e+09, device='cuda:0')
c= tensor(5.3454e+09, device='cuda:0')
c= tensor(5.3454e+09, device='cuda:0')
c= tensor(5.3454e+09, device='cuda:0')
c= tensor(5.3582e+09, device='cuda:0')
c= tensor(5.3638e+09, device='cuda:0')
c= tensor(5.3645e+09, device='cuda:0')
c= tensor(5.3645e+09, device='cuda:0')
c= tensor(5.3645e+09, device='cuda:0')
c= tensor(5.3650e+09, device='cuda:0')
c= tensor(5.3652e+09, device='cuda:0')
c= tensor(5.3701e+09, device='cuda:0')
c= tensor(5.3702e+09, device='cuda:0')
c= tensor(5.3706e+09, device='cuda:0')
c= tensor(5.3713e+09, device='cuda:0')
c= tensor(5.3715e+09, device='cuda:0')
c= tensor(5.3790e+09, device='cuda:0')
c= tensor(5.3792e+09, device='cuda:0')
c= tensor(5.3832e+09, device='cuda:0')
c= tensor(5.4034e+09, device='cuda:0')
c= tensor(5.4038e+09, device='cuda:0')
c= tensor(5.4048e+09, device='cuda:0')
c= tensor(5.4074e+09, device='cuda:0')
c= tensor(5.4074e+09, device='cuda:0')
c= tensor(5.4082e+09, device='cuda:0')
c= tensor(5.4083e+09, device='cuda:0')
c= tensor(5.4318e+09, device='cuda:0')
c= tensor(5.4342e+09, device='cuda:0')
c= tensor(5.4357e+09, device='cuda:0')
c= tensor(5.4399e+09, device='cuda:0')
c= tensor(5.4402e+09, device='cuda:0')
c= tensor(5.4407e+09, device='cuda:0')
c= tensor(5.4408e+09, device='cuda:0')
c= tensor(5.4411e+09, device='cuda:0')
c= tensor(5.4612e+09, device='cuda:0')
c= tensor(5.5306e+09, device='cuda:0')
c= tensor(5.5327e+09, device='cuda:0')
c= tensor(5.5328e+09, device='cuda:0')
c= tensor(5.5330e+09, device='cuda:0')
c= tensor(5.6299e+09, device='cuda:0')
c= tensor(5.6302e+09, device='cuda:0')
c= tensor(5.6303e+09, device='cuda:0')
c= tensor(5.6314e+09, device='cuda:0')
c= tensor(5.6326e+09, device='cuda:0')
c= tensor(5.6326e+09, device='cuda:0')
c= tensor(5.6327e+09, device='cuda:0')
c= tensor(5.6372e+09, device='cuda:0')
memory (bytes)
5144752128
time for making loss 2 is 12.628463506698608
p0 True
it  0 : 2357241856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 48% |
shape of L is 
torch.Size([])
memory (bytes)
5144973312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 11% |
memory (bytes)
5145526272
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  101841700000.0
relative error loss 18.065931
shape of L is 
torch.Size([])
memory (bytes)
5303390208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  4% | 12% |
memory (bytes)
5303459840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 12% |
error is  101841310000.0
relative error loss 18.065863
shape of L is 
torch.Size([])
memory (bytes)
5309992960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5310013440
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  101839490000.0
relative error loss 18.065538
shape of L is 
torch.Size([])
memory (bytes)
5312069632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5312073728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  101827720000.0
relative error loss 18.063452
shape of L is 
torch.Size([])
memory (bytes)
5314232320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5314232320
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  101764760000.0
relative error loss 18.052282
shape of L is 
torch.Size([])
memory (bytes)
5316288512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 12% |
memory (bytes)
5316345856
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  101418566000.0
relative error loss 17.99087
shape of L is 
torch.Size([])
memory (bytes)
5318451200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 12% |
memory (bytes)
5318459392
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  97674560000.0
relative error loss 17.326714
shape of L is 
torch.Size([])
memory (bytes)
5320568832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5320581120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  80594680000.0
relative error loss 14.296874
shape of L is 
torch.Size([])
memory (bytes)
5322706944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5322715136
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  21994959000.0
relative error loss 3.9017358
shape of L is 
torch.Size([])
memory (bytes)
5324832768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5324857344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  11536195000.0
relative error loss 2.0464318
time to take a step is 215.22571730613708
it  1 : 2763069952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5326991360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5326999552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  11536195000.0
relative error loss 2.0464318
shape of L is 
torch.Size([])
memory (bytes)
5329149952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5329154048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  7265263600.0
relative error loss 1.2888017
shape of L is 
torch.Size([])
memory (bytes)
5331025920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5331025920
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  6098415000.0
relative error loss 1.0818118
shape of L is 
torch.Size([])
memory (bytes)
5333393408
| ID | GPU | MEM |
------------------
|  0 |  2% |  0% |
|  1 | 23% | 12% |
memory (bytes)
5333393408
| ID | GPU  | MEM |
-------------------
|  0 |  25% |  0% |
|  1 | 100% | 12% |
error is  8196349400.0
relative error loss 1.4539691
shape of L is 
torch.Size([])
memory (bytes)
5335519232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5335527424
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  5822641700.0
relative error loss 1.0328916
shape of L is 
torch.Size([])
memory (bytes)
5337710592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
memory (bytes)
5337710592
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  5547629600.0
relative error loss 0.98410666
shape of L is 
torch.Size([])
memory (bytes)
5339762688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 12% |
memory (bytes)
5339762688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  5377560000.0
relative error loss 0.95393765
shape of L is 
torch.Size([])
memory (bytes)
5341888512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% | 12% |
memory (bytes)
5341888512
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  5099877400.0
relative error loss 0.9046789
shape of L is 
torch.Size([])
memory (bytes)
5344063488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 12% |
memory (bytes)
5344063488
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  5075940400.0
relative error loss 0.90043265
shape of L is 
torch.Size([])
memory (bytes)
5346095104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 12% |
memory (bytes)
5346095104
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  4875791000.0
relative error loss 0.86492765
time to take a step is 168.65547466278076
it  2 : 2877903872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5348294656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5348294656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% | 12% |
error is  4875791000.0
relative error loss 0.86492765
shape of L is 
torch.Size([])
memory (bytes)
5350387712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5350412288
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  4639938000.0
relative error loss 0.8230892
shape of L is 
torch.Size([])
memory (bytes)
5352521728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5352525824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% | 12% |
error is  4270396000.0
relative error loss 0.7575353
shape of L is 
torch.Size([])
memory (bytes)
5354663936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5354663936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  4015167500.0
relative error loss 0.7122597
shape of L is 
torch.Size([])
memory (bytes)
5356781568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5356785664
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  3828354600.0
relative error loss 0.67912054
shape of L is 
torch.Size([])
memory (bytes)
5358886912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5358886912
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  3527637500.0
relative error loss 0.62577564
shape of L is 
torch.Size([])
memory (bytes)
5361012736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 12% |
memory (bytes)
5361012736
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  3366423300.0
relative error loss 0.5971775
shape of L is 
torch.Size([])
memory (bytes)
5363048448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5363150848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  3240839000.0
relative error loss 0.5748998
shape of L is 
torch.Size([])
memory (bytes)
5365280768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5365288960
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  3410381600.0
relative error loss 0.60497534
shape of L is 
torch.Size([])
memory (bytes)
5367435264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5367435264
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  2863340800.0
relative error loss 0.5079346
time to take a step is 199.34914088249207
it  3 : 2878773248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5369569280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5369573376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  2863340800.0
relative error loss 0.5079346
shape of L is 
torch.Size([])
memory (bytes)
5371707392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5371707392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  2608396800.0
relative error loss 0.46270946
shape of L is 
torch.Size([])
memory (bytes)
5373849600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5373849600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  2233038600.0
relative error loss 0.3961238
shape of L is 
torch.Size([])
memory (bytes)
5375782912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5375782912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  2018391300.0
relative error loss 0.35804704
shape of L is 
torch.Size([])
memory (bytes)
5377777664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5377777664
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1851042800.0
relative error loss 0.3283607
shape of L is 
torch.Size([])
memory (bytes)
5380251648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
memory (bytes)
5380259840
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1759272200.0
relative error loss 0.3120813
shape of L is 
torch.Size([])
memory (bytes)
5382385664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5382393856
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1665728300.0
relative error loss 0.29548734
shape of L is 
torch.Size([])
memory (bytes)
5384531968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5384540160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1621860100.0
relative error loss 0.28770545
shape of L is 
torch.Size([])
memory (bytes)
5386657792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5386665984
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1511307000.0
relative error loss 0.26809418
shape of L is 
torch.Size([])
memory (bytes)
5388800000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 12% |
memory (bytes)
5388808192
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1401673700.0
relative error loss 0.2486461
time to take a step is 204.62300658226013
c= tensor(3032.5371, device='cuda:0')
c= tensor(205429.2969, device='cuda:0')
c= tensor(210257.4531, device='cuda:0')
c= tensor(218338.1250, device='cuda:0')
c= tensor(4195295., device='cuda:0')
c= tensor(5463638., device='cuda:0')
c= tensor(6469248., device='cuda:0')
c= tensor(7189663., device='cuda:0')
c= tensor(7801846., device='cuda:0')
c= tensor(24530990., device='cuda:0')
c= tensor(24849328., device='cuda:0')
c= tensor(27845460., device='cuda:0')
c= tensor(27875880., device='cuda:0')
c= tensor(46474876., device='cuda:0')
c= tensor(46723112., device='cuda:0')
c= tensor(47153748., device='cuda:0')
c= tensor(47850952., device='cuda:0')
c= tensor(49209836., device='cuda:0')
c= tensor(56829060., device='cuda:0')
c= tensor(61802392., device='cuda:0')
c= tensor(63231256., device='cuda:0')
c= tensor(74791248., device='cuda:0')
c= tensor(74872752., device='cuda:0')
c= tensor(74989504., device='cuda:0')
c= tensor(76886568., device='cuda:0')
c= tensor(77990480., device='cuda:0')
c= tensor(80334920., device='cuda:0')
c= tensor(80519312., device='cuda:0')
c= tensor(92521480., device='cuda:0')
c= tensor(4.6098e+08, device='cuda:0')
c= tensor(4.6116e+08, device='cuda:0')
c= tensor(7.2989e+08, device='cuda:0')
c= tensor(7.2995e+08, device='cuda:0')
c= tensor(7.3028e+08, device='cuda:0')
c= tensor(7.3117e+08, device='cuda:0')
c= tensor(7.7390e+08, device='cuda:0')
c= tensor(7.7498e+08, device='cuda:0')
c= tensor(7.7498e+08, device='cuda:0')
c= tensor(7.7499e+08, device='cuda:0')
c= tensor(7.7500e+08, device='cuda:0')
c= tensor(7.7501e+08, device='cuda:0')
c= tensor(7.7501e+08, device='cuda:0')
c= tensor(7.7501e+08, device='cuda:0')
c= tensor(7.7502e+08, device='cuda:0')
c= tensor(7.7502e+08, device='cuda:0')
c= tensor(7.7502e+08, device='cuda:0')
c= tensor(7.7502e+08, device='cuda:0')
c= tensor(7.7505e+08, device='cuda:0')
c= tensor(7.7506e+08, device='cuda:0')
c= tensor(7.7508e+08, device='cuda:0')
c= tensor(7.7515e+08, device='cuda:0')
c= tensor(7.7515e+08, device='cuda:0')
c= tensor(7.7516e+08, device='cuda:0')
c= tensor(7.7517e+08, device='cuda:0')
c= tensor(7.7519e+08, device='cuda:0')
c= tensor(7.7520e+08, device='cuda:0')
c= tensor(7.7521e+08, device='cuda:0')
c= tensor(7.7522e+08, device='cuda:0')
c= tensor(7.7523e+08, device='cuda:0')
c= tensor(7.7523e+08, device='cuda:0')
c= tensor(7.7524e+08, device='cuda:0')
c= tensor(7.7524e+08, device='cuda:0')
c= tensor(7.7525e+08, device='cuda:0')
c= tensor(7.7535e+08, device='cuda:0')
c= tensor(7.7536e+08, device='cuda:0')
c= tensor(7.7537e+08, device='cuda:0')
c= tensor(7.7537e+08, device='cuda:0')
c= tensor(7.7539e+08, device='cuda:0')
c= tensor(7.7541e+08, device='cuda:0')
c= tensor(7.7542e+08, device='cuda:0')
c= tensor(7.7542e+08, device='cuda:0')
c= tensor(7.7544e+08, device='cuda:0')
c= tensor(7.7544e+08, device='cuda:0')
c= tensor(7.7545e+08, device='cuda:0')
c= tensor(7.7546e+08, device='cuda:0')
c= tensor(7.7553e+08, device='cuda:0')
c= tensor(7.7553e+08, device='cuda:0')
c= tensor(7.7553e+08, device='cuda:0')
c= tensor(7.7554e+08, device='cuda:0')
c= tensor(7.7564e+08, device='cuda:0')
c= tensor(7.7564e+08, device='cuda:0')
c= tensor(7.7564e+08, device='cuda:0')
c= tensor(7.7565e+08, device='cuda:0')
c= tensor(7.7565e+08, device='cuda:0')
c= tensor(7.7566e+08, device='cuda:0')
c= tensor(7.7566e+08, device='cuda:0')
c= tensor(7.7566e+08, device='cuda:0')
c= tensor(7.7566e+08, device='cuda:0')
c= tensor(7.7567e+08, device='cuda:0')
c= tensor(7.7568e+08, device='cuda:0')
c= tensor(7.7570e+08, device='cuda:0')
c= tensor(7.7570e+08, device='cuda:0')
c= tensor(7.7570e+08, device='cuda:0')
c= tensor(7.7573e+08, device='cuda:0')
c= tensor(7.7574e+08, device='cuda:0')
c= tensor(7.7577e+08, device='cuda:0')
c= tensor(7.7577e+08, device='cuda:0')
c= tensor(7.7578e+08, device='cuda:0')
c= tensor(7.7579e+08, device='cuda:0')
c= tensor(7.7581e+08, device='cuda:0')
c= tensor(7.7586e+08, device='cuda:0')
c= tensor(7.7586e+08, device='cuda:0')
c= tensor(7.7588e+08, device='cuda:0')
c= tensor(7.7588e+08, device='cuda:0')
c= tensor(7.7597e+08, device='cuda:0')
c= tensor(7.7598e+08, device='cuda:0')
c= tensor(7.7598e+08, device='cuda:0')
c= tensor(7.7599e+08, device='cuda:0')
c= tensor(7.7599e+08, device='cuda:0')
c= tensor(7.7599e+08, device='cuda:0')
c= tensor(7.7601e+08, device='cuda:0')
c= tensor(7.7601e+08, device='cuda:0')
c= tensor(7.7601e+08, device='cuda:0')
c= tensor(7.7602e+08, device='cuda:0')
c= tensor(7.7603e+08, device='cuda:0')
c= tensor(7.7604e+08, device='cuda:0')
c= tensor(7.7604e+08, device='cuda:0')
c= tensor(7.7604e+08, device='cuda:0')
c= tensor(7.7608e+08, device='cuda:0')
c= tensor(7.7608e+08, device='cuda:0')
c= tensor(7.7614e+08, device='cuda:0')
c= tensor(7.7615e+08, device='cuda:0')
c= tensor(7.7615e+08, device='cuda:0')
c= tensor(7.7616e+08, device='cuda:0')
c= tensor(7.7617e+08, device='cuda:0')
c= tensor(7.7617e+08, device='cuda:0')
c= tensor(7.7618e+08, device='cuda:0')
c= tensor(7.7619e+08, device='cuda:0')
c= tensor(7.7622e+08, device='cuda:0')
c= tensor(7.7623e+08, device='cuda:0')
c= tensor(7.7637e+08, device='cuda:0')
c= tensor(7.7637e+08, device='cuda:0')
c= tensor(7.7638e+08, device='cuda:0')
c= tensor(7.7638e+08, device='cuda:0')
c= tensor(7.7638e+08, device='cuda:0')
c= tensor(7.7639e+08, device='cuda:0')
c= tensor(7.7639e+08, device='cuda:0')
c= tensor(7.7640e+08, device='cuda:0')
c= tensor(7.7641e+08, device='cuda:0')
c= tensor(7.7641e+08, device='cuda:0')
c= tensor(7.7642e+08, device='cuda:0')
c= tensor(7.7642e+08, device='cuda:0')
c= tensor(7.7646e+08, device='cuda:0')
c= tensor(7.7648e+08, device='cuda:0')
c= tensor(7.7649e+08, device='cuda:0')
c= tensor(7.7650e+08, device='cuda:0')
c= tensor(7.7650e+08, device='cuda:0')
c= tensor(7.7650e+08, device='cuda:0')
c= tensor(7.7651e+08, device='cuda:0')
c= tensor(7.7651e+08, device='cuda:0')
c= tensor(7.7652e+08, device='cuda:0')
c= tensor(7.7653e+08, device='cuda:0')
c= tensor(7.7653e+08, device='cuda:0')
c= tensor(7.7654e+08, device='cuda:0')
c= tensor(7.7655e+08, device='cuda:0')
c= tensor(7.7658e+08, device='cuda:0')
c= tensor(7.7659e+08, device='cuda:0')
c= tensor(7.7660e+08, device='cuda:0')
c= tensor(7.7660e+08, device='cuda:0')
c= tensor(7.7660e+08, device='cuda:0')
c= tensor(7.7668e+08, device='cuda:0')
c= tensor(7.7668e+08, device='cuda:0')
c= tensor(7.7675e+08, device='cuda:0')
c= tensor(7.7675e+08, device='cuda:0')
c= tensor(7.7676e+08, device='cuda:0')
c= tensor(7.7677e+08, device='cuda:0')
c= tensor(7.7678e+08, device='cuda:0')
c= tensor(7.7678e+08, device='cuda:0')
c= tensor(7.7679e+08, device='cuda:0')
c= tensor(7.7679e+08, device='cuda:0')
c= tensor(7.7680e+08, device='cuda:0')
c= tensor(7.7680e+08, device='cuda:0')
c= tensor(7.7682e+08, device='cuda:0')
c= tensor(7.7682e+08, device='cuda:0')
c= tensor(7.7683e+08, device='cuda:0')
c= tensor(7.7684e+08, device='cuda:0')
c= tensor(7.7685e+08, device='cuda:0')
c= tensor(7.7686e+08, device='cuda:0')
c= tensor(7.7686e+08, device='cuda:0')
c= tensor(7.7687e+08, device='cuda:0')
c= tensor(7.7689e+08, device='cuda:0')
c= tensor(7.7689e+08, device='cuda:0')
c= tensor(7.7690e+08, device='cuda:0')
c= tensor(7.7690e+08, device='cuda:0')
c= tensor(7.7691e+08, device='cuda:0')
c= tensor(7.7693e+08, device='cuda:0')
c= tensor(7.7693e+08, device='cuda:0')
c= tensor(7.7694e+08, device='cuda:0')
c= tensor(7.7696e+08, device='cuda:0')
c= tensor(7.7698e+08, device='cuda:0')
c= tensor(7.7699e+08, device='cuda:0')
c= tensor(7.7699e+08, device='cuda:0')
c= tensor(7.7700e+08, device='cuda:0')
c= tensor(7.7700e+08, device='cuda:0')
c= tensor(7.7701e+08, device='cuda:0')
c= tensor(7.7702e+08, device='cuda:0')
c= tensor(7.7702e+08, device='cuda:0')
c= tensor(7.7703e+08, device='cuda:0')
c= tensor(7.7704e+08, device='cuda:0')
c= tensor(7.7704e+08, device='cuda:0')
c= tensor(7.7705e+08, device='cuda:0')
c= tensor(7.7706e+08, device='cuda:0')
c= tensor(7.7709e+08, device='cuda:0')
c= tensor(7.7709e+08, device='cuda:0')
c= tensor(7.7712e+08, device='cuda:0')
c= tensor(7.7713e+08, device='cuda:0')
c= tensor(7.7713e+08, device='cuda:0')
c= tensor(7.7714e+08, device='cuda:0')
c= tensor(7.7714e+08, device='cuda:0')
c= tensor(7.7717e+08, device='cuda:0')
c= tensor(7.7718e+08, device='cuda:0')
c= tensor(7.7718e+08, device='cuda:0')
c= tensor(7.7719e+08, device='cuda:0')
c= tensor(7.7723e+08, device='cuda:0')
c= tensor(7.7723e+08, device='cuda:0')
c= tensor(7.7723e+08, device='cuda:0')
c= tensor(7.7723e+08, device='cuda:0')
c= tensor(7.7723e+08, device='cuda:0')
c= tensor(7.7729e+08, device='cuda:0')
c= tensor(7.7730e+08, device='cuda:0')
c= tensor(7.7731e+08, device='cuda:0')
c= tensor(7.7731e+08, device='cuda:0')
c= tensor(7.7733e+08, device='cuda:0')
c= tensor(7.7734e+08, device='cuda:0')
c= tensor(7.7734e+08, device='cuda:0')
c= tensor(7.7735e+08, device='cuda:0')
c= tensor(7.7736e+08, device='cuda:0')
c= tensor(7.7737e+08, device='cuda:0')
c= tensor(7.7738e+08, device='cuda:0')
c= tensor(7.7738e+08, device='cuda:0')
c= tensor(7.7738e+08, device='cuda:0')
c= tensor(7.7739e+08, device='cuda:0')
c= tensor(7.7739e+08, device='cuda:0')
c= tensor(7.7740e+08, device='cuda:0')
c= tensor(7.7741e+08, device='cuda:0')
c= tensor(7.7742e+08, device='cuda:0')
c= tensor(7.7743e+08, device='cuda:0')
c= tensor(7.7744e+08, device='cuda:0')
c= tensor(7.7745e+08, device='cuda:0')
c= tensor(7.7749e+08, device='cuda:0')
c= tensor(7.7754e+08, device='cuda:0')
c= tensor(7.7766e+08, device='cuda:0')
c= tensor(7.7769e+08, device='cuda:0')
c= tensor(7.7771e+08, device='cuda:0')
c= tensor(7.7776e+08, device='cuda:0')
c= tensor(7.7818e+08, device='cuda:0')
c= tensor(7.7834e+08, device='cuda:0')
c= tensor(7.7835e+08, device='cuda:0')
c= tensor(7.8724e+08, device='cuda:0')
c= tensor(7.9203e+08, device='cuda:0')
c= tensor(7.9433e+08, device='cuda:0')
c= tensor(7.9772e+08, device='cuda:0')
c= tensor(7.9772e+08, device='cuda:0')
c= tensor(7.9796e+08, device='cuda:0')
c= tensor(8.3753e+08, device='cuda:0')
c= tensor(8.8563e+08, device='cuda:0')
c= tensor(8.8563e+08, device='cuda:0')
c= tensor(8.8587e+08, device='cuda:0')
c= tensor(8.9171e+08, device='cuda:0')
c= tensor(8.9188e+08, device='cuda:0')
c= tensor(8.9227e+08, device='cuda:0')
c= tensor(8.9290e+08, device='cuda:0')
c= tensor(8.9308e+08, device='cuda:0')
c= tensor(8.9314e+08, device='cuda:0')
c= tensor(8.9336e+08, device='cuda:0')
c= tensor(8.9483e+08, device='cuda:0')
c= tensor(8.9514e+08, device='cuda:0')
c= tensor(8.9516e+08, device='cuda:0')
c= tensor(8.9556e+08, device='cuda:0')
c= tensor(8.9590e+08, device='cuda:0')
c= tensor(9.1658e+08, device='cuda:0')
c= tensor(9.1777e+08, device='cuda:0')
c= tensor(9.1782e+08, device='cuda:0')
c= tensor(9.1837e+08, device='cuda:0')
c= tensor(9.1838e+08, device='cuda:0')
c= tensor(9.1869e+08, device='cuda:0')
c= tensor(9.2006e+08, device='cuda:0')
c= tensor(9.2850e+08, device='cuda:0')
c= tensor(9.3497e+08, device='cuda:0')
c= tensor(9.3497e+08, device='cuda:0')
c= tensor(9.3499e+08, device='cuda:0')
c= tensor(9.3656e+08, device='cuda:0')
c= tensor(9.4072e+08, device='cuda:0')
c= tensor(9.4170e+08, device='cuda:0')
c= tensor(9.4170e+08, device='cuda:0')
c= tensor(9.6324e+08, device='cuda:0')
c= tensor(9.6333e+08, device='cuda:0')
c= tensor(9.6350e+08, device='cuda:0')
c= tensor(9.6546e+08, device='cuda:0')
c= tensor(9.6546e+08, device='cuda:0')
c= tensor(9.6693e+08, device='cuda:0')
c= tensor(9.6919e+08, device='cuda:0')
c= tensor(1.0186e+09, device='cuda:0')
c= tensor(1.0197e+09, device='cuda:0')
c= tensor(1.0205e+09, device='cuda:0')
c= tensor(1.0207e+09, device='cuda:0')
c= tensor(1.0207e+09, device='cuda:0')
c= tensor(1.0215e+09, device='cuda:0')
c= tensor(1.0216e+09, device='cuda:0')
c= tensor(1.0220e+09, device='cuda:0')
c= tensor(1.0307e+09, device='cuda:0')
c= tensor(1.0313e+09, device='cuda:0')
c= tensor(1.0314e+09, device='cuda:0')
c= tensor(1.0314e+09, device='cuda:0')
c= tensor(1.0840e+09, device='cuda:0')
c= tensor(1.0841e+09, device='cuda:0')
c= tensor(1.0841e+09, device='cuda:0')
c= tensor(1.0841e+09, device='cuda:0')
c= tensor(1.0973e+09, device='cuda:0')
c= tensor(1.0976e+09, device='cuda:0')
c= tensor(1.0980e+09, device='cuda:0')
c= tensor(1.0983e+09, device='cuda:0')
c= tensor(1.0989e+09, device='cuda:0')
c= tensor(1.0996e+09, device='cuda:0')
c= tensor(1.1893e+09, device='cuda:0')
c= tensor(1.1907e+09, device='cuda:0')
c= tensor(1.1909e+09, device='cuda:0')
c= tensor(1.1955e+09, device='cuda:0')
c= tensor(1.1988e+09, device='cuda:0')
c= tensor(1.1989e+09, device='cuda:0')
c= tensor(1.2025e+09, device='cuda:0')
c= tensor(1.2106e+09, device='cuda:0')
c= tensor(1.2876e+09, device='cuda:0')
c= tensor(1.2880e+09, device='cuda:0')
c= tensor(1.2880e+09, device='cuda:0')
c= tensor(1.2881e+09, device='cuda:0')
c= tensor(1.2894e+09, device='cuda:0')
c= tensor(1.2899e+09, device='cuda:0')
c= tensor(1.2902e+09, device='cuda:0')
c= tensor(1.2902e+09, device='cuda:0')
c= tensor(1.2906e+09, device='cuda:0')
c= tensor(1.2950e+09, device='cuda:0')
c= tensor(1.3159e+09, device='cuda:0')
c= tensor(1.3159e+09, device='cuda:0')
c= tensor(1.3293e+09, device='cuda:0')
c= tensor(1.3294e+09, device='cuda:0')
c= tensor(1.3296e+09, device='cuda:0')
c= tensor(1.3297e+09, device='cuda:0')
c= tensor(1.3297e+09, device='cuda:0')
c= tensor(1.3647e+09, device='cuda:0')
c= tensor(1.3654e+09, device='cuda:0')
c= tensor(1.3655e+09, device='cuda:0')
c= tensor(1.3660e+09, device='cuda:0')
c= tensor(1.3660e+09, device='cuda:0')
c= tensor(1.4244e+09, device='cuda:0')
c= tensor(1.4246e+09, device='cuda:0')
c= tensor(1.4382e+09, device='cuda:0')
c= tensor(1.4382e+09, device='cuda:0')
c= tensor(1.4382e+09, device='cuda:0')
c= tensor(1.4383e+09, device='cuda:0')
c= tensor(1.4386e+09, device='cuda:0')
c= tensor(1.4387e+09, device='cuda:0')
c= tensor(1.4387e+09, device='cuda:0')
c= tensor(1.4388e+09, device='cuda:0')
c= tensor(1.4388e+09, device='cuda:0')
c= tensor(1.4565e+09, device='cuda:0')
c= tensor(1.4570e+09, device='cuda:0')
c= tensor(1.4574e+09, device='cuda:0')
c= tensor(1.4596e+09, device='cuda:0')
c= tensor(1.4824e+09, device='cuda:0')
c= tensor(1.4825e+09, device='cuda:0')
c= tensor(1.4825e+09, device='cuda:0')
c= tensor(1.4826e+09, device='cuda:0')
c= tensor(1.4826e+09, device='cuda:0')
c= tensor(1.4828e+09, device='cuda:0')
c= tensor(1.4829e+09, device='cuda:0')
c= tensor(1.4830e+09, device='cuda:0')
c= tensor(1.4830e+09, device='cuda:0')
c= tensor(1.4839e+09, device='cuda:0')
c= tensor(1.4841e+09, device='cuda:0')
c= tensor(1.7342e+09, device='cuda:0')
c= tensor(1.7342e+09, device='cuda:0')
c= tensor(1.7349e+09, device='cuda:0')
c= tensor(1.7353e+09, device='cuda:0')
c= tensor(1.7353e+09, device='cuda:0')
c= tensor(1.7484e+09, device='cuda:0')
c= tensor(1.8144e+09, device='cuda:0')
c= tensor(1.8909e+09, device='cuda:0')
c= tensor(1.8932e+09, device='cuda:0')
c= tensor(1.8936e+09, device='cuda:0')
c= tensor(1.8936e+09, device='cuda:0')
c= tensor(1.8987e+09, device='cuda:0')
c= tensor(2.2454e+09, device='cuda:0')
c= tensor(2.2469e+09, device='cuda:0')
c= tensor(2.2471e+09, device='cuda:0')
c= tensor(2.2503e+09, device='cuda:0')
c= tensor(2.2778e+09, device='cuda:0')
c= tensor(2.2802e+09, device='cuda:0')
c= tensor(2.2803e+09, device='cuda:0')
c= tensor(2.2803e+09, device='cuda:0')
c= tensor(2.2804e+09, device='cuda:0')
c= tensor(2.2805e+09, device='cuda:0')
c= tensor(2.3255e+09, device='cuda:0')
c= tensor(2.3256e+09, device='cuda:0')
c= tensor(2.3257e+09, device='cuda:0')
c= tensor(2.3265e+09, device='cuda:0')
c= tensor(2.3268e+09, device='cuda:0')
c= tensor(2.3268e+09, device='cuda:0')
c= tensor(2.3284e+09, device='cuda:0')
c= tensor(2.3301e+09, device='cuda:0')
c= tensor(2.3325e+09, device='cuda:0')
c= tensor(2.3359e+09, device='cuda:0')
c= tensor(2.3399e+09, device='cuda:0')
c= tensor(2.3402e+09, device='cuda:0')
c= tensor(2.3427e+09, device='cuda:0')
c= tensor(2.3449e+09, device='cuda:0')
c= tensor(2.3689e+09, device='cuda:0')
c= tensor(2.3690e+09, device='cuda:0')
c= tensor(2.4159e+09, device='cuda:0')
c= tensor(2.4243e+09, device='cuda:0')
c= tensor(2.4283e+09, device='cuda:0')
c= tensor(2.4295e+09, device='cuda:0')
c= tensor(2.4305e+09, device='cuda:0')
c= tensor(2.4307e+09, device='cuda:0')
c= tensor(2.4308e+09, device='cuda:0')
c= tensor(2.4357e+09, device='cuda:0')
c= tensor(2.4469e+09, device='cuda:0')
c= tensor(2.4522e+09, device='cuda:0')
c= tensor(2.4693e+09, device='cuda:0')
c= tensor(2.4721e+09, device='cuda:0')
c= tensor(2.4766e+09, device='cuda:0')
c= tensor(2.4768e+09, device='cuda:0')
c= tensor(2.4857e+09, device='cuda:0')
c= tensor(2.4857e+09, device='cuda:0')
c= tensor(2.4858e+09, device='cuda:0')
c= tensor(2.5013e+09, device='cuda:0')
c= tensor(2.5019e+09, device='cuda:0')
c= tensor(2.5019e+09, device='cuda:0')
c= tensor(2.5026e+09, device='cuda:0')
c= tensor(2.5117e+09, device='cuda:0')
c= tensor(2.5127e+09, device='cuda:0')
c= tensor(2.5139e+09, device='cuda:0')
c= tensor(2.5140e+09, device='cuda:0')
c= tensor(2.5140e+09, device='cuda:0')
c= tensor(2.5140e+09, device='cuda:0')
c= tensor(2.5168e+09, device='cuda:0')
c= tensor(2.5172e+09, device='cuda:0')
c= tensor(2.5201e+09, device='cuda:0')
c= tensor(2.5201e+09, device='cuda:0')
c= tensor(2.5239e+09, device='cuda:0')
c= tensor(2.5240e+09, device='cuda:0')
c= tensor(2.5244e+09, device='cuda:0')
c= tensor(2.5250e+09, device='cuda:0')
c= tensor(2.5298e+09, device='cuda:0')
c= tensor(2.5298e+09, device='cuda:0')
c= tensor(2.5336e+09, device='cuda:0')
c= tensor(2.5342e+09, device='cuda:0')
c= tensor(2.5346e+09, device='cuda:0')
c= tensor(2.5351e+09, device='cuda:0')
c= tensor(2.5939e+09, device='cuda:0')
c= tensor(2.5940e+09, device='cuda:0')
c= tensor(2.5942e+09, device='cuda:0')
c= tensor(2.6016e+09, device='cuda:0')
c= tensor(2.6018e+09, device='cuda:0')
c= tensor(2.6535e+09, device='cuda:0')
c= tensor(2.6536e+09, device='cuda:0')
c= tensor(2.6587e+09, device='cuda:0')
c= tensor(2.6832e+09, device='cuda:0')
c= tensor(2.6833e+09, device='cuda:0')
c= tensor(2.6948e+09, device='cuda:0')
c= tensor(2.6954e+09, device='cuda:0')
c= tensor(2.7022e+09, device='cuda:0')
c= tensor(2.7025e+09, device='cuda:0')
c= tensor(2.7033e+09, device='cuda:0')
c= tensor(2.7033e+09, device='cuda:0')
c= tensor(2.7033e+09, device='cuda:0')
c= tensor(2.7033e+09, device='cuda:0')
c= tensor(2.7040e+09, device='cuda:0')
c= tensor(2.7044e+09, device='cuda:0')
c= tensor(2.7156e+09, device='cuda:0')
c= tensor(2.7158e+09, device='cuda:0')
c= tensor(2.7158e+09, device='cuda:0')
c= tensor(2.7159e+09, device='cuda:0')
c= tensor(2.7161e+09, device='cuda:0')
c= tensor(2.7161e+09, device='cuda:0')
c= tensor(2.7226e+09, device='cuda:0')
c= tensor(2.7248e+09, device='cuda:0')
c= tensor(2.7249e+09, device='cuda:0')
c= tensor(2.7249e+09, device='cuda:0')
c= tensor(2.7250e+09, device='cuda:0')
c= tensor(2.8671e+09, device='cuda:0')
c= tensor(2.8671e+09, device='cuda:0')
c= tensor(2.8673e+09, device='cuda:0')
c= tensor(2.8729e+09, device='cuda:0')
c= tensor(2.8740e+09, device='cuda:0')
c= tensor(2.8741e+09, device='cuda:0')
c= tensor(2.8741e+09, device='cuda:0')
c= tensor(2.8784e+09, device='cuda:0')
c= tensor(2.8838e+09, device='cuda:0')
c= tensor(2.8860e+09, device='cuda:0')
c= tensor(2.8861e+09, device='cuda:0')
c= tensor(2.8862e+09, device='cuda:0')
c= tensor(2.8872e+09, device='cuda:0')
c= tensor(2.8947e+09, device='cuda:0')
c= tensor(2.8978e+09, device='cuda:0')
c= tensor(2.8978e+09, device='cuda:0')
c= tensor(2.9011e+09, device='cuda:0')
c= tensor(2.9011e+09, device='cuda:0')
c= tensor(2.9036e+09, device='cuda:0')
c= tensor(2.9037e+09, device='cuda:0')
c= tensor(2.9037e+09, device='cuda:0')
c= tensor(2.9039e+09, device='cuda:0')
c= tensor(2.9064e+09, device='cuda:0')
c= tensor(2.9067e+09, device='cuda:0')
c= tensor(2.9077e+09, device='cuda:0')
c= tensor(2.9080e+09, device='cuda:0')
c= tensor(2.9363e+09, device='cuda:0')
c= tensor(2.9363e+09, device='cuda:0')
c= tensor(2.9363e+09, device='cuda:0')
c= tensor(2.9365e+09, device='cuda:0')
c= tensor(2.9368e+09, device='cuda:0')
c= tensor(2.9368e+09, device='cuda:0')
c= tensor(2.9369e+09, device='cuda:0')
c= tensor(2.9372e+09, device='cuda:0')
c= tensor(2.9470e+09, device='cuda:0')
c= tensor(2.9471e+09, device='cuda:0')
c= tensor(2.9471e+09, device='cuda:0')
c= tensor(2.9471e+09, device='cuda:0')
c= tensor(2.9755e+09, device='cuda:0')
c= tensor(3.1724e+09, device='cuda:0')
c= tensor(3.1728e+09, device='cuda:0')
c= tensor(3.1728e+09, device='cuda:0')
c= tensor(3.1925e+09, device='cuda:0')
c= tensor(3.1971e+09, device='cuda:0')
c= tensor(3.1971e+09, device='cuda:0')
c= tensor(3.1973e+09, device='cuda:0')
c= tensor(3.1979e+09, device='cuda:0')
c= tensor(3.2805e+09, device='cuda:0')
c= tensor(3.5833e+09, device='cuda:0')
c= tensor(3.5881e+09, device='cuda:0')
c= tensor(3.5884e+09, device='cuda:0')
c= tensor(3.5884e+09, device='cuda:0')
c= tensor(3.5885e+09, device='cuda:0')
c= tensor(3.5938e+09, device='cuda:0')
c= tensor(3.5939e+09, device='cuda:0')
c= tensor(3.5951e+09, device='cuda:0')
c= tensor(3.6005e+09, device='cuda:0')
c= tensor(3.6380e+09, device='cuda:0')
c= tensor(3.6380e+09, device='cuda:0')
c= tensor(3.6380e+09, device='cuda:0')
c= tensor(3.6387e+09, device='cuda:0')
c= tensor(3.6537e+09, device='cuda:0')
c= tensor(3.6548e+09, device='cuda:0')
c= tensor(3.6549e+09, device='cuda:0')
c= tensor(3.6549e+09, device='cuda:0')
c= tensor(3.6551e+09, device='cuda:0')
c= tensor(3.6551e+09, device='cuda:0')
c= tensor(3.6552e+09, device='cuda:0')
c= tensor(3.6552e+09, device='cuda:0')
c= tensor(3.6552e+09, device='cuda:0')
c= tensor(3.6554e+09, device='cuda:0')
c= tensor(3.6554e+09, device='cuda:0')
c= tensor(3.6554e+09, device='cuda:0')
c= tensor(3.6607e+09, device='cuda:0')
c= tensor(3.6870e+09, device='cuda:0')
c= tensor(3.6894e+09, device='cuda:0')
c= tensor(3.6912e+09, device='cuda:0')
c= tensor(3.6913e+09, device='cuda:0')
c= tensor(3.6914e+09, device='cuda:0')
c= tensor(3.6918e+09, device='cuda:0')
c= tensor(3.6971e+09, device='cuda:0')
c= tensor(3.6973e+09, device='cuda:0')
c= tensor(3.6992e+09, device='cuda:0')
c= tensor(3.6993e+09, device='cuda:0')
c= tensor(3.8750e+09, device='cuda:0')
c= tensor(3.8754e+09, device='cuda:0')
c= tensor(3.8773e+09, device='cuda:0')
c= tensor(3.8899e+09, device='cuda:0')
c= tensor(3.8917e+09, device='cuda:0')
c= tensor(3.8920e+09, device='cuda:0')
c= tensor(3.9346e+09, device='cuda:0')
c= tensor(3.9450e+09, device='cuda:0')
c= tensor(3.9452e+09, device='cuda:0')
c= tensor(3.9453e+09, device='cuda:0')
c= tensor(3.9458e+09, device='cuda:0')
c= tensor(3.9459e+09, device='cuda:0')
c= tensor(3.9476e+09, device='cuda:0')
c= tensor(3.9853e+09, device='cuda:0')
c= tensor(3.9887e+09, device='cuda:0')
c= tensor(3.9931e+09, device='cuda:0')
c= tensor(3.9932e+09, device='cuda:0')
c= tensor(3.9932e+09, device='cuda:0')
c= tensor(3.9935e+09, device='cuda:0')
c= tensor(3.9936e+09, device='cuda:0')
c= tensor(4.0051e+09, device='cuda:0')
c= tensor(4.0393e+09, device='cuda:0')
c= tensor(4.0393e+09, device='cuda:0')
c= tensor(4.3897e+09, device='cuda:0')
c= tensor(4.3963e+09, device='cuda:0')
c= tensor(4.3984e+09, device='cuda:0')
c= tensor(4.3986e+09, device='cuda:0')
c= tensor(4.4001e+09, device='cuda:0')
c= tensor(4.4096e+09, device='cuda:0')
c= tensor(4.4096e+09, device='cuda:0')
c= tensor(4.5181e+09, device='cuda:0')
c= tensor(4.5186e+09, device='cuda:0')
c= tensor(4.5198e+09, device='cuda:0')
c= tensor(4.5201e+09, device='cuda:0')
c= tensor(4.5203e+09, device='cuda:0')
c= tensor(4.5203e+09, device='cuda:0')
c= tensor(4.5205e+09, device='cuda:0')
c= tensor(4.5205e+09, device='cuda:0')
c= tensor(4.5214e+09, device='cuda:0')
c= tensor(4.6360e+09, device='cuda:0')
c= tensor(4.6391e+09, device='cuda:0')
c= tensor(4.6688e+09, device='cuda:0')
c= tensor(4.6688e+09, device='cuda:0')
c= tensor(4.6692e+09, device='cuda:0')
c= tensor(4.6692e+09, device='cuda:0')
c= tensor(4.6713e+09, device='cuda:0')
c= tensor(4.6777e+09, device='cuda:0')
c= tensor(5.0544e+09, device='cuda:0')
c= tensor(5.0547e+09, device='cuda:0')
c= tensor(5.0620e+09, device='cuda:0')
c= tensor(5.0621e+09, device='cuda:0')
c= tensor(5.0680e+09, device='cuda:0')
c= tensor(5.0781e+09, device='cuda:0')
c= tensor(5.0788e+09, device='cuda:0')
c= tensor(5.0788e+09, device='cuda:0')
c= tensor(5.0825e+09, device='cuda:0')
c= tensor(5.0836e+09, device='cuda:0')
c= tensor(5.0875e+09, device='cuda:0')
c= tensor(5.1322e+09, device='cuda:0')
c= tensor(5.1329e+09, device='cuda:0')
c= tensor(5.1335e+09, device='cuda:0')
c= tensor(5.1339e+09, device='cuda:0')
c= tensor(5.1412e+09, device='cuda:0')
c= tensor(5.1544e+09, device='cuda:0')
c= tensor(5.1565e+09, device='cuda:0')
c= tensor(5.1566e+09, device='cuda:0')
c= tensor(5.2024e+09, device='cuda:0')
c= tensor(5.2052e+09, device='cuda:0')
c= tensor(5.2429e+09, device='cuda:0')
c= tensor(5.2532e+09, device='cuda:0')
c= tensor(5.2579e+09, device='cuda:0')
c= tensor(5.2583e+09, device='cuda:0')
c= tensor(5.2786e+09, device='cuda:0')
c= tensor(5.3105e+09, device='cuda:0')
c= tensor(5.3122e+09, device='cuda:0')
c= tensor(5.3122e+09, device='cuda:0')
c= tensor(5.3126e+09, device='cuda:0')
c= tensor(5.3134e+09, device='cuda:0')
c= tensor(5.3178e+09, device='cuda:0')
c= tensor(5.3186e+09, device='cuda:0')
c= tensor(5.3186e+09, device='cuda:0')
c= tensor(5.3189e+09, device='cuda:0')
c= tensor(5.3251e+09, device='cuda:0')
c= tensor(5.3266e+09, device='cuda:0')
c= tensor(5.3270e+09, device='cuda:0')
c= tensor(5.3279e+09, device='cuda:0')
c= tensor(5.3292e+09, device='cuda:0')
c= tensor(5.3295e+09, device='cuda:0')
c= tensor(5.3295e+09, device='cuda:0')
c= tensor(5.3295e+09, device='cuda:0')
c= tensor(5.3434e+09, device='cuda:0')
c= tensor(5.3437e+09, device='cuda:0')
c= tensor(5.3454e+09, device='cuda:0')
c= tensor(5.3454e+09, device='cuda:0')
c= tensor(5.3454e+09, device='cuda:0')
c= tensor(5.3582e+09, device='cuda:0')
c= tensor(5.3638e+09, device='cuda:0')
c= tensor(5.3645e+09, device='cuda:0')
c= tensor(5.3645e+09, device='cuda:0')
c= tensor(5.3645e+09, device='cuda:0')
c= tensor(5.3650e+09, device='cuda:0')
c= tensor(5.3652e+09, device='cuda:0')
c= tensor(5.3701e+09, device='cuda:0')
c= tensor(5.3702e+09, device='cuda:0')
c= tensor(5.3706e+09, device='cuda:0')
c= tensor(5.3713e+09, device='cuda:0')
c= tensor(5.3715e+09, device='cuda:0')
c= tensor(5.3790e+09, device='cuda:0')
c= tensor(5.3792e+09, device='cuda:0')
c= tensor(5.3832e+09, device='cuda:0')
c= tensor(5.4034e+09, device='cuda:0')
c= tensor(5.4038e+09, device='cuda:0')
c= tensor(5.4048e+09, device='cuda:0')
c= tensor(5.4074e+09, device='cuda:0')
c= tensor(5.4074e+09, device='cuda:0')
c= tensor(5.4082e+09, device='cuda:0')
c= tensor(5.4083e+09, device='cuda:0')
c= tensor(5.4318e+09, device='cuda:0')
c= tensor(5.4342e+09, device='cuda:0')
c= tensor(5.4357e+09, device='cuda:0')
c= tensor(5.4399e+09, device='cuda:0')
c= tensor(5.4402e+09, device='cuda:0')
c= tensor(5.4407e+09, device='cuda:0')
c= tensor(5.4408e+09, device='cuda:0')
c= tensor(5.4411e+09, device='cuda:0')
c= tensor(5.4612e+09, device='cuda:0')
c= tensor(5.5306e+09, device='cuda:0')
c= tensor(5.5327e+09, device='cuda:0')
c= tensor(5.5328e+09, device='cuda:0')
c= tensor(5.5330e+09, device='cuda:0')
c= tensor(5.6299e+09, device='cuda:0')
c= tensor(5.6302e+09, device='cuda:0')
c= tensor(5.6303e+09, device='cuda:0')
c= tensor(5.6314e+09, device='cuda:0')
c= tensor(5.6326e+09, device='cuda:0')
c= tensor(5.6326e+09, device='cuda:0')
c= tensor(5.6327e+09, device='cuda:0')
c= tensor(5.6372e+09, device='cuda:0')
time to make c is 9.771310091018677
time for making loss is 9.771386623382568
p0 True
it  0 : 2357489152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5390987264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 12% |
memory (bytes)
5391126528
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1401673700.0
relative error loss 0.2486461
shape of L is 
torch.Size([])
memory (bytes)
5417701376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5417701376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1390711600.0
relative error loss 0.2467015
shape of L is 
torch.Size([])
memory (bytes)
5421314048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 12% |
memory (bytes)
5421314048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1335247400.0
relative error loss 0.23686257
shape of L is 
torch.Size([])
memory (bytes)
5424488448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 12% |
memory (bytes)
5424517120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1311609900.0
relative error loss 0.23266946
shape of L is 
torch.Size([])
memory (bytes)
5427687424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 12% |
memory (bytes)
5427728384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1296110100.0
relative error loss 0.22991993
shape of L is 
torch.Size([])
memory (bytes)
5430751232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5430751232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 12% |
error is  1284092400.0
relative error loss 0.22778808
shape of L is 
torch.Size([])
memory (bytes)
5434134528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 12% |
memory (bytes)
5434134528
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1273401900.0
relative error loss 0.22589165
shape of L is 
torch.Size([])
memory (bytes)
5437341696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5437341696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% | 12% |
error is  1266157000.0
relative error loss 0.22460648
shape of L is 
torch.Size([])
memory (bytes)
5440552960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 12% |
memory (bytes)
5440552960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% | 12% |
error is  1257279500.0
relative error loss 0.22303167
shape of L is 
torch.Size([])
memory (bytes)
5443751936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
memory (bytes)
5443756032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% | 12% |
error is  1252290600.0
relative error loss 0.22214667
time to take a step is 266.6330544948578
it  1 : 2880371712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5446946816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5446971392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1252290600.0
relative error loss 0.22214667
shape of L is 
torch.Size([])
memory (bytes)
5450149888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 12% |
memory (bytes)
5450174464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1247293400.0
relative error loss 0.22126022
shape of L is 
torch.Size([])
memory (bytes)
5453361152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
memory (bytes)
5453406208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% | 12% |
error is  1242916400.0
relative error loss 0.22048377
shape of L is 
torch.Size([])
memory (bytes)
5456433152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 12% |
memory (bytes)
5456625664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1239820800.0
relative error loss 0.21993464
shape of L is 
torch.Size([])
memory (bytes)
5459832832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 12% |
memory (bytes)
5459832832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1236024800.0
relative error loss 0.21926126
shape of L is 
torch.Size([])
memory (bytes)
5463003136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 12% |
memory (bytes)
5463003136
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1233729500.0
relative error loss 0.2188541
shape of L is 
torch.Size([])
memory (bytes)
5466238976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 12% |
memory (bytes)
5466263552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1231724500.0
relative error loss 0.21849842
shape of L is 
torch.Size([])
memory (bytes)
5469462528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 12% |
memory (bytes)
5469462528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1229109800.0
relative error loss 0.21803458
shape of L is 
torch.Size([])
memory (bytes)
5472649216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 12% |
memory (bytes)
5472673792
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1227573800.0
relative error loss 0.21776211
shape of L is 
torch.Size([])
memory (bytes)
5475860480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 12% |
memory (bytes)
5475885056
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1226096600.0
relative error loss 0.21750008
time to take a step is 264.4252574443817
it  2 : 2880371712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5479096320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 12% |
memory (bytes)
5479096320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1226096600.0
relative error loss 0.21750008
shape of L is 
torch.Size([])
memory (bytes)
5482283008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 12% |
memory (bytes)
5482311680
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1224976900.0
relative error loss 0.21730144
shape of L is 
torch.Size([])
memory (bytes)
5485395968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 12% |
memory (bytes)
5485395968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1224075300.0
relative error loss 0.2171415
shape of L is 
torch.Size([])
memory (bytes)
5488726016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5488730112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 12% |
error is  1222372400.0
relative error loss 0.21683942
shape of L is 
torch.Size([])
memory (bytes)
5491949568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 12% |
memory (bytes)
5491953664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1222023200.0
relative error loss 0.21677747
shape of L is 
torch.Size([])
memory (bytes)
5495123968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 12% |
memory (bytes)
5495148544
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1220640300.0
relative error loss 0.21653216
shape of L is 
torch.Size([])
memory (bytes)
5498347520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 12% |
memory (bytes)
5498376192
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1219847200.0
relative error loss 0.21639147
shape of L is 
torch.Size([])
memory (bytes)
5501562880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 12% |
memory (bytes)
5501587456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1218816000.0
relative error loss 0.21620855
shape of L is 
torch.Size([])
memory (bytes)
5504806912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
memory (bytes)
5504806912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1217654300.0
relative error loss 0.21600246
shape of L is 
torch.Size([])
memory (bytes)
5508014080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 12% |
memory (bytes)
5508014080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1216450000.0
relative error loss 0.21578884
time to take a step is 261.9369490146637
it  3 : 2880371712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  4% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5511225344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  4% | 12% |
memory (bytes)
5511229440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% | 12% |
error is  1216450000.0
relative error loss 0.21578884
shape of L is 
torch.Size([])
memory (bytes)
5514424320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 12% |
memory (bytes)
5514428416
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1215428600.0
relative error loss 0.21560764
shape of L is 
torch.Size([])
memory (bytes)
5517647872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 12% |
memory (bytes)
5517647872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1214698500.0
relative error loss 0.21547814
shape of L is 
torch.Size([])
memory (bytes)
5520863232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5520863232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 12% |
error is  1214168000.0
relative error loss 0.21538404
shape of L is 
torch.Size([])
memory (bytes)
5524045824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 12% |
memory (bytes)
5524070400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1213180900.0
relative error loss 0.21520893
shape of L is 
torch.Size([])
memory (bytes)
5527248896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 12% |
memory (bytes)
5527273472
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1212803100.0
relative error loss 0.2151419
shape of L is 
torch.Size([])
memory (bytes)
5530497024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 12% |
memory (bytes)
5530497024
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1212375600.0
relative error loss 0.21506606
shape of L is 
torch.Size([])
memory (bytes)
5533700096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  0% | 12% |
memory (bytes)
5533700096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1211675100.0
relative error loss 0.21494181
shape of L is 
torch.Size([])
memory (bytes)
5536882688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 12% |
memory (bytes)
5536911360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1210997200.0
relative error loss 0.21482156
shape of L is 
torch.Size([])
memory (bytes)
5540118528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
memory (bytes)
5540126720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 89% | 12% |
error is  1210547200.0
relative error loss 0.21474172
time to take a step is 268.8671929836273
it  4 : 2880371712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5543337984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5543337984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1210547200.0
relative error loss 0.21474172
shape of L is 
torch.Size([])
memory (bytes)
5546524672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 12% |
memory (bytes)
5546545152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 12% |
error is  1209879000.0
relative error loss 0.2146232
shape of L is 
torch.Size([])
memory (bytes)
5549670400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 12% |
memory (bytes)
5549670400
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1209592800.0
relative error loss 0.21457243
shape of L is 
torch.Size([])
memory (bytes)
5552955392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 12% |
memory (bytes)
5552955392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1209095700.0
relative error loss 0.21448424
shape of L is 
torch.Size([])
memory (bytes)
5556162560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 12% |
memory (bytes)
5556162560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1208747500.0
relative error loss 0.21442248
shape of L is 
torch.Size([])
memory (bytes)
5559398400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
memory (bytes)
5559398400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1208476700.0
relative error loss 0.21437444
shape of L is 
torch.Size([])
memory (bytes)
5562609664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
memory (bytes)
5562609664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1207896600.0
relative error loss 0.21427153
shape of L is 
torch.Size([])
memory (bytes)
5565829120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 12% |
memory (bytes)
5565829120
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1207562800.0
relative error loss 0.21421231
shape of L is 
torch.Size([])
memory (bytes)
5569036288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5569036288
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1207223800.0
relative error loss 0.21415219
shape of L is 
torch.Size([])
memory (bytes)
5572255744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 12% |
memory (bytes)
5572255744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% | 12% |
error is  1207025700.0
relative error loss 0.21411704
time to take a step is 260.175350189209
it  5 : 2880371712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5575430144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 12% |
memory (bytes)
5575458816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1207025700.0
relative error loss 0.21411704
shape of L is 
torch.Size([])
memory (bytes)
5578649600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 12% |
memory (bytes)
5578661888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1206596600.0
relative error loss 0.21404092
shape of L is 
torch.Size([])
memory (bytes)
5581848576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 12% |
memory (bytes)
5581873152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1206104600.0
relative error loss 0.21395364
shape of L is 
torch.Size([])
memory (bytes)
5585051648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5585076224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1206316000.0
relative error loss 0.21399115
shape of L is 
torch.Size([])
memory (bytes)
5588283392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 12% |
memory (bytes)
5588283392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1205851600.0
relative error loss 0.21390878
shape of L is 
torch.Size([])
memory (bytes)
5591359488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
memory (bytes)
5591494656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1205538800.0
relative error loss 0.21385328
shape of L is 
torch.Size([])
memory (bytes)
5594697728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 12% |
memory (bytes)
5594697728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1205274600.0
relative error loss 0.2138064
shape of L is 
torch.Size([])
memory (bytes)
5597880320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 12% |
memory (bytes)
5597904896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1204973600.0
relative error loss 0.213753
shape of L is 
torch.Size([])
memory (bytes)
5601112064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 12% |
memory (bytes)
5601116160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1204727300.0
relative error loss 0.21370932
shape of L is 
torch.Size([])
memory (bytes)
5604233216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 12% |
memory (bytes)
5604233216
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1204544000.0
relative error loss 0.21367681
time to take a step is 258.89411783218384
it  6 : 2880371712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5607542784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 12% |
memory (bytes)
5607542784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1204544000.0
relative error loss 0.21367681
shape of L is 
torch.Size([])
memory (bytes)
5610749952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 12% |
memory (bytes)
5610749952
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1204373500.0
relative error loss 0.21364656
shape of L is 
torch.Size([])
memory (bytes)
5613932544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 12% |
memory (bytes)
5613957120
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1204282400.0
relative error loss 0.2136304
shape of L is 
torch.Size([])
memory (bytes)
5617168384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 12% |
memory (bytes)
5617168384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1204019700.0
relative error loss 0.2135838
shape of L is 
torch.Size([])
memory (bytes)
5620375552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 12% |
memory (bytes)
5620375552
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1203950100.0
relative error loss 0.21357144
shape of L is 
torch.Size([])
memory (bytes)
5623558144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 12% |
memory (bytes)
5623582720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1203839500.0
relative error loss 0.21355183
shape of L is 
torch.Size([])
memory (bytes)
5626798080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 12% |
memory (bytes)
5626798080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1203694600.0
relative error loss 0.21352613
shape of L is 
torch.Size([])
memory (bytes)
5630001152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 12% |
memory (bytes)
5630013440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1203593700.0
relative error loss 0.21350823
shape of L is 
torch.Size([])
memory (bytes)
5633220608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 12% |
memory (bytes)
5633220608
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1203422700.0
relative error loss 0.2134779
shape of L is 
torch.Size([])
memory (bytes)
5636427776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 12% |
memory (bytes)
5636427776
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1203279400.0
relative error loss 0.21345247
time to take a step is 262.49412083625793
it  7 : 2880371712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5639634944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 12% |
memory (bytes)
5639647232
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1203279400.0
relative error loss 0.21345247
shape of L is 
torch.Size([])
memory (bytes)
5642854400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
memory (bytes)
5642854400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1203116000.0
relative error loss 0.21342349
shape of L is 
torch.Size([])
memory (bytes)
5646073856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 12% |
memory (bytes)
5646073856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1202903600.0
relative error loss 0.2133858
shape of L is 
torch.Size([])
memory (bytes)
5649260544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 12% |
memory (bytes)
5649260544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1202673700.0
relative error loss 0.21334502
shape of L is 
torch.Size([])
memory (bytes)
5652492288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 12% |
memory (bytes)
5652492288
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1202369500.0
relative error loss 0.21329108
shape of L is 
torch.Size([])
memory (bytes)
5655695360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 12% |
memory (bytes)
5655695360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1202187300.0
relative error loss 0.21325874
shape of L is 
torch.Size([])
memory (bytes)
5658918912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 12% |
memory (bytes)
5658923008
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1201983500.0
relative error loss 0.2132226
shape of L is 
torch.Size([])
memory (bytes)
5662113792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 12% |
memory (bytes)
5662113792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1201833500.0
relative error loss 0.21319598
shape of L is 
torch.Size([])
memory (bytes)
5665292288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 12% |
memory (bytes)
5665316864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1202071000.0
relative error loss 0.21323812
shape of L is 
torch.Size([])
memory (bytes)
5668483072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 12% |
memory (bytes)
5668528128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1201741800.0
relative error loss 0.21317972
time to take a step is 258.92187690734863
it  8 : 2880372224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5671718912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 12% |
memory (bytes)
5671718912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1201741800.0
relative error loss 0.21317972
shape of L is 
torch.Size([])
memory (bytes)
5674954752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 12% |
memory (bytes)
5674954752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1201627600.0
relative error loss 0.21315947
shape of L is 
torch.Size([])
memory (bytes)
5678137344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5678166016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% | 12% |
error is  1201498100.0
relative error loss 0.2131365
shape of L is 
torch.Size([])
memory (bytes)
5681356800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 12% |
memory (bytes)
5681381376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1201338400.0
relative error loss 0.21310815
shape of L is 
torch.Size([])
memory (bytes)
5684592640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 12% |
memory (bytes)
5684592640
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1201209300.0
relative error loss 0.21308526
shape of L is 
torch.Size([])
memory (bytes)
5687799808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 12% |
memory (bytes)
5687799808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1201122300.0
relative error loss 0.21306983
shape of L is 
torch.Size([])
memory (bytes)
5691015168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 12% |
memory (bytes)
5691015168
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1201001000.0
relative error loss 0.2130483
shape of L is 
torch.Size([])
memory (bytes)
5694218240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 12% |
memory (bytes)
5694218240
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1200917000.0
relative error loss 0.21303341
shape of L is 
torch.Size([])
memory (bytes)
5697425408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5697425408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 12% |
error is  1200800300.0
relative error loss 0.2130127
shape of L is 
torch.Size([])
memory (bytes)
5700636672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 12% |
memory (bytes)
5700636672
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1200735200.0
relative error loss 0.21300116
time to take a step is 266.1322455406189
it  9 : 2880371712
| ID | GPU | MEM |
------------------
|  0 |  2% |  0% |
|  1 | 12% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5703847936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5703847936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1200735200.0
relative error loss 0.21300116
shape of L is 
torch.Size([])
memory (bytes)
5707030528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 12% |
memory (bytes)
5707055104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1200683500.0
relative error loss 0.21299198
shape of L is 
torch.Size([])
memory (bytes)
5710143488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 12% |
memory (bytes)
5710286848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1200538600.0
relative error loss 0.21296628
shape of L is 
torch.Size([])
memory (bytes)
5713494016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 12% |
memory (bytes)
5713494016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1200428500.0
relative error loss 0.21294676
shape of L is 
torch.Size([])
memory (bytes)
5716709376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 12% |
memory (bytes)
5716709376
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1200343000.0
relative error loss 0.21293159
shape of L is 
torch.Size([])
memory (bytes)
5719896064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 12% |
memory (bytes)
5719924736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1200195100.0
relative error loss 0.21290533
shape of L is 
torch.Size([])
memory (bytes)
5723107328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 12% |
memory (bytes)
5723136000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1200147000.0
relative error loss 0.2128968
shape of L is 
torch.Size([])
memory (bytes)
5726339072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 12% |
memory (bytes)
5726339072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1200020000.0
relative error loss 0.21287428
shape of L is 
torch.Size([])
memory (bytes)
5729554432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 12% |
memory (bytes)
5729554432
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1199923700.0
relative error loss 0.2128572
shape of L is 
torch.Size([])
memory (bytes)
5732671488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 12% |
memory (bytes)
5732671488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 12% |
error is  1199845400.0
relative error loss 0.2128433
time to take a step is 258.4698441028595
it  10 : 2880371712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5735960576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 12% |
memory (bytes)
5735960576
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1199845400.0
relative error loss 0.2128433
shape of L is 
torch.Size([])
memory (bytes)
5739143168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 12% |
memory (bytes)
5739143168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1199677400.0
relative error loss 0.21281351
shape of L is 
torch.Size([])
memory (bytes)
5742350336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 12% |
memory (bytes)
5742350336
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1199541800.0
relative error loss 0.21278945
shape of L is 
torch.Size([])
memory (bytes)
5745561600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 12% |
memory (bytes)
5745561600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1199352300.0
relative error loss 0.21275584
shape of L is 
torch.Size([])
memory (bytes)
5748654080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 12% |
memory (bytes)
5748797440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1199222800.0
relative error loss 0.21273287
shape of L is 
torch.Size([])
memory (bytes)
5752012800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 12% |
memory (bytes)
5752012800
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1199039500.0
relative error loss 0.21270035
shape of L is 
torch.Size([])
memory (bytes)
5755211776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 12% |
memory (bytes)
5755211776
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1199034900.0
relative error loss 0.21269953
shape of L is 
torch.Size([])
memory (bytes)
5758423040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 12% |
memory (bytes)
5758423040
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1198934500.0
relative error loss 0.21268173
shape of L is 
torch.Size([])
memory (bytes)
5761617920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 12% |
memory (bytes)
5761617920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1198791200.0
relative error loss 0.2126563
shape of L is 
torch.Size([])
memory (bytes)
5764841472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 12% |
memory (bytes)
5764841472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% | 12% |
error is  1198665200.0
relative error loss 0.21263395
time to take a step is 262.2060515880585
it  11 : 2880371712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5767921664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5767921664
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1198665200.0
relative error loss 0.21263395
shape of L is 
torch.Size([])
memory (bytes)
5771264000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 12% |
memory (bytes)
5771264000
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1198575100.0
relative error loss 0.21261796
shape of L is 
torch.Size([])
memory (bytes)
5774475264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
memory (bytes)
5774475264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% | 12% |
error is  1198491100.0
relative error loss 0.21260308
shape of L is 
torch.Size([])
memory (bytes)
5777682432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 12% |
memory (bytes)
5777682432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1198345200.0
relative error loss 0.2125772
shape of L is 
torch.Size([])
memory (bytes)
5780856832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5780881408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 12% |
error is  1198231600.0
relative error loss 0.21255703
shape of L is 
torch.Size([])
memory (bytes)
5784088576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
memory (bytes)
5784088576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1198126100.0
relative error loss 0.21253832
shape of L is 
torch.Size([])
memory (bytes)
5787279360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 12% |
memory (bytes)
5787303936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1198046200.0
relative error loss 0.21252415
shape of L is 
torch.Size([])
memory (bytes)
5790515200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 12% |
memory (bytes)
5790515200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1197979100.0
relative error loss 0.21251225
shape of L is 
torch.Size([])
memory (bytes)
5793722368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 12% |
memory (bytes)
5793722368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1197890000.0
relative error loss 0.21249644
shape of L is 
torch.Size([])
memory (bytes)
5796900864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 12% |
memory (bytes)
5796925440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% | 12% |
error is  1197845500.0
relative error loss 0.21248855
time to take a step is 266.462956905365
it  12 : 2880371712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5800140800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 12% |
memory (bytes)
5800140800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 12% |
error is  1197845500.0
relative error loss 0.21248855
shape of L is 
torch.Size([])
memory (bytes)
5803278336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
memory (bytes)
5803278336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% | 12% |
error is  1197706800.0
relative error loss 0.21246393
shape of L is 
torch.Size([])
memory (bytes)
5806563328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 12% |
memory (bytes)
5806563328
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1197605400.0
relative error loss 0.21244594
shape of L is 
torch.Size([])
memory (bytes)
5809774592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
memory (bytes)
5809774592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% | 12% |
error is  1197579800.0
relative error loss 0.2124414
shape of L is 
torch.Size([])
memory (bytes)
5812977664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5812977664
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1197431300.0
relative error loss 0.21241507
shape of L is 
torch.Size([])
memory (bytes)
5816193024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
memory (bytes)
5816193024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1197379100.0
relative error loss 0.2124058
shape of L is 
torch.Size([])
memory (bytes)
5819375616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 12% |
memory (bytes)
5819400192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1197270500.0
relative error loss 0.21238655
shape of L is 
torch.Size([])
memory (bytes)
5822586880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 12% |
memory (bytes)
5822611456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1197234700.0
relative error loss 0.21238019
shape of L is 
torch.Size([])
memory (bytes)
5825826816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 12% |
memory (bytes)
5825826816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1197133300.0
relative error loss 0.2123622
shape of L is 
torch.Size([])
memory (bytes)
5829005312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5829042176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 12% |
error is  1197067800.0
relative error loss 0.21235058
time to take a step is 269.659024477005
it  13 : 2880371712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5832253440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 12% |
memory (bytes)
5832257536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1197067800.0
relative error loss 0.21235058
shape of L is 
torch.Size([])
memory (bytes)
5835464704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 12% |
memory (bytes)
5835464704
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1197009400.0
relative error loss 0.21234022
shape of L is 
torch.Size([])
memory (bytes)
5838680064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 12% |
memory (bytes)
5838680064
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1196925400.0
relative error loss 0.21232533
shape of L is 
torch.Size([])
memory (bytes)
5841891328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 12% |
memory (bytes)
5841891328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1197018100.0
relative error loss 0.21234177
shape of L is 
torch.Size([])
memory (bytes)
5845098496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 12% |
memory (bytes)
5845098496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% | 12% |
error is  1196870100.0
relative error loss 0.21231553
shape of L is 
torch.Size([])
memory (bytes)
5848322048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 12% |
memory (bytes)
5848322048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1196783100.0
relative error loss 0.21230008
shape of L is 
torch.Size([])
memory (bytes)
5851512832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 12% |
memory (bytes)
5851512832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1196709900.0
relative error loss 0.2122871
shape of L is 
torch.Size([])
memory (bytes)
5854736384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 12% |
memory (bytes)
5854736384
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1196640800.0
relative error loss 0.21227483
shape of L is 
torch.Size([])
memory (bytes)
5857947648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 12% |
memory (bytes)
5857947648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1196561900.0
relative error loss 0.21226084
shape of L is 
torch.Size([])
memory (bytes)
5861167104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5861167104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1196518400.0
relative error loss 0.21225312
time to take a step is 268.89652609825134
it  14 : 2880371712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
5864370176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 12% |
memory (bytes)
5864370176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1196518400.0
relative error loss 0.21225312
shape of L is 
torch.Size([])
memory (bytes)
5867581440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 12% |
memory (bytes)
5867581440
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1196431400.0
relative error loss 0.21223769
shape of L is 
torch.Size([])
memory (bytes)
5870800896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 12% |
memory (bytes)
5870800896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1196382200.0
relative error loss 0.21222897
shape of L is 
torch.Size([])
memory (bytes)
5874008064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 12% |
memory (bytes)
5874008064
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1196317200.0
relative error loss 0.21221744
shape of L is 
torch.Size([])
memory (bytes)
5877215232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
memory (bytes)
5877215232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 12% |
error is  1196248600.0
relative error loss 0.21220526
shape of L is 
torch.Size([])
memory (bytes)
5880307712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 12% |
memory (bytes)
5880307712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1196203000.0
relative error loss 0.21219718
shape of L is 
torch.Size([])
memory (bytes)
5883625472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5883625472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 12% |
error is  1196155900.0
relative error loss 0.21218883
shape of L is 
torch.Size([])
memory (bytes)
5886844928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 12% |
memory (bytes)
5886844928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% | 12% |
error is  1196055600.0
relative error loss 0.21217102
shape of L is 
torch.Size([])
memory (bytes)
5890048000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 12% |
memory (bytes)
5890052096
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 12% |
error is  1196025300.0
relative error loss 0.21216567
shape of L is 
torch.Size([])
memory (bytes)
5893271552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 12% |
memory (bytes)
5893271552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% | 12% |
error is  1195969500.0
relative error loss 0.21215576
time to take a step is 271.6933104991913
sum tnnu_Z after tensor(15749482., device='cuda:0')
shape of features
(5151,)
shape of features
(5151,)
number of orig particles 20602
number of new particles after remove low mass 18907
tnuZ shape should be parts x labs
torch.Size([20602, 702])
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
error without small mass is  1401574500.0
relative error without small mass is  0.2486285
nnu_Z shape should be number of particles by maxV
(20602, 702)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
shape of features
(20602,)
Tue Jan 31 19:20:55 EST 2023
