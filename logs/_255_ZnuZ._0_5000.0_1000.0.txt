Fri Feb 3 04:38:25 EST 2023
| ID | GPU | MEM |
------------------
|  0 | 18% |  0% |
|  1 |  0% |  0% |
numbers of X: 26879655
numbers of Z: 14355
shape of features
(14355,)
shape of features
(14355,)
ZX	Vol	Parts	Cubes	Eps
Z	0.011952968561915479	14355	14.355	0.09407860306947827
X	0.010278411677493339	1074	1.074	0.21231169081664492
X	0.01029013028624791	17065	17.065	0.0844834279001886
X	0.010829544885384457	2117	2.117	0.17230443229241965
X	0.010292405044016455	1789	1.789	0.17918521665099177
X	0.01135422645563711	41343	41.343	0.0650007730122283
X	0.010986557723887905	35135	35.135	0.06787459137050667
X	0.01034285177701528	29978	29.978	0.07013678943974419
X	0.010964333210688031	37059	37.059	0.06663404479342085
X	0.01098220563920253	8096	8.096	0.11069797863906802
X	0.010938983805367938	22110	22.11	0.07909142717053147
X	0.010347981646609257	4012	4.012	0.13714024104666167
X	0.010206364302316806	59179	59.179	0.055663154478201096
X	0.010349833075498954	7800	7.8	0.1098869824438057
X	0.01083391663447012	153822	153.822	0.0412973652914638
X	0.010360975932829664	16016	16.016	0.08648660876018216
X	0.010935449555056626	33263	33.263	0.06901737042670286
X	0.010641439984462254	29456	29.456	0.07122113268159558
X	0.011054659211351191	21903	21.903	0.07961848666873426
X	0.01042994234628654	191112	191.112	0.03793132237101365
X	0.011064768526177287	72039	72.039	0.05355395538607699
X	0.010316689963320936	10640	10.64	0.0989766879898413
X	0.011710246900730317	285337	285.337	0.034493398596501856
X	0.010947302182311668	9594	9.594	0.10449667313982809
X	0.010868613867319998	8400	8.4	0.10896782719780922
X	0.010721048032128798	7508	7.508	0.11260845309337522
X	0.01066535763501618	52498	52.498	0.05878649331971951
X	0.010942869868722192	33371	33.371	0.06895842566521755
X	0.010321358382457816	7146	7.146	0.11303803918118803
X	0.010345139550563796	43852	43.852	0.06178963778196125
X	0.011904514551103926	1596976	1596.976	0.019534595986091415
X	0.010376643265874156	4271	4.271	0.134433995065944
X	0.011105568021150706	252704	252.704	0.035289287239327755
X	0.011002860933899135	14941	14.941	0.09030434620471804
X	0.010348432250761281	6306	6.306	0.1179524224883492
X	0.010357741468897197	12379	12.379	0.09423086286462783
X	0.01101988509644157	58182	58.182	0.05742876453386325
X	0.011001601832278985	66116	66.116	0.05500258708712936
X	0.010228312516979882	852	0.852	0.2289750336024725
X	0.010270862492597541	3247	3.247	0.1467939675476967
X	0.010128498368874306	1643	1.643	0.18335911312396436
X	0.010283110472854018	3034	3.034	0.1502114280757287
X	0.009807155062007699	558	0.558	0.259997764803797
X	0.010043436068890963	569	0.569	0.2603694451953578
X	0.009991508162513709	1444	1.444	0.1905554106817367
X	0.010349933266012546	815	0.815	0.2333063970637188
X	0.010255718564193779	824	0.824	0.23174638105209103
X	0.010974770805153574	5206	5.206	0.12822235974414659
X	0.010222795922604895	2816	2.816	0.15369002463201767
X	0.010263042998373132	2911	2.911	0.15219883015584373
X	0.010331741376501004	6073	6.073	0.1193777571270285
X	0.010339336359382972	9841	9.841	0.10166024314165945
X	0.010082926054616624	1145	1.145	0.20650331693083154
X	0.010215270481302144	3243	3.243	0.14658886219665943
X	0.010126732748889098	1537	1.537	0.18747002059819404
X	0.010852815224337922	5802	5.802	0.12321253553867342
X	0.01011556660430794	4270	4.27	0.1333073554509719
X	0.010065134788290013	1336	1.336	0.1960368485849822
X	0.010286494649706103	2572	2.572	0.15873236248298225
X	0.010287792058723115	2042	2.042	0.1714307847759451
X	0.010196084938700058	1992	1.992	0.17233812761859965
X	0.010364026641863266	8626	8.626	0.10630973175007127
X	0.010272593607935587	1432	1.432	0.19286156939766252
X	0.01091019604783288	11284	11.284	0.09888334724135128
X	0.010325103615279236	4798	4.798	0.1291054943617652
X	0.010323027324196937	2541	2.541	0.15956371432977015
X	0.01001210668116564	1746	1.746	0.1789893027944126
X	0.010270723705359306	1693	1.693	0.18238174335790017
X	0.010220986890424683	3053	3.053	0.1495966984803566
X	0.010298135362238591	3146	3.146	0.14847954427941573
X	0.010160718803115372	1852	1.852	0.17637125966722134
X	0.010423134755411905	4992	4.992	0.12781287148835138
X	0.010216651152648821	2971	2.971	0.15093917466865897
X	0.010307829546851669	1976	1.976	0.17343101375904538
X	0.010171487849229135	1318	1.318	0.19761641881569145
X	0.010497530775043826	3095	3.095	0.15024804660284444
X	0.010304980995069019	3675	3.675	0.1410143415696296
X	0.010369018286210155	1119	1.119	0.21004024584820127
X	0.010183584618454884	424	0.424	0.28852156104839255
X	0.010274918338067264	3013	3.013	0.15051960931871036
X	0.010281071361546856	7731	7.731	0.10996831403731923
X	0.01030061878225462	1882	1.882	0.17623056428645004
X	0.010157189092578293	1041	1.041	0.2136851142556557
X	0.011368618448954744	5596	5.596	0.1266514000707359
X	0.010205421371009564	1103	1.103	0.20993514182922074
X	0.010163173881055703	1987	1.987	0.17229678072101604
X	0.009969422018074417	596	0.596	0.2557454447554333
X	0.010327884058773123	2534	2.534	0.15973554987417665
X	0.010089734477851445	678	0.678	0.24597068345484954
X	0.010364258497323782	4222	4.222	0.13489835976693815
X	0.010188922474610696	2376	2.376	0.16246529638115081
X	0.010270728626078989	3902	3.902	0.13807169714113848
X	0.010363434795944527	825	0.825	0.23246090464758282
X	0.010161598623384003	982	0.982	0.21791318896618894
X	0.010276858953024754	3890	3.89	0.13824102063832036
X	0.010244057876504356	3746	3.746	0.13984107380459895
X	0.010171522900126944	2657	2.657	0.1564341171704979
X	0.010200799904260312	2468	2.468	0.1604832427174221
X	0.01014554636319761	4076	4.076	0.13552323341641534
X	0.010388307617260522	7324	7.324	0.11235666035676961
X	0.010257718307574498	2369	2.369	0.1629903538477599
X	0.010314645945478877	9564	9.564	0.10255061108047145
X	0.010943516504851624	5263	5.263	0.1276363890410653
X	0.01024946525865861	3470	3.47	0.14347973888336818
X	0.01006467708608478	967	0.967	0.21833555134313834
X	0.010173507194223825	2269	2.269	0.1648967389114032
X	0.010264210601675143	1798	1.798	0.1787222495946494
X	0.010322724829503215	3944	3.944	0.13781161264514732
X	0.010218192985284915	339	0.339	0.3112132895797413
X	0.010307108825913841	1482	1.482	0.19088108105114823
X	0.010022685997101387	1030	1.03	0.21349234125427452
X	0.010148084183443016	984	0.984	0.2176688691264897
X	0.010099418735047301	1707	1.707	0.18086489723788055
X	0.010185545410619437	1465	1.465	0.19086027923810758
X	0.010888407104573603	2925	2.925	0.15498136915775348
X	0.01023665821005382	2890	2.89	0.15243573415318046
X	0.009626722290613798	1316	1.316	0.1941217441696152
X	0.010374340713440578	1268	1.268	0.20150249029976564
X	0.010231961187083445	914	0.914	0.22370253013482866
X	0.010333063464573838	5537	5.537	0.12311704723515174
X	0.010005286193578076	810	0.81	0.23116114251983474
X	0.010653277143442011	13366	13.366	0.09271726015217015
X	0.010322110008052464	1443	1.443	0.19267885228481807
X	0.010204279358985564	1434	1.434	0.1923435952040421
X	0.010302556380653784	2438	2.438	0.16167285198107098
X	0.010004135954502386	1511	1.511	0.18777526068788217
X	0.010431563075294718	3987	3.987	0.13779529352200928
X	0.01029980926407989	797	0.797	0.2346696651548035
X	0.01023813971425057	2438	2.438	0.16133519444974914
X	0.010802122413644519	13894	13.894	0.09195187724422382
X	0.010853531169129954	1927	1.927	0.17792205295715008
X	0.01093856147745148	6175	6.175	0.1209969482168896
X	0.010242817794024816	866	0.866	0.22784203543052806
X	0.010332217008009023	3809	3.809	0.13946359270264366
X	0.009915121004893102	712	0.712	0.24058728711489924
X	0.010284190859180222	1983	1.983	0.17309418220436418
X	0.010261706145841171	1930	1.93	0.17453696575188626
X	0.010138976853242323	1175	1.175	0.20510933268333678
X	0.010264292356048511	1069	1.069	0.21254477643903552
X	0.010254904289050114	1013	1.013	0.21632533686149427
X	0.010302817134205219	1692	1.692	0.18260747204212888
X	0.01075992615735585	1704	1.704	0.18483316496262928
X	0.010116554883278426	648	0.648	0.24993047353256287
X	0.010812268588202563	5735	5.735	0.12353626529241864
X	0.010324436738371365	17843	17.843	0.08332963841822842
X	0.010486987598811883	7266	7.266	0.11301041080607356
X	0.010079975642929511	1491	1.491	0.18908653917201998
X	0.010222136611575955	1417	1.417	0.1932223130962497
X	0.01026526688624894	1072	1.072	0.21235304192175705
X	0.010230170641359811	1042	1.042	0.21412714161826255
X	0.010247748059097806	1557	1.557	0.18740446864341467
X	0.010238506653767823	1006	1.006	0.21671029602588057
X	0.010598965942006986	4940	4.94	0.12897697372343508
X	0.010320039640164222	2775	2.775	0.15493140500913913
X	0.01030307325021784	5903	5.903	0.1204013439773209
X	0.010339835180599805	2680	2.68	0.1568410016954058
X	0.01094936841748152	10374	10.374	0.10181558831788359
X	0.010252364295432888	1865	1.865	0.1764879523860723
X	0.010226044094345861	1377	1.377	0.19510028640459254
X	0.010332466997606388	3551	3.551	0.1427636966514345
X	0.01007499609704747	433	0.433	0.28548655727286487
X	0.010949178871771725	5014	5.014	0.12973756434658804
X	0.010282843527647651	1228	1.228	0.20306650155367437
X	0.010313006481056677	2797	2.797	0.1544890163932095
X	0.010055946890163838	701	0.701	0.24297876351828504
X	0.01018448052579317	2262	2.262	0.16512598724522498
X	0.010246412466352125	2302	2.302	0.16449606181211138
X	0.010326613883977409	1699	1.699	0.18249663176330364
X	0.01024904790475193	2235	2.235	0.1661378803215826
X	0.010277580538890305	3538	3.538	0.14268479208063123
X	0.010210491567240836	281	0.281	0.3312176287836802
X	0.010191529795927847	766	0.766	0.23695736018476704
X	0.00989121895917056	1368	1.368	0.19336946967072477
X	0.010291095811418878	5905	5.905	0.12034108049613675
X	0.010266514185426187	1423	1.423	0.19322917686179666
X	0.010316897739342	3412	3.412	0.14460390942808687
X	0.010375425712210975	3600	3.6	0.14230969072241131
X	0.010277903346054777	2786	2.786	0.15451636129122595
X	0.01032957998772939	3139	3.139	0.14874091510678492
X	0.010307694797655198	3625	3.625	0.1416721593287206
X	0.010094988137857442	1446	1.446	0.19112280323416178
X	0.01037090495597686	4479	4.479	0.13229553814921105
X	0.010297347497138455	1944	1.944	0.17431832174447037
X	0.010204040827718516	3618	3.618	0.14128667619552016
X	0.010194280809033618	1197	1.197	0.20421492567568422
X	0.010315827906571593	4768	4.768	0.12933694958828965
X	0.01037312383493499	3509	3.509	0.1435187791766561
X	0.010159656892600523	1199	1.199	0.20386998271580173
X	0.010290969648348924	3023	3.023	0.15043170691623067
X	0.010342680375375462	2725	2.725	0.15598716523864498
X	0.01031073268502815	12121	12.121	0.09475097597946319
X	0.010267792963728768	1593	1.593	0.18610317214157054
X	0.010270982988002103	513	0.513	0.2715385835721124
X	0.010287636358074415	2190	2.19	0.1674777675808523
X	0.01028102642782317	1274	1.274	0.20058064145193827
X	0.010207128477349944	1903	1.903	0.17504710434514284
X	0.010120777562024653	1511	1.511	0.18850222111545709
X	0.010243865258254834	1200	1.2	0.20437489040263931
X	0.010241889186458297	1026	1.026	0.21531662837340318
X	0.01034155988003135	3095	3.095	0.14950020879391562
X	0.010303175831749457	708	0.708	0.24414394129922773
X	0.010340140134126676	7156	7.156	0.113053851971462
X	0.01022084224286687	1478	1.478	0.1905186155761858
X	0.010377626800879326	19439	19.439	0.08112250937118073
X	0.0103122324781555	3400	3.4	0.1447520065767648
X	0.01023866396721974	2569	2.569	0.1585476205011162
X	0.01006478175111588	688	0.688	0.24457119630921223
X	0.011149451753242955	5426	5.426	0.1271328865374694
X	0.010241128110032366	3247	3.247	0.1466521733256935
X	0.01036170228564538	2410	2.41	0.16260654509284425
X	0.011668631104630019	8895	8.895	0.10946904245334363
X	0.010868998468518404	6012	6.012	0.12182138102131115
X	0.01007714818351506	1064	1.064	0.2115754820169573
X	0.010170432190392613	1293	1.293	0.19887504666637723
X	0.010322381639165358	3182	3.182	0.14803346752934973
X	0.010224361354169896	625	0.625	0.25385480659017123
X	0.010005865533135252	642	0.642	0.2497889436415894
X	0.010259398196325138	691	0.691	0.24578079594366176
X	0.010159188461544474	525	0.525	0.2684725603116236
X	0.010365149473144082	3225	3.225	0.1475758466436364
X	0.010182706348630829	1124	1.124	0.20846459332222123
X	0.010136274002028559	3488	3.488	0.1427032799345329
X	0.010291336973331748	1173	1.173	0.20624872011516554
X	0.010185876346928422	3523	3.523	0.142460758451036
X	0.010284311207984306	1354	1.354	0.19657072913755014
X	0.010199603185833767	1819	1.819	0.17765749149329946
X	0.010269173715313277	3472	3.472	0.14354407179528247
X	0.0102623664265731	967	0.967	0.21975580080339663
X	0.010241913956398124	783	0.783	0.235616908642847
X	0.010856468627987339	4058	4.058	0.13882217560757107
X	0.009718535962522392	955	0.955	0.21670344966225485
X	0.010176141973514258	735	0.735	0.24012199307493393
X	0.010140859040442148	1279	1.279	0.19940439850353148
X	0.010292753108721454	2313	2.313	0.16448209953715912
X	0.01039065445980374	3980	3.98	0.13769556218344708
X	0.010288744144878794	903	0.903	0.2250219440555815
X	0.010090109108035721	817	0.817	0.23114860066628448
X	0.010340434918678117	3061	3.061	0.15004625441388625
X	0.0107940828320334	5706	5.706	0.12367578046981972
X	0.010236410193846123	899	0.899	0.2249724457125374
X	0.01043065818162684	5672	5.672	0.12251572390300684
X	0.011007634608140884	57756	57.756	0.057548273519961134
X	0.010364663841666538	15742	15.742	0.08699583035171446
X	0.010379461384848444	4567	4.567	0.13147643974767265
X	0.01020884316357552	501	0.501	0.27313641320504073
X	0.010193365028388527	3442	3.442	0.14360478251255315
X	0.010290551502286383	7036	7.036	0.11351091032143043
X	0.010950354327585912	72446	72.446	0.05326859640455623
X	0.01034163597305008	2299	2.299	0.16507580548009035
X	0.010999033749087169	82474	82.474	0.051091167313046866
X	0.010396060972816607	32355	32.355	0.06849241371264427
X	0.01031312043159316	7654	7.654	0.11045037900161658
X	0.011516010739997381	72246	72.246	0.054220404283527125
X	0.010206339333858823	1529	1.529	0.18828721828991749
X	0.01029810823967573	5623	5.623	0.12234788219301813
X	0.011057083356151562	148943	148.943	0.042028139216091165
X	0.010993220192128799	295091	295.091	0.03339819434770043
X	0.010361368059754803	4815	4.815	0.1291041699702119
X	0.010915368357397932	24340	24.34	0.07654308124756971
X	0.010362770780677185	6967	6.967	0.11415020447837904
X	0.01086889842224657	91106	91.106	0.049228074615666646
X	0.010354175300370356	40782	40.782	0.06332117975462079
X	0.01096987320951373	149702	149.702	0.041846390342901434
X	0.01099916445640933	22972	22.972	0.07823245565937174
X	0.010335494183869297	7451	7.451	0.11152499895439097
X	0.010164970218606326	1857	1.857	0.17623739668359104
X	0.011457357046838439	155112	155.112	0.04195782976261119
X	0.010386409689668006	3887	3.887	0.13876618100629412
X	0.010107151641446576	1799	1.799	0.17777302921346208
X	0.010378600943738344	25864	25.864	0.07375889027954625
X	0.0109528197859631	32307	32.307	0.06972841126797177
X	0.010906903212076421	131003	131.003	0.04366566440652717
X	0.01083732433699282	102738	102.738	0.047249475072311975
X	0.009900506258466286	1150	1.15	0.20495248454124457
X	0.010984614787265434	15837	15.837	0.08851916425621825
X	0.010274935027959094	7186	7.186	0.11265849809908995
X	0.010370770142476159	39842	39.842	0.06384937138162604
X	0.010824941330916785	56197	56.197	0.05775250234561513
X	0.010812259406757604	13119	13.119	0.09375733225315636
X	0.010434967552298707	30693	30.693	0.06979386587589555
X	0.009840856643859086	599	0.599	0.25421550049911745
X	0.010365824518373631	4617	4.617	0.13094271033102087
X	0.010903936094684381	38195	38.195	0.0658454233658177
X	0.010358580139741037	45275	45.275	0.061161846780635604
X	0.010333302617209056	46908	46.908	0.0603945028400331
X	0.010118037601479609	4186	4.186	0.13420407045894736
X	0.01099699718152248	194385	194.385	0.03838873384707391
X	0.010986764065651416	8173	8.173	0.11036450840534631
X	0.011043435656518497	18320	18.32	0.08447461969792205
X	0.011524935640119496	52239	52.239	0.06042470674461089
X	0.010323225861605473	2482	2.482	0.16081919507219006
X	0.010287393288272639	56779	56.779	0.05658559253585258
X	0.011073411651680421	187635	187.635	0.03893341787899935
X	0.011008809166740116	157998	157.998	0.04114934417735478
X	0.010350602848170852	17568	17.568	0.0838328905334678
X	0.010336318468709567	16953	16.953	0.08479555881301992
X	0.010091671212997982	5779	5.779	0.1204213429230668
X	0.010326598776411046	3398	3.398	0.14484760198470575
X	0.010982523266966233	24230	24.23	0.07681562573248649
X	0.010412738061066027	7037	7.037	0.11395300900889481
X	0.010343170552144674	12758	12.758	0.09324460124403329
X	0.010460934192458188	77996	77.996	0.05118775074074569
X	0.01050087409881246	35137	35.137	0.0668580323363242
X	0.010271958879639671	6725	6.725	0.11516495118160579
X	0.010864451033812573	6322	6.322	0.11978003393701649
X	0.01041815601316888	23541	23.541	0.07620590935195902
X	0.01032518772160232	6720	6.72	0.11539214003356459
X	0.010261759946149899	4884	4.884	0.1280802201011919
X	0.011450859901998572	18845	18.845	0.08469946227141098
X	0.011567113872852981	123793	123.793	0.0453777517030613
X	0.010949900268812977	9830	9.83	0.10366183251320593
X	0.010323188056031572	34187	34.187	0.06708894291365831
X	0.01029633754340113	2557	2.557	0.15909285706441093
X	0.010924872020326356	25563	25.563	0.0753242551882667
X	0.010417187036489063	16248	16.248	0.08622837143429504
X	0.010945943477731078	127793	127.793	0.0440807232858691
X	0.010979576774025588	31846	31.846	0.0701202683857188
X	0.010344331297298013	6956	6.956	0.11414256190374973
X	0.01100229519227731	69653	69.653	0.05405648744835105
X	0.011032882014053969	84182	84.182	0.050795266682385834
X	0.010447157341019381	7075	7.075	0.11387382194329837
X	0.011007903012962643	110551	110.551	0.04634974043364971
X	0.010420868336625234	118314	118.314	0.044492750795483274
X	0.011067858568130999	281812	281.812	0.03399132074431969
X	0.010904878457825739	37353	37.353	0.06633841851338876
X	0.009760025944660116	1293	1.293	0.19616317511178943
X	0.010885201283286777	15789	15.789	0.08834065462358485
X	0.010496852740751203	21385	21.385	0.07888297537445409
X	0.010959173859014612	14317	14.317	0.09147630150708083
X	0.010427970693203942	14368	14.368	0.08986718972115279
X	0.010332037607631414	1632	1.632	0.18499299118159004
X	0.011048322573703862	51642	51.642	0.0598087093750366
X	0.011011398066012333	105521	105.521	0.047079790489649956
X	0.010073410347492527	14968	14.968	0.08763365329158891
X	0.010276431517942232	4107	4.107	0.13576022316116074
X	0.010667869660350084	13138	13.138	0.09329309071797322
X	0.010589867334068031	9827	9.827	0.10252344425685835
X	0.010878657396208058	6268	6.268	0.12017536152138852
X	0.010391759811709946	11678	11.678	0.09618489526493881
X	0.010298094409920038	4014	4.014	0.1368967591627079
X	0.010377186092678512	17170	17.17	0.08454795415910635
X	0.011044798469156715	30600	30.6	0.0711998026366298
X	0.010470440083113354	18274	18.274	0.08305716536423752
X	0.010833469921403103	9152	9.152	0.10578331925679342
X	0.01040988337376682	5300	5.3	0.12523431119852416
X	0.011040325370742347	299058	299.058	0.03329728443906695
X	0.010314209515783877	8820	8.82	0.10535515442206421
X	0.01096184168439687	77768	77.768	0.052042825255664796
X	0.010313694514736968	1738	1.738	0.1810456789415362
X	0.010331212479154507	2918	2.918	0.152412996662513
X	0.010551088827794139	2787	2.787	0.15585477177650425
X	0.010719091588470404	21808	21.808	0.07891884387564713
X	0.010441675527650351	3756	3.756	0.14060956671289757
X	0.010169175885479482	6990	6.99	0.1133103360946535
X	0.01028616569609382	3769	3.769	0.1397469890577371
X	0.010130410732553816	3447	3.447	0.1432392121168891
X	0.010964540355517832	184583	184.583	0.03901811131263836
X	0.011060750320425848	23062	23.062	0.07827610498146896
X	0.011027002409663306	20681	20.681	0.08108900627583863
X	0.011282953684439908	75532	75.532	0.05305959617189454
X	0.010905816262392842	134272	134.272	0.04330694744446287
X	0.010289119235164696	3709	3.709	0.1405099639139624
X	0.010238598007326256	7109	7.109	0.11293034611972579
X	0.010246779084011776	3788	3.788	0.13933465104120216
X	0.010229304454729233	671	0.671	0.2479559465802271
X	0.010361318955755074	3154	3.154	0.1486566875249827
X	0.010845105172928353	11499	11.499	0.0980673781905872
X	0.010190894634559324	2047	2.047	0.17075161613691342
X	0.0102856770948848	3993	3.993	0.13708118771392727
X	0.010201628593206573	6489	6.489	0.11627791739729275
X	0.010382440549495551	3908	3.908	0.1384995299030882
X	0.011014661879483025	169056	169.056	0.04023897044622532
X	0.01029715174211544	9077	9.077	0.10429374418677086
X	0.010277802111776866	37221	37.221	0.0651184525406618
X	0.010450937036815865	3056	3.056	0.15066092417402444
X	0.010210915478974418	11530	11.53	0.09603107518428831
X	0.010926251104626609	27383	27.383	0.07362015320917285
X	0.011036285478654118	416590	416.59	0.029810644242426118
X	0.011006316770742198	434586	434.586	0.029366718853559817
X	0.010397293399610302	14474	14.474	0.08955927661389328
X	0.011032012393637948	38866	38.866	0.0657196080893934
X	0.010245595204475618	2284	2.284	0.1649226728101212
X	0.011086507458123802	14922	14.922	0.09057101594104998
X	0.010944639571135254	32785	32.785	0.06937059592727075
X	0.011013577657420434	21177	21.177	0.08041825786980851
X	0.010391795988179513	6560	6.56	0.1165723605573158
X	0.01042271947341052	32039	32.039	0.06877554366879367
X	0.010981060610523154	269195	269.195	0.0344238229863983
X	0.010397137199122393	15307	15.307	0.08790385018401412
X	0.010339343562703182	6228	6.228	0.11840811140702426
X	0.01144642436547532	11515	11.515	0.09980109359702818
X	0.010304678359628424	17441	17.441	0.08391141014228037
X	0.010347060408098626	2649	2.649	0.15748710829722937
X	0.011545245592401157	64547	64.547	0.0563433118603392
X	0.01029503337509053	18417	18.417	0.08237642814037024
X	0.010111000870547482	828	0.828	0.23027914254224388
X	0.011059237730386048	69414	69.414	0.05421166019381427
X	0.010697488628503893	17291	17.291	0.08520934774338733
X	0.01014126583917682	731	0.731	0.24028404265154926
X	0.010961372181923239	55465	55.465	0.05824812752947825
X	0.011043091020407484	84296	84.296	0.0507880137201734
X	0.011081954031595833	57101	57.101	0.05789719764984779
X	0.010821073336987355	42136	42.136	0.0635631890679987
X	0.011052963347627148	142581	142.581	0.042638870444222245
X	0.010282144722365879	4197	4.197	0.13480776681674317
X	0.010321413687770434	20487	20.487	0.07957077827982616
X	0.010939307027782574	59453	59.453	0.056877261655071905
X	0.010977839488319908	95406	95.406	0.04863852500203775
X	0.010319830547826004	8494	8.494	0.1067054913246487
X	0.011191476446685554	127934	127.934	0.04439156629194934
X	0.011004377223274442	128856	128.856	0.04403725850206371
X	0.010976844751513722	44102	44.102	0.06290324919350831
X	0.010631621409345333	27648	27.648	0.07271855841082922
X	0.01037797932473059	14136	14.136	0.09021153823385544
X	0.010989770468223341	13047	13.047	0.09444072661515736
X	0.01096718152802883	4647	4.647	0.13313966294273824
X	0.010217554093323046	2927	2.927	0.15169621370713934
X	0.01049535735506013	107430	107.43	0.046056459731563325
X	0.011040518463186828	78097	78.097	0.05209368012993981
X	0.010949104366108135	186185	186.185	0.0388876144344412
X	0.011612073392407215	60666	60.666	0.05763082063431656
X	0.010923132309467736	42670	42.67	0.06349529088297227
X	0.010905119846879734	9676	9.676	0.1040666387995349
X	0.01097126405735662	67796	67.796	0.05449427331399019
X	0.01024467131234371	2500	2.5	0.16002432605312233
X	0.010289752949870654	8369	8.369	0.1071298409081027
X	0.010936255466470141	120260	120.26	0.044969267733847504
X	0.010988024654364223	32479	32.479	0.06967960156331426
X	0.010298277290610714	2922	2.922	0.1521813584813671
X	0.010497519577118926	7287	7.287	0.11293952960768955
X	0.011032069495948078	86709	86.709	0.05029570929085636
X	0.010875991420685084	16205	16.205	0.087553451963627
X	0.010998857725110186	44886	44.886	0.0625766379872855
X	0.010257442951500502	8247	8.247	0.10754272544403519
X	0.010219732156335	9959	9.959	0.10086517827544578
X	0.0103197211431784	3323	3.323	0.1458968359829114
X	0.010340046645267758	1978	1.978	0.17355297803121897
X	0.010853296237423314	25259	25.259	0.07545971629687905
X	0.010402783500159609	8896	8.896	0.10535413732633894
X	0.010172532250225347	1773	1.773	0.1790221546837695
X	0.010999492850076014	73246	73.246	0.0531532204897226
X	0.010279817051696812	2201	2.201	0.16715592725315698
X	0.011382520285660001	21453	21.453	0.08095620349727245
X	0.010340384800154288	4923	4.923	0.12806652388488313
X	0.010344133322901907	58756	58.756	0.056046344991036535
X	0.010728747847202626	15201	15.201	0.08903463031664348
X	0.010278854558633347	18060	18.06	0.08287222356877688
X	0.01032897223570677	14188	14.188	0.08995914312926728
X	0.010335559453680076	20411	20.411	0.07970579580823539
X	0.010928721341486074	39256	39.256	0.06529617918098045
X	0.010389228001123327	200571	200.571	0.0372767790226524
X	0.010276233759684623	7360	7.36	0.11176831925785621
X	0.010221352408631871	2399	2.399	0.1621160591516567
X	0.010343304040442623	35477	35.477	0.06630874388758044
X	0.010368588154211994	7798	7.798	0.10996271763452241
X	0.011050492868877581	309600	309.6	0.032925084549840965
X	0.010916852972780453	1850	1.85	0.18070711162192324
X	0.010979212955436	82358	82.358	0.05108442065309677
X	0.011382546144371024	257305	257.305	0.035366905718255216
X	0.010265418036659545	3022	3.022	0.15032367802000643
X	0.010338722754037128	107903	107.903	0.045759133284005105
X	0.01032308893326519	9323	9.323	0.10345496263557982
X	0.011067228567545277	390127	390.127	0.030498439864512828
X	0.010343762282430405	5649	5.649	0.12234014347145716
X	0.010337913492282151	16705	16.705	0.08521750299807716
X	0.011026481419745108	13039	13.039	0.09456510045159063
X	0.010922965920589662	2601	2.601	0.1613367888743971
X	0.010340358507543624	4574	4.574	0.13124410693455238
X	0.010510748221693471	29555	29.555	0.07084908129495833
X	0.010927222326744587	14330	14.33	0.09135967157150233
X	0.011085939208272475	242781	242.781	0.035742582828790594
X	0.010350729219043798	12195	12.195	0.09468104607885633
X	0.010265951380618397	4100	4.1	0.13579124874358214
X	0.010297236580837407	4270	4.27	0.1341006671928233
X	0.011041467232218605	89939	89.939	0.04970036495484534
X	0.010915855222992063	64439	64.439	0.055331145108428466
X	0.011323264271269822	134791	134.791	0.04379624897589951
X	0.011032855989007544	25057	25.057	0.0760769264956003
X	0.010999356390561144	3746	3.746	0.14319675185512215
X	0.010339696006536808	3664	3.664	0.14131364604578453
X	0.010232707617185458	5922	5.922	0.11999801278021088
X	0.011358114086295702	462671	462.671	0.02906328155493282
X	0.01100270038396162	5433	5.433	0.12651825518333015
X	0.01028428323502277	11581	11.581	0.09611901738660816
X	0.010913703807665114	56316	56.316	0.05786910792637633
X	0.010432561986029923	24374	24.374	0.07536240030075154
X	0.01035476869237267	7277	7.277	0.11247672842574735
X	0.010270061875066516	3997	3.997	0.13696606243177523
X	0.010960823796297068	24505	24.505	0.07647676282216072
X	0.010534671460910403	32057	32.057	0.06900798921811935
X	0.010253913216899935	4055	4.055	0.13623841832264302
X	0.010946346958432352	22309	22.309	0.07887324374646039
X	0.011710896985424633	211288	211.288	0.03812750992877354
X	0.010929687582264987	51231	51.231	0.05975280604994563
X	0.011342369128651442	139119	139.119	0.04336164769596808
X	0.011018064244934971	55792	55.792	0.058234156613988945
X	0.010627652578929971	7279	7.279	0.11344583163201649
X	0.010426438575377974	37899	37.899	0.06503833470976332
X	0.010475934440533343	6100	6.1	0.11975334292678441
X	0.010275388189028265	5638	5.638	0.12214932051335556
X	0.010310901220378664	8546	8.546	0.10645790431915757
X	0.010276096547256698	4476	4.476	0.13192062474717564
X	0.010369430928109059	62239	62.239	0.05502551369397061
X	0.010217119599985672	10155	10.155	0.10020349044426735
X	0.010220697373952146	1603	1.603	0.18543099798480686
X	0.010300631703268228	11825	11.825	0.09550385525032346
X	0.01051775474816447	10578	10.578	0.09980979395162431
X	0.010296972338181136	9423	9.423	0.10300068626813956
X	0.010264446646173967	5809	5.809	0.12089584869808837
X	0.010168580685354879	8372	8.372	0.10669491123615636
X	0.010314663704028348	6519	6.519	0.1165264987006794
X	0.010290778439750022	12712	12.712	0.09319904193257647
X	0.010236812657106763	2725	2.725	0.15545311045810017
X	0.010383477216047276	6777	6.777	0.11528384235753983
X	0.010362568921867432	14386	14.386	0.08964150211739032
X	0.010995159176212962	75908	75.908	0.05251757415750576
X	0.010231714290379956	4912	4.912	0.12771150282930976
X	0.0100882785921045	1105	1.105	0.20900255549829277
X	0.010171558605195039	2875	2.875	0.1523759941896847
X	0.010917983606998836	44211	44.211	0.0627389674057691
X	0.011233062491231923	232545	232.545	0.03641919898983161
X	0.010851122973285256	35140	35.14	0.06759132748832374
X	0.010302745326975709	4182	4.182	0.13505882862242222
X	0.01104874905127671	49241	49.241	0.060766200439012416
X	0.011173440724137966	119616	119.616	0.04537317999419825
X	0.010246994766320842	1392	1.392	0.19452962953015746
X	0.010308891790805014	5648	5.648	0.12220972429078053
X	0.010984388100574843	22369	22.369	0.07889384087873438
X	0.010359483778110667	11968	11.968	0.09530276729721322
X	0.010314832466242919	22029	22.029	0.07765256370039064
X	0.011049201502165443	63675	63.675	0.0557768495156876
X	0.010709491782392038	15461	15.461	0.0884797242740701
X	0.010306723663389306	2990	2.99	0.1510601162671323
X	0.010363322910242131	2337	2.337	0.16429087376711074
X	0.010373159428802212	19826	19.826	0.08057963892155762
X	0.010424340554301342	21026	21.026	0.07914630487172064
X	0.011014993741396011	24970	24.97	0.07612405320493959
X	0.010530686679681076	35420	35.42	0.06674253708934924
X	0.011007700046177004	248379	248.379	0.03538837093643274
X	0.010247661451060371	4874	4.874	0.12810903156013653
X	0.010290133091983037	2110	2.11	0.16958190601756745
X	0.011150033292932812	10307	10.307	0.10265528097637086
X	0.010983175963973557	36279	36.279	0.0671466285222779
X	0.010270793452123322	16221	16.221	0.08587013066598274
X	0.010227999086020585	2092	2.092	0.16972389901259474
X	0.010136449883172778	1700	1.7	0.18133389299281547
X	0.01111229958427529	17101	17.101	0.08661520643142706
X	0.010301488727494921	2046	2.046	0.1713949891019764
X	0.010879435774843012	10578	10.578	0.10094100010963694
X	0.01026635001166843	2419	2.419	0.1619049273450875
X	0.01090986633163674	10838	10.838	0.10022054520106352
X	0.01091203001564885	20301	20.301	0.08130726892929445
X	0.010298195743009206	5687	5.687	0.12188753796656861
X	0.010146979640267848	4062	4.062	0.13568514089917424
X	0.010500219453527576	98378	98.378	0.047435133686019024
X	0.011016721943798378	164861	164.861	0.040579947994872315
X	0.010938501410802255	46887	46.887	0.061560451125146964
X	0.011025905186524474	93859	93.859	0.048975563727981744
X	0.010972671299186445	11900	11.9	0.09733188538706537
X	0.010238548223448475	17329	17.329	0.08391152734838302
X	0.010860877612690237	10837	10.837	0.10007339084395933
X	0.011055927377500534	111927	111.927	0.046226048571581024
X	0.01180576575680523	15073	15.073	0.09217889285881459
X	0.010985939148148013	21098	21.098	0.08045109638699867
X	0.010267149774376936	4854	4.854	0.1283660098739527
X	0.011382543962320269	264484	264.484	0.035043970867570315
X	0.011166802995936012	23232	23.232	0.07833347959677564
X	0.011551001816658617	20588	20.588	0.08247743842067068
X	0.011569682726006572	235946	235.946	0.036601842126330886
X	0.01049590305814153	31683	31.683	0.06919337341951898
X	0.011033660566454951	35088	35.088	0.06800178848305562
X	0.011073907096954943	124761	124.761	0.04460744440374754
X	0.011691924246755154	86088	86.088	0.05140213108551096
X	0.010988060645517144	40931	40.931	0.0645093843976962
X	0.01026496675397458	7681	7.681	0.1101488398656759
X	0.011003450659272286	26174	26.174	0.07491226599572799
X	0.01037166962878981	4181	4.181	0.13537012679425237
X	0.010357546719124774	57908	57.908	0.05634293223420181
X	0.011772597684632196	458527	458.527	0.029500936060516854
X	0.011030032832300887	15300	15.3	0.08966613663847017
X	0.010940448595705522	50792	50.792	0.05994412224902226
X	0.01028001619982038	9418	9.418	0.10296233214464948
X	0.011046797386851825	63920	63.92	0.055701455133558744
X	0.011604061451122444	40193	40.193	0.06609265511636332
X	0.010281531976638713	3910	3.91	0.13802582692614487
X	0.011285459967487355	24191	24.191	0.07755714512862179
X	0.011091668424991553	292762	292.762	0.03358618874759761
X	0.010921233835918878	21202	21.202	0.08016133819352547
X	0.01036608440343859	180194	180.194	0.03860333315532089
X	0.01103256325807495	20686	20.686	0.08109609992944052
X	0.010369899820469737	24460	24.46	0.07512296190692819
X	0.010292220292545567	3263	3.263	0.14665515496146128
X	0.010505772724039918	29298	29.298	0.07104442585727995
X	0.010992413234273492	97375	97.375	0.048329821583191414
X	0.01031319530186543	1522	1.522	0.18923125219431153
X	0.01111127565981984	448522	448.522	0.02915144300743371
X	0.011022627771720363	27901	27.901	0.07337618435845623
X	0.010987004703623215	32044	32.044	0.06999132187473951
X	0.010498807094572422	7048	7.048	0.11420663971756764
X	0.01082927721052429	9276	9.276	0.10529625139982238
X	0.010226563212047284	2419	2.419	0.16169550470572047
X	0.010164230244496083	3361	3.361	0.144611221308137
X	0.010913835280619582	6758	6.758	0.11732387606301216
X	0.010989720951431676	100816	100.816	0.0477696875775405
X	0.011038469068994625	1052386	1052.386	0.02188994116818984
X	0.010274377906351474	4849	4.849	0.12844024259481185
X	0.01092287051074137	92207	92.207	0.04911237814840821
X	0.01096303044098507	3974	3.974	0.14024936999920387
X	0.010316515189639216	11278	11.278	0.09707341553315815
X	0.01029292836592616	11502	11.502	0.09636556284537832
X	0.011446857527774553	47229	47.229	0.062348476007848035
X	0.01028569813419479	6809	6.809	0.11474051991164577
X	0.011071951200691032	697011	697.011	0.025137852694360786
X	0.010315772436482308	4533	4.533	0.13153419706039396
X	0.011052397517577372	76875	76.875	0.0523870314208746
X	0.010323926445815246	8643	8.643	0.10610278811768235
X	0.010419071116905654	78798	78.798	0.0509453561399972
X	0.010948802882504884	132223	132.223	0.04358662179832731
X	0.010327075545727735	10495	10.495	0.09946378236514063
X	0.010299072121213696	4384	4.384	0.13293597279156752
X	0.010860050395586305	66370	66.37	0.054695719664475353
X	0.01073395601264952	6677	6.677	0.11714564421075457
X	0.010271376349810485	6397	6.397	0.11709834683996484
X	0.010487060608781672	86110	86.11	0.049567847839836865
X	0.010452707175607399	46580	46.58	0.06076820852262622
X	0.011032885358005864	20374	20.374	0.08150875655743965
X	0.010228171497488	12647	12.647	0.09316872685365278
X	0.01133138975979672	131945	131.945	0.04411944902078199
X	0.010938349738253143	86841	86.841	0.05012745584648445
X	0.010896442814086821	20070	20.07	0.08157914020921249
X	0.010660470335366313	4866	4.866	0.12987774925018505
X	0.010283010254541362	66888	66.888	0.05357029272365299
X	0.010421399390992117	21700	21.7	0.0783108796088851
X	0.011217078043437594	77548	77.548	0.052493199799340445
X	0.011057132741301618	23483	23.483	0.077797023583762
X	0.010937256707988677	19972	19.972	0.08181424922206026
X	0.010887207745991657	13463	13.463	0.09316616552223794
X	0.010280798116090136	242287	242.287	0.034879124310469155
X	0.010346781585078235	37921	37.921	0.06485973451494506
X	0.010427297648602035	9433	9.433	0.10339685094655848
X	0.010362274147264743	3859	3.859	0.138993161416555
X	0.010989954391891321	13985	13.985	0.0922807551560519
X	0.010911004164493266	19273	19.273	0.08272532208147035
X	0.010955865565079645	108944	108.944	0.04650301658897573
X	0.010880138497364616	65897	65.897	0.054860057701195356
X	0.010386769980874292	9249	9.249	0.10394299943109063
X	0.010383724160363686	18402	18.402	0.08263474770265004
X	0.010934173535672588	23049	23.049	0.07799102440349891
X	0.011089220828958015	168726	168.726	0.0403558352525938
X	0.01106216266024614	35177	35.177	0.06800284468916615
X	0.01038763999413029	38970	38.97	0.06435695604788735
X	0.010379463522005139	8897	8.897	0.1052714097326266
X	0.010386819781392541	24530	24.53	0.07509223248525264
X	0.010351373415269375	2316	2.316	0.16472258162903775
X	0.010284280417623014	3838	3.838	0.1388959704536444
X	0.011041411652919364	119580	119.58	0.04519829137863852
X	0.01029361424729152	15324	15.324	0.08757871766261946
X	0.011040233209788362	53555	53.555	0.05907350522901666
X	0.01077506254665715	2180	2.18	0.17034174899148616
X	0.010161276016850653	2183	2.183	0.16696733073805942
X	0.010373206536555834	45996	45.996	0.060869208319539875
X	0.010958392068775382	46993	46.993	0.06155139324779852
X	0.010374864239605467	25211	25.211	0.07438136032231066
X	0.01043448471363982	3068	3.068	0.15038524140271553
X	0.010294187100148332	3870	3.87	0.1385565403476185
X	0.0109864173958554	14339	14.339	0.09150519341344976
X	0.010806322513370471	15656	15.656	0.08837559906566375
X	0.010401925046301348	105197	105.197	0.04624201906296077
X	0.010350978589635289	4778	4.778	0.12939328982751908
X	0.010280023755790386	4154	4.154	0.1352620220154501
X	0.010970111768025712	22660	22.66	0.07852061918214405
X	0.010444612302291803	9370	9.37	0.1036853928896558
X	0.011038476700407918	152869	152.869	0.041641839592172485
X	0.010359280129013632	8269	8.269	0.10780167984612254
X	0.010306534001140075	27680	27.68	0.07194194661635371
X	0.011019929437511455	63192	63.192	0.055869172334236786
X	0.01034268075908433	68390	68.39	0.053277868144134925
X	0.011024564319864972	44467	44.467	0.06282143728935836
X	0.011033639993533921	130548	130.548	0.04388500939006906
X	0.010257455915920506	1718	1.718	0.1814145711739429
X	0.010431766682865705	28281	28.281	0.07171682168107041
X	0.010342556326476602	11997	11.997	0.09517401964171693
X	0.010758222608705991	142577	142.577	0.04225684009531758
X	0.01032105224638815	62056	62.056	0.05499375814375885
X	0.01035409611160133	20759	20.759	0.07930524680577242
X	0.011003511184160718	49114	49.114	0.06073541375641304
X	0.010265680350037405	7940	7.94	0.10894042827061813
X	0.0102763815709541	10449	10.449	0.09944626979381059
X	0.010359973009124284	4197	4.197	0.13514704416561543
X	0.010343750673485986	14630	14.63	0.08908636509591031
X	0.01100894142310097	126159	126.159	0.04435498543856248
X	0.011374310071620736	88016	88.016	0.050557758594583786
X	0.011044845118410708	9155	9.155	0.10645525236599343
X	0.010404412282753166	17060	17.06	0.08480331748723574
X	0.010334302754858339	8154	8.154	0.10821899367867396
X	0.011776343488491534	107475	107.475	0.047852100213949915
X	0.01043753711736964	13867	13.867	0.09096447956258748
X	0.010328020434322157	10602	10.602	0.0991310629219309
X	0.010450025567121391	70784	70.784	0.05285191777769423
X	0.011653772339517159	87382	87.382	0.051091444670334725
X	0.010928767397342	9599	9.599	0.10441952910288625
X	0.010354839519982617	19796	19.796	0.08057283456707695
X	0.010705182275905358	4691	4.691	0.13165667350160004
time for making epsilon is 1.4252941608428955
epsilons are
[0.21231169081664492, 0.0844834279001886, 0.17230443229241965, 0.17918521665099177, 0.0650007730122283, 0.06787459137050667, 0.07013678943974419, 0.06663404479342085, 0.11069797863906802, 0.07909142717053147, 0.13714024104666167, 0.055663154478201096, 0.1098869824438057, 0.0412973652914638, 0.08648660876018216, 0.06901737042670286, 0.07122113268159558, 0.07961848666873426, 0.03793132237101365, 0.05355395538607699, 0.0989766879898413, 0.034493398596501856, 0.10449667313982809, 0.10896782719780922, 0.11260845309337522, 0.05878649331971951, 0.06895842566521755, 0.11303803918118803, 0.06178963778196125, 0.019534595986091415, 0.134433995065944, 0.035289287239327755, 0.09030434620471804, 0.1179524224883492, 0.09423086286462783, 0.05742876453386325, 0.05500258708712936, 0.2289750336024725, 0.1467939675476967, 0.18335911312396436, 0.1502114280757287, 0.259997764803797, 0.2603694451953578, 0.1905554106817367, 0.2333063970637188, 0.23174638105209103, 0.12822235974414659, 0.15369002463201767, 0.15219883015584373, 0.1193777571270285, 0.10166024314165945, 0.20650331693083154, 0.14658886219665943, 0.18747002059819404, 0.12321253553867342, 0.1333073554509719, 0.1960368485849822, 0.15873236248298225, 0.1714307847759451, 0.17233812761859965, 0.10630973175007127, 0.19286156939766252, 0.09888334724135128, 0.1291054943617652, 0.15956371432977015, 0.1789893027944126, 0.18238174335790017, 0.1495966984803566, 0.14847954427941573, 0.17637125966722134, 0.12781287148835138, 0.15093917466865897, 0.17343101375904538, 0.19761641881569145, 0.15024804660284444, 0.1410143415696296, 0.21004024584820127, 0.28852156104839255, 0.15051960931871036, 0.10996831403731923, 0.17623056428645004, 0.2136851142556557, 0.1266514000707359, 0.20993514182922074, 0.17229678072101604, 0.2557454447554333, 0.15973554987417665, 0.24597068345484954, 0.13489835976693815, 0.16246529638115081, 0.13807169714113848, 0.23246090464758282, 0.21791318896618894, 0.13824102063832036, 0.13984107380459895, 0.1564341171704979, 0.1604832427174221, 0.13552323341641534, 0.11235666035676961, 0.1629903538477599, 0.10255061108047145, 0.1276363890410653, 0.14347973888336818, 0.21833555134313834, 0.1648967389114032, 0.1787222495946494, 0.13781161264514732, 0.3112132895797413, 0.19088108105114823, 0.21349234125427452, 0.2176688691264897, 0.18086489723788055, 0.19086027923810758, 0.15498136915775348, 0.15243573415318046, 0.1941217441696152, 0.20150249029976564, 0.22370253013482866, 0.12311704723515174, 0.23116114251983474, 0.09271726015217015, 0.19267885228481807, 0.1923435952040421, 0.16167285198107098, 0.18777526068788217, 0.13779529352200928, 0.2346696651548035, 0.16133519444974914, 0.09195187724422382, 0.17792205295715008, 0.1209969482168896, 0.22784203543052806, 0.13946359270264366, 0.24058728711489924, 0.17309418220436418, 0.17453696575188626, 0.20510933268333678, 0.21254477643903552, 0.21632533686149427, 0.18260747204212888, 0.18483316496262928, 0.24993047353256287, 0.12353626529241864, 0.08332963841822842, 0.11301041080607356, 0.18908653917201998, 0.1932223130962497, 0.21235304192175705, 0.21412714161826255, 0.18740446864341467, 0.21671029602588057, 0.12897697372343508, 0.15493140500913913, 0.1204013439773209, 0.1568410016954058, 0.10181558831788359, 0.1764879523860723, 0.19510028640459254, 0.1427636966514345, 0.28548655727286487, 0.12973756434658804, 0.20306650155367437, 0.1544890163932095, 0.24297876351828504, 0.16512598724522498, 0.16449606181211138, 0.18249663176330364, 0.1661378803215826, 0.14268479208063123, 0.3312176287836802, 0.23695736018476704, 0.19336946967072477, 0.12034108049613675, 0.19322917686179666, 0.14460390942808687, 0.14230969072241131, 0.15451636129122595, 0.14874091510678492, 0.1416721593287206, 0.19112280323416178, 0.13229553814921105, 0.17431832174447037, 0.14128667619552016, 0.20421492567568422, 0.12933694958828965, 0.1435187791766561, 0.20386998271580173, 0.15043170691623067, 0.15598716523864498, 0.09475097597946319, 0.18610317214157054, 0.2715385835721124, 0.1674777675808523, 0.20058064145193827, 0.17504710434514284, 0.18850222111545709, 0.20437489040263931, 0.21531662837340318, 0.14950020879391562, 0.24414394129922773, 0.113053851971462, 0.1905186155761858, 0.08112250937118073, 0.1447520065767648, 0.1585476205011162, 0.24457119630921223, 0.1271328865374694, 0.1466521733256935, 0.16260654509284425, 0.10946904245334363, 0.12182138102131115, 0.2115754820169573, 0.19887504666637723, 0.14803346752934973, 0.25385480659017123, 0.2497889436415894, 0.24578079594366176, 0.2684725603116236, 0.1475758466436364, 0.20846459332222123, 0.1427032799345329, 0.20624872011516554, 0.142460758451036, 0.19657072913755014, 0.17765749149329946, 0.14354407179528247, 0.21975580080339663, 0.235616908642847, 0.13882217560757107, 0.21670344966225485, 0.24012199307493393, 0.19940439850353148, 0.16448209953715912, 0.13769556218344708, 0.2250219440555815, 0.23114860066628448, 0.15004625441388625, 0.12367578046981972, 0.2249724457125374, 0.12251572390300684, 0.057548273519961134, 0.08699583035171446, 0.13147643974767265, 0.27313641320504073, 0.14360478251255315, 0.11351091032143043, 0.05326859640455623, 0.16507580548009035, 0.051091167313046866, 0.06849241371264427, 0.11045037900161658, 0.054220404283527125, 0.18828721828991749, 0.12234788219301813, 0.042028139216091165, 0.03339819434770043, 0.1291041699702119, 0.07654308124756971, 0.11415020447837904, 0.049228074615666646, 0.06332117975462079, 0.041846390342901434, 0.07823245565937174, 0.11152499895439097, 0.17623739668359104, 0.04195782976261119, 0.13876618100629412, 0.17777302921346208, 0.07375889027954625, 0.06972841126797177, 0.04366566440652717, 0.047249475072311975, 0.20495248454124457, 0.08851916425621825, 0.11265849809908995, 0.06384937138162604, 0.05775250234561513, 0.09375733225315636, 0.06979386587589555, 0.25421550049911745, 0.13094271033102087, 0.0658454233658177, 0.061161846780635604, 0.0603945028400331, 0.13420407045894736, 0.03838873384707391, 0.11036450840534631, 0.08447461969792205, 0.06042470674461089, 0.16081919507219006, 0.05658559253585258, 0.03893341787899935, 0.04114934417735478, 0.0838328905334678, 0.08479555881301992, 0.1204213429230668, 0.14484760198470575, 0.07681562573248649, 0.11395300900889481, 0.09324460124403329, 0.05118775074074569, 0.0668580323363242, 0.11516495118160579, 0.11978003393701649, 0.07620590935195902, 0.11539214003356459, 0.1280802201011919, 0.08469946227141098, 0.0453777517030613, 0.10366183251320593, 0.06708894291365831, 0.15909285706441093, 0.0753242551882667, 0.08622837143429504, 0.0440807232858691, 0.0701202683857188, 0.11414256190374973, 0.05405648744835105, 0.050795266682385834, 0.11387382194329837, 0.04634974043364971, 0.044492750795483274, 0.03399132074431969, 0.06633841851338876, 0.19616317511178943, 0.08834065462358485, 0.07888297537445409, 0.09147630150708083, 0.08986718972115279, 0.18499299118159004, 0.0598087093750366, 0.047079790489649956, 0.08763365329158891, 0.13576022316116074, 0.09329309071797322, 0.10252344425685835, 0.12017536152138852, 0.09618489526493881, 0.1368967591627079, 0.08454795415910635, 0.0711998026366298, 0.08305716536423752, 0.10578331925679342, 0.12523431119852416, 0.03329728443906695, 0.10535515442206421, 0.052042825255664796, 0.1810456789415362, 0.152412996662513, 0.15585477177650425, 0.07891884387564713, 0.14060956671289757, 0.1133103360946535, 0.1397469890577371, 0.1432392121168891, 0.03901811131263836, 0.07827610498146896, 0.08108900627583863, 0.05305959617189454, 0.04330694744446287, 0.1405099639139624, 0.11293034611972579, 0.13933465104120216, 0.2479559465802271, 0.1486566875249827, 0.0980673781905872, 0.17075161613691342, 0.13708118771392727, 0.11627791739729275, 0.1384995299030882, 0.04023897044622532, 0.10429374418677086, 0.0651184525406618, 0.15066092417402444, 0.09603107518428831, 0.07362015320917285, 0.029810644242426118, 0.029366718853559817, 0.08955927661389328, 0.0657196080893934, 0.1649226728101212, 0.09057101594104998, 0.06937059592727075, 0.08041825786980851, 0.1165723605573158, 0.06877554366879367, 0.0344238229863983, 0.08790385018401412, 0.11840811140702426, 0.09980109359702818, 0.08391141014228037, 0.15748710829722937, 0.0563433118603392, 0.08237642814037024, 0.23027914254224388, 0.05421166019381427, 0.08520934774338733, 0.24028404265154926, 0.05824812752947825, 0.0507880137201734, 0.05789719764984779, 0.0635631890679987, 0.042638870444222245, 0.13480776681674317, 0.07957077827982616, 0.056877261655071905, 0.04863852500203775, 0.1067054913246487, 0.04439156629194934, 0.04403725850206371, 0.06290324919350831, 0.07271855841082922, 0.09021153823385544, 0.09444072661515736, 0.13313966294273824, 0.15169621370713934, 0.046056459731563325, 0.05209368012993981, 0.0388876144344412, 0.05763082063431656, 0.06349529088297227, 0.1040666387995349, 0.05449427331399019, 0.16002432605312233, 0.1071298409081027, 0.044969267733847504, 0.06967960156331426, 0.1521813584813671, 0.11293952960768955, 0.05029570929085636, 0.087553451963627, 0.0625766379872855, 0.10754272544403519, 0.10086517827544578, 0.1458968359829114, 0.17355297803121897, 0.07545971629687905, 0.10535413732633894, 0.1790221546837695, 0.0531532204897226, 0.16715592725315698, 0.08095620349727245, 0.12806652388488313, 0.056046344991036535, 0.08903463031664348, 0.08287222356877688, 0.08995914312926728, 0.07970579580823539, 0.06529617918098045, 0.0372767790226524, 0.11176831925785621, 0.1621160591516567, 0.06630874388758044, 0.10996271763452241, 0.032925084549840965, 0.18070711162192324, 0.05108442065309677, 0.035366905718255216, 0.15032367802000643, 0.045759133284005105, 0.10345496263557982, 0.030498439864512828, 0.12234014347145716, 0.08521750299807716, 0.09456510045159063, 0.1613367888743971, 0.13124410693455238, 0.07084908129495833, 0.09135967157150233, 0.035742582828790594, 0.09468104607885633, 0.13579124874358214, 0.1341006671928233, 0.04970036495484534, 0.055331145108428466, 0.04379624897589951, 0.0760769264956003, 0.14319675185512215, 0.14131364604578453, 0.11999801278021088, 0.02906328155493282, 0.12651825518333015, 0.09611901738660816, 0.05786910792637633, 0.07536240030075154, 0.11247672842574735, 0.13696606243177523, 0.07647676282216072, 0.06900798921811935, 0.13623841832264302, 0.07887324374646039, 0.03812750992877354, 0.05975280604994563, 0.04336164769596808, 0.058234156613988945, 0.11344583163201649, 0.06503833470976332, 0.11975334292678441, 0.12214932051335556, 0.10645790431915757, 0.13192062474717564, 0.05502551369397061, 0.10020349044426735, 0.18543099798480686, 0.09550385525032346, 0.09980979395162431, 0.10300068626813956, 0.12089584869808837, 0.10669491123615636, 0.1165264987006794, 0.09319904193257647, 0.15545311045810017, 0.11528384235753983, 0.08964150211739032, 0.05251757415750576, 0.12771150282930976, 0.20900255549829277, 0.1523759941896847, 0.0627389674057691, 0.03641919898983161, 0.06759132748832374, 0.13505882862242222, 0.060766200439012416, 0.04537317999419825, 0.19452962953015746, 0.12220972429078053, 0.07889384087873438, 0.09530276729721322, 0.07765256370039064, 0.0557768495156876, 0.0884797242740701, 0.1510601162671323, 0.16429087376711074, 0.08057963892155762, 0.07914630487172064, 0.07612405320493959, 0.06674253708934924, 0.03538837093643274, 0.12810903156013653, 0.16958190601756745, 0.10265528097637086, 0.0671466285222779, 0.08587013066598274, 0.16972389901259474, 0.18133389299281547, 0.08661520643142706, 0.1713949891019764, 0.10094100010963694, 0.1619049273450875, 0.10022054520106352, 0.08130726892929445, 0.12188753796656861, 0.13568514089917424, 0.047435133686019024, 0.040579947994872315, 0.061560451125146964, 0.048975563727981744, 0.09733188538706537, 0.08391152734838302, 0.10007339084395933, 0.046226048571581024, 0.09217889285881459, 0.08045109638699867, 0.1283660098739527, 0.035043970867570315, 0.07833347959677564, 0.08247743842067068, 0.036601842126330886, 0.06919337341951898, 0.06800178848305562, 0.04460744440374754, 0.05140213108551096, 0.0645093843976962, 0.1101488398656759, 0.07491226599572799, 0.13537012679425237, 0.05634293223420181, 0.029500936060516854, 0.08966613663847017, 0.05994412224902226, 0.10296233214464948, 0.055701455133558744, 0.06609265511636332, 0.13802582692614487, 0.07755714512862179, 0.03358618874759761, 0.08016133819352547, 0.03860333315532089, 0.08109609992944052, 0.07512296190692819, 0.14665515496146128, 0.07104442585727995, 0.048329821583191414, 0.18923125219431153, 0.02915144300743371, 0.07337618435845623, 0.06999132187473951, 0.11420663971756764, 0.10529625139982238, 0.16169550470572047, 0.144611221308137, 0.11732387606301216, 0.0477696875775405, 0.02188994116818984, 0.12844024259481185, 0.04911237814840821, 0.14024936999920387, 0.09707341553315815, 0.09636556284537832, 0.062348476007848035, 0.11474051991164577, 0.025137852694360786, 0.13153419706039396, 0.0523870314208746, 0.10610278811768235, 0.0509453561399972, 0.04358662179832731, 0.09946378236514063, 0.13293597279156752, 0.054695719664475353, 0.11714564421075457, 0.11709834683996484, 0.049567847839836865, 0.06076820852262622, 0.08150875655743965, 0.09316872685365278, 0.04411944902078199, 0.05012745584648445, 0.08157914020921249, 0.12987774925018505, 0.05357029272365299, 0.0783108796088851, 0.052493199799340445, 0.077797023583762, 0.08181424922206026, 0.09316616552223794, 0.034879124310469155, 0.06485973451494506, 0.10339685094655848, 0.138993161416555, 0.0922807551560519, 0.08272532208147035, 0.04650301658897573, 0.054860057701195356, 0.10394299943109063, 0.08263474770265004, 0.07799102440349891, 0.0403558352525938, 0.06800284468916615, 0.06435695604788735, 0.1052714097326266, 0.07509223248525264, 0.16472258162903775, 0.1388959704536444, 0.04519829137863852, 0.08757871766261946, 0.05907350522901666, 0.17034174899148616, 0.16696733073805942, 0.060869208319539875, 0.06155139324779852, 0.07438136032231066, 0.15038524140271553, 0.1385565403476185, 0.09150519341344976, 0.08837559906566375, 0.04624201906296077, 0.12939328982751908, 0.1352620220154501, 0.07852061918214405, 0.1036853928896558, 0.041641839592172485, 0.10780167984612254, 0.07194194661635371, 0.055869172334236786, 0.053277868144134925, 0.06282143728935836, 0.04388500939006906, 0.1814145711739429, 0.07171682168107041, 0.09517401964171693, 0.04225684009531758, 0.05499375814375885, 0.07930524680577242, 0.06073541375641304, 0.10894042827061813, 0.09944626979381059, 0.13514704416561543, 0.08908636509591031, 0.04435498543856248, 0.050557758594583786, 0.10645525236599343, 0.08480331748723574, 0.10821899367867396, 0.047852100213949915, 0.09096447956258748, 0.0991310629219309, 0.05285191777769423, 0.051091444670334725, 0.10441952910288625, 0.08057283456707695, 0.13165667350160004]
0.09407860306947827
Making ranges
torch.Size([24134, 2])
We keep 4.01e+06/2.06e+08 =  1% of the original kernel matrix.

torch.Size([2507, 2])
We keep 1.06e+05/1.15e+06 =  9% of the original kernel matrix.

torch.Size([8439, 2])
We keep 6.66e+05/1.54e+07 =  4% of the original kernel matrix.

torch.Size([26377, 2])
We keep 8.97e+06/2.91e+08 =  3% of the original kernel matrix.

torch.Size([25034, 2])
We keep 5.28e+06/2.45e+08 =  2% of the original kernel matrix.

torch.Size([4311, 2])
We keep 2.71e+05/4.48e+06 =  6% of the original kernel matrix.

torch.Size([10555, 2])
We keep 1.05e+06/3.04e+07 =  3% of the original kernel matrix.

torch.Size([3920, 2])
We keep 2.16e+05/3.20e+06 =  6% of the original kernel matrix.

torch.Size([10057, 2])
We keep 9.30e+05/2.57e+07 =  3% of the original kernel matrix.

torch.Size([61257, 2])
We keep 3.80e+07/1.71e+09 =  2% of the original kernel matrix.

torch.Size([37218, 2])
We keep 1.09e+07/5.93e+08 =  1% of the original kernel matrix.

torch.Size([53094, 2])
We keep 3.13e+07/1.23e+09 =  2% of the original kernel matrix.

torch.Size([34493, 2])
We keep 9.72e+06/5.04e+08 =  1% of the original kernel matrix.

torch.Size([47305, 2])
We keep 2.07e+07/8.99e+08 =  2% of the original kernel matrix.

torch.Size([32537, 2])
We keep 8.40e+06/4.30e+08 =  1% of the original kernel matrix.

torch.Size([56823, 2])
We keep 2.71e+07/1.37e+09 =  1% of the original kernel matrix.

torch.Size([35680, 2])
We keep 1.01e+07/5.32e+08 =  1% of the original kernel matrix.

torch.Size([12697, 2])
We keep 2.99e+06/6.55e+07 =  4% of the original kernel matrix.

torch.Size([17478, 2])
We keep 2.81e+06/1.16e+08 =  2% of the original kernel matrix.

torch.Size([28118, 2])
We keep 1.82e+07/4.89e+08 =  3% of the original kernel matrix.

torch.Size([25600, 2])
We keep 6.13e+06/3.17e+08 =  1% of the original kernel matrix.

torch.Size([7675, 2])
We keep 8.46e+05/1.61e+07 =  5% of the original kernel matrix.

torch.Size([13502, 2])
We keep 1.69e+06/5.76e+07 =  2% of the original kernel matrix.

torch.Size([80293, 2])
We keep 8.74e+07/3.50e+09 =  2% of the original kernel matrix.

torch.Size([42108, 2])
We keep 1.48e+07/8.50e+08 =  1% of the original kernel matrix.

torch.Size([12920, 2])
We keep 2.01e+06/6.08e+07 =  3% of the original kernel matrix.

torch.Size([17779, 2])
We keep 2.80e+06/1.12e+08 =  2% of the original kernel matrix.

torch.Size([232926, 2])
We keep 3.79e+08/2.37e+10 =  1% of the original kernel matrix.

torch.Size([75231, 2])
We keep 3.53e+07/2.21e+09 =  1% of the original kernel matrix.

torch.Size([24556, 2])
We keep 6.02e+06/2.57e+08 =  2% of the original kernel matrix.

torch.Size([24170, 2])
We keep 4.98e+06/2.30e+08 =  2% of the original kernel matrix.

torch.Size([51805, 2])
We keep 2.42e+07/1.11e+09 =  2% of the original kernel matrix.

torch.Size([33959, 2])
We keep 9.26e+06/4.77e+08 =  1% of the original kernel matrix.

torch.Size([46470, 2])
We keep 1.71e+07/8.68e+08 =  1% of the original kernel matrix.

torch.Size([32711, 2])
We keep 8.11e+06/4.23e+08 =  1% of the original kernel matrix.

torch.Size([31185, 2])
We keep 1.37e+07/4.80e+08 =  2% of the original kernel matrix.

torch.Size([26833, 2])
We keep 6.41e+06/3.14e+08 =  2% of the original kernel matrix.

torch.Size([301852, 2])
We keep 8.00e+08/3.65e+10 =  2% of the original kernel matrix.

torch.Size([85354, 2])
We keep 4.24e+07/2.74e+09 =  1% of the original kernel matrix.

torch.Size([105203, 2])
We keep 9.91e+07/5.19e+09 =  1% of the original kernel matrix.

torch.Size([50201, 2])
We keep 1.80e+07/1.03e+09 =  1% of the original kernel matrix.

torch.Size([15628, 2])
We keep 4.77e+06/1.13e+08 =  4% of the original kernel matrix.

torch.Size([19346, 2])
We keep 3.70e+06/1.53e+08 =  2% of the original kernel matrix.

torch.Size([416239, 2])
We keep 9.15e+08/8.14e+10 =  1% of the original kernel matrix.

torch.Size([102911, 2])
We keep 5.93e+07/4.10e+09 =  1% of the original kernel matrix.

torch.Size([14983, 2])
We keep 2.86e+06/9.20e+07 =  3% of the original kernel matrix.

torch.Size([19139, 2])
We keep 3.33e+06/1.38e+08 =  2% of the original kernel matrix.

torch.Size([13738, 2])
We keep 2.23e+06/7.06e+07 =  3% of the original kernel matrix.

torch.Size([18228, 2])
We keep 2.99e+06/1.21e+08 =  2% of the original kernel matrix.

torch.Size([9468, 2])
We keep 5.21e+06/5.64e+07 =  9% of the original kernel matrix.

torch.Size([14448, 2])
We keep 2.48e+06/1.08e+08 =  2% of the original kernel matrix.

torch.Size([80147, 2])
We keep 4.51e+07/2.76e+09 =  1% of the original kernel matrix.

torch.Size([43030, 2])
We keep 1.34e+07/7.54e+08 =  1% of the original kernel matrix.

torch.Size([35868, 2])
We keep 4.45e+07/1.11e+09 =  3% of the original kernel matrix.

torch.Size([27709, 2])
We keep 9.52e+06/4.79e+08 =  1% of the original kernel matrix.

torch.Size([12378, 2])
We keep 1.73e+06/5.11e+07 =  3% of the original kernel matrix.

torch.Size([17257, 2])
We keep 2.58e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([60230, 2])
We keep 7.07e+07/1.92e+09 =  3% of the original kernel matrix.

torch.Size([36569, 2])
We keep 1.15e+07/6.29e+08 =  1% of the original kernel matrix.

torch.Size([2748726, 2])
We keep 2.00e+10/2.55e+12 =  0% of the original kernel matrix.

torch.Size([277255, 2])
We keep 3.02e+08/2.29e+10 =  1% of the original kernel matrix.

torch.Size([8019, 2])
We keep 1.17e+06/1.82e+07 =  6% of the original kernel matrix.

torch.Size([13830, 2])
We keep 1.77e+06/6.13e+07 =  2% of the original kernel matrix.

torch.Size([371037, 2])
We keep 1.22e+09/6.39e+10 =  1% of the original kernel matrix.

torch.Size([97044, 2])
We keep 5.54e+07/3.63e+09 =  1% of the original kernel matrix.

torch.Size([20166, 2])
We keep 1.21e+07/2.23e+08 =  5% of the original kernel matrix.

torch.Size([22090, 2])
We keep 4.97e+06/2.14e+08 =  2% of the original kernel matrix.

torch.Size([11379, 2])
We keep 1.34e+06/3.98e+07 =  3% of the original kernel matrix.

torch.Size([16524, 2])
We keep 2.34e+06/9.05e+07 =  2% of the original kernel matrix.

torch.Size([17990, 2])
We keep 5.93e+06/1.53e+08 =  3% of the original kernel matrix.

torch.Size([19692, 2])
We keep 3.82e+06/1.78e+08 =  2% of the original kernel matrix.

torch.Size([80456, 2])
We keep 8.27e+07/3.39e+09 =  2% of the original kernel matrix.

torch.Size([43101, 2])
We keep 1.50e+07/8.35e+08 =  1% of the original kernel matrix.

torch.Size([96077, 2])
We keep 8.30e+07/4.37e+09 =  1% of the original kernel matrix.

torch.Size([47911, 2])
We keep 1.67e+07/9.49e+08 =  1% of the original kernel matrix.

torch.Size([2220, 2])
We keep 6.57e+04/7.26e+05 =  9% of the original kernel matrix.

torch.Size([8061, 2])
We keep 5.50e+05/1.22e+07 =  4% of the original kernel matrix.

torch.Size([6457, 2])
We keep 6.24e+05/1.05e+07 =  5% of the original kernel matrix.

torch.Size([12341, 2])
We keep 1.45e+06/4.66e+07 =  3% of the original kernel matrix.

torch.Size([3626, 2])
We keep 2.17e+05/2.70e+06 =  8% of the original kernel matrix.

torch.Size([9662, 2])
We keep 8.87e+05/2.36e+07 =  3% of the original kernel matrix.

torch.Size([6335, 2])
We keep 4.43e+05/9.21e+06 =  4% of the original kernel matrix.

torch.Size([12484, 2])
We keep 1.34e+06/4.36e+07 =  3% of the original kernel matrix.

torch.Size([951, 2])
We keep 6.22e+04/3.11e+05 = 19% of the original kernel matrix.

torch.Size([5130, 2])
We keep 4.41e+05/8.01e+06 =  5% of the original kernel matrix.

torch.Size([1396, 2])
We keep 3.72e+04/3.24e+05 = 11% of the original kernel matrix.

torch.Size([6565, 2])
We keep 4.32e+05/8.17e+06 =  5% of the original kernel matrix.

torch.Size([3123, 2])
We keep 1.80e+05/2.09e+06 =  8% of the original kernel matrix.

torch.Size([8838, 2])
We keep 8.08e+05/2.07e+07 =  3% of the original kernel matrix.

torch.Size([2015, 2])
We keep 5.39e+04/6.64e+05 =  8% of the original kernel matrix.

torch.Size([7910, 2])
We keep 5.12e+05/1.17e+07 =  4% of the original kernel matrix.

torch.Size([2064, 2])
We keep 6.23e+04/6.79e+05 =  9% of the original kernel matrix.

torch.Size([7918, 2])
We keep 5.39e+05/1.18e+07 =  4% of the original kernel matrix.

torch.Size([8517, 2])
We keep 1.59e+06/2.71e+07 =  5% of the original kernel matrix.

torch.Size([14320, 2])
We keep 2.10e+06/7.47e+07 =  2% of the original kernel matrix.

torch.Size([5714, 2])
We keep 4.70e+05/7.93e+06 =  5% of the original kernel matrix.

torch.Size([11652, 2])
We keep 1.31e+06/4.04e+07 =  3% of the original kernel matrix.

torch.Size([5963, 2])
We keep 5.50e+05/8.47e+06 =  6% of the original kernel matrix.

torch.Size([11888, 2])
We keep 1.35e+06/4.18e+07 =  3% of the original kernel matrix.

torch.Size([10040, 2])
We keep 2.30e+06/3.69e+07 =  6% of the original kernel matrix.

torch.Size([15374, 2])
We keep 2.38e+06/8.72e+07 =  2% of the original kernel matrix.

torch.Size([15749, 2])
We keep 4.28e+06/9.68e+07 =  4% of the original kernel matrix.

torch.Size([19523, 2])
We keep 3.45e+06/1.41e+08 =  2% of the original kernel matrix.

torch.Size([2506, 2])
We keep 1.17e+05/1.31e+06 =  8% of the original kernel matrix.

torch.Size([8230, 2])
We keep 6.97e+05/1.64e+07 =  4% of the original kernel matrix.

torch.Size([6175, 2])
We keep 6.95e+05/1.05e+07 =  6% of the original kernel matrix.

torch.Size([12132, 2])
We keep 1.47e+06/4.66e+07 =  3% of the original kernel matrix.

torch.Size([3368, 2])
We keep 2.30e+05/2.36e+06 =  9% of the original kernel matrix.

torch.Size([9189, 2])
We keep 8.56e+05/2.21e+07 =  3% of the original kernel matrix.

torch.Size([9567, 2])
We keep 1.61e+06/3.37e+07 =  4% of the original kernel matrix.

torch.Size([15035, 2])
We keep 2.31e+06/8.33e+07 =  2% of the original kernel matrix.

torch.Size([6933, 2])
We keep 1.53e+06/1.82e+07 =  8% of the original kernel matrix.

torch.Size([12629, 2])
We keep 1.81e+06/6.13e+07 =  2% of the original kernel matrix.

torch.Size([2979, 2])
We keep 1.31e+05/1.78e+06 =  7% of the original kernel matrix.

torch.Size([9014, 2])
We keep 7.55e+05/1.92e+07 =  3% of the original kernel matrix.

torch.Size([5494, 2])
We keep 4.55e+05/6.62e+06 =  6% of the original kernel matrix.

torch.Size([11519, 2])
We keep 1.24e+06/3.69e+07 =  3% of the original kernel matrix.

torch.Size([4311, 2])
We keep 2.65e+05/4.17e+06 =  6% of the original kernel matrix.

torch.Size([10497, 2])
We keep 1.00e+06/2.93e+07 =  3% of the original kernel matrix.

torch.Size([3954, 2])
We keep 4.27e+05/3.97e+06 = 10% of the original kernel matrix.

torch.Size([9873, 2])
We keep 1.03e+06/2.86e+07 =  3% of the original kernel matrix.

torch.Size([14281, 2])
We keep 2.59e+06/7.44e+07 =  3% of the original kernel matrix.

torch.Size([18663, 2])
We keep 2.99e+06/1.24e+08 =  2% of the original kernel matrix.

torch.Size([3224, 2])
We keep 1.59e+05/2.05e+06 =  7% of the original kernel matrix.

torch.Size([9386, 2])
We keep 7.90e+05/2.06e+07 =  3% of the original kernel matrix.

torch.Size([16413, 2])
We keep 4.89e+06/1.27e+08 =  3% of the original kernel matrix.

torch.Size([20335, 2])
We keep 3.92e+06/1.62e+08 =  2% of the original kernel matrix.

torch.Size([8938, 2])
We keep 1.05e+06/2.30e+07 =  4% of the original kernel matrix.

torch.Size([14434, 2])
We keep 1.95e+06/6.89e+07 =  2% of the original kernel matrix.

torch.Size([5174, 2])
We keep 4.16e+05/6.46e+06 =  6% of the original kernel matrix.

torch.Size([11352, 2])
We keep 1.19e+06/3.65e+07 =  3% of the original kernel matrix.

torch.Size([4029, 2])
We keep 2.22e+05/3.05e+06 =  7% of the original kernel matrix.

torch.Size([10226, 2])
We keep 9.21e+05/2.51e+07 =  3% of the original kernel matrix.

torch.Size([3752, 2])
We keep 2.19e+05/2.87e+06 =  7% of the original kernel matrix.

torch.Size([9877, 2])
We keep 8.99e+05/2.43e+07 =  3% of the original kernel matrix.

torch.Size([6085, 2])
We keep 5.61e+05/9.32e+06 =  6% of the original kernel matrix.

torch.Size([11851, 2])
We keep 1.40e+06/4.38e+07 =  3% of the original kernel matrix.

torch.Size([6426, 2])
We keep 5.13e+05/9.90e+06 =  5% of the original kernel matrix.

torch.Size([12456, 2])
We keep 1.39e+06/4.52e+07 =  3% of the original kernel matrix.

torch.Size([4309, 2])
We keep 2.40e+05/3.43e+06 =  6% of the original kernel matrix.

torch.Size([10313, 2])
We keep 9.65e+05/2.66e+07 =  3% of the original kernel matrix.

torch.Size([7849, 2])
We keep 1.55e+06/2.49e+07 =  6% of the original kernel matrix.

torch.Size([13437, 2])
We keep 1.99e+06/7.17e+07 =  2% of the original kernel matrix.

torch.Size([5753, 2])
We keep 5.64e+05/8.83e+06 =  6% of the original kernel matrix.

torch.Size([11447, 2])
We keep 1.37e+06/4.26e+07 =  3% of the original kernel matrix.

torch.Size([4408, 2])
We keep 2.57e+05/3.90e+06 =  6% of the original kernel matrix.

torch.Size([10496, 2])
We keep 1.01e+06/2.84e+07 =  3% of the original kernel matrix.

torch.Size([2482, 2])
We keep 2.03e+05/1.74e+06 = 11% of the original kernel matrix.

torch.Size([7913, 2])
We keep 7.82e+05/1.89e+07 =  4% of the original kernel matrix.

torch.Size([6235, 2])
We keep 6.00e+05/9.58e+06 =  6% of the original kernel matrix.

torch.Size([12072, 2])
We keep 1.41e+06/4.44e+07 =  3% of the original kernel matrix.

torch.Size([6774, 2])
We keep 8.64e+05/1.35e+07 =  6% of the original kernel matrix.

torch.Size([12681, 2])
We keep 1.60e+06/5.28e+07 =  3% of the original kernel matrix.

torch.Size([2525, 2])
We keep 9.51e+04/1.25e+06 =  7% of the original kernel matrix.

torch.Size([8582, 2])
We keep 6.53e+05/1.61e+07 =  4% of the original kernel matrix.

torch.Size([1104, 2])
We keep 2.28e+04/1.80e+05 = 12% of the original kernel matrix.

torch.Size([6201, 2])
We keep 3.50e+05/6.09e+06 =  5% of the original kernel matrix.

torch.Size([6344, 2])
We keep 5.03e+05/9.08e+06 =  5% of the original kernel matrix.

torch.Size([12248, 2])
We keep 1.35e+06/4.33e+07 =  3% of the original kernel matrix.

torch.Size([12054, 2])
We keep 2.24e+06/5.98e+07 =  3% of the original kernel matrix.

torch.Size([16696, 2])
We keep 2.82e+06/1.11e+08 =  2% of the original kernel matrix.

torch.Size([4413, 2])
We keep 2.36e+05/3.54e+06 =  6% of the original kernel matrix.

torch.Size([10575, 2])
We keep 9.68e+05/2.70e+07 =  3% of the original kernel matrix.

torch.Size([2447, 2])
We keep 1.02e+05/1.08e+06 =  9% of the original kernel matrix.

torch.Size([8209, 2])
We keep 6.54e+05/1.49e+07 =  4% of the original kernel matrix.

torch.Size([9333, 2])
We keep 1.79e+06/3.13e+07 =  5% of the original kernel matrix.

torch.Size([15009, 2])
We keep 2.26e+06/8.03e+07 =  2% of the original kernel matrix.

torch.Size([2473, 2])
We keep 1.09e+05/1.22e+06 =  8% of the original kernel matrix.

torch.Size([8225, 2])
We keep 6.82e+05/1.58e+07 =  4% of the original kernel matrix.

torch.Size([4082, 2])
We keep 2.84e+05/3.95e+06 =  7% of the original kernel matrix.

torch.Size([10015, 2])
We keep 1.01e+06/2.85e+07 =  3% of the original kernel matrix.

torch.Size([1524, 2])
We keep 3.69e+04/3.55e+05 = 10% of the original kernel matrix.

torch.Size([6923, 2])
We keep 4.35e+05/8.56e+06 =  5% of the original kernel matrix.

torch.Size([4932, 2])
We keep 5.84e+05/6.42e+06 =  9% of the original kernel matrix.

torch.Size([10980, 2])
We keep 1.25e+06/3.64e+07 =  3% of the original kernel matrix.

torch.Size([1673, 2])
We keep 5.09e+04/4.60e+05 = 11% of the original kernel matrix.

torch.Size([7103, 2])
We keep 4.79e+05/9.73e+06 =  4% of the original kernel matrix.

torch.Size([8180, 2])
We keep 9.59e+05/1.78e+07 =  5% of the original kernel matrix.

torch.Size([13735, 2])
We keep 1.78e+06/6.06e+07 =  2% of the original kernel matrix.

torch.Size([5143, 2])
We keep 3.30e+05/5.65e+06 =  5% of the original kernel matrix.

torch.Size([11281, 2])
We keep 1.14e+06/3.41e+07 =  3% of the original kernel matrix.

torch.Size([7553, 2])
We keep 7.97e+05/1.52e+07 =  5% of the original kernel matrix.

torch.Size([13266, 2])
We keep 1.66e+06/5.60e+07 =  2% of the original kernel matrix.

torch.Size([2064, 2])
We keep 5.78e+04/6.81e+05 =  8% of the original kernel matrix.

torch.Size([7850, 2])
We keep 5.41e+05/1.18e+07 =  4% of the original kernel matrix.

torch.Size([2316, 2])
We keep 1.03e+05/9.64e+05 = 10% of the original kernel matrix.

torch.Size([8137, 2])
We keep 6.19e+05/1.41e+07 =  4% of the original kernel matrix.

torch.Size([7698, 2])
We keep 7.69e+05/1.51e+07 =  5% of the original kernel matrix.

torch.Size([13534, 2])
We keep 1.66e+06/5.58e+07 =  2% of the original kernel matrix.

torch.Size([6992, 2])
We keep 9.42e+05/1.40e+07 =  6% of the original kernel matrix.

torch.Size([12693, 2])
We keep 1.64e+06/5.38e+07 =  3% of the original kernel matrix.

torch.Size([5627, 2])
We keep 4.21e+05/7.06e+06 =  5% of the original kernel matrix.

torch.Size([11592, 2])
We keep 1.25e+06/3.81e+07 =  3% of the original kernel matrix.

torch.Size([4923, 2])
We keep 4.19e+05/6.09e+06 =  6% of the original kernel matrix.

torch.Size([10908, 2])
We keep 1.19e+06/3.54e+07 =  3% of the original kernel matrix.

torch.Size([7190, 2])
We keep 1.13e+06/1.66e+07 =  6% of the original kernel matrix.

torch.Size([12826, 2])
We keep 1.75e+06/5.85e+07 =  2% of the original kernel matrix.

torch.Size([12490, 2])
We keep 2.24e+06/5.36e+07 =  4% of the original kernel matrix.

torch.Size([17240, 2])
We keep 2.70e+06/1.05e+08 =  2% of the original kernel matrix.

torch.Size([5282, 2])
We keep 3.17e+05/5.61e+06 =  5% of the original kernel matrix.

torch.Size([11305, 2])
We keep 1.14e+06/3.40e+07 =  3% of the original kernel matrix.

torch.Size([15280, 2])
We keep 3.28e+06/9.15e+07 =  3% of the original kernel matrix.

torch.Size([19098, 2])
We keep 3.34e+06/1.37e+08 =  2% of the original kernel matrix.

torch.Size([9286, 2])
We keep 1.50e+06/2.77e+07 =  5% of the original kernel matrix.

torch.Size([14849, 2])
We keep 2.13e+06/7.56e+07 =  2% of the original kernel matrix.

torch.Size([7212, 2])
We keep 7.16e+05/1.20e+07 =  5% of the original kernel matrix.

torch.Size([13102, 2])
We keep 1.53e+06/4.98e+07 =  3% of the original kernel matrix.

torch.Size([2259, 2])
We keep 9.38e+04/9.35e+05 = 10% of the original kernel matrix.

torch.Size([8064, 2])
We keep 6.20e+05/1.39e+07 =  4% of the original kernel matrix.

torch.Size([4674, 2])
We keep 3.30e+05/5.15e+06 =  6% of the original kernel matrix.

torch.Size([10613, 2])
We keep 1.10e+06/3.26e+07 =  3% of the original kernel matrix.

torch.Size([3626, 2])
We keep 3.11e+05/3.23e+06 =  9% of the original kernel matrix.

torch.Size([9521, 2])
We keep 9.69e+05/2.58e+07 =  3% of the original kernel matrix.

torch.Size([7848, 2])
We keep 7.36e+05/1.56e+07 =  4% of the original kernel matrix.

torch.Size([13588, 2])
We keep 1.67e+06/5.66e+07 =  2% of the original kernel matrix.

torch.Size([1086, 2])
We keep 1.46e+04/1.15e+05 = 12% of the original kernel matrix.

torch.Size([6385, 2])
We keep 3.03e+05/4.87e+06 =  6% of the original kernel matrix.

torch.Size([3272, 2])
We keep 1.72e+05/2.20e+06 =  7% of the original kernel matrix.

torch.Size([9311, 2])
We keep 8.22e+05/2.13e+07 =  3% of the original kernel matrix.

torch.Size([2381, 2])
We keep 1.03e+05/1.06e+06 =  9% of the original kernel matrix.

torch.Size([8181, 2])
We keep 6.41e+05/1.48e+07 =  4% of the original kernel matrix.

torch.Size([2312, 2])
We keep 8.89e+04/9.68e+05 =  9% of the original kernel matrix.

torch.Size([7984, 2])
We keep 6.10e+05/1.41e+07 =  4% of the original kernel matrix.

torch.Size([3304, 2])
We keep 2.67e+05/2.91e+06 =  9% of the original kernel matrix.

torch.Size([9056, 2])
We keep 9.23e+05/2.45e+07 =  3% of the original kernel matrix.

torch.Size([3257, 2])
We keep 1.96e+05/2.15e+06 =  9% of the original kernel matrix.

torch.Size([9233, 2])
We keep 8.36e+05/2.10e+07 =  3% of the original kernel matrix.

torch.Size([5687, 2])
We keep 5.16e+05/8.56e+06 =  6% of the original kernel matrix.

torch.Size([11845, 2])
We keep 1.33e+06/4.20e+07 =  3% of the original kernel matrix.

torch.Size([5591, 2])
We keep 5.46e+05/8.35e+06 =  6% of the original kernel matrix.

torch.Size([11505, 2])
We keep 1.33e+06/4.15e+07 =  3% of the original kernel matrix.

torch.Size([2924, 2])
We keep 1.50e+05/1.73e+06 =  8% of the original kernel matrix.

torch.Size([8614, 2])
We keep 7.48e+05/1.89e+07 =  3% of the original kernel matrix.

torch.Size([2962, 2])
We keep 1.23e+05/1.61e+06 =  7% of the original kernel matrix.

torch.Size([8916, 2])
We keep 7.15e+05/1.82e+07 =  3% of the original kernel matrix.

torch.Size([2242, 2])
We keep 7.63e+04/8.35e+05 =  9% of the original kernel matrix.

torch.Size([8004, 2])
We keep 5.85e+05/1.31e+07 =  4% of the original kernel matrix.

torch.Size([9577, 2])
We keep 1.28e+06/3.07e+07 =  4% of the original kernel matrix.

torch.Size([15189, 2])
We keep 2.14e+06/7.95e+07 =  2% of the original kernel matrix.

torch.Size([1833, 2])
We keep 7.26e+04/6.56e+05 = 11% of the original kernel matrix.

torch.Size([7222, 2])
We keep 5.45e+05/1.16e+07 =  4% of the original kernel matrix.

torch.Size([20343, 2])
We keep 5.02e+06/1.79e+08 =  2% of the original kernel matrix.

torch.Size([22543, 2])
We keep 4.41e+06/1.92e+08 =  2% of the original kernel matrix.

torch.Size([3206, 2])
We keep 1.58e+05/2.08e+06 =  7% of the original kernel matrix.

torch.Size([9189, 2])
We keep 8.08e+05/2.07e+07 =  3% of the original kernel matrix.

torch.Size([3061, 2])
We keep 1.81e+05/2.06e+06 =  8% of the original kernel matrix.

torch.Size([8848, 2])
We keep 8.14e+05/2.06e+07 =  3% of the original kernel matrix.

torch.Size([5055, 2])
We keep 3.10e+05/5.94e+06 =  5% of the original kernel matrix.

torch.Size([11209, 2])
We keep 1.11e+06/3.50e+07 =  3% of the original kernel matrix.

torch.Size([3410, 2])
We keep 1.71e+05/2.28e+06 =  7% of the original kernel matrix.

torch.Size([9437, 2])
We keep 8.41e+05/2.17e+07 =  3% of the original kernel matrix.

torch.Size([7799, 2])
We keep 8.34e+05/1.59e+07 =  5% of the original kernel matrix.

torch.Size([13506, 2])
We keep 1.69e+06/5.72e+07 =  2% of the original kernel matrix.

torch.Size([1918, 2])
We keep 5.64e+04/6.35e+05 =  8% of the original kernel matrix.

torch.Size([7611, 2])
We keep 5.36e+05/1.14e+07 =  4% of the original kernel matrix.

torch.Size([5265, 2])
We keep 3.72e+05/5.94e+06 =  6% of the original kernel matrix.

torch.Size([11334, 2])
We keep 1.17e+06/3.50e+07 =  3% of the original kernel matrix.

torch.Size([20503, 2])
We keep 7.75e+06/1.93e+08 =  4% of the original kernel matrix.

torch.Size([22737, 2])
We keep 4.67e+06/1.99e+08 =  2% of the original kernel matrix.

torch.Size([4174, 2])
We keep 2.58e+05/3.71e+06 =  6% of the original kernel matrix.

torch.Size([10318, 2])
We keep 1.00e+06/2.77e+07 =  3% of the original kernel matrix.

torch.Size([10450, 2])
We keep 1.76e+06/3.81e+07 =  4% of the original kernel matrix.

torch.Size([15778, 2])
We keep 2.38e+06/8.86e+07 =  2% of the original kernel matrix.

torch.Size([2191, 2])
We keep 6.64e+04/7.50e+05 =  8% of the original kernel matrix.

torch.Size([8161, 2])
We keep 5.49e+05/1.24e+07 =  4% of the original kernel matrix.

torch.Size([7318, 2])
We keep 8.21e+05/1.45e+07 =  5% of the original kernel matrix.

torch.Size([13091, 2])
We keep 1.66e+06/5.47e+07 =  3% of the original kernel matrix.

torch.Size([1417, 2])
We keep 8.13e+04/5.07e+05 = 16% of the original kernel matrix.

torch.Size([6542, 2])
We keep 5.05e+05/1.02e+07 =  4% of the original kernel matrix.

torch.Size([4317, 2])
We keep 2.72e+05/3.93e+06 =  6% of the original kernel matrix.

torch.Size([10340, 2])
We keep 1.01e+06/2.85e+07 =  3% of the original kernel matrix.

torch.Size([4190, 2])
We keep 2.50e+05/3.72e+06 =  6% of the original kernel matrix.

torch.Size([10260, 2])
We keep 9.68e+05/2.77e+07 =  3% of the original kernel matrix.

torch.Size([2600, 2])
We keep 1.37e+05/1.38e+06 =  9% of the original kernel matrix.

torch.Size([8405, 2])
We keep 7.10e+05/1.69e+07 =  4% of the original kernel matrix.

torch.Size([2315, 2])
We keep 1.35e+05/1.14e+06 = 11% of the original kernel matrix.

torch.Size([7942, 2])
We keep 6.69e+05/1.53e+07 =  4% of the original kernel matrix.

torch.Size([2382, 2])
We keep 1.00e+05/1.03e+06 =  9% of the original kernel matrix.

torch.Size([8092, 2])
We keep 6.40e+05/1.45e+07 =  4% of the original kernel matrix.

torch.Size([3823, 2])
We keep 1.78e+05/2.86e+06 =  6% of the original kernel matrix.

torch.Size([9915, 2])
We keep 8.71e+05/2.43e+07 =  3% of the original kernel matrix.

torch.Size([3774, 2])
We keep 2.28e+05/2.90e+06 =  7% of the original kernel matrix.

torch.Size([9864, 2])
We keep 9.26e+05/2.45e+07 =  3% of the original kernel matrix.

torch.Size([1699, 2])
We keep 4.49e+04/4.20e+05 = 10% of the original kernel matrix.

torch.Size([7343, 2])
We keep 4.67e+05/9.30e+06 =  5% of the original kernel matrix.

torch.Size([9357, 2])
We keep 1.65e+06/3.29e+07 =  5% of the original kernel matrix.

torch.Size([14719, 2])
We keep 2.29e+06/8.23e+07 =  2% of the original kernel matrix.

torch.Size([27176, 2])
We keep 1.09e+07/3.18e+08 =  3% of the original kernel matrix.

torch.Size([25306, 2])
We keep 5.54e+06/2.56e+08 =  2% of the original kernel matrix.

torch.Size([12024, 2])
We keep 2.49e+06/5.28e+07 =  4% of the original kernel matrix.

torch.Size([17025, 2])
We keep 2.73e+06/1.04e+08 =  2% of the original kernel matrix.

torch.Size([3535, 2])
We keep 1.57e+05/2.22e+06 =  7% of the original kernel matrix.

torch.Size([9610, 2])
We keep 8.24e+05/2.14e+07 =  3% of the original kernel matrix.

torch.Size([3325, 2])
We keep 1.59e+05/2.01e+06 =  7% of the original kernel matrix.

torch.Size([9477, 2])
We keep 7.89e+05/2.03e+07 =  3% of the original kernel matrix.

torch.Size([2346, 2])
We keep 1.34e+05/1.15e+06 = 11% of the original kernel matrix.

torch.Size([8114, 2])
We keep 6.70e+05/1.54e+07 =  4% of the original kernel matrix.

torch.Size([2636, 2])
We keep 9.60e+04/1.09e+06 =  8% of the original kernel matrix.

torch.Size([8560, 2])
We keep 6.46e+05/1.50e+07 =  4% of the original kernel matrix.

torch.Size([3542, 2])
We keep 1.96e+05/2.42e+06 =  8% of the original kernel matrix.

torch.Size([9734, 2])
We keep 8.59e+05/2.24e+07 =  3% of the original kernel matrix.

torch.Size([2245, 2])
We keep 8.81e+04/1.01e+06 =  8% of the original kernel matrix.

torch.Size([7911, 2])
We keep 6.28e+05/1.44e+07 =  4% of the original kernel matrix.

torch.Size([8574, 2])
We keep 1.56e+06/2.44e+07 =  6% of the original kernel matrix.

torch.Size([14308, 2])
We keep 2.02e+06/7.09e+07 =  2% of the original kernel matrix.

torch.Size([5910, 2])
We keep 4.63e+05/7.70e+06 =  6% of the original kernel matrix.

torch.Size([11937, 2])
We keep 1.29e+06/3.98e+07 =  3% of the original kernel matrix.

torch.Size([8761, 2])
We keep 2.59e+06/3.48e+07 =  7% of the original kernel matrix.

torch.Size([14284, 2])
We keep 2.32e+06/8.47e+07 =  2% of the original kernel matrix.

torch.Size([5522, 2])
We keep 4.60e+05/7.18e+06 =  6% of the original kernel matrix.

torch.Size([11598, 2])
We keep 1.26e+06/3.85e+07 =  3% of the original kernel matrix.

torch.Size([15503, 2])
We keep 4.54e+06/1.08e+08 =  4% of the original kernel matrix.

torch.Size([19223, 2])
We keep 3.64e+06/1.49e+08 =  2% of the original kernel matrix.

torch.Size([4452, 2])
We keep 2.19e+05/3.48e+06 =  6% of the original kernel matrix.

torch.Size([10678, 2])
We keep 9.61e+05/2.68e+07 =  3% of the original kernel matrix.

torch.Size([3135, 2])
We keep 1.68e+05/1.90e+06 =  8% of the original kernel matrix.

torch.Size([9207, 2])
We keep 7.80e+05/1.98e+07 =  3% of the original kernel matrix.

torch.Size([7117, 2])
We keep 6.84e+05/1.26e+07 =  5% of the original kernel matrix.

torch.Size([13043, 2])
We keep 1.54e+06/5.10e+07 =  3% of the original kernel matrix.

torch.Size([1179, 2])
We keep 2.23e+04/1.87e+05 = 11% of the original kernel matrix.

torch.Size([6457, 2])
We keep 3.51e+05/6.22e+06 =  5% of the original kernel matrix.

torch.Size([9023, 2])
We keep 1.36e+06/2.51e+07 =  5% of the original kernel matrix.

torch.Size([14664, 2])
We keep 2.06e+06/7.20e+07 =  2% of the original kernel matrix.

torch.Size([2794, 2])
We keep 1.41e+05/1.51e+06 =  9% of the original kernel matrix.

torch.Size([8534, 2])
We keep 7.34e+05/1.76e+07 =  4% of the original kernel matrix.

torch.Size([5670, 2])
We keep 5.18e+05/7.82e+06 =  6% of the original kernel matrix.

torch.Size([11657, 2])
We keep 1.31e+06/4.02e+07 =  3% of the original kernel matrix.

torch.Size([1548, 2])
We keep 5.88e+04/4.91e+05 = 11% of the original kernel matrix.

torch.Size([6756, 2])
We keep 5.02e+05/1.01e+07 =  4% of the original kernel matrix.

torch.Size([4512, 2])
We keep 3.83e+05/5.12e+06 =  7% of the original kernel matrix.

torch.Size([10434, 2])
We keep 1.13e+06/3.25e+07 =  3% of the original kernel matrix.

torch.Size([4869, 2])
We keep 3.11e+05/5.30e+06 =  5% of the original kernel matrix.

torch.Size([10874, 2])
We keep 1.12e+06/3.30e+07 =  3% of the original kernel matrix.

torch.Size([3619, 2])
We keep 1.71e+05/2.89e+06 =  5% of the original kernel matrix.

torch.Size([9852, 2])
We keep 8.54e+05/2.44e+07 =  3% of the original kernel matrix.

torch.Size([4769, 2])
We keep 3.51e+05/5.00e+06 =  7% of the original kernel matrix.

torch.Size([10667, 2])
We keep 1.11e+06/3.21e+07 =  3% of the original kernel matrix.

torch.Size([7116, 2])
We keep 6.97e+05/1.25e+07 =  5% of the original kernel matrix.

torch.Size([12959, 2])
We keep 1.56e+06/5.08e+07 =  3% of the original kernel matrix.

torch.Size([800, 2])
We keep 1.18e+04/7.90e+04 = 14% of the original kernel matrix.

torch.Size([5564, 2])
We keep 2.70e+05/4.03e+06 =  6% of the original kernel matrix.

torch.Size([1773, 2])
We keep 5.84e+04/5.87e+05 =  9% of the original kernel matrix.

torch.Size([7247, 2])
We keep 5.27e+05/1.10e+07 =  4% of the original kernel matrix.

torch.Size([3036, 2])
We keep 1.46e+05/1.87e+06 =  7% of the original kernel matrix.

torch.Size([8881, 2])
We keep 7.73e+05/1.96e+07 =  3% of the original kernel matrix.

torch.Size([10354, 2])
We keep 1.49e+06/3.49e+07 =  4% of the original kernel matrix.

torch.Size([15658, 2])
We keep 2.30e+06/8.48e+07 =  2% of the original kernel matrix.

torch.Size([3204, 2])
We keep 1.52e+05/2.02e+06 =  7% of the original kernel matrix.

torch.Size([9280, 2])
We keep 7.90e+05/2.04e+07 =  3% of the original kernel matrix.

torch.Size([6845, 2])
We keep 6.83e+05/1.16e+07 =  5% of the original kernel matrix.

torch.Size([12909, 2])
We keep 1.48e+06/4.90e+07 =  3% of the original kernel matrix.

torch.Size([6697, 2])
We keep 6.84e+05/1.30e+07 =  5% of the original kernel matrix.

torch.Size([12776, 2])
We keep 1.51e+06/5.17e+07 =  2% of the original kernel matrix.

torch.Size([5813, 2])
We keep 4.91e+05/7.76e+06 =  6% of the original kernel matrix.

torch.Size([11803, 2])
We keep 1.31e+06/4.00e+07 =  3% of the original kernel matrix.

torch.Size([6335, 2])
We keep 5.47e+05/9.85e+06 =  5% of the original kernel matrix.

torch.Size([12299, 2])
We keep 1.41e+06/4.51e+07 =  3% of the original kernel matrix.

torch.Size([6737, 2])
We keep 9.98e+05/1.31e+07 =  7% of the original kernel matrix.

torch.Size([12444, 2])
We keep 1.60e+06/5.20e+07 =  3% of the original kernel matrix.

torch.Size([3202, 2])
We keep 1.93e+05/2.09e+06 =  9% of the original kernel matrix.

torch.Size([9066, 2])
We keep 8.15e+05/2.08e+07 =  3% of the original kernel matrix.

torch.Size([8879, 2])
We keep 8.71e+05/2.01e+07 =  4% of the original kernel matrix.

torch.Size([14606, 2])
We keep 1.80e+06/6.43e+07 =  2% of the original kernel matrix.

torch.Size([4143, 2])
We keep 3.20e+05/3.78e+06 =  8% of the original kernel matrix.

torch.Size([10118, 2])
We keep 1.02e+06/2.79e+07 =  3% of the original kernel matrix.

torch.Size([6889, 2])
We keep 9.18e+05/1.31e+07 =  7% of the original kernel matrix.

torch.Size([12789, 2])
We keep 1.57e+06/5.19e+07 =  3% of the original kernel matrix.

torch.Size([2776, 2])
We keep 1.14e+05/1.43e+06 =  7% of the original kernel matrix.

torch.Size([8724, 2])
We keep 7.04e+05/1.72e+07 =  4% of the original kernel matrix.

torch.Size([8856, 2])
We keep 1.17e+06/2.27e+07 =  5% of the original kernel matrix.

torch.Size([14460, 2])
We keep 1.95e+06/6.84e+07 =  2% of the original kernel matrix.

torch.Size([7298, 2])
We keep 5.84e+05/1.23e+07 =  4% of the original kernel matrix.

torch.Size([13267, 2])
We keep 1.52e+06/5.04e+07 =  3% of the original kernel matrix.

torch.Size([2852, 2])
We keep 1.18e+05/1.44e+06 =  8% of the original kernel matrix.

torch.Size([8862, 2])
We keep 7.08e+05/1.72e+07 =  4% of the original kernel matrix.

torch.Size([5673, 2])
We keep 6.56e+05/9.14e+06 =  7% of the original kernel matrix.

torch.Size([11479, 2])
We keep 1.39e+06/4.34e+07 =  3% of the original kernel matrix.

torch.Size([5742, 2])
We keep 4.11e+05/7.43e+06 =  5% of the original kernel matrix.

torch.Size([11754, 2])
We keep 1.25e+06/3.91e+07 =  3% of the original kernel matrix.

torch.Size([18211, 2])
We keep 6.90e+06/1.47e+08 =  4% of the original kernel matrix.

torch.Size([20073, 2])
We keep 3.87e+06/1.74e+08 =  2% of the original kernel matrix.

torch.Size([3446, 2])
We keep 2.14e+05/2.54e+06 =  8% of the original kernel matrix.

torch.Size([9280, 2])
We keep 8.76e+05/2.29e+07 =  3% of the original kernel matrix.

torch.Size([1410, 2])
We keep 2.50e+04/2.63e+05 =  9% of the original kernel matrix.

torch.Size([6942, 2])
We keep 3.83e+05/7.36e+06 =  5% of the original kernel matrix.

torch.Size([4821, 2])
We keep 3.02e+05/4.80e+06 =  6% of the original kernel matrix.

torch.Size([10996, 2])
We keep 1.08e+06/3.14e+07 =  3% of the original kernel matrix.

torch.Size([3097, 2])
We keep 1.19e+05/1.62e+06 =  7% of the original kernel matrix.

torch.Size([9279, 2])
We keep 7.31e+05/1.83e+07 =  3% of the original kernel matrix.

torch.Size([3931, 2])
We keep 3.41e+05/3.62e+06 =  9% of the original kernel matrix.

torch.Size([9864, 2])
We keep 1.00e+06/2.73e+07 =  3% of the original kernel matrix.

torch.Size([3402, 2])
We keep 1.75e+05/2.28e+06 =  7% of the original kernel matrix.

torch.Size([9437, 2])
We keep 8.28e+05/2.17e+07 =  3% of the original kernel matrix.

torch.Size([2687, 2])
We keep 1.15e+05/1.44e+06 =  7% of the original kernel matrix.

torch.Size([8577, 2])
We keep 6.99e+05/1.72e+07 =  4% of the original kernel matrix.

torch.Size([2225, 2])
We keep 9.77e+04/1.05e+06 =  9% of the original kernel matrix.

torch.Size([8015, 2])
We keep 6.26e+05/1.47e+07 =  4% of the original kernel matrix.

torch.Size([6372, 2])
We keep 5.60e+05/9.58e+06 =  5% of the original kernel matrix.

torch.Size([12260, 2])
We keep 1.40e+06/4.44e+07 =  3% of the original kernel matrix.

torch.Size([1748, 2])
We keep 4.82e+04/5.01e+05 =  9% of the original kernel matrix.

torch.Size([7499, 2])
We keep 4.91e+05/1.02e+07 =  4% of the original kernel matrix.

torch.Size([12459, 2])
We keep 1.63e+06/5.12e+07 =  3% of the original kernel matrix.

torch.Size([17346, 2])
We keep 2.52e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([3387, 2])
We keep 1.60e+05/2.18e+06 =  7% of the original kernel matrix.

torch.Size([9418, 2])
We keep 8.12e+05/2.12e+07 =  3% of the original kernel matrix.

torch.Size([29875, 2])
We keep 9.75e+06/3.78e+08 =  2% of the original kernel matrix.

torch.Size([26729, 2])
We keep 5.90e+06/2.79e+08 =  2% of the original kernel matrix.

torch.Size([6916, 2])
We keep 6.81e+05/1.16e+07 =  5% of the original kernel matrix.

torch.Size([12946, 2])
We keep 1.50e+06/4.88e+07 =  3% of the original kernel matrix.

torch.Size([5448, 2])
We keep 3.86e+05/6.60e+06 =  5% of the original kernel matrix.

torch.Size([11566, 2])
We keep 1.22e+06/3.69e+07 =  3% of the original kernel matrix.

torch.Size([1727, 2])
We keep 4.81e+04/4.73e+05 = 10% of the original kernel matrix.

torch.Size([7216, 2])
We keep 4.88e+05/9.88e+06 =  4% of the original kernel matrix.

torch.Size([8666, 2])
We keep 2.10e+06/2.94e+07 =  7% of the original kernel matrix.

torch.Size([14507, 2])
We keep 2.23e+06/7.79e+07 =  2% of the original kernel matrix.

torch.Size([6194, 2])
We keep 7.38e+05/1.05e+07 =  7% of the original kernel matrix.

torch.Size([12063, 2])
We keep 1.46e+06/4.66e+07 =  3% of the original kernel matrix.

torch.Size([4936, 2])
We keep 4.42e+05/5.81e+06 =  7% of the original kernel matrix.

torch.Size([11026, 2])
We keep 1.17e+06/3.46e+07 =  3% of the original kernel matrix.

torch.Size([13045, 2])
We keep 3.67e+06/7.91e+07 =  4% of the original kernel matrix.

torch.Size([17910, 2])
We keep 3.26e+06/1.28e+08 =  2% of the original kernel matrix.

torch.Size([10253, 2])
We keep 1.60e+06/3.61e+07 =  4% of the original kernel matrix.

torch.Size([15420, 2])
We keep 2.34e+06/8.63e+07 =  2% of the original kernel matrix.

torch.Size([2567, 2])
We keep 1.04e+05/1.13e+06 =  9% of the original kernel matrix.

torch.Size([8386, 2])
We keep 6.53e+05/1.53e+07 =  4% of the original kernel matrix.

torch.Size([2872, 2])
We keep 1.51e+05/1.67e+06 =  9% of the original kernel matrix.

torch.Size([8599, 2])
We keep 7.59e+05/1.86e+07 =  4% of the original kernel matrix.

torch.Size([6445, 2])
We keep 5.70e+05/1.01e+07 =  5% of the original kernel matrix.

torch.Size([12399, 2])
We keep 1.43e+06/4.57e+07 =  3% of the original kernel matrix.

torch.Size([1591, 2])
We keep 4.60e+04/3.91e+05 = 11% of the original kernel matrix.

torch.Size([7210, 2])
We keep 4.51e+05/8.97e+06 =  5% of the original kernel matrix.

torch.Size([1586, 2])
We keep 4.87e+04/4.12e+05 = 11% of the original kernel matrix.

torch.Size([6922, 2])
We keep 4.70e+05/9.22e+06 =  5% of the original kernel matrix.

torch.Size([1750, 2])
We keep 4.53e+04/4.77e+05 =  9% of the original kernel matrix.

torch.Size([7483, 2])
We keep 4.79e+05/9.92e+06 =  4% of the original kernel matrix.

torch.Size([1506, 2])
We keep 2.99e+04/2.76e+05 = 10% of the original kernel matrix.

torch.Size([7138, 2])
We keep 4.05e+05/7.54e+06 =  5% of the original kernel matrix.

torch.Size([5877, 2])
We keep 7.36e+05/1.04e+07 =  7% of the original kernel matrix.

torch.Size([11514, 2])
We keep 1.48e+06/4.63e+07 =  3% of the original kernel matrix.

torch.Size([2628, 2])
We keep 1.01e+05/1.26e+06 =  7% of the original kernel matrix.

torch.Size([8572, 2])
We keep 6.71e+05/1.61e+07 =  4% of the original kernel matrix.

torch.Size([5041, 2])
We keep 1.20e+06/1.22e+07 =  9% of the original kernel matrix.

torch.Size([10680, 2])
We keep 1.58e+06/5.01e+07 =  3% of the original kernel matrix.

torch.Size([2657, 2])
We keep 1.02e+05/1.38e+06 =  7% of the original kernel matrix.

torch.Size([8790, 2])
We keep 6.45e+05/1.68e+07 =  3% of the original kernel matrix.

torch.Size([6904, 2])
We keep 7.10e+05/1.24e+07 =  5% of the original kernel matrix.

torch.Size([12730, 2])
We keep 1.55e+06/5.06e+07 =  3% of the original kernel matrix.

torch.Size([2819, 2])
We keep 1.79e+05/1.83e+06 =  9% of the original kernel matrix.

torch.Size([8616, 2])
We keep 7.86e+05/1.94e+07 =  4% of the original kernel matrix.

torch.Size([3896, 2])
We keep 2.61e+05/3.31e+06 =  7% of the original kernel matrix.

torch.Size([9959, 2])
We keep 9.61e+05/2.61e+07 =  3% of the original kernel matrix.

torch.Size([6796, 2])
We keep 8.62e+05/1.21e+07 =  7% of the original kernel matrix.

torch.Size([12617, 2])
We keep 1.56e+06/4.98e+07 =  3% of the original kernel matrix.

torch.Size([1993, 2])
We keep 8.28e+04/9.35e+05 =  8% of the original kernel matrix.

torch.Size([7495, 2])
We keep 6.03e+05/1.39e+07 =  4% of the original kernel matrix.

torch.Size([1856, 2])
We keep 6.69e+04/6.13e+05 = 10% of the original kernel matrix.

torch.Size([7403, 2])
We keep 5.32e+05/1.12e+07 =  4% of the original kernel matrix.

torch.Size([7580, 2])
We keep 9.24e+05/1.65e+07 =  5% of the original kernel matrix.

torch.Size([13279, 2])
We keep 1.75e+06/5.83e+07 =  2% of the original kernel matrix.

torch.Size([2247, 2])
We keep 1.08e+05/9.12e+05 = 11% of the original kernel matrix.

torch.Size([7782, 2])
We keep 6.12e+05/1.37e+07 =  4% of the original kernel matrix.

torch.Size([1844, 2])
We keep 5.02e+04/5.40e+05 =  9% of the original kernel matrix.

torch.Size([7603, 2])
We keep 5.00e+05/1.06e+07 =  4% of the original kernel matrix.

torch.Size([2891, 2])
We keep 1.59e+05/1.64e+06 =  9% of the original kernel matrix.

torch.Size([8723, 2])
We keep 7.33e+05/1.84e+07 =  3% of the original kernel matrix.

torch.Size([4887, 2])
We keep 4.21e+05/5.35e+06 =  7% of the original kernel matrix.

torch.Size([10880, 2])
We keep 1.14e+06/3.32e+07 =  3% of the original kernel matrix.

torch.Size([7814, 2])
We keep 7.90e+05/1.58e+07 =  4% of the original kernel matrix.

torch.Size([13682, 2])
We keep 1.67e+06/5.71e+07 =  2% of the original kernel matrix.

torch.Size([2160, 2])
We keep 9.22e+04/8.15e+05 = 11% of the original kernel matrix.

torch.Size([7880, 2])
We keep 5.93e+05/1.30e+07 =  4% of the original kernel matrix.

torch.Size([1818, 2])
We keep 7.98e+04/6.67e+05 = 11% of the original kernel matrix.

torch.Size([7298, 2])
We keep 5.52e+05/1.17e+07 =  4% of the original kernel matrix.

torch.Size([6272, 2])
We keep 5.12e+05/9.37e+06 =  5% of the original kernel matrix.

torch.Size([12400, 2])
We keep 1.36e+06/4.39e+07 =  3% of the original kernel matrix.

torch.Size([9691, 2])
We keep 1.54e+06/3.26e+07 =  4% of the original kernel matrix.

torch.Size([15337, 2])
We keep 2.24e+06/8.19e+07 =  2% of the original kernel matrix.

torch.Size([2170, 2])
We keep 7.61e+04/8.08e+05 =  9% of the original kernel matrix.

torch.Size([7895, 2])
We keep 5.79e+05/1.29e+07 =  4% of the original kernel matrix.

torch.Size([10015, 2])
We keep 1.30e+06/3.22e+07 =  4% of the original kernel matrix.

torch.Size([15381, 2])
We keep 2.20e+06/8.14e+07 =  2% of the original kernel matrix.

torch.Size([57762, 2])
We keep 1.58e+08/3.34e+09 =  4% of the original kernel matrix.

torch.Size([35387, 2])
We keep 1.52e+07/8.29e+08 =  1% of the original kernel matrix.

torch.Size([20237, 2])
We keep 1.17e+07/2.48e+08 =  4% of the original kernel matrix.

torch.Size([21959, 2])
We keep 4.49e+06/2.26e+08 =  1% of the original kernel matrix.

torch.Size([8742, 2])
We keep 1.09e+06/2.09e+07 =  5% of the original kernel matrix.

torch.Size([14530, 2])
We keep 1.85e+06/6.56e+07 =  2% of the original kernel matrix.

torch.Size([1261, 2])
We keep 2.93e+04/2.51e+05 = 11% of the original kernel matrix.

torch.Size([6293, 2])
We keep 3.87e+05/7.19e+06 =  5% of the original kernel matrix.

torch.Size([7140, 2])
We keep 6.03e+05/1.18e+07 =  5% of the original kernel matrix.

torch.Size([12981, 2])
We keep 1.51e+06/4.94e+07 =  3% of the original kernel matrix.

torch.Size([11767, 2])
We keep 2.45e+06/4.95e+07 =  4% of the original kernel matrix.

torch.Size([16713, 2])
We keep 2.56e+06/1.01e+08 =  2% of the original kernel matrix.

torch.Size([87122, 2])
We keep 1.94e+08/5.25e+09 =  3% of the original kernel matrix.

torch.Size([44573, 2])
We keep 1.84e+07/1.04e+09 =  1% of the original kernel matrix.

torch.Size([4898, 2])
We keep 2.94e+05/5.29e+06 =  5% of the original kernel matrix.

torch.Size([11164, 2])
We keep 1.08e+06/3.30e+07 =  3% of the original kernel matrix.

torch.Size([110729, 2])
We keep 1.81e+08/6.80e+09 =  2% of the original kernel matrix.

torch.Size([51106, 2])
We keep 2.06e+07/1.18e+09 =  1% of the original kernel matrix.

torch.Size([40247, 2])
We keep 4.77e+07/1.05e+09 =  4% of the original kernel matrix.

torch.Size([29199, 2])
We keep 8.85e+06/4.64e+08 =  1% of the original kernel matrix.

torch.Size([10601, 2])
We keep 5.12e+06/5.86e+07 =  8% of the original kernel matrix.

torch.Size([15702, 2])
We keep 2.75e+06/1.10e+08 =  2% of the original kernel matrix.

torch.Size([90959, 2])
We keep 1.75e+08/5.22e+09 =  3% of the original kernel matrix.

torch.Size([46756, 2])
We keep 1.74e+07/1.04e+09 =  1% of the original kernel matrix.

torch.Size([3516, 2])
We keep 1.65e+05/2.34e+06 =  7% of the original kernel matrix.

torch.Size([9782, 2])
We keep 8.41e+05/2.19e+07 =  3% of the original kernel matrix.

torch.Size([10123, 2])
We keep 1.40e+06/3.16e+07 =  4% of the original kernel matrix.

torch.Size([15460, 2])
We keep 2.17e+06/8.07e+07 =  2% of the original kernel matrix.

torch.Size([209654, 2])
We keep 3.93e+08/2.22e+10 =  1% of the original kernel matrix.

torch.Size([71117, 2])
We keep 3.46e+07/2.14e+09 =  1% of the original kernel matrix.

torch.Size([445663, 2])
We keep 1.18e+09/8.71e+10 =  1% of the original kernel matrix.

torch.Size([106470, 2])
We keep 6.25e+07/4.24e+09 =  1% of the original kernel matrix.

torch.Size([8637, 2])
We keep 1.38e+06/2.32e+07 =  5% of the original kernel matrix.

torch.Size([14319, 2])
We keep 1.97e+06/6.91e+07 =  2% of the original kernel matrix.

torch.Size([36263, 2])
We keep 1.64e+07/5.92e+08 =  2% of the original kernel matrix.

torch.Size([29161, 2])
We keep 7.12e+06/3.49e+08 =  2% of the original kernel matrix.

torch.Size([11219, 2])
We keep 2.41e+06/4.85e+07 =  4% of the original kernel matrix.

torch.Size([16201, 2])
We keep 2.62e+06/1.00e+08 =  2% of the original kernel matrix.

torch.Size([120411, 2])
We keep 3.76e+08/8.30e+09 =  4% of the original kernel matrix.

torch.Size([53750, 2])
We keep 2.25e+07/1.31e+09 =  1% of the original kernel matrix.

torch.Size([50721, 2])
We keep 1.09e+08/1.66e+09 =  6% of the original kernel matrix.

torch.Size([33181, 2])
We keep 1.06e+07/5.85e+08 =  1% of the original kernel matrix.

torch.Size([178965, 2])
We keep 6.46e+08/2.24e+10 =  2% of the original kernel matrix.

torch.Size([63993, 2])
We keep 3.49e+07/2.15e+09 =  1% of the original kernel matrix.

torch.Size([32325, 2])
We keep 2.03e+07/5.28e+08 =  3% of the original kernel matrix.

torch.Size([27794, 2])
We keep 6.70e+06/3.30e+08 =  2% of the original kernel matrix.

torch.Size([12783, 2])
We keep 2.09e+06/5.55e+07 =  3% of the original kernel matrix.

torch.Size([17551, 2])
We keep 2.65e+06/1.07e+08 =  2% of the original kernel matrix.

torch.Size([4111, 2])
We keep 2.61e+05/3.45e+06 =  7% of the original kernel matrix.

torch.Size([10178, 2])
We keep 9.40e+05/2.67e+07 =  3% of the original kernel matrix.

torch.Size([157941, 2])
We keep 6.23e+08/2.41e+10 =  2% of the original kernel matrix.

torch.Size([60044, 2])
We keep 3.49e+07/2.23e+09 =  1% of the original kernel matrix.

torch.Size([7539, 2])
We keep 9.67e+05/1.51e+07 =  6% of the original kernel matrix.

torch.Size([13363, 2])
We keep 1.60e+06/5.58e+07 =  2% of the original kernel matrix.

torch.Size([4004, 2])
We keep 2.17e+05/3.24e+06 =  6% of the original kernel matrix.

torch.Size([10088, 2])
We keep 9.37e+05/2.58e+07 =  3% of the original kernel matrix.

torch.Size([38941, 2])
We keep 2.31e+07/6.69e+08 =  3% of the original kernel matrix.

torch.Size([29644, 2])
We keep 7.12e+06/3.71e+08 =  1% of the original kernel matrix.

torch.Size([40743, 2])
We keep 3.75e+07/1.04e+09 =  3% of the original kernel matrix.

torch.Size([29109, 2])
We keep 9.27e+06/4.64e+08 =  1% of the original kernel matrix.

torch.Size([191546, 2])
We keep 3.28e+08/1.72e+10 =  1% of the original kernel matrix.

torch.Size([68523, 2])
We keep 3.00e+07/1.88e+09 =  1% of the original kernel matrix.

torch.Size([148065, 2])
We keep 1.78e+08/1.06e+10 =  1% of the original kernel matrix.

torch.Size([60358, 2])
We keep 2.47e+07/1.47e+09 =  1% of the original kernel matrix.

torch.Size([2697, 2])
We keep 1.13e+05/1.32e+06 =  8% of the original kernel matrix.

torch.Size([8531, 2])
We keep 6.81e+05/1.65e+07 =  4% of the original kernel matrix.

torch.Size([22945, 2])
We keep 9.19e+06/2.51e+08 =  3% of the original kernel matrix.

torch.Size([23442, 2])
We keep 5.01e+06/2.27e+08 =  2% of the original kernel matrix.

torch.Size([11130, 2])
We keep 3.15e+06/5.16e+07 =  6% of the original kernel matrix.

torch.Size([16082, 2])
We keep 2.72e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([48325, 2])
We keep 8.34e+07/1.59e+09 =  5% of the original kernel matrix.

torch.Size([32939, 2])
We keep 1.09e+07/5.72e+08 =  1% of the original kernel matrix.

torch.Size([83523, 2])
We keep 6.06e+07/3.16e+09 =  1% of the original kernel matrix.

torch.Size([44248, 2])
We keep 1.41e+07/8.07e+08 =  1% of the original kernel matrix.

torch.Size([17470, 2])
We keep 3.75e+07/1.72e+08 = 21% of the original kernel matrix.

torch.Size([19329, 2])
We keep 3.62e+06/1.88e+08 =  1% of the original kernel matrix.

torch.Size([47487, 2])
We keep 2.13e+07/9.42e+08 =  2% of the original kernel matrix.

torch.Size([32636, 2])
We keep 8.51e+06/4.41e+08 =  1% of the original kernel matrix.

torch.Size([1503, 2])
We keep 4.25e+04/3.59e+05 = 11% of the original kernel matrix.

torch.Size([6684, 2])
We keep 4.48e+05/8.60e+06 =  5% of the original kernel matrix.

torch.Size([8785, 2])
We keep 1.09e+06/2.13e+07 =  5% of the original kernel matrix.

torch.Size([14437, 2])
We keep 1.89e+06/6.63e+07 =  2% of the original kernel matrix.

torch.Size([44995, 2])
We keep 6.30e+07/1.46e+09 =  4% of the original kernel matrix.

torch.Size([31259, 2])
We keep 1.07e+07/5.48e+08 =  1% of the original kernel matrix.

torch.Size([60124, 2])
We keep 7.55e+07/2.05e+09 =  3% of the original kernel matrix.

torch.Size([35995, 2])
We keep 1.16e+07/6.50e+08 =  1% of the original kernel matrix.

torch.Size([62239, 2])
We keep 7.38e+07/2.20e+09 =  3% of the original kernel matrix.

torch.Size([36414, 2])
We keep 1.26e+07/6.73e+08 =  1% of the original kernel matrix.

torch.Size([7316, 2])
We keep 1.10e+06/1.75e+07 =  6% of the original kernel matrix.

torch.Size([13086, 2])
We keep 1.75e+06/6.01e+07 =  2% of the original kernel matrix.

torch.Size([274249, 2])
We keep 4.36e+08/3.78e+10 =  1% of the original kernel matrix.

torch.Size([81913, 2])
We keep 4.26e+07/2.79e+09 =  1% of the original kernel matrix.

torch.Size([12969, 2])
We keep 1.96e+06/6.68e+07 =  2% of the original kernel matrix.

torch.Size([17828, 2])
We keep 2.86e+06/1.17e+08 =  2% of the original kernel matrix.

torch.Size([26499, 2])
We keep 8.30e+06/3.36e+08 =  2% of the original kernel matrix.

torch.Size([24943, 2])
We keep 5.64e+06/2.63e+08 =  2% of the original kernel matrix.

torch.Size([72508, 2])
We keep 5.01e+07/2.73e+09 =  1% of the original kernel matrix.

torch.Size([40965, 2])
We keep 1.33e+07/7.50e+08 =  1% of the original kernel matrix.

torch.Size([5276, 2])
We keep 4.17e+05/6.16e+06 =  6% of the original kernel matrix.

torch.Size([11373, 2])
We keep 1.19e+06/3.56e+07 =  3% of the original kernel matrix.

torch.Size([86761, 2])
We keep 7.63e+07/3.22e+09 =  2% of the original kernel matrix.

torch.Size([44592, 2])
We keep 1.47e+07/8.15e+08 =  1% of the original kernel matrix.

torch.Size([244945, 2])
We keep 8.41e+08/3.52e+10 =  2% of the original kernel matrix.

torch.Size([76591, 2])
We keep 4.29e+07/2.69e+09 =  1% of the original kernel matrix.

torch.Size([220698, 2])
We keep 4.19e+08/2.50e+10 =  1% of the original kernel matrix.

torch.Size([72456, 2])
We keep 3.61e+07/2.27e+09 =  1% of the original kernel matrix.

torch.Size([25845, 2])
We keep 1.09e+07/3.09e+08 =  3% of the original kernel matrix.

torch.Size([24760, 2])
We keep 5.38e+06/2.52e+08 =  2% of the original kernel matrix.

torch.Size([25142, 2])
We keep 9.39e+06/2.87e+08 =  3% of the original kernel matrix.

torch.Size([23995, 2])
We keep 5.20e+06/2.43e+08 =  2% of the original kernel matrix.

torch.Size([9775, 2])
We keep 1.49e+06/3.34e+07 =  4% of the original kernel matrix.

torch.Size([14811, 2])
We keep 2.23e+06/8.30e+07 =  2% of the original kernel matrix.

torch.Size([7034, 2])
We keep 5.43e+05/1.15e+07 =  4% of the original kernel matrix.

torch.Size([12991, 2])
We keep 1.47e+06/4.88e+07 =  3% of the original kernel matrix.

torch.Size([31930, 2])
We keep 2.26e+07/5.87e+08 =  3% of the original kernel matrix.

torch.Size([27434, 2])
We keep 6.70e+06/3.48e+08 =  1% of the original kernel matrix.

torch.Size([12044, 2])
We keep 1.87e+06/4.95e+07 =  3% of the original kernel matrix.

torch.Size([17099, 2])
We keep 2.58e+06/1.01e+08 =  2% of the original kernel matrix.

torch.Size([19364, 2])
We keep 4.85e+06/1.63e+08 =  2% of the original kernel matrix.

torch.Size([21162, 2])
We keep 4.02e+06/1.83e+08 =  2% of the original kernel matrix.

torch.Size([89965, 2])
We keep 1.98e+08/6.08e+09 =  3% of the original kernel matrix.

torch.Size([46504, 2])
We keep 1.64e+07/1.12e+09 =  1% of the original kernel matrix.

torch.Size([50492, 2])
We keep 4.16e+07/1.23e+09 =  3% of the original kernel matrix.

torch.Size([34210, 2])
We keep 9.25e+06/5.04e+08 =  1% of the original kernel matrix.

torch.Size([10441, 2])
We keep 2.41e+06/4.52e+07 =  5% of the original kernel matrix.

torch.Size([15416, 2])
We keep 2.55e+06/9.65e+07 =  2% of the original kernel matrix.

torch.Size([10391, 2])
We keep 1.55e+06/4.00e+07 =  3% of the original kernel matrix.

torch.Size([15870, 2])
We keep 2.25e+06/9.08e+07 =  2% of the original kernel matrix.

torch.Size([16282, 2])
We keep 2.72e+07/5.54e+08 =  4% of the original kernel matrix.

torch.Size([19015, 2])
We keep 5.84e+06/3.38e+08 =  1% of the original kernel matrix.

torch.Size([11743, 2])
We keep 1.50e+06/4.52e+07 =  3% of the original kernel matrix.

torch.Size([16804, 2])
We keep 2.34e+06/9.65e+07 =  2% of the original kernel matrix.

torch.Size([8088, 2])
We keep 1.61e+06/2.39e+07 =  6% of the original kernel matrix.

torch.Size([13845, 2])
We keep 1.99e+06/7.01e+07 =  2% of the original kernel matrix.

torch.Size([26698, 2])
We keep 1.23e+07/3.55e+08 =  3% of the original kernel matrix.

torch.Size([25342, 2])
We keep 5.88e+06/2.71e+08 =  2% of the original kernel matrix.

torch.Size([130895, 2])
We keep 4.56e+08/1.53e+10 =  2% of the original kernel matrix.

torch.Size([55823, 2])
We keep 2.96e+07/1.78e+09 =  1% of the original kernel matrix.

torch.Size([14924, 2])
We keep 3.94e+06/9.66e+07 =  4% of the original kernel matrix.

torch.Size([19116, 2])
We keep 3.48e+06/1.41e+08 =  2% of the original kernel matrix.

torch.Size([46411, 2])
We keep 6.49e+07/1.17e+09 =  5% of the original kernel matrix.

torch.Size([32404, 2])
We keep 9.46e+06/4.91e+08 =  1% of the original kernel matrix.

torch.Size([5073, 2])
We keep 4.12e+05/6.54e+06 =  6% of the original kernel matrix.

torch.Size([11251, 2])
We keep 1.17e+06/3.67e+07 =  3% of the original kernel matrix.

torch.Size([26634, 2])
We keep 2.64e+07/6.53e+08 =  4% of the original kernel matrix.

torch.Size([23165, 2])
We keep 7.59e+06/3.67e+08 =  2% of the original kernel matrix.

torch.Size([24650, 2])
We keep 8.85e+06/2.64e+08 =  3% of the original kernel matrix.

torch.Size([24295, 2])
We keep 4.68e+06/2.33e+08 =  2% of the original kernel matrix.

torch.Size([97456, 2])
We keep 7.42e+08/1.63e+10 =  4% of the original kernel matrix.

torch.Size([43181, 2])
We keep 3.02e+07/1.83e+09 =  1% of the original kernel matrix.

torch.Size([42633, 2])
We keep 2.02e+07/1.01e+09 =  1% of the original kernel matrix.

torch.Size([31331, 2])
We keep 8.85e+06/4.57e+08 =  1% of the original kernel matrix.

torch.Size([11973, 2])
We keep 1.63e+06/4.84e+07 =  3% of the original kernel matrix.

torch.Size([16986, 2])
We keep 2.54e+06/9.99e+07 =  2% of the original kernel matrix.

torch.Size([99500, 2])
We keep 7.20e+07/4.85e+09 =  1% of the original kernel matrix.

torch.Size([48294, 2])
We keep 1.73e+07/1.00e+09 =  1% of the original kernel matrix.

torch.Size([123044, 2])
We keep 9.90e+07/7.09e+09 =  1% of the original kernel matrix.

torch.Size([54423, 2])
We keep 2.00e+07/1.21e+09 =  1% of the original kernel matrix.

torch.Size([12047, 2])
We keep 2.17e+06/5.01e+07 =  4% of the original kernel matrix.

torch.Size([17243, 2])
We keep 2.56e+06/1.02e+08 =  2% of the original kernel matrix.

torch.Size([154105, 2])
We keep 2.57e+08/1.22e+10 =  2% of the original kernel matrix.

torch.Size([61293, 2])
We keep 2.64e+07/1.59e+09 =  1% of the original kernel matrix.

torch.Size([177809, 2])
We keep 1.88e+08/1.40e+10 =  1% of the original kernel matrix.

torch.Size([65823, 2])
We keep 2.76e+07/1.70e+09 =  1% of the original kernel matrix.

torch.Size([433986, 2])
We keep 9.29e+08/7.94e+10 =  1% of the original kernel matrix.

torch.Size([106178, 2])
We keep 6.08e+07/4.05e+09 =  1% of the original kernel matrix.

torch.Size([49960, 2])
We keep 5.33e+07/1.40e+09 =  3% of the original kernel matrix.

torch.Size([33755, 2])
We keep 9.65e+06/5.36e+08 =  1% of the original kernel matrix.

torch.Size([2797, 2])
We keep 1.72e+05/1.67e+06 = 10% of the original kernel matrix.

torch.Size([8605, 2])
We keep 7.12e+05/1.86e+07 =  3% of the original kernel matrix.

torch.Size([22736, 2])
We keep 6.55e+06/2.49e+08 =  2% of the original kernel matrix.

torch.Size([23425, 2])
We keep 5.04e+06/2.27e+08 =  2% of the original kernel matrix.

torch.Size([24748, 2])
We keep 1.53e+07/4.57e+08 =  3% of the original kernel matrix.

torch.Size([23904, 2])
We keep 5.83e+06/3.07e+08 =  1% of the original kernel matrix.

torch.Size([21448, 2])
We keep 5.58e+06/2.05e+08 =  2% of the original kernel matrix.

torch.Size([22911, 2])
We keep 4.58e+06/2.06e+08 =  2% of the original kernel matrix.

torch.Size([22092, 2])
We keep 7.28e+06/2.06e+08 =  3% of the original kernel matrix.

torch.Size([23230, 2])
We keep 4.64e+06/2.06e+08 =  2% of the original kernel matrix.

torch.Size([3744, 2])
We keep 1.83e+05/2.66e+06 =  6% of the original kernel matrix.

torch.Size([9816, 2])
We keep 8.78e+05/2.34e+07 =  3% of the original kernel matrix.

torch.Size([76038, 2])
We keep 6.99e+07/2.67e+09 =  2% of the original kernel matrix.

torch.Size([42341, 2])
We keep 1.36e+07/7.41e+08 =  1% of the original kernel matrix.

torch.Size([149781, 2])
We keep 1.82e+08/1.11e+10 =  1% of the original kernel matrix.

torch.Size([61378, 2])
We keep 2.56e+07/1.51e+09 =  1% of the original kernel matrix.

torch.Size([12816, 2])
We keep 2.39e+07/2.24e+08 = 10% of the original kernel matrix.

torch.Size([16243, 2])
We keep 4.31e+06/2.15e+08 =  2% of the original kernel matrix.

torch.Size([7833, 2])
We keep 9.91e+05/1.69e+07 =  5% of the original kernel matrix.

torch.Size([13643, 2])
We keep 1.74e+06/5.90e+07 =  2% of the original kernel matrix.

torch.Size([17742, 2])
We keep 7.11e+06/1.73e+08 =  4% of the original kernel matrix.

torch.Size([19793, 2])
We keep 4.26e+06/1.89e+08 =  2% of the original kernel matrix.

torch.Size([15479, 2])
We keep 3.25e+06/9.66e+07 =  3% of the original kernel matrix.

torch.Size([19425, 2])
We keep 3.43e+06/1.41e+08 =  2% of the original kernel matrix.

torch.Size([10597, 2])
We keep 1.58e+06/3.93e+07 =  4% of the original kernel matrix.

torch.Size([15878, 2])
We keep 2.42e+06/9.00e+07 =  2% of the original kernel matrix.

torch.Size([17805, 2])
We keep 3.70e+06/1.36e+08 =  2% of the original kernel matrix.

torch.Size([21226, 2])
We keep 3.93e+06/1.68e+08 =  2% of the original kernel matrix.

torch.Size([7928, 2])
We keep 9.22e+05/1.61e+07 =  5% of the original kernel matrix.

torch.Size([13803, 2])
We keep 1.64e+06/5.76e+07 =  2% of the original kernel matrix.

torch.Size([23410, 2])
We keep 1.29e+07/2.95e+08 =  4% of the original kernel matrix.

torch.Size([22801, 2])
We keep 5.06e+06/2.46e+08 =  2% of the original kernel matrix.

torch.Size([46552, 2])
We keep 1.79e+07/9.36e+08 =  1% of the original kernel matrix.

torch.Size([32787, 2])
We keep 8.41e+06/4.39e+08 =  1% of the original kernel matrix.

torch.Size([28372, 2])
We keep 9.11e+06/3.34e+08 =  2% of the original kernel matrix.

torch.Size([26200, 2])
We keep 5.39e+06/2.62e+08 =  2% of the original kernel matrix.

torch.Size([13559, 2])
We keep 3.84e+06/8.38e+07 =  4% of the original kernel matrix.

torch.Size([18082, 2])
We keep 3.27e+06/1.31e+08 =  2% of the original kernel matrix.

torch.Size([9034, 2])
We keep 1.88e+06/2.81e+07 =  6% of the original kernel matrix.

torch.Size([14568, 2])
We keep 2.16e+06/7.61e+07 =  2% of the original kernel matrix.

torch.Size([437058, 2])
We keep 1.26e+09/8.94e+10 =  1% of the original kernel matrix.

torch.Size([105463, 2])
We keep 6.25e+07/4.29e+09 =  1% of the original kernel matrix.

torch.Size([14759, 2])
We keep 2.91e+06/7.78e+07 =  3% of the original kernel matrix.

torch.Size([18793, 2])
We keep 3.14e+06/1.27e+08 =  2% of the original kernel matrix.

torch.Size([109187, 2])
We keep 1.03e+08/6.05e+09 =  1% of the original kernel matrix.

torch.Size([50499, 2])
We keep 1.93e+07/1.12e+09 =  1% of the original kernel matrix.

torch.Size([3665, 2])
We keep 1.95e+05/3.02e+06 =  6% of the original kernel matrix.

torch.Size([9917, 2])
We keep 8.78e+05/2.49e+07 =  3% of the original kernel matrix.

torch.Size([5982, 2])
We keep 5.80e+05/8.51e+06 =  6% of the original kernel matrix.

torch.Size([11945, 2])
We keep 1.34e+06/4.19e+07 =  3% of the original kernel matrix.

torch.Size([5740, 2])
We keep 4.69e+05/7.77e+06 =  6% of the original kernel matrix.

torch.Size([11665, 2])
We keep 1.31e+06/4.00e+07 =  3% of the original kernel matrix.

torch.Size([32299, 2])
We keep 1.79e+07/4.76e+08 =  3% of the original kernel matrix.

torch.Size([27970, 2])
We keep 5.79e+06/3.13e+08 =  1% of the original kernel matrix.

torch.Size([7093, 2])
We keep 8.12e+05/1.41e+07 =  5% of the original kernel matrix.

torch.Size([12947, 2])
We keep 1.63e+06/5.39e+07 =  3% of the original kernel matrix.

torch.Size([10886, 2])
We keep 2.33e+06/4.89e+07 =  4% of the original kernel matrix.

torch.Size([16028, 2])
We keep 2.61e+06/1.00e+08 =  2% of the original kernel matrix.

torch.Size([7251, 2])
We keep 8.42e+05/1.42e+07 =  5% of the original kernel matrix.

torch.Size([12928, 2])
We keep 1.63e+06/5.41e+07 =  3% of the original kernel matrix.

torch.Size([7100, 2])
We keep 6.79e+05/1.19e+07 =  5% of the original kernel matrix.

torch.Size([12969, 2])
We keep 1.51e+06/4.95e+07 =  3% of the original kernel matrix.

torch.Size([275003, 2])
We keep 5.57e+08/3.41e+10 =  1% of the original kernel matrix.

torch.Size([81793, 2])
We keep 4.15e+07/2.65e+09 =  1% of the original kernel matrix.

torch.Size([32395, 2])
We keep 2.16e+07/5.32e+08 =  4% of the original kernel matrix.

torch.Size([27609, 2])
We keep 6.95e+06/3.31e+08 =  2% of the original kernel matrix.

torch.Size([28756, 2])
We keep 1.65e+07/4.28e+08 =  3% of the original kernel matrix.

torch.Size([26229, 2])
We keep 6.37e+06/2.97e+08 =  2% of the original kernel matrix.

torch.Size([104505, 2])
We keep 1.58e+08/5.71e+09 =  2% of the original kernel matrix.

torch.Size([50090, 2])
We keep 1.90e+07/1.08e+09 =  1% of the original kernel matrix.

torch.Size([139002, 2])
We keep 4.50e+08/1.80e+10 =  2% of the original kernel matrix.

torch.Size([57482, 2])
We keep 3.17e+07/1.93e+09 =  1% of the original kernel matrix.

torch.Size([6846, 2])
We keep 7.36e+05/1.38e+07 =  5% of the original kernel matrix.

torch.Size([12662, 2])
We keep 1.60e+06/5.32e+07 =  3% of the original kernel matrix.

torch.Size([12337, 2])
We keep 1.74e+06/5.05e+07 =  3% of the original kernel matrix.

torch.Size([17120, 2])
We keep 2.61e+06/1.02e+08 =  2% of the original kernel matrix.

torch.Size([7500, 2])
We keep 7.16e+05/1.43e+07 =  4% of the original kernel matrix.

torch.Size([13334, 2])
We keep 1.60e+06/5.44e+07 =  2% of the original kernel matrix.

torch.Size([1428, 2])
We keep 5.62e+04/4.50e+05 = 12% of the original kernel matrix.

torch.Size([6551, 2])
We keep 4.88e+05/9.63e+06 =  5% of the original kernel matrix.

torch.Size([6336, 2])
We keep 5.75e+05/9.95e+06 =  5% of the original kernel matrix.

torch.Size([12186, 2])
We keep 1.41e+06/4.53e+07 =  3% of the original kernel matrix.

torch.Size([17039, 2])
We keep 5.44e+06/1.32e+08 =  4% of the original kernel matrix.

torch.Size([20446, 2])
We keep 3.91e+06/1.65e+08 =  2% of the original kernel matrix.

torch.Size([4082, 2])
We keep 3.11e+05/4.19e+06 =  7% of the original kernel matrix.

torch.Size([10098, 2])
We keep 1.02e+06/2.94e+07 =  3% of the original kernel matrix.

torch.Size([7886, 2])
We keep 8.38e+05/1.59e+07 =  5% of the original kernel matrix.

torch.Size([13635, 2])
We keep 1.69e+06/5.73e+07 =  2% of the original kernel matrix.

torch.Size([11048, 2])
We keep 1.55e+06/4.21e+07 =  3% of the original kernel matrix.

torch.Size([16050, 2])
We keep 2.43e+06/9.31e+07 =  2% of the original kernel matrix.

torch.Size([7628, 2])
We keep 7.02e+05/1.53e+07 =  4% of the original kernel matrix.

torch.Size([13516, 2])
We keep 1.66e+06/5.61e+07 =  2% of the original kernel matrix.

torch.Size([244408, 2])
We keep 4.50e+08/2.86e+10 =  1% of the original kernel matrix.

torch.Size([77452, 2])
We keep 3.80e+07/2.43e+09 =  1% of the original kernel matrix.

torch.Size([11555, 2])
We keep 4.74e+06/8.24e+07 =  5% of the original kernel matrix.

torch.Size([16040, 2])
We keep 3.27e+06/1.30e+08 =  2% of the original kernel matrix.

torch.Size([56831, 2])
We keep 4.21e+07/1.39e+09 =  3% of the original kernel matrix.

torch.Size([35589, 2])
We keep 1.02e+07/5.34e+08 =  1% of the original kernel matrix.

torch.Size([5951, 2])
We keep 5.67e+05/9.34e+06 =  6% of the original kernel matrix.

torch.Size([12030, 2])
We keep 1.28e+06/4.39e+07 =  2% of the original kernel matrix.

torch.Size([14993, 2])
We keep 7.40e+06/1.33e+08 =  5% of the original kernel matrix.

torch.Size([18654, 2])
We keep 3.74e+06/1.66e+08 =  2% of the original kernel matrix.

torch.Size([35076, 2])
We keep 3.30e+07/7.50e+08 =  4% of the original kernel matrix.

torch.Size([28020, 2])
We keep 8.01e+06/3.93e+08 =  2% of the original kernel matrix.

torch.Size([529771, 2])
We keep 3.92e+09/1.74e+11 =  2% of the original kernel matrix.

torch.Size([115916, 2])
We keep 7.94e+07/5.98e+09 =  1% of the original kernel matrix.

torch.Size([706434, 2])
We keep 1.75e+09/1.89e+11 =  0% of the original kernel matrix.

torch.Size([134477, 2])
We keep 8.80e+07/6.24e+09 =  1% of the original kernel matrix.

torch.Size([22740, 2])
We keep 5.12e+06/2.09e+08 =  2% of the original kernel matrix.

torch.Size([23717, 2])
We keep 4.34e+06/2.08e+08 =  2% of the original kernel matrix.

torch.Size([57511, 2])
We keep 4.56e+07/1.51e+09 =  3% of the original kernel matrix.

torch.Size([36042, 2])
We keep 1.06e+07/5.58e+08 =  1% of the original kernel matrix.

torch.Size([5057, 2])
We keep 3.25e+05/5.22e+06 =  6% of the original kernel matrix.

torch.Size([11175, 2])
We keep 1.09e+06/3.28e+07 =  3% of the original kernel matrix.

torch.Size([15120, 2])
We keep 2.67e+07/2.23e+08 = 11% of the original kernel matrix.

torch.Size([18390, 2])
We keep 4.57e+06/2.14e+08 =  2% of the original kernel matrix.

torch.Size([43157, 2])
We keep 6.41e+07/1.07e+09 =  5% of the original kernel matrix.

torch.Size([31282, 2])
We keep 9.30e+06/4.71e+08 =  1% of the original kernel matrix.

torch.Size([31878, 2])
We keep 1.72e+07/4.48e+08 =  3% of the original kernel matrix.

torch.Size([27985, 2])
We keep 5.88e+06/3.04e+08 =  1% of the original kernel matrix.

torch.Size([11242, 2])
We keep 1.65e+06/4.30e+07 =  3% of the original kernel matrix.

torch.Size([16409, 2])
We keep 2.47e+06/9.42e+07 =  2% of the original kernel matrix.

torch.Size([48197, 2])
We keep 2.31e+07/1.03e+09 =  2% of the original kernel matrix.

torch.Size([32855, 2])
We keep 8.79e+06/4.60e+08 =  1% of the original kernel matrix.

torch.Size([386926, 2])
We keep 1.27e+09/7.25e+10 =  1% of the original kernel matrix.

torch.Size([97775, 2])
We keep 5.91e+07/3.86e+09 =  1% of the original kernel matrix.

torch.Size([19931, 2])
We keep 7.78e+06/2.34e+08 =  3% of the original kernel matrix.

torch.Size([20769, 2])
We keep 4.79e+06/2.20e+08 =  2% of the original kernel matrix.

torch.Size([10268, 2])
We keep 1.98e+06/3.88e+07 =  5% of the original kernel matrix.

torch.Size([15301, 2])
We keep 2.39e+06/8.94e+07 =  2% of the original kernel matrix.

torch.Size([17475, 2])
We keep 6.59e+06/1.33e+08 =  4% of the original kernel matrix.

torch.Size([21260, 2])
We keep 4.00e+06/1.65e+08 =  2% of the original kernel matrix.

torch.Size([26270, 2])
We keep 8.80e+06/3.04e+08 =  2% of the original kernel matrix.

torch.Size([25009, 2])
We keep 5.44e+06/2.50e+08 =  2% of the original kernel matrix.

torch.Size([5717, 2])
We keep 3.50e+05/7.02e+06 =  4% of the original kernel matrix.

torch.Size([11860, 2])
We keep 1.18e+06/3.80e+07 =  3% of the original kernel matrix.

torch.Size([88857, 2])
We keep 8.26e+07/4.17e+09 =  1% of the original kernel matrix.

torch.Size([45717, 2])
We keep 1.64e+07/9.27e+08 =  1% of the original kernel matrix.

torch.Size([26919, 2])
We keep 1.30e+07/3.39e+08 =  3% of the original kernel matrix.

torch.Size([24731, 2])
We keep 5.50e+06/2.64e+08 =  2% of the original kernel matrix.

torch.Size([1974, 2])
We keep 6.82e+04/6.86e+05 =  9% of the original kernel matrix.

torch.Size([7476, 2])
We keep 5.58e+05/1.19e+07 =  4% of the original kernel matrix.

torch.Size([96779, 2])
We keep 1.10e+08/4.82e+09 =  2% of the original kernel matrix.

torch.Size([47945, 2])
We keep 1.71e+07/9.96e+08 =  1% of the original kernel matrix.

torch.Size([26222, 2])
We keep 7.98e+06/2.99e+08 =  2% of the original kernel matrix.

torch.Size([25186, 2])
We keep 5.29e+06/2.48e+08 =  2% of the original kernel matrix.

torch.Size([1856, 2])
We keep 5.88e+04/5.34e+05 = 11% of the original kernel matrix.

torch.Size([7582, 2])
We keep 5.05e+05/1.05e+07 =  4% of the original kernel matrix.

torch.Size([68044, 2])
We keep 1.18e+08/3.08e+09 =  3% of the original kernel matrix.

torch.Size([39458, 2])
We keep 1.45e+07/7.96e+08 =  1% of the original kernel matrix.

torch.Size([119578, 2])
We keep 1.17e+08/7.11e+09 =  1% of the original kernel matrix.

torch.Size([53712, 2])
We keep 2.05e+07/1.21e+09 =  1% of the original kernel matrix.

torch.Size([83103, 2])
We keep 9.03e+07/3.26e+09 =  2% of the original kernel matrix.

torch.Size([43990, 2])
We keep 1.47e+07/8.20e+08 =  1% of the original kernel matrix.

torch.Size([62248, 2])
We keep 6.16e+07/1.78e+09 =  3% of the original kernel matrix.

torch.Size([37259, 2])
We keep 1.11e+07/6.05e+08 =  1% of the original kernel matrix.

torch.Size([187666, 2])
We keep 4.58e+08/2.03e+10 =  2% of the original kernel matrix.

torch.Size([66586, 2])
We keep 3.36e+07/2.05e+09 =  1% of the original kernel matrix.

torch.Size([7595, 2])
We keep 1.30e+06/1.76e+07 =  7% of the original kernel matrix.

torch.Size([13253, 2])
We keep 1.74e+06/6.02e+07 =  2% of the original kernel matrix.

torch.Size([29077, 2])
We keep 1.79e+07/4.20e+08 =  4% of the original kernel matrix.

torch.Size([25602, 2])
We keep 5.93e+06/2.94e+08 =  2% of the original kernel matrix.

torch.Size([87278, 2])
We keep 5.54e+07/3.53e+09 =  1% of the original kernel matrix.

torch.Size([45219, 2])
We keep 1.50e+07/8.53e+08 =  1% of the original kernel matrix.

torch.Size([137625, 2])
We keep 1.37e+08/9.10e+09 =  1% of the original kernel matrix.

torch.Size([57532, 2])
We keep 2.24e+07/1.37e+09 =  1% of the original kernel matrix.

torch.Size([13689, 2])
We keep 2.70e+06/7.21e+07 =  3% of the original kernel matrix.

torch.Size([18296, 2])
We keep 3.06e+06/1.22e+08 =  2% of the original kernel matrix.

torch.Size([140496, 2])
We keep 5.19e+08/1.64e+10 =  3% of the original kernel matrix.

torch.Size([58665, 2])
We keep 2.58e+07/1.84e+09 =  1% of the original kernel matrix.

torch.Size([171218, 2])
We keep 3.15e+08/1.66e+10 =  1% of the original kernel matrix.

torch.Size([65085, 2])
We keep 2.77e+07/1.85e+09 =  1% of the original kernel matrix.

torch.Size([64750, 2])
We keep 4.59e+07/1.94e+09 =  2% of the original kernel matrix.

torch.Size([38635, 2])
We keep 1.15e+07/6.33e+08 =  1% of the original kernel matrix.

torch.Size([31931, 2])
We keep 3.04e+07/7.64e+08 =  3% of the original kernel matrix.

torch.Size([26862, 2])
We keep 7.22e+06/3.97e+08 =  1% of the original kernel matrix.

torch.Size([13629, 2])
We keep 2.22e+07/2.00e+08 = 11% of the original kernel matrix.

torch.Size([17186, 2])
We keep 4.61e+06/2.03e+08 =  2% of the original kernel matrix.

torch.Size([18482, 2])
We keep 1.09e+07/1.70e+08 =  6% of the original kernel matrix.

torch.Size([20008, 2])
We keep 4.11e+06/1.87e+08 =  2% of the original kernel matrix.

torch.Size([8258, 2])
We keep 9.53e+05/2.16e+07 =  4% of the original kernel matrix.

torch.Size([13916, 2])
We keep 1.91e+06/6.67e+07 =  2% of the original kernel matrix.

torch.Size([5424, 2])
We keep 6.77e+05/8.57e+06 =  7% of the original kernel matrix.

torch.Size([11301, 2])
We keep 1.30e+06/4.20e+07 =  3% of the original kernel matrix.

torch.Size([160069, 2])
We keep 1.59e+08/1.15e+10 =  1% of the original kernel matrix.

torch.Size([63233, 2])
We keep 2.49e+07/1.54e+09 =  1% of the original kernel matrix.

torch.Size([112432, 2])
We keep 8.72e+07/6.10e+09 =  1% of the original kernel matrix.

torch.Size([51894, 2])
We keep 1.89e+07/1.12e+09 =  1% of the original kernel matrix.

torch.Size([261339, 2])
We keep 5.70e+08/3.47e+10 =  1% of the original kernel matrix.

torch.Size([78424, 2])
We keep 4.17e+07/2.67e+09 =  1% of the original kernel matrix.

torch.Size([80091, 2])
We keep 1.54e+08/3.68e+09 =  4% of the original kernel matrix.

torch.Size([43133, 2])
We keep 1.57e+07/8.71e+08 =  1% of the original kernel matrix.

torch.Size([62485, 2])
We keep 5.08e+07/1.82e+09 =  2% of the original kernel matrix.

torch.Size([37601, 2])
We keep 1.14e+07/6.13e+08 =  1% of the original kernel matrix.

torch.Size([13755, 2])
We keep 2.93e+06/9.36e+07 =  3% of the original kernel matrix.

torch.Size([17846, 2])
We keep 3.33e+06/1.39e+08 =  2% of the original kernel matrix.

torch.Size([100513, 2])
We keep 7.72e+07/4.60e+09 =  1% of the original kernel matrix.

torch.Size([49086, 2])
We keep 1.68e+07/9.73e+08 =  1% of the original kernel matrix.

torch.Size([4905, 2])
We keep 5.66e+05/6.25e+06 =  9% of the original kernel matrix.

torch.Size([10834, 2])
We keep 1.23e+06/3.59e+07 =  3% of the original kernel matrix.

torch.Size([12713, 2])
We keep 3.31e+06/7.00e+07 =  4% of the original kernel matrix.

torch.Size([17170, 2])
We keep 2.97e+06/1.20e+08 =  2% of the original kernel matrix.

torch.Size([172330, 2])
We keep 2.64e+08/1.45e+10 =  1% of the original kernel matrix.

torch.Size([64803, 2])
We keep 2.79e+07/1.73e+09 =  1% of the original kernel matrix.

torch.Size([46998, 2])
We keep 3.51e+07/1.05e+09 =  3% of the original kernel matrix.

torch.Size([32677, 2])
We keep 9.23e+06/4.66e+08 =  1% of the original kernel matrix.

torch.Size([5764, 2])
We keep 6.41e+05/8.54e+06 =  7% of the original kernel matrix.

torch.Size([11732, 2])
We keep 1.37e+06/4.19e+07 =  3% of the original kernel matrix.

torch.Size([11727, 2])
We keep 1.84e+06/5.31e+07 =  3% of the original kernel matrix.

torch.Size([16640, 2])
We keep 2.63e+06/1.05e+08 =  2% of the original kernel matrix.

torch.Size([105058, 2])
We keep 2.36e+08/7.52e+09 =  3% of the original kernel matrix.

torch.Size([50120, 2])
We keep 2.14e+07/1.24e+09 =  1% of the original kernel matrix.

torch.Size([22131, 2])
We keep 8.76e+06/2.63e+08 =  3% of the original kernel matrix.

torch.Size([22655, 2])
We keep 5.06e+06/2.33e+08 =  2% of the original kernel matrix.

torch.Size([65484, 2])
We keep 5.06e+07/2.01e+09 =  2% of the original kernel matrix.

torch.Size([38977, 2])
We keep 1.17e+07/6.44e+08 =  1% of the original kernel matrix.

torch.Size([9768, 2])
We keep 4.51e+06/6.80e+07 =  6% of the original kernel matrix.

torch.Size([14643, 2])
We keep 2.84e+06/1.18e+08 =  2% of the original kernel matrix.

torch.Size([15059, 2])
We keep 5.15e+06/9.92e+07 =  5% of the original kernel matrix.

torch.Size([18512, 2])
We keep 3.34e+06/1.43e+08 =  2% of the original kernel matrix.

torch.Size([6284, 2])
We keep 7.29e+05/1.10e+07 =  6% of the original kernel matrix.

torch.Size([12213, 2])
We keep 1.48e+06/4.77e+07 =  3% of the original kernel matrix.

torch.Size([4355, 2])
We keep 2.49e+05/3.91e+06 =  6% of the original kernel matrix.

torch.Size([10644, 2])
We keep 9.93e+05/2.84e+07 =  3% of the original kernel matrix.

torch.Size([38691, 2])
We keep 1.55e+07/6.38e+08 =  2% of the original kernel matrix.

torch.Size([30327, 2])
We keep 7.29e+06/3.63e+08 =  2% of the original kernel matrix.

torch.Size([14244, 2])
We keep 3.26e+06/7.91e+07 =  4% of the original kernel matrix.

torch.Size([18183, 2])
We keep 3.09e+06/1.28e+08 =  2% of the original kernel matrix.

torch.Size([3662, 2])
We keep 2.36e+05/3.14e+06 =  7% of the original kernel matrix.

torch.Size([9671, 2])
We keep 9.35e+05/2.55e+07 =  3% of the original kernel matrix.

torch.Size([99142, 2])
We keep 1.17e+08/5.36e+09 =  2% of the original kernel matrix.

torch.Size([48209, 2])
We keep 1.77e+07/1.05e+09 =  1% of the original kernel matrix.

torch.Size([4676, 2])
We keep 3.26e+05/4.84e+06 =  6% of the original kernel matrix.

torch.Size([10771, 2])
We keep 1.09e+06/3.16e+07 =  3% of the original kernel matrix.

torch.Size([30030, 2])
We keep 1.10e+07/4.60e+08 =  2% of the original kernel matrix.

torch.Size([26897, 2])
We keep 6.46e+06/3.08e+08 =  2% of the original kernel matrix.

torch.Size([8373, 2])
We keep 1.20e+06/2.42e+07 =  4% of the original kernel matrix.

torch.Size([14203, 2])
We keep 1.94e+06/7.07e+07 =  2% of the original kernel matrix.

torch.Size([89242, 2])
We keep 6.24e+07/3.45e+09 =  1% of the original kernel matrix.

torch.Size([45492, 2])
We keep 1.48e+07/8.43e+08 =  1% of the original kernel matrix.

torch.Size([18329, 2])
We keep 2.42e+07/2.31e+08 = 10% of the original kernel matrix.

torch.Size([20432, 2])
We keep 5.00e+06/2.18e+08 =  2% of the original kernel matrix.

torch.Size([27326, 2])
We keep 9.42e+06/3.26e+08 =  2% of the original kernel matrix.

torch.Size([25267, 2])
We keep 5.51e+06/2.59e+08 =  2% of the original kernel matrix.

torch.Size([21693, 2])
We keep 5.40e+06/2.01e+08 =  2% of the original kernel matrix.

torch.Size([22860, 2])
We keep 4.56e+06/2.04e+08 =  2% of the original kernel matrix.

torch.Size([30917, 2])
We keep 1.26e+07/4.17e+08 =  3% of the original kernel matrix.

torch.Size([26801, 2])
We keep 6.07e+06/2.93e+08 =  2% of the original kernel matrix.

torch.Size([56400, 2])
We keep 4.13e+07/1.54e+09 =  2% of the original kernel matrix.

torch.Size([35904, 2])
We keep 1.08e+07/5.64e+08 =  1% of the original kernel matrix.

torch.Size([279652, 2])
We keep 7.08e+08/4.02e+10 =  1% of the original kernel matrix.

torch.Size([81923, 2])
We keep 4.37e+07/2.88e+09 =  1% of the original kernel matrix.

torch.Size([13119, 2])
We keep 1.89e+06/5.42e+07 =  3% of the original kernel matrix.

torch.Size([17766, 2])
We keep 2.69e+06/1.06e+08 =  2% of the original kernel matrix.

torch.Size([4439, 2])
We keep 4.79e+05/5.76e+06 =  8% of the original kernel matrix.

torch.Size([10244, 2])
We keep 1.11e+06/3.44e+07 =  3% of the original kernel matrix.

torch.Size([53679, 2])
We keep 3.49e+07/1.26e+09 =  2% of the original kernel matrix.

torch.Size([34492, 2])
We keep 9.60e+06/5.09e+08 =  1% of the original kernel matrix.

torch.Size([12758, 2])
We keep 2.41e+06/6.08e+07 =  3% of the original kernel matrix.

torch.Size([17548, 2])
We keep 2.84e+06/1.12e+08 =  2% of the original kernel matrix.

torch.Size([468754, 2])
We keep 1.22e+09/9.59e+10 =  1% of the original kernel matrix.

torch.Size([110027, 2])
We keep 6.59e+07/4.44e+09 =  1% of the original kernel matrix.

torch.Size([3752, 2])
We keep 2.57e+05/3.42e+06 =  7% of the original kernel matrix.

torch.Size([9812, 2])
We keep 9.61e+05/2.66e+07 =  3% of the original kernel matrix.

torch.Size([115454, 2])
We keep 1.23e+08/6.78e+09 =  1% of the original kernel matrix.

torch.Size([52285, 2])
We keep 2.01e+07/1.18e+09 =  1% of the original kernel matrix.

torch.Size([391276, 2])
We keep 7.04e+08/6.62e+10 =  1% of the original kernel matrix.

torch.Size([98636, 2])
We keep 5.51e+07/3.69e+09 =  1% of the original kernel matrix.

torch.Size([6231, 2])
We keep 5.68e+05/9.13e+06 =  6% of the original kernel matrix.

torch.Size([12075, 2])
We keep 1.38e+06/4.34e+07 =  3% of the original kernel matrix.

torch.Size([121800, 2])
We keep 2.63e+08/1.16e+10 =  2% of the original kernel matrix.

torch.Size([51618, 2])
We keep 2.51e+07/1.55e+09 =  1% of the original kernel matrix.

torch.Size([11252, 2])
We keep 5.75e+06/8.69e+07 =  6% of the original kernel matrix.

torch.Size([16136, 2])
We keep 3.32e+06/1.34e+08 =  2% of the original kernel matrix.

torch.Size([590184, 2])
We keep 2.71e+09/1.52e+11 =  1% of the original kernel matrix.

torch.Size([124565, 2])
We keep 8.28e+07/5.60e+09 =  1% of the original kernel matrix.

torch.Size([10226, 2])
We keep 1.48e+06/3.19e+07 =  4% of the original kernel matrix.

torch.Size([15449, 2])
We keep 2.20e+06/8.11e+07 =  2% of the original kernel matrix.

torch.Size([23838, 2])
We keep 1.30e+07/2.79e+08 =  4% of the original kernel matrix.

torch.Size([23266, 2])
We keep 4.94e+06/2.40e+08 =  2% of the original kernel matrix.

torch.Size([12669, 2])
We keep 2.94e+07/1.70e+08 = 17% of the original kernel matrix.

torch.Size([16453, 2])
We keep 4.17e+06/1.87e+08 =  2% of the original kernel matrix.

torch.Size([5094, 2])
We keep 4.07e+05/6.77e+06 =  6% of the original kernel matrix.

torch.Size([11113, 2])
We keep 1.24e+06/3.73e+07 =  3% of the original kernel matrix.

torch.Size([7764, 2])
We keep 1.36e+06/2.09e+07 =  6% of the original kernel matrix.

torch.Size([13422, 2])
We keep 1.92e+06/6.57e+07 =  2% of the original kernel matrix.

torch.Size([44538, 2])
We keep 2.54e+07/8.73e+08 =  2% of the original kernel matrix.

torch.Size([32190, 2])
We keep 8.08e+06/4.24e+08 =  1% of the original kernel matrix.

torch.Size([19577, 2])
We keep 1.11e+07/2.05e+08 =  5% of the original kernel matrix.

torch.Size([21422, 2])
We keep 4.44e+06/2.06e+08 =  2% of the original kernel matrix.

torch.Size([341726, 2])
We keep 1.53e+09/5.89e+10 =  2% of the original kernel matrix.

torch.Size([92785, 2])
We keep 5.42e+07/3.49e+09 =  1% of the original kernel matrix.

torch.Size([18853, 2])
We keep 4.39e+06/1.49e+08 =  2% of the original kernel matrix.

torch.Size([20466, 2])
We keep 3.82e+06/1.75e+08 =  2% of the original kernel matrix.

torch.Size([7698, 2])
We keep 9.93e+05/1.68e+07 =  5% of the original kernel matrix.

torch.Size([13389, 2])
We keep 1.74e+06/5.89e+07 =  2% of the original kernel matrix.

torch.Size([7341, 2])
We keep 1.61e+06/1.82e+07 =  8% of the original kernel matrix.

torch.Size([13126, 2])
We keep 1.78e+06/6.13e+07 =  2% of the original kernel matrix.

torch.Size([104786, 2])
We keep 2.58e+08/8.09e+09 =  3% of the original kernel matrix.

torch.Size([50199, 2])
We keep 2.23e+07/1.29e+09 =  1% of the original kernel matrix.

torch.Size([70666, 2])
We keep 2.37e+08/4.15e+09 =  5% of the original kernel matrix.

torch.Size([40481, 2])
We keep 1.69e+07/9.25e+08 =  1% of the original kernel matrix.

torch.Size([186581, 2])
We keep 2.20e+08/1.82e+10 =  1% of the original kernel matrix.

torch.Size([69023, 2])
We keep 3.08e+07/1.93e+09 =  1% of the original kernel matrix.

torch.Size([36448, 2])
We keep 4.33e+07/6.28e+08 =  6% of the original kernel matrix.

torch.Size([29151, 2])
We keep 6.73e+06/3.60e+08 =  1% of the original kernel matrix.

torch.Size([6457, 2])
We keep 1.03e+06/1.40e+07 =  7% of the original kernel matrix.

torch.Size([12565, 2])
We keep 1.66e+06/5.38e+07 =  3% of the original kernel matrix.

torch.Size([7262, 2])
We keep 6.87e+05/1.34e+07 =  5% of the original kernel matrix.

torch.Size([13089, 2])
We keep 1.57e+06/5.26e+07 =  2% of the original kernel matrix.

torch.Size([8451, 2])
We keep 3.44e+06/3.51e+07 =  9% of the original kernel matrix.

torch.Size([13846, 2])
We keep 2.28e+06/8.50e+07 =  2% of the original kernel matrix.

torch.Size([729277, 2])
We keep 3.42e+09/2.14e+11 =  1% of the original kernel matrix.

torch.Size([138265, 2])
We keep 8.91e+07/6.64e+09 =  1% of the original kernel matrix.

torch.Size([9828, 2])
We keep 1.30e+06/2.95e+07 =  4% of the original kernel matrix.

torch.Size([15387, 2])
We keep 2.15e+06/7.80e+07 =  2% of the original kernel matrix.

torch.Size([17811, 2])
We keep 4.04e+06/1.34e+08 =  3% of the original kernel matrix.

torch.Size([21328, 2])
We keep 3.93e+06/1.66e+08 =  2% of the original kernel matrix.

torch.Size([78244, 2])
We keep 9.52e+07/3.17e+09 =  3% of the original kernel matrix.

torch.Size([42112, 2])
We keep 1.39e+07/8.08e+08 =  1% of the original kernel matrix.

torch.Size([28142, 2])
We keep 3.28e+07/5.94e+08 =  5% of the original kernel matrix.

torch.Size([24203, 2])
We keep 6.87e+06/3.50e+08 =  1% of the original kernel matrix.

torch.Size([12442, 2])
We keep 2.56e+06/5.30e+07 =  4% of the original kernel matrix.

torch.Size([17167, 2])
We keep 2.57e+06/1.04e+08 =  2% of the original kernel matrix.

torch.Size([7609, 2])
We keep 8.49e+05/1.60e+07 =  5% of the original kernel matrix.

torch.Size([13356, 2])
We keep 1.68e+06/5.74e+07 =  2% of the original kernel matrix.

torch.Size([28785, 2])
We keep 2.91e+07/6.00e+08 =  4% of the original kernel matrix.

torch.Size([25339, 2])
We keep 7.18e+06/3.52e+08 =  2% of the original kernel matrix.

torch.Size([48049, 2])
We keep 3.04e+07/1.03e+09 =  2% of the original kernel matrix.

torch.Size([33214, 2])
We keep 8.94e+06/4.60e+08 =  1% of the original kernel matrix.

torch.Size([7276, 2])
We keep 9.30e+05/1.64e+07 =  5% of the original kernel matrix.

torch.Size([13138, 2])
We keep 1.70e+06/5.82e+07 =  2% of the original kernel matrix.

torch.Size([31140, 2])
We keep 1.37e+07/4.98e+08 =  2% of the original kernel matrix.

torch.Size([27196, 2])
We keep 6.47e+06/3.20e+08 =  2% of the original kernel matrix.

torch.Size([224049, 2])
We keep 1.35e+09/4.46e+10 =  3% of the original kernel matrix.

torch.Size([72249, 2])
We keep 4.82e+07/3.03e+09 =  1% of the original kernel matrix.

torch.Size([69280, 2])
We keep 1.02e+08/2.62e+09 =  3% of the original kernel matrix.

torch.Size([39957, 2])
We keep 1.36e+07/7.35e+08 =  1% of the original kernel matrix.

torch.Size([189048, 2])
We keep 3.62e+08/1.94e+10 =  1% of the original kernel matrix.

torch.Size([69054, 2])
We keep 3.23e+07/2.00e+09 =  1% of the original kernel matrix.

torch.Size([82110, 2])
We keep 5.97e+07/3.11e+09 =  1% of the original kernel matrix.

torch.Size([43859, 2])
We keep 1.43e+07/8.01e+08 =  1% of the original kernel matrix.

torch.Size([11264, 2])
We keep 3.16e+06/5.30e+07 =  5% of the original kernel matrix.

torch.Size([16466, 2])
We keep 2.62e+06/1.04e+08 =  2% of the original kernel matrix.

torch.Size([59508, 2])
We keep 2.77e+07/1.44e+09 =  1% of the original kernel matrix.

torch.Size([36997, 2])
We keep 9.81e+06/5.44e+08 =  1% of the original kernel matrix.

torch.Size([10254, 2])
We keep 2.06e+06/3.72e+07 =  5% of the original kernel matrix.

torch.Size([15723, 2])
We keep 2.33e+06/8.76e+07 =  2% of the original kernel matrix.

torch.Size([10293, 2])
We keep 1.55e+06/3.18e+07 =  4% of the original kernel matrix.

torch.Size([15612, 2])
We keep 2.19e+06/8.09e+07 =  2% of the original kernel matrix.

torch.Size([13298, 2])
We keep 2.50e+06/7.30e+07 =  3% of the original kernel matrix.

torch.Size([17578, 2])
We keep 3.01e+06/1.23e+08 =  2% of the original kernel matrix.

torch.Size([8675, 2])
We keep 8.61e+05/2.00e+07 =  4% of the original kernel matrix.

torch.Size([14467, 2])
We keep 1.79e+06/6.43e+07 =  2% of the original kernel matrix.

torch.Size([76526, 2])
We keep 2.15e+08/3.87e+09 =  5% of the original kernel matrix.

torch.Size([42119, 2])
We keep 1.50e+07/8.93e+08 =  1% of the original kernel matrix.

torch.Size([15551, 2])
We keep 3.61e+06/1.03e+08 =  3% of the original kernel matrix.

torch.Size([19042, 2])
We keep 3.50e+06/1.46e+08 =  2% of the original kernel matrix.

torch.Size([3188, 2])
We keep 2.51e+05/2.57e+06 =  9% of the original kernel matrix.

torch.Size([8980, 2])
We keep 8.41e+05/2.30e+07 =  3% of the original kernel matrix.

torch.Size([18563, 2])
We keep 4.06e+06/1.40e+08 =  2% of the original kernel matrix.

torch.Size([21359, 2])
We keep 3.92e+06/1.70e+08 =  2% of the original kernel matrix.

torch.Size([11791, 2])
We keep 6.07e+06/1.12e+08 =  5% of the original kernel matrix.

torch.Size([16330, 2])
We keep 3.08e+06/1.52e+08 =  2% of the original kernel matrix.

torch.Size([14546, 2])
We keep 4.93e+06/8.88e+07 =  5% of the original kernel matrix.

torch.Size([18549, 2])
We keep 3.10e+06/1.35e+08 =  2% of the original kernel matrix.

torch.Size([9371, 2])
We keep 2.29e+06/3.37e+07 =  6% of the original kernel matrix.

torch.Size([14729, 2])
We keep 2.28e+06/8.34e+07 =  2% of the original kernel matrix.

torch.Size([13021, 2])
We keep 2.47e+06/7.01e+07 =  3% of the original kernel matrix.

torch.Size([17732, 2])
We keep 2.93e+06/1.20e+08 =  2% of the original kernel matrix.

torch.Size([8393, 2])
We keep 3.83e+06/4.25e+07 =  9% of the original kernel matrix.

torch.Size([13743, 2])
We keep 2.40e+06/9.36e+07 =  2% of the original kernel matrix.

torch.Size([18271, 2])
We keep 8.50e+06/1.62e+08 =  5% of the original kernel matrix.

torch.Size([20425, 2])
We keep 4.07e+06/1.82e+08 =  2% of the original kernel matrix.

torch.Size([5813, 2])
We keep 4.68e+05/7.43e+06 =  6% of the original kernel matrix.

torch.Size([11855, 2])
We keep 1.27e+06/3.91e+07 =  3% of the original kernel matrix.

torch.Size([11670, 2])
We keep 1.80e+06/4.59e+07 =  3% of the original kernel matrix.

torch.Size([16523, 2])
We keep 2.54e+06/9.73e+07 =  2% of the original kernel matrix.

torch.Size([22355, 2])
We keep 4.53e+06/2.07e+08 =  2% of the original kernel matrix.

torch.Size([23634, 2])
We keep 4.46e+06/2.07e+08 =  2% of the original kernel matrix.

torch.Size([108042, 2])
We keep 1.03e+08/5.76e+09 =  1% of the original kernel matrix.

torch.Size([50736, 2])
We keep 1.86e+07/1.09e+09 =  1% of the original kernel matrix.

torch.Size([8764, 2])
We keep 1.32e+06/2.41e+07 =  5% of the original kernel matrix.

torch.Size([14426, 2])
We keep 1.99e+06/7.05e+07 =  2% of the original kernel matrix.

torch.Size([2467, 2])
We keep 1.11e+05/1.22e+06 =  9% of the original kernel matrix.

torch.Size([8086, 2])
We keep 6.67e+05/1.59e+07 =  4% of the original kernel matrix.

torch.Size([5691, 2])
We keep 6.10e+05/8.27e+06 =  7% of the original kernel matrix.

torch.Size([11510, 2])
We keep 1.35e+06/4.13e+07 =  3% of the original kernel matrix.

torch.Size([63499, 2])
We keep 5.42e+07/1.95e+09 =  2% of the original kernel matrix.

torch.Size([37819, 2])
We keep 1.15e+07/6.35e+08 =  1% of the original kernel matrix.

torch.Size([357079, 2])
We keep 9.38e+08/5.41e+10 =  1% of the original kernel matrix.

torch.Size([94885, 2])
We keep 5.11e+07/3.34e+09 =  1% of the original kernel matrix.

torch.Size([45573, 2])
We keep 4.37e+07/1.23e+09 =  3% of the original kernel matrix.

torch.Size([31093, 2])
We keep 9.89e+06/5.04e+08 =  1% of the original kernel matrix.

torch.Size([7973, 2])
We keep 1.14e+06/1.75e+07 =  6% of the original kernel matrix.

torch.Size([13769, 2])
We keep 1.77e+06/6.00e+07 =  2% of the original kernel matrix.

torch.Size([67367, 2])
We keep 8.79e+07/2.42e+09 =  3% of the original kernel matrix.

torch.Size([39565, 2])
We keep 1.29e+07/7.07e+08 =  1% of the original kernel matrix.

torch.Size([165730, 2])
We keep 1.86e+08/1.43e+10 =  1% of the original kernel matrix.

torch.Size([64867, 2])
We keep 2.80e+07/1.72e+09 =  1% of the original kernel matrix.

torch.Size([3073, 2])
We keep 1.44e+05/1.94e+06 =  7% of the original kernel matrix.

torch.Size([8924, 2])
We keep 7.82e+05/2.00e+07 =  3% of the original kernel matrix.

torch.Size([10032, 2])
We keep 1.27e+06/3.19e+07 =  3% of the original kernel matrix.

torch.Size([15426, 2])
We keep 2.15e+06/8.11e+07 =  2% of the original kernel matrix.

torch.Size([32269, 2])
We keep 1.01e+07/5.00e+08 =  2% of the original kernel matrix.

torch.Size([27826, 2])
We keep 6.53e+06/3.21e+08 =  2% of the original kernel matrix.

torch.Size([14474, 2])
We keep 7.19e+06/1.43e+08 =  5% of the original kernel matrix.

torch.Size([18277, 2])
We keep 3.91e+06/1.72e+08 =  2% of the original kernel matrix.

torch.Size([30112, 2])
We keep 2.40e+07/4.85e+08 =  4% of the original kernel matrix.

torch.Size([26347, 2])
We keep 6.16e+06/3.16e+08 =  1% of the original kernel matrix.

torch.Size([91503, 2])
We keep 8.09e+07/4.05e+09 =  1% of the original kernel matrix.

torch.Size([46521, 2])
We keep 1.62e+07/9.14e+08 =  1% of the original kernel matrix.

torch.Size([23121, 2])
We keep 6.03e+06/2.39e+08 =  2% of the original kernel matrix.

torch.Size([23868, 2])
We keep 4.87e+06/2.22e+08 =  2% of the original kernel matrix.

torch.Size([6314, 2])
We keep 4.47e+05/8.94e+06 =  5% of the original kernel matrix.

torch.Size([12361, 2])
We keep 1.32e+06/4.29e+07 =  3% of the original kernel matrix.

torch.Size([4756, 2])
We keep 4.22e+05/5.46e+06 =  7% of the original kernel matrix.

torch.Size([10611, 2])
We keep 1.16e+06/3.35e+07 =  3% of the original kernel matrix.

torch.Size([29295, 2])
We keep 1.32e+07/3.93e+08 =  3% of the original kernel matrix.

torch.Size([25999, 2])
We keep 5.83e+06/2.85e+08 =  2% of the original kernel matrix.

torch.Size([31470, 2])
We keep 1.45e+07/4.42e+08 =  3% of the original kernel matrix.

torch.Size([27083, 2])
We keep 6.29e+06/3.02e+08 =  2% of the original kernel matrix.

torch.Size([34665, 2])
We keep 1.72e+07/6.24e+08 =  2% of the original kernel matrix.

torch.Size([28113, 2])
We keep 7.25e+06/3.58e+08 =  2% of the original kernel matrix.

torch.Size([52449, 2])
We keep 3.79e+07/1.25e+09 =  3% of the original kernel matrix.

torch.Size([33935, 2])
We keep 9.81e+06/5.08e+08 =  1% of the original kernel matrix.

torch.Size([387726, 2])
We keep 7.06e+08/6.17e+10 =  1% of the original kernel matrix.

torch.Size([97079, 2])
We keep 5.38e+07/3.57e+09 =  1% of the original kernel matrix.

torch.Size([8718, 2])
We keep 1.16e+06/2.38e+07 =  4% of the original kernel matrix.

torch.Size([14392, 2])
We keep 1.95e+06/7.00e+07 =  2% of the original kernel matrix.

torch.Size([4447, 2])
We keep 3.27e+05/4.45e+06 =  7% of the original kernel matrix.

torch.Size([10401, 2])
We keep 1.06e+06/3.03e+07 =  3% of the original kernel matrix.

torch.Size([15185, 2])
We keep 3.57e+06/1.06e+08 =  3% of the original kernel matrix.

torch.Size([19239, 2])
We keep 3.60e+06/1.48e+08 =  2% of the original kernel matrix.

torch.Size([54476, 2])
We keep 2.95e+07/1.32e+09 =  2% of the original kernel matrix.

torch.Size([35446, 2])
We keep 9.65e+06/5.21e+08 =  1% of the original kernel matrix.

torch.Size([24346, 2])
We keep 6.86e+06/2.63e+08 =  2% of the original kernel matrix.

torch.Size([24002, 2])
We keep 5.04e+06/2.33e+08 =  2% of the original kernel matrix.

torch.Size([4425, 2])
We keep 3.71e+05/4.38e+06 =  8% of the original kernel matrix.

torch.Size([10566, 2])
We keep 1.02e+06/3.00e+07 =  3% of the original kernel matrix.

torch.Size([3489, 2])
We keep 2.79e+05/2.89e+06 =  9% of the original kernel matrix.

torch.Size([9477, 2])
We keep 9.28e+05/2.44e+07 =  3% of the original kernel matrix.

torch.Size([25115, 2])
We keep 8.81e+06/2.92e+08 =  3% of the original kernel matrix.

torch.Size([24542, 2])
We keep 5.31e+06/2.45e+08 =  2% of the original kernel matrix.

torch.Size([4451, 2])
We keep 2.42e+05/4.19e+06 =  5% of the original kernel matrix.

torch.Size([10680, 2])
We keep 1.01e+06/2.94e+07 =  3% of the original kernel matrix.

torch.Size([14497, 2])
We keep 5.91e+06/1.12e+08 =  5% of the original kernel matrix.

torch.Size([18418, 2])
We keep 3.68e+06/1.52e+08 =  2% of the original kernel matrix.

torch.Size([5262, 2])
We keep 3.44e+05/5.85e+06 =  5% of the original kernel matrix.

torch.Size([11372, 2])
We keep 1.16e+06/3.47e+07 =  3% of the original kernel matrix.

torch.Size([16767, 2])
We keep 3.24e+06/1.17e+08 =  2% of the original kernel matrix.

torch.Size([20307, 2])
We keep 3.63e+06/1.56e+08 =  2% of the original kernel matrix.

torch.Size([28524, 2])
We keep 1.96e+07/4.12e+08 =  4% of the original kernel matrix.

torch.Size([25893, 2])
We keep 6.27e+06/2.91e+08 =  2% of the original kernel matrix.

torch.Size([10548, 2])
We keep 1.21e+06/3.23e+07 =  3% of the original kernel matrix.

torch.Size([15786, 2])
We keep 2.18e+06/8.16e+07 =  2% of the original kernel matrix.

torch.Size([7638, 2])
We keep 9.51e+05/1.65e+07 =  5% of the original kernel matrix.

torch.Size([13258, 2])
We keep 1.72e+06/5.83e+07 =  2% of the original kernel matrix.

torch.Size([148195, 2])
We keep 1.45e+08/9.68e+09 =  1% of the original kernel matrix.

torch.Size([59829, 2])
We keep 2.33e+07/1.41e+09 =  1% of the original kernel matrix.

torch.Size([243684, 2])
We keep 4.22e+08/2.72e+10 =  1% of the original kernel matrix.

torch.Size([76682, 2])
We keep 3.69e+07/2.37e+09 =  1% of the original kernel matrix.

torch.Size([69385, 2])
We keep 3.97e+07/2.20e+09 =  1% of the original kernel matrix.

torch.Size([40011, 2])
We keep 1.18e+07/6.73e+08 =  1% of the original kernel matrix.

torch.Size([129886, 2])
We keep 2.87e+08/8.81e+09 =  3% of the original kernel matrix.

torch.Size([56005, 2])
We keep 2.30e+07/1.35e+09 =  1% of the original kernel matrix.

torch.Size([18428, 2])
We keep 3.51e+06/1.42e+08 =  2% of the original kernel matrix.

torch.Size([21349, 2])
We keep 3.89e+06/1.71e+08 =  2% of the original kernel matrix.

torch.Size([24945, 2])
We keep 1.39e+07/3.00e+08 =  4% of the original kernel matrix.

torch.Size([24026, 2])
We keep 5.49e+06/2.49e+08 =  2% of the original kernel matrix.

torch.Size([16317, 2])
We keep 5.15e+06/1.17e+08 =  4% of the original kernel matrix.

torch.Size([19856, 2])
We keep 3.75e+06/1.56e+08 =  2% of the original kernel matrix.

torch.Size([159578, 2])
We keep 1.65e+08/1.25e+10 =  1% of the original kernel matrix.

torch.Size([63505, 2])
We keep 2.64e+07/1.61e+09 =  1% of the original kernel matrix.

torch.Size([21021, 2])
We keep 9.66e+06/2.27e+08 =  4% of the original kernel matrix.

torch.Size([23365, 2])
We keep 5.10e+06/2.16e+08 =  2% of the original kernel matrix.

torch.Size([29546, 2])
We keep 2.17e+07/4.45e+08 =  4% of the original kernel matrix.

torch.Size([26415, 2])
We keep 6.46e+06/3.03e+08 =  2% of the original kernel matrix.

torch.Size([8738, 2])
We keep 1.08e+06/2.36e+07 =  4% of the original kernel matrix.

torch.Size([14252, 2])
We keep 1.96e+06/6.97e+07 =  2% of the original kernel matrix.

torch.Size([326117, 2])
We keep 2.16e+09/7.00e+10 =  3% of the original kernel matrix.

torch.Size([89377, 2])
We keep 5.91e+07/3.80e+09 =  1% of the original kernel matrix.

torch.Size([32733, 2])
We keep 2.04e+07/5.40e+08 =  3% of the original kernel matrix.

torch.Size([27542, 2])
We keep 6.76e+06/3.33e+08 =  2% of the original kernel matrix.

torch.Size([28893, 2])
We keep 1.33e+07/4.24e+08 =  3% of the original kernel matrix.

torch.Size([26360, 2])
We keep 6.21e+06/2.96e+08 =  2% of the original kernel matrix.

torch.Size([333410, 2])
We keep 1.38e+09/5.57e+10 =  2% of the original kernel matrix.

torch.Size([90625, 2])
We keep 5.19e+07/3.39e+09 =  1% of the original kernel matrix.

torch.Size([50569, 2])
We keep 1.99e+07/1.00e+09 =  1% of the original kernel matrix.

torch.Size([33911, 2])
We keep 8.71e+06/4.55e+08 =  1% of the original kernel matrix.

torch.Size([48961, 2])
We keep 3.11e+07/1.23e+09 =  2% of the original kernel matrix.

torch.Size([33657, 2])
We keep 9.54e+06/5.04e+08 =  1% of the original kernel matrix.

torch.Size([171633, 2])
We keep 4.04e+08/1.56e+10 =  2% of the original kernel matrix.

torch.Size([65292, 2])
We keep 2.96e+07/1.79e+09 =  1% of the original kernel matrix.

torch.Size([115497, 2])
We keep 1.08e+08/7.41e+09 =  1% of the original kernel matrix.

torch.Size([52331, 2])
We keep 2.10e+07/1.24e+09 =  1% of the original kernel matrix.

torch.Size([62861, 2])
We keep 3.48e+07/1.68e+09 =  2% of the original kernel matrix.

torch.Size([37385, 2])
We keep 1.10e+07/5.88e+08 =  1% of the original kernel matrix.

torch.Size([11228, 2])
We keep 3.22e+06/5.90e+07 =  5% of the original kernel matrix.

torch.Size([16265, 2])
We keep 2.72e+06/1.10e+08 =  2% of the original kernel matrix.

torch.Size([37363, 2])
We keep 2.34e+07/6.85e+08 =  3% of the original kernel matrix.

torch.Size([29547, 2])
We keep 7.36e+06/3.76e+08 =  1% of the original kernel matrix.

torch.Size([8039, 2])
We keep 6.85e+05/1.75e+07 =  3% of the original kernel matrix.

torch.Size([14041, 2])
We keep 1.62e+06/6.00e+07 =  2% of the original kernel matrix.

torch.Size([88841, 2])
We keep 5.55e+07/3.35e+09 =  1% of the original kernel matrix.

torch.Size([45642, 2])
We keep 1.47e+07/8.31e+08 =  1% of the original kernel matrix.

torch.Size([692893, 2])
We keep 2.67e+09/2.10e+11 =  1% of the original kernel matrix.

torch.Size([133788, 2])
We keep 9.52e+07/6.58e+09 =  1% of the original kernel matrix.

torch.Size([21907, 2])
We keep 6.47e+06/2.34e+08 =  2% of the original kernel matrix.

torch.Size([22886, 2])
We keep 4.74e+06/2.20e+08 =  2% of the original kernel matrix.

torch.Size([59568, 2])
We keep 1.01e+08/2.58e+09 =  3% of the original kernel matrix.

torch.Size([36092, 2])
We keep 1.37e+07/7.29e+08 =  1% of the original kernel matrix.

torch.Size([15468, 2])
We keep 2.59e+06/8.87e+07 =  2% of the original kernel matrix.

torch.Size([19439, 2])
We keep 3.15e+06/1.35e+08 =  2% of the original kernel matrix.

torch.Size([61488, 2])
We keep 1.57e+08/4.09e+09 =  3% of the original kernel matrix.

torch.Size([36818, 2])
We keep 1.66e+07/9.18e+08 =  1% of the original kernel matrix.

torch.Size([54090, 2])
We keep 9.79e+07/1.62e+09 =  6% of the original kernel matrix.

torch.Size([35214, 2])
We keep 1.13e+07/5.77e+08 =  1% of the original kernel matrix.

torch.Size([6931, 2])
We keep 8.22e+05/1.53e+07 =  5% of the original kernel matrix.

torch.Size([12592, 2])
We keep 1.61e+06/5.61e+07 =  2% of the original kernel matrix.

torch.Size([33671, 2])
We keep 2.08e+07/5.85e+08 =  3% of the original kernel matrix.

torch.Size([28184, 2])
We keep 7.11e+06/3.47e+08 =  2% of the original kernel matrix.

torch.Size([450093, 2])
We keep 8.85e+08/8.57e+10 =  1% of the original kernel matrix.

torch.Size([107642, 2])
We keep 6.24e+07/4.20e+09 =  1% of the original kernel matrix.

torch.Size([22650, 2])
We keep 2.37e+07/4.50e+08 =  5% of the original kernel matrix.

torch.Size([22311, 2])
We keep 6.58e+06/3.04e+08 =  2% of the original kernel matrix.

torch.Size([199598, 2])
We keep 9.61e+08/3.25e+10 =  2% of the original kernel matrix.

torch.Size([66361, 2])
We keep 4.06e+07/2.59e+09 =  1% of the original kernel matrix.

torch.Size([29273, 2])
We keep 1.55e+07/4.28e+08 =  3% of the original kernel matrix.

torch.Size([26250, 2])
We keep 6.26e+06/2.97e+08 =  2% of the original kernel matrix.

torch.Size([37707, 2])
We keep 1.29e+07/5.98e+08 =  2% of the original kernel matrix.

torch.Size([29395, 2])
We keep 7.08e+06/3.51e+08 =  2% of the original kernel matrix.

torch.Size([6265, 2])
We keep 6.68e+05/1.06e+07 =  6% of the original kernel matrix.

torch.Size([12193, 2])
We keep 1.44e+06/4.68e+07 =  3% of the original kernel matrix.

torch.Size([46450, 2])
We keep 1.88e+07/8.58e+08 =  2% of the original kernel matrix.

torch.Size([32693, 2])
We keep 8.04e+06/4.21e+08 =  1% of the original kernel matrix.

torch.Size([140683, 2])
We keep 1.31e+08/9.48e+09 =  1% of the original kernel matrix.

torch.Size([58679, 2])
We keep 2.32e+07/1.40e+09 =  1% of the original kernel matrix.

torch.Size([3439, 2])
We keep 1.72e+05/2.32e+06 =  7% of the original kernel matrix.

torch.Size([9506, 2])
We keep 8.32e+05/2.18e+07 =  3% of the original kernel matrix.

torch.Size([742227, 2])
We keep 2.13e+09/2.01e+11 =  1% of the original kernel matrix.

torch.Size([136402, 2])
We keep 9.19e+07/6.44e+09 =  1% of the original kernel matrix.

torch.Size([35334, 2])
We keep 2.32e+07/7.78e+08 =  2% of the original kernel matrix.

torch.Size([28939, 2])
We keep 7.12e+06/4.01e+08 =  1% of the original kernel matrix.

torch.Size([45043, 2])
We keep 3.29e+07/1.03e+09 =  3% of the original kernel matrix.

torch.Size([31649, 2])
We keep 9.19e+06/4.60e+08 =  1% of the original kernel matrix.

torch.Size([11274, 2])
We keep 1.95e+06/4.97e+07 =  3% of the original kernel matrix.

torch.Size([16624, 2])
We keep 2.32e+06/1.01e+08 =  2% of the original kernel matrix.

torch.Size([14219, 2])
We keep 3.25e+06/8.60e+07 =  3% of the original kernel matrix.

torch.Size([18334, 2])
We keep 3.25e+06/1.33e+08 =  2% of the original kernel matrix.

torch.Size([4955, 2])
We keep 5.64e+05/5.85e+06 =  9% of the original kernel matrix.

torch.Size([10899, 2])
We keep 1.19e+06/3.47e+07 =  3% of the original kernel matrix.

torch.Size([5593, 2])
We keep 1.10e+06/1.13e+07 =  9% of the original kernel matrix.

torch.Size([11205, 2])
We keep 1.49e+06/4.82e+07 =  3% of the original kernel matrix.

torch.Size([11425, 2])
We keep 1.71e+06/4.57e+07 =  3% of the original kernel matrix.

torch.Size([16545, 2])
We keep 2.52e+06/9.70e+07 =  2% of the original kernel matrix.

torch.Size([129865, 2])
We keep 2.82e+08/1.02e+10 =  2% of the original kernel matrix.

torch.Size([54759, 2])
We keep 2.37e+07/1.45e+09 =  1% of the original kernel matrix.

torch.Size([1103053, 2])
We keep 2.09e+10/1.11e+12 =  1% of the original kernel matrix.

torch.Size([143129, 2])
We keep 2.09e+08/1.51e+10 =  1% of the original kernel matrix.

torch.Size([8212, 2])
We keep 1.44e+06/2.35e+07 =  6% of the original kernel matrix.

torch.Size([13727, 2])
We keep 1.98e+06/6.96e+07 =  2% of the original kernel matrix.

torch.Size([133396, 2])
We keep 1.66e+08/8.50e+09 =  1% of the original kernel matrix.

torch.Size([56539, 2])
We keep 2.23e+07/1.32e+09 =  1% of the original kernel matrix.

torch.Size([7137, 2])
We keep 9.57e+05/1.58e+07 =  6% of the original kernel matrix.

torch.Size([13007, 2])
We keep 1.72e+06/5.70e+07 =  3% of the original kernel matrix.

torch.Size([17216, 2])
We keep 4.49e+06/1.27e+08 =  3% of the original kernel matrix.

torch.Size([20410, 2])
We keep 3.74e+06/1.62e+08 =  2% of the original kernel matrix.

torch.Size([14843, 2])
We keep 8.86e+06/1.32e+08 =  6% of the original kernel matrix.

torch.Size([18577, 2])
We keep 3.99e+06/1.65e+08 =  2% of the original kernel matrix.

torch.Size([63690, 2])
We keep 6.98e+07/2.23e+09 =  3% of the original kernel matrix.

torch.Size([37919, 2])
We keep 1.27e+07/6.78e+08 =  1% of the original kernel matrix.

torch.Size([11282, 2])
We keep 2.23e+06/4.64e+07 =  4% of the original kernel matrix.

torch.Size([16246, 2])
We keep 2.47e+06/9.77e+07 =  2% of the original kernel matrix.

torch.Size([1160359, 2])
We keep 4.13e+09/4.86e+11 =  0% of the original kernel matrix.

torch.Size([176346, 2])
We keep 1.38e+08/1.00e+10 =  1% of the original kernel matrix.

torch.Size([8366, 2])
We keep 1.00e+06/2.05e+07 =  4% of the original kernel matrix.

torch.Size([14123, 2])
We keep 1.85e+06/6.51e+07 =  2% of the original kernel matrix.

torch.Size([111828, 2])
We keep 1.29e+08/5.91e+09 =  2% of the original kernel matrix.

torch.Size([51799, 2])
We keep 1.90e+07/1.10e+09 =  1% of the original kernel matrix.

torch.Size([14199, 2])
We keep 2.32e+06/7.47e+07 =  3% of the original kernel matrix.

torch.Size([18571, 2])
We keep 2.87e+06/1.24e+08 =  2% of the original kernel matrix.

torch.Size([117829, 2])
We keep 1.51e+08/6.21e+09 =  2% of the original kernel matrix.

torch.Size([52823, 2])
We keep 1.94e+07/1.13e+09 =  1% of the original kernel matrix.

torch.Size([178207, 2])
We keep 2.85e+08/1.75e+10 =  1% of the original kernel matrix.

torch.Size([65515, 2])
We keep 3.03e+07/1.90e+09 =  1% of the original kernel matrix.

torch.Size([16195, 2])
We keep 3.51e+06/1.10e+08 =  3% of the original kernel matrix.

torch.Size([19639, 2])
We keep 3.53e+06/1.51e+08 =  2% of the original kernel matrix.

torch.Size([8135, 2])
We keep 1.08e+06/1.92e+07 =  5% of the original kernel matrix.

torch.Size([13814, 2])
We keep 1.83e+06/6.29e+07 =  2% of the original kernel matrix.

torch.Size([98748, 2])
We keep 7.23e+07/4.40e+09 =  1% of the original kernel matrix.

torch.Size([48432, 2])
We keep 1.66e+07/9.53e+08 =  1% of the original kernel matrix.

torch.Size([11424, 2])
We keep 1.94e+06/4.46e+07 =  4% of the original kernel matrix.

torch.Size([16534, 2])
We keep 2.53e+06/9.58e+07 =  2% of the original kernel matrix.

torch.Size([9665, 2])
We keep 2.26e+06/4.09e+07 =  5% of the original kernel matrix.

torch.Size([14866, 2])
We keep 2.47e+06/9.18e+07 =  2% of the original kernel matrix.

torch.Size([124071, 2])
We keep 1.34e+08/7.41e+09 =  1% of the original kernel matrix.

torch.Size([53832, 2])
We keep 2.09e+07/1.24e+09 =  1% of the original kernel matrix.

torch.Size([59872, 2])
We keep 7.58e+07/2.17e+09 =  3% of the original kernel matrix.

torch.Size([37663, 2])
We keep 1.16e+07/6.69e+08 =  1% of the original kernel matrix.

torch.Size([29953, 2])
We keep 1.32e+07/4.15e+08 =  3% of the original kernel matrix.

torch.Size([27135, 2])
We keep 6.19e+06/2.92e+08 =  2% of the original kernel matrix.

torch.Size([17568, 2])
We keep 6.04e+06/1.60e+08 =  3% of the original kernel matrix.

torch.Size([19567, 2])
We keep 4.10e+06/1.82e+08 =  2% of the original kernel matrix.

torch.Size([171241, 2])
We keep 4.46e+08/1.74e+10 =  2% of the original kernel matrix.

torch.Size([64583, 2])
We keep 3.14e+07/1.89e+09 =  1% of the original kernel matrix.

torch.Size([111749, 2])
We keep 3.42e+08/7.54e+09 =  4% of the original kernel matrix.

torch.Size([51327, 2])
We keep 2.13e+07/1.25e+09 =  1% of the original kernel matrix.

torch.Size([29377, 2])
We keep 1.11e+07/4.03e+08 =  2% of the original kernel matrix.

torch.Size([26271, 2])
We keep 6.05e+06/2.88e+08 =  2% of the original kernel matrix.

torch.Size([8971, 2])
We keep 1.36e+06/2.37e+07 =  5% of the original kernel matrix.

torch.Size([14756, 2])
We keep 1.99e+06/6.99e+07 =  2% of the original kernel matrix.

torch.Size([92894, 2])
We keep 1.42e+08/4.47e+09 =  3% of the original kernel matrix.

torch.Size([46002, 2])
We keep 1.62e+07/9.60e+08 =  1% of the original kernel matrix.

torch.Size([31702, 2])
We keep 1.59e+07/4.71e+08 =  3% of the original kernel matrix.

torch.Size([26556, 2])
We keep 6.44e+06/3.12e+08 =  2% of the original kernel matrix.

torch.Size([106342, 2])
We keep 1.64e+08/6.01e+09 =  2% of the original kernel matrix.

torch.Size([49784, 2])
We keep 1.92e+07/1.11e+09 =  1% of the original kernel matrix.

torch.Size([30876, 2])
We keep 3.02e+07/5.51e+08 =  5% of the original kernel matrix.

torch.Size([26350, 2])
We keep 6.95e+06/3.37e+08 =  2% of the original kernel matrix.

torch.Size([25459, 2])
We keep 1.71e+07/3.99e+08 =  4% of the original kernel matrix.

torch.Size([23837, 2])
We keep 6.01e+06/2.87e+08 =  2% of the original kernel matrix.

torch.Size([18260, 2])
We keep 1.27e+07/1.81e+08 =  7% of the original kernel matrix.

torch.Size([20455, 2])
We keep 4.28e+06/1.93e+08 =  2% of the original kernel matrix.

torch.Size([271083, 2])
We keep 1.59e+09/5.87e+10 =  2% of the original kernel matrix.

torch.Size([76614, 2])
We keep 5.25e+07/3.48e+09 =  1% of the original kernel matrix.

torch.Size([56369, 2])
We keep 6.24e+07/1.44e+09 =  4% of the original kernel matrix.

torch.Size([35753, 2])
We keep 9.94e+06/5.44e+08 =  1% of the original kernel matrix.

torch.Size([15063, 2])
We keep 2.91e+06/8.90e+07 =  3% of the original kernel matrix.

torch.Size([19025, 2])
We keep 3.26e+06/1.35e+08 =  2% of the original kernel matrix.

torch.Size([7511, 2])
We keep 1.02e+06/1.49e+07 =  6% of the original kernel matrix.

torch.Size([13258, 2])
We keep 1.67e+06/5.54e+07 =  3% of the original kernel matrix.

torch.Size([20651, 2])
We keep 6.17e+06/1.96e+08 =  3% of the original kernel matrix.

torch.Size([22680, 2])
We keep 4.61e+06/2.01e+08 =  2% of the original kernel matrix.

torch.Size([25146, 2])
We keep 1.49e+07/3.71e+08 =  4% of the original kernel matrix.

torch.Size([23602, 2])
We keep 5.93e+06/2.77e+08 =  2% of the original kernel matrix.

torch.Size([147805, 2])
We keep 3.04e+08/1.19e+10 =  2% of the original kernel matrix.

torch.Size([59345, 2])
We keep 2.61e+07/1.56e+09 =  1% of the original kernel matrix.

torch.Size([96397, 2])
We keep 7.84e+07/4.34e+09 =  1% of the original kernel matrix.

torch.Size([47808, 2])
We keep 1.65e+07/9.46e+08 =  1% of the original kernel matrix.

torch.Size([14674, 2])
We keep 3.21e+06/8.55e+07 =  3% of the original kernel matrix.

torch.Size([18823, 2])
We keep 3.27e+06/1.33e+08 =  2% of the original kernel matrix.

torch.Size([28311, 2])
We keep 7.60e+06/3.39e+08 =  2% of the original kernel matrix.

torch.Size([25789, 2])
We keep 5.52e+06/2.64e+08 =  2% of the original kernel matrix.

torch.Size([33737, 2])
We keep 2.10e+07/5.31e+08 =  3% of the original kernel matrix.

torch.Size([27890, 2])
We keep 6.72e+06/3.31e+08 =  2% of the original kernel matrix.

torch.Size([239623, 2])
We keep 1.32e+09/2.85e+10 =  4% of the original kernel matrix.

torch.Size([76496, 2])
We keep 3.75e+07/2.42e+09 =  1% of the original kernel matrix.

torch.Size([50268, 2])
We keep 4.46e+07/1.24e+09 =  3% of the original kernel matrix.

torch.Size([33805, 2])
We keep 9.64e+06/5.05e+08 =  1% of the original kernel matrix.

torch.Size([62669, 2])
We keep 3.25e+07/1.52e+09 =  2% of the original kernel matrix.

torch.Size([37757, 2])
We keep 1.02e+07/5.59e+08 =  1% of the original kernel matrix.

torch.Size([12836, 2])
We keep 5.49e+06/7.92e+07 =  6% of the original kernel matrix.

torch.Size([17290, 2])
We keep 3.07e+06/1.28e+08 =  2% of the original kernel matrix.

torch.Size([38459, 2])
We keep 1.32e+07/6.02e+08 =  2% of the original kernel matrix.

torch.Size([29965, 2])
We keep 6.69e+06/3.52e+08 =  1% of the original kernel matrix.

torch.Size([4514, 2])
We keep 4.07e+05/5.36e+06 =  7% of the original kernel matrix.

torch.Size([10575, 2])
We keep 1.12e+06/3.32e+07 =  3% of the original kernel matrix.

torch.Size([7546, 2])
We keep 7.17e+05/1.47e+07 =  4% of the original kernel matrix.

torch.Size([13299, 2])
We keep 1.62e+06/5.51e+07 =  2% of the original kernel matrix.

torch.Size([170782, 2])
We keep 1.65e+08/1.43e+10 =  1% of the original kernel matrix.

torch.Size([65265, 2])
We keep 2.71e+07/1.72e+09 =  1% of the original kernel matrix.

torch.Size([21822, 2])
We keep 7.87e+06/2.35e+08 =  3% of the original kernel matrix.

torch.Size([22558, 2])
We keep 4.70e+06/2.20e+08 =  2% of the original kernel matrix.

torch.Size([78449, 2])
We keep 4.44e+07/2.87e+09 =  1% of the original kernel matrix.

torch.Size([42809, 2])
We keep 1.35e+07/7.69e+08 =  1% of the original kernel matrix.

torch.Size([4394, 2])
We keep 2.72e+05/4.75e+06 =  5% of the original kernel matrix.

torch.Size([10572, 2])
We keep 1.07e+06/3.13e+07 =  3% of the original kernel matrix.

torch.Size([4546, 2])
We keep 3.39e+05/4.77e+06 =  7% of the original kernel matrix.

torch.Size([10631, 2])
We keep 1.07e+06/3.13e+07 =  3% of the original kernel matrix.

torch.Size([68852, 2])
We keep 5.64e+07/2.12e+09 =  2% of the original kernel matrix.

torch.Size([39357, 2])
We keep 1.17e+07/6.60e+08 =  1% of the original kernel matrix.

torch.Size([66925, 2])
We keep 4.79e+07/2.21e+09 =  2% of the original kernel matrix.

torch.Size([38734, 2])
We keep 1.25e+07/6.75e+08 =  1% of the original kernel matrix.

torch.Size([39833, 2])
We keep 1.42e+07/6.36e+08 =  2% of the original kernel matrix.

torch.Size([30304, 2])
We keep 7.16e+06/3.62e+08 =  1% of the original kernel matrix.

torch.Size([6521, 2])
We keep 4.65e+05/9.41e+06 =  4% of the original kernel matrix.

torch.Size([12593, 2])
We keep 1.36e+06/4.40e+07 =  3% of the original kernel matrix.

torch.Size([7365, 2])
We keep 9.98e+05/1.50e+07 =  6% of the original kernel matrix.

torch.Size([13172, 2])
We keep 1.68e+06/5.56e+07 =  3% of the original kernel matrix.

torch.Size([20887, 2])
We keep 5.46e+06/2.06e+08 =  2% of the original kernel matrix.

torch.Size([22727, 2])
We keep 4.62e+06/2.06e+08 =  2% of the original kernel matrix.

torch.Size([23067, 2])
We keep 6.66e+06/2.45e+08 =  2% of the original kernel matrix.

torch.Size([23554, 2])
We keep 4.93e+06/2.25e+08 =  2% of the original kernel matrix.

torch.Size([137646, 2])
We keep 5.95e+08/1.11e+10 =  5% of the original kernel matrix.

torch.Size([57388, 2])
We keep 2.42e+07/1.51e+09 =  1% of the original kernel matrix.

torch.Size([8508, 2])
We keep 1.20e+06/2.28e+07 =  5% of the original kernel matrix.

torch.Size([14258, 2])
We keep 1.90e+06/6.86e+07 =  2% of the original kernel matrix.

torch.Size([6787, 2])
We keep 1.51e+06/1.73e+07 =  8% of the original kernel matrix.

torch.Size([12400, 2])
We keep 1.73e+06/5.96e+07 =  2% of the original kernel matrix.

torch.Size([30397, 2])
We keep 2.35e+07/5.13e+08 =  4% of the original kernel matrix.

torch.Size([26108, 2])
We keep 6.89e+06/3.25e+08 =  2% of the original kernel matrix.

torch.Size([13968, 2])
We keep 4.89e+06/8.78e+07 =  5% of the original kernel matrix.

torch.Size([18077, 2])
We keep 3.29e+06/1.35e+08 =  2% of the original kernel matrix.

torch.Size([220906, 2])
We keep 4.05e+08/2.34e+10 =  1% of the original kernel matrix.

torch.Size([74150, 2])
We keep 3.18e+07/2.19e+09 =  1% of the original kernel matrix.

torch.Size([13880, 2])
We keep 2.48e+06/6.84e+07 =  3% of the original kernel matrix.

torch.Size([18316, 2])
We keep 2.84e+06/1.19e+08 =  2% of the original kernel matrix.

torch.Size([41850, 2])
We keep 2.01e+07/7.66e+08 =  2% of the original kernel matrix.

torch.Size([30648, 2])
We keep 7.92e+06/3.97e+08 =  1% of the original kernel matrix.

torch.Size([91508, 2])
We keep 6.87e+07/3.99e+09 =  1% of the original kernel matrix.

torch.Size([46696, 2])
We keep 1.59e+07/9.07e+08 =  1% of the original kernel matrix.

torch.Size([102560, 2])
We keep 1.59e+08/4.68e+09 =  3% of the original kernel matrix.

torch.Size([49190, 2])
We keep 1.71e+07/9.82e+08 =  1% of the original kernel matrix.

torch.Size([66363, 2])
We keep 5.21e+07/1.98e+09 =  2% of the original kernel matrix.

torch.Size([39016, 2])
We keep 1.19e+07/6.38e+08 =  1% of the original kernel matrix.

torch.Size([185418, 2])
We keep 2.35e+08/1.70e+10 =  1% of the original kernel matrix.

torch.Size([68141, 2])
We keep 3.03e+07/1.87e+09 =  1% of the original kernel matrix.

torch.Size([3829, 2])
We keep 1.93e+05/2.95e+06 =  6% of the original kernel matrix.

torch.Size([9978, 2])
We keep 8.82e+05/2.47e+07 =  3% of the original kernel matrix.

torch.Size([44071, 2])
We keep 1.87e+07/8.00e+08 =  2% of the original kernel matrix.

torch.Size([31658, 2])
We keep 8.00e+06/4.06e+08 =  1% of the original kernel matrix.

torch.Size([19527, 2])
We keep 3.48e+06/1.44e+08 =  2% of the original kernel matrix.

torch.Size([21788, 2])
We keep 3.84e+06/1.72e+08 =  2% of the original kernel matrix.

torch.Size([210923, 2])
We keep 3.83e+08/2.03e+10 =  1% of the original kernel matrix.

torch.Size([71595, 2])
We keep 3.24e+07/2.05e+09 =  1% of the original kernel matrix.

torch.Size([86991, 2])
We keep 9.29e+07/3.85e+09 =  2% of the original kernel matrix.

torch.Size([44904, 2])
We keep 1.55e+07/8.91e+08 =  1% of the original kernel matrix.

torch.Size([29471, 2])
We keep 1.28e+07/4.31e+08 =  2% of the original kernel matrix.

torch.Size([25643, 2])
We keep 6.14e+06/2.98e+08 =  2% of the original kernel matrix.

torch.Size([72215, 2])
We keep 5.68e+07/2.41e+09 =  2% of the original kernel matrix.

torch.Size([40870, 2])
We keep 1.28e+07/7.05e+08 =  1% of the original kernel matrix.

torch.Size([13428, 2])
We keep 1.95e+06/6.30e+07 =  3% of the original kernel matrix.

torch.Size([17730, 2])
We keep 2.80e+06/1.14e+08 =  2% of the original kernel matrix.

torch.Size([13092, 2])
We keep 6.54e+06/1.09e+08 =  5% of the original kernel matrix.

torch.Size([17131, 2])
We keep 3.66e+06/1.50e+08 =  2% of the original kernel matrix.

torch.Size([7863, 2])
We keep 1.31e+06/1.76e+07 =  7% of the original kernel matrix.

torch.Size([13672, 2])
We keep 1.75e+06/6.02e+07 =  2% of the original kernel matrix.

torch.Size([21506, 2])
We keep 9.42e+06/2.14e+08 =  4% of the original kernel matrix.

torch.Size([22360, 2])
We keep 4.75e+06/2.10e+08 =  2% of the original kernel matrix.

torch.Size([179146, 2])
We keep 2.91e+08/1.59e+10 =  1% of the original kernel matrix.

torch.Size([67034, 2])
We keep 2.97e+07/1.81e+09 =  1% of the original kernel matrix.

torch.Size([118832, 2])
We keep 2.00e+08/7.75e+09 =  2% of the original kernel matrix.

torch.Size([53800, 2])
We keep 2.16e+07/1.26e+09 =  1% of the original kernel matrix.

torch.Size([10757, 2])
We keep 6.40e+06/8.38e+07 =  7% of the original kernel matrix.

torch.Size([15603, 2])
We keep 3.27e+06/1.31e+08 =  2% of the original kernel matrix.

torch.Size([26683, 2])
We keep 6.79e+06/2.91e+08 =  2% of the original kernel matrix.

torch.Size([25298, 2])
We keep 4.96e+06/2.45e+08 =  2% of the original kernel matrix.

torch.Size([14087, 2])
We keep 2.13e+06/6.65e+07 =  3% of the original kernel matrix.

torch.Size([18517, 2])
We keep 2.85e+06/1.17e+08 =  2% of the original kernel matrix.

torch.Size([138292, 2])
We keep 2.54e+08/1.16e+10 =  2% of the original kernel matrix.

torch.Size([58431, 2])
We keep 2.46e+07/1.54e+09 =  1% of the original kernel matrix.

torch.Size([21416, 2])
We keep 4.67e+06/1.92e+08 =  2% of the original kernel matrix.

torch.Size([22963, 2])
We keep 4.45e+06/1.99e+08 =  2% of the original kernel matrix.

torch.Size([15519, 2])
We keep 4.87e+06/1.12e+08 =  4% of the original kernel matrix.

torch.Size([19263, 2])
We keep 3.56e+06/1.52e+08 =  2% of the original kernel matrix.

torch.Size([93199, 2])
We keep 1.43e+08/5.01e+09 =  2% of the original kernel matrix.

torch.Size([46972, 2])
We keep 1.71e+07/1.02e+09 =  1% of the original kernel matrix.

torch.Size([120398, 2])
We keep 1.61e+08/7.64e+09 =  2% of the original kernel matrix.

torch.Size([53880, 2])
We keep 2.16e+07/1.25e+09 =  1% of the original kernel matrix.

torch.Size([12397, 2])
We keep 5.38e+06/9.21e+07 =  5% of the original kernel matrix.

torch.Size([16655, 2])
We keep 3.44e+06/1.38e+08 =  2% of the original kernel matrix.

torch.Size([30194, 2])
We keep 1.25e+07/3.92e+08 =  3% of the original kernel matrix.

torch.Size([26706, 2])
We keep 5.89e+06/2.84e+08 =  2% of the original kernel matrix.

torch.Size([8668, 2])
We keep 8.65e+05/2.20e+07 =  3% of the original kernel matrix.

torch.Size([14573, 2])
We keep 1.81e+06/6.73e+07 =  2% of the original kernel matrix.

time for making ranges is 2.628074884414673
Sorting X and nu_X
time for sorting X is 0.07914257049560547
Sorting Z and nu_Z
time for sorting Z is 0.0002682209014892578
Starting Optim
sum tnu_Z before tensor(23367328., device='cuda:0')
c= tensor(1768.9528, device='cuda:0')
c= tensor(146047.6094, device='cuda:0')
c= tensor(150351.5781, device='cuda:0')
c= tensor(153671.8906, device='cuda:0')
c= tensor(999166.8750, device='cuda:0')
c= tensor(1543136., device='cuda:0')
c= tensor(1930792.2500, device='cuda:0')
c= tensor(2407396.5000, device='cuda:0')
c= tensor(2491860.2500, device='cuda:0')
c= tensor(6257307., device='cuda:0')
c= tensor(6268346., device='cuda:0')
c= tensor(8758570., device='cuda:0')
c= tensor(8787599., device='cuda:0')
c= tensor(18552816., device='cuda:0')
c= tensor(18701186., device='cuda:0')
c= tensor(19084140., device='cuda:0')
c= tensor(19470850., device='cuda:0')
c= tensor(19831862., device='cuda:0')
c= tensor(42821772., device='cuda:0')
c= tensor(45021648., device='cuda:0')
c= tensor(45111544., device='cuda:0')
c= tensor(82076328., device='cuda:0')
c= tensor(82117384., device='cuda:0')
c= tensor(82157488., device='cuda:0')
c= tensor(82379296., device='cuda:0')
c= tensor(83393072., device='cuda:0')
c= tensor(84452016., device='cuda:0')
c= tensor(84480792., device='cuda:0')
c= tensor(87427688., device='cuda:0')
c= tensor(9.1462e+08, device='cuda:0')
c= tensor(9.1464e+08, device='cuda:0')
c= tensor(9.5409e+08, device='cuda:0')
c= tensor(9.5432e+08, device='cuda:0')
c= tensor(9.5435e+08, device='cuda:0')
c= tensor(9.5444e+08, device='cuda:0')
c= tensor(9.5623e+08, device='cuda:0')
c= tensor(9.5808e+08, device='cuda:0')
c= tensor(9.5808e+08, device='cuda:0')
c= tensor(9.5809e+08, device='cuda:0')
c= tensor(9.5809e+08, device='cuda:0')
c= tensor(9.5810e+08, device='cuda:0')
c= tensor(9.5810e+08, device='cuda:0')
c= tensor(9.5810e+08, device='cuda:0')
c= tensor(9.5810e+08, device='cuda:0')
c= tensor(9.5810e+08, device='cuda:0')
c= tensor(9.5810e+08, device='cuda:0')
c= tensor(9.5812e+08, device='cuda:0')
c= tensor(9.5813e+08, device='cuda:0')
c= tensor(9.5813e+08, device='cuda:0')
c= tensor(9.5816e+08, device='cuda:0')
c= tensor(9.5823e+08, device='cuda:0')
c= tensor(9.5823e+08, device='cuda:0')
c= tensor(9.5824e+08, device='cuda:0')
c= tensor(9.5824e+08, device='cuda:0')
c= tensor(9.5826e+08, device='cuda:0')
c= tensor(9.5828e+08, device='cuda:0')
c= tensor(9.5828e+08, device='cuda:0')
c= tensor(9.5829e+08, device='cuda:0')
c= tensor(9.5829e+08, device='cuda:0')
c= tensor(9.5830e+08, device='cuda:0')
c= tensor(9.5834e+08, device='cuda:0')
c= tensor(9.5835e+08, device='cuda:0')
c= tensor(9.5842e+08, device='cuda:0')
c= tensor(9.5843e+08, device='cuda:0')
c= tensor(9.5844e+08, device='cuda:0')
c= tensor(9.5844e+08, device='cuda:0')
c= tensor(9.5844e+08, device='cuda:0')
c= tensor(9.5845e+08, device='cuda:0')
c= tensor(9.5846e+08, device='cuda:0')
c= tensor(9.5846e+08, device='cuda:0')
c= tensor(9.5848e+08, device='cuda:0')
c= tensor(9.5849e+08, device='cuda:0')
c= tensor(9.5849e+08, device='cuda:0')
c= tensor(9.5849e+08, device='cuda:0')
c= tensor(9.5850e+08, device='cuda:0')
c= tensor(9.5851e+08, device='cuda:0')
c= tensor(9.5851e+08, device='cuda:0')
c= tensor(9.5851e+08, device='cuda:0')
c= tensor(9.5852e+08, device='cuda:0')
c= tensor(9.5855e+08, device='cuda:0')
c= tensor(9.5856e+08, device='cuda:0')
c= tensor(9.5856e+08, device='cuda:0')
c= tensor(9.5858e+08, device='cuda:0')
c= tensor(9.5858e+08, device='cuda:0')
c= tensor(9.5859e+08, device='cuda:0')
c= tensor(9.5859e+08, device='cuda:0')
c= tensor(9.5859e+08, device='cuda:0')
c= tensor(9.5859e+08, device='cuda:0')
c= tensor(9.5861e+08, device='cuda:0')
c= tensor(9.5861e+08, device='cuda:0')
c= tensor(9.5862e+08, device='cuda:0')
c= tensor(9.5862e+08, device='cuda:0')
c= tensor(9.5862e+08, device='cuda:0')
c= tensor(9.5863e+08, device='cuda:0')
c= tensor(9.5864e+08, device='cuda:0')
c= tensor(9.5865e+08, device='cuda:0')
c= tensor(9.5866e+08, device='cuda:0')
c= tensor(9.5867e+08, device='cuda:0')
c= tensor(9.5870e+08, device='cuda:0')
c= tensor(9.5870e+08, device='cuda:0')
c= tensor(9.5876e+08, device='cuda:0')
c= tensor(9.5877e+08, device='cuda:0')
c= tensor(9.5878e+08, device='cuda:0')
c= tensor(9.5878e+08, device='cuda:0')
c= tensor(9.5879e+08, device='cuda:0')
c= tensor(9.5879e+08, device='cuda:0')
c= tensor(9.5880e+08, device='cuda:0')
c= tensor(9.5880e+08, device='cuda:0')
c= tensor(9.5880e+08, device='cuda:0')
c= tensor(9.5880e+08, device='cuda:0')
c= tensor(9.5881e+08, device='cuda:0')
c= tensor(9.5881e+08, device='cuda:0')
c= tensor(9.5881e+08, device='cuda:0')
c= tensor(9.5882e+08, device='cuda:0')
c= tensor(9.5883e+08, device='cuda:0')
c= tensor(9.5883e+08, device='cuda:0')
c= tensor(9.5883e+08, device='cuda:0')
c= tensor(9.5883e+08, device='cuda:0')
c= tensor(9.5885e+08, device='cuda:0')
c= tensor(9.5885e+08, device='cuda:0')
c= tensor(9.5893e+08, device='cuda:0')
c= tensor(9.5894e+08, device='cuda:0')
c= tensor(9.5894e+08, device='cuda:0')
c= tensor(9.5894e+08, device='cuda:0')
c= tensor(9.5895e+08, device='cuda:0')
c= tensor(9.5896e+08, device='cuda:0')
c= tensor(9.5896e+08, device='cuda:0')
c= tensor(9.5896e+08, device='cuda:0')
c= tensor(9.5909e+08, device='cuda:0')
c= tensor(9.5909e+08, device='cuda:0')
c= tensor(9.5911e+08, device='cuda:0')
c= tensor(9.5912e+08, device='cuda:0')
c= tensor(9.5913e+08, device='cuda:0')
c= tensor(9.5913e+08, device='cuda:0')
c= tensor(9.5913e+08, device='cuda:0')
c= tensor(9.5913e+08, device='cuda:0')
c= tensor(9.5914e+08, device='cuda:0')
c= tensor(9.5914e+08, device='cuda:0')
c= tensor(9.5914e+08, device='cuda:0')
c= tensor(9.5914e+08, device='cuda:0')
c= tensor(9.5914e+08, device='cuda:0')
c= tensor(9.5915e+08, device='cuda:0')
c= tensor(9.5917e+08, device='cuda:0')
c= tensor(9.5937e+08, device='cuda:0')
c= tensor(9.5941e+08, device='cuda:0')
c= tensor(9.5941e+08, device='cuda:0')
c= tensor(9.5941e+08, device='cuda:0')
c= tensor(9.5941e+08, device='cuda:0')
c= tensor(9.5941e+08, device='cuda:0')
c= tensor(9.5942e+08, device='cuda:0')
c= tensor(9.5942e+08, device='cuda:0')
c= tensor(9.5944e+08, device='cuda:0')
c= tensor(9.5944e+08, device='cuda:0')
c= tensor(9.5948e+08, device='cuda:0')
c= tensor(9.5948e+08, device='cuda:0')
c= tensor(9.5955e+08, device='cuda:0')
c= tensor(9.5956e+08, device='cuda:0')
c= tensor(9.5956e+08, device='cuda:0')
c= tensor(9.5957e+08, device='cuda:0')
c= tensor(9.5957e+08, device='cuda:0')
c= tensor(9.5958e+08, device='cuda:0')
c= tensor(9.5958e+08, device='cuda:0')
c= tensor(9.5959e+08, device='cuda:0')
c= tensor(9.5959e+08, device='cuda:0')
c= tensor(9.5960e+08, device='cuda:0')
c= tensor(9.5960e+08, device='cuda:0')
c= tensor(9.5961e+08, device='cuda:0')
c= tensor(9.5961e+08, device='cuda:0')
c= tensor(9.5962e+08, device='cuda:0')
c= tensor(9.5962e+08, device='cuda:0')
c= tensor(9.5962e+08, device='cuda:0')
c= tensor(9.5962e+08, device='cuda:0')
c= tensor(9.5964e+08, device='cuda:0')
c= tensor(9.5964e+08, device='cuda:0')
c= tensor(9.5965e+08, device='cuda:0')
c= tensor(9.5967e+08, device='cuda:0')
c= tensor(9.5967e+08, device='cuda:0')
c= tensor(9.5968e+08, device='cuda:0')
c= tensor(9.5969e+08, device='cuda:0')
c= tensor(9.5970e+08, device='cuda:0')
c= tensor(9.5971e+08, device='cuda:0')
c= tensor(9.5971e+08, device='cuda:0')
c= tensor(9.5972e+08, device='cuda:0')
c= tensor(9.5973e+08, device='cuda:0')
c= tensor(9.5974e+08, device='cuda:0')
c= tensor(9.5975e+08, device='cuda:0')
c= tensor(9.5975e+08, device='cuda:0')
c= tensor(9.5976e+08, device='cuda:0')
c= tensor(9.5976e+08, device='cuda:0')
c= tensor(9.5987e+08, device='cuda:0')
c= tensor(9.5987e+08, device='cuda:0')
c= tensor(9.5987e+08, device='cuda:0')
c= tensor(9.5988e+08, device='cuda:0')
c= tensor(9.5988e+08, device='cuda:0')
c= tensor(9.5988e+08, device='cuda:0')
c= tensor(9.5989e+08, device='cuda:0')
c= tensor(9.5989e+08, device='cuda:0')
c= tensor(9.5989e+08, device='cuda:0')
c= tensor(9.5990e+08, device='cuda:0')
c= tensor(9.5990e+08, device='cuda:0')
c= tensor(9.5993e+08, device='cuda:0')
c= tensor(9.5993e+08, device='cuda:0')
c= tensor(9.6009e+08, device='cuda:0')
c= tensor(9.6009e+08, device='cuda:0')
c= tensor(9.6010e+08, device='cuda:0')
c= tensor(9.6010e+08, device='cuda:0')
c= tensor(9.6012e+08, device='cuda:0')
c= tensor(9.6013e+08, device='cuda:0')
c= tensor(9.6014e+08, device='cuda:0')
c= tensor(9.6019e+08, device='cuda:0')
c= tensor(9.6021e+08, device='cuda:0')
c= tensor(9.6021e+08, device='cuda:0')
c= tensor(9.6021e+08, device='cuda:0')
c= tensor(9.6022e+08, device='cuda:0')
c= tensor(9.6022e+08, device='cuda:0')
c= tensor(9.6022e+08, device='cuda:0')
c= tensor(9.6022e+08, device='cuda:0')
c= tensor(9.6023e+08, device='cuda:0')
c= tensor(9.6023e+08, device='cuda:0')
c= tensor(9.6024e+08, device='cuda:0')
c= tensor(9.6025e+08, device='cuda:0')
c= tensor(9.6025e+08, device='cuda:0')
c= tensor(9.6026e+08, device='cuda:0')
c= tensor(9.6026e+08, device='cuda:0')
c= tensor(9.6027e+08, device='cuda:0')
c= tensor(9.6028e+08, device='cuda:0')
c= tensor(9.6028e+08, device='cuda:0')
c= tensor(9.6028e+08, device='cuda:0')
c= tensor(9.6029e+08, device='cuda:0')
c= tensor(9.6029e+08, device='cuda:0')
c= tensor(9.6029e+08, device='cuda:0')
c= tensor(9.6030e+08, device='cuda:0')
c= tensor(9.6030e+08, device='cuda:0')
c= tensor(9.6031e+08, device='cuda:0')
c= tensor(9.6031e+08, device='cuda:0')
c= tensor(9.6031e+08, device='cuda:0')
c= tensor(9.6032e+08, device='cuda:0')
c= tensor(9.6034e+08, device='cuda:0')
c= tensor(9.6034e+08, device='cuda:0')
c= tensor(9.6037e+08, device='cuda:0')
c= tensor(9.6369e+08, device='cuda:0')
c= tensor(9.6419e+08, device='cuda:0')
c= tensor(9.6421e+08, device='cuda:0')
c= tensor(9.6421e+08, device='cuda:0')
c= tensor(9.6422e+08, device='cuda:0')
c= tensor(9.6432e+08, device='cuda:0')
c= tensor(9.6883e+08, device='cuda:0')
c= tensor(9.6884e+08, device='cuda:0')
c= tensor(9.7319e+08, device='cuda:0')
c= tensor(9.7426e+08, device='cuda:0')
c= tensor(9.7434e+08, device='cuda:0')
c= tensor(9.8232e+08, device='cuda:0')
c= tensor(9.8233e+08, device='cuda:0')
c= tensor(9.8235e+08, device='cuda:0')
c= tensor(9.9205e+08, device='cuda:0')
c= tensor(1.0320e+09, device='cuda:0')
c= tensor(1.0321e+09, device='cuda:0')
c= tensor(1.0324e+09, device='cuda:0')
c= tensor(1.0324e+09, device='cuda:0')
c= tensor(1.0452e+09, device='cuda:0')
c= tensor(1.0475e+09, device='cuda:0')
c= tensor(1.0659e+09, device='cuda:0')
c= tensor(1.0664e+09, device='cuda:0')
c= tensor(1.0664e+09, device='cuda:0')
c= tensor(1.0664e+09, device='cuda:0')
c= tensor(1.0936e+09, device='cuda:0')
c= tensor(1.0937e+09, device='cuda:0')
c= tensor(1.0937e+09, device='cuda:0')
c= tensor(1.0942e+09, device='cuda:0')
c= tensor(1.0949e+09, device='cuda:0')
c= tensor(1.1065e+09, device='cuda:0')
c= tensor(1.1109e+09, device='cuda:0')
c= tensor(1.1109e+09, device='cuda:0')
c= tensor(1.1111e+09, device='cuda:0')
c= tensor(1.1111e+09, device='cuda:0')
c= tensor(1.1127e+09, device='cuda:0')
c= tensor(1.1148e+09, device='cuda:0')
c= tensor(1.1169e+09, device='cuda:0')
c= tensor(1.1174e+09, device='cuda:0')
c= tensor(1.1174e+09, device='cuda:0')
c= tensor(1.1175e+09, device='cuda:0')
c= tensor(1.1186e+09, device='cuda:0')
c= tensor(1.1205e+09, device='cuda:0')
c= tensor(1.1220e+09, device='cuda:0')
c= tensor(1.1220e+09, device='cuda:0')
c= tensor(1.1465e+09, device='cuda:0')
c= tensor(1.1466e+09, device='cuda:0')
c= tensor(1.1467e+09, device='cuda:0')
c= tensor(1.1484e+09, device='cuda:0')
c= tensor(1.1484e+09, device='cuda:0')
c= tensor(1.1500e+09, device='cuda:0')
c= tensor(1.1734e+09, device='cuda:0')
c= tensor(1.1933e+09, device='cuda:0')
c= tensor(1.1935e+09, device='cuda:0')
c= tensor(1.1937e+09, device='cuda:0')
c= tensor(1.1937e+09, device='cuda:0')
c= tensor(1.1937e+09, device='cuda:0')
c= tensor(1.1949e+09, device='cuda:0')
c= tensor(1.1949e+09, device='cuda:0')
c= tensor(1.1951e+09, device='cuda:0')
c= tensor(1.2141e+09, device='cuda:0')
c= tensor(1.2153e+09, device='cuda:0')
c= tensor(1.2154e+09, device='cuda:0')
c= tensor(1.2155e+09, device='cuda:0')
c= tensor(1.2179e+09, device='cuda:0')
c= tensor(1.2180e+09, device='cuda:0')
c= tensor(1.2180e+09, device='cuda:0')
c= tensor(1.2182e+09, device='cuda:0')
c= tensor(1.2311e+09, device='cuda:0')
c= tensor(1.2312e+09, device='cuda:0')
c= tensor(1.2329e+09, device='cuda:0')
c= tensor(1.2329e+09, device='cuda:0')
c= tensor(1.2343e+09, device='cuda:0')
c= tensor(1.2349e+09, device='cuda:0')
c= tensor(1.2544e+09, device='cuda:0')
c= tensor(1.2554e+09, device='cuda:0')
c= tensor(1.2555e+09, device='cuda:0')
c= tensor(1.2580e+09, device='cuda:0')
c= tensor(1.2607e+09, device='cuda:0')
c= tensor(1.2608e+09, device='cuda:0')
c= tensor(1.2666e+09, device='cuda:0')
c= tensor(1.2746e+09, device='cuda:0')
c= tensor(1.3022e+09, device='cuda:0')
c= tensor(1.3036e+09, device='cuda:0')
c= tensor(1.3036e+09, device='cuda:0')
c= tensor(1.3037e+09, device='cuda:0')
c= tensor(1.3046e+09, device='cuda:0')
c= tensor(1.3048e+09, device='cuda:0')
c= tensor(1.3050e+09, device='cuda:0')
c= tensor(1.3050e+09, device='cuda:0')
c= tensor(1.3063e+09, device='cuda:0')
c= tensor(1.3110e+09, device='cuda:0')
c= tensor(1.3121e+09, device='cuda:0')
c= tensor(1.3121e+09, device='cuda:0')
c= tensor(1.3124e+09, device='cuda:0')
c= tensor(1.3124e+09, device='cuda:0')
c= tensor(1.3124e+09, device='cuda:0')
c= tensor(1.3125e+09, device='cuda:0')
c= tensor(1.3125e+09, device='cuda:0')
c= tensor(1.3130e+09, device='cuda:0')
c= tensor(1.3134e+09, device='cuda:0')
c= tensor(1.3137e+09, device='cuda:0')
c= tensor(1.3138e+09, device='cuda:0')
c= tensor(1.3138e+09, device='cuda:0')
c= tensor(1.3639e+09, device='cuda:0')
c= tensor(1.3640e+09, device='cuda:0')
c= tensor(1.3673e+09, device='cuda:0')
c= tensor(1.3673e+09, device='cuda:0')
c= tensor(1.3673e+09, device='cuda:0')
c= tensor(1.3673e+09, device='cuda:0')
c= tensor(1.3685e+09, device='cuda:0')
c= tensor(1.3686e+09, device='cuda:0')
c= tensor(1.3686e+09, device='cuda:0')
c= tensor(1.3686e+09, device='cuda:0')
c= tensor(1.3686e+09, device='cuda:0')
c= tensor(1.3837e+09, device='cuda:0')
c= tensor(1.3841e+09, device='cuda:0')
c= tensor(1.3845e+09, device='cuda:0')
c= tensor(1.3880e+09, device='cuda:0')
c= tensor(1.3994e+09, device='cuda:0')
c= tensor(1.3994e+09, device='cuda:0')
c= tensor(1.3994e+09, device='cuda:0')
c= tensor(1.3994e+09, device='cuda:0')
c= tensor(1.3994e+09, device='cuda:0')
c= tensor(1.3995e+09, device='cuda:0')
c= tensor(1.3995e+09, device='cuda:0')
c= tensor(1.3996e+09, device='cuda:0')
c= tensor(1.3996e+09, device='cuda:0')
c= tensor(1.3996e+09, device='cuda:0')
c= tensor(1.3996e+09, device='cuda:0')
c= tensor(1.4198e+09, device='cuda:0')
c= tensor(1.4199e+09, device='cuda:0')
c= tensor(1.4210e+09, device='cuda:0')
c= tensor(1.4210e+09, device='cuda:0')
c= tensor(1.4212e+09, device='cuda:0')
c= tensor(1.4218e+09, device='cuda:0')
c= tensor(1.7087e+09, device='cuda:0')
c= tensor(1.7763e+09, device='cuda:0')
c= tensor(1.7765e+09, device='cuda:0')
c= tensor(1.7774e+09, device='cuda:0')
c= tensor(1.7774e+09, device='cuda:0')
c= tensor(1.7783e+09, device='cuda:0')
c= tensor(1.7799e+09, device='cuda:0')
c= tensor(1.7805e+09, device='cuda:0')
c= tensor(1.7805e+09, device='cuda:0')
c= tensor(1.7814e+09, device='cuda:0')
c= tensor(1.8236e+09, device='cuda:0')
c= tensor(1.8237e+09, device='cuda:0')
c= tensor(1.8238e+09, device='cuda:0')
c= tensor(1.8239e+09, device='cuda:0')
c= tensor(1.8241e+09, device='cuda:0')
c= tensor(1.8241e+09, device='cuda:0')
c= tensor(1.8264e+09, device='cuda:0')
c= tensor(1.8268e+09, device='cuda:0')
c= tensor(1.8268e+09, device='cuda:0')
c= tensor(1.8292e+09, device='cuda:0')
c= tensor(1.8294e+09, device='cuda:0')
c= tensor(1.8294e+09, device='cuda:0')
c= tensor(1.8316e+09, device='cuda:0')
c= tensor(1.8346e+09, device='cuda:0')
c= tensor(1.8371e+09, device='cuda:0')
c= tensor(1.8383e+09, device='cuda:0')
c= tensor(1.8508e+09, device='cuda:0')
c= tensor(1.8508e+09, device='cuda:0')
c= tensor(1.8512e+09, device='cuda:0')
c= tensor(1.8524e+09, device='cuda:0')
c= tensor(1.8560e+09, device='cuda:0')
c= tensor(1.8561e+09, device='cuda:0')
c= tensor(1.9146e+09, device='cuda:0')
c= tensor(1.9389e+09, device='cuda:0')
c= tensor(1.9403e+09, device='cuda:0')
c= tensor(1.9422e+09, device='cuda:0')
c= tensor(1.9428e+09, device='cuda:0')
c= tensor(1.9431e+09, device='cuda:0')
c= tensor(1.9431e+09, device='cuda:0')
c= tensor(1.9432e+09, device='cuda:0')
c= tensor(1.9470e+09, device='cuda:0')
c= tensor(1.9499e+09, device='cuda:0')
c= tensor(1.9719e+09, device='cuda:0')
c= tensor(1.9752e+09, device='cuda:0')
c= tensor(1.9764e+09, device='cuda:0')
c= tensor(1.9766e+09, device='cuda:0')
c= tensor(1.9783e+09, device='cuda:0')
c= tensor(1.9783e+09, device='cuda:0')
c= tensor(1.9784e+09, device='cuda:0')
c= tensor(1.9861e+09, device='cuda:0')
c= tensor(1.9867e+09, device='cuda:0')
c= tensor(1.9867e+09, device='cuda:0')
c= tensor(1.9867e+09, device='cuda:0')
c= tensor(1.9936e+09, device='cuda:0')
c= tensor(1.9938e+09, device='cuda:0')
c= tensor(1.9949e+09, device='cuda:0')
c= tensor(1.9950e+09, device='cuda:0')
c= tensor(1.9952e+09, device='cuda:0')
c= tensor(1.9952e+09, device='cuda:0')
c= tensor(1.9952e+09, device='cuda:0')
c= tensor(1.9954e+09, device='cuda:0')
c= tensor(1.9955e+09, device='cuda:0')
c= tensor(1.9955e+09, device='cuda:0')
c= tensor(1.9982e+09, device='cuda:0')
c= tensor(1.9982e+09, device='cuda:0')
c= tensor(1.9984e+09, device='cuda:0')
c= tensor(1.9984e+09, device='cuda:0')
c= tensor(1.9996e+09, device='cuda:0')
c= tensor(2.0000e+09, device='cuda:0')
c= tensor(2.0002e+09, device='cuda:0')
c= tensor(2.0003e+09, device='cuda:0')
c= tensor(2.0006e+09, device='cuda:0')
c= tensor(2.0015e+09, device='cuda:0')
c= tensor(2.0278e+09, device='cuda:0')
c= tensor(2.0278e+09, device='cuda:0')
c= tensor(2.0278e+09, device='cuda:0')
c= tensor(2.0287e+09, device='cuda:0')
c= tensor(2.0288e+09, device='cuda:0')
c= tensor(2.0704e+09, device='cuda:0')
c= tensor(2.0704e+09, device='cuda:0')
c= tensor(2.0735e+09, device='cuda:0')
c= tensor(2.0932e+09, device='cuda:0')
c= tensor(2.0932e+09, device='cuda:0')
c= tensor(2.1019e+09, device='cuda:0')
c= tensor(2.1021e+09, device='cuda:0')
c= tensor(2.1890e+09, device='cuda:0')
c= tensor(2.1891e+09, device='cuda:0')
c= tensor(2.1893e+09, device='cuda:0')
c= tensor(2.1898e+09, device='cuda:0')
c= tensor(2.1898e+09, device='cuda:0')
c= tensor(2.1898e+09, device='cuda:0')
c= tensor(2.1910e+09, device='cuda:0')
c= tensor(2.1912e+09, device='cuda:0')
c= tensor(2.2349e+09, device='cuda:0')
c= tensor(2.2350e+09, device='cuda:0')
c= tensor(2.2350e+09, device='cuda:0')
c= tensor(2.2350e+09, device='cuda:0')
c= tensor(2.2411e+09, device='cuda:0')
c= tensor(2.2473e+09, device='cuda:0')
c= tensor(2.2532e+09, device='cuda:0')
c= tensor(2.2550e+09, device='cuda:0')
c= tensor(2.2550e+09, device='cuda:0')
c= tensor(2.2550e+09, device='cuda:0')
c= tensor(2.2551e+09, device='cuda:0')
c= tensor(2.4928e+09, device='cuda:0')
c= tensor(2.4928e+09, device='cuda:0')
c= tensor(2.4928e+09, device='cuda:0')
c= tensor(2.4966e+09, device='cuda:0')
c= tensor(2.4979e+09, device='cuda:0')
c= tensor(2.4979e+09, device='cuda:0')
c= tensor(2.4980e+09, device='cuda:0')
c= tensor(2.4993e+09, device='cuda:0')
c= tensor(2.4999e+09, device='cuda:0')
c= tensor(2.5000e+09, device='cuda:0')
c= tensor(2.5002e+09, device='cuda:0')
c= tensor(2.5358e+09, device='cuda:0')
c= tensor(2.5382e+09, device='cuda:0')
c= tensor(2.5491e+09, device='cuda:0')
c= tensor(2.5506e+09, device='cuda:0')
c= tensor(2.5507e+09, device='cuda:0')
c= tensor(2.5515e+09, device='cuda:0')
c= tensor(2.5515e+09, device='cuda:0')
c= tensor(2.5516e+09, device='cuda:0')
c= tensor(2.5516e+09, device='cuda:0')
c= tensor(2.5516e+09, device='cuda:0')
c= tensor(2.5577e+09, device='cuda:0')
c= tensor(2.5577e+09, device='cuda:0')
c= tensor(2.5577e+09, device='cuda:0')
c= tensor(2.5578e+09, device='cuda:0')
c= tensor(2.5583e+09, device='cuda:0')
c= tensor(2.5584e+09, device='cuda:0')
c= tensor(2.5585e+09, device='cuda:0')
c= tensor(2.5586e+09, device='cuda:0')
c= tensor(2.5586e+09, device='cuda:0')
c= tensor(2.5588e+09, device='cuda:0')
c= tensor(2.5588e+09, device='cuda:0')
c= tensor(2.5588e+09, device='cuda:0')
c= tensor(2.5589e+09, device='cuda:0')
c= tensor(2.5622e+09, device='cuda:0')
c= tensor(2.5622e+09, device='cuda:0')
c= tensor(2.5622e+09, device='cuda:0')
c= tensor(2.5622e+09, device='cuda:0')
c= tensor(2.5638e+09, device='cuda:0')
c= tensor(2.5919e+09, device='cuda:0')
c= tensor(2.5928e+09, device='cuda:0')
c= tensor(2.5928e+09, device='cuda:0')
c= tensor(2.5947e+09, device='cuda:0')
c= tensor(2.5992e+09, device='cuda:0')
c= tensor(2.5992e+09, device='cuda:0')
c= tensor(2.5993e+09, device='cuda:0')
c= tensor(2.5995e+09, device='cuda:0')
c= tensor(2.6005e+09, device='cuda:0')
c= tensor(2.6014e+09, device='cuda:0')
c= tensor(2.6033e+09, device='cuda:0')
c= tensor(2.6034e+09, device='cuda:0')
c= tensor(2.6034e+09, device='cuda:0')
c= tensor(2.6034e+09, device='cuda:0')
c= tensor(2.6037e+09, device='cuda:0')
c= tensor(2.6040e+09, device='cuda:0')
c= tensor(2.6051e+09, device='cuda:0')
c= tensor(2.6059e+09, device='cuda:0')
c= tensor(2.6243e+09, device='cuda:0')
c= tensor(2.6244e+09, device='cuda:0')
c= tensor(2.6244e+09, device='cuda:0')
c= tensor(2.6244e+09, device='cuda:0')
c= tensor(2.6253e+09, device='cuda:0')
c= tensor(2.6254e+09, device='cuda:0')
c= tensor(2.6254e+09, device='cuda:0')
c= tensor(2.6254e+09, device='cuda:0')
c= tensor(2.6256e+09, device='cuda:0')
c= tensor(2.6256e+09, device='cuda:0')
c= tensor(2.6257e+09, device='cuda:0')
c= tensor(2.6257e+09, device='cuda:0')
c= tensor(2.6258e+09, device='cuda:0')
c= tensor(2.6263e+09, device='cuda:0')
c= tensor(2.6263e+09, device='cuda:0')
c= tensor(2.6263e+09, device='cuda:0')
c= tensor(2.6306e+09, device='cuda:0')
c= tensor(2.6428e+09, device='cuda:0')
c= tensor(2.6440e+09, device='cuda:0')
c= tensor(2.6515e+09, device='cuda:0')
c= tensor(2.6516e+09, device='cuda:0')
c= tensor(2.6518e+09, device='cuda:0')
c= tensor(2.6519e+09, device='cuda:0')
c= tensor(2.6556e+09, device='cuda:0')
c= tensor(2.6558e+09, device='cuda:0')
c= tensor(2.6564e+09, device='cuda:0')
c= tensor(2.6564e+09, device='cuda:0')
c= tensor(2.7226e+09, device='cuda:0')
c= tensor(2.7230e+09, device='cuda:0')
c= tensor(2.7232e+09, device='cuda:0')
c= tensor(2.7835e+09, device='cuda:0')
c= tensor(2.7838e+09, device='cuda:0')
c= tensor(2.7847e+09, device='cuda:0')
c= tensor(2.7957e+09, device='cuda:0')
c= tensor(2.7997e+09, device='cuda:0')
c= tensor(2.8004e+09, device='cuda:0')
c= tensor(2.8004e+09, device='cuda:0')
c= tensor(2.8009e+09, device='cuda:0')
c= tensor(2.8009e+09, device='cuda:0')
c= tensor(2.8021e+09, device='cuda:0')
c= tensor(2.8869e+09, device='cuda:0')
c= tensor(2.8870e+09, device='cuda:0')
c= tensor(2.8892e+09, device='cuda:0')
c= tensor(2.8892e+09, device='cuda:0')
c= tensor(2.8928e+09, device='cuda:0')
c= tensor(2.8947e+09, device='cuda:0')
c= tensor(2.8947e+09, device='cuda:0')
c= tensor(2.8950e+09, device='cuda:0')
c= tensor(2.9234e+09, device='cuda:0')
c= tensor(2.9238e+09, device='cuda:0')
c= tensor(2.9627e+09, device='cuda:0')
c= tensor(2.9631e+09, device='cuda:0')
c= tensor(2.9634e+09, device='cuda:0')
c= tensor(2.9634e+09, device='cuda:0')
c= tensor(2.9640e+09, device='cuda:0')
c= tensor(2.9672e+09, device='cuda:0')
c= tensor(2.9672e+09, device='cuda:0')
c= tensor(3.0300e+09, device='cuda:0')
c= tensor(3.0314e+09, device='cuda:0')
c= tensor(3.0320e+09, device='cuda:0')
c= tensor(3.0321e+09, device='cuda:0')
c= tensor(3.0322e+09, device='cuda:0')
c= tensor(3.0322e+09, device='cuda:0')
c= tensor(3.0322e+09, device='cuda:0')
c= tensor(3.0322e+09, device='cuda:0')
c= tensor(3.0402e+09, device='cuda:0')
c= tensor(3.8726e+09, device='cuda:0')
c= tensor(3.8728e+09, device='cuda:0')
c= tensor(3.8765e+09, device='cuda:0')
c= tensor(3.8765e+09, device='cuda:0')
c= tensor(3.8766e+09, device='cuda:0')
c= tensor(3.8767e+09, device='cuda:0')
c= tensor(3.8784e+09, device='cuda:0')
c= tensor(3.8784e+09, device='cuda:0')
c= tensor(4.0441e+09, device='cuda:0')
c= tensor(4.0442e+09, device='cuda:0')
c= tensor(4.0473e+09, device='cuda:0')
c= tensor(4.0474e+09, device='cuda:0')
c= tensor(4.0520e+09, device='cuda:0')
c= tensor(4.0623e+09, device='cuda:0')
c= tensor(4.0623e+09, device='cuda:0')
c= tensor(4.0623e+09, device='cuda:0')
c= tensor(4.0639e+09, device='cuda:0')
c= tensor(4.0640e+09, device='cuda:0')
c= tensor(4.0640e+09, device='cuda:0')
c= tensor(4.0672e+09, device='cuda:0')
c= tensor(4.0729e+09, device='cuda:0')
c= tensor(4.0732e+09, device='cuda:0')
c= tensor(4.0733e+09, device='cuda:0')
c= tensor(4.0858e+09, device='cuda:0')
c= tensor(4.0948e+09, device='cuda:0')
c= tensor(4.0950e+09, device='cuda:0')
c= tensor(4.0950e+09, device='cuda:0')
c= tensor(4.1013e+09, device='cuda:0')
c= tensor(4.1016e+09, device='cuda:0')
c= tensor(4.1051e+09, device='cuda:0')
c= tensor(4.1056e+09, device='cuda:0')
c= tensor(4.1060e+09, device='cuda:0')
c= tensor(4.1063e+09, device='cuda:0')
c= tensor(4.1733e+09, device='cuda:0')
c= tensor(4.1749e+09, device='cuda:0')
c= tensor(4.1749e+09, device='cuda:0')
c= tensor(4.1749e+09, device='cuda:0')
c= tensor(4.1750e+09, device='cuda:0')
c= tensor(4.1754e+09, device='cuda:0')
c= tensor(4.1840e+09, device='cuda:0')
c= tensor(4.1859e+09, device='cuda:0')
c= tensor(4.1860e+09, device='cuda:0')
c= tensor(4.1862e+09, device='cuda:0')
c= tensor(4.1867e+09, device='cuda:0')
c= tensor(4.2215e+09, device='cuda:0')
c= tensor(4.2223e+09, device='cuda:0')
c= tensor(4.2232e+09, device='cuda:0')
c= tensor(4.2238e+09, device='cuda:0')
c= tensor(4.2243e+09, device='cuda:0')
c= tensor(4.2243e+09, device='cuda:0')
c= tensor(4.2243e+09, device='cuda:0')
c= tensor(4.2302e+09, device='cuda:0')
c= tensor(4.2305e+09, device='cuda:0')
c= tensor(4.2315e+09, device='cuda:0')
c= tensor(4.2315e+09, device='cuda:0')
c= tensor(4.2315e+09, device='cuda:0')
c= tensor(4.2329e+09, device='cuda:0')
c= tensor(4.2339e+09, device='cuda:0')
c= tensor(4.2341e+09, device='cuda:0')
c= tensor(4.2341e+09, device='cuda:0')
c= tensor(4.2342e+09, device='cuda:0')
c= tensor(4.2343e+09, device='cuda:0')
c= tensor(4.2344e+09, device='cuda:0')
c= tensor(4.2531e+09, device='cuda:0')
c= tensor(4.2532e+09, device='cuda:0')
c= tensor(4.2532e+09, device='cuda:0')
c= tensor(4.2536e+09, device='cuda:0')
c= tensor(4.2537e+09, device='cuda:0')
c= tensor(4.2836e+09, device='cuda:0')
c= tensor(4.2837e+09, device='cuda:0')
c= tensor(4.2842e+09, device='cuda:0')
c= tensor(4.2856e+09, device='cuda:0')
c= tensor(4.2893e+09, device='cuda:0')
c= tensor(4.2904e+09, device='cuda:0')
c= tensor(4.2967e+09, device='cuda:0')
c= tensor(4.2967e+09, device='cuda:0')
c= tensor(4.2971e+09, device='cuda:0')
c= tensor(4.2972e+09, device='cuda:0')
c= tensor(4.3066e+09, device='cuda:0')
c= tensor(4.3091e+09, device='cuda:0')
c= tensor(4.3094e+09, device='cuda:0')
c= tensor(4.3107e+09, device='cuda:0')
c= tensor(4.3107e+09, device='cuda:0')
c= tensor(4.3113e+09, device='cuda:0')
c= tensor(4.3113e+09, device='cuda:0')
c= tensor(4.3115e+09, device='cuda:0')
c= tensor(4.3189e+09, device='cuda:0')
c= tensor(4.3245e+09, device='cuda:0')
c= tensor(4.3246e+09, device='cuda:0')
c= tensor(4.3249e+09, device='cuda:0')
c= tensor(4.3249e+09, device='cuda:0')
c= tensor(4.3334e+09, device='cuda:0')
c= tensor(4.3334e+09, device='cuda:0')
c= tensor(4.3335e+09, device='cuda:0')
c= tensor(4.3368e+09, device='cuda:0')
c= tensor(4.3402e+09, device='cuda:0')
c= tensor(4.3403e+09, device='cuda:0')
c= tensor(4.3405e+09, device='cuda:0')
c= tensor(4.3406e+09, device='cuda:0')
memory (bytes)
4285046784
time for making loss 2 is 14.02022123336792
p0 True
it  0 : 1486555136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 32% |
shape of L is 
torch.Size([])
memory (bytes)
4285251584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4285849600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% | 10% |
error is  41730023000.0
relative error loss 9.613984
shape of L is 
torch.Size([])
memory (bytes)
4497620992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  5% | 10% |
memory (bytes)
4497883136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% | 10% |
error is  41729760000.0
relative error loss 9.613923
shape of L is 
torch.Size([])
memory (bytes)
4501254144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 10% |
memory (bytes)
4501254144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  41728610000.0
relative error loss 9.613658
shape of L is 
torch.Size([])
memory (bytes)
4503363584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 10% |
memory (bytes)
4503400448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% | 10% |
error is  41721030000.0
relative error loss 9.611912
shape of L is 
torch.Size([])
memory (bytes)
4505501696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 10% |
memory (bytes)
4505554944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  41680510000.0
relative error loss 9.602577
shape of L is 
torch.Size([])
memory (bytes)
4507455488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 10% |
memory (bytes)
4507455488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  41237135000.0
relative error loss 9.50043
shape of L is 
torch.Size([])
memory (bytes)
4509765632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 10% |
memory (bytes)
4509782016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  39078570000.0
relative error loss 9.003128
shape of L is 
torch.Size([])
memory (bytes)
4511776768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 10% |
memory (bytes)
4511776768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  28877660000.0
relative error loss 6.6529884
shape of L is 
torch.Size([])
memory (bytes)
4513865728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 10% |
memory (bytes)
4513865728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  10091049000.0
relative error loss 2.3248293
shape of L is 
torch.Size([])
memory (bytes)
4516024320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 10% |
memory (bytes)
4516024320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  6339709000.0
relative error loss 1.4605757
time to take a step is 144.1133997440338
it  1 : 1768722432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
4518334464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 10% |
memory (bytes)
4518334464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  6339709000.0
relative error loss 1.4605757
shape of L is 
torch.Size([])
memory (bytes)
4520308736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 10% |
memory (bytes)
4520308736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  4869268500.0
relative error loss 1.1218078
shape of L is 
torch.Size([])
memory (bytes)
4522528768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 10% |
memory (bytes)
4522582016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  4318803000.0
relative error loss 0.9949887
shape of L is 
torch.Size([])
memory (bytes)
4524748800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 10% |
memory (bytes)
4524769280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  5030108000.0
relative error loss 1.158863
shape of L is 
torch.Size([])
memory (bytes)
4526649344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 10% |
memory (bytes)
4526649344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  3878194200.0
relative error loss 0.89347893
shape of L is 
torch.Size([])
memory (bytes)
4528955392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 10% |
memory (bytes)
4528955392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  3641773000.0
relative error loss 0.83901095
shape of L is 
torch.Size([])
memory (bytes)
4531011584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 10% |
memory (bytes)
4531011584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  3358770000.0
relative error loss 0.7738112
shape of L is 
torch.Size([])
memory (bytes)
4533104640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 10% |
memory (bytes)
4533309440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  3176385000.0
relative error loss 0.7317924
shape of L is 
torch.Size([])
memory (bytes)
4535406592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 10% |
memory (bytes)
4535406592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  3017762300.0
relative error loss 0.69524807
shape of L is 
torch.Size([])
memory (bytes)
4537470976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 10% |
memory (bytes)
4537470976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  2745735200.0
relative error loss 0.632577
time to take a step is 127.44714069366455
it  2 : 1849341952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
4539297792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 10% |
memory (bytes)
4539297792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  2745735200.0
relative error loss 0.632577
shape of L is 
torch.Size([])
memory (bytes)
4541517824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 10% |
memory (bytes)
4541517824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  2591281200.0
relative error loss 0.5969931
shape of L is 
torch.Size([])
memory (bytes)
4543807488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 10% |
memory (bytes)
4543807488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  2435708200.0
relative error loss 0.5611513
shape of L is 
torch.Size([])
memory (bytes)
4545990656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
memory (bytes)
4545990656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  2278733300.0
relative error loss 0.5249866
shape of L is 
torch.Size([])
memory (bytes)
4547993600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 10% |
memory (bytes)
4547993600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  2096313300.0
relative error loss 0.48295978
shape of L is 
torch.Size([])
memory (bytes)
4550086656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4550086656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  1893406000.0
relative error loss 0.4362129
shape of L is 
torch.Size([])
memory (bytes)
4552159232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 10% |
memory (bytes)
4552159232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  1741929200.0
relative error loss 0.40131488
shape of L is 
torch.Size([])
memory (bytes)
4554313728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 10% |
memory (bytes)
4554313728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  1610394900.0
relative error loss 0.37101132
shape of L is 
torch.Size([])
memory (bytes)
4556627968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 10% |
memory (bytes)
4556636160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  1475130900.0
relative error loss 0.33984846
shape of L is 
torch.Size([])
memory (bytes)
4558635008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 10% |
memory (bytes)
4558635008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  1334889500.0
relative error loss 0.3075389
time to take a step is 175.14136695861816
it  3 : 1849341952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
4560670720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 10% |
memory (bytes)
4560670720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  1334889500.0
relative error loss 0.3075389
shape of L is 
torch.Size([])
memory (bytes)
4562800640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 10% |
memory (bytes)
4562800640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% | 10% |
error is  1249893600.0
relative error loss 0.2879571
shape of L is 
torch.Size([])
memory (bytes)
4565155840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 10% |
memory (bytes)
4565221376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  1128771300.0
relative error loss 0.26005232
shape of L is 
torch.Size([])
memory (bytes)
4567351296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 10% |
memory (bytes)
4567359488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  1018833400.0
relative error loss 0.23472424
shape of L is 
torch.Size([])
memory (bytes)
4569489408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 10% |
memory (bytes)
4569489408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  941725700.0
relative error loss 0.21695976
shape of L is 
torch.Size([])
memory (bytes)
4571471872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 10% |
memory (bytes)
4571471872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% | 10% |
error is  902680060.0
relative error loss 0.20796421
shape of L is 
torch.Size([])
memory (bytes)
4573716480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 10% |
memory (bytes)
4573716480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  840597500.0
relative error loss 0.1936613
shape of L is 
torch.Size([])
memory (bytes)
4575911936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 10% |
memory (bytes)
4575911936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  783761400.0
relative error loss 0.1805671
shape of L is 
torch.Size([])
memory (bytes)
4577906688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 10% |
memory (bytes)
4577906688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  751251700.0
relative error loss 0.17307734
shape of L is 
torch.Size([])
memory (bytes)
4579958784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 10% |
memory (bytes)
4580208640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  672995840.0
relative error loss 0.15504834
time to take a step is 219.6057026386261
c= tensor(1768.9528, device='cuda:0')
c= tensor(146047.6094, device='cuda:0')
c= tensor(150351.5781, device='cuda:0')
c= tensor(153671.8906, device='cuda:0')
c= tensor(999166.8750, device='cuda:0')
c= tensor(1543136., device='cuda:0')
c= tensor(1930792.2500, device='cuda:0')
c= tensor(2407396.5000, device='cuda:0')
c= tensor(2491860.2500, device='cuda:0')
c= tensor(6257307., device='cuda:0')
c= tensor(6268346., device='cuda:0')
c= tensor(8758570., device='cuda:0')
c= tensor(8787599., device='cuda:0')
c= tensor(18552816., device='cuda:0')
c= tensor(18701186., device='cuda:0')
c= tensor(19084140., device='cuda:0')
c= tensor(19470850., device='cuda:0')
c= tensor(19831862., device='cuda:0')
c= tensor(42821772., device='cuda:0')
c= tensor(45021648., device='cuda:0')
c= tensor(45111544., device='cuda:0')
c= tensor(82076328., device='cuda:0')
c= tensor(82117384., device='cuda:0')
c= tensor(82157488., device='cuda:0')
c= tensor(82379296., device='cuda:0')
c= tensor(83393072., device='cuda:0')
c= tensor(84452016., device='cuda:0')
c= tensor(84480792., device='cuda:0')
c= tensor(87427688., device='cuda:0')
c= tensor(9.1462e+08, device='cuda:0')
c= tensor(9.1464e+08, device='cuda:0')
c= tensor(9.5409e+08, device='cuda:0')
c= tensor(9.5432e+08, device='cuda:0')
c= tensor(9.5435e+08, device='cuda:0')
c= tensor(9.5444e+08, device='cuda:0')
c= tensor(9.5623e+08, device='cuda:0')
c= tensor(9.5808e+08, device='cuda:0')
c= tensor(9.5808e+08, device='cuda:0')
c= tensor(9.5809e+08, device='cuda:0')
c= tensor(9.5809e+08, device='cuda:0')
c= tensor(9.5810e+08, device='cuda:0')
c= tensor(9.5810e+08, device='cuda:0')
c= tensor(9.5810e+08, device='cuda:0')
c= tensor(9.5810e+08, device='cuda:0')
c= tensor(9.5810e+08, device='cuda:0')
c= tensor(9.5810e+08, device='cuda:0')
c= tensor(9.5812e+08, device='cuda:0')
c= tensor(9.5813e+08, device='cuda:0')
c= tensor(9.5813e+08, device='cuda:0')
c= tensor(9.5816e+08, device='cuda:0')
c= tensor(9.5823e+08, device='cuda:0')
c= tensor(9.5823e+08, device='cuda:0')
c= tensor(9.5824e+08, device='cuda:0')
c= tensor(9.5824e+08, device='cuda:0')
c= tensor(9.5826e+08, device='cuda:0')
c= tensor(9.5828e+08, device='cuda:0')
c= tensor(9.5828e+08, device='cuda:0')
c= tensor(9.5829e+08, device='cuda:0')
c= tensor(9.5829e+08, device='cuda:0')
c= tensor(9.5830e+08, device='cuda:0')
c= tensor(9.5834e+08, device='cuda:0')
c= tensor(9.5835e+08, device='cuda:0')
c= tensor(9.5842e+08, device='cuda:0')
c= tensor(9.5843e+08, device='cuda:0')
c= tensor(9.5844e+08, device='cuda:0')
c= tensor(9.5844e+08, device='cuda:0')
c= tensor(9.5844e+08, device='cuda:0')
c= tensor(9.5845e+08, device='cuda:0')
c= tensor(9.5846e+08, device='cuda:0')
c= tensor(9.5846e+08, device='cuda:0')
c= tensor(9.5848e+08, device='cuda:0')
c= tensor(9.5849e+08, device='cuda:0')
c= tensor(9.5849e+08, device='cuda:0')
c= tensor(9.5849e+08, device='cuda:0')
c= tensor(9.5850e+08, device='cuda:0')
c= tensor(9.5851e+08, device='cuda:0')
c= tensor(9.5851e+08, device='cuda:0')
c= tensor(9.5851e+08, device='cuda:0')
c= tensor(9.5852e+08, device='cuda:0')
c= tensor(9.5855e+08, device='cuda:0')
c= tensor(9.5856e+08, device='cuda:0')
c= tensor(9.5856e+08, device='cuda:0')
c= tensor(9.5858e+08, device='cuda:0')
c= tensor(9.5858e+08, device='cuda:0')
c= tensor(9.5859e+08, device='cuda:0')
c= tensor(9.5859e+08, device='cuda:0')
c= tensor(9.5859e+08, device='cuda:0')
c= tensor(9.5859e+08, device='cuda:0')
c= tensor(9.5861e+08, device='cuda:0')
c= tensor(9.5861e+08, device='cuda:0')
c= tensor(9.5862e+08, device='cuda:0')
c= tensor(9.5862e+08, device='cuda:0')
c= tensor(9.5862e+08, device='cuda:0')
c= tensor(9.5863e+08, device='cuda:0')
c= tensor(9.5864e+08, device='cuda:0')
c= tensor(9.5865e+08, device='cuda:0')
c= tensor(9.5866e+08, device='cuda:0')
c= tensor(9.5867e+08, device='cuda:0')
c= tensor(9.5870e+08, device='cuda:0')
c= tensor(9.5870e+08, device='cuda:0')
c= tensor(9.5876e+08, device='cuda:0')
c= tensor(9.5877e+08, device='cuda:0')
c= tensor(9.5878e+08, device='cuda:0')
c= tensor(9.5878e+08, device='cuda:0')
c= tensor(9.5879e+08, device='cuda:0')
c= tensor(9.5879e+08, device='cuda:0')
c= tensor(9.5880e+08, device='cuda:0')
c= tensor(9.5880e+08, device='cuda:0')
c= tensor(9.5880e+08, device='cuda:0')
c= tensor(9.5880e+08, device='cuda:0')
c= tensor(9.5881e+08, device='cuda:0')
c= tensor(9.5881e+08, device='cuda:0')
c= tensor(9.5881e+08, device='cuda:0')
c= tensor(9.5882e+08, device='cuda:0')
c= tensor(9.5883e+08, device='cuda:0')
c= tensor(9.5883e+08, device='cuda:0')
c= tensor(9.5883e+08, device='cuda:0')
c= tensor(9.5883e+08, device='cuda:0')
c= tensor(9.5885e+08, device='cuda:0')
c= tensor(9.5885e+08, device='cuda:0')
c= tensor(9.5893e+08, device='cuda:0')
c= tensor(9.5894e+08, device='cuda:0')
c= tensor(9.5894e+08, device='cuda:0')
c= tensor(9.5894e+08, device='cuda:0')
c= tensor(9.5895e+08, device='cuda:0')
c= tensor(9.5896e+08, device='cuda:0')
c= tensor(9.5896e+08, device='cuda:0')
c= tensor(9.5896e+08, device='cuda:0')
c= tensor(9.5909e+08, device='cuda:0')
c= tensor(9.5909e+08, device='cuda:0')
c= tensor(9.5911e+08, device='cuda:0')
c= tensor(9.5912e+08, device='cuda:0')
c= tensor(9.5913e+08, device='cuda:0')
c= tensor(9.5913e+08, device='cuda:0')
c= tensor(9.5913e+08, device='cuda:0')
c= tensor(9.5913e+08, device='cuda:0')
c= tensor(9.5914e+08, device='cuda:0')
c= tensor(9.5914e+08, device='cuda:0')
c= tensor(9.5914e+08, device='cuda:0')
c= tensor(9.5914e+08, device='cuda:0')
c= tensor(9.5914e+08, device='cuda:0')
c= tensor(9.5915e+08, device='cuda:0')
c= tensor(9.5917e+08, device='cuda:0')
c= tensor(9.5937e+08, device='cuda:0')
c= tensor(9.5941e+08, device='cuda:0')
c= tensor(9.5941e+08, device='cuda:0')
c= tensor(9.5941e+08, device='cuda:0')
c= tensor(9.5941e+08, device='cuda:0')
c= tensor(9.5941e+08, device='cuda:0')
c= tensor(9.5942e+08, device='cuda:0')
c= tensor(9.5942e+08, device='cuda:0')
c= tensor(9.5944e+08, device='cuda:0')
c= tensor(9.5944e+08, device='cuda:0')
c= tensor(9.5948e+08, device='cuda:0')
c= tensor(9.5948e+08, device='cuda:0')
c= tensor(9.5955e+08, device='cuda:0')
c= tensor(9.5956e+08, device='cuda:0')
c= tensor(9.5956e+08, device='cuda:0')
c= tensor(9.5957e+08, device='cuda:0')
c= tensor(9.5957e+08, device='cuda:0')
c= tensor(9.5958e+08, device='cuda:0')
c= tensor(9.5958e+08, device='cuda:0')
c= tensor(9.5959e+08, device='cuda:0')
c= tensor(9.5959e+08, device='cuda:0')
c= tensor(9.5960e+08, device='cuda:0')
c= tensor(9.5960e+08, device='cuda:0')
c= tensor(9.5961e+08, device='cuda:0')
c= tensor(9.5961e+08, device='cuda:0')
c= tensor(9.5962e+08, device='cuda:0')
c= tensor(9.5962e+08, device='cuda:0')
c= tensor(9.5962e+08, device='cuda:0')
c= tensor(9.5962e+08, device='cuda:0')
c= tensor(9.5964e+08, device='cuda:0')
c= tensor(9.5964e+08, device='cuda:0')
c= tensor(9.5965e+08, device='cuda:0')
c= tensor(9.5967e+08, device='cuda:0')
c= tensor(9.5967e+08, device='cuda:0')
c= tensor(9.5968e+08, device='cuda:0')
c= tensor(9.5969e+08, device='cuda:0')
c= tensor(9.5970e+08, device='cuda:0')
c= tensor(9.5971e+08, device='cuda:0')
c= tensor(9.5971e+08, device='cuda:0')
c= tensor(9.5972e+08, device='cuda:0')
c= tensor(9.5973e+08, device='cuda:0')
c= tensor(9.5974e+08, device='cuda:0')
c= tensor(9.5975e+08, device='cuda:0')
c= tensor(9.5975e+08, device='cuda:0')
c= tensor(9.5976e+08, device='cuda:0')
c= tensor(9.5976e+08, device='cuda:0')
c= tensor(9.5987e+08, device='cuda:0')
c= tensor(9.5987e+08, device='cuda:0')
c= tensor(9.5987e+08, device='cuda:0')
c= tensor(9.5988e+08, device='cuda:0')
c= tensor(9.5988e+08, device='cuda:0')
c= tensor(9.5988e+08, device='cuda:0')
c= tensor(9.5989e+08, device='cuda:0')
c= tensor(9.5989e+08, device='cuda:0')
c= tensor(9.5989e+08, device='cuda:0')
c= tensor(9.5990e+08, device='cuda:0')
c= tensor(9.5990e+08, device='cuda:0')
c= tensor(9.5993e+08, device='cuda:0')
c= tensor(9.5993e+08, device='cuda:0')
c= tensor(9.6009e+08, device='cuda:0')
c= tensor(9.6009e+08, device='cuda:0')
c= tensor(9.6010e+08, device='cuda:0')
c= tensor(9.6010e+08, device='cuda:0')
c= tensor(9.6012e+08, device='cuda:0')
c= tensor(9.6013e+08, device='cuda:0')
c= tensor(9.6014e+08, device='cuda:0')
c= tensor(9.6019e+08, device='cuda:0')
c= tensor(9.6021e+08, device='cuda:0')
c= tensor(9.6021e+08, device='cuda:0')
c= tensor(9.6021e+08, device='cuda:0')
c= tensor(9.6022e+08, device='cuda:0')
c= tensor(9.6022e+08, device='cuda:0')
c= tensor(9.6022e+08, device='cuda:0')
c= tensor(9.6022e+08, device='cuda:0')
c= tensor(9.6023e+08, device='cuda:0')
c= tensor(9.6023e+08, device='cuda:0')
c= tensor(9.6024e+08, device='cuda:0')
c= tensor(9.6025e+08, device='cuda:0')
c= tensor(9.6025e+08, device='cuda:0')
c= tensor(9.6026e+08, device='cuda:0')
c= tensor(9.6026e+08, device='cuda:0')
c= tensor(9.6027e+08, device='cuda:0')
c= tensor(9.6028e+08, device='cuda:0')
c= tensor(9.6028e+08, device='cuda:0')
c= tensor(9.6028e+08, device='cuda:0')
c= tensor(9.6029e+08, device='cuda:0')
c= tensor(9.6029e+08, device='cuda:0')
c= tensor(9.6029e+08, device='cuda:0')
c= tensor(9.6030e+08, device='cuda:0')
c= tensor(9.6030e+08, device='cuda:0')
c= tensor(9.6031e+08, device='cuda:0')
c= tensor(9.6031e+08, device='cuda:0')
c= tensor(9.6031e+08, device='cuda:0')
c= tensor(9.6032e+08, device='cuda:0')
c= tensor(9.6034e+08, device='cuda:0')
c= tensor(9.6034e+08, device='cuda:0')
c= tensor(9.6037e+08, device='cuda:0')
c= tensor(9.6369e+08, device='cuda:0')
c= tensor(9.6419e+08, device='cuda:0')
c= tensor(9.6421e+08, device='cuda:0')
c= tensor(9.6421e+08, device='cuda:0')
c= tensor(9.6422e+08, device='cuda:0')
c= tensor(9.6432e+08, device='cuda:0')
c= tensor(9.6883e+08, device='cuda:0')
c= tensor(9.6884e+08, device='cuda:0')
c= tensor(9.7319e+08, device='cuda:0')
c= tensor(9.7426e+08, device='cuda:0')
c= tensor(9.7434e+08, device='cuda:0')
c= tensor(9.8232e+08, device='cuda:0')
c= tensor(9.8233e+08, device='cuda:0')
c= tensor(9.8235e+08, device='cuda:0')
c= tensor(9.9205e+08, device='cuda:0')
c= tensor(1.0320e+09, device='cuda:0')
c= tensor(1.0321e+09, device='cuda:0')
c= tensor(1.0324e+09, device='cuda:0')
c= tensor(1.0324e+09, device='cuda:0')
c= tensor(1.0452e+09, device='cuda:0')
c= tensor(1.0475e+09, device='cuda:0')
c= tensor(1.0659e+09, device='cuda:0')
c= tensor(1.0664e+09, device='cuda:0')
c= tensor(1.0664e+09, device='cuda:0')
c= tensor(1.0664e+09, device='cuda:0')
c= tensor(1.0936e+09, device='cuda:0')
c= tensor(1.0937e+09, device='cuda:0')
c= tensor(1.0937e+09, device='cuda:0')
c= tensor(1.0942e+09, device='cuda:0')
c= tensor(1.0949e+09, device='cuda:0')
c= tensor(1.1065e+09, device='cuda:0')
c= tensor(1.1109e+09, device='cuda:0')
c= tensor(1.1109e+09, device='cuda:0')
c= tensor(1.1111e+09, device='cuda:0')
c= tensor(1.1111e+09, device='cuda:0')
c= tensor(1.1127e+09, device='cuda:0')
c= tensor(1.1148e+09, device='cuda:0')
c= tensor(1.1169e+09, device='cuda:0')
c= tensor(1.1174e+09, device='cuda:0')
c= tensor(1.1174e+09, device='cuda:0')
c= tensor(1.1175e+09, device='cuda:0')
c= tensor(1.1186e+09, device='cuda:0')
c= tensor(1.1205e+09, device='cuda:0')
c= tensor(1.1220e+09, device='cuda:0')
c= tensor(1.1220e+09, device='cuda:0')
c= tensor(1.1465e+09, device='cuda:0')
c= tensor(1.1466e+09, device='cuda:0')
c= tensor(1.1467e+09, device='cuda:0')
c= tensor(1.1484e+09, device='cuda:0')
c= tensor(1.1484e+09, device='cuda:0')
c= tensor(1.1500e+09, device='cuda:0')
c= tensor(1.1734e+09, device='cuda:0')
c= tensor(1.1933e+09, device='cuda:0')
c= tensor(1.1935e+09, device='cuda:0')
c= tensor(1.1937e+09, device='cuda:0')
c= tensor(1.1937e+09, device='cuda:0')
c= tensor(1.1937e+09, device='cuda:0')
c= tensor(1.1949e+09, device='cuda:0')
c= tensor(1.1949e+09, device='cuda:0')
c= tensor(1.1951e+09, device='cuda:0')
c= tensor(1.2141e+09, device='cuda:0')
c= tensor(1.2153e+09, device='cuda:0')
c= tensor(1.2154e+09, device='cuda:0')
c= tensor(1.2155e+09, device='cuda:0')
c= tensor(1.2179e+09, device='cuda:0')
c= tensor(1.2180e+09, device='cuda:0')
c= tensor(1.2180e+09, device='cuda:0')
c= tensor(1.2182e+09, device='cuda:0')
c= tensor(1.2311e+09, device='cuda:0')
c= tensor(1.2312e+09, device='cuda:0')
c= tensor(1.2329e+09, device='cuda:0')
c= tensor(1.2329e+09, device='cuda:0')
c= tensor(1.2343e+09, device='cuda:0')
c= tensor(1.2349e+09, device='cuda:0')
c= tensor(1.2544e+09, device='cuda:0')
c= tensor(1.2554e+09, device='cuda:0')
c= tensor(1.2555e+09, device='cuda:0')
c= tensor(1.2580e+09, device='cuda:0')
c= tensor(1.2607e+09, device='cuda:0')
c= tensor(1.2608e+09, device='cuda:0')
c= tensor(1.2666e+09, device='cuda:0')
c= tensor(1.2746e+09, device='cuda:0')
c= tensor(1.3022e+09, device='cuda:0')
c= tensor(1.3036e+09, device='cuda:0')
c= tensor(1.3036e+09, device='cuda:0')
c= tensor(1.3037e+09, device='cuda:0')
c= tensor(1.3046e+09, device='cuda:0')
c= tensor(1.3048e+09, device='cuda:0')
c= tensor(1.3050e+09, device='cuda:0')
c= tensor(1.3050e+09, device='cuda:0')
c= tensor(1.3063e+09, device='cuda:0')
c= tensor(1.3110e+09, device='cuda:0')
c= tensor(1.3121e+09, device='cuda:0')
c= tensor(1.3121e+09, device='cuda:0')
c= tensor(1.3124e+09, device='cuda:0')
c= tensor(1.3124e+09, device='cuda:0')
c= tensor(1.3124e+09, device='cuda:0')
c= tensor(1.3125e+09, device='cuda:0')
c= tensor(1.3125e+09, device='cuda:0')
c= tensor(1.3130e+09, device='cuda:0')
c= tensor(1.3134e+09, device='cuda:0')
c= tensor(1.3137e+09, device='cuda:0')
c= tensor(1.3138e+09, device='cuda:0')
c= tensor(1.3138e+09, device='cuda:0')
c= tensor(1.3639e+09, device='cuda:0')
c= tensor(1.3640e+09, device='cuda:0')
c= tensor(1.3673e+09, device='cuda:0')
c= tensor(1.3673e+09, device='cuda:0')
c= tensor(1.3673e+09, device='cuda:0')
c= tensor(1.3673e+09, device='cuda:0')
c= tensor(1.3685e+09, device='cuda:0')
c= tensor(1.3686e+09, device='cuda:0')
c= tensor(1.3686e+09, device='cuda:0')
c= tensor(1.3686e+09, device='cuda:0')
c= tensor(1.3686e+09, device='cuda:0')
c= tensor(1.3837e+09, device='cuda:0')
c= tensor(1.3841e+09, device='cuda:0')
c= tensor(1.3845e+09, device='cuda:0')
c= tensor(1.3880e+09, device='cuda:0')
c= tensor(1.3994e+09, device='cuda:0')
c= tensor(1.3994e+09, device='cuda:0')
c= tensor(1.3994e+09, device='cuda:0')
c= tensor(1.3994e+09, device='cuda:0')
c= tensor(1.3994e+09, device='cuda:0')
c= tensor(1.3995e+09, device='cuda:0')
c= tensor(1.3995e+09, device='cuda:0')
c= tensor(1.3996e+09, device='cuda:0')
c= tensor(1.3996e+09, device='cuda:0')
c= tensor(1.3996e+09, device='cuda:0')
c= tensor(1.3996e+09, device='cuda:0')
c= tensor(1.4198e+09, device='cuda:0')
c= tensor(1.4199e+09, device='cuda:0')
c= tensor(1.4210e+09, device='cuda:0')
c= tensor(1.4210e+09, device='cuda:0')
c= tensor(1.4212e+09, device='cuda:0')
c= tensor(1.4218e+09, device='cuda:0')
c= tensor(1.7087e+09, device='cuda:0')
c= tensor(1.7763e+09, device='cuda:0')
c= tensor(1.7765e+09, device='cuda:0')
c= tensor(1.7774e+09, device='cuda:0')
c= tensor(1.7774e+09, device='cuda:0')
c= tensor(1.7783e+09, device='cuda:0')
c= tensor(1.7799e+09, device='cuda:0')
c= tensor(1.7805e+09, device='cuda:0')
c= tensor(1.7805e+09, device='cuda:0')
c= tensor(1.7814e+09, device='cuda:0')
c= tensor(1.8236e+09, device='cuda:0')
c= tensor(1.8237e+09, device='cuda:0')
c= tensor(1.8238e+09, device='cuda:0')
c= tensor(1.8239e+09, device='cuda:0')
c= tensor(1.8241e+09, device='cuda:0')
c= tensor(1.8241e+09, device='cuda:0')
c= tensor(1.8264e+09, device='cuda:0')
c= tensor(1.8268e+09, device='cuda:0')
c= tensor(1.8268e+09, device='cuda:0')
c= tensor(1.8292e+09, device='cuda:0')
c= tensor(1.8294e+09, device='cuda:0')
c= tensor(1.8294e+09, device='cuda:0')
c= tensor(1.8316e+09, device='cuda:0')
c= tensor(1.8346e+09, device='cuda:0')
c= tensor(1.8371e+09, device='cuda:0')
c= tensor(1.8383e+09, device='cuda:0')
c= tensor(1.8508e+09, device='cuda:0')
c= tensor(1.8508e+09, device='cuda:0')
c= tensor(1.8512e+09, device='cuda:0')
c= tensor(1.8524e+09, device='cuda:0')
c= tensor(1.8560e+09, device='cuda:0')
c= tensor(1.8561e+09, device='cuda:0')
c= tensor(1.9146e+09, device='cuda:0')
c= tensor(1.9389e+09, device='cuda:0')
c= tensor(1.9403e+09, device='cuda:0')
c= tensor(1.9422e+09, device='cuda:0')
c= tensor(1.9428e+09, device='cuda:0')
c= tensor(1.9431e+09, device='cuda:0')
c= tensor(1.9431e+09, device='cuda:0')
c= tensor(1.9432e+09, device='cuda:0')
c= tensor(1.9470e+09, device='cuda:0')
c= tensor(1.9499e+09, device='cuda:0')
c= tensor(1.9719e+09, device='cuda:0')
c= tensor(1.9752e+09, device='cuda:0')
c= tensor(1.9764e+09, device='cuda:0')
c= tensor(1.9766e+09, device='cuda:0')
c= tensor(1.9783e+09, device='cuda:0')
c= tensor(1.9783e+09, device='cuda:0')
c= tensor(1.9784e+09, device='cuda:0')
c= tensor(1.9861e+09, device='cuda:0')
c= tensor(1.9867e+09, device='cuda:0')
c= tensor(1.9867e+09, device='cuda:0')
c= tensor(1.9867e+09, device='cuda:0')
c= tensor(1.9936e+09, device='cuda:0')
c= tensor(1.9938e+09, device='cuda:0')
c= tensor(1.9949e+09, device='cuda:0')
c= tensor(1.9950e+09, device='cuda:0')
c= tensor(1.9952e+09, device='cuda:0')
c= tensor(1.9952e+09, device='cuda:0')
c= tensor(1.9952e+09, device='cuda:0')
c= tensor(1.9954e+09, device='cuda:0')
c= tensor(1.9955e+09, device='cuda:0')
c= tensor(1.9955e+09, device='cuda:0')
c= tensor(1.9982e+09, device='cuda:0')
c= tensor(1.9982e+09, device='cuda:0')
c= tensor(1.9984e+09, device='cuda:0')
c= tensor(1.9984e+09, device='cuda:0')
c= tensor(1.9996e+09, device='cuda:0')
c= tensor(2.0000e+09, device='cuda:0')
c= tensor(2.0002e+09, device='cuda:0')
c= tensor(2.0003e+09, device='cuda:0')
c= tensor(2.0006e+09, device='cuda:0')
c= tensor(2.0015e+09, device='cuda:0')
c= tensor(2.0278e+09, device='cuda:0')
c= tensor(2.0278e+09, device='cuda:0')
c= tensor(2.0278e+09, device='cuda:0')
c= tensor(2.0287e+09, device='cuda:0')
c= tensor(2.0288e+09, device='cuda:0')
c= tensor(2.0704e+09, device='cuda:0')
c= tensor(2.0704e+09, device='cuda:0')
c= tensor(2.0735e+09, device='cuda:0')
c= tensor(2.0932e+09, device='cuda:0')
c= tensor(2.0932e+09, device='cuda:0')
c= tensor(2.1019e+09, device='cuda:0')
c= tensor(2.1021e+09, device='cuda:0')
c= tensor(2.1890e+09, device='cuda:0')
c= tensor(2.1891e+09, device='cuda:0')
c= tensor(2.1893e+09, device='cuda:0')
c= tensor(2.1898e+09, device='cuda:0')
c= tensor(2.1898e+09, device='cuda:0')
c= tensor(2.1898e+09, device='cuda:0')
c= tensor(2.1910e+09, device='cuda:0')
c= tensor(2.1912e+09, device='cuda:0')
c= tensor(2.2349e+09, device='cuda:0')
c= tensor(2.2350e+09, device='cuda:0')
c= tensor(2.2350e+09, device='cuda:0')
c= tensor(2.2350e+09, device='cuda:0')
c= tensor(2.2411e+09, device='cuda:0')
c= tensor(2.2473e+09, device='cuda:0')
c= tensor(2.2532e+09, device='cuda:0')
c= tensor(2.2550e+09, device='cuda:0')
c= tensor(2.2550e+09, device='cuda:0')
c= tensor(2.2550e+09, device='cuda:0')
c= tensor(2.2551e+09, device='cuda:0')
c= tensor(2.4928e+09, device='cuda:0')
c= tensor(2.4928e+09, device='cuda:0')
c= tensor(2.4928e+09, device='cuda:0')
c= tensor(2.4966e+09, device='cuda:0')
c= tensor(2.4979e+09, device='cuda:0')
c= tensor(2.4979e+09, device='cuda:0')
c= tensor(2.4980e+09, device='cuda:0')
c= tensor(2.4993e+09, device='cuda:0')
c= tensor(2.4999e+09, device='cuda:0')
c= tensor(2.5000e+09, device='cuda:0')
c= tensor(2.5002e+09, device='cuda:0')
c= tensor(2.5358e+09, device='cuda:0')
c= tensor(2.5382e+09, device='cuda:0')
c= tensor(2.5491e+09, device='cuda:0')
c= tensor(2.5506e+09, device='cuda:0')
c= tensor(2.5507e+09, device='cuda:0')
c= tensor(2.5515e+09, device='cuda:0')
c= tensor(2.5515e+09, device='cuda:0')
c= tensor(2.5516e+09, device='cuda:0')
c= tensor(2.5516e+09, device='cuda:0')
c= tensor(2.5516e+09, device='cuda:0')
c= tensor(2.5577e+09, device='cuda:0')
c= tensor(2.5577e+09, device='cuda:0')
c= tensor(2.5577e+09, device='cuda:0')
c= tensor(2.5578e+09, device='cuda:0')
c= tensor(2.5583e+09, device='cuda:0')
c= tensor(2.5584e+09, device='cuda:0')
c= tensor(2.5585e+09, device='cuda:0')
c= tensor(2.5586e+09, device='cuda:0')
c= tensor(2.5586e+09, device='cuda:0')
c= tensor(2.5588e+09, device='cuda:0')
c= tensor(2.5588e+09, device='cuda:0')
c= tensor(2.5588e+09, device='cuda:0')
c= tensor(2.5589e+09, device='cuda:0')
c= tensor(2.5622e+09, device='cuda:0')
c= tensor(2.5622e+09, device='cuda:0')
c= tensor(2.5622e+09, device='cuda:0')
c= tensor(2.5622e+09, device='cuda:0')
c= tensor(2.5638e+09, device='cuda:0')
c= tensor(2.5919e+09, device='cuda:0')
c= tensor(2.5928e+09, device='cuda:0')
c= tensor(2.5928e+09, device='cuda:0')
c= tensor(2.5947e+09, device='cuda:0')
c= tensor(2.5992e+09, device='cuda:0')
c= tensor(2.5992e+09, device='cuda:0')
c= tensor(2.5993e+09, device='cuda:0')
c= tensor(2.5995e+09, device='cuda:0')
c= tensor(2.6005e+09, device='cuda:0')
c= tensor(2.6014e+09, device='cuda:0')
c= tensor(2.6033e+09, device='cuda:0')
c= tensor(2.6034e+09, device='cuda:0')
c= tensor(2.6034e+09, device='cuda:0')
c= tensor(2.6034e+09, device='cuda:0')
c= tensor(2.6037e+09, device='cuda:0')
c= tensor(2.6040e+09, device='cuda:0')
c= tensor(2.6051e+09, device='cuda:0')
c= tensor(2.6059e+09, device='cuda:0')
c= tensor(2.6243e+09, device='cuda:0')
c= tensor(2.6244e+09, device='cuda:0')
c= tensor(2.6244e+09, device='cuda:0')
c= tensor(2.6244e+09, device='cuda:0')
c= tensor(2.6253e+09, device='cuda:0')
c= tensor(2.6254e+09, device='cuda:0')
c= tensor(2.6254e+09, device='cuda:0')
c= tensor(2.6254e+09, device='cuda:0')
c= tensor(2.6256e+09, device='cuda:0')
c= tensor(2.6256e+09, device='cuda:0')
c= tensor(2.6257e+09, device='cuda:0')
c= tensor(2.6257e+09, device='cuda:0')
c= tensor(2.6258e+09, device='cuda:0')
c= tensor(2.6263e+09, device='cuda:0')
c= tensor(2.6263e+09, device='cuda:0')
c= tensor(2.6263e+09, device='cuda:0')
c= tensor(2.6306e+09, device='cuda:0')
c= tensor(2.6428e+09, device='cuda:0')
c= tensor(2.6440e+09, device='cuda:0')
c= tensor(2.6515e+09, device='cuda:0')
c= tensor(2.6516e+09, device='cuda:0')
c= tensor(2.6518e+09, device='cuda:0')
c= tensor(2.6519e+09, device='cuda:0')
c= tensor(2.6556e+09, device='cuda:0')
c= tensor(2.6558e+09, device='cuda:0')
c= tensor(2.6564e+09, device='cuda:0')
c= tensor(2.6564e+09, device='cuda:0')
c= tensor(2.7226e+09, device='cuda:0')
c= tensor(2.7230e+09, device='cuda:0')
c= tensor(2.7232e+09, device='cuda:0')
c= tensor(2.7835e+09, device='cuda:0')
c= tensor(2.7838e+09, device='cuda:0')
c= tensor(2.7847e+09, device='cuda:0')
c= tensor(2.7957e+09, device='cuda:0')
c= tensor(2.7997e+09, device='cuda:0')
c= tensor(2.8004e+09, device='cuda:0')
c= tensor(2.8004e+09, device='cuda:0')
c= tensor(2.8009e+09, device='cuda:0')
c= tensor(2.8009e+09, device='cuda:0')
c= tensor(2.8021e+09, device='cuda:0')
c= tensor(2.8869e+09, device='cuda:0')
c= tensor(2.8870e+09, device='cuda:0')
c= tensor(2.8892e+09, device='cuda:0')
c= tensor(2.8892e+09, device='cuda:0')
c= tensor(2.8928e+09, device='cuda:0')
c= tensor(2.8947e+09, device='cuda:0')
c= tensor(2.8947e+09, device='cuda:0')
c= tensor(2.8950e+09, device='cuda:0')
c= tensor(2.9234e+09, device='cuda:0')
c= tensor(2.9238e+09, device='cuda:0')
c= tensor(2.9627e+09, device='cuda:0')
c= tensor(2.9631e+09, device='cuda:0')
c= tensor(2.9634e+09, device='cuda:0')
c= tensor(2.9634e+09, device='cuda:0')
c= tensor(2.9640e+09, device='cuda:0')
c= tensor(2.9672e+09, device='cuda:0')
c= tensor(2.9672e+09, device='cuda:0')
c= tensor(3.0300e+09, device='cuda:0')
c= tensor(3.0314e+09, device='cuda:0')
c= tensor(3.0320e+09, device='cuda:0')
c= tensor(3.0321e+09, device='cuda:0')
c= tensor(3.0322e+09, device='cuda:0')
c= tensor(3.0322e+09, device='cuda:0')
c= tensor(3.0322e+09, device='cuda:0')
c= tensor(3.0322e+09, device='cuda:0')
c= tensor(3.0402e+09, device='cuda:0')
c= tensor(3.8726e+09, device='cuda:0')
c= tensor(3.8728e+09, device='cuda:0')
c= tensor(3.8765e+09, device='cuda:0')
c= tensor(3.8765e+09, device='cuda:0')
c= tensor(3.8766e+09, device='cuda:0')
c= tensor(3.8767e+09, device='cuda:0')
c= tensor(3.8784e+09, device='cuda:0')
c= tensor(3.8784e+09, device='cuda:0')
c= tensor(4.0441e+09, device='cuda:0')
c= tensor(4.0442e+09, device='cuda:0')
c= tensor(4.0473e+09, device='cuda:0')
c= tensor(4.0474e+09, device='cuda:0')
c= tensor(4.0520e+09, device='cuda:0')
c= tensor(4.0623e+09, device='cuda:0')
c= tensor(4.0623e+09, device='cuda:0')
c= tensor(4.0623e+09, device='cuda:0')
c= tensor(4.0639e+09, device='cuda:0')
c= tensor(4.0640e+09, device='cuda:0')
c= tensor(4.0640e+09, device='cuda:0')
c= tensor(4.0672e+09, device='cuda:0')
c= tensor(4.0729e+09, device='cuda:0')
c= tensor(4.0732e+09, device='cuda:0')
c= tensor(4.0733e+09, device='cuda:0')
c= tensor(4.0858e+09, device='cuda:0')
c= tensor(4.0948e+09, device='cuda:0')
c= tensor(4.0950e+09, device='cuda:0')
c= tensor(4.0950e+09, device='cuda:0')
c= tensor(4.1013e+09, device='cuda:0')
c= tensor(4.1016e+09, device='cuda:0')
c= tensor(4.1051e+09, device='cuda:0')
c= tensor(4.1056e+09, device='cuda:0')
c= tensor(4.1060e+09, device='cuda:0')
c= tensor(4.1063e+09, device='cuda:0')
c= tensor(4.1733e+09, device='cuda:0')
c= tensor(4.1749e+09, device='cuda:0')
c= tensor(4.1749e+09, device='cuda:0')
c= tensor(4.1749e+09, device='cuda:0')
c= tensor(4.1750e+09, device='cuda:0')
c= tensor(4.1754e+09, device='cuda:0')
c= tensor(4.1840e+09, device='cuda:0')
c= tensor(4.1859e+09, device='cuda:0')
c= tensor(4.1860e+09, device='cuda:0')
c= tensor(4.1862e+09, device='cuda:0')
c= tensor(4.1867e+09, device='cuda:0')
c= tensor(4.2215e+09, device='cuda:0')
c= tensor(4.2223e+09, device='cuda:0')
c= tensor(4.2232e+09, device='cuda:0')
c= tensor(4.2238e+09, device='cuda:0')
c= tensor(4.2243e+09, device='cuda:0')
c= tensor(4.2243e+09, device='cuda:0')
c= tensor(4.2243e+09, device='cuda:0')
c= tensor(4.2302e+09, device='cuda:0')
c= tensor(4.2305e+09, device='cuda:0')
c= tensor(4.2315e+09, device='cuda:0')
c= tensor(4.2315e+09, device='cuda:0')
c= tensor(4.2315e+09, device='cuda:0')
c= tensor(4.2329e+09, device='cuda:0')
c= tensor(4.2339e+09, device='cuda:0')
c= tensor(4.2341e+09, device='cuda:0')
c= tensor(4.2341e+09, device='cuda:0')
c= tensor(4.2342e+09, device='cuda:0')
c= tensor(4.2343e+09, device='cuda:0')
c= tensor(4.2344e+09, device='cuda:0')
c= tensor(4.2531e+09, device='cuda:0')
c= tensor(4.2532e+09, device='cuda:0')
c= tensor(4.2532e+09, device='cuda:0')
c= tensor(4.2536e+09, device='cuda:0')
c= tensor(4.2537e+09, device='cuda:0')
c= tensor(4.2836e+09, device='cuda:0')
c= tensor(4.2837e+09, device='cuda:0')
c= tensor(4.2842e+09, device='cuda:0')
c= tensor(4.2856e+09, device='cuda:0')
c= tensor(4.2893e+09, device='cuda:0')
c= tensor(4.2904e+09, device='cuda:0')
c= tensor(4.2967e+09, device='cuda:0')
c= tensor(4.2967e+09, device='cuda:0')
c= tensor(4.2971e+09, device='cuda:0')
c= tensor(4.2972e+09, device='cuda:0')
c= tensor(4.3066e+09, device='cuda:0')
c= tensor(4.3091e+09, device='cuda:0')
c= tensor(4.3094e+09, device='cuda:0')
c= tensor(4.3107e+09, device='cuda:0')
c= tensor(4.3107e+09, device='cuda:0')
c= tensor(4.3113e+09, device='cuda:0')
c= tensor(4.3113e+09, device='cuda:0')
c= tensor(4.3115e+09, device='cuda:0')
c= tensor(4.3189e+09, device='cuda:0')
c= tensor(4.3245e+09, device='cuda:0')
c= tensor(4.3246e+09, device='cuda:0')
c= tensor(4.3249e+09, device='cuda:0')
c= tensor(4.3249e+09, device='cuda:0')
c= tensor(4.3334e+09, device='cuda:0')
c= tensor(4.3334e+09, device='cuda:0')
c= tensor(4.3335e+09, device='cuda:0')
c= tensor(4.3368e+09, device='cuda:0')
c= tensor(4.3402e+09, device='cuda:0')
c= tensor(4.3403e+09, device='cuda:0')
c= tensor(4.3405e+09, device='cuda:0')
c= tensor(4.3406e+09, device='cuda:0')
time to make c is 11.185432195663452
time for making loss is 11.185454368591309
p0 True
it  0 : 1486727680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
4582379520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
4582535168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  672995840.0
relative error loss 0.15504834
shape of L is 
torch.Size([])
memory (bytes)
4609564672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
memory (bytes)
4609564672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  661722900.0
relative error loss 0.15245122
shape of L is 
torch.Size([])
memory (bytes)
4613099520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4613259264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  627022340.0
relative error loss 0.14445673
shape of L is 
torch.Size([])
memory (bytes)
4616437760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4616437760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  612879360.0
relative error loss 0.1411984
shape of L is 
torch.Size([])
memory (bytes)
4619644928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4619644928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  602300400.0
relative error loss 0.13876116
shape of L is 
torch.Size([])
memory (bytes)
4622864384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 10% |
memory (bytes)
4622864384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  590800640.0
relative error loss 0.13611178
shape of L is 
torch.Size([])
memory (bytes)
4625821696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 10% |
memory (bytes)
4625821696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  587751400.0
relative error loss 0.13540928
shape of L is 
torch.Size([])
memory (bytes)
4629270528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4629286912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  582515460.0
relative error loss 0.134203
shape of L is 
torch.Size([])
memory (bytes)
4632395776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
4632395776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  578738940.0
relative error loss 0.13333294
shape of L is 
torch.Size([])
memory (bytes)
4635447296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
4635713536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  576220700.0
relative error loss 0.13275278
time to take a step is 284.54635643959045
it  1 : 1851062784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
4638855168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4638916608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  576220700.0
relative error loss 0.13275278
shape of L is 
torch.Size([])
memory (bytes)
4642136064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 10% |
memory (bytes)
4642136064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  573418750.0
relative error loss 0.13210724
shape of L is 
torch.Size([])
memory (bytes)
4645240832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4645351424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  569366000.0
relative error loss 0.13117357
shape of L is 
torch.Size([])
memory (bytes)
4648562688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4648562688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  568173800.0
relative error loss 0.1308989
shape of L is 
torch.Size([])
memory (bytes)
4651720704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4651720704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  566547460.0
relative error loss 0.1305242
shape of L is 
torch.Size([])
memory (bytes)
4654960640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 10% |
memory (bytes)
4654997504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  564258800.0
relative error loss 0.12999694
shape of L is 
torch.Size([])
memory (bytes)
4658085888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 10% |
memory (bytes)
4658085888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  563743000.0
relative error loss 0.12987809
shape of L is 
torch.Size([])
memory (bytes)
4661420032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4661436416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  561777900.0
relative error loss 0.12942538
shape of L is 
torch.Size([])
memory (bytes)
4664549376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4664549376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  561093400.0
relative error loss 0.12926766
shape of L is 
torch.Size([])
memory (bytes)
4667752448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
4667891712
| ID | GPU | MEM |
------------------
|  0 |  2% |  0% |
|  1 | 99% | 10% |
error is  560008200.0
relative error loss 0.12901765
time to take a step is 277.19361090660095
it  2 : 1851062784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
4671098880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4671098880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  560008200.0
relative error loss 0.12901765
shape of L is 
torch.Size([])
memory (bytes)
4674244608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 10% |
memory (bytes)
4674342912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  558784800.0
relative error loss 0.1287358
shape of L is 
torch.Size([])
memory (bytes)
4677558272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
4677558272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  557857000.0
relative error loss 0.12852205
shape of L is 
torch.Size([])
memory (bytes)
4680630272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
4680630272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  557222660.0
relative error loss 0.1283759
shape of L is 
torch.Size([])
memory (bytes)
4683968512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
4683984896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  556503800.0
relative error loss 0.12821029
shape of L is 
torch.Size([])
memory (bytes)
4687134720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4687134720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  555678460.0
relative error loss 0.12802015
shape of L is 
torch.Size([])
memory (bytes)
4690427904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4690427904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  555315700.0
relative error loss 0.12793657
shape of L is 
torch.Size([])
memory (bytes)
4693532672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4693532672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  554804000.0
relative error loss 0.12781867
shape of L is 
torch.Size([])
memory (bytes)
4696862720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4696862720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  554477060.0
relative error loss 0.12774336
shape of L is 
torch.Size([])
memory (bytes)
4700082176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4700082176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  554020100.0
relative error loss 0.12763809
time to take a step is 278.110671043396
it  3 : 1851062784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
4703211520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 10% |
memory (bytes)
4703211520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  554020100.0
relative error loss 0.12763809
shape of L is 
torch.Size([])
memory (bytes)
4706488320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4706504704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  553330200.0
relative error loss 0.12747914
shape of L is 
torch.Size([])
memory (bytes)
4709687296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
4709687296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  553089800.0
relative error loss 0.12742376
shape of L is 
torch.Size([])
memory (bytes)
4712935424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4712935424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% | 10% |
error is  552626940.0
relative error loss 0.12731713
shape of L is 
torch.Size([])
memory (bytes)
4716142592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4716154880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  552433660.0
relative error loss 0.12727259
shape of L is 
torch.Size([])
memory (bytes)
4719276032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4719276032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  551967200.0
relative error loss 0.12716514
shape of L is 
torch.Size([])
memory (bytes)
4722556928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4722556928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  551784700.0
relative error loss 0.12712309
shape of L is 
torch.Size([])
memory (bytes)
4725800960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
4725813248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  551400200.0
relative error loss 0.1270345
shape of L is 
torch.Size([])
memory (bytes)
4728979456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4728979456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  551192600.0
relative error loss 0.12698667
shape of L is 
torch.Size([])
memory (bytes)
4732231680
| ID | GPU | MEM |
------------------
|  0 |  2% |  0% |
|  1 | 11% | 10% |
memory (bytes)
4732243968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  551010800.0
relative error loss 0.1269448
time to take a step is 278.2791705131531
it  4 : 1851062784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
4735352832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4735352832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  551010800.0
relative error loss 0.1269448
shape of L is 
torch.Size([])
memory (bytes)
4738674688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4738686976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 10% |
error is  550742800.0
relative error loss 0.12688304
shape of L is 
torch.Size([])
memory (bytes)
4741877760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4741877760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  550519300.0
relative error loss 0.12683155
shape of L is 
torch.Size([])
memory (bytes)
4745121792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 10% |
memory (bytes)
4745134080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  550351600.0
relative error loss 0.12679292
shape of L is 
torch.Size([])
memory (bytes)
4748333056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4748353536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  550138900.0
relative error loss 0.12674391
shape of L is 
torch.Size([])
memory (bytes)
4751486976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 10% |
memory (bytes)
4751486976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  549884900.0
relative error loss 0.1266854
shape of L is 
torch.Size([])
memory (bytes)
4754771968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4754788352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  549894900.0
relative error loss 0.1266877
shape of L is 
torch.Size([])
memory (bytes)
4757962752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4757962752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  549679900.0
relative error loss 0.12663816
shape of L is 
torch.Size([])
memory (bytes)
4761001984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4761231360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  549460200.0
relative error loss 0.12658755
shape of L is 
torch.Size([])
memory (bytes)
4764442624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4764442624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  549310460.0
relative error loss 0.12655306
time to take a step is 283.44348978996277
it  5 : 1851062784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
4767657984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
memory (bytes)
4767657984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  549310460.0
relative error loss 0.12655306
shape of L is 
torch.Size([])
memory (bytes)
4770877440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4770889728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  549113600.0
relative error loss 0.1265077
shape of L is 
torch.Size([])
memory (bytes)
4773986304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4773986304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  549065700.0
relative error loss 0.12649667
shape of L is 
torch.Size([])
memory (bytes)
4777320448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4777320448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  548845600.0
relative error loss 0.12644595
shape of L is 
torch.Size([])
memory (bytes)
4780437504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4780437504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  548759550.0
relative error loss 0.12642613
shape of L is 
torch.Size([])
memory (bytes)
4783665152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 10% |
memory (bytes)
4783730688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  548626700.0
relative error loss 0.12639552
shape of L is 
torch.Size([])
memory (bytes)
4786950144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4786962432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  548462340.0
relative error loss 0.12635766
shape of L is 
torch.Size([])
memory (bytes)
4790145024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4790145024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  548295400.0
relative error loss 0.1263192
shape of L is 
torch.Size([])
memory (bytes)
4793409536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
memory (bytes)
4793409536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  548185600.0
relative error loss 0.1262939
shape of L is 
torch.Size([])
memory (bytes)
4796604416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
4796604416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  548035600.0
relative error loss 0.12625934
time to take a step is 278.9563412666321
it  6 : 1851062784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
4799827968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4799827968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  548035600.0
relative error loss 0.12625934
shape of L is 
torch.Size([])
memory (bytes)
4803047424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4803047424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  547863550.0
relative error loss 0.1262197
shape of L is 
torch.Size([])
memory (bytes)
4806074368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4806270976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  547747300.0
relative error loss 0.12619293
shape of L is 
torch.Size([])
memory (bytes)
4809478144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4809490432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  547672600.0
relative error loss 0.12617572
shape of L is 
torch.Size([])
memory (bytes)
4812689408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4812689408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  547627500.0
relative error loss 0.12616533
shape of L is 
torch.Size([])
memory (bytes)
4815831040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 10% |
memory (bytes)
4815831040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  547470340.0
relative error loss 0.12612912
shape of L is 
torch.Size([])
memory (bytes)
4819132416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 10% |
memory (bytes)
4819132416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  547506700.0
relative error loss 0.1261375
shape of L is 
torch.Size([])
memory (bytes)
4822323200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4822323200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  547410940.0
relative error loss 0.12611543
shape of L is 
torch.Size([])
memory (bytes)
4825587712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4825587712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% | 10% |
error is  547337500.0
relative error loss 0.12609851
shape of L is 
torch.Size([])
memory (bytes)
4828688384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4828790784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  547196160.0
relative error loss 0.12606595
time to take a step is 277.67665004730225
it  7 : 1851062784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
4831866880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4832014336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  547196160.0
relative error loss 0.12606595
shape of L is 
torch.Size([])
memory (bytes)
4835225600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4835241984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  547145700.0
relative error loss 0.12605433
shape of L is 
torch.Size([])
memory (bytes)
4838318080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4838453248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  547059700.0
relative error loss 0.12603451
shape of L is 
torch.Size([])
memory (bytes)
4841664512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4841680896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  547004400.0
relative error loss 0.12602177
shape of L is 
torch.Size([])
memory (bytes)
4844802048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 10% |
memory (bytes)
4844802048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  546959360.0
relative error loss 0.1260114
shape of L is 
torch.Size([])
memory (bytes)
4848115712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4848115712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  546852600.0
relative error loss 0.1259868
shape of L is 
torch.Size([])
memory (bytes)
4851290112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 10% |
memory (bytes)
4851335168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  546926850.0
relative error loss 0.1260039
shape of L is 
torch.Size([])
memory (bytes)
4854419456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
4854554624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  546799100.0
relative error loss 0.12597448
shape of L is 
torch.Size([])
memory (bytes)
4857786368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4857786368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  546704640.0
relative error loss 0.1259527
shape of L is 
torch.Size([])
memory (bytes)
4860846080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4860846080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 10% |
error is  546618100.0
relative error loss 0.12593278
time to take a step is 279.14167761802673
it  8 : 1851062784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
4864196608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 10% |
memory (bytes)
4864196608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  546618100.0
relative error loss 0.12593278
shape of L is 
torch.Size([])
memory (bytes)
4867379200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4867379200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  546562800.0
relative error loss 0.12592004
shape of L is 
torch.Size([])
memory (bytes)
4870635520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
4870635520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  546479900.0
relative error loss 0.12590092
shape of L is 
torch.Size([])
memory (bytes)
4873818112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4873818112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  546443800.0
relative error loss 0.12589261
shape of L is 
torch.Size([])
memory (bytes)
4876873728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4877070336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  546386700.0
relative error loss 0.12587947
shape of L is 
torch.Size([])
memory (bytes)
4880265216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4880277504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  546303000.0
relative error loss 0.12586017
shape of L is 
torch.Size([])
memory (bytes)
4883415040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 10% |
memory (bytes)
4883415040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  546275600.0
relative error loss 0.12585387
shape of L is 
torch.Size([])
memory (bytes)
4886716416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4886732800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  546210560.0
relative error loss 0.12583888
shape of L is 
torch.Size([])
memory (bytes)
4889935872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 10% |
memory (bytes)
4889935872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  546188800.0
relative error loss 0.12583387
shape of L is 
torch.Size([])
memory (bytes)
4893163520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4893163520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  546122500.0
relative error loss 0.1258186
time to take a step is 279.07909631729126
it  9 : 1851062784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
4896342016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
4896378880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  546122500.0
relative error loss 0.1258186
shape of L is 
torch.Size([])
memory (bytes)
4899454976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4899594240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  546125060.0
relative error loss 0.12581918
shape of L is 
torch.Size([])
memory (bytes)
4902813696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4902813696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  546094600.0
relative error loss 0.12581216
shape of L is 
torch.Size([])
memory (bytes)
4905897984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4906041344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  546050560.0
relative error loss 0.12580203
shape of L is 
torch.Size([])
memory (bytes)
4909248512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
4909260800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  545953300.0
relative error loss 0.12577961
shape of L is 
torch.Size([])
memory (bytes)
4912406528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 10% |
memory (bytes)
4912406528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  545964540.0
relative error loss 0.1257822
shape of L is 
torch.Size([])
memory (bytes)
4915695616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4915699712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  545904900.0
relative error loss 0.12576847
shape of L is 
torch.Size([])
memory (bytes)
4918874112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
4918910976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  545854700.0
relative error loss 0.1257569
shape of L is 
torch.Size([])
memory (bytes)
4922007552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 10% |
memory (bytes)
4922122240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  545792500.0
relative error loss 0.12574257
shape of L is 
torch.Size([])
memory (bytes)
4925325312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 10% |
memory (bytes)
4925341696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  545753340.0
relative error loss 0.12573354
time to take a step is 279.3858993053436
it  10 : 1851062784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
4928389120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
memory (bytes)
4928389120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  545753340.0
relative error loss 0.12573354
shape of L is 
torch.Size([])
memory (bytes)
4931784704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4931788800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  545709600.0
relative error loss 0.12572347
shape of L is 
torch.Size([])
memory (bytes)
4934983680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4934983680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  545648640.0
relative error loss 0.12570943
shape of L is 
torch.Size([])
memory (bytes)
4938215424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
memory (bytes)
4938215424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  545598460.0
relative error loss 0.12569787
shape of L is 
torch.Size([])
memory (bytes)
4941422592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4941422592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  545574660.0
relative error loss 0.12569238
shape of L is 
torch.Size([])
memory (bytes)
4944572416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4944572416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  545534200.0
relative error loss 0.12568305
shape of L is 
torch.Size([])
memory (bytes)
4947845120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4947857408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% | 10% |
error is  545512450.0
relative error loss 0.12567805
shape of L is 
torch.Size([])
memory (bytes)
4951031808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4951031808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  545487360.0
relative error loss 0.12567227
shape of L is 
torch.Size([])
memory (bytes)
4954271744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4954288128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  545433860.0
relative error loss 0.12565994
shape of L is 
torch.Size([])
memory (bytes)
4957458432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4957458432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  545403140.0
relative error loss 0.12565286
time to take a step is 279.0851788520813
it  11 : 1851062784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
4960706560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 10% |
memory (bytes)
4960706560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  545403140.0
relative error loss 0.12565286
shape of L is 
torch.Size([])
memory (bytes)
4963905536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4963905536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  545345800.0
relative error loss 0.12563965
shape of L is 
torch.Size([])
memory (bytes)
4967100416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
4967100416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  545280260.0
relative error loss 0.12562455
shape of L is 
torch.Size([])
memory (bytes)
4970348544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4970348544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  545268000.0
relative error loss 0.12562172
shape of L is 
torch.Size([])
memory (bytes)
4973477888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4973477888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  545201400.0
relative error loss 0.12560639
shape of L is 
torch.Size([])
memory (bytes)
4976758784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 10% |
memory (bytes)
4976771072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  545185000.0
relative error loss 0.12560262
shape of L is 
torch.Size([])
memory (bytes)
4979965952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4979990528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  545151200.0
relative error loss 0.12559482
shape of L is 
torch.Size([])
memory (bytes)
4983136256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
4983214080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  545114600.0
relative error loss 0.12558639
shape of L is 
torch.Size([])
memory (bytes)
4986417152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 10% |
memory (bytes)
4986429440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  545137150.0
relative error loss 0.12559159
shape of L is 
torch.Size([])
memory (bytes)
4989521920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4989521920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  545090560.0
relative error loss 0.12558085
time to take a step is 280.5046055316925
it  12 : 1851063296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
4992847872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
memory (bytes)
4992860160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  545090560.0
relative error loss 0.12558085
shape of L is 
torch.Size([])
memory (bytes)
4996026368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
4996026368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  545059600.0
relative error loss 0.12557371
shape of L is 
torch.Size([])
memory (bytes)
4999290880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4999290880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 10% |
error is  545034500.0
relative error loss 0.12556793
shape of L is 
torch.Size([])
memory (bytes)
5002498048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
5002498048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  545004300.0
relative error loss 0.12556097
shape of L is 
torch.Size([])
memory (bytes)
5005623296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 10% |
memory (bytes)
5005725696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  544976640.0
relative error loss 0.1255546
shape of L is 
torch.Size([])
memory (bytes)
5008936960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
5008936960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  544958460.0
relative error loss 0.12555042
shape of L is 
torch.Size([])
memory (bytes)
5012049920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
5012049920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% | 10% |
error is  544934140.0
relative error loss 0.12554482
shape of L is 
torch.Size([])
memory (bytes)
5015384064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
5015384064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% | 10% |
error is  544908000.0
relative error loss 0.1255388
shape of L is 
torch.Size([])
memory (bytes)
5018570752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 10% |
memory (bytes)
5018570752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  544892400.0
relative error loss 0.1255352
shape of L is 
torch.Size([])
memory (bytes)
5021700096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
5021810688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  544870140.0
relative error loss 0.12553006
time to take a step is 281.15595602989197
it  13 : 1851062784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
5025009664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
5025021952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 10% |
error is  544870140.0
relative error loss 0.12553006
shape of L is 
torch.Size([])
memory (bytes)
5028188160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
5028188160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  544860900.0
relative error loss 0.12552795
shape of L is 
torch.Size([])
memory (bytes)
5031444480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
memory (bytes)
5031444480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  544844000.0
relative error loss 0.12552406
shape of L is 
torch.Size([])
memory (bytes)
5034622976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
5034622976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  544845060.0
relative error loss 0.1255243
shape of L is 
torch.Size([])
memory (bytes)
5037707264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
5037862912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  544831500.0
relative error loss 0.12552117
shape of L is 
torch.Size([])
memory (bytes)
5041070080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
5041082368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  544809700.0
relative error loss 0.12551615
shape of L is 
torch.Size([])
memory (bytes)
5044191232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
5044191232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  544784900.0
relative error loss 0.12551042
shape of L is 
torch.Size([])
memory (bytes)
5047525376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
5047525376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  544761340.0
relative error loss 0.125505
shape of L is 
torch.Size([])
memory (bytes)
5050712064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
5050712064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  544734700.0
relative error loss 0.12549888
shape of L is 
torch.Size([])
memory (bytes)
5053853696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
5053960192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  544701950.0
relative error loss 0.12549132
time to take a step is 280.63298201560974
it  14 : 1851062784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
5057175552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 10% |
memory (bytes)
5057179648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  544701950.0
relative error loss 0.12549132
shape of L is 
torch.Size([])
memory (bytes)
5060276224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
5060276224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  544661500.0
relative error loss 0.12548201
shape of L is 
torch.Size([])
memory (bytes)
5063602176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 10% |
memory (bytes)
5063602176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  544626400.0
relative error loss 0.12547392
shape of L is 
torch.Size([])
memory (bytes)
5066801152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
5066801152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  544601100.0
relative error loss 0.12546809
shape of L is 
torch.Size([])
memory (bytes)
5069987840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
5069987840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  544580600.0
relative error loss 0.12546337
shape of L is 
torch.Size([])
memory (bytes)
5073231872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
5073248256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% | 10% |
error is  544561150.0
relative error loss 0.12545888
shape of L is 
torch.Size([])
memory (bytes)
5076287488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
5076287488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  544533250.0
relative error loss 0.12545246
shape of L is 
torch.Size([])
memory (bytes)
5079683072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
5079687168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  544507650.0
relative error loss 0.12544656
shape of L is 
torch.Size([])
memory (bytes)
5082890240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
5082906624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  544496100.0
relative error loss 0.1254439
shape of L is 
torch.Size([])
memory (bytes)
5086015488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 10% |
memory (bytes)
5086015488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  544476160.0
relative error loss 0.1254393
time to take a step is 280.0929205417633
sum tnnu_Z after tensor(11736792., device='cuda:0')
shape of features
(3589,)
shape of features
(3589,)
number of orig particles 14355
number of new particles after remove low mass 12288
tnuZ shape should be parts x labs
torch.Size([14355, 702])
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
error without small mass is  672963140.0
relative error without small mass is  0.15504082
nnu_Z shape should be number of particles by maxV
(14355, 702)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
shape of features
(14355,)
Fri Feb 3 06:01:18 EST 2023
