Thu Feb 2 14:53:10 EST 2023
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  0% |  0% |
numbers of X: 15919433
numbers of Z: 9592
shape of features
(9592,)
shape of features
(9592,)
ZX	Vol	Parts	Cubes	Eps
Z	0.012026401157449613	9592	9.592	0.10783063731783538
X	0.010059727009072001	656	0.656	0.2484433931303386
X	0.01035674252753437	5716	5.716	0.12191121320192937
X	0.00986593343339799	1223	1.223	0.20055673059292511
X	0.009854315158286515	1519	1.519	0.18650460433691643
X	0.010388332487722543	35398	35.398	0.06645419049815586
X	0.010261466253996048	18187	18.187	0.08263222586068444
X	0.010840992555219878	17242	17.242	0.08566967811565843
X	0.010374674469971179	21410	21.41	0.07854513137298887
X	0.010055835732328391	2616	2.616	0.1566487054518223
X	0.011053802508053203	43609	43.609	0.06328661360463449
X	0.010024865661660136	2523	2.523	0.15838732971768857
X	0.01023034717026945	59512	59.512	0.05560262247626726
X	0.01006145634218886	4895	4.895	0.12714600442474636
X	0.011105094703341478	61710	61.71	0.056457577587798104
X	0.010210628712169783	9242	9.242	0.1033781700030999
X	0.011085523210697043	19603	19.603	0.08269473983983244
X	0.010371207088215102	20923	20.923	0.07914104558221574
X	0.010272797396805415	15896	15.896	0.08645702545610176
X	0.010415590634481421	98567	98.567	0.04727709554909544
X	0.011441681578435918	39418	39.418	0.06621124548041549
X	0.01035136947727008	5627	5.627	0.1225293976126258
X	0.01127395906071603	303788	303.788	0.03335558892242694
X	0.01015023686319136	6210	6.21	0.11779536229967669
X	0.01022681769857593	4967	4.967	0.12721806475462205
X	0.009325174857926548	10975	10.975	0.09471472116376495
X	0.010712486161517999	31011	31.011	0.07016569548399341
X	0.010362823598539803	7014	7.014	0.11389485725415013
X	0.010025336096216841	6838	6.838	0.1136030387014364
X	0.010328525086589253	29212	29.212	0.07071187267736541
X	0.01197719181219894	925791	925.791	0.02347545490558696
X	0.009854284891614717	2521	2.521	0.15752545982895488
X	0.011671877775001269	84288	84.288	0.05173585857009048
X	0.010455893974069683	3705	3.705	0.14131588613170318
X	0.010171229709687903	4252	4.252	0.13373961508634596
X	0.010237495144786252	3018	3.018	0.15025357861167374
X	0.010254160271425674	27691	27.691	0.07181036825034733
X	0.011259610282055459	27649	27.649	0.07412214466966824
X	0.009725779215627353	762	0.762	0.23369879784407896
X	0.0099584123055535	1602	1.602	0.18386928919904247
X	0.010053705896332336	891	0.891	0.22429322604310076
X	0.010205977905014504	2708	2.708	0.1556211622826953
X	0.00900940289513004	159	0.159	0.3840899657418716
X	0.01018275643464818	346	0.346	0.3087425018034565
X	0.00958749302542276	1206	1.206	0.19958102607789224
X	0.0100508914801773	550	0.55	0.2633989047726691
X	0.009586106944419163	650	0.65	0.245231528114868
X	0.010401137497424384	1917	1.917	0.17571934465328992
X	0.01028820362846745	2109	2.109	0.1695981031198221
X	0.010140783246876348	1375	1.375	0.19465083330738533
X	0.010141083182300399	2025	2.025	0.17108808785558327
X	0.009883842898457292	3560	3.56	0.14054819696644896
X	0.009554251319662874	488	0.488	0.26952079540836027
X	0.010137651707207795	1819	1.819	0.1772970688693084
X	0.00977216763880851	805	0.805	0.2298255683228364
X	0.010655113801194204	3406	3.406	0.1462528988707091
X	0.009869185646005033	1529	1.529	0.18619068269365707
X	0.010113388870601285	1058	1.058	0.21222848783215872
X	0.00999759848326165	1361	1.361	0.19439235833124013
X	0.010113622201560594	1233	1.233	0.20167305782967923
X	0.009903147142447721	730	0.73	0.23849729475901632
X	0.010710766952535922	5417	5.417	0.12551257649774192
X	0.00959677992734204	709	0.709	0.23831960305219493
X	0.010205316409874005	2382	2.382	0.16241578631824805
X	0.010098516010319074	3676	3.676	0.14005351678056063
X	0.009898292292938394	1671	1.671	0.18093723170895334
X	0.009902205396616001	1125	1.125	0.20647135250571494
X	0.00991806458342304	1087	1.087	0.20896126848475502
X	0.01021900209942384	2854	2.854	0.15298593549956124
X	0.010126473725543117	1507	1.507	0.18870424181844764
X	0.009709548837554791	1469	1.469	0.1876689474491992
X	0.01001644138408014	5267	5.267	0.12389393453492338
X	0.0104064912713084	1885	1.885	0.17673842884361515
X	0.00992922073206336	1616	1.616	0.1831574561297826
X	0.00987222752047366	895	0.895	0.22260284307235115
X	0.00988230440611521	1555	1.555	0.18522912620906185
X	0.01040249019318362	1837	1.837	0.1782417289751558
X	0.009701207398543682	489	0.489	0.2707108380930437
X	0.009832406594008043	385	0.385	0.2944871619393936
X	0.010333125941937603	1806	1.806	0.17885647158583493
X	0.00985564674948088	6243	6.243	0.11643867764642414
X	0.009896248835864145	1057	1.057	0.2107650228164643
X	0.009732669157532101	539	0.539	0.2623499979604263
X	0.00984754275679101	1595	1.595	0.18345197681098488
X	0.009434194667983283	695	0.695	0.23854666150562595
X	0.009892451205385452	800	0.8	0.23124444558666085
X	0.009773978851724036	796	0.796	0.2307027523197622
X	0.009621703557378222	714	0.714	0.2379676490785721
X	0.00945694930250879	359	0.359	0.29754468990677047
X	0.009706300440535616	2128	2.128	0.16584181245925972
X	0.009907251532083359	1606	1.606	0.18340135899721924
X	0.010182161681622377	2898	2.898	0.15202459994992293
X	0.00995285433333408	637	0.637	0.24999773380119752
X	0.00964788199583819	570	0.57	0.25675507845023327
X	0.009856075726748572	2134	2.134	0.1665341119568135
X	0.010017007587727686	1813	1.813	0.17678553733175015
X	0.009856020884953952	2048	2.048	0.16883295467003434
X	0.00969110619236994	1398	1.398	0.19067279247475816
X	0.010212883675293752	1915	1.915	0.1747135288595048
X	0.010112584575466398	2908	2.908	0.15150347288703586
X	0.009973124224210785	2029	2.029	0.17002641344240643
X	0.010256701644284242	3685	3.685	0.14066630408059608
X	0.01021319524972781	2858	2.858	0.15288556059327613
X	0.010150338239589921	1334	1.334	0.19668665523077547
X	0.00920484638142612	392	0.392	0.28635855307588665
X	0.01014260982310906	2057	2.057	0.17020480214314823
X	0.0097429818796089	646	0.646	0.2470698731630701
X	0.009863294231098002	3665	3.665	0.1390964362651632
X	0.009825812929196003	335	0.335	0.3083954023321713
X	0.010285199272494824	808	0.808	0.2334893717169708
X	0.009975376358716122	657	0.657	0.24762128529520355
X	0.009928270664151	579	0.579	0.2578685321838357
X	0.009415503453214543	491	0.491	0.2676623900582632
X	0.009623994536109874	848	0.848	0.22472592247647874
X	0.010361080355720897	1671	1.671	0.183714258399581
X	0.009928101758649022	1245	1.245	0.199786262684917
X	0.009828461879741932	879	0.879	0.22361395048765442
X	0.009996299953311092	1116	1.116	0.20767857485727612
X	0.009784996286228087	596	0.596	0.25415859545709507
X	0.01040286048930352	2242	2.242	0.16679091403529409
X	0.009801699501261599	402	0.402	0.2899735888266539
X	0.010157871403662738	7336	7.336	0.11145882556231033
X	0.009935042022542652	1028	1.028	0.21300617705729588
X	0.010036728198828749	795	0.795	0.23284934014676295
X	0.009927729444346984	1721	1.721	0.17934517103952916
X	0.009929364380379292	852	0.852	0.2267221499404858
X	0.00979164341010258	1580	1.58	0.18368181722788857
X	0.009509616730221488	513	0.513	0.26465608444794125
X	0.009838435027930503	2105	2.105	0.1671955216312065
X	0.010241177855199913	3421	3.421	0.14412266298977233
X	0.010044532500738123	1382	1.382	0.19370487988551086
X	0.01021857494023872	3507	3.507	0.14282958997864775
X	0.010046698307388888	820	0.82	0.2305348053060802
X	0.010070434505903798	2696	2.696	0.15515869105243757
X	0.00922900081687761	373	0.373	0.2913949029549183
X	0.009534892150726202	1232	1.232	0.19780397999969102
X	0.009813996907628169	1020	1.02	0.2126907397128873
X	0.010125431110064007	796	0.796	0.23343545478353842
X	0.009650624884732804	628	0.628	0.24861763476790424
X	0.009864099145224585	651	0.651	0.2474526714495421
X	0.009967029805802299	1161	1.161	0.20475971312529342
X	0.0098050984025135	1205	1.205	0.20113530259931253
X	0.010125566292810239	392	0.392	0.2956045297614634
X	0.010114697037131097	2717	2.717	0.15498431454016898
X	0.010253275260539417	7538	7.538	0.11079891166630756
X	0.009915802216871402	1681	1.681	0.18068414600881372
X	0.01007705684191464	1099	1.099	0.2093045512240468
X	0.010119265346431079	1022	1.022	0.21473328240531017
X	0.009840739910853134	557	0.557	0.26044989468056445
X	0.0099216326040218	805	0.805	0.23099136944192256
X	0.010056498843015026	1091	1.091	0.20967211971882135
X	0.009681759939623249	821	0.821	0.2276164877473101
X	0.009864025220727463	2244	2.244	0.16381126866743195
X	0.009814383181506991	901	0.901	0.22167296982256196
X	0.010149098909393714	3143	3.143	0.1478067841142945
X	0.01014180790030194	1367	1.367	0.19503637743931718
X	0.01038652223504388	4346	4.346	0.1336985850408012
X	0.010102759510684501	1311	1.311	0.1975206204429849
X	0.009867106259988335	792	0.792	0.23182212076063338
X	0.010115025298044976	1416	1.416	0.19259037256850622
X	0.009971245264082199	265	0.265	0.3350949181371439
X	0.010723717337079039	2154	2.154	0.17075229637123313
X	0.009631906561376665	838	0.838	0.22567809858245183
X	0.009894036090456543	1580	1.58	0.1843198610788719
X	0.00965295032937768	394	0.394	0.2904387885144068
X	0.009492265963744325	1222	1.222	0.19804607092312868
X	0.0097020642041001	1167	1.167	0.20258055785881707
X	0.009953733420152938	1147	1.147	0.20549796671824244
X	0.00977741061798917	1134	1.134	0.20505495428305248
X	0.009573439443801849	1306	1.306	0.19425622249565647
X	0.009507766272923521	348	0.348	0.3011848235610462
X	0.00928552567033225	412	0.412	0.2824683282564964
X	0.009569238258865081	639	0.639	0.24648595052837144
X	0.01006375206305517	2868	2.868	0.15195917505695217
X	0.009922854599721632	1298	1.298	0.19699444437878996
X	0.010553580448460725	1851	1.851	0.1786478533730599
X	0.010324028520562663	1479	1.479	0.191114515545332
X	0.010078177220645822	1502	1.502	0.18861259318146095
X	0.00984183307099223	1980	1.98	0.1706620408412648
X	0.01030899990755198	2100	2.1	0.169954400399822
X	0.009785223347852288	723	0.723	0.23831103263947032
X	0.010119629778055002	2676	2.676	0.1557971486462817
X	0.009936156346235776	881	0.881	0.22425777178681136
X	0.00997743922610907	1680	1.68	0.1810936698633095
X	0.009808778686890618	1273	1.273	0.19751292050679126
X	0.009984818151397469	2953	2.953	0.1500924698623999
X	0.010325552161873539	2108	2.108	0.169829928258155
X	0.009937483404419021	626	0.626	0.25132403733957787
X	0.00995235344435804	1180	1.18	0.20355475938800702
X	0.010214513608650987	1959	1.959	0.1734047880848625
X	0.010263433601809105	4763	4.763	0.1291627744416126
X	0.009751422079156602	857	0.857	0.2249205020762178
X	0.00931568689946844	465	0.465	0.2715942952703657
X	0.010135860686979723	1271	1.271	0.19978905563268137
X	0.00986347342734202	1139	1.139	0.20535347273403332
X	0.009840114963700233	1099	1.099	0.2076510636283351
X	0.009630381044807425	684	0.684	0.2414695965478867
X	0.009652754514300262	841	0.841	0.22557194722758223
X	0.010056630644606004	1319	1.319	0.19682000330686727
X	0.009933575028600001	1744	1.744	0.17858831181235085
X	0.00957511362254472	617	0.617	0.2494324336594383
X	0.010259906275259422	5190	5.19	0.12550432103130763
X	0.0098059267361058	1557	1.557	0.18467155687357892
X	0.010248657956541082	8539	8.539	0.1062722792618871
X	0.009714622796074043	1842	1.842	0.1740652609363451
X	0.0100001910909883	2008	2.008	0.17077129172895467
X	0.009849573093633025	483	0.483	0.2732059115383072
X	0.01010499287166624	1041	1.041	0.21331845390228843
X	0.010086893644240874	1235	1.235	0.2013864106043584
X	0.010155962733275698	834	0.834	0.230065597856615
X	0.01054388977331573	3174	3.174	0.14921001644474546
X	0.009750541441030318	1881	1.881	0.17306662013258392
X	0.010090440664804802	1153	1.153	0.20607577504342184
X	0.010277539436984402	818	0.818	0.23247626313690298
X	0.010200040282590651	1937	1.937	0.1739765664143171
X	0.010163860861048129	361	0.361	0.3042172501615356
X	0.009061136392595174	416	0.416	0.2792735328931437
X	0.009597594060222392	471	0.471	0.2731369716695406
X	0.009601058238872239	505	0.505	0.2668963136074652
X	0.009738013461327313	1135	1.135	0.20471900858344091
X	0.009698053906367281	672	0.672	0.24346606707040416
X	0.009836305935116712	1381	1.381	0.19240341810123787
X	0.010155728514303452	692	0.692	0.24483208406783097
X	0.01019948886737756	1927	1.927	0.17427385225232314
X	0.009855809520056193	798	0.798	0.23115136906738837
X	0.009873816955754039	778	0.778	0.23325725381824952
X	0.010840408781960985	941	0.941	0.22584926349476542
X	0.009575150009103682	748	0.748	0.23392752696328395
X	0.009763031180159813	527	0.527	0.2646009223361512
X	0.00976469400103296	1520	1.52	0.1858967013243494
X	0.009439422528251652	502	0.502	0.26591769427839534
X	0.00914581825989912	449	0.449	0.2731023320846607
X	0.010173764176155904	749	0.749	0.23859789965329575
X	0.010066242358399218	806	0.806	0.23201217756940765
X	0.009898518391019802	1873	1.873	0.17418502116948248
X	0.00986265336287511	645	0.645	0.24820547187682251
X	0.010005133753711143	661	0.661	0.24736626962731562
X	0.010413712824141858	2995	2.995	0.15149661300120734
X	0.010730858362836802	2979	2.979	0.15329264568961798
X	0.009736613415237171	1457	1.457	0.18835744542735797
X	0.01015718741852198	3174	3.174	0.1473631335592096
X	0.01024260397756479	5804	5.804	0.12084471427605212
X	0.01042841620059714	7567	7.567	0.11128369583762512
X	0.010102743466015049	2288	2.288	0.16405687512336856
X	0.010066264936265352	297	0.297	0.32362145221042804
X	0.009944754058804321	2244	2.244	0.16425694135574836
X	0.011348158906658877	86401	86.401	0.05083180676198446
X	0.010356224916219839	53482	53.482	0.05785371849481416
X	0.00988478749297216	1546	1.546	0.18560340920667695
X	0.011383752468247741	16378	16.378	0.08858119313200542
X	0.010114952215537801	26213	26.213	0.0728029598152941
X	0.01012983325984194	8638	8.638	0.10545399384468301
X	0.010811302682803631	84337	84.337	0.05042198737536408
X	0.010202110080465116	1030	1.03	0.21475877934978674
X	0.010302942323811378	3501	3.501	0.14330336417116998
X	0.010304317835346957	63394	63.394	0.05457457514661977
X	0.010352189130153422	252714	252.714	0.03447209135101399
X	0.009927565659550401	3979	3.979	0.13563016069916983
X	0.010204628648328842	12112	12.112	0.09444822076254535
X	0.011183350600930321	44618	44.618	0.06305031786688951
X	0.010426371562826872	95578	95.578	0.04778135040978077
X	0.01014359176663896	7502	7.502	0.11057872231575717
X	0.010021752955943717	14719	14.719	0.08797416008291208
X	0.010421013318600176	23769	23.769	0.07596840590861405
X	0.010193780722278103	5521	5.521	0.1226796457882494
X	0.009827275323953678	2700	2.7	0.15382365266058046
X	0.011942403821905933	68702	68.702	0.05580941335619761
X	0.009878965601180609	2509	2.509	0.15790780765522153
X	0.009802274020067298	1417	1.417	0.19053978151696335
X	0.010209589217159701	10988	10.988	0.09758053749008726
X	0.009917414851668	4964	4.964	0.12594731183294083
X	0.011466013194372382	103357	103.357	0.0480497184613725
X	0.010291524114346199	57847	57.847	0.05624271601445876
X	0.01016738295805721	749	0.749	0.2385480045294173
X	0.010107231635500093	9541	9.541	0.1019403470251711
X	0.010298321678050126	2072	2.072	0.17065757393299547
X	0.009907128437702759	7851	7.851	0.10806231620972957
X	0.01039334520208218	36630	36.63	0.06571121204007822
X	0.00990964907671808	10700	10.7	0.09747460842999686
X	0.010167255401793876	30716	30.716	0.06917455246499553
X	0.009572785519959676	323	0.323	0.3094652831016911
X	0.010171422151920524	1703	1.703	0.18143553257258407
X	0.010403985760744791	11004	11.004	0.09814836194537024
X	0.010095151375264007	20303	20.303	0.07922292511329765
X	0.010347807889228114	7297	7.297	0.11234868762572699
X	0.009913143495009833	17775	17.775	0.08231277838539965
X	0.010719068384228345	106670	106.67	0.04649128843596965
X	0.01040016694447643	5583	5.583	0.12304318401847801
X	0.010254090943370249	8855	8.855	0.10501133986476834
X	0.010878563208392002	27091	27.091	0.07377606730391507
X	0.009681317720970719	1696	1.696	0.17871851274929704
X	0.01027600620557383	33461	33.461	0.06746758227026549
X	0.012012702384827318	31368	31.368	0.07261917254877434
X	0.011770008558025149	65512	65.512	0.056426785056854945
X	0.010011063902237154	7516	7.516	0.11002663241418702
X	0.01009766721694151	7163	7.163	0.11212661228482157
X	0.00973991525910679	5228	5.228	0.12304761486337017
X	0.010168908734314553	2848	2.848	0.15284273069207407
X	0.010418082821487649	4666	4.666	0.1307016360087645
X	0.01009365924623248	3634	3.634	0.1405684659799769
X	0.010053122268879	7446	7.446	0.11052469138343284
X	0.010445834694220137	56069	56.069	0.057113675906422405
X	0.010867298852630575	21736	21.736	0.07936834571273993
X	0.01036358077639663	3179	3.179	0.1482767652126168
X	0.010232183314567862	5213	5.213	0.12520649621669996
X	0.010463439929513704	26454	26.454	0.07340534971480302
X	0.010382656860840792	8094	8.094	0.10865467914949307
X	0.009991117499106592	1176	1.176	0.20404952191883932
X	0.011347283579610505	6651	6.651	0.11949094254748853
X	0.010816078810389367	95518	95.518	0.04837951840779233
X	0.00996309147212652	4152	4.152	0.13387893767868309
X	0.00984210671325396	33482	33.482	0.06649039186554803
X	0.010303134576906832	1433	1.433	0.19300759271761478
X	0.011342451632097824	12166	12.166	0.09769065945241939
X	0.01017525918706035	15508	15.508	0.08689535518228582
X	0.011989704409470302	45833	45.833	0.06395532797855058
X	0.010257815203511038	22665	22.665	0.0767773377967286
X	0.010228393403343777	5176	5.176	0.12548863210194602
X	0.010380002613599254	43803	43.803	0.06188202767976126
X	0.01069448145781069	48737	48.737	0.06031615469172053
X	0.010052607860041153	2398	2.398	0.16124138538095154
X	0.01022083191844315	22238	22.238	0.0771726665720556
X	0.010261818457505225	61441	61.441	0.0550708801174893
X	0.011814400549867285	115181	115.181	0.046810536147850405
X	0.010197980676063182	21879	21.879	0.07753460382563679
X	0.010258658464252994	532	0.532	0.2681593718757795
X	0.010243751203944001	6749	6.749	0.11492288801072231
X	0.011882234915595362	46403	46.403	0.06350150599239865
X	0.011263354267532404	84269	84.269	0.05112892182053369
X	0.010228883802976098	8501	8.501	0.1063618974455261
X	0.0103290166402985	1055	1.055	0.21392859512356016
X	0.010382440696567203	23575	23.575	0.07608211655919968
X	0.0106573932994527	45401	45.401	0.06168721897241099
X	0.010013673308846593	56472	56.472	0.05618064874746215
X	0.01027010861890284	2472	2.472	0.1607590841978883
X	0.010250283165729216	6271	6.271	0.11779667195664374
X	0.0103666502559945	6364	6.364	0.11766196363128441
X	0.010113970478054495	4324	4.324	0.13274311317748916
X	0.01019429501069568	6240	6.24	0.11777619946456355
X	0.010305641561110529	3718	3.718	0.14047160650864934
X	0.01079538691102138	27032	27.032	0.07364105660583889
X	0.010388116717503717	16908	16.908	0.08501225321737704
X	0.010432597838329684	13041	13.041	0.09283117705220308
X	0.0098116146054886	4610	4.61	0.12863125845490964
X	0.009963064757461322	2176	2.176	0.1660522204542253
X	0.010526934624655683	333852	333.852	0.031592402063483016
X	0.010219670064398098	4508	4.508	0.1313664072666089
X	0.010319102142732003	59118	59.118	0.05588656186746115
X	0.009910968568916576	1009	1.009	0.21416162041513562
X	0.010390072755108599	1705	1.705	0.1826549313126479
X	0.009529583275908832	1661	1.661	0.17901994761327406
X	0.010435052952513294	19653	19.653	0.08097580379964595
X	0.010134510025117902	1610	1.61	0.18463993001597856
X	0.010483247640260384	11391	11.391	0.09726979435935604
X	0.009908830020254864	2851	2.851	0.15147526479840495
X	0.010329615686142003	1583	1.583	0.18686777576425442
X	0.010408790906119939	112480	112.48	0.045231569628352716
X	0.010259756027133377	12786	12.786	0.09292532980683132
X	0.010259284047919103	5081	5.081	0.1263928828952002
X	0.011455755664898698	31775	31.775	0.07117258301522747
X	0.011433120216404006	29613	29.613	0.07281610668529855
X	0.010070932008476492	2597	2.597	0.15710833934715615
X	0.010118535059513762	4924	4.924	0.12713541124139044
X	0.010327095345160256	3760	3.76	0.14004365833881685
X	0.009841974810776848	265	0.265	0.3336405218045
X	0.010137621138369662	2223	2.223	0.16583092810917577
X	0.01014326084458773	5111	5.111	0.12566776879377883
X	0.010225050393855602	923	0.923	0.22292284928881842
X	0.010053189968180916	2479	2.479	0.1594688225066826
X	0.010191368694733846	4152	4.152	0.1348937173189398
X	0.009993512361788122	1763	1.763	0.17830161650934961
X	0.010701275140370215	115114	115.114	0.04530044432222451
X	0.010097211210427073	2116	2.116	0.16835599758047218
X	0.01024974728597504	22375	22.375	0.0770873952408355
X	0.009636009843228297	4873	4.873	0.12551634219490876
X	0.010079464357334105	9299	9.299	0.10272285198236478
X	0.010780235529810661	16149	16.149	0.08739653307832518
X	0.011609006431311358	644770	644.77	0.026209782066959324
X	0.010441234611844081	224734	224.734	0.035949624422232984
X	0.01044625856653148	14462	14.462	0.08972444997253348
X	0.010644069114285356	23668	23.668	0.07661527682112426
X	0.010175807163937126	1036	1.036	0.2141590200649123
X	0.010131429419440294	11089	11.089	0.0970344867946538
X	0.010304720423650203	16707	16.707	0.08512280252901568
X	0.010458558492984785	29696	29.696	0.0706194978375967
X	0.009998025835600562	2744	2.744	0.15387806479232244
X	0.010277910923717475	25022	25.022	0.07433515931770429
X	0.01140751523870888	71784	71.784	0.05416529768162762
X	0.009857250922614394	9966	9.966	0.09963493528034818
X	0.010124543646771842	2378	2.378	0.16207693013919777
X	0.011878883700716002	8550	8.55	0.11158432018960322
X	0.010661824720285939	9979	9.979	0.10223074019406106
X	0.01021985571407717	2240	2.24	0.16585640539429838
X	0.010405140811518304	61112	61.112	0.05542518545300512
X	0.010052153288987143	10529	10.529	0.09846698924294488
X	0.009740745402879569	756	0.756	0.234435548084353
X	0.0108024133913381	41880	41.88	0.06365580871535259
X	0.010642711366447477	5482	5.482	0.12474919475435918
X	0.010170473699801938	341	0.341	0.31011939927105553
X	0.011094453815937452	24106	24.106	0.0772076365367395
X	0.011153443267174948	34164	34.164	0.06885678636919484
X	0.011918526901407277	25597	25.597	0.0775076383545702
X	0.011410042084420998	32882	32.882	0.07027103342189774
X	0.010676441184353557	13322	13.322	0.09288644911035887
X	0.00995955334601337	2387	2.387	0.16098885604444274
X	0.010100682464386244	14009	14.009	0.08967014272947128
X	0.01074421281796096	45735	45.735	0.061703335247073506
X	0.01141819457018459	56432	56.432	0.0587071051261452
X	0.010021232196194576	5152	5.152	0.12482888468810965
X	0.010853542113733764	144333	144.333	0.042208697666036445
X	0.010442108140093895	101511	101.511	0.04685526086500521
X	0.0104439199076496	31242	31.242	0.06940246592023416
X	0.010429958861698427	11704	11.704	0.0962312426875396
X	0.010059245401407341	3519	3.519	0.14192166974587112
X	0.010139630328502755	3263	3.263	0.14592678554080143
X	0.010063499192581872	3333	3.333	0.14453440340058427
X	0.010211472569175373	2173	2.173	0.16749797062685925
X	0.010422343718257017	80762	80.762	0.05053428307382438
X	0.010381130249849404	43734	43.734	0.061916796704713874
X	0.010410195391491838	102110	102.11	0.046715772512232454
X	0.011276801571855821	39878	39.878	0.0656373224498334
X	0.010398876634845717	23974	23.974	0.07569757640948305
X	0.010201485933710227	6081	6.081	0.11882180459241337
X	0.011379711731899389	25977	25.977	0.07594760601693132
X	0.009883956355615005	700	0.7	0.24170052458875907
X	0.010295446023474153	7353	7.353	0.11187340761195137
X	0.011327978101313158	66253	66.253	0.05550289259939931
X	0.01026678966024375	13005	13.005	0.09242187379970929
X	0.009986882608346563	1103	1.103	0.20842580018322965
X	0.010240846790610183	5741	5.741	0.12127820877572579
X	0.010885591905375443	17628	17.628	0.08515621222973223
X	0.010084343497440607	11756	11.756	0.09501584120771633
X	0.010452643415658401	19719	19.719	0.08093078433285815
X	0.010286713701093707	8846	8.846	0.10515822314604088
X	0.01029403465547337	12066	12.066	0.09484347001088769
X	0.01026833931318195	1370	1.37	0.19570107655353092
X	0.01006305192621576	1054	1.054	0.21214348596006605
X	0.010161099372088962	17497	17.497	0.08343066443278636
X	0.011327546389681602	9399	9.399	0.10641872873865371
X	0.009940519157287442	2729	2.729	0.1538633475455447
X	0.010418538114571205	80333	80.333	0.05061791665112383
X	0.00989382781323045	2451	2.451	0.15922344353810497
X	0.011620059831854244	12985	12.985	0.09636562373565427
X	0.0101012952280446	2837	2.837	0.15270007235382613
X	0.010321384894164962	35974	35.974	0.06595530724058318
X	0.009976976323705432	2751	2.751	0.15363946412662194
X	0.010569731265553103	16622	16.622	0.08599243019377982
X	0.010285225249062094	5766	5.766	0.12127735679856404
X	0.010033667750946901	6073	6.073	0.11821850917838043
X	0.011305515563159764	14843	14.843	0.09132497491140967
X	0.01026080132442	242761	242.761	0.034833794622473095
X	0.01028886250347606	5032	5.032	0.12692355965994778
X	0.009588046161728276	2743	2.743	0.15176376876462183
X	0.0102519259572159	28708	28.708	0.07094702261680025
X	0.010226998460997498	4743	4.743	0.12919083006959547
X	0.01196503681671168	122141	122.141	0.046098223513236405
X	0.009718057343394274	1922	1.922	0.17163612119453886
X	0.011482979081882942	34492	34.492	0.06930726037348707
X	0.011906382376429969	149803	149.803	0.04299518002454567
X	0.009892435761745354	1481	1.481	0.18832850552470246
X	0.010126046297625216	143003	143.003	0.041371212892050896
X	0.010295956530596125	10851	10.851	0.09826502725495435
X	0.011925277257255682	62300	62.3	0.05763152956686317
X	0.01014188536569411	3331	3.331	0.1449376965395107
X	0.01012952792863286	23253	23.253	0.07580596923000807
X	0.009834497794996	1147	1.147	0.20467411591003415
X	0.010310366023494513	1855	1.855	0.1771373145751864
X	0.009951120773176319	2103	2.103	0.16788461850918973
X	0.01043006639123224	22354	22.354	0.07756109684267397
X	0.010417093760322082	13243	13.243	0.09231098774314565
X	0.010864980793106764	24288	24.288	0.07647962260914243
X	0.010093924701569518	5600	5.6	0.12170002792655527
X	0.01007428114190802	2010	2.01	0.17113519591272333
X	0.00990332360237052	1592	1.592	0.18391308764313144
X	0.011064151967307182	29786	29.786	0.07188450982840791
X	0.010160520244136901	5804	5.804	0.12052103350870726
X	0.011907043476595873	114828	114.828	0.04698061665564724
X	0.011202354635788803	10632	10.632	0.10175711444347704
X	0.010080027615017736	2385	2.385	0.1616805506055424
X	0.010294331115045116	2387	2.387	0.16277282641380697
X	0.010141230401601265	1760	1.76	0.17927758527115936
X	0.011919830327089605	425423	425.423	0.030372674499524764
X	0.009912991091198907	2360	2.36	0.16134824192838296
X	0.010313610836496985	6010	6.01	0.1197233212305864
X	0.01019984835013185	33675	33.675	0.06715763134406258
X	0.010002060101102654	17989	17.989	0.08222946777962685
X	0.01006837474363474	2514	2.514	0.15880519280236402
X	0.010227115230272431	2106	2.106	0.16934209467147554
X	0.01175235808044747	10715	10.715	0.10312824172437668
X	0.010250538687712163	10176	10.176	0.10024357123742003
X	0.010167608208681941	10951	10.951	0.09755622980078772
X	0.01035948716462936	22754	22.754	0.07692960472330691
X	0.01032075701141928	26763	26.763	0.07278785474427588
X	0.01135488930041105	13036	13.036	0.09550208450334566
X	0.011894011936394529	58355	58.355	0.058850492122026934
X	0.01034044109471396	30100	30.1	0.07003646052908923
X	0.009932575119944621	6246	6.246	0.11672215382223854
X	0.01043150348550787	48313	48.313	0.05999213260456603
X	0.010119443128502622	6754	6.754	0.11442788018593904
X	0.010283306829377004	3364	3.364	0.14513058162011777
X	0.010098803724543833	5836	5.836	0.1200562801005981
X	0.010099598542422405	4438	4.438	0.1315342757061743
X	0.010231359347868481	26101	26.101	0.0731855594795348
X	0.010112063544896951	5895	5.895	0.11970675557087321
X	0.009775210194915162	3887	3.887	0.135989023331847
X	0.010395429331703463	10056	10.056	0.10111270336617051
X	0.010052593536819593	7191	7.191	0.11181402418675716
X	0.010189156760136339	14834	14.834	0.08823210822381711
X	0.009987090216434938	12777	12.777	0.09211634395579471
X	0.010145777781110399	5749	5.749	0.12084564732972461
X	0.009886417137594831	6897	6.897	0.11275251424559397
X	0.01057942900131312	5595	5.595	0.12365757130652522
X	0.0100502016383649	1134	1.134	0.20694450871649697
X	0.010194463905147388	2303	2.303	0.1641938242596302
X	0.010279381236768543	11961	11.961	0.09507503507297188
X	0.011349322282524481	79990	79.99	0.052156856752329506
X	0.010316012373219016	2222	2.222	0.16682300243920128
X	0.009909310212966604	579	0.579	0.25770427331717205
X	0.010296312054156	1360	1.36	0.1963575460894577
X	0.010411041885947247	58067	58.067	0.056388204777160424
X	0.011389482752928002	118385	118.385	0.04582147948594225
X	0.01007911531102063	11887	11.887	0.09464914976373519
X	0.01034060225691712	2468	2.468	0.16121306172892755
X	0.01135578326068688	21091	21.091	0.08135295035521127
X	0.011942887886928665	71990	71.99	0.05494722033719723
X	0.0097822982446665	1107	1.107	0.20674313279622233
X	0.01010871232644111	5351	5.351	0.12361949417008272
X	0.010382370259145266	15583	15.583	0.08734039412741249
X	0.010400257850653193	2536	2.536	0.1600657028123517
X	0.01040725346140399	10115	10.115	0.10095397294265562
X	0.011040086549118838	25536	25.536	0.07561475270157415
X	0.010351452665144928	9495	9.495	0.1029205528685934
X	0.0100938362261709	2480	2.48	0.1596619850146526
X	0.009720199489830014	1042	1.042	0.21050826559722
X	0.010013054533949593	13974	13.974	0.08948466450500261
X	0.01036181098631939	11989	11.989	0.0952542222213487
X	0.010220280590530423	11856	11.856	0.09517178383873989
X	0.01151078767663872	13300	13.3	0.09529814945909845
X	0.011449429485275341	106166	106.166	0.04759917921660297
X	0.009916654401615358	3536	3.536	0.14102116797135408
X	0.009613761293437365	1105	1.105	0.2056728749724354
X	0.00983145716689538	4630	4.63	0.12853230625105153
X	0.010222497918790697	19493	19.493	0.0806416581877116
X	0.01008000340739595	13692	13.692	0.09029518583518378
X	0.009916144699902704	1512	1.512	0.18718182967184904
X	0.009839130005129601	1014	1.014	0.2132911887167606
X	0.010279764368465444	23461	23.461	0.07595310746123335
X	0.01012705337612375	1995	1.995	0.17186208106409548
X	0.009886895425225	9242	9.242	0.10227386270598125
X	0.010185658297345438	1539	1.539	0.18775153409714362
X	0.010856088461121	6724	6.724	0.11731365242381799
X	0.010418586167245092	7851	7.851	0.10989078359048919
X	0.0101796506281961	4100	4.1	0.13540966763081372
X	0.009835093034283853	2504	2.504	0.15777858224957542
X	0.010510853079141475	54971	54.971	0.05761047323367338
X	0.010425938976883658	63858	63.858	0.05465542663483847
X	0.010376014080431872	49526	49.526	0.05939261305337626
X	0.010240462384160315	53929	53.929	0.057477658331752064
X	0.010302801463067848	12154	12.154	0.09464086470440065
X	0.010147046863723922	5665	5.665	0.12144507776795788
X	0.01113715735732956	5363	5.363	0.12758183755102392
X	0.010427371368268802	49248	49.248	0.05960217125344006
X	0.011606970231971166	4169	4.169	0.14067890907630348
X	0.010243600628308843	7902	7.902	0.1090364920751487
X	0.010621278181077758	3465	3.465	0.1452639496437336
X	0.01184021370223185	41639	41.639	0.06575867946696898
X	0.010153274215457631	18846	18.846	0.08136962408033414
X	0.0113011020075084	10360	10.36	0.1029406753644727
X	0.011109673478454731	58654	58.654	0.057429436283475264
X	0.010308902169559538	21718	21.718	0.0780065127092659
X	0.010429485460847548	8951	8.951	0.1052277881938823
X	0.01133264111222448	42346	42.346	0.06444257922295853
X	0.010243672944865053	57216	57.216	0.05636109261762451
X	0.011374927864825845	25377	25.377	0.0765307772112985
X	0.010207871772321565	4736	4.736	0.12917382033181424
X	0.010541826810519968	29962	29.962	0.07059625976161522
X	0.010360346368766399	3814	3.814	0.1395290151367738
X	0.01040183370470281	27322	27.322	0.0724768216443523
X	0.011856187206148895	253177	253.177	0.036044604636034636
X	0.011510771468759028	26404	26.404	0.07582485163230465
X	0.010253995480774243	10210	10.21	0.1001434294436697
X	0.01029426731897332	4315	4.315	0.13362002455293992
X	0.0102415522312055	7584	7.584	0.11053228630888379
X	0.010325583066489757	4527	4.527	0.13163398532451642
X	0.0099811319480924	4069	4.069	0.13486441379591202
X	0.011549546483118664	10709	10.709	0.10255070984181157
X	0.011898599791178119	134939	134.939	0.044509507618234107
X	0.009954926220417902	1055	1.055	0.2113141254087256
X	0.01020988375261497	95245	95.245	0.04750356504240023
X	0.010189871894649295	11001	11.001	0.0974792516625041
X	0.010331754896578027	11174	11.174	0.09742157412499079
X	0.010383215610379378	3945	3.945	0.13806861073280405
X	0.010514331737384641	31839	31.839	0.06912058717449611
X	0.010444646562794082	81863	81.863	0.05034256785885259
X	0.009979113646362604	1856	1.856	0.1751881254330343
X	0.01129651044545468	292154	292.154	0.03381511150255315
X	0.010421066752854157	14471	14.471	0.08963367674879044
X	0.010352831053827501	25936	25.936	0.07362953252236178
X	0.01038556292150182	5819	5.819	0.121299933104964
X	0.010294077646921501	5499	5.499	0.12324460200659039
X	0.01011467689218327	880	0.88	0.22567828905646287
X	0.009565620054610162	668	0.668	0.2428355252106251
X	0.011192832341743364	4712	4.712	0.13342736381830783
X	0.0101900278092797	48191	48.191	0.05957579004293963
X	0.011985480634480624	280371	280.371	0.03496558413541155
X	0.010073200451268508	3553	3.553	0.14153290570246171
X	0.010325035530462403	44374	44.374	0.06150649681865104
X	0.010285882082048524	3066	3.066	0.1497004584119459
X	0.010866489909157792	11652	11.652	0.09770038348187332
X	0.009722450773552261	3060	3.06	0.14701154512537257
X	0.011403317249552583	37301	37.301	0.06736540276023283
X	0.009868366864904428	3191	3.191	0.14569327106259763
X	0.011952187104044667	456016	456.016	0.029704512830243352
X	0.010197065523432204	2693	2.693	0.155864167960245
X	0.011496568266889417	34246	34.246	0.06950021104740352
X	0.010423160472020503	7566	7.566	0.11126989948008142
X	0.010323987227080948	33993	33.993	0.06721806217965867
X	0.01030049228085984	138313	138.313	0.042072500724961416
X	0.01031634665530216	11438	11.438	0.09661812643403174
X	0.01002165269083572	2116	2.116	0.16793500368930478
X	0.01059339647305152	46010	46.01	0.06129066467429652
X	0.01031525449142728	4052	4.052	0.136543229382222
X	0.00989673893363889	6209	6.209	0.11681272397525129
X	0.010070675234850782	61179	61.179	0.05480481788642376
X	0.01043510862384463	77106	77.106	0.051341623792232005
X	0.010893509008569271	7992	7.992	0.11087597211117574
X	0.009926372175912085	16127	16.127	0.08506397165771432
X	0.010360636628815983	45340	45.34	0.061136650620364934
X	0.011339825077018802	44565	44.565	0.06336811885248397
X	0.010329494859586601	13316	13.316	0.09188298573181249
X	0.010074373743037967	2093	2.093	0.16884295462220272
X	0.010893325953245065	91022	91.022	0.049280076689356946
X	0.01020243289397066	13113	13.113	0.09197445616288454
X	0.011451249012662255	17759	17.759	0.08639291286954127
X	0.010384094570656949	8436	8.436	0.10717101763531549
X	0.010234816887679023	10126	10.126	0.10035693398449601
X	0.011553300530662051	24712	24.712	0.077612688046301
X	0.010107741760999302	133456	133.456	0.04230957647178221
X	0.010375362383395995	14241	14.241	0.08998171424074894
X	0.01018471475103524	8013	8.013	0.10832230905286958
X	0.010123442853689442	1228	1.228	0.20201174630301727
X	0.011370838007805481	16866	16.866	0.08768529999742318
X	0.010132892273868202	13439	13.439	0.09101692466290223
X	0.010623990226543485	26530	26.53	0.07370836947920738
X	0.01042906792690017	50394	50.394	0.05915010975703006
X	0.01015325062635286	3266	3.266	0.14594738189870274
X	0.010346522769683297	10105	10.105	0.10079044592918225
X	0.011176883344951682	20964	20.964	0.08108655613870276
X	0.010268860902149653	30732	30.732	0.06939217214767568
X	0.01180472886559587	13836	13.836	0.09484515582626557
X	0.01038216477075057	36498	36.498	0.06576673524830252
X	0.010654534632216795	194048	194.048	0.03800800764932706
X	0.010412443469055604	18427	18.427	0.08267343964381
X	0.009545102791505822	815	0.815	0.22709504947361706
X	0.010185542945723375	2305	2.305	0.16409842704629743
X	0.010410902835751199	85168	85.168	0.04962920459198765
X	0.010107267217987501	14738	14.738	0.08818574709760736
X	0.01046836311881725	34377	34.377	0.06727756557074888
X	0.009776314563480623	1524	1.524	0.18580756844387034
X	0.009621185379937263	940	0.94	0.21712020632756893
X	0.011575567418698661	49598	49.598	0.061568613678444864
X	0.010393971656307822	27583	27.583	0.07222928951325337
X	0.01128477722155573	23670	23.67	0.07812048136067405
X	0.010016030770537712	2131	2.131	0.1675087123227687
X	0.010176821371644836	2585	2.585	0.15790063192299106
X	0.010187734482475551	8966	8.966	0.10435012197298503
X	0.01048317753128252	13914	13.914	0.09099418660214448
X	0.01082288619687472	18789	18.789	0.08320443486111156
X	0.01003585950511174	3819	3.819	0.13799657412643984
X	0.010378029282302019	38160	38.16	0.06478913403376596
X	0.010203883397926198	5921	5.921	0.11989198261236883
X	0.01024743950815768	7735	7.735	0.10982933358413227
X	0.011813116527771002	85020	85.02	0.05179419995969185
X	0.01085365539009641	5244	5.244	0.1274396143169366
X	0.010236234821720612	14494	14.494	0.08905343196035555
X	0.010321283310438101	57201	57.201	0.05650801164393825
X	0.01016666676718784	11172	11.172	0.09690567715046683
X	0.011280343802172443	21906	21.906	0.08015299428406633
X	0.011795465207875198	64684	64.684	0.05670735641837541
X	0.010017679352118017	1135	1.135	0.2066603156334494
X	0.010373823315608481	18685	18.685	0.0821892936859365
X	0.010357571235216783	9727	9.727	0.10211581417162822
X	0.010619023801675682	123169	123.169	0.044176814129703604
X	0.010412931705142911	59013	59.013	0.05608866574128796
X	0.010102301353308004	12029	12.029	0.09434747133323218
X	0.010266343079096477	22288	22.288	0.07722920391026582
X	0.010300266433663779	6775	6.775	0.11498637592739244
X	0.010241311406983672	4725	4.725	0.12941499096619488
X	0.010274072933589298	4138	4.138	0.13540999415602195
X	0.010113351309285565	4628	4.628	0.1297678913881606
X	0.010376819323870525	49973	49.973	0.059216528578557
X	0.01197880190443705	38111	38.111	0.06799145673954095
X	0.010473630316108805	1152	1.152	0.20871240085546428
X	0.0102794862163802	11603	11.603	0.0960432922553309
X	0.0102555827070004	5297	5.297	0.12463598668528764
X	0.011974231242235341	106439	106.439	0.048274259246167094
X	0.010306896463971841	13762	13.762	0.0908131597243919
X	0.010137167259491027	4715	4.715	0.1290659369189353
X	0.011267813419962303	26865	26.865	0.07485444603902762
X	0.010330352099610399	32525	32.525	0.06822851826029944
X	0.010269859262576325	5009	5.009	0.1270392204509701
X	0.010217575383591455	18663	18.663	0.08180668571239312
X	0.010228406096207126	6441	6.441	0.1166679466608462
time for making epsilon is 0.8644077777862549
epsilons are
[0.2484433931303386, 0.12191121320192937, 0.20055673059292511, 0.18650460433691643, 0.06645419049815586, 0.08263222586068444, 0.08566967811565843, 0.07854513137298887, 0.1566487054518223, 0.06328661360463449, 0.15838732971768857, 0.05560262247626726, 0.12714600442474636, 0.056457577587798104, 0.1033781700030999, 0.08269473983983244, 0.07914104558221574, 0.08645702545610176, 0.04727709554909544, 0.06621124548041549, 0.1225293976126258, 0.03335558892242694, 0.11779536229967669, 0.12721806475462205, 0.09471472116376495, 0.07016569548399341, 0.11389485725415013, 0.1136030387014364, 0.07071187267736541, 0.02347545490558696, 0.15752545982895488, 0.05173585857009048, 0.14131588613170318, 0.13373961508634596, 0.15025357861167374, 0.07181036825034733, 0.07412214466966824, 0.23369879784407896, 0.18386928919904247, 0.22429322604310076, 0.1556211622826953, 0.3840899657418716, 0.3087425018034565, 0.19958102607789224, 0.2633989047726691, 0.245231528114868, 0.17571934465328992, 0.1695981031198221, 0.19465083330738533, 0.17108808785558327, 0.14054819696644896, 0.26952079540836027, 0.1772970688693084, 0.2298255683228364, 0.1462528988707091, 0.18619068269365707, 0.21222848783215872, 0.19439235833124013, 0.20167305782967923, 0.23849729475901632, 0.12551257649774192, 0.23831960305219493, 0.16241578631824805, 0.14005351678056063, 0.18093723170895334, 0.20647135250571494, 0.20896126848475502, 0.15298593549956124, 0.18870424181844764, 0.1876689474491992, 0.12389393453492338, 0.17673842884361515, 0.1831574561297826, 0.22260284307235115, 0.18522912620906185, 0.1782417289751558, 0.2707108380930437, 0.2944871619393936, 0.17885647158583493, 0.11643867764642414, 0.2107650228164643, 0.2623499979604263, 0.18345197681098488, 0.23854666150562595, 0.23124444558666085, 0.2307027523197622, 0.2379676490785721, 0.29754468990677047, 0.16584181245925972, 0.18340135899721924, 0.15202459994992293, 0.24999773380119752, 0.25675507845023327, 0.1665341119568135, 0.17678553733175015, 0.16883295467003434, 0.19067279247475816, 0.1747135288595048, 0.15150347288703586, 0.17002641344240643, 0.14066630408059608, 0.15288556059327613, 0.19668665523077547, 0.28635855307588665, 0.17020480214314823, 0.2470698731630701, 0.1390964362651632, 0.3083954023321713, 0.2334893717169708, 0.24762128529520355, 0.2578685321838357, 0.2676623900582632, 0.22472592247647874, 0.183714258399581, 0.199786262684917, 0.22361395048765442, 0.20767857485727612, 0.25415859545709507, 0.16679091403529409, 0.2899735888266539, 0.11145882556231033, 0.21300617705729588, 0.23284934014676295, 0.17934517103952916, 0.2267221499404858, 0.18368181722788857, 0.26465608444794125, 0.1671955216312065, 0.14412266298977233, 0.19370487988551086, 0.14282958997864775, 0.2305348053060802, 0.15515869105243757, 0.2913949029549183, 0.19780397999969102, 0.2126907397128873, 0.23343545478353842, 0.24861763476790424, 0.2474526714495421, 0.20475971312529342, 0.20113530259931253, 0.2956045297614634, 0.15498431454016898, 0.11079891166630756, 0.18068414600881372, 0.2093045512240468, 0.21473328240531017, 0.26044989468056445, 0.23099136944192256, 0.20967211971882135, 0.2276164877473101, 0.16381126866743195, 0.22167296982256196, 0.1478067841142945, 0.19503637743931718, 0.1336985850408012, 0.1975206204429849, 0.23182212076063338, 0.19259037256850622, 0.3350949181371439, 0.17075229637123313, 0.22567809858245183, 0.1843198610788719, 0.2904387885144068, 0.19804607092312868, 0.20258055785881707, 0.20549796671824244, 0.20505495428305248, 0.19425622249565647, 0.3011848235610462, 0.2824683282564964, 0.24648595052837144, 0.15195917505695217, 0.19699444437878996, 0.1786478533730599, 0.191114515545332, 0.18861259318146095, 0.1706620408412648, 0.169954400399822, 0.23831103263947032, 0.1557971486462817, 0.22425777178681136, 0.1810936698633095, 0.19751292050679126, 0.1500924698623999, 0.169829928258155, 0.25132403733957787, 0.20355475938800702, 0.1734047880848625, 0.1291627744416126, 0.2249205020762178, 0.2715942952703657, 0.19978905563268137, 0.20535347273403332, 0.2076510636283351, 0.2414695965478867, 0.22557194722758223, 0.19682000330686727, 0.17858831181235085, 0.2494324336594383, 0.12550432103130763, 0.18467155687357892, 0.1062722792618871, 0.1740652609363451, 0.17077129172895467, 0.2732059115383072, 0.21331845390228843, 0.2013864106043584, 0.230065597856615, 0.14921001644474546, 0.17306662013258392, 0.20607577504342184, 0.23247626313690298, 0.1739765664143171, 0.3042172501615356, 0.2792735328931437, 0.2731369716695406, 0.2668963136074652, 0.20471900858344091, 0.24346606707040416, 0.19240341810123787, 0.24483208406783097, 0.17427385225232314, 0.23115136906738837, 0.23325725381824952, 0.22584926349476542, 0.23392752696328395, 0.2646009223361512, 0.1858967013243494, 0.26591769427839534, 0.2731023320846607, 0.23859789965329575, 0.23201217756940765, 0.17418502116948248, 0.24820547187682251, 0.24736626962731562, 0.15149661300120734, 0.15329264568961798, 0.18835744542735797, 0.1473631335592096, 0.12084471427605212, 0.11128369583762512, 0.16405687512336856, 0.32362145221042804, 0.16425694135574836, 0.05083180676198446, 0.05785371849481416, 0.18560340920667695, 0.08858119313200542, 0.0728029598152941, 0.10545399384468301, 0.05042198737536408, 0.21475877934978674, 0.14330336417116998, 0.05457457514661977, 0.03447209135101399, 0.13563016069916983, 0.09444822076254535, 0.06305031786688951, 0.04778135040978077, 0.11057872231575717, 0.08797416008291208, 0.07596840590861405, 0.1226796457882494, 0.15382365266058046, 0.05580941335619761, 0.15790780765522153, 0.19053978151696335, 0.09758053749008726, 0.12594731183294083, 0.0480497184613725, 0.05624271601445876, 0.2385480045294173, 0.1019403470251711, 0.17065757393299547, 0.10806231620972957, 0.06571121204007822, 0.09747460842999686, 0.06917455246499553, 0.3094652831016911, 0.18143553257258407, 0.09814836194537024, 0.07922292511329765, 0.11234868762572699, 0.08231277838539965, 0.04649128843596965, 0.12304318401847801, 0.10501133986476834, 0.07377606730391507, 0.17871851274929704, 0.06746758227026549, 0.07261917254877434, 0.056426785056854945, 0.11002663241418702, 0.11212661228482157, 0.12304761486337017, 0.15284273069207407, 0.1307016360087645, 0.1405684659799769, 0.11052469138343284, 0.057113675906422405, 0.07936834571273993, 0.1482767652126168, 0.12520649621669996, 0.07340534971480302, 0.10865467914949307, 0.20404952191883932, 0.11949094254748853, 0.04837951840779233, 0.13387893767868309, 0.06649039186554803, 0.19300759271761478, 0.09769065945241939, 0.08689535518228582, 0.06395532797855058, 0.0767773377967286, 0.12548863210194602, 0.06188202767976126, 0.06031615469172053, 0.16124138538095154, 0.0771726665720556, 0.0550708801174893, 0.046810536147850405, 0.07753460382563679, 0.2681593718757795, 0.11492288801072231, 0.06350150599239865, 0.05112892182053369, 0.1063618974455261, 0.21392859512356016, 0.07608211655919968, 0.06168721897241099, 0.05618064874746215, 0.1607590841978883, 0.11779667195664374, 0.11766196363128441, 0.13274311317748916, 0.11777619946456355, 0.14047160650864934, 0.07364105660583889, 0.08501225321737704, 0.09283117705220308, 0.12863125845490964, 0.1660522204542253, 0.031592402063483016, 0.1313664072666089, 0.05588656186746115, 0.21416162041513562, 0.1826549313126479, 0.17901994761327406, 0.08097580379964595, 0.18463993001597856, 0.09726979435935604, 0.15147526479840495, 0.18686777576425442, 0.045231569628352716, 0.09292532980683132, 0.1263928828952002, 0.07117258301522747, 0.07281610668529855, 0.15710833934715615, 0.12713541124139044, 0.14004365833881685, 0.3336405218045, 0.16583092810917577, 0.12566776879377883, 0.22292284928881842, 0.1594688225066826, 0.1348937173189398, 0.17830161650934961, 0.04530044432222451, 0.16835599758047218, 0.0770873952408355, 0.12551634219490876, 0.10272285198236478, 0.08739653307832518, 0.026209782066959324, 0.035949624422232984, 0.08972444997253348, 0.07661527682112426, 0.2141590200649123, 0.0970344867946538, 0.08512280252901568, 0.0706194978375967, 0.15387806479232244, 0.07433515931770429, 0.05416529768162762, 0.09963493528034818, 0.16207693013919777, 0.11158432018960322, 0.10223074019406106, 0.16585640539429838, 0.05542518545300512, 0.09846698924294488, 0.234435548084353, 0.06365580871535259, 0.12474919475435918, 0.31011939927105553, 0.0772076365367395, 0.06885678636919484, 0.0775076383545702, 0.07027103342189774, 0.09288644911035887, 0.16098885604444274, 0.08967014272947128, 0.061703335247073506, 0.0587071051261452, 0.12482888468810965, 0.042208697666036445, 0.04685526086500521, 0.06940246592023416, 0.0962312426875396, 0.14192166974587112, 0.14592678554080143, 0.14453440340058427, 0.16749797062685925, 0.05053428307382438, 0.061916796704713874, 0.046715772512232454, 0.0656373224498334, 0.07569757640948305, 0.11882180459241337, 0.07594760601693132, 0.24170052458875907, 0.11187340761195137, 0.05550289259939931, 0.09242187379970929, 0.20842580018322965, 0.12127820877572579, 0.08515621222973223, 0.09501584120771633, 0.08093078433285815, 0.10515822314604088, 0.09484347001088769, 0.19570107655353092, 0.21214348596006605, 0.08343066443278636, 0.10641872873865371, 0.1538633475455447, 0.05061791665112383, 0.15922344353810497, 0.09636562373565427, 0.15270007235382613, 0.06595530724058318, 0.15363946412662194, 0.08599243019377982, 0.12127735679856404, 0.11821850917838043, 0.09132497491140967, 0.034833794622473095, 0.12692355965994778, 0.15176376876462183, 0.07094702261680025, 0.12919083006959547, 0.046098223513236405, 0.17163612119453886, 0.06930726037348707, 0.04299518002454567, 0.18832850552470246, 0.041371212892050896, 0.09826502725495435, 0.05763152956686317, 0.1449376965395107, 0.07580596923000807, 0.20467411591003415, 0.1771373145751864, 0.16788461850918973, 0.07756109684267397, 0.09231098774314565, 0.07647962260914243, 0.12170002792655527, 0.17113519591272333, 0.18391308764313144, 0.07188450982840791, 0.12052103350870726, 0.04698061665564724, 0.10175711444347704, 0.1616805506055424, 0.16277282641380697, 0.17927758527115936, 0.030372674499524764, 0.16134824192838296, 0.1197233212305864, 0.06715763134406258, 0.08222946777962685, 0.15880519280236402, 0.16934209467147554, 0.10312824172437668, 0.10024357123742003, 0.09755622980078772, 0.07692960472330691, 0.07278785474427588, 0.09550208450334566, 0.058850492122026934, 0.07003646052908923, 0.11672215382223854, 0.05999213260456603, 0.11442788018593904, 0.14513058162011777, 0.1200562801005981, 0.1315342757061743, 0.0731855594795348, 0.11970675557087321, 0.135989023331847, 0.10111270336617051, 0.11181402418675716, 0.08823210822381711, 0.09211634395579471, 0.12084564732972461, 0.11275251424559397, 0.12365757130652522, 0.20694450871649697, 0.1641938242596302, 0.09507503507297188, 0.052156856752329506, 0.16682300243920128, 0.25770427331717205, 0.1963575460894577, 0.056388204777160424, 0.04582147948594225, 0.09464914976373519, 0.16121306172892755, 0.08135295035521127, 0.05494722033719723, 0.20674313279622233, 0.12361949417008272, 0.08734039412741249, 0.1600657028123517, 0.10095397294265562, 0.07561475270157415, 0.1029205528685934, 0.1596619850146526, 0.21050826559722, 0.08948466450500261, 0.0952542222213487, 0.09517178383873989, 0.09529814945909845, 0.04759917921660297, 0.14102116797135408, 0.2056728749724354, 0.12853230625105153, 0.0806416581877116, 0.09029518583518378, 0.18718182967184904, 0.2132911887167606, 0.07595310746123335, 0.17186208106409548, 0.10227386270598125, 0.18775153409714362, 0.11731365242381799, 0.10989078359048919, 0.13540966763081372, 0.15777858224957542, 0.05761047323367338, 0.05465542663483847, 0.05939261305337626, 0.057477658331752064, 0.09464086470440065, 0.12144507776795788, 0.12758183755102392, 0.05960217125344006, 0.14067890907630348, 0.1090364920751487, 0.1452639496437336, 0.06575867946696898, 0.08136962408033414, 0.1029406753644727, 0.057429436283475264, 0.0780065127092659, 0.1052277881938823, 0.06444257922295853, 0.05636109261762451, 0.0765307772112985, 0.12917382033181424, 0.07059625976161522, 0.1395290151367738, 0.0724768216443523, 0.036044604636034636, 0.07582485163230465, 0.1001434294436697, 0.13362002455293992, 0.11053228630888379, 0.13163398532451642, 0.13486441379591202, 0.10255070984181157, 0.044509507618234107, 0.2113141254087256, 0.04750356504240023, 0.0974792516625041, 0.09742157412499079, 0.13806861073280405, 0.06912058717449611, 0.05034256785885259, 0.1751881254330343, 0.03381511150255315, 0.08963367674879044, 0.07362953252236178, 0.121299933104964, 0.12324460200659039, 0.22567828905646287, 0.2428355252106251, 0.13342736381830783, 0.05957579004293963, 0.03496558413541155, 0.14153290570246171, 0.06150649681865104, 0.1497004584119459, 0.09770038348187332, 0.14701154512537257, 0.06736540276023283, 0.14569327106259763, 0.029704512830243352, 0.155864167960245, 0.06950021104740352, 0.11126989948008142, 0.06721806217965867, 0.042072500724961416, 0.09661812643403174, 0.16793500368930478, 0.06129066467429652, 0.136543229382222, 0.11681272397525129, 0.05480481788642376, 0.051341623792232005, 0.11087597211117574, 0.08506397165771432, 0.061136650620364934, 0.06336811885248397, 0.09188298573181249, 0.16884295462220272, 0.049280076689356946, 0.09197445616288454, 0.08639291286954127, 0.10717101763531549, 0.10035693398449601, 0.077612688046301, 0.04230957647178221, 0.08998171424074894, 0.10832230905286958, 0.20201174630301727, 0.08768529999742318, 0.09101692466290223, 0.07370836947920738, 0.05915010975703006, 0.14594738189870274, 0.10079044592918225, 0.08108655613870276, 0.06939217214767568, 0.09484515582626557, 0.06576673524830252, 0.03800800764932706, 0.08267343964381, 0.22709504947361706, 0.16409842704629743, 0.04962920459198765, 0.08818574709760736, 0.06727756557074888, 0.18580756844387034, 0.21712020632756893, 0.061568613678444864, 0.07222928951325337, 0.07812048136067405, 0.1675087123227687, 0.15790063192299106, 0.10435012197298503, 0.09099418660214448, 0.08320443486111156, 0.13799657412643984, 0.06478913403376596, 0.11989198261236883, 0.10982933358413227, 0.05179419995969185, 0.1274396143169366, 0.08905343196035555, 0.05650801164393825, 0.09690567715046683, 0.08015299428406633, 0.05670735641837541, 0.2066603156334494, 0.0821892936859365, 0.10211581417162822, 0.044176814129703604, 0.05608866574128796, 0.09434747133323218, 0.07722920391026582, 0.11498637592739244, 0.12941499096619488, 0.13540999415602195, 0.1297678913881606, 0.059216528578557, 0.06799145673954095, 0.20871240085546428, 0.0960432922553309, 0.12463598668528764, 0.048274259246167094, 0.0908131597243919, 0.1290659369189353, 0.07485444603902762, 0.06822851826029944, 0.1270392204509701, 0.08180668571239312, 0.1166679466608462]
0.10783063731783538
Making ranges
torch.Size([12265, 2])
We keep 3.16e+06/9.20e+07 =  3% of the original kernel matrix.

torch.Size([1248, 2])
We keep 6.03e+04/4.30e+05 = 14% of the original kernel matrix.

torch.Size([4178, 2])
We keep 4.73e+05/6.29e+06 =  7% of the original kernel matrix.

torch.Size([6726, 2])
We keep 2.59e+06/3.27e+07 =  7% of the original kernel matrix.

torch.Size([8634, 2])
We keep 2.28e+06/5.48e+07 =  4% of the original kernel matrix.

torch.Size([2013, 2])
We keep 1.49e+05/1.50e+06 =  9% of the original kernel matrix.

torch.Size([5032, 2])
We keep 6.79e+05/1.17e+07 =  5% of the original kernel matrix.

torch.Size([2465, 2])
We keep 2.54e+05/2.31e+06 = 11% of the original kernel matrix.

torch.Size([5414, 2])
We keep 8.34e+05/1.46e+07 =  5% of the original kernel matrix.

torch.Size([37850, 2])
We keep 5.32e+07/1.25e+09 =  4% of the original kernel matrix.

torch.Size([20488, 2])
We keep 1.05e+07/3.40e+08 =  3% of the original kernel matrix.

torch.Size([19288, 2])
We keep 1.39e+07/3.31e+08 =  4% of the original kernel matrix.

torch.Size([14972, 2])
We keep 5.78e+06/1.74e+08 =  3% of the original kernel matrix.

torch.Size([16764, 2])
We keep 1.27e+07/2.97e+08 =  4% of the original kernel matrix.

torch.Size([13913, 2])
We keep 5.55e+06/1.65e+08 =  3% of the original kernel matrix.

torch.Size([23526, 2])
We keep 1.45e+07/4.58e+08 =  3% of the original kernel matrix.

torch.Size([16435, 2])
We keep 6.62e+06/2.05e+08 =  3% of the original kernel matrix.

torch.Size([3981, 2])
We keep 6.09e+05/6.84e+06 =  8% of the original kernel matrix.

torch.Size([6750, 2])
We keep 1.19e+06/2.51e+07 =  4% of the original kernel matrix.

torch.Size([28191, 2])
We keep 2.82e+08/1.90e+09 = 14% of the original kernel matrix.

torch.Size([17500, 2])
We keep 1.22e+07/4.18e+08 =  2% of the original kernel matrix.

torch.Size([3564, 2])
We keep 8.51e+05/6.37e+06 = 13% of the original kernel matrix.

torch.Size([6452, 2])
We keep 1.18e+06/2.42e+07 =  4% of the original kernel matrix.

torch.Size([56360, 2])
We keep 1.05e+08/3.54e+09 =  2% of the original kernel matrix.

torch.Size([24820, 2])
We keep 1.60e+07/5.71e+08 =  2% of the original kernel matrix.

torch.Size([6371, 2])
We keep 1.49e+06/2.40e+07 =  6% of the original kernel matrix.

torch.Size([8437, 2])
We keep 2.01e+06/4.70e+07 =  4% of the original kernel matrix.

torch.Size([61389, 2])
We keep 1.28e+08/3.81e+09 =  3% of the original kernel matrix.

torch.Size([26652, 2])
We keep 1.67e+07/5.92e+08 =  2% of the original kernel matrix.

torch.Size([9772, 2])
We keep 3.69e+06/8.54e+07 =  4% of the original kernel matrix.

torch.Size([10514, 2])
We keep 3.30e+06/8.86e+07 =  3% of the original kernel matrix.

torch.Size([19170, 2])
We keep 1.86e+07/3.84e+08 =  4% of the original kernel matrix.

torch.Size([14980, 2])
We keep 6.20e+06/1.88e+08 =  3% of the original kernel matrix.

torch.Size([21773, 2])
We keep 1.90e+07/4.38e+08 =  4% of the original kernel matrix.

torch.Size([15757, 2])
We keep 6.46e+06/2.01e+08 =  3% of the original kernel matrix.

torch.Size([16397, 2])
We keep 1.85e+07/2.53e+08 =  7% of the original kernel matrix.

torch.Size([13574, 2])
We keep 5.28e+06/1.52e+08 =  3% of the original kernel matrix.

torch.Size([94059, 2])
We keep 3.19e+08/9.72e+09 =  3% of the original kernel matrix.

torch.Size([33489, 2])
We keep 2.49e+07/9.45e+08 =  2% of the original kernel matrix.

torch.Size([37856, 2])
We keep 6.92e+07/1.55e+09 =  4% of the original kernel matrix.

torch.Size([20480, 2])
We keep 1.14e+07/3.78e+08 =  3% of the original kernel matrix.

torch.Size([5697, 2])
We keep 4.67e+06/3.17e+07 = 14% of the original kernel matrix.

torch.Size([7942, 2])
We keep 2.19e+06/5.40e+07 =  4% of the original kernel matrix.

torch.Size([306362, 2])
We keep 1.98e+09/9.23e+10 =  2% of the original kernel matrix.

torch.Size([63231, 2])
We keep 6.95e+07/2.91e+09 =  2% of the original kernel matrix.

torch.Size([7879, 2])
We keep 2.17e+06/3.86e+07 =  5% of the original kernel matrix.

torch.Size([9376, 2])
We keep 2.42e+06/5.96e+07 =  4% of the original kernel matrix.

torch.Size([6496, 2])
We keep 1.48e+06/2.47e+07 =  5% of the original kernel matrix.

torch.Size([8611, 2])
We keep 2.01e+06/4.76e+07 =  4% of the original kernel matrix.

torch.Size([8973, 2])
We keep 3.76e+07/1.20e+08 = 31% of the original kernel matrix.

torch.Size([9408, 2])
We keep 3.31e+06/1.05e+08 =  3% of the original kernel matrix.

torch.Size([32701, 2])
We keep 2.75e+07/9.62e+08 =  2% of the original kernel matrix.

torch.Size([19061, 2])
We keep 9.02e+06/2.97e+08 =  3% of the original kernel matrix.

torch.Size([5919, 2])
We keep 1.09e+07/4.92e+07 = 22% of the original kernel matrix.

torch.Size([8048, 2])
We keep 2.50e+06/6.73e+07 =  3% of the original kernel matrix.

torch.Size([8480, 2])
We keep 3.14e+06/4.68e+07 =  6% of the original kernel matrix.

torch.Size([9773, 2])
We keep 2.71e+06/6.56e+07 =  4% of the original kernel matrix.

torch.Size([25238, 2])
We keep 7.13e+07/8.53e+08 =  8% of the original kernel matrix.

torch.Size([16564, 2])
We keep 8.95e+06/2.80e+08 =  3% of the original kernel matrix.

torch.Size([989459, 2])
We keep 1.13e+10/8.57e+11 =  1% of the original kernel matrix.

torch.Size([116596, 2])
We keep 1.95e+08/8.88e+09 =  2% of the original kernel matrix.

torch.Size([3394, 2])
We keep 1.68e+06/6.36e+06 = 26% of the original kernel matrix.

torch.Size([6220, 2])
We keep 1.20e+06/2.42e+07 =  4% of the original kernel matrix.

torch.Size([73327, 2])
We keep 5.13e+08/7.10e+09 =  7% of the original kernel matrix.

torch.Size([29771, 2])
We keep 2.16e+07/8.08e+08 =  2% of the original kernel matrix.

torch.Size([5107, 2])
We keep 9.84e+05/1.37e+07 =  7% of the original kernel matrix.

torch.Size([7539, 2])
We keep 1.65e+06/3.55e+07 =  4% of the original kernel matrix.

torch.Size([5780, 2])
We keep 1.36e+06/1.81e+07 =  7% of the original kernel matrix.

torch.Size([7992, 2])
We keep 1.80e+06/4.08e+07 =  4% of the original kernel matrix.

torch.Size([3906, 2])
We keep 1.35e+06/9.11e+06 = 14% of the original kernel matrix.

torch.Size([6685, 2])
We keep 1.35e+06/2.89e+07 =  4% of the original kernel matrix.

torch.Size([25384, 2])
We keep 8.95e+07/7.67e+08 = 11% of the original kernel matrix.

torch.Size([16662, 2])
We keep 8.00e+06/2.66e+08 =  3% of the original kernel matrix.

torch.Size([27914, 2])
We keep 3.02e+07/7.64e+08 =  3% of the original kernel matrix.

torch.Size([17711, 2])
We keep 7.98e+06/2.65e+08 =  3% of the original kernel matrix.

torch.Size([1390, 2])
We keep 8.46e+04/5.81e+05 = 14% of the original kernel matrix.

torch.Size([4260, 2])
We keep 5.26e+05/7.31e+06 =  7% of the original kernel matrix.

torch.Size([2491, 2])
We keep 2.96e+05/2.57e+06 = 11% of the original kernel matrix.

torch.Size([5404, 2])
We keep 8.99e+05/1.54e+07 =  5% of the original kernel matrix.

torch.Size([1485, 2])
We keep 9.73e+04/7.94e+05 = 12% of the original kernel matrix.

torch.Size([4401, 2])
We keep 5.65e+05/8.55e+06 =  6% of the original kernel matrix.

torch.Size([3725, 2])
We keep 6.56e+05/7.33e+06 =  8% of the original kernel matrix.

torch.Size([6454, 2])
We keep 1.26e+06/2.60e+07 =  4% of the original kernel matrix.

torch.Size([323, 2])
We keep 7.29e+03/2.53e+04 = 28% of the original kernel matrix.

torch.Size([2502, 2])
We keep 1.64e+05/1.53e+06 = 10% of the original kernel matrix.

torch.Size([653, 2])
We keep 2.40e+04/1.20e+05 = 20% of the original kernel matrix.

torch.Size([3258, 2])
We keep 3.03e+05/3.32e+06 =  9% of the original kernel matrix.

torch.Size([1942, 2])
We keep 1.82e+05/1.45e+06 = 12% of the original kernel matrix.

torch.Size([4842, 2])
We keep 7.00e+05/1.16e+07 =  6% of the original kernel matrix.

torch.Size([937, 2])
We keep 4.58e+04/3.02e+05 = 15% of the original kernel matrix.

torch.Size([3775, 2])
We keep 4.00e+05/5.28e+06 =  7% of the original kernel matrix.

torch.Size([1227, 2])
We keep 6.67e+04/4.22e+05 = 15% of the original kernel matrix.

torch.Size([4065, 2])
We keep 4.41e+05/6.23e+06 =  7% of the original kernel matrix.

torch.Size([2665, 2])
We keep 5.90e+05/3.67e+06 = 16% of the original kernel matrix.

torch.Size([5678, 2])
We keep 1.00e+06/1.84e+07 =  5% of the original kernel matrix.

torch.Size([3154, 2])
We keep 4.16e+05/4.45e+06 =  9% of the original kernel matrix.

torch.Size([5971, 2])
We keep 1.08e+06/2.02e+07 =  5% of the original kernel matrix.

torch.Size([2245, 2])
We keep 2.04e+05/1.89e+06 = 10% of the original kernel matrix.

torch.Size([5211, 2])
We keep 7.76e+05/1.32e+07 =  5% of the original kernel matrix.

torch.Size([3222, 2])
We keep 4.69e+05/4.10e+06 = 11% of the original kernel matrix.

torch.Size([6116, 2])
We keep 1.01e+06/1.94e+07 =  5% of the original kernel matrix.

torch.Size([4820, 2])
We keep 1.19e+06/1.27e+07 =  9% of the original kernel matrix.

torch.Size([7286, 2])
We keep 1.52e+06/3.41e+07 =  4% of the original kernel matrix.

torch.Size([869, 2])
We keep 3.70e+04/2.38e+05 = 15% of the original kernel matrix.

torch.Size([3620, 2])
We keep 3.72e+05/4.68e+06 =  7% of the original kernel matrix.

torch.Size([2747, 2])
We keep 3.34e+05/3.31e+06 = 10% of the original kernel matrix.

torch.Size([5623, 2])
We keep 9.42e+05/1.74e+07 =  5% of the original kernel matrix.

torch.Size([1347, 2])
We keep 1.02e+05/6.48e+05 = 15% of the original kernel matrix.

torch.Size([4177, 2])
We keep 5.15e+05/7.72e+06 =  6% of the original kernel matrix.

torch.Size([4708, 2])
We keep 1.21e+06/1.16e+07 = 10% of the original kernel matrix.

torch.Size([7117, 2])
We keep 1.58e+06/3.27e+07 =  4% of the original kernel matrix.

torch.Size([2340, 2])
We keep 3.81e+05/2.34e+06 = 16% of the original kernel matrix.

torch.Size([5280, 2])
We keep 8.14e+05/1.47e+07 =  5% of the original kernel matrix.

torch.Size([1655, 2])
We keep 1.32e+05/1.12e+06 = 11% of the original kernel matrix.

torch.Size([4553, 2])
We keep 6.43e+05/1.01e+07 =  6% of the original kernel matrix.

torch.Size([2198, 2])
We keep 2.20e+05/1.85e+06 = 11% of the original kernel matrix.

torch.Size([5183, 2])
We keep 7.72e+05/1.31e+07 =  5% of the original kernel matrix.

torch.Size([1904, 2])
We keep 1.62e+05/1.52e+06 = 10% of the original kernel matrix.

torch.Size([4963, 2])
We keep 7.20e+05/1.18e+07 =  6% of the original kernel matrix.

torch.Size([1206, 2])
We keep 1.20e+05/5.33e+05 = 22% of the original kernel matrix.

torch.Size([4023, 2])
We keep 4.88e+05/7.00e+06 =  6% of the original kernel matrix.

torch.Size([6538, 2])
We keep 2.01e+06/2.93e+07 =  6% of the original kernel matrix.

torch.Size([8578, 2])
We keep 2.16e+06/5.20e+07 =  4% of the original kernel matrix.

torch.Size([1097, 2])
We keep 7.15e+04/5.03e+05 = 14% of the original kernel matrix.

torch.Size([3918, 2])
We keep 4.77e+05/6.80e+06 =  7% of the original kernel matrix.

torch.Size([3464, 2])
We keep 5.72e+05/5.67e+06 = 10% of the original kernel matrix.

torch.Size([6302, 2])
We keep 1.15e+06/2.28e+07 =  5% of the original kernel matrix.

torch.Size([5033, 2])
We keep 9.54e+05/1.35e+07 =  7% of the original kernel matrix.

torch.Size([7446, 2])
We keep 1.61e+06/3.53e+07 =  4% of the original kernel matrix.

torch.Size([2641, 2])
We keep 3.07e+05/2.79e+06 = 11% of the original kernel matrix.

torch.Size([5605, 2])
We keep 8.97e+05/1.60e+07 =  5% of the original kernel matrix.

torch.Size([1888, 2])
We keep 1.67e+05/1.27e+06 = 13% of the original kernel matrix.

torch.Size([4868, 2])
We keep 6.84e+05/1.08e+07 =  6% of the original kernel matrix.

torch.Size([2065, 2])
We keep 1.37e+05/1.18e+06 = 11% of the original kernel matrix.

torch.Size([5100, 2])
We keep 6.52e+05/1.04e+07 =  6% of the original kernel matrix.

torch.Size([4051, 2])
We keep 6.26e+05/8.15e+06 =  7% of the original kernel matrix.

torch.Size([6639, 2])
We keep 1.34e+06/2.74e+07 =  4% of the original kernel matrix.

torch.Size([2531, 2])
We keep 2.30e+05/2.27e+06 = 10% of the original kernel matrix.

torch.Size([5587, 2])
We keep 8.02e+05/1.45e+07 =  5% of the original kernel matrix.

torch.Size([2403, 2])
We keep 2.37e+05/2.16e+06 = 10% of the original kernel matrix.

torch.Size([5271, 2])
We keep 8.05e+05/1.41e+07 =  5% of the original kernel matrix.

torch.Size([6279, 2])
We keep 1.86e+06/2.77e+07 =  6% of the original kernel matrix.

torch.Size([8296, 2])
We keep 2.13e+06/5.05e+07 =  4% of the original kernel matrix.

torch.Size([2784, 2])
We keep 3.22e+05/3.55e+06 =  9% of the original kernel matrix.

torch.Size([5712, 2])
We keep 9.95e+05/1.81e+07 =  5% of the original kernel matrix.

torch.Size([2482, 2])
We keep 2.43e+05/2.61e+06 =  9% of the original kernel matrix.

torch.Size([5384, 2])
We keep 8.74e+05/1.55e+07 =  5% of the original kernel matrix.

torch.Size([1293, 2])
We keep 1.23e+05/8.01e+05 = 15% of the original kernel matrix.

torch.Size([4147, 2])
We keep 5.45e+05/8.58e+06 =  6% of the original kernel matrix.

torch.Size([2527, 2])
We keep 2.74e+05/2.42e+06 = 11% of the original kernel matrix.

torch.Size([5422, 2])
We keep 8.60e+05/1.49e+07 =  5% of the original kernel matrix.

torch.Size([2849, 2])
We keep 4.83e+05/3.37e+06 = 14% of the original kernel matrix.

torch.Size([5810, 2])
We keep 9.79e+05/1.76e+07 =  5% of the original kernel matrix.

torch.Size([940, 2])
We keep 3.57e+04/2.39e+05 = 14% of the original kernel matrix.

torch.Size([3759, 2])
We keep 3.73e+05/4.69e+06 =  7% of the original kernel matrix.

torch.Size([784, 2])
We keep 2.68e+04/1.48e+05 = 18% of the original kernel matrix.

torch.Size([3520, 2])
We keep 3.23e+05/3.69e+06 =  8% of the original kernel matrix.

torch.Size([2944, 2])
We keep 3.48e+05/3.26e+06 = 10% of the original kernel matrix.

torch.Size([5851, 2])
We keep 9.71e+05/1.73e+07 =  5% of the original kernel matrix.

torch.Size([7619, 2])
We keep 2.19e+06/3.90e+07 =  5% of the original kernel matrix.

torch.Size([9121, 2])
We keep 2.47e+06/5.99e+07 =  4% of the original kernel matrix.

torch.Size([1794, 2])
We keep 1.28e+05/1.12e+06 = 11% of the original kernel matrix.

torch.Size([4691, 2])
We keep 6.39e+05/1.01e+07 =  6% of the original kernel matrix.

torch.Size([1019, 2])
We keep 4.86e+04/2.91e+05 = 16% of the original kernel matrix.

torch.Size([3767, 2])
We keep 3.97e+05/5.17e+06 =  7% of the original kernel matrix.

torch.Size([2387, 2])
We keep 3.14e+05/2.54e+06 = 12% of the original kernel matrix.

torch.Size([5399, 2])
We keep 8.61e+05/1.53e+07 =  5% of the original kernel matrix.

torch.Size([1265, 2])
We keep 6.36e+04/4.83e+05 = 13% of the original kernel matrix.

torch.Size([4110, 2])
We keep 4.63e+05/6.67e+06 =  6% of the original kernel matrix.

torch.Size([1458, 2])
We keep 1.03e+05/6.40e+05 = 16% of the original kernel matrix.

torch.Size([4385, 2])
We keep 5.23e+05/7.67e+06 =  6% of the original kernel matrix.

torch.Size([1442, 2])
We keep 1.13e+05/6.34e+05 = 17% of the original kernel matrix.

torch.Size([4350, 2])
We keep 5.51e+05/7.64e+06 =  7% of the original kernel matrix.

torch.Size([1219, 2])
We keep 8.87e+04/5.10e+05 = 17% of the original kernel matrix.

torch.Size([4123, 2])
We keep 4.77e+05/6.85e+06 =  6% of the original kernel matrix.

torch.Size([763, 2])
We keep 2.45e+04/1.29e+05 = 19% of the original kernel matrix.

torch.Size([3493, 2])
We keep 2.93e+05/3.44e+06 =  8% of the original kernel matrix.

torch.Size([3349, 2])
We keep 4.69e+05/4.53e+06 = 10% of the original kernel matrix.

torch.Size([6124, 2])
We keep 1.09e+06/2.04e+07 =  5% of the original kernel matrix.

torch.Size([2431, 2])
We keep 2.94e+05/2.58e+06 = 11% of the original kernel matrix.

torch.Size([5346, 2])
We keep 8.74e+05/1.54e+07 =  5% of the original kernel matrix.

torch.Size([4450, 2])
We keep 6.29e+05/8.40e+06 =  7% of the original kernel matrix.

torch.Size([7014, 2])
We keep 1.36e+06/2.78e+07 =  4% of the original kernel matrix.

torch.Size([1325, 2])
We keep 5.24e+04/4.06e+05 = 12% of the original kernel matrix.

torch.Size([4347, 2])
We keep 4.48e+05/6.11e+06 =  7% of the original kernel matrix.

torch.Size([1048, 2])
We keep 4.72e+04/3.25e+05 = 14% of the original kernel matrix.

torch.Size([3890, 2])
We keep 3.99e+05/5.47e+06 =  7% of the original kernel matrix.

torch.Size([3472, 2])
We keep 4.51e+05/4.55e+06 =  9% of the original kernel matrix.

torch.Size([6319, 2])
We keep 1.09e+06/2.05e+07 =  5% of the original kernel matrix.

torch.Size([2697, 2])
We keep 3.54e+05/3.29e+06 = 10% of the original kernel matrix.

torch.Size([5528, 2])
We keep 9.51e+05/1.74e+07 =  5% of the original kernel matrix.

torch.Size([2911, 2])
We keep 6.16e+05/4.19e+06 = 14% of the original kernel matrix.

torch.Size([5710, 2])
We keep 1.07e+06/1.96e+07 =  5% of the original kernel matrix.

torch.Size([2403, 2])
We keep 1.92e+05/1.95e+06 =  9% of the original kernel matrix.

torch.Size([5387, 2])
We keep 7.71e+05/1.34e+07 =  5% of the original kernel matrix.

torch.Size([2270, 2])
We keep 5.86e+05/3.67e+06 = 15% of the original kernel matrix.

torch.Size([5095, 2])
We keep 9.76e+05/1.84e+07 =  5% of the original kernel matrix.

torch.Size([4388, 2])
We keep 7.70e+05/8.46e+06 =  9% of the original kernel matrix.

torch.Size([7047, 2])
We keep 1.34e+06/2.79e+07 =  4% of the original kernel matrix.

torch.Size([3222, 2])
We keep 3.55e+05/4.12e+06 =  8% of the original kernel matrix.

torch.Size([6046, 2])
We keep 1.02e+06/1.95e+07 =  5% of the original kernel matrix.

torch.Size([4924, 2])
We keep 1.04e+06/1.36e+07 =  7% of the original kernel matrix.

torch.Size([7379, 2])
We keep 1.60e+06/3.53e+07 =  4% of the original kernel matrix.

torch.Size([4171, 2])
We keep 7.26e+05/8.17e+06 =  8% of the original kernel matrix.

torch.Size([6834, 2])
We keep 1.33e+06/2.74e+07 =  4% of the original kernel matrix.

torch.Size([2226, 2])
We keep 2.05e+05/1.78e+06 = 11% of the original kernel matrix.

torch.Size([5125, 2])
We keep 7.70e+05/1.28e+07 =  6% of the original kernel matrix.

torch.Size([710, 2])
We keep 2.64e+04/1.54e+05 = 17% of the original kernel matrix.

torch.Size([3268, 2])
We keep 3.19e+05/3.76e+06 =  8% of the original kernel matrix.

torch.Size([3005, 2])
We keep 4.55e+05/4.23e+06 = 10% of the original kernel matrix.

torch.Size([5785, 2])
We keep 1.03e+06/1.97e+07 =  5% of the original kernel matrix.

torch.Size([1191, 2])
We keep 6.50e+04/4.17e+05 = 15% of the original kernel matrix.

torch.Size([3971, 2])
We keep 4.58e+05/6.20e+06 =  7% of the original kernel matrix.

torch.Size([5119, 2])
We keep 1.40e+06/1.34e+07 = 10% of the original kernel matrix.

torch.Size([7475, 2])
We keep 1.58e+06/3.52e+07 =  4% of the original kernel matrix.

torch.Size([679, 2])
We keep 2.30e+04/1.12e+05 = 20% of the original kernel matrix.

torch.Size([3366, 2])
We keep 2.90e+05/3.21e+06 =  9% of the original kernel matrix.

torch.Size([1466, 2])
We keep 8.68e+04/6.53e+05 = 13% of the original kernel matrix.

torch.Size([4514, 2])
We keep 5.25e+05/7.75e+06 =  6% of the original kernel matrix.

torch.Size([1284, 2])
We keep 7.01e+04/4.32e+05 = 16% of the original kernel matrix.

torch.Size([4285, 2])
We keep 4.49e+05/6.30e+06 =  7% of the original kernel matrix.

torch.Size([1190, 2])
We keep 4.41e+04/3.35e+05 = 13% of the original kernel matrix.

torch.Size([4191, 2])
We keep 4.21e+05/5.55e+06 =  7% of the original kernel matrix.

torch.Size([829, 2])
We keep 3.61e+04/2.41e+05 = 14% of the original kernel matrix.

torch.Size([3551, 2])
We keep 3.75e+05/4.71e+06 =  7% of the original kernel matrix.

torch.Size([1365, 2])
We keep 1.03e+05/7.19e+05 = 14% of the original kernel matrix.

torch.Size([4204, 2])
We keep 5.39e+05/8.13e+06 =  6% of the original kernel matrix.

torch.Size([2443, 2])
We keep 4.03e+05/2.79e+06 = 14% of the original kernel matrix.

torch.Size([5402, 2])
We keep 8.84e+05/1.60e+07 =  5% of the original kernel matrix.

torch.Size([2059, 2])
We keep 2.20e+05/1.55e+06 = 14% of the original kernel matrix.

torch.Size([5086, 2])
We keep 7.09e+05/1.19e+07 =  5% of the original kernel matrix.

torch.Size([1253, 2])
We keep 1.11e+05/7.73e+05 = 14% of the original kernel matrix.

torch.Size([4073, 2])
We keep 5.40e+05/8.43e+06 =  6% of the original kernel matrix.

torch.Size([2028, 2])
We keep 1.31e+05/1.25e+06 = 10% of the original kernel matrix.

torch.Size([5105, 2])
We keep 6.51e+05/1.07e+07 =  6% of the original kernel matrix.

torch.Size([1056, 2])
We keep 4.88e+04/3.55e+05 = 13% of the original kernel matrix.

torch.Size([3978, 2])
We keep 4.36e+05/5.72e+06 =  7% of the original kernel matrix.

torch.Size([3631, 2])
We keep 4.12e+05/5.03e+06 =  8% of the original kernel matrix.

torch.Size([6512, 2])
We keep 1.10e+06/2.15e+07 =  5% of the original kernel matrix.

torch.Size([717, 2])
We keep 3.22e+04/1.62e+05 = 19% of the original kernel matrix.

torch.Size([3393, 2])
We keep 3.24e+05/3.86e+06 =  8% of the original kernel matrix.

torch.Size([8873, 2])
We keep 3.27e+06/5.38e+07 =  6% of the original kernel matrix.

torch.Size([9975, 2])
We keep 2.72e+06/7.04e+07 =  3% of the original kernel matrix.

torch.Size([1727, 2])
We keep 1.23e+05/1.06e+06 = 11% of the original kernel matrix.

torch.Size([4736, 2])
We keep 6.44e+05/9.86e+06 =  6% of the original kernel matrix.

torch.Size([1372, 2])
We keep 1.15e+05/6.32e+05 = 18% of the original kernel matrix.

torch.Size([4276, 2])
We keep 5.22e+05/7.63e+06 =  6% of the original kernel matrix.

torch.Size([2699, 2])
We keep 3.00e+05/2.96e+06 = 10% of the original kernel matrix.

torch.Size([5693, 2])
We keep 9.06e+05/1.65e+07 =  5% of the original kernel matrix.

torch.Size([1477, 2])
We keep 9.75e+04/7.26e+05 = 13% of the original kernel matrix.

torch.Size([4377, 2])
We keep 5.59e+05/8.17e+06 =  6% of the original kernel matrix.

torch.Size([2492, 2])
We keep 2.60e+05/2.50e+06 = 10% of the original kernel matrix.

torch.Size([5348, 2])
We keep 8.75e+05/1.52e+07 =  5% of the original kernel matrix.

torch.Size([1109, 2])
We keep 3.80e+04/2.63e+05 = 14% of the original kernel matrix.

torch.Size([4064, 2])
We keep 3.90e+05/4.92e+06 =  7% of the original kernel matrix.

torch.Size([3327, 2])
We keep 4.25e+05/4.43e+06 =  9% of the original kernel matrix.

torch.Size([6200, 2])
We keep 1.04e+06/2.02e+07 =  5% of the original kernel matrix.

torch.Size([4458, 2])
We keep 8.81e+05/1.17e+07 =  7% of the original kernel matrix.

torch.Size([7001, 2])
We keep 1.53e+06/3.28e+07 =  4% of the original kernel matrix.

torch.Size([2194, 2])
We keep 2.61e+05/1.91e+06 = 13% of the original kernel matrix.

torch.Size([5051, 2])
We keep 8.17e+05/1.33e+07 =  6% of the original kernel matrix.

torch.Size([5174, 2])
We keep 1.12e+06/1.23e+07 =  9% of the original kernel matrix.

torch.Size([7591, 2])
We keep 1.54e+06/3.36e+07 =  4% of the original kernel matrix.

torch.Size([1379, 2])
We keep 1.08e+05/6.72e+05 = 16% of the original kernel matrix.

torch.Size([4268, 2])
We keep 5.37e+05/7.87e+06 =  6% of the original kernel matrix.

torch.Size([3682, 2])
We keep 5.96e+05/7.27e+06 =  8% of the original kernel matrix.

torch.Size([6416, 2])
We keep 1.25e+06/2.59e+07 =  4% of the original kernel matrix.

torch.Size([611, 2])
We keep 3.16e+04/1.39e+05 = 22% of the original kernel matrix.

torch.Size([3041, 2])
We keep 2.95e+05/3.58e+06 =  8% of the original kernel matrix.

torch.Size([2115, 2])
We keep 1.65e+05/1.52e+06 = 10% of the original kernel matrix.

torch.Size([4985, 2])
We keep 7.26e+05/1.18e+07 =  6% of the original kernel matrix.

torch.Size([1645, 2])
We keep 1.34e+05/1.04e+06 = 12% of the original kernel matrix.

torch.Size([4610, 2])
We keep 6.42e+05/9.78e+06 =  6% of the original kernel matrix.

torch.Size([1464, 2])
We keep 8.69e+04/6.34e+05 = 13% of the original kernel matrix.

torch.Size([4454, 2])
We keep 5.25e+05/7.64e+06 =  6% of the original kernel matrix.

torch.Size([1161, 2])
We keep 5.87e+04/3.94e+05 = 14% of the original kernel matrix.

torch.Size([3926, 2])
We keep 4.42e+05/6.02e+06 =  7% of the original kernel matrix.

torch.Size([1208, 2])
We keep 6.44e+04/4.24e+05 = 15% of the original kernel matrix.

torch.Size([4131, 2])
We keep 4.57e+05/6.24e+06 =  7% of the original kernel matrix.

torch.Size([1863, 2])
We keep 1.73e+05/1.35e+06 = 12% of the original kernel matrix.

torch.Size([4771, 2])
We keep 6.99e+05/1.11e+07 =  6% of the original kernel matrix.

torch.Size([1823, 2])
We keep 1.74e+05/1.45e+06 = 11% of the original kernel matrix.

torch.Size([4698, 2])
We keep 7.03e+05/1.16e+07 =  6% of the original kernel matrix.

torch.Size([752, 2])
We keep 2.86e+04/1.54e+05 = 18% of the original kernel matrix.

torch.Size([3501, 2])
We keep 3.27e+05/3.76e+06 =  8% of the original kernel matrix.

torch.Size([3580, 2])
We keep 8.56e+05/7.38e+06 = 11% of the original kernel matrix.

torch.Size([6264, 2])
We keep 1.31e+06/2.61e+07 =  5% of the original kernel matrix.

torch.Size([8576, 2])
We keep 3.81e+06/5.68e+07 =  6% of the original kernel matrix.

torch.Size([9844, 2])
We keep 2.88e+06/7.23e+07 =  3% of the original kernel matrix.

torch.Size([2720, 2])
We keep 3.20e+05/2.83e+06 = 11% of the original kernel matrix.

torch.Size([5738, 2])
We keep 8.86e+05/1.61e+07 =  5% of the original kernel matrix.

torch.Size([1956, 2])
We keep 1.26e+05/1.21e+06 = 10% of the original kernel matrix.

torch.Size([4996, 2])
We keep 6.66e+05/1.05e+07 =  6% of the original kernel matrix.

torch.Size([1718, 2])
We keep 1.24e+05/1.04e+06 = 11% of the original kernel matrix.

torch.Size([4742, 2])
We keep 6.11e+05/9.80e+06 =  6% of the original kernel matrix.

torch.Size([1028, 2])
We keep 6.22e+04/3.10e+05 = 20% of the original kernel matrix.

torch.Size([3847, 2])
We keep 4.02e+05/5.34e+06 =  7% of the original kernel matrix.

torch.Size([1508, 2])
We keep 8.66e+04/6.48e+05 = 13% of the original kernel matrix.

torch.Size([4505, 2])
We keep 5.24e+05/7.72e+06 =  6% of the original kernel matrix.

torch.Size([1856, 2])
We keep 1.58e+05/1.19e+06 = 13% of the original kernel matrix.

torch.Size([4889, 2])
We keep 6.59e+05/1.05e+07 =  6% of the original kernel matrix.

torch.Size([1404, 2])
We keep 9.55e+04/6.74e+05 = 14% of the original kernel matrix.

torch.Size([4196, 2])
We keep 5.46e+05/7.88e+06 =  6% of the original kernel matrix.

torch.Size([3154, 2])
We keep 5.50e+05/5.04e+06 = 10% of the original kernel matrix.

torch.Size([6029, 2])
We keep 1.15e+06/2.15e+07 =  5% of the original kernel matrix.

torch.Size([1685, 2])
We keep 9.57e+04/8.12e+05 = 11% of the original kernel matrix.

torch.Size([4654, 2])
We keep 5.55e+05/8.64e+06 =  6% of the original kernel matrix.

torch.Size([4105, 2])
We keep 1.63e+06/9.88e+06 = 16% of the original kernel matrix.

torch.Size([6821, 2])
We keep 1.41e+06/3.01e+07 =  4% of the original kernel matrix.

torch.Size([2105, 2])
We keep 2.10e+05/1.87e+06 = 11% of the original kernel matrix.

torch.Size([5047, 2])
We keep 7.59e+05/1.31e+07 =  5% of the original kernel matrix.

torch.Size([5398, 2])
We keep 1.97e+06/1.89e+07 = 10% of the original kernel matrix.

torch.Size([7714, 2])
We keep 1.84e+06/4.17e+07 =  4% of the original kernel matrix.

torch.Size([2268, 2])
We keep 1.73e+05/1.72e+06 = 10% of the original kernel matrix.

torch.Size([5293, 2])
We keep 7.45e+05/1.26e+07 =  5% of the original kernel matrix.

torch.Size([1459, 2])
We keep 8.23e+04/6.27e+05 = 13% of the original kernel matrix.

torch.Size([4422, 2])
We keep 5.19e+05/7.60e+06 =  6% of the original kernel matrix.

torch.Size([2249, 2])
We keep 2.28e+05/2.01e+06 = 11% of the original kernel matrix.

torch.Size([5265, 2])
We keep 7.80e+05/1.36e+07 =  5% of the original kernel matrix.

torch.Size([537, 2])
We keep 1.33e+04/7.02e+04 = 18% of the original kernel matrix.

torch.Size([3091, 2])
We keep 2.50e+05/2.54e+06 =  9% of the original kernel matrix.

torch.Size([3268, 2])
We keep 5.33e+05/4.64e+06 = 11% of the original kernel matrix.

torch.Size([6208, 2])
We keep 1.10e+06/2.07e+07 =  5% of the original kernel matrix.

torch.Size([1419, 2])
We keep 9.84e+04/7.02e+05 = 14% of the original kernel matrix.

torch.Size([4281, 2])
We keep 5.63e+05/8.04e+06 =  7% of the original kernel matrix.

torch.Size([2460, 2])
We keep 3.26e+05/2.50e+06 = 13% of the original kernel matrix.

torch.Size([5348, 2])
We keep 8.52e+05/1.52e+07 =  5% of the original kernel matrix.

torch.Size([713, 2])
We keep 3.33e+04/1.55e+05 = 21% of the original kernel matrix.

torch.Size([3274, 2])
We keep 3.19e+05/3.78e+06 =  8% of the original kernel matrix.

torch.Size([2109, 2])
We keep 1.85e+05/1.49e+06 = 12% of the original kernel matrix.

torch.Size([5017, 2])
We keep 6.96e+05/1.17e+07 =  5% of the original kernel matrix.

torch.Size([1971, 2])
We keep 1.64e+05/1.36e+06 = 12% of the original kernel matrix.

torch.Size([5001, 2])
We keep 7.02e+05/1.12e+07 =  6% of the original kernel matrix.

torch.Size([1954, 2])
We keep 1.59e+05/1.32e+06 = 12% of the original kernel matrix.

torch.Size([4983, 2])
We keep 6.60e+05/1.10e+07 =  5% of the original kernel matrix.

torch.Size([1932, 2])
We keep 1.40e+05/1.29e+06 = 10% of the original kernel matrix.

torch.Size([4830, 2])
We keep 6.66e+05/1.09e+07 =  6% of the original kernel matrix.

torch.Size([2188, 2])
We keep 1.69e+05/1.71e+06 =  9% of the original kernel matrix.

torch.Size([5132, 2])
We keep 7.40e+05/1.25e+07 =  5% of the original kernel matrix.

torch.Size([695, 2])
We keep 2.13e+04/1.21e+05 = 17% of the original kernel matrix.

torch.Size([3360, 2])
We keep 2.76e+05/3.34e+06 =  8% of the original kernel matrix.

torch.Size([681, 2])
We keep 2.84e+04/1.70e+05 = 16% of the original kernel matrix.

torch.Size([3243, 2])
We keep 3.27e+05/3.95e+06 =  8% of the original kernel matrix.

torch.Size([1268, 2])
We keep 5.90e+04/4.08e+05 = 14% of the original kernel matrix.

torch.Size([4158, 2])
We keep 4.45e+05/6.13e+06 =  7% of the original kernel matrix.

torch.Size([4357, 2])
We keep 6.99e+05/8.23e+06 =  8% of the original kernel matrix.

torch.Size([6947, 2])
We keep 1.33e+06/2.75e+07 =  4% of the original kernel matrix.

torch.Size([2278, 2])
We keep 1.83e+05/1.68e+06 = 10% of the original kernel matrix.

torch.Size([5293, 2])
We keep 7.46e+05/1.25e+07 =  5% of the original kernel matrix.

torch.Size([2852, 2])
We keep 3.78e+05/3.43e+06 = 11% of the original kernel matrix.

torch.Size([5806, 2])
We keep 9.56e+05/1.78e+07 =  5% of the original kernel matrix.

torch.Size([2108, 2])
We keep 2.69e+05/2.19e+06 = 12% of the original kernel matrix.

torch.Size([5129, 2])
We keep 7.82e+05/1.42e+07 =  5% of the original kernel matrix.

torch.Size([2554, 2])
We keep 2.42e+05/2.26e+06 = 10% of the original kernel matrix.

torch.Size([5520, 2])
We keep 8.19e+05/1.44e+07 =  5% of the original kernel matrix.

torch.Size([3015, 2])
We keep 4.58e+05/3.92e+06 = 11% of the original kernel matrix.

torch.Size([5858, 2])
We keep 1.05e+06/1.90e+07 =  5% of the original kernel matrix.

torch.Size([3098, 2])
We keep 5.72e+05/4.41e+06 = 12% of the original kernel matrix.

torch.Size([6030, 2])
We keep 1.05e+06/2.01e+07 =  5% of the original kernel matrix.

torch.Size([1196, 2])
We keep 8.00e+04/5.23e+05 = 15% of the original kernel matrix.

torch.Size([4067, 2])
We keep 4.85e+05/6.94e+06 =  7% of the original kernel matrix.

torch.Size([4053, 2])
We keep 5.52e+05/7.16e+06 =  7% of the original kernel matrix.

torch.Size([6769, 2])
We keep 1.24e+06/2.57e+07 =  4% of the original kernel matrix.

torch.Size([1440, 2])
We keep 9.64e+04/7.76e+05 = 12% of the original kernel matrix.

torch.Size([4301, 2])
We keep 5.61e+05/8.45e+06 =  6% of the original kernel matrix.

torch.Size([2704, 2])
We keep 3.63e+05/2.82e+06 = 12% of the original kernel matrix.

torch.Size([5671, 2])
We keep 8.99e+05/1.61e+07 =  5% of the original kernel matrix.

torch.Size([2008, 2])
We keep 2.90e+05/1.62e+06 = 17% of the original kernel matrix.

torch.Size([4971, 2])
We keep 7.25e+05/1.22e+07 =  5% of the original kernel matrix.

torch.Size([4248, 2])
We keep 7.31e+05/8.72e+06 =  8% of the original kernel matrix.

torch.Size([6803, 2])
We keep 1.35e+06/2.83e+07 =  4% of the original kernel matrix.

torch.Size([3138, 2])
We keep 4.20e+05/4.44e+06 =  9% of the original kernel matrix.

torch.Size([5932, 2])
We keep 1.06e+06/2.02e+07 =  5% of the original kernel matrix.

torch.Size([1017, 2])
We keep 5.41e+04/3.92e+05 = 13% of the original kernel matrix.

torch.Size([3793, 2])
We keep 4.46e+05/6.00e+06 =  7% of the original kernel matrix.

torch.Size([1857, 2])
We keep 1.78e+05/1.39e+06 = 12% of the original kernel matrix.

torch.Size([4782, 2])
We keep 6.87e+05/1.13e+07 =  6% of the original kernel matrix.

torch.Size([3015, 2])
We keep 3.75e+05/3.84e+06 =  9% of the original kernel matrix.

torch.Size([5882, 2])
We keep 9.99e+05/1.88e+07 =  5% of the original kernel matrix.

torch.Size([5750, 2])
We keep 3.10e+06/2.27e+07 = 13% of the original kernel matrix.

torch.Size([7914, 2])
We keep 1.95e+06/4.57e+07 =  4% of the original kernel matrix.

torch.Size([1407, 2])
We keep 8.81e+04/7.34e+05 = 11% of the original kernel matrix.

torch.Size([4298, 2])
We keep 5.36e+05/8.22e+06 =  6% of the original kernel matrix.

torch.Size([773, 2])
We keep 3.41e+04/2.16e+05 = 15% of the original kernel matrix.

torch.Size([3356, 2])
We keep 3.57e+05/4.46e+06 =  8% of the original kernel matrix.

torch.Size([2225, 2])
We keep 1.78e+05/1.62e+06 = 11% of the original kernel matrix.

torch.Size([5294, 2])
We keep 7.27e+05/1.22e+07 =  5% of the original kernel matrix.

torch.Size([1943, 2])
We keep 1.50e+05/1.30e+06 = 11% of the original kernel matrix.

torch.Size([4904, 2])
We keep 6.61e+05/1.09e+07 =  6% of the original kernel matrix.

torch.Size([1970, 2])
We keep 1.70e+05/1.21e+06 = 14% of the original kernel matrix.

torch.Size([4978, 2])
We keep 6.67e+05/1.05e+07 =  6% of the original kernel matrix.

torch.Size([1224, 2])
We keep 5.97e+04/4.68e+05 = 12% of the original kernel matrix.

torch.Size([4115, 2])
We keep 4.60e+05/6.56e+06 =  7% of the original kernel matrix.

torch.Size([1502, 2])
We keep 8.36e+04/7.07e+05 = 11% of the original kernel matrix.

torch.Size([4480, 2])
We keep 5.40e+05/8.07e+06 =  6% of the original kernel matrix.

torch.Size([1943, 2])
We keep 2.26e+05/1.74e+06 = 12% of the original kernel matrix.

torch.Size([4925, 2])
We keep 7.45e+05/1.27e+07 =  5% of the original kernel matrix.

torch.Size([2889, 2])
We keep 2.81e+05/3.04e+06 =  9% of the original kernel matrix.

torch.Size([5726, 2])
We keep 9.20e+05/1.67e+07 =  5% of the original kernel matrix.

torch.Size([1239, 2])
We keep 4.85e+04/3.81e+05 = 12% of the original kernel matrix.

torch.Size([4188, 2])
We keep 4.24e+05/5.92e+06 =  7% of the original kernel matrix.

torch.Size([6804, 2])
We keep 1.51e+06/2.69e+07 =  5% of the original kernel matrix.

torch.Size([8698, 2])
We keep 2.10e+06/4.98e+07 =  4% of the original kernel matrix.

torch.Size([2537, 2])
We keep 2.45e+05/2.42e+06 = 10% of the original kernel matrix.

torch.Size([5445, 2])
We keep 8.53e+05/1.49e+07 =  5% of the original kernel matrix.

torch.Size([9732, 2])
We keep 3.15e+06/7.29e+07 =  4% of the original kernel matrix.

torch.Size([10779, 2])
We keep 3.15e+06/8.19e+07 =  3% of the original kernel matrix.

torch.Size([2790, 2])
We keep 4.43e+05/3.39e+06 = 13% of the original kernel matrix.

torch.Size([5647, 2])
We keep 9.89e+05/1.77e+07 =  5% of the original kernel matrix.

torch.Size([2991, 2])
We keep 4.18e+05/4.03e+06 = 10% of the original kernel matrix.

torch.Size([5804, 2])
We keep 1.04e+06/1.93e+07 =  5% of the original kernel matrix.

torch.Size([931, 2])
We keep 3.46e+04/2.33e+05 = 14% of the original kernel matrix.

torch.Size([3722, 2])
We keep 3.72e+05/4.63e+06 =  8% of the original kernel matrix.

torch.Size([1718, 2])
We keep 1.38e+05/1.08e+06 = 12% of the original kernel matrix.

torch.Size([4725, 2])
We keep 6.37e+05/9.99e+06 =  6% of the original kernel matrix.

torch.Size([1793, 2])
We keep 1.97e+05/1.53e+06 = 12% of the original kernel matrix.

torch.Size([4721, 2])
We keep 6.95e+05/1.18e+07 =  5% of the original kernel matrix.

torch.Size([1451, 2])
We keep 1.16e+05/6.96e+05 = 16% of the original kernel matrix.

torch.Size([4402, 2])
We keep 5.30e+05/8.00e+06 =  6% of the original kernel matrix.

torch.Size([4077, 2])
We keep 8.53e+05/1.01e+07 =  8% of the original kernel matrix.

torch.Size([6700, 2])
We keep 1.48e+06/3.04e+07 =  4% of the original kernel matrix.

torch.Size([3035, 2])
We keep 4.19e+05/3.54e+06 = 11% of the original kernel matrix.

torch.Size([5934, 2])
We keep 9.78e+05/1.80e+07 =  5% of the original kernel matrix.

torch.Size([1902, 2])
We keep 1.66e+05/1.33e+06 = 12% of the original kernel matrix.

torch.Size([4836, 2])
We keep 7.02e+05/1.11e+07 =  6% of the original kernel matrix.

torch.Size([1442, 2])
We keep 8.34e+04/6.69e+05 = 12% of the original kernel matrix.

torch.Size([4339, 2])
We keep 5.43e+05/7.85e+06 =  6% of the original kernel matrix.

torch.Size([3075, 2])
We keep 3.46e+05/3.75e+06 =  9% of the original kernel matrix.

torch.Size([6006, 2])
We keep 9.83e+05/1.86e+07 =  5% of the original kernel matrix.

torch.Size([716, 2])
We keep 2.86e+04/1.30e+05 = 21% of the original kernel matrix.

torch.Size([3404, 2])
We keep 2.95e+05/3.46e+06 =  8% of the original kernel matrix.

torch.Size([886, 2])
We keep 2.81e+04/1.73e+05 = 16% of the original kernel matrix.

torch.Size([3619, 2])
We keep 3.33e+05/3.99e+06 =  8% of the original kernel matrix.

torch.Size([773, 2])
We keep 3.74e+04/2.22e+05 = 16% of the original kernel matrix.

torch.Size([3423, 2])
We keep 3.58e+05/4.52e+06 =  7% of the original kernel matrix.

torch.Size([985, 2])
We keep 3.60e+04/2.55e+05 = 14% of the original kernel matrix.

torch.Size([3835, 2])
We keep 3.55e+05/4.84e+06 =  7% of the original kernel matrix.

torch.Size([1830, 2])
We keep 1.83e+05/1.29e+06 = 14% of the original kernel matrix.

torch.Size([4776, 2])
We keep 6.52e+05/1.09e+07 =  5% of the original kernel matrix.

torch.Size([1325, 2])
We keep 5.80e+04/4.52e+05 = 12% of the original kernel matrix.

torch.Size([4236, 2])
We keep 4.55e+05/6.45e+06 =  7% of the original kernel matrix.

torch.Size([1962, 2])
We keep 2.71e+05/1.91e+06 = 14% of the original kernel matrix.

torch.Size([4894, 2])
We keep 7.40e+05/1.32e+07 =  5% of the original kernel matrix.

torch.Size([1146, 2])
We keep 8.13e+04/4.79e+05 = 16% of the original kernel matrix.

torch.Size([3987, 2])
We keep 4.52e+05/6.64e+06 =  6% of the original kernel matrix.

torch.Size([2712, 2])
We keep 3.75e+05/3.71e+06 = 10% of the original kernel matrix.

torch.Size([5512, 2])
We keep 9.83e+05/1.85e+07 =  5% of the original kernel matrix.

torch.Size([1491, 2])
We keep 8.86e+04/6.37e+05 = 13% of the original kernel matrix.

torch.Size([4424, 2])
We keep 5.17e+05/7.65e+06 =  6% of the original kernel matrix.

torch.Size([1420, 2])
We keep 1.13e+05/6.05e+05 = 18% of the original kernel matrix.

torch.Size([4337, 2])
We keep 5.15e+05/7.46e+06 =  6% of the original kernel matrix.

torch.Size([1439, 2])
We keep 1.14e+05/8.85e+05 = 12% of the original kernel matrix.

torch.Size([4325, 2])
We keep 6.09e+05/9.03e+06 =  6% of the original kernel matrix.

torch.Size([1427, 2])
We keep 8.17e+04/5.60e+05 = 14% of the original kernel matrix.

torch.Size([4395, 2])
We keep 5.14e+05/7.17e+06 =  7% of the original kernel matrix.

torch.Size([1009, 2])
We keep 4.14e+04/2.78e+05 = 14% of the original kernel matrix.

torch.Size([3868, 2])
We keep 3.83e+05/5.05e+06 =  7% of the original kernel matrix.

torch.Size([2321, 2])
We keep 2.44e+05/2.31e+06 = 10% of the original kernel matrix.

torch.Size([5204, 2])
We keep 8.23e+05/1.46e+07 =  5% of the original kernel matrix.

torch.Size([916, 2])
We keep 3.50e+04/2.52e+05 = 13% of the original kernel matrix.

torch.Size([3620, 2])
We keep 3.70e+05/4.82e+06 =  7% of the original kernel matrix.

torch.Size([908, 2])
We keep 3.17e+04/2.02e+05 = 15% of the original kernel matrix.

torch.Size([3700, 2])
We keep 3.46e+05/4.31e+06 =  8% of the original kernel matrix.

torch.Size([1289, 2])
We keep 7.55e+04/5.61e+05 = 13% of the original kernel matrix.

torch.Size([4172, 2])
We keep 5.08e+05/7.18e+06 =  7% of the original kernel matrix.

torch.Size([1508, 2])
We keep 8.55e+04/6.50e+05 = 13% of the original kernel matrix.

torch.Size([4465, 2])
We keep 5.34e+05/7.73e+06 =  6% of the original kernel matrix.

torch.Size([2858, 2])
We keep 3.39e+05/3.51e+06 =  9% of the original kernel matrix.

torch.Size([5776, 2])
We keep 9.62e+05/1.80e+07 =  5% of the original kernel matrix.

torch.Size([1348, 2])
We keep 5.80e+04/4.16e+05 = 13% of the original kernel matrix.

torch.Size([4347, 2])
We keep 4.41e+05/6.19e+06 =  7% of the original kernel matrix.

torch.Size([1336, 2])
We keep 6.92e+04/4.37e+05 = 15% of the original kernel matrix.

torch.Size([4215, 2])
We keep 4.50e+05/6.34e+06 =  7% of the original kernel matrix.

torch.Size([4138, 2])
We keep 9.43e+05/8.97e+06 = 10% of the original kernel matrix.

torch.Size([6864, 2])
We keep 1.36e+06/2.87e+07 =  4% of the original kernel matrix.

torch.Size([4293, 2])
We keep 8.08e+05/8.87e+06 =  9% of the original kernel matrix.

torch.Size([7027, 2])
We keep 1.37e+06/2.86e+07 =  4% of the original kernel matrix.

torch.Size([2435, 2])
We keep 3.58e+05/2.12e+06 = 16% of the original kernel matrix.

torch.Size([5459, 2])
We keep 8.00e+05/1.40e+07 =  5% of the original kernel matrix.

torch.Size([4564, 2])
We keep 7.79e+05/1.01e+07 =  7% of the original kernel matrix.

torch.Size([7243, 2])
We keep 1.43e+06/3.04e+07 =  4% of the original kernel matrix.

torch.Size([5789, 2])
We keep 5.08e+06/3.37e+07 = 15% of the original kernel matrix.

torch.Size([8087, 2])
We keep 2.30e+06/5.57e+07 =  4% of the original kernel matrix.

torch.Size([8474, 2])
We keep 4.15e+06/5.73e+07 =  7% of the original kernel matrix.

torch.Size([9765, 2])
We keep 2.65e+06/7.26e+07 =  3% of the original kernel matrix.

torch.Size([3521, 2])
We keep 4.53e+05/5.23e+06 =  8% of the original kernel matrix.

torch.Size([6406, 2])
We keep 1.11e+06/2.19e+07 =  5% of the original kernel matrix.

torch.Size([598, 2])
We keep 1.70e+04/8.82e+04 = 19% of the original kernel matrix.

torch.Size([3215, 2])
We keep 2.62e+05/2.85e+06 =  9% of the original kernel matrix.

torch.Size([3455, 2])
We keep 5.26e+05/5.04e+06 = 10% of the original kernel matrix.

torch.Size([6238, 2])
We keep 1.13e+06/2.15e+07 =  5% of the original kernel matrix.

torch.Size([51936, 2])
We keep 1.02e+09/7.47e+09 = 13% of the original kernel matrix.

torch.Size([23668, 2])
We keep 2.23e+07/8.29e+08 =  2% of the original kernel matrix.

torch.Size([36182, 2])
We keep 4.08e+08/2.86e+09 = 14% of the original kernel matrix.

torch.Size([20106, 2])
We keep 1.42e+07/5.13e+08 =  2% of the original kernel matrix.

torch.Size([2558, 2])
We keep 2.22e+05/2.39e+06 =  9% of the original kernel matrix.

torch.Size([5533, 2])
We keep 8.26e+05/1.48e+07 =  5% of the original kernel matrix.

torch.Size([14250, 2])
We keep 2.80e+07/2.68e+08 = 10% of the original kernel matrix.

torch.Size([12688, 2])
We keep 5.44e+06/1.57e+08 =  3% of the original kernel matrix.

torch.Size([20995, 2])
We keep 1.46e+08/6.87e+08 = 21% of the original kernel matrix.

torch.Size([14474, 2])
We keep 8.45e+06/2.51e+08 =  3% of the original kernel matrix.

torch.Size([8316, 2])
We keep 1.07e+07/7.46e+07 = 14% of the original kernel matrix.

torch.Size([9696, 2])
We keep 3.21e+06/8.29e+07 =  3% of the original kernel matrix.

torch.Size([70542, 2])
We keep 3.84e+08/7.11e+09 =  5% of the original kernel matrix.

torch.Size([28557, 2])
We keep 2.21e+07/8.09e+08 =  2% of the original kernel matrix.

torch.Size([1791, 2])
We keep 1.19e+05/1.06e+06 = 11% of the original kernel matrix.

torch.Size([4797, 2])
We keep 6.12e+05/9.88e+06 =  6% of the original kernel matrix.

torch.Size([4952, 2])
We keep 9.96e+05/1.23e+07 =  8% of the original kernel matrix.

torch.Size([7438, 2])
We keep 1.57e+06/3.36e+07 =  4% of the original kernel matrix.

torch.Size([58360, 2])
We keep 1.62e+08/4.02e+09 =  4% of the original kernel matrix.

torch.Size([25240, 2])
We keep 1.65e+07/6.08e+08 =  2% of the original kernel matrix.

torch.Size([272786, 2])
We keep 1.17e+09/6.39e+10 =  1% of the original kernel matrix.

torch.Size([58214, 2])
We keep 5.82e+07/2.42e+09 =  2% of the original kernel matrix.

torch.Size([4969, 2])
We keep 1.57e+06/1.58e+07 =  9% of the original kernel matrix.

torch.Size([7474, 2])
We keep 1.68e+06/3.82e+07 =  4% of the original kernel matrix.

torch.Size([13005, 2])
We keep 7.92e+06/1.47e+08 =  5% of the original kernel matrix.

torch.Size([12188, 2])
We keep 4.19e+06/1.16e+08 =  3% of the original kernel matrix.

torch.Size([23581, 2])
We keep 1.26e+09/1.99e+09 = 63% of the original kernel matrix.

torch.Size([15596, 2])
We keep 1.03e+07/4.28e+08 =  2% of the original kernel matrix.

torch.Size([80026, 2])
We keep 1.36e+09/9.14e+09 = 14% of the original kernel matrix.

torch.Size([30360, 2])
We keep 2.44e+07/9.17e+08 =  2% of the original kernel matrix.

torch.Size([8303, 2])
We keep 4.64e+06/5.63e+07 =  8% of the original kernel matrix.

torch.Size([9603, 2])
We keep 2.89e+06/7.20e+07 =  4% of the original kernel matrix.

torch.Size([12277, 2])
We keep 1.51e+07/2.17e+08 =  6% of the original kernel matrix.

torch.Size([11076, 2])
We keep 4.95e+06/1.41e+08 =  3% of the original kernel matrix.

torch.Size([23264, 2])
We keep 2.97e+07/5.65e+08 =  5% of the original kernel matrix.

torch.Size([16160, 2])
We keep 7.22e+06/2.28e+08 =  3% of the original kernel matrix.

torch.Size([6776, 2])
We keep 2.06e+06/3.05e+07 =  6% of the original kernel matrix.

torch.Size([8706, 2])
We keep 2.16e+06/5.30e+07 =  4% of the original kernel matrix.

torch.Size([3465, 2])
We keep 1.77e+06/7.29e+06 = 24% of the original kernel matrix.

torch.Size([6143, 2])
We keep 1.36e+06/2.59e+07 =  5% of the original kernel matrix.

torch.Size([36684, 2])
We keep 7.39e+08/4.72e+09 = 15% of the original kernel matrix.

torch.Size([20475, 2])
We keep 1.79e+07/6.59e+08 =  2% of the original kernel matrix.

torch.Size([3697, 2])
We keep 5.20e+05/6.30e+06 =  8% of the original kernel matrix.

torch.Size([6472, 2])
We keep 1.19e+06/2.41e+07 =  4% of the original kernel matrix.

torch.Size([2279, 2])
We keep 2.51e+05/2.01e+06 = 12% of the original kernel matrix.

torch.Size([5218, 2])
We keep 8.12e+05/1.36e+07 =  5% of the original kernel matrix.

torch.Size([12275, 2])
We keep 6.33e+06/1.21e+08 =  5% of the original kernel matrix.

torch.Size([11897, 2])
We keep 3.80e+06/1.05e+08 =  3% of the original kernel matrix.

torch.Size([5043, 2])
We keep 2.06e+06/2.46e+07 =  8% of the original kernel matrix.

torch.Size([7279, 2])
We keep 1.99e+06/4.76e+07 =  4% of the original kernel matrix.

torch.Size([91404, 2])
We keep 4.13e+08/1.07e+10 =  3% of the original kernel matrix.

torch.Size([33258, 2])
We keep 2.63e+07/9.91e+08 =  2% of the original kernel matrix.

torch.Size([60206, 2])
We keep 8.29e+07/3.35e+09 =  2% of the original kernel matrix.

torch.Size([26350, 2])
We keep 1.56e+07/5.55e+08 =  2% of the original kernel matrix.

torch.Size([1122, 2])
We keep 1.01e+05/5.61e+05 = 17% of the original kernel matrix.

torch.Size([3874, 2])
We keep 5.07e+05/7.18e+06 =  7% of the original kernel matrix.

torch.Size([10549, 2])
We keep 8.66e+06/9.10e+07 =  9% of the original kernel matrix.

torch.Size([10995, 2])
We keep 3.35e+06/9.15e+07 =  3% of the original kernel matrix.

torch.Size([2944, 2])
We keep 7.68e+05/4.29e+06 = 17% of the original kernel matrix.

torch.Size([5847, 2])
We keep 1.05e+06/1.99e+07 =  5% of the original kernel matrix.

torch.Size([9115, 2])
We keep 4.77e+06/6.16e+07 =  7% of the original kernel matrix.

torch.Size([9901, 2])
We keep 2.80e+06/7.53e+07 =  3% of the original kernel matrix.

torch.Size([38476, 2])
We keep 4.48e+07/1.34e+09 =  3% of the original kernel matrix.

torch.Size([20914, 2])
We keep 1.02e+07/3.51e+08 =  2% of the original kernel matrix.

torch.Size([9495, 2])
We keep 6.42e+07/1.14e+08 = 56% of the original kernel matrix.

torch.Size([10103, 2])
We keep 3.12e+06/1.03e+08 =  3% of the original kernel matrix.

torch.Size([32791, 2])
We keep 3.23e+07/9.43e+08 =  3% of the original kernel matrix.

torch.Size([18817, 2])
We keep 9.09e+06/2.95e+08 =  3% of the original kernel matrix.

torch.Size([647, 2])
We keep 1.71e+04/1.04e+05 = 16% of the original kernel matrix.

torch.Size([3284, 2])
We keep 2.69e+05/3.10e+06 =  8% of the original kernel matrix.

torch.Size([2858, 2])
We keep 3.10e+05/2.90e+06 = 10% of the original kernel matrix.

torch.Size([5817, 2])
We keep 8.86e+05/1.63e+07 =  5% of the original kernel matrix.

torch.Size([9392, 2])
We keep 1.93e+07/1.21e+08 = 15% of the original kernel matrix.

torch.Size([10285, 2])
We keep 3.71e+06/1.06e+08 =  3% of the original kernel matrix.

torch.Size([19299, 2])
We keep 3.25e+07/4.12e+08 =  7% of the original kernel matrix.

torch.Size([14428, 2])
We keep 6.20e+06/1.95e+08 =  3% of the original kernel matrix.

torch.Size([7127, 2])
We keep 4.18e+06/5.32e+07 =  7% of the original kernel matrix.

torch.Size([8878, 2])
We keep 2.80e+06/7.00e+07 =  4% of the original kernel matrix.

torch.Size([13239, 2])
We keep 3.43e+07/3.16e+08 = 10% of the original kernel matrix.

torch.Size([11482, 2])
We keep 6.08e+06/1.70e+08 =  3% of the original kernel matrix.

torch.Size([98820, 2])
We keep 2.33e+08/1.14e+10 =  2% of the original kernel matrix.

torch.Size([34385, 2])
We keep 2.66e+07/1.02e+09 =  2% of the original kernel matrix.

torch.Size([6864, 2])
We keep 1.80e+06/3.12e+07 =  5% of the original kernel matrix.

torch.Size([8846, 2])
We keep 2.18e+06/5.36e+07 =  4% of the original kernel matrix.

torch.Size([10105, 2])
We keep 3.63e+06/7.84e+07 =  4% of the original kernel matrix.

torch.Size([10721, 2])
We keep 3.22e+06/8.49e+07 =  3% of the original kernel matrix.

torch.Size([26988, 2])
We keep 2.78e+07/7.34e+08 =  3% of the original kernel matrix.

torch.Size([17455, 2])
We keep 7.97e+06/2.60e+08 =  3% of the original kernel matrix.

torch.Size([2554, 2])
We keep 6.47e+05/2.88e+06 = 22% of the original kernel matrix.

torch.Size([5478, 2])
We keep 8.90e+05/1.63e+07 =  5% of the original kernel matrix.

torch.Size([35083, 2])
We keep 4.54e+07/1.12e+09 =  4% of the original kernel matrix.

torch.Size([19476, 2])
We keep 9.78e+06/3.21e+08 =  3% of the original kernel matrix.

torch.Size([24271, 2])
We keep 1.23e+08/9.84e+08 = 12% of the original kernel matrix.

torch.Size([16418, 2])
We keep 9.10e+06/3.01e+08 =  3% of the original kernel matrix.

torch.Size([58151, 2])
We keep 2.81e+08/4.29e+09 =  6% of the original kernel matrix.

torch.Size([25667, 2])
We keep 1.73e+07/6.28e+08 =  2% of the original kernel matrix.

torch.Size([8831, 2])
We keep 4.63e+06/5.65e+07 =  8% of the original kernel matrix.

torch.Size([9998, 2])
We keep 2.76e+06/7.21e+07 =  3% of the original kernel matrix.

torch.Size([7639, 2])
We keep 3.48e+06/5.13e+07 =  6% of the original kernel matrix.

torch.Size([9236, 2])
We keep 2.72e+06/6.87e+07 =  3% of the original kernel matrix.

torch.Size([5502, 2])
We keep 1.98e+06/2.73e+07 =  7% of the original kernel matrix.

torch.Size([7429, 2])
We keep 2.10e+06/5.01e+07 =  4% of the original kernel matrix.

torch.Size([4357, 2])
We keep 5.88e+05/8.11e+06 =  7% of the original kernel matrix.

torch.Size([6976, 2])
We keep 1.32e+06/2.73e+07 =  4% of the original kernel matrix.

torch.Size([5240, 2])
We keep 3.42e+06/2.18e+07 = 15% of the original kernel matrix.

torch.Size([7663, 2])
We keep 1.84e+06/4.48e+07 =  4% of the original kernel matrix.

torch.Size([4864, 2])
We keep 1.05e+06/1.32e+07 =  7% of the original kernel matrix.

torch.Size([7397, 2])
We keep 1.55e+06/3.49e+07 =  4% of the original kernel matrix.

torch.Size([8600, 2])
We keep 3.22e+06/5.54e+07 =  5% of the original kernel matrix.

torch.Size([9791, 2])
We keep 2.81e+06/7.14e+07 =  3% of the original kernel matrix.

torch.Size([42558, 2])
We keep 2.15e+08/3.14e+09 =  6% of the original kernel matrix.

torch.Size([22211, 2])
We keep 1.40e+07/5.38e+08 =  2% of the original kernel matrix.

torch.Size([20265, 2])
We keep 5.03e+07/4.72e+08 = 10% of the original kernel matrix.

torch.Size([15227, 2])
We keep 6.73e+06/2.08e+08 =  3% of the original kernel matrix.

torch.Size([3603, 2])
We keep 1.02e+06/1.01e+07 = 10% of the original kernel matrix.

torch.Size([6323, 2])
We keep 1.42e+06/3.05e+07 =  4% of the original kernel matrix.

torch.Size([6754, 2])
We keep 1.82e+06/2.72e+07 =  6% of the original kernel matrix.

torch.Size([8662, 2])
We keep 2.07e+06/5.00e+07 =  4% of the original kernel matrix.

torch.Size([13582, 2])
We keep 5.49e+07/7.00e+08 =  7% of the original kernel matrix.

torch.Size([11710, 2])
We keep 7.47e+06/2.54e+08 =  2% of the original kernel matrix.

torch.Size([8676, 2])
We keep 5.22e+06/6.55e+07 =  7% of the original kernel matrix.

torch.Size([9894, 2])
We keep 2.85e+06/7.76e+07 =  3% of the original kernel matrix.

torch.Size([1877, 2])
We keep 2.00e+05/1.38e+06 = 14% of the original kernel matrix.

torch.Size([4848, 2])
We keep 6.80e+05/1.13e+07 =  6% of the original kernel matrix.

torch.Size([7579, 2])
We keep 3.41e+06/4.42e+07 =  7% of the original kernel matrix.

torch.Size([9372, 2])
We keep 2.62e+06/6.38e+07 =  4% of the original kernel matrix.

torch.Size([66512, 2])
We keep 8.04e+08/9.12e+09 =  8% of the original kernel matrix.

torch.Size([27548, 2])
We keep 2.47e+07/9.16e+08 =  2% of the original kernel matrix.

torch.Size([5691, 2])
We keep 1.19e+06/1.72e+07 =  6% of the original kernel matrix.

torch.Size([7840, 2])
We keep 1.77e+06/3.98e+07 =  4% of the original kernel matrix.

torch.Size([27018, 2])
We keep 2.79e+08/1.12e+09 = 24% of the original kernel matrix.

torch.Size([16994, 2])
We keep 9.52e+06/3.21e+08 =  2% of the original kernel matrix.

torch.Size([2065, 2])
We keep 2.42e+05/2.05e+06 = 11% of the original kernel matrix.

torch.Size([4986, 2])
We keep 8.08e+05/1.37e+07 =  5% of the original kernel matrix.

torch.Size([6392, 2])
We keep 1.71e+07/1.48e+08 = 11% of the original kernel matrix.

torch.Size([7597, 2])
We keep 4.30e+06/1.17e+08 =  3% of the original kernel matrix.

torch.Size([13667, 2])
We keep 2.40e+07/2.40e+08 =  9% of the original kernel matrix.

torch.Size([12391, 2])
We keep 4.86e+06/1.49e+08 =  3% of the original kernel matrix.

torch.Size([14863, 2])
We keep 3.57e+08/2.10e+09 = 16% of the original kernel matrix.

torch.Size([12076, 2])
We keep 1.23e+07/4.40e+08 =  2% of the original kernel matrix.

torch.Size([22895, 2])
We keep 1.73e+07/5.14e+08 =  3% of the original kernel matrix.

torch.Size([15985, 2])
We keep 6.95e+06/2.17e+08 =  3% of the original kernel matrix.

torch.Size([6767, 2])
We keep 1.67e+06/2.68e+07 =  6% of the original kernel matrix.

torch.Size([8717, 2])
We keep 2.10e+06/4.96e+07 =  4% of the original kernel matrix.

torch.Size([45164, 2])
We keep 5.40e+07/1.92e+09 =  2% of the original kernel matrix.

torch.Size([22498, 2])
We keep 1.22e+07/4.20e+08 =  2% of the original kernel matrix.

torch.Size([50661, 2])
We keep 6.00e+07/2.38e+09 =  2% of the original kernel matrix.

torch.Size([24163, 2])
We keep 1.32e+07/4.67e+08 =  2% of the original kernel matrix.

torch.Size([3456, 2])
We keep 4.78e+05/5.75e+06 =  8% of the original kernel matrix.

torch.Size([6287, 2])
We keep 1.07e+06/2.30e+07 =  4% of the original kernel matrix.

torch.Size([24055, 2])
We keep 2.43e+07/4.95e+08 =  4% of the original kernel matrix.

torch.Size([16463, 2])
We keep 6.98e+06/2.13e+08 =  3% of the original kernel matrix.

torch.Size([62803, 2])
We keep 9.04e+07/3.77e+09 =  2% of the original kernel matrix.

torch.Size([27096, 2])
We keep 1.63e+07/5.89e+08 =  2% of the original kernel matrix.

torch.Size([100205, 2])
We keep 4.49e+08/1.33e+10 =  3% of the original kernel matrix.

torch.Size([35355, 2])
We keep 2.90e+07/1.10e+09 =  2% of the original kernel matrix.

torch.Size([20469, 2])
We keep 3.16e+07/4.79e+08 =  6% of the original kernel matrix.

torch.Size([15132, 2])
We keep 6.63e+06/2.10e+08 =  3% of the original kernel matrix.

torch.Size([867, 2])
We keep 4.59e+04/2.83e+05 = 16% of the original kernel matrix.

torch.Size([3587, 2])
We keep 4.03e+05/5.10e+06 =  7% of the original kernel matrix.

torch.Size([7457, 2])
We keep 2.43e+06/4.55e+07 =  5% of the original kernel matrix.

torch.Size([9169, 2])
We keep 2.58e+06/6.47e+07 =  3% of the original kernel matrix.

torch.Size([26551, 2])
We keep 2.82e+08/2.15e+09 = 13% of the original kernel matrix.

torch.Size([16903, 2])
We keep 1.30e+07/4.45e+08 =  2% of the original kernel matrix.

torch.Size([71176, 2])
We keep 4.74e+08/7.10e+09 =  6% of the original kernel matrix.

torch.Size([28363, 2])
We keep 2.32e+07/8.08e+08 =  2% of the original kernel matrix.

torch.Size([9690, 2])
We keep 3.78e+06/7.23e+07 =  5% of the original kernel matrix.

torch.Size([10649, 2])
We keep 3.11e+06/8.15e+07 =  3% of the original kernel matrix.

torch.Size([1753, 2])
We keep 1.32e+05/1.11e+06 = 11% of the original kernel matrix.

torch.Size([4741, 2])
We keep 6.54e+05/1.01e+07 =  6% of the original kernel matrix.

torch.Size([24478, 2])
We keep 2.95e+07/5.56e+08 =  5% of the original kernel matrix.

torch.Size([16548, 2])
We keep 7.08e+06/2.26e+08 =  3% of the original kernel matrix.

torch.Size([44568, 2])
We keep 7.00e+07/2.06e+09 =  3% of the original kernel matrix.

torch.Size([22239, 2])
We keep 1.26e+07/4.35e+08 =  2% of the original kernel matrix.

torch.Size([30950, 2])
We keep 2.12e+08/3.19e+09 =  6% of the original kernel matrix.

torch.Size([16782, 2])
We keep 1.58e+07/5.42e+08 =  2% of the original kernel matrix.

torch.Size([3497, 2])
We keep 6.19e+05/6.11e+06 = 10% of the original kernel matrix.

torch.Size([6233, 2])
We keep 1.17e+06/2.37e+07 =  4% of the original kernel matrix.

torch.Size([7125, 2])
We keep 4.87e+06/3.93e+07 = 12% of the original kernel matrix.

torch.Size([8869, 2])
We keep 2.37e+06/6.02e+07 =  3% of the original kernel matrix.

torch.Size([7610, 2])
We keep 2.70e+06/4.05e+07 =  6% of the original kernel matrix.

torch.Size([9196, 2])
We keep 2.55e+06/6.10e+07 =  4% of the original kernel matrix.

torch.Size([5860, 2])
We keep 1.30e+06/1.87e+07 =  6% of the original kernel matrix.

torch.Size([8091, 2])
We keep 1.85e+06/4.15e+07 =  4% of the original kernel matrix.

torch.Size([7649, 2])
We keep 2.08e+06/3.89e+07 =  5% of the original kernel matrix.

torch.Size([9256, 2])
We keep 2.44e+06/5.99e+07 =  4% of the original kernel matrix.

torch.Size([4831, 2])
We keep 3.43e+06/1.38e+07 = 24% of the original kernel matrix.

torch.Size([7354, 2])
We keep 1.69e+06/3.57e+07 =  4% of the original kernel matrix.

torch.Size([20315, 2])
We keep 8.51e+07/7.31e+08 = 11% of the original kernel matrix.

torch.Size([14485, 2])
We keep 8.08e+06/2.59e+08 =  3% of the original kernel matrix.

torch.Size([18730, 2])
We keep 1.17e+07/2.86e+08 =  4% of the original kernel matrix.

torch.Size([14830, 2])
We keep 5.41e+06/1.62e+08 =  3% of the original kernel matrix.

torch.Size([14389, 2])
We keep 1.00e+07/1.70e+08 =  5% of the original kernel matrix.

torch.Size([12891, 2])
We keep 4.30e+06/1.25e+08 =  3% of the original kernel matrix.

torch.Size([5672, 2])
We keep 5.57e+06/2.13e+07 = 26% of the original kernel matrix.

torch.Size([7830, 2])
We keep 1.92e+06/4.42e+07 =  4% of the original kernel matrix.

torch.Size([3250, 2])
We keep 5.05e+05/4.73e+06 = 10% of the original kernel matrix.

torch.Size([6217, 2])
We keep 1.04e+06/2.09e+07 =  4% of the original kernel matrix.

torch.Size([341607, 2])
We keep 2.02e+09/1.11e+11 =  1% of the original kernel matrix.

torch.Size([65699, 2])
We keep 7.57e+07/3.20e+09 =  2% of the original kernel matrix.

torch.Size([5739, 2])
We keep 1.68e+06/2.03e+07 =  8% of the original kernel matrix.

torch.Size([7987, 2])
We keep 1.94e+06/4.32e+07 =  4% of the original kernel matrix.

torch.Size([59865, 2])
We keep 1.11e+08/3.49e+09 =  3% of the original kernel matrix.

torch.Size([26141, 2])
We keep 1.62e+07/5.67e+08 =  2% of the original kernel matrix.

torch.Size([1620, 2])
We keep 1.19e+05/1.02e+06 = 11% of the original kernel matrix.

torch.Size([4590, 2])
We keep 5.86e+05/9.68e+06 =  6% of the original kernel matrix.

torch.Size([2539, 2])
We keep 3.38e+05/2.91e+06 = 11% of the original kernel matrix.

torch.Size([5492, 2])
We keep 9.09e+05/1.64e+07 =  5% of the original kernel matrix.

torch.Size([2968, 2])
We keep 2.51e+05/2.76e+06 =  9% of the original kernel matrix.

torch.Size([5782, 2])
We keep 8.82e+05/1.59e+07 =  5% of the original kernel matrix.

torch.Size([21043, 2])
We keep 1.83e+07/3.86e+08 =  4% of the original kernel matrix.

torch.Size([15670, 2])
We keep 5.88e+06/1.89e+08 =  3% of the original kernel matrix.

torch.Size([2507, 2])
We keep 2.42e+05/2.59e+06 =  9% of the original kernel matrix.

torch.Size([5408, 2])
We keep 8.51e+05/1.54e+07 =  5% of the original kernel matrix.

torch.Size([7121, 2])
We keep 4.36e+07/1.30e+08 = 33% of the original kernel matrix.

torch.Size([8673, 2])
We keep 3.78e+06/1.09e+08 =  3% of the original kernel matrix.

torch.Size([4217, 2])
We keep 7.44e+05/8.13e+06 =  9% of the original kernel matrix.

torch.Size([6753, 2])
We keep 1.34e+06/2.73e+07 =  4% of the original kernel matrix.

torch.Size([2468, 2])
We keep 2.90e+05/2.51e+06 = 11% of the original kernel matrix.

torch.Size([5435, 2])
We keep 8.76e+05/1.52e+07 =  5% of the original kernel matrix.

torch.Size([102205, 2])
We keep 4.00e+08/1.27e+10 =  3% of the original kernel matrix.

torch.Size([34792, 2])
We keep 2.80e+07/1.08e+09 =  2% of the original kernel matrix.

torch.Size([13453, 2])
We keep 2.03e+07/1.63e+08 = 12% of the original kernel matrix.

torch.Size([12341, 2])
We keep 4.30e+06/1.23e+08 =  3% of the original kernel matrix.

torch.Size([6214, 2])
We keep 2.22e+06/2.58e+07 =  8% of the original kernel matrix.

torch.Size([8372, 2])
We keep 2.03e+06/4.87e+07 =  4% of the original kernel matrix.

torch.Size([30584, 2])
We keep 1.05e+08/1.01e+09 = 10% of the original kernel matrix.

torch.Size([18803, 2])
We keep 9.30e+06/3.05e+08 =  3% of the original kernel matrix.

torch.Size([23086, 2])
We keep 1.16e+08/8.77e+08 = 13% of the original kernel matrix.

torch.Size([16000, 2])
We keep 8.55e+06/2.84e+08 =  3% of the original kernel matrix.

torch.Size([3925, 2])
We keep 5.95e+05/6.74e+06 =  8% of the original kernel matrix.

torch.Size([6607, 2])
We keep 1.27e+06/2.49e+07 =  5% of the original kernel matrix.

torch.Size([6416, 2])
We keep 1.37e+06/2.42e+07 =  5% of the original kernel matrix.

torch.Size([8455, 2])
We keep 2.01e+06/4.72e+07 =  4% of the original kernel matrix.

torch.Size([4877, 2])
We keep 3.13e+06/1.41e+07 = 22% of the original kernel matrix.

torch.Size([7429, 2])
We keep 1.49e+06/3.61e+07 =  4% of the original kernel matrix.

torch.Size([479, 2])
We keep 1.57e+04/7.02e+04 = 22% of the original kernel matrix.

torch.Size([2876, 2])
We keep 2.49e+05/2.54e+06 =  9% of the original kernel matrix.

torch.Size([3551, 2])
We keep 5.21e+05/4.94e+06 = 10% of the original kernel matrix.

torch.Size([6404, 2])
We keep 1.14e+06/2.13e+07 =  5% of the original kernel matrix.

torch.Size([5637, 2])
We keep 2.99e+06/2.61e+07 = 11% of the original kernel matrix.

torch.Size([7854, 2])
We keep 2.08e+06/4.90e+07 =  4% of the original kernel matrix.

torch.Size([1355, 2])
We keep 1.58e+05/8.52e+05 = 18% of the original kernel matrix.

torch.Size([4125, 2])
We keep 6.09e+05/8.85e+06 =  6% of the original kernel matrix.

torch.Size([4032, 2])
We keep 5.32e+05/6.15e+06 =  8% of the original kernel matrix.

torch.Size([6769, 2])
We keep 1.20e+06/2.38e+07 =  5% of the original kernel matrix.

torch.Size([5263, 2])
We keep 1.29e+06/1.72e+07 =  7% of the original kernel matrix.

torch.Size([7542, 2])
We keep 1.80e+06/3.98e+07 =  4% of the original kernel matrix.

torch.Size([2704, 2])
We keep 3.07e+05/3.11e+06 =  9% of the original kernel matrix.

torch.Size([5684, 2])
We keep 9.30e+05/1.69e+07 =  5% of the original kernel matrix.

torch.Size([97531, 2])
We keep 4.20e+08/1.33e+10 =  3% of the original kernel matrix.

torch.Size([33511, 2])
We keep 2.90e+07/1.10e+09 =  2% of the original kernel matrix.

torch.Size([2465, 2])
We keep 7.37e+05/4.48e+06 = 16% of the original kernel matrix.

torch.Size([5293, 2])
We keep 1.03e+06/2.03e+07 =  5% of the original kernel matrix.

torch.Size([23637, 2])
We keep 5.10e+07/5.01e+08 = 10% of the original kernel matrix.

torch.Size([16371, 2])
We keep 6.85e+06/2.15e+08 =  3% of the original kernel matrix.

torch.Size([5268, 2])
We keep 2.96e+06/2.37e+07 = 12% of the original kernel matrix.

torch.Size([7581, 2])
We keep 1.88e+06/4.67e+07 =  4% of the original kernel matrix.

torch.Size([9016, 2])
We keep 6.95e+06/8.65e+07 =  8% of the original kernel matrix.

torch.Size([9697, 2])
We keep 3.43e+06/8.92e+07 =  3% of the original kernel matrix.

torch.Size([9482, 2])
We keep 4.93e+07/2.61e+08 = 18% of the original kernel matrix.

torch.Size([10050, 2])
We keep 5.22e+06/1.55e+08 =  3% of the original kernel matrix.

torch.Size([553783, 2])
We keep 1.51e+10/4.16e+11 =  3% of the original kernel matrix.

torch.Size([84064, 2])
We keep 1.38e+08/6.18e+09 =  2% of the original kernel matrix.

torch.Size([244334, 2])
We keep 8.93e+08/5.05e+10 =  1% of the original kernel matrix.

torch.Size([55493, 2])
We keep 5.19e+07/2.16e+09 =  2% of the original kernel matrix.

torch.Size([15334, 2])
We keep 9.18e+06/2.09e+08 =  4% of the original kernel matrix.

torch.Size([13324, 2])
We keep 4.64e+06/1.39e+08 =  3% of the original kernel matrix.

torch.Size([23726, 2])
We keep 6.33e+07/5.60e+08 = 11% of the original kernel matrix.

torch.Size([16444, 2])
We keep 7.00e+06/2.27e+08 =  3% of the original kernel matrix.

torch.Size([1766, 2])
We keep 1.22e+05/1.07e+06 = 11% of the original kernel matrix.

torch.Size([4784, 2])
We keep 6.29e+05/9.94e+06 =  6% of the original kernel matrix.

torch.Size([9037, 2])
We keep 3.25e+07/1.23e+08 = 26% of the original kernel matrix.

torch.Size([9817, 2])
We keep 3.81e+06/1.06e+08 =  3% of the original kernel matrix.

torch.Size([14358, 2])
We keep 3.85e+07/2.79e+08 = 13% of the original kernel matrix.

torch.Size([12573, 2])
We keep 5.54e+06/1.60e+08 =  3% of the original kernel matrix.

torch.Size([30729, 2])
We keep 4.28e+07/8.82e+08 =  4% of the original kernel matrix.

torch.Size([18621, 2])
We keep 8.94e+06/2.85e+08 =  3% of the original kernel matrix.

torch.Size([4272, 2])
We keep 5.88e+05/7.53e+06 =  7% of the original kernel matrix.

torch.Size([6918, 2])
We keep 1.30e+06/2.63e+07 =  4% of the original kernel matrix.

torch.Size([25821, 2])
We keep 2.65e+07/6.26e+08 =  4% of the original kernel matrix.

torch.Size([16923, 2])
We keep 7.65e+06/2.40e+08 =  3% of the original kernel matrix.

torch.Size([58858, 2])
We keep 2.84e+08/5.15e+09 =  5% of the original kernel matrix.

torch.Size([25812, 2])
We keep 1.88e+07/6.89e+08 =  2% of the original kernel matrix.

torch.Size([9512, 2])
We keep 5.34e+06/9.93e+07 =  5% of the original kernel matrix.

torch.Size([9788, 2])
We keep 3.49e+06/9.56e+07 =  3% of the original kernel matrix.

torch.Size([3013, 2])
We keep 6.57e+05/5.65e+06 = 11% of the original kernel matrix.

torch.Size([5789, 2])
We keep 1.15e+06/2.28e+07 =  5% of the original kernel matrix.

torch.Size([7685, 2])
We keep 1.23e+07/7.31e+07 = 16% of the original kernel matrix.

torch.Size([9504, 2])
We keep 3.19e+06/8.20e+07 =  3% of the original kernel matrix.

torch.Size([10968, 2])
We keep 5.34e+06/9.96e+07 =  5% of the original kernel matrix.

torch.Size([11185, 2])
We keep 3.57e+06/9.57e+07 =  3% of the original kernel matrix.

torch.Size([3745, 2])
We keep 4.06e+05/5.02e+06 =  8% of the original kernel matrix.

torch.Size([6652, 2])
We keep 1.06e+06/2.15e+07 =  4% of the original kernel matrix.

torch.Size([60776, 2])
We keep 1.60e+08/3.73e+09 =  4% of the original kernel matrix.

torch.Size([26271, 2])
We keep 1.62e+07/5.86e+08 =  2% of the original kernel matrix.

torch.Size([10472, 2])
We keep 6.20e+06/1.11e+08 =  5% of the original kernel matrix.

torch.Size([10547, 2])
We keep 3.69e+06/1.01e+08 =  3% of the original kernel matrix.

torch.Size([1460, 2])
We keep 7.88e+04/5.72e+05 = 13% of the original kernel matrix.

torch.Size([4475, 2])
We keep 4.96e+05/7.25e+06 =  6% of the original kernel matrix.

torch.Size([44442, 2])
We keep 6.11e+07/1.75e+09 =  3% of the original kernel matrix.

torch.Size([22376, 2])
We keep 1.15e+07/4.02e+08 =  2% of the original kernel matrix.

torch.Size([7312, 2])
We keep 1.80e+06/3.01e+07 =  5% of the original kernel matrix.

torch.Size([9187, 2])
We keep 2.18e+06/5.26e+07 =  4% of the original kernel matrix.

torch.Size([651, 2])
We keep 1.89e+04/1.16e+05 = 16% of the original kernel matrix.

torch.Size([3326, 2])
We keep 2.94e+05/3.27e+06 =  8% of the original kernel matrix.

torch.Size([20995, 2])
We keep 9.13e+07/5.81e+08 = 15% of the original kernel matrix.

torch.Size([15201, 2])
We keep 7.31e+06/2.31e+08 =  3% of the original kernel matrix.

torch.Size([35005, 2])
We keep 3.56e+07/1.17e+09 =  3% of the original kernel matrix.

torch.Size([19751, 2])
We keep 9.96e+06/3.28e+08 =  3% of the original kernel matrix.

torch.Size([23132, 2])
We keep 2.79e+07/6.55e+08 =  4% of the original kernel matrix.

torch.Size([16295, 2])
We keep 7.87e+06/2.46e+08 =  3% of the original kernel matrix.

torch.Size([28976, 2])
We keep 9.63e+07/1.08e+09 =  8% of the original kernel matrix.

torch.Size([17759, 2])
We keep 9.76e+06/3.15e+08 =  3% of the original kernel matrix.

torch.Size([12843, 2])
We keep 1.02e+07/1.77e+08 =  5% of the original kernel matrix.

torch.Size([11843, 2])
We keep 4.43e+06/1.28e+08 =  3% of the original kernel matrix.

torch.Size([3029, 2])
We keep 6.28e+05/5.70e+06 = 11% of the original kernel matrix.

torch.Size([5776, 2])
We keep 1.16e+06/2.29e+07 =  5% of the original kernel matrix.

torch.Size([13861, 2])
We keep 1.02e+07/1.96e+08 =  5% of the original kernel matrix.

torch.Size([12344, 2])
We keep 4.68e+06/1.34e+08 =  3% of the original kernel matrix.

torch.Size([48357, 2])
We keep 5.40e+07/2.09e+09 =  2% of the original kernel matrix.

torch.Size([23345, 2])
We keep 1.27e+07/4.39e+08 =  2% of the original kernel matrix.

torch.Size([53676, 2])
We keep 8.87e+07/3.18e+09 =  2% of the original kernel matrix.

torch.Size([24903, 2])
We keep 1.55e+07/5.41e+08 =  2% of the original kernel matrix.

torch.Size([6853, 2])
We keep 1.63e+06/2.65e+07 =  6% of the original kernel matrix.

torch.Size([8674, 2])
We keep 2.08e+06/4.94e+07 =  4% of the original kernel matrix.

torch.Size([99984, 2])
We keep 1.28e+09/2.08e+10 =  6% of the original kernel matrix.

torch.Size([34775, 2])
We keep 3.27e+07/1.38e+09 =  2% of the original kernel matrix.

torch.Size([94701, 2])
We keep 3.79e+08/1.03e+10 =  3% of the original kernel matrix.

torch.Size([34027, 2])
We keep 2.42e+07/9.74e+08 =  2% of the original kernel matrix.

torch.Size([32001, 2])
We keep 4.41e+07/9.76e+08 =  4% of the original kernel matrix.

torch.Size([19064, 2])
We keep 9.04e+06/3.00e+08 =  3% of the original kernel matrix.

torch.Size([10901, 2])
We keep 9.67e+06/1.37e+08 =  7% of the original kernel matrix.

torch.Size([11097, 2])
We keep 3.67e+06/1.12e+08 =  3% of the original kernel matrix.

torch.Size([3583, 2])
We keep 5.43e+06/1.24e+07 = 43% of the original kernel matrix.

torch.Size([6162, 2])
We keep 1.53e+06/3.38e+07 =  4% of the original kernel matrix.

torch.Size([4497, 2])
We keep 1.07e+06/1.06e+07 = 10% of the original kernel matrix.

torch.Size([7073, 2])
We keep 1.48e+06/3.13e+07 =  4% of the original kernel matrix.

torch.Size([4474, 2])
We keep 7.93e+05/1.11e+07 =  7% of the original kernel matrix.

torch.Size([6972, 2])
We keep 1.50e+06/3.20e+07 =  4% of the original kernel matrix.

torch.Size([2552, 2])
We keep 1.38e+06/4.72e+06 = 29% of the original kernel matrix.

torch.Size([5332, 2])
We keep 9.46e+05/2.08e+07 =  4% of the original kernel matrix.

torch.Size([83411, 2])
We keep 1.37e+08/6.52e+09 =  2% of the original kernel matrix.

torch.Size([31543, 2])
We keep 2.08e+07/7.75e+08 =  2% of the original kernel matrix.

torch.Size([45247, 2])
We keep 5.34e+07/1.91e+09 =  2% of the original kernel matrix.

torch.Size([22561, 2])
We keep 1.21e+07/4.19e+08 =  2% of the original kernel matrix.

torch.Size([88170, 2])
We keep 5.65e+08/1.04e+10 =  5% of the original kernel matrix.

torch.Size([31944, 2])
We keep 2.56e+07/9.79e+08 =  2% of the original kernel matrix.

torch.Size([34514, 2])
We keep 3.29e+08/1.59e+09 = 20% of the original kernel matrix.

torch.Size([19719, 2])
We keep 1.12e+07/3.83e+08 =  2% of the original kernel matrix.

torch.Size([23944, 2])
We keep 3.46e+07/5.75e+08 =  6% of the original kernel matrix.

torch.Size([16149, 2])
We keep 7.40e+06/2.30e+08 =  3% of the original kernel matrix.

torch.Size([6754, 2])
We keep 2.00e+06/3.70e+07 =  5% of the original kernel matrix.

torch.Size([8643, 2])
We keep 2.42e+06/5.83e+07 =  4% of the original kernel matrix.

torch.Size([25215, 2])
We keep 2.26e+07/6.75e+08 =  3% of the original kernel matrix.

torch.Size([16939, 2])
We keep 7.76e+06/2.49e+08 =  3% of the original kernel matrix.

torch.Size([1246, 2])
We keep 7.30e+04/4.90e+05 = 14% of the original kernel matrix.

torch.Size([4172, 2])
We keep 4.61e+05/6.71e+06 =  6% of the original kernel matrix.

torch.Size([7604, 2])
We keep 6.66e+06/5.41e+07 = 12% of the original kernel matrix.

torch.Size([9186, 2])
We keep 2.93e+06/7.05e+07 =  4% of the original kernel matrix.

torch.Size([62998, 2])
We keep 1.66e+08/4.39e+09 =  3% of the original kernel matrix.

torch.Size([26919, 2])
We keep 1.77e+07/6.35e+08 =  2% of the original kernel matrix.

torch.Size([14354, 2])
We keep 8.03e+06/1.69e+08 =  4% of the original kernel matrix.

torch.Size([12860, 2])
We keep 4.39e+06/1.25e+08 =  3% of the original kernel matrix.

torch.Size([1897, 2])
We keep 1.42e+05/1.22e+06 = 11% of the original kernel matrix.

torch.Size([4829, 2])
We keep 6.63e+05/1.06e+07 =  6% of the original kernel matrix.

torch.Size([6825, 2])
We keep 2.07e+06/3.30e+07 =  6% of the original kernel matrix.

torch.Size([8685, 2])
We keep 2.28e+06/5.51e+07 =  4% of the original kernel matrix.

torch.Size([13652, 2])
We keep 3.39e+07/3.11e+08 = 10% of the original kernel matrix.

torch.Size([12155, 2])
We keep 5.70e+06/1.69e+08 =  3% of the original kernel matrix.

torch.Size([12374, 2])
We keep 7.74e+06/1.38e+08 =  5% of the original kernel matrix.

torch.Size([11709, 2])
We keep 4.00e+06/1.13e+08 =  3% of the original kernel matrix.

torch.Size([20669, 2])
We keep 3.45e+07/3.89e+08 =  8% of the original kernel matrix.

torch.Size([15411, 2])
We keep 6.12e+06/1.89e+08 =  3% of the original kernel matrix.

torch.Size([8163, 2])
We keep 5.80e+06/7.83e+07 =  7% of the original kernel matrix.

torch.Size([9341, 2])
We keep 3.32e+06/8.49e+07 =  3% of the original kernel matrix.

torch.Size([10357, 2])
We keep 1.62e+07/1.46e+08 = 11% of the original kernel matrix.

torch.Size([10359, 2])
We keep 4.33e+06/1.16e+08 =  3% of the original kernel matrix.

torch.Size([2001, 2])
We keep 2.25e+05/1.88e+06 = 11% of the original kernel matrix.

torch.Size([4949, 2])
We keep 7.53e+05/1.31e+07 =  5% of the original kernel matrix.

torch.Size([1694, 2])
We keep 1.42e+05/1.11e+06 = 12% of the original kernel matrix.

torch.Size([4694, 2])
We keep 6.36e+05/1.01e+07 =  6% of the original kernel matrix.

torch.Size([18712, 2])
We keep 1.76e+07/3.06e+08 =  5% of the original kernel matrix.

torch.Size([14609, 2])
We keep 5.60e+06/1.68e+08 =  3% of the original kernel matrix.

torch.Size([8730, 2])
We keep 9.13e+06/8.83e+07 = 10% of the original kernel matrix.

torch.Size([9850, 2])
We keep 3.45e+06/9.02e+07 =  3% of the original kernel matrix.

torch.Size([3413, 2])
We keep 1.54e+06/7.45e+06 = 20% of the original kernel matrix.

torch.Size([6170, 2])
We keep 1.39e+06/2.62e+07 =  5% of the original kernel matrix.

torch.Size([75924, 2])
We keep 1.68e+08/6.45e+09 =  2% of the original kernel matrix.

torch.Size([29373, 2])
We keep 2.10e+07/7.71e+08 =  2% of the original kernel matrix.

torch.Size([3562, 2])
We keep 9.01e+05/6.01e+06 = 15% of the original kernel matrix.

torch.Size([6403, 2])
We keep 1.18e+06/2.35e+07 =  5% of the original kernel matrix.

torch.Size([12434, 2])
We keep 6.62e+06/1.69e+08 =  3% of the original kernel matrix.

torch.Size([12113, 2])
We keep 4.47e+06/1.25e+08 =  3% of the original kernel matrix.

torch.Size([3686, 2])
We keep 9.45e+05/8.05e+06 = 11% of the original kernel matrix.

torch.Size([6494, 2])
We keep 1.22e+06/2.72e+07 =  4% of the original kernel matrix.

torch.Size([38875, 2])
We keep 3.87e+07/1.29e+09 =  2% of the original kernel matrix.

torch.Size([20372, 2])
We keep 1.03e+07/3.45e+08 =  2% of the original kernel matrix.

torch.Size([3949, 2])
We keep 8.07e+05/7.57e+06 = 10% of the original kernel matrix.

torch.Size([6663, 2])
We keep 1.28e+06/2.64e+07 =  4% of the original kernel matrix.

torch.Size([15362, 2])
We keep 3.79e+07/2.76e+08 = 13% of the original kernel matrix.

torch.Size([13161, 2])
We keep 5.30e+06/1.59e+08 =  3% of the original kernel matrix.

torch.Size([7337, 2])
We keep 2.72e+06/3.32e+07 =  8% of the original kernel matrix.

torch.Size([8999, 2])
We keep 2.30e+06/5.53e+07 =  4% of the original kernel matrix.

torch.Size([6927, 2])
We keep 2.46e+06/3.69e+07 =  6% of the original kernel matrix.

torch.Size([8768, 2])
We keep 2.40e+06/5.83e+07 =  4% of the original kernel matrix.

torch.Size([15017, 2])
We keep 9.97e+06/2.20e+08 =  4% of the original kernel matrix.

torch.Size([13278, 2])
We keep 4.87e+06/1.42e+08 =  3% of the original kernel matrix.

torch.Size([235032, 2])
We keep 1.26e+09/5.89e+10 =  2% of the original kernel matrix.

torch.Size([52850, 2])
We keep 5.65e+07/2.33e+09 =  2% of the original kernel matrix.

torch.Size([6271, 2])
We keep 1.59e+06/2.53e+07 =  6% of the original kernel matrix.

torch.Size([8356, 2])
We keep 2.08e+06/4.83e+07 =  4% of the original kernel matrix.

torch.Size([3784, 2])
We keep 8.63e+05/7.52e+06 = 11% of the original kernel matrix.

torch.Size([6445, 2])
We keep 1.25e+06/2.63e+07 =  4% of the original kernel matrix.

torch.Size([28431, 2])
We keep 4.22e+07/8.24e+08 =  5% of the original kernel matrix.

torch.Size([17629, 2])
We keep 8.33e+06/2.75e+08 =  3% of the original kernel matrix.

torch.Size([6215, 2])
We keep 1.55e+06/2.25e+07 =  6% of the original kernel matrix.

torch.Size([8285, 2])
We keep 1.93e+06/4.55e+07 =  4% of the original kernel matrix.

torch.Size([111346, 2])
We keep 4.75e+08/1.49e+10 =  3% of the original kernel matrix.

torch.Size([37260, 2])
We keep 3.00e+07/1.17e+09 =  2% of the original kernel matrix.

torch.Size([3040, 2])
We keep 4.92e+05/3.69e+06 = 13% of the original kernel matrix.

torch.Size([5905, 2])
We keep 1.02e+06/1.84e+07 =  5% of the original kernel matrix.

torch.Size([27109, 2])
We keep 5.15e+07/1.19e+09 =  4% of the original kernel matrix.

torch.Size([16918, 2])
We keep 1.03e+07/3.31e+08 =  3% of the original kernel matrix.

torch.Size([136104, 2])
We keep 4.12e+08/2.24e+10 =  1% of the original kernel matrix.

torch.Size([41500, 2])
We keep 3.66e+07/1.44e+09 =  2% of the original kernel matrix.

torch.Size([2391, 2])
We keep 2.53e+05/2.19e+06 = 11% of the original kernel matrix.

torch.Size([5275, 2])
We keep 8.23e+05/1.42e+07 =  5% of the original kernel matrix.

torch.Size([128240, 2])
We keep 5.07e+08/2.04e+10 =  2% of the original kernel matrix.

torch.Size([37985, 2])
We keep 3.50e+07/1.37e+09 =  2% of the original kernel matrix.

torch.Size([5573, 2])
We keep 2.42e+07/1.18e+08 = 20% of the original kernel matrix.

torch.Size([7516, 2])
We keep 3.73e+06/1.04e+08 =  3% of the original kernel matrix.

torch.Size([51881, 2])
We keep 7.63e+08/3.88e+09 = 19% of the original kernel matrix.

torch.Size([24602, 2])
We keep 1.67e+07/5.98e+08 =  2% of the original kernel matrix.

torch.Size([4540, 2])
We keep 8.78e+05/1.11e+07 =  7% of the original kernel matrix.

torch.Size([7021, 2])
We keep 1.51e+06/3.20e+07 =  4% of the original kernel matrix.

torch.Size([21665, 2])
We keep 3.56e+07/5.41e+08 =  6% of the original kernel matrix.

torch.Size([15306, 2])
We keep 7.06e+06/2.23e+08 =  3% of the original kernel matrix.

torch.Size([1696, 2])
We keep 1.76e+05/1.32e+06 = 13% of the original kernel matrix.

torch.Size([4695, 2])
We keep 6.54e+05/1.10e+07 =  5% of the original kernel matrix.

torch.Size([2728, 2])
We keep 3.75e+05/3.44e+06 = 10% of the original kernel matrix.

torch.Size([5595, 2])
We keep 9.90e+05/1.78e+07 =  5% of the original kernel matrix.

torch.Size([2941, 2])
We keep 5.22e+05/4.42e+06 = 11% of the original kernel matrix.

torch.Size([5788, 2])
We keep 1.06e+06/2.02e+07 =  5% of the original kernel matrix.

torch.Size([22395, 2])
We keep 2.30e+07/5.00e+08 =  4% of the original kernel matrix.

torch.Size([16034, 2])
We keep 6.87e+06/2.14e+08 =  3% of the original kernel matrix.

torch.Size([13192, 2])
We keep 1.16e+07/1.75e+08 =  6% of the original kernel matrix.

torch.Size([12202, 2])
We keep 4.55e+06/1.27e+08 =  3% of the original kernel matrix.

torch.Size([24153, 2])
We keep 2.88e+07/5.90e+08 =  4% of the original kernel matrix.

torch.Size([16587, 2])
We keep 7.46e+06/2.33e+08 =  3% of the original kernel matrix.

torch.Size([6633, 2])
We keep 1.99e+06/3.14e+07 =  6% of the original kernel matrix.

torch.Size([8565, 2])
We keep 2.24e+06/5.37e+07 =  4% of the original kernel matrix.

torch.Size([3294, 2])
We keep 3.35e+05/4.04e+06 =  8% of the original kernel matrix.

torch.Size([6109, 2])
We keep 1.01e+06/1.93e+07 =  5% of the original kernel matrix.

torch.Size([2458, 2])
We keep 3.22e+05/2.53e+06 = 12% of the original kernel matrix.

torch.Size([5397, 2])
We keep 8.41e+05/1.53e+07 =  5% of the original kernel matrix.

torch.Size([19547, 2])
We keep 1.03e+08/8.87e+08 = 11% of the original kernel matrix.

torch.Size([14777, 2])
We keep 8.60e+06/2.86e+08 =  3% of the original kernel matrix.

torch.Size([7162, 2])
We keep 4.10e+06/3.37e+07 = 12% of the original kernel matrix.

torch.Size([9093, 2])
We keep 2.26e+06/5.57e+07 =  4% of the original kernel matrix.

torch.Size([104250, 2])
We keep 3.95e+08/1.32e+10 =  2% of the original kernel matrix.

torch.Size([36172, 2])
We keep 2.84e+07/1.10e+09 =  2% of the original kernel matrix.

torch.Size([11297, 2])
We keep 7.72e+06/1.13e+08 =  6% of the original kernel matrix.

torch.Size([11567, 2])
We keep 3.79e+06/1.02e+08 =  3% of the original kernel matrix.

torch.Size([3105, 2])
We keep 7.64e+05/5.69e+06 = 13% of the original kernel matrix.

torch.Size([5973, 2])
We keep 1.13e+06/2.29e+07 =  4% of the original kernel matrix.

torch.Size([3394, 2])
We keep 5.54e+05/5.70e+06 =  9% of the original kernel matrix.

torch.Size([6163, 2])
We keep 1.16e+06/2.29e+07 =  5% of the original kernel matrix.

torch.Size([2302, 2])
We keep 5.69e+05/3.10e+06 = 18% of the original kernel matrix.

torch.Size([5068, 2])
We keep 9.43e+05/1.69e+07 =  5% of the original kernel matrix.

torch.Size([402431, 2])
We keep 3.61e+09/1.81e+11 =  1% of the original kernel matrix.

torch.Size([73548, 2])
We keep 9.27e+07/4.08e+09 =  2% of the original kernel matrix.

torch.Size([3502, 2])
We keep 5.18e+05/5.57e+06 =  9% of the original kernel matrix.

torch.Size([6255, 2])
We keep 1.13e+06/2.26e+07 =  4% of the original kernel matrix.

torch.Size([7500, 2])
We keep 2.74e+06/3.61e+07 =  7% of the original kernel matrix.

torch.Size([9243, 2])
We keep 2.38e+06/5.76e+07 =  4% of the original kernel matrix.

torch.Size([29224, 2])
We keep 9.05e+07/1.13e+09 =  7% of the original kernel matrix.

torch.Size([17681, 2])
We keep 9.94e+06/3.23e+08 =  3% of the original kernel matrix.

torch.Size([12827, 2])
We keep 2.34e+07/3.24e+08 =  7% of the original kernel matrix.

torch.Size([11028, 2])
We keep 5.45e+06/1.73e+08 =  3% of the original kernel matrix.

torch.Size([3786, 2])
We keep 6.07e+05/6.32e+06 =  9% of the original kernel matrix.

torch.Size([6578, 2])
We keep 1.22e+06/2.41e+07 =  5% of the original kernel matrix.

torch.Size([2947, 2])
We keep 4.57e+05/4.44e+06 = 10% of the original kernel matrix.

torch.Size([5830, 2])
We keep 1.06e+06/2.02e+07 =  5% of the original kernel matrix.

torch.Size([8941, 2])
We keep 2.57e+07/1.15e+08 = 22% of the original kernel matrix.

torch.Size([10078, 2])
We keep 3.79e+06/1.03e+08 =  3% of the original kernel matrix.

torch.Size([11696, 2])
We keep 5.59e+06/1.04e+08 =  5% of the original kernel matrix.

torch.Size([11615, 2])
We keep 3.52e+06/9.76e+07 =  3% of the original kernel matrix.

torch.Size([8709, 2])
We keep 2.06e+07/1.20e+08 = 17% of the original kernel matrix.

torch.Size([9692, 2])
We keep 3.72e+06/1.05e+08 =  3% of the original kernel matrix.

torch.Size([24249, 2])
We keep 2.89e+07/5.18e+08 =  5% of the original kernel matrix.

torch.Size([16662, 2])
We keep 6.83e+06/2.18e+08 =  3% of the original kernel matrix.

torch.Size([26760, 2])
We keep 5.28e+07/7.16e+08 =  7% of the original kernel matrix.

torch.Size([17377, 2])
We keep 8.25e+06/2.57e+08 =  3% of the original kernel matrix.

torch.Size([12181, 2])
We keep 2.20e+07/1.70e+08 = 12% of the original kernel matrix.

torch.Size([11829, 2])
We keep 4.38e+06/1.25e+08 =  3% of the original kernel matrix.

torch.Size([42591, 2])
We keep 1.80e+08/3.41e+09 =  5% of the original kernel matrix.

torch.Size([21654, 2])
We keep 1.59e+07/5.60e+08 =  2% of the original kernel matrix.

torch.Size([32597, 2])
We keep 3.36e+07/9.06e+08 =  3% of the original kernel matrix.

torch.Size([19087, 2])
We keep 8.63e+06/2.89e+08 =  2% of the original kernel matrix.

torch.Size([7457, 2])
We keep 3.58e+06/3.90e+07 =  9% of the original kernel matrix.

torch.Size([9149, 2])
We keep 2.39e+06/5.99e+07 =  3% of the original kernel matrix.

torch.Size([48383, 2])
We keep 1.05e+08/2.33e+09 =  4% of the original kernel matrix.

torch.Size([23293, 2])
We keep 1.37e+07/4.63e+08 =  2% of the original kernel matrix.

torch.Size([7192, 2])
We keep 1.20e+07/4.56e+07 = 26% of the original kernel matrix.

torch.Size([8921, 2])
We keep 2.45e+06/6.48e+07 =  3% of the original kernel matrix.

torch.Size([4580, 2])
We keep 8.53e+05/1.13e+07 =  7% of the original kernel matrix.

torch.Size([7133, 2])
We keep 1.51e+06/3.23e+07 =  4% of the original kernel matrix.

torch.Size([6828, 2])
We keep 2.18e+06/3.41e+07 =  6% of the original kernel matrix.

torch.Size([8679, 2])
We keep 2.33e+06/5.60e+07 =  4% of the original kernel matrix.

torch.Size([5943, 2])
We keep 1.37e+06/1.97e+07 =  6% of the original kernel matrix.

torch.Size([8162, 2])
We keep 1.87e+06/4.26e+07 =  4% of the original kernel matrix.

torch.Size([24750, 2])
We keep 5.32e+07/6.81e+08 =  7% of the original kernel matrix.

torch.Size([16581, 2])
We keep 7.54e+06/2.50e+08 =  3% of the original kernel matrix.

torch.Size([6602, 2])
We keep 2.53e+06/3.48e+07 =  7% of the original kernel matrix.

torch.Size([8374, 2])
We keep 2.38e+06/5.65e+07 =  4% of the original kernel matrix.

torch.Size([4707, 2])
We keep 2.02e+06/1.51e+07 = 13% of the original kernel matrix.

torch.Size([7100, 2])
We keep 1.78e+06/3.73e+07 =  4% of the original kernel matrix.

torch.Size([10351, 2])
We keep 6.53e+06/1.01e+08 =  6% of the original kernel matrix.

torch.Size([10903, 2])
We keep 3.67e+06/9.65e+07 =  3% of the original kernel matrix.

torch.Size([4873, 2])
We keep 6.73e+06/5.17e+07 = 13% of the original kernel matrix.

torch.Size([7142, 2])
We keep 2.38e+06/6.90e+07 =  3% of the original kernel matrix.

torch.Size([11774, 2])
We keep 3.40e+07/2.20e+08 = 15% of the original kernel matrix.

torch.Size([11244, 2])
We keep 4.88e+06/1.42e+08 =  3% of the original kernel matrix.

torch.Size([11019, 2])
We keep 3.49e+07/1.63e+08 = 21% of the original kernel matrix.

torch.Size([10940, 2])
We keep 4.67e+06/1.23e+08 =  3% of the original kernel matrix.

torch.Size([6945, 2])
We keep 2.15e+06/3.31e+07 =  6% of the original kernel matrix.

torch.Size([8760, 2])
We keep 2.29e+06/5.51e+07 =  4% of the original kernel matrix.

torch.Size([7296, 2])
We keep 3.98e+06/4.76e+07 =  8% of the original kernel matrix.

torch.Size([8791, 2])
We keep 2.72e+06/6.62e+07 =  4% of the original kernel matrix.

torch.Size([6196, 2])
We keep 3.14e+06/3.13e+07 = 10% of the original kernel matrix.

torch.Size([8324, 2])
We keep 2.27e+06/5.37e+07 =  4% of the original kernel matrix.

torch.Size([2016, 2])
We keep 1.39e+05/1.29e+06 = 10% of the original kernel matrix.

torch.Size([5073, 2])
We keep 6.74e+05/1.09e+07 =  6% of the original kernel matrix.

torch.Size([3482, 2])
We keep 4.74e+05/5.30e+06 =  8% of the original kernel matrix.

torch.Size([6203, 2])
We keep 1.13e+06/2.21e+07 =  5% of the original kernel matrix.

torch.Size([13587, 2])
We keep 6.57e+06/1.43e+08 =  4% of the original kernel matrix.

torch.Size([12686, 2])
We keep 4.09e+06/1.15e+08 =  3% of the original kernel matrix.

torch.Size([74449, 2])
We keep 2.31e+08/6.40e+09 =  3% of the original kernel matrix.

torch.Size([29602, 2])
We keep 2.12e+07/7.67e+08 =  2% of the original kernel matrix.

torch.Size([3556, 2])
We keep 5.37e+05/4.94e+06 = 10% of the original kernel matrix.

torch.Size([6423, 2])
We keep 1.08e+06/2.13e+07 =  5% of the original kernel matrix.

torch.Size([1112, 2])
We keep 4.93e+04/3.35e+05 = 14% of the original kernel matrix.

torch.Size([3930, 2])
We keep 4.24e+05/5.55e+06 =  7% of the original kernel matrix.

torch.Size([2208, 2])
We keep 2.32e+05/1.85e+06 = 12% of the original kernel matrix.

torch.Size([5198, 2])
We keep 7.52e+05/1.30e+07 =  5% of the original kernel matrix.

torch.Size([58337, 2])
We keep 1.85e+08/3.37e+09 =  5% of the original kernel matrix.

torch.Size([25781, 2])
We keep 1.56e+07/5.57e+08 =  2% of the original kernel matrix.

torch.Size([104872, 2])
We keep 4.15e+08/1.40e+10 =  2% of the original kernel matrix.

torch.Size([35695, 2])
We keep 2.88e+07/1.14e+09 =  2% of the original kernel matrix.

torch.Size([11253, 2])
We keep 9.42e+06/1.41e+08 =  6% of the original kernel matrix.

torch.Size([10980, 2])
We keep 4.12e+06/1.14e+08 =  3% of the original kernel matrix.

torch.Size([3274, 2])
We keep 8.38e+05/6.09e+06 = 13% of the original kernel matrix.

torch.Size([6037, 2])
We keep 1.19e+06/2.37e+07 =  5% of the original kernel matrix.

torch.Size([19431, 2])
We keep 4.99e+07/4.45e+08 = 11% of the original kernel matrix.

torch.Size([15041, 2])
We keep 6.47e+06/2.02e+08 =  3% of the original kernel matrix.

torch.Size([65773, 2])
We keep 1.18e+08/5.18e+09 =  2% of the original kernel matrix.

torch.Size([27911, 2])
We keep 1.91e+07/6.91e+08 =  2% of the original kernel matrix.

torch.Size([1944, 2])
We keep 1.35e+05/1.23e+06 = 11% of the original kernel matrix.

torch.Size([4888, 2])
We keep 6.60e+05/1.06e+07 =  6% of the original kernel matrix.

torch.Size([6121, 2])
We keep 3.13e+06/2.86e+07 = 10% of the original kernel matrix.

torch.Size([8243, 2])
We keep 2.14e+06/5.13e+07 =  4% of the original kernel matrix.

torch.Size([16185, 2])
We keep 8.23e+06/2.43e+08 =  3% of the original kernel matrix.

torch.Size([13875, 2])
We keep 5.04e+06/1.49e+08 =  3% of the original kernel matrix.

torch.Size([3058, 2])
We keep 6.65e+05/6.43e+06 = 10% of the original kernel matrix.

torch.Size([6004, 2])
We keep 1.21e+06/2.43e+07 =  4% of the original kernel matrix.

torch.Size([10242, 2])
We keep 6.81e+06/1.02e+08 =  6% of the original kernel matrix.

torch.Size([10736, 2])
We keep 3.50e+06/9.70e+07 =  3% of the original kernel matrix.

torch.Size([26574, 2])
We keep 2.82e+07/6.52e+08 =  4% of the original kernel matrix.

torch.Size([17356, 2])
We keep 7.65e+06/2.45e+08 =  3% of the original kernel matrix.

torch.Size([10538, 2])
We keep 3.88e+06/9.02e+07 =  4% of the original kernel matrix.

torch.Size([11172, 2])
We keep 3.41e+06/9.11e+07 =  3% of the original kernel matrix.

torch.Size([3870, 2])
We keep 5.92e+05/6.15e+06 =  9% of the original kernel matrix.

torch.Size([6655, 2])
We keep 1.19e+06/2.38e+07 =  4% of the original kernel matrix.

torch.Size([1674, 2])
We keep 1.26e+05/1.09e+06 = 11% of the original kernel matrix.

torch.Size([4578, 2])
We keep 6.34e+05/9.99e+06 =  6% of the original kernel matrix.

torch.Size([14024, 2])
We keep 1.05e+07/1.95e+08 =  5% of the original kernel matrix.

torch.Size([12413, 2])
We keep 4.67e+06/1.34e+08 =  3% of the original kernel matrix.

torch.Size([13066, 2])
We keep 1.03e+07/1.44e+08 =  7% of the original kernel matrix.

torch.Size([12308, 2])
We keep 4.22e+06/1.15e+08 =  3% of the original kernel matrix.

torch.Size([9606, 2])
We keep 1.26e+07/1.41e+08 =  8% of the original kernel matrix.

torch.Size([10124, 2])
We keep 4.27e+06/1.14e+08 =  3% of the original kernel matrix.

torch.Size([12759, 2])
We keep 9.38e+06/1.77e+08 =  5% of the original kernel matrix.

torch.Size([12105, 2])
We keep 4.59e+06/1.28e+08 =  3% of the original kernel matrix.

torch.Size([95916, 2])
We keep 2.26e+08/1.13e+10 =  2% of the original kernel matrix.

torch.Size([34100, 2])
We keep 2.67e+07/1.02e+09 =  2% of the original kernel matrix.

torch.Size([4586, 2])
We keep 1.42e+06/1.25e+07 = 11% of the original kernel matrix.

torch.Size([7020, 2])
We keep 1.59e+06/3.39e+07 =  4% of the original kernel matrix.

torch.Size([1842, 2])
We keep 2.26e+05/1.22e+06 = 18% of the original kernel matrix.

torch.Size([4708, 2])
We keep 7.02e+05/1.06e+07 =  6% of the original kernel matrix.

torch.Size([6631, 2])
We keep 1.34e+06/2.14e+07 =  6% of the original kernel matrix.

torch.Size([8538, 2])
We keep 1.88e+06/4.44e+07 =  4% of the original kernel matrix.

torch.Size([20648, 2])
We keep 1.63e+07/3.80e+08 =  4% of the original kernel matrix.

torch.Size([15345, 2])
We keep 5.93e+06/1.87e+08 =  3% of the original kernel matrix.

torch.Size([14329, 2])
We keep 7.03e+06/1.87e+08 =  3% of the original kernel matrix.

torch.Size([12744, 2])
We keep 4.63e+06/1.31e+08 =  3% of the original kernel matrix.

torch.Size([2259, 2])
We keep 2.73e+05/2.29e+06 = 11% of the original kernel matrix.

torch.Size([5170, 2])
We keep 8.28e+05/1.45e+07 =  5% of the original kernel matrix.

torch.Size([1596, 2])
We keep 1.27e+05/1.03e+06 = 12% of the original kernel matrix.

torch.Size([4494, 2])
We keep 6.35e+05/9.73e+06 =  6% of the original kernel matrix.

torch.Size([24519, 2])
We keep 3.39e+07/5.50e+08 =  6% of the original kernel matrix.

torch.Size([16619, 2])
We keep 6.98e+06/2.25e+08 =  3% of the original kernel matrix.

torch.Size([3179, 2])
We keep 3.49e+05/3.98e+06 =  8% of the original kernel matrix.

torch.Size([6064, 2])
We keep 9.95e+05/1.91e+07 =  5% of the original kernel matrix.

torch.Size([9134, 2])
We keep 1.26e+07/8.54e+07 = 14% of the original kernel matrix.

torch.Size([9945, 2])
We keep 3.08e+06/8.86e+07 =  3% of the original kernel matrix.

torch.Size([2519, 2])
We keep 2.34e+05/2.37e+06 =  9% of the original kernel matrix.

torch.Size([5466, 2])
We keep 8.42e+05/1.48e+07 =  5% of the original kernel matrix.

torch.Size([7940, 2])
We keep 2.30e+06/4.52e+07 =  5% of the original kernel matrix.

torch.Size([9476, 2])
We keep 2.55e+06/6.45e+07 =  3% of the original kernel matrix.

torch.Size([8436, 2])
We keep 5.75e+06/6.16e+07 =  9% of the original kernel matrix.

torch.Size([9754, 2])
We keep 2.88e+06/7.53e+07 =  3% of the original kernel matrix.

torch.Size([5594, 2])
We keep 1.06e+06/1.68e+07 =  6% of the original kernel matrix.

torch.Size([7823, 2])
We keep 1.75e+06/3.93e+07 =  4% of the original kernel matrix.

torch.Size([3802, 2])
We keep 4.76e+05/6.27e+06 =  7% of the original kernel matrix.

torch.Size([6552, 2])
We keep 1.19e+06/2.40e+07 =  4% of the original kernel matrix.

torch.Size([56826, 2])
We keep 1.02e+08/3.02e+09 =  3% of the original kernel matrix.

torch.Size([25656, 2])
We keep 1.51e+07/5.27e+08 =  2% of the original kernel matrix.

torch.Size([65423, 2])
We keep 2.17e+08/4.08e+09 =  5% of the original kernel matrix.

torch.Size([27382, 2])
We keep 1.70e+07/6.13e+08 =  2% of the original kernel matrix.

torch.Size([49557, 2])
We keep 9.49e+07/2.45e+09 =  3% of the original kernel matrix.

torch.Size([23774, 2])
We keep 1.37e+07/4.75e+08 =  2% of the original kernel matrix.

torch.Size([47883, 2])
We keep 2.61e+08/2.91e+09 =  8% of the original kernel matrix.

torch.Size([22775, 2])
We keep 1.43e+07/5.17e+08 =  2% of the original kernel matrix.

torch.Size([13974, 2])
We keep 5.66e+06/1.48e+08 =  3% of the original kernel matrix.

torch.Size([12778, 2])
We keep 4.07e+06/1.17e+08 =  3% of the original kernel matrix.

torch.Size([7123, 2])
We keep 2.62e+06/3.21e+07 =  8% of the original kernel matrix.

torch.Size([8874, 2])
We keep 2.28e+06/5.43e+07 =  4% of the original kernel matrix.

torch.Size([6080, 2])
We keep 3.69e+06/2.88e+07 = 12% of the original kernel matrix.

torch.Size([8331, 2])
We keep 2.23e+06/5.14e+07 =  4% of the original kernel matrix.

torch.Size([52630, 2])
We keep 6.35e+07/2.43e+09 =  2% of the original kernel matrix.

torch.Size([24729, 2])
We keep 1.33e+07/4.72e+08 =  2% of the original kernel matrix.

torch.Size([4848, 2])
We keep 2.72e+06/1.74e+07 = 15% of the original kernel matrix.

torch.Size([7451, 2])
We keep 1.85e+06/4.00e+07 =  4% of the original kernel matrix.

torch.Size([8983, 2])
We keep 3.36e+06/6.24e+07 =  5% of the original kernel matrix.

torch.Size([10164, 2])
We keep 2.89e+06/7.58e+07 =  3% of the original kernel matrix.

torch.Size([4674, 2])
We keep 9.58e+05/1.20e+07 =  7% of the original kernel matrix.

torch.Size([7227, 2])
We keep 1.56e+06/3.32e+07 =  4% of the original kernel matrix.

torch.Size([33811, 2])
We keep 1.92e+08/1.73e+09 = 11% of the original kernel matrix.

torch.Size([19603, 2])
We keep 1.16e+07/3.99e+08 =  2% of the original kernel matrix.

torch.Size([19387, 2])
We keep 2.62e+07/3.55e+08 =  7% of the original kernel matrix.

torch.Size([14790, 2])
We keep 5.95e+06/1.81e+08 =  3% of the original kernel matrix.

torch.Size([10233, 2])
We keep 7.93e+06/1.07e+08 =  7% of the original kernel matrix.

torch.Size([10837, 2])
We keep 3.70e+06/9.94e+07 =  3% of the original kernel matrix.

torch.Size([55630, 2])
We keep 1.47e+08/3.44e+09 =  4% of the original kernel matrix.

torch.Size([25370, 2])
We keep 1.57e+07/5.63e+08 =  2% of the original kernel matrix.

torch.Size([23517, 2])
We keep 2.34e+07/4.72e+08 =  4% of the original kernel matrix.

torch.Size([16409, 2])
We keep 6.79e+06/2.08e+08 =  3% of the original kernel matrix.

torch.Size([9734, 2])
We keep 4.19e+06/8.01e+07 =  5% of the original kernel matrix.

torch.Size([10600, 2])
We keep 3.12e+06/8.59e+07 =  3% of the original kernel matrix.

torch.Size([39339, 2])
We keep 9.24e+07/1.79e+09 =  5% of the original kernel matrix.

torch.Size([21020, 2])
We keep 1.19e+07/4.06e+08 =  2% of the original kernel matrix.

torch.Size([60092, 2])
We keep 7.94e+07/3.27e+09 =  2% of the original kernel matrix.

torch.Size([26202, 2])
We keep 1.53e+07/5.49e+08 =  2% of the original kernel matrix.

torch.Size([23865, 2])
We keep 3.94e+07/6.44e+08 =  6% of the original kernel matrix.

torch.Size([16383, 2])
We keep 7.84e+06/2.43e+08 =  3% of the original kernel matrix.

torch.Size([5863, 2])
We keep 1.62e+06/2.24e+07 =  7% of the original kernel matrix.

torch.Size([8007, 2])
We keep 1.99e+06/4.54e+07 =  4% of the original kernel matrix.

torch.Size([30217, 2])
We keep 3.94e+07/8.98e+08 =  4% of the original kernel matrix.

torch.Size([18349, 2])
We keep 8.88e+06/2.87e+08 =  3% of the original kernel matrix.

torch.Size([5112, 2])
We keep 1.19e+06/1.45e+07 =  8% of the original kernel matrix.

torch.Size([7592, 2])
We keep 1.60e+06/3.66e+07 =  4% of the original kernel matrix.

torch.Size([29208, 2])
We keep 2.65e+07/7.46e+08 =  3% of the original kernel matrix.

torch.Size([17888, 2])
We keep 8.07e+06/2.62e+08 =  3% of the original kernel matrix.

torch.Size([223328, 2])
We keep 1.76e+09/6.41e+10 =  2% of the original kernel matrix.

torch.Size([51277, 2])
We keep 5.91e+07/2.43e+09 =  2% of the original kernel matrix.

torch.Size([23217, 2])
We keep 1.05e+08/6.97e+08 = 15% of the original kernel matrix.

torch.Size([16178, 2])
We keep 7.98e+06/2.53e+08 =  3% of the original kernel matrix.

torch.Size([6677, 2])
We keep 2.56e+07/1.04e+08 = 24% of the original kernel matrix.

torch.Size([8554, 2])
We keep 3.62e+06/9.79e+07 =  3% of the original kernel matrix.

torch.Size([5741, 2])
We keep 1.41e+06/1.86e+07 =  7% of the original kernel matrix.

torch.Size([8059, 2])
We keep 1.87e+06/4.14e+07 =  4% of the original kernel matrix.

torch.Size([7696, 2])
We keep 1.50e+07/5.75e+07 = 26% of the original kernel matrix.

torch.Size([9306, 2])
We keep 2.75e+06/7.27e+07 =  3% of the original kernel matrix.

torch.Size([5856, 2])
We keep 2.25e+06/2.05e+07 = 10% of the original kernel matrix.

torch.Size([8214, 2])
We keep 1.96e+06/4.34e+07 =  4% of the original kernel matrix.

torch.Size([4696, 2])
We keep 1.61e+06/1.66e+07 =  9% of the original kernel matrix.

torch.Size([7213, 2])
We keep 1.76e+06/3.90e+07 =  4% of the original kernel matrix.

torch.Size([11016, 2])
We keep 1.17e+07/1.15e+08 = 10% of the original kernel matrix.

torch.Size([11304, 2])
We keep 3.72e+06/1.03e+08 =  3% of the original kernel matrix.

torch.Size([123418, 2])
We keep 3.43e+08/1.82e+10 =  1% of the original kernel matrix.

torch.Size([39286, 2])
We keep 3.34e+07/1.29e+09 =  2% of the original kernel matrix.

torch.Size([1752, 2])
We keep 1.77e+05/1.11e+06 = 15% of the original kernel matrix.

torch.Size([4723, 2])
We keep 6.36e+05/1.01e+07 =  6% of the original kernel matrix.

torch.Size([61392, 2])
We keep 8.47e+08/9.07e+09 =  9% of the original kernel matrix.

torch.Size([25894, 2])
We keep 2.35e+07/9.14e+08 =  2% of the original kernel matrix.

torch.Size([12091, 2])
We keep 6.99e+06/1.21e+08 =  5% of the original kernel matrix.

torch.Size([11800, 2])
We keep 3.78e+06/1.06e+08 =  3% of the original kernel matrix.

torch.Size([12216, 2])
We keep 5.43e+06/1.25e+08 =  4% of the original kernel matrix.

torch.Size([11811, 2])
We keep 3.90e+06/1.07e+08 =  3% of the original kernel matrix.

torch.Size([4841, 2])
We keep 2.78e+06/1.56e+07 = 17% of the original kernel matrix.

torch.Size([7246, 2])
We keep 1.69e+06/3.78e+07 =  4% of the original kernel matrix.

torch.Size([33825, 2])
We keep 3.56e+07/1.01e+09 =  3% of the original kernel matrix.

torch.Size([19479, 2])
We keep 9.41e+06/3.05e+08 =  3% of the original kernel matrix.

torch.Size([83676, 2])
We keep 1.48e+08/6.70e+09 =  2% of the original kernel matrix.

torch.Size([31369, 2])
We keep 2.11e+07/7.85e+08 =  2% of the original kernel matrix.

torch.Size([2678, 2])
We keep 5.61e+05/3.44e+06 = 16% of the original kernel matrix.

torch.Size([5571, 2])
We keep 9.53e+05/1.78e+07 =  5% of the original kernel matrix.

torch.Size([292096, 2])
We keep 1.52e+09/8.54e+10 =  1% of the original kernel matrix.

torch.Size([61091, 2])
We keep 6.64e+07/2.80e+09 =  2% of the original kernel matrix.

torch.Size([14856, 2])
We keep 1.05e+07/2.09e+08 =  4% of the original kernel matrix.

torch.Size([13111, 2])
We keep 4.65e+06/1.39e+08 =  3% of the original kernel matrix.

torch.Size([23363, 2])
We keep 6.93e+07/6.73e+08 = 10% of the original kernel matrix.

torch.Size([15854, 2])
We keep 7.86e+06/2.49e+08 =  3% of the original kernel matrix.

torch.Size([5746, 2])
We keep 4.08e+06/3.39e+07 = 12% of the original kernel matrix.

torch.Size([8052, 2])
We keep 2.21e+06/5.58e+07 =  3% of the original kernel matrix.

torch.Size([6562, 2])
We keep 2.08e+06/3.02e+07 =  6% of the original kernel matrix.

torch.Size([8520, 2])
We keep 2.24e+06/5.27e+07 =  4% of the original kernel matrix.

torch.Size([1517, 2])
We keep 9.66e+04/7.74e+05 = 12% of the original kernel matrix.

torch.Size([4501, 2])
We keep 5.54e+05/8.44e+06 =  6% of the original kernel matrix.

torch.Size([1201, 2])
We keep 6.90e+04/4.46e+05 = 15% of the original kernel matrix.

torch.Size([3991, 2])
We keep 4.57e+05/6.41e+06 =  7% of the original kernel matrix.

torch.Size([5808, 2])
We keep 1.77e+06/2.22e+07 =  7% of the original kernel matrix.

torch.Size([8066, 2])
We keep 1.98e+06/4.52e+07 =  4% of the original kernel matrix.

torch.Size([42596, 2])
We keep 1.91e+08/2.32e+09 =  8% of the original kernel matrix.

torch.Size([21505, 2])
We keep 1.32e+07/4.62e+08 =  2% of the original kernel matrix.

torch.Size([74843, 2])
We keep 6.54e+09/7.86e+10 =  8% of the original kernel matrix.

torch.Size([24527, 2])
We keep 6.43e+07/2.69e+09 =  2% of the original kernel matrix.

torch.Size([4418, 2])
We keep 1.72e+06/1.26e+07 = 13% of the original kernel matrix.

torch.Size([6982, 2])
We keep 1.65e+06/3.41e+07 =  4% of the original kernel matrix.

torch.Size([47571, 2])
We keep 8.29e+07/1.97e+09 =  4% of the original kernel matrix.

torch.Size([23175, 2])
We keep 1.21e+07/4.26e+08 =  2% of the original kernel matrix.

torch.Size([4050, 2])
We keep 1.41e+06/9.40e+06 = 15% of the original kernel matrix.

torch.Size([6762, 2])
We keep 1.35e+06/2.94e+07 =  4% of the original kernel matrix.

torch.Size([12085, 2])
We keep 5.96e+06/1.36e+08 =  4% of the original kernel matrix.

torch.Size([11782, 2])
We keep 4.09e+06/1.12e+08 =  3% of the original kernel matrix.

torch.Size([3781, 2])
We keep 1.88e+06/9.36e+06 = 20% of the original kernel matrix.

torch.Size([6386, 2])
We keep 1.34e+06/2.94e+07 =  4% of the original kernel matrix.

torch.Size([29768, 2])
We keep 2.18e+08/1.39e+09 = 15% of the original kernel matrix.

torch.Size([18073, 2])
We keep 1.07e+07/3.58e+08 =  2% of the original kernel matrix.

torch.Size([4259, 2])
We keep 9.90e+05/1.02e+07 =  9% of the original kernel matrix.

torch.Size([6836, 2])
We keep 1.49e+06/3.06e+07 =  4% of the original kernel matrix.

torch.Size([448778, 2])
We keep 4.65e+09/2.08e+11 =  2% of the original kernel matrix.

torch.Size([76735, 2])
We keep 1.03e+08/4.37e+09 =  2% of the original kernel matrix.

torch.Size([3629, 2])
We keep 6.11e+05/7.25e+06 =  8% of the original kernel matrix.

torch.Size([6393, 2])
We keep 1.22e+06/2.58e+07 =  4% of the original kernel matrix.

torch.Size([33380, 2])
We keep 7.63e+07/1.17e+09 =  6% of the original kernel matrix.

torch.Size([19492, 2])
We keep 9.84e+06/3.28e+08 =  2% of the original kernel matrix.

torch.Size([8508, 2])
We keep 4.95e+06/5.72e+07 =  8% of the original kernel matrix.

torch.Size([9884, 2])
We keep 2.72e+06/7.26e+07 =  3% of the original kernel matrix.

torch.Size([33856, 2])
We keep 6.65e+07/1.16e+09 =  5% of the original kernel matrix.

torch.Size([19425, 2])
We keep 1.01e+07/3.26e+08 =  3% of the original kernel matrix.

torch.Size([131269, 2])
We keep 4.20e+08/1.91e+10 =  2% of the original kernel matrix.

torch.Size([39337, 2])
We keep 3.39e+07/1.33e+09 =  2% of the original kernel matrix.

torch.Size([11942, 2])
We keep 5.38e+06/1.31e+08 =  4% of the original kernel matrix.

torch.Size([11532, 2])
We keep 4.00e+06/1.10e+08 =  3% of the original kernel matrix.

torch.Size([3048, 2])
We keep 4.96e+05/4.48e+06 = 11% of the original kernel matrix.

torch.Size([5883, 2])
We keep 1.07e+06/2.03e+07 =  5% of the original kernel matrix.

torch.Size([48981, 2])
We keep 6.21e+07/2.12e+09 =  2% of the original kernel matrix.

torch.Size([23628, 2])
We keep 1.28e+07/4.41e+08 =  2% of the original kernel matrix.

torch.Size([5330, 2])
We keep 1.51e+06/1.64e+07 =  9% of the original kernel matrix.

torch.Size([7731, 2])
We keep 1.74e+06/3.89e+07 =  4% of the original kernel matrix.

torch.Size([5855, 2])
We keep 4.90e+06/3.86e+07 = 12% of the original kernel matrix.

torch.Size([7879, 2])
We keep 2.40e+06/5.96e+07 =  4% of the original kernel matrix.

torch.Size([63301, 2])
We keep 1.01e+08/3.74e+09 =  2% of the original kernel matrix.

torch.Size([26692, 2])
We keep 1.63e+07/5.87e+08 =  2% of the original kernel matrix.

torch.Size([43503, 2])
We keep 4.87e+08/5.95e+09 =  8% of the original kernel matrix.

torch.Size([21697, 2])
We keep 1.91e+07/7.40e+08 =  2% of the original kernel matrix.

torch.Size([8971, 2])
We keep 5.11e+06/6.39e+07 =  7% of the original kernel matrix.

torch.Size([10062, 2])
We keep 3.02e+06/7.67e+07 =  3% of the original kernel matrix.

torch.Size([16702, 2])
We keep 1.05e+07/2.60e+08 =  4% of the original kernel matrix.

torch.Size([13485, 2])
We keep 5.26e+06/1.55e+08 =  3% of the original kernel matrix.

torch.Size([40191, 2])
We keep 1.37e+08/2.06e+09 =  6% of the original kernel matrix.

torch.Size([20843, 2])
We keep 1.24e+07/4.35e+08 =  2% of the original kernel matrix.

torch.Size([35067, 2])
We keep 2.35e+08/1.99e+09 = 11% of the original kernel matrix.

torch.Size([19586, 2])
We keep 1.23e+07/4.27e+08 =  2% of the original kernel matrix.

torch.Size([13629, 2])
We keep 6.88e+06/1.77e+08 =  3% of the original kernel matrix.

torch.Size([12629, 2])
We keep 4.47e+06/1.28e+08 =  3% of the original kernel matrix.

torch.Size([3133, 2])
We keep 4.08e+05/4.38e+06 =  9% of the original kernel matrix.

torch.Size([6031, 2])
We keep 1.04e+06/2.01e+07 =  5% of the original kernel matrix.

torch.Size([67419, 2])
We keep 3.97e+08/8.29e+09 =  4% of the original kernel matrix.

torch.Size([26814, 2])
We keep 2.35e+07/8.73e+08 =  2% of the original kernel matrix.

torch.Size([12187, 2])
We keep 1.02e+07/1.72e+08 =  5% of the original kernel matrix.

torch.Size([11500, 2])
We keep 4.35e+06/1.26e+08 =  3% of the original kernel matrix.

torch.Size([16050, 2])
We keep 1.88e+07/3.15e+08 =  5% of the original kernel matrix.

torch.Size([13506, 2])
We keep 5.94e+06/1.70e+08 =  3% of the original kernel matrix.

torch.Size([8751, 2])
We keep 7.72e+06/7.12e+07 = 10% of the original kernel matrix.

torch.Size([9851, 2])
We keep 2.95e+06/8.09e+07 =  3% of the original kernel matrix.

torch.Size([7544, 2])
We keep 1.88e+07/1.03e+08 = 18% of the original kernel matrix.

torch.Size([8926, 2])
We keep 3.67e+06/9.71e+07 =  3% of the original kernel matrix.

torch.Size([19748, 2])
We keep 3.95e+07/6.11e+08 =  6% of the original kernel matrix.

torch.Size([14646, 2])
We keep 7.73e+06/2.37e+08 =  3% of the original kernel matrix.

torch.Size([87948, 2])
We keep 1.27e+09/1.78e+10 =  7% of the original kernel matrix.

torch.Size([30024, 2])
We keep 3.17e+07/1.28e+09 =  2% of the original kernel matrix.

torch.Size([14308, 2])
We keep 1.54e+07/2.03e+08 =  7% of the original kernel matrix.

torch.Size([12786, 2])
We keep 4.74e+06/1.37e+08 =  3% of the original kernel matrix.

torch.Size([9429, 2])
We keep 3.52e+06/6.42e+07 =  5% of the original kernel matrix.

torch.Size([10103, 2])
We keep 2.93e+06/7.69e+07 =  3% of the original kernel matrix.

torch.Size([1854, 2])
We keep 2.75e+05/1.51e+06 = 18% of the original kernel matrix.

torch.Size([4775, 2])
We keep 7.08e+05/1.18e+07 =  6% of the original kernel matrix.

torch.Size([13823, 2])
We keep 6.00e+07/2.84e+08 = 21% of the original kernel matrix.

torch.Size([12362, 2])
We keep 6.02e+06/1.62e+08 =  3% of the original kernel matrix.

torch.Size([12363, 2])
We keep 2.25e+07/1.81e+08 = 12% of the original kernel matrix.

torch.Size([11417, 2])
We keep 4.54e+06/1.29e+08 =  3% of the original kernel matrix.

torch.Size([24303, 2])
We keep 4.12e+07/7.04e+08 =  5% of the original kernel matrix.

torch.Size([15970, 2])
We keep 7.90e+06/2.54e+08 =  3% of the original kernel matrix.

torch.Size([52102, 2])
We keep 8.76e+07/2.54e+09 =  3% of the original kernel matrix.

torch.Size([24257, 2])
We keep 1.36e+07/4.83e+08 =  2% of the original kernel matrix.

torch.Size([4685, 2])
We keep 9.79e+05/1.07e+07 =  9% of the original kernel matrix.

torch.Size([7180, 2])
We keep 1.49e+06/3.13e+07 =  4% of the original kernel matrix.

torch.Size([10898, 2])
We keep 4.35e+06/1.02e+08 =  4% of the original kernel matrix.

torch.Size([11188, 2])
We keep 3.57e+06/9.69e+07 =  3% of the original kernel matrix.

torch.Size([19769, 2])
We keep 3.18e+07/4.39e+08 =  7% of the original kernel matrix.

torch.Size([14857, 2])
We keep 6.68e+06/2.01e+08 =  3% of the original kernel matrix.

torch.Size([33099, 2])
We keep 6.76e+07/9.44e+08 =  7% of the original kernel matrix.

torch.Size([19374, 2])
We keep 9.19e+06/2.95e+08 =  3% of the original kernel matrix.

torch.Size([13312, 2])
We keep 9.09e+06/1.91e+08 =  4% of the original kernel matrix.

torch.Size([12420, 2])
We keep 4.66e+06/1.33e+08 =  3% of the original kernel matrix.

torch.Size([40512, 2])
We keep 4.95e+07/1.33e+09 =  3% of the original kernel matrix.

torch.Size([21160, 2])
We keep 1.02e+07/3.50e+08 =  2% of the original kernel matrix.

torch.Size([116650, 2])
We keep 2.48e+09/3.77e+10 =  6% of the original kernel matrix.

torch.Size([33249, 2])
We keep 4.90e+07/1.86e+09 =  2% of the original kernel matrix.

torch.Size([19056, 2])
We keep 1.28e+07/3.40e+08 =  3% of the original kernel matrix.

torch.Size([14903, 2])
We keep 5.69e+06/1.77e+08 =  3% of the original kernel matrix.

torch.Size([1434, 2])
We keep 8.56e+04/6.64e+05 = 12% of the original kernel matrix.

torch.Size([4388, 2])
We keep 5.13e+05/7.82e+06 =  6% of the original kernel matrix.

torch.Size([3494, 2])
We keep 5.15e+05/5.31e+06 =  9% of the original kernel matrix.

torch.Size([6323, 2])
We keep 1.12e+06/2.21e+07 =  5% of the original kernel matrix.

torch.Size([86699, 2])
We keep 1.64e+08/7.25e+09 =  2% of the original kernel matrix.

torch.Size([32309, 2])
We keep 2.15e+07/8.17e+08 =  2% of the original kernel matrix.

torch.Size([13755, 2])
We keep 1.64e+07/2.17e+08 =  7% of the original kernel matrix.

torch.Size([12214, 2])
We keep 4.90e+06/1.41e+08 =  3% of the original kernel matrix.

torch.Size([36765, 2])
We keep 3.41e+07/1.18e+09 =  2% of the original kernel matrix.

torch.Size([20152, 2])
We keep 9.85e+06/3.30e+08 =  2% of the original kernel matrix.

torch.Size([2508, 2])
We keep 2.26e+05/2.32e+06 =  9% of the original kernel matrix.

torch.Size([5417, 2])
We keep 8.29e+05/1.46e+07 =  5% of the original kernel matrix.

torch.Size([1684, 2])
We keep 1.04e+05/8.84e+05 = 11% of the original kernel matrix.

torch.Size([4659, 2])
We keep 5.91e+05/9.02e+06 =  6% of the original kernel matrix.

torch.Size([44375, 2])
We keep 1.03e+08/2.46e+09 =  4% of the original kernel matrix.

torch.Size([22152, 2])
We keep 1.42e+07/4.76e+08 =  2% of the original kernel matrix.

torch.Size([27236, 2])
We keep 3.47e+07/7.61e+08 =  4% of the original kernel matrix.

torch.Size([17222, 2])
We keep 8.15e+06/2.65e+08 =  3% of the original kernel matrix.

torch.Size([23439, 2])
We keep 2.08e+07/5.60e+08 =  3% of the original kernel matrix.

torch.Size([16307, 2])
We keep 7.22e+06/2.27e+08 =  3% of the original kernel matrix.

torch.Size([2976, 2])
We keep 4.20e+05/4.54e+06 =  9% of the original kernel matrix.

torch.Size([5923, 2])
We keep 1.02e+06/2.04e+07 =  5% of the original kernel matrix.

torch.Size([3829, 2])
We keep 6.82e+05/6.68e+06 = 10% of the original kernel matrix.

torch.Size([6601, 2])
We keep 1.23e+06/2.48e+07 =  4% of the original kernel matrix.

torch.Size([10050, 2])
We keep 4.94e+06/8.04e+07 =  6% of the original kernel matrix.

torch.Size([10686, 2])
We keep 3.27e+06/8.60e+07 =  3% of the original kernel matrix.

torch.Size([14410, 2])
We keep 1.65e+07/1.94e+08 =  8% of the original kernel matrix.

torch.Size([12785, 2])
We keep 4.73e+06/1.33e+08 =  3% of the original kernel matrix.

torch.Size([18085, 2])
We keep 4.78e+07/3.53e+08 = 13% of the original kernel matrix.

torch.Size([14442, 2])
We keep 6.00e+06/1.80e+08 =  3% of the original kernel matrix.

torch.Size([4694, 2])
We keep 1.35e+06/1.46e+07 =  9% of the original kernel matrix.

torch.Size([7181, 2])
We keep 1.62e+06/3.66e+07 =  4% of the original kernel matrix.

torch.Size([12117, 2])
We keep 4.64e+08/1.46e+09 = 31% of the original kernel matrix.

torch.Size([10675, 2])
We keep 1.11e+07/3.66e+08 =  3% of the original kernel matrix.

torch.Size([6259, 2])
We keep 4.08e+06/3.51e+07 = 11% of the original kernel matrix.

torch.Size([8314, 2])
We keep 2.34e+06/5.68e+07 =  4% of the original kernel matrix.

torch.Size([7741, 2])
We keep 8.38e+06/5.98e+07 = 14% of the original kernel matrix.

torch.Size([9306, 2])
We keep 2.80e+06/7.42e+07 =  3% of the original kernel matrix.

torch.Size([75475, 2])
We keep 3.30e+08/7.23e+09 =  4% of the original kernel matrix.

torch.Size([30088, 2])
We keep 2.08e+07/8.16e+08 =  2% of the original kernel matrix.

torch.Size([6317, 2])
We keep 2.41e+06/2.75e+07 =  8% of the original kernel matrix.

torch.Size([8524, 2])
We keep 2.10e+06/5.03e+07 =  4% of the original kernel matrix.

torch.Size([15005, 2])
We keep 1.13e+07/2.10e+08 =  5% of the original kernel matrix.

torch.Size([12938, 2])
We keep 4.87e+06/1.39e+08 =  3% of the original kernel matrix.

torch.Size([61603, 2])
We keep 1.59e+08/3.27e+09 =  4% of the original kernel matrix.

torch.Size([26776, 2])
We keep 1.52e+07/5.49e+08 =  2% of the original kernel matrix.

torch.Size([12641, 2])
We keep 7.47e+06/1.25e+08 =  5% of the original kernel matrix.

torch.Size([12161, 2])
We keep 3.87e+06/1.07e+08 =  3% of the original kernel matrix.

torch.Size([21683, 2])
We keep 3.28e+07/4.80e+08 =  6% of the original kernel matrix.

torch.Size([15897, 2])
We keep 6.79e+06/2.10e+08 =  3% of the original kernel matrix.

torch.Size([58430, 2])
We keep 1.33e+08/4.18e+09 =  3% of the original kernel matrix.

torch.Size([26146, 2])
We keep 1.75e+07/6.20e+08 =  2% of the original kernel matrix.

torch.Size([2036, 2])
We keep 1.50e+05/1.29e+06 = 11% of the original kernel matrix.

torch.Size([5090, 2])
We keep 6.85e+05/1.09e+07 =  6% of the original kernel matrix.

torch.Size([19044, 2])
We keep 1.37e+07/3.49e+08 =  3% of the original kernel matrix.

torch.Size([14742, 2])
We keep 5.90e+06/1.79e+08 =  3% of the original kernel matrix.

torch.Size([11211, 2])
We keep 3.99e+06/9.46e+07 =  4% of the original kernel matrix.

torch.Size([11517, 2])
We keep 3.48e+06/9.33e+07 =  3% of the original kernel matrix.

torch.Size([115434, 2])
We keep 7.76e+08/1.52e+10 =  5% of the original kernel matrix.

torch.Size([37586, 2])
We keep 3.07e+07/1.18e+09 =  2% of the original kernel matrix.

torch.Size([57426, 2])
We keep 9.50e+07/3.48e+09 =  2% of the original kernel matrix.

torch.Size([25536, 2])
We keep 1.59e+07/5.66e+08 =  2% of the original kernel matrix.

torch.Size([9374, 2])
We keep 1.88e+07/1.45e+08 = 12% of the original kernel matrix.

torch.Size([9914, 2])
We keep 4.11e+06/1.15e+08 =  3% of the original kernel matrix.

torch.Size([24774, 2])
We keep 2.01e+07/4.97e+08 =  4% of the original kernel matrix.

torch.Size([16824, 2])
We keep 6.72e+06/2.14e+08 =  3% of the original kernel matrix.

torch.Size([7850, 2])
We keep 2.36e+06/4.59e+07 =  5% of the original kernel matrix.

torch.Size([9385, 2])
We keep 2.63e+06/6.50e+07 =  4% of the original kernel matrix.

torch.Size([3073, 2])
We keep 5.59e+06/2.23e+07 = 25% of the original kernel matrix.

torch.Size([5632, 2])
We keep 2.01e+06/4.53e+07 =  4% of the original kernel matrix.

torch.Size([4962, 2])
We keep 2.86e+06/1.71e+07 = 16% of the original kernel matrix.

torch.Size([7442, 2])
We keep 1.65e+06/3.97e+07 =  4% of the original kernel matrix.

torch.Size([5289, 2])
We keep 1.91e+06/2.14e+07 =  8% of the original kernel matrix.

torch.Size([7501, 2])
We keep 1.95e+06/4.44e+07 =  4% of the original kernel matrix.

torch.Size([51190, 2])
We keep 8.59e+07/2.50e+09 =  3% of the original kernel matrix.

torch.Size([23984, 2])
We keep 1.40e+07/4.79e+08 =  2% of the original kernel matrix.

torch.Size([34148, 2])
We keep 1.22e+08/1.45e+09 =  8% of the original kernel matrix.

torch.Size([19661, 2])
We keep 1.09e+07/3.66e+08 =  2% of the original kernel matrix.

torch.Size([1823, 2])
We keep 1.87e+05/1.33e+06 = 14% of the original kernel matrix.

torch.Size([4730, 2])
We keep 6.74e+05/1.10e+07 =  6% of the original kernel matrix.

torch.Size([12414, 2])
We keep 6.37e+06/1.35e+08 =  4% of the original kernel matrix.

torch.Size([12007, 2])
We keep 3.94e+06/1.11e+08 =  3% of the original kernel matrix.

torch.Size([7085, 2])
We keep 1.59e+06/2.81e+07 =  5% of the original kernel matrix.

torch.Size([8967, 2])
We keep 2.10e+06/5.08e+07 =  4% of the original kernel matrix.

torch.Size([86335, 2])
We keep 3.56e+08/1.13e+10 =  3% of the original kernel matrix.

torch.Size([31777, 2])
We keep 2.77e+07/1.02e+09 =  2% of the original kernel matrix.

torch.Size([15315, 2])
We keep 6.84e+06/1.89e+08 =  3% of the original kernel matrix.

torch.Size([13388, 2])
We keep 4.61e+06/1.32e+08 =  3% of the original kernel matrix.

torch.Size([6338, 2])
We keep 1.56e+06/2.22e+07 =  7% of the original kernel matrix.

torch.Size([8310, 2])
We keep 2.00e+06/4.52e+07 =  4% of the original kernel matrix.

torch.Size([25728, 2])
We keep 3.23e+07/7.22e+08 =  4% of the original kernel matrix.

torch.Size([17056, 2])
We keep 8.31e+06/2.58e+08 =  3% of the original kernel matrix.

torch.Size([34427, 2])
We keep 3.17e+07/1.06e+09 =  2% of the original kernel matrix.

torch.Size([19404, 2])
We keep 9.37e+06/3.12e+08 =  3% of the original kernel matrix.

torch.Size([5381, 2])
We keep 5.25e+06/2.51e+07 = 20% of the original kernel matrix.

torch.Size([7663, 2])
We keep 2.10e+06/4.80e+07 =  4% of the original kernel matrix.

torch.Size([20003, 2])
We keep 1.78e+07/3.48e+08 =  5% of the original kernel matrix.

torch.Size([14985, 2])
We keep 5.83e+06/1.79e+08 =  3% of the original kernel matrix.

torch.Size([7627, 2])
We keep 6.13e+06/4.15e+07 = 14% of the original kernel matrix.

torch.Size([9270, 2])
We keep 2.44e+06/6.18e+07 =  3% of the original kernel matrix.

time for making ranges is 1.7543649673461914
Sorting X and nu_X
time for sorting X is 0.06528258323669434
Sorting Z and nu_Z
time for sorting Z is 0.00026226043701171875
Starting Optim
sum tnu_Z before tensor(18016548., device='cuda:0')
c= tensor(1049.5090, device='cuda:0')
c= tensor(35612.7422, device='cuda:0')
c= tensor(38815.4961, device='cuda:0')
c= tensor(42776.3633, device='cuda:0')
c= tensor(1133271.6250, device='cuda:0')
c= tensor(1393245.7500, device='cuda:0')
c= tensor(1654737., device='cuda:0')
c= tensor(1929568.7500, device='cuda:0')
c= tensor(1949212.7500, device='cuda:0')
c= tensor(26412092., device='cuda:0')
c= tensor(26422820., device='cuda:0')
c= tensor(29386340., device='cuda:0')
c= tensor(29408194., device='cuda:0')
c= tensor(32735016., device='cuda:0')
c= tensor(32829600., device='cuda:0')
c= tensor(33177210., device='cuda:0')
c= tensor(33608344., device='cuda:0')
c= tensor(34091856., device='cuda:0')
c= tensor(43970440., device='cuda:0')
c= tensor(45702964., device='cuda:0')
c= tensor(45794076., device='cuda:0')
c= tensor(1.2356e+08, device='cuda:0')
c= tensor(1.2359e+08, device='cuda:0')
c= tensor(1.2361e+08, device='cuda:0')
c= tensor(1.2491e+08, device='cuda:0')
c= tensor(1.2552e+08, device='cuda:0')
c= tensor(1.2570e+08, device='cuda:0')
c= tensor(1.2576e+08, device='cuda:0')
c= tensor(1.2809e+08, device='cuda:0')
c= tensor(6.0086e+08, device='cuda:0')
c= tensor(6.0088e+08, device='cuda:0')
c= tensor(6.1683e+08, device='cuda:0')
c= tensor(6.1685e+08, device='cuda:0')
c= tensor(6.1687e+08, device='cuda:0')
c= tensor(6.1688e+08, device='cuda:0')
c= tensor(6.1866e+08, device='cuda:0')
c= tensor(6.1947e+08, device='cuda:0')
c= tensor(6.1947e+08, device='cuda:0')
c= tensor(6.1947e+08, device='cuda:0')
c= tensor(6.1947e+08, device='cuda:0')
c= tensor(6.1948e+08, device='cuda:0')
c= tensor(6.1948e+08, device='cuda:0')
c= tensor(6.1948e+08, device='cuda:0')
c= tensor(6.1949e+08, device='cuda:0')
c= tensor(6.1949e+08, device='cuda:0')
c= tensor(6.1949e+08, device='cuda:0')
c= tensor(6.1950e+08, device='cuda:0')
c= tensor(6.1950e+08, device='cuda:0')
c= tensor(6.1951e+08, device='cuda:0')
c= tensor(6.1951e+08, device='cuda:0')
c= tensor(6.1953e+08, device='cuda:0')
c= tensor(6.1953e+08, device='cuda:0')
c= tensor(6.1953e+08, device='cuda:0')
c= tensor(6.1954e+08, device='cuda:0')
c= tensor(6.1955e+08, device='cuda:0')
c= tensor(6.1956e+08, device='cuda:0')
c= tensor(6.1956e+08, device='cuda:0')
c= tensor(6.1956e+08, device='cuda:0')
c= tensor(6.1956e+08, device='cuda:0')
c= tensor(6.1957e+08, device='cuda:0')
c= tensor(6.1960e+08, device='cuda:0')
c= tensor(6.1960e+08, device='cuda:0')
c= tensor(6.1961e+08, device='cuda:0')
c= tensor(6.1962e+08, device='cuda:0')
c= tensor(6.1963e+08, device='cuda:0')
c= tensor(6.1963e+08, device='cuda:0')
c= tensor(6.1963e+08, device='cuda:0')
c= tensor(6.1964e+08, device='cuda:0')
c= tensor(6.1964e+08, device='cuda:0')
c= tensor(6.1965e+08, device='cuda:0')
c= tensor(6.1968e+08, device='cuda:0')
c= tensor(6.1968e+08, device='cuda:0')
c= tensor(6.1969e+08, device='cuda:0')
c= tensor(6.1969e+08, device='cuda:0')
c= tensor(6.1969e+08, device='cuda:0')
c= tensor(6.1970e+08, device='cuda:0')
c= tensor(6.1970e+08, device='cuda:0')
c= tensor(6.1970e+08, device='cuda:0')
c= tensor(6.1970e+08, device='cuda:0')
c= tensor(6.1974e+08, device='cuda:0')
c= tensor(6.1974e+08, device='cuda:0')
c= tensor(6.1974e+08, device='cuda:0')
c= tensor(6.1975e+08, device='cuda:0')
c= tensor(6.1975e+08, device='cuda:0')
c= tensor(6.1975e+08, device='cuda:0')
c= tensor(6.1975e+08, device='cuda:0')
c= tensor(6.1975e+08, device='cuda:0')
c= tensor(6.1975e+08, device='cuda:0')
c= tensor(6.1976e+08, device='cuda:0')
c= tensor(6.1976e+08, device='cuda:0')
c= tensor(6.1977e+08, device='cuda:0')
c= tensor(6.1977e+08, device='cuda:0')
c= tensor(6.1977e+08, device='cuda:0')
c= tensor(6.1978e+08, device='cuda:0')
c= tensor(6.1978e+08, device='cuda:0')
c= tensor(6.1979e+08, device='cuda:0')
c= tensor(6.1979e+08, device='cuda:0')
c= tensor(6.1980e+08, device='cuda:0')
c= tensor(6.1981e+08, device='cuda:0')
c= tensor(6.1981e+08, device='cuda:0')
c= tensor(6.1983e+08, device='cuda:0')
c= tensor(6.1984e+08, device='cuda:0')
c= tensor(6.1984e+08, device='cuda:0')
c= tensor(6.1984e+08, device='cuda:0')
c= tensor(6.1985e+08, device='cuda:0')
c= tensor(6.1985e+08, device='cuda:0')
c= tensor(6.1987e+08, device='cuda:0')
c= tensor(6.1987e+08, device='cuda:0')
c= tensor(6.1987e+08, device='cuda:0')
c= tensor(6.1987e+08, device='cuda:0')
c= tensor(6.1987e+08, device='cuda:0')
c= tensor(6.1987e+08, device='cuda:0')
c= tensor(6.1988e+08, device='cuda:0')
c= tensor(6.1988e+08, device='cuda:0')
c= tensor(6.1988e+08, device='cuda:0')
c= tensor(6.1989e+08, device='cuda:0')
c= tensor(6.1989e+08, device='cuda:0')
c= tensor(6.1989e+08, device='cuda:0')
c= tensor(6.1990e+08, device='cuda:0')
c= tensor(6.1990e+08, device='cuda:0')
c= tensor(6.1995e+08, device='cuda:0')
c= tensor(6.1995e+08, device='cuda:0')
c= tensor(6.1995e+08, device='cuda:0')
c= tensor(6.1996e+08, device='cuda:0')
c= tensor(6.1996e+08, device='cuda:0')
c= tensor(6.1996e+08, device='cuda:0')
c= tensor(6.1996e+08, device='cuda:0')
c= tensor(6.1997e+08, device='cuda:0')
c= tensor(6.1998e+08, device='cuda:0')
c= tensor(6.1998e+08, device='cuda:0')
c= tensor(6.2000e+08, device='cuda:0')
c= tensor(6.2000e+08, device='cuda:0')
c= tensor(6.2001e+08, device='cuda:0')
c= tensor(6.2001e+08, device='cuda:0')
c= tensor(6.2001e+08, device='cuda:0')
c= tensor(6.2001e+08, device='cuda:0')
c= tensor(6.2002e+08, device='cuda:0')
c= tensor(6.2002e+08, device='cuda:0')
c= tensor(6.2002e+08, device='cuda:0')
c= tensor(6.2002e+08, device='cuda:0')
c= tensor(6.2002e+08, device='cuda:0')
c= tensor(6.2002e+08, device='cuda:0')
c= tensor(6.2003e+08, device='cuda:0')
c= tensor(6.2011e+08, device='cuda:0')
c= tensor(6.2011e+08, device='cuda:0')
c= tensor(6.2011e+08, device='cuda:0')
c= tensor(6.2011e+08, device='cuda:0')
c= tensor(6.2012e+08, device='cuda:0')
c= tensor(6.2012e+08, device='cuda:0')
c= tensor(6.2012e+08, device='cuda:0')
c= tensor(6.2012e+08, device='cuda:0')
c= tensor(6.2013e+08, device='cuda:0')
c= tensor(6.2013e+08, device='cuda:0')
c= tensor(6.2015e+08, device='cuda:0')
c= tensor(6.2016e+08, device='cuda:0')
c= tensor(6.2019e+08, device='cuda:0')
c= tensor(6.2019e+08, device='cuda:0')
c= tensor(6.2019e+08, device='cuda:0')
c= tensor(6.2019e+08, device='cuda:0')
c= tensor(6.2019e+08, device='cuda:0')
c= tensor(6.2020e+08, device='cuda:0')
c= tensor(6.2020e+08, device='cuda:0')
c= tensor(6.2021e+08, device='cuda:0')
c= tensor(6.2021e+08, device='cuda:0')
c= tensor(6.2021e+08, device='cuda:0')
c= tensor(6.2021e+08, device='cuda:0')
c= tensor(6.2022e+08, device='cuda:0')
c= tensor(6.2022e+08, device='cuda:0')
c= tensor(6.2022e+08, device='cuda:0')
c= tensor(6.2022e+08, device='cuda:0')
c= tensor(6.2022e+08, device='cuda:0')
c= tensor(6.2022e+08, device='cuda:0')
c= tensor(6.2023e+08, device='cuda:0')
c= tensor(6.2023e+08, device='cuda:0')
c= tensor(6.2024e+08, device='cuda:0')
c= tensor(6.2025e+08, device='cuda:0')
c= tensor(6.2025e+08, device='cuda:0')
c= tensor(6.2026e+08, device='cuda:0')
c= tensor(6.2026e+08, device='cuda:0')
c= tensor(6.2026e+08, device='cuda:0')
c= tensor(6.2027e+08, device='cuda:0')
c= tensor(6.2027e+08, device='cuda:0')
c= tensor(6.2028e+08, device='cuda:0')
c= tensor(6.2028e+08, device='cuda:0')
c= tensor(6.2029e+08, device='cuda:0')
c= tensor(6.2030e+08, device='cuda:0')
c= tensor(6.2030e+08, device='cuda:0')
c= tensor(6.2030e+08, device='cuda:0')
c= tensor(6.2031e+08, device='cuda:0')
c= tensor(6.2035e+08, device='cuda:0')
c= tensor(6.2035e+08, device='cuda:0')
c= tensor(6.2035e+08, device='cuda:0')
c= tensor(6.2036e+08, device='cuda:0')
c= tensor(6.2036e+08, device='cuda:0')
c= tensor(6.2036e+08, device='cuda:0')
c= tensor(6.2036e+08, device='cuda:0')
c= tensor(6.2036e+08, device='cuda:0')
c= tensor(6.2037e+08, device='cuda:0')
c= tensor(6.2037e+08, device='cuda:0')
c= tensor(6.2037e+08, device='cuda:0')
c= tensor(6.2039e+08, device='cuda:0')
c= tensor(6.2040e+08, device='cuda:0')
c= tensor(6.2044e+08, device='cuda:0')
c= tensor(6.2045e+08, device='cuda:0')
c= tensor(6.2045e+08, device='cuda:0')
c= tensor(6.2045e+08, device='cuda:0')
c= tensor(6.2046e+08, device='cuda:0')
c= tensor(6.2046e+08, device='cuda:0')
c= tensor(6.2046e+08, device='cuda:0')
c= tensor(6.2047e+08, device='cuda:0')
c= tensor(6.2048e+08, device='cuda:0')
c= tensor(6.2048e+08, device='cuda:0')
c= tensor(6.2048e+08, device='cuda:0')
c= tensor(6.2049e+08, device='cuda:0')
c= tensor(6.2049e+08, device='cuda:0')
c= tensor(6.2049e+08, device='cuda:0')
c= tensor(6.2049e+08, device='cuda:0')
c= tensor(6.2049e+08, device='cuda:0')
c= tensor(6.2049e+08, device='cuda:0')
c= tensor(6.2049e+08, device='cuda:0')
c= tensor(6.2050e+08, device='cuda:0')
c= tensor(6.2050e+08, device='cuda:0')
c= tensor(6.2051e+08, device='cuda:0')
c= tensor(6.2051e+08, device='cuda:0')
c= tensor(6.2051e+08, device='cuda:0')
c= tensor(6.2051e+08, device='cuda:0')
c= tensor(6.2051e+08, device='cuda:0')
c= tensor(6.2051e+08, device='cuda:0')
c= tensor(6.2052e+08, device='cuda:0')
c= tensor(6.2052e+08, device='cuda:0')
c= tensor(6.2052e+08, device='cuda:0')
c= tensor(6.2052e+08, device='cuda:0')
c= tensor(6.2052e+08, device='cuda:0')
c= tensor(6.2052e+08, device='cuda:0')
c= tensor(6.2053e+08, device='cuda:0')
c= tensor(6.2053e+08, device='cuda:0')
c= tensor(6.2054e+08, device='cuda:0')
c= tensor(6.2055e+08, device='cuda:0')
c= tensor(6.2056e+08, device='cuda:0')
c= tensor(6.2058e+08, device='cuda:0')
c= tensor(6.2067e+08, device='cuda:0')
c= tensor(6.2085e+08, device='cuda:0')
c= tensor(6.2086e+08, device='cuda:0')
c= tensor(6.2086e+08, device='cuda:0')
c= tensor(6.2087e+08, device='cuda:0')
c= tensor(6.5657e+08, device='cuda:0')
c= tensor(6.6806e+08, device='cuda:0')
c= tensor(6.6806e+08, device='cuda:0')
c= tensor(6.6860e+08, device='cuda:0')
c= tensor(6.7210e+08, device='cuda:0')
c= tensor(6.7227e+08, device='cuda:0')
c= tensor(6.8606e+08, device='cuda:0')
c= tensor(6.8606e+08, device='cuda:0')
c= tensor(6.8608e+08, device='cuda:0')
c= tensor(6.9033e+08, device='cuda:0')
c= tensor(7.3511e+08, device='cuda:0')
c= tensor(7.3515e+08, device='cuda:0')
c= tensor(7.3533e+08, device='cuda:0')
c= tensor(8.1358e+08, device='cuda:0')
c= tensor(8.6351e+08, device='cuda:0')
c= tensor(8.6362e+08, device='cuda:0')
c= tensor(8.6399e+08, device='cuda:0')
c= tensor(8.6477e+08, device='cuda:0')
c= tensor(8.6481e+08, device='cuda:0')
c= tensor(8.6487e+08, device='cuda:0')
c= tensor(8.8538e+08, device='cuda:0')
c= tensor(8.8539e+08, device='cuda:0')
c= tensor(8.8539e+08, device='cuda:0')
c= tensor(8.8552e+08, device='cuda:0')
c= tensor(8.8557e+08, device='cuda:0')
c= tensor(8.9918e+08, device='cuda:0')
c= tensor(9.0109e+08, device='cuda:0')
c= tensor(9.0110e+08, device='cuda:0')
c= tensor(9.0131e+08, device='cuda:0')
c= tensor(9.0132e+08, device='cuda:0')
c= tensor(9.0149e+08, device='cuda:0')
c= tensor(9.0290e+08, device='cuda:0')
c= tensor(9.0515e+08, device='cuda:0')
c= tensor(9.0596e+08, device='cuda:0')
c= tensor(9.0596e+08, device='cuda:0')
c= tensor(9.0596e+08, device='cuda:0')
c= tensor(9.0633e+08, device='cuda:0')
c= tensor(9.0718e+08, device='cuda:0')
c= tensor(9.0726e+08, device='cuda:0')
c= tensor(9.0826e+08, device='cuda:0')
c= tensor(9.2202e+08, device='cuda:0')
c= tensor(9.2204e+08, device='cuda:0')
c= tensor(9.2212e+08, device='cuda:0')
c= tensor(9.2295e+08, device='cuda:0')
c= tensor(9.2295e+08, device='cuda:0')
c= tensor(9.2410e+08, device='cuda:0')
c= tensor(9.2694e+08, device='cuda:0')
c= tensor(9.3552e+08, device='cuda:0')
c= tensor(9.3561e+08, device='cuda:0')
c= tensor(9.3567e+08, device='cuda:0')
c= tensor(9.3570e+08, device='cuda:0')
c= tensor(9.3571e+08, device='cuda:0')
c= tensor(9.3578e+08, device='cuda:0')
c= tensor(9.3580e+08, device='cuda:0')
c= tensor(9.3598e+08, device='cuda:0')
c= tensor(9.5002e+08, device='cuda:0')
c= tensor(9.5125e+08, device='cuda:0')
c= tensor(9.5129e+08, device='cuda:0')
c= tensor(9.5133e+08, device='cuda:0')
c= tensor(9.5385e+08, device='cuda:0')
c= tensor(9.5408e+08, device='cuda:0')
c= tensor(9.5408e+08, device='cuda:0')
c= tensor(9.5413e+08, device='cuda:0')
c= tensor(9.8090e+08, device='cuda:0')
c= tensor(9.8092e+08, device='cuda:0')
c= tensor(9.8751e+08, device='cuda:0')
c= tensor(9.8752e+08, device='cuda:0')
c= tensor(9.8838e+08, device='cuda:0')
c= tensor(9.8977e+08, device='cuda:0')
c= tensor(1.0014e+09, device='cuda:0')
c= tensor(1.0022e+09, device='cuda:0')
c= tensor(1.0023e+09, device='cuda:0')
c= tensor(1.0040e+09, device='cuda:0')
c= tensor(1.0056e+09, device='cuda:0')
c= tensor(1.0056e+09, device='cuda:0')
c= tensor(1.0060e+09, device='cuda:0')
c= tensor(1.0098e+09, device='cuda:0')
c= tensor(1.0264e+09, device='cuda:0')
c= tensor(1.0282e+09, device='cuda:0')
c= tensor(1.0282e+09, device='cuda:0')
c= tensor(1.0282e+09, device='cuda:0')
c= tensor(1.0371e+09, device='cuda:0')
c= tensor(1.0555e+09, device='cuda:0')
c= tensor(1.0556e+09, device='cuda:0')
c= tensor(1.0556e+09, device='cuda:0')
c= tensor(1.0562e+09, device='cuda:0')
c= tensor(1.0579e+09, device='cuda:0')
c= tensor(1.0658e+09, device='cuda:0')
c= tensor(1.0659e+09, device='cuda:0')
c= tensor(1.0660e+09, device='cuda:0')
c= tensor(1.0660e+09, device='cuda:0')
c= tensor(1.0660e+09, device='cuda:0')
c= tensor(1.0661e+09, device='cuda:0')
c= tensor(1.0661e+09, device='cuda:0')
c= tensor(1.0686e+09, device='cuda:0')
c= tensor(1.0688e+09, device='cuda:0')
c= tensor(1.0690e+09, device='cuda:0')
c= tensor(1.0691e+09, device='cuda:0')
c= tensor(1.0692e+09, device='cuda:0')
c= tensor(1.1579e+09, device='cuda:0')
c= tensor(1.1579e+09, device='cuda:0')
c= tensor(1.1613e+09, device='cuda:0')
c= tensor(1.1613e+09, device='cuda:0')
c= tensor(1.1613e+09, device='cuda:0')
c= tensor(1.1613e+09, device='cuda:0')
c= tensor(1.1620e+09, device='cuda:0')
c= tensor(1.1620e+09, device='cuda:0')
c= tensor(1.1627e+09, device='cuda:0')
c= tensor(1.1628e+09, device='cuda:0')
c= tensor(1.1628e+09, device='cuda:0')
c= tensor(1.1749e+09, device='cuda:0')
c= tensor(1.1754e+09, device='cuda:0')
c= tensor(1.1754e+09, device='cuda:0')
c= tensor(1.1776e+09, device='cuda:0')
c= tensor(1.1798e+09, device='cuda:0')
c= tensor(1.1798e+09, device='cuda:0')
c= tensor(1.1799e+09, device='cuda:0')
c= tensor(1.1800e+09, device='cuda:0')
c= tensor(1.1800e+09, device='cuda:0')
c= tensor(1.1800e+09, device='cuda:0')
c= tensor(1.1800e+09, device='cuda:0')
c= tensor(1.1800e+09, device='cuda:0')
c= tensor(1.1800e+09, device='cuda:0')
c= tensor(1.1800e+09, device='cuda:0')
c= tensor(1.1800e+09, device='cuda:0')
c= tensor(1.1979e+09, device='cuda:0')
c= tensor(1.1979e+09, device='cuda:0')
c= tensor(1.1990e+09, device='cuda:0')
c= tensor(1.1991e+09, device='cuda:0')
c= tensor(1.1993e+09, device='cuda:0')
c= tensor(1.2001e+09, device='cuda:0')
c= tensor(1.8667e+09, device='cuda:0')
c= tensor(1.8982e+09, device='cuda:0')
c= tensor(1.8985e+09, device='cuda:0')
c= tensor(1.9005e+09, device='cuda:0')
c= tensor(1.9005e+09, device='cuda:0')
c= tensor(1.9013e+09, device='cuda:0')
c= tensor(1.9023e+09, device='cuda:0')
c= tensor(1.9034e+09, device='cuda:0')
c= tensor(1.9034e+09, device='cuda:0')
c= tensor(1.9042e+09, device='cuda:0')
c= tensor(1.9129e+09, device='cuda:0')
c= tensor(1.9130e+09, device='cuda:0')
c= tensor(1.9130e+09, device='cuda:0')
c= tensor(1.9134e+09, device='cuda:0')
c= tensor(1.9135e+09, device='cuda:0')
c= tensor(1.9135e+09, device='cuda:0')
c= tensor(1.9179e+09, device='cuda:0')
c= tensor(1.9180e+09, device='cuda:0')
c= tensor(1.9180e+09, device='cuda:0')
c= tensor(1.9194e+09, device='cuda:0')
c= tensor(1.9194e+09, device='cuda:0')
c= tensor(1.9194e+09, device='cuda:0')
c= tensor(1.9209e+09, device='cuda:0')
c= tensor(1.9218e+09, device='cuda:0')
c= tensor(1.9225e+09, device='cuda:0')
c= tensor(1.9244e+09, device='cuda:0')
c= tensor(1.9246e+09, device='cuda:0')
c= tensor(1.9246e+09, device='cuda:0')
c= tensor(1.9249e+09, device='cuda:0')
c= tensor(1.9261e+09, device='cuda:0')
c= tensor(1.9282e+09, device='cuda:0')
c= tensor(1.9282e+09, device='cuda:0')
c= tensor(2.0294e+09, device='cuda:0')
c= tensor(2.0564e+09, device='cuda:0')
c= tensor(2.0580e+09, device='cuda:0')
c= tensor(2.0585e+09, device='cuda:0')
c= tensor(2.0586e+09, device='cuda:0')
c= tensor(2.0587e+09, device='cuda:0')
c= tensor(2.0587e+09, device='cuda:0')
c= tensor(2.0587e+09, device='cuda:0')
c= tensor(2.0619e+09, device='cuda:0')
c= tensor(2.0637e+09, device='cuda:0')
c= tensor(2.0857e+09, device='cuda:0')
c= tensor(2.0935e+09, device='cuda:0')
c= tensor(2.0942e+09, device='cuda:0')
c= tensor(2.0944e+09, device='cuda:0')
c= tensor(2.0949e+09, device='cuda:0')
c= tensor(2.0949e+09, device='cuda:0')
c= tensor(2.0950e+09, device='cuda:0')
c= tensor(2.1000e+09, device='cuda:0')
c= tensor(2.1001e+09, device='cuda:0')
c= tensor(2.1001e+09, device='cuda:0')
c= tensor(2.1002e+09, device='cuda:0')
c= tensor(2.1010e+09, device='cuda:0')
c= tensor(2.1012e+09, device='cuda:0')
c= tensor(2.1018e+09, device='cuda:0')
c= tensor(2.1020e+09, device='cuda:0')
c= tensor(2.1023e+09, device='cuda:0')
c= tensor(2.1023e+09, device='cuda:0')
c= tensor(2.1023e+09, device='cuda:0')
c= tensor(2.1026e+09, device='cuda:0')
c= tensor(2.1028e+09, device='cuda:0')
c= tensor(2.1028e+09, device='cuda:0')
c= tensor(2.1069e+09, device='cuda:0')
c= tensor(2.1069e+09, device='cuda:0')
c= tensor(2.1070e+09, device='cuda:0')
c= tensor(2.1071e+09, device='cuda:0')
c= tensor(2.1077e+09, device='cuda:0')
c= tensor(2.1077e+09, device='cuda:0')
c= tensor(2.1083e+09, device='cuda:0')
c= tensor(2.1084e+09, device='cuda:0')
c= tensor(2.1084e+09, device='cuda:0')
c= tensor(2.1087e+09, device='cuda:0')
c= tensor(2.1607e+09, device='cuda:0')
c= tensor(2.1607e+09, device='cuda:0')
c= tensor(2.1607e+09, device='cuda:0')
c= tensor(2.1622e+09, device='cuda:0')
c= tensor(2.1622e+09, device='cuda:0')
c= tensor(2.1797e+09, device='cuda:0')
c= tensor(2.1797e+09, device='cuda:0')
c= tensor(2.1809e+09, device='cuda:0')
c= tensor(2.1919e+09, device='cuda:0')
c= tensor(2.1919e+09, device='cuda:0')
c= tensor(2.2114e+09, device='cuda:0')
c= tensor(2.2119e+09, device='cuda:0')
c= tensor(2.2294e+09, device='cuda:0')
c= tensor(2.2294e+09, device='cuda:0')
c= tensor(2.2303e+09, device='cuda:0')
c= tensor(2.2303e+09, device='cuda:0')
c= tensor(2.2303e+09, device='cuda:0')
c= tensor(2.2303e+09, device='cuda:0')
c= tensor(2.2312e+09, device='cuda:0')
c= tensor(2.2315e+09, device='cuda:0')
c= tensor(2.2321e+09, device='cuda:0')
c= tensor(2.2322e+09, device='cuda:0')
c= tensor(2.2322e+09, device='cuda:0')
c= tensor(2.2322e+09, device='cuda:0')
c= tensor(2.2345e+09, device='cuda:0')
c= tensor(2.2346e+09, device='cuda:0')
c= tensor(2.2464e+09, device='cuda:0')
c= tensor(2.2466e+09, device='cuda:0')
c= tensor(2.2466e+09, device='cuda:0')
c= tensor(2.2466e+09, device='cuda:0')
c= tensor(2.2466e+09, device='cuda:0')
c= tensor(2.4264e+09, device='cuda:0')
c= tensor(2.4264e+09, device='cuda:0')
c= tensor(2.4265e+09, device='cuda:0')
c= tensor(2.4286e+09, device='cuda:0')
c= tensor(2.4294e+09, device='cuda:0')
c= tensor(2.4294e+09, device='cuda:0')
c= tensor(2.4295e+09, device='cuda:0')
c= tensor(2.4301e+09, device='cuda:0')
c= tensor(2.4303e+09, device='cuda:0')
c= tensor(2.4309e+09, device='cuda:0')
c= tensor(2.4314e+09, device='cuda:0')
c= tensor(2.4327e+09, device='cuda:0')
c= tensor(2.4330e+09, device='cuda:0')
c= tensor(2.4377e+09, device='cuda:0')
c= tensor(2.4386e+09, device='cuda:0')
c= tensor(2.4387e+09, device='cuda:0')
c= tensor(2.4414e+09, device='cuda:0')
c= tensor(2.4418e+09, device='cuda:0')
c= tensor(2.4418e+09, device='cuda:0')
c= tensor(2.4419e+09, device='cuda:0')
c= tensor(2.4419e+09, device='cuda:0')
c= tensor(2.4440e+09, device='cuda:0')
c= tensor(2.4441e+09, device='cuda:0')
c= tensor(2.4441e+09, device='cuda:0')
c= tensor(2.4442e+09, device='cuda:0')
c= tensor(2.4445e+09, device='cuda:0')
c= tensor(2.4454e+09, device='cuda:0')
c= tensor(2.4461e+09, device='cuda:0')
c= tensor(2.4461e+09, device='cuda:0')
c= tensor(2.4462e+09, device='cuda:0')
c= tensor(2.4463e+09, device='cuda:0')
c= tensor(2.4463e+09, device='cuda:0')
c= tensor(2.4463e+09, device='cuda:0')
c= tensor(2.4464e+09, device='cuda:0')
c= tensor(2.4536e+09, device='cuda:0')
c= tensor(2.4536e+09, device='cuda:0')
c= tensor(2.4536e+09, device='cuda:0')
c= tensor(2.4536e+09, device='cuda:0')
c= tensor(2.4587e+09, device='cuda:0')
c= tensor(2.4733e+09, device='cuda:0')
c= tensor(2.4735e+09, device='cuda:0')
c= tensor(2.4735e+09, device='cuda:0')
c= tensor(2.4746e+09, device='cuda:0')
c= tensor(2.4772e+09, device='cuda:0')
c= tensor(2.4772e+09, device='cuda:0')
c= tensor(2.4774e+09, device='cuda:0')
c= tensor(2.4775e+09, device='cuda:0')
c= tensor(2.4775e+09, device='cuda:0')
c= tensor(2.4780e+09, device='cuda:0')
c= tensor(2.4785e+09, device='cuda:0')
c= tensor(2.4786e+09, device='cuda:0')
c= tensor(2.4786e+09, device='cuda:0')
c= tensor(2.4786e+09, device='cuda:0')
c= tensor(2.4789e+09, device='cuda:0')
c= tensor(2.4791e+09, device='cuda:0')
c= tensor(2.4795e+09, device='cuda:0')
c= tensor(2.4797e+09, device='cuda:0')
c= tensor(2.4852e+09, device='cuda:0')
c= tensor(2.4852e+09, device='cuda:0')
c= tensor(2.4852e+09, device='cuda:0')
c= tensor(2.4853e+09, device='cuda:0')
c= tensor(2.4857e+09, device='cuda:0')
c= tensor(2.4858e+09, device='cuda:0')
c= tensor(2.4858e+09, device='cuda:0')
c= tensor(2.4858e+09, device='cuda:0')
c= tensor(2.4870e+09, device='cuda:0')
c= tensor(2.4870e+09, device='cuda:0')
c= tensor(2.4875e+09, device='cuda:0')
c= tensor(2.4875e+09, device='cuda:0')
c= tensor(2.4875e+09, device='cuda:0')
c= tensor(2.4877e+09, device='cuda:0')
c= tensor(2.4877e+09, device='cuda:0')
c= tensor(2.4877e+09, device='cuda:0')
c= tensor(2.4905e+09, device='cuda:0')
c= tensor(2.4965e+09, device='cuda:0')
c= tensor(2.4992e+09, device='cuda:0')
c= tensor(2.5055e+09, device='cuda:0')
c= tensor(2.5056e+09, device='cuda:0')
c= tensor(2.5057e+09, device='cuda:0')
c= tensor(2.5057e+09, device='cuda:0')
c= tensor(2.5070e+09, device='cuda:0')
c= tensor(2.5070e+09, device='cuda:0')
c= tensor(2.5071e+09, device='cuda:0')
c= tensor(2.5071e+09, device='cuda:0')
c= tensor(2.5120e+09, device='cuda:0')
c= tensor(2.5125e+09, device='cuda:0')
c= tensor(2.5127e+09, device='cuda:0')
c= tensor(2.5174e+09, device='cuda:0')
c= tensor(2.5179e+09, device='cuda:0')
c= tensor(2.5180e+09, device='cuda:0')
c= tensor(2.5202e+09, device='cuda:0')
c= tensor(2.5232e+09, device='cuda:0')
c= tensor(2.5241e+09, device='cuda:0')
c= tensor(2.5241e+09, device='cuda:0')
c= tensor(2.5250e+09, device='cuda:0')
c= tensor(2.5250e+09, device='cuda:0')
c= tensor(2.5255e+09, device='cuda:0')
c= tensor(2.5838e+09, device='cuda:0')
c= tensor(2.5871e+09, device='cuda:0')
c= tensor(2.5875e+09, device='cuda:0')
c= tensor(2.5875e+09, device='cuda:0')
c= tensor(2.5878e+09, device='cuda:0')
c= tensor(2.5878e+09, device='cuda:0')
c= tensor(2.5879e+09, device='cuda:0')
c= tensor(2.5881e+09, device='cuda:0')
c= tensor(2.5981e+09, device='cuda:0')
c= tensor(2.5981e+09, device='cuda:0')
c= tensor(2.6325e+09, device='cuda:0')
c= tensor(2.6327e+09, device='cuda:0')
c= tensor(2.6328e+09, device='cuda:0')
c= tensor(2.6328e+09, device='cuda:0')
c= tensor(2.6339e+09, device='cuda:0')
c= tensor(2.6375e+09, device='cuda:0')
c= tensor(2.6375e+09, device='cuda:0')
c= tensor(2.6843e+09, device='cuda:0')
c= tensor(2.6848e+09, device='cuda:0')
c= tensor(2.6863e+09, device='cuda:0')
c= tensor(2.6864e+09, device='cuda:0')
c= tensor(2.6864e+09, device='cuda:0')
c= tensor(2.6865e+09, device='cuda:0')
c= tensor(2.6865e+09, device='cuda:0')
c= tensor(2.6865e+09, device='cuda:0')
c= tensor(2.6923e+09, device='cuda:0')
c= tensor(2.9100e+09, device='cuda:0')
c= tensor(2.9101e+09, device='cuda:0')
c= tensor(2.9118e+09, device='cuda:0')
c= tensor(2.9119e+09, device='cuda:0')
c= tensor(2.9120e+09, device='cuda:0')
c= tensor(2.9120e+09, device='cuda:0')
c= tensor(2.9170e+09, device='cuda:0')
c= tensor(2.9171e+09, device='cuda:0')
c= tensor(3.0907e+09, device='cuda:0')
c= tensor(3.0907e+09, device='cuda:0')
c= tensor(3.0924e+09, device='cuda:0')
c= tensor(3.0925e+09, device='cuda:0')
c= tensor(3.0943e+09, device='cuda:0')
c= tensor(3.1101e+09, device='cuda:0')
c= tensor(3.1102e+09, device='cuda:0')
c= tensor(3.1102e+09, device='cuda:0')
c= tensor(3.1116e+09, device='cuda:0')
c= tensor(3.1116e+09, device='cuda:0')
c= tensor(3.1117e+09, device='cuda:0')
c= tensor(3.1142e+09, device='cuda:0')
c= tensor(3.1375e+09, device='cuda:0')
c= tensor(3.1376e+09, device='cuda:0')
c= tensor(3.1378e+09, device='cuda:0')
c= tensor(3.1412e+09, device='cuda:0')
c= tensor(3.1465e+09, device='cuda:0')
c= tensor(3.1466e+09, device='cuda:0')
c= tensor(3.1466e+09, device='cuda:0')
c= tensor(3.1641e+09, device='cuda:0')
c= tensor(3.1644e+09, device='cuda:0')
c= tensor(3.1648e+09, device='cuda:0')
c= tensor(3.1650e+09, device='cuda:0')
c= tensor(3.1654e+09, device='cuda:0')
c= tensor(3.1663e+09, device='cuda:0')
c= tensor(3.2212e+09, device='cuda:0')
c= tensor(3.2216e+09, device='cuda:0')
c= tensor(3.2216e+09, device='cuda:0')
c= tensor(3.2216e+09, device='cuda:0')
c= tensor(3.2232e+09, device='cuda:0')
c= tensor(3.2237e+09, device='cuda:0')
c= tensor(3.2246e+09, device='cuda:0')
c= tensor(3.2268e+09, device='cuda:0')
c= tensor(3.2268e+09, device='cuda:0')
c= tensor(3.2269e+09, device='cuda:0')
c= tensor(3.2277e+09, device='cuda:0')
c= tensor(3.2302e+09, device='cuda:0')
c= tensor(3.2303e+09, device='cuda:0')
c= tensor(3.2320e+09, device='cuda:0')
c= tensor(3.3492e+09, device='cuda:0')
c= tensor(3.3495e+09, device='cuda:0')
c= tensor(3.3495e+09, device='cuda:0')
c= tensor(3.3495e+09, device='cuda:0')
c= tensor(3.3555e+09, device='cuda:0')
c= tensor(3.3565e+09, device='cuda:0')
c= tensor(3.3572e+09, device='cuda:0')
c= tensor(3.3572e+09, device='cuda:0')
c= tensor(3.3572e+09, device='cuda:0')
c= tensor(3.3600e+09, device='cuda:0')
c= tensor(3.3607e+09, device='cuda:0')
c= tensor(3.3612e+09, device='cuda:0')
c= tensor(3.3612e+09, device='cuda:0')
c= tensor(3.3612e+09, device='cuda:0')
c= tensor(3.3613e+09, device='cuda:0')
c= tensor(3.3616e+09, device='cuda:0')
c= tensor(3.3626e+09, device='cuda:0')
c= tensor(3.3627e+09, device='cuda:0')
c= tensor(3.3741e+09, device='cuda:0')
c= tensor(3.3741e+09, device='cuda:0')
c= tensor(3.3743e+09, device='cuda:0')
c= tensor(3.3894e+09, device='cuda:0')
c= tensor(3.3894e+09, device='cuda:0')
c= tensor(3.3897e+09, device='cuda:0')
c= tensor(3.3936e+09, device='cuda:0')
c= tensor(3.3937e+09, device='cuda:0')
c= tensor(3.3944e+09, device='cuda:0')
c= tensor(3.3977e+09, device='cuda:0')
c= tensor(3.3977e+09, device='cuda:0')
c= tensor(3.3980e+09, device='cuda:0')
c= tensor(3.3980e+09, device='cuda:0')
c= tensor(3.4201e+09, device='cuda:0')
c= tensor(3.4229e+09, device='cuda:0')
c= tensor(3.4233e+09, device='cuda:0')
c= tensor(3.4239e+09, device='cuda:0')
c= tensor(3.4239e+09, device='cuda:0')
c= tensor(3.4242e+09, device='cuda:0')
c= tensor(3.4243e+09, device='cuda:0')
c= tensor(3.4243e+09, device='cuda:0')
c= tensor(3.4264e+09, device='cuda:0')
c= tensor(3.4297e+09, device='cuda:0')
c= tensor(3.4297e+09, device='cuda:0')
c= tensor(3.4299e+09, device='cuda:0')
c= tensor(3.4299e+09, device='cuda:0')
c= tensor(3.4410e+09, device='cuda:0')
c= tensor(3.4411e+09, device='cuda:0')
c= tensor(3.4411e+09, device='cuda:0')
c= tensor(3.4418e+09, device='cuda:0')
c= tensor(3.4424e+09, device='cuda:0')
c= tensor(3.4425e+09, device='cuda:0')
c= tensor(3.4429e+09, device='cuda:0')
c= tensor(3.4430e+09, device='cuda:0')
memory (bytes)
3723640832
time for making loss 2 is 15.718543529510498
p0 True
it  0 : 705112576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 13% |
shape of L is 
torch.Size([])
memory (bytes)
3723898880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  6% |
memory (bytes)
3724713984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% |  8% |
error is  38039695000.0
relative error loss 11.048315
shape of L is 
torch.Size([])
memory (bytes)
3948371968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  4% |  8% |
memory (bytes)
3948580864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  38039347000.0
relative error loss 11.048214
shape of L is 
torch.Size([])
memory (bytes)
3950919680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
3950968832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% |  8% |
error is  38038140000.0
relative error loss 11.047863
shape of L is 
torch.Size([])
memory (bytes)
3953111040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  8% |
memory (bytes)
3953111040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  38030664000.0
relative error loss 11.045692
shape of L is 
torch.Size([])
memory (bytes)
3955200000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
3955236864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  8% |
error is  37949567000.0
relative error loss 11.022139
shape of L is 
torch.Size([])
memory (bytes)
3957338112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
3957338112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  37545886000.0
relative error loss 10.904892
shape of L is 
torch.Size([])
memory (bytes)
3959422976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  8% |
memory (bytes)
3959422976
| ID | GPU | MEM |
------------------
|  0 |  1% |  0% |
|  1 | 94% |  8% |
error is  35390693000.0
relative error loss 10.2789345
shape of L is 
torch.Size([])
memory (bytes)
3961540608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
3961569280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  25385460000.0
relative error loss 7.3729973
shape of L is 
torch.Size([])
memory (bytes)
3963604992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
3963604992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  8612194000.0
relative error loss 2.5013406
shape of L is 
torch.Size([])
memory (bytes)
3965800448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
3965833216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  5195280400.0
relative error loss 1.5089263
time to take a step is 331.14907574653625
it  1 : 893986816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
3967746048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  8% |
memory (bytes)
3967746048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  5195280400.0
relative error loss 1.5089263
shape of L is 
torch.Size([])
memory (bytes)
3970043904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
3970072576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  3895486000.0
relative error loss 1.1314117
shape of L is 
torch.Size([])
memory (bytes)
3971973120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% |  8% |
memory (bytes)
3972177920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  3393365200.0
relative error loss 0.9855749
shape of L is 
torch.Size([])
memory (bytes)
3974197248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
3974197248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  6559731700.0
relative error loss 1.9052199
shape of L is 
torch.Size([])
memory (bytes)
3976392704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
3976392704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  3058532900.0
relative error loss 0.88832563
shape of L is 
torch.Size([])
memory (bytes)
3978309632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
3978309632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  2895269400.0
relative error loss 0.8409071
shape of L is 
torch.Size([])
memory (bytes)
3980673024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
3980701696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  2708644600.0
relative error loss 0.78670347
shape of L is 
torch.Size([])
memory (bytes)
3982733312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
3982733312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  2516382700.0
relative error loss 0.73086256
shape of L is 
torch.Size([])
memory (bytes)
3984965632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
3984965632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  2720234200.0
relative error loss 0.7900696
shape of L is 
torch.Size([])
memory (bytes)
3986927616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
3987087360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  2372622300.0
relative error loss 0.68910855
time to take a step is 315.210045337677
it  2 : 947529728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
3988942848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  8% |
memory (bytes)
3988942848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  2372622300.0
relative error loss 0.68910855
shape of L is 
torch.Size([])
memory (bytes)
3991293952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% |  8% |
memory (bytes)
3991293952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  2181670000.0
relative error loss 0.633648
shape of L is 
torch.Size([])
memory (bytes)
3993088000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
3993341952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% |  8% |
error is  1967851400.0
relative error loss 0.5715462
shape of L is 
torch.Size([])
memory (bytes)
3995512832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
3995512832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  1798183800.0
relative error loss 0.52226764
shape of L is 
torch.Size([])
memory (bytes)
3997601792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% |  8% |
memory (bytes)
3997634560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  1607085800.0
relative error loss 0.4667648
shape of L is 
torch.Size([])
memory (bytes)
3999625216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
3999625216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% |  8% |
error is  1440172800.0
relative error loss 0.4182863
shape of L is 
torch.Size([])
memory (bytes)
4001865728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
4001894400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% |  8% |
error is  1293213700.0
relative error loss 0.37560323
shape of L is 
torch.Size([])
memory (bytes)
4003926016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% |  8% |
memory (bytes)
4003926016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  1172554800.0
relative error loss 0.34055886
shape of L is 
torch.Size([])
memory (bytes)
4006158336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  8% |
memory (bytes)
4006158336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  1095152600.0
relative error loss 0.31807804
shape of L is 
torch.Size([])
memory (bytes)
4008284160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
4008284160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  986822400.0
relative error loss 0.28661442
time to take a step is 315.56355476379395
it  3 : 947529216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4010442752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
4010442752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  986822400.0
relative error loss 0.28661442
shape of L is 
torch.Size([])
memory (bytes)
4012544000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  8% |
memory (bytes)
4012572672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  908397300.0
relative error loss 0.2638365
shape of L is 
torch.Size([])
memory (bytes)
4014718976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
4014718976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  880783100.0
relative error loss 0.2558162
shape of L is 
torch.Size([])
memory (bytes)
4016861184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  8% |
memory (bytes)
4016869376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  753189400.0
relative error loss 0.21875763
shape of L is 
torch.Size([])
memory (bytes)
4019023872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
4019023872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% |  8% |
error is  701858050.0
relative error loss 0.20384887
shape of L is 
torch.Size([])
memory (bytes)
4021170176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
4021170176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  655121400.0
relative error loss 0.1902746
shape of L is 
torch.Size([])
memory (bytes)
4023316480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  8% |
memory (bytes)
4023316480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  780683260.0
relative error loss 0.22674301
shape of L is 
torch.Size([])
memory (bytes)
4025442304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
4025470976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  611764000.0
relative error loss 0.17768179
shape of L is 
torch.Size([])
memory (bytes)
4027637760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
4027637760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  575497700.0
relative error loss 0.16714856
shape of L is 
torch.Size([])
memory (bytes)
4029759488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
4029759488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  533778430.0
relative error loss 0.15503153
time to take a step is 316.7776563167572
c= tensor(1049.5090, device='cuda:0')
c= tensor(35612.7422, device='cuda:0')
c= tensor(38815.4961, device='cuda:0')
c= tensor(42776.3633, device='cuda:0')
c= tensor(1133271.6250, device='cuda:0')
c= tensor(1393245.7500, device='cuda:0')
c= tensor(1654737., device='cuda:0')
c= tensor(1929568.7500, device='cuda:0')
c= tensor(1949212.7500, device='cuda:0')
c= tensor(26412092., device='cuda:0')
c= tensor(26422820., device='cuda:0')
c= tensor(29386340., device='cuda:0')
c= tensor(29408194., device='cuda:0')
c= tensor(32735016., device='cuda:0')
c= tensor(32829600., device='cuda:0')
c= tensor(33177210., device='cuda:0')
c= tensor(33608344., device='cuda:0')
c= tensor(34091856., device='cuda:0')
c= tensor(43970440., device='cuda:0')
c= tensor(45702964., device='cuda:0')
c= tensor(45794076., device='cuda:0')
c= tensor(1.2356e+08, device='cuda:0')
c= tensor(1.2359e+08, device='cuda:0')
c= tensor(1.2361e+08, device='cuda:0')
c= tensor(1.2491e+08, device='cuda:0')
c= tensor(1.2552e+08, device='cuda:0')
c= tensor(1.2570e+08, device='cuda:0')
c= tensor(1.2576e+08, device='cuda:0')
c= tensor(1.2809e+08, device='cuda:0')
c= tensor(6.0086e+08, device='cuda:0')
c= tensor(6.0088e+08, device='cuda:0')
c= tensor(6.1683e+08, device='cuda:0')
c= tensor(6.1685e+08, device='cuda:0')
c= tensor(6.1687e+08, device='cuda:0')
c= tensor(6.1688e+08, device='cuda:0')
c= tensor(6.1866e+08, device='cuda:0')
c= tensor(6.1947e+08, device='cuda:0')
c= tensor(6.1947e+08, device='cuda:0')
c= tensor(6.1947e+08, device='cuda:0')
c= tensor(6.1947e+08, device='cuda:0')
c= tensor(6.1948e+08, device='cuda:0')
c= tensor(6.1948e+08, device='cuda:0')
c= tensor(6.1948e+08, device='cuda:0')
c= tensor(6.1949e+08, device='cuda:0')
c= tensor(6.1949e+08, device='cuda:0')
c= tensor(6.1949e+08, device='cuda:0')
c= tensor(6.1950e+08, device='cuda:0')
c= tensor(6.1950e+08, device='cuda:0')
c= tensor(6.1951e+08, device='cuda:0')
c= tensor(6.1951e+08, device='cuda:0')
c= tensor(6.1953e+08, device='cuda:0')
c= tensor(6.1953e+08, device='cuda:0')
c= tensor(6.1953e+08, device='cuda:0')
c= tensor(6.1954e+08, device='cuda:0')
c= tensor(6.1955e+08, device='cuda:0')
c= tensor(6.1956e+08, device='cuda:0')
c= tensor(6.1956e+08, device='cuda:0')
c= tensor(6.1956e+08, device='cuda:0')
c= tensor(6.1956e+08, device='cuda:0')
c= tensor(6.1957e+08, device='cuda:0')
c= tensor(6.1960e+08, device='cuda:0')
c= tensor(6.1960e+08, device='cuda:0')
c= tensor(6.1961e+08, device='cuda:0')
c= tensor(6.1962e+08, device='cuda:0')
c= tensor(6.1963e+08, device='cuda:0')
c= tensor(6.1963e+08, device='cuda:0')
c= tensor(6.1963e+08, device='cuda:0')
c= tensor(6.1964e+08, device='cuda:0')
c= tensor(6.1964e+08, device='cuda:0')
c= tensor(6.1965e+08, device='cuda:0')
c= tensor(6.1968e+08, device='cuda:0')
c= tensor(6.1968e+08, device='cuda:0')
c= tensor(6.1969e+08, device='cuda:0')
c= tensor(6.1969e+08, device='cuda:0')
c= tensor(6.1969e+08, device='cuda:0')
c= tensor(6.1970e+08, device='cuda:0')
c= tensor(6.1970e+08, device='cuda:0')
c= tensor(6.1970e+08, device='cuda:0')
c= tensor(6.1970e+08, device='cuda:0')
c= tensor(6.1974e+08, device='cuda:0')
c= tensor(6.1974e+08, device='cuda:0')
c= tensor(6.1974e+08, device='cuda:0')
c= tensor(6.1975e+08, device='cuda:0')
c= tensor(6.1975e+08, device='cuda:0')
c= tensor(6.1975e+08, device='cuda:0')
c= tensor(6.1975e+08, device='cuda:0')
c= tensor(6.1975e+08, device='cuda:0')
c= tensor(6.1975e+08, device='cuda:0')
c= tensor(6.1976e+08, device='cuda:0')
c= tensor(6.1976e+08, device='cuda:0')
c= tensor(6.1977e+08, device='cuda:0')
c= tensor(6.1977e+08, device='cuda:0')
c= tensor(6.1977e+08, device='cuda:0')
c= tensor(6.1978e+08, device='cuda:0')
c= tensor(6.1978e+08, device='cuda:0')
c= tensor(6.1979e+08, device='cuda:0')
c= tensor(6.1979e+08, device='cuda:0')
c= tensor(6.1980e+08, device='cuda:0')
c= tensor(6.1981e+08, device='cuda:0')
c= tensor(6.1981e+08, device='cuda:0')
c= tensor(6.1983e+08, device='cuda:0')
c= tensor(6.1984e+08, device='cuda:0')
c= tensor(6.1984e+08, device='cuda:0')
c= tensor(6.1984e+08, device='cuda:0')
c= tensor(6.1985e+08, device='cuda:0')
c= tensor(6.1985e+08, device='cuda:0')
c= tensor(6.1987e+08, device='cuda:0')
c= tensor(6.1987e+08, device='cuda:0')
c= tensor(6.1987e+08, device='cuda:0')
c= tensor(6.1987e+08, device='cuda:0')
c= tensor(6.1987e+08, device='cuda:0')
c= tensor(6.1987e+08, device='cuda:0')
c= tensor(6.1988e+08, device='cuda:0')
c= tensor(6.1988e+08, device='cuda:0')
c= tensor(6.1988e+08, device='cuda:0')
c= tensor(6.1989e+08, device='cuda:0')
c= tensor(6.1989e+08, device='cuda:0')
c= tensor(6.1989e+08, device='cuda:0')
c= tensor(6.1990e+08, device='cuda:0')
c= tensor(6.1990e+08, device='cuda:0')
c= tensor(6.1995e+08, device='cuda:0')
c= tensor(6.1995e+08, device='cuda:0')
c= tensor(6.1995e+08, device='cuda:0')
c= tensor(6.1996e+08, device='cuda:0')
c= tensor(6.1996e+08, device='cuda:0')
c= tensor(6.1996e+08, device='cuda:0')
c= tensor(6.1996e+08, device='cuda:0')
c= tensor(6.1997e+08, device='cuda:0')
c= tensor(6.1998e+08, device='cuda:0')
c= tensor(6.1998e+08, device='cuda:0')
c= tensor(6.2000e+08, device='cuda:0')
c= tensor(6.2000e+08, device='cuda:0')
c= tensor(6.2001e+08, device='cuda:0')
c= tensor(6.2001e+08, device='cuda:0')
c= tensor(6.2001e+08, device='cuda:0')
c= tensor(6.2001e+08, device='cuda:0')
c= tensor(6.2002e+08, device='cuda:0')
c= tensor(6.2002e+08, device='cuda:0')
c= tensor(6.2002e+08, device='cuda:0')
c= tensor(6.2002e+08, device='cuda:0')
c= tensor(6.2002e+08, device='cuda:0')
c= tensor(6.2002e+08, device='cuda:0')
c= tensor(6.2003e+08, device='cuda:0')
c= tensor(6.2011e+08, device='cuda:0')
c= tensor(6.2011e+08, device='cuda:0')
c= tensor(6.2011e+08, device='cuda:0')
c= tensor(6.2011e+08, device='cuda:0')
c= tensor(6.2012e+08, device='cuda:0')
c= tensor(6.2012e+08, device='cuda:0')
c= tensor(6.2012e+08, device='cuda:0')
c= tensor(6.2012e+08, device='cuda:0')
c= tensor(6.2013e+08, device='cuda:0')
c= tensor(6.2013e+08, device='cuda:0')
c= tensor(6.2015e+08, device='cuda:0')
c= tensor(6.2016e+08, device='cuda:0')
c= tensor(6.2019e+08, device='cuda:0')
c= tensor(6.2019e+08, device='cuda:0')
c= tensor(6.2019e+08, device='cuda:0')
c= tensor(6.2019e+08, device='cuda:0')
c= tensor(6.2019e+08, device='cuda:0')
c= tensor(6.2020e+08, device='cuda:0')
c= tensor(6.2020e+08, device='cuda:0')
c= tensor(6.2021e+08, device='cuda:0')
c= tensor(6.2021e+08, device='cuda:0')
c= tensor(6.2021e+08, device='cuda:0')
c= tensor(6.2021e+08, device='cuda:0')
c= tensor(6.2022e+08, device='cuda:0')
c= tensor(6.2022e+08, device='cuda:0')
c= tensor(6.2022e+08, device='cuda:0')
c= tensor(6.2022e+08, device='cuda:0')
c= tensor(6.2022e+08, device='cuda:0')
c= tensor(6.2022e+08, device='cuda:0')
c= tensor(6.2023e+08, device='cuda:0')
c= tensor(6.2023e+08, device='cuda:0')
c= tensor(6.2024e+08, device='cuda:0')
c= tensor(6.2025e+08, device='cuda:0')
c= tensor(6.2025e+08, device='cuda:0')
c= tensor(6.2026e+08, device='cuda:0')
c= tensor(6.2026e+08, device='cuda:0')
c= tensor(6.2026e+08, device='cuda:0')
c= tensor(6.2027e+08, device='cuda:0')
c= tensor(6.2027e+08, device='cuda:0')
c= tensor(6.2028e+08, device='cuda:0')
c= tensor(6.2028e+08, device='cuda:0')
c= tensor(6.2029e+08, device='cuda:0')
c= tensor(6.2030e+08, device='cuda:0')
c= tensor(6.2030e+08, device='cuda:0')
c= tensor(6.2030e+08, device='cuda:0')
c= tensor(6.2031e+08, device='cuda:0')
c= tensor(6.2035e+08, device='cuda:0')
c= tensor(6.2035e+08, device='cuda:0')
c= tensor(6.2035e+08, device='cuda:0')
c= tensor(6.2036e+08, device='cuda:0')
c= tensor(6.2036e+08, device='cuda:0')
c= tensor(6.2036e+08, device='cuda:0')
c= tensor(6.2036e+08, device='cuda:0')
c= tensor(6.2036e+08, device='cuda:0')
c= tensor(6.2037e+08, device='cuda:0')
c= tensor(6.2037e+08, device='cuda:0')
c= tensor(6.2037e+08, device='cuda:0')
c= tensor(6.2039e+08, device='cuda:0')
c= tensor(6.2040e+08, device='cuda:0')
c= tensor(6.2044e+08, device='cuda:0')
c= tensor(6.2045e+08, device='cuda:0')
c= tensor(6.2045e+08, device='cuda:0')
c= tensor(6.2045e+08, device='cuda:0')
c= tensor(6.2046e+08, device='cuda:0')
c= tensor(6.2046e+08, device='cuda:0')
c= tensor(6.2046e+08, device='cuda:0')
c= tensor(6.2047e+08, device='cuda:0')
c= tensor(6.2048e+08, device='cuda:0')
c= tensor(6.2048e+08, device='cuda:0')
c= tensor(6.2048e+08, device='cuda:0')
c= tensor(6.2049e+08, device='cuda:0')
c= tensor(6.2049e+08, device='cuda:0')
c= tensor(6.2049e+08, device='cuda:0')
c= tensor(6.2049e+08, device='cuda:0')
c= tensor(6.2049e+08, device='cuda:0')
c= tensor(6.2049e+08, device='cuda:0')
c= tensor(6.2049e+08, device='cuda:0')
c= tensor(6.2050e+08, device='cuda:0')
c= tensor(6.2050e+08, device='cuda:0')
c= tensor(6.2051e+08, device='cuda:0')
c= tensor(6.2051e+08, device='cuda:0')
c= tensor(6.2051e+08, device='cuda:0')
c= tensor(6.2051e+08, device='cuda:0')
c= tensor(6.2051e+08, device='cuda:0')
c= tensor(6.2051e+08, device='cuda:0')
c= tensor(6.2052e+08, device='cuda:0')
c= tensor(6.2052e+08, device='cuda:0')
c= tensor(6.2052e+08, device='cuda:0')
c= tensor(6.2052e+08, device='cuda:0')
c= tensor(6.2052e+08, device='cuda:0')
c= tensor(6.2052e+08, device='cuda:0')
c= tensor(6.2053e+08, device='cuda:0')
c= tensor(6.2053e+08, device='cuda:0')
c= tensor(6.2054e+08, device='cuda:0')
c= tensor(6.2055e+08, device='cuda:0')
c= tensor(6.2056e+08, device='cuda:0')
c= tensor(6.2058e+08, device='cuda:0')
c= tensor(6.2067e+08, device='cuda:0')
c= tensor(6.2085e+08, device='cuda:0')
c= tensor(6.2086e+08, device='cuda:0')
c= tensor(6.2086e+08, device='cuda:0')
c= tensor(6.2087e+08, device='cuda:0')
c= tensor(6.5657e+08, device='cuda:0')
c= tensor(6.6806e+08, device='cuda:0')
c= tensor(6.6806e+08, device='cuda:0')
c= tensor(6.6860e+08, device='cuda:0')
c= tensor(6.7210e+08, device='cuda:0')
c= tensor(6.7227e+08, device='cuda:0')
c= tensor(6.8606e+08, device='cuda:0')
c= tensor(6.8606e+08, device='cuda:0')
c= tensor(6.8608e+08, device='cuda:0')
c= tensor(6.9033e+08, device='cuda:0')
c= tensor(7.3511e+08, device='cuda:0')
c= tensor(7.3515e+08, device='cuda:0')
c= tensor(7.3533e+08, device='cuda:0')
c= tensor(8.1358e+08, device='cuda:0')
c= tensor(8.6351e+08, device='cuda:0')
c= tensor(8.6362e+08, device='cuda:0')
c= tensor(8.6399e+08, device='cuda:0')
c= tensor(8.6477e+08, device='cuda:0')
c= tensor(8.6481e+08, device='cuda:0')
c= tensor(8.6487e+08, device='cuda:0')
c= tensor(8.8538e+08, device='cuda:0')
c= tensor(8.8539e+08, device='cuda:0')
c= tensor(8.8539e+08, device='cuda:0')
c= tensor(8.8552e+08, device='cuda:0')
c= tensor(8.8557e+08, device='cuda:0')
c= tensor(8.9918e+08, device='cuda:0')
c= tensor(9.0109e+08, device='cuda:0')
c= tensor(9.0110e+08, device='cuda:0')
c= tensor(9.0131e+08, device='cuda:0')
c= tensor(9.0132e+08, device='cuda:0')
c= tensor(9.0149e+08, device='cuda:0')
c= tensor(9.0290e+08, device='cuda:0')
c= tensor(9.0515e+08, device='cuda:0')
c= tensor(9.0596e+08, device='cuda:0')
c= tensor(9.0596e+08, device='cuda:0')
c= tensor(9.0596e+08, device='cuda:0')
c= tensor(9.0633e+08, device='cuda:0')
c= tensor(9.0718e+08, device='cuda:0')
c= tensor(9.0726e+08, device='cuda:0')
c= tensor(9.0826e+08, device='cuda:0')
c= tensor(9.2202e+08, device='cuda:0')
c= tensor(9.2204e+08, device='cuda:0')
c= tensor(9.2212e+08, device='cuda:0')
c= tensor(9.2295e+08, device='cuda:0')
c= tensor(9.2295e+08, device='cuda:0')
c= tensor(9.2410e+08, device='cuda:0')
c= tensor(9.2694e+08, device='cuda:0')
c= tensor(9.3552e+08, device='cuda:0')
c= tensor(9.3561e+08, device='cuda:0')
c= tensor(9.3567e+08, device='cuda:0')
c= tensor(9.3570e+08, device='cuda:0')
c= tensor(9.3571e+08, device='cuda:0')
c= tensor(9.3578e+08, device='cuda:0')
c= tensor(9.3580e+08, device='cuda:0')
c= tensor(9.3598e+08, device='cuda:0')
c= tensor(9.5002e+08, device='cuda:0')
c= tensor(9.5125e+08, device='cuda:0')
c= tensor(9.5129e+08, device='cuda:0')
c= tensor(9.5133e+08, device='cuda:0')
c= tensor(9.5385e+08, device='cuda:0')
c= tensor(9.5408e+08, device='cuda:0')
c= tensor(9.5408e+08, device='cuda:0')
c= tensor(9.5413e+08, device='cuda:0')
c= tensor(9.8090e+08, device='cuda:0')
c= tensor(9.8092e+08, device='cuda:0')
c= tensor(9.8751e+08, device='cuda:0')
c= tensor(9.8752e+08, device='cuda:0')
c= tensor(9.8838e+08, device='cuda:0')
c= tensor(9.8977e+08, device='cuda:0')
c= tensor(1.0014e+09, device='cuda:0')
c= tensor(1.0022e+09, device='cuda:0')
c= tensor(1.0023e+09, device='cuda:0')
c= tensor(1.0040e+09, device='cuda:0')
c= tensor(1.0056e+09, device='cuda:0')
c= tensor(1.0056e+09, device='cuda:0')
c= tensor(1.0060e+09, device='cuda:0')
c= tensor(1.0098e+09, device='cuda:0')
c= tensor(1.0264e+09, device='cuda:0')
c= tensor(1.0282e+09, device='cuda:0')
c= tensor(1.0282e+09, device='cuda:0')
c= tensor(1.0282e+09, device='cuda:0')
c= tensor(1.0371e+09, device='cuda:0')
c= tensor(1.0555e+09, device='cuda:0')
c= tensor(1.0556e+09, device='cuda:0')
c= tensor(1.0556e+09, device='cuda:0')
c= tensor(1.0562e+09, device='cuda:0')
c= tensor(1.0579e+09, device='cuda:0')
c= tensor(1.0658e+09, device='cuda:0')
c= tensor(1.0659e+09, device='cuda:0')
c= tensor(1.0660e+09, device='cuda:0')
c= tensor(1.0660e+09, device='cuda:0')
c= tensor(1.0660e+09, device='cuda:0')
c= tensor(1.0661e+09, device='cuda:0')
c= tensor(1.0661e+09, device='cuda:0')
c= tensor(1.0686e+09, device='cuda:0')
c= tensor(1.0688e+09, device='cuda:0')
c= tensor(1.0690e+09, device='cuda:0')
c= tensor(1.0691e+09, device='cuda:0')
c= tensor(1.0692e+09, device='cuda:0')
c= tensor(1.1579e+09, device='cuda:0')
c= tensor(1.1579e+09, device='cuda:0')
c= tensor(1.1613e+09, device='cuda:0')
c= tensor(1.1613e+09, device='cuda:0')
c= tensor(1.1613e+09, device='cuda:0')
c= tensor(1.1613e+09, device='cuda:0')
c= tensor(1.1620e+09, device='cuda:0')
c= tensor(1.1620e+09, device='cuda:0')
c= tensor(1.1627e+09, device='cuda:0')
c= tensor(1.1628e+09, device='cuda:0')
c= tensor(1.1628e+09, device='cuda:0')
c= tensor(1.1749e+09, device='cuda:0')
c= tensor(1.1754e+09, device='cuda:0')
c= tensor(1.1754e+09, device='cuda:0')
c= tensor(1.1776e+09, device='cuda:0')
c= tensor(1.1798e+09, device='cuda:0')
c= tensor(1.1798e+09, device='cuda:0')
c= tensor(1.1799e+09, device='cuda:0')
c= tensor(1.1800e+09, device='cuda:0')
c= tensor(1.1800e+09, device='cuda:0')
c= tensor(1.1800e+09, device='cuda:0')
c= tensor(1.1800e+09, device='cuda:0')
c= tensor(1.1800e+09, device='cuda:0')
c= tensor(1.1800e+09, device='cuda:0')
c= tensor(1.1800e+09, device='cuda:0')
c= tensor(1.1800e+09, device='cuda:0')
c= tensor(1.1979e+09, device='cuda:0')
c= tensor(1.1979e+09, device='cuda:0')
c= tensor(1.1990e+09, device='cuda:0')
c= tensor(1.1991e+09, device='cuda:0')
c= tensor(1.1993e+09, device='cuda:0')
c= tensor(1.2001e+09, device='cuda:0')
c= tensor(1.8667e+09, device='cuda:0')
c= tensor(1.8982e+09, device='cuda:0')
c= tensor(1.8985e+09, device='cuda:0')
c= tensor(1.9005e+09, device='cuda:0')
c= tensor(1.9005e+09, device='cuda:0')
c= tensor(1.9013e+09, device='cuda:0')
c= tensor(1.9023e+09, device='cuda:0')
c= tensor(1.9034e+09, device='cuda:0')
c= tensor(1.9034e+09, device='cuda:0')
c= tensor(1.9042e+09, device='cuda:0')
c= tensor(1.9129e+09, device='cuda:0')
c= tensor(1.9130e+09, device='cuda:0')
c= tensor(1.9130e+09, device='cuda:0')
c= tensor(1.9134e+09, device='cuda:0')
c= tensor(1.9135e+09, device='cuda:0')
c= tensor(1.9135e+09, device='cuda:0')
c= tensor(1.9179e+09, device='cuda:0')
c= tensor(1.9180e+09, device='cuda:0')
c= tensor(1.9180e+09, device='cuda:0')
c= tensor(1.9194e+09, device='cuda:0')
c= tensor(1.9194e+09, device='cuda:0')
c= tensor(1.9194e+09, device='cuda:0')
c= tensor(1.9209e+09, device='cuda:0')
c= tensor(1.9218e+09, device='cuda:0')
c= tensor(1.9225e+09, device='cuda:0')
c= tensor(1.9244e+09, device='cuda:0')
c= tensor(1.9246e+09, device='cuda:0')
c= tensor(1.9246e+09, device='cuda:0')
c= tensor(1.9249e+09, device='cuda:0')
c= tensor(1.9261e+09, device='cuda:0')
c= tensor(1.9282e+09, device='cuda:0')
c= tensor(1.9282e+09, device='cuda:0')
c= tensor(2.0294e+09, device='cuda:0')
c= tensor(2.0564e+09, device='cuda:0')
c= tensor(2.0580e+09, device='cuda:0')
c= tensor(2.0585e+09, device='cuda:0')
c= tensor(2.0586e+09, device='cuda:0')
c= tensor(2.0587e+09, device='cuda:0')
c= tensor(2.0587e+09, device='cuda:0')
c= tensor(2.0587e+09, device='cuda:0')
c= tensor(2.0619e+09, device='cuda:0')
c= tensor(2.0637e+09, device='cuda:0')
c= tensor(2.0857e+09, device='cuda:0')
c= tensor(2.0935e+09, device='cuda:0')
c= tensor(2.0942e+09, device='cuda:0')
c= tensor(2.0944e+09, device='cuda:0')
c= tensor(2.0949e+09, device='cuda:0')
c= tensor(2.0949e+09, device='cuda:0')
c= tensor(2.0950e+09, device='cuda:0')
c= tensor(2.1000e+09, device='cuda:0')
c= tensor(2.1001e+09, device='cuda:0')
c= tensor(2.1001e+09, device='cuda:0')
c= tensor(2.1002e+09, device='cuda:0')
c= tensor(2.1010e+09, device='cuda:0')
c= tensor(2.1012e+09, device='cuda:0')
c= tensor(2.1018e+09, device='cuda:0')
c= tensor(2.1020e+09, device='cuda:0')
c= tensor(2.1023e+09, device='cuda:0')
c= tensor(2.1023e+09, device='cuda:0')
c= tensor(2.1023e+09, device='cuda:0')
c= tensor(2.1026e+09, device='cuda:0')
c= tensor(2.1028e+09, device='cuda:0')
c= tensor(2.1028e+09, device='cuda:0')
c= tensor(2.1069e+09, device='cuda:0')
c= tensor(2.1069e+09, device='cuda:0')
c= tensor(2.1070e+09, device='cuda:0')
c= tensor(2.1071e+09, device='cuda:0')
c= tensor(2.1077e+09, device='cuda:0')
c= tensor(2.1077e+09, device='cuda:0')
c= tensor(2.1083e+09, device='cuda:0')
c= tensor(2.1084e+09, device='cuda:0')
c= tensor(2.1084e+09, device='cuda:0')
c= tensor(2.1087e+09, device='cuda:0')
c= tensor(2.1607e+09, device='cuda:0')
c= tensor(2.1607e+09, device='cuda:0')
c= tensor(2.1607e+09, device='cuda:0')
c= tensor(2.1622e+09, device='cuda:0')
c= tensor(2.1622e+09, device='cuda:0')
c= tensor(2.1797e+09, device='cuda:0')
c= tensor(2.1797e+09, device='cuda:0')
c= tensor(2.1809e+09, device='cuda:0')
c= tensor(2.1919e+09, device='cuda:0')
c= tensor(2.1919e+09, device='cuda:0')
c= tensor(2.2114e+09, device='cuda:0')
c= tensor(2.2119e+09, device='cuda:0')
c= tensor(2.2294e+09, device='cuda:0')
c= tensor(2.2294e+09, device='cuda:0')
c= tensor(2.2303e+09, device='cuda:0')
c= tensor(2.2303e+09, device='cuda:0')
c= tensor(2.2303e+09, device='cuda:0')
c= tensor(2.2303e+09, device='cuda:0')
c= tensor(2.2312e+09, device='cuda:0')
c= tensor(2.2315e+09, device='cuda:0')
c= tensor(2.2321e+09, device='cuda:0')
c= tensor(2.2322e+09, device='cuda:0')
c= tensor(2.2322e+09, device='cuda:0')
c= tensor(2.2322e+09, device='cuda:0')
c= tensor(2.2345e+09, device='cuda:0')
c= tensor(2.2346e+09, device='cuda:0')
c= tensor(2.2464e+09, device='cuda:0')
c= tensor(2.2466e+09, device='cuda:0')
c= tensor(2.2466e+09, device='cuda:0')
c= tensor(2.2466e+09, device='cuda:0')
c= tensor(2.2466e+09, device='cuda:0')
c= tensor(2.4264e+09, device='cuda:0')
c= tensor(2.4264e+09, device='cuda:0')
c= tensor(2.4265e+09, device='cuda:0')
c= tensor(2.4286e+09, device='cuda:0')
c= tensor(2.4294e+09, device='cuda:0')
c= tensor(2.4294e+09, device='cuda:0')
c= tensor(2.4295e+09, device='cuda:0')
c= tensor(2.4301e+09, device='cuda:0')
c= tensor(2.4303e+09, device='cuda:0')
c= tensor(2.4309e+09, device='cuda:0')
c= tensor(2.4314e+09, device='cuda:0')
c= tensor(2.4327e+09, device='cuda:0')
c= tensor(2.4330e+09, device='cuda:0')
c= tensor(2.4377e+09, device='cuda:0')
c= tensor(2.4386e+09, device='cuda:0')
c= tensor(2.4387e+09, device='cuda:0')
c= tensor(2.4414e+09, device='cuda:0')
c= tensor(2.4418e+09, device='cuda:0')
c= tensor(2.4418e+09, device='cuda:0')
c= tensor(2.4419e+09, device='cuda:0')
c= tensor(2.4419e+09, device='cuda:0')
c= tensor(2.4440e+09, device='cuda:0')
c= tensor(2.4441e+09, device='cuda:0')
c= tensor(2.4441e+09, device='cuda:0')
c= tensor(2.4442e+09, device='cuda:0')
c= tensor(2.4445e+09, device='cuda:0')
c= tensor(2.4454e+09, device='cuda:0')
c= tensor(2.4461e+09, device='cuda:0')
c= tensor(2.4461e+09, device='cuda:0')
c= tensor(2.4462e+09, device='cuda:0')
c= tensor(2.4463e+09, device='cuda:0')
c= tensor(2.4463e+09, device='cuda:0')
c= tensor(2.4463e+09, device='cuda:0')
c= tensor(2.4464e+09, device='cuda:0')
c= tensor(2.4536e+09, device='cuda:0')
c= tensor(2.4536e+09, device='cuda:0')
c= tensor(2.4536e+09, device='cuda:0')
c= tensor(2.4536e+09, device='cuda:0')
c= tensor(2.4587e+09, device='cuda:0')
c= tensor(2.4733e+09, device='cuda:0')
c= tensor(2.4735e+09, device='cuda:0')
c= tensor(2.4735e+09, device='cuda:0')
c= tensor(2.4746e+09, device='cuda:0')
c= tensor(2.4772e+09, device='cuda:0')
c= tensor(2.4772e+09, device='cuda:0')
c= tensor(2.4774e+09, device='cuda:0')
c= tensor(2.4775e+09, device='cuda:0')
c= tensor(2.4775e+09, device='cuda:0')
c= tensor(2.4780e+09, device='cuda:0')
c= tensor(2.4785e+09, device='cuda:0')
c= tensor(2.4786e+09, device='cuda:0')
c= tensor(2.4786e+09, device='cuda:0')
c= tensor(2.4786e+09, device='cuda:0')
c= tensor(2.4789e+09, device='cuda:0')
c= tensor(2.4791e+09, device='cuda:0')
c= tensor(2.4795e+09, device='cuda:0')
c= tensor(2.4797e+09, device='cuda:0')
c= tensor(2.4852e+09, device='cuda:0')
c= tensor(2.4852e+09, device='cuda:0')
c= tensor(2.4852e+09, device='cuda:0')
c= tensor(2.4853e+09, device='cuda:0')
c= tensor(2.4857e+09, device='cuda:0')
c= tensor(2.4858e+09, device='cuda:0')
c= tensor(2.4858e+09, device='cuda:0')
c= tensor(2.4858e+09, device='cuda:0')
c= tensor(2.4870e+09, device='cuda:0')
c= tensor(2.4870e+09, device='cuda:0')
c= tensor(2.4875e+09, device='cuda:0')
c= tensor(2.4875e+09, device='cuda:0')
c= tensor(2.4875e+09, device='cuda:0')
c= tensor(2.4877e+09, device='cuda:0')
c= tensor(2.4877e+09, device='cuda:0')
c= tensor(2.4877e+09, device='cuda:0')
c= tensor(2.4905e+09, device='cuda:0')
c= tensor(2.4965e+09, device='cuda:0')
c= tensor(2.4992e+09, device='cuda:0')
c= tensor(2.5055e+09, device='cuda:0')
c= tensor(2.5056e+09, device='cuda:0')
c= tensor(2.5057e+09, device='cuda:0')
c= tensor(2.5057e+09, device='cuda:0')
c= tensor(2.5070e+09, device='cuda:0')
c= tensor(2.5070e+09, device='cuda:0')
c= tensor(2.5071e+09, device='cuda:0')
c= tensor(2.5071e+09, device='cuda:0')
c= tensor(2.5120e+09, device='cuda:0')
c= tensor(2.5125e+09, device='cuda:0')
c= tensor(2.5127e+09, device='cuda:0')
c= tensor(2.5174e+09, device='cuda:0')
c= tensor(2.5179e+09, device='cuda:0')
c= tensor(2.5180e+09, device='cuda:0')
c= tensor(2.5202e+09, device='cuda:0')
c= tensor(2.5232e+09, device='cuda:0')
c= tensor(2.5241e+09, device='cuda:0')
c= tensor(2.5241e+09, device='cuda:0')
c= tensor(2.5250e+09, device='cuda:0')
c= tensor(2.5250e+09, device='cuda:0')
c= tensor(2.5255e+09, device='cuda:0')
c= tensor(2.5838e+09, device='cuda:0')
c= tensor(2.5871e+09, device='cuda:0')
c= tensor(2.5875e+09, device='cuda:0')
c= tensor(2.5875e+09, device='cuda:0')
c= tensor(2.5878e+09, device='cuda:0')
c= tensor(2.5878e+09, device='cuda:0')
c= tensor(2.5879e+09, device='cuda:0')
c= tensor(2.5881e+09, device='cuda:0')
c= tensor(2.5981e+09, device='cuda:0')
c= tensor(2.5981e+09, device='cuda:0')
c= tensor(2.6325e+09, device='cuda:0')
c= tensor(2.6327e+09, device='cuda:0')
c= tensor(2.6328e+09, device='cuda:0')
c= tensor(2.6328e+09, device='cuda:0')
c= tensor(2.6339e+09, device='cuda:0')
c= tensor(2.6375e+09, device='cuda:0')
c= tensor(2.6375e+09, device='cuda:0')
c= tensor(2.6843e+09, device='cuda:0')
c= tensor(2.6848e+09, device='cuda:0')
c= tensor(2.6863e+09, device='cuda:0')
c= tensor(2.6864e+09, device='cuda:0')
c= tensor(2.6864e+09, device='cuda:0')
c= tensor(2.6865e+09, device='cuda:0')
c= tensor(2.6865e+09, device='cuda:0')
c= tensor(2.6865e+09, device='cuda:0')
c= tensor(2.6923e+09, device='cuda:0')
c= tensor(2.9100e+09, device='cuda:0')
c= tensor(2.9101e+09, device='cuda:0')
c= tensor(2.9118e+09, device='cuda:0')
c= tensor(2.9119e+09, device='cuda:0')
c= tensor(2.9120e+09, device='cuda:0')
c= tensor(2.9120e+09, device='cuda:0')
c= tensor(2.9170e+09, device='cuda:0')
c= tensor(2.9171e+09, device='cuda:0')
c= tensor(3.0907e+09, device='cuda:0')
c= tensor(3.0907e+09, device='cuda:0')
c= tensor(3.0924e+09, device='cuda:0')
c= tensor(3.0925e+09, device='cuda:0')
c= tensor(3.0943e+09, device='cuda:0')
c= tensor(3.1101e+09, device='cuda:0')
c= tensor(3.1102e+09, device='cuda:0')
c= tensor(3.1102e+09, device='cuda:0')
c= tensor(3.1116e+09, device='cuda:0')
c= tensor(3.1116e+09, device='cuda:0')
c= tensor(3.1117e+09, device='cuda:0')
c= tensor(3.1142e+09, device='cuda:0')
c= tensor(3.1375e+09, device='cuda:0')
c= tensor(3.1376e+09, device='cuda:0')
c= tensor(3.1378e+09, device='cuda:0')
c= tensor(3.1412e+09, device='cuda:0')
c= tensor(3.1465e+09, device='cuda:0')
c= tensor(3.1466e+09, device='cuda:0')
c= tensor(3.1466e+09, device='cuda:0')
c= tensor(3.1641e+09, device='cuda:0')
c= tensor(3.1644e+09, device='cuda:0')
c= tensor(3.1648e+09, device='cuda:0')
c= tensor(3.1650e+09, device='cuda:0')
c= tensor(3.1654e+09, device='cuda:0')
c= tensor(3.1663e+09, device='cuda:0')
c= tensor(3.2212e+09, device='cuda:0')
c= tensor(3.2216e+09, device='cuda:0')
c= tensor(3.2216e+09, device='cuda:0')
c= tensor(3.2216e+09, device='cuda:0')
c= tensor(3.2232e+09, device='cuda:0')
c= tensor(3.2237e+09, device='cuda:0')
c= tensor(3.2246e+09, device='cuda:0')
c= tensor(3.2268e+09, device='cuda:0')
c= tensor(3.2268e+09, device='cuda:0')
c= tensor(3.2269e+09, device='cuda:0')
c= tensor(3.2277e+09, device='cuda:0')
c= tensor(3.2302e+09, device='cuda:0')
c= tensor(3.2303e+09, device='cuda:0')
c= tensor(3.2320e+09, device='cuda:0')
c= tensor(3.3492e+09, device='cuda:0')
c= tensor(3.3495e+09, device='cuda:0')
c= tensor(3.3495e+09, device='cuda:0')
c= tensor(3.3495e+09, device='cuda:0')
c= tensor(3.3555e+09, device='cuda:0')
c= tensor(3.3565e+09, device='cuda:0')
c= tensor(3.3572e+09, device='cuda:0')
c= tensor(3.3572e+09, device='cuda:0')
c= tensor(3.3572e+09, device='cuda:0')
c= tensor(3.3600e+09, device='cuda:0')
c= tensor(3.3607e+09, device='cuda:0')
c= tensor(3.3612e+09, device='cuda:0')
c= tensor(3.3612e+09, device='cuda:0')
c= tensor(3.3612e+09, device='cuda:0')
c= tensor(3.3613e+09, device='cuda:0')
c= tensor(3.3616e+09, device='cuda:0')
c= tensor(3.3626e+09, device='cuda:0')
c= tensor(3.3627e+09, device='cuda:0')
c= tensor(3.3741e+09, device='cuda:0')
c= tensor(3.3741e+09, device='cuda:0')
c= tensor(3.3743e+09, device='cuda:0')
c= tensor(3.3894e+09, device='cuda:0')
c= tensor(3.3894e+09, device='cuda:0')
c= tensor(3.3897e+09, device='cuda:0')
c= tensor(3.3936e+09, device='cuda:0')
c= tensor(3.3937e+09, device='cuda:0')
c= tensor(3.3944e+09, device='cuda:0')
c= tensor(3.3977e+09, device='cuda:0')
c= tensor(3.3977e+09, device='cuda:0')
c= tensor(3.3980e+09, device='cuda:0')
c= tensor(3.3980e+09, device='cuda:0')
c= tensor(3.4201e+09, device='cuda:0')
c= tensor(3.4229e+09, device='cuda:0')
c= tensor(3.4233e+09, device='cuda:0')
c= tensor(3.4239e+09, device='cuda:0')
c= tensor(3.4239e+09, device='cuda:0')
c= tensor(3.4242e+09, device='cuda:0')
c= tensor(3.4243e+09, device='cuda:0')
c= tensor(3.4243e+09, device='cuda:0')
c= tensor(3.4264e+09, device='cuda:0')
c= tensor(3.4297e+09, device='cuda:0')
c= tensor(3.4297e+09, device='cuda:0')
c= tensor(3.4299e+09, device='cuda:0')
c= tensor(3.4299e+09, device='cuda:0')
c= tensor(3.4410e+09, device='cuda:0')
c= tensor(3.4411e+09, device='cuda:0')
c= tensor(3.4411e+09, device='cuda:0')
c= tensor(3.4418e+09, device='cuda:0')
c= tensor(3.4424e+09, device='cuda:0')
c= tensor(3.4425e+09, device='cuda:0')
c= tensor(3.4429e+09, device='cuda:0')
c= tensor(3.4430e+09, device='cuda:0')
time to make c is 12.536906003952026
time for making loss is 12.536925792694092
p0 True
it  0 : 705227776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4031967232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  8% |
memory (bytes)
4032180224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  533778430.0
relative error loss 0.15503153
shape of L is 
torch.Size([])
memory (bytes)
4062593024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
4062691328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  518915330.0
relative error loss 0.15071467
shape of L is 
torch.Size([])
memory (bytes)
4065931264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% |  8% |
memory (bytes)
4066140160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  497826560.0
relative error loss 0.14458962
shape of L is 
torch.Size([])
memory (bytes)
4069404672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  8% |
memory (bytes)
4069404672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  482611970.0
relative error loss 0.14017066
shape of L is 
torch.Size([])
memory (bytes)
4072538112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4072538112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  473468930.0
relative error loss 0.13751514
shape of L is 
torch.Size([])
memory (bytes)
4075819008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4075819008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  465672200.0
relative error loss 0.13525064
shape of L is 
torch.Size([])
memory (bytes)
4079054848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4079054848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  461885200.0
relative error loss 0.13415074
shape of L is 
torch.Size([])
memory (bytes)
4082163712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4082163712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  456190980.0
relative error loss 0.1324969
shape of L is 
torch.Size([])
memory (bytes)
4085493760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4085497856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  453886200.0
relative error loss 0.1318275
shape of L is 
torch.Size([])
memory (bytes)
4088655872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  8% |
memory (bytes)
4088655872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  450449150.0
relative error loss 0.13082923
time to take a step is 417.94998598098755
it  1 : 948676608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4091944960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4091944960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  450449150.0
relative error loss 0.13082923
shape of L is 
torch.Size([])
memory (bytes)
4095188992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4095188992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  448888320.0
relative error loss 0.1303759
shape of L is 
torch.Size([])
memory (bytes)
4098420736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% |  8% |
memory (bytes)
4098420736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  446686980.0
relative error loss 0.12973654
shape of L is 
torch.Size([])
memory (bytes)
4101685248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  4% |  8% |
memory (bytes)
4101685248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  445287680.0
relative error loss 0.12933013
shape of L is 
torch.Size([])
memory (bytes)
4104912896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  8% |
memory (bytes)
4104912896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  443088400.0
relative error loss 0.12869136
shape of L is 
torch.Size([])
memory (bytes)
4108152832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
4108152832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  441958400.0
relative error loss 0.12836316
shape of L is 
torch.Size([])
memory (bytes)
4111392768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
4111392768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  441059840.0
relative error loss 0.12810218
shape of L is 
torch.Size([])
memory (bytes)
4114616320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4114616320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  440177400.0
relative error loss 0.1278459
shape of L is 
torch.Size([])
memory (bytes)
4117733376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4117848064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  439098620.0
relative error loss 0.12753257
shape of L is 
torch.Size([])
memory (bytes)
4121083904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4121083904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  438410750.0
relative error loss 0.12733278
time to take a step is 421.55174374580383
it  2 : 948676608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4124307456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4124307456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  8% |
error is  438410750.0
relative error loss 0.12733278
shape of L is 
torch.Size([])
memory (bytes)
4127494144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4127543296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  437740800.0
relative error loss 0.1271382
shape of L is 
torch.Size([])
memory (bytes)
4130775040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% |  8% |
memory (bytes)
4130775040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  437276930.0
relative error loss 0.12700348
shape of L is 
torch.Size([])
memory (bytes)
4133953536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4134006784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  436216830.0
relative error loss 0.12669557
shape of L is 
torch.Size([])
memory (bytes)
4137209856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% |  8% |
memory (bytes)
4137230336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  435858430.0
relative error loss 0.12659149
shape of L is 
torch.Size([])
memory (bytes)
4140457984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4140457984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  435452160.0
relative error loss 0.12647349
shape of L is 
torch.Size([])
memory (bytes)
4143636480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4143636480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  434797570.0
relative error loss 0.12628336
shape of L is 
torch.Size([])
memory (bytes)
4146921472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4146921472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  434298620.0
relative error loss 0.12613845
shape of L is 
torch.Size([])
memory (bytes)
4150046720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4150157312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  433919740.0
relative error loss 0.1260284
shape of L is 
torch.Size([])
memory (bytes)
4153380864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4153380864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% |  8% |
error is  433610500.0
relative error loss 0.12593858
time to take a step is 414.19792675971985
it  3 : 948676608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4156608512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4156608512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  433610500.0
relative error loss 0.12593858
shape of L is 
torch.Size([])
memory (bytes)
4159758336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% |  8% |
memory (bytes)
4159758336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  433142000.0
relative error loss 0.12580252
shape of L is 
torch.Size([])
memory (bytes)
4163072000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  8% |
memory (bytes)
4163072000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  433059070.0
relative error loss 0.12577844
shape of L is 
torch.Size([])
memory (bytes)
4166303744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4166303744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  432508400.0
relative error loss 0.1256185
shape of L is 
torch.Size([])
memory (bytes)
4169547776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4169547776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  432281340.0
relative error loss 0.12555255
shape of L is 
torch.Size([])
memory (bytes)
4172775424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4172775424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  431955460.0
relative error loss 0.1254579
shape of L is 
torch.Size([])
memory (bytes)
4175974400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4175974400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  431578100.0
relative error loss 0.1253483
shape of L is 
torch.Size([])
memory (bytes)
4179238912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4179238912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  431384830.0
relative error loss 0.12529217
shape of L is 
torch.Size([])
memory (bytes)
4182466560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  8% |
memory (bytes)
4182466560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  431082000.0
relative error loss 0.1252042
shape of L is 
torch.Size([])
memory (bytes)
4185673728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  8% |
memory (bytes)
4185673728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  8% |
error is  430946560.0
relative error loss 0.12516487
time to take a step is 427.3601586818695
it  4 : 948676608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4188934144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  8% |
memory (bytes)
4188934144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  8% |
error is  430946560.0
relative error loss 0.12516487
shape of L is 
torch.Size([])
memory (bytes)
4192157696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
4192157696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  430726660.0
relative error loss 0.125101
shape of L is 
torch.Size([])
memory (bytes)
4195401728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
4195401728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  430554620.0
relative error loss 0.12505104
shape of L is 
torch.Size([])
memory (bytes)
4198621184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4198621184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  430357000.0
relative error loss 0.12499364
shape of L is 
torch.Size([])
memory (bytes)
4201873408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
4201873408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  430201340.0
relative error loss 0.12494843
shape of L is 
torch.Size([])
memory (bytes)
4204978176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4204978176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  430048500.0
relative error loss 0.12490404
shape of L is 
torch.Size([])
memory (bytes)
4208316416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4208316416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  430046720.0
relative error loss 0.124903515
shape of L is 
torch.Size([])
memory (bytes)
4211544064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4211544064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  429917700.0
relative error loss 0.124866046
shape of L is 
torch.Size([])
memory (bytes)
4214644736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
4214784000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  429723400.0
relative error loss 0.12480961
shape of L is 
torch.Size([])
memory (bytes)
4218007552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4218007552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  429560830.0
relative error loss 0.12476239
time to take a step is 446.4386157989502
it  5 : 948676608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4221186048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  8% |
memory (bytes)
4221235200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  429560830.0
relative error loss 0.12476239
shape of L is 
torch.Size([])
memory (bytes)
4224475136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  8% |
memory (bytes)
4224475136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  429417730.0
relative error loss 0.124720834
shape of L is 
torch.Size([])
memory (bytes)
4227710976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4227719168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  429309950.0
relative error loss 0.12468953
shape of L is 
torch.Size([])
memory (bytes)
4230811648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4230950912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  429205000.0
relative error loss 0.12465905
shape of L is 
torch.Size([])
memory (bytes)
4234178560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% |  8% |
memory (bytes)
4234178560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  429072130.0
relative error loss 0.12462045
shape of L is 
torch.Size([])
memory (bytes)
4237393920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
4237393920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  428976640.0
relative error loss 0.12459272
shape of L is 
torch.Size([])
memory (bytes)
4240629760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4240629760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  429079040.0
relative error loss 0.124622464
shape of L is 
torch.Size([])
memory (bytes)
4243869696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4243869696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  428885500.0
relative error loss 0.12456625
shape of L is 
torch.Size([])
memory (bytes)
4246986752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4247097344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  428770300.0
relative error loss 0.1245328
shape of L is 
torch.Size([])
memory (bytes)
4250320896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  8% |
memory (bytes)
4250320896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  428724480.0
relative error loss 0.12451948
time to take a step is 413.29394006729126
it  6 : 948676608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4253544448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4253544448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  428724480.0
relative error loss 0.12451948
shape of L is 
torch.Size([])
memory (bytes)
4256624640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4256772096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  428664320.0
relative error loss 0.12450201
shape of L is 
torch.Size([])
memory (bytes)
4260007936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4260007936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  428540400.0
relative error loss 0.124466024
shape of L is 
torch.Size([])
memory (bytes)
4263235584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4263235584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  428533760.0
relative error loss 0.12446409
shape of L is 
torch.Size([])
memory (bytes)
4266455040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4266455040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  428465150.0
relative error loss 0.124444164
shape of L is 
torch.Size([])
memory (bytes)
4269686784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
4269686784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  428381700.0
relative error loss 0.12441993
shape of L is 
torch.Size([])
memory (bytes)
4272865280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4272865280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  8% |
error is  428296450.0
relative error loss 0.12439516
shape of L is 
torch.Size([])
memory (bytes)
4276137984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4276137984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  428187900.0
relative error loss 0.12436364
shape of L is 
torch.Size([])
memory (bytes)
4279373824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  8% |
memory (bytes)
4279373824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  428117250.0
relative error loss 0.12434312
shape of L is 
torch.Size([])
memory (bytes)
4282609664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4282609664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% |  8% |
error is  427991550.0
relative error loss 0.12430661
time to take a step is 434.7166967391968
it  7 : 948676608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4285841408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  8% |
memory (bytes)
4285841408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% |  8% |
error is  427991550.0
relative error loss 0.12430661
shape of L is 
torch.Size([])
memory (bytes)
4288925696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4289064960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  427940350.0
relative error loss 0.12429174
shape of L is 
torch.Size([])
memory (bytes)
4292313088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
4292313088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  427904500.0
relative error loss 0.12428133
shape of L is 
torch.Size([])
memory (bytes)
4295540736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4295540736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  427756300.0
relative error loss 0.12423828
shape of L is 
torch.Size([])
memory (bytes)
4298657792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
4298657792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% |  8% |
error is  427736320.0
relative error loss 0.12423248
shape of L is 
torch.Size([])
memory (bytes)
4302004224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  8% |
memory (bytes)
4302004224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  427635460.0
relative error loss 0.12420318
shape of L is 
torch.Size([])
memory (bytes)
4305231872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% |  8% |
memory (bytes)
4305231872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  427606270.0
relative error loss 0.12419471
shape of L is 
torch.Size([])
memory (bytes)
4308439040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4308439040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  427526140.0
relative error loss 0.124171436
shape of L is 
torch.Size([])
memory (bytes)
4311711744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
4311711744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  427433730.0
relative error loss 0.1241446
shape of L is 
torch.Size([])
memory (bytes)
4314800128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% |  8% |
memory (bytes)
4314939392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  427410180.0
relative error loss 0.12413775
time to take a step is 436.385737657547
it  8 : 948676608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4318171136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  8% |
memory (bytes)
4318171136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  427410180.0
relative error loss 0.12413775
shape of L is 
torch.Size([])
memory (bytes)
4321398784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4321398784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  427270400.0
relative error loss 0.12409716
shape of L is 
torch.Size([])
memory (bytes)
4324495360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4324630528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  427228160.0
relative error loss 0.12408489
shape of L is 
torch.Size([])
memory (bytes)
4327837696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4327837696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  427153660.0
relative error loss 0.12406325
shape of L is 
torch.Size([])
memory (bytes)
4331065344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4331065344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  427111680.0
relative error loss 0.12405106
shape of L is 
torch.Size([])
memory (bytes)
4334292992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4334292992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  427067140.0
relative error loss 0.12403812
shape of L is 
torch.Size([])
memory (bytes)
4337537024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% |  8% |
memory (bytes)
4337537024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  427018240.0
relative error loss 0.12402392
shape of L is 
torch.Size([])
memory (bytes)
4340654080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4340764672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  426977020.0
relative error loss 0.12401195
shape of L is 
torch.Size([])
memory (bytes)
4344000512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4344000512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  426913280.0
relative error loss 0.123993434
shape of L is 
torch.Size([])
memory (bytes)
4347125760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% |  8% |
memory (bytes)
4347236352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  426837500.0
relative error loss 0.123971425
time to take a step is 412.05476212501526
it  9 : 948676608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4350472192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4350472192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  426837500.0
relative error loss 0.123971425
shape of L is 
torch.Size([])
memory (bytes)
4353687552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4353687552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  426791420.0
relative error loss 0.123958044
shape of L is 
torch.Size([])
memory (bytes)
4356907008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4356907008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  426745600.0
relative error loss 0.12394474
shape of L is 
torch.Size([])
memory (bytes)
4360163328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4360163328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  426743550.0
relative error loss 0.12394414
shape of L is 
torch.Size([])
memory (bytes)
4363255808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  8% |
memory (bytes)
4363390976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  426704130.0
relative error loss 0.12393269
shape of L is 
torch.Size([])
memory (bytes)
4366635008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4366635008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% |  8% |
error is  426645250.0
relative error loss 0.12391559
shape of L is 
torch.Size([])
memory (bytes)
4369866752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4369866752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  426614270.0
relative error loss 0.12390659
shape of L is 
torch.Size([])
memory (bytes)
4372979712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4373090304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  426578180.0
relative error loss 0.12389611
shape of L is 
torch.Size([])
memory (bytes)
4376317952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4376317952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  426546940.0
relative error loss 0.12388704
shape of L is 
torch.Size([])
memory (bytes)
4379500544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4379500544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  426519550.0
relative error loss 0.12387908
time to take a step is 418.5313665866852
it  10 : 948676608
| ID | GPU | MEM |
------------------
|  0 | 15% |  0% |
|  1 |  9% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4382781440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4382781440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  426519550.0
relative error loss 0.12387908
shape of L is 
torch.Size([])
memory (bytes)
4386017280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4386017280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  426494980.0
relative error loss 0.123871945
shape of L is 
torch.Size([])
memory (bytes)
4389113856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4389249024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% |  8% |
error is  426473730.0
relative error loss 0.12386577
shape of L is 
torch.Size([])
memory (bytes)
4392464384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4392464384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 89% |  8% |
error is  426486800.0
relative error loss 0.12386956
shape of L is 
torch.Size([])
memory (bytes)
4395618304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4395618304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  426457340.0
relative error loss 0.123861015
shape of L is 
torch.Size([])
memory (bytes)
4398927872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4398931968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  426436600.0
relative error loss 0.12385499
shape of L is 
torch.Size([])
memory (bytes)
4402159616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4402159616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  426421760.0
relative error loss 0.12385068
shape of L is 
torch.Size([])
memory (bytes)
4405256192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% |  8% |
memory (bytes)
4405256192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  426399740.0
relative error loss 0.12384428
shape of L is 
torch.Size([])
memory (bytes)
4408606720
| ID | GPU | MEM |
------------------
|  0 |  2% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4408606720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  426382600.0
relative error loss 0.123839304
shape of L is 
torch.Size([])
memory (bytes)
4411842560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
4411842560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  426366460.0
relative error loss 0.12383462
time to take a step is 418.1009452342987
it  11 : 948676608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4415074304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
4415074304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  426366460.0
relative error loss 0.12383462
shape of L is 
torch.Size([])
memory (bytes)
4418306048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4418306048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  426346500.0
relative error loss 0.12382882
shape of L is 
torch.Size([])
memory (bytes)
4421406720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4421545984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  426322180.0
relative error loss 0.12382176
shape of L is 
torch.Size([])
memory (bytes)
4424773632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4424773632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  426316030.0
relative error loss 0.12381997
shape of L is 
torch.Size([])
memory (bytes)
4428001280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4428001280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  426258180.0
relative error loss 0.12380317
shape of L is 
torch.Size([])
memory (bytes)
4431171584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  8% |
memory (bytes)
4431171584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  426245120.0
relative error loss 0.123799376
shape of L is 
torch.Size([])
memory (bytes)
4434460672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  8% |
memory (bytes)
4434460672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  426202370.0
relative error loss 0.123786956
shape of L is 
torch.Size([])
memory (bytes)
4437512192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4437676032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  426171650.0
relative error loss 0.12377804
shape of L is 
torch.Size([])
memory (bytes)
4440915968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  8% |
memory (bytes)
4440915968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  426118140.0
relative error loss 0.123762496
shape of L is 
torch.Size([])
memory (bytes)
4444147712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% |  8% |
memory (bytes)
4444147712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  426085380.0
relative error loss 0.12375298
time to take a step is 416.04005908966064
it  12 : 948676608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4447232000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
4447367168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% |  8% |
error is  426085380.0
relative error loss 0.12375298
shape of L is 
torch.Size([])
memory (bytes)
4450607104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4450607104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  426048260.0
relative error loss 0.1237422
shape of L is 
torch.Size([])
memory (bytes)
4453834752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4453834752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  425996800.0
relative error loss 0.123727255
shape of L is 
torch.Size([])
memory (bytes)
4457070592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
4457070592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  425972480.0
relative error loss 0.12372019
shape of L is 
torch.Size([])
memory (bytes)
4460294144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4460294144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  425936130.0
relative error loss 0.12370963
shape of L is 
torch.Size([])
memory (bytes)
4463460352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4463460352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  425921540.0
relative error loss 0.123705395
shape of L is 
torch.Size([])
memory (bytes)
4466753536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% |  8% |
memory (bytes)
4466753536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  425892600.0
relative error loss 0.12369699
shape of L is 
torch.Size([])
memory (bytes)
4469985280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4469985280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  8% |
error is  425893900.0
relative error loss 0.12369736
shape of L is 
torch.Size([])
memory (bytes)
4473217024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4473217024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  425876740.0
relative error loss 0.12369238
shape of L is 
torch.Size([])
memory (bytes)
4476448768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% |  8% |
memory (bytes)
4476448768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  425848580.0
relative error loss 0.1236842
time to take a step is 412.9235908985138
it  13 : 948676608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4479631360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  8% |
memory (bytes)
4479631360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  425848580.0
relative error loss 0.1236842
shape of L is 
torch.Size([])
memory (bytes)
4482904064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
4482904064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  425820670.0
relative error loss 0.1236761
shape of L is 
torch.Size([])
memory (bytes)
4486041600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4486152192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  425798400.0
relative error loss 0.12366963
shape of L is 
torch.Size([])
memory (bytes)
4489388032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4489388032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  425775870.0
relative error loss 0.12366308
shape of L is 
torch.Size([])
memory (bytes)
4492623872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4492623872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  425756930.0
relative error loss 0.123657584
shape of L is 
torch.Size([])
memory (bytes)
4495781888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
4495781888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  425736700.0
relative error loss 0.123651706
shape of L is 
torch.Size([])
memory (bytes)
4499091456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4499091456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  425726460.0
relative error loss 0.12364873
shape of L is 
torch.Size([])
memory (bytes)
4502282240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% |  8% |
memory (bytes)
4502282240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  425688060.0
relative error loss 0.12363758
shape of L is 
torch.Size([])
memory (bytes)
4505554944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4505554944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  425667840.0
relative error loss 0.12363171
shape of L is 
torch.Size([])
memory (bytes)
4508786688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4508786688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  425647360.0
relative error loss 0.12362576
time to take a step is 413.8461310863495
it  14 : 948676608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4511887360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
4511887360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  425647360.0
relative error loss 0.12362576
shape of L is 
torch.Size([])
memory (bytes)
4515241984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% |  8% |
memory (bytes)
4515241984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  425615600.0
relative error loss 0.12361654
shape of L is 
torch.Size([])
memory (bytes)
4518432768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  8% |
memory (bytes)
4518432768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  425566720.0
relative error loss 0.12360234
shape of L is 
torch.Size([])
memory (bytes)
4521713664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4521713664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  425521150.0
relative error loss 0.123589106
shape of L is 
torch.Size([])
memory (bytes)
4524941312
| ID | GPU | MEM |
------------------
|  0 | 19% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4524941312
| ID | GPU | MEM |
------------------
|  0 | 17% |  0% |
|  1 | 90% |  8% |
error is  425492740.0
relative error loss 0.12358085
shape of L is 
torch.Size([])
memory (bytes)
4528181248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4528181248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  425464580.0
relative error loss 0.12357267
shape of L is 
torch.Size([])
memory (bytes)
4531412992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4531412992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  425433340.0
relative error loss 0.1235636
shape of L is 
torch.Size([])
memory (bytes)
4534620160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4534620160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  425420540.0
relative error loss 0.123559885
shape of L is 
torch.Size([])
memory (bytes)
4537880576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4537880576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  425387260.0
relative error loss 0.123550214
shape of L is 
torch.Size([])
memory (bytes)
4541104128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4541104128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  425360900.0
relative error loss 0.12354256
time to take a step is 412.21762013435364
sum tnnu_Z after tensor(6536824., device='cuda:0')
shape of features
(2398,)
shape of features
(2398,)
number of orig particles 9592
number of new particles after remove low mass 8231
tnuZ shape should be parts x labs
torch.Size([9592, 702])
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
error without small mass is  533756400.0
relative error without small mass is  0.15502514
nnu_Z shape should be number of particles by maxV
(9592, 702)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
shape of features
(9592,)
Thu Feb 2 17:01:17 EST 2023
