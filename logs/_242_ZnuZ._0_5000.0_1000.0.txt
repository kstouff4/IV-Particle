Thu Feb 2 09:00:08 EST 2023
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  0% |  0% |
numbers of X: 19034837
numbers of Z: 15833
shape of features
(15833,)
shape of features
(15833,)
ZX	Vol	Parts	Cubes	Eps
Z	0.014981647042111832	15833	15.833	0.09817451922198706
X	0.01018094180219568	488	0.488	0.27528932925222493
X	0.012920598045587558	7922	7.922	0.1177106955556531
X	0.013268290779702794	1845	1.845	0.1930217298365795
X	0.01040422244894864	1147	1.147	0.20855250166571912
X	0.012005743194338619	18930	18.93	0.08591707405239478
X	0.010748274478155203	25342	25.342	0.0751333270422907
X	0.011891590704500408	31840	31.84	0.07201488938170444
X	0.010755984287064802	21336	21.336	0.07958765685670836
X	0.011375113391622201	2584	2.584	0.16389073707498752
X	0.010997976231318445	17744	17.744	0.08526165321442447
X	0.01066695747752142	4497	4.497	0.133364188108789
X	0.01068981849394917	38510	38.51	0.06523273751170293
X	0.01256914458302085	3436	3.436	0.1540816269759639
X	0.011859360601541741	124608	124.608	0.045656762894503634
X	0.01069777311672504	11236	11.236	0.09837707010674626
X	0.011279719860987667	32510	32.51	0.07026846201216068
X	0.010718633366550998	33113	33.113	0.06866155407731946
X	0.010620189286578303	18114	18.114	0.08369624320353872
X	0.01202769685848752	108243	108.243	0.04807591416600175
X	0.010770266904112799	65574	65.574	0.05476439108520487
X	0.010485325161473248	7032	7.032	0.11424424895946396
X	0.010882746384143402	175948	175.948	0.03954741105041961
X	0.010694674824348402	7981	7.981	0.110247840433353
X	0.010689342324535372	6493	6.493	0.118077874678765
X	0.010652781209350929	11825	11.825	0.09658001888881759
X	0.010783797500970664	35952	35.952	0.06693956559286099
X	0.012929849003887137	18384	18.384	0.0889305846091208
X	0.011087086724883803	6393	6.393	0.1201445974808202
X	0.010766312369232003	29442	29.442	0.07150996342464949
X	0.011217967278984001	852442	852.442	0.02360929995239208
X	0.010665364673644663	2976	2.976	0.15303153255314667
X	0.013872369610095792	224521	224.521	0.039533435912939965
X	0.0106661244733393	4386	4.386	0.13447637907983906
X	0.010655779537727348	4247	4.247	0.13588379116435362
X	0.010657987307628498	3757	3.757	0.1415613409540774
X	0.0119983398556614	53612	53.612	0.0607136556974849
X	0.010803110830919404	34457	34.457	0.06793451140503644
X	0.010560235264483518	1287	1.287	0.20169649390178862
X	0.010763415934805683	3446	3.446	0.146176743784857
X	0.010639931303116862	1445	1.445	0.19454660613902008
X	0.010582975145087522	2497	2.497	0.16183150683956027
X	0.010265827069505771	628	0.628	0.25379210048461237
X	0.010442286016409279	454	0.454	0.28438929450305095
X	0.011026508396237382	1222	1.222	0.20818795643860885
X	0.010300338730048382	621	0.621	0.2550272780137178
X	0.011201782842603316	679	0.679	0.2545691612385145
X	0.010613782053100084	2880	2.88	0.15446350755251118
X	0.01073779347946769	2962	2.962	0.1536184330604948
X	0.011661822897035412	678	0.678	0.2581342207524515
X	0.010663029506012585	4678	4.678	0.1316053257536206
X	0.011268516935149306	6980	6.98	0.11731056136831827
X	0.010612654443046082	1198	1.198	0.20691361505315253
X	0.010686951620359002	2367	2.367	0.1652793242204468
X	0.010289601442802	1335	1.335	0.19753274358632458
X	0.011093492030972547	3183	3.183	0.1516155965460419
X	0.010493489227853788	2330	2.33	0.16514090743135015
X	0.01059168772974922	1524	1.524	0.19083589544521914
X	0.01062028839172662	1911	1.911	0.17712989873407128
X	0.01056104143483158	1390	1.39	0.1965911585607883
X	0.010573046501013362	1160	1.16	0.20888826899212581
X	0.011907191018141706	3322	3.322	0.15303939172972303
X	0.01068160856896827	1208	1.208	0.20678700425616311
X	0.011793059921832291	4444	4.444	0.1384471919655713
X	0.010689354900780138	4552	4.552	0.1329177806024354
X	0.010625129259051201	1605	1.605	0.18776719929754343
X	0.010626721252157845	1771	1.771	0.1817161704962665
X	0.01091420375537973	1851	1.851	0.18065994742631813
X	0.010515544813704002	2280	2.28	0.16645581718227206
X	0.010590780809312732	2120	2.12	0.17094800609259952
X	0.010468264861790403	925	0.925	0.22451446307601863
X	0.010170011894251643	1894	1.894	0.1751111615173357
X	0.010716420435176196	1648	1.648	0.18665122540151938
X	0.010582806790232699	1504	1.504	0.19162448926829187
X	0.01002183279598125	1249	1.249	0.20019884677645278
X	0.010402720183825204	1715	1.715	0.18237316318308675
X	0.010758113969398941	1819	1.819	0.18084276861315823
X	0.010463644474840005	774	0.774	0.23822134452603247
X	0.010463900653493667	520	0.52	0.27199656620567436
X	0.010625935698998102	1414	1.414	0.19587210532725097
X	0.013926237392577088	5036	5.036	0.14036193851266313
X	0.011061623147550203	1963	1.963	0.17795062582133048
X	0.010055434793872615	574	0.574	0.2597145737747211
X	0.01194969345689403	3570	3.57	0.1495878026846071
X	0.0103668992027736	870	0.87	0.22840724445686356
X	0.01049452091293725	799	0.799	0.23594201242678173
X	0.01061896520861265	1143	1.143	0.21022223829773745
X	0.010790758599026801	1461	1.461	0.19474542040959728
X	0.0102817862947422	467	0.467	0.28027470068237126
X	0.012283769907390482	3055	3.055	0.1590157539704821
X	0.010740972913692203	1880	1.88	0.17877025407325123
X	0.01062310393182546	1737	1.737	0.1828733991250453
X	0.010610875308116	909	0.909	0.2268449467510346
X	0.010450407341801506	672	0.672	0.2496057861832157
X	0.012458522751877019	1621	1.621	0.19734578043735634
X	0.010320260959183649	2437	2.437	0.16178753195259485
X	0.0105927465816198	2278	2.278	0.16691099813055993
X	0.011439515447773926	2267	2.267	0.17152155408586162
X	0.010598847594509206	3327	3.327	0.1471415349295355
X	0.010530960051548799	2272	2.272	0.16673235309736564
X	0.010802611705888771	1937	1.937	0.1773371461020765
X	0.010617233691299077	4921	4.921	0.12921690575766145
X	0.010536113307149429	1927	1.927	0.1761703860114018
X	0.010572802985011554	2493	2.493	0.16186611890998526
X	0.010487507662922882	768	0.768	0.23902153092382936
X	0.010510575762639293	2427	2.427	0.16299923903906247
X	0.010408634050895201	990	0.99	0.21907165123913747
X	0.01071845127133424	4427	4.427	0.1342788166460126
X	0.010414162899960033	399	0.399	0.2966314242559329
X	0.010532325483942225	1152	1.152	0.20910155583730733
X	0.010572853588550576	945	0.945	0.22365921858057597
X	0.010870253614105955	1161	1.161	0.2107669377468183
X	0.010362717175279822	366	0.366	0.30478775847776585
X	0.01071476244619851	1226	1.226	0.20598267424381744
X	0.011877217767707988	2245	2.245	0.17424732700908965
X	0.012304000276091137	2411	2.411	0.17216691885043917
X	0.010568224697762607	1072	1.072	0.214421872546252
X	0.010708893366760603	1254	1.254	0.20440068765631575
X	0.01018202261954745	465	0.465	0.2797652063908482
X	0.01186681769267635	3464	3.464	0.15074819786798824
X	0.010500518333212501	829	0.829	0.2331052576063969
X	0.010601509716451993	7423	7.423	0.11261495951962984
X	0.010477328817989249	1144	1.144	0.2092224017118623
X	0.011929350031149324	859	0.859	0.24036683187410732
X	0.010510682936556265	1198	1.198	0.20624877247898799
X	0.010651372227514992	951	0.951	0.22373899433973135
X	0.010723224461448322	1330	1.33	0.20052010281822927
X	0.010677299003981301	915	0.915	0.22681935105433718
X	0.0105918436333017	1548	1.548	0.1898454537248385
X	0.012339189798846403	8098	8.098	0.11507199010201843
X	0.01054337977107234	1748	1.748	0.18203132930088825
X	0.010757987807842253	6034	6.034	0.12125749179664183
X	0.010418237496606001	895	0.895	0.22663330877368404
X	0.010641237254020702	2421	2.421	0.1638069958386652
X	0.010254519869268001	682	0.682	0.24681808898479973
X	0.0107424356166854	1713	1.713	0.1844088666923615
X	0.01017203024798268	1017	1.017	0.21545780445271637
X	0.010441237186979042	676	0.676	0.24903960671938796
X	0.011233598713248428	1342	1.342	0.2030433623873294
X	0.010960749642469951	982	0.982	0.22348219250118692
X	0.010415057391682742	1560	1.56	0.1882979924122458
X	0.010543195684728	1943	1.943	0.1757248395886981
X	0.010617351701834718	429	0.429	0.29142019010647496
X	0.012035324108195454	4373	4.373	0.14013913664906705
X	0.010674029080957203	5155	5.155	0.1274578448060719
X	0.011710609588301828	2870	2.87	0.15979598702079195
X	0.01040191941530735	995	0.995	0.218657041699728
X	0.0106121531353088	864	0.864	0.23072602743992157
X	0.0104049073877112	1470	1.47	0.1920025344261805
X	0.010669083897226652	1208	1.208	0.20670615025788433
X	0.010684025848022399	1150	1.15	0.21022241936365493
X	0.011731327071979963	1588	1.588	0.19475960349034382
X	0.013169903548877827	3072	3.072	0.16245027879228702
X	0.010614772182505469	1967	1.967	0.17540236276044519
X	0.010511282341989538	5239	5.239	0.12612536452573708
X	0.01019138927199783	1741	1.741	0.1802235498583218
X	0.013470945604451466	6185	6.185	0.12962421107029887
X	0.010635398182891467	1958	1.958	0.17578440997686148
X	0.010424723798865713	1201	1.201	0.20551357620846408
X	0.010633466089566133	1633	1.633	0.18673664647635077
X	0.010313667523707494	298	0.298	0.3258859379989957
X	0.013306231094001569	4563	4.563	0.14286796684152142
X	0.010403288490571141	898	0.898	0.22627232278187664
X	0.01130758897235842	2033	2.033	0.1771784419534321
X	0.010104792451016802	749	0.749	0.238057495353627
X	0.010623649866807859	1348	1.348	0.19900376157850194
X	0.01051278691419151	866	0.866	0.22982643999074306
X	0.010555679703132202	1279	1.279	0.20208708202480397
X	0.010424505166161481	1594	1.594	0.18700607308039255
X	0.010299856480407021	1443	1.443	0.19254028664866465
X	0.010006219728639038	372	0.372	0.29962337972452696
X	0.010227734857966781	617	0.617	0.25497529267482194
X	0.010157565794215522	800	0.8	0.23329202115649073
X	0.01092739901877193	3104	3.104	0.15212421952035446
X	0.010436472660905761	1179	1.179	0.20686164703932733
X	0.010726781833207704	2995	2.995	0.15299980372808564
X	0.01156065187986276	1640	1.64	0.19174004511179507
X	0.010493539461407112	1955	1.955	0.17508881876617183
X	0.01038855147624086	2431	2.431	0.16227689116819063
X	0.010472840254760755	2308	2.308	0.16555522489336372
X	0.010449887199093223	1547	1.547	0.18903421998524472
X	0.010669601505177003	1700	1.7	0.18445896154704522
X	0.011051319401644479	1499	1.499	0.19462748291708526
X	0.010578561039680023	2001	2.001	0.17420468712867482
X	0.010563052308459202	1241	1.241	0.20417656023531022
X	0.011366563475153217	4319	4.319	0.13806448984438147
X	0.010700877709086802	2616	2.616	0.1599289995264309
X	0.010522225026367711	1546	1.546	0.18951024740016806
X	0.010562494220496002	1723	1.723	0.18301797071403839
X	0.012522818477860048	2861	2.861	0.16357919847918134
X	0.011622411756686192	8893	8.893	0.10933251080342803
X	0.010436538712141457	1184	1.184	0.20657048158215918
X	0.01139185363811235	474	0.474	0.28858384684061816
X	0.010655468437816007	1556	1.556	0.18989825973073385
X	0.011396960423716247	1041	1.041	0.22204761027130987
X	0.010548956554605613	1707	1.707	0.1835095460358588
X	0.012020556274401462	1524	1.524	0.19905810544555436
X	0.010484517729996782	1188	1.188	0.20665409041969743
X	0.01062576033405747	712	0.712	0.24620300016108815
X	0.01047153167360802	1409	1.409	0.19514903401014
X	0.010587066973311424	688	0.688	0.2487305178531501
X	0.011627449151960281	2661	2.661	0.16348573273937653
X	0.01039324723704788	875	0.875	0.2281643221548449
X	0.01072974729552018	3983	3.983	0.13914247659502385
X	0.013223936346246903	2860	2.86	0.16659612886846722
X	0.010676135376152866	2067	2.067	0.17285868247601244
X	0.01032971770189485	668	0.668	0.2491364834807453
X	0.014679349436498767	2385	2.385	0.18326216314675425
X	0.010576840477004763	1990	1.99	0.17451561521852643
X	0.010427622396407835	1184	1.184	0.20651163792226543
X	0.013305572086940097	5083	5.083	0.1378175359432374
X	0.010528763059766102	4286	4.286	0.13492996112318278
X	0.010234643364338152	1395	1.395	0.1943119464060834
X	0.01052853178392864	1639	1.639	0.185892964358433
X	0.0111857988191262	1311	1.311	0.20434065013402788
X	0.009809437450311412	586	0.586	0.2558088088253476
X	0.010331388052342351	695	0.695	0.24588080323892272
X	0.011410245300477852	538	0.538	0.27680306953749395
X	0.010464379526969603	582	0.582	0.2619772019063579
X	0.010455320573126302	1626	1.626	0.18595403393887516
X	0.010555559007685861	1663	1.663	0.1851525351518118
X	0.010542226466553598	1965	1.965	0.17506121120035442
X	0.010649124915289037	931	0.931	0.22531394939735563
X	0.010652029247287517	2449	2.449	0.16323546536667366
X	0.011822157057527956	1282	1.282	0.20970215805107656
X	0.011001643071539037	1902	1.902	0.1795073956624302
X	0.010522952448302205	2152	2.152	0.16973254127645146
X	0.010440142481488758	1264	1.264	0.2021403983720831
X	0.010704737392315901	475	0.475	0.2824625026461164
X	0.010398556774693991	2346	2.346	0.16426625391613875
X	0.01033714066207977	418	0.418	0.29134532122645457
X	0.01063568864996513	908	0.908	0.22710494407657644
X	0.010647645896185944	1273	1.273	0.2029901971213329
X	0.011912720710843297	1635	1.635	0.193864252229917
X	0.014251653034789848	2440	2.44	0.18009091505713562
X	0.010609205645518299	1300	1.3	0.20133216876083593
X	0.010382756717247976	1337	1.337	0.19802822294696343
X	0.01154529211946055	1338	1.338	0.2051081594635784
X	0.011601327565904165	2808	2.808	0.16046125895937377
X	0.011245545763108019	794	0.794	0.24194687193429776
X	0.010671734947556764	3453	3.453	0.14566196039303925
X	0.010711121267204112	24213	24.213	0.07619540434739684
X	0.010740709000634303	6466	6.466	0.1184310964426199
X	0.010713129696396288	2713	2.713	0.15806007716564272
X	0.010103077240406743	363	0.363	0.30305097301431816
X	0.011140917701820059	5824	5.824	0.12413660581871548
X	0.011536106346040503	14177	14.177	0.09335945415733914
X	0.010733129656597349	126666	126.666	0.04392267412262331
X	0.010618939271198722	1510	1.51	0.19158789388427427
X	0.011660870447783828	35614	35.614	0.06892393623952099
X	0.0107443311236577	12337	12.337	0.09549704643960122
X	0.010678873735805677	18747	18.747	0.08289555127561267
X	0.0115584723772488	100499	100.499	0.04863096636439937
X	0.010635729791943	819	0.819	0.23505047391581999
X	0.010485791978563386	2350	2.35	0.1646308138650084
X	0.010754673024553518	89406	89.406	0.049363981624214404
X	0.011520282072063492	203643	203.643	0.03838821120218073
X	0.010542017178514742	2088	2.088	0.17155277272747896
X	0.010743174591057551	16971	16.971	0.08586346296703387
X	0.0113752692959136	26245	26.245	0.07567835846693767
X	0.010553087345699782	40164	40.164	0.06404927106034669
X	0.010753558297133666	23525	23.525	0.07703253452349297
X	0.010707546189623808	12202	12.202	0.09573843235341173
X	0.010841706421315	18092	18.092	0.08430830434925972
X	0.010951389326586502	5847	5.847	0.12326658799257786
X	0.010629551304645523	1598	1.598	0.188067052962044
X	0.012520351650850494	196942	196.942	0.03991101085385523
X	0.010811690293242263	2463	2.463	0.16373557963969815
X	0.011228525081683101	2585	2.585	0.1631626393759151
X	0.01161640899003115	16167	16.167	0.08956688808004863
X	0.010503984040321199	12427	12.427	0.09455022898259442
X	0.011608357665757251	145693	145.693	0.04303059634835506
X	0.010811315676119983	43233	43.233	0.06300200804527675
X	0.010513041947572863	620	0.62	0.2569087538635581
X	0.011606881247612735	11984	11.984	0.09893984984296698
X	0.010461813575349201	4317	4.317	0.13432029678732968
X	0.010765626034644604	10512	10.512	0.10079786014188294
X	0.010847202200321702	32607	32.607	0.06928974517056062
X	0.01038698995809969	2285	2.285	0.16565371086935388
X	0.010737598603975182	18850	18.85	0.08289568732411849
X	0.0105264214444736	365	0.365	0.30666388253392596
X	0.010668716212102403	2798	2.798	0.15622654115859796
X	0.010720950053197823	32672	32.672	0.06897406957383416
X	0.010666799387833669	13417	13.417	0.09263879610610762
X	0.010660115677419998	14326	14.326	0.09061755154937369
X	0.010259837958796383	22432	22.432	0.0770473137741368
X	0.010972758920766665	127548	127.548	0.04414491939293195
X	0.011766323741989694	5180	5.18	0.13145295236823037
X	0.010718691530533689	9952	9.952	0.10250470539462449
X	0.011216228078140681	35360	35.36	0.06819902908446906
X	0.011133214415009283	2717	2.717	0.160020999340024
X	0.010710526257543321	54341	54.341	0.05819616877260175
X	0.012420815492386768	116127	116.127	0.04746851380767795
X	0.011014226577652138	151485	151.485	0.0417376626989788
X	0.010884746824458473	8542	8.542	0.10841422134424361
X	0.0106479701186871	9851	9.851	0.10262712629495753
X	0.010665555818233497	4963	4.963	0.12904655224013253
X	0.010586197132877822	2657	2.657	0.15853170879985432
X	0.010773742789136587	12987	12.987	0.09396207253441126
X	0.012745624138812358	5587	5.587	0.1316419010339435
X	0.010829002321598313	7430	7.43	0.11337915371894593
X	0.012440144943216214	41386	41.386	0.06698702065566245
X	0.010824589399098763	32255	32.255	0.06949253083416017
X	0.010528951499668005	4003	4.003	0.13803837720078596
X	0.013396702352428553	4580	4.58	0.1430136453595515
X	0.012000202245979396	74657	74.657	0.054371670588287895
X	0.01073507706955516	8088	8.088	0.10989754935260315
X	0.010734502409642481	2044	2.044	0.1738202387534515
X	0.011553855184237363	5147	5.147	0.13093555211859154
X	0.013409744527337454	314429	314.429	0.03493810158160555
X	0.010660779215699	5872	5.872	0.12199282846828223
X	0.01057600832896466	64716	64.716	0.05467263923506425
X	0.01059996286011696	2375	2.375	0.16464437956839886
X	0.0111891720689235	26317	26.317	0.07519469302264428
X	0.011167178448177132	8858	8.858	0.10802789825247858
X	0.013452234335392667	182007	182.007	0.04196643818374182
X	0.010652968382740002	24708	24.708	0.07554592012504613
X	0.010626722431923032	3253	3.253	0.14837876702175892
X	0.010759738802163453	56322	56.322	0.05759364345776983
X	0.01246457302155744	45548	45.548	0.06492361565298693
X	0.010707888017827112	9862	9.862	0.1027810241484915
X	0.010793440072228204	24450	24.45	0.07614247809399304
X	0.013835455675687851	55352	55.352	0.06299219133435494
X	0.013847620685544643	123908	123.908	0.04816799885914183
X	0.01073374145467958	14388	14.388	0.09069504633271284
X	0.010761924055503023	1268	1.268	0.2039812379445983
X	0.010740237674688081	6489	6.489	0.11828927572624486
X	0.010748915535205801	28364	28.364	0.07236563730430114
X	0.0107406183272917	71757	71.757	0.05309516851206858
X	0.010761219437862403	5269	5.269	0.1268754988869098
X	0.012027121020462292	1059	1.059	0.22477874406403714
X	0.011430286056525	15327	15.327	0.09068455244192711
X	0.012851692984856777	58536	58.536	0.06032709786462436
X	0.010475962305279661	53745	53.745	0.05798094462751626
X	0.010707666549806136	2753	2.753	0.15726408131689273
X	0.010739056082767267	12048	12.048	0.0962388409281989
X	0.013269401894161576	8407	8.407	0.11643139479949534
X	0.011782347539096896	3712	3.712	0.14696298138062902
X	0.010687430530032179	5578	5.578	0.12420286054063105
X	0.0108154589660988	3757	3.757	0.1422551243534786
X	0.012191370172828413	48911	48.911	0.06293351544866403
X	0.010814762283770762	19878	19.878	0.08163594614462005
X	0.011118432072407524	8633	8.633	0.10879959249556709
X	0.011799379282982506	9208	9.208	0.10861703673024721
X	0.010665437228743437	3995	3.995	0.13872475296768924
X	0.010847818430222124	198004	198.004	0.037980103846730645
X	0.01065600643626486	3061	3.061	0.1515573657762896
X	0.011576775091371635	59542	59.542	0.0579323829144498
X	0.010479410116768001	1200	1.2	0.20592948354601084
X	0.01061946788402472	1762	1.762	0.1819836133786294
X	0.010496132264866837	1927	1.927	0.17594726756499693
X	0.011290231609084081	11654	11.654	0.09894851462594151
X	0.013430622225247124	2318	2.318	0.17960878181046178
X	0.010665231036414239	19682	19.682	0.08152679072169103
X	0.013231395598117915	3167	3.167	0.16105932093248154
X	0.010926115171977483	2743	2.743	0.1585185126694396
X	0.011956385815207868	109457	109.457	0.04780266691437452
X	0.010783504221972053	12387	12.387	0.09548414384590566
X	0.011277221402799242	8792	8.792	0.10865210446014348
X	0.010728130816892983	52967	52.967	0.058727236057271294
X	0.011870045721682187	71658	71.658	0.05491983659188906
X	0.011801299415624222	3025	3.025	0.15742298753018355
X	0.010588247736108707	3446	3.446	0.1453794228231281
X	0.011814369747803252	9932	9.932	0.10595577097105141
X	0.01051758095223956	590	0.59	0.2612289374434453
X	0.013709826604326082	2373	2.373	0.1794367555138125
X	0.010721041653446351	9483	9.483	0.10417505640119165
X	0.010319805327296701	1415	1.415	0.19392703389475496
X	0.0104787432609601	2598	2.598	0.15918055614605134
X	0.010663408540802497	2679	2.679	0.1584799935764691
X	0.010665949841946602	2244	2.244	0.16813529678298675
X	0.01080869431102261	125779	125.779	0.04412875744161681
X	0.010561399864733208	5066	5.066	0.12774740889747008
X	0.010748850698766717	26955	26.955	0.07360504199280012
X	0.010881294986015998	5213	5.213	0.12780002918709302
X	0.010600491727526355	8609	8.609	0.10718263621453995
X	0.010623444955544044	14347	14.347	0.0904693405276424
X	0.012196881260507685	257716	257.716	0.036171718131684044
X	0.01232816860597399	288953	288.953	0.034942753069616146
X	0.010841294320517605	20336	20.336	0.08108462898693225
X	0.010779732621565109	34829	34.829	0.06764291703266107
X	0.010627636415881997	1258	1.258	0.2036660696163707
X	0.010787635280372654	37327	37.327	0.06611516042526332
X	0.010748370361660594	11239	11.239	0.09852315645038527
X	0.012304899438506236	46932	46.932	0.06400339366276067
X	0.010632626528494803	5309	5.309	0.12604993074394313
X	0.010760816719653418	32310	32.31	0.06931641483348361
X	0.013224138693000301	161225	161.225	0.04344885505506571
X	0.010617690177503506	25775	25.775	0.07440642837053278
X	0.010667666036158378	2625	2.625	0.15958057826114813
X	0.014729350723003257	13506	13.506	0.10293244431295824
X	0.010672536309411371	4859	4.859	0.12998909315269933
X	0.010704462840439358	1815	1.815	0.18067417619269957
X	0.011880529484287087	63223	63.223	0.05727787520599528
X	0.010603705335083643	10495	10.495	0.10034407547458069
X	0.010529366301908863	1096	1.096	0.2125839929419219
X	0.010835583336496079	35547	35.547	0.06730021435558856
X	0.014335908897586395	5980	5.98	0.1338365345426252
X	0.01028652949073932	468	0.468	0.2801179939307035
X	0.011524102988392085	61942	61.942	0.05708744887129291
X	0.012036546836373459	37054	37.054	0.06874201701481916
X	0.01146682072255093	14700	14.7	0.09205383708816783
X	0.010785112829667803	37007	37.007	0.0663000115794608
X	0.010759966106814189	40331	40.331	0.0643759974297155
X	0.01033747769739296	4497	4.497	0.13197669021232875
X	0.010736669127628343	18223	18.223	0.08383330054699593
X	0.012533413852290504	52846	52.846	0.061899302214232926
X	0.011296956758456921	51333	51.333	0.06037469365529331
X	0.0106603339824128	4147	4.147	0.1369868645169697
X	0.0124168027570451	47421	47.421	0.06397540218350681
X	0.012426431121942078	79504	79.504	0.05386654810694524
X	0.012404768546480793	31384	31.384	0.07338829366008472
X	0.010729819607670294	20361	20.361	0.08077266997060732
X	0.011300422960722	15232	15.232	0.09052723503342379
X	0.01072160493427774	4442	4.442	0.1341406527685164
X	0.010720090532798402	3117	3.117	0.150945632050785
X	0.010792712976196562	3701	3.701	0.14286872888669355
X	0.010850149640359399	72047	72.047	0.053203467985601306
X	0.012761928089615127	60491	60.491	0.05953081702571555
X	0.012451693852347862	145375	145.375	0.04408047193248814
X	0.01156564443096328	89634	89.634	0.05053190677570993
X	0.011263746030792573	29319	29.319	0.0726961200197323
X	0.01066003589031008	8372	8.372	0.10838682195967493
X	0.010820975547971703	32171	32.171	0.06954521994942921
X	0.0104149770160272	957	0.957	0.2216063119057078
X	0.010550729869105864	4969	4.969	0.12852999847532814
X	0.011629986817137825	90365	90.365	0.05048856065280858
X	0.010730227204246963	10087	10.087	0.10208195193097273
X	0.010336831298079002	1444	1.444	0.19272589821041536
X	0.0106575783219978	6730	6.73	0.11655953948322312
X	0.011035781296592433	22962	22.962	0.07833054066274432
X	0.0107680497841927	14588	14.588	0.09037474814989874
X	0.011891137755900493	31102	31.102	0.07257911992654094
X	0.011519647234720562	17477	17.477	0.0870276850208549
X	0.010685530873918924	12486	12.486	0.09494182835717419
X	0.010621458531376504	2483	2.483	0.16233137954165375
X	0.012299234744323317	1932	1.932	0.1853348067367646
X	0.012331137891884503	22233	22.233	0.08216150049714203
X	0.0107958696186642	26706	26.706	0.07394059291346156
X	0.010521352153070321	6529	6.529	0.11723977134089421
X	0.011981391846712321	51822	51.822	0.061375875559360936
X	0.01062175000109571	2719	2.719	0.15749337481186346
X	0.013198298111026676	10001	10.001	0.1096877602992224
X	0.010649034267657452	2131	2.131	0.17096567487008876
X	0.010827829062471603	50108	50.108	0.060008316182727305
X	0.01148780921816394	2576	2.576	0.16460023538405297
X	0.01200791808476503	19669	19.669	0.08483241137859088
X	0.010737875655097519	4517	4.517	0.1334615281659956
X	0.010819442431700271	7574	7.574	0.11262285344515356
X	0.010680406089671008	21658	21.658	0.079005344676769
X	0.012656827491707661	131812	131.812	0.04579203778021033
X	0.010772061619189921	4647	4.647	0.13234535976179357
X	0.010610042561495099	5033	5.033	0.12822225150148875
X	0.010799432955494142	56863	56.863	0.057480922311627096
X	0.010554854287887	5258	5.258	0.12614708523567347
X	0.01160379432437808	233109	233.109	0.03678581030931602
X	0.010510766151071863	1592	1.592	0.18759896447589602
X	0.011292566901754704	42777	42.777	0.06414962414158389
X	0.012882174943612707	146650	146.65	0.04445313302291374
X	0.010373975973213502	1221	1.221	0.20405309730857185
X	0.010672949817118759	91559	91.559	0.048849614661980505
X	0.01066253758506195	12742	12.742	0.09423400691446901
X	0.012032140107269329	165496	165.496	0.041736733824893416
X	0.010596765797134462	4507	4.507	0.1329725281708305
X	0.012244281608285966	19428	19.428	0.08573702403015575
X	0.010339083937491141	888	0.888	0.22665033490555406
X	0.012230490322486142	3335	3.335	0.1542112331536661
X	0.0106971515993579	3837	3.837	0.1407425613714722
X	0.010996105010550662	32796	32.796	0.06947139189772224
X	0.010576194035873234	10740	10.74	0.09948899478570312
X	0.011022977661899398	38175	38.175	0.06609571330649605
X	0.010743653176165898	7344	7.344	0.11352018859816956
X	0.01038604403324136	2119	2.119	0.16986597683922192
X	0.010755767739738424	2075	2.075	0.1730644121266704
X	0.01149357994439842	61445	61.445	0.05719037235047924
X	0.0105103160544544	13316	13.316	0.0924160345938328
X	0.011436784736466206	152690	152.69	0.04215325515057187
X	0.012468218655949763	23343	23.343	0.08113643344660953
X	0.010740944314471208	3817	3.817	0.14118008369680257
X	0.010588115979240803	4324	4.324	0.1347858602618976
X	0.01041572405771863	5183	5.183	0.1261932384438734
X	0.013704606419465491	471749	471.749	0.030741037907210115
X	0.011666891463791868	5563	5.563	0.12800159045566042
X	0.010742733887409888	9915	9.915	0.10270873171214427
X	0.010737837843948517	50792	50.792	0.059571770683402143
X	0.010456818420673053	20883	20.883	0.07940884746415636
X	0.010683726516364083	1814	1.814	0.18059060788524653
X	0.010608712436486417	1423	1.423	0.19535262850688806
X	0.01224199671884038	69444	69.444	0.05607111299273147
X	0.011872066664529446	15393	15.393	0.09170669010235721
X	0.010745717717525475	31759	31.759	0.06968236298053644
X	0.010798433554580667	32397	32.397	0.06933491510619025
X	0.013510666505134305	31886	31.886	0.07510904570465476
X	0.010960568349683522	51033	51.033	0.05988627842602221
X	0.012181396852275482	114903	114.903	0.047328409294401706
X	0.011221457953220285	72402	72.402	0.05371549404234546
X	0.010744730410278003	7306	7.306	0.11372046281250105
X	0.010733062834341249	46104	46.104	0.06151698210856736
X	0.011156879607505117	6635	6.635	0.11891426060679429
X	0.01075464998393484	5963	5.963	0.12172426394373387
X	0.010743933886368698	7514	7.514	0.11265852101204551
X	0.010655307392851943	3668	3.668	0.14268518588761603
X	0.010643644212362243	53947	53.947	0.05821581968884613
X	0.011737653762484186	7227	7.227	0.11754597066112173
X	0.010471607928127501	1805	1.805	0.179685095862784
X	0.010700317710216204	9745	9.745	0.10316640009141967
X	0.010730015612466984	5658	5.658	0.12377866763192072
X	0.011402420848018223	38647	38.647	0.06657233036168975
X	0.01070504394607496	4499	4.499	0.1335029371128114
X	0.010445337445116417	7500	7.5	0.11167442811567521
X	0.010709033805350538	5223	5.223	0.12704085888438077
X	0.011029216637129517	22476	22.476	0.07887545568219974
X	0.012276092707667245	2088	2.088	0.1804858760244027
X	0.011110367454318362	3450	3.45	0.14767368395785582
X	0.01082788603663778	12244	12.244	0.09598575495451653
X	0.011962032401802634	82620	82.62	0.052509747259352084
X	0.011862571646277489	3282	3.282	0.1534664418076593
X	0.010207151488254672	630	0.63	0.2530393152311753
X	0.010642791049685178	2063	2.063	0.1727900654940052
X	0.013272168604535882	144530	144.53	0.045115727255889276
X	0.010708875020204681	85181	85.181	0.050095691054799565
X	0.010520311937151907	20190	20.19	0.08046940055051376
X	0.011916927705125757	2878	2.878	0.1605798957721972
X	0.010776822141436758	49948	49.948	0.05997784683053166
X	0.012128117635007847	58820	58.82	0.05907759313294592
X	0.010555729315191801	1217	1.217	0.2054624940367256
X	0.0107744042854905	4861	4.861	0.13038347411455445
X	0.010707789373739073	14053	14.053	0.09133633715825627
X	0.011752993600074884	4778	4.778	0.1349897474255576
X	0.012211759510608152	10087	10.087	0.10657910626256291
X	0.010775330440231264	51282	51.282	0.05945045931184792
X	0.010769336662695118	8156	8.156	0.1097077341764722
X	0.011307765496966597	20081	20.081	0.08257789710029843
X	0.011060861143563002	1784	1.784	0.18370939351236476
X	0.010764043361813884	15163	15.163	0.08920655497364498
X	0.010735967147246882	9532	9.532	0.10404448065384458
X	0.010752681332886241	15803	15.803	0.08795470541491689
X	0.010565189410948481	25572	25.572	0.07447963344050172
X	0.01204100211450189	133182	133.182	0.04488201708716566
X	0.010596435693411526	3508	3.508	0.14455509631092064
X	0.010759607687073656	1888	1.888	0.17862057463536993
X	0.010777994554495436	6807	6.807	0.11655405917003435
X	0.010831343028197968	18407	18.407	0.08379789306444088
X	0.010968414997714563	13688	13.688	0.0928826560309359
X	0.010636941886491752	2226	2.226	0.16843430239709029
X	0.01120981842340713	1438	1.438	0.19828045065312386
X	0.0106330278681458	15318	15.318	0.08854246918309354
X	0.010514370526802997	1349	1.349	0.1982700468804262
X	0.0104791131327784	9842	9.842	0.10211284807125619
X	0.01067672420516464	2395	2.395	0.16458016834836228
X	0.010805725931711254	7129	7.129	0.11487048229626272
X	0.010533476996829603	4490	4.49	0.1328745489785042
X	0.010486265044167362	2975	2.975	0.15218713926379404
X	0.010952098817622552	1967	1.967	0.17724105696215503
X	0.01079303086517145	52958	52.958	0.058848755254433616
X	0.010992592532201282	112595	112.595	0.04604618721645618
X	0.011424988554873344	98361	98.361	0.04879151230963217
X	0.010481812995991564	37794	37.794	0.0652135510770792
X	0.010808233941746922	14992	14.992	0.08966681029275658
X	0.010596604294104603	3920	3.92	0.13930291892217653
X	0.01365260189045845	5068	5.068	0.13914243739102977
X	0.010997447972083355	58998	58.998	0.05712394717003325
X	0.0139666951922317	5594	5.594	0.13566164046905502
X	0.010714320071716557	12960	12.96	0.09385409035718796
X	0.01058842594078632	6708	6.708	0.1164339014036334
X	0.013760839156317616	114113	114.113	0.04940488945386957
X	0.010566369970414486	10571	10.571	0.09998539808367428
X	0.012418147624502888	17317	17.317	0.08950790394389604
X	0.012072503379888256	73155	73.155	0.054850986188130994
X	0.01108596493523217	20735	20.735	0.0811627001680008
X	0.010751981568498192	11460	11.46	0.09789668018923765
X	0.01081116021914781	76684	76.684	0.05204621862394578
X	0.01094794165025316	46548	46.548	0.061727281304982146
X	0.010773099644019344	21407	21.407	0.07954171597659485
X	0.010546561731247682	10567	10.567	0.09993548638878794
X	0.010746572661393613	16423	16.423	0.0868172107652845
X	0.01063340420993855	4701	4.701	0.131268550791062
X	0.012389098048971751	39573	39.573	0.06790163602570128
X	0.01378599848896598	223234	223.234	0.039526888482150566
X	0.012501011733828242	107624	107.624	0.04879163197207748
X	0.011468243509588805	46568	46.568	0.0626810755386773
X	0.01140513610697108	4802	4.802	0.13342158806146046
X	0.011252778684473252	13794	13.794	0.09343792497115981
X	0.013890440479644101	10671	10.671	0.10918684054253427
X	0.01055425989728136	11501	11.501	0.09717712814919416
X	0.010822870878335228	23779	23.779	0.07692183396152899
X	0.011581835346947615	127803	127.803	0.044917141755294224
X	0.010189978161519042	2466	2.466	0.16046983042031476
X	0.010587814748287645	64075	64.075	0.05487475231961356
X	0.01076107501014975	31117	31.117	0.0701917353487532
X	0.01071357552581837	18196	18.196	0.08381456637119827
X	0.011198616439557033	2914	2.914	0.15663600846331555
X	0.01413369562088516	26115	26.115	0.0814932994033286
X	0.011663350653967175	58093	58.093	0.058555340314271474
X	0.010426848482013605	1988	1.988	0.1737449648434486
X	0.011023206347044066	334656	334.656	0.032055540725054156
X	0.010723175495821252	20864	20.864	0.08010173934766177
X	0.010710174561528	36535	36.535	0.06642972876565308
X	0.01191428511990474	9909	9.909	0.10633577161859903
X	0.010570296869270282	5671	5.671	0.1230672546216162
X	0.010376009078451605	1049	1.049	0.21466024380745227
X	0.010353404255910904	2356	2.356	0.1637957498362823
X	0.010655623143259262	7939	7.939	0.1103073243076787
X	0.011248216414534893	51241	51.241	0.06032379965988466
X	0.01376862632558272	972372	972.372	0.024192795606949966
X	0.010709639319378603	2246	2.246	0.1683145644681263
X	0.010840848551391	53383	53.383	0.058778716736237566
X	0.011903847026158999	3483	3.483	0.15062993461752225
X	0.012651840444552566	9990	9.99	0.10819223008879066
X	0.010731729374976963	7629	7.629	0.11204712026672985
X	0.010737836936322444	97888	97.888	0.04786989222882609
X	0.01157925911140651	5624	5.624	0.12721700102677538
X	0.012016061731599635	468112	468.112	0.029498784790821745
X	0.010651599874878393	3234	3.234	0.1487847008471056
X	0.010836353688009238	64202	64.202	0.055264325192614684
X	0.012082995405351677	14919	14.919	0.09321338915025187
X	0.010803778779584564	44932	44.932	0.06218323485823352
X	0.010755669660824323	97409	97.409	0.0479747577155117
X	0.010507808707642402	9189	9.189	0.10457181248645321
X	0.01136284469410548	4431	4.431	0.13687635776991433
X	0.011880400207313282	37451	37.451	0.06820076048859483
X	0.010679431183177834	3144	3.144	0.15032176327362692
X	0.0101773840895454	7679	7.679	0.10984420899106051
X	0.011845817269867304	44813	44.813	0.06417812184866437
X	0.01077834281877694	27343	27.343	0.07332216445586787
X	0.014881731519472984	20147	20.147	0.09039568350387342
X	0.010677681790754881	14269	14.269	0.09078786685484641
X	0.011929505387294512	97041	97.041	0.04972281695152274
X	0.010728507900100786	192593	192.593	0.03819146128649045
X	0.010498283484065804	11635	11.635	0.09663118440979741
X	0.012785028303691374	4494	4.494	0.1416955491157181
X	0.010819204346716	73065	73.065	0.052904828592664495
X	0.010732638170801537	14394	14.394	0.0906793357124902
X	0.010738398855340003	15556	15.556	0.08837861651500009
X	0.010663985461690626	16261	16.261	0.08688085739781228
X	0.010793549008255542	26512	26.512	0.07411519556501955
X	0.0108006875390064	40418	40.418	0.0644108248636771
X	0.010731163047356667	74212	74.212	0.05248770121382101
X	0.010744984524817978	13219	13.219	0.09332594522641881
X	0.011700021901774176	7893	7.893	0.11402012793720742
X	0.010418455222705464	2337	2.337	0.1645816981468401
X	0.011787396506087218	9167	9.167	0.10874189251948882
X	0.011824431307826616	48866	48.866	0.062314793090922214
X	0.010626821013311999	38773	38.773	0.06495680473756434
X	0.010765219655718306	65939	65.939	0.054654615881031585
X	0.010746802266076822	4217	4.217	0.13659198584441776
X	0.010829303524137064	13484	13.484	0.09295239712365633
X	0.012985497041603606	22291	22.291	0.08351728003512134
X	0.011992261368803426	39610	39.61	0.06714784526380164
X	0.01069744891109972	16247	16.247	0.08699661342187187
X	0.010791738379140433	28432	28.432	0.07240379542519775
X	0.011488923049814082	181561	181.561	0.03984913679170895
X	0.0107724517139344	14030	14.03	0.09156980619667755
X	0.010295770936342695	1073	1.073	0.21249712074641236
X	0.010543422143265856	3887	3.887	0.139461934447059
X	0.010902518903918582	53009	53.009	0.05902813732573527
X	0.01074730130192782	15423	15.423	0.0886564196755684
X	0.01122564057502208	28981	28.981	0.07289525428876245
X	0.010705557556473348	1001	1.001	0.22032227350254327
X	0.010549253589937009	2224	2.224	0.1680205169096219
X	0.01071334879335075	82301	82.301	0.05068040118499858
X	0.010668298120481453	36221	36.221	0.06653419433430437
X	0.0107328820196728	18514	18.514	0.08338194703197378
X	0.010685621854638373	1919	1.919	0.17724538746097507
X	0.0105353029652292	2794	2.794	0.15564680460474972
X	0.010800698801046657	13940	13.94	0.09184658794260961
X	0.010701301915469823	20659	20.659	0.08031113687541046
X	0.010596792263132431	34224	34.224	0.06765208997549631
X	0.010583944984725755	2417	2.417	0.1636026401201162
X	0.011375632792228104	41373	41.373	0.06502587184317417
X	0.010652777355673824	5673	5.673	0.12337202378207152
X	0.010468373964877502	13818	13.818	0.09116147569095748
X	0.012483483050936819	135013	135.013	0.045218907225763744
X	0.012280458353216058	3923	3.923	0.146284649734669
X	0.010720358977938091	19214	19.214	0.0823247677071514
X	0.010841467838472132	57474	57.474	0.05735072169516192
X	0.010828985858169897	13723	13.723	0.09240871001777709
X	0.012380925692852861	21318	21.318	0.08343255685353283
X	0.010837020088473003	55121	55.121	0.05814748430954617
X	0.010639466153245306	1402	1.402	0.19651269329546084
X	0.010800117712824	18404	18.404	0.08372183813028462
X	0.010831639199098358	10214	10.214	0.10197634543116321
X	0.012029600693998811	46271	46.271	0.0638235233307656
X	0.010805186140597422	53050	53.05	0.058836787870470496
X	0.01070638289157701	20087	20.087	0.08107915809929844
X	0.011375538499137604	32994	32.994	0.07012060666570559
X	0.0106694950467994	7079	7.079	0.11465438028236348
X	0.010708072990029802	6362	6.362	0.11895222027905308
X	0.010533577269166679	4943	4.943	0.12868514758814467
X	0.01064073719292837	3884	3.884	0.13992570334718618
X	0.010837328137187222	55080	55.08	0.05816245960445341
X	0.011564657327605898	100001	100.001	0.0487202463117414
X	0.01127190515382537	2190	2.19	0.17265708471566515
X	0.010727438029378422	10054	10.054	0.10218466162785195
X	0.010761153679575301	6131	6.131	0.12062643860177802
X	0.01331463934817473	122152	122.152	0.04776864713527664
X	0.010642405110450844	14483	14.483	0.09023889154319538
X	0.010552473618341279	7153	7.153	0.11383837384993849
X	0.012729934357574286	36287	36.287	0.07052748000669719
X	0.010765384007450866	45039	45.039	0.062060260690488096
X	0.011336281223568471	4218	4.218	0.13903410489590998
X	0.011742290753407303	21650	21.65	0.08155144169387707
X	0.010488147395950909	2205	2.205	0.1681757588893909
time for making epsilon is 1.0300912857055664
epsilons are
[0.27528932925222493, 0.1177106955556531, 0.1930217298365795, 0.20855250166571912, 0.08591707405239478, 0.0751333270422907, 0.07201488938170444, 0.07958765685670836, 0.16389073707498752, 0.08526165321442447, 0.133364188108789, 0.06523273751170293, 0.1540816269759639, 0.045656762894503634, 0.09837707010674626, 0.07026846201216068, 0.06866155407731946, 0.08369624320353872, 0.04807591416600175, 0.05476439108520487, 0.11424424895946396, 0.03954741105041961, 0.110247840433353, 0.118077874678765, 0.09658001888881759, 0.06693956559286099, 0.0889305846091208, 0.1201445974808202, 0.07150996342464949, 0.02360929995239208, 0.15303153255314667, 0.039533435912939965, 0.13447637907983906, 0.13588379116435362, 0.1415613409540774, 0.0607136556974849, 0.06793451140503644, 0.20169649390178862, 0.146176743784857, 0.19454660613902008, 0.16183150683956027, 0.25379210048461237, 0.28438929450305095, 0.20818795643860885, 0.2550272780137178, 0.2545691612385145, 0.15446350755251118, 0.1536184330604948, 0.2581342207524515, 0.1316053257536206, 0.11731056136831827, 0.20691361505315253, 0.1652793242204468, 0.19753274358632458, 0.1516155965460419, 0.16514090743135015, 0.19083589544521914, 0.17712989873407128, 0.1965911585607883, 0.20888826899212581, 0.15303939172972303, 0.20678700425616311, 0.1384471919655713, 0.1329177806024354, 0.18776719929754343, 0.1817161704962665, 0.18065994742631813, 0.16645581718227206, 0.17094800609259952, 0.22451446307601863, 0.1751111615173357, 0.18665122540151938, 0.19162448926829187, 0.20019884677645278, 0.18237316318308675, 0.18084276861315823, 0.23822134452603247, 0.27199656620567436, 0.19587210532725097, 0.14036193851266313, 0.17795062582133048, 0.2597145737747211, 0.1495878026846071, 0.22840724445686356, 0.23594201242678173, 0.21022223829773745, 0.19474542040959728, 0.28027470068237126, 0.1590157539704821, 0.17877025407325123, 0.1828733991250453, 0.2268449467510346, 0.2496057861832157, 0.19734578043735634, 0.16178753195259485, 0.16691099813055993, 0.17152155408586162, 0.1471415349295355, 0.16673235309736564, 0.1773371461020765, 0.12921690575766145, 0.1761703860114018, 0.16186611890998526, 0.23902153092382936, 0.16299923903906247, 0.21907165123913747, 0.1342788166460126, 0.2966314242559329, 0.20910155583730733, 0.22365921858057597, 0.2107669377468183, 0.30478775847776585, 0.20598267424381744, 0.17424732700908965, 0.17216691885043917, 0.214421872546252, 0.20440068765631575, 0.2797652063908482, 0.15074819786798824, 0.2331052576063969, 0.11261495951962984, 0.2092224017118623, 0.24036683187410732, 0.20624877247898799, 0.22373899433973135, 0.20052010281822927, 0.22681935105433718, 0.1898454537248385, 0.11507199010201843, 0.18203132930088825, 0.12125749179664183, 0.22663330877368404, 0.1638069958386652, 0.24681808898479973, 0.1844088666923615, 0.21545780445271637, 0.24903960671938796, 0.2030433623873294, 0.22348219250118692, 0.1882979924122458, 0.1757248395886981, 0.29142019010647496, 0.14013913664906705, 0.1274578448060719, 0.15979598702079195, 0.218657041699728, 0.23072602743992157, 0.1920025344261805, 0.20670615025788433, 0.21022241936365493, 0.19475960349034382, 0.16245027879228702, 0.17540236276044519, 0.12612536452573708, 0.1802235498583218, 0.12962421107029887, 0.17578440997686148, 0.20551357620846408, 0.18673664647635077, 0.3258859379989957, 0.14286796684152142, 0.22627232278187664, 0.1771784419534321, 0.238057495353627, 0.19900376157850194, 0.22982643999074306, 0.20208708202480397, 0.18700607308039255, 0.19254028664866465, 0.29962337972452696, 0.25497529267482194, 0.23329202115649073, 0.15212421952035446, 0.20686164703932733, 0.15299980372808564, 0.19174004511179507, 0.17508881876617183, 0.16227689116819063, 0.16555522489336372, 0.18903421998524472, 0.18445896154704522, 0.19462748291708526, 0.17420468712867482, 0.20417656023531022, 0.13806448984438147, 0.1599289995264309, 0.18951024740016806, 0.18301797071403839, 0.16357919847918134, 0.10933251080342803, 0.20657048158215918, 0.28858384684061816, 0.18989825973073385, 0.22204761027130987, 0.1835095460358588, 0.19905810544555436, 0.20665409041969743, 0.24620300016108815, 0.19514903401014, 0.2487305178531501, 0.16348573273937653, 0.2281643221548449, 0.13914247659502385, 0.16659612886846722, 0.17285868247601244, 0.2491364834807453, 0.18326216314675425, 0.17451561521852643, 0.20651163792226543, 0.1378175359432374, 0.13492996112318278, 0.1943119464060834, 0.185892964358433, 0.20434065013402788, 0.2558088088253476, 0.24588080323892272, 0.27680306953749395, 0.2619772019063579, 0.18595403393887516, 0.1851525351518118, 0.17506121120035442, 0.22531394939735563, 0.16323546536667366, 0.20970215805107656, 0.1795073956624302, 0.16973254127645146, 0.2021403983720831, 0.2824625026461164, 0.16426625391613875, 0.29134532122645457, 0.22710494407657644, 0.2029901971213329, 0.193864252229917, 0.18009091505713562, 0.20133216876083593, 0.19802822294696343, 0.2051081594635784, 0.16046125895937377, 0.24194687193429776, 0.14566196039303925, 0.07619540434739684, 0.1184310964426199, 0.15806007716564272, 0.30305097301431816, 0.12413660581871548, 0.09335945415733914, 0.04392267412262331, 0.19158789388427427, 0.06892393623952099, 0.09549704643960122, 0.08289555127561267, 0.04863096636439937, 0.23505047391581999, 0.1646308138650084, 0.049363981624214404, 0.03838821120218073, 0.17155277272747896, 0.08586346296703387, 0.07567835846693767, 0.06404927106034669, 0.07703253452349297, 0.09573843235341173, 0.08430830434925972, 0.12326658799257786, 0.188067052962044, 0.03991101085385523, 0.16373557963969815, 0.1631626393759151, 0.08956688808004863, 0.09455022898259442, 0.04303059634835506, 0.06300200804527675, 0.2569087538635581, 0.09893984984296698, 0.13432029678732968, 0.10079786014188294, 0.06928974517056062, 0.16565371086935388, 0.08289568732411849, 0.30666388253392596, 0.15622654115859796, 0.06897406957383416, 0.09263879610610762, 0.09061755154937369, 0.0770473137741368, 0.04414491939293195, 0.13145295236823037, 0.10250470539462449, 0.06819902908446906, 0.160020999340024, 0.05819616877260175, 0.04746851380767795, 0.0417376626989788, 0.10841422134424361, 0.10262712629495753, 0.12904655224013253, 0.15853170879985432, 0.09396207253441126, 0.1316419010339435, 0.11337915371894593, 0.06698702065566245, 0.06949253083416017, 0.13803837720078596, 0.1430136453595515, 0.054371670588287895, 0.10989754935260315, 0.1738202387534515, 0.13093555211859154, 0.03493810158160555, 0.12199282846828223, 0.05467263923506425, 0.16464437956839886, 0.07519469302264428, 0.10802789825247858, 0.04196643818374182, 0.07554592012504613, 0.14837876702175892, 0.05759364345776983, 0.06492361565298693, 0.1027810241484915, 0.07614247809399304, 0.06299219133435494, 0.04816799885914183, 0.09069504633271284, 0.2039812379445983, 0.11828927572624486, 0.07236563730430114, 0.05309516851206858, 0.1268754988869098, 0.22477874406403714, 0.09068455244192711, 0.06032709786462436, 0.05798094462751626, 0.15726408131689273, 0.0962388409281989, 0.11643139479949534, 0.14696298138062902, 0.12420286054063105, 0.1422551243534786, 0.06293351544866403, 0.08163594614462005, 0.10879959249556709, 0.10861703673024721, 0.13872475296768924, 0.037980103846730645, 0.1515573657762896, 0.0579323829144498, 0.20592948354601084, 0.1819836133786294, 0.17594726756499693, 0.09894851462594151, 0.17960878181046178, 0.08152679072169103, 0.16105932093248154, 0.1585185126694396, 0.04780266691437452, 0.09548414384590566, 0.10865210446014348, 0.058727236057271294, 0.05491983659188906, 0.15742298753018355, 0.1453794228231281, 0.10595577097105141, 0.2612289374434453, 0.1794367555138125, 0.10417505640119165, 0.19392703389475496, 0.15918055614605134, 0.1584799935764691, 0.16813529678298675, 0.04412875744161681, 0.12774740889747008, 0.07360504199280012, 0.12780002918709302, 0.10718263621453995, 0.0904693405276424, 0.036171718131684044, 0.034942753069616146, 0.08108462898693225, 0.06764291703266107, 0.2036660696163707, 0.06611516042526332, 0.09852315645038527, 0.06400339366276067, 0.12604993074394313, 0.06931641483348361, 0.04344885505506571, 0.07440642837053278, 0.15958057826114813, 0.10293244431295824, 0.12998909315269933, 0.18067417619269957, 0.05727787520599528, 0.10034407547458069, 0.2125839929419219, 0.06730021435558856, 0.1338365345426252, 0.2801179939307035, 0.05708744887129291, 0.06874201701481916, 0.09205383708816783, 0.0663000115794608, 0.0643759974297155, 0.13197669021232875, 0.08383330054699593, 0.061899302214232926, 0.06037469365529331, 0.1369868645169697, 0.06397540218350681, 0.05386654810694524, 0.07338829366008472, 0.08077266997060732, 0.09052723503342379, 0.1341406527685164, 0.150945632050785, 0.14286872888669355, 0.053203467985601306, 0.05953081702571555, 0.04408047193248814, 0.05053190677570993, 0.0726961200197323, 0.10838682195967493, 0.06954521994942921, 0.2216063119057078, 0.12852999847532814, 0.05048856065280858, 0.10208195193097273, 0.19272589821041536, 0.11655953948322312, 0.07833054066274432, 0.09037474814989874, 0.07257911992654094, 0.0870276850208549, 0.09494182835717419, 0.16233137954165375, 0.1853348067367646, 0.08216150049714203, 0.07394059291346156, 0.11723977134089421, 0.061375875559360936, 0.15749337481186346, 0.1096877602992224, 0.17096567487008876, 0.060008316182727305, 0.16460023538405297, 0.08483241137859088, 0.1334615281659956, 0.11262285344515356, 0.079005344676769, 0.04579203778021033, 0.13234535976179357, 0.12822225150148875, 0.057480922311627096, 0.12614708523567347, 0.03678581030931602, 0.18759896447589602, 0.06414962414158389, 0.04445313302291374, 0.20405309730857185, 0.048849614661980505, 0.09423400691446901, 0.041736733824893416, 0.1329725281708305, 0.08573702403015575, 0.22665033490555406, 0.1542112331536661, 0.1407425613714722, 0.06947139189772224, 0.09948899478570312, 0.06609571330649605, 0.11352018859816956, 0.16986597683922192, 0.1730644121266704, 0.05719037235047924, 0.0924160345938328, 0.04215325515057187, 0.08113643344660953, 0.14118008369680257, 0.1347858602618976, 0.1261932384438734, 0.030741037907210115, 0.12800159045566042, 0.10270873171214427, 0.059571770683402143, 0.07940884746415636, 0.18059060788524653, 0.19535262850688806, 0.05607111299273147, 0.09170669010235721, 0.06968236298053644, 0.06933491510619025, 0.07510904570465476, 0.05988627842602221, 0.047328409294401706, 0.05371549404234546, 0.11372046281250105, 0.06151698210856736, 0.11891426060679429, 0.12172426394373387, 0.11265852101204551, 0.14268518588761603, 0.05821581968884613, 0.11754597066112173, 0.179685095862784, 0.10316640009141967, 0.12377866763192072, 0.06657233036168975, 0.1335029371128114, 0.11167442811567521, 0.12704085888438077, 0.07887545568219974, 0.1804858760244027, 0.14767368395785582, 0.09598575495451653, 0.052509747259352084, 0.1534664418076593, 0.2530393152311753, 0.1727900654940052, 0.045115727255889276, 0.050095691054799565, 0.08046940055051376, 0.1605798957721972, 0.05997784683053166, 0.05907759313294592, 0.2054624940367256, 0.13038347411455445, 0.09133633715825627, 0.1349897474255576, 0.10657910626256291, 0.05945045931184792, 0.1097077341764722, 0.08257789710029843, 0.18370939351236476, 0.08920655497364498, 0.10404448065384458, 0.08795470541491689, 0.07447963344050172, 0.04488201708716566, 0.14455509631092064, 0.17862057463536993, 0.11655405917003435, 0.08379789306444088, 0.0928826560309359, 0.16843430239709029, 0.19828045065312386, 0.08854246918309354, 0.1982700468804262, 0.10211284807125619, 0.16458016834836228, 0.11487048229626272, 0.1328745489785042, 0.15218713926379404, 0.17724105696215503, 0.058848755254433616, 0.04604618721645618, 0.04879151230963217, 0.0652135510770792, 0.08966681029275658, 0.13930291892217653, 0.13914243739102977, 0.05712394717003325, 0.13566164046905502, 0.09385409035718796, 0.1164339014036334, 0.04940488945386957, 0.09998539808367428, 0.08950790394389604, 0.054850986188130994, 0.0811627001680008, 0.09789668018923765, 0.05204621862394578, 0.061727281304982146, 0.07954171597659485, 0.09993548638878794, 0.0868172107652845, 0.131268550791062, 0.06790163602570128, 0.039526888482150566, 0.04879163197207748, 0.0626810755386773, 0.13342158806146046, 0.09343792497115981, 0.10918684054253427, 0.09717712814919416, 0.07692183396152899, 0.044917141755294224, 0.16046983042031476, 0.05487475231961356, 0.0701917353487532, 0.08381456637119827, 0.15663600846331555, 0.0814932994033286, 0.058555340314271474, 0.1737449648434486, 0.032055540725054156, 0.08010173934766177, 0.06642972876565308, 0.10633577161859903, 0.1230672546216162, 0.21466024380745227, 0.1637957498362823, 0.1103073243076787, 0.06032379965988466, 0.024192795606949966, 0.1683145644681263, 0.058778716736237566, 0.15062993461752225, 0.10819223008879066, 0.11204712026672985, 0.04786989222882609, 0.12721700102677538, 0.029498784790821745, 0.1487847008471056, 0.055264325192614684, 0.09321338915025187, 0.06218323485823352, 0.0479747577155117, 0.10457181248645321, 0.13687635776991433, 0.06820076048859483, 0.15032176327362692, 0.10984420899106051, 0.06417812184866437, 0.07332216445586787, 0.09039568350387342, 0.09078786685484641, 0.04972281695152274, 0.03819146128649045, 0.09663118440979741, 0.1416955491157181, 0.052904828592664495, 0.0906793357124902, 0.08837861651500009, 0.08688085739781228, 0.07411519556501955, 0.0644108248636771, 0.05248770121382101, 0.09332594522641881, 0.11402012793720742, 0.1645816981468401, 0.10874189251948882, 0.062314793090922214, 0.06495680473756434, 0.054654615881031585, 0.13659198584441776, 0.09295239712365633, 0.08351728003512134, 0.06714784526380164, 0.08699661342187187, 0.07240379542519775, 0.03984913679170895, 0.09156980619667755, 0.21249712074641236, 0.139461934447059, 0.05902813732573527, 0.0886564196755684, 0.07289525428876245, 0.22032227350254327, 0.1680205169096219, 0.05068040118499858, 0.06653419433430437, 0.08338194703197378, 0.17724538746097507, 0.15564680460474972, 0.09184658794260961, 0.08031113687541046, 0.06765208997549631, 0.1636026401201162, 0.06502587184317417, 0.12337202378207152, 0.09116147569095748, 0.045218907225763744, 0.146284649734669, 0.0823247677071514, 0.05735072169516192, 0.09240871001777709, 0.08343255685353283, 0.05814748430954617, 0.19651269329546084, 0.08372183813028462, 0.10197634543116321, 0.0638235233307656, 0.058836787870470496, 0.08107915809929844, 0.07012060666570559, 0.11465438028236348, 0.11895222027905308, 0.12868514758814467, 0.13992570334718618, 0.05816245960445341, 0.0487202463117414, 0.17265708471566515, 0.10218466162785195, 0.12062643860177802, 0.04776864713527664, 0.09023889154319538, 0.11383837384993849, 0.07052748000669719, 0.062060260690488096, 0.13903410489590998, 0.08155144169387707, 0.1681757588893909]
0.09817451922198706
Making ranges
torch.Size([26429, 2])
We keep 4.54e+06/2.51e+08 =  1% of the original kernel matrix.

torch.Size([1297, 2])
We keep 2.62e+04/2.38e+05 = 10% of the original kernel matrix.

torch.Size([6314, 2])
We keep 4.03e+05/7.73e+06 =  5% of the original kernel matrix.

torch.Size([12645, 2])
We keep 2.41e+06/6.28e+07 =  3% of the original kernel matrix.

torch.Size([16906, 2])
We keep 3.09e+06/1.25e+08 =  2% of the original kernel matrix.

torch.Size([3680, 2])
We keep 2.35e+05/3.40e+06 =  6% of the original kernel matrix.

torch.Size([9984, 2])
We keep 1.05e+06/2.92e+07 =  3% of the original kernel matrix.

torch.Size([2778, 2])
We keep 1.19e+05/1.32e+06 =  9% of the original kernel matrix.

torch.Size([8535, 2])
We keep 7.16e+05/1.82e+07 =  3% of the original kernel matrix.

torch.Size([26952, 2])
We keep 1.44e+07/3.58e+08 =  4% of the original kernel matrix.

torch.Size([24624, 2])
We keep 5.93e+06/3.00e+08 =  1% of the original kernel matrix.

torch.Size([42628, 2])
We keep 1.46e+07/6.42e+08 =  2% of the original kernel matrix.

torch.Size([30602, 2])
We keep 7.61e+06/4.01e+08 =  1% of the original kernel matrix.

torch.Size([47919, 2])
We keep 2.19e+07/1.01e+09 =  2% of the original kernel matrix.

torch.Size([32329, 2])
We keep 9.37e+06/5.04e+08 =  1% of the original kernel matrix.

torch.Size([35941, 2])
We keep 9.59e+06/4.55e+08 =  2% of the original kernel matrix.

torch.Size([28498, 2])
We keep 6.50e+06/3.38e+08 =  1% of the original kernel matrix.

torch.Size([5565, 2])
We keep 3.80e+05/6.68e+06 =  5% of the original kernel matrix.

torch.Size([11387, 2])
We keep 1.27e+06/4.09e+07 =  3% of the original kernel matrix.

torch.Size([20736, 2])
We keep 3.00e+07/3.15e+08 =  9% of the original kernel matrix.

torch.Size([21263, 2])
We keep 5.42e+06/2.81e+08 =  1% of the original kernel matrix.

torch.Size([8939, 2])
We keep 9.63e+05/2.02e+07 =  4% of the original kernel matrix.

torch.Size([14094, 2])
We keep 1.92e+06/7.12e+07 =  2% of the original kernel matrix.

torch.Size([53710, 2])
We keep 4.67e+07/1.48e+09 =  3% of the original kernel matrix.

torch.Size([33159, 2])
We keep 1.06e+07/6.10e+08 =  1% of the original kernel matrix.

torch.Size([6769, 2])
We keep 5.89e+05/1.18e+07 =  4% of the original kernel matrix.

torch.Size([12504, 2])
We keep 1.61e+06/5.44e+07 =  2% of the original kernel matrix.

torch.Size([184828, 2])
We keep 2.58e+08/1.55e+10 =  1% of the original kernel matrix.

torch.Size([66687, 2])
We keep 3.01e+07/1.97e+09 =  1% of the original kernel matrix.

torch.Size([19016, 2])
We keep 3.27e+06/1.26e+08 =  2% of the original kernel matrix.

torch.Size([19790, 2])
We keep 3.66e+06/1.78e+08 =  2% of the original kernel matrix.

torch.Size([54517, 2])
We keep 2.64e+07/1.06e+09 =  2% of the original kernel matrix.

torch.Size([34344, 2])
We keep 9.43e+06/5.15e+08 =  1% of the original kernel matrix.

torch.Size([56535, 2])
We keep 2.63e+07/1.10e+09 =  2% of the original kernel matrix.

torch.Size([35163, 2])
We keep 9.44e+06/5.24e+08 =  1% of the original kernel matrix.

torch.Size([29086, 2])
We keep 8.90e+06/3.28e+08 =  2% of the original kernel matrix.

torch.Size([25576, 2])
We keep 5.72e+06/2.87e+08 =  1% of the original kernel matrix.

torch.Size([157548, 2])
We keep 2.59e+08/1.17e+10 =  2% of the original kernel matrix.

torch.Size([60656, 2])
We keep 2.64e+07/1.71e+09 =  1% of the original kernel matrix.

torch.Size([109578, 2])
We keep 8.23e+07/4.30e+09 =  1% of the original kernel matrix.

torch.Size([49826, 2])
We keep 1.70e+07/1.04e+09 =  1% of the original kernel matrix.

torch.Size([11413, 2])
We keep 2.66e+06/4.94e+07 =  5% of the original kernel matrix.

torch.Size([15752, 2])
We keep 2.74e+06/1.11e+08 =  2% of the original kernel matrix.

torch.Size([305840, 2])
We keep 5.02e+08/3.10e+10 =  1% of the original kernel matrix.

torch.Size([85147, 2])
We keep 3.98e+07/2.79e+09 =  1% of the original kernel matrix.

torch.Size([14828, 2])
We keep 2.27e+06/6.37e+07 =  3% of the original kernel matrix.

torch.Size([18267, 2])
We keep 2.93e+06/1.26e+08 =  2% of the original kernel matrix.

torch.Size([12515, 2])
We keep 1.41e+06/4.22e+07 =  3% of the original kernel matrix.

torch.Size([16794, 2])
We keep 2.53e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([14351, 2])
We keep 8.66e+06/1.40e+08 =  6% of the original kernel matrix.

torch.Size([17439, 2])
We keep 3.89e+06/1.87e+08 =  2% of the original kernel matrix.

torch.Size([61307, 2])
We keep 2.38e+07/1.29e+09 =  1% of the original kernel matrix.

torch.Size([36503, 2])
We keep 1.00e+07/5.69e+08 =  1% of the original kernel matrix.

torch.Size([21343, 2])
We keep 1.52e+07/3.38e+08 =  4% of the original kernel matrix.

torch.Size([21667, 2])
We keep 6.07e+06/2.91e+08 =  2% of the original kernel matrix.

torch.Size([11340, 2])
We keep 4.11e+06/4.09e+07 = 10% of the original kernel matrix.

torch.Size([15909, 2])
We keep 2.42e+06/1.01e+08 =  2% of the original kernel matrix.

torch.Size([44979, 2])
We keep 2.50e+07/8.67e+08 =  2% of the original kernel matrix.

torch.Size([30961, 2])
We keep 8.63e+06/4.66e+08 =  1% of the original kernel matrix.

torch.Size([1638436, 2])
We keep 6.44e+09/7.27e+11 =  0% of the original kernel matrix.

torch.Size([208019, 2])
We keep 1.72e+08/1.35e+10 =  1% of the original kernel matrix.

torch.Size([6063, 2])
We keep 9.27e+05/8.86e+06 = 10% of the original kernel matrix.

torch.Size([11730, 2])
We keep 1.40e+06/4.71e+07 =  2% of the original kernel matrix.

torch.Size([279667, 2])
We keep 1.16e+09/5.04e+10 =  2% of the original kernel matrix.

torch.Size([81170, 2])
We keep 5.28e+07/3.55e+09 =  1% of the original kernel matrix.

torch.Size([8989, 2])
We keep 9.17e+05/1.92e+07 =  4% of the original kernel matrix.

torch.Size([14107, 2])
We keep 1.89e+06/6.94e+07 =  2% of the original kernel matrix.

torch.Size([9070, 2])
We keep 7.60e+05/1.80e+07 =  4% of the original kernel matrix.

torch.Size([14084, 2])
We keep 1.80e+06/6.72e+07 =  2% of the original kernel matrix.

torch.Size([7286, 2])
We keep 8.44e+05/1.41e+07 =  5% of the original kernel matrix.

torch.Size([12760, 2])
We keep 1.68e+06/5.95e+07 =  2% of the original kernel matrix.

torch.Size([69313, 2])
We keep 1.05e+08/2.87e+09 =  3% of the original kernel matrix.

torch.Size([38704, 2])
We keep 1.48e+07/8.49e+08 =  1% of the original kernel matrix.

torch.Size([59512, 2])
We keep 2.68e+07/1.19e+09 =  2% of the original kernel matrix.

torch.Size([36049, 2])
We keep 9.76e+06/5.46e+08 =  1% of the original kernel matrix.

torch.Size([3012, 2])
We keep 1.34e+05/1.66e+06 =  8% of the original kernel matrix.

torch.Size([8644, 2])
We keep 7.71e+05/2.04e+07 =  3% of the original kernel matrix.

torch.Size([7478, 2])
We keep 7.45e+05/1.19e+07 =  6% of the original kernel matrix.

torch.Size([12815, 2])
We keep 1.59e+06/5.46e+07 =  2% of the original kernel matrix.

torch.Size([3436, 2])
We keep 1.59e+05/2.09e+06 =  7% of the original kernel matrix.

torch.Size([9142, 2])
We keep 8.44e+05/2.29e+07 =  3% of the original kernel matrix.

torch.Size([5596, 2])
We keep 3.40e+05/6.24e+06 =  5% of the original kernel matrix.

torch.Size([11350, 2])
We keep 1.23e+06/3.95e+07 =  3% of the original kernel matrix.

torch.Size([1288, 2])
We keep 5.15e+04/3.94e+05 = 13% of the original kernel matrix.

torch.Size([5906, 2])
We keep 4.72e+05/9.94e+06 =  4% of the original kernel matrix.

torch.Size([1183, 2])
We keep 2.43e+04/2.06e+05 = 11% of the original kernel matrix.

torch.Size([6126, 2])
We keep 3.93e+05/7.19e+06 =  5% of the original kernel matrix.

torch.Size([2849, 2])
We keep 1.18e+05/1.49e+06 =  7% of the original kernel matrix.

torch.Size([8443, 2])
We keep 7.50e+05/1.93e+07 =  3% of the original kernel matrix.

torch.Size([1643, 2])
We keep 3.78e+04/3.86e+05 =  9% of the original kernel matrix.

torch.Size([7085, 2])
We keep 4.60e+05/9.83e+06 =  4% of the original kernel matrix.

torch.Size([1720, 2])
We keep 4.80e+04/4.61e+05 = 10% of the original kernel matrix.

torch.Size([7058, 2])
We keep 5.10e+05/1.08e+07 =  4% of the original kernel matrix.

torch.Size([5589, 2])
We keep 5.84e+05/8.29e+06 =  7% of the original kernel matrix.

torch.Size([11214, 2])
We keep 1.40e+06/4.56e+07 =  3% of the original kernel matrix.

torch.Size([6441, 2])
We keep 4.70e+05/8.77e+06 =  5% of the original kernel matrix.

torch.Size([12077, 2])
We keep 1.42e+06/4.69e+07 =  3% of the original kernel matrix.

torch.Size([1729, 2])
We keep 4.45e+04/4.60e+05 =  9% of the original kernel matrix.

torch.Size([7151, 2])
We keep 5.18e+05/1.07e+07 =  4% of the original kernel matrix.

torch.Size([9014, 2])
We keep 1.17e+06/2.19e+07 =  5% of the original kernel matrix.

torch.Size([13894, 2])
We keep 1.99e+06/7.41e+07 =  2% of the original kernel matrix.

torch.Size([12562, 2])
We keep 1.87e+06/4.87e+07 =  3% of the original kernel matrix.

torch.Size([16856, 2])
We keep 2.73e+06/1.11e+08 =  2% of the original kernel matrix.

torch.Size([2914, 2])
We keep 1.09e+05/1.44e+06 =  7% of the original kernel matrix.

torch.Size([8573, 2])
We keep 7.31e+05/1.90e+07 =  3% of the original kernel matrix.

torch.Size([5002, 2])
We keep 3.64e+05/5.60e+06 =  6% of the original kernel matrix.

torch.Size([10777, 2])
We keep 1.21e+06/3.75e+07 =  3% of the original kernel matrix.

torch.Size([3253, 2])
We keep 1.42e+05/1.78e+06 =  7% of the original kernel matrix.

torch.Size([8962, 2])
We keep 7.89e+05/2.11e+07 =  3% of the original kernel matrix.

torch.Size([6963, 2])
We keep 5.38e+05/1.01e+07 =  5% of the original kernel matrix.

torch.Size([12485, 2])
We keep 1.50e+06/5.04e+07 =  2% of the original kernel matrix.

torch.Size([5238, 2])
We keep 3.85e+05/5.43e+06 =  7% of the original kernel matrix.

torch.Size([10935, 2])
We keep 1.18e+06/3.69e+07 =  3% of the original kernel matrix.

torch.Size([3732, 2])
We keep 1.63e+05/2.32e+06 =  7% of the original kernel matrix.

torch.Size([9647, 2])
We keep 8.71e+05/2.41e+07 =  3% of the original kernel matrix.

torch.Size([4682, 2])
We keep 2.25e+05/3.65e+06 =  6% of the original kernel matrix.

torch.Size([10525, 2])
We keep 1.02e+06/3.03e+07 =  3% of the original kernel matrix.

torch.Size([3444, 2])
We keep 1.42e+05/1.93e+06 =  7% of the original kernel matrix.

torch.Size([9121, 2])
We keep 8.16e+05/2.20e+07 =  3% of the original kernel matrix.

torch.Size([2506, 2])
We keep 1.21e+05/1.35e+06 =  8% of the original kernel matrix.

torch.Size([7882, 2])
We keep 7.23e+05/1.84e+07 =  3% of the original kernel matrix.

torch.Size([6538, 2])
We keep 5.69e+05/1.10e+07 =  5% of the original kernel matrix.

torch.Size([12208, 2])
We keep 1.54e+06/5.26e+07 =  2% of the original kernel matrix.

torch.Size([3031, 2])
We keep 1.17e+05/1.46e+06 =  8% of the original kernel matrix.

torch.Size([8771, 2])
We keep 7.48e+05/1.91e+07 =  3% of the original kernel matrix.

torch.Size([8336, 2])
We keep 9.49e+05/1.97e+07 =  4% of the original kernel matrix.

torch.Size([13527, 2])
We keep 1.94e+06/7.04e+07 =  2% of the original kernel matrix.

torch.Size([9231, 2])
We keep 8.59e+05/2.07e+07 =  4% of the original kernel matrix.

torch.Size([14168, 2])
We keep 1.93e+06/7.21e+07 =  2% of the original kernel matrix.

torch.Size([3807, 2])
We keep 1.86e+05/2.58e+06 =  7% of the original kernel matrix.

torch.Size([9639, 2])
We keep 9.08e+05/2.54e+07 =  3% of the original kernel matrix.

torch.Size([4015, 2])
We keep 2.18e+05/3.14e+06 =  6% of the original kernel matrix.

torch.Size([9792, 2])
We keep 9.71e+05/2.80e+07 =  3% of the original kernel matrix.

torch.Size([4025, 2])
We keep 2.60e+05/3.43e+06 =  7% of the original kernel matrix.

torch.Size([9696, 2])
We keep 1.01e+06/2.93e+07 =  3% of the original kernel matrix.

torch.Size([5060, 2])
We keep 3.25e+05/5.20e+06 =  6% of the original kernel matrix.

torch.Size([10594, 2])
We keep 1.16e+06/3.61e+07 =  3% of the original kernel matrix.

torch.Size([5113, 2])
We keep 2.73e+05/4.49e+06 =  6% of the original kernel matrix.

torch.Size([10949, 2])
We keep 1.09e+06/3.36e+07 =  3% of the original kernel matrix.

torch.Size([2355, 2])
We keep 7.50e+04/8.56e+05 =  8% of the original kernel matrix.

torch.Size([7766, 2])
We keep 6.13e+05/1.46e+07 =  4% of the original kernel matrix.

torch.Size([3869, 2])
We keep 3.19e+05/3.59e+06 =  8% of the original kernel matrix.

torch.Size([9355, 2])
We keep 9.88e+05/3.00e+07 =  3% of the original kernel matrix.

torch.Size([3697, 2])
We keep 1.75e+05/2.72e+06 =  6% of the original kernel matrix.

torch.Size([9401, 2])
We keep 9.17e+05/2.61e+07 =  3% of the original kernel matrix.

torch.Size([3522, 2])
We keep 1.61e+05/2.26e+06 =  7% of the original kernel matrix.

torch.Size([9250, 2])
We keep 8.63e+05/2.38e+07 =  3% of the original kernel matrix.

torch.Size([2824, 2])
We keep 1.24e+05/1.56e+06 =  7% of the original kernel matrix.

torch.Size([8264, 2])
We keep 7.44e+05/1.98e+07 =  3% of the original kernel matrix.

torch.Size([3868, 2])
We keep 2.01e+05/2.94e+06 =  6% of the original kernel matrix.

torch.Size([9447, 2])
We keep 9.44e+05/2.72e+07 =  3% of the original kernel matrix.

torch.Size([4021, 2])
We keep 2.67e+05/3.31e+06 =  8% of the original kernel matrix.

torch.Size([9766, 2])
We keep 9.85e+05/2.88e+07 =  3% of the original kernel matrix.

torch.Size([1895, 2])
We keep 5.29e+04/5.99e+05 =  8% of the original kernel matrix.

torch.Size([7298, 2])
We keep 5.39e+05/1.23e+07 =  4% of the original kernel matrix.

torch.Size([1434, 2])
We keep 3.00e+04/2.70e+05 = 11% of the original kernel matrix.

torch.Size([6665, 2])
We keep 4.23e+05/8.23e+06 =  5% of the original kernel matrix.

torch.Size([3468, 2])
We keep 1.46e+05/2.00e+06 =  7% of the original kernel matrix.

torch.Size([9185, 2])
We keep 8.20e+05/2.24e+07 =  3% of the original kernel matrix.

torch.Size([8315, 2])
We keep 1.18e+06/2.54e+07 =  4% of the original kernel matrix.

torch.Size([13852, 2])
We keep 2.22e+06/7.97e+07 =  2% of the original kernel matrix.

torch.Size([4431, 2])
We keep 2.49e+05/3.85e+06 =  6% of the original kernel matrix.

torch.Size([10147, 2])
We keep 1.06e+06/3.11e+07 =  3% of the original kernel matrix.

torch.Size([1528, 2])
We keep 3.35e+04/3.29e+05 = 10% of the original kernel matrix.

torch.Size([6609, 2])
We keep 4.41e+05/9.09e+06 =  4% of the original kernel matrix.

torch.Size([6866, 2])
We keep 6.91e+05/1.27e+07 =  5% of the original kernel matrix.

torch.Size([12498, 2])
We keep 1.65e+06/5.65e+07 =  2% of the original kernel matrix.

torch.Size([2138, 2])
We keep 6.86e+04/7.57e+05 =  9% of the original kernel matrix.

torch.Size([7509, 2])
We keep 5.98e+05/1.38e+07 =  4% of the original kernel matrix.

torch.Size([1985, 2])
We keep 5.38e+04/6.38e+05 =  8% of the original kernel matrix.

torch.Size([7459, 2])
We keep 5.53e+05/1.27e+07 =  4% of the original kernel matrix.

torch.Size([2814, 2])
We keep 1.10e+05/1.31e+06 =  8% of the original kernel matrix.

torch.Size([8611, 2])
We keep 7.18e+05/1.81e+07 =  3% of the original kernel matrix.

torch.Size([3361, 2])
We keep 1.65e+05/2.13e+06 =  7% of the original kernel matrix.

torch.Size([9004, 2])
We keep 8.53e+05/2.31e+07 =  3% of the original kernel matrix.

torch.Size([1205, 2])
We keep 2.65e+04/2.18e+05 = 12% of the original kernel matrix.

torch.Size([6016, 2])
We keep 3.96e+05/7.39e+06 =  5% of the original kernel matrix.

torch.Size([6031, 2])
We keep 6.90e+05/9.33e+06 =  7% of the original kernel matrix.

torch.Size([11687, 2])
We keep 1.51e+06/4.84e+07 =  3% of the original kernel matrix.

torch.Size([4580, 2])
We keep 2.34e+05/3.53e+06 =  6% of the original kernel matrix.

torch.Size([10460, 2])
We keep 1.02e+06/2.98e+07 =  3% of the original kernel matrix.

torch.Size([3809, 2])
We keep 2.57e+05/3.02e+06 =  8% of the original kernel matrix.

torch.Size([9439, 2])
We keep 9.57e+05/2.75e+07 =  3% of the original kernel matrix.

torch.Size([2173, 2])
We keep 7.54e+04/8.26e+05 =  9% of the original kernel matrix.

torch.Size([7653, 2])
We keep 6.10e+05/1.44e+07 =  4% of the original kernel matrix.

torch.Size([1827, 2])
We keep 4.51e+04/4.52e+05 =  9% of the original kernel matrix.

torch.Size([7281, 2])
We keep 4.97e+05/1.06e+07 =  4% of the original kernel matrix.

torch.Size([3403, 2])
We keep 2.55e+05/2.63e+06 =  9% of the original kernel matrix.

torch.Size([9214, 2])
We keep 9.61e+05/2.57e+07 =  3% of the original kernel matrix.

torch.Size([5375, 2])
We keep 3.38e+05/5.94e+06 =  5% of the original kernel matrix.

torch.Size([10980, 2])
We keep 1.21e+06/3.86e+07 =  3% of the original kernel matrix.

torch.Size([5188, 2])
We keep 2.98e+05/5.19e+06 =  5% of the original kernel matrix.

torch.Size([10844, 2])
We keep 1.15e+06/3.61e+07 =  3% of the original kernel matrix.

torch.Size([5127, 2])
We keep 3.36e+05/5.14e+06 =  6% of the original kernel matrix.

torch.Size([11084, 2])
We keep 1.19e+06/3.59e+07 =  3% of the original kernel matrix.

torch.Size([6400, 2])
We keep 7.45e+05/1.11e+07 =  6% of the original kernel matrix.

torch.Size([11830, 2])
We keep 1.55e+06/5.27e+07 =  2% of the original kernel matrix.

torch.Size([4948, 2])
We keep 3.24e+05/5.16e+06 =  6% of the original kernel matrix.

torch.Size([10512, 2])
We keep 1.15e+06/3.60e+07 =  3% of the original kernel matrix.

torch.Size([4706, 2])
We keep 2.24e+05/3.75e+06 =  5% of the original kernel matrix.

torch.Size([10626, 2])
We keep 1.03e+06/3.07e+07 =  3% of the original kernel matrix.

torch.Size([9547, 2])
We keep 1.01e+06/2.42e+07 =  4% of the original kernel matrix.

torch.Size([14385, 2])
We keep 2.06e+06/7.79e+07 =  2% of the original kernel matrix.

torch.Size([4544, 2])
We keep 2.71e+05/3.71e+06 =  7% of the original kernel matrix.

torch.Size([10212, 2])
We keep 1.03e+06/3.05e+07 =  3% of the original kernel matrix.

torch.Size([5531, 2])
We keep 3.56e+05/6.22e+06 =  5% of the original kernel matrix.

torch.Size([11072, 2])
We keep 1.23e+06/3.95e+07 =  3% of the original kernel matrix.

torch.Size([1969, 2])
We keep 5.41e+04/5.90e+05 =  9% of the original kernel matrix.

torch.Size([7362, 2])
We keep 5.50e+05/1.22e+07 =  4% of the original kernel matrix.

torch.Size([5234, 2])
We keep 3.58e+05/5.89e+06 =  6% of the original kernel matrix.

torch.Size([10797, 2])
We keep 1.21e+06/3.84e+07 =  3% of the original kernel matrix.

torch.Size([2433, 2])
We keep 8.97e+04/9.80e+05 =  9% of the original kernel matrix.

torch.Size([7922, 2])
We keep 6.54e+05/1.57e+07 =  4% of the original kernel matrix.

torch.Size([9306, 2])
We keep 1.02e+06/1.96e+07 =  5% of the original kernel matrix.

torch.Size([14317, 2])
We keep 1.90e+06/7.01e+07 =  2% of the original kernel matrix.

torch.Size([1212, 2])
We keep 1.87e+04/1.59e+05 = 11% of the original kernel matrix.

torch.Size([6418, 2])
We keep 3.41e+05/6.32e+06 =  5% of the original kernel matrix.

torch.Size([2681, 2])
We keep 1.02e+05/1.33e+06 =  7% of the original kernel matrix.

torch.Size([8232, 2])
We keep 7.20e+05/1.82e+07 =  3% of the original kernel matrix.

torch.Size([2323, 2])
We keep 1.47e+05/8.93e+05 = 16% of the original kernel matrix.

torch.Size([7870, 2])
We keep 6.30e+05/1.50e+07 =  4% of the original kernel matrix.

torch.Size([2525, 2])
We keep 1.09e+05/1.35e+06 =  8% of the original kernel matrix.

torch.Size([8120, 2])
We keep 7.22e+05/1.84e+07 =  3% of the original kernel matrix.

torch.Size([1030, 2])
We keep 1.68e+04/1.34e+05 = 12% of the original kernel matrix.

torch.Size([5897, 2])
We keep 3.33e+05/5.79e+06 =  5% of the original kernel matrix.

torch.Size([2740, 2])
We keep 1.34e+05/1.50e+06 =  8% of the original kernel matrix.

torch.Size([8291, 2])
We keep 7.58e+05/1.94e+07 =  3% of the original kernel matrix.

torch.Size([4512, 2])
We keep 3.41e+05/5.04e+06 =  6% of the original kernel matrix.

torch.Size([10368, 2])
We keep 1.18e+06/3.55e+07 =  3% of the original kernel matrix.

torch.Size([4785, 2])
We keep 3.69e+05/5.81e+06 =  6% of the original kernel matrix.

torch.Size([10632, 2])
We keep 1.25e+06/3.82e+07 =  3% of the original kernel matrix.

torch.Size([2399, 2])
We keep 1.00e+05/1.15e+06 =  8% of the original kernel matrix.

torch.Size([7940, 2])
We keep 6.86e+05/1.70e+07 =  4% of the original kernel matrix.

torch.Size([3096, 2])
We keep 1.14e+05/1.57e+06 =  7% of the original kernel matrix.

torch.Size([8954, 2])
We keep 7.44e+05/1.99e+07 =  3% of the original kernel matrix.

torch.Size([1237, 2])
We keep 2.43e+04/2.16e+05 = 11% of the original kernel matrix.

torch.Size([6194, 2])
We keep 3.87e+05/7.36e+06 =  5% of the original kernel matrix.

torch.Size([6736, 2])
We keep 6.86e+05/1.20e+07 =  5% of the original kernel matrix.

torch.Size([12360, 2])
We keep 1.63e+06/5.48e+07 =  2% of the original kernel matrix.

torch.Size([2056, 2])
We keep 6.31e+04/6.87e+05 =  9% of the original kernel matrix.

torch.Size([7293, 2])
We keep 5.79e+05/1.31e+07 =  4% of the original kernel matrix.

torch.Size([13624, 2])
We keep 1.91e+06/5.51e+07 =  3% of the original kernel matrix.

torch.Size([17485, 2])
We keep 2.77e+06/1.18e+08 =  2% of the original kernel matrix.

torch.Size([2676, 2])
We keep 1.12e+05/1.31e+06 =  8% of the original kernel matrix.

torch.Size([8341, 2])
We keep 7.14e+05/1.81e+07 =  3% of the original kernel matrix.

torch.Size([2008, 2])
We keep 7.17e+04/7.38e+05 =  9% of the original kernel matrix.

torch.Size([7578, 2])
We keep 6.15e+05/1.36e+07 =  4% of the original kernel matrix.

torch.Size([3199, 2])
We keep 1.03e+05/1.44e+06 =  7% of the original kernel matrix.

torch.Size([9135, 2])
We keep 7.21e+05/1.90e+07 =  3% of the original kernel matrix.

torch.Size([2361, 2])
We keep 7.40e+04/9.04e+05 =  8% of the original kernel matrix.

torch.Size([7936, 2])
We keep 6.24e+05/1.51e+07 =  4% of the original kernel matrix.

torch.Size([3061, 2])
We keep 1.28e+05/1.77e+06 =  7% of the original kernel matrix.

torch.Size([8805, 2])
We keep 7.98e+05/2.11e+07 =  3% of the original kernel matrix.

torch.Size([2218, 2])
We keep 7.74e+04/8.37e+05 =  9% of the original kernel matrix.

torch.Size([7654, 2])
We keep 6.16e+05/1.45e+07 =  4% of the original kernel matrix.

torch.Size([3698, 2])
We keep 1.66e+05/2.40e+06 =  6% of the original kernel matrix.

torch.Size([9360, 2])
We keep 8.77e+05/2.45e+07 =  3% of the original kernel matrix.

torch.Size([13266, 2])
We keep 2.18e+06/6.56e+07 =  3% of the original kernel matrix.

torch.Size([17521, 2])
We keep 3.07e+06/1.28e+08 =  2% of the original kernel matrix.

torch.Size([4080, 2])
We keep 1.94e+05/3.06e+06 =  6% of the original kernel matrix.

torch.Size([9795, 2])
We keep 9.52e+05/2.77e+07 =  3% of the original kernel matrix.

torch.Size([11413, 2])
We keep 1.49e+06/3.64e+07 =  4% of the original kernel matrix.

torch.Size([15904, 2])
We keep 2.41e+06/9.55e+07 =  2% of the original kernel matrix.

torch.Size([2239, 2])
We keep 7.43e+04/8.01e+05 =  9% of the original kernel matrix.

torch.Size([7779, 2])
We keep 6.05e+05/1.42e+07 =  4% of the original kernel matrix.

torch.Size([5225, 2])
We keep 3.91e+05/5.86e+06 =  6% of the original kernel matrix.

torch.Size([10888, 2])
We keep 1.23e+06/3.83e+07 =  3% of the original kernel matrix.

torch.Size([1626, 2])
We keep 4.86e+04/4.65e+05 = 10% of the original kernel matrix.

torch.Size([6726, 2])
We keep 4.98e+05/1.08e+07 =  4% of the original kernel matrix.

torch.Size([3844, 2])
We keep 2.32e+05/2.93e+06 =  7% of the original kernel matrix.

torch.Size([9662, 2])
We keep 9.60e+05/2.71e+07 =  3% of the original kernel matrix.

torch.Size([2469, 2])
We keep 8.64e+04/1.03e+06 =  8% of the original kernel matrix.

torch.Size([7881, 2])
We keep 6.50e+05/1.61e+07 =  4% of the original kernel matrix.

torch.Size([1779, 2])
We keep 4.54e+04/4.57e+05 =  9% of the original kernel matrix.

torch.Size([7166, 2])
We keep 4.99e+05/1.07e+07 =  4% of the original kernel matrix.

torch.Size([3067, 2])
We keep 1.46e+05/1.80e+06 =  8% of the original kernel matrix.

torch.Size([8814, 2])
We keep 8.22e+05/2.12e+07 =  3% of the original kernel matrix.

torch.Size([2369, 2])
We keep 8.16e+04/9.64e+05 =  8% of the original kernel matrix.

torch.Size([7921, 2])
We keep 6.49e+05/1.55e+07 =  4% of the original kernel matrix.

torch.Size([3714, 2])
We keep 1.89e+05/2.43e+06 =  7% of the original kernel matrix.

torch.Size([9483, 2])
We keep 8.68e+05/2.47e+07 =  3% of the original kernel matrix.

torch.Size([4393, 2])
We keep 2.54e+05/3.78e+06 =  6% of the original kernel matrix.

torch.Size([10006, 2])
We keep 1.03e+06/3.08e+07 =  3% of the original kernel matrix.

torch.Size([1241, 2])
We keep 2.23e+04/1.84e+05 = 12% of the original kernel matrix.

torch.Size([6322, 2])
We keep 3.74e+05/6.79e+06 =  5% of the original kernel matrix.

torch.Size([8093, 2])
We keep 9.40e+05/1.91e+07 =  4% of the original kernel matrix.

torch.Size([13542, 2])
We keep 1.96e+06/6.92e+07 =  2% of the original kernel matrix.

torch.Size([9648, 2])
We keep 1.21e+06/2.66e+07 =  4% of the original kernel matrix.

torch.Size([14472, 2])
We keep 2.15e+06/8.16e+07 =  2% of the original kernel matrix.

torch.Size([5937, 2])
We keep 4.62e+05/8.24e+06 =  5% of the original kernel matrix.

torch.Size([11604, 2])
We keep 1.41e+06/4.54e+07 =  3% of the original kernel matrix.

torch.Size([2540, 2])
We keep 8.26e+04/9.90e+05 =  8% of the original kernel matrix.

torch.Size([7990, 2])
We keep 6.46e+05/1.58e+07 =  4% of the original kernel matrix.

torch.Size([2242, 2])
We keep 6.97e+04/7.46e+05 =  9% of the original kernel matrix.

torch.Size([7780, 2])
We keep 5.87e+05/1.37e+07 =  4% of the original kernel matrix.

torch.Size([3299, 2])
We keep 2.41e+05/2.16e+06 = 11% of the original kernel matrix.

torch.Size([8983, 2])
We keep 8.62e+05/2.33e+07 =  3% of the original kernel matrix.

torch.Size([3190, 2])
We keep 1.08e+05/1.46e+06 =  7% of the original kernel matrix.

torch.Size([9025, 2])
We keep 7.35e+05/1.91e+07 =  3% of the original kernel matrix.

torch.Size([2751, 2])
We keep 1.11e+05/1.32e+06 =  8% of the original kernel matrix.

torch.Size([8409, 2])
We keep 7.26e+05/1.82e+07 =  3% of the original kernel matrix.

torch.Size([3334, 2])
We keep 2.05e+05/2.52e+06 =  8% of the original kernel matrix.

torch.Size([9005, 2])
We keep 9.25e+05/2.51e+07 =  3% of the original kernel matrix.

torch.Size([5390, 2])
We keep 9.69e+05/9.44e+06 = 10% of the original kernel matrix.

torch.Size([11171, 2])
We keep 1.55e+06/4.86e+07 =  3% of the original kernel matrix.

torch.Size([4701, 2])
We keep 2.39e+05/3.87e+06 =  6% of the original kernel matrix.

torch.Size([10509, 2])
We keep 1.04e+06/3.11e+07 =  3% of the original kernel matrix.

torch.Size([7780, 2])
We keep 1.70e+06/2.74e+07 =  6% of the original kernel matrix.

torch.Size([12777, 2])
We keep 2.20e+06/8.29e+07 =  2% of the original kernel matrix.

torch.Size([3877, 2])
We keep 1.99e+05/3.03e+06 =  6% of the original kernel matrix.

torch.Size([9422, 2])
We keep 9.34e+05/2.76e+07 =  3% of the original kernel matrix.

torch.Size([9866, 2])
We keep 1.61e+06/3.83e+07 =  4% of the original kernel matrix.

torch.Size([14872, 2])
We keep 2.57e+06/9.79e+07 =  2% of the original kernel matrix.

torch.Size([4726, 2])
We keep 2.36e+05/3.83e+06 =  6% of the original kernel matrix.

torch.Size([10527, 2])
We keep 1.03e+06/3.10e+07 =  3% of the original kernel matrix.

torch.Size([2999, 2])
We keep 1.07e+05/1.44e+06 =  7% of the original kernel matrix.

torch.Size([8663, 2])
We keep 7.29e+05/1.90e+07 =  3% of the original kernel matrix.

torch.Size([3903, 2])
We keep 1.77e+05/2.67e+06 =  6% of the original kernel matrix.

torch.Size([9755, 2])
We keep 9.18e+05/2.59e+07 =  3% of the original kernel matrix.

torch.Size([945, 2])
We keep 1.16e+04/8.88e+04 = 13% of the original kernel matrix.

torch.Size([5640, 2])
We keep 2.86e+05/4.72e+06 =  6% of the original kernel matrix.

torch.Size([8455, 2])
We keep 9.87e+05/2.08e+07 =  4% of the original kernel matrix.

torch.Size([14254, 2])
We keep 2.03e+06/7.22e+07 =  2% of the original kernel matrix.

torch.Size([2179, 2])
We keep 6.78e+04/8.06e+05 =  8% of the original kernel matrix.

torch.Size([7678, 2])
We keep 6.01e+05/1.42e+07 =  4% of the original kernel matrix.

torch.Size([4492, 2])
We keep 2.80e+05/4.13e+06 =  6% of the original kernel matrix.

torch.Size([10311, 2])
We keep 1.08e+06/3.22e+07 =  3% of the original kernel matrix.

torch.Size([1917, 2])
We keep 5.55e+04/5.61e+05 =  9% of the original kernel matrix.

torch.Size([7251, 2])
We keep 5.30e+05/1.19e+07 =  4% of the original kernel matrix.

torch.Size([3179, 2])
We keep 1.38e+05/1.82e+06 =  7% of the original kernel matrix.

torch.Size([8770, 2])
We keep 7.98e+05/2.13e+07 =  3% of the original kernel matrix.

torch.Size([2253, 2])
We keep 6.22e+04/7.50e+05 =  8% of the original kernel matrix.

torch.Size([7740, 2])
We keep 5.89e+05/1.37e+07 =  4% of the original kernel matrix.

torch.Size([3063, 2])
We keep 1.22e+05/1.64e+06 =  7% of the original kernel matrix.

torch.Size([8873, 2])
We keep 7.50e+05/2.03e+07 =  3% of the original kernel matrix.

torch.Size([3928, 2])
We keep 1.67e+05/2.54e+06 =  6% of the original kernel matrix.

torch.Size([9713, 2])
We keep 8.92e+05/2.52e+07 =  3% of the original kernel matrix.

torch.Size([3302, 2])
We keep 1.44e+05/2.08e+06 =  6% of the original kernel matrix.

torch.Size([8865, 2])
We keep 8.33e+05/2.28e+07 =  3% of the original kernel matrix.

torch.Size([1087, 2])
We keep 1.71e+04/1.38e+05 = 12% of the original kernel matrix.

torch.Size([5907, 2])
We keep 3.31e+05/5.89e+06 =  5% of the original kernel matrix.

torch.Size([1354, 2])
We keep 4.21e+04/3.81e+05 = 11% of the original kernel matrix.

torch.Size([6284, 2])
We keep 4.68e+05/9.77e+06 =  4% of the original kernel matrix.

torch.Size([1965, 2])
We keep 7.35e+04/6.40e+05 = 11% of the original kernel matrix.

torch.Size([7169, 2])
We keep 5.63e+05/1.27e+07 =  4% of the original kernel matrix.

torch.Size([6835, 2])
We keep 4.86e+05/9.63e+06 =  5% of the original kernel matrix.

torch.Size([12294, 2])
We keep 1.44e+06/4.91e+07 =  2% of the original kernel matrix.

torch.Size([2825, 2])
We keep 1.17e+05/1.39e+06 =  8% of the original kernel matrix.

torch.Size([8530, 2])
We keep 7.29e+05/1.87e+07 =  3% of the original kernel matrix.

torch.Size([6283, 2])
We keep 5.05e+05/8.97e+06 =  5% of the original kernel matrix.

torch.Size([12089, 2])
We keep 1.41e+06/4.74e+07 =  2% of the original kernel matrix.

torch.Size([3564, 2])
We keep 1.84e+05/2.69e+06 =  6% of the original kernel matrix.

torch.Size([9532, 2])
We keep 9.34e+05/2.60e+07 =  3% of the original kernel matrix.

torch.Size([4506, 2])
We keep 2.61e+05/3.82e+06 =  6% of the original kernel matrix.

torch.Size([10133, 2])
We keep 1.05e+06/3.10e+07 =  3% of the original kernel matrix.

torch.Size([5490, 2])
We keep 3.89e+05/5.91e+06 =  6% of the original kernel matrix.

torch.Size([11112, 2])
We keep 1.21e+06/3.85e+07 =  3% of the original kernel matrix.

torch.Size([4852, 2])
We keep 3.76e+05/5.33e+06 =  7% of the original kernel matrix.

torch.Size([10479, 2])
We keep 1.18e+06/3.65e+07 =  3% of the original kernel matrix.

torch.Size([3609, 2])
We keep 1.65e+05/2.39e+06 =  6% of the original kernel matrix.

torch.Size([9259, 2])
We keep 8.83e+05/2.45e+07 =  3% of the original kernel matrix.

torch.Size([4026, 2])
We keep 1.94e+05/2.89e+06 =  6% of the original kernel matrix.

torch.Size([9916, 2])
We keep 9.41e+05/2.69e+07 =  3% of the original kernel matrix.

torch.Size([3430, 2])
We keep 1.83e+05/2.25e+06 =  8% of the original kernel matrix.

torch.Size([9087, 2])
We keep 8.80e+05/2.37e+07 =  3% of the original kernel matrix.

torch.Size([4133, 2])
We keep 3.09e+05/4.00e+06 =  7% of the original kernel matrix.

torch.Size([9783, 2])
We keep 1.07e+06/3.17e+07 =  3% of the original kernel matrix.

torch.Size([3052, 2])
We keep 1.25e+05/1.54e+06 =  8% of the original kernel matrix.

torch.Size([8750, 2])
We keep 7.53e+05/1.96e+07 =  3% of the original kernel matrix.

torch.Size([8422, 2])
We keep 8.70e+05/1.87e+07 =  4% of the original kernel matrix.

torch.Size([13600, 2])
We keep 1.89e+06/6.84e+07 =  2% of the original kernel matrix.

torch.Size([5860, 2])
We keep 4.03e+05/6.84e+06 =  5% of the original kernel matrix.

torch.Size([11567, 2])
We keep 1.28e+06/4.14e+07 =  3% of the original kernel matrix.

torch.Size([3859, 2])
We keep 1.65e+05/2.39e+06 =  6% of the original kernel matrix.

torch.Size([9708, 2])
We keep 8.81e+05/2.45e+07 =  3% of the original kernel matrix.

torch.Size([3806, 2])
We keep 2.15e+05/2.97e+06 =  7% of the original kernel matrix.

torch.Size([9471, 2])
We keep 9.62e+05/2.73e+07 =  3% of the original kernel matrix.

torch.Size([5729, 2])
We keep 4.83e+05/8.19e+06 =  5% of the original kernel matrix.

torch.Size([11573, 2])
We keep 1.42e+06/4.53e+07 =  3% of the original kernel matrix.

torch.Size([13576, 2])
We keep 3.73e+06/7.91e+07 =  4% of the original kernel matrix.

torch.Size([17324, 2])
We keep 3.35e+06/1.41e+08 =  2% of the original kernel matrix.

torch.Size([2993, 2])
We keep 1.09e+05/1.40e+06 =  7% of the original kernel matrix.

torch.Size([8602, 2])
We keep 7.34e+05/1.87e+07 =  3% of the original kernel matrix.

torch.Size([1095, 2])
We keep 2.80e+04/2.25e+05 = 12% of the original kernel matrix.

torch.Size([5944, 2])
We keep 4.15e+05/7.50e+06 =  5% of the original kernel matrix.

torch.Size([3651, 2])
We keep 1.72e+05/2.42e+06 =  7% of the original kernel matrix.

torch.Size([9404, 2])
We keep 8.84e+05/2.46e+07 =  3% of the original kernel matrix.

torch.Size([2382, 2])
We keep 9.10e+04/1.08e+06 =  8% of the original kernel matrix.

torch.Size([8031, 2])
We keep 6.81e+05/1.65e+07 =  4% of the original kernel matrix.

torch.Size([3688, 2])
We keep 2.10e+05/2.91e+06 =  7% of the original kernel matrix.

torch.Size([9362, 2])
We keep 9.46e+05/2.70e+07 =  3% of the original kernel matrix.

torch.Size([3433, 2])
We keep 1.68e+05/2.32e+06 =  7% of the original kernel matrix.

torch.Size([9139, 2])
We keep 9.08e+05/2.41e+07 =  3% of the original kernel matrix.

torch.Size([3032, 2])
We keep 1.14e+05/1.41e+06 =  8% of the original kernel matrix.

torch.Size([8815, 2])
We keep 7.25e+05/1.88e+07 =  3% of the original kernel matrix.

torch.Size([1857, 2])
We keep 5.64e+04/5.07e+05 = 11% of the original kernel matrix.

torch.Size([7191, 2])
We keep 5.23e+05/1.13e+07 =  4% of the original kernel matrix.

torch.Size([3493, 2])
We keep 1.60e+05/1.99e+06 =  8% of the original kernel matrix.

torch.Size([9276, 2])
We keep 8.28e+05/2.23e+07 =  3% of the original kernel matrix.

torch.Size([1851, 2])
We keep 4.74e+04/4.73e+05 = 10% of the original kernel matrix.

torch.Size([7312, 2])
We keep 5.02e+05/1.09e+07 =  4% of the original kernel matrix.

torch.Size([5832, 2])
We keep 4.01e+05/7.08e+06 =  5% of the original kernel matrix.

torch.Size([11586, 2])
We keep 1.32e+06/4.21e+07 =  3% of the original kernel matrix.

torch.Size([2156, 2])
We keep 6.84e+04/7.66e+05 =  8% of the original kernel matrix.

torch.Size([7580, 2])
We keep 5.88e+05/1.39e+07 =  4% of the original kernel matrix.

torch.Size([8215, 2])
We keep 7.07e+05/1.59e+07 =  4% of the original kernel matrix.

torch.Size([13410, 2])
We keep 1.76e+06/6.31e+07 =  2% of the original kernel matrix.

torch.Size([5211, 2])
We keep 6.88e+05/8.18e+06 =  8% of the original kernel matrix.

torch.Size([11066, 2])
We keep 1.48e+06/4.53e+07 =  3% of the original kernel matrix.

torch.Size([4539, 2])
We keep 2.67e+05/4.27e+06 =  6% of the original kernel matrix.

torch.Size([10242, 2])
We keep 1.08e+06/3.27e+07 =  3% of the original kernel matrix.

torch.Size([1682, 2])
We keep 4.89e+04/4.46e+05 = 10% of the original kernel matrix.

torch.Size([6927, 2])
We keep 4.89e+05/1.06e+07 =  4% of the original kernel matrix.

torch.Size([4180, 2])
We keep 4.21e+05/5.69e+06 =  7% of the original kernel matrix.

torch.Size([10499, 2])
We keep 1.32e+06/3.78e+07 =  3% of the original kernel matrix.

torch.Size([4422, 2])
We keep 2.72e+05/3.96e+06 =  6% of the original kernel matrix.

torch.Size([10141, 2])
We keep 1.05e+06/3.15e+07 =  3% of the original kernel matrix.

torch.Size([2917, 2])
We keep 1.14e+05/1.40e+06 =  8% of the original kernel matrix.

torch.Size([8616, 2])
We keep 7.28e+05/1.87e+07 =  3% of the original kernel matrix.

torch.Size([9081, 2])
We keep 1.38e+06/2.58e+07 =  5% of the original kernel matrix.

torch.Size([14466, 2])
We keep 2.22e+06/8.05e+07 =  2% of the original kernel matrix.

torch.Size([9001, 2])
We keep 8.66e+05/1.84e+07 =  4% of the original kernel matrix.

torch.Size([14147, 2])
We keep 1.84e+06/6.79e+07 =  2% of the original kernel matrix.

torch.Size([3201, 2])
We keep 1.73e+05/1.95e+06 =  8% of the original kernel matrix.

torch.Size([8832, 2])
We keep 8.23e+05/2.21e+07 =  3% of the original kernel matrix.

torch.Size([3684, 2])
We keep 1.88e+05/2.69e+06 =  7% of the original kernel matrix.

torch.Size([9332, 2])
We keep 9.22e+05/2.60e+07 =  3% of the original kernel matrix.

torch.Size([3126, 2])
We keep 1.29e+05/1.72e+06 =  7% of the original kernel matrix.

torch.Size([8889, 2])
We keep 7.98e+05/2.08e+07 =  3% of the original kernel matrix.

torch.Size([1576, 2])
We keep 3.90e+04/3.43e+05 = 11% of the original kernel matrix.

torch.Size([6812, 2])
We keep 4.48e+05/9.28e+06 =  4% of the original kernel matrix.

torch.Size([1890, 2])
We keep 4.59e+04/4.83e+05 =  9% of the original kernel matrix.

torch.Size([7341, 2])
We keep 5.08e+05/1.10e+07 =  4% of the original kernel matrix.

torch.Size([1468, 2])
We keep 3.09e+04/2.89e+05 = 10% of the original kernel matrix.

torch.Size([6847, 2])
We keep 4.29e+05/8.52e+06 =  5% of the original kernel matrix.

torch.Size([1387, 2])
We keep 4.18e+04/3.39e+05 = 12% of the original kernel matrix.

torch.Size([6386, 2])
We keep 4.44e+05/9.21e+06 =  4% of the original kernel matrix.

torch.Size([3980, 2])
We keep 1.75e+05/2.64e+06 =  6% of the original kernel matrix.

torch.Size([9776, 2])
We keep 8.91e+05/2.57e+07 =  3% of the original kernel matrix.

torch.Size([3892, 2])
We keep 1.93e+05/2.77e+06 =  6% of the original kernel matrix.

torch.Size([9663, 2])
We keep 9.34e+05/2.63e+07 =  3% of the original kernel matrix.

torch.Size([4052, 2])
We keep 3.33e+05/3.86e+06 =  8% of the original kernel matrix.

torch.Size([9678, 2])
We keep 1.06e+06/3.11e+07 =  3% of the original kernel matrix.

torch.Size([2309, 2])
We keep 7.71e+04/8.67e+05 =  8% of the original kernel matrix.

torch.Size([7998, 2])
We keep 6.07e+05/1.47e+07 =  4% of the original kernel matrix.

torch.Size([5410, 2])
We keep 3.47e+05/6.00e+06 =  5% of the original kernel matrix.

torch.Size([11052, 2])
We keep 1.22e+06/3.88e+07 =  3% of the original kernel matrix.

torch.Size([2723, 2])
We keep 1.46e+05/1.64e+06 =  8% of the original kernel matrix.

torch.Size([8333, 2])
We keep 8.09e+05/2.03e+07 =  3% of the original kernel matrix.

torch.Size([3862, 2])
We keep 2.98e+05/3.62e+06 =  8% of the original kernel matrix.

torch.Size([9610, 2])
We keep 1.03e+06/3.01e+07 =  3% of the original kernel matrix.

torch.Size([4957, 2])
We keep 2.93e+05/4.63e+06 =  6% of the original kernel matrix.

torch.Size([10583, 2])
We keep 1.11e+06/3.41e+07 =  3% of the original kernel matrix.

torch.Size([2994, 2])
We keep 1.27e+05/1.60e+06 =  7% of the original kernel matrix.

torch.Size([8632, 2])
We keep 7.57e+05/2.00e+07 =  3% of the original kernel matrix.

torch.Size([1226, 2])
We keep 2.67e+04/2.26e+05 = 11% of the original kernel matrix.

torch.Size([6158, 2])
We keep 4.01e+05/7.52e+06 =  5% of the original kernel matrix.

torch.Size([5394, 2])
We keep 3.28e+05/5.50e+06 =  5% of the original kernel matrix.

torch.Size([11025, 2])
We keep 1.18e+06/3.71e+07 =  3% of the original kernel matrix.

torch.Size([1157, 2])
We keep 1.98e+04/1.75e+05 = 11% of the original kernel matrix.

torch.Size([6041, 2])
We keep 3.65e+05/6.62e+06 =  5% of the original kernel matrix.

torch.Size([2242, 2])
We keep 7.48e+04/8.24e+05 =  9% of the original kernel matrix.

torch.Size([7868, 2])
We keep 6.08e+05/1.44e+07 =  4% of the original kernel matrix.

torch.Size([2797, 2])
We keep 1.36e+05/1.62e+06 =  8% of the original kernel matrix.

torch.Size([8350, 2])
We keep 7.77e+05/2.02e+07 =  3% of the original kernel matrix.

torch.Size([3374, 2])
We keep 2.05e+05/2.67e+06 =  7% of the original kernel matrix.

torch.Size([9084, 2])
We keep 9.53e+05/2.59e+07 =  3% of the original kernel matrix.

torch.Size([4513, 2])
We keep 3.65e+05/5.95e+06 =  6% of the original kernel matrix.

torch.Size([10670, 2])
We keep 1.30e+06/3.86e+07 =  3% of the original kernel matrix.

torch.Size([3012, 2])
We keep 1.28e+05/1.69e+06 =  7% of the original kernel matrix.

torch.Size([8741, 2])
We keep 7.77e+05/2.06e+07 =  3% of the original kernel matrix.

torch.Size([3285, 2])
We keep 1.50e+05/1.79e+06 =  8% of the original kernel matrix.

torch.Size([9076, 2])
We keep 7.97e+05/2.12e+07 =  3% of the original kernel matrix.

torch.Size([3259, 2])
We keep 1.33e+05/1.79e+06 =  7% of the original kernel matrix.

torch.Size([9204, 2])
We keep 8.14e+05/2.12e+07 =  3% of the original kernel matrix.

torch.Size([5811, 2])
We keep 4.46e+05/7.88e+06 =  5% of the original kernel matrix.

torch.Size([11556, 2])
We keep 1.36e+06/4.45e+07 =  3% of the original kernel matrix.

torch.Size([1923, 2])
We keep 5.90e+04/6.30e+05 =  9% of the original kernel matrix.

torch.Size([7318, 2])
We keep 5.74e+05/1.26e+07 =  4% of the original kernel matrix.

torch.Size([7336, 2])
We keep 6.05e+05/1.19e+07 =  5% of the original kernel matrix.

torch.Size([12811, 2])
We keep 1.58e+06/5.47e+07 =  2% of the original kernel matrix.

torch.Size([27593, 2])
We keep 3.54e+07/5.86e+08 =  6% of the original kernel matrix.

torch.Size([24084, 2])
We keep 7.37e+06/3.83e+08 =  1% of the original kernel matrix.

torch.Size([10649, 2])
We keep 3.43e+06/4.18e+07 =  8% of the original kernel matrix.

torch.Size([15568, 2])
We keep 2.41e+06/1.02e+08 =  2% of the original kernel matrix.

torch.Size([5944, 2])
We keep 4.95e+05/7.36e+06 =  6% of the original kernel matrix.

torch.Size([11571, 2])
We keep 1.31e+06/4.30e+07 =  3% of the original kernel matrix.

torch.Size([1027, 2])
We keep 1.64e+04/1.32e+05 = 12% of the original kernel matrix.

torch.Size([5868, 2])
We keep 3.25e+05/5.75e+06 =  5% of the original kernel matrix.

torch.Size([11124, 2])
We keep 1.93e+06/3.39e+07 =  5% of the original kernel matrix.

torch.Size([15675, 2])
We keep 2.24e+06/9.22e+07 =  2% of the original kernel matrix.

torch.Size([21204, 2])
We keep 1.52e+07/2.01e+08 =  7% of the original kernel matrix.

torch.Size([21783, 2])
We keep 4.78e+06/2.24e+08 =  2% of the original kernel matrix.

torch.Size([135125, 2])
We keep 4.85e+08/1.60e+10 =  3% of the original kernel matrix.

torch.Size([53714, 2])
We keep 3.13e+07/2.01e+09 =  1% of the original kernel matrix.

torch.Size([3611, 2])
We keep 1.49e+05/2.28e+06 =  6% of the original kernel matrix.

torch.Size([9532, 2])
We keep 8.52e+05/2.39e+07 =  3% of the original kernel matrix.

torch.Size([51150, 2])
We keep 4.70e+07/1.27e+09 =  3% of the original kernel matrix.

torch.Size([33069, 2])
We keep 1.04e+07/5.64e+08 =  1% of the original kernel matrix.

torch.Size([19183, 2])
We keep 4.99e+06/1.52e+08 =  3% of the original kernel matrix.

torch.Size([20513, 2])
We keep 4.18e+06/1.95e+08 =  2% of the original kernel matrix.

torch.Size([25200, 2])
We keep 1.87e+07/3.51e+08 =  5% of the original kernel matrix.

torch.Size([23113, 2])
We keep 6.03e+06/2.97e+08 =  2% of the original kernel matrix.

torch.Size([136040, 2])
We keep 3.04e+08/1.01e+10 =  3% of the original kernel matrix.

torch.Size([55804, 2])
We keep 2.44e+07/1.59e+09 =  1% of the original kernel matrix.

torch.Size([2144, 2])
We keep 5.65e+04/6.71e+05 =  8% of the original kernel matrix.

torch.Size([7650, 2])
We keep 5.61e+05/1.30e+07 =  4% of the original kernel matrix.

torch.Size([5166, 2])
We keep 3.64e+05/5.52e+06 =  6% of the original kernel matrix.

torch.Size([10892, 2])
We keep 1.15e+06/3.72e+07 =  3% of the original kernel matrix.

torch.Size([141150, 2])
We keep 1.48e+08/7.99e+09 =  1% of the original kernel matrix.

torch.Size([56565, 2])
We keep 2.25e+07/1.42e+09 =  1% of the original kernel matrix.

torch.Size([337785, 2])
We keep 6.42e+08/4.15e+10 =  1% of the original kernel matrix.

torch.Size([89035, 2])
We keep 4.59e+07/3.22e+09 =  1% of the original kernel matrix.

torch.Size([4746, 2])
We keep 3.02e+05/4.36e+06 =  6% of the original kernel matrix.

torch.Size([10567, 2])
We keep 1.10e+06/3.31e+07 =  3% of the original kernel matrix.

torch.Size([28617, 2])
We keep 8.14e+06/2.88e+08 =  2% of the original kernel matrix.

torch.Size([25563, 2])
We keep 5.44e+06/2.69e+08 =  2% of the original kernel matrix.

torch.Size([34775, 2])
We keep 5.80e+07/6.89e+08 =  8% of the original kernel matrix.

torch.Size([27313, 2])
We keep 7.66e+06/4.16e+08 =  1% of the original kernel matrix.

torch.Size([54743, 2])
We keep 1.00e+08/1.61e+09 =  6% of the original kernel matrix.

torch.Size([33874, 2])
We keep 1.09e+07/6.36e+08 =  1% of the original kernel matrix.

torch.Size([30176, 2])
We keep 4.14e+07/5.53e+08 =  7% of the original kernel matrix.

torch.Size([25116, 2])
We keep 6.88e+06/3.72e+08 =  1% of the original kernel matrix.

torch.Size([17923, 2])
We keep 5.49e+06/1.49e+08 =  3% of the original kernel matrix.

torch.Size([19471, 2])
We keep 3.98e+06/1.93e+08 =  2% of the original kernel matrix.

torch.Size([26260, 2])
We keep 1.26e+07/3.27e+08 =  3% of the original kernel matrix.

torch.Size([24124, 2])
We keep 5.70e+06/2.86e+08 =  1% of the original kernel matrix.

torch.Size([11347, 2])
We keep 1.52e+06/3.42e+07 =  4% of the original kernel matrix.

torch.Size([15961, 2])
We keep 2.26e+06/9.26e+07 =  2% of the original kernel matrix.

torch.Size([3691, 2])
We keep 1.94e+05/2.55e+06 =  7% of the original kernel matrix.

torch.Size([9371, 2])
We keep 8.77e+05/2.53e+07 =  3% of the original kernel matrix.

torch.Size([153876, 2])
We keep 1.19e+09/3.88e+10 =  3% of the original kernel matrix.

torch.Size([54486, 2])
We keep 4.62e+07/3.12e+09 =  1% of the original kernel matrix.

torch.Size([4911, 2])
We keep 5.43e+05/6.07e+06 =  8% of the original kernel matrix.

torch.Size([10510, 2])
We keep 1.24e+06/3.90e+07 =  3% of the original kernel matrix.

torch.Size([4463, 2])
We keep 8.00e+05/6.68e+06 = 11% of the original kernel matrix.

torch.Size([10029, 2])
We keep 1.33e+06/4.09e+07 =  3% of the original kernel matrix.

torch.Size([23729, 2])
We keep 9.19e+06/2.61e+08 =  3% of the original kernel matrix.

torch.Size([23144, 2])
We keep 5.17e+06/2.56e+08 =  2% of the original kernel matrix.

torch.Size([18007, 2])
We keep 7.07e+06/1.54e+08 =  4% of the original kernel matrix.

torch.Size([19451, 2])
We keep 4.18e+06/1.97e+08 =  2% of the original kernel matrix.

torch.Size([208776, 2])
We keep 4.92e+08/2.12e+10 =  2% of the original kernel matrix.

torch.Size([70070, 2])
We keep 3.52e+07/2.31e+09 =  1% of the original kernel matrix.

torch.Size([74086, 2])
We keep 3.97e+07/1.87e+09 =  2% of the original kernel matrix.

torch.Size([40169, 2])
We keep 1.18e+07/6.85e+08 =  1% of the original kernel matrix.

torch.Size([1698, 2])
We keep 3.89e+04/3.84e+05 = 10% of the original kernel matrix.

torch.Size([7066, 2])
We keep 4.68e+05/9.82e+06 =  4% of the original kernel matrix.

torch.Size([18187, 2])
We keep 6.07e+06/1.44e+08 =  4% of the original kernel matrix.

torch.Size([20108, 2])
We keep 4.13e+06/1.90e+08 =  2% of the original kernel matrix.

torch.Size([8146, 2])
We keep 1.22e+06/1.86e+07 =  6% of the original kernel matrix.

torch.Size([13315, 2])
We keep 1.86e+06/6.84e+07 =  2% of the original kernel matrix.

torch.Size([15909, 2])
We keep 4.12e+06/1.11e+08 =  3% of the original kernel matrix.

torch.Size([18949, 2])
We keep 3.76e+06/1.66e+08 =  2% of the original kernel matrix.

torch.Size([53313, 2])
We keep 2.30e+07/1.06e+09 =  2% of the original kernel matrix.

torch.Size([34371, 2])
We keep 9.01e+06/5.16e+08 =  1% of the original kernel matrix.

torch.Size([5341, 2])
We keep 3.08e+05/5.22e+06 =  5% of the original kernel matrix.

torch.Size([10983, 2])
We keep 1.16e+06/3.62e+07 =  3% of the original kernel matrix.

torch.Size([30369, 2])
We keep 1.11e+07/3.55e+08 =  3% of the original kernel matrix.

torch.Size([25883, 2])
We keep 5.81e+06/2.98e+08 =  1% of the original kernel matrix.

torch.Size([983, 2])
We keep 1.74e+04/1.33e+05 = 13% of the original kernel matrix.

torch.Size([5674, 2])
We keep 3.31e+05/5.78e+06 =  5% of the original kernel matrix.

torch.Size([6280, 2])
We keep 4.62e+05/7.83e+06 =  5% of the original kernel matrix.

torch.Size([11922, 2])
We keep 1.34e+06/4.43e+07 =  3% of the original kernel matrix.

torch.Size([40187, 2])
We keep 4.18e+07/1.07e+09 =  3% of the original kernel matrix.

torch.Size([28678, 2])
We keep 9.53e+06/5.17e+08 =  1% of the original kernel matrix.

torch.Size([21571, 2])
We keep 8.53e+06/1.80e+08 =  4% of the original kernel matrix.

torch.Size([21711, 2])
We keep 4.35e+06/2.12e+08 =  2% of the original kernel matrix.

torch.Size([21874, 2])
We keep 7.50e+06/2.05e+08 =  3% of the original kernel matrix.

torch.Size([21717, 2])
We keep 4.70e+06/2.27e+08 =  2% of the original kernel matrix.

torch.Size([23506, 2])
We keep 8.42e+07/5.03e+08 = 16% of the original kernel matrix.

torch.Size([21403, 2])
We keep 7.02e+06/3.55e+08 =  1% of the original kernel matrix.

torch.Size([186604, 2])
We keep 1.98e+08/1.63e+10 =  1% of the original kernel matrix.

torch.Size([66226, 2])
We keep 3.01e+07/2.02e+09 =  1% of the original kernel matrix.

torch.Size([10079, 2])
We keep 1.04e+06/2.68e+07 =  3% of the original kernel matrix.

torch.Size([14985, 2])
We keep 2.13e+06/8.20e+07 =  2% of the original kernel matrix.

torch.Size([17268, 2])
We keep 2.84e+06/9.90e+07 =  2% of the original kernel matrix.

torch.Size([19630, 2])
We keep 3.47e+06/1.58e+08 =  2% of the original kernel matrix.

torch.Size([54631, 2])
We keep 2.65e+07/1.25e+09 =  2% of the original kernel matrix.

torch.Size([34690, 2])
We keep 9.87e+06/5.60e+08 =  1% of the original kernel matrix.

torch.Size([5328, 2])
We keep 6.95e+05/7.38e+06 =  9% of the original kernel matrix.

torch.Size([11000, 2])
We keep 1.35e+06/4.30e+07 =  3% of the original kernel matrix.

torch.Size([85552, 2])
We keep 8.63e+07/2.95e+09 =  2% of the original kernel matrix.

torch.Size([43278, 2])
We keep 1.43e+07/8.60e+08 =  1% of the original kernel matrix.

torch.Size([125413, 2])
We keep 3.47e+08/1.35e+10 =  2% of the original kernel matrix.

torch.Size([52777, 2])
We keep 2.88e+07/1.84e+09 =  1% of the original kernel matrix.

torch.Size([224585, 2])
We keep 5.03e+08/2.29e+10 =  2% of the original kernel matrix.

torch.Size([71847, 2])
We keep 3.60e+07/2.40e+09 =  1% of the original kernel matrix.

torch.Size([13672, 2])
We keep 3.43e+06/7.30e+07 =  4% of the original kernel matrix.

torch.Size([17423, 2])
We keep 3.15e+06/1.35e+08 =  2% of the original kernel matrix.

torch.Size([15601, 2])
We keep 4.64e+06/9.70e+07 =  4% of the original kernel matrix.

torch.Size([18465, 2])
We keep 3.39e+06/1.56e+08 =  2% of the original kernel matrix.

torch.Size([8810, 2])
We keep 1.33e+06/2.46e+07 =  5% of the original kernel matrix.

torch.Size([13670, 2])
We keep 2.07e+06/7.86e+07 =  2% of the original kernel matrix.

torch.Size([6050, 2])
We keep 3.91e+05/7.06e+06 =  5% of the original kernel matrix.

torch.Size([11625, 2])
We keep 1.28e+06/4.21e+07 =  3% of the original kernel matrix.

torch.Size([18287, 2])
We keep 7.95e+06/1.69e+08 =  4% of the original kernel matrix.

torch.Size([20002, 2])
We keep 4.32e+06/2.06e+08 =  2% of the original kernel matrix.

torch.Size([10074, 2])
We keep 1.34e+06/3.12e+07 =  4% of the original kernel matrix.

torch.Size([15146, 2])
We keep 2.33e+06/8.85e+07 =  2% of the original kernel matrix.

torch.Size([13038, 2])
We keep 2.21e+06/5.52e+07 =  4% of the original kernel matrix.

torch.Size([16948, 2])
We keep 2.79e+06/1.18e+08 =  2% of the original kernel matrix.

torch.Size([47674, 2])
We keep 9.42e+07/1.71e+09 =  5% of the original kernel matrix.

torch.Size([32566, 2])
We keep 1.04e+07/6.55e+08 =  1% of the original kernel matrix.

torch.Size([42433, 2])
We keep 6.52e+07/1.04e+09 =  6% of the original kernel matrix.

torch.Size([30499, 2])
We keep 9.14e+06/5.11e+08 =  1% of the original kernel matrix.

torch.Size([6441, 2])
We keep 9.21e+05/1.60e+07 =  5% of the original kernel matrix.

torch.Size([11767, 2])
We keep 1.77e+06/6.34e+07 =  2% of the original kernel matrix.

torch.Size([7938, 2])
We keep 1.01e+06/2.10e+07 =  4% of the original kernel matrix.

torch.Size([13527, 2])
We keep 2.00e+06/7.25e+07 =  2% of the original kernel matrix.

torch.Size([43949, 2])
We keep 3.16e+08/5.57e+09 =  5% of the original kernel matrix.

torch.Size([28926, 2])
We keep 1.92e+07/1.18e+09 =  1% of the original kernel matrix.

torch.Size([13132, 2])
We keep 3.22e+06/6.54e+07 =  4% of the original kernel matrix.

torch.Size([17239, 2])
We keep 2.88e+06/1.28e+08 =  2% of the original kernel matrix.

torch.Size([4627, 2])
We keep 2.96e+05/4.18e+06 =  7% of the original kernel matrix.

torch.Size([10427, 2])
We keep 1.07e+06/3.24e+07 =  3% of the original kernel matrix.

torch.Size([9109, 2])
We keep 1.40e+06/2.65e+07 =  5% of the original kernel matrix.

torch.Size([14196, 2])
We keep 2.20e+06/8.15e+07 =  2% of the original kernel matrix.

torch.Size([352779, 2])
We keep 3.02e+09/9.89e+10 =  3% of the original kernel matrix.

torch.Size([90198, 2])
We keep 7.15e+07/4.98e+09 =  1% of the original kernel matrix.

torch.Size([10907, 2])
We keep 1.58e+06/3.45e+07 =  4% of the original kernel matrix.

torch.Size([15460, 2])
We keep 2.36e+06/9.30e+07 =  2% of the original kernel matrix.

torch.Size([56804, 2])
We keep 4.85e+08/4.19e+09 = 11% of the original kernel matrix.

torch.Size([33330, 2])
We keep 1.65e+07/1.02e+09 =  1% of the original kernel matrix.

torch.Size([5363, 2])
We keep 3.78e+05/5.64e+06 =  6% of the original kernel matrix.

torch.Size([11154, 2])
We keep 1.21e+06/3.76e+07 =  3% of the original kernel matrix.

torch.Size([27302, 2])
We keep 2.91e+07/6.93e+08 =  4% of the original kernel matrix.

torch.Size([22835, 2])
We keep 7.92e+06/4.17e+08 =  1% of the original kernel matrix.

torch.Size([14408, 2])
We keep 5.81e+06/7.85e+07 =  7% of the original kernel matrix.

torch.Size([17968, 2])
We keep 3.02e+06/1.40e+08 =  2% of the original kernel matrix.

torch.Size([151949, 2])
We keep 1.23e+09/3.31e+10 =  3% of the original kernel matrix.

torch.Size([54025, 2])
We keep 4.37e+07/2.88e+09 =  1% of the original kernel matrix.

torch.Size([36910, 2])
We keep 1.24e+07/6.10e+08 =  2% of the original kernel matrix.

torch.Size([28502, 2])
We keep 7.39e+06/3.91e+08 =  1% of the original kernel matrix.

torch.Size([6982, 2])
We keep 5.18e+05/1.06e+07 =  4% of the original kernel matrix.

torch.Size([12402, 2])
We keep 1.50e+06/5.15e+07 =  2% of the original kernel matrix.

torch.Size([94385, 2])
We keep 5.49e+07/3.17e+09 =  1% of the original kernel matrix.

torch.Size([45968, 2])
We keep 1.49e+07/8.92e+08 =  1% of the original kernel matrix.

torch.Size([69373, 2])
We keep 3.62e+07/2.07e+09 =  1% of the original kernel matrix.

torch.Size([38988, 2])
We keep 1.24e+07/7.21e+08 =  1% of the original kernel matrix.

torch.Size([13192, 2])
We keep 1.39e+07/9.73e+07 = 14% of the original kernel matrix.

torch.Size([16912, 2])
We keep 3.32e+06/1.56e+08 =  2% of the original kernel matrix.

torch.Size([40921, 2])
We keep 1.48e+07/5.98e+08 =  2% of the original kernel matrix.

torch.Size([30095, 2])
We keep 7.29e+06/3.87e+08 =  1% of the original kernel matrix.

torch.Size([70883, 2])
We keep 5.29e+07/3.06e+09 =  1% of the original kernel matrix.

torch.Size([39823, 2])
We keep 1.51e+07/8.76e+08 =  1% of the original kernel matrix.

torch.Size([162193, 2])
We keep 1.92e+08/1.54e+10 =  1% of the original kernel matrix.

torch.Size([62062, 2])
We keep 3.03e+07/1.96e+09 =  1% of the original kernel matrix.

torch.Size([21278, 2])
We keep 2.16e+07/2.07e+08 = 10% of the original kernel matrix.

torch.Size([21807, 2])
We keep 4.57e+06/2.28e+08 =  2% of the original kernel matrix.

torch.Size([2676, 2])
We keep 1.38e+05/1.61e+06 =  8% of the original kernel matrix.

torch.Size([8056, 2])
We keep 7.57e+05/2.01e+07 =  3% of the original kernel matrix.

torch.Size([12243, 2])
We keep 1.51e+06/4.21e+07 =  3% of the original kernel matrix.

torch.Size([16438, 2])
We keep 2.54e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([30482, 2])
We keep 4.32e+07/8.05e+08 =  5% of the original kernel matrix.

torch.Size([25492, 2])
We keep 8.39e+06/4.49e+08 =  1% of the original kernel matrix.

torch.Size([90058, 2])
We keep 6.48e+08/5.15e+09 = 12% of the original kernel matrix.

torch.Size([44322, 2])
We keep 1.89e+07/1.14e+09 =  1% of the original kernel matrix.

torch.Size([10781, 2])
We keep 1.13e+06/2.78e+07 =  4% of the original kernel matrix.

torch.Size([15468, 2])
We keep 2.14e+06/8.34e+07 =  2% of the original kernel matrix.

torch.Size([2350, 2])
We keep 9.32e+04/1.12e+06 =  8% of the original kernel matrix.

torch.Size([7968, 2])
We keep 6.95e+05/1.68e+07 =  4% of the original kernel matrix.

torch.Size([24740, 2])
We keep 6.46e+06/2.35e+08 =  2% of the original kernel matrix.

torch.Size([23849, 2])
We keep 4.85e+06/2.43e+08 =  1% of the original kernel matrix.

torch.Size([82919, 2])
We keep 6.03e+07/3.43e+09 =  1% of the original kernel matrix.

torch.Size([42781, 2])
We keep 1.58e+07/9.27e+08 =  1% of the original kernel matrix.

torch.Size([49545, 2])
We keep 2.35e+08/2.89e+09 =  8% of the original kernel matrix.

torch.Size([30176, 2])
We keep 1.45e+07/8.51e+08 =  1% of the original kernel matrix.

torch.Size([5753, 2])
We keep 5.42e+05/7.58e+06 =  7% of the original kernel matrix.

torch.Size([11271, 2])
We keep 1.35e+06/4.36e+07 =  3% of the original kernel matrix.

torch.Size([18579, 2])
We keep 5.82e+06/1.45e+08 =  4% of the original kernel matrix.

torch.Size([20477, 2])
We keep 4.19e+06/1.91e+08 =  2% of the original kernel matrix.

torch.Size([13013, 2])
We keep 3.60e+06/7.07e+07 =  5% of the original kernel matrix.

torch.Size([17251, 2])
We keep 3.29e+06/1.33e+08 =  2% of the original kernel matrix.

torch.Size([7478, 2])
We keep 8.04e+05/1.38e+07 =  5% of the original kernel matrix.

torch.Size([12879, 2])
We keep 1.69e+06/5.88e+07 =  2% of the original kernel matrix.

torch.Size([11347, 2])
We keep 1.18e+06/3.11e+07 =  3% of the original kernel matrix.

torch.Size([15777, 2])
We keep 2.23e+06/8.83e+07 =  2% of the original kernel matrix.

torch.Size([7915, 2])
We keep 1.39e+06/1.41e+07 =  9% of the original kernel matrix.

torch.Size([13217, 2])
We keep 1.66e+06/5.95e+07 =  2% of the original kernel matrix.

torch.Size([53067, 2])
We keep 2.92e+08/2.39e+09 = 12% of the original kernel matrix.

torch.Size([33527, 2])
We keep 1.35e+07/7.74e+08 =  1% of the original kernel matrix.

torch.Size([33943, 2])
We keep 8.12e+06/3.95e+08 =  2% of the original kernel matrix.

torch.Size([27824, 2])
We keep 6.13e+06/3.15e+08 =  1% of the original kernel matrix.

torch.Size([14976, 2])
We keep 2.69e+06/7.45e+07 =  3% of the original kernel matrix.

torch.Size([18546, 2])
We keep 3.14e+06/1.37e+08 =  2% of the original kernel matrix.

torch.Size([13761, 2])
We keep 5.41e+06/8.48e+07 =  6% of the original kernel matrix.

torch.Size([17417, 2])
We keep 3.47e+06/1.46e+08 =  2% of the original kernel matrix.

torch.Size([7628, 2])
We keep 9.73e+05/1.60e+07 =  6% of the original kernel matrix.

torch.Size([12979, 2])
We keep 1.78e+06/6.33e+07 =  2% of the original kernel matrix.

torch.Size([308892, 2])
We keep 6.72e+08/3.92e+10 =  1% of the original kernel matrix.

torch.Size([85394, 2])
We keep 4.45e+07/3.13e+09 =  1% of the original kernel matrix.

torch.Size([6575, 2])
We keep 4.80e+05/9.37e+06 =  5% of the original kernel matrix.

torch.Size([12075, 2])
We keep 1.42e+06/4.85e+07 =  2% of the original kernel matrix.

torch.Size([92656, 2])
We keep 6.58e+07/3.55e+09 =  1% of the original kernel matrix.

torch.Size([45197, 2])
We keep 1.58e+07/9.43e+08 =  1% of the original kernel matrix.

torch.Size([2877, 2])
We keep 1.11e+05/1.44e+06 =  7% of the original kernel matrix.

torch.Size([8735, 2])
We keep 7.20e+05/1.90e+07 =  3% of the original kernel matrix.

torch.Size([4029, 2])
We keep 2.39e+05/3.10e+06 =  7% of the original kernel matrix.

torch.Size([9733, 2])
We keep 9.78e+05/2.79e+07 =  3% of the original kernel matrix.

torch.Size([4666, 2])
We keep 2.27e+05/3.71e+06 =  6% of the original kernel matrix.

torch.Size([10398, 2])
We keep 1.02e+06/3.05e+07 =  3% of the original kernel matrix.

torch.Size([19444, 2])
We keep 7.23e+06/1.36e+08 =  5% of the original kernel matrix.

torch.Size([21107, 2])
We keep 3.80e+06/1.85e+08 =  2% of the original kernel matrix.

torch.Size([4551, 2])
We keep 3.67e+05/5.37e+06 =  6% of the original kernel matrix.

torch.Size([10617, 2])
We keep 1.25e+06/3.67e+07 =  3% of the original kernel matrix.

torch.Size([24694, 2])
We keep 2.48e+07/3.87e+08 =  6% of the original kernel matrix.

torch.Size([22626, 2])
We keep 6.24e+06/3.12e+08 =  2% of the original kernel matrix.

torch.Size([5783, 2])
We keep 5.93e+05/1.00e+07 =  5% of the original kernel matrix.

torch.Size([11521, 2])
We keep 1.58e+06/5.01e+07 =  3% of the original kernel matrix.

torch.Size([5894, 2])
We keep 4.30e+05/7.52e+06 =  5% of the original kernel matrix.

torch.Size([11537, 2])
We keep 1.34e+06/4.34e+07 =  3% of the original kernel matrix.

torch.Size([152663, 2])
We keep 2.57e+08/1.20e+10 =  2% of the original kernel matrix.

torch.Size([59547, 2])
We keep 2.66e+07/1.73e+09 =  1% of the original kernel matrix.

torch.Size([20135, 2])
We keep 4.70e+06/1.53e+08 =  3% of the original kernel matrix.

torch.Size([21203, 2])
We keep 4.19e+06/1.96e+08 =  2% of the original kernel matrix.

torch.Size([13828, 2])
We keep 3.77e+06/7.73e+07 =  4% of the original kernel matrix.

torch.Size([17572, 2])
We keep 3.30e+06/1.39e+08 =  2% of the original kernel matrix.

torch.Size([86138, 2])
We keep 9.66e+07/2.81e+09 =  3% of the original kernel matrix.

torch.Size([43913, 2])
We keep 1.43e+07/8.39e+08 =  1% of the original kernel matrix.

torch.Size([70799, 2])
We keep 1.70e+08/5.13e+09 =  3% of the original kernel matrix.

torch.Size([37058, 2])
We keep 1.89e+07/1.13e+09 =  1% of the original kernel matrix.

torch.Size([6204, 2])
We keep 6.04e+05/9.15e+06 =  6% of the original kernel matrix.

torch.Size([11817, 2])
We keep 1.48e+06/4.79e+07 =  3% of the original kernel matrix.

torch.Size([7343, 2])
We keep 6.51e+05/1.19e+07 =  5% of the original kernel matrix.

torch.Size([12686, 2])
We keep 1.57e+06/5.46e+07 =  2% of the original kernel matrix.

torch.Size([14670, 2])
We keep 5.32e+06/9.86e+07 =  5% of the original kernel matrix.

torch.Size([18110, 2])
We keep 3.64e+06/1.57e+08 =  2% of the original kernel matrix.

torch.Size([1454, 2])
We keep 4.20e+04/3.48e+05 = 12% of the original kernel matrix.

torch.Size([6484, 2])
We keep 4.64e+05/9.34e+06 =  4% of the original kernel matrix.

torch.Size([4510, 2])
We keep 3.50e+05/5.63e+06 =  6% of the original kernel matrix.

torch.Size([10363, 2])
We keep 1.28e+06/3.76e+07 =  3% of the original kernel matrix.

torch.Size([15023, 2])
We keep 3.40e+06/8.99e+07 =  3% of the original kernel matrix.

torch.Size([18424, 2])
We keep 3.43e+06/1.50e+08 =  2% of the original kernel matrix.

torch.Size([3197, 2])
We keep 1.97e+05/2.00e+06 =  9% of the original kernel matrix.

torch.Size([8817, 2])
We keep 8.31e+05/2.24e+07 =  3% of the original kernel matrix.

torch.Size([6128, 2])
We keep 3.80e+05/6.75e+06 =  5% of the original kernel matrix.

torch.Size([11745, 2])
We keep 1.27e+06/4.11e+07 =  3% of the original kernel matrix.

torch.Size([6153, 2])
We keep 4.01e+05/7.18e+06 =  5% of the original kernel matrix.

torch.Size([11599, 2])
We keep 1.30e+06/4.24e+07 =  3% of the original kernel matrix.

torch.Size([4965, 2])
We keep 2.94e+05/5.04e+06 =  5% of the original kernel matrix.

torch.Size([10662, 2])
We keep 1.12e+06/3.55e+07 =  3% of the original kernel matrix.

torch.Size([188451, 2])
We keep 2.62e+08/1.58e+10 =  1% of the original kernel matrix.

torch.Size([66058, 2])
We keep 2.98e+07/1.99e+09 =  1% of the original kernel matrix.

torch.Size([8846, 2])
We keep 1.34e+06/2.57e+07 =  5% of the original kernel matrix.

torch.Size([13758, 2])
We keep 2.10e+06/8.02e+07 =  2% of the original kernel matrix.

torch.Size([42211, 2])
We keep 2.54e+07/7.27e+08 =  3% of the original kernel matrix.

torch.Size([30126, 2])
We keep 7.99e+06/4.27e+08 =  1% of the original kernel matrix.

torch.Size([7712, 2])
We keep 2.50e+06/2.72e+07 =  9% of the original kernel matrix.

torch.Size([13061, 2])
We keep 2.14e+06/8.25e+07 =  2% of the original kernel matrix.

torch.Size([11668, 2])
We keep 5.66e+06/7.41e+07 =  7% of the original kernel matrix.

torch.Size([15542, 2])
We keep 3.08e+06/1.36e+08 =  2% of the original kernel matrix.

torch.Size([19724, 2])
We keep 8.86e+06/2.06e+08 =  4% of the original kernel matrix.

torch.Size([20559, 2])
We keep 4.83e+06/2.27e+08 =  2% of the original kernel matrix.

torch.Size([361882, 2])
We keep 1.77e+09/6.64e+10 =  2% of the original kernel matrix.

torch.Size([91796, 2])
We keep 5.56e+07/4.08e+09 =  1% of the original kernel matrix.

torch.Size([450501, 2])
We keep 9.32e+08/8.35e+10 =  1% of the original kernel matrix.

torch.Size([104967, 2])
We keep 6.36e+07/4.57e+09 =  1% of the original kernel matrix.

torch.Size([31818, 2])
We keep 1.61e+07/4.14e+08 =  3% of the original kernel matrix.

torch.Size([26737, 2])
We keep 6.15e+06/3.22e+08 =  1% of the original kernel matrix.

torch.Size([56866, 2])
We keep 3.68e+07/1.21e+09 =  3% of the original kernel matrix.

torch.Size([35275, 2])
We keep 9.90e+06/5.51e+08 =  1% of the original kernel matrix.

torch.Size([3190, 2])
We keep 1.23e+05/1.58e+06 =  7% of the original kernel matrix.

torch.Size([9017, 2])
We keep 7.53e+05/1.99e+07 =  3% of the original kernel matrix.

torch.Size([37641, 2])
We keep 1.07e+08/1.39e+09 =  7% of the original kernel matrix.

torch.Size([27059, 2])
We keep 1.06e+07/5.91e+08 =  1% of the original kernel matrix.

torch.Size([14185, 2])
We keep 1.04e+07/1.26e+08 =  8% of the original kernel matrix.

torch.Size([16973, 2])
We keep 3.65e+06/1.78e+08 =  2% of the original kernel matrix.

torch.Size([57779, 2])
We keep 1.71e+08/2.20e+09 =  7% of the original kernel matrix.

torch.Size([35395, 2])
We keep 1.27e+07/7.43e+08 =  1% of the original kernel matrix.

torch.Size([9637, 2])
We keep 3.54e+06/2.82e+07 = 12% of the original kernel matrix.

torch.Size([14419, 2])
We keep 2.21e+06/8.41e+07 =  2% of the original kernel matrix.

torch.Size([49764, 2])
We keep 2.71e+07/1.04e+09 =  2% of the original kernel matrix.

torch.Size([32820, 2])
We keep 9.18e+06/5.12e+08 =  1% of the original kernel matrix.

torch.Size([199637, 2])
We keep 5.58e+08/2.60e+10 =  2% of the original kernel matrix.

torch.Size([68372, 2])
We keep 3.89e+07/2.55e+09 =  1% of the original kernel matrix.

torch.Size([38913, 2])
We keep 2.09e+07/6.64e+08 =  3% of the original kernel matrix.

torch.Size([28551, 2])
We keep 7.72e+06/4.08e+08 =  1% of the original kernel matrix.

torch.Size([5428, 2])
We keep 4.51e+05/6.89e+06 =  6% of the original kernel matrix.

torch.Size([10990, 2])
We keep 1.28e+06/4.16e+07 =  3% of the original kernel matrix.

torch.Size([18855, 2])
We keep 1.17e+07/1.82e+08 =  6% of the original kernel matrix.

torch.Size([22798, 2])
We keep 4.82e+06/2.14e+08 =  2% of the original kernel matrix.

torch.Size([9396, 2])
We keep 1.00e+06/2.36e+07 =  4% of the original kernel matrix.

torch.Size([14351, 2])
We keep 2.04e+06/7.69e+07 =  2% of the original kernel matrix.

torch.Size([4223, 2])
We keep 2.24e+05/3.29e+06 =  6% of the original kernel matrix.

torch.Size([10135, 2])
We keep 9.78e+05/2.87e+07 =  3% of the original kernel matrix.

torch.Size([92850, 2])
We keep 9.58e+07/4.00e+09 =  2% of the original kernel matrix.

torch.Size([45494, 2])
We keep 1.69e+07/1.00e+09 =  1% of the original kernel matrix.

torch.Size([15754, 2])
We keep 6.09e+06/1.10e+08 =  5% of the original kernel matrix.

torch.Size([18554, 2])
We keep 3.60e+06/1.66e+08 =  2% of the original kernel matrix.

torch.Size([2736, 2])
We keep 9.77e+04/1.20e+06 =  8% of the original kernel matrix.

torch.Size([8378, 2])
We keep 6.85e+05/1.74e+07 =  3% of the original kernel matrix.

torch.Size([61039, 2])
We keep 3.12e+07/1.26e+09 =  2% of the original kernel matrix.

torch.Size([36295, 2])
We keep 9.89e+06/5.63e+08 =  1% of the original kernel matrix.

torch.Size([9359, 2])
We keep 1.56e+06/3.58e+07 =  4% of the original kernel matrix.

torch.Size([14537, 2])
We keep 2.54e+06/9.47e+07 =  2% of the original kernel matrix.

torch.Size([1314, 2])
We keep 2.52e+04/2.19e+05 = 11% of the original kernel matrix.

torch.Size([6302, 2])
We keep 3.90e+05/7.41e+06 =  5% of the original kernel matrix.

torch.Size([77360, 2])
We keep 1.27e+08/3.84e+09 =  3% of the original kernel matrix.

torch.Size([40846, 2])
We keep 1.66e+07/9.81e+08 =  1% of the original kernel matrix.

torch.Size([55034, 2])
We keep 3.14e+07/1.37e+09 =  2% of the original kernel matrix.

torch.Size([34731, 2])
We keep 1.04e+07/5.87e+08 =  1% of the original kernel matrix.

torch.Size([23438, 2])
We keep 6.95e+06/2.16e+08 =  3% of the original kernel matrix.

torch.Size([23144, 2])
We keep 4.89e+06/2.33e+08 =  2% of the original kernel matrix.

torch.Size([61136, 2])
We keep 5.09e+07/1.37e+09 =  3% of the original kernel matrix.

torch.Size([35943, 2])
We keep 1.04e+07/5.86e+08 =  1% of the original kernel matrix.

torch.Size([61166, 2])
We keep 5.31e+07/1.63e+09 =  3% of the original kernel matrix.

torch.Size([36009, 2])
We keep 1.11e+07/6.39e+08 =  1% of the original kernel matrix.

torch.Size([8402, 2])
We keep 1.16e+06/2.02e+07 =  5% of the original kernel matrix.

torch.Size([13447, 2])
We keep 1.90e+06/7.12e+07 =  2% of the original kernel matrix.

torch.Size([27240, 2])
We keep 1.14e+07/3.32e+08 =  3% of the original kernel matrix.

torch.Size([24464, 2])
We keep 5.80e+06/2.89e+08 =  2% of the original kernel matrix.

torch.Size([78159, 2])
We keep 4.73e+07/2.79e+09 =  1% of the original kernel matrix.

torch.Size([41411, 2])
We keep 1.43e+07/8.37e+08 =  1% of the original kernel matrix.

torch.Size([84074, 2])
We keep 5.09e+07/2.64e+09 =  1% of the original kernel matrix.

torch.Size([43121, 2])
We keep 1.35e+07/8.13e+08 =  1% of the original kernel matrix.

torch.Size([8814, 2])
We keep 7.80e+05/1.72e+07 =  4% of the original kernel matrix.

torch.Size([13849, 2])
We keep 1.79e+06/6.57e+07 =  2% of the original kernel matrix.

torch.Size([56300, 2])
We keep 1.68e+08/2.25e+09 =  7% of the original kernel matrix.

torch.Size([35647, 2])
We keep 1.18e+07/7.51e+08 =  1% of the original kernel matrix.

torch.Size([106634, 2])
We keep 1.79e+08/6.32e+09 =  2% of the original kernel matrix.

torch.Size([49476, 2])
We keep 1.90e+07/1.26e+09 =  1% of the original kernel matrix.

torch.Size([45401, 2])
We keep 2.85e+07/9.85e+08 =  2% of the original kernel matrix.

torch.Size([31740, 2])
We keep 9.19e+06/4.97e+08 =  1% of the original kernel matrix.

torch.Size([27404, 2])
We keep 1.74e+07/4.15e+08 =  4% of the original kernel matrix.

torch.Size([24621, 2])
We keep 6.00e+06/3.22e+08 =  1% of the original kernel matrix.

torch.Size([15810, 2])
We keep 3.18e+07/2.32e+08 = 13% of the original kernel matrix.

torch.Size([18185, 2])
We keep 4.99e+06/2.41e+08 =  2% of the original kernel matrix.

torch.Size([8317, 2])
We keep 1.20e+06/1.97e+07 =  6% of the original kernel matrix.

torch.Size([13559, 2])
We keep 1.91e+06/7.03e+07 =  2% of the original kernel matrix.

torch.Size([6943, 2])
We keep 4.65e+05/9.72e+06 =  4% of the original kernel matrix.

torch.Size([12425, 2])
We keep 1.46e+06/4.94e+07 =  2% of the original kernel matrix.

torch.Size([6013, 2])
We keep 1.49e+06/1.37e+07 = 10% of the original kernel matrix.

torch.Size([11449, 2])
We keep 1.53e+06/5.86e+07 =  2% of the original kernel matrix.

torch.Size([121799, 2])
We keep 8.01e+07/5.19e+09 =  1% of the original kernel matrix.

torch.Size([52834, 2])
We keep 1.84e+07/1.14e+09 =  1% of the original kernel matrix.

torch.Size([88211, 2])
We keep 6.09e+07/3.66e+09 =  1% of the original kernel matrix.

torch.Size([44549, 2])
We keep 1.59e+07/9.58e+08 =  1% of the original kernel matrix.

torch.Size([191988, 2])
We keep 3.42e+08/2.11e+10 =  1% of the original kernel matrix.

torch.Size([66745, 2])
We keep 3.49e+07/2.30e+09 =  1% of the original kernel matrix.

torch.Size([108447, 2])
We keep 2.41e+08/8.03e+09 =  2% of the original kernel matrix.

torch.Size([49066, 2])
We keep 2.31e+07/1.42e+09 =  1% of the original kernel matrix.

torch.Size([45786, 2])
We keep 1.97e+07/8.60e+08 =  2% of the original kernel matrix.

torch.Size([31526, 2])
We keep 8.45e+06/4.64e+08 =  1% of the original kernel matrix.

torch.Size([12504, 2])
We keep 2.24e+06/7.01e+07 =  3% of the original kernel matrix.

torch.Size([16582, 2])
We keep 3.02e+06/1.33e+08 =  2% of the original kernel matrix.

torch.Size([55838, 2])
We keep 1.93e+07/1.03e+09 =  1% of the original kernel matrix.

torch.Size([34895, 2])
We keep 8.96e+06/5.09e+08 =  1% of the original kernel matrix.

torch.Size([2291, 2])
We keep 9.00e+04/9.16e+05 =  9% of the original kernel matrix.

torch.Size([7789, 2])
We keep 6.41e+05/1.52e+07 =  4% of the original kernel matrix.

torch.Size([9428, 2])
We keep 1.21e+06/2.47e+07 =  4% of the original kernel matrix.

torch.Size([14252, 2])
We keep 2.02e+06/7.87e+07 =  2% of the original kernel matrix.

torch.Size([137547, 2])
We keep 1.48e+08/8.17e+09 =  1% of the original kernel matrix.

torch.Size([56374, 2])
We keep 2.28e+07/1.43e+09 =  1% of the original kernel matrix.

torch.Size([17487, 2])
We keep 3.45e+06/1.02e+08 =  3% of the original kernel matrix.

torch.Size([19830, 2])
We keep 3.55e+06/1.60e+08 =  2% of the original kernel matrix.

torch.Size([3365, 2])
We keep 1.53e+05/2.09e+06 =  7% of the original kernel matrix.

torch.Size([8962, 2])
We keep 8.32e+05/2.29e+07 =  3% of the original kernel matrix.

torch.Size([11655, 2])
We keep 2.47e+06/4.53e+07 =  5% of the original kernel matrix.

torch.Size([16186, 2])
We keep 2.58e+06/1.07e+08 =  2% of the original kernel matrix.

torch.Size([31196, 2])
We keep 2.14e+07/5.27e+08 =  4% of the original kernel matrix.

torch.Size([25807, 2])
We keep 7.03e+06/3.64e+08 =  1% of the original kernel matrix.

torch.Size([23857, 2])
We keep 8.35e+06/2.13e+08 =  3% of the original kernel matrix.

torch.Size([23036, 2])
We keep 4.77e+06/2.31e+08 =  2% of the original kernel matrix.

torch.Size([47427, 2])
We keep 2.89e+07/9.67e+08 =  2% of the original kernel matrix.

torch.Size([32494, 2])
We keep 9.14e+06/4.92e+08 =  1% of the original kernel matrix.

torch.Size([15060, 2])
We keep 3.30e+07/3.05e+08 = 10% of the original kernel matrix.

torch.Size([16903, 2])
We keep 5.50e+06/2.77e+08 =  1% of the original kernel matrix.

torch.Size([13403, 2])
We keep 1.29e+07/1.56e+08 =  8% of the original kernel matrix.

torch.Size([16481, 2])
We keep 4.16e+06/1.98e+08 =  2% of the original kernel matrix.

torch.Size([5364, 2])
We keep 4.22e+05/6.17e+06 =  6% of the original kernel matrix.

torch.Size([11023, 2])
We keep 1.24e+06/3.93e+07 =  3% of the original kernel matrix.

torch.Size([3999, 2])
We keep 2.72e+05/3.73e+06 =  7% of the original kernel matrix.

torch.Size([10020, 2])
We keep 1.07e+06/3.06e+07 =  3% of the original kernel matrix.

torch.Size([31708, 2])
We keep 1.43e+07/4.94e+08 =  2% of the original kernel matrix.

torch.Size([26900, 2])
We keep 6.95e+06/3.52e+08 =  1% of the original kernel matrix.

torch.Size([41121, 2])
We keep 2.85e+07/7.13e+08 =  3% of the original kernel matrix.

torch.Size([29713, 2])
We keep 8.10e+06/4.23e+08 =  1% of the original kernel matrix.

torch.Size([8140, 2])
We keep 6.84e+06/4.26e+07 = 16% of the original kernel matrix.

torch.Size([12916, 2])
We keep 2.50e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([68332, 2])
We keep 7.84e+07/2.69e+09 =  2% of the original kernel matrix.

torch.Size([38614, 2])
We keep 1.38e+07/8.20e+08 =  1% of the original kernel matrix.

torch.Size([5302, 2])
We keep 5.97e+05/7.39e+06 =  8% of the original kernel matrix.

torch.Size([10967, 2])
We keep 1.35e+06/4.30e+07 =  3% of the original kernel matrix.

torch.Size([14975, 2])
We keep 3.09e+06/1.00e+08 =  3% of the original kernel matrix.

torch.Size([18593, 2])
We keep 3.67e+06/1.58e+08 =  2% of the original kernel matrix.

torch.Size([4189, 2])
We keep 3.97e+05/4.54e+06 =  8% of the original kernel matrix.

torch.Size([10091, 2])
We keep 1.07e+06/3.37e+07 =  3% of the original kernel matrix.

torch.Size([84382, 2])
We keep 4.08e+07/2.51e+09 =  1% of the original kernel matrix.

torch.Size([42978, 2])
We keep 1.35e+07/7.93e+08 =  1% of the original kernel matrix.

torch.Size([5131, 2])
We keep 6.63e+05/6.64e+06 =  9% of the original kernel matrix.

torch.Size([10879, 2])
We keep 1.31e+06/4.08e+07 =  3% of the original kernel matrix.

torch.Size([26186, 2])
We keep 2.00e+07/3.87e+08 =  5% of the original kernel matrix.

torch.Size([24042, 2])
We keep 6.44e+06/3.11e+08 =  2% of the original kernel matrix.

torch.Size([9382, 2])
We keep 8.72e+05/2.04e+07 =  4% of the original kernel matrix.

torch.Size([14396, 2])
We keep 1.89e+06/7.15e+07 =  2% of the original kernel matrix.

torch.Size([13340, 2])
We keep 2.23e+06/5.74e+07 =  3% of the original kernel matrix.

torch.Size([17130, 2])
We keep 2.76e+06/1.20e+08 =  2% of the original kernel matrix.

torch.Size([35627, 2])
We keep 1.13e+07/4.69e+08 =  2% of the original kernel matrix.

torch.Size([28206, 2])
We keep 6.64e+06/3.43e+08 =  1% of the original kernel matrix.

torch.Size([147394, 2])
We keep 4.07e+08/1.74e+10 =  2% of the original kernel matrix.

torch.Size([57749, 2])
We keep 3.15e+07/2.09e+09 =  1% of the original kernel matrix.

torch.Size([9650, 2])
We keep 9.43e+05/2.16e+07 =  4% of the original kernel matrix.

torch.Size([14530, 2])
We keep 1.98e+06/7.36e+07 =  2% of the original kernel matrix.

torch.Size([8263, 2])
We keep 2.31e+06/2.53e+07 =  9% of the original kernel matrix.

torch.Size([13212, 2])
We keep 2.09e+06/7.97e+07 =  2% of the original kernel matrix.

torch.Size([87669, 2])
We keep 8.45e+07/3.23e+09 =  2% of the original kernel matrix.

torch.Size([44149, 2])
We keep 1.51e+07/9.00e+08 =  1% of the original kernel matrix.

torch.Size([9564, 2])
We keep 1.23e+06/2.76e+07 =  4% of the original kernel matrix.

torch.Size([14445, 2])
We keep 2.18e+06/8.32e+07 =  2% of the original kernel matrix.

torch.Size([386946, 2])
We keep 6.99e+08/5.43e+10 =  1% of the original kernel matrix.

torch.Size([96076, 2])
We keep 5.28e+07/3.69e+09 =  1% of the original kernel matrix.

torch.Size([3618, 2])
We keep 1.87e+05/2.53e+06 =  7% of the original kernel matrix.

torch.Size([9291, 2])
We keep 8.83e+05/2.52e+07 =  3% of the original kernel matrix.

torch.Size([66531, 2])
We keep 3.75e+07/1.83e+09 =  2% of the original kernel matrix.

torch.Size([37668, 2])
We keep 1.19e+07/6.77e+08 =  1% of the original kernel matrix.

torch.Size([203022, 2])
We keep 2.39e+08/2.15e+10 =  1% of the original kernel matrix.

torch.Size([70474, 2])
We keep 3.50e+07/2.32e+09 =  1% of the original kernel matrix.

torch.Size([2929, 2])
We keep 1.35e+05/1.49e+06 =  9% of the original kernel matrix.

torch.Size([8451, 2])
We keep 7.40e+05/1.93e+07 =  3% of the original kernel matrix.

torch.Size([98072, 2])
We keep 2.25e+08/8.38e+09 =  2% of the original kernel matrix.

torch.Size([44536, 2])
We keep 2.23e+07/1.45e+09 =  1% of the original kernel matrix.

torch.Size([12781, 2])
We keep 1.02e+07/1.62e+08 =  6% of the original kernel matrix.

torch.Size([15761, 2])
We keep 4.29e+06/2.02e+08 =  2% of the original kernel matrix.

torch.Size([244010, 2])
We keep 9.31e+08/2.74e+10 =  3% of the original kernel matrix.

torch.Size([75625, 2])
We keep 3.90e+07/2.62e+09 =  1% of the original kernel matrix.

torch.Size([9192, 2])
We keep 9.42e+05/2.03e+07 =  4% of the original kernel matrix.

torch.Size([14145, 2])
We keep 1.89e+06/7.14e+07 =  2% of the original kernel matrix.

torch.Size([27020, 2])
We keep 1.07e+07/3.77e+08 =  2% of the original kernel matrix.

torch.Size([24453, 2])
We keep 6.15e+06/3.08e+08 =  2% of the original kernel matrix.

torch.Size([2030, 2])
We keep 7.47e+04/7.89e+05 =  9% of the original kernel matrix.

torch.Size([7379, 2])
We keep 5.99e+05/1.41e+07 =  4% of the original kernel matrix.

torch.Size([6158, 2])
We keep 6.18e+05/1.11e+07 =  5% of the original kernel matrix.

torch.Size([11823, 2])
We keep 1.59e+06/5.28e+07 =  3% of the original kernel matrix.

torch.Size([7164, 2])
We keep 8.78e+05/1.47e+07 =  5% of the original kernel matrix.

torch.Size([12559, 2])
We keep 1.73e+06/6.08e+07 =  2% of the original kernel matrix.

torch.Size([50524, 2])
We keep 5.57e+07/1.08e+09 =  5% of the original kernel matrix.

torch.Size([33479, 2])
We keep 9.35e+06/5.19e+08 =  1% of the original kernel matrix.

torch.Size([16930, 2])
We keep 5.30e+06/1.15e+08 =  4% of the original kernel matrix.

torch.Size([19467, 2])
We keep 3.79e+06/1.70e+08 =  2% of the original kernel matrix.

torch.Size([60597, 2])
We keep 4.16e+07/1.46e+09 =  2% of the original kernel matrix.

torch.Size([36353, 2])
We keep 1.08e+07/6.04e+08 =  1% of the original kernel matrix.

torch.Size([13495, 2])
We keep 1.93e+06/5.39e+07 =  3% of the original kernel matrix.

torch.Size([17246, 2])
We keep 2.79e+06/1.16e+08 =  2% of the original kernel matrix.

torch.Size([4943, 2])
We keep 2.71e+05/4.49e+06 =  6% of the original kernel matrix.

torch.Size([10673, 2])
We keep 1.10e+06/3.36e+07 =  3% of the original kernel matrix.

torch.Size([4612, 2])
We keep 2.87e+05/4.31e+06 =  6% of the original kernel matrix.

torch.Size([10362, 2])
We keep 1.08e+06/3.29e+07 =  3% of the original kernel matrix.

torch.Size([75855, 2])
We keep 1.18e+08/3.78e+09 =  3% of the original kernel matrix.

torch.Size([40562, 2])
We keep 1.65e+07/9.73e+08 =  1% of the original kernel matrix.

torch.Size([17129, 2])
We keep 1.65e+07/1.77e+08 =  9% of the original kernel matrix.

torch.Size([19028, 2])
We keep 4.45e+06/2.11e+08 =  2% of the original kernel matrix.

torch.Size([245230, 2])
We keep 3.73e+08/2.33e+10 =  1% of the original kernel matrix.

torch.Size([76624, 2])
We keep 3.62e+07/2.42e+09 =  1% of the original kernel matrix.

torch.Size([33592, 2])
We keep 2.02e+07/5.45e+08 =  3% of the original kernel matrix.

torch.Size([27447, 2])
We keep 6.99e+06/3.70e+08 =  1% of the original kernel matrix.

torch.Size([6987, 2])
We keep 1.10e+06/1.46e+07 =  7% of the original kernel matrix.

torch.Size([12576, 2])
We keep 1.71e+06/6.04e+07 =  2% of the original kernel matrix.

torch.Size([7051, 2])
We keep 4.38e+06/1.87e+07 = 23% of the original kernel matrix.

torch.Size([12282, 2])
We keep 1.70e+06/6.85e+07 =  2% of the original kernel matrix.

torch.Size([8238, 2])
We keep 2.59e+06/2.69e+07 =  9% of the original kernel matrix.

torch.Size([13235, 2])
We keep 2.15e+06/8.21e+07 =  2% of the original kernel matrix.

torch.Size([657795, 2])
We keep 3.24e+09/2.23e+11 =  1% of the original kernel matrix.

torch.Size([130686, 2])
We keep 9.95e+07/7.47e+09 =  1% of the original kernel matrix.

torch.Size([10417, 2])
We keep 1.45e+06/3.09e+07 =  4% of the original kernel matrix.

torch.Size([15304, 2])
We keep 2.31e+06/8.81e+07 =  2% of the original kernel matrix.

torch.Size([16236, 2])
We keep 4.25e+06/9.83e+07 =  4% of the original kernel matrix.

torch.Size([19064, 2])
We keep 3.50e+06/1.57e+08 =  2% of the original kernel matrix.

torch.Size([73370, 2])
We keep 7.08e+07/2.58e+09 =  2% of the original kernel matrix.

torch.Size([39682, 2])
We keep 1.39e+07/8.04e+08 =  1% of the original kernel matrix.

torch.Size([28976, 2])
We keep 1.65e+07/4.36e+08 =  3% of the original kernel matrix.

torch.Size([24353, 2])
We keep 6.39e+06/3.31e+08 =  1% of the original kernel matrix.

torch.Size([4184, 2])
We keep 2.73e+05/3.29e+06 =  8% of the original kernel matrix.

torch.Size([10001, 2])
We keep 9.77e+05/2.87e+07 =  3% of the original kernel matrix.

torch.Size([3237, 2])
We keep 1.45e+05/2.02e+06 =  7% of the original kernel matrix.

torch.Size([8870, 2])
We keep 8.30e+05/2.25e+07 =  3% of the original kernel matrix.

torch.Size([67328, 2])
We keep 2.64e+08/4.82e+09 =  5% of the original kernel matrix.

torch.Size([37652, 2])
We keep 1.85e+07/1.10e+09 =  1% of the original kernel matrix.

torch.Size([22779, 2])
We keep 7.36e+06/2.37e+08 =  3% of the original kernel matrix.

torch.Size([22933, 2])
We keep 5.15e+06/2.44e+08 =  2% of the original kernel matrix.

torch.Size([32889, 2])
We keep 5.94e+07/1.01e+09 =  5% of the original kernel matrix.

torch.Size([25667, 2])
We keep 9.29e+06/5.03e+08 =  1% of the original kernel matrix.

torch.Size([53140, 2])
We keep 3.39e+07/1.05e+09 =  3% of the original kernel matrix.

torch.Size([33828, 2])
We keep 9.39e+06/5.13e+08 =  1% of the original kernel matrix.

torch.Size([41572, 2])
We keep 4.33e+07/1.02e+09 =  4% of the original kernel matrix.

torch.Size([30842, 2])
We keep 9.49e+06/5.05e+08 =  1% of the original kernel matrix.

torch.Size([73052, 2])
We keep 8.34e+07/2.60e+09 =  3% of the original kernel matrix.

torch.Size([39980, 2])
We keep 1.39e+07/8.08e+08 =  1% of the original kernel matrix.

torch.Size([149252, 2])
We keep 2.42e+08/1.32e+10 =  1% of the original kernel matrix.

torch.Size([58491, 2])
We keep 2.83e+07/1.82e+09 =  1% of the original kernel matrix.

torch.Size([114356, 2])
We keep 1.55e+08/5.24e+09 =  2% of the original kernel matrix.

torch.Size([51168, 2])
We keep 1.86e+07/1.15e+09 =  1% of the original kernel matrix.

torch.Size([11488, 2])
We keep 3.21e+06/5.34e+07 =  6% of the original kernel matrix.

torch.Size([15965, 2])
We keep 2.86e+06/1.16e+08 =  2% of the original kernel matrix.

torch.Size([72913, 2])
We keep 1.02e+08/2.13e+09 =  4% of the original kernel matrix.

torch.Size([40171, 2])
We keep 1.24e+07/7.30e+08 =  1% of the original kernel matrix.

torch.Size([10786, 2])
We keep 3.11e+06/4.40e+07 =  7% of the original kernel matrix.

torch.Size([15485, 2])
We keep 2.64e+06/1.05e+08 =  2% of the original kernel matrix.

torch.Size([10750, 2])
We keep 3.87e+06/3.56e+07 = 10% of the original kernel matrix.

torch.Size([15521, 2])
We keep 2.30e+06/9.44e+07 =  2% of the original kernel matrix.

torch.Size([13605, 2])
We keep 2.03e+06/5.65e+07 =  3% of the original kernel matrix.

torch.Size([17361, 2])
We keep 2.84e+06/1.19e+08 =  2% of the original kernel matrix.

torch.Size([7871, 2])
We keep 6.50e+05/1.35e+07 =  4% of the original kernel matrix.

torch.Size([13340, 2])
We keep 1.64e+06/5.81e+07 =  2% of the original kernel matrix.

torch.Size([49974, 2])
We keep 2.96e+08/2.91e+09 = 10% of the original kernel matrix.

torch.Size([32070, 2])
We keep 1.40e+07/8.54e+08 =  1% of the original kernel matrix.

torch.Size([11730, 2])
We keep 2.25e+06/5.22e+07 =  4% of the original kernel matrix.

torch.Size([16035, 2])
We keep 2.79e+06/1.14e+08 =  2% of the original kernel matrix.

torch.Size([3681, 2])
We keep 4.84e+05/3.26e+06 = 14% of the original kernel matrix.

torch.Size([9076, 2])
We keep 9.96e+05/2.86e+07 =  3% of the original kernel matrix.

torch.Size([16280, 2])
We keep 3.65e+06/9.50e+07 =  3% of the original kernel matrix.

torch.Size([19054, 2])
We keep 3.43e+06/1.54e+08 =  2% of the original kernel matrix.

torch.Size([6852, 2])
We keep 3.38e+06/3.20e+07 = 10% of the original kernel matrix.

torch.Size([12027, 2])
We keep 2.05e+06/8.96e+07 =  2% of the original kernel matrix.

torch.Size([39011, 2])
We keep 8.60e+07/1.49e+09 =  5% of the original kernel matrix.

torch.Size([27949, 2])
We keep 1.11e+07/6.12e+08 =  1% of the original kernel matrix.

torch.Size([7655, 2])
We keep 1.14e+06/2.02e+07 =  5% of the original kernel matrix.

torch.Size([12850, 2])
We keep 1.92e+06/7.12e+07 =  2% of the original kernel matrix.

torch.Size([13621, 2])
We keep 2.25e+06/5.62e+07 =  4% of the original kernel matrix.

torch.Size([17550, 2])
We keep 2.80e+06/1.19e+08 =  2% of the original kernel matrix.

torch.Size([7332, 2])
We keep 1.93e+06/2.73e+07 =  7% of the original kernel matrix.

torch.Size([12109, 2])
We keep 2.07e+06/8.27e+07 =  2% of the original kernel matrix.

torch.Size([30225, 2])
We keep 2.16e+07/5.05e+08 =  4% of the original kernel matrix.

torch.Size([25601, 2])
We keep 7.05e+06/3.56e+08 =  1% of the original kernel matrix.

torch.Size([4356, 2])
We keep 2.92e+05/4.36e+06 =  6% of the original kernel matrix.

torch.Size([10185, 2])
We keep 1.13e+06/3.31e+07 =  3% of the original kernel matrix.

torch.Size([6917, 2])
We keep 5.99e+05/1.19e+07 =  5% of the original kernel matrix.

torch.Size([12284, 2])
We keep 1.58e+06/5.46e+07 =  2% of the original kernel matrix.

torch.Size([21000, 2])
We keep 3.94e+06/1.50e+08 =  2% of the original kernel matrix.

torch.Size([22231, 2])
We keep 4.15e+06/1.94e+08 =  2% of the original kernel matrix.

torch.Size([116326, 2])
We keep 1.31e+08/6.83e+09 =  1% of the original kernel matrix.

torch.Size([51319, 2])
We keep 2.12e+07/1.31e+09 =  1% of the original kernel matrix.

torch.Size([6448, 2])
We keep 6.56e+05/1.08e+07 =  6% of the original kernel matrix.

torch.Size([12046, 2])
We keep 1.57e+06/5.20e+07 =  3% of the original kernel matrix.

torch.Size([1561, 2])
We keep 4.00e+04/3.97e+05 = 10% of the original kernel matrix.

torch.Size([6698, 2])
We keep 4.75e+05/9.97e+06 =  4% of the original kernel matrix.

torch.Size([4149, 2])
We keep 3.03e+05/4.26e+06 =  7% of the original kernel matrix.

torch.Size([9744, 2])
We keep 1.10e+06/3.27e+07 =  3% of the original kernel matrix.

torch.Size([179004, 2])
We keep 4.51e+08/2.09e+10 =  2% of the original kernel matrix.

torch.Size([64901, 2])
We keep 3.55e+07/2.29e+09 =  1% of the original kernel matrix.

torch.Size([139236, 2])
We keep 1.59e+08/7.26e+09 =  2% of the original kernel matrix.

torch.Size([56659, 2])
We keep 2.08e+07/1.35e+09 =  1% of the original kernel matrix.

torch.Size([28563, 2])
We keep 1.19e+07/4.08e+08 =  2% of the original kernel matrix.

torch.Size([24478, 2])
We keep 6.26e+06/3.20e+08 =  1% of the original kernel matrix.

torch.Size([5579, 2])
We keep 5.30e+05/8.28e+06 =  6% of the original kernel matrix.

torch.Size([11232, 2])
We keep 1.41e+06/4.56e+07 =  3% of the original kernel matrix.

torch.Size([68655, 2])
We keep 7.92e+07/2.49e+09 =  3% of the original kernel matrix.

torch.Size([38715, 2])
We keep 1.36e+07/7.91e+08 =  1% of the original kernel matrix.

torch.Size([87872, 2])
We keep 5.66e+07/3.46e+09 =  1% of the original kernel matrix.

torch.Size([44459, 2])
We keep 1.57e+07/9.31e+08 =  1% of the original kernel matrix.

torch.Size([3076, 2])
We keep 1.15e+05/1.48e+06 =  7% of the original kernel matrix.

torch.Size([8733, 2])
We keep 7.43e+05/1.93e+07 =  3% of the original kernel matrix.

torch.Size([8999, 2])
We keep 1.23e+06/2.36e+07 =  5% of the original kernel matrix.

torch.Size([14075, 2])
We keep 2.01e+06/7.70e+07 =  2% of the original kernel matrix.

torch.Size([23129, 2])
We keep 4.38e+06/1.97e+08 =  2% of the original kernel matrix.

torch.Size([23332, 2])
We keep 4.58e+06/2.23e+08 =  2% of the original kernel matrix.

torch.Size([7644, 2])
We keep 1.69e+06/2.28e+07 =  7% of the original kernel matrix.

torch.Size([13147, 2])
We keep 1.96e+06/7.57e+07 =  2% of the original kernel matrix.

torch.Size([14230, 2])
We keep 6.12e+06/1.02e+08 =  6% of the original kernel matrix.

torch.Size([17740, 2])
We keep 3.68e+06/1.60e+08 =  2% of the original kernel matrix.

torch.Size([86193, 2])
We keep 5.15e+07/2.63e+09 =  1% of the original kernel matrix.

torch.Size([43801, 2])
We keep 1.38e+07/8.12e+08 =  1% of the original kernel matrix.

torch.Size([15160, 2])
We keep 2.12e+06/6.65e+07 =  3% of the original kernel matrix.

torch.Size([18582, 2])
We keep 2.99e+06/1.29e+08 =  2% of the original kernel matrix.

torch.Size([21889, 2])
We keep 5.34e+07/4.03e+08 = 13% of the original kernel matrix.

torch.Size([21487, 2])
We keep 6.23e+06/3.18e+08 =  1% of the original kernel matrix.

torch.Size([3942, 2])
We keep 2.13e+05/3.18e+06 =  6% of the original kernel matrix.

torch.Size([9630, 2])
We keep 9.95e+05/2.82e+07 =  3% of the original kernel matrix.

torch.Size([24015, 2])
We keep 8.59e+06/2.30e+08 =  3% of the original kernel matrix.

torch.Size([23123, 2])
We keep 4.90e+06/2.40e+08 =  2% of the original kernel matrix.

torch.Size([16888, 2])
We keep 2.82e+06/9.09e+07 =  3% of the original kernel matrix.

torch.Size([19699, 2])
We keep 3.44e+06/1.51e+08 =  2% of the original kernel matrix.

torch.Size([22655, 2])
We keep 1.09e+07/2.50e+08 =  4% of the original kernel matrix.

torch.Size([22273, 2])
We keep 5.02e+06/2.50e+08 =  2% of the original kernel matrix.

torch.Size([39713, 2])
We keep 4.04e+07/6.54e+08 =  6% of the original kernel matrix.

torch.Size([29152, 2])
We keep 7.61e+06/4.05e+08 =  1% of the original kernel matrix.

torch.Size([196147, 2])
We keep 2.21e+08/1.77e+10 =  1% of the original kernel matrix.

torch.Size([69007, 2])
We keep 3.18e+07/2.11e+09 =  1% of the original kernel matrix.

torch.Size([7159, 2])
We keep 6.85e+05/1.23e+07 =  5% of the original kernel matrix.

torch.Size([12472, 2])
We keep 1.61e+06/5.55e+07 =  2% of the original kernel matrix.

torch.Size([3781, 2])
We keep 4.60e+05/3.56e+06 = 12% of the original kernel matrix.

torch.Size([9386, 2])
We keep 9.73e+05/2.99e+07 =  3% of the original kernel matrix.

torch.Size([12684, 2])
We keep 1.57e+06/4.63e+07 =  3% of the original kernel matrix.

torch.Size([16807, 2])
We keep 2.62e+06/1.08e+08 =  2% of the original kernel matrix.

torch.Size([30237, 2])
We keep 7.34e+06/3.39e+08 =  2% of the original kernel matrix.

torch.Size([26474, 2])
We keep 5.71e+06/2.91e+08 =  1% of the original kernel matrix.

torch.Size([21992, 2])
We keep 5.24e+06/1.87e+08 =  2% of the original kernel matrix.

torch.Size([22172, 2])
We keep 4.50e+06/2.17e+08 =  2% of the original kernel matrix.

torch.Size([4449, 2])
We keep 3.55e+05/4.96e+06 =  7% of the original kernel matrix.

torch.Size([10186, 2])
We keep 1.11e+06/3.52e+07 =  3% of the original kernel matrix.

torch.Size([3273, 2])
We keep 1.60e+05/2.07e+06 =  7% of the original kernel matrix.

torch.Size([9033, 2])
We keep 8.60e+05/2.28e+07 =  3% of the original kernel matrix.

torch.Size([24939, 2])
We keep 9.05e+06/2.35e+08 =  3% of the original kernel matrix.

torch.Size([23528, 2])
We keep 4.91e+06/2.43e+08 =  2% of the original kernel matrix.

torch.Size([3315, 2])
We keep 1.41e+05/1.82e+06 =  7% of the original kernel matrix.

torch.Size([9167, 2])
We keep 7.93e+05/2.14e+07 =  3% of the original kernel matrix.

torch.Size([13827, 2])
We keep 4.78e+06/9.69e+07 =  4% of the original kernel matrix.

torch.Size([16997, 2])
We keep 3.47e+06/1.56e+08 =  2% of the original kernel matrix.

torch.Size([5531, 2])
We keep 3.45e+05/5.74e+06 =  6% of the original kernel matrix.

torch.Size([11343, 2])
We keep 1.21e+06/3.79e+07 =  3% of the original kernel matrix.

torch.Size([13309, 2])
We keep 1.59e+06/5.08e+07 =  3% of the original kernel matrix.

torch.Size([17392, 2])
We keep 2.70e+06/1.13e+08 =  2% of the original kernel matrix.

torch.Size([8385, 2])
We keep 1.38e+06/2.02e+07 =  6% of the original kernel matrix.

torch.Size([13459, 2])
We keep 1.87e+06/7.11e+07 =  2% of the original kernel matrix.

torch.Size([6799, 2])
We keep 4.59e+05/8.85e+06 =  5% of the original kernel matrix.

torch.Size([12242, 2])
We keep 1.41e+06/4.71e+07 =  2% of the original kernel matrix.

torch.Size([4429, 2])
We keep 2.46e+05/3.87e+06 =  6% of the original kernel matrix.

torch.Size([10123, 2])
We keep 1.06e+06/3.11e+07 =  3% of the original kernel matrix.

torch.Size([90275, 2])
We keep 4.74e+07/2.80e+09 =  1% of the original kernel matrix.

torch.Size([44857, 2])
We keep 1.40e+07/8.38e+08 =  1% of the original kernel matrix.

torch.Size([182346, 2])
We keep 2.19e+08/1.27e+10 =  1% of the original kernel matrix.

torch.Size([65821, 2])
We keep 2.72e+07/1.78e+09 =  1% of the original kernel matrix.

torch.Size([143975, 2])
We keep 2.39e+08/9.67e+09 =  2% of the original kernel matrix.

torch.Size([57863, 2])
We keep 2.46e+07/1.56e+09 =  1% of the original kernel matrix.

torch.Size([57561, 2])
We keep 7.01e+07/1.43e+09 =  4% of the original kernel matrix.

torch.Size([34618, 2])
We keep 1.04e+07/5.98e+08 =  1% of the original kernel matrix.

torch.Size([24956, 2])
We keep 7.06e+06/2.25e+08 =  3% of the original kernel matrix.

torch.Size([23948, 2])
We keep 4.93e+06/2.37e+08 =  2% of the original kernel matrix.

torch.Size([7476, 2])
We keep 8.11e+05/1.54e+07 =  5% of the original kernel matrix.

torch.Size([12604, 2])
We keep 1.74e+06/6.21e+07 =  2% of the original kernel matrix.

torch.Size([8618, 2])
We keep 1.25e+06/2.57e+07 =  4% of the original kernel matrix.

torch.Size([14605, 2])
We keep 2.20e+06/8.02e+07 =  2% of the original kernel matrix.

torch.Size([98156, 2])
We keep 5.30e+07/3.48e+09 =  1% of the original kernel matrix.

torch.Size([47320, 2])
We keep 1.55e+07/9.34e+08 =  1% of the original kernel matrix.

torch.Size([9304, 2])
We keep 1.59e+06/3.13e+07 =  5% of the original kernel matrix.

torch.Size([15333, 2])
We keep 2.40e+06/8.86e+07 =  2% of the original kernel matrix.

torch.Size([21268, 2])
We keep 6.64e+06/1.68e+08 =  3% of the original kernel matrix.

torch.Size([21774, 2])
We keep 4.39e+06/2.05e+08 =  2% of the original kernel matrix.

torch.Size([12037, 2])
We keep 3.08e+06/4.50e+07 =  6% of the original kernel matrix.

torch.Size([16277, 2])
We keep 2.60e+06/1.06e+08 =  2% of the original kernel matrix.

torch.Size([125546, 2])
We keep 4.24e+08/1.30e+10 =  3% of the original kernel matrix.

torch.Size([53370, 2])
We keep 2.85e+07/1.81e+09 =  1% of the original kernel matrix.

torch.Size([17252, 2])
We keep 6.41e+06/1.12e+08 =  5% of the original kernel matrix.

torch.Size([19860, 2])
We keep 3.60e+06/1.67e+08 =  2% of the original kernel matrix.

torch.Size([23421, 2])
We keep 9.15e+06/3.00e+08 =  3% of the original kernel matrix.

torch.Size([23259, 2])
We keep 5.68e+06/2.74e+08 =  2% of the original kernel matrix.

torch.Size([105881, 2])
We keep 1.58e+08/5.35e+09 =  2% of the original kernel matrix.

torch.Size([49056, 2])
We keep 1.87e+07/1.16e+09 =  1% of the original kernel matrix.

torch.Size([34295, 2])
We keep 8.80e+06/4.30e+08 =  2% of the original kernel matrix.

torch.Size([28011, 2])
We keep 6.37e+06/3.28e+08 =  1% of the original kernel matrix.

torch.Size([18554, 2])
We keep 7.77e+06/1.31e+08 =  5% of the original kernel matrix.

torch.Size([19843, 2])
We keep 3.68e+06/1.81e+08 =  2% of the original kernel matrix.

torch.Size([120168, 2])
We keep 1.48e+08/5.88e+09 =  2% of the original kernel matrix.

torch.Size([52255, 2])
We keep 1.95e+07/1.21e+09 =  1% of the original kernel matrix.

torch.Size([76165, 2])
We keep 3.79e+07/2.17e+09 =  1% of the original kernel matrix.

torch.Size([40840, 2])
We keep 1.25e+07/7.37e+08 =  1% of the original kernel matrix.

torch.Size([31739, 2])
We keep 2.31e+07/4.58e+08 =  5% of the original kernel matrix.

torch.Size([26449, 2])
We keep 6.47e+06/3.39e+08 =  1% of the original kernel matrix.

torch.Size([14754, 2])
We keep 1.03e+07/1.12e+08 =  9% of the original kernel matrix.

torch.Size([17968, 2])
We keep 3.66e+06/1.67e+08 =  2% of the original kernel matrix.

torch.Size([26569, 2])
We keep 8.46e+06/2.70e+08 =  3% of the original kernel matrix.

torch.Size([24550, 2])
We keep 5.16e+06/2.60e+08 =  1% of the original kernel matrix.

torch.Size([9335, 2])
We keep 1.14e+06/2.21e+07 =  5% of the original kernel matrix.

torch.Size([14568, 2])
We keep 1.95e+06/7.44e+07 =  2% of the original kernel matrix.

torch.Size([60318, 2])
We keep 3.39e+07/1.57e+09 =  2% of the original kernel matrix.

torch.Size([36047, 2])
We keep 1.11e+07/6.27e+08 =  1% of the original kernel matrix.

torch.Size([306317, 2])
We keep 8.26e+08/4.98e+10 =  1% of the original kernel matrix.

torch.Size([84543, 2])
We keep 5.10e+07/3.53e+09 =  1% of the original kernel matrix.

torch.Size([124644, 2])
We keep 6.31e+08/1.16e+10 =  5% of the original kernel matrix.

torch.Size([53151, 2])
We keep 2.67e+07/1.70e+09 =  1% of the original kernel matrix.

torch.Size([44920, 2])
We keep 8.85e+07/2.17e+09 =  4% of the original kernel matrix.

torch.Size([29429, 2])
We keep 1.30e+07/7.37e+08 =  1% of the original kernel matrix.

torch.Size([9008, 2])
We keep 1.61e+06/2.31e+07 =  6% of the original kernel matrix.

torch.Size([14230, 2])
We keep 1.99e+06/7.60e+07 =  2% of the original kernel matrix.

torch.Size([19328, 2])
We keep 1.07e+07/1.90e+08 =  5% of the original kernel matrix.

torch.Size([20723, 2])
We keep 4.62e+06/2.18e+08 =  2% of the original kernel matrix.

torch.Size([15163, 2])
We keep 3.85e+06/1.14e+08 =  3% of the original kernel matrix.

torch.Size([19112, 2])
We keep 3.94e+06/1.69e+08 =  2% of the original kernel matrix.

torch.Size([17179, 2])
We keep 7.76e+06/1.32e+08 =  5% of the original kernel matrix.

torch.Size([19315, 2])
We keep 4.03e+06/1.82e+08 =  2% of the original kernel matrix.

torch.Size([37225, 2])
We keep 1.90e+07/5.65e+08 =  3% of the original kernel matrix.

torch.Size([28644, 2])
We keep 7.21e+06/3.76e+08 =  1% of the original kernel matrix.

torch.Size([199978, 2])
We keep 1.91e+08/1.63e+10 =  1% of the original kernel matrix.

torch.Size([69332, 2])
We keep 3.06e+07/2.02e+09 =  1% of the original kernel matrix.

torch.Size([4256, 2])
We keep 9.57e+05/6.08e+06 = 15% of the original kernel matrix.

torch.Size([9688, 2])
We keep 1.18e+06/3.90e+07 =  3% of the original kernel matrix.

torch.Size([59850, 2])
We keep 2.72e+08/4.11e+09 =  6% of the original kernel matrix.

torch.Size([35013, 2])
We keep 1.61e+07/1.01e+09 =  1% of the original kernel matrix.

torch.Size([48914, 2])
We keep 2.89e+07/9.68e+08 =  2% of the original kernel matrix.

torch.Size([32688, 2])
We keep 8.91e+06/4.93e+08 =  1% of the original kernel matrix.

torch.Size([29907, 2])
We keep 8.35e+06/3.31e+08 =  2% of the original kernel matrix.

torch.Size([25954, 2])
We keep 5.72e+06/2.88e+08 =  1% of the original kernel matrix.

torch.Size([5497, 2])
We keep 7.03e+05/8.49e+06 =  8% of the original kernel matrix.

torch.Size([11227, 2])
We keep 1.41e+06/4.61e+07 =  3% of the original kernel matrix.

torch.Size([34245, 2])
We keep 2.07e+07/6.82e+08 =  3% of the original kernel matrix.

torch.Size([28006, 2])
We keep 8.00e+06/4.13e+08 =  1% of the original kernel matrix.

torch.Size([91556, 2])
We keep 6.54e+07/3.37e+09 =  1% of the original kernel matrix.

torch.Size([45186, 2])
We keep 1.53e+07/9.20e+08 =  1% of the original kernel matrix.

torch.Size([4170, 2])
We keep 3.09e+05/3.95e+06 =  7% of the original kernel matrix.

torch.Size([9912, 2])
We keep 1.06e+06/3.15e+07 =  3% of the original kernel matrix.

torch.Size([574509, 2])
We keep 1.15e+09/1.12e+11 =  1% of the original kernel matrix.

torch.Size([122191, 2])
We keep 7.25e+07/5.30e+09 =  1% of the original kernel matrix.

torch.Size([30068, 2])
We keep 1.70e+07/4.35e+08 =  3% of the original kernel matrix.

torch.Size([25905, 2])
We keep 6.18e+06/3.30e+08 =  1% of the original kernel matrix.

torch.Size([54072, 2])
We keep 5.88e+07/1.33e+09 =  4% of the original kernel matrix.

torch.Size([33795, 2])
We keep 1.05e+07/5.78e+08 =  1% of the original kernel matrix.

torch.Size([11572, 2])
We keep 1.55e+07/9.82e+07 = 15% of the original kernel matrix.

torch.Size([16030, 2])
We keep 3.53e+06/1.57e+08 =  2% of the original kernel matrix.

torch.Size([10889, 2])
We keep 1.51e+06/3.22e+07 =  4% of the original kernel matrix.

torch.Size([15461, 2])
We keep 2.28e+06/8.98e+07 =  2% of the original kernel matrix.

torch.Size([2571, 2])
We keep 9.55e+04/1.10e+06 =  8% of the original kernel matrix.

torch.Size([8158, 2])
We keep 6.65e+05/1.66e+07 =  4% of the original kernel matrix.

torch.Size([5038, 2])
We keep 4.88e+05/5.55e+06 =  8% of the original kernel matrix.

torch.Size([10716, 2])
We keep 1.18e+06/3.73e+07 =  3% of the original kernel matrix.

torch.Size([14226, 2])
We keep 3.02e+06/6.30e+07 =  4% of the original kernel matrix.

torch.Size([17716, 2])
We keep 2.96e+06/1.26e+08 =  2% of the original kernel matrix.

torch.Size([62573, 2])
We keep 1.65e+08/2.63e+09 =  6% of the original kernel matrix.

torch.Size([36260, 2])
We keep 1.35e+07/8.11e+08 =  1% of the original kernel matrix.

torch.Size([869063, 2])
We keep 1.71e+10/9.46e+11 =  1% of the original kernel matrix.

torch.Size([127751, 2])
We keep 2.02e+08/1.54e+10 =  1% of the original kernel matrix.

torch.Size([4936, 2])
We keep 3.11e+05/5.04e+06 =  6% of the original kernel matrix.

torch.Size([10840, 2])
We keep 1.13e+06/3.56e+07 =  3% of the original kernel matrix.

torch.Size([89965, 2])
We keep 4.87e+07/2.85e+09 =  1% of the original kernel matrix.

torch.Size([44751, 2])
We keep 1.42e+07/8.45e+08 =  1% of the original kernel matrix.

torch.Size([6645, 2])
We keep 7.01e+05/1.21e+07 =  5% of the original kernel matrix.

torch.Size([12255, 2])
We keep 1.62e+06/5.51e+07 =  2% of the original kernel matrix.

torch.Size([15291, 2])
We keep 3.19e+06/9.98e+07 =  3% of the original kernel matrix.

torch.Size([18564, 2])
We keep 3.65e+06/1.58e+08 =  2% of the original kernel matrix.

torch.Size([12037, 2])
We keep 3.37e+06/5.82e+07 =  5% of the original kernel matrix.

torch.Size([16067, 2])
We keep 2.91e+06/1.21e+08 =  2% of the original kernel matrix.

torch.Size([110278, 2])
We keep 3.81e+08/9.58e+09 =  3% of the original kernel matrix.

torch.Size([48264, 2])
We keep 2.46e+07/1.55e+09 =  1% of the original kernel matrix.

torch.Size([9640, 2])
We keep 1.62e+06/3.16e+07 =  5% of the original kernel matrix.

torch.Size([14494, 2])
We keep 2.27e+06/8.90e+07 =  2% of the original kernel matrix.

torch.Size([775972, 2])
We keep 2.08e+09/2.19e+11 =  0% of the original kernel matrix.

torch.Size([139549, 2])
We keep 9.96e+07/7.41e+09 =  1% of the original kernel matrix.

torch.Size([6622, 2])
We keep 5.49e+05/1.05e+07 =  5% of the original kernel matrix.

torch.Size([12140, 2])
We keep 1.49e+06/5.12e+07 =  2% of the original kernel matrix.

torch.Size([106671, 2])
We keep 9.69e+07/4.12e+09 =  2% of the original kernel matrix.

torch.Size([49175, 2])
We keep 1.69e+07/1.02e+09 =  1% of the original kernel matrix.

torch.Size([20303, 2])
We keep 1.31e+07/2.23e+08 =  5% of the original kernel matrix.

torch.Size([21407, 2])
We keep 4.99e+06/2.36e+08 =  2% of the original kernel matrix.

torch.Size([70453, 2])
We keep 4.64e+07/2.02e+09 =  2% of the original kernel matrix.

torch.Size([39116, 2])
We keep 1.22e+07/7.11e+08 =  1% of the original kernel matrix.

torch.Size([141255, 2])
We keep 1.77e+08/9.49e+09 =  1% of the original kernel matrix.

torch.Size([56556, 2])
We keep 2.37e+07/1.54e+09 =  1% of the original kernel matrix.

torch.Size([15828, 2])
We keep 2.72e+06/8.44e+07 =  3% of the original kernel matrix.

torch.Size([18706, 2])
We keep 3.28e+06/1.45e+08 =  2% of the original kernel matrix.

torch.Size([8496, 2])
We keep 1.06e+06/1.96e+07 =  5% of the original kernel matrix.

torch.Size([13641, 2])
We keep 1.92e+06/7.02e+07 =  2% of the original kernel matrix.

torch.Size([59112, 2])
We keep 2.68e+07/1.40e+09 =  1% of the original kernel matrix.

torch.Size([35873, 2])
We keep 1.07e+07/5.93e+08 =  1% of the original kernel matrix.

torch.Size([6813, 2])
We keep 5.38e+05/9.88e+06 =  5% of the original kernel matrix.

torch.Size([12449, 2])
We keep 1.48e+06/4.98e+07 =  2% of the original kernel matrix.

torch.Size([11473, 2])
We keep 3.65e+06/5.90e+07 =  6% of the original kernel matrix.

torch.Size([15288, 2])
We keep 2.83e+06/1.22e+08 =  2% of the original kernel matrix.

torch.Size([66787, 2])
We keep 5.14e+07/2.01e+09 =  2% of the original kernel matrix.

torch.Size([37657, 2])
We keep 1.24e+07/7.10e+08 =  1% of the original kernel matrix.

torch.Size([30693, 2])
We keep 6.30e+07/7.48e+08 =  8% of the original kernel matrix.

torch.Size([25705, 2])
We keep 7.56e+06/4.33e+08 =  1% of the original kernel matrix.

torch.Size([33185, 2])
We keep 1.14e+07/4.06e+08 =  2% of the original kernel matrix.

torch.Size([31335, 2])
We keep 6.47e+06/3.19e+08 =  2% of the original kernel matrix.

torch.Size([20892, 2])
We keep 8.24e+06/2.04e+08 =  4% of the original kernel matrix.

torch.Size([21048, 2])
We keep 4.69e+06/2.26e+08 =  2% of the original kernel matrix.

torch.Size([128200, 2])
We keep 2.59e+08/9.42e+09 =  2% of the original kernel matrix.

torch.Size([54122, 2])
We keep 2.47e+07/1.54e+09 =  1% of the original kernel matrix.

torch.Size([252747, 2])
We keep 8.66e+08/3.71e+10 =  2% of the original kernel matrix.

torch.Size([74084, 2])
We keep 4.47e+07/3.05e+09 =  1% of the original kernel matrix.

torch.Size([19951, 2])
We keep 4.12e+06/1.35e+08 =  3% of the original kernel matrix.

torch.Size([21724, 2])
We keep 4.09e+06/1.84e+08 =  2% of the original kernel matrix.

torch.Size([8420, 2])
We keep 9.49e+05/2.02e+07 =  4% of the original kernel matrix.

torch.Size([13865, 2])
We keep 1.97e+06/7.12e+07 =  2% of the original kernel matrix.

torch.Size([83985, 2])
We keep 4.15e+08/5.34e+09 =  7% of the original kernel matrix.

torch.Size([42359, 2])
We keep 1.89e+07/1.16e+09 =  1% of the original kernel matrix.

torch.Size([23343, 2])
We keep 6.38e+06/2.07e+08 =  3% of the original kernel matrix.

torch.Size([22455, 2])
We keep 4.68e+06/2.28e+08 =  2% of the original kernel matrix.

torch.Size([25653, 2])
We keep 9.97e+06/2.42e+08 =  4% of the original kernel matrix.

torch.Size([23925, 2])
We keep 4.95e+06/2.46e+08 =  2% of the original kernel matrix.

torch.Size([25873, 2])
We keep 9.26e+06/2.64e+08 =  3% of the original kernel matrix.

torch.Size([23898, 2])
We keep 5.32e+06/2.57e+08 =  2% of the original kernel matrix.

torch.Size([30499, 2])
We keep 3.67e+07/7.03e+08 =  5% of the original kernel matrix.

torch.Size([24740, 2])
We keep 7.93e+06/4.20e+08 =  1% of the original kernel matrix.

torch.Size([60918, 2])
We keep 4.63e+07/1.63e+09 =  2% of the original kernel matrix.

torch.Size([35755, 2])
We keep 1.14e+07/6.40e+08 =  1% of the original kernel matrix.

torch.Size([59428, 2])
We keep 2.98e+08/5.51e+09 =  5% of the original kernel matrix.

torch.Size([32849, 2])
We keep 1.84e+07/1.17e+09 =  1% of the original kernel matrix.

torch.Size([20929, 2])
We keep 5.62e+06/1.75e+08 =  3% of the original kernel matrix.

torch.Size([21683, 2])
We keep 4.37e+06/2.09e+08 =  2% of the original kernel matrix.

torch.Size([13690, 2])
We keep 2.24e+06/6.23e+07 =  3% of the original kernel matrix.

torch.Size([17585, 2])
We keep 3.03e+06/1.25e+08 =  2% of the original kernel matrix.

torch.Size([5330, 2])
We keep 3.27e+05/5.46e+06 =  5% of the original kernel matrix.

torch.Size([10975, 2])
We keep 1.18e+06/3.70e+07 =  3% of the original kernel matrix.

torch.Size([14751, 2])
We keep 5.27e+06/8.40e+07 =  6% of the original kernel matrix.

torch.Size([18362, 2])
We keep 3.40e+06/1.45e+08 =  2% of the original kernel matrix.

torch.Size([58485, 2])
We keep 8.77e+07/2.39e+09 =  3% of the original kernel matrix.

torch.Size([35136, 2])
We keep 1.36e+07/7.74e+08 =  1% of the original kernel matrix.

torch.Size([61226, 2])
We keep 4.63e+07/1.50e+09 =  3% of the original kernel matrix.

torch.Size([36310, 2])
We keep 1.09e+07/6.14e+08 =  1% of the original kernel matrix.

torch.Size([109959, 2])
We keep 9.78e+07/4.35e+09 =  2% of the original kernel matrix.

torch.Size([50016, 2])
We keep 1.71e+07/1.04e+09 =  1% of the original kernel matrix.

torch.Size([8702, 2])
We keep 8.51e+05/1.78e+07 =  4% of the original kernel matrix.

torch.Size([13839, 2])
We keep 1.84e+06/6.68e+07 =  2% of the original kernel matrix.

torch.Size([21771, 2])
We keep 9.01e+06/1.82e+08 =  4% of the original kernel matrix.

torch.Size([22188, 2])
We keep 4.30e+06/2.13e+08 =  2% of the original kernel matrix.

torch.Size([29547, 2])
We keep 1.90e+07/4.97e+08 =  3% of the original kernel matrix.

torch.Size([25713, 2])
We keep 7.10e+06/3.53e+08 =  2% of the original kernel matrix.

torch.Size([60689, 2])
We keep 6.38e+07/1.57e+09 =  4% of the original kernel matrix.

torch.Size([36490, 2])
We keep 1.07e+07/6.27e+08 =  1% of the original kernel matrix.

torch.Size([25679, 2])
We keep 9.30e+06/2.64e+08 =  3% of the original kernel matrix.

torch.Size([23964, 2])
We keep 5.08e+06/2.57e+08 =  1% of the original kernel matrix.

torch.Size([49352, 2])
We keep 1.76e+07/8.08e+08 =  2% of the original kernel matrix.

torch.Size([33133, 2])
We keep 8.17e+06/4.50e+08 =  1% of the original kernel matrix.

torch.Size([129447, 2])
We keep 4.66e+09/3.30e+10 = 14% of the original kernel matrix.

torch.Size([47589, 2])
We keep 4.35e+07/2.87e+09 =  1% of the original kernel matrix.

torch.Size([22602, 2])
We keep 5.75e+06/1.97e+08 =  2% of the original kernel matrix.

torch.Size([23032, 2])
We keep 4.44e+06/2.22e+08 =  2% of the original kernel matrix.

torch.Size([2617, 2])
We keep 9.71e+04/1.15e+06 =  8% of the original kernel matrix.

torch.Size([8165, 2])
We keep 6.70e+05/1.70e+07 =  3% of the original kernel matrix.

torch.Size([7953, 2])
We keep 7.14e+05/1.51e+07 =  4% of the original kernel matrix.

torch.Size([13221, 2])
We keep 1.70e+06/6.15e+07 =  2% of the original kernel matrix.

torch.Size([88022, 2])
We keep 4.69e+07/2.81e+09 =  1% of the original kernel matrix.

torch.Size([44399, 2])
We keep 1.39e+07/8.39e+08 =  1% of the original kernel matrix.

torch.Size([21802, 2])
We keep 1.57e+07/2.38e+08 =  6% of the original kernel matrix.

torch.Size([21719, 2])
We keep 4.94e+06/2.44e+08 =  2% of the original kernel matrix.

torch.Size([47773, 2])
We keep 1.58e+07/8.40e+08 =  1% of the original kernel matrix.

torch.Size([32483, 2])
We keep 8.43e+06/4.59e+08 =  1% of the original kernel matrix.

torch.Size([2464, 2])
We keep 8.21e+04/1.00e+06 =  8% of the original kernel matrix.

torch.Size([8176, 2])
We keep 6.37e+05/1.58e+07 =  4% of the original kernel matrix.

torch.Size([4932, 2])
We keep 3.18e+05/4.95e+06 =  6% of the original kernel matrix.

torch.Size([10619, 2])
We keep 1.14e+06/3.52e+07 =  3% of the original kernel matrix.

torch.Size([128620, 2])
We keep 1.71e+08/6.77e+09 =  2% of the original kernel matrix.

torch.Size([53956, 2])
We keep 2.09e+07/1.30e+09 =  1% of the original kernel matrix.

torch.Size([59949, 2])
We keep 2.85e+07/1.31e+09 =  2% of the original kernel matrix.

torch.Size([35819, 2])
We keep 1.03e+07/5.73e+08 =  1% of the original kernel matrix.

torch.Size([30919, 2])
We keep 8.07e+06/3.43e+08 =  2% of the original kernel matrix.

torch.Size([26630, 2])
We keep 5.84e+06/2.93e+08 =  1% of the original kernel matrix.

torch.Size([4546, 2])
We keep 2.47e+05/3.68e+06 =  6% of the original kernel matrix.

torch.Size([10302, 2])
We keep 1.02e+06/3.04e+07 =  3% of the original kernel matrix.

torch.Size([6084, 2])
We keep 4.38e+05/7.81e+06 =  5% of the original kernel matrix.

torch.Size([11721, 2])
We keep 1.34e+06/4.42e+07 =  3% of the original kernel matrix.

torch.Size([22852, 2])
We keep 1.34e+07/1.94e+08 =  6% of the original kernel matrix.

torch.Size([22797, 2])
We keep 4.52e+06/2.21e+08 =  2% of the original kernel matrix.

torch.Size([32948, 2])
We keep 1.32e+07/4.27e+08 =  3% of the original kernel matrix.

torch.Size([27040, 2])
We keep 6.37e+06/3.27e+08 =  1% of the original kernel matrix.

torch.Size([51845, 2])
We keep 5.39e+07/1.17e+09 =  4% of the original kernel matrix.

torch.Size([33580, 2])
We keep 9.90e+06/5.42e+08 =  1% of the original kernel matrix.

torch.Size([5060, 2])
We keep 4.43e+05/5.84e+06 =  7% of the original kernel matrix.

torch.Size([10916, 2])
We keep 1.16e+06/3.83e+07 =  3% of the original kernel matrix.

torch.Size([26920, 2])
We keep 2.76e+08/1.71e+09 = 16% of the original kernel matrix.

torch.Size([22162, 2])
We keep 1.17e+07/6.55e+08 =  1% of the original kernel matrix.

torch.Size([10514, 2])
We keep 1.64e+06/3.22e+07 =  5% of the original kernel matrix.

torch.Size([15122, 2])
We keep 2.26e+06/8.98e+07 =  2% of the original kernel matrix.

torch.Size([17136, 2])
We keep 1.11e+07/1.91e+08 =  5% of the original kernel matrix.

torch.Size([18910, 2])
We keep 4.73e+06/2.19e+08 =  2% of the original kernel matrix.

torch.Size([184755, 2])
We keep 3.89e+08/1.82e+10 =  2% of the original kernel matrix.

torch.Size([66504, 2])
We keep 3.19e+07/2.14e+09 =  1% of the original kernel matrix.

torch.Size([7236, 2])
We keep 1.24e+06/1.54e+07 =  8% of the original kernel matrix.

torch.Size([12783, 2])
We keep 1.70e+06/6.21e+07 =  2% of the original kernel matrix.

torch.Size([29860, 2])
We keep 1.27e+07/3.69e+08 =  3% of the original kernel matrix.

torch.Size([25686, 2])
We keep 5.95e+06/3.04e+08 =  1% of the original kernel matrix.

torch.Size([95965, 2])
We keep 7.93e+07/3.30e+09 =  2% of the original kernel matrix.

torch.Size([46579, 2])
We keep 1.52e+07/9.10e+08 =  1% of the original kernel matrix.

torch.Size([23755, 2])
We keep 6.80e+06/1.88e+08 =  3% of the original kernel matrix.

torch.Size([23430, 2])
We keep 4.48e+06/2.17e+08 =  2% of the original kernel matrix.

torch.Size([31675, 2])
We keep 1.52e+07/4.54e+08 =  3% of the original kernel matrix.

torch.Size([26968, 2])
We keep 6.66e+06/3.38e+08 =  1% of the original kernel matrix.

torch.Size([92442, 2])
We keep 6.07e+07/3.04e+09 =  1% of the original kernel matrix.

torch.Size([45552, 2])
We keep 1.46e+07/8.73e+08 =  1% of the original kernel matrix.

torch.Size([3510, 2])
We keep 1.38e+05/1.97e+06 =  7% of the original kernel matrix.

torch.Size([9303, 2])
We keep 8.17e+05/2.22e+07 =  3% of the original kernel matrix.

torch.Size([29995, 2])
We keep 8.05e+06/3.39e+08 =  2% of the original kernel matrix.

torch.Size([26284, 2])
We keep 5.83e+06/2.91e+08 =  2% of the original kernel matrix.

torch.Size([18308, 2])
We keep 3.53e+06/1.04e+08 =  3% of the original kernel matrix.

torch.Size([20569, 2])
We keep 3.55e+06/1.62e+08 =  2% of the original kernel matrix.

torch.Size([69491, 2])
We keep 6.87e+07/2.14e+09 =  3% of the original kernel matrix.

torch.Size([38719, 2])
We keep 1.26e+07/7.33e+08 =  1% of the original kernel matrix.

torch.Size([80532, 2])
We keep 6.43e+07/2.81e+09 =  2% of the original kernel matrix.

torch.Size([41952, 2])
We keep 1.40e+07/8.40e+08 =  1% of the original kernel matrix.

torch.Size([28455, 2])
We keep 1.45e+07/4.03e+08 =  3% of the original kernel matrix.

torch.Size([24532, 2])
We keep 6.29e+06/3.18e+08 =  1% of the original kernel matrix.

torch.Size([54278, 2])
We keep 3.02e+07/1.09e+09 =  2% of the original kernel matrix.

torch.Size([34570, 2])
We keep 9.42e+06/5.22e+08 =  1% of the original kernel matrix.

torch.Size([12819, 2])
We keep 1.84e+06/5.01e+07 =  3% of the original kernel matrix.

torch.Size([16875, 2])
We keep 2.68e+06/1.12e+08 =  2% of the original kernel matrix.

torch.Size([7763, 2])
We keep 2.68e+06/4.05e+07 =  6% of the original kernel matrix.

torch.Size([12681, 2])
We keep 2.54e+06/1.01e+08 =  2% of the original kernel matrix.

torch.Size([8924, 2])
We keep 3.26e+06/2.44e+07 = 13% of the original kernel matrix.

torch.Size([13998, 2])
We keep 2.01e+06/7.83e+07 =  2% of the original kernel matrix.

torch.Size([7445, 2])
We keep 9.14e+05/1.51e+07 =  6% of the original kernel matrix.

torch.Size([12654, 2])
We keep 1.71e+06/6.15e+07 =  2% of the original kernel matrix.

torch.Size([89637, 2])
We keep 6.97e+07/3.03e+09 =  2% of the original kernel matrix.

torch.Size([44803, 2])
We keep 1.47e+07/8.72e+08 =  1% of the original kernel matrix.

torch.Size([134100, 2])
We keep 2.51e+08/1.00e+10 =  2% of the original kernel matrix.

torch.Size([55105, 2])
We keep 2.51e+07/1.58e+09 =  1% of the original kernel matrix.

torch.Size([4381, 2])
We keep 3.06e+05/4.80e+06 =  6% of the original kernel matrix.

torch.Size([10084, 2])
We keep 1.12e+06/3.47e+07 =  3% of the original kernel matrix.

torch.Size([17643, 2])
We keep 3.08e+06/1.01e+08 =  3% of the original kernel matrix.

torch.Size([20034, 2])
We keep 3.41e+06/1.59e+08 =  2% of the original kernel matrix.

torch.Size([12065, 2])
We keep 1.38e+06/3.76e+07 =  3% of the original kernel matrix.

torch.Size([16509, 2])
We keep 2.38e+06/9.71e+07 =  2% of the original kernel matrix.

torch.Size([149611, 2])
We keep 2.92e+08/1.49e+10 =  1% of the original kernel matrix.

torch.Size([59111, 2])
We keep 2.98e+07/1.93e+09 =  1% of the original kernel matrix.

torch.Size([24409, 2])
We keep 5.59e+06/2.10e+08 =  2% of the original kernel matrix.

torch.Size([23635, 2])
We keep 4.77e+06/2.29e+08 =  2% of the original kernel matrix.

torch.Size([12552, 2])
We keep 3.30e+06/5.12e+07 =  6% of the original kernel matrix.

torch.Size([16682, 2])
We keep 2.69e+06/1.13e+08 =  2% of the original kernel matrix.

torch.Size([46210, 2])
We keep 6.46e+07/1.32e+09 =  4% of the original kernel matrix.

torch.Size([31736, 2])
We keep 1.03e+07/5.75e+08 =  1% of the original kernel matrix.

torch.Size([77604, 2])
We keep 4.52e+07/2.03e+09 =  2% of the original kernel matrix.

torch.Size([41179, 2])
We keep 1.22e+07/7.13e+08 =  1% of the original kernel matrix.

torch.Size([7506, 2])
We keep 9.71e+05/1.78e+07 =  5% of the original kernel matrix.

torch.Size([12764, 2])
We keep 1.83e+06/6.68e+07 =  2% of the original kernel matrix.

torch.Size([32986, 2])
We keep 1.41e+07/4.69e+08 =  3% of the original kernel matrix.

torch.Size([27249, 2])
We keep 6.78e+06/3.43e+08 =  1% of the original kernel matrix.

torch.Size([5079, 2])
We keep 2.85e+05/4.86e+06 =  5% of the original kernel matrix.

torch.Size([11024, 2])
We keep 1.08e+06/3.49e+07 =  3% of the original kernel matrix.

time for making ranges is 2.2829017639160156
Sorting X and nu_X
time for sorting X is 0.07003450393676758
Sorting Z and nu_Z
time for sorting Z is 0.00024318695068359375
Starting Optim
sum tnu_Z before tensor(22716674., device='cuda:0')
c= tensor(578.5757, device='cuda:0')
c= tensor(38449.7461, device='cuda:0')
c= tensor(41649.9375, device='cuda:0')
c= tensor(43512.4102, device='cuda:0')
c= tensor(348930.4688, device='cuda:0')
c= tensor(669078.2500, device='cuda:0')
c= tensor(1153168.7500, device='cuda:0')
c= tensor(1325513.6250, device='cuda:0')
c= tensor(1333792.7500, device='cuda:0')
c= tensor(5040139., device='cuda:0')
c= tensor(5054844.5000, device='cuda:0')
c= tensor(6229451., device='cuda:0')
c= tensor(6236596., device='cuda:0')
c= tensor(13957196., device='cuda:0')
c= tensor(14038671., device='cuda:0')
c= tensor(14600180., device='cuda:0')
c= tensor(15270721., device='cuda:0')
c= tensor(15538672., device='cuda:0')
c= tensor(22688092., device='cuda:0')
c= tensor(24893732., device='cuda:0')
c= tensor(24941574., device='cuda:0')
c= tensor(43815480., device='cuda:0')
c= tensor(43852652., device='cuda:0')
c= tensor(43877396., device='cuda:0')
c= tensor(44187236., device='cuda:0')
c= tensor(44702008., device='cuda:0')
c= tensor(45010088., device='cuda:0')
c= tensor(45117012., device='cuda:0')
c= tensor(46023636., device='cuda:0')
c= tensor(2.9411e+08, device='cuda:0')
c= tensor(2.9412e+08, device='cuda:0')
c= tensor(3.3752e+08, device='cuda:0')
c= tensor(3.3754e+08, device='cuda:0')
c= tensor(3.3755e+08, device='cuda:0')
c= tensor(3.3756e+08, device='cuda:0')
c= tensor(3.3969e+08, device='cuda:0')
c= tensor(3.4030e+08, device='cuda:0')
c= tensor(3.4030e+08, device='cuda:0')
c= tensor(3.4031e+08, device='cuda:0')
c= tensor(3.4031e+08, device='cuda:0')
c= tensor(3.4032e+08, device='cuda:0')
c= tensor(3.4032e+08, device='cuda:0')
c= tensor(3.4032e+08, device='cuda:0')
c= tensor(3.4032e+08, device='cuda:0')
c= tensor(3.4032e+08, device='cuda:0')
c= tensor(3.4032e+08, device='cuda:0')
c= tensor(3.4033e+08, device='cuda:0')
c= tensor(3.4034e+08, device='cuda:0')
c= tensor(3.4034e+08, device='cuda:0')
c= tensor(3.4035e+08, device='cuda:0')
c= tensor(3.4038e+08, device='cuda:0')
c= tensor(3.4038e+08, device='cuda:0')
c= tensor(3.4039e+08, device='cuda:0')
c= tensor(3.4039e+08, device='cuda:0')
c= tensor(3.4040e+08, device='cuda:0')
c= tensor(3.4040e+08, device='cuda:0')
c= tensor(3.4040e+08, device='cuda:0')
c= tensor(3.4041e+08, device='cuda:0')
c= tensor(3.4041e+08, device='cuda:0')
c= tensor(3.4041e+08, device='cuda:0')
c= tensor(3.4042e+08, device='cuda:0')
c= tensor(3.4042e+08, device='cuda:0')
c= tensor(3.4043e+08, device='cuda:0')
c= tensor(3.4045e+08, device='cuda:0')
c= tensor(3.4045e+08, device='cuda:0')
c= tensor(3.4045e+08, device='cuda:0')
c= tensor(3.4046e+08, device='cuda:0')
c= tensor(3.4046e+08, device='cuda:0')
c= tensor(3.4046e+08, device='cuda:0')
c= tensor(3.4047e+08, device='cuda:0')
c= tensor(3.4047e+08, device='cuda:0')
c= tensor(3.4047e+08, device='cuda:0')
c= tensor(3.4048e+08, device='cuda:0')
c= tensor(3.4048e+08, device='cuda:0')
c= tensor(3.4048e+08, device='cuda:0')
c= tensor(3.4048e+08, device='cuda:0')
c= tensor(3.4049e+08, device='cuda:0')
c= tensor(3.4049e+08, device='cuda:0')
c= tensor(3.4049e+08, device='cuda:0')
c= tensor(3.4050e+08, device='cuda:0')
c= tensor(3.4051e+08, device='cuda:0')
c= tensor(3.4051e+08, device='cuda:0')
c= tensor(3.4052e+08, device='cuda:0')
c= tensor(3.4052e+08, device='cuda:0')
c= tensor(3.4052e+08, device='cuda:0')
c= tensor(3.4052e+08, device='cuda:0')
c= tensor(3.4052e+08, device='cuda:0')
c= tensor(3.4052e+08, device='cuda:0')
c= tensor(3.4053e+08, device='cuda:0')
c= tensor(3.4054e+08, device='cuda:0')
c= tensor(3.4054e+08, device='cuda:0')
c= tensor(3.4054e+08, device='cuda:0')
c= tensor(3.4054e+08, device='cuda:0')
c= tensor(3.4055e+08, device='cuda:0')
c= tensor(3.4055e+08, device='cuda:0')
c= tensor(3.4055e+08, device='cuda:0')
c= tensor(3.4056e+08, device='cuda:0')
c= tensor(3.4057e+08, device='cuda:0')
c= tensor(3.4057e+08, device='cuda:0')
c= tensor(3.4058e+08, device='cuda:0')
c= tensor(3.4059e+08, device='cuda:0')
c= tensor(3.4060e+08, device='cuda:0')
c= tensor(3.4060e+08, device='cuda:0')
c= tensor(3.4060e+08, device='cuda:0')
c= tensor(3.4061e+08, device='cuda:0')
c= tensor(3.4061e+08, device='cuda:0')
c= tensor(3.4062e+08, device='cuda:0')
c= tensor(3.4062e+08, device='cuda:0')
c= tensor(3.4062e+08, device='cuda:0')
c= tensor(3.4063e+08, device='cuda:0')
c= tensor(3.4063e+08, device='cuda:0')
c= tensor(3.4063e+08, device='cuda:0')
c= tensor(3.4063e+08, device='cuda:0')
c= tensor(3.4063e+08, device='cuda:0')
c= tensor(3.4064e+08, device='cuda:0')
c= tensor(3.4064e+08, device='cuda:0')
c= tensor(3.4064e+08, device='cuda:0')
c= tensor(3.4064e+08, device='cuda:0')
c= tensor(3.4065e+08, device='cuda:0')
c= tensor(3.4065e+08, device='cuda:0')
c= tensor(3.4068e+08, device='cuda:0')
c= tensor(3.4068e+08, device='cuda:0')
c= tensor(3.4068e+08, device='cuda:0')
c= tensor(3.4069e+08, device='cuda:0')
c= tensor(3.4069e+08, device='cuda:0')
c= tensor(3.4069e+08, device='cuda:0')
c= tensor(3.4069e+08, device='cuda:0')
c= tensor(3.4069e+08, device='cuda:0')
c= tensor(3.4072e+08, device='cuda:0')
c= tensor(3.4073e+08, device='cuda:0')
c= tensor(3.4075e+08, device='cuda:0')
c= tensor(3.4075e+08, device='cuda:0')
c= tensor(3.4076e+08, device='cuda:0')
c= tensor(3.4076e+08, device='cuda:0')
c= tensor(3.4076e+08, device='cuda:0')
c= tensor(3.4076e+08, device='cuda:0')
c= tensor(3.4076e+08, device='cuda:0')
c= tensor(3.4076e+08, device='cuda:0')
c= tensor(3.4077e+08, device='cuda:0')
c= tensor(3.4077e+08, device='cuda:0')
c= tensor(3.4077e+08, device='cuda:0')
c= tensor(3.4078e+08, device='cuda:0')
c= tensor(3.4079e+08, device='cuda:0')
c= tensor(3.4081e+08, device='cuda:0')
c= tensor(3.4081e+08, device='cuda:0')
c= tensor(3.4081e+08, device='cuda:0')
c= tensor(3.4082e+08, device='cuda:0')
c= tensor(3.4082e+08, device='cuda:0')
c= tensor(3.4082e+08, device='cuda:0')
c= tensor(3.4082e+08, device='cuda:0')
c= tensor(3.4083e+08, device='cuda:0')
c= tensor(3.4084e+08, device='cuda:0')
c= tensor(3.4084e+08, device='cuda:0')
c= tensor(3.4087e+08, device='cuda:0')
c= tensor(3.4087e+08, device='cuda:0')
c= tensor(3.4089e+08, device='cuda:0')
c= tensor(3.4090e+08, device='cuda:0')
c= tensor(3.4090e+08, device='cuda:0')
c= tensor(3.4090e+08, device='cuda:0')
c= tensor(3.4090e+08, device='cuda:0')
c= tensor(3.4092e+08, device='cuda:0')
c= tensor(3.4092e+08, device='cuda:0')
c= tensor(3.4092e+08, device='cuda:0')
c= tensor(3.4092e+08, device='cuda:0')
c= tensor(3.4092e+08, device='cuda:0')
c= tensor(3.4093e+08, device='cuda:0')
c= tensor(3.4093e+08, device='cuda:0')
c= tensor(3.4093e+08, device='cuda:0')
c= tensor(3.4093e+08, device='cuda:0')
c= tensor(3.4093e+08, device='cuda:0')
c= tensor(3.4093e+08, device='cuda:0')
c= tensor(3.4094e+08, device='cuda:0')
c= tensor(3.4094e+08, device='cuda:0')
c= tensor(3.4094e+08, device='cuda:0')
c= tensor(3.4095e+08, device='cuda:0')
c= tensor(3.4095e+08, device='cuda:0')
c= tensor(3.4096e+08, device='cuda:0')
c= tensor(3.4096e+08, device='cuda:0')
c= tensor(3.4097e+08, device='cuda:0')
c= tensor(3.4097e+08, device='cuda:0')
c= tensor(3.4097e+08, device='cuda:0')
c= tensor(3.4098e+08, device='cuda:0')
c= tensor(3.4098e+08, device='cuda:0')
c= tensor(3.4098e+08, device='cuda:0')
c= tensor(3.4099e+08, device='cuda:0')
c= tensor(3.4100e+08, device='cuda:0')
c= tensor(3.4100e+08, device='cuda:0')
c= tensor(3.4101e+08, device='cuda:0')
c= tensor(3.4101e+08, device='cuda:0')
c= tensor(3.4109e+08, device='cuda:0')
c= tensor(3.4109e+08, device='cuda:0')
c= tensor(3.4109e+08, device='cuda:0')
c= tensor(3.4109e+08, device='cuda:0')
c= tensor(3.4109e+08, device='cuda:0')
c= tensor(3.4110e+08, device='cuda:0')
c= tensor(3.4110e+08, device='cuda:0')
c= tensor(3.4110e+08, device='cuda:0')
c= tensor(3.4110e+08, device='cuda:0')
c= tensor(3.4110e+08, device='cuda:0')
c= tensor(3.4111e+08, device='cuda:0')
c= tensor(3.4111e+08, device='cuda:0')
c= tensor(3.4111e+08, device='cuda:0')
c= tensor(3.4112e+08, device='cuda:0')
c= tensor(3.4113e+08, device='cuda:0')
c= tensor(3.4113e+08, device='cuda:0')
c= tensor(3.4113e+08, device='cuda:0')
c= tensor(3.4114e+08, device='cuda:0')
c= tensor(3.4114e+08, device='cuda:0')
c= tensor(3.4115e+08, device='cuda:0')
c= tensor(3.4116e+08, device='cuda:0')
c= tensor(3.4117e+08, device='cuda:0')
c= tensor(3.4118e+08, device='cuda:0')
c= tensor(3.4118e+08, device='cuda:0')
c= tensor(3.4118e+08, device='cuda:0')
c= tensor(3.4118e+08, device='cuda:0')
c= tensor(3.4118e+08, device='cuda:0')
c= tensor(3.4118e+08, device='cuda:0')
c= tensor(3.4118e+08, device='cuda:0')
c= tensor(3.4119e+08, device='cuda:0')
c= tensor(3.4119e+08, device='cuda:0')
c= tensor(3.4119e+08, device='cuda:0')
c= tensor(3.4120e+08, device='cuda:0')
c= tensor(3.4120e+08, device='cuda:0')
c= tensor(3.4120e+08, device='cuda:0')
c= tensor(3.4121e+08, device='cuda:0')
c= tensor(3.4121e+08, device='cuda:0')
c= tensor(3.4121e+08, device='cuda:0')
c= tensor(3.4121e+08, device='cuda:0')
c= tensor(3.4122e+08, device='cuda:0')
c= tensor(3.4122e+08, device='cuda:0')
c= tensor(3.4122e+08, device='cuda:0')
c= tensor(3.4122e+08, device='cuda:0')
c= tensor(3.4123e+08, device='cuda:0')
c= tensor(3.4123e+08, device='cuda:0')
c= tensor(3.4123e+08, device='cuda:0')
c= tensor(3.4123e+08, device='cuda:0')
c= tensor(3.4124e+08, device='cuda:0')
c= tensor(3.4124e+08, device='cuda:0')
c= tensor(3.4124e+08, device='cuda:0')
c= tensor(3.4126e+08, device='cuda:0')
c= tensor(3.4227e+08, device='cuda:0')
c= tensor(3.4236e+08, device='cuda:0')
c= tensor(3.4237e+08, device='cuda:0')
c= tensor(3.4237e+08, device='cuda:0')
c= tensor(3.4243e+08, device='cuda:0')
c= tensor(3.4300e+08, device='cuda:0')
c= tensor(3.5814e+08, device='cuda:0')
c= tensor(3.5814e+08, device='cuda:0')
c= tensor(3.5919e+08, device='cuda:0')
c= tensor(3.5932e+08, device='cuda:0')
c= tensor(3.5969e+08, device='cuda:0')
c= tensor(3.7234e+08, device='cuda:0')
c= tensor(3.7234e+08, device='cuda:0')
c= tensor(3.7235e+08, device='cuda:0')
c= tensor(3.7622e+08, device='cuda:0')
c= tensor(3.9706e+08, device='cuda:0')
c= tensor(3.9707e+08, device='cuda:0')
c= tensor(3.9722e+08, device='cuda:0')
c= tensor(3.9837e+08, device='cuda:0')
c= tensor(4.0097e+08, device='cuda:0')
c= tensor(4.0224e+08, device='cuda:0')
c= tensor(4.0241e+08, device='cuda:0')
c= tensor(4.0276e+08, device='cuda:0')
c= tensor(4.0281e+08, device='cuda:0')
c= tensor(4.0282e+08, device='cuda:0')
c= tensor(4.4215e+08, device='cuda:0')
c= tensor(4.4217e+08, device='cuda:0')
c= tensor(4.4218e+08, device='cuda:0')
c= tensor(4.4237e+08, device='cuda:0')
c= tensor(4.4252e+08, device='cuda:0')
c= tensor(4.5974e+08, device='cuda:0')
c= tensor(4.6051e+08, device='cuda:0')
c= tensor(4.6051e+08, device='cuda:0')
c= tensor(4.6066e+08, device='cuda:0')
c= tensor(4.6068e+08, device='cuda:0')
c= tensor(4.6079e+08, device='cuda:0')
c= tensor(4.6151e+08, device='cuda:0')
c= tensor(4.6151e+08, device='cuda:0')
c= tensor(4.6177e+08, device='cuda:0')
c= tensor(4.6177e+08, device='cuda:0')
c= tensor(4.6177e+08, device='cuda:0')
c= tensor(4.6283e+08, device='cuda:0')
c= tensor(4.6321e+08, device='cuda:0')
c= tensor(4.6339e+08, device='cuda:0')
c= tensor(4.6570e+08, device='cuda:0')
c= tensor(4.7763e+08, device='cuda:0')
c= tensor(4.7764e+08, device='cuda:0')
c= tensor(4.7771e+08, device='cuda:0')
c= tensor(4.7849e+08, device='cuda:0')
c= tensor(4.7850e+08, device='cuda:0')
c= tensor(4.8082e+08, device='cuda:0')
c= tensor(4.9077e+08, device='cuda:0')
c= tensor(5.1253e+08, device='cuda:0')
c= tensor(5.1259e+08, device='cuda:0')
c= tensor(5.1267e+08, device='cuda:0')
c= tensor(5.1269e+08, device='cuda:0')
c= tensor(5.1269e+08, device='cuda:0')
c= tensor(5.1298e+08, device='cuda:0')
c= tensor(5.1300e+08, device='cuda:0')
c= tensor(5.1310e+08, device='cuda:0')
c= tensor(5.2011e+08, device='cuda:0')
c= tensor(5.2400e+08, device='cuda:0')
c= tensor(5.2402e+08, device='cuda:0')
c= tensor(5.2404e+08, device='cuda:0')
c= tensor(5.3814e+08, device='cuda:0')
c= tensor(5.3823e+08, device='cuda:0')
c= tensor(5.3824e+08, device='cuda:0')
c= tensor(5.3826e+08, device='cuda:0')
c= tensor(6.7490e+08, device='cuda:0')
c= tensor(6.7492e+08, device='cuda:0')
c= tensor(6.8651e+08, device='cuda:0')
c= tensor(6.8652e+08, device='cuda:0')
c= tensor(6.8814e+08, device='cuda:0')
c= tensor(6.8850e+08, device='cuda:0')
c= tensor(7.3942e+08, device='cuda:0')
c= tensor(7.3996e+08, device='cuda:0')
c= tensor(7.3997e+08, device='cuda:0')
c= tensor(7.4167e+08, device='cuda:0')
c= tensor(7.4251e+08, device='cuda:0')
c= tensor(7.4276e+08, device='cuda:0')
c= tensor(7.4301e+08, device='cuda:0')
c= tensor(7.4534e+08, device='cuda:0')
c= tensor(7.5050e+08, device='cuda:0')
c= tensor(7.5139e+08, device='cuda:0')
c= tensor(7.5139e+08, device='cuda:0')
c= tensor(7.5141e+08, device='cuda:0')
c= tensor(7.5284e+08, device='cuda:0')
c= tensor(7.7360e+08, device='cuda:0')
c= tensor(7.7362e+08, device='cuda:0')
c= tensor(7.7362e+08, device='cuda:0')
c= tensor(7.7377e+08, device='cuda:0')
c= tensor(7.7521e+08, device='cuda:0')
c= tensor(7.8226e+08, device='cuda:0')
c= tensor(7.8227e+08, device='cuda:0')
c= tensor(7.8245e+08, device='cuda:0')
c= tensor(7.8251e+08, device='cuda:0')
c= tensor(7.8252e+08, device='cuda:0')
c= tensor(7.8254e+08, device='cuda:0')
c= tensor(7.8256e+08, device='cuda:0')
c= tensor(7.9049e+08, device='cuda:0')
c= tensor(7.9066e+08, device='cuda:0')
c= tensor(7.9071e+08, device='cuda:0')
c= tensor(7.9088e+08, device='cuda:0')
c= tensor(7.9090e+08, device='cuda:0')
c= tensor(8.1576e+08, device='cuda:0')
c= tensor(8.1577e+08, device='cuda:0')
c= tensor(8.1764e+08, device='cuda:0')
c= tensor(8.1764e+08, device='cuda:0')
c= tensor(8.1764e+08, device='cuda:0')
c= tensor(8.1764e+08, device='cuda:0')
c= tensor(8.1793e+08, device='cuda:0')
c= tensor(8.1793e+08, device='cuda:0')
c= tensor(8.1848e+08, device='cuda:0')
c= tensor(8.1848e+08, device='cuda:0')
c= tensor(8.1849e+08, device='cuda:0')
c= tensor(8.2578e+08, device='cuda:0')
c= tensor(8.2594e+08, device='cuda:0')
c= tensor(8.2607e+08, device='cuda:0')
c= tensor(8.2891e+08, device='cuda:0')
c= tensor(8.3332e+08, device='cuda:0')
c= tensor(8.3332e+08, device='cuda:0')
c= tensor(8.3333e+08, device='cuda:0')
c= tensor(8.3350e+08, device='cuda:0')
c= tensor(8.3350e+08, device='cuda:0')
c= tensor(8.3350e+08, device='cuda:0')
c= tensor(8.3357e+08, device='cuda:0')
c= tensor(8.3358e+08, device='cuda:0')
c= tensor(8.3358e+08, device='cuda:0')
c= tensor(8.3359e+08, device='cuda:0')
c= tensor(8.3359e+08, device='cuda:0')
c= tensor(8.4529e+08, device='cuda:0')
c= tensor(8.4534e+08, device='cuda:0')
c= tensor(8.4607e+08, device='cuda:0')
c= tensor(8.4614e+08, device='cuda:0')
c= tensor(8.4627e+08, device='cuda:0')
c= tensor(8.4642e+08, device='cuda:0')
c= tensor(9.4087e+08, device='cuda:0')
c= tensor(9.7253e+08, device='cuda:0')
c= tensor(9.7317e+08, device='cuda:0')
c= tensor(9.7417e+08, device='cuda:0')
c= tensor(9.7418e+08, device='cuda:0')
c= tensor(9.7779e+08, device='cuda:0')
c= tensor(9.7805e+08, device='cuda:0')
c= tensor(9.8162e+08, device='cuda:0')
c= tensor(9.8169e+08, device='cuda:0')
c= tensor(9.8264e+08, device='cuda:0')
c= tensor(1.0044e+09, device='cuda:0')
c= tensor(1.0048e+09, device='cuda:0')
c= tensor(1.0049e+09, device='cuda:0')
c= tensor(1.0051e+09, device='cuda:0')
c= tensor(1.0051e+09, device='cuda:0')
c= tensor(1.0051e+09, device='cuda:0')
c= tensor(1.0078e+09, device='cuda:0')
c= tensor(1.0080e+09, device='cuda:0')
c= tensor(1.0080e+09, device='cuda:0')
c= tensor(1.0085e+09, device='cuda:0')
c= tensor(1.0085e+09, device='cuda:0')
c= tensor(1.0085e+09, device='cuda:0')
c= tensor(1.0113e+09, device='cuda:0')
c= tensor(1.0120e+09, device='cuda:0')
c= tensor(1.0122e+09, device='cuda:0')
c= tensor(1.0135e+09, device='cuda:0')
c= tensor(1.0149e+09, device='cuda:0')
c= tensor(1.0150e+09, device='cuda:0')
c= tensor(1.0153e+09, device='cuda:0')
c= tensor(1.0162e+09, device='cuda:0')
c= tensor(1.0175e+09, device='cuda:0')
c= tensor(1.0175e+09, device='cuda:0')
c= tensor(1.0301e+09, device='cuda:0')
c= tensor(1.0432e+09, device='cuda:0')
c= tensor(1.0441e+09, device='cuda:0')
c= tensor(1.0450e+09, device='cuda:0')
c= tensor(1.0462e+09, device='cuda:0')
c= tensor(1.0462e+09, device='cuda:0')
c= tensor(1.0462e+09, device='cuda:0')
c= tensor(1.0463e+09, device='cuda:0')
c= tensor(1.0480e+09, device='cuda:0')
c= tensor(1.0500e+09, device='cuda:0')
c= tensor(1.0645e+09, device='cuda:0')
c= tensor(1.0723e+09, device='cuda:0')
c= tensor(1.0728e+09, device='cuda:0')
c= tensor(1.0730e+09, device='cuda:0')
c= tensor(1.0734e+09, device='cuda:0')
c= tensor(1.0734e+09, device='cuda:0')
c= tensor(1.0734e+09, device='cuda:0')
c= tensor(1.0782e+09, device='cuda:0')
c= tensor(1.0783e+09, device='cuda:0')
c= tensor(1.0783e+09, device='cuda:0')
c= tensor(1.0783e+09, device='cuda:0')
c= tensor(1.0789e+09, device='cuda:0')
c= tensor(1.0791e+09, device='cuda:0')
c= tensor(1.0797e+09, device='cuda:0')
c= tensor(1.0803e+09, device='cuda:0')
c= tensor(1.0805e+09, device='cuda:0')
c= tensor(1.0805e+09, device='cuda:0')
c= tensor(1.0805e+09, device='cuda:0')
c= tensor(1.0808e+09, device='cuda:0')
c= tensor(1.0814e+09, device='cuda:0')
c= tensor(1.0815e+09, device='cuda:0')
c= tensor(1.0831e+09, device='cuda:0')
c= tensor(1.0831e+09, device='cuda:0')
c= tensor(1.0832e+09, device='cuda:0')
c= tensor(1.0832e+09, device='cuda:0')
c= tensor(1.0840e+09, device='cuda:0')
c= tensor(1.0840e+09, device='cuda:0')
c= tensor(1.0843e+09, device='cuda:0')
c= tensor(1.0843e+09, device='cuda:0')
c= tensor(1.0844e+09, device='cuda:0')
c= tensor(1.0847e+09, device='cuda:0')
c= tensor(1.0990e+09, device='cuda:0')
c= tensor(1.0990e+09, device='cuda:0')
c= tensor(1.0990e+09, device='cuda:0')
c= tensor(1.1019e+09, device='cuda:0')
c= tensor(1.1019e+09, device='cuda:0')
c= tensor(1.1260e+09, device='cuda:0')
c= tensor(1.1260e+09, device='cuda:0')
c= tensor(1.1269e+09, device='cuda:0')
c= tensor(1.1330e+09, device='cuda:0')
c= tensor(1.1330e+09, device='cuda:0')
c= tensor(1.1406e+09, device='cuda:0')
c= tensor(1.1410e+09, device='cuda:0')
c= tensor(1.1686e+09, device='cuda:0')
c= tensor(1.1687e+09, device='cuda:0')
c= tensor(1.1689e+09, device='cuda:0')
c= tensor(1.1689e+09, device='cuda:0')
c= tensor(1.1689e+09, device='cuda:0')
c= tensor(1.1689e+09, device='cuda:0')
c= tensor(1.1709e+09, device='cuda:0')
c= tensor(1.1710e+09, device='cuda:0')
c= tensor(1.1720e+09, device='cuda:0')
c= tensor(1.1720e+09, device='cuda:0')
c= tensor(1.1720e+09, device='cuda:0')
c= tensor(1.1720e+09, device='cuda:0')
c= tensor(1.1753e+09, device='cuda:0')
c= tensor(1.1757e+09, device='cuda:0')
c= tensor(1.1875e+09, device='cuda:0')
c= tensor(1.1880e+09, device='cuda:0')
c= tensor(1.1880e+09, device='cuda:0')
c= tensor(1.1882e+09, device='cuda:0')
c= tensor(1.1883e+09, device='cuda:0')
c= tensor(1.3572e+09, device='cuda:0')
c= tensor(1.3572e+09, device='cuda:0')
c= tensor(1.3573e+09, device='cuda:0')
c= tensor(1.3590e+09, device='cuda:0')
c= tensor(1.3597e+09, device='cuda:0')
c= tensor(1.3597e+09, device='cuda:0')
c= tensor(1.3597e+09, device='cuda:0')
c= tensor(1.3715e+09, device='cuda:0')
c= tensor(1.3717e+09, device='cuda:0')
c= tensor(1.3734e+09, device='cuda:0')
c= tensor(1.3740e+09, device='cuda:0')
c= tensor(1.3750e+09, device='cuda:0')
c= tensor(1.3774e+09, device='cuda:0')
c= tensor(1.3851e+09, device='cuda:0')
c= tensor(1.3903e+09, device='cuda:0')
c= tensor(1.3903e+09, device='cuda:0')
c= tensor(1.3929e+09, device='cuda:0')
c= tensor(1.3930e+09, device='cuda:0')
c= tensor(1.3931e+09, device='cuda:0')
c= tensor(1.3931e+09, device='cuda:0')
c= tensor(1.3932e+09, device='cuda:0')
c= tensor(1.3999e+09, device='cuda:0')
c= tensor(1.3999e+09, device='cuda:0')
c= tensor(1.3999e+09, device='cuda:0')
c= tensor(1.4000e+09, device='cuda:0')
c= tensor(1.4002e+09, device='cuda:0')
c= tensor(1.4024e+09, device='cuda:0')
c= tensor(1.4024e+09, device='cuda:0')
c= tensor(1.4025e+09, device='cuda:0')
c= tensor(1.4025e+09, device='cuda:0')
c= tensor(1.4030e+09, device='cuda:0')
c= tensor(1.4030e+09, device='cuda:0')
c= tensor(1.4030e+09, device='cuda:0')
c= tensor(1.4030e+09, device='cuda:0')
c= tensor(1.4070e+09, device='cuda:0')
c= tensor(1.4070e+09, device='cuda:0')
c= tensor(1.4070e+09, device='cuda:0')
c= tensor(1.4070e+09, device='cuda:0')
c= tensor(1.4211e+09, device='cuda:0')
c= tensor(1.4259e+09, device='cuda:0')
c= tensor(1.4262e+09, device='cuda:0')
c= tensor(1.4262e+09, device='cuda:0')
c= tensor(1.4286e+09, device='cuda:0')
c= tensor(1.4298e+09, device='cuda:0')
c= tensor(1.4298e+09, device='cuda:0')
c= tensor(1.4299e+09, device='cuda:0')
c= tensor(1.4299e+09, device='cuda:0')
c= tensor(1.4300e+09, device='cuda:0')
c= tensor(1.4303e+09, device='cuda:0')
c= tensor(1.4316e+09, device='cuda:0')
c= tensor(1.4316e+09, device='cuda:0')
c= tensor(1.4325e+09, device='cuda:0')
c= tensor(1.4325e+09, device='cuda:0')
c= tensor(1.4327e+09, device='cuda:0')
c= tensor(1.4327e+09, device='cuda:0')
c= tensor(1.4331e+09, device='cuda:0')
c= tensor(1.4340e+09, device='cuda:0')
c= tensor(1.4396e+09, device='cuda:0')
c= tensor(1.4397e+09, device='cuda:0')
c= tensor(1.4397e+09, device='cuda:0')
c= tensor(1.4397e+09, device='cuda:0')
c= tensor(1.4399e+09, device='cuda:0')
c= tensor(1.4400e+09, device='cuda:0')
c= tensor(1.4400e+09, device='cuda:0')
c= tensor(1.4400e+09, device='cuda:0')
c= tensor(1.4403e+09, device='cuda:0')
c= tensor(1.4403e+09, device='cuda:0')
c= tensor(1.4404e+09, device='cuda:0')
c= tensor(1.4404e+09, device='cuda:0')
c= tensor(1.4404e+09, device='cuda:0')
c= tensor(1.4405e+09, device='cuda:0')
c= tensor(1.4405e+09, device='cuda:0')
c= tensor(1.4405e+09, device='cuda:0')
c= tensor(1.4417e+09, device='cuda:0')
c= tensor(1.4496e+09, device='cuda:0')
c= tensor(1.4568e+09, device='cuda:0')
c= tensor(1.4583e+09, device='cuda:0')
c= tensor(1.4585e+09, device='cuda:0')
c= tensor(1.4585e+09, device='cuda:0')
c= tensor(1.4585e+09, device='cuda:0')
c= tensor(1.4596e+09, device='cuda:0')
c= tensor(1.4596e+09, device='cuda:0')
c= tensor(1.4598e+09, device='cuda:0')
c= tensor(1.4599e+09, device='cuda:0')
c= tensor(1.4741e+09, device='cuda:0')
c= tensor(1.4743e+09, device='cuda:0')
c= tensor(1.4744e+09, device='cuda:0')
c= tensor(1.4802e+09, device='cuda:0')
c= tensor(1.4804e+09, device='cuda:0')
c= tensor(1.4806e+09, device='cuda:0')
c= tensor(1.4844e+09, device='cuda:0')
c= tensor(1.4856e+09, device='cuda:0')
c= tensor(1.4860e+09, device='cuda:0')
c= tensor(1.4862e+09, device='cuda:0')
c= tensor(1.4864e+09, device='cuda:0')
c= tensor(1.4864e+09, device='cuda:0')
c= tensor(1.4870e+09, device='cuda:0')
c= tensor(1.5098e+09, device='cuda:0')
c= tensor(1.5282e+09, device='cuda:0')
c= tensor(1.5303e+09, device='cuda:0')
c= tensor(1.5303e+09, device='cuda:0')
c= tensor(1.5307e+09, device='cuda:0')
c= tensor(1.5307e+09, device='cuda:0')
c= tensor(1.5309e+09, device='cuda:0')
c= tensor(1.5314e+09, device='cuda:0')
c= tensor(1.5368e+09, device='cuda:0')
c= tensor(1.5368e+09, device='cuda:0')
c= tensor(1.5495e+09, device='cuda:0')
c= tensor(1.5505e+09, device='cuda:0')
c= tensor(1.5507e+09, device='cuda:0')
c= tensor(1.5507e+09, device='cuda:0')
c= tensor(1.5513e+09, device='cuda:0')
c= tensor(1.5527e+09, device='cuda:0')
c= tensor(1.5527e+09, device='cuda:0')
c= tensor(1.5889e+09, device='cuda:0')
c= tensor(1.5895e+09, device='cuda:0')
c= tensor(1.5908e+09, device='cuda:0')
c= tensor(1.5911e+09, device='cuda:0')
c= tensor(1.5911e+09, device='cuda:0')
c= tensor(1.5911e+09, device='cuda:0')
c= tensor(1.5911e+09, device='cuda:0')
c= tensor(1.5912e+09, device='cuda:0')
c= tensor(1.5948e+09, device='cuda:0')
c= tensor(2.3748e+09, device='cuda:0')
c= tensor(2.3748e+09, device='cuda:0')
c= tensor(2.3757e+09, device='cuda:0')
c= tensor(2.3758e+09, device='cuda:0')
c= tensor(2.3758e+09, device='cuda:0')
c= tensor(2.3759e+09, device='cuda:0')
c= tensor(2.3891e+09, device='cuda:0')
c= tensor(2.3892e+09, device='cuda:0')
c= tensor(2.4675e+09, device='cuda:0')
c= tensor(2.4675e+09, device='cuda:0')
c= tensor(2.4702e+09, device='cuda:0')
c= tensor(2.4707e+09, device='cuda:0')
c= tensor(2.4722e+09, device='cuda:0')
c= tensor(2.4782e+09, device='cuda:0')
c= tensor(2.4782e+09, device='cuda:0')
c= tensor(2.4783e+09, device='cuda:0')
c= tensor(2.4788e+09, device='cuda:0')
c= tensor(2.4788e+09, device='cuda:0')
c= tensor(2.4789e+09, device='cuda:0')
c= tensor(2.4799e+09, device='cuda:0')
c= tensor(2.4837e+09, device='cuda:0')
c= tensor(2.4839e+09, device='cuda:0')
c= tensor(2.4842e+09, device='cuda:0')
c= tensor(2.4918e+09, device='cuda:0')
c= tensor(2.5187e+09, device='cuda:0')
c= tensor(2.5188e+09, device='cuda:0')
c= tensor(2.5188e+09, device='cuda:0')
c= tensor(2.5343e+09, device='cuda:0')
c= tensor(2.5344e+09, device='cuda:0')
c= tensor(2.5346e+09, device='cuda:0')
c= tensor(2.5349e+09, device='cuda:0')
c= tensor(2.5359e+09, device='cuda:0')
c= tensor(2.5370e+09, device='cuda:0')
c= tensor(2.5499e+09, device='cuda:0')
c= tensor(2.5500e+09, device='cuda:0')
c= tensor(2.5501e+09, device='cuda:0')
c= tensor(2.5501e+09, device='cuda:0')
c= tensor(2.5502e+09, device='cuda:0')
c= tensor(2.5526e+09, device='cuda:0')
c= tensor(2.5539e+09, device='cuda:0')
c= tensor(2.5566e+09, device='cuda:0')
c= tensor(2.5566e+09, device='cuda:0')
c= tensor(2.5569e+09, device='cuda:0')
c= tensor(2.5575e+09, device='cuda:0')
c= tensor(2.5591e+09, device='cuda:0')
c= tensor(2.5593e+09, device='cuda:0')
c= tensor(2.5598e+09, device='cuda:0')
c= tensor(2.7367e+09, device='cuda:0')
c= tensor(2.7369e+09, device='cuda:0')
c= tensor(2.7369e+09, device='cuda:0')
c= tensor(2.7369e+09, device='cuda:0')
c= tensor(2.7381e+09, device='cuda:0')
c= tensor(2.7385e+09, device='cuda:0')
c= tensor(2.7388e+09, device='cuda:0')
c= tensor(2.7388e+09, device='cuda:0')
c= tensor(2.7388e+09, device='cuda:0')
c= tensor(2.7440e+09, device='cuda:0')
c= tensor(2.7446e+09, device='cuda:0')
c= tensor(2.7448e+09, device='cuda:0')
c= tensor(2.7448e+09, device='cuda:0')
c= tensor(2.7448e+09, device='cuda:0')
c= tensor(2.7450e+09, device='cuda:0')
c= tensor(2.7453e+09, device='cuda:0')
c= tensor(2.7467e+09, device='cuda:0')
c= tensor(2.7467e+09, device='cuda:0')
c= tensor(2.7532e+09, device='cuda:0')
c= tensor(2.7532e+09, device='cuda:0')
c= tensor(2.7534e+09, device='cuda:0')
c= tensor(2.7684e+09, device='cuda:0')
c= tensor(2.7684e+09, device='cuda:0')
c= tensor(2.7687e+09, device='cuda:0')
c= tensor(2.7705e+09, device='cuda:0')
c= tensor(2.7706e+09, device='cuda:0')
c= tensor(2.7709e+09, device='cuda:0')
c= tensor(2.7725e+09, device='cuda:0')
c= tensor(2.7725e+09, device='cuda:0')
c= tensor(2.7726e+09, device='cuda:0')
c= tensor(2.7727e+09, device='cuda:0')
c= tensor(2.7739e+09, device='cuda:0')
c= tensor(2.7756e+09, device='cuda:0')
c= tensor(2.7760e+09, device='cuda:0')
c= tensor(2.7768e+09, device='cuda:0')
c= tensor(2.7768e+09, device='cuda:0')
c= tensor(2.7771e+09, device='cuda:0')
c= tensor(2.7772e+09, device='cuda:0')
c= tensor(2.7772e+09, device='cuda:0')
c= tensor(2.7790e+09, device='cuda:0')
c= tensor(2.7869e+09, device='cuda:0')
c= tensor(2.7869e+09, device='cuda:0')
c= tensor(2.7870e+09, device='cuda:0')
c= tensor(2.7870e+09, device='cuda:0')
c= tensor(2.7957e+09, device='cuda:0')
c= tensor(2.7958e+09, device='cuda:0')
c= tensor(2.7959e+09, device='cuda:0')
c= tensor(2.7970e+09, device='cuda:0')
c= tensor(2.7979e+09, device='cuda:0')
c= tensor(2.7979e+09, device='cuda:0')
c= tensor(2.7982e+09, device='cuda:0')
c= tensor(2.7982e+09, device='cuda:0')
memory (bytes)
3898433536
time for making loss 2 is 15.253009557723999
p0 True
it  0 : 1122109952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 21% |
shape of L is 
torch.Size([])
memory (bytes)
3898703872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  5% |
memory (bytes)
3899265024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% |  6% |
error is  35722564000.0
relative error loss 12.76606
shape of L is 
torch.Size([])
memory (bytes)
4127879168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  6% |
memory (bytes)
4128112640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  35722340000.0
relative error loss 12.765979
shape of L is 
torch.Size([])
memory (bytes)
4129849344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  7% |
memory (bytes)
4130086912
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  35721322000.0
relative error loss 12.765615
shape of L is 
torch.Size([])
memory (bytes)
4132216832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  7% |
memory (bytes)
4132216832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  35715727000.0
relative error loss 12.763617
shape of L is 
torch.Size([])
memory (bytes)
4134215680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4134215680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  35684900000.0
relative error loss 12.7526
shape of L is 
torch.Size([])
memory (bytes)
4136394752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4136443904
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  35347520000.0
relative error loss 12.632031
shape of L is 
torch.Size([])
memory (bytes)
4138635264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% |  7% |
memory (bytes)
4138635264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  32122173000.0
relative error loss 11.4794
shape of L is 
torch.Size([])
memory (bytes)
4140531712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4140781568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  19565193000.0
relative error loss 6.991951
shape of L is 
torch.Size([])
memory (bytes)
4142899200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% |  7% |
memory (bytes)
4142899200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  7161880600.0
relative error loss 2.5594187
shape of L is 
torch.Size([])
memory (bytes)
4144975872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% |  7% |
memory (bytes)
4144975872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  4253841000.0
relative error loss 1.5201817
time to take a step is 217.9142143726349
it  1 : 1433329152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4147204096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4147204096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  4253841000.0
relative error loss 1.5201817
shape of L is 
torch.Size([])
memory (bytes)
4149284864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4149284864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  3188732200.0
relative error loss 1.1395471
shape of L is 
torch.Size([])
memory (bytes)
4151455744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4151455744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  2901330400.0
relative error loss 1.0368392
shape of L is 
torch.Size([])
memory (bytes)
4153573376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4153573376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  2750068000.0
relative error loss 0.9827831
shape of L is 
torch.Size([])
memory (bytes)
4155695104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4155777024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  2600282000.0
relative error loss 0.9292546
shape of L is 
torch.Size([])
memory (bytes)
4157882368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4157882368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  2316351000.0
relative error loss 0.82778704
shape of L is 
torch.Size([])
memory (bytes)
4160008192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4160008192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  2196722200.0
relative error loss 0.78503567
shape of L is 
torch.Size([])
memory (bytes)
4162138112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4162138112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  2417836000.0
relative error loss 0.8640544
shape of L is 
torch.Size([])
memory (bytes)
4164263936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4164263936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  2085811700.0
relative error loss 0.74539995
shape of L is 
torch.Size([])
memory (bytes)
4166189056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% |  7% |
memory (bytes)
4166369280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  2006124000.0
relative error loss 0.7169222
time to take a step is 231.51406693458557
it  2 : 1522249216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4168470528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4168470528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  2006124000.0
relative error loss 0.7169222
shape of L is 
torch.Size([])
memory (bytes)
4170461184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4170461184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  1859082400.0
relative error loss 0.66437435
shape of L is 
torch.Size([])
memory (bytes)
4172611584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4172611584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  1735016300.0
relative error loss 0.6200373
shape of L is 
torch.Size([])
memory (bytes)
4174831616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4174831616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  1601867900.0
relative error loss 0.57245445
shape of L is 
torch.Size([])
memory (bytes)
4176883712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4176883712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  1453969400.0
relative error loss 0.51960045
shape of L is 
torch.Size([])
memory (bytes)
4178886656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
4179070976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  1308121500.0
relative error loss 0.46747923
shape of L is 
torch.Size([])
memory (bytes)
4181192704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% |  7% |
memory (bytes)
4181192704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  1181891000.0
relative error loss 0.42236862
shape of L is 
torch.Size([])
memory (bytes)
4183199744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4183199744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  1089121900.0
relative error loss 0.38921607
shape of L is 
torch.Size([])
memory (bytes)
4185395200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4185395200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  986802800.0
relative error loss 0.3526506
shape of L is 
torch.Size([])
memory (bytes)
4187586560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4187586560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  903468900.0
relative error loss 0.32286984
time to take a step is 225.91878533363342
it  3 : 1522249216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4189675520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% |  7% |
memory (bytes)
4189675520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  903468900.0
relative error loss 0.32286984
shape of L is 
torch.Size([])
memory (bytes)
4191793152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% |  7% |
memory (bytes)
4191793152
| ID | GPU | MEM |
------------------
|  0 |  5% |  0% |
|  1 | 99% |  7% |
error is  828872060.0
relative error loss 0.2962114
shape of L is 
torch.Size([])
memory (bytes)
4193910784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4193910784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  765855360.0
relative error loss 0.2736913
shape of L is 
torch.Size([])
memory (bytes)
4196167680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4196167680
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  699659140.0
relative error loss 0.25003496
shape of L is 
torch.Size([])
memory (bytes)
4198227968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% |  7% |
memory (bytes)
4198227968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  644973060.0
relative error loss 0.23049198
shape of L is 
torch.Size([])
memory (bytes)
4200292352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  7% |
memory (bytes)
4200292352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  594847740.0
relative error loss 0.21257885
shape of L is 
torch.Size([])
memory (bytes)
4202541056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4202598400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  550196740.0
relative error loss 0.19662206
shape of L is 
torch.Size([])
memory (bytes)
4204740608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
4204740608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% |  7% |
error is  518988300.0
relative error loss 0.1854692
shape of L is 
torch.Size([])
memory (bytes)
4206661632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
4206886912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  490773760.0
relative error loss 0.17538626
shape of L is 
torch.Size([])
memory (bytes)
4208955392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4208955392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  441780480.0
relative error loss 0.1578777
time to take a step is 227.62861680984497
c= tensor(578.5757, device='cuda:0')
c= tensor(38449.7461, device='cuda:0')
c= tensor(41649.9375, device='cuda:0')
c= tensor(43512.4102, device='cuda:0')
c= tensor(348930.4688, device='cuda:0')
c= tensor(669078.2500, device='cuda:0')
c= tensor(1153168.7500, device='cuda:0')
c= tensor(1325513.6250, device='cuda:0')
c= tensor(1333792.7500, device='cuda:0')
c= tensor(5040139., device='cuda:0')
c= tensor(5054844.5000, device='cuda:0')
c= tensor(6229451., device='cuda:0')
c= tensor(6236596., device='cuda:0')
c= tensor(13957196., device='cuda:0')
c= tensor(14038671., device='cuda:0')
c= tensor(14600180., device='cuda:0')
c= tensor(15270721., device='cuda:0')
c= tensor(15538672., device='cuda:0')
c= tensor(22688092., device='cuda:0')
c= tensor(24893732., device='cuda:0')
c= tensor(24941574., device='cuda:0')
c= tensor(43815480., device='cuda:0')
c= tensor(43852652., device='cuda:0')
c= tensor(43877396., device='cuda:0')
c= tensor(44187236., device='cuda:0')
c= tensor(44702008., device='cuda:0')
c= tensor(45010088., device='cuda:0')
c= tensor(45117012., device='cuda:0')
c= tensor(46023636., device='cuda:0')
c= tensor(2.9411e+08, device='cuda:0')
c= tensor(2.9412e+08, device='cuda:0')
c= tensor(3.3752e+08, device='cuda:0')
c= tensor(3.3754e+08, device='cuda:0')
c= tensor(3.3755e+08, device='cuda:0')
c= tensor(3.3756e+08, device='cuda:0')
c= tensor(3.3969e+08, device='cuda:0')
c= tensor(3.4030e+08, device='cuda:0')
c= tensor(3.4030e+08, device='cuda:0')
c= tensor(3.4031e+08, device='cuda:0')
c= tensor(3.4031e+08, device='cuda:0')
c= tensor(3.4032e+08, device='cuda:0')
c= tensor(3.4032e+08, device='cuda:0')
c= tensor(3.4032e+08, device='cuda:0')
c= tensor(3.4032e+08, device='cuda:0')
c= tensor(3.4032e+08, device='cuda:0')
c= tensor(3.4032e+08, device='cuda:0')
c= tensor(3.4033e+08, device='cuda:0')
c= tensor(3.4034e+08, device='cuda:0')
c= tensor(3.4034e+08, device='cuda:0')
c= tensor(3.4035e+08, device='cuda:0')
c= tensor(3.4038e+08, device='cuda:0')
c= tensor(3.4038e+08, device='cuda:0')
c= tensor(3.4039e+08, device='cuda:0')
c= tensor(3.4039e+08, device='cuda:0')
c= tensor(3.4040e+08, device='cuda:0')
c= tensor(3.4040e+08, device='cuda:0')
c= tensor(3.4040e+08, device='cuda:0')
c= tensor(3.4041e+08, device='cuda:0')
c= tensor(3.4041e+08, device='cuda:0')
c= tensor(3.4041e+08, device='cuda:0')
c= tensor(3.4042e+08, device='cuda:0')
c= tensor(3.4042e+08, device='cuda:0')
c= tensor(3.4043e+08, device='cuda:0')
c= tensor(3.4045e+08, device='cuda:0')
c= tensor(3.4045e+08, device='cuda:0')
c= tensor(3.4045e+08, device='cuda:0')
c= tensor(3.4046e+08, device='cuda:0')
c= tensor(3.4046e+08, device='cuda:0')
c= tensor(3.4046e+08, device='cuda:0')
c= tensor(3.4047e+08, device='cuda:0')
c= tensor(3.4047e+08, device='cuda:0')
c= tensor(3.4047e+08, device='cuda:0')
c= tensor(3.4048e+08, device='cuda:0')
c= tensor(3.4048e+08, device='cuda:0')
c= tensor(3.4048e+08, device='cuda:0')
c= tensor(3.4048e+08, device='cuda:0')
c= tensor(3.4049e+08, device='cuda:0')
c= tensor(3.4049e+08, device='cuda:0')
c= tensor(3.4049e+08, device='cuda:0')
c= tensor(3.4050e+08, device='cuda:0')
c= tensor(3.4051e+08, device='cuda:0')
c= tensor(3.4051e+08, device='cuda:0')
c= tensor(3.4052e+08, device='cuda:0')
c= tensor(3.4052e+08, device='cuda:0')
c= tensor(3.4052e+08, device='cuda:0')
c= tensor(3.4052e+08, device='cuda:0')
c= tensor(3.4052e+08, device='cuda:0')
c= tensor(3.4052e+08, device='cuda:0')
c= tensor(3.4053e+08, device='cuda:0')
c= tensor(3.4054e+08, device='cuda:0')
c= tensor(3.4054e+08, device='cuda:0')
c= tensor(3.4054e+08, device='cuda:0')
c= tensor(3.4054e+08, device='cuda:0')
c= tensor(3.4055e+08, device='cuda:0')
c= tensor(3.4055e+08, device='cuda:0')
c= tensor(3.4055e+08, device='cuda:0')
c= tensor(3.4056e+08, device='cuda:0')
c= tensor(3.4057e+08, device='cuda:0')
c= tensor(3.4057e+08, device='cuda:0')
c= tensor(3.4058e+08, device='cuda:0')
c= tensor(3.4059e+08, device='cuda:0')
c= tensor(3.4060e+08, device='cuda:0')
c= tensor(3.4060e+08, device='cuda:0')
c= tensor(3.4060e+08, device='cuda:0')
c= tensor(3.4061e+08, device='cuda:0')
c= tensor(3.4061e+08, device='cuda:0')
c= tensor(3.4062e+08, device='cuda:0')
c= tensor(3.4062e+08, device='cuda:0')
c= tensor(3.4062e+08, device='cuda:0')
c= tensor(3.4063e+08, device='cuda:0')
c= tensor(3.4063e+08, device='cuda:0')
c= tensor(3.4063e+08, device='cuda:0')
c= tensor(3.4063e+08, device='cuda:0')
c= tensor(3.4063e+08, device='cuda:0')
c= tensor(3.4064e+08, device='cuda:0')
c= tensor(3.4064e+08, device='cuda:0')
c= tensor(3.4064e+08, device='cuda:0')
c= tensor(3.4064e+08, device='cuda:0')
c= tensor(3.4065e+08, device='cuda:0')
c= tensor(3.4065e+08, device='cuda:0')
c= tensor(3.4068e+08, device='cuda:0')
c= tensor(3.4068e+08, device='cuda:0')
c= tensor(3.4068e+08, device='cuda:0')
c= tensor(3.4069e+08, device='cuda:0')
c= tensor(3.4069e+08, device='cuda:0')
c= tensor(3.4069e+08, device='cuda:0')
c= tensor(3.4069e+08, device='cuda:0')
c= tensor(3.4069e+08, device='cuda:0')
c= tensor(3.4072e+08, device='cuda:0')
c= tensor(3.4073e+08, device='cuda:0')
c= tensor(3.4075e+08, device='cuda:0')
c= tensor(3.4075e+08, device='cuda:0')
c= tensor(3.4076e+08, device='cuda:0')
c= tensor(3.4076e+08, device='cuda:0')
c= tensor(3.4076e+08, device='cuda:0')
c= tensor(3.4076e+08, device='cuda:0')
c= tensor(3.4076e+08, device='cuda:0')
c= tensor(3.4076e+08, device='cuda:0')
c= tensor(3.4077e+08, device='cuda:0')
c= tensor(3.4077e+08, device='cuda:0')
c= tensor(3.4077e+08, device='cuda:0')
c= tensor(3.4078e+08, device='cuda:0')
c= tensor(3.4079e+08, device='cuda:0')
c= tensor(3.4081e+08, device='cuda:0')
c= tensor(3.4081e+08, device='cuda:0')
c= tensor(3.4081e+08, device='cuda:0')
c= tensor(3.4082e+08, device='cuda:0')
c= tensor(3.4082e+08, device='cuda:0')
c= tensor(3.4082e+08, device='cuda:0')
c= tensor(3.4082e+08, device='cuda:0')
c= tensor(3.4083e+08, device='cuda:0')
c= tensor(3.4084e+08, device='cuda:0')
c= tensor(3.4084e+08, device='cuda:0')
c= tensor(3.4087e+08, device='cuda:0')
c= tensor(3.4087e+08, device='cuda:0')
c= tensor(3.4089e+08, device='cuda:0')
c= tensor(3.4090e+08, device='cuda:0')
c= tensor(3.4090e+08, device='cuda:0')
c= tensor(3.4090e+08, device='cuda:0')
c= tensor(3.4090e+08, device='cuda:0')
c= tensor(3.4092e+08, device='cuda:0')
c= tensor(3.4092e+08, device='cuda:0')
c= tensor(3.4092e+08, device='cuda:0')
c= tensor(3.4092e+08, device='cuda:0')
c= tensor(3.4092e+08, device='cuda:0')
c= tensor(3.4093e+08, device='cuda:0')
c= tensor(3.4093e+08, device='cuda:0')
c= tensor(3.4093e+08, device='cuda:0')
c= tensor(3.4093e+08, device='cuda:0')
c= tensor(3.4093e+08, device='cuda:0')
c= tensor(3.4093e+08, device='cuda:0')
c= tensor(3.4094e+08, device='cuda:0')
c= tensor(3.4094e+08, device='cuda:0')
c= tensor(3.4094e+08, device='cuda:0')
c= tensor(3.4095e+08, device='cuda:0')
c= tensor(3.4095e+08, device='cuda:0')
c= tensor(3.4096e+08, device='cuda:0')
c= tensor(3.4096e+08, device='cuda:0')
c= tensor(3.4097e+08, device='cuda:0')
c= tensor(3.4097e+08, device='cuda:0')
c= tensor(3.4097e+08, device='cuda:0')
c= tensor(3.4098e+08, device='cuda:0')
c= tensor(3.4098e+08, device='cuda:0')
c= tensor(3.4098e+08, device='cuda:0')
c= tensor(3.4099e+08, device='cuda:0')
c= tensor(3.4100e+08, device='cuda:0')
c= tensor(3.4100e+08, device='cuda:0')
c= tensor(3.4101e+08, device='cuda:0')
c= tensor(3.4101e+08, device='cuda:0')
c= tensor(3.4109e+08, device='cuda:0')
c= tensor(3.4109e+08, device='cuda:0')
c= tensor(3.4109e+08, device='cuda:0')
c= tensor(3.4109e+08, device='cuda:0')
c= tensor(3.4109e+08, device='cuda:0')
c= tensor(3.4110e+08, device='cuda:0')
c= tensor(3.4110e+08, device='cuda:0')
c= tensor(3.4110e+08, device='cuda:0')
c= tensor(3.4110e+08, device='cuda:0')
c= tensor(3.4110e+08, device='cuda:0')
c= tensor(3.4111e+08, device='cuda:0')
c= tensor(3.4111e+08, device='cuda:0')
c= tensor(3.4111e+08, device='cuda:0')
c= tensor(3.4112e+08, device='cuda:0')
c= tensor(3.4113e+08, device='cuda:0')
c= tensor(3.4113e+08, device='cuda:0')
c= tensor(3.4113e+08, device='cuda:0')
c= tensor(3.4114e+08, device='cuda:0')
c= tensor(3.4114e+08, device='cuda:0')
c= tensor(3.4115e+08, device='cuda:0')
c= tensor(3.4116e+08, device='cuda:0')
c= tensor(3.4117e+08, device='cuda:0')
c= tensor(3.4118e+08, device='cuda:0')
c= tensor(3.4118e+08, device='cuda:0')
c= tensor(3.4118e+08, device='cuda:0')
c= tensor(3.4118e+08, device='cuda:0')
c= tensor(3.4118e+08, device='cuda:0')
c= tensor(3.4118e+08, device='cuda:0')
c= tensor(3.4118e+08, device='cuda:0')
c= tensor(3.4119e+08, device='cuda:0')
c= tensor(3.4119e+08, device='cuda:0')
c= tensor(3.4119e+08, device='cuda:0')
c= tensor(3.4120e+08, device='cuda:0')
c= tensor(3.4120e+08, device='cuda:0')
c= tensor(3.4120e+08, device='cuda:0')
c= tensor(3.4121e+08, device='cuda:0')
c= tensor(3.4121e+08, device='cuda:0')
c= tensor(3.4121e+08, device='cuda:0')
c= tensor(3.4121e+08, device='cuda:0')
c= tensor(3.4122e+08, device='cuda:0')
c= tensor(3.4122e+08, device='cuda:0')
c= tensor(3.4122e+08, device='cuda:0')
c= tensor(3.4122e+08, device='cuda:0')
c= tensor(3.4123e+08, device='cuda:0')
c= tensor(3.4123e+08, device='cuda:0')
c= tensor(3.4123e+08, device='cuda:0')
c= tensor(3.4123e+08, device='cuda:0')
c= tensor(3.4124e+08, device='cuda:0')
c= tensor(3.4124e+08, device='cuda:0')
c= tensor(3.4124e+08, device='cuda:0')
c= tensor(3.4126e+08, device='cuda:0')
c= tensor(3.4227e+08, device='cuda:0')
c= tensor(3.4236e+08, device='cuda:0')
c= tensor(3.4237e+08, device='cuda:0')
c= tensor(3.4237e+08, device='cuda:0')
c= tensor(3.4243e+08, device='cuda:0')
c= tensor(3.4300e+08, device='cuda:0')
c= tensor(3.5814e+08, device='cuda:0')
c= tensor(3.5814e+08, device='cuda:0')
c= tensor(3.5919e+08, device='cuda:0')
c= tensor(3.5932e+08, device='cuda:0')
c= tensor(3.5969e+08, device='cuda:0')
c= tensor(3.7234e+08, device='cuda:0')
c= tensor(3.7234e+08, device='cuda:0')
c= tensor(3.7235e+08, device='cuda:0')
c= tensor(3.7622e+08, device='cuda:0')
c= tensor(3.9706e+08, device='cuda:0')
c= tensor(3.9707e+08, device='cuda:0')
c= tensor(3.9722e+08, device='cuda:0')
c= tensor(3.9837e+08, device='cuda:0')
c= tensor(4.0097e+08, device='cuda:0')
c= tensor(4.0224e+08, device='cuda:0')
c= tensor(4.0241e+08, device='cuda:0')
c= tensor(4.0276e+08, device='cuda:0')
c= tensor(4.0281e+08, device='cuda:0')
c= tensor(4.0282e+08, device='cuda:0')
c= tensor(4.4215e+08, device='cuda:0')
c= tensor(4.4217e+08, device='cuda:0')
c= tensor(4.4218e+08, device='cuda:0')
c= tensor(4.4237e+08, device='cuda:0')
c= tensor(4.4252e+08, device='cuda:0')
c= tensor(4.5974e+08, device='cuda:0')
c= tensor(4.6051e+08, device='cuda:0')
c= tensor(4.6051e+08, device='cuda:0')
c= tensor(4.6066e+08, device='cuda:0')
c= tensor(4.6068e+08, device='cuda:0')
c= tensor(4.6079e+08, device='cuda:0')
c= tensor(4.6151e+08, device='cuda:0')
c= tensor(4.6151e+08, device='cuda:0')
c= tensor(4.6177e+08, device='cuda:0')
c= tensor(4.6177e+08, device='cuda:0')
c= tensor(4.6177e+08, device='cuda:0')
c= tensor(4.6283e+08, device='cuda:0')
c= tensor(4.6321e+08, device='cuda:0')
c= tensor(4.6339e+08, device='cuda:0')
c= tensor(4.6570e+08, device='cuda:0')
c= tensor(4.7763e+08, device='cuda:0')
c= tensor(4.7764e+08, device='cuda:0')
c= tensor(4.7771e+08, device='cuda:0')
c= tensor(4.7849e+08, device='cuda:0')
c= tensor(4.7850e+08, device='cuda:0')
c= tensor(4.8082e+08, device='cuda:0')
c= tensor(4.9077e+08, device='cuda:0')
c= tensor(5.1253e+08, device='cuda:0')
c= tensor(5.1259e+08, device='cuda:0')
c= tensor(5.1267e+08, device='cuda:0')
c= tensor(5.1269e+08, device='cuda:0')
c= tensor(5.1269e+08, device='cuda:0')
c= tensor(5.1298e+08, device='cuda:0')
c= tensor(5.1300e+08, device='cuda:0')
c= tensor(5.1310e+08, device='cuda:0')
c= tensor(5.2011e+08, device='cuda:0')
c= tensor(5.2400e+08, device='cuda:0')
c= tensor(5.2402e+08, device='cuda:0')
c= tensor(5.2404e+08, device='cuda:0')
c= tensor(5.3814e+08, device='cuda:0')
c= tensor(5.3823e+08, device='cuda:0')
c= tensor(5.3824e+08, device='cuda:0')
c= tensor(5.3826e+08, device='cuda:0')
c= tensor(6.7490e+08, device='cuda:0')
c= tensor(6.7492e+08, device='cuda:0')
c= tensor(6.8651e+08, device='cuda:0')
c= tensor(6.8652e+08, device='cuda:0')
c= tensor(6.8814e+08, device='cuda:0')
c= tensor(6.8850e+08, device='cuda:0')
c= tensor(7.3942e+08, device='cuda:0')
c= tensor(7.3996e+08, device='cuda:0')
c= tensor(7.3997e+08, device='cuda:0')
c= tensor(7.4167e+08, device='cuda:0')
c= tensor(7.4251e+08, device='cuda:0')
c= tensor(7.4276e+08, device='cuda:0')
c= tensor(7.4301e+08, device='cuda:0')
c= tensor(7.4534e+08, device='cuda:0')
c= tensor(7.5050e+08, device='cuda:0')
c= tensor(7.5139e+08, device='cuda:0')
c= tensor(7.5139e+08, device='cuda:0')
c= tensor(7.5141e+08, device='cuda:0')
c= tensor(7.5284e+08, device='cuda:0')
c= tensor(7.7360e+08, device='cuda:0')
c= tensor(7.7362e+08, device='cuda:0')
c= tensor(7.7362e+08, device='cuda:0')
c= tensor(7.7377e+08, device='cuda:0')
c= tensor(7.7521e+08, device='cuda:0')
c= tensor(7.8226e+08, device='cuda:0')
c= tensor(7.8227e+08, device='cuda:0')
c= tensor(7.8245e+08, device='cuda:0')
c= tensor(7.8251e+08, device='cuda:0')
c= tensor(7.8252e+08, device='cuda:0')
c= tensor(7.8254e+08, device='cuda:0')
c= tensor(7.8256e+08, device='cuda:0')
c= tensor(7.9049e+08, device='cuda:0')
c= tensor(7.9066e+08, device='cuda:0')
c= tensor(7.9071e+08, device='cuda:0')
c= tensor(7.9088e+08, device='cuda:0')
c= tensor(7.9090e+08, device='cuda:0')
c= tensor(8.1576e+08, device='cuda:0')
c= tensor(8.1577e+08, device='cuda:0')
c= tensor(8.1764e+08, device='cuda:0')
c= tensor(8.1764e+08, device='cuda:0')
c= tensor(8.1764e+08, device='cuda:0')
c= tensor(8.1764e+08, device='cuda:0')
c= tensor(8.1793e+08, device='cuda:0')
c= tensor(8.1793e+08, device='cuda:0')
c= tensor(8.1848e+08, device='cuda:0')
c= tensor(8.1848e+08, device='cuda:0')
c= tensor(8.1849e+08, device='cuda:0')
c= tensor(8.2578e+08, device='cuda:0')
c= tensor(8.2594e+08, device='cuda:0')
c= tensor(8.2607e+08, device='cuda:0')
c= tensor(8.2891e+08, device='cuda:0')
c= tensor(8.3332e+08, device='cuda:0')
c= tensor(8.3332e+08, device='cuda:0')
c= tensor(8.3333e+08, device='cuda:0')
c= tensor(8.3350e+08, device='cuda:0')
c= tensor(8.3350e+08, device='cuda:0')
c= tensor(8.3350e+08, device='cuda:0')
c= tensor(8.3357e+08, device='cuda:0')
c= tensor(8.3358e+08, device='cuda:0')
c= tensor(8.3358e+08, device='cuda:0')
c= tensor(8.3359e+08, device='cuda:0')
c= tensor(8.3359e+08, device='cuda:0')
c= tensor(8.4529e+08, device='cuda:0')
c= tensor(8.4534e+08, device='cuda:0')
c= tensor(8.4607e+08, device='cuda:0')
c= tensor(8.4614e+08, device='cuda:0')
c= tensor(8.4627e+08, device='cuda:0')
c= tensor(8.4642e+08, device='cuda:0')
c= tensor(9.4087e+08, device='cuda:0')
c= tensor(9.7253e+08, device='cuda:0')
c= tensor(9.7317e+08, device='cuda:0')
c= tensor(9.7417e+08, device='cuda:0')
c= tensor(9.7418e+08, device='cuda:0')
c= tensor(9.7779e+08, device='cuda:0')
c= tensor(9.7805e+08, device='cuda:0')
c= tensor(9.8162e+08, device='cuda:0')
c= tensor(9.8169e+08, device='cuda:0')
c= tensor(9.8264e+08, device='cuda:0')
c= tensor(1.0044e+09, device='cuda:0')
c= tensor(1.0048e+09, device='cuda:0')
c= tensor(1.0049e+09, device='cuda:0')
c= tensor(1.0051e+09, device='cuda:0')
c= tensor(1.0051e+09, device='cuda:0')
c= tensor(1.0051e+09, device='cuda:0')
c= tensor(1.0078e+09, device='cuda:0')
c= tensor(1.0080e+09, device='cuda:0')
c= tensor(1.0080e+09, device='cuda:0')
c= tensor(1.0085e+09, device='cuda:0')
c= tensor(1.0085e+09, device='cuda:0')
c= tensor(1.0085e+09, device='cuda:0')
c= tensor(1.0113e+09, device='cuda:0')
c= tensor(1.0120e+09, device='cuda:0')
c= tensor(1.0122e+09, device='cuda:0')
c= tensor(1.0135e+09, device='cuda:0')
c= tensor(1.0149e+09, device='cuda:0')
c= tensor(1.0150e+09, device='cuda:0')
c= tensor(1.0153e+09, device='cuda:0')
c= tensor(1.0162e+09, device='cuda:0')
c= tensor(1.0175e+09, device='cuda:0')
c= tensor(1.0175e+09, device='cuda:0')
c= tensor(1.0301e+09, device='cuda:0')
c= tensor(1.0432e+09, device='cuda:0')
c= tensor(1.0441e+09, device='cuda:0')
c= tensor(1.0450e+09, device='cuda:0')
c= tensor(1.0462e+09, device='cuda:0')
c= tensor(1.0462e+09, device='cuda:0')
c= tensor(1.0462e+09, device='cuda:0')
c= tensor(1.0463e+09, device='cuda:0')
c= tensor(1.0480e+09, device='cuda:0')
c= tensor(1.0500e+09, device='cuda:0')
c= tensor(1.0645e+09, device='cuda:0')
c= tensor(1.0723e+09, device='cuda:0')
c= tensor(1.0728e+09, device='cuda:0')
c= tensor(1.0730e+09, device='cuda:0')
c= tensor(1.0734e+09, device='cuda:0')
c= tensor(1.0734e+09, device='cuda:0')
c= tensor(1.0734e+09, device='cuda:0')
c= tensor(1.0782e+09, device='cuda:0')
c= tensor(1.0783e+09, device='cuda:0')
c= tensor(1.0783e+09, device='cuda:0')
c= tensor(1.0783e+09, device='cuda:0')
c= tensor(1.0789e+09, device='cuda:0')
c= tensor(1.0791e+09, device='cuda:0')
c= tensor(1.0797e+09, device='cuda:0')
c= tensor(1.0803e+09, device='cuda:0')
c= tensor(1.0805e+09, device='cuda:0')
c= tensor(1.0805e+09, device='cuda:0')
c= tensor(1.0805e+09, device='cuda:0')
c= tensor(1.0808e+09, device='cuda:0')
c= tensor(1.0814e+09, device='cuda:0')
c= tensor(1.0815e+09, device='cuda:0')
c= tensor(1.0831e+09, device='cuda:0')
c= tensor(1.0831e+09, device='cuda:0')
c= tensor(1.0832e+09, device='cuda:0')
c= tensor(1.0832e+09, device='cuda:0')
c= tensor(1.0840e+09, device='cuda:0')
c= tensor(1.0840e+09, device='cuda:0')
c= tensor(1.0843e+09, device='cuda:0')
c= tensor(1.0843e+09, device='cuda:0')
c= tensor(1.0844e+09, device='cuda:0')
c= tensor(1.0847e+09, device='cuda:0')
c= tensor(1.0990e+09, device='cuda:0')
c= tensor(1.0990e+09, device='cuda:0')
c= tensor(1.0990e+09, device='cuda:0')
c= tensor(1.1019e+09, device='cuda:0')
c= tensor(1.1019e+09, device='cuda:0')
c= tensor(1.1260e+09, device='cuda:0')
c= tensor(1.1260e+09, device='cuda:0')
c= tensor(1.1269e+09, device='cuda:0')
c= tensor(1.1330e+09, device='cuda:0')
c= tensor(1.1330e+09, device='cuda:0')
c= tensor(1.1406e+09, device='cuda:0')
c= tensor(1.1410e+09, device='cuda:0')
c= tensor(1.1686e+09, device='cuda:0')
c= tensor(1.1687e+09, device='cuda:0')
c= tensor(1.1689e+09, device='cuda:0')
c= tensor(1.1689e+09, device='cuda:0')
c= tensor(1.1689e+09, device='cuda:0')
c= tensor(1.1689e+09, device='cuda:0')
c= tensor(1.1709e+09, device='cuda:0')
c= tensor(1.1710e+09, device='cuda:0')
c= tensor(1.1720e+09, device='cuda:0')
c= tensor(1.1720e+09, device='cuda:0')
c= tensor(1.1720e+09, device='cuda:0')
c= tensor(1.1720e+09, device='cuda:0')
c= tensor(1.1753e+09, device='cuda:0')
c= tensor(1.1757e+09, device='cuda:0')
c= tensor(1.1875e+09, device='cuda:0')
c= tensor(1.1880e+09, device='cuda:0')
c= tensor(1.1880e+09, device='cuda:0')
c= tensor(1.1882e+09, device='cuda:0')
c= tensor(1.1883e+09, device='cuda:0')
c= tensor(1.3572e+09, device='cuda:0')
c= tensor(1.3572e+09, device='cuda:0')
c= tensor(1.3573e+09, device='cuda:0')
c= tensor(1.3590e+09, device='cuda:0')
c= tensor(1.3597e+09, device='cuda:0')
c= tensor(1.3597e+09, device='cuda:0')
c= tensor(1.3597e+09, device='cuda:0')
c= tensor(1.3715e+09, device='cuda:0')
c= tensor(1.3717e+09, device='cuda:0')
c= tensor(1.3734e+09, device='cuda:0')
c= tensor(1.3740e+09, device='cuda:0')
c= tensor(1.3750e+09, device='cuda:0')
c= tensor(1.3774e+09, device='cuda:0')
c= tensor(1.3851e+09, device='cuda:0')
c= tensor(1.3903e+09, device='cuda:0')
c= tensor(1.3903e+09, device='cuda:0')
c= tensor(1.3929e+09, device='cuda:0')
c= tensor(1.3930e+09, device='cuda:0')
c= tensor(1.3931e+09, device='cuda:0')
c= tensor(1.3931e+09, device='cuda:0')
c= tensor(1.3932e+09, device='cuda:0')
c= tensor(1.3999e+09, device='cuda:0')
c= tensor(1.3999e+09, device='cuda:0')
c= tensor(1.3999e+09, device='cuda:0')
c= tensor(1.4000e+09, device='cuda:0')
c= tensor(1.4002e+09, device='cuda:0')
c= tensor(1.4024e+09, device='cuda:0')
c= tensor(1.4024e+09, device='cuda:0')
c= tensor(1.4025e+09, device='cuda:0')
c= tensor(1.4025e+09, device='cuda:0')
c= tensor(1.4030e+09, device='cuda:0')
c= tensor(1.4030e+09, device='cuda:0')
c= tensor(1.4030e+09, device='cuda:0')
c= tensor(1.4030e+09, device='cuda:0')
c= tensor(1.4070e+09, device='cuda:0')
c= tensor(1.4070e+09, device='cuda:0')
c= tensor(1.4070e+09, device='cuda:0')
c= tensor(1.4070e+09, device='cuda:0')
c= tensor(1.4211e+09, device='cuda:0')
c= tensor(1.4259e+09, device='cuda:0')
c= tensor(1.4262e+09, device='cuda:0')
c= tensor(1.4262e+09, device='cuda:0')
c= tensor(1.4286e+09, device='cuda:0')
c= tensor(1.4298e+09, device='cuda:0')
c= tensor(1.4298e+09, device='cuda:0')
c= tensor(1.4299e+09, device='cuda:0')
c= tensor(1.4299e+09, device='cuda:0')
c= tensor(1.4300e+09, device='cuda:0')
c= tensor(1.4303e+09, device='cuda:0')
c= tensor(1.4316e+09, device='cuda:0')
c= tensor(1.4316e+09, device='cuda:0')
c= tensor(1.4325e+09, device='cuda:0')
c= tensor(1.4325e+09, device='cuda:0')
c= tensor(1.4327e+09, device='cuda:0')
c= tensor(1.4327e+09, device='cuda:0')
c= tensor(1.4331e+09, device='cuda:0')
c= tensor(1.4340e+09, device='cuda:0')
c= tensor(1.4396e+09, device='cuda:0')
c= tensor(1.4397e+09, device='cuda:0')
c= tensor(1.4397e+09, device='cuda:0')
c= tensor(1.4397e+09, device='cuda:0')
c= tensor(1.4399e+09, device='cuda:0')
c= tensor(1.4400e+09, device='cuda:0')
c= tensor(1.4400e+09, device='cuda:0')
c= tensor(1.4400e+09, device='cuda:0')
c= tensor(1.4403e+09, device='cuda:0')
c= tensor(1.4403e+09, device='cuda:0')
c= tensor(1.4404e+09, device='cuda:0')
c= tensor(1.4404e+09, device='cuda:0')
c= tensor(1.4404e+09, device='cuda:0')
c= tensor(1.4405e+09, device='cuda:0')
c= tensor(1.4405e+09, device='cuda:0')
c= tensor(1.4405e+09, device='cuda:0')
c= tensor(1.4417e+09, device='cuda:0')
c= tensor(1.4496e+09, device='cuda:0')
c= tensor(1.4568e+09, device='cuda:0')
c= tensor(1.4583e+09, device='cuda:0')
c= tensor(1.4585e+09, device='cuda:0')
c= tensor(1.4585e+09, device='cuda:0')
c= tensor(1.4585e+09, device='cuda:0')
c= tensor(1.4596e+09, device='cuda:0')
c= tensor(1.4596e+09, device='cuda:0')
c= tensor(1.4598e+09, device='cuda:0')
c= tensor(1.4599e+09, device='cuda:0')
c= tensor(1.4741e+09, device='cuda:0')
c= tensor(1.4743e+09, device='cuda:0')
c= tensor(1.4744e+09, device='cuda:0')
c= tensor(1.4802e+09, device='cuda:0')
c= tensor(1.4804e+09, device='cuda:0')
c= tensor(1.4806e+09, device='cuda:0')
c= tensor(1.4844e+09, device='cuda:0')
c= tensor(1.4856e+09, device='cuda:0')
c= tensor(1.4860e+09, device='cuda:0')
c= tensor(1.4862e+09, device='cuda:0')
c= tensor(1.4864e+09, device='cuda:0')
c= tensor(1.4864e+09, device='cuda:0')
c= tensor(1.4870e+09, device='cuda:0')
c= tensor(1.5098e+09, device='cuda:0')
c= tensor(1.5282e+09, device='cuda:0')
c= tensor(1.5303e+09, device='cuda:0')
c= tensor(1.5303e+09, device='cuda:0')
c= tensor(1.5307e+09, device='cuda:0')
c= tensor(1.5307e+09, device='cuda:0')
c= tensor(1.5309e+09, device='cuda:0')
c= tensor(1.5314e+09, device='cuda:0')
c= tensor(1.5368e+09, device='cuda:0')
c= tensor(1.5368e+09, device='cuda:0')
c= tensor(1.5495e+09, device='cuda:0')
c= tensor(1.5505e+09, device='cuda:0')
c= tensor(1.5507e+09, device='cuda:0')
c= tensor(1.5507e+09, device='cuda:0')
c= tensor(1.5513e+09, device='cuda:0')
c= tensor(1.5527e+09, device='cuda:0')
c= tensor(1.5527e+09, device='cuda:0')
c= tensor(1.5889e+09, device='cuda:0')
c= tensor(1.5895e+09, device='cuda:0')
c= tensor(1.5908e+09, device='cuda:0')
c= tensor(1.5911e+09, device='cuda:0')
c= tensor(1.5911e+09, device='cuda:0')
c= tensor(1.5911e+09, device='cuda:0')
c= tensor(1.5911e+09, device='cuda:0')
c= tensor(1.5912e+09, device='cuda:0')
c= tensor(1.5948e+09, device='cuda:0')
c= tensor(2.3748e+09, device='cuda:0')
c= tensor(2.3748e+09, device='cuda:0')
c= tensor(2.3757e+09, device='cuda:0')
c= tensor(2.3758e+09, device='cuda:0')
c= tensor(2.3758e+09, device='cuda:0')
c= tensor(2.3759e+09, device='cuda:0')
c= tensor(2.3891e+09, device='cuda:0')
c= tensor(2.3892e+09, device='cuda:0')
c= tensor(2.4675e+09, device='cuda:0')
c= tensor(2.4675e+09, device='cuda:0')
c= tensor(2.4702e+09, device='cuda:0')
c= tensor(2.4707e+09, device='cuda:0')
c= tensor(2.4722e+09, device='cuda:0')
c= tensor(2.4782e+09, device='cuda:0')
c= tensor(2.4782e+09, device='cuda:0')
c= tensor(2.4783e+09, device='cuda:0')
c= tensor(2.4788e+09, device='cuda:0')
c= tensor(2.4788e+09, device='cuda:0')
c= tensor(2.4789e+09, device='cuda:0')
c= tensor(2.4799e+09, device='cuda:0')
c= tensor(2.4837e+09, device='cuda:0')
c= tensor(2.4839e+09, device='cuda:0')
c= tensor(2.4842e+09, device='cuda:0')
c= tensor(2.4918e+09, device='cuda:0')
c= tensor(2.5187e+09, device='cuda:0')
c= tensor(2.5188e+09, device='cuda:0')
c= tensor(2.5188e+09, device='cuda:0')
c= tensor(2.5343e+09, device='cuda:0')
c= tensor(2.5344e+09, device='cuda:0')
c= tensor(2.5346e+09, device='cuda:0')
c= tensor(2.5349e+09, device='cuda:0')
c= tensor(2.5359e+09, device='cuda:0')
c= tensor(2.5370e+09, device='cuda:0')
c= tensor(2.5499e+09, device='cuda:0')
c= tensor(2.5500e+09, device='cuda:0')
c= tensor(2.5501e+09, device='cuda:0')
c= tensor(2.5501e+09, device='cuda:0')
c= tensor(2.5502e+09, device='cuda:0')
c= tensor(2.5526e+09, device='cuda:0')
c= tensor(2.5539e+09, device='cuda:0')
c= tensor(2.5566e+09, device='cuda:0')
c= tensor(2.5566e+09, device='cuda:0')
c= tensor(2.5569e+09, device='cuda:0')
c= tensor(2.5575e+09, device='cuda:0')
c= tensor(2.5591e+09, device='cuda:0')
c= tensor(2.5593e+09, device='cuda:0')
c= tensor(2.5598e+09, device='cuda:0')
c= tensor(2.7367e+09, device='cuda:0')
c= tensor(2.7369e+09, device='cuda:0')
c= tensor(2.7369e+09, device='cuda:0')
c= tensor(2.7369e+09, device='cuda:0')
c= tensor(2.7381e+09, device='cuda:0')
c= tensor(2.7385e+09, device='cuda:0')
c= tensor(2.7388e+09, device='cuda:0')
c= tensor(2.7388e+09, device='cuda:0')
c= tensor(2.7388e+09, device='cuda:0')
c= tensor(2.7440e+09, device='cuda:0')
c= tensor(2.7446e+09, device='cuda:0')
c= tensor(2.7448e+09, device='cuda:0')
c= tensor(2.7448e+09, device='cuda:0')
c= tensor(2.7448e+09, device='cuda:0')
c= tensor(2.7450e+09, device='cuda:0')
c= tensor(2.7453e+09, device='cuda:0')
c= tensor(2.7467e+09, device='cuda:0')
c= tensor(2.7467e+09, device='cuda:0')
c= tensor(2.7532e+09, device='cuda:0')
c= tensor(2.7532e+09, device='cuda:0')
c= tensor(2.7534e+09, device='cuda:0')
c= tensor(2.7684e+09, device='cuda:0')
c= tensor(2.7684e+09, device='cuda:0')
c= tensor(2.7687e+09, device='cuda:0')
c= tensor(2.7705e+09, device='cuda:0')
c= tensor(2.7706e+09, device='cuda:0')
c= tensor(2.7709e+09, device='cuda:0')
c= tensor(2.7725e+09, device='cuda:0')
c= tensor(2.7725e+09, device='cuda:0')
c= tensor(2.7726e+09, device='cuda:0')
c= tensor(2.7727e+09, device='cuda:0')
c= tensor(2.7739e+09, device='cuda:0')
c= tensor(2.7756e+09, device='cuda:0')
c= tensor(2.7760e+09, device='cuda:0')
c= tensor(2.7768e+09, device='cuda:0')
c= tensor(2.7768e+09, device='cuda:0')
c= tensor(2.7771e+09, device='cuda:0')
c= tensor(2.7772e+09, device='cuda:0')
c= tensor(2.7772e+09, device='cuda:0')
c= tensor(2.7790e+09, device='cuda:0')
c= tensor(2.7869e+09, device='cuda:0')
c= tensor(2.7869e+09, device='cuda:0')
c= tensor(2.7870e+09, device='cuda:0')
c= tensor(2.7870e+09, device='cuda:0')
c= tensor(2.7957e+09, device='cuda:0')
c= tensor(2.7958e+09, device='cuda:0')
c= tensor(2.7959e+09, device='cuda:0')
c= tensor(2.7970e+09, device='cuda:0')
c= tensor(2.7979e+09, device='cuda:0')
c= tensor(2.7979e+09, device='cuda:0')
c= tensor(2.7982e+09, device='cuda:0')
c= tensor(2.7982e+09, device='cuda:0')
time to make c is 11.6722252368927
time for making loss is 11.672246217727661
p0 True
it  0 : 1122300416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  6% |
shape of L is 
torch.Size([])
memory (bytes)
4211200000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  6% |
memory (bytes)
4211433472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  441780480.0
relative error loss 0.1578777
shape of L is 
torch.Size([])
memory (bytes)
4238295040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  6% |
memory (bytes)
4238295040
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  432746500.0
relative error loss 0.15464924
shape of L is 
torch.Size([])
memory (bytes)
4242030592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4242030592
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  412172300.0
relative error loss 0.14729671
shape of L is 
torch.Size([])
memory (bytes)
4245172224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  7% |
memory (bytes)
4245291008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  403930620.0
relative error loss 0.14435141
shape of L is 
torch.Size([])
memory (bytes)
4248363008
| ID | GPU | MEM |
------------------
|  0 | 15% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4248485888
| ID | GPU | MEM |
------------------
|  0 | 15% |  0% |
|  1 | 99% |  7% |
error is  397388030.0
relative error loss 0.1420133
shape of L is 
torch.Size([])
memory (bytes)
4251721728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% |  7% |
memory (bytes)
4251721728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  391369470.0
relative error loss 0.13986246
shape of L is 
torch.Size([])
memory (bytes)
4254932992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4254932992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  388332540.0
relative error loss 0.13877717
shape of L is 
torch.Size([])
memory (bytes)
4258140160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4258140160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  384551420.0
relative error loss 0.13742593
shape of L is 
torch.Size([])
memory (bytes)
4261359616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% |  7% |
memory (bytes)
4261359616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  382680320.0
relative error loss 0.13675725
shape of L is 
torch.Size([])
memory (bytes)
4264456192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4264456192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% |  7% |
error is  380278530.0
relative error loss 0.13589893
time to take a step is 293.08718490600586
it  1 : 1524149248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4267806720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4267806720
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  380278530.0
relative error loss 0.13589893
shape of L is 
torch.Size([])
memory (bytes)
4270891008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4271005696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  379868670.0
relative error loss 0.13575247
shape of L is 
torch.Size([])
memory (bytes)
4274241536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% |  7% |
memory (bytes)
4274241536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  376914940.0
relative error loss 0.1346969
shape of L is 
torch.Size([])
memory (bytes)
4277276672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4277276672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  375893000.0
relative error loss 0.13433169
shape of L is 
torch.Size([])
memory (bytes)
4280524800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4280524800
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  374952960.0
relative error loss 0.13399576
shape of L is 
torch.Size([])
memory (bytes)
4283904000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% |  7% |
memory (bytes)
4283904000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  373373950.0
relative error loss 0.13343146
shape of L is 
torch.Size([])
memory (bytes)
4287123456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4287123456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  373032450.0
relative error loss 0.13330942
shape of L is 
torch.Size([])
memory (bytes)
4290342912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4290342912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  371697400.0
relative error loss 0.13283233
shape of L is 
torch.Size([])
memory (bytes)
4293390336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4293390336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  371163140.0
relative error loss 0.13264139
shape of L is 
torch.Size([])
memory (bytes)
4296617984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4296781824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% |  7% |
error is  370279170.0
relative error loss 0.1323255
time to take a step is 287.50626945495605
it  2 : 1524149248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4300001280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% |  7% |
memory (bytes)
4300001280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  370279170.0
relative error loss 0.1323255
shape of L is 
torch.Size([])
memory (bytes)
4303175680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4303175680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  369306880.0
relative error loss 0.13197803
shape of L is 
torch.Size([])
memory (bytes)
4306427904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4306427904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  369002240.0
relative error loss 0.13186917
shape of L is 
torch.Size([])
memory (bytes)
4309606400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4309606400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  367769340.0
relative error loss 0.13142857
shape of L is 
torch.Size([])
memory (bytes)
4312887296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4312895488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  7% |
error is  367394800.0
relative error loss 0.13129473
shape of L is 
torch.Size([])
memory (bytes)
4316024832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% |  7% |
memory (bytes)
4316024832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  366798600.0
relative error loss 0.13108166
shape of L is 
torch.Size([])
memory (bytes)
4319305728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4319305728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  366585860.0
relative error loss 0.13100563
shape of L is 
torch.Size([])
memory (bytes)
4322521088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% |  7% |
memory (bytes)
4322521088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  366150140.0
relative error loss 0.13084991
shape of L is 
torch.Size([])
memory (bytes)
4325711872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4325711872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  365811200.0
relative error loss 0.1307288
shape of L is 
torch.Size([])
memory (bytes)
4328968192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4328968192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  365502720.0
relative error loss 0.13061856
time to take a step is 291.85411858558655
it  3 : 1524149248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4332068864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4332068864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  365502720.0
relative error loss 0.13061856
shape of L is 
torch.Size([])
memory (bytes)
4335382528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  7% |
memory (bytes)
4335382528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  364934400.0
relative error loss 0.13041545
shape of L is 
torch.Size([])
memory (bytes)
4338601984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4338601984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  365338100.0
relative error loss 0.13055973
shape of L is 
torch.Size([])
memory (bytes)
4341829632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4341829632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  364652800.0
relative error loss 0.13031481
shape of L is 
torch.Size([])
memory (bytes)
4344926208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4345044992
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  364354800.0
relative error loss 0.13020833
shape of L is 
torch.Size([])
memory (bytes)
4348256256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4348256256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  364086270.0
relative error loss 0.13011236
shape of L is 
torch.Size([])
memory (bytes)
4351475712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4351475712
| ID | GPU | MEM |
------------------
|  0 |  2% |  0% |
|  1 | 99% |  8% |
error is  363759360.0
relative error loss 0.12999552
shape of L is 
torch.Size([])
memory (bytes)
4354564096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4354691072
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  363463680.0
relative error loss 0.12988986
shape of L is 
torch.Size([])
memory (bytes)
4357914624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4357914624
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  363315460.0
relative error loss 0.12983689
shape of L is 
torch.Size([])
memory (bytes)
4361121792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% |  7% |
memory (bytes)
4361121792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  363150850.0
relative error loss 0.12977807
time to take a step is 289.5189380645752
it  4 : 1524149248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4364357632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4364357632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  363150850.0
relative error loss 0.12977807
shape of L is 
torch.Size([])
memory (bytes)
4367454208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4367454208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  362893300.0
relative error loss 0.12968603
shape of L is 
torch.Size([])
memory (bytes)
4370644992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4370792448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  362742000.0
relative error loss 0.12963197
shape of L is 
torch.Size([])
memory (bytes)
4374016000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4374016000
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  362651650.0
relative error loss 0.12959968
shape of L is 
torch.Size([])
memory (bytes)
4377231360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4377231360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  362536700.0
relative error loss 0.1295586
shape of L is 
torch.Size([])
memory (bytes)
4380459008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4380459008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  362387460.0
relative error loss 0.12950526
shape of L is 
torch.Size([])
memory (bytes)
4383670272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4383670272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  362278400.0
relative error loss 0.12946628
shape of L is 
torch.Size([])
memory (bytes)
4386902016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  7% |
memory (bytes)
4386902016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  362054900.0
relative error loss 0.12938643
shape of L is 
torch.Size([])
memory (bytes)
4390121472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4390121472
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  361950720.0
relative error loss 0.12934919
shape of L is 
torch.Size([])
memory (bytes)
4393308160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4393340928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  361812220.0
relative error loss 0.12929969
time to take a step is 289.14775586128235
it  5 : 1524149248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4396556288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4396556288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  361812220.0
relative error loss 0.12929969
shape of L is 
torch.Size([])
memory (bytes)
4399669248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4399771648
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  361623800.0
relative error loss 0.12923236
shape of L is 
torch.Size([])
memory (bytes)
4402995200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% |  7% |
memory (bytes)
4402995200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  361436930.0
relative error loss 0.12916557
shape of L is 
torch.Size([])
memory (bytes)
4406132736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4406214656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  361337600.0
relative error loss 0.12913008
shape of L is 
torch.Size([])
memory (bytes)
4409430016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4409430016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  361249540.0
relative error loss 0.12909861
shape of L is 
torch.Size([])
memory (bytes)
4412530688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  7% |
memory (bytes)
4412530688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  361029120.0
relative error loss 0.12901983
shape of L is 
torch.Size([])
memory (bytes)
4415868928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4415868928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  360918270.0
relative error loss 0.12898022
shape of L is 
torch.Size([])
memory (bytes)
4418961408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4419084288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  360777200.0
relative error loss 0.12892981
shape of L is 
torch.Size([])
memory (bytes)
4422209536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4422295552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  360640770.0
relative error loss 0.12888105
shape of L is 
torch.Size([])
memory (bytes)
4425510912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  7% |
memory (bytes)
4425510912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  360516100.0
relative error loss 0.1288365
time to take a step is 290.2205193042755
it  6 : 1524149248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4428730368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4428730368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  360516100.0
relative error loss 0.1288365
shape of L is 
torch.Size([])
memory (bytes)
4431941632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4431941632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  360433920.0
relative error loss 0.12880713
shape of L is 
torch.Size([])
memory (bytes)
4435173376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4435173376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  360360200.0
relative error loss 0.12878078
shape of L is 
torch.Size([])
memory (bytes)
4438392832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4438392832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  360292100.0
relative error loss 0.12875645
shape of L is 
torch.Size([])
memory (bytes)
4441604096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% |  8% |
memory (bytes)
4441604096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  360210940.0
relative error loss 0.12872745
shape of L is 
torch.Size([])
memory (bytes)
4444700672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4444823552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  360184830.0
relative error loss 0.12871811
shape of L is 
torch.Size([])
memory (bytes)
4448038912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4448038912
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  360162560.0
relative error loss 0.12871015
shape of L is 
torch.Size([])
memory (bytes)
4451221504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4451221504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  360120320.0
relative error loss 0.12869506
shape of L is 
torch.Size([])
memory (bytes)
4454469632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4454469632
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  360065800.0
relative error loss 0.12867557
shape of L is 
torch.Size([])
memory (bytes)
4457541632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% |  8% |
memory (bytes)
4457689088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  360084220.0
relative error loss 0.12868217
shape of L is 
torch.Size([])
memory (bytes)
4460912640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% |  8% |
memory (bytes)
4460912640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  360013300.0
relative error loss 0.12865682
time to take a step is 321.1293578147888
it  7 : 1524149760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4464140288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4464140288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  360013300.0
relative error loss 0.12865682
shape of L is 
torch.Size([])
memory (bytes)
4467294208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% |  7% |
memory (bytes)
4467355648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  359949060.0
relative error loss 0.12863386
shape of L is 
torch.Size([])
memory (bytes)
4470587392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% |  7% |
memory (bytes)
4470587392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  359896580.0
relative error loss 0.1286151
shape of L is 
torch.Size([])
memory (bytes)
4473671680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4473798656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  359831550.0
relative error loss 0.12859187
shape of L is 
torch.Size([])
memory (bytes)
4476895232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4477018112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% |  8% |
error is  359765760.0
relative error loss 0.12856835
shape of L is 
torch.Size([])
memory (bytes)
4480237568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% |  8% |
memory (bytes)
4480237568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  359720700.0
relative error loss 0.12855224
shape of L is 
torch.Size([])
memory (bytes)
4483338240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% |  8% |
memory (bytes)
4483448832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  359675140.0
relative error loss 0.12853597
shape of L is 
torch.Size([])
memory (bytes)
4486668288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% |  8% |
memory (bytes)
4486668288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  359658750.0
relative error loss 0.12853011
shape of L is 
torch.Size([])
memory (bytes)
4489773056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4489773056
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  359569400.0
relative error loss 0.12849818
shape of L is 
torch.Size([])
memory (bytes)
4493094912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
4493094912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  359545600.0
relative error loss 0.12848967
time to take a step is 289.3662140369415
it  8 : 1524149248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4496314368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4496314368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  359545600.0
relative error loss 0.12848967
shape of L is 
torch.Size([])
memory (bytes)
4499419136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4499517440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  359515650.0
relative error loss 0.12847897
shape of L is 
torch.Size([])
memory (bytes)
4502753280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4502753280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% |  8% |
error is  359473400.0
relative error loss 0.12846388
shape of L is 
torch.Size([])
memory (bytes)
4505890816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4505890816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  359433470.0
relative error loss 0.1284496
shape of L is 
torch.Size([])
memory (bytes)
4509097984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4509188096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  359377920.0
relative error loss 0.12842976
shape of L is 
torch.Size([])
memory (bytes)
4512403456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  8% |
memory (bytes)
4512403456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% |  8% |
error is  359348740.0
relative error loss 0.12841932
shape of L is 
torch.Size([])
memory (bytes)
4515565568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4515565568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  359286530.0
relative error loss 0.12839709
shape of L is 
torch.Size([])
memory (bytes)
4518846464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4518846464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  359249660.0
relative error loss 0.12838392
shape of L is 
torch.Size([])
memory (bytes)
4522065920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  8% |
memory (bytes)
4522065920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% |  8% |
error is  359162880.0
relative error loss 0.1283529
shape of L is 
torch.Size([])
memory (bytes)
4525150208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4525277184
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  359090940.0
relative error loss 0.12832719
time to take a step is 290.316237449646
it  9 : 1524149248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4528488448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4528488448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  359090940.0
relative error loss 0.12832719
shape of L is 
torch.Size([])
memory (bytes)
4531691520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% |  7% |
memory (bytes)
4531691520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  359035900.0
relative error loss 0.12830752
shape of L is 
torch.Size([])
memory (bytes)
4534910976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4534910976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  358926080.0
relative error loss 0.12826827
shape of L is 
torch.Size([])
memory (bytes)
4538130432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4538130432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  358851600.0
relative error loss 0.12824166
shape of L is 
torch.Size([])
memory (bytes)
4541247488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4541345792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  358764030.0
relative error loss 0.12821037
shape of L is 
torch.Size([])
memory (bytes)
4544561152
| ID | GPU | MEM |
------------------
|  0 |  2% |  0% |
|  1 |  8% |  8% |
memory (bytes)
4544561152
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  358727170.0
relative error loss 0.1281972
shape of L is 
torch.Size([])
memory (bytes)
4547735552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
4547735552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  358614270.0
relative error loss 0.12815684
shape of L is 
torch.Size([])
memory (bytes)
4551008256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4551008256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  358641150.0
relative error loss 0.12816645
shape of L is 
torch.Size([])
memory (bytes)
4554113024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% |  8% |
memory (bytes)
4554223616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  358571260.0
relative error loss 0.12814148
shape of L is 
torch.Size([])
memory (bytes)
4557381632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% |  8% |
memory (bytes)
4557438976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  358511100.0
relative error loss 0.12811998
time to take a step is 288.968496799469
it  10 : 1524149248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4560642048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4560642048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  358511100.0
relative error loss 0.12811998
shape of L is 
torch.Size([])
memory (bytes)
4563845120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4563845120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  358445820.0
relative error loss 0.12809666
shape of L is 
torch.Size([])
memory (bytes)
4567085056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% |  7% |
memory (bytes)
4567085056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  358404350.0
relative error loss 0.12808183
shape of L is 
torch.Size([])
memory (bytes)
4570275840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4570275840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  358371600.0
relative error loss 0.12807012
shape of L is 
torch.Size([])
memory (bytes)
4573417472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4573417472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  358335500.0
relative error loss 0.12805721
shape of L is 
torch.Size([])
memory (bytes)
4576739328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4576739328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  358304770.0
relative error loss 0.12804624
shape of L is 
torch.Size([])
memory (bytes)
4579954688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4579954688
| ID | GPU | MEM |
------------------
|  0 |  4% |  0% |
|  1 | 99% |  8% |
error is  358231040.0
relative error loss 0.1280199
shape of L is 
torch.Size([])
memory (bytes)
4583186432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% |  7% |
memory (bytes)
4583186432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  358172670.0
relative error loss 0.12799904
shape of L is 
torch.Size([])
memory (bytes)
4586397696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4586397696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  358131700.0
relative error loss 0.12798439
shape of L is 
torch.Size([])
memory (bytes)
4589613056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% |  7% |
memory (bytes)
4589613056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  358069500.0
relative error loss 0.12796217
time to take a step is 289.4561538696289
it  11 : 1524149248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4592807936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4592807936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  358069500.0
relative error loss 0.12796217
shape of L is 
torch.Size([])
memory (bytes)
4596043776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4596043776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  358027520.0
relative error loss 0.12794717
shape of L is 
torch.Size([])
memory (bytes)
4599283712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4599283712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  8% |
error is  357992960.0
relative error loss 0.12793481
shape of L is 
torch.Size([])
memory (bytes)
4602437632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% |  7% |
memory (bytes)
4602507264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  357970940.0
relative error loss 0.12792695
shape of L is 
torch.Size([])
memory (bytes)
4605710336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4605710336
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  357952770.0
relative error loss 0.12792045
shape of L is 
torch.Size([])
memory (bytes)
4608925696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4608925696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  357909000.0
relative error loss 0.1279048
shape of L is 
torch.Size([])
memory (bytes)
4612145152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4612145152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  357870080.0
relative error loss 0.1278909
shape of L is 
torch.Size([])
memory (bytes)
4615348224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4615348224
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  357825800.0
relative error loss 0.12787507
shape of L is 
torch.Size([])
memory (bytes)
4618465280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  0% |  8% |
memory (bytes)
4618567680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  357798140.0
relative error loss 0.1278652
shape of L is 
torch.Size([])
memory (bytes)
4621795328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4621795328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% |  8% |
error is  357760260.0
relative error loss 0.12785165
time to take a step is 293.8067889213562
it  12 : 1524149248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4624834560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4625010688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  357760260.0
relative error loss 0.12785165
shape of L is 
torch.Size([])
memory (bytes)
4628213760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  7% |
memory (bytes)
4628213760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  357731330.0
relative error loss 0.12784131
shape of L is 
torch.Size([])
memory (bytes)
4631298048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% |  7% |
memory (bytes)
4631445504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  357701380.0
relative error loss 0.12783061
shape of L is 
torch.Size([])
memory (bytes)
4634537984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4634664960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  357663230.0
relative error loss 0.12781698
shape of L is 
torch.Size([])
memory (bytes)
4637884416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
4637884416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% |  8% |
error is  357649400.0
relative error loss 0.12781204
shape of L is 
torch.Size([])
memory (bytes)
4641030144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4641030144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% |  8% |
error is  357634050.0
relative error loss 0.12780654
shape of L is 
torch.Size([])
memory (bytes)
4644306944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% |  8% |
memory (bytes)
4644306944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  357603070.0
relative error loss 0.12779547
shape of L is 
torch.Size([])
memory (bytes)
4647514112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4647514112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  357588220.0
relative error loss 0.12779017
shape of L is 
torch.Size([])
memory (bytes)
4650733568
| ID | GPU | MEM |
------------------
|  0 |  1% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4650733568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% |  8% |
error is  357581300.0
relative error loss 0.1277877
shape of L is 
torch.Size([])
memory (bytes)
4653940736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  8% |
memory (bytes)
4653940736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  357526530.0
relative error loss 0.12776813
time to take a step is 291.08729577064514
it  13 : 1524149248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4657102848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4657164288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  357526530.0
relative error loss 0.12776813
shape of L is 
torch.Size([])
memory (bytes)
4660371456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% |  7% |
memory (bytes)
4660371456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  357507840.0
relative error loss 0.12776144
shape of L is 
torch.Size([])
memory (bytes)
4663533568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4663533568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  357484540.0
relative error loss 0.12775312
shape of L is 
torch.Size([])
memory (bytes)
4666818560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  7% |
memory (bytes)
4666818560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% |  8% |
error is  357457920.0
relative error loss 0.1277436
shape of L is 
torch.Size([])
memory (bytes)
4669943808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% |  8% |
memory (bytes)
4670046208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  357424900.0
relative error loss 0.1277318
shape of L is 
torch.Size([])
memory (bytes)
4673265664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% |  8% |
memory (bytes)
4673265664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  357390340.0
relative error loss 0.12771945
shape of L is 
torch.Size([])
memory (bytes)
4676489216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
4676489216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  357362430.0
relative error loss 0.12770948
shape of L is 
torch.Size([])
memory (bytes)
4679581696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4679696384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  357352200.0
relative error loss 0.12770583
shape of L is 
torch.Size([])
memory (bytes)
4682911744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
4682911744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% |  8% |
error is  357314300.0
relative error loss 0.12769228
shape of L is 
torch.Size([])
memory (bytes)
4686008320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% |  8% |
memory (bytes)
4686123008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  357298700.0
relative error loss 0.1276867
time to take a step is 291.96283292770386
it  14 : 1524149248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4689334272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4689334272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  357298700.0
relative error loss 0.1276867
shape of L is 
torch.Size([])
memory (bytes)
4692545536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4692545536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  357282050.0
relative error loss 0.12768075
shape of L is 
torch.Size([])
memory (bytes)
4695769088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4695769088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% |  8% |
error is  357260540.0
relative error loss 0.12767307
shape of L is 
torch.Size([])
memory (bytes)
4698980352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4698980352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  357237500.0
relative error loss 0.12766483
shape of L is 
torch.Size([])
memory (bytes)
4702199808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  8% |
memory (bytes)
4702199808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  357225730.0
relative error loss 0.12766063
shape of L is 
torch.Size([])
memory (bytes)
4705423360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  8% |
memory (bytes)
4705423360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  357205500.0
relative error loss 0.1276534
shape of L is 
torch.Size([])
memory (bytes)
4708528128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4708646912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  357185800.0
relative error loss 0.12764636
shape of L is 
torch.Size([])
memory (bytes)
4711862272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  8% |
memory (bytes)
4711862272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  357171200.0
relative error loss 0.12764114
shape of L is 
torch.Size([])
memory (bytes)
4714991616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  8% |
memory (bytes)
4714991616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  357148670.0
relative error loss 0.1276331
shape of L is 
torch.Size([])
memory (bytes)
4718288896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4718288896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  357136900.0
relative error loss 0.12762888
time to take a step is 292.3719274997711
sum tnnu_Z after tensor(7550210.5000, device='cuda:0')
shape of features
(3958,)
shape of features
(3958,)
number of orig particles 15833
number of new particles after remove low mass 13659
tnuZ shape should be parts x labs
torch.Size([15833, 702])
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
error without small mass is  441757470.0
relative error without small mass is  0.15786947
nnu_Z shape should be number of particles by maxV
(15833, 702)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
shape of features
(15833,)
Thu Feb 2 10:29:59 EST 2023
