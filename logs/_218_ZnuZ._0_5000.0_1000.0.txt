Wed Feb 1 04:18:25 EST 2023
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  0% |  0% |
numbers of X: 30300002
numbers of Z: 17586
shape of features
(17586,)
shape of features
(17586,)
ZX	Vol	Parts	Cubes	Eps
Z	0.018060804964520095	17586	17.586	0.10089198763399734
X	0.01546212319407629	1267	1.267	0.23023119698892847
X	0.01581395496384474	14216	14.216	0.10361461738235936
X	0.015412172842268734	2024	2.024	0.1967362795356266
X	0.015747776101319957	14707	14.707	0.10230535805367878
X	0.015556274249163003	11722	11.722	0.1098924758302223
X	0.01594314003451232	31945	31.945	0.07932139122656519
X	0.015712958713563912	37370	37.37	0.07491663989861991
X	0.015915573825649672	31631	31.631	0.07953710750057806
X	0.01588224245468378	7686	7.686	0.1273709771175076
X	0.015595646630090606	25003	25.003	0.08544180644274246
X	0.015608920854600212	6437	6.437	0.13434756202528828
X	0.015615835658375765	121387	121.387	0.0504813565854963
X	0.015863455932431433	6677	6.677	0.1334357323078702
X	0.015940129614039254	266748	266.748	0.03909581090956522
X	0.01577815831288327	18829	18.829	0.09427785319475607
X	0.015971381008443853	35265	35.265	0.07679498513653904
X	0.015860412774474728	43859	43.859	0.0712445946957137
X	0.015876121026230337	15006	15.006	0.1018966296882992
X	0.016828288543564225	105062	105.062	0.05430811913161394
X	0.016059382097571052	64492	64.492	0.06291308697031975
X	0.01585664076680988	34725	34.725	0.07700571854455322
X	0.01610379516213687	192089	192.089	0.0437665097243247
X	0.01577401937085588	7652	7.652	0.12726895246079817
X	0.01589203364143288	36574	36.574	0.07574180869076287
X	0.015354524291645617	3885	3.885	0.15810603501512457
X	0.0159069910647299	65167	65.167	0.0624961747858161
X	0.01588486439666369	48174	48.174	0.06908603492702164
X	0.015901953622560153	6705	6.705	0.1333574349306321
X	0.015846079997546177	19083	19.083	0.09399218189887827
X	0.016863502990503498	969466	969.466	0.025910257979354503
X	0.015734336179057708	8094	8.094	0.12480397227212305
X	0.017246358877929528	482739	482.739	0.03293530408383754
X	0.015798416950133305	21805	21.805	0.08981574849111727
X	0.015838200280563652	6971	6.971	0.13146301328325186
X	0.015559645741508872	15854	15.854	0.09937724443826355
X	0.016118967718408813	76475	76.475	0.05951218740296095
X	0.016153850986890844	74391	74.391	0.060106101241077244
X	0.015343406220801303	1001	1.001	0.24840627855787722
X	0.0153724686077231	3077	3.077	0.1709511550620911
X	0.01556376240809889	1766	1.766	0.206557656522287
X	0.015612900782302937	2985	2.985	0.17358535746722326
X	0.014900410659152732	2276	2.276	0.18707194430383653
X	0.01511995794022097	649	0.649	0.2856068561827694
X	0.015507296153626952	3778	3.778	0.1601123047788715
X	0.015245713298918936	940	0.94	0.2531278701572671
X	0.015252078343786082	1040	1.04	0.2447739686587434
X	0.01554612641102558	1821	1.821	0.20437953952281848
X	0.015561823815084691	3183	3.183	0.16972322880151872
X	0.015348200785997714	2594	2.594	0.180868318102901
X	0.015707319639046917	9839	9.839	0.116873766508133
X	0.015696755615885294	8703	8.703	0.1217251460814633
X	0.015225928638681142	1314	1.314	0.22628866914469462
X	0.0154517683579442	6049	6.049	0.13669885223829667
X	0.015261532828562295	3220	3.22	0.1679760744993244
X	0.01574898599437039	2981	2.981	0.17416606519154176
X	0.0154061788926786	6055	6.055	0.13651915651791077
X	0.01556314438176138	1978	1.978	0.19889491794507988
X	0.015503472513806148	4377	4.377	0.15243480715485053
X	0.015603587191246557	2616	2.616	0.1813547371149444
X	0.015485649852784484	1926	1.926	0.20033540920680784
X	0.015401653993656981	3648	3.648	0.16162337731006388
X	0.015516366423805605	1167	1.167	0.23690436186920075
X	0.015524436023840541	3857	3.857	0.15907016480722516
X	0.015686924307122735	7622	7.622	0.1272007344319921
X	0.015700994082644727	2412	2.412	0.1867167511749713
X	0.01582919006440363	1879	1.879	0.2034748035706856
X	0.015586513043164846	2704	2.704	0.17930019422701574
X	0.015262514277353055	4506	4.506	0.15017983814752198
X	0.015761721833056085	4251	4.251	0.15477620168809647
X	0.015656836539332454	2722	2.722	0.1791727525186595
X	0.015485766698185997	5371	5.371	0.14232871221855387
X	0.015359748156477792	3918	3.918	0.1576787691248174
X	0.015599067400607522	3437	3.437	0.16556671579676818
X	0.015035257951795472	2734	2.734	0.17651059994281248
X	0.015097021244693476	3240	3.24	0.1670252326394269
X	0.015437500106915842	3775	3.775	0.15991406846401937
X	0.015339200043858525	627	0.627	0.2902986771467087
X	0.015644790944312068	1252	1.252	0.23205368984062977
X	0.015470668902783945	2960	2.96	0.17354247764182362
X	0.016043387479088445	14942	14.942	0.10239901526505904
X	0.015352426259323619	2520	2.52	0.1826384277086627
X	0.014734704041323382	1616	1.616	0.20891358748900146
X	0.01568516214521155	2952	2.952	0.1744981738952645
X	0.015099277444110392	1985	1.985	0.19666717737281794
X	0.015257751214535301	1309	1.309	0.22673416239354072
X	0.01536468948003069	1232	1.232	0.23190192832465872
X	0.01567224482754015	2102	2.102	0.19535873387035746
X	0.015656848845117123	1180	1.18	0.23674134855785187
X	0.01560558061363971	3179	3.179	0.16995337804687613
X	0.01534073589101744	2803	2.803	0.17622777789330205
X	0.015602985980719384	4204	4.204	0.15482758402174707
X	0.015177532636100683	1837	1.837	0.20216098255433498
X	0.01579899197619216	1669	1.669	0.21153927615418514
X	0.01575279230166357	1899	1.899	0.2024312362684805
X	0.015805557420607772	4348	4.348	0.1537588399727866
X	0.015476199678687913	3619	3.619	0.16231497173211762
X	0.015498153871947605	2108	2.108	0.194447838847775
X	0.015450653822473842	5612	5.612	0.14015536709780993
X	0.015793380586568033	4540	4.54	0.15152108890892121
X	0.015498219149040992	2347	2.347	0.18761010910418915
X	0.015681912964547174	5676	5.676	0.1403197764501039
X	0.01534792392546075	2818	2.818	0.17594201022830852
X	0.0155907915776654	4729	4.729	0.14883313071394536
X	0.01526627568168119	1342	1.342	0.2249021091425426
X	0.015629505808356956	3505	3.505	0.1645959233767696
X	0.01533122970338031	1613	1.613	0.2118267848251244
X	0.015701088495710412	3781	3.781	0.16073397855608368
X	0.014828610599235207	1255	1.255	0.22776426064831673
X	0.015450787952976375	1853	1.853	0.20277997771608017
X	0.015573119954584361	1616	1.616	0.21280316887623987
X	0.015472587658592864	1869	1.869	0.2022947211872475
X	0.015136947490033408	1229	1.229	0.23093803906619184
X	0.015448104447142607	1879	1.879	0.20182864553070173
X	0.01558847700086258	2901	2.901	0.17515343247270576
X	0.015582771325897802	4086	4.086	0.1562363598395367
X	0.015459696174127105	2887	2.887	0.17495165570101498
X	0.01543377687827922	2550	2.55	0.1822401282436558
X	0.015321562777601523	791	0.791	0.2685605352634522
X	0.015549966827621265	5448	5.448	0.14185047334022047
X	0.015631432501556474	1512	1.512	0.21784570211762722
X	0.015912563137067633	6152	6.152	0.13726967465270107
X	0.01563636705309424	1475	1.475	0.21967532559913377
X	0.015248531156519897	1959	1.959	0.19818214523869587
X	0.015338521137409751	1954	1.954	0.19874046880046103
X	0.014806433821812182	1477	1.477	0.21562047156719386
X	0.015633176423797697	1902	1.902	0.20181134282134439
X	0.015467891144051255	1338	1.338	0.22611272184675613
X	0.015531552172670358	3351	3.351	0.1667298952594275
X	0.01590566238466559	6033	6.033	0.13814637015864303
X	0.015717732615373303	2077	2.077	0.1963290054780579
X	0.015815925602864787	7056	7.056	0.13087158043923758
X	0.01540869244507094	1597	1.597	0.2128891968551348
X	0.015695635704963646	2538	2.538	0.18355339888668604
X	0.015627268136637635	1247	1.247	0.23227664091382808
X	0.015431451019900509	3379	3.379	0.1659101124371821
X	0.015094991150653601	1549	1.549	0.21359634406696648
X	0.015196348087961331	1609	1.609	0.2113786222877979
X	0.015292183570398509	1309	1.309	0.22690459218354495
X	0.015030037395626257	2176	2.176	0.1904438956689967
X	0.015293587478590803	1396	1.396	0.2220962970626997
X	0.01508201731450009	2022	2.022	0.19538569374894146
X	0.015022068296454544	1296	1.296	0.22631234438297457
X	0.015389028911043719	9061	9.061	0.11931041648105911
X	0.015741125141327927	4867	4.867	0.1478852324947612
X	0.01564214731019936	3122	3.122	0.17111489860610743
X	0.015306047622103577	1403	1.403	0.22178651140759167
X	0.015461763036977242	1632	1.632	0.2115986793902955
X	0.017000061205081962	2345	2.345	0.19353932080845154
X	0.015018711162885614	1425	1.425	0.21924983081781316
X	0.015298115388341922	2576	2.576	0.18109120470772583
X	0.015540334640417547	2321	2.321	0.18847846754586384
X	0.01550292973661371	1951	1.951	0.19955020109119526
X	0.015934784147361897	3702	3.702	0.16266842316250896
X	0.015546004571197836	7239	7.239	0.12901694029224509
X	0.015000520975354004	2644	2.644	0.17835349764125236
X	0.016786102971629933	15239	15.239	0.10327561850982175
X	0.015534303975597396	2389	2.389	0.186648806174829
X	0.015442594903853604	2872	2.872	0.17519106301127874
X	0.015532751740897641	2147	2.147	0.19340696898508336
X	0.014746528965084391	487	0.487	0.31168814359757924
X	0.016941526047993382	9074	9.074	0.12313587031968398
X	0.01559686992521789	1466	1.466	0.2199384499530563
X	0.01581308713527293	2770	2.77	0.1787223906155287
X	0.015099304620316924	1383	1.383	0.22184258759270875
X	0.01569246644719323	3402	3.402	0.16646347883401802
X	0.015824679885118514	1761	1.761	0.2079019323185802
X	0.01551091825693175	1646	1.646	0.211220415567085
X	0.01554370085317434	2986	2.986	0.17330916985804143
X	0.015417597673751766	3235	3.235	0.16828580110200653
X	0.015060031618864876	907	0.907	0.25511707809519607
X	0.014948249804598827	752	0.752	0.2708886665739505
X	0.015468750567902225	929	0.929	0.25535625915917654
X	0.01575668013097987	5016	5.016	0.1464543909035784
X	0.015295848427474511	1545	1.545	0.21472454733340607
X	0.016850384924228193	3284	3.284	0.17247873537653222
X	0.015389664869381535	10165	10.165	0.1148260943637638
X	0.015359778874376034	3785	3.785	0.15950453176862642
X	0.015515392092925531	1961	1.961	0.19926379038249178
X	0.015705256998843155	4444	4.444	0.1523200238663119
X	0.015527617813894057	2506	2.506	0.1836712812206603
X	0.015367247452220966	3581	3.581	0.1625039497412727
X	0.01511439003118336	2021	2.021	0.19555763234533666
X	0.015760932842200173	4090	4.09	0.15677839117374529
X	0.015119518503401475	1824	1.824	0.20238160802602062
X	0.015779450037311682	3098	3.098	0.17205612980153587
X	0.015330377722253666	3081	3.081	0.170721041832564
X	0.015359916398554765	2341	2.341	0.18721004431244123
X	0.015567071125424815	3275	3.275	0.16813773852022298
X	0.016950589332539033	4831	4.831	0.15195532423929112
X	0.015726784695769692	4018	4.018	0.15759532720896108
X	0.015470078120168807	2408	2.408	0.18589969392782418
X	0.014676840484856136	858	0.858	0.2576608462051873
X	0.015801993787606473	1949	1.949	0.20089386883562987
X	0.015409096470736539	5308	5.308	0.14265338671622962
X	0.01573708333618347	6348	6.348	0.1353409151744194
X	0.015752650940102385	2059	2.059	0.19704515813121334
X	0.015565847273198659	1878	1.878	0.2023760238926018
X	0.015220154875683551	1192	1.192	0.23372989937697777
X	0.01553694249503757	1964	1.964	0.19925444797125608
X	0.015414286031108174	1022	1.022	0.24707236513695435
X	0.016389293201567522	3226	3.226	0.171909002590618
X	0.015377372397822152	2170	2.17	0.19207645543701327
X	0.01582590069663155	5362	5.362	0.14344339061403746
X	0.0156047971313558	1877	1.877	0.2025806458928306
X	0.015705227483383922	4103	4.103	0.15642790830728934
X	0.015546008028814238	1631	1.631	0.21202560390780234
X	0.015368550358014597	2686	2.686	0.17885824514649354
X	0.015668157727986343	2598	2.598	0.18202299782784362
X	0.015373802915372865	1653	1.653	0.21029848882454522
X	0.01719538430849827	5354	5.354	0.14754047361136774
X	0.015707043660020265	6631	6.631	0.133302555231496
X	0.014989086215675646	3591	3.591	0.1610101486021815
X	0.01526610023079633	3489	3.489	0.1635594770585033
X	0.015210090209853441	3005	3.005	0.17169689360178275
X	0.01503044027900954	1421	1.421	0.2195124758555659
X	0.015136382182758526	1280	1.28	0.22782639511305453
X	0.015513178237918283	1273	1.273	0.23012163953444265
X	0.015428500127331483	1052	1.052	0.24477627582038894
X	0.01542667406306173	5864	5.864	0.13804672693606157
X	0.015832048971194307	1911	1.911	0.20234484555108545
X	0.015277958279043899	2530	2.53	0.1821020910877393
X	0.015387148554783993	923	0.923	0.25545759265787427
X	0.015824660740709186	4898	4.898	0.14783316528656446
X	0.01503631287452184	2626	2.626	0.17890214349642142
X	0.01518038515283346	2286	2.286	0.18796147949664524
X	0.015430863796030474	3571	3.571	0.16287963817170736
X	0.015075974675631074	1187	1.187	0.233316199427308
X	0.015409186116130522	1979	1.979	0.1982034922016042
X	0.01554603440751053	2948	2.948	0.17405935978700862
X	0.014870677435100696	1180	1.18	0.23271064434208868
X	0.015992724865773602	2254	2.254	0.19215698889540153
X	0.015189742669900853	2322	2.322	0.1870234512598893
X	0.015546364350084167	2080	2.08	0.19551879063023342
X	0.01587077769767815	3178	3.178	0.17092861430729592
X	0.015343610309366505	1692	1.692	0.2085334526124006
X	0.015640941179192232	2696	2.696	0.1796860375260036
X	0.01556392569954172	4324	4.324	0.15325376565329984
X	0.01578344678647292	4427	4.427	0.1527674312541573
X	0.015370067845742845	2295	2.295	0.1884940727417985
X	0.015649785600251296	16964	16.964	0.09734792737972556
X	0.015852559617821317	42481	42.481	0.07199487045005422
X	0.015599528661319318	4274	4.274	0.15396629276127827
X	0.015492309053538178	4991	4.991	0.1458734297294525
X	0.01483291921352296	861	0.861	0.2582703156431226
X	0.015589349722639678	5634	5.634	0.1403902839425321
X	0.01573361592192569	12036	12.036	0.10934076843319007
X	0.015917949063744035	112002	112.002	0.05218597208223076
X	0.015648593789342594	2104	2.104	0.19519852301729093
X	0.016768729293982758	103891	103.891	0.05444701508960111
X	0.015612940050762587	19292	19.292	0.0931899645013508
X	0.015810890784220145	18855	18.855	0.09429961811902389
X	0.015735889025600306	20675	20.675	0.09130234288092244
X	0.0154347674626692	2203	2.203	0.19135002849730245
X	0.015497494070774366	4317	4.317	0.15311808071808186
X	0.015908779542532232	94514	94.514	0.0552137070475699
X	0.01595515361185024	491209	491.209	0.03190643080505824
X	0.01547038366510666	2647	2.647	0.18012847864519788
X	0.01589413292237705	28859	28.859	0.08196941850129559
X	0.015814471159449037	14364	14.364	0.1032586459353558
X	0.016408561176405677	29450	29.45	0.08228646403596648
X	0.015750388804403843	83412	83.412	0.05737036084270449
X	0.015662039917144002	44482	44.482	0.07061354370841777
X	0.015901452151170205	27981	27.981	0.08283067205881399
X	0.015816867667501282	15974	15.974	0.09967102710479608
X	0.015384342976994612	2303	2.303	0.18833383003962087
X	0.01593963601786916	123390	123.39	0.05055135626194105
X	0.0156553873769847	8238	8.238	0.12386463965584134
X	0.015694422088763555	1978	1.978	0.19945259007226074
X	0.0158595202278365	19292	19.292	0.09367799724251664
X	0.015773647596204577	58382	58.382	0.06464739881840012
X	0.01716238159973587	90830	90.83	0.05738293355257801
X	0.015901325740544325	76293	76.293	0.059290195252226126
X	0.01578204682445303	1649	1.649	0.2123151039347895
X	0.015832147041872073	15782	15.782	0.10010580412629905
X	0.015677843996465705	4605	4.605	0.1504361584479821
X	0.01580238467832632	34283	34.283	0.07724693745347655
X	0.015869114774324095	51464	51.464	0.06755897537487487
X	0.01566969488615963	6320	6.32	0.13534674652261341
X	0.01576381994216569	63979	63.979	0.06269140541623885
X	0.014481639406111387	755	0.755	0.26768472684088707
X	0.01579636974095358	4595	4.595	0.1509236355692924
X	0.015735825250142617	60151	60.151	0.06395613698821107
X	0.015552226561729983	34294	34.294	0.07682893450829197
X	0.01584733669262893	60582	60.582	0.0639544683177218
X	0.015326274252646066	2463	2.463	0.1839320425224949
X	0.015950149357628306	193286	193.286	0.04353662761706557
X	0.01586032067245223	15280	15.28	0.10125027307878272
X	0.015987518065544177	21619	21.619	0.09043054514136709
X	0.015901407932951257	47891	47.891	0.06924587240722781
X	0.015266951800472033	1686	1.686	0.20843225225035475
X	0.015898638118674428	80230	80.23	0.05830078162893946
X	0.016793500748770066	126650	126.65	0.05099317044740767
X	0.01610190168250525	188547	188.547	0.0440371476459861
X	0.015736251408981914	24964	24.964	0.08574241347260045
X	0.015645519199660778	18626	18.626	0.09435323472855563
X	0.015266290100138302	6381	6.381	0.13374624221135106
X	0.015750893785750467	5207	5.207	0.14462361349665992
X	0.01588063807754055	61482	61.482	0.06368541558866143
X	0.01647488731790592	7226	7.226	0.13161584161687298
X	0.015801053333826552	18391	18.391	0.09506636715453541
X	0.01592084983813708	43786	43.786	0.07137459514393849
X	0.0160088609478467	24549	24.549	0.08671792460888061
X	0.015539641897456224	10895	10.895	0.11256531490482254
X	0.015827065506289442	3587	3.587	0.16401730603882803
X	0.015866078816462674	64183	64.183	0.06276003804819912
X	0.015574223587133233	34643	34.643	0.07660615010819309
X	0.015811806059095553	16093	16.093	0.09941413906756537
X	0.01578821283072803	4616	4.616	0.15066847435174313
X	0.01585273085989113	44381	44.381	0.0709527121559092
X	0.01582429505108483	12830	12.83	0.10724224704959037
X	0.015907115434174245	128010	128.01	0.049901757916977325
X	0.01579542167605292	5856	5.856	0.1392013302158534
X	0.015721256190329796	31306	31.306	0.07948525185642326
X	0.015822995091269117	12122	12.122	0.10928774656830145
X	0.016022986393528154	73642	73.642	0.06014589102107476
X	0.015835698496169563	31261	31.261	0.07971586921878329
X	0.01573700228530503	7044	7.044	0.13072767988757397
X	0.01598787337193331	94739	94.739	0.05526124111422299
X	0.01593583008228059	122971	122.971	0.05060467754771426
X	0.015597126562130251	3919	3.919	0.15847342501937967
X	0.015977086180973963	98873	98.873	0.05446781520763454
X	0.01599604564977069	110498	110.498	0.05250726602553351
X	0.017536558529189353	248190	248.19	0.04134154440890385
X	0.015708395508888626	16689	16.689	0.09800175151768122
X	0.015205570787611882	2297	2.297	0.18776468401462967
X	0.015902549061783525	11610	11.61	0.11105671968928368
X	0.015887226749797645	16896	16.896	0.09796886223755805
X	0.015777000210170224	20762	20.762	0.09125396594507765
X	0.015848251906314226	33952	33.952	0.07757206174602282
X	0.015686979728035386	1440	1.44	0.2216795414411689
X	0.016078089992663476	48427	48.427	0.06924402371710542
X	0.015876300180754678	108204	108.204	0.05274346554424748
X	0.01563447054173718	4627	4.627	0.1500587161104898
X	0.01548779100328511	6399	6.399	0.1342638582677032
X	0.01583057824657148	22720	22.72	0.08865355399958477
X	0.015632258640593522	7601	7.601	0.1271697057726864
X	0.015711685681989496	4361	4.361	0.15330120381430162
X	0.015840150032344888	8426	8.426	0.1234182200417399
X	0.01539896727759463	2734	2.734	0.17792256406809814
X	0.015849428713004158	10766	10.766	0.11375916940300498
X	0.015921277501321496	28246	28.246	0.08260512367146332
X	0.016803434910812338	16895	16.895	0.0998190174486555
X	0.017344573943649282	25391	25.391	0.08806998929848169
X	0.016390891631867372	6564	6.564	0.13566818101616934
X	0.01627848447660838	708543	708.543	0.028428192830649252
X	0.01571867771206416	4782	4.782	0.1486855667377047
X	0.01589302563060887	72618	72.618	0.060263465155776144
X	0.015371922504088528	1282	1.282	0.228882920875484
X	0.01524278117665472	3070	3.07	0.1705985281097991
X	0.015203737946567342	2985	2.985	0.17205554752889962
X	0.015797393821681563	13426	13.426	0.1055713967311509
X	0.015579755020404259	3780	3.78	0.16033300841065976
X	0.01605691537372556	78900	78.9	0.058820453933770604
X	0.015336318831784561	4314	4.314	0.15262077784007486
X	0.015769985697733538	2774	2.774	0.17847399601859223
X	0.016084696154177208	149713	149.713	0.04753911758128603
X	0.015949975813106917	47169	47.169	0.06966817807185584
X	0.016007750605259396	29596	29.596	0.08147651231248058
X	0.016783420136576636	101574	101.574	0.05487391804703829
X	0.01676769247216585	203563	203.563	0.043510240702243616
X	0.016392054935692162	5484	5.484	0.14404957620435754
X	0.015576911581182847	6666	6.666	0.13270033374759393
X	0.015763553760102456	17481	17.481	0.09661160264275014
X	0.015061899271612928	1728	1.728	0.20579998253825274
X	0.01722689656130887	3618	3.618	0.16823363463676974
X	0.01572151565830731	10429	10.429	0.11466140217274504
X	0.015544047931593965	2591	2.591	0.18170445327056778
X	0.015402435813494425	5170	5.17	0.14389077603623382
X	0.015705399405722895	3669	3.669	0.16236799378041722
X	0.015819469128599235	3759	3.759	0.1614506730631656
X	0.016134949797431356	129162	129.162	0.04998936488672544
X	0.015435675664532807	11454	11.454	0.1104560449548333
X	0.015785898741711317	55236	55.236	0.06586919621992485
X	0.015408036928879624	2297	2.297	0.1885943908331357
X	0.015155737087060594	2685	2.685	0.17805093457020899
X	0.015922632911241186	36824	36.824	0.07561848526275224
X	0.0161540083108424	526175	526.175	0.03131242459854266
X	0.01600860520333977	343332	343.332	0.03599258760019651
X	0.015828032649241295	10191	10.191	0.11580750682291328
X	0.015892639456767856	32799	32.799	0.07854378200644974
X	0.01626567240030642	2479	2.479	0.18721120189465512
X	0.015699908194950853	3385	3.385	0.16676803566005768
X	0.016551303445497687	81128	81.128	0.05886906576586639
X	0.015693636064856235	13363	13.363	0.10550503913121667
X	0.015460278451316858	5640	5.64	0.13995209453761656
X	0.01572665830114042	37627	37.627	0.07476740125193641
X	0.016845952467917168	411370	411.37	0.03446838893137066
X	0.015496869875455697	20463	20.463	0.09115032623440766
X	0.015195674648151003	9647	9.647	0.11635254915038781
X	0.01608091061354044	14250	14.25	0.10411147303269941
X	0.01567569979535742	11155	11.155	0.11200887992700326
X	0.015925537646484474	4808	4.808	0.14906529641549815
X	0.015798245176086457	104186	104.186	0.05332528210182276
X	0.01571670067392462	13678	13.678	0.10474008633769764
X	0.01543561527571868	1434	1.434	0.22079620169197453
X	0.015986784188253136	68509	68.509	0.061565559123969084
X	0.017133838197447356	10237	10.237	0.11873005054453901
X	0.015629107064008198	2918	2.918	0.17496437358446296
X	0.01599721763167969	35278	35.278	0.07682693352579234
X	0.015883842339822533	88214	88.214	0.05646846956763718
X	0.017249721009304022	131841	131.841	0.05076649682121745
X	0.016837528498039333	114578	114.578	0.05277064091674912
X	0.015989652984377187	123521	123.521	0.050586280347209266
X	0.015727772719300304	14602	14.602	0.10250655232340015
X	0.01576512411907436	22360	22.36	0.08900379642120929
X	0.015957470050592944	107331	107.331	0.052976055790169216
X	0.01602756690355685	144470	144.47	0.04805035956497898
X	0.01584325935290904	6640	6.64	0.13362636382328583
X	0.015768029064282835	94413	94.413	0.05507001625850384
X	0.016728714642915636	180897	180.897	0.04522137216855656
X	0.015930961072329674	144769	144.769	0.04792058751845174
X	0.015940864678228285	97462	97.462	0.054688018164117724
X	0.015866361009038703	48791	48.791	0.06876686156000823
X	0.015650743600574387	7765	7.765	0.1263177861263453
X	0.015571667136083852	4179	4.179	0.15503184427296932
X	0.015461702759271915	3775	3.775	0.15999759502204575
X	0.016003219252922578	96903	96.903	0.054864325489720624
X	0.01598413951226146	73173	73.173	0.06022536862046051
X	0.01591511883981206	201965	201.965	0.04287241263338711
X	0.015820317333510303	57897	57.897	0.06489128534187821
X	0.015884096251614986	38377	38.377	0.07452417253781453
X	0.01561923938432243	11948	11.948	0.1093423043811832
X	0.015896818927397933	76374	76.374	0.059263627399070234
X	0.015611637615362023	5125	5.125	0.14496111828255875
X	0.015410545745077473	7116	7.116	0.12937807742806218
X	0.015892903124905704	83623	83.623	0.05749443767927119
X	0.01580510385407849	22504	22.504	0.08888856155293191
X	0.015367396531844432	3712	3.712	0.16056989007442662
X	0.015857527535715933	13212	13.212	0.10627284400936257
X	0.016822973249618813	415557	415.557	0.03433660749213957
X	0.015638842010569188	12546	12.546	0.10762168061554483
X	0.016733100546178388	106840	106.84	0.05390315436211441
X	0.015310395905224542	2882	2.882	0.17448743087170446
X	0.015431986190717041	3710	3.71	0.16082342447003187
X	0.015460773337960166	4189	4.189	0.1545397780936792
X	0.015448616994855325	2539	2.539	0.18256140458196515
X	0.016683986165199803	15130	15.13	0.1033126808152513
X	0.015712701904286065	29533	29.533	0.081030360046732
X	0.015441292023962674	5247	5.247	0.14330373970884033
X	0.01573467788643183	141064	141.064	0.048137169963867785
X	0.015390141258056307	4424	4.424	0.15152204713997963
X	0.016666678304404284	46853	46.853	0.07085500770836174
X	0.015840195809169	8700	8.7	0.12210883779384277
X	0.015898279407105354	61238	61.238	0.06379349198826016
X	0.015352929070705151	4577	4.577	0.14969367265324662
X	0.015836725662432678	11101	11.101	0.11257304934877442
X	0.015971245344549165	16769	16.769	0.09838839412432808
X	0.015811276373344833	15550	15.55	0.10055697005494661
X	0.016114082333324332	51527	51.527	0.0678771460115578
X	0.015790192447082164	508689	508.689	0.03142763439263527
X	0.01733775346106283	5667	5.667	0.14517100056978124
X	0.015798150701727368	4060	4.06	0.15728720904904517
X	0.01593000907794613	25992	25.992	0.08494257878155004
X	0.015532681075285637	12809	12.809	0.10663760923706703
X	0.015947342208844025	315228	315.228	0.03698462126309807
X	0.015325825112364886	2069	2.069	0.19493408845511523
X	0.016040610397713543	74112	74.112	0.06004047495015212
X	0.01609079227797898	163904	163.904	0.046131328176650735
X	0.015528760152516703	2316	2.316	0.18856716671749682
X	0.015649292316285875	221992	221.992	0.0413097398181621
X	0.015471775203009132	11255	11.255	0.11118978294945164
X	0.01767468218547454	405756	405.756	0.03518537798972169
X	0.01584558928833831	7491	7.491	0.1283678298792671
X	0.015615032947322175	11180	11.18	0.11178075375089767
X	0.015713093887430196	7345	7.345	0.12885164531234583
X	0.01591307575574685	3301	3.301	0.16892847778572745
X	0.015579942707480298	7321	7.321	0.1286269040054221
X	0.015881137206400442	22414	22.414	0.08914987599706187
X	0.01574923920665737	3176	3.176	0.17052695159719278
X	0.01601868462569694	77641	77.641	0.05908971703295557
X	0.015874268240315653	10026	10.026	0.1165526104110738
X	0.015770362815850918	4130	4.13	0.15630176339339882
X	0.01584503613136435	12255	12.255	0.1089414888044971
X	0.01680398245112038	143854	143.854	0.048883627454206796
X	0.016753699613740604	87738	87.738	0.057584773731480125
X	0.016944837992750972	220433	220.433	0.04251938130941597
X	0.015906227657883146	11727	11.727	0.11069468182222278
X	0.01581499039518938	6263	6.263	0.13617441145502698
X	0.01568895833955638	2137	2.137	0.19435536310963936
X	0.015444278616794819	2843	2.843	0.1757911157751793
X	0.015946956475531435	118457	118.457	0.05125139526311622
X	0.015433358716832336	5092	5.092	0.14471848870416137
X	0.015828165628212675	18209	18.209	0.09543657169669204
X	0.015828523276868525	17797	17.797	0.09616813628197211
X	0.015881644953883983	21118	21.118	0.09093845380155867
X	0.015769521983253102	10149	10.149	0.11582396431008597
X	0.015174542454831087	4014	4.014	0.15578037524118737
X	0.015910712748706972	141985	141.985	0.04821132377842214
X	0.015889106887123024	34289	34.289	0.07738347392712955
X	0.01559736676532034	5580	5.58	0.1408658418920396
X	0.015918043664332065	12998	12.998	0.10698865298653013
X	0.015834547170373955	40129	40.129	0.07334701378835631
X	0.01591933668790733	83109	83.109	0.0576446451937949
X	0.01592397365504694	286703	286.703	0.038153970334116945
X	0.016014740819347287	75282	75.282	0.059695682526248836
X	0.01569220438218431	3233	3.233	0.16931395341621847
X	0.01596538753553796	27688	27.688	0.08323307879845306
X	0.01573128499535552	29773	29.773	0.08084388963721009
X	0.015429541846864106	3976	3.976	0.15714559740028794
X	0.01553293852040625	9057	9.057	0.11969878767970817
X	0.015405472742358409	5616	5.616	0.13998536808520456
X	0.01576661059303157	17860	17.86	0.09592952535176416
X	0.01587005883614445	17575	17.575	0.09665575874664599
X	0.014774164823267284	2531	2.531	0.1800543417460658
X	0.015892881601048892	9593	9.593	0.11832669504473545
X	0.015630688586456155	6760	6.76	0.13223420388338697
X	0.015415829445418635	9207	9.207	0.1187452513808865
X	0.015098379183539937	4301	4.301	0.15198018256147441
X	0.015863953963077694	20440	20.44	0.09189887250979327
X	0.015464024826725353	14841	14.841	0.10138019550393984
X	0.015704864608547444	7920	7.92	0.1256327901530354
X	0.015619661722395265	5260	5.26	0.14373481024475057
X	0.015700388925594565	8675	8.675	0.12186536935310208
X	0.015979278031552708	19579	19.579	0.09345206068242723
X	0.015931586484058128	114879	114.879	0.05176141335692607
X	0.015708741289537437	4662	4.662	0.14991889506018663
X	0.015293411435071886	1332	1.332	0.2255970324371427
X	0.015462146125464829	4995	4.995	0.14573977461858753
X	0.015861148164899076	63841	63.841	0.06286539475126716
X	0.01683586869700733	179174	179.174	0.04546252088596088
X	0.015863035826905313	54113	54.113	0.06642958884700569
X	0.015466336336578587	4454	4.454	0.15143017091462643
X	0.01583961775023875	111515	111.515	0.052175962502673486
X	0.015927784728360966	108238	108.238	0.05279488799251367
X	0.015385091627662674	2133	2.133	0.19321302968926563
X	0.015312381532957567	5097	5.097	0.1442921493757549
X	0.015896590629643128	26660	26.66	0.08416818233953148
X	0.015907790396487688	38172	38.172	0.07469444676615272
X	0.01590926176241048	44577	44.577	0.07093268455740136
X	0.015940877152086714	122195	122.195	0.05071692643436907
X	0.015784361942102126	13836	13.836	0.10448938167033897
X	0.01564131707962919	3935	3.935	0.1584076657833629
X	0.016790754207246038	2261	2.261	0.19509967560674438
X	0.01578314905938417	19800	19.8	0.09272058348389121
X	0.015718837696520594	8420	8.42	0.12313157823639431
X	0.01583402735559326	15156	15.156	0.1014695181651542
X	0.015803134787572393	88764	88.764	0.05625599276196355
X	0.016886556695155968	376346	376.346	0.035534586489480206
X	0.01563918126512829	6386	6.386	0.13479124966145525
X	0.015112985716681591	1389	1.389	0.22158958394114528
X	0.015705429233948957	10528	10.528	0.11426186577238354
X	0.015853891410168797	36589	36.589	0.07567082107655757
X	0.015703067967610097	35243	35.243	0.0763783983232957
X	0.01527490923633822	6351	6.351	0.13398170758457445
X	0.015346694078531542	2686	2.686	0.17877341757538
X	0.01592250310518533	28930	28.93	0.0819510085836717
X	0.015470689685290267	3005	3.005	0.17267192588031569
X	0.01583066132070377	40825	40.825	0.07292184050251782
X	0.015602448259447952	3655	3.655	0.16221902616846298
X	0.015760847883586913	12086	12.086	0.10925273786845255
X	0.01557485489985129	5861	5.861	0.13851094617970783
X	0.01563164869979812	4251	4.251	0.15434926311664912
X	0.015334428602341448	5753	5.753	0.13865144750224134
X	0.015981162652446517	105877	105.877	0.05324378423499725
X	0.01614285648347049	186548	186.548	0.04423132297609527
X	0.015872083971911158	69302	69.302	0.061182806535462535
X	0.015823599391345845	73906	73.906	0.059823965019731244
X	0.01585838591152808	18063	18.063	0.0957538768885309
X	0.015833574378701518	9633	9.633	0.11801552301495558
X	0.01627374555576244	12025	12.025	0.1106116350626356
X	0.016076513413467244	90665	90.665	0.056180301697702884
X	0.015792896525116566	26881	26.881	0.08375398046479422
X	0.01581520669693822	19829	19.829	0.09273806283022161
X	0.015863686280950664	10147	10.147	0.11606167057460623
X	0.017812898282956548	663338	663.338	0.029945615018095683
X	0.015836595266330147	15340	15.34	0.10106764650263056
X	0.015857904039587353	42213	42.213	0.07215501521093366
X	0.01594952027597872	71950	71.95	0.060520932215053426
X	0.015982546451539285	35975	35.975	0.07630419077493561
X	0.01587391235784031	18975	18.975	0.0942252715807957
X	0.016797461413473342	376969	376.969	0.035452429728539944
X	0.016008530272599655	85082	85.082	0.05730219485486994
X	0.015868176330846393	50488	50.488	0.06799019449728588
X	0.015782012092182947	9859	9.859	0.11697952001746909
X	0.01567855718466873	29833	29.833	0.08069929154252438
X	0.01576096435329156	5298	5.298	0.14382143276972084
X	0.016086490609112474	100649	100.649	0.054268849565530734
X	0.01712188744833189	726824	726.824	0.028666463999345925
X	0.015834610540207277	65968	65.968	0.06214764689941301
X	0.015954250015589814	115050	115.05	0.051760276801137324
X	0.015912567995214885	72017	72.017	0.06045539764955138
X	0.015738573365361858	23699	23.699	0.0872459174619719
X	0.015962327649280073	18398	18.398	0.09537660708768854
X	0.015384080809461041	10456	10.456	0.11373705861184262
X	0.01604585323341937	13391	13.391	0.10621436780348036
X	0.015950569616837038	247547	247.547	0.040090318800668524
X	0.015901406762139317	66768	66.768	0.06198535030226353
X	0.015908572636770056	207414	207.414	0.04248781356523369
X	0.016258996141783284	100738	100.738	0.05444610310050397
X	0.016001473124976164	27890	27.89	0.08309415658723274
X	0.015254748538213304	4891	4.891	0.14610636300184446
X	0.01691888977488104	39513	39.513	0.07537210055591205
X	0.015960373294626306	105761	105.761	0.05324013713026652
X	0.015549241512593778	6502	6.502	0.1337275068742946
X	0.01597937184312094	309393	309.393	0.03724057744684229
X	0.01589466223037767	20335	20.335	0.09211613527336271
X	0.015718253560902284	22008	22.008	0.08938704306457701
X	0.015648815295765603	13185	13.185	0.10587671142716627
X	0.015674247712899082	7430	7.43	0.12825253679520127
X	0.01561763109855643	1634	1.634	0.21222068044047668
X	0.01529886998071555	2679	2.679	0.17874293301300911
X	0.015823942373061933	12686	12.686	0.10764569395882344
X	0.015437317832962346	27290	27.29	0.08270328687150358
X	0.016864606504417658	2050821	2050.821	0.02018441431679846
X	0.015402166941479722	7542	7.542	0.12687179939631382
X	0.01598681105140554	99640	99.64	0.054338716297845646
X	0.015491898782506244	6223	6.223	0.13552984759170122
X	0.01579268303323099	17963	17.963	0.09579856779604103
X	0.015587062718380397	6403	6.403	0.13452208615558048
X	0.015943383156004593	154334	154.334	0.04692161557020049
X	0.015509458046572258	8379	8.379	0.12278203033629763
X	0.016747883927893052	487853	487.853	0.032500520834204905
X	0.015259607503250198	4088	4.088	0.1551234632043148
X	0.01611160018451072	112395	112.395	0.05233560249206392
X	0.015895210395264987	17414	17.414	0.0970039115357029
X	0.015857010921993743	102500	102.5	0.05368246957627737
X	0.015816118139073798	239612	239.612	0.04041387048901264
X	0.01576991631140726	19699	19.699	0.09285281388269068
X	0.0156486438393862	8842	8.842	0.12096007994924907
X	0.016008964122785106	60996	60.996	0.06402564444651496
X	0.016126846990676062	8867	8.867	0.12206493353694155
X	0.015900910075494115	15037	15.037	0.10187952870865775
X	0.015896487579801524	68495	68.495	0.06145361496232001
X	0.01584701999489153	7299	7.299	0.12948756996851016
X	0.017670952591821998	43892	43.892	0.0738399732022512
X	0.015679131925654958	29106	29.106	0.08136666137203509
X	0.015889684117324974	87177	87.177	0.05669844062252438
X	0.015901614982781895	120550	120.55	0.05090471454205254
X	0.01577983443109668	34869	34.869	0.07677520472569026
X	0.01632512090310951	9032	9.032	0.12181221091728994
X	0.015814041745714463	62413	62.413	0.06327846577423803
X	0.015866004213527473	54349	54.349	0.06633743410075167
X	0.016645693195192043	25760	25.76	0.08645404988404495
X	0.01570370906098186	16291	16.291	0.09878359614865206
X	0.015817154676055507	22235	22.235	0.08926826128917184
X	0.015870763570265893	21432	21.432	0.09047148254579575
X	0.01525378952811768	98864	98.864	0.05363478251638937
X	0.015728346234684196	46281	46.281	0.06978468281512935
X	0.015808325284752005	12248	12.248	0.10887802357724985
X	0.015445821837246335	4187	4.187	0.15451454003221285
X	0.015302221791055785	4376	4.376	0.15178390655757265
X	0.015886409672734397	46646	46.646	0.0698345678425425
X	0.01606134490009411	59510	59.51	0.06462451835289153
X	0.016036356166026146	58551	58.551	0.06494172079339056
X	0.015708216309897417	7569	7.569	0.1275545992458825
X	0.01586132049143803	18512	18.512	0.09497923478480955
X	0.016747873941768486	25299	25.299	0.0871536224409457
X	0.0159620462056857	43503	43.503	0.07159067415871337
X	0.01561039036097772	21860	21.86	0.08938291937795212
X	0.015946537396817236	48417	48.417	0.06905940619961491
X	0.01600566204278492	5282	5.282	0.144707735892458
X	0.015932311855781287	23322	23.322	0.08807196156733203
X	0.01540031557575441	1870	1.87	0.20194324840004932
X	0.015441742139013271	4105	4.105	0.1555229115686319
X	0.01600757037377156	96542	96.542	0.05493760334687623
X	0.01580745061723964	13090	13.09	0.10648964904141765
X	0.01592187687907008	57589	57.589	0.06514557294494819
X	0.015443718983560199	1783	1.783	0.20536835091502825
X	0.015612099363883128	4487	4.487	0.1515310894747239
X	0.015842162293447745	20050	20.05	0.09244853363773785
X	0.015902598508956752	37432	37.432	0.07517527418592922
X	0.01586530231035829	20446	20.446	0.09189248550864357
X	0.015573108889782401	3856	3.856	0.15924999657952674
X	0.015538201247699553	7159	7.159	0.12947406791429958
X	0.015892145327727883	29854	29.854	0.0810450820846094
X	0.01592870621295318	14821	14.821	0.10243168847867395
X	0.01591100054680632	44329	44.329	0.07106730591350624
X	0.015699066488542274	4061	4.061	0.15694480559835056
X	0.01558060838300375	7247	7.247	0.12906506867151396
X	0.015632195543154544	30981	30.981	0.07961132444770234
X	0.015688265919714516	4358	4.358	0.15326014733824242
X	0.01676111585228783	104827	104.827	0.054276261175958046
X	0.015660700153587632	11817	11.817	0.1098418856091264
X	0.01575358459460969	49049	49.049	0.06848304735258458
X	0.016782809557629026	88845	88.845	0.05737779989848853
X	0.015888689864159023	69645	69.645	0.06110349453326559
X	0.016079304366401554	101482	101.482	0.05411189536518963
X	0.015999212614902603	83621	83.621	0.05762280796279181
X	0.01568358506155633	1985	1.985	0.19917199685827405
X	0.0159170935636731	33875	33.875	0.07774303454024055
X	0.01595411953169328	18828	18.828	0.09462870220882191
X	0.015872158087007766	38354	38.354	0.0745203876670804
X	0.015865929676382368	110147	110.147	0.052420072505853885
X	0.015673141373463824	20904	20.904	0.0908466432163643
X	0.015938309769585744	43906	43.906	0.07133556906444347
X	0.015875790497529237	11119	11.119	0.1126047065075703
X	0.01581080766070125	27455	27.455	0.08319761355598444
X	0.015526804008195361	3054	3.054	0.17195085756308026
X	0.015560867230769283	6153	6.153	0.13624344841269292
X	0.01577434600502573	103454	103.454	0.05342378966616211
X	0.01770862222597676	188502	188.502	0.04545932019867709
X	0.01550978353976072	11243	11.243	0.11132033604374579
X	0.015840775139951847	14680	14.68	0.10256915819819656
X	0.01589629439336091	17859	17.859	0.09619361660916147
X	0.016269010325443804	41062	41.062	0.07344693528166775
X	0.01589875006499099	34943	34.943	0.07691320896061778
X	0.01560955920798429	4724	4.724	0.148945338855596
X	0.015884648725555443	24208	24.208	0.08689729044193215
X	0.016786783284770904	109610	109.61	0.053502297920394765
X	0.01580643827408193	8287	8.287	0.1240163352659053
X	0.01587785968564038	41779	41.779	0.07243436207434552
X	0.015812690513313694	9966	9.966	0.11663483949446729
time for making epsilon is 1.6148405075073242
epsilons are
[0.23023119698892847, 0.10361461738235936, 0.1967362795356266, 0.10230535805367878, 0.1098924758302223, 0.07932139122656519, 0.07491663989861991, 0.07953710750057806, 0.1273709771175076, 0.08544180644274246, 0.13434756202528828, 0.0504813565854963, 0.1334357323078702, 0.03909581090956522, 0.09427785319475607, 0.07679498513653904, 0.0712445946957137, 0.1018966296882992, 0.05430811913161394, 0.06291308697031975, 0.07700571854455322, 0.0437665097243247, 0.12726895246079817, 0.07574180869076287, 0.15810603501512457, 0.0624961747858161, 0.06908603492702164, 0.1333574349306321, 0.09399218189887827, 0.025910257979354503, 0.12480397227212305, 0.03293530408383754, 0.08981574849111727, 0.13146301328325186, 0.09937724443826355, 0.05951218740296095, 0.060106101241077244, 0.24840627855787722, 0.1709511550620911, 0.206557656522287, 0.17358535746722326, 0.18707194430383653, 0.2856068561827694, 0.1601123047788715, 0.2531278701572671, 0.2447739686587434, 0.20437953952281848, 0.16972322880151872, 0.180868318102901, 0.116873766508133, 0.1217251460814633, 0.22628866914469462, 0.13669885223829667, 0.1679760744993244, 0.17416606519154176, 0.13651915651791077, 0.19889491794507988, 0.15243480715485053, 0.1813547371149444, 0.20033540920680784, 0.16162337731006388, 0.23690436186920075, 0.15907016480722516, 0.1272007344319921, 0.1867167511749713, 0.2034748035706856, 0.17930019422701574, 0.15017983814752198, 0.15477620168809647, 0.1791727525186595, 0.14232871221855387, 0.1576787691248174, 0.16556671579676818, 0.17651059994281248, 0.1670252326394269, 0.15991406846401937, 0.2902986771467087, 0.23205368984062977, 0.17354247764182362, 0.10239901526505904, 0.1826384277086627, 0.20891358748900146, 0.1744981738952645, 0.19666717737281794, 0.22673416239354072, 0.23190192832465872, 0.19535873387035746, 0.23674134855785187, 0.16995337804687613, 0.17622777789330205, 0.15482758402174707, 0.20216098255433498, 0.21153927615418514, 0.2024312362684805, 0.1537588399727866, 0.16231497173211762, 0.194447838847775, 0.14015536709780993, 0.15152108890892121, 0.18761010910418915, 0.1403197764501039, 0.17594201022830852, 0.14883313071394536, 0.2249021091425426, 0.1645959233767696, 0.2118267848251244, 0.16073397855608368, 0.22776426064831673, 0.20277997771608017, 0.21280316887623987, 0.2022947211872475, 0.23093803906619184, 0.20182864553070173, 0.17515343247270576, 0.1562363598395367, 0.17495165570101498, 0.1822401282436558, 0.2685605352634522, 0.14185047334022047, 0.21784570211762722, 0.13726967465270107, 0.21967532559913377, 0.19818214523869587, 0.19874046880046103, 0.21562047156719386, 0.20181134282134439, 0.22611272184675613, 0.1667298952594275, 0.13814637015864303, 0.1963290054780579, 0.13087158043923758, 0.2128891968551348, 0.18355339888668604, 0.23227664091382808, 0.1659101124371821, 0.21359634406696648, 0.2113786222877979, 0.22690459218354495, 0.1904438956689967, 0.2220962970626997, 0.19538569374894146, 0.22631234438297457, 0.11931041648105911, 0.1478852324947612, 0.17111489860610743, 0.22178651140759167, 0.2115986793902955, 0.19353932080845154, 0.21924983081781316, 0.18109120470772583, 0.18847846754586384, 0.19955020109119526, 0.16266842316250896, 0.12901694029224509, 0.17835349764125236, 0.10327561850982175, 0.186648806174829, 0.17519106301127874, 0.19340696898508336, 0.31168814359757924, 0.12313587031968398, 0.2199384499530563, 0.1787223906155287, 0.22184258759270875, 0.16646347883401802, 0.2079019323185802, 0.211220415567085, 0.17330916985804143, 0.16828580110200653, 0.25511707809519607, 0.2708886665739505, 0.25535625915917654, 0.1464543909035784, 0.21472454733340607, 0.17247873537653222, 0.1148260943637638, 0.15950453176862642, 0.19926379038249178, 0.1523200238663119, 0.1836712812206603, 0.1625039497412727, 0.19555763234533666, 0.15677839117374529, 0.20238160802602062, 0.17205612980153587, 0.170721041832564, 0.18721004431244123, 0.16813773852022298, 0.15195532423929112, 0.15759532720896108, 0.18589969392782418, 0.2576608462051873, 0.20089386883562987, 0.14265338671622962, 0.1353409151744194, 0.19704515813121334, 0.2023760238926018, 0.23372989937697777, 0.19925444797125608, 0.24707236513695435, 0.171909002590618, 0.19207645543701327, 0.14344339061403746, 0.2025806458928306, 0.15642790830728934, 0.21202560390780234, 0.17885824514649354, 0.18202299782784362, 0.21029848882454522, 0.14754047361136774, 0.133302555231496, 0.1610101486021815, 0.1635594770585033, 0.17169689360178275, 0.2195124758555659, 0.22782639511305453, 0.23012163953444265, 0.24477627582038894, 0.13804672693606157, 0.20234484555108545, 0.1821020910877393, 0.25545759265787427, 0.14783316528656446, 0.17890214349642142, 0.18796147949664524, 0.16287963817170736, 0.233316199427308, 0.1982034922016042, 0.17405935978700862, 0.23271064434208868, 0.19215698889540153, 0.1870234512598893, 0.19551879063023342, 0.17092861430729592, 0.2085334526124006, 0.1796860375260036, 0.15325376565329984, 0.1527674312541573, 0.1884940727417985, 0.09734792737972556, 0.07199487045005422, 0.15396629276127827, 0.1458734297294525, 0.2582703156431226, 0.1403902839425321, 0.10934076843319007, 0.05218597208223076, 0.19519852301729093, 0.05444701508960111, 0.0931899645013508, 0.09429961811902389, 0.09130234288092244, 0.19135002849730245, 0.15311808071808186, 0.0552137070475699, 0.03190643080505824, 0.18012847864519788, 0.08196941850129559, 0.1032586459353558, 0.08228646403596648, 0.05737036084270449, 0.07061354370841777, 0.08283067205881399, 0.09967102710479608, 0.18833383003962087, 0.05055135626194105, 0.12386463965584134, 0.19945259007226074, 0.09367799724251664, 0.06464739881840012, 0.05738293355257801, 0.059290195252226126, 0.2123151039347895, 0.10010580412629905, 0.1504361584479821, 0.07724693745347655, 0.06755897537487487, 0.13534674652261341, 0.06269140541623885, 0.26768472684088707, 0.1509236355692924, 0.06395613698821107, 0.07682893450829197, 0.0639544683177218, 0.1839320425224949, 0.04353662761706557, 0.10125027307878272, 0.09043054514136709, 0.06924587240722781, 0.20843225225035475, 0.05830078162893946, 0.05099317044740767, 0.0440371476459861, 0.08574241347260045, 0.09435323472855563, 0.13374624221135106, 0.14462361349665992, 0.06368541558866143, 0.13161584161687298, 0.09506636715453541, 0.07137459514393849, 0.08671792460888061, 0.11256531490482254, 0.16401730603882803, 0.06276003804819912, 0.07660615010819309, 0.09941413906756537, 0.15066847435174313, 0.0709527121559092, 0.10724224704959037, 0.049901757916977325, 0.1392013302158534, 0.07948525185642326, 0.10928774656830145, 0.06014589102107476, 0.07971586921878329, 0.13072767988757397, 0.05526124111422299, 0.05060467754771426, 0.15847342501937967, 0.05446781520763454, 0.05250726602553351, 0.04134154440890385, 0.09800175151768122, 0.18776468401462967, 0.11105671968928368, 0.09796886223755805, 0.09125396594507765, 0.07757206174602282, 0.2216795414411689, 0.06924402371710542, 0.05274346554424748, 0.1500587161104898, 0.1342638582677032, 0.08865355399958477, 0.1271697057726864, 0.15330120381430162, 0.1234182200417399, 0.17792256406809814, 0.11375916940300498, 0.08260512367146332, 0.0998190174486555, 0.08806998929848169, 0.13566818101616934, 0.028428192830649252, 0.1486855667377047, 0.060263465155776144, 0.228882920875484, 0.1705985281097991, 0.17205554752889962, 0.1055713967311509, 0.16033300841065976, 0.058820453933770604, 0.15262077784007486, 0.17847399601859223, 0.04753911758128603, 0.06966817807185584, 0.08147651231248058, 0.05487391804703829, 0.043510240702243616, 0.14404957620435754, 0.13270033374759393, 0.09661160264275014, 0.20579998253825274, 0.16823363463676974, 0.11466140217274504, 0.18170445327056778, 0.14389077603623382, 0.16236799378041722, 0.1614506730631656, 0.04998936488672544, 0.1104560449548333, 0.06586919621992485, 0.1885943908331357, 0.17805093457020899, 0.07561848526275224, 0.03131242459854266, 0.03599258760019651, 0.11580750682291328, 0.07854378200644974, 0.18721120189465512, 0.16676803566005768, 0.05886906576586639, 0.10550503913121667, 0.13995209453761656, 0.07476740125193641, 0.03446838893137066, 0.09115032623440766, 0.11635254915038781, 0.10411147303269941, 0.11200887992700326, 0.14906529641549815, 0.05332528210182276, 0.10474008633769764, 0.22079620169197453, 0.061565559123969084, 0.11873005054453901, 0.17496437358446296, 0.07682693352579234, 0.05646846956763718, 0.05076649682121745, 0.05277064091674912, 0.050586280347209266, 0.10250655232340015, 0.08900379642120929, 0.052976055790169216, 0.04805035956497898, 0.13362636382328583, 0.05507001625850384, 0.04522137216855656, 0.04792058751845174, 0.054688018164117724, 0.06876686156000823, 0.1263177861263453, 0.15503184427296932, 0.15999759502204575, 0.054864325489720624, 0.06022536862046051, 0.04287241263338711, 0.06489128534187821, 0.07452417253781453, 0.1093423043811832, 0.059263627399070234, 0.14496111828255875, 0.12937807742806218, 0.05749443767927119, 0.08888856155293191, 0.16056989007442662, 0.10627284400936257, 0.03433660749213957, 0.10762168061554483, 0.05390315436211441, 0.17448743087170446, 0.16082342447003187, 0.1545397780936792, 0.18256140458196515, 0.1033126808152513, 0.081030360046732, 0.14330373970884033, 0.048137169963867785, 0.15152204713997963, 0.07085500770836174, 0.12210883779384277, 0.06379349198826016, 0.14969367265324662, 0.11257304934877442, 0.09838839412432808, 0.10055697005494661, 0.0678771460115578, 0.03142763439263527, 0.14517100056978124, 0.15728720904904517, 0.08494257878155004, 0.10663760923706703, 0.03698462126309807, 0.19493408845511523, 0.06004047495015212, 0.046131328176650735, 0.18856716671749682, 0.0413097398181621, 0.11118978294945164, 0.03518537798972169, 0.1283678298792671, 0.11178075375089767, 0.12885164531234583, 0.16892847778572745, 0.1286269040054221, 0.08914987599706187, 0.17052695159719278, 0.05908971703295557, 0.1165526104110738, 0.15630176339339882, 0.1089414888044971, 0.048883627454206796, 0.057584773731480125, 0.04251938130941597, 0.11069468182222278, 0.13617441145502698, 0.19435536310963936, 0.1757911157751793, 0.05125139526311622, 0.14471848870416137, 0.09543657169669204, 0.09616813628197211, 0.09093845380155867, 0.11582396431008597, 0.15578037524118737, 0.04821132377842214, 0.07738347392712955, 0.1408658418920396, 0.10698865298653013, 0.07334701378835631, 0.0576446451937949, 0.038153970334116945, 0.059695682526248836, 0.16931395341621847, 0.08323307879845306, 0.08084388963721009, 0.15714559740028794, 0.11969878767970817, 0.13998536808520456, 0.09592952535176416, 0.09665575874664599, 0.1800543417460658, 0.11832669504473545, 0.13223420388338697, 0.1187452513808865, 0.15198018256147441, 0.09189887250979327, 0.10138019550393984, 0.1256327901530354, 0.14373481024475057, 0.12186536935310208, 0.09345206068242723, 0.05176141335692607, 0.14991889506018663, 0.2255970324371427, 0.14573977461858753, 0.06286539475126716, 0.04546252088596088, 0.06642958884700569, 0.15143017091462643, 0.052175962502673486, 0.05279488799251367, 0.19321302968926563, 0.1442921493757549, 0.08416818233953148, 0.07469444676615272, 0.07093268455740136, 0.05071692643436907, 0.10448938167033897, 0.1584076657833629, 0.19509967560674438, 0.09272058348389121, 0.12313157823639431, 0.1014695181651542, 0.05625599276196355, 0.035534586489480206, 0.13479124966145525, 0.22158958394114528, 0.11426186577238354, 0.07567082107655757, 0.0763783983232957, 0.13398170758457445, 0.17877341757538, 0.0819510085836717, 0.17267192588031569, 0.07292184050251782, 0.16221902616846298, 0.10925273786845255, 0.13851094617970783, 0.15434926311664912, 0.13865144750224134, 0.05324378423499725, 0.04423132297609527, 0.061182806535462535, 0.059823965019731244, 0.0957538768885309, 0.11801552301495558, 0.1106116350626356, 0.056180301697702884, 0.08375398046479422, 0.09273806283022161, 0.11606167057460623, 0.029945615018095683, 0.10106764650263056, 0.07215501521093366, 0.060520932215053426, 0.07630419077493561, 0.0942252715807957, 0.035452429728539944, 0.05730219485486994, 0.06799019449728588, 0.11697952001746909, 0.08069929154252438, 0.14382143276972084, 0.054268849565530734, 0.028666463999345925, 0.06214764689941301, 0.051760276801137324, 0.06045539764955138, 0.0872459174619719, 0.09537660708768854, 0.11373705861184262, 0.10621436780348036, 0.040090318800668524, 0.06198535030226353, 0.04248781356523369, 0.05444610310050397, 0.08309415658723274, 0.14610636300184446, 0.07537210055591205, 0.05324013713026652, 0.1337275068742946, 0.03724057744684229, 0.09211613527336271, 0.08938704306457701, 0.10587671142716627, 0.12825253679520127, 0.21222068044047668, 0.17874293301300911, 0.10764569395882344, 0.08270328687150358, 0.02018441431679846, 0.12687179939631382, 0.054338716297845646, 0.13552984759170122, 0.09579856779604103, 0.13452208615558048, 0.04692161557020049, 0.12278203033629763, 0.032500520834204905, 0.1551234632043148, 0.05233560249206392, 0.0970039115357029, 0.05368246957627737, 0.04041387048901264, 0.09285281388269068, 0.12096007994924907, 0.06402564444651496, 0.12206493353694155, 0.10187952870865775, 0.06145361496232001, 0.12948756996851016, 0.0738399732022512, 0.08136666137203509, 0.05669844062252438, 0.05090471454205254, 0.07677520472569026, 0.12181221091728994, 0.06327846577423803, 0.06633743410075167, 0.08645404988404495, 0.09878359614865206, 0.08926826128917184, 0.09047148254579575, 0.05363478251638937, 0.06978468281512935, 0.10887802357724985, 0.15451454003221285, 0.15178390655757265, 0.0698345678425425, 0.06462451835289153, 0.06494172079339056, 0.1275545992458825, 0.09497923478480955, 0.0871536224409457, 0.07159067415871337, 0.08938291937795212, 0.06905940619961491, 0.144707735892458, 0.08807196156733203, 0.20194324840004932, 0.1555229115686319, 0.05493760334687623, 0.10648964904141765, 0.06514557294494819, 0.20536835091502825, 0.1515310894747239, 0.09244853363773785, 0.07517527418592922, 0.09189248550864357, 0.15924999657952674, 0.12947406791429958, 0.0810450820846094, 0.10243168847867395, 0.07106730591350624, 0.15694480559835056, 0.12906506867151396, 0.07961132444770234, 0.15326014733824242, 0.054276261175958046, 0.1098418856091264, 0.06848304735258458, 0.05737779989848853, 0.06110349453326559, 0.05411189536518963, 0.05762280796279181, 0.19917199685827405, 0.07774303454024055, 0.09462870220882191, 0.0745203876670804, 0.052420072505853885, 0.0908466432163643, 0.07133556906444347, 0.1126047065075703, 0.08319761355598444, 0.17195085756308026, 0.13624344841269292, 0.05342378966616211, 0.04545932019867709, 0.11132033604374579, 0.10256915819819656, 0.09619361660916147, 0.07344693528166775, 0.07691320896061778, 0.148945338855596, 0.08689729044193215, 0.053502297920394765, 0.1240163352659053, 0.07243436207434552, 0.11663483949446729]
0.10089198763399734
Making ranges
torch.Size([25247, 2])
We keep 5.52e+06/3.09e+08 =  1% of the original kernel matrix.

torch.Size([2845, 2])
We keep 1.26e+05/1.61e+06 =  7% of the original kernel matrix.

torch.Size([9207, 2])
We keep 8.78e+05/2.23e+07 =  3% of the original kernel matrix.

torch.Size([19830, 2])
We keep 6.05e+06/2.02e+08 =  2% of the original kernel matrix.

torch.Size([22810, 2])
We keep 5.28e+06/2.50e+08 =  2% of the original kernel matrix.

torch.Size([3909, 2])
We keep 2.53e+05/4.10e+06 =  6% of the original kernel matrix.

torch.Size([10419, 2])
We keep 1.17e+06/3.56e+07 =  3% of the original kernel matrix.

torch.Size([17131, 2])
We keep 8.53e+06/2.16e+08 =  3% of the original kernel matrix.

torch.Size([20846, 2])
We keep 5.37e+06/2.59e+08 =  2% of the original kernel matrix.

torch.Size([18027, 2])
We keep 7.98e+06/1.37e+08 =  5% of the original kernel matrix.

torch.Size([21398, 2])
We keep 4.36e+06/2.06e+08 =  2% of the original kernel matrix.

torch.Size([43901, 2])
We keep 1.80e+07/1.02e+09 =  1% of the original kernel matrix.

torch.Size([33595, 2])
We keep 9.95e+06/5.62e+08 =  1% of the original kernel matrix.

torch.Size([52427, 2])
We keep 2.62e+07/1.40e+09 =  1% of the original kernel matrix.

torch.Size([36367, 2])
We keep 1.14e+07/6.57e+08 =  1% of the original kernel matrix.

torch.Size([44053, 2])
We keep 1.63e+07/1.00e+09 =  1% of the original kernel matrix.

torch.Size([33777, 2])
We keep 9.79e+06/5.56e+08 =  1% of the original kernel matrix.

torch.Size([11841, 2])
We keep 2.56e+06/5.91e+07 =  4% of the original kernel matrix.

torch.Size([17175, 2])
We keep 3.15e+06/1.35e+08 =  2% of the original kernel matrix.

torch.Size([29746, 2])
We keep 3.51e+07/6.25e+08 =  5% of the original kernel matrix.

torch.Size([27307, 2])
We keep 7.89e+06/4.40e+08 =  1% of the original kernel matrix.

torch.Size([10849, 2])
We keep 1.75e+06/4.14e+07 =  4% of the original kernel matrix.

torch.Size([16548, 2])
We keep 2.76e+06/1.13e+08 =  2% of the original kernel matrix.

torch.Size([152595, 2])
We keep 3.04e+08/1.47e+10 =  2% of the original kernel matrix.

torch.Size([62731, 2])
We keep 3.11e+07/2.13e+09 =  1% of the original kernel matrix.

torch.Size([11396, 2])
We keep 1.60e+06/4.46e+07 =  3% of the original kernel matrix.

torch.Size([16783, 2])
We keep 2.90e+06/1.17e+08 =  2% of the original kernel matrix.

torch.Size([369612, 2])
We keep 8.51e+08/7.12e+10 =  1% of the original kernel matrix.

torch.Size([99896, 2])
We keep 6.33e+07/4.69e+09 =  1% of the original kernel matrix.

torch.Size([25768, 2])
We keep 7.50e+06/3.55e+08 =  2% of the original kernel matrix.

torch.Size([25821, 2])
We keep 6.39e+06/3.31e+08 =  1% of the original kernel matrix.

torch.Size([47194, 2])
We keep 2.40e+07/1.24e+09 =  1% of the original kernel matrix.

torch.Size([34633, 2])
We keep 1.08e+07/6.20e+08 =  1% of the original kernel matrix.

torch.Size([60233, 2])
We keep 3.88e+07/1.92e+09 =  2% of the original kernel matrix.

torch.Size([38805, 2])
We keep 1.28e+07/7.71e+08 =  1% of the original kernel matrix.

torch.Size([21144, 2])
We keep 6.01e+06/2.25e+08 =  2% of the original kernel matrix.

torch.Size([23328, 2])
We keep 5.42e+06/2.64e+08 =  2% of the original kernel matrix.

torch.Size([123143, 2])
We keep 2.01e+08/1.10e+10 =  1% of the original kernel matrix.

torch.Size([56471, 2])
We keep 2.81e+07/1.85e+09 =  1% of the original kernel matrix.

torch.Size([89776, 2])
We keep 9.79e+07/4.16e+09 =  2% of the original kernel matrix.

torch.Size([47507, 2])
We keep 1.77e+07/1.13e+09 =  1% of the original kernel matrix.

torch.Size([38072, 2])
We keep 3.05e+07/1.21e+09 =  2% of the original kernel matrix.

torch.Size([30174, 2])
We keep 1.08e+07/6.11e+08 =  1% of the original kernel matrix.

torch.Size([260906, 2])
We keep 4.70e+08/3.69e+10 =  1% of the original kernel matrix.

torch.Size([85034, 2])
We keep 4.49e+07/3.38e+09 =  1% of the original kernel matrix.

torch.Size([12884, 2])
We keep 1.99e+06/5.86e+07 =  3% of the original kernel matrix.

torch.Size([17917, 2])
We keep 3.18e+06/1.35e+08 =  2% of the original kernel matrix.

torch.Size([47822, 2])
We keep 5.22e+07/1.34e+09 =  3% of the original kernel matrix.

torch.Size([34663, 2])
We keep 1.13e+07/6.43e+08 =  1% of the original kernel matrix.

torch.Size([5781, 2])
We keep 5.14e+06/1.51e+07 = 34% of the original kernel matrix.

torch.Size([12090, 2])
We keep 1.81e+06/6.83e+07 =  2% of the original kernel matrix.

torch.Size([91410, 2])
We keep 6.31e+07/4.25e+09 =  1% of the original kernel matrix.

torch.Size([47942, 2])
We keep 1.78e+07/1.15e+09 =  1% of the original kernel matrix.

torch.Size([64990, 2])
We keep 4.31e+07/2.32e+09 =  1% of the original kernel matrix.

torch.Size([40188, 2])
We keep 1.41e+07/8.47e+08 =  1% of the original kernel matrix.

torch.Size([11264, 2])
We keep 1.97e+06/4.50e+07 =  4% of the original kernel matrix.

torch.Size([16732, 2])
We keep 2.85e+06/1.18e+08 =  2% of the original kernel matrix.

torch.Size([24803, 2])
We keep 1.91e+07/3.64e+08 =  5% of the original kernel matrix.

torch.Size([25046, 2])
We keep 6.15e+06/3.36e+08 =  1% of the original kernel matrix.

torch.Size([1431326, 2])
We keep 6.36e+09/9.40e+11 =  0% of the original kernel matrix.

torch.Size([206391, 2])
We keep 2.03e+08/1.70e+10 =  1% of the original kernel matrix.

torch.Size([10863, 2])
We keep 3.40e+06/6.55e+07 =  5% of the original kernel matrix.

torch.Size([16274, 2])
We keep 3.28e+06/1.42e+08 =  2% of the original kernel matrix.

torch.Size([576294, 2])
We keep 2.56e+09/2.33e+11 =  1% of the original kernel matrix.

torch.Size([128210, 2])
We keep 1.08e+08/8.49e+09 =  1% of the original kernel matrix.

torch.Size([28524, 2])
We keep 1.78e+07/4.75e+08 =  3% of the original kernel matrix.

torch.Size([27025, 2])
We keep 7.06e+06/3.83e+08 =  1% of the original kernel matrix.

torch.Size([12406, 2])
We keep 1.73e+06/4.86e+07 =  3% of the original kernel matrix.

torch.Size([17604, 2])
We keep 2.88e+06/1.23e+08 =  2% of the original kernel matrix.

torch.Size([20360, 2])
We keep 6.83e+06/2.51e+08 =  2% of the original kernel matrix.

torch.Size([22981, 2])
We keep 5.75e+06/2.79e+08 =  2% of the original kernel matrix.

torch.Size([95013, 2])
We keep 1.47e+08/5.85e+09 =  2% of the original kernel matrix.

torch.Size([48956, 2])
We keep 2.07e+07/1.34e+09 =  1% of the original kernel matrix.

torch.Size([103665, 2])
We keep 9.74e+07/5.53e+09 =  1% of the original kernel matrix.

torch.Size([51278, 2])
We keep 2.05e+07/1.31e+09 =  1% of the original kernel matrix.

torch.Size([2219, 2])
We keep 8.43e+04/1.00e+06 =  8% of the original kernel matrix.

torch.Size([8520, 2])
We keep 7.52e+05/1.76e+07 =  4% of the original kernel matrix.

torch.Size([6202, 2])
We keep 5.33e+05/9.47e+06 =  5% of the original kernel matrix.

torch.Size([12616, 2])
We keep 1.62e+06/5.41e+07 =  2% of the original kernel matrix.

torch.Size([3539, 2])
We keep 2.26e+05/3.12e+06 =  7% of the original kernel matrix.

torch.Size([9997, 2])
We keep 1.10e+06/3.11e+07 =  3% of the original kernel matrix.

torch.Size([6011, 2])
We keep 4.38e+05/8.91e+06 =  4% of the original kernel matrix.

torch.Size([12490, 2])
We keep 1.57e+06/5.25e+07 =  2% of the original kernel matrix.

torch.Size([3418, 2])
We keep 4.40e+05/5.18e+06 =  8% of the original kernel matrix.

torch.Size([8867, 2])
We keep 1.32e+06/4.00e+07 =  3% of the original kernel matrix.

torch.Size([1471, 2])
We keep 4.20e+04/4.21e+05 =  9% of the original kernel matrix.

torch.Size([7180, 2])
We keep 5.61e+05/1.14e+07 =  4% of the original kernel matrix.

torch.Size([6957, 2])
We keep 7.12e+05/1.43e+07 =  4% of the original kernel matrix.

torch.Size([13051, 2])
We keep 1.91e+06/6.64e+07 =  2% of the original kernel matrix.

torch.Size([2228, 2])
We keep 6.88e+04/8.84e+05 =  7% of the original kernel matrix.

torch.Size([8559, 2])
We keep 6.87e+05/1.65e+07 =  4% of the original kernel matrix.

torch.Size([2314, 2])
We keep 8.36e+04/1.08e+06 =  7% of the original kernel matrix.

torch.Size([8493, 2])
We keep 7.64e+05/1.83e+07 =  4% of the original kernel matrix.

torch.Size([3687, 2])
We keep 2.16e+05/3.32e+06 =  6% of the original kernel matrix.

torch.Size([10180, 2])
We keep 1.11e+06/3.20e+07 =  3% of the original kernel matrix.

torch.Size([6361, 2])
We keep 5.42e+05/1.01e+07 =  5% of the original kernel matrix.

torch.Size([12775, 2])
We keep 1.66e+06/5.60e+07 =  2% of the original kernel matrix.

torch.Size([4691, 2])
We keep 4.38e+05/6.73e+06 =  6% of the original kernel matrix.

torch.Size([11201, 2])
We keep 1.43e+06/4.56e+07 =  3% of the original kernel matrix.

torch.Size([15193, 2])
We keep 3.38e+06/9.68e+07 =  3% of the original kernel matrix.

torch.Size([19485, 2])
We keep 3.94e+06/1.73e+08 =  2% of the original kernel matrix.

torch.Size([13943, 2])
We keep 2.65e+06/7.57e+07 =  3% of the original kernel matrix.

torch.Size([18789, 2])
We keep 3.52e+06/1.53e+08 =  2% of the original kernel matrix.

torch.Size([2727, 2])
We keep 1.40e+05/1.73e+06 =  8% of the original kernel matrix.

torch.Size([8974, 2])
We keep 8.98e+05/2.31e+07 =  3% of the original kernel matrix.

torch.Size([8761, 2])
We keep 1.87e+06/3.66e+07 =  5% of the original kernel matrix.

torch.Size([14488, 2])
We keep 2.68e+06/1.06e+08 =  2% of the original kernel matrix.

torch.Size([5860, 2])
We keep 5.75e+05/1.04e+07 =  5% of the original kernel matrix.

torch.Size([12226, 2])
We keep 1.70e+06/5.66e+07 =  3% of the original kernel matrix.

torch.Size([5964, 2])
We keep 4.59e+05/8.89e+06 =  5% of the original kernel matrix.

torch.Size([12422, 2])
We keep 1.59e+06/5.24e+07 =  3% of the original kernel matrix.

torch.Size([9352, 2])
We keep 1.84e+06/3.67e+07 =  5% of the original kernel matrix.

torch.Size([15059, 2])
We keep 2.68e+06/1.06e+08 =  2% of the original kernel matrix.

torch.Size([4043, 2])
We keep 2.72e+05/3.91e+06 =  6% of the original kernel matrix.

torch.Size([10736, 2])
We keep 1.19e+06/3.48e+07 =  3% of the original kernel matrix.

torch.Size([8486, 2])
We keep 8.52e+05/1.92e+07 =  4% of the original kernel matrix.

torch.Size([14529, 2])
We keep 2.09e+06/7.70e+07 =  2% of the original kernel matrix.

torch.Size([4923, 2])
We keep 4.00e+05/6.84e+06 =  5% of the original kernel matrix.

torch.Size([11541, 2])
We keep 1.45e+06/4.60e+07 =  3% of the original kernel matrix.

torch.Size([3693, 2])
We keep 2.61e+05/3.71e+06 =  7% of the original kernel matrix.

torch.Size([9992, 2])
We keep 1.16e+06/3.39e+07 =  3% of the original kernel matrix.

torch.Size([7458, 2])
We keep 6.31e+05/1.33e+07 =  4% of the original kernel matrix.

torch.Size([13780, 2])
We keep 1.80e+06/6.42e+07 =  2% of the original kernel matrix.

torch.Size([2335, 2])
We keep 1.10e+05/1.36e+06 =  8% of the original kernel matrix.

torch.Size([8573, 2])
We keep 8.22e+05/2.05e+07 =  4% of the original kernel matrix.

torch.Size([6863, 2])
We keep 8.05e+05/1.49e+07 =  5% of the original kernel matrix.

torch.Size([13157, 2])
We keep 1.92e+06/6.78e+07 =  2% of the original kernel matrix.

torch.Size([13029, 2])
We keep 1.98e+06/5.81e+07 =  3% of the original kernel matrix.

torch.Size([18021, 2])
We keep 3.18e+06/1.34e+08 =  2% of the original kernel matrix.

torch.Size([4630, 2])
We keep 3.45e+05/5.82e+06 =  5% of the original kernel matrix.

torch.Size([11130, 2])
We keep 1.36e+06/4.24e+07 =  3% of the original kernel matrix.

torch.Size([3722, 2])
We keep 2.48e+05/3.53e+06 =  7% of the original kernel matrix.

torch.Size([10243, 2])
We keep 1.15e+06/3.30e+07 =  3% of the original kernel matrix.

torch.Size([5668, 2])
We keep 4.01e+05/7.31e+06 =  5% of the original kernel matrix.

torch.Size([12217, 2])
We keep 1.47e+06/4.76e+07 =  3% of the original kernel matrix.

torch.Size([8437, 2])
We keep 9.06e+05/2.03e+07 =  4% of the original kernel matrix.

torch.Size([14496, 2])
We keep 2.13e+06/7.92e+07 =  2% of the original kernel matrix.

torch.Size([8057, 2])
We keep 7.85e+05/1.81e+07 =  4% of the original kernel matrix.

torch.Size([14176, 2])
We keep 2.02e+06/7.48e+07 =  2% of the original kernel matrix.

torch.Size([5108, 2])
We keep 4.83e+05/7.41e+06 =  6% of the original kernel matrix.

torch.Size([11537, 2])
We keep 1.51e+06/4.79e+07 =  3% of the original kernel matrix.

torch.Size([8743, 2])
We keep 1.86e+06/2.88e+07 =  6% of the original kernel matrix.

torch.Size([14634, 2])
We keep 2.46e+06/9.45e+07 =  2% of the original kernel matrix.

torch.Size([7662, 2])
We keep 6.90e+05/1.54e+07 =  4% of the original kernel matrix.

torch.Size([13874, 2])
We keep 1.94e+06/6.89e+07 =  2% of the original kernel matrix.

torch.Size([6912, 2])
We keep 5.52e+05/1.18e+07 =  4% of the original kernel matrix.

torch.Size([13203, 2])
We keep 1.74e+06/6.04e+07 =  2% of the original kernel matrix.

torch.Size([5120, 2])
We keep 4.59e+05/7.47e+06 =  6% of the original kernel matrix.

torch.Size([11451, 2])
We keep 1.50e+06/4.81e+07 =  3% of the original kernel matrix.

torch.Size([5883, 2])
We keep 6.04e+05/1.05e+07 =  5% of the original kernel matrix.

torch.Size([12246, 2])
We keep 1.68e+06/5.70e+07 =  2% of the original kernel matrix.

torch.Size([6876, 2])
We keep 7.90e+05/1.43e+07 =  5% of the original kernel matrix.

torch.Size([13147, 2])
We keep 1.88e+06/6.64e+07 =  2% of the original kernel matrix.

torch.Size([1451, 2])
We keep 3.65e+04/3.93e+05 =  9% of the original kernel matrix.

torch.Size([7294, 2])
We keep 5.45e+05/1.10e+07 =  4% of the original kernel matrix.

torch.Size([2424, 2])
We keep 1.41e+05/1.57e+06 =  9% of the original kernel matrix.

torch.Size([8492, 2])
We keep 8.78e+05/2.20e+07 =  3% of the original kernel matrix.

torch.Size([5809, 2])
We keep 5.05e+05/8.76e+06 =  5% of the original kernel matrix.

torch.Size([12300, 2])
We keep 1.53e+06/5.21e+07 =  2% of the original kernel matrix.

torch.Size([20492, 2])
We keep 5.79e+06/2.23e+08 =  2% of the original kernel matrix.

torch.Size([23315, 2])
We keep 5.51e+06/2.63e+08 =  2% of the original kernel matrix.

torch.Size([4586, 2])
We keep 4.06e+05/6.35e+06 =  6% of the original kernel matrix.

torch.Size([10904, 2])
We keep 1.40e+06/4.43e+07 =  3% of the original kernel matrix.

torch.Size([2902, 2])
We keep 2.17e+05/2.61e+06 =  8% of the original kernel matrix.

torch.Size([8806, 2])
We keep 1.03e+06/2.84e+07 =  3% of the original kernel matrix.

torch.Size([5153, 2])
We keep 5.37e+05/8.71e+06 =  6% of the original kernel matrix.

torch.Size([11537, 2])
We keep 1.59e+06/5.19e+07 =  3% of the original kernel matrix.

torch.Size([4110, 2])
We keep 2.59e+05/3.94e+06 =  6% of the original kernel matrix.

torch.Size([10578, 2])
We keep 1.19e+06/3.49e+07 =  3% of the original kernel matrix.

torch.Size([2588, 2])
We keep 1.35e+05/1.71e+06 =  7% of the original kernel matrix.

torch.Size([8834, 2])
We keep 8.95e+05/2.30e+07 =  3% of the original kernel matrix.

torch.Size([2807, 2])
We keep 1.11e+05/1.52e+06 =  7% of the original kernel matrix.

torch.Size([9304, 2])
We keep 8.51e+05/2.17e+07 =  3% of the original kernel matrix.

torch.Size([4410, 2])
We keep 2.90e+05/4.42e+06 =  6% of the original kernel matrix.

torch.Size([10999, 2])
We keep 1.25e+06/3.70e+07 =  3% of the original kernel matrix.

torch.Size([2540, 2])
We keep 1.07e+05/1.39e+06 =  7% of the original kernel matrix.

torch.Size([8905, 2])
We keep 8.31e+05/2.08e+07 =  4% of the original kernel matrix.

torch.Size([6157, 2])
We keep 5.62e+05/1.01e+07 =  5% of the original kernel matrix.

torch.Size([12519, 2])
We keep 1.67e+06/5.59e+07 =  2% of the original kernel matrix.

torch.Size([5369, 2])
We keep 5.10e+05/7.86e+06 =  6% of the original kernel matrix.

torch.Size([11892, 2])
We keep 1.52e+06/4.93e+07 =  3% of the original kernel matrix.

torch.Size([7952, 2])
We keep 9.20e+05/1.77e+07 =  5% of the original kernel matrix.

torch.Size([14158, 2])
We keep 1.99e+06/7.39e+07 =  2% of the original kernel matrix.

torch.Size([3677, 2])
We keep 2.36e+05/3.37e+06 =  6% of the original kernel matrix.

torch.Size([10138, 2])
We keep 1.12e+06/3.23e+07 =  3% of the original kernel matrix.

torch.Size([3346, 2])
We keep 1.91e+05/2.79e+06 =  6% of the original kernel matrix.

torch.Size([9811, 2])
We keep 1.06e+06/2.94e+07 =  3% of the original kernel matrix.

torch.Size([3692, 2])
We keep 2.52e+05/3.61e+06 =  6% of the original kernel matrix.

torch.Size([10196, 2])
We keep 1.16e+06/3.34e+07 =  3% of the original kernel matrix.

torch.Size([7533, 2])
We keep 9.62e+05/1.89e+07 =  5% of the original kernel matrix.

torch.Size([13576, 2])
We keep 2.11e+06/7.65e+07 =  2% of the original kernel matrix.

torch.Size([7070, 2])
We keep 6.55e+05/1.31e+07 =  5% of the original kernel matrix.

torch.Size([13340, 2])
We keep 1.81e+06/6.36e+07 =  2% of the original kernel matrix.

torch.Size([4572, 2])
We keep 2.75e+05/4.44e+06 =  6% of the original kernel matrix.

torch.Size([11131, 2])
We keep 1.25e+06/3.71e+07 =  3% of the original kernel matrix.

torch.Size([7852, 2])
We keep 1.62e+06/3.15e+07 =  5% of the original kernel matrix.

torch.Size([13524, 2])
We keep 2.55e+06/9.87e+07 =  2% of the original kernel matrix.

torch.Size([8205, 2])
We keep 1.07e+06/2.06e+07 =  5% of the original kernel matrix.

torch.Size([14371, 2])
We keep 2.18e+06/7.98e+07 =  2% of the original kernel matrix.

torch.Size([5010, 2])
We keep 3.20e+05/5.51e+06 =  5% of the original kernel matrix.

torch.Size([11618, 2])
We keep 1.33e+06/4.13e+07 =  3% of the original kernel matrix.

torch.Size([9964, 2])
We keep 1.45e+06/3.22e+07 =  4% of the original kernel matrix.

torch.Size([15706, 2])
We keep 2.55e+06/9.98e+07 =  2% of the original kernel matrix.

torch.Size([5752, 2])
We keep 4.15e+05/7.94e+06 =  5% of the original kernel matrix.

torch.Size([12266, 2])
We keep 1.52e+06/4.96e+07 =  3% of the original kernel matrix.

torch.Size([8800, 2])
We keep 9.39e+05/2.24e+07 =  4% of the original kernel matrix.

torch.Size([14793, 2])
We keep 2.20e+06/8.32e+07 =  2% of the original kernel matrix.

torch.Size([2535, 2])
We keep 1.60e+05/1.80e+06 =  8% of the original kernel matrix.

torch.Size([8541, 2])
We keep 9.15e+05/2.36e+07 =  3% of the original kernel matrix.

torch.Size([6367, 2])
We keep 6.73e+05/1.23e+07 =  5% of the original kernel matrix.

torch.Size([12732, 2])
We keep 1.78e+06/6.16e+07 =  2% of the original kernel matrix.

torch.Size([3114, 2])
We keep 1.82e+05/2.60e+06 =  6% of the original kernel matrix.

torch.Size([9418, 2])
We keep 1.01e+06/2.84e+07 =  3% of the original kernel matrix.

torch.Size([7333, 2])
We keep 6.73e+05/1.43e+07 =  4% of the original kernel matrix.

torch.Size([13510, 2])
We keep 1.86e+06/6.65e+07 =  2% of the original kernel matrix.

torch.Size([2596, 2])
We keep 1.22e+05/1.58e+06 =  7% of the original kernel matrix.

torch.Size([8758, 2])
We keep 8.47e+05/2.21e+07 =  3% of the original kernel matrix.

torch.Size([3703, 2])
We keep 2.36e+05/3.43e+06 =  6% of the original kernel matrix.

torch.Size([10146, 2])
We keep 1.14e+06/3.26e+07 =  3% of the original kernel matrix.

torch.Size([3161, 2])
We keep 1.99e+05/2.61e+06 =  7% of the original kernel matrix.

torch.Size([9550, 2])
We keep 1.05e+06/2.84e+07 =  3% of the original kernel matrix.

torch.Size([3801, 2])
We keep 2.31e+05/3.49e+06 =  6% of the original kernel matrix.

torch.Size([10326, 2])
We keep 1.12e+06/3.29e+07 =  3% of the original kernel matrix.

torch.Size([2547, 2])
We keep 1.36e+05/1.51e+06 =  9% of the original kernel matrix.

torch.Size([8795, 2])
We keep 8.53e+05/2.16e+07 =  3% of the original kernel matrix.

torch.Size([3674, 2])
We keep 2.76e+05/3.53e+06 =  7% of the original kernel matrix.

torch.Size([10054, 2])
We keep 1.17e+06/3.30e+07 =  3% of the original kernel matrix.

torch.Size([5521, 2])
We keep 4.75e+05/8.42e+06 =  5% of the original kernel matrix.

torch.Size([12049, 2])
We keep 1.54e+06/5.10e+07 =  3% of the original kernel matrix.

torch.Size([7259, 2])
We keep 9.31e+05/1.67e+07 =  5% of the original kernel matrix.

torch.Size([13537, 2])
We keep 2.01e+06/7.19e+07 =  2% of the original kernel matrix.

torch.Size([4535, 2])
We keep 5.58e+05/8.33e+06 =  6% of the original kernel matrix.

torch.Size([10721, 2])
We keep 1.58e+06/5.08e+07 =  3% of the original kernel matrix.

torch.Size([4883, 2])
We keep 3.73e+05/6.50e+06 =  5% of the original kernel matrix.

torch.Size([11437, 2])
We keep 1.40e+06/4.48e+07 =  3% of the original kernel matrix.

torch.Size([1754, 2])
We keep 5.45e+04/6.26e+05 =  8% of the original kernel matrix.

torch.Size([7686, 2])
We keep 6.41e+05/1.39e+07 =  4% of the original kernel matrix.

torch.Size([9682, 2])
We keep 1.34e+06/2.97e+07 =  4% of the original kernel matrix.

torch.Size([15438, 2])
We keep 2.46e+06/9.58e+07 =  2% of the original kernel matrix.

torch.Size([3272, 2])
We keep 1.70e+05/2.29e+06 =  7% of the original kernel matrix.

torch.Size([9774, 2])
We keep 9.93e+05/2.66e+07 =  3% of the original kernel matrix.

torch.Size([10772, 2])
We keep 1.43e+06/3.78e+07 =  3% of the original kernel matrix.

torch.Size([16319, 2])
We keep 2.69e+06/1.08e+08 =  2% of the original kernel matrix.

torch.Size([2976, 2])
We keep 1.74e+05/2.18e+06 =  7% of the original kernel matrix.

torch.Size([9341, 2])
We keep 9.83e+05/2.59e+07 =  3% of the original kernel matrix.

torch.Size([3590, 2])
We keep 2.79e+05/3.84e+06 =  7% of the original kernel matrix.

torch.Size([9868, 2])
We keep 1.18e+06/3.45e+07 =  3% of the original kernel matrix.

torch.Size([3844, 2])
We keep 2.46e+05/3.82e+06 =  6% of the original kernel matrix.

torch.Size([10377, 2])
We keep 1.16e+06/3.44e+07 =  3% of the original kernel matrix.

torch.Size([3255, 2])
We keep 1.55e+05/2.18e+06 =  7% of the original kernel matrix.

torch.Size([9679, 2])
We keep 9.45e+05/2.60e+07 =  3% of the original kernel matrix.

torch.Size([3936, 2])
We keep 2.33e+05/3.62e+06 =  6% of the original kernel matrix.

torch.Size([10436, 2])
We keep 1.15e+06/3.34e+07 =  3% of the original kernel matrix.

torch.Size([2655, 2])
We keep 1.41e+05/1.79e+06 =  7% of the original kernel matrix.

torch.Size([8828, 2])
We keep 9.10e+05/2.35e+07 =  3% of the original kernel matrix.

torch.Size([6664, 2])
We keep 5.32e+05/1.12e+07 =  4% of the original kernel matrix.

torch.Size([12990, 2])
We keep 1.71e+06/5.89e+07 =  2% of the original kernel matrix.

torch.Size([10617, 2])
We keep 1.48e+06/3.64e+07 =  4% of the original kernel matrix.

torch.Size([16305, 2])
We keep 2.68e+06/1.06e+08 =  2% of the original kernel matrix.

torch.Size([4304, 2])
We keep 2.76e+05/4.31e+06 =  6% of the original kernel matrix.

torch.Size([10938, 2])
We keep 1.23e+06/3.65e+07 =  3% of the original kernel matrix.

torch.Size([12023, 2])
We keep 1.78e+06/4.98e+07 =  3% of the original kernel matrix.

torch.Size([17281, 2])
We keep 2.98e+06/1.24e+08 =  2% of the original kernel matrix.

torch.Size([3241, 2])
We keep 1.96e+05/2.55e+06 =  7% of the original kernel matrix.

torch.Size([9731, 2])
We keep 1.02e+06/2.81e+07 =  3% of the original kernel matrix.

torch.Size([4831, 2])
We keep 4.05e+05/6.44e+06 =  6% of the original kernel matrix.

torch.Size([11429, 2])
We keep 1.42e+06/4.46e+07 =  3% of the original kernel matrix.

torch.Size([2540, 2])
We keep 1.39e+05/1.56e+06 =  8% of the original kernel matrix.

torch.Size([8719, 2])
We keep 8.77e+05/2.19e+07 =  3% of the original kernel matrix.

torch.Size([6055, 2])
We keep 1.07e+06/1.14e+07 =  9% of the original kernel matrix.

torch.Size([12495, 2])
We keep 1.66e+06/5.94e+07 =  2% of the original kernel matrix.

torch.Size([3257, 2])
We keep 1.72e+05/2.40e+06 =  7% of the original kernel matrix.

torch.Size([9671, 2])
We keep 9.94e+05/2.72e+07 =  3% of the original kernel matrix.

torch.Size([3288, 2])
We keep 1.84e+05/2.59e+06 =  7% of the original kernel matrix.

torch.Size([9749, 2])
We keep 1.02e+06/2.83e+07 =  3% of the original kernel matrix.

torch.Size([2759, 2])
We keep 1.36e+05/1.71e+06 =  7% of the original kernel matrix.

torch.Size([9013, 2])
We keep 8.93e+05/2.30e+07 =  3% of the original kernel matrix.

torch.Size([4110, 2])
We keep 2.91e+05/4.73e+06 =  6% of the original kernel matrix.

torch.Size([10512, 2])
We keep 1.26e+06/3.83e+07 =  3% of the original kernel matrix.

torch.Size([2769, 2])
We keep 1.41e+05/1.95e+06 =  7% of the original kernel matrix.

torch.Size([9096, 2])
We keep 9.14e+05/2.46e+07 =  3% of the original kernel matrix.

torch.Size([3891, 2])
We keep 2.90e+05/4.09e+06 =  7% of the original kernel matrix.

torch.Size([10134, 2])
We keep 1.21e+06/3.56e+07 =  3% of the original kernel matrix.

torch.Size([2826, 2])
We keep 1.22e+05/1.68e+06 =  7% of the original kernel matrix.

torch.Size([9130, 2])
We keep 8.62e+05/2.28e+07 =  3% of the original kernel matrix.

torch.Size([12861, 2])
We keep 3.03e+06/8.21e+07 =  3% of the original kernel matrix.

torch.Size([17741, 2])
We keep 3.60e+06/1.59e+08 =  2% of the original kernel matrix.

torch.Size([8025, 2])
We keep 1.21e+06/2.37e+07 =  5% of the original kernel matrix.

torch.Size([14109, 2])
We keep 2.30e+06/8.56e+07 =  2% of the original kernel matrix.

torch.Size([5944, 2])
We keep 5.68e+05/9.75e+06 =  5% of the original kernel matrix.

torch.Size([12338, 2])
We keep 1.65e+06/5.49e+07 =  3% of the original kernel matrix.

torch.Size([3032, 2])
We keep 1.33e+05/1.97e+06 =  6% of the original kernel matrix.

torch.Size([9425, 2])
We keep 9.37e+05/2.47e+07 =  3% of the original kernel matrix.

torch.Size([3427, 2])
We keep 2.01e+05/2.66e+06 =  7% of the original kernel matrix.

torch.Size([9953, 2])
We keep 1.04e+06/2.87e+07 =  3% of the original kernel matrix.

torch.Size([4493, 2])
We keep 3.48e+05/5.50e+06 =  6% of the original kernel matrix.

torch.Size([11158, 2])
We keep 1.36e+06/4.12e+07 =  3% of the original kernel matrix.

torch.Size([3275, 2])
We keep 1.55e+05/2.03e+06 =  7% of the original kernel matrix.

torch.Size([9827, 2])
We keep 9.22e+05/2.51e+07 =  3% of the original kernel matrix.

torch.Size([4211, 2])
We keep 4.66e+05/6.64e+06 =  7% of the original kernel matrix.

torch.Size([10395, 2])
We keep 1.44e+06/4.53e+07 =  3% of the original kernel matrix.

torch.Size([4338, 2])
We keep 3.60e+05/5.39e+06 =  6% of the original kernel matrix.

torch.Size([10664, 2])
We keep 1.34e+06/4.08e+07 =  3% of the original kernel matrix.

torch.Size([3720, 2])
We keep 2.65e+05/3.81e+06 =  6% of the original kernel matrix.

torch.Size([10115, 2])
We keep 1.17e+06/3.43e+07 =  3% of the original kernel matrix.

torch.Size([7144, 2])
We keep 6.81e+05/1.37e+07 =  4% of the original kernel matrix.

torch.Size([13495, 2])
We keep 1.86e+06/6.51e+07 =  2% of the original kernel matrix.

torch.Size([10617, 2])
We keep 2.29e+06/5.24e+07 =  4% of the original kernel matrix.

torch.Size([16038, 2])
We keep 3.08e+06/1.27e+08 =  2% of the original kernel matrix.

torch.Size([4924, 2])
We keep 4.43e+05/6.99e+06 =  6% of the original kernel matrix.

torch.Size([11264, 2])
We keep 1.46e+06/4.65e+07 =  3% of the original kernel matrix.

torch.Size([17769, 2])
We keep 7.20e+06/2.32e+08 =  3% of the original kernel matrix.

torch.Size([21395, 2])
We keep 5.70e+06/2.68e+08 =  2% of the original kernel matrix.

torch.Size([5116, 2])
We keep 3.24e+05/5.71e+06 =  5% of the original kernel matrix.

torch.Size([11683, 2])
We keep 1.34e+06/4.20e+07 =  3% of the original kernel matrix.

torch.Size([5209, 2])
We keep 5.39e+05/8.25e+06 =  6% of the original kernel matrix.

torch.Size([11532, 2])
We keep 1.54e+06/5.05e+07 =  3% of the original kernel matrix.

torch.Size([4392, 2])
We keep 2.78e+05/4.61e+06 =  6% of the original kernel matrix.

torch.Size([10963, 2])
We keep 1.25e+06/3.78e+07 =  3% of the original kernel matrix.

torch.Size([1261, 2])
We keep 2.68e+04/2.37e+05 = 11% of the original kernel matrix.

torch.Size([6883, 2])
We keep 4.63e+05/8.56e+06 =  5% of the original kernel matrix.

torch.Size([14151, 2])
We keep 3.25e+06/8.23e+07 =  3% of the original kernel matrix.

torch.Size([19239, 2])
We keep 3.72e+06/1.60e+08 =  2% of the original kernel matrix.

torch.Size([3058, 2])
We keep 1.54e+05/2.15e+06 =  7% of the original kernel matrix.

torch.Size([9465, 2])
We keep 9.62e+05/2.58e+07 =  3% of the original kernel matrix.

torch.Size([5463, 2])
We keep 4.38e+05/7.67e+06 =  5% of the original kernel matrix.

torch.Size([11969, 2])
We keep 1.50e+06/4.87e+07 =  3% of the original kernel matrix.

torch.Size([2921, 2])
We keep 1.49e+05/1.91e+06 =  7% of the original kernel matrix.

torch.Size([9259, 2])
We keep 9.42e+05/2.43e+07 =  3% of the original kernel matrix.

torch.Size([5546, 2])
We keep 7.09e+05/1.16e+07 =  6% of the original kernel matrix.

torch.Size([11830, 2])
We keep 1.75e+06/5.98e+07 =  2% of the original kernel matrix.

torch.Size([3807, 2])
We keep 2.07e+05/3.10e+06 =  6% of the original kernel matrix.

torch.Size([10491, 2])
We keep 1.09e+06/3.10e+07 =  3% of the original kernel matrix.

torch.Size([3661, 2])
We keep 1.73e+05/2.71e+06 =  6% of the original kernel matrix.

torch.Size([10304, 2])
We keep 1.01e+06/2.89e+07 =  3% of the original kernel matrix.

torch.Size([5722, 2])
We keep 5.14e+05/8.92e+06 =  5% of the original kernel matrix.

torch.Size([12154, 2])
We keep 1.59e+06/5.25e+07 =  3% of the original kernel matrix.

torch.Size([6426, 2])
We keep 5.04e+05/1.05e+07 =  4% of the original kernel matrix.

torch.Size([12737, 2])
We keep 1.67e+06/5.69e+07 =  2% of the original kernel matrix.

torch.Size([2242, 2])
We keep 7.09e+04/8.23e+05 =  8% of the original kernel matrix.

torch.Size([8591, 2])
We keep 6.95e+05/1.60e+07 =  4% of the original kernel matrix.

torch.Size([1590, 2])
We keep 6.29e+04/5.66e+05 = 11% of the original kernel matrix.

torch.Size([7139, 2])
We keep 6.22e+05/1.32e+07 =  4% of the original kernel matrix.

torch.Size([1975, 2])
We keep 7.65e+04/8.63e+05 =  8% of the original kernel matrix.

torch.Size([7971, 2])
We keep 7.11e+05/1.63e+07 =  4% of the original kernel matrix.

torch.Size([9244, 2])
We keep 1.16e+06/2.52e+07 =  4% of the original kernel matrix.

torch.Size([15115, 2])
We keep 2.31e+06/8.82e+07 =  2% of the original kernel matrix.

torch.Size([3379, 2])
We keep 1.68e+05/2.39e+06 =  7% of the original kernel matrix.

torch.Size([9907, 2])
We keep 9.98e+05/2.72e+07 =  3% of the original kernel matrix.

torch.Size([6294, 2])
We keep 5.74e+05/1.08e+07 =  5% of the original kernel matrix.

torch.Size([12804, 2])
We keep 1.70e+06/5.78e+07 =  2% of the original kernel matrix.

torch.Size([11035, 2])
We keep 2.60e+07/1.03e+08 = 25% of the original kernel matrix.

torch.Size([16402, 2])
We keep 3.54e+06/1.79e+08 =  1% of the original kernel matrix.

torch.Size([6934, 2])
We keep 7.32e+05/1.43e+07 =  5% of the original kernel matrix.

torch.Size([13174, 2])
We keep 1.87e+06/6.66e+07 =  2% of the original kernel matrix.

torch.Size([3674, 2])
We keep 2.58e+05/3.85e+06 =  6% of the original kernel matrix.

torch.Size([10100, 2])
We keep 1.18e+06/3.45e+07 =  3% of the original kernel matrix.

torch.Size([7969, 2])
We keep 1.12e+06/1.97e+07 =  5% of the original kernel matrix.

torch.Size([14160, 2])
We keep 2.15e+06/7.82e+07 =  2% of the original kernel matrix.

torch.Size([4981, 2])
We keep 3.74e+05/6.28e+06 =  5% of the original kernel matrix.

torch.Size([11503, 2])
We keep 1.40e+06/4.41e+07 =  3% of the original kernel matrix.

torch.Size([6792, 2])
We keep 6.45e+05/1.28e+07 =  5% of the original kernel matrix.

torch.Size([13150, 2])
We keep 1.78e+06/6.30e+07 =  2% of the original kernel matrix.

torch.Size([3843, 2])
We keep 3.04e+05/4.08e+06 =  7% of the original kernel matrix.

torch.Size([10168, 2])
We keep 1.21e+06/3.55e+07 =  3% of the original kernel matrix.

torch.Size([6957, 2])
We keep 9.61e+05/1.67e+07 =  5% of the original kernel matrix.

torch.Size([13260, 2])
We keep 2.04e+06/7.19e+07 =  2% of the original kernel matrix.

torch.Size([3860, 2])
We keep 2.10e+05/3.33e+06 =  6% of the original kernel matrix.

torch.Size([10454, 2])
We keep 1.11e+06/3.21e+07 =  3% of the original kernel matrix.

torch.Size([6215, 2])
We keep 5.18e+05/9.60e+06 =  5% of the original kernel matrix.

torch.Size([12772, 2])
We keep 1.63e+06/5.45e+07 =  2% of the original kernel matrix.

torch.Size([6416, 2])
We keep 4.91e+05/9.49e+06 =  5% of the original kernel matrix.

torch.Size([12883, 2])
We keep 1.61e+06/5.42e+07 =  2% of the original kernel matrix.

torch.Size([4916, 2])
We keep 3.27e+05/5.48e+06 =  5% of the original kernel matrix.

torch.Size([11524, 2])
We keep 1.34e+06/4.12e+07 =  3% of the original kernel matrix.

torch.Size([5877, 2])
We keep 6.30e+05/1.07e+07 =  5% of the original kernel matrix.

torch.Size([12228, 2])
We keep 1.71e+06/5.76e+07 =  2% of the original kernel matrix.

torch.Size([8628, 2])
We keep 1.15e+06/2.33e+07 =  4% of the original kernel matrix.

torch.Size([14840, 2])
We keep 2.33e+06/8.50e+07 =  2% of the original kernel matrix.

torch.Size([7538, 2])
We keep 8.10e+05/1.61e+07 =  5% of the original kernel matrix.

torch.Size([13828, 2])
We keep 1.97e+06/7.07e+07 =  2% of the original kernel matrix.

torch.Size([4890, 2])
We keep 3.44e+05/5.80e+06 =  5% of the original kernel matrix.

torch.Size([11505, 2])
We keep 1.32e+06/4.23e+07 =  3% of the original kernel matrix.

torch.Size([1722, 2])
We keep 7.40e+04/7.36e+05 = 10% of the original kernel matrix.

torch.Size([7340, 2])
We keep 6.68e+05/1.51e+07 =  4% of the original kernel matrix.

torch.Size([4025, 2])
We keep 2.32e+05/3.80e+06 =  6% of the original kernel matrix.

torch.Size([10485, 2])
We keep 1.16e+06/3.43e+07 =  3% of the original kernel matrix.

torch.Size([8350, 2])
We keep 1.40e+06/2.82e+07 =  4% of the original kernel matrix.

torch.Size([14464, 2])
We keep 2.32e+06/9.33e+07 =  2% of the original kernel matrix.

torch.Size([9885, 2])
We keep 1.90e+06/4.03e+07 =  4% of the original kernel matrix.

torch.Size([15750, 2])
We keep 2.85e+06/1.12e+08 =  2% of the original kernel matrix.

torch.Size([4071, 2])
We keep 2.75e+05/4.24e+06 =  6% of the original kernel matrix.

torch.Size([10594, 2])
We keep 1.23e+06/3.62e+07 =  3% of the original kernel matrix.

torch.Size([3967, 2])
We keep 2.16e+05/3.53e+06 =  6% of the original kernel matrix.

torch.Size([10532, 2])
We keep 1.14e+06/3.30e+07 =  3% of the original kernel matrix.

torch.Size([2466, 2])
We keep 1.07e+05/1.42e+06 =  7% of the original kernel matrix.

torch.Size([8682, 2])
We keep 8.28e+05/2.10e+07 =  3% of the original kernel matrix.

torch.Size([4024, 2])
We keep 2.20e+05/3.86e+06 =  5% of the original kernel matrix.

torch.Size([10488, 2])
We keep 1.15e+06/3.45e+07 =  3% of the original kernel matrix.

torch.Size([2347, 2])
We keep 7.77e+04/1.04e+06 =  7% of the original kernel matrix.

torch.Size([8712, 2])
We keep 7.41e+05/1.80e+07 =  4% of the original kernel matrix.

torch.Size([6375, 2])
We keep 5.06e+05/1.04e+07 =  4% of the original kernel matrix.

torch.Size([12913, 2])
We keep 1.69e+06/5.67e+07 =  2% of the original kernel matrix.

torch.Size([4404, 2])
We keep 3.09e+05/4.71e+06 =  6% of the original kernel matrix.

torch.Size([10927, 2])
We keep 1.28e+06/3.82e+07 =  3% of the original kernel matrix.

torch.Size([9662, 2])
We keep 1.13e+06/2.88e+07 =  3% of the original kernel matrix.

torch.Size([15517, 2])
We keep 2.44e+06/9.43e+07 =  2% of the original kernel matrix.

torch.Size([3911, 2])
We keep 2.39e+05/3.52e+06 =  6% of the original kernel matrix.

torch.Size([10410, 2])
We keep 1.14e+06/3.30e+07 =  3% of the original kernel matrix.

torch.Size([7578, 2])
We keep 8.74e+05/1.68e+07 =  5% of the original kernel matrix.

torch.Size([13885, 2])
We keep 2.00e+06/7.22e+07 =  2% of the original kernel matrix.

torch.Size([3426, 2])
We keep 1.82e+05/2.66e+06 =  6% of the original kernel matrix.

torch.Size([9844, 2])
We keep 1.02e+06/2.87e+07 =  3% of the original kernel matrix.

torch.Size([4760, 2])
We keep 4.86e+05/7.21e+06 =  6% of the original kernel matrix.

torch.Size([10965, 2])
We keep 1.47e+06/4.72e+07 =  3% of the original kernel matrix.

torch.Size([4899, 2])
We keep 3.95e+05/6.75e+06 =  5% of the original kernel matrix.

torch.Size([11404, 2])
We keep 1.45e+06/4.57e+07 =  3% of the original kernel matrix.

torch.Size([3602, 2])
We keep 1.83e+05/2.73e+06 =  6% of the original kernel matrix.

torch.Size([10200, 2])
We keep 1.04e+06/2.91e+07 =  3% of the original kernel matrix.

torch.Size([8937, 2])
We keep 1.35e+06/2.87e+07 =  4% of the original kernel matrix.

torch.Size([14933, 2])
We keep 2.41e+06/9.42e+07 =  2% of the original kernel matrix.

torch.Size([10383, 2])
We keep 2.02e+06/4.40e+07 =  4% of the original kernel matrix.

torch.Size([15994, 2])
We keep 2.93e+06/1.17e+08 =  2% of the original kernel matrix.

torch.Size([6412, 2])
We keep 8.01e+05/1.29e+07 =  6% of the original kernel matrix.

torch.Size([12684, 2])
We keep 1.82e+06/6.32e+07 =  2% of the original kernel matrix.

torch.Size([6825, 2])
We keep 6.54e+05/1.22e+07 =  5% of the original kernel matrix.

torch.Size([13187, 2])
We keep 1.79e+06/6.14e+07 =  2% of the original kernel matrix.

torch.Size([6061, 2])
We keep 4.63e+05/9.03e+06 =  5% of the original kernel matrix.

torch.Size([12479, 2])
We keep 1.58e+06/5.28e+07 =  2% of the original kernel matrix.

torch.Size([2976, 2])
We keep 1.69e+05/2.02e+06 =  8% of the original kernel matrix.

torch.Size([9315, 2])
We keep 9.50e+05/2.50e+07 =  3% of the original kernel matrix.

torch.Size([2794, 2])
We keep 1.37e+05/1.64e+06 =  8% of the original kernel matrix.

torch.Size([9054, 2])
We keep 8.81e+05/2.25e+07 =  3% of the original kernel matrix.

torch.Size([2985, 2])
We keep 1.12e+05/1.62e+06 =  6% of the original kernel matrix.

torch.Size([9533, 2])
We keep 8.66e+05/2.24e+07 =  3% of the original kernel matrix.

torch.Size([2281, 2])
We keep 9.63e+04/1.11e+06 =  8% of the original kernel matrix.

torch.Size([8531, 2])
We keep 7.65e+05/1.85e+07 =  4% of the original kernel matrix.

torch.Size([9414, 2])
We keep 1.71e+06/3.44e+07 =  4% of the original kernel matrix.

torch.Size([15192, 2])
We keep 2.63e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([4122, 2])
We keep 2.33e+05/3.65e+06 =  6% of the original kernel matrix.

torch.Size([10724, 2])
We keep 1.16e+06/3.36e+07 =  3% of the original kernel matrix.

torch.Size([4523, 2])
We keep 3.74e+05/6.40e+06 =  5% of the original kernel matrix.

torch.Size([10870, 2])
We keep 1.39e+06/4.45e+07 =  3% of the original kernel matrix.

torch.Size([2030, 2])
We keep 7.53e+04/8.52e+05 =  8% of the original kernel matrix.

torch.Size([8098, 2])
We keep 6.92e+05/1.62e+07 =  4% of the original kernel matrix.

torch.Size([8316, 2])
We keep 1.15e+06/2.40e+07 =  4% of the original kernel matrix.

torch.Size([14261, 2])
We keep 2.29e+06/8.61e+07 =  2% of the original kernel matrix.

torch.Size([5164, 2])
We keep 4.34e+05/6.90e+06 =  6% of the original kernel matrix.

torch.Size([11594, 2])
We keep 1.44e+06/4.62e+07 =  3% of the original kernel matrix.

torch.Size([4112, 2])
We keep 3.57e+05/5.23e+06 =  6% of the original kernel matrix.

torch.Size([10374, 2])
We keep 1.32e+06/4.02e+07 =  3% of the original kernel matrix.

torch.Size([6174, 2])
We keep 7.62e+05/1.28e+07 =  5% of the original kernel matrix.

torch.Size([12471, 2])
We keep 1.83e+06/6.28e+07 =  2% of the original kernel matrix.

torch.Size([2495, 2])
We keep 1.07e+05/1.41e+06 =  7% of the original kernel matrix.

torch.Size([8802, 2])
We keep 8.29e+05/2.09e+07 =  3% of the original kernel matrix.

torch.Size([3821, 2])
We keep 2.54e+05/3.92e+06 =  6% of the original kernel matrix.

torch.Size([10319, 2])
We keep 1.20e+06/3.48e+07 =  3% of the original kernel matrix.

torch.Size([5393, 2])
We keep 5.38e+05/8.69e+06 =  6% of the original kernel matrix.

torch.Size([11719, 2])
We keep 1.58e+06/5.18e+07 =  3% of the original kernel matrix.

torch.Size([2496, 2])
We keep 1.20e+05/1.39e+06 =  8% of the original kernel matrix.

torch.Size([8679, 2])
We keep 8.37e+05/2.08e+07 =  4% of the original kernel matrix.

torch.Size([3870, 2])
We keep 3.44e+05/5.08e+06 =  6% of the original kernel matrix.

torch.Size([10196, 2])
We keep 1.31e+06/3.96e+07 =  3% of the original kernel matrix.

torch.Size([4442, 2])
We keep 3.44e+05/5.39e+06 =  6% of the original kernel matrix.

torch.Size([10777, 2])
We keep 1.32e+06/4.08e+07 =  3% of the original kernel matrix.

torch.Size([4483, 2])
We keep 2.76e+05/4.33e+06 =  6% of the original kernel matrix.

torch.Size([11190, 2])
We keep 1.23e+06/3.66e+07 =  3% of the original kernel matrix.

torch.Size([5502, 2])
We keep 6.48e+05/1.01e+07 =  6% of the original kernel matrix.

torch.Size([11831, 2])
We keep 1.67e+06/5.59e+07 =  2% of the original kernel matrix.

torch.Size([3509, 2])
We keep 2.02e+05/2.86e+06 =  7% of the original kernel matrix.

torch.Size([9934, 2])
We keep 1.06e+06/2.98e+07 =  3% of the original kernel matrix.

torch.Size([4913, 2])
We keep 4.64e+05/7.27e+06 =  6% of the original kernel matrix.

torch.Size([11367, 2])
We keep 1.49e+06/4.74e+07 =  3% of the original kernel matrix.

torch.Size([7743, 2])
We keep 9.18e+05/1.87e+07 =  4% of the original kernel matrix.

torch.Size([14070, 2])
We keep 2.08e+06/7.60e+07 =  2% of the original kernel matrix.

torch.Size([8435, 2])
We keep 8.37e+05/1.96e+07 =  4% of the original kernel matrix.

torch.Size([14616, 2])
We keep 2.07e+06/7.79e+07 =  2% of the original kernel matrix.

torch.Size([4264, 2])
We keep 3.83e+05/5.27e+06 =  7% of the original kernel matrix.

torch.Size([10736, 2])
We keep 1.26e+06/4.04e+07 =  3% of the original kernel matrix.

torch.Size([22655, 2])
We keep 8.51e+06/2.88e+08 =  2% of the original kernel matrix.

torch.Size([24073, 2])
We keep 5.96e+06/2.98e+08 =  1% of the original kernel matrix.

torch.Size([49891, 2])
We keep 5.65e+07/1.80e+09 =  3% of the original kernel matrix.

torch.Size([34898, 2])
We keep 1.27e+07/7.47e+08 =  1% of the original kernel matrix.

torch.Size([7561, 2])
We keep 1.05e+06/1.83e+07 =  5% of the original kernel matrix.

torch.Size([14037, 2])
We keep 1.88e+06/7.52e+07 =  2% of the original kernel matrix.

torch.Size([8182, 2])
We keep 1.40e+06/2.49e+07 =  5% of the original kernel matrix.

torch.Size([14098, 2])
We keep 2.33e+06/8.78e+07 =  2% of the original kernel matrix.

torch.Size([1994, 2])
We keep 7.08e+04/7.41e+05 =  9% of the original kernel matrix.

torch.Size([8054, 2])
We keep 6.68e+05/1.51e+07 =  4% of the original kernel matrix.

torch.Size([10031, 2])
We keep 1.31e+06/3.17e+07 =  4% of the original kernel matrix.

torch.Size([15798, 2])
We keep 2.53e+06/9.91e+07 =  2% of the original kernel matrix.

torch.Size([17688, 2])
We keep 9.93e+06/1.45e+08 =  6% of the original kernel matrix.

torch.Size([21221, 2])
We keep 4.67e+06/2.12e+08 =  2% of the original kernel matrix.

torch.Size([126566, 2])
We keep 2.88e+08/1.25e+10 =  2% of the original kernel matrix.

torch.Size([56453, 2])
We keep 2.93e+07/1.97e+09 =  1% of the original kernel matrix.

torch.Size([4266, 2])
We keep 2.62e+05/4.43e+06 =  5% of the original kernel matrix.

torch.Size([10961, 2])
We keep 1.22e+06/3.70e+07 =  3% of the original kernel matrix.

torch.Size([109099, 2])
We keep 2.11e+08/1.08e+10 =  1% of the original kernel matrix.

torch.Size([52041, 2])
We keep 2.76e+07/1.83e+09 =  1% of the original kernel matrix.

torch.Size([25057, 2])
We keep 3.00e+07/3.72e+08 =  8% of the original kernel matrix.

torch.Size([25041, 2])
We keep 6.61e+06/3.39e+08 =  1% of the original kernel matrix.

torch.Size([23204, 2])
We keep 1.53e+07/3.56e+08 =  4% of the original kernel matrix.

torch.Size([24058, 2])
We keep 6.36e+06/3.32e+08 =  1% of the original kernel matrix.

torch.Size([28527, 2])
We keep 1.79e+07/4.27e+08 =  4% of the original kernel matrix.

torch.Size([27153, 2])
We keep 6.99e+06/3.64e+08 =  1% of the original kernel matrix.

torch.Size([4530, 2])
We keep 2.94e+05/4.85e+06 =  6% of the original kernel matrix.

torch.Size([11088, 2])
We keep 1.26e+06/3.87e+07 =  3% of the original kernel matrix.

torch.Size([7705, 2])
We keep 9.20e+05/1.86e+07 =  4% of the original kernel matrix.

torch.Size([13818, 2])
We keep 2.09e+06/7.59e+07 =  2% of the original kernel matrix.

torch.Size([107780, 2])
We keep 1.56e+08/8.93e+09 =  1% of the original kernel matrix.

torch.Size([51547, 2])
We keep 2.48e+07/1.66e+09 =  1% of the original kernel matrix.

torch.Size([693204, 2])
We keep 3.27e+09/2.41e+11 =  1% of the original kernel matrix.

torch.Size([142204, 2])
We keep 1.09e+08/8.64e+09 =  1% of the original kernel matrix.

torch.Size([5180, 2])
We keep 3.97e+05/7.01e+06 =  5% of the original kernel matrix.

torch.Size([11610, 2])
We keep 1.42e+06/4.66e+07 =  3% of the original kernel matrix.

torch.Size([37544, 2])
We keep 1.89e+07/8.33e+08 =  2% of the original kernel matrix.

torch.Size([31197, 2])
We keep 9.20e+06/5.08e+08 =  1% of the original kernel matrix.

torch.Size([19457, 2])
We keep 1.48e+07/2.06e+08 =  7% of the original kernel matrix.

torch.Size([22393, 2])
We keep 5.26e+06/2.53e+08 =  2% of the original kernel matrix.

torch.Size([37654, 2])
We keep 6.29e+07/8.67e+08 =  7% of the original kernel matrix.

torch.Size([31015, 2])
We keep 9.62e+06/5.18e+08 =  1% of the original kernel matrix.

torch.Size([86891, 2])
We keep 1.52e+08/6.96e+09 =  2% of the original kernel matrix.

torch.Size([45260, 2])
We keep 2.28e+07/1.47e+09 =  1% of the original kernel matrix.

torch.Size([43016, 2])
We keep 9.00e+07/1.98e+09 =  4% of the original kernel matrix.

torch.Size([31151, 2])
We keep 1.32e+07/7.82e+08 =  1% of the original kernel matrix.

torch.Size([39019, 2])
We keep 2.59e+07/7.83e+08 =  3% of the original kernel matrix.

torch.Size([31595, 2])
We keep 8.48e+06/4.92e+08 =  1% of the original kernel matrix.

torch.Size([21962, 2])
We keep 1.16e+07/2.55e+08 =  4% of the original kernel matrix.

torch.Size([24015, 2])
We keep 5.58e+06/2.81e+08 =  1% of the original kernel matrix.

torch.Size([4552, 2])
We keep 3.23e+05/5.30e+06 =  6% of the original kernel matrix.

torch.Size([11094, 2])
We keep 1.30e+06/4.05e+07 =  3% of the original kernel matrix.

torch.Size([122296, 2])
We keep 2.95e+08/1.52e+10 =  1% of the original kernel matrix.

torch.Size([54097, 2])
We keep 3.15e+07/2.17e+09 =  1% of the original kernel matrix.

torch.Size([12552, 2])
We keep 6.51e+06/6.79e+07 =  9% of the original kernel matrix.

torch.Size([17625, 2])
We keep 3.10e+06/1.45e+08 =  2% of the original kernel matrix.

torch.Size([3687, 2])
We keep 2.45e+05/3.91e+06 =  6% of the original kernel matrix.

torch.Size([10173, 2])
We keep 1.17e+06/3.48e+07 =  3% of the original kernel matrix.

torch.Size([25867, 2])
We keep 1.10e+07/3.72e+08 =  2% of the original kernel matrix.

torch.Size([25963, 2])
We keep 6.68e+06/3.39e+08 =  1% of the original kernel matrix.

torch.Size([59099, 2])
We keep 9.19e+07/3.41e+09 =  2% of the original kernel matrix.

torch.Size([36079, 2])
We keep 1.68e+07/1.03e+09 =  1% of the original kernel matrix.

torch.Size([111029, 2])
We keep 1.41e+08/8.25e+09 =  1% of the original kernel matrix.

torch.Size([53244, 2])
We keep 2.45e+07/1.60e+09 =  1% of the original kernel matrix.

torch.Size([106024, 2])
We keep 7.98e+07/5.82e+09 =  1% of the original kernel matrix.

torch.Size([51969, 2])
We keep 2.09e+07/1.34e+09 =  1% of the original kernel matrix.

torch.Size([3469, 2])
We keep 2.15e+05/2.72e+06 =  7% of the original kernel matrix.

torch.Size([9922, 2])
We keep 1.05e+06/2.90e+07 =  3% of the original kernel matrix.

torch.Size([21386, 2])
We keep 1.03e+07/2.49e+08 =  4% of the original kernel matrix.

torch.Size([23180, 2])
We keep 5.58e+06/2.78e+08 =  2% of the original kernel matrix.

torch.Size([8101, 2])
We keep 1.13e+06/2.12e+07 =  5% of the original kernel matrix.

torch.Size([14265, 2])
We keep 2.21e+06/8.10e+07 =  2% of the original kernel matrix.

torch.Size([42621, 2])
We keep 2.61e+07/1.18e+09 =  2% of the original kernel matrix.

torch.Size([32699, 2])
We keep 1.05e+07/6.03e+08 =  1% of the original kernel matrix.

torch.Size([69235, 2])
We keep 9.32e+07/2.65e+09 =  3% of the original kernel matrix.

torch.Size([41883, 2])
We keep 1.39e+07/9.05e+08 =  1% of the original kernel matrix.

torch.Size([9875, 2])
We keep 2.21e+06/3.99e+07 =  5% of the original kernel matrix.

torch.Size([15732, 2])
We keep 2.79e+06/1.11e+08 =  2% of the original kernel matrix.

torch.Size([88624, 2])
We keep 7.88e+07/4.09e+09 =  1% of the original kernel matrix.

torch.Size([46818, 2])
We keep 1.79e+07/1.13e+09 =  1% of the original kernel matrix.

torch.Size([1798, 2])
We keep 5.65e+04/5.70e+05 =  9% of the original kernel matrix.

torch.Size([7677, 2])
We keep 6.20e+05/1.33e+07 =  4% of the original kernel matrix.

torch.Size([8218, 2])
We keep 1.06e+06/2.11e+07 =  5% of the original kernel matrix.

torch.Size([14306, 2])
We keep 2.15e+06/8.08e+07 =  2% of the original kernel matrix.

torch.Size([70183, 2])
We keep 7.79e+07/3.62e+09 =  2% of the original kernel matrix.

torch.Size([40821, 2])
We keep 1.71e+07/1.06e+09 =  1% of the original kernel matrix.

torch.Size([38905, 2])
We keep 3.17e+07/1.18e+09 =  2% of the original kernel matrix.

torch.Size([30588, 2])
We keep 1.07e+07/6.03e+08 =  1% of the original kernel matrix.

torch.Size([65083, 2])
We keep 8.14e+07/3.67e+09 =  2% of the original kernel matrix.

torch.Size([38366, 2])
We keep 1.73e+07/1.07e+09 =  1% of the original kernel matrix.

torch.Size([4764, 2])
We keep 4.08e+05/6.07e+06 =  6% of the original kernel matrix.

torch.Size([11337, 2])
We keep 1.38e+06/4.33e+07 =  3% of the original kernel matrix.

torch.Size([233019, 2])
We keep 3.87e+08/3.74e+10 =  1% of the original kernel matrix.

torch.Size([78999, 2])
We keep 4.61e+07/3.40e+09 =  1% of the original kernel matrix.

torch.Size([22762, 2])
We keep 5.69e+06/2.33e+08 =  2% of the original kernel matrix.

torch.Size([23469, 2])
We keep 5.04e+06/2.69e+08 =  1% of the original kernel matrix.

torch.Size([29760, 2])
We keep 1.01e+07/4.67e+08 =  2% of the original kernel matrix.

torch.Size([27740, 2])
We keep 7.15e+06/3.80e+08 =  1% of the original kernel matrix.

torch.Size([58618, 2])
We keep 5.09e+07/2.29e+09 =  2% of the original kernel matrix.

torch.Size([37970, 2])
We keep 1.38e+07/8.42e+08 =  1% of the original kernel matrix.

torch.Size([3603, 2])
We keep 1.92e+05/2.84e+06 =  6% of the original kernel matrix.

torch.Size([10095, 2])
We keep 1.04e+06/2.96e+07 =  3% of the original kernel matrix.

torch.Size([102973, 2])
We keep 1.22e+08/6.44e+09 =  1% of the original kernel matrix.

torch.Size([51023, 2])
We keep 2.20e+07/1.41e+09 =  1% of the original kernel matrix.

torch.Size([137533, 2])
We keep 4.49e+08/1.60e+10 =  2% of the original kernel matrix.

torch.Size([59692, 2])
We keep 3.31e+07/2.23e+09 =  1% of the original kernel matrix.

torch.Size([243429, 2])
We keep 4.79e+08/3.55e+10 =  1% of the original kernel matrix.

torch.Size([81423, 2])
We keep 4.58e+07/3.32e+09 =  1% of the original kernel matrix.

torch.Size([28255, 2])
We keep 1.63e+07/6.23e+08 =  2% of the original kernel matrix.

torch.Size([26361, 2])
We keep 8.23e+06/4.39e+08 =  1% of the original kernel matrix.

torch.Size([24051, 2])
We keep 1.02e+07/3.47e+08 =  2% of the original kernel matrix.

torch.Size([24664, 2])
We keep 6.45e+06/3.28e+08 =  1% of the original kernel matrix.

torch.Size([8541, 2])
We keep 2.20e+06/4.07e+07 =  5% of the original kernel matrix.

torch.Size([14061, 2])
We keep 2.81e+06/1.12e+08 =  2% of the original kernel matrix.

torch.Size([9507, 2])
We keep 1.03e+06/2.71e+07 =  3% of the original kernel matrix.

torch.Size([15321, 2])
We keep 2.35e+06/9.16e+07 =  2% of the original kernel matrix.

torch.Size([64359, 2])
We keep 9.16e+07/3.78e+09 =  2% of the original kernel matrix.

torch.Size([38780, 2])
We keep 1.68e+07/1.08e+09 =  1% of the original kernel matrix.

torch.Size([11557, 2])
We keep 1.89e+06/5.22e+07 =  3% of the original kernel matrix.

torch.Size([17029, 2])
We keep 3.06e+06/1.27e+08 =  2% of the original kernel matrix.

torch.Size([23269, 2])
We keep 1.11e+07/3.38e+08 =  3% of the original kernel matrix.

torch.Size([24274, 2])
We keep 6.24e+06/3.23e+08 =  1% of the original kernel matrix.

torch.Size([50413, 2])
We keep 7.84e+07/1.92e+09 =  4% of the original kernel matrix.

torch.Size([35583, 2])
We keep 1.11e+07/7.70e+08 =  1% of the original kernel matrix.

torch.Size([30405, 2])
We keep 1.24e+08/6.03e+08 = 20% of the original kernel matrix.

torch.Size([27932, 2])
We keep 7.32e+06/4.32e+08 =  1% of the original kernel matrix.

torch.Size([12851, 2])
We keep 5.21e+06/1.19e+08 =  4% of the original kernel matrix.

torch.Size([17480, 2])
We keep 4.25e+06/1.92e+08 =  2% of the original kernel matrix.

torch.Size([7006, 2])
We keep 5.71e+05/1.29e+07 =  4% of the original kernel matrix.

torch.Size([13467, 2])
We keep 1.76e+06/6.31e+07 =  2% of the original kernel matrix.

torch.Size([43994, 2])
We keep 1.56e+08/4.12e+09 =  3% of the original kernel matrix.

torch.Size([30859, 2])
We keep 1.75e+07/1.13e+09 =  1% of the original kernel matrix.

torch.Size([34970, 2])
We keep 1.70e+08/1.20e+09 = 14% of the original kernel matrix.

torch.Size([29185, 2])
We keep 9.40e+06/6.09e+08 =  1% of the original kernel matrix.

torch.Size([18654, 2])
We keep 2.11e+07/2.59e+08 =  8% of the original kernel matrix.

torch.Size([21593, 2])
We keep 5.72e+06/2.83e+08 =  2% of the original kernel matrix.

torch.Size([8154, 2])
We keep 1.04e+06/2.13e+07 =  4% of the original kernel matrix.

torch.Size([14282, 2])
We keep 2.18e+06/8.12e+07 =  2% of the original kernel matrix.

torch.Size([55256, 2])
We keep 6.30e+07/1.97e+09 =  3% of the original kernel matrix.

torch.Size([37181, 2])
We keep 1.31e+07/7.80e+08 =  1% of the original kernel matrix.

torch.Size([19019, 2])
We keep 4.89e+06/1.65e+08 =  2% of the original kernel matrix.

torch.Size([22239, 2])
We keep 4.76e+06/2.26e+08 =  2% of the original kernel matrix.

torch.Size([138094, 2])
We keep 3.99e+08/1.64e+10 =  2% of the original kernel matrix.

torch.Size([59144, 2])
We keep 3.31e+07/2.25e+09 =  1% of the original kernel matrix.

torch.Size([9210, 2])
We keep 1.60e+06/3.43e+07 =  4% of the original kernel matrix.

torch.Size([15192, 2])
We keep 2.57e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([32271, 2])
We keep 2.56e+07/9.80e+08 =  2% of the original kernel matrix.

torch.Size([27205, 2])
We keep 9.96e+06/5.51e+08 =  1% of the original kernel matrix.

torch.Size([17645, 2])
We keep 1.26e+07/1.47e+08 =  8% of the original kernel matrix.

torch.Size([21216, 2])
We keep 4.31e+06/2.13e+08 =  2% of the original kernel matrix.

torch.Size([79348, 2])
We keep 1.87e+08/5.42e+09 =  3% of the original kernel matrix.

torch.Size([43429, 2])
We keep 2.04e+07/1.30e+09 =  1% of the original kernel matrix.

torch.Size([38716, 2])
We keep 1.63e+07/9.77e+08 =  1% of the original kernel matrix.

torch.Size([31233, 2])
We keep 9.70e+06/5.50e+08 =  1% of the original kernel matrix.

torch.Size([12132, 2])
We keep 1.70e+06/4.96e+07 =  3% of the original kernel matrix.

torch.Size([17539, 2])
We keep 2.90e+06/1.24e+08 =  2% of the original kernel matrix.

torch.Size([128542, 2])
We keep 1.14e+08/8.98e+09 =  1% of the original kernel matrix.

torch.Size([57758, 2])
We keep 2.47e+07/1.67e+09 =  1% of the original kernel matrix.

torch.Size([168484, 2])
We keep 1.80e+08/1.51e+10 =  1% of the original kernel matrix.

torch.Size([66804, 2])
We keep 3.07e+07/2.16e+09 =  1% of the original kernel matrix.

torch.Size([7120, 2])
We keep 8.10e+05/1.54e+07 =  5% of the original kernel matrix.

torch.Size([13517, 2])
We keep 1.91e+06/6.89e+07 =  2% of the original kernel matrix.

torch.Size([129192, 2])
We keep 3.07e+08/9.78e+09 =  3% of the original kernel matrix.

torch.Size([58071, 2])
We keep 2.59e+07/1.74e+09 =  1% of the original kernel matrix.

torch.Size([137019, 2])
We keep 1.51e+08/1.22e+10 =  1% of the original kernel matrix.

torch.Size([59965, 2])
We keep 2.86e+07/1.94e+09 =  1% of the original kernel matrix.

torch.Size([308270, 2])
We keep 7.59e+08/6.16e+10 =  1% of the original kernel matrix.

torch.Size([91674, 2])
We keep 5.92e+07/4.36e+09 =  1% of the original kernel matrix.

torch.Size([24092, 2])
We keep 8.28e+06/2.79e+08 =  2% of the original kernel matrix.

torch.Size([24891, 2])
We keep 5.64e+06/2.93e+08 =  1% of the original kernel matrix.

torch.Size([4483, 2])
We keep 3.23e+05/5.28e+06 =  6% of the original kernel matrix.

torch.Size([11054, 2])
We keep 1.31e+06/4.04e+07 =  3% of the original kernel matrix.

torch.Size([16913, 2])
We keep 3.59e+06/1.35e+08 =  2% of the original kernel matrix.

torch.Size([20762, 2])
We keep 4.39e+06/2.04e+08 =  2% of the original kernel matrix.

torch.Size([18745, 2])
We keep 3.37e+07/2.85e+08 = 11% of the original kernel matrix.

torch.Size([21743, 2])
We keep 5.26e+06/2.97e+08 =  1% of the original kernel matrix.

torch.Size([27880, 2])
We keep 1.17e+07/4.31e+08 =  2% of the original kernel matrix.

torch.Size([26788, 2])
We keep 6.97e+06/3.65e+08 =  1% of the original kernel matrix.

torch.Size([47283, 2])
We keep 3.85e+07/1.15e+09 =  3% of the original kernel matrix.

torch.Size([34613, 2])
We keep 1.05e+07/5.97e+08 =  1% of the original kernel matrix.

torch.Size([3047, 2])
We keep 1.40e+05/2.07e+06 =  6% of the original kernel matrix.

torch.Size([9509, 2])
We keep 9.49e+05/2.53e+07 =  3% of the original kernel matrix.

torch.Size([67432, 2])
We keep 4.81e+07/2.35e+09 =  2% of the original kernel matrix.

torch.Size([40908, 2])
We keep 1.42e+07/8.52e+08 =  1% of the original kernel matrix.

torch.Size([138366, 2])
We keep 1.78e+08/1.17e+10 =  1% of the original kernel matrix.

torch.Size([59999, 2])
We keep 2.84e+07/1.90e+09 =  1% of the original kernel matrix.

torch.Size([5615, 2])
We keep 1.36e+07/2.14e+07 = 63% of the original kernel matrix.

torch.Size([11964, 2])
We keep 2.13e+06/8.14e+07 =  2% of the original kernel matrix.

torch.Size([10698, 2])
We keep 1.65e+06/4.09e+07 =  4% of the original kernel matrix.

torch.Size([16331, 2])
We keep 2.77e+06/1.13e+08 =  2% of the original kernel matrix.

torch.Size([29122, 2])
We keep 1.38e+07/5.16e+08 =  2% of the original kernel matrix.

torch.Size([27099, 2])
We keep 7.44e+06/4.00e+08 =  1% of the original kernel matrix.

torch.Size([12584, 2])
We keep 2.80e+06/5.78e+07 =  4% of the original kernel matrix.

torch.Size([17798, 2])
We keep 2.93e+06/1.34e+08 =  2% of the original kernel matrix.

torch.Size([7809, 2])
We keep 1.09e+06/1.90e+07 =  5% of the original kernel matrix.

torch.Size([14035, 2])
We keep 2.07e+06/7.67e+07 =  2% of the original kernel matrix.

torch.Size([13548, 2])
We keep 2.23e+06/7.10e+07 =  3% of the original kernel matrix.

torch.Size([18536, 2])
We keep 3.39e+06/1.48e+08 =  2% of the original kernel matrix.

torch.Size([5809, 2])
We keep 3.94e+05/7.47e+06 =  5% of the original kernel matrix.

torch.Size([12354, 2])
We keep 1.47e+06/4.81e+07 =  3% of the original kernel matrix.

torch.Size([13805, 2])
We keep 1.27e+07/1.16e+08 = 10% of the original kernel matrix.

torch.Size([18404, 2])
We keep 3.99e+06/1.89e+08 =  2% of the original kernel matrix.

torch.Size([39330, 2])
We keep 1.46e+07/7.98e+08 =  1% of the original kernel matrix.

torch.Size([31959, 2])
We keep 8.85e+06/4.97e+08 =  1% of the original kernel matrix.

torch.Size([22397, 2])
We keep 7.04e+06/2.85e+08 =  2% of the original kernel matrix.

torch.Size([24427, 2])
We keep 6.01e+06/2.97e+08 =  2% of the original kernel matrix.

torch.Size([27560, 2])
We keep 2.41e+07/6.45e+08 =  3% of the original kernel matrix.

torch.Size([26258, 2])
We keep 8.24e+06/4.47e+08 =  1% of the original kernel matrix.

torch.Size([10459, 2])
We keep 1.86e+06/4.31e+07 =  4% of the original kernel matrix.

torch.Size([16030, 2])
We keep 2.88e+06/1.15e+08 =  2% of the original kernel matrix.

torch.Size([1052482, 2])
We keep 9.42e+09/5.02e+11 =  1% of the original kernel matrix.

torch.Size([175000, 2])
We keep 1.47e+08/1.25e+10 =  1% of the original kernel matrix.

torch.Size([8321, 2])
We keep 1.12e+06/2.29e+07 =  4% of the original kernel matrix.

torch.Size([14295, 2])
We keep 2.25e+06/8.41e+07 =  2% of the original kernel matrix.

torch.Size([100921, 2])
We keep 7.41e+07/5.27e+09 =  1% of the original kernel matrix.

torch.Size([50558, 2])
We keep 1.99e+07/1.28e+09 =  1% of the original kernel matrix.

torch.Size([2871, 2])
We keep 1.27e+05/1.64e+06 =  7% of the original kernel matrix.

torch.Size([9271, 2])
We keep 8.70e+05/2.25e+07 =  3% of the original kernel matrix.

torch.Size([6223, 2])
We keep 5.21e+05/9.42e+06 =  5% of the original kernel matrix.

torch.Size([12635, 2])
We keep 1.62e+06/5.40e+07 =  2% of the original kernel matrix.

torch.Size([6058, 2])
We keep 4.57e+05/8.91e+06 =  5% of the original kernel matrix.

torch.Size([12520, 2])
We keep 1.56e+06/5.25e+07 =  2% of the original kernel matrix.

torch.Size([20157, 2])
We keep 4.83e+06/1.80e+08 =  2% of the original kernel matrix.

torch.Size([22914, 2])
We keep 4.67e+06/2.36e+08 =  1% of the original kernel matrix.

torch.Size([7324, 2])
We keep 6.56e+05/1.43e+07 =  4% of the original kernel matrix.

torch.Size([13633, 2])
We keep 1.86e+06/6.65e+07 =  2% of the original kernel matrix.

torch.Size([98060, 2])
We keep 1.65e+08/6.23e+09 =  2% of the original kernel matrix.

torch.Size([49320, 2])
We keep 2.19e+07/1.39e+09 =  1% of the original kernel matrix.

torch.Size([8370, 2])
We keep 8.33e+05/1.86e+07 =  4% of the original kernel matrix.

torch.Size([14496, 2])
We keep 2.09e+06/7.59e+07 =  2% of the original kernel matrix.

torch.Size([5424, 2])
We keep 4.52e+05/7.70e+06 =  5% of the original kernel matrix.

torch.Size([11906, 2])
We keep 1.51e+06/4.88e+07 =  3% of the original kernel matrix.

torch.Size([174682, 2])
We keep 3.42e+08/2.24e+10 =  1% of the original kernel matrix.

torch.Size([68027, 2])
We keep 3.79e+07/2.63e+09 =  1% of the original kernel matrix.

torch.Size([60996, 2])
We keep 5.13e+07/2.22e+09 =  2% of the original kernel matrix.

torch.Size([38710, 2])
We keep 1.40e+07/8.30e+08 =  1% of the original kernel matrix.

torch.Size([35935, 2])
We keep 2.17e+07/8.76e+08 =  2% of the original kernel matrix.

torch.Size([30128, 2])
We keep 9.43e+06/5.20e+08 =  1% of the original kernel matrix.

torch.Size([123022, 2])
We keep 1.84e+08/1.03e+10 =  1% of the original kernel matrix.

torch.Size([56640, 2])
We keep 2.70e+07/1.79e+09 =  1% of the original kernel matrix.

torch.Size([255357, 2])
We keep 4.81e+08/4.14e+10 =  1% of the original kernel matrix.

torch.Size([83946, 2])
We keep 4.96e+07/3.58e+09 =  1% of the original kernel matrix.

torch.Size([9579, 2])
We keep 1.22e+06/3.01e+07 =  4% of the original kernel matrix.

torch.Size([15268, 2])
We keep 2.51e+06/9.64e+07 =  2% of the original kernel matrix.

torch.Size([11446, 2])
We keep 1.54e+06/4.44e+07 =  3% of the original kernel matrix.

torch.Size([16783, 2])
We keep 2.91e+06/1.17e+08 =  2% of the original kernel matrix.

torch.Size([21455, 2])
We keep 1.36e+07/3.06e+08 =  4% of the original kernel matrix.

torch.Size([23265, 2])
We keep 6.06e+06/3.07e+08 =  1% of the original kernel matrix.

torch.Size([3324, 2])
We keep 2.16e+05/2.99e+06 =  7% of the original kernel matrix.

torch.Size([9698, 2])
We keep 1.05e+06/3.04e+07 =  3% of the original kernel matrix.

torch.Size([6670, 2])
We keep 6.27e+05/1.31e+07 =  4% of the original kernel matrix.

torch.Size([13091, 2])
We keep 1.87e+06/6.36e+07 =  2% of the original kernel matrix.

torch.Size([15026, 2])
We keep 3.59e+06/1.09e+08 =  3% of the original kernel matrix.

torch.Size([19521, 2])
We keep 4.06e+06/1.83e+08 =  2% of the original kernel matrix.

torch.Size([5005, 2])
We keep 3.99e+05/6.71e+06 =  5% of the original kernel matrix.

torch.Size([11553, 2])
We keep 1.40e+06/4.56e+07 =  3% of the original kernel matrix.

torch.Size([9505, 2])
We keep 1.10e+06/2.67e+07 =  4% of the original kernel matrix.

torch.Size([15315, 2])
We keep 2.36e+06/9.09e+07 =  2% of the original kernel matrix.

torch.Size([7076, 2])
We keep 6.62e+05/1.35e+07 =  4% of the original kernel matrix.

torch.Size([13315, 2])
We keep 1.84e+06/6.45e+07 =  2% of the original kernel matrix.

torch.Size([6940, 2])
We keep 6.77e+05/1.41e+07 =  4% of the original kernel matrix.

torch.Size([13346, 2])
We keep 1.83e+06/6.61e+07 =  2% of the original kernel matrix.

torch.Size([160387, 2])
We keep 2.37e+08/1.67e+10 =  1% of the original kernel matrix.

torch.Size([65108, 2])
We keep 3.27e+07/2.27e+09 =  1% of the original kernel matrix.

torch.Size([14763, 2])
We keep 4.78e+06/1.31e+08 =  3% of the original kernel matrix.

torch.Size([18971, 2])
We keep 4.37e+06/2.01e+08 =  2% of the original kernel matrix.

torch.Size([70703, 2])
We keep 7.06e+07/3.05e+09 =  2% of the original kernel matrix.

torch.Size([41775, 2])
We keep 1.60e+07/9.71e+08 =  1% of the original kernel matrix.

torch.Size([4577, 2])
We keep 2.88e+05/5.28e+06 =  5% of the original kernel matrix.

torch.Size([11260, 2])
We keep 1.28e+06/4.04e+07 =  3% of the original kernel matrix.

torch.Size([5054, 2])
We keep 4.21e+05/7.21e+06 =  5% of the original kernel matrix.

torch.Size([11457, 2])
We keep 1.47e+06/4.72e+07 =  3% of the original kernel matrix.

torch.Size([41824, 2])
We keep 3.16e+07/1.36e+09 =  2% of the original kernel matrix.

torch.Size([31764, 2])
We keep 1.13e+07/6.48e+08 =  1% of the original kernel matrix.

torch.Size([691882, 2])
We keep 4.31e+09/2.77e+11 =  1% of the original kernel matrix.

torch.Size([142729, 2])
We keep 1.11e+08/9.25e+09 =  1% of the original kernel matrix.

torch.Size([489119, 2])
We keep 1.00e+09/1.18e+11 =  0% of the original kernel matrix.

torch.Size([116224, 2])
We keep 7.66e+07/6.04e+09 =  1% of the original kernel matrix.

torch.Size([16051, 2])
We keep 2.97e+06/1.04e+08 =  2% of the original kernel matrix.

torch.Size([20198, 2])
We keep 3.85e+06/1.79e+08 =  2% of the original kernel matrix.

torch.Size([44755, 2])
We keep 2.37e+07/1.08e+09 =  2% of the original kernel matrix.

torch.Size([33916, 2])
We keep 9.96e+06/5.77e+08 =  1% of the original kernel matrix.

torch.Size([4992, 2])
We keep 3.49e+05/6.15e+06 =  5% of the original kernel matrix.

torch.Size([11655, 2])
We keep 1.39e+06/4.36e+07 =  3% of the original kernel matrix.

torch.Size([5767, 2])
We keep 2.28e+06/1.15e+07 = 19% of the original kernel matrix.

torch.Size([12297, 2])
We keep 1.69e+06/5.95e+07 =  2% of the original kernel matrix.

torch.Size([75021, 2])
We keep 5.26e+08/6.58e+09 =  7% of the original kernel matrix.

torch.Size([42458, 2])
We keep 2.02e+07/1.43e+09 =  1% of the original kernel matrix.

torch.Size([19815, 2])
We keep 7.77e+06/1.79e+08 =  4% of the original kernel matrix.

torch.Size([22797, 2])
We keep 4.83e+06/2.35e+08 =  2% of the original kernel matrix.

torch.Size([9841, 2])
We keep 1.35e+06/3.18e+07 =  4% of the original kernel matrix.

torch.Size([15570, 2])
We keep 2.54e+06/9.92e+07 =  2% of the original kernel matrix.

torch.Size([50327, 2])
We keep 3.84e+07/1.42e+09 =  2% of the original kernel matrix.

torch.Size([35484, 2])
We keep 1.15e+07/6.62e+08 =  1% of the original kernel matrix.

torch.Size([440597, 2])
We keep 2.12e+09/1.69e+11 =  1% of the original kernel matrix.

torch.Size([107661, 2])
We keep 9.45e+07/7.23e+09 =  1% of the original kernel matrix.

torch.Size([22456, 2])
We keep 1.09e+07/4.19e+08 =  2% of the original kernel matrix.

torch.Size([23120, 2])
We keep 6.88e+06/3.60e+08 =  1% of the original kernel matrix.

torch.Size([13268, 2])
We keep 3.41e+06/9.31e+07 =  3% of the original kernel matrix.

torch.Size([17944, 2])
We keep 3.83e+06/1.70e+08 =  2% of the original kernel matrix.

torch.Size([17695, 2])
We keep 7.17e+06/2.03e+08 =  3% of the original kernel matrix.

torch.Size([21367, 2])
We keep 5.29e+06/2.51e+08 =  2% of the original kernel matrix.

torch.Size([16639, 2])
We keep 3.45e+06/1.24e+08 =  2% of the original kernel matrix.

torch.Size([20553, 2])
We keep 4.34e+06/1.96e+08 =  2% of the original kernel matrix.

torch.Size([8434, 2])
We keep 1.45e+06/2.31e+07 =  6% of the original kernel matrix.

torch.Size([14544, 2])
We keep 2.18e+06/8.46e+07 =  2% of the original kernel matrix.

torch.Size([133641, 2])
We keep 2.09e+08/1.09e+10 =  1% of the original kernel matrix.

torch.Size([58812, 2])
We keep 2.72e+07/1.83e+09 =  1% of the original kernel matrix.

torch.Size([17301, 2])
We keep 9.24e+06/1.87e+08 =  4% of the original kernel matrix.

torch.Size([20810, 2])
We keep 5.10e+06/2.41e+08 =  2% of the original kernel matrix.

torch.Size([2809, 2])
We keep 1.72e+05/2.06e+06 =  8% of the original kernel matrix.

torch.Size([8985, 2])
We keep 9.61e+05/2.52e+07 =  3% of the original kernel matrix.

torch.Size([96110, 2])
We keep 7.01e+07/4.69e+09 =  1% of the original kernel matrix.

torch.Size([48991, 2])
We keep 1.86e+07/1.20e+09 =  1% of the original kernel matrix.

torch.Size([15512, 2])
We keep 3.42e+06/1.05e+08 =  3% of the original kernel matrix.

torch.Size([19946, 2])
We keep 4.06e+06/1.80e+08 =  2% of the original kernel matrix.

torch.Size([4145, 2])
We keep 3.55e+06/8.51e+06 = 41% of the original kernel matrix.

torch.Size([10250, 2])
We keep 1.44e+06/5.13e+07 =  2% of the original kernel matrix.

torch.Size([44853, 2])
We keep 2.74e+07/1.24e+09 =  2% of the original kernel matrix.

torch.Size([33609, 2])
We keep 1.08e+07/6.20e+08 =  1% of the original kernel matrix.

torch.Size([120313, 2])
We keep 1.06e+08/7.78e+09 =  1% of the original kernel matrix.

torch.Size([55733, 2])
We keep 2.36e+07/1.55e+09 =  1% of the original kernel matrix.

torch.Size([144806, 2])
We keep 4.16e+08/1.74e+10 =  2% of the original kernel matrix.

torch.Size([61472, 2])
We keep 3.43e+07/2.32e+09 =  1% of the original kernel matrix.

torch.Size([131779, 2])
We keep 3.44e+08/1.31e+10 =  2% of the original kernel matrix.

torch.Size([57640, 2])
We keep 3.00e+07/2.01e+09 =  1% of the original kernel matrix.

torch.Size([130923, 2])
We keep 3.23e+08/1.53e+10 =  2% of the original kernel matrix.

torch.Size([57134, 2])
We keep 3.17e+07/2.17e+09 =  1% of the original kernel matrix.

torch.Size([15976, 2])
We keep 9.27e+06/2.13e+08 =  4% of the original kernel matrix.

torch.Size([19907, 2])
We keep 5.42e+06/2.57e+08 =  2% of the original kernel matrix.

torch.Size([28045, 2])
We keep 2.63e+07/5.00e+08 =  5% of the original kernel matrix.

torch.Size([26548, 2])
We keep 7.08e+06/3.93e+08 =  1% of the original kernel matrix.

torch.Size([147378, 2])
We keep 1.47e+08/1.15e+10 =  1% of the original kernel matrix.

torch.Size([62185, 2])
We keep 2.77e+07/1.89e+09 =  1% of the original kernel matrix.

torch.Size([194175, 2])
We keep 2.82e+08/2.09e+10 =  1% of the original kernel matrix.

torch.Size([72658, 2])
We keep 3.46e+07/2.54e+09 =  1% of the original kernel matrix.

torch.Size([11020, 2])
We keep 1.69e+06/4.41e+07 =  3% of the original kernel matrix.

torch.Size([16604, 2])
We keep 2.86e+06/1.17e+08 =  2% of the original kernel matrix.

torch.Size([99343, 2])
We keep 5.35e+08/8.91e+09 =  6% of the original kernel matrix.

torch.Size([50956, 2])
We keep 2.15e+07/1.66e+09 =  1% of the original kernel matrix.

torch.Size([200992, 2])
We keep 1.99e+09/3.27e+10 =  6% of the original kernel matrix.

torch.Size([73500, 2])
We keep 3.99e+07/3.18e+09 =  1% of the original kernel matrix.

torch.Size([180987, 2])
We keep 3.55e+08/2.10e+10 =  1% of the original kernel matrix.

torch.Size([69669, 2])
We keep 3.62e+07/2.55e+09 =  1% of the original kernel matrix.

torch.Size([109980, 2])
We keep 1.73e+08/9.50e+09 =  1% of the original kernel matrix.

torch.Size([52562, 2])
We keep 2.58e+07/1.71e+09 =  1% of the original kernel matrix.

torch.Size([44953, 2])
We keep 8.78e+07/2.38e+09 =  3% of the original kernel matrix.

torch.Size([30890, 2])
We keep 1.42e+07/8.58e+08 =  1% of the original kernel matrix.

torch.Size([11331, 2])
We keep 3.07e+06/6.03e+07 =  5% of the original kernel matrix.

torch.Size([16730, 2])
We keep 3.29e+06/1.37e+08 =  2% of the original kernel matrix.

torch.Size([7893, 2])
We keep 7.48e+05/1.75e+07 =  4% of the original kernel matrix.

torch.Size([14160, 2])
We keep 2.00e+06/7.35e+07 =  2% of the original kernel matrix.

torch.Size([6343, 2])
We keep 8.24e+05/1.43e+07 =  5% of the original kernel matrix.

torch.Size([12501, 2])
We keep 1.81e+06/6.64e+07 =  2% of the original kernel matrix.

torch.Size([132562, 2])
We keep 1.14e+08/9.39e+09 =  1% of the original kernel matrix.

torch.Size([58957, 2])
We keep 2.50e+07/1.70e+09 =  1% of the original kernel matrix.

torch.Size([99363, 2])
We keep 7.40e+07/5.35e+09 =  1% of the original kernel matrix.

torch.Size([50324, 2])
We keep 1.98e+07/1.29e+09 =  1% of the original kernel matrix.

torch.Size([229400, 2])
We keep 6.00e+08/4.08e+10 =  1% of the original kernel matrix.

torch.Size([77870, 2])
We keep 4.96e+07/3.55e+09 =  1% of the original kernel matrix.

torch.Size([73470, 2])
We keep 9.82e+07/3.35e+09 =  2% of the original kernel matrix.

torch.Size([42781, 2])
We keep 1.64e+07/1.02e+09 =  1% of the original kernel matrix.

torch.Size([50837, 2])
We keep 2.95e+07/1.47e+09 =  2% of the original kernel matrix.

torch.Size([35697, 2])
We keep 1.17e+07/6.75e+08 =  1% of the original kernel matrix.

torch.Size([16100, 2])
We keep 3.63e+06/1.43e+08 =  2% of the original kernel matrix.

torch.Size([19963, 2])
We keep 4.44e+06/2.10e+08 =  2% of the original kernel matrix.

torch.Size([105495, 2])
We keep 9.85e+07/5.83e+09 =  1% of the original kernel matrix.

torch.Size([51797, 2])
We keep 2.05e+07/1.34e+09 =  1% of the original kernel matrix.

torch.Size([6921, 2])
We keep 1.54e+06/2.63e+07 =  5% of the original kernel matrix.

torch.Size([12567, 2])
We keep 2.39e+06/9.01e+07 =  2% of the original kernel matrix.

torch.Size([10802, 2])
We keep 2.10e+06/5.06e+07 =  4% of the original kernel matrix.

torch.Size([16359, 2])
We keep 3.01e+06/1.25e+08 =  2% of the original kernel matrix.

torch.Size([115636, 2])
We keep 1.05e+08/6.99e+09 =  1% of the original kernel matrix.

torch.Size([54438, 2])
We keep 2.23e+07/1.47e+09 =  1% of the original kernel matrix.

torch.Size([29928, 2])
We keep 1.37e+07/5.06e+08 =  2% of the original kernel matrix.

torch.Size([27750, 2])
We keep 7.39e+06/3.96e+08 =  1% of the original kernel matrix.

torch.Size([6731, 2])
We keep 7.23e+05/1.38e+07 =  5% of the original kernel matrix.

torch.Size([13054, 2])
We keep 1.86e+06/6.53e+07 =  2% of the original kernel matrix.

torch.Size([19602, 2])
We keep 4.22e+06/1.75e+08 =  2% of the original kernel matrix.

torch.Size([22355, 2])
We keep 4.73e+06/2.32e+08 =  2% of the original kernel matrix.

torch.Size([414167, 2])
We keep 2.91e+09/1.73e+11 =  1% of the original kernel matrix.

torch.Size([101821, 2])
We keep 9.56e+07/7.31e+09 =  1% of the original kernel matrix.

torch.Size([17933, 2])
We keep 4.83e+06/1.57e+08 =  3% of the original kernel matrix.

torch.Size([21427, 2])
We keep 4.70e+06/2.21e+08 =  2% of the original kernel matrix.

torch.Size([128831, 2])
We keep 2.03e+08/1.14e+10 =  1% of the original kernel matrix.

torch.Size([58036, 2])
We keep 2.84e+07/1.88e+09 =  1% of the original kernel matrix.

torch.Size([5358, 2])
We keep 1.11e+06/8.31e+06 = 13% of the original kernel matrix.

torch.Size([11826, 2])
We keep 1.53e+06/5.07e+07 =  3% of the original kernel matrix.

torch.Size([6473, 2])
We keep 1.47e+06/1.38e+07 = 10% of the original kernel matrix.

torch.Size([12730, 2])
We keep 1.83e+06/6.52e+07 =  2% of the original kernel matrix.

torch.Size([6548, 2])
We keep 9.61e+05/1.75e+07 =  5% of the original kernel matrix.

torch.Size([12555, 2])
We keep 2.05e+06/7.37e+07 =  2% of the original kernel matrix.

torch.Size([4588, 2])
We keep 4.30e+05/6.45e+06 =  6% of the original kernel matrix.

torch.Size([10922, 2])
We keep 1.41e+06/4.47e+07 =  3% of the original kernel matrix.

torch.Size([20992, 2])
We keep 5.93e+06/2.29e+08 =  2% of the original kernel matrix.

torch.Size([23589, 2])
We keep 5.56e+06/2.66e+08 =  2% of the original kernel matrix.

torch.Size([36805, 2])
We keep 2.53e+07/8.72e+08 =  2% of the original kernel matrix.

torch.Size([30380, 2])
We keep 9.35e+06/5.19e+08 =  1% of the original kernel matrix.

torch.Size([8872, 2])
We keep 2.54e+06/2.75e+07 =  9% of the original kernel matrix.

torch.Size([14948, 2])
We keep 2.15e+06/9.23e+07 =  2% of the original kernel matrix.

torch.Size([188935, 2])
We keep 3.63e+08/1.99e+10 =  1% of the original kernel matrix.

torch.Size([71377, 2])
We keep 3.52e+07/2.48e+09 =  1% of the original kernel matrix.

torch.Size([7738, 2])
We keep 1.45e+06/1.96e+07 =  7% of the original kernel matrix.

torch.Size([13933, 2])
We keep 1.99e+06/7.78e+07 =  2% of the original kernel matrix.

torch.Size([64726, 2])
We keep 6.22e+07/2.20e+09 =  2% of the original kernel matrix.

torch.Size([40421, 2])
We keep 1.31e+07/8.24e+08 =  1% of the original kernel matrix.

torch.Size([12996, 2])
We keep 3.06e+06/7.57e+07 =  4% of the original kernel matrix.

torch.Size([18046, 2])
We keep 3.55e+06/1.53e+08 =  2% of the original kernel matrix.

torch.Size([84365, 2])
We keep 7.10e+07/3.75e+09 =  1% of the original kernel matrix.

torch.Size([45340, 2])
We keep 1.72e+07/1.08e+09 =  1% of the original kernel matrix.

torch.Size([7780, 2])
We keep 1.19e+06/2.09e+07 =  5% of the original kernel matrix.

torch.Size([13879, 2])
We keep 2.18e+06/8.05e+07 =  2% of the original kernel matrix.

torch.Size([16169, 2])
We keep 4.75e+06/1.23e+08 =  3% of the original kernel matrix.

torch.Size([20216, 2])
We keep 4.30e+06/1.95e+08 =  2% of the original kernel matrix.

torch.Size([23078, 2])
We keep 6.74e+06/2.81e+08 =  2% of the original kernel matrix.

torch.Size([24744, 2])
We keep 5.94e+06/2.95e+08 =  2% of the original kernel matrix.

torch.Size([19849, 2])
We keep 7.92e+06/2.42e+08 =  3% of the original kernel matrix.

torch.Size([21634, 2])
We keep 5.37e+06/2.73e+08 =  1% of the original kernel matrix.

torch.Size([69146, 2])
We keep 4.53e+07/2.66e+09 =  1% of the original kernel matrix.

torch.Size([41557, 2])
We keep 1.49e+07/9.06e+08 =  1% of the original kernel matrix.

torch.Size([659394, 2])
We keep 4.31e+09/2.59e+11 =  1% of the original kernel matrix.

torch.Size([136217, 2])
We keep 1.13e+08/8.95e+09 =  1% of the original kernel matrix.

torch.Size([9742, 2])
We keep 1.23e+06/3.21e+07 =  3% of the original kernel matrix.

torch.Size([15485, 2])
We keep 2.56e+06/9.97e+07 =  2% of the original kernel matrix.

torch.Size([7463, 2])
We keep 8.40e+05/1.65e+07 =  5% of the original kernel matrix.

torch.Size([13836, 2])
We keep 2.00e+06/7.14e+07 =  2% of the original kernel matrix.

torch.Size([33522, 2])
We keep 1.92e+07/6.76e+08 =  2% of the original kernel matrix.

torch.Size([29205, 2])
We keep 8.29e+06/4.57e+08 =  1% of the original kernel matrix.

torch.Size([18548, 2])
We keep 5.37e+06/1.64e+08 =  3% of the original kernel matrix.

torch.Size([21802, 2])
We keep 4.77e+06/2.25e+08 =  2% of the original kernel matrix.

torch.Size([448726, 2])
We keep 9.16e+08/9.94e+10 =  0% of the original kernel matrix.

torch.Size([111033, 2])
We keep 7.22e+07/5.54e+09 =  1% of the original kernel matrix.

torch.Size([3951, 2])
We keep 3.38e+05/4.28e+06 =  7% of the original kernel matrix.

torch.Size([10436, 2])
We keep 1.24e+06/3.64e+07 =  3% of the original kernel matrix.

torch.Size([89555, 2])
We keep 1.02e+08/5.49e+09 =  1% of the original kernel matrix.

torch.Size([47104, 2])
We keep 2.06e+07/1.30e+09 =  1% of the original kernel matrix.

torch.Size([215558, 2])
We keep 2.57e+08/2.69e+10 =  0% of the original kernel matrix.

torch.Size([77213, 2])
We keep 4.00e+07/2.88e+09 =  1% of the original kernel matrix.

torch.Size([4501, 2])
We keep 3.49e+05/5.36e+06 =  6% of the original kernel matrix.

torch.Size([11044, 2])
We keep 1.32e+06/4.07e+07 =  3% of the original kernel matrix.

torch.Size([248572, 2])
We keep 1.02e+09/4.93e+10 =  2% of the original kernel matrix.

torch.Size([79984, 2])
We keep 5.32e+07/3.90e+09 =  1% of the original kernel matrix.

torch.Size([13077, 2])
We keep 5.68e+06/1.27e+08 =  4% of the original kernel matrix.

torch.Size([17740, 2])
We keep 4.08e+06/1.98e+08 =  2% of the original kernel matrix.

torch.Size([476525, 2])
We keep 1.97e+09/1.65e+11 =  1% of the original kernel matrix.

torch.Size([113522, 2])
We keep 9.35e+07/7.14e+09 =  1% of the original kernel matrix.

torch.Size([12461, 2])
We keep 1.93e+06/5.61e+07 =  3% of the original kernel matrix.

torch.Size([17570, 2])
We keep 3.11e+06/1.32e+08 =  2% of the original kernel matrix.

torch.Size([16433, 2])
We keep 4.64e+06/1.25e+08 =  3% of the original kernel matrix.

torch.Size([20374, 2])
We keep 4.13e+06/1.97e+08 =  2% of the original kernel matrix.

torch.Size([9269, 2])
We keep 2.70e+06/5.39e+07 =  5% of the original kernel matrix.

torch.Size([14888, 2])
We keep 3.17e+06/1.29e+08 =  2% of the original kernel matrix.

torch.Size([6120, 2])
We keep 5.73e+05/1.09e+07 =  5% of the original kernel matrix.

torch.Size([12596, 2])
We keep 1.66e+06/5.81e+07 =  2% of the original kernel matrix.

torch.Size([10084, 2])
We keep 2.34e+06/5.36e+07 =  4% of the original kernel matrix.

torch.Size([15605, 2])
We keep 3.14e+06/1.29e+08 =  2% of the original kernel matrix.

torch.Size([30451, 2])
We keep 1.61e+07/5.02e+08 =  3% of the original kernel matrix.

torch.Size([28078, 2])
We keep 7.20e+06/3.94e+08 =  1% of the original kernel matrix.

torch.Size([6309, 2])
We keep 6.06e+05/1.01e+07 =  6% of the original kernel matrix.

torch.Size([12750, 2])
We keep 1.64e+06/5.59e+07 =  2% of the original kernel matrix.

torch.Size([98270, 2])
We keep 1.13e+08/6.03e+09 =  1% of the original kernel matrix.

torch.Size([49912, 2])
We keep 2.12e+07/1.37e+09 =  1% of the original kernel matrix.

torch.Size([14647, 2])
We keep 3.32e+06/1.01e+08 =  3% of the original kernel matrix.

torch.Size([19166, 2])
We keep 3.94e+06/1.76e+08 =  2% of the original kernel matrix.

torch.Size([7788, 2])
We keep 8.25e+05/1.71e+07 =  4% of the original kernel matrix.

torch.Size([14028, 2])
We keep 2.02e+06/7.26e+07 =  2% of the original kernel matrix.

torch.Size([16134, 2])
We keep 4.77e+06/1.50e+08 =  3% of the original kernel matrix.

torch.Size([20140, 2])
We keep 4.52e+06/2.16e+08 =  2% of the original kernel matrix.

torch.Size([153355, 2])
We keep 3.20e+08/2.07e+10 =  1% of the original kernel matrix.

torch.Size([62860, 2])
We keep 3.69e+07/2.53e+09 =  1% of the original kernel matrix.

torch.Size([98723, 2])
We keep 1.59e+08/7.70e+09 =  2% of the original kernel matrix.

torch.Size([49504, 2])
We keep 2.42e+07/1.54e+09 =  1% of the original kernel matrix.

torch.Size([284955, 2])
We keep 4.89e+08/4.86e+10 =  1% of the original kernel matrix.

torch.Size([89012, 2])
We keep 5.28e+07/3.88e+09 =  1% of the original kernel matrix.

torch.Size([17738, 2])
We keep 7.76e+06/1.38e+08 =  5% of the original kernel matrix.

torch.Size([21243, 2])
We keep 4.30e+06/2.06e+08 =  2% of the original kernel matrix.

torch.Size([9822, 2])
We keep 1.53e+06/3.92e+07 =  3% of the original kernel matrix.

torch.Size([15710, 2])
We keep 2.73e+06/1.10e+08 =  2% of the original kernel matrix.

torch.Size([4392, 2])
We keep 2.64e+05/4.57e+06 =  5% of the original kernel matrix.

torch.Size([10983, 2])
We keep 1.25e+06/3.76e+07 =  3% of the original kernel matrix.

torch.Size([4889, 2])
We keep 5.77e+05/8.08e+06 =  7% of the original kernel matrix.

torch.Size([11304, 2])
We keep 1.55e+06/5.00e+07 =  3% of the original kernel matrix.

torch.Size([157554, 2])
We keep 5.02e+08/1.40e+10 =  3% of the original kernel matrix.

torch.Size([64761, 2])
We keep 2.67e+07/2.08e+09 =  1% of the original kernel matrix.

torch.Size([9114, 2])
We keep 1.17e+06/2.59e+07 =  4% of the original kernel matrix.

torch.Size([15040, 2])
We keep 2.35e+06/8.95e+07 =  2% of the original kernel matrix.

torch.Size([24017, 2])
We keep 8.97e+06/3.32e+08 =  2% of the original kernel matrix.

torch.Size([25010, 2])
We keep 6.29e+06/3.20e+08 =  1% of the original kernel matrix.

torch.Size([22952, 2])
We keep 1.75e+07/3.17e+08 =  5% of the original kernel matrix.

torch.Size([24032, 2])
We keep 6.10e+06/3.13e+08 =  1% of the original kernel matrix.

torch.Size([23726, 2])
We keep 1.24e+07/4.46e+08 =  2% of the original kernel matrix.

torch.Size([23851, 2])
We keep 7.11e+06/3.71e+08 =  1% of the original kernel matrix.

torch.Size([13282, 2])
We keep 5.13e+06/1.03e+08 =  4% of the original kernel matrix.

torch.Size([18103, 2])
We keep 4.01e+06/1.78e+08 =  2% of the original kernel matrix.

torch.Size([6436, 2])
We keep 1.04e+06/1.61e+07 =  6% of the original kernel matrix.

torch.Size([12440, 2])
We keep 1.98e+06/7.06e+07 =  2% of the original kernel matrix.

torch.Size([149706, 2])
We keep 3.46e+08/2.02e+10 =  1% of the original kernel matrix.

torch.Size([61446, 2])
We keep 3.62e+07/2.50e+09 =  1% of the original kernel matrix.

torch.Size([42139, 2])
We keep 2.92e+07/1.18e+09 =  2% of the original kernel matrix.

torch.Size([32486, 2])
We keep 1.06e+07/6.03e+08 =  1% of the original kernel matrix.

torch.Size([8974, 2])
We keep 1.33e+06/3.11e+07 =  4% of the original kernel matrix.

torch.Size([14974, 2])
We keep 2.34e+06/9.81e+07 =  2% of the original kernel matrix.

torch.Size([19180, 2])
We keep 5.11e+06/1.69e+08 =  3% of the original kernel matrix.

torch.Size([22254, 2])
We keep 4.50e+06/2.29e+08 =  1% of the original kernel matrix.

torch.Size([53673, 2])
We keep 4.02e+07/1.61e+09 =  2% of the original kernel matrix.

torch.Size([36674, 2])
We keep 1.22e+07/7.06e+08 =  1% of the original kernel matrix.

torch.Size([103587, 2])
We keep 1.22e+08/6.91e+09 =  1% of the original kernel matrix.

torch.Size([51160, 2])
We keep 2.26e+07/1.46e+09 =  1% of the original kernel matrix.

torch.Size([381768, 2])
We keep 1.14e+09/8.22e+10 =  1% of the original kernel matrix.

torch.Size([100848, 2])
We keep 6.79e+07/5.04e+09 =  1% of the original kernel matrix.

torch.Size([100805, 2])
We keep 1.02e+08/5.67e+09 =  1% of the original kernel matrix.

torch.Size([50757, 2])
We keep 2.04e+07/1.32e+09 =  1% of the original kernel matrix.

torch.Size([6138, 2])
We keep 4.99e+05/1.05e+07 =  4% of the original kernel matrix.

torch.Size([12722, 2])
We keep 1.63e+06/5.69e+07 =  2% of the original kernel matrix.

torch.Size([36805, 2])
We keep 1.92e+07/7.67e+08 =  2% of the original kernel matrix.

torch.Size([30721, 2])
We keep 8.61e+06/4.87e+08 =  1% of the original kernel matrix.

torch.Size([29944, 2])
We keep 3.23e+07/8.86e+08 =  3% of the original kernel matrix.

torch.Size([26022, 2])
We keep 9.47e+06/5.24e+08 =  1% of the original kernel matrix.

torch.Size([7776, 2])
We keep 7.64e+05/1.58e+07 =  4% of the original kernel matrix.

torch.Size([14083, 2])
We keep 1.92e+06/6.99e+07 =  2% of the original kernel matrix.

torch.Size([14123, 2])
We keep 2.69e+06/8.20e+07 =  3% of the original kernel matrix.

torch.Size([18777, 2])
We keep 3.66e+06/1.59e+08 =  2% of the original kernel matrix.

torch.Size([10365, 2])
We keep 1.14e+06/3.15e+07 =  3% of the original kernel matrix.

torch.Size([16080, 2])
We keep 2.46e+06/9.88e+07 =  2% of the original kernel matrix.

torch.Size([24462, 2])
We keep 4.37e+07/3.19e+08 = 13% of the original kernel matrix.

torch.Size([25028, 2])
We keep 6.13e+06/3.14e+08 =  1% of the original kernel matrix.

torch.Size([19493, 2])
We keep 9.93e+06/3.09e+08 =  3% of the original kernel matrix.

torch.Size([21882, 2])
We keep 6.27e+06/3.09e+08 =  2% of the original kernel matrix.

torch.Size([5171, 2])
We keep 3.72e+05/6.41e+06 =  5% of the original kernel matrix.

torch.Size([11763, 2])
We keep 1.40e+06/4.45e+07 =  3% of the original kernel matrix.

torch.Size([14524, 2])
We keep 2.99e+06/9.20e+07 =  3% of the original kernel matrix.

torch.Size([19157, 2])
We keep 3.82e+06/1.69e+08 =  2% of the original kernel matrix.

torch.Size([9431, 2])
We keep 1.91e+06/4.57e+07 =  4% of the original kernel matrix.

torch.Size([14996, 2])
We keep 2.72e+06/1.19e+08 =  2% of the original kernel matrix.

torch.Size([11087, 2])
We keep 6.19e+06/8.48e+07 =  7% of the original kernel matrix.

torch.Size([16614, 2])
We keep 3.42e+06/1.62e+08 =  2% of the original kernel matrix.

torch.Size([7692, 2])
We keep 1.00e+06/1.85e+07 =  5% of the original kernel matrix.

torch.Size([13766, 2])
We keep 2.07e+06/7.56e+07 =  2% of the original kernel matrix.

torch.Size([28486, 2])
We keep 1.12e+07/4.18e+08 =  2% of the original kernel matrix.

torch.Size([27350, 2])
We keep 6.80e+06/3.59e+08 =  1% of the original kernel matrix.

torch.Size([14139, 2])
We keep 3.16e+07/2.20e+08 = 14% of the original kernel matrix.

torch.Size([18017, 2])
We keep 4.51e+06/2.61e+08 =  1% of the original kernel matrix.

torch.Size([12004, 2])
We keep 2.33e+06/6.27e+07 =  3% of the original kernel matrix.

torch.Size([17286, 2])
We keep 3.21e+06/1.39e+08 =  2% of the original kernel matrix.

torch.Size([8294, 2])
We keep 1.41e+06/2.77e+07 =  5% of the original kernel matrix.

torch.Size([14204, 2])
We keep 2.41e+06/9.25e+07 =  2% of the original kernel matrix.

torch.Size([14062, 2])
We keep 2.47e+06/7.53e+07 =  3% of the original kernel matrix.

torch.Size([18804, 2])
We keep 3.53e+06/1.53e+08 =  2% of the original kernel matrix.

torch.Size([27540, 2])
We keep 1.03e+07/3.83e+08 =  2% of the original kernel matrix.

torch.Size([27171, 2])
We keep 6.36e+06/3.44e+08 =  1% of the original kernel matrix.

torch.Size([148785, 2])
We keep 2.91e+08/1.32e+10 =  2% of the original kernel matrix.

torch.Size([62541, 2])
We keep 3.01e+07/2.02e+09 =  1% of the original kernel matrix.

torch.Size([8561, 2])
We keep 9.82e+05/2.17e+07 =  4% of the original kernel matrix.

torch.Size([14689, 2])
We keep 2.20e+06/8.20e+07 =  2% of the original kernel matrix.

torch.Size([2784, 2])
We keep 1.38e+05/1.77e+06 =  7% of the original kernel matrix.

torch.Size([9099, 2])
We keep 9.03e+05/2.34e+07 =  3% of the original kernel matrix.

torch.Size([8569, 2])
We keep 1.26e+06/2.50e+07 =  5% of the original kernel matrix.

torch.Size([14460, 2])
We keep 2.34e+06/8.78e+07 =  2% of the original kernel matrix.

torch.Size([82020, 2])
We keep 9.36e+07/4.08e+09 =  2% of the original kernel matrix.

torch.Size([45414, 2])
We keep 1.78e+07/1.12e+09 =  1% of the original kernel matrix.

torch.Size([229197, 2])
We keep 4.40e+08/3.21e+10 =  1% of the original kernel matrix.

torch.Size([79638, 2])
We keep 4.21e+07/3.15e+09 =  1% of the original kernel matrix.

torch.Size([55037, 2])
We keep 6.95e+07/2.93e+09 =  2% of the original kernel matrix.

torch.Size([34549, 2])
We keep 1.59e+07/9.52e+08 =  1% of the original kernel matrix.

torch.Size([7976, 2])
We keep 1.00e+06/1.98e+07 =  5% of the original kernel matrix.

torch.Size([14116, 2])
We keep 2.12e+06/7.83e+07 =  2% of the original kernel matrix.

torch.Size([127813, 2])
We keep 2.36e+08/1.24e+10 =  1% of the original kernel matrix.

torch.Size([56881, 2])
We keep 2.93e+07/1.96e+09 =  1% of the original kernel matrix.

torch.Size([151120, 2])
We keep 1.32e+08/1.17e+10 =  1% of the original kernel matrix.

torch.Size([62923, 2])
We keep 2.79e+07/1.90e+09 =  1% of the original kernel matrix.

torch.Size([4599, 2])
We keep 2.74e+05/4.55e+06 =  6% of the original kernel matrix.

torch.Size([11125, 2])
We keep 1.25e+06/3.75e+07 =  3% of the original kernel matrix.

torch.Size([8200, 2])
We keep 1.43e+06/2.60e+07 =  5% of the original kernel matrix.

torch.Size([14341, 2])
We keep 2.31e+06/8.96e+07 =  2% of the original kernel matrix.

torch.Size([37516, 2])
We keep 1.22e+07/7.11e+08 =  1% of the original kernel matrix.

torch.Size([31473, 2])
We keep 8.30e+06/4.69e+08 =  1% of the original kernel matrix.

torch.Size([38314, 2])
We keep 7.20e+07/1.46e+09 =  4% of the original kernel matrix.

torch.Size([30108, 2])
We keep 1.10e+07/6.71e+08 =  1% of the original kernel matrix.

torch.Size([49726, 2])
We keep 1.81e+08/1.99e+09 =  9% of the original kernel matrix.

torch.Size([34841, 2])
We keep 1.20e+07/7.84e+08 =  1% of the original kernel matrix.

torch.Size([155575, 2])
We keep 2.23e+08/1.49e+10 =  1% of the original kernel matrix.

torch.Size([64101, 2])
We keep 3.13e+07/2.15e+09 =  1% of the original kernel matrix.

torch.Size([20577, 2])
We keep 5.72e+06/1.91e+08 =  2% of the original kernel matrix.

torch.Size([23261, 2])
We keep 4.87e+06/2.43e+08 =  2% of the original kernel matrix.

torch.Size([7628, 2])
We keep 7.20e+05/1.55e+07 =  4% of the original kernel matrix.

torch.Size([13872, 2])
We keep 1.94e+06/6.92e+07 =  2% of the original kernel matrix.

torch.Size([4393, 2])
We keep 2.94e+05/5.11e+06 =  5% of the original kernel matrix.

torch.Size([11005, 2])
We keep 1.34e+06/3.98e+07 =  3% of the original kernel matrix.

torch.Size([24473, 2])
We keep 1.30e+07/3.92e+08 =  3% of the original kernel matrix.

torch.Size([24876, 2])
We keep 6.63e+06/3.48e+08 =  1% of the original kernel matrix.

torch.Size([13619, 2])
We keep 2.22e+06/7.09e+07 =  3% of the original kernel matrix.

torch.Size([18487, 2])
We keep 3.42e+06/1.48e+08 =  2% of the original kernel matrix.

torch.Size([19074, 2])
We keep 7.04e+06/2.30e+08 =  3% of the original kernel matrix.

torch.Size([21424, 2])
We keep 5.40e+06/2.67e+08 =  2% of the original kernel matrix.

torch.Size([116196, 2])
We keep 1.49e+08/7.88e+09 =  1% of the original kernel matrix.

torch.Size([54212, 2])
We keep 2.40e+07/1.56e+09 =  1% of the original kernel matrix.

torch.Size([521855, 2])
We keep 1.31e+09/1.42e+11 =  0% of the original kernel matrix.

torch.Size([119413, 2])
We keep 8.61e+07/6.62e+09 =  1% of the original kernel matrix.

torch.Size([10221, 2])
We keep 2.18e+06/4.08e+07 =  5% of the original kernel matrix.

torch.Size([15771, 2])
We keep 2.80e+06/1.12e+08 =  2% of the original kernel matrix.

torch.Size([2962, 2])
We keep 1.31e+05/1.93e+06 =  6% of the original kernel matrix.

torch.Size([9276, 2])
We keep 9.22e+05/2.44e+07 =  3% of the original kernel matrix.

torch.Size([14787, 2])
We keep 4.19e+06/1.11e+08 =  3% of the original kernel matrix.

torch.Size([19360, 2])
We keep 4.11e+06/1.85e+08 =  2% of the original kernel matrix.

torch.Size([49922, 2])
We keep 5.59e+07/1.34e+09 =  4% of the original kernel matrix.

torch.Size([35720, 2])
We keep 1.10e+07/6.43e+08 =  1% of the original kernel matrix.

torch.Size([49568, 2])
We keep 3.14e+07/1.24e+09 =  2% of the original kernel matrix.

torch.Size([35283, 2])
We keep 1.06e+07/6.20e+08 =  1% of the original kernel matrix.

torch.Size([9406, 2])
We keep 6.04e+06/4.03e+07 = 14% of the original kernel matrix.

torch.Size([15239, 2])
We keep 2.43e+06/1.12e+08 =  2% of the original kernel matrix.

torch.Size([5663, 2])
We keep 4.15e+05/7.21e+06 =  5% of the original kernel matrix.

torch.Size([12109, 2])
We keep 1.48e+06/4.72e+07 =  3% of the original kernel matrix.

torch.Size([38368, 2])
We keep 2.33e+07/8.37e+08 =  2% of the original kernel matrix.

torch.Size([31261, 2])
We keep 9.15e+06/5.09e+08 =  1% of the original kernel matrix.

torch.Size([5666, 2])
We keep 5.56e+05/9.03e+06 =  6% of the original kernel matrix.

torch.Size([12193, 2])
We keep 1.58e+06/5.28e+07 =  2% of the original kernel matrix.

torch.Size([43287, 2])
We keep 4.65e+07/1.67e+09 =  2% of the original kernel matrix.

torch.Size([31627, 2])
We keep 1.25e+07/7.18e+08 =  1% of the original kernel matrix.

torch.Size([7046, 2])
We keep 7.10e+05/1.34e+07 =  5% of the original kernel matrix.

torch.Size([13351, 2])
We keep 1.84e+06/6.43e+07 =  2% of the original kernel matrix.

torch.Size([18157, 2])
We keep 3.50e+06/1.46e+08 =  2% of the original kernel matrix.

torch.Size([21761, 2])
We keep 4.45e+06/2.13e+08 =  2% of the original kernel matrix.

torch.Size([9082, 2])
We keep 1.63e+06/3.44e+07 =  4% of the original kernel matrix.

torch.Size([15020, 2])
We keep 2.65e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([7895, 2])
We keep 7.52e+05/1.81e+07 =  4% of the original kernel matrix.

torch.Size([14042, 2])
We keep 2.01e+06/7.48e+07 =  2% of the original kernel matrix.

torch.Size([10376, 2])
We keep 1.27e+06/3.31e+07 =  3% of the original kernel matrix.

torch.Size([15881, 2])
We keep 2.57e+06/1.01e+08 =  2% of the original kernel matrix.

torch.Size([143080, 2])
We keep 1.46e+08/1.12e+10 =  1% of the original kernel matrix.

torch.Size([61448, 2])
We keep 2.73e+07/1.86e+09 =  1% of the original kernel matrix.

torch.Size([248321, 2])
We keep 4.28e+08/3.48e+10 =  1% of the original kernel matrix.

torch.Size([82652, 2])
We keep 4.55e+07/3.28e+09 =  1% of the original kernel matrix.

torch.Size([86823, 2])
We keep 1.18e+08/4.80e+09 =  2% of the original kernel matrix.

torch.Size([46856, 2])
We keep 1.91e+07/1.22e+09 =  1% of the original kernel matrix.

torch.Size([93124, 2])
We keep 1.28e+08/5.46e+09 =  2% of the original kernel matrix.

torch.Size([47951, 2])
We keep 2.07e+07/1.30e+09 =  1% of the original kernel matrix.

torch.Size([25861, 2])
We keep 6.95e+06/3.26e+08 =  2% of the original kernel matrix.

torch.Size([25995, 2])
We keep 6.03e+06/3.18e+08 =  1% of the original kernel matrix.

torch.Size([14385, 2])
We keep 4.13e+06/9.28e+07 =  4% of the original kernel matrix.

torch.Size([18974, 2])
We keep 3.83e+06/1.69e+08 =  2% of the original kernel matrix.

torch.Size([17050, 2])
We keep 4.43e+06/1.45e+08 =  3% of the original kernel matrix.

torch.Size([20852, 2])
We keep 4.62e+06/2.11e+08 =  2% of the original kernel matrix.

torch.Size([120419, 2])
We keep 1.15e+08/8.22e+09 =  1% of the original kernel matrix.

torch.Size([56077, 2])
We keep 2.40e+07/1.59e+09 =  1% of the original kernel matrix.

torch.Size([31275, 2])
We keep 2.05e+07/7.23e+08 =  2% of the original kernel matrix.

torch.Size([27949, 2])
We keep 8.69e+06/4.73e+08 =  1% of the original kernel matrix.

torch.Size([26179, 2])
We keep 1.63e+07/3.93e+08 =  4% of the original kernel matrix.

torch.Size([25819, 2])
We keep 6.74e+06/3.49e+08 =  1% of the original kernel matrix.

torch.Size([14399, 2])
We keep 3.75e+06/1.03e+08 =  3% of the original kernel matrix.

torch.Size([18912, 2])
We keep 3.99e+06/1.78e+08 =  2% of the original kernel matrix.

torch.Size([813066, 2])
We keep 4.59e+09/4.40e+11 =  1% of the original kernel matrix.

torch.Size([152956, 2])
We keep 1.47e+08/1.17e+10 =  1% of the original kernel matrix.

torch.Size([22274, 2])
We keep 6.30e+06/2.35e+08 =  2% of the original kernel matrix.

torch.Size([22849, 2])
We keep 5.05e+06/2.70e+08 =  1% of the original kernel matrix.

torch.Size([52310, 2])
We keep 3.87e+07/1.78e+09 =  2% of the original kernel matrix.

torch.Size([35760, 2])
We keep 1.24e+07/7.42e+08 =  1% of the original kernel matrix.

torch.Size([86901, 2])
We keep 1.07e+08/5.18e+09 =  2% of the original kernel matrix.

torch.Size([46744, 2])
We keep 1.93e+07/1.27e+09 =  1% of the original kernel matrix.

torch.Size([51355, 2])
We keep 2.75e+07/1.29e+09 =  2% of the original kernel matrix.

torch.Size([36104, 2])
We keep 1.04e+07/6.33e+08 =  1% of the original kernel matrix.

torch.Size([26540, 2])
We keep 9.63e+06/3.60e+08 =  2% of the original kernel matrix.

torch.Size([26223, 2])
We keep 6.52e+06/3.34e+08 =  1% of the original kernel matrix.

torch.Size([446357, 2])
We keep 2.24e+09/1.42e+11 =  1% of the original kernel matrix.

torch.Size([109788, 2])
We keep 8.68e+07/6.63e+09 =  1% of the original kernel matrix.

torch.Size([116120, 2])
We keep 9.09e+07/7.24e+09 =  1% of the original kernel matrix.

torch.Size([54514, 2])
We keep 2.26e+07/1.50e+09 =  1% of the original kernel matrix.

torch.Size([64452, 2])
We keep 6.03e+07/2.55e+09 =  2% of the original kernel matrix.

torch.Size([39567, 2])
We keep 1.46e+07/8.88e+08 =  1% of the original kernel matrix.

torch.Size([14841, 2])
We keep 7.95e+06/9.72e+07 =  8% of the original kernel matrix.

torch.Size([19309, 2])
We keep 3.57e+06/1.73e+08 =  2% of the original kernel matrix.

torch.Size([38882, 2])
We keep 2.57e+07/8.90e+08 =  2% of the original kernel matrix.

torch.Size([31496, 2])
We keep 9.45e+06/5.25e+08 =  1% of the original kernel matrix.

torch.Size([9471, 2])
We keep 1.35e+06/2.81e+07 =  4% of the original kernel matrix.

torch.Size([15652, 2])
We keep 2.22e+06/9.32e+07 =  2% of the original kernel matrix.

torch.Size([132409, 2])
We keep 3.33e+08/1.01e+10 =  3% of the original kernel matrix.

torch.Size([58671, 2])
We keep 2.61e+07/1.77e+09 =  1% of the original kernel matrix.

torch.Size([995979, 2])
We keep 5.28e+09/5.28e+11 =  0% of the original kernel matrix.

torch.Size([167193, 2])
We keep 1.60e+08/1.28e+10 =  1% of the original kernel matrix.

torch.Size([84121, 2])
We keep 2.03e+08/4.35e+09 =  4% of the original kernel matrix.

torch.Size([45692, 2])
We keep 1.86e+07/1.16e+09 =  1% of the original kernel matrix.

torch.Size([127868, 2])
We keep 2.22e+08/1.32e+10 =  1% of the original kernel matrix.

torch.Size([56464, 2])
We keep 3.00e+07/2.02e+09 =  1% of the original kernel matrix.

torch.Size([81724, 2])
We keep 2.03e+08/5.19e+09 =  3% of the original kernel matrix.

torch.Size([44940, 2])
We keep 1.99e+07/1.27e+09 =  1% of the original kernel matrix.

torch.Size([30951, 2])
We keep 1.75e+07/5.62e+08 =  3% of the original kernel matrix.

torch.Size([28125, 2])
We keep 7.88e+06/4.17e+08 =  1% of the original kernel matrix.

torch.Size([22976, 2])
We keep 1.20e+07/3.38e+08 =  3% of the original kernel matrix.

torch.Size([24307, 2])
We keep 6.39e+06/3.24e+08 =  1% of the original kernel matrix.

torch.Size([14169, 2])
We keep 3.91e+06/1.09e+08 =  3% of the original kernel matrix.

torch.Size([18628, 2])
We keep 4.06e+06/1.84e+08 =  2% of the original kernel matrix.

torch.Size([18182, 2])
We keep 4.44e+07/1.79e+08 = 24% of the original kernel matrix.

torch.Size([21456, 2])
We keep 4.18e+06/2.35e+08 =  1% of the original kernel matrix.

torch.Size([348531, 2])
We keep 5.75e+08/6.13e+10 =  0% of the original kernel matrix.

torch.Size([97536, 2])
We keep 5.82e+07/4.35e+09 =  1% of the original kernel matrix.

torch.Size([71329, 2])
We keep 9.82e+07/4.46e+09 =  2% of the original kernel matrix.

torch.Size([40474, 2])
We keep 1.88e+07/1.17e+09 =  1% of the original kernel matrix.

torch.Size([210315, 2])
We keep 1.03e+09/4.30e+10 =  2% of the original kernel matrix.

torch.Size([72691, 2])
We keep 4.92e+07/3.65e+09 =  1% of the original kernel matrix.

torch.Size([111495, 2])
We keep 2.36e+08/1.01e+10 =  2% of the original kernel matrix.

torch.Size([53060, 2])
We keep 2.68e+07/1.77e+09 =  1% of the original kernel matrix.

torch.Size([36939, 2])
We keep 1.51e+07/7.78e+08 =  1% of the original kernel matrix.

torch.Size([30794, 2])
We keep 8.87e+06/4.90e+08 =  1% of the original kernel matrix.

torch.Size([8403, 2])
We keep 1.42e+06/2.39e+07 =  5% of the original kernel matrix.

torch.Size([14419, 2])
We keep 2.28e+06/8.60e+07 =  2% of the original kernel matrix.

torch.Size([51055, 2])
We keep 3.72e+07/1.56e+09 =  2% of the original kernel matrix.

torch.Size([36152, 2])
We keep 1.20e+07/6.95e+08 =  1% of the original kernel matrix.

torch.Size([144571, 2])
We keep 1.55e+08/1.12e+10 =  1% of the original kernel matrix.

torch.Size([61643, 2])
We keep 2.72e+07/1.86e+09 =  1% of the original kernel matrix.

torch.Size([9013, 2])
We keep 2.26e+06/4.23e+07 =  5% of the original kernel matrix.

torch.Size([14661, 2])
We keep 2.83e+06/1.14e+08 =  2% of the original kernel matrix.

torch.Size([450140, 2])
We keep 9.20e+08/9.57e+10 =  0% of the original kernel matrix.

torch.Size([110681, 2])
We keep 7.07e+07/5.44e+09 =  1% of the original kernel matrix.

torch.Size([27596, 2])
We keep 1.46e+07/4.14e+08 =  3% of the original kernel matrix.

torch.Size([26706, 2])
We keep 6.20e+06/3.58e+08 =  1% of the original kernel matrix.

torch.Size([28775, 2])
We keep 1.36e+07/4.84e+08 =  2% of the original kernel matrix.

torch.Size([27102, 2])
We keep 7.42e+06/3.87e+08 =  1% of the original kernel matrix.

torch.Size([16385, 2])
We keep 8.68e+07/1.74e+08 = 49% of the original kernel matrix.

torch.Size([20267, 2])
We keep 4.42e+06/2.32e+08 =  1% of the original kernel matrix.

torch.Size([11906, 2])
We keep 2.09e+06/5.52e+07 =  3% of the original kernel matrix.

torch.Size([17084, 2])
We keep 3.10e+06/1.31e+08 =  2% of the original kernel matrix.

torch.Size([3124, 2])
We keep 2.06e+05/2.67e+06 =  7% of the original kernel matrix.

torch.Size([9418, 2])
We keep 1.05e+06/2.87e+07 =  3% of the original kernel matrix.

torch.Size([4738, 2])
We keep 9.84e+05/7.18e+06 = 13% of the original kernel matrix.

torch.Size([11097, 2])
We keep 1.35e+06/4.71e+07 =  2% of the original kernel matrix.

torch.Size([18463, 2])
We keep 5.73e+06/1.61e+08 =  3% of the original kernel matrix.

torch.Size([21787, 2])
We keep 4.72e+06/2.23e+08 =  2% of the original kernel matrix.

torch.Size([34814, 2])
We keep 2.46e+07/7.45e+08 =  3% of the original kernel matrix.

torch.Size([29535, 2])
We keep 8.87e+06/4.80e+08 =  1% of the original kernel matrix.

torch.Size([2821816, 2])
We keep 3.65e+10/4.21e+12 =  0% of the original kernel matrix.

torch.Size([286133, 2])
We keep 4.21e+08/3.61e+10 =  1% of the original kernel matrix.

torch.Size([9990, 2])
We keep 2.61e+06/5.69e+07 =  4% of the original kernel matrix.

torch.Size([15637, 2])
We keep 3.11e+06/1.33e+08 =  2% of the original kernel matrix.

torch.Size([135674, 2])
We keep 1.23e+08/9.93e+09 =  1% of the original kernel matrix.

torch.Size([59466, 2])
We keep 2.58e+07/1.75e+09 =  1% of the original kernel matrix.

torch.Size([10835, 2])
We keep 1.66e+06/3.87e+07 =  4% of the original kernel matrix.

torch.Size([16329, 2])
We keep 2.72e+06/1.09e+08 =  2% of the original kernel matrix.

torch.Size([25173, 2])
We keep 7.38e+06/3.23e+08 =  2% of the original kernel matrix.

torch.Size([25542, 2])
We keep 6.16e+06/3.16e+08 =  1% of the original kernel matrix.

torch.Size([8999, 2])
We keep 2.19e+06/4.10e+07 =  5% of the original kernel matrix.

torch.Size([14763, 2])
We keep 2.86e+06/1.13e+08 =  2% of the original kernel matrix.

torch.Size([166292, 2])
We keep 4.25e+08/2.38e+10 =  1% of the original kernel matrix.

torch.Size([65490, 2])
We keep 3.90e+07/2.71e+09 =  1% of the original kernel matrix.

torch.Size([12718, 2])
We keep 1.17e+07/7.02e+07 = 16% of the original kernel matrix.

torch.Size([17756, 2])
We keep 3.06e+06/1.47e+08 =  2% of the original kernel matrix.

torch.Size([662555, 2])
We keep 2.07e+09/2.38e+11 =  0% of the original kernel matrix.

torch.Size([138851, 2])
We keep 1.09e+08/8.58e+09 =  1% of the original kernel matrix.

torch.Size([7617, 2])
We keep 8.57e+05/1.67e+07 =  5% of the original kernel matrix.

torch.Size([13766, 2])
We keep 1.93e+06/7.19e+07 =  2% of the original kernel matrix.

torch.Size([149882, 2])
We keep 2.01e+08/1.26e+10 =  1% of the original kernel matrix.

torch.Size([62855, 2])
We keep 2.90e+07/1.98e+09 =  1% of the original kernel matrix.

torch.Size([24042, 2])
We keep 1.34e+07/3.03e+08 =  4% of the original kernel matrix.

torch.Size([25103, 2])
We keep 5.91e+06/3.06e+08 =  1% of the original kernel matrix.

torch.Size([124423, 2])
We keep 2.02e+08/1.05e+10 =  1% of the original kernel matrix.

torch.Size([56485, 2])
We keep 2.71e+07/1.80e+09 =  1% of the original kernel matrix.

torch.Size([288080, 2])
We keep 1.01e+09/5.74e+10 =  1% of the original kernel matrix.

torch.Size([87653, 2])
We keep 5.71e+07/4.21e+09 =  1% of the original kernel matrix.

torch.Size([26423, 2])
We keep 1.04e+07/3.88e+08 =  2% of the original kernel matrix.

torch.Size([26116, 2])
We keep 6.58e+06/3.46e+08 =  1% of the original kernel matrix.

torch.Size([13769, 2])
We keep 2.71e+06/7.82e+07 =  3% of the original kernel matrix.

torch.Size([18627, 2])
We keep 3.56e+06/1.55e+08 =  2% of the original kernel matrix.

torch.Size([86567, 2])
We keep 5.38e+07/3.72e+09 =  1% of the original kernel matrix.

torch.Size([46736, 2])
We keep 1.69e+07/1.07e+09 =  1% of the original kernel matrix.

torch.Size([14197, 2])
We keep 3.00e+06/7.86e+07 =  3% of the original kernel matrix.

torch.Size([18938, 2])
We keep 3.53e+06/1.56e+08 =  2% of the original kernel matrix.

torch.Size([20371, 2])
We keep 9.81e+06/2.26e+08 =  4% of the original kernel matrix.

torch.Size([22667, 2])
We keep 5.32e+06/2.64e+08 =  2% of the original kernel matrix.

torch.Size([92023, 2])
We keep 8.29e+07/4.69e+09 =  1% of the original kernel matrix.

torch.Size([47845, 2])
We keep 1.88e+07/1.20e+09 =  1% of the original kernel matrix.

torch.Size([10738, 2])
We keep 1.99e+06/5.33e+07 =  3% of the original kernel matrix.

torch.Size([16486, 2])
We keep 2.89e+06/1.28e+08 =  2% of the original kernel matrix.

torch.Size([48078, 2])
We keep 5.52e+07/1.93e+09 =  2% of the original kernel matrix.

torch.Size([34666, 2])
We keep 1.35e+07/7.72e+08 =  1% of the original kernel matrix.

torch.Size([37118, 2])
We keep 2.74e+07/8.47e+08 =  3% of the original kernel matrix.

torch.Size([30231, 2])
We keep 9.17e+06/5.12e+08 =  1% of the original kernel matrix.

torch.Size([104687, 2])
We keep 1.56e+08/7.60e+09 =  2% of the original kernel matrix.

torch.Size([51498, 2])
We keep 2.37e+07/1.53e+09 =  1% of the original kernel matrix.

torch.Size([138034, 2])
We keep 3.05e+08/1.45e+10 =  2% of the original kernel matrix.

torch.Size([59609, 2])
We keep 3.12e+07/2.12e+09 =  1% of the original kernel matrix.

torch.Size([47416, 2])
We keep 2.53e+07/1.22e+09 =  2% of the original kernel matrix.

torch.Size([34572, 2])
We keep 1.07e+07/6.13e+08 =  1% of the original kernel matrix.

torch.Size([13919, 2])
We keep 2.89e+06/8.16e+07 =  3% of the original kernel matrix.

torch.Size([18866, 2])
We keep 3.50e+06/1.59e+08 =  2% of the original kernel matrix.

torch.Size([67342, 2])
We keep 1.21e+08/3.90e+09 =  3% of the original kernel matrix.

torch.Size([39838, 2])
We keep 1.74e+07/1.10e+09 =  1% of the original kernel matrix.

torch.Size([63499, 2])
We keep 7.43e+07/2.95e+09 =  2% of the original kernel matrix.

torch.Size([38574, 2])
We keep 1.57e+07/9.56e+08 =  1% of the original kernel matrix.

torch.Size([29619, 2])
We keep 1.48e+08/6.64e+08 = 22% of the original kernel matrix.

torch.Size([27090, 2])
We keep 7.09e+06/4.53e+08 =  1% of the original kernel matrix.

torch.Size([23183, 2])
We keep 7.73e+06/2.65e+08 =  2% of the original kernel matrix.

torch.Size([24487, 2])
We keep 5.72e+06/2.86e+08 =  1% of the original kernel matrix.

torch.Size([24839, 2])
We keep 1.80e+07/4.94e+08 =  3% of the original kernel matrix.

torch.Size([24571, 2])
We keep 6.91e+06/3.91e+08 =  1% of the original kernel matrix.

torch.Size([29024, 2])
We keep 1.66e+07/4.59e+08 =  3% of the original kernel matrix.

torch.Size([27301, 2])
We keep 7.05e+06/3.77e+08 =  1% of the original kernel matrix.

torch.Size([87528, 2])
We keep 2.76e+08/9.77e+09 =  2% of the original kernel matrix.

torch.Size([43808, 2])
We keep 2.66e+07/1.74e+09 =  1% of the original kernel matrix.

torch.Size([50875, 2])
We keep 1.09e+08/2.14e+09 =  5% of the original kernel matrix.

torch.Size([35003, 2])
We keep 1.33e+07/8.14e+08 =  1% of the original kernel matrix.

torch.Size([18215, 2])
We keep 3.77e+06/1.50e+08 =  2% of the original kernel matrix.

torch.Size([21594, 2])
We keep 4.55e+06/2.15e+08 =  2% of the original kernel matrix.

torch.Size([7368, 2])
We keep 9.46e+05/1.75e+07 =  5% of the original kernel matrix.

torch.Size([13542, 2])
We keep 2.02e+06/7.36e+07 =  2% of the original kernel matrix.

torch.Size([8365, 2])
We keep 9.35e+05/1.91e+07 =  4% of the original kernel matrix.

torch.Size([14621, 2])
We keep 2.08e+06/7.70e+07 =  2% of the original kernel matrix.

torch.Size([48046, 2])
We keep 6.39e+07/2.18e+09 =  2% of the original kernel matrix.

torch.Size([33411, 2])
We keep 1.37e+07/8.20e+08 =  1% of the original kernel matrix.

torch.Size([67384, 2])
We keep 1.18e+08/3.54e+09 =  3% of the original kernel matrix.

torch.Size([39928, 2])
We keep 1.70e+07/1.05e+09 =  1% of the original kernel matrix.

torch.Size([83563, 2])
We keep 5.61e+07/3.43e+09 =  1% of the original kernel matrix.

torch.Size([45570, 2])
We keep 1.65e+07/1.03e+09 =  1% of the original kernel matrix.

torch.Size([12294, 2])
We keep 2.22e+06/5.73e+07 =  3% of the original kernel matrix.

torch.Size([17493, 2])
We keep 3.19e+06/1.33e+08 =  2% of the original kernel matrix.

torch.Size([24233, 2])
We keep 7.46e+06/3.43e+08 =  2% of the original kernel matrix.

torch.Size([25014, 2])
We keep 6.29e+06/3.26e+08 =  1% of the original kernel matrix.

torch.Size([32543, 2])
We keep 1.69e+07/6.40e+08 =  2% of the original kernel matrix.

torch.Size([28986, 2])
We keep 8.39e+06/4.45e+08 =  1% of the original kernel matrix.

torch.Size([61590, 2])
We keep 3.75e+07/1.89e+09 =  1% of the original kernel matrix.

torch.Size([39322, 2])
We keep 1.24e+07/7.65e+08 =  1% of the original kernel matrix.

torch.Size([29380, 2])
We keep 1.86e+07/4.78e+08 =  3% of the original kernel matrix.

torch.Size([27437, 2])
We keep 7.43e+06/3.84e+08 =  1% of the original kernel matrix.

torch.Size([69769, 2])
We keep 3.64e+07/2.34e+09 =  1% of the original kernel matrix.

torch.Size([41583, 2])
We keep 1.40e+07/8.51e+08 =  1% of the original kernel matrix.

torch.Size([8152, 2])
We keep 2.38e+06/2.79e+07 =  8% of the original kernel matrix.

torch.Size([14471, 2])
We keep 2.46e+06/9.29e+07 =  2% of the original kernel matrix.

torch.Size([31572, 2])
We keep 1.57e+07/5.44e+08 =  2% of the original kernel matrix.

torch.Size([28791, 2])
We keep 7.46e+06/4.10e+08 =  1% of the original kernel matrix.

torch.Size([3600, 2])
We keep 2.37e+05/3.50e+06 =  6% of the original kernel matrix.

torch.Size([9990, 2])
We keep 1.11e+06/3.29e+07 =  3% of the original kernel matrix.

torch.Size([7591, 2])
We keep 9.00e+05/1.69e+07 =  5% of the original kernel matrix.

torch.Size([13859, 2])
We keep 2.00e+06/7.22e+07 =  2% of the original kernel matrix.

torch.Size([132349, 2])
We keep 1.49e+08/9.32e+09 =  1% of the original kernel matrix.

torch.Size([58860, 2])
We keep 2.42e+07/1.70e+09 =  1% of the original kernel matrix.

torch.Size([16795, 2])
We keep 5.85e+06/1.71e+08 =  3% of the original kernel matrix.

torch.Size([20478, 2])
We keep 4.83e+06/2.30e+08 =  2% of the original kernel matrix.

torch.Size([83992, 2])
We keep 5.63e+07/3.32e+09 =  1% of the original kernel matrix.

torch.Size([45730, 2])
We keep 1.58e+07/1.01e+09 =  1% of the original kernel matrix.

torch.Size([3730, 2])
We keep 2.07e+05/3.18e+06 =  6% of the original kernel matrix.

torch.Size([10415, 2])
We keep 1.10e+06/3.14e+07 =  3% of the original kernel matrix.

torch.Size([7517, 2])
We keep 1.82e+06/2.01e+07 =  9% of the original kernel matrix.

torch.Size([13836, 2])
We keep 2.10e+06/7.89e+07 =  2% of the original kernel matrix.

torch.Size([27008, 2])
We keep 3.63e+07/4.02e+08 =  9% of the original kernel matrix.

torch.Size([26289, 2])
We keep 6.62e+06/3.53e+08 =  1% of the original kernel matrix.

torch.Size([48648, 2])
We keep 2.86e+07/1.40e+09 =  2% of the original kernel matrix.

torch.Size([34787, 2])
We keep 1.14e+07/6.58e+08 =  1% of the original kernel matrix.

torch.Size([28097, 2])
We keep 9.50e+06/4.18e+08 =  2% of the original kernel matrix.

torch.Size([27275, 2])
We keep 6.88e+06/3.60e+08 =  1% of the original kernel matrix.

torch.Size([6712, 2])
We keep 7.51e+05/1.49e+07 =  5% of the original kernel matrix.

torch.Size([13051, 2])
We keep 1.87e+06/6.78e+07 =  2% of the original kernel matrix.

torch.Size([11585, 2])
We keep 2.07e+06/5.13e+07 =  4% of the original kernel matrix.

torch.Size([16949, 2])
We keep 3.09e+06/1.26e+08 =  2% of the original kernel matrix.

torch.Size([39025, 2])
We keep 2.55e+07/8.91e+08 =  2% of the original kernel matrix.

torch.Size([31619, 2])
We keep 9.64e+06/5.25e+08 =  1% of the original kernel matrix.

torch.Size([20858, 2])
We keep 5.53e+06/2.20e+08 =  2% of the original kernel matrix.

torch.Size([23640, 2])
We keep 5.38e+06/2.61e+08 =  2% of the original kernel matrix.

torch.Size([58246, 2])
We keep 7.44e+07/1.97e+09 =  3% of the original kernel matrix.

torch.Size([38128, 2])
We keep 1.29e+07/7.80e+08 =  1% of the original kernel matrix.

torch.Size([7361, 2])
We keep 7.60e+05/1.65e+07 =  4% of the original kernel matrix.

torch.Size([13827, 2])
We keep 1.93e+06/7.14e+07 =  2% of the original kernel matrix.

torch.Size([9649, 2])
We keep 1.22e+07/5.25e+07 = 23% of the original kernel matrix.

torch.Size([15174, 2])
We keep 3.03e+06/1.27e+08 =  2% of the original kernel matrix.

torch.Size([36974, 2])
We keep 2.58e+07/9.60e+08 =  2% of the original kernel matrix.

torch.Size([30156, 2])
We keep 9.84e+06/5.45e+08 =  1% of the original kernel matrix.

torch.Size([7112, 2])
We keep 1.04e+06/1.90e+07 =  5% of the original kernel matrix.

torch.Size([13211, 2])
We keep 2.14e+06/7.66e+07 =  2% of the original kernel matrix.

torch.Size([124804, 2])
We keep 2.04e+08/1.10e+10 =  1% of the original kernel matrix.

torch.Size([56990, 2])
We keep 2.67e+07/1.84e+09 =  1% of the original kernel matrix.

torch.Size([17145, 2])
We keep 1.42e+07/1.40e+08 = 10% of the original kernel matrix.

torch.Size([20904, 2])
We keep 3.83e+06/2.08e+08 =  1% of the original kernel matrix.

torch.Size([58666, 2])
We keep 6.12e+07/2.41e+09 =  2% of the original kernel matrix.

torch.Size([37743, 2])
We keep 1.45e+07/8.63e+08 =  1% of the original kernel matrix.

torch.Size([116723, 2])
We keep 1.21e+08/7.89e+09 =  1% of the original kernel matrix.

torch.Size([55064, 2])
We keep 2.36e+07/1.56e+09 =  1% of the original kernel matrix.

torch.Size([84895, 2])
We keep 2.11e+08/4.85e+09 =  4% of the original kernel matrix.

torch.Size([46128, 2])
We keep 1.97e+07/1.22e+09 =  1% of the original kernel matrix.

torch.Size([127260, 2])
We keep 1.75e+08/1.03e+10 =  1% of the original kernel matrix.

torch.Size([57546, 2])
We keep 2.70e+07/1.78e+09 =  1% of the original kernel matrix.

torch.Size([116911, 2])
We keep 9.63e+07/6.99e+09 =  1% of the original kernel matrix.

torch.Size([54994, 2])
We keep 2.23e+07/1.47e+09 =  1% of the original kernel matrix.

torch.Size([4094, 2])
We keep 2.32e+05/3.94e+06 =  5% of the original kernel matrix.

torch.Size([10661, 2])
We keep 1.18e+06/3.49e+07 =  3% of the original kernel matrix.

torch.Size([47039, 2])
We keep 2.10e+07/1.15e+09 =  1% of the original kernel matrix.

torch.Size([34607, 2])
We keep 1.05e+07/5.96e+08 =  1% of the original kernel matrix.

torch.Size([26641, 2])
We keep 1.01e+07/3.54e+08 =  2% of the original kernel matrix.

torch.Size([26537, 2])
We keep 6.13e+06/3.31e+08 =  1% of the original kernel matrix.

torch.Size([52532, 2])
We keep 3.00e+07/1.47e+09 =  2% of the original kernel matrix.

torch.Size([36421, 2])
We keep 1.15e+07/6.74e+08 =  1% of the original kernel matrix.

torch.Size([135790, 2])
We keep 2.41e+08/1.21e+10 =  1% of the original kernel matrix.

torch.Size([59234, 2])
We keep 2.86e+07/1.94e+09 =  1% of the original kernel matrix.

torch.Size([22244, 2])
We keep 1.85e+07/4.37e+08 =  4% of the original kernel matrix.

torch.Size([23139, 2])
We keep 7.00e+06/3.68e+08 =  1% of the original kernel matrix.

torch.Size([60876, 2])
We keep 3.20e+07/1.93e+09 =  1% of the original kernel matrix.

torch.Size([38964, 2])
We keep 1.29e+07/7.72e+08 =  1% of the original kernel matrix.

torch.Size([16870, 2])
We keep 3.98e+06/1.24e+08 =  3% of the original kernel matrix.

torch.Size([20694, 2])
We keep 4.20e+06/1.96e+08 =  2% of the original kernel matrix.

torch.Size([26690, 2])
We keep 2.63e+07/7.54e+08 =  3% of the original kernel matrix.

torch.Size([24553, 2])
We keep 8.79e+06/4.83e+08 =  1% of the original kernel matrix.

torch.Size([6162, 2])
We keep 4.77e+05/9.33e+06 =  5% of the original kernel matrix.

torch.Size([12659, 2])
We keep 1.60e+06/5.37e+07 =  2% of the original kernel matrix.

torch.Size([9770, 2])
We keep 1.97e+06/3.79e+07 =  5% of the original kernel matrix.

torch.Size([15502, 2])
We keep 2.74e+06/1.08e+08 =  2% of the original kernel matrix.

torch.Size([137738, 2])
We keep 2.65e+08/1.07e+10 =  2% of the original kernel matrix.

torch.Size([60086, 2])
We keep 2.69e+07/1.82e+09 =  1% of the original kernel matrix.

torch.Size([216537, 2])
We keep 5.09e+08/3.55e+10 =  1% of the original kernel matrix.

torch.Size([77035, 2])
We keep 4.64e+07/3.31e+09 =  1% of the original kernel matrix.

torch.Size([13683, 2])
We keep 4.38e+06/1.26e+08 =  3% of the original kernel matrix.

torch.Size([18208, 2])
We keep 4.29e+06/1.98e+08 =  2% of the original kernel matrix.

torch.Size([20119, 2])
We keep 5.34e+06/2.16e+08 =  2% of the original kernel matrix.

torch.Size([23142, 2])
We keep 5.04e+06/2.58e+08 =  1% of the original kernel matrix.

torch.Size([24853, 2])
We keep 8.17e+06/3.19e+08 =  2% of the original kernel matrix.

torch.Size([25500, 2])
We keep 6.16e+06/3.14e+08 =  1% of the original kernel matrix.

torch.Size([55763, 2])
We keep 6.46e+07/1.69e+09 =  3% of the original kernel matrix.

torch.Size([37700, 2])
We keep 1.21e+07/7.22e+08 =  1% of the original kernel matrix.

torch.Size([47206, 2])
We keep 2.16e+07/1.22e+09 =  1% of the original kernel matrix.

torch.Size([34653, 2])
We keep 1.07e+07/6.15e+08 =  1% of the original kernel matrix.

torch.Size([8333, 2])
We keep 9.07e+05/2.23e+07 =  4% of the original kernel matrix.

torch.Size([14494, 2])
We keep 2.16e+06/8.31e+07 =  2% of the original kernel matrix.

torch.Size([33098, 2])
We keep 1.31e+07/5.86e+08 =  2% of the original kernel matrix.

torch.Size([29719, 2])
We keep 7.58e+06/4.26e+08 =  1% of the original kernel matrix.

torch.Size([141746, 2])
We keep 1.68e+08/1.20e+10 =  1% of the original kernel matrix.

torch.Size([60997, 2])
We keep 2.84e+07/1.93e+09 =  1% of the original kernel matrix.

torch.Size([12756, 2])
We keep 2.79e+06/6.87e+07 =  4% of the original kernel matrix.

torch.Size([17756, 2])
We keep 3.45e+06/1.46e+08 =  2% of the original kernel matrix.

torch.Size([55326, 2])
We keep 5.02e+07/1.75e+09 =  2% of the original kernel matrix.

torch.Size([37106, 2])
We keep 1.27e+07/7.35e+08 =  1% of the original kernel matrix.

torch.Size([15735, 2])
We keep 6.22e+06/9.93e+07 =  6% of the original kernel matrix.

torch.Size([19979, 2])
We keep 3.70e+06/1.75e+08 =  2% of the original kernel matrix.

time for making ranges is 2.922008752822876
Sorting X and nu_X
time for sorting X is 0.08320355415344238
Sorting Z and nu_Z
time for sorting Z is 0.0002682209014892578
Starting Optim
sum tnu_Z before tensor(26988376., device='cuda:0')
c= tensor(1794.7571, device='cuda:0')
c= tensor(88957.7500, device='cuda:0')
c= tensor(92606.4297, device='cuda:0')
c= tensor(220890.3750, device='cuda:0')
c= tensor(418116.2500, device='cuda:0')
c= tensor(737577., device='cuda:0')
c= tensor(1205132.3750, device='cuda:0')
c= tensor(1483499.3750, device='cuda:0')
c= tensor(1523864.3750, device='cuda:0')
c= tensor(7083574., device='cuda:0')
c= tensor(7106994., device='cuda:0')
c= tensor(16100010., device='cuda:0')
c= tensor(16120583., device='cuda:0')
c= tensor(39408536., device='cuda:0')
c= tensor(39583924., device='cuda:0')
c= tensor(39982432., device='cuda:0')
c= tensor(40767640., device='cuda:0')
c= tensor(40943156., device='cuda:0')
c= tensor(46104900., device='cuda:0')
c= tensor(48683312., device='cuda:0')
c= tensor(49227412., device='cuda:0')
c= tensor(68441416., device='cuda:0')
c= tensor(68465256., device='cuda:0')
c= tensor(69399552., device='cuda:0')
c= tensor(69612808., device='cuda:0')
c= tensor(71224744., device='cuda:0')
c= tensor(72751104., device='cuda:0')
c= tensor(72791416., device='cuda:0')
c= tensor(73372592., device='cuda:0')
c= tensor(3.1460e+08, device='cuda:0')
c= tensor(3.1464e+08, device='cuda:0')
c= tensor(4.0439e+08, device='cuda:0')
c= tensor(4.0488e+08, device='cuda:0')
c= tensor(4.0491e+08, device='cuda:0')
c= tensor(4.0502e+08, device='cuda:0')
c= tensor(4.0829e+08, device='cuda:0')
c= tensor(4.1083e+08, device='cuda:0')
c= tensor(4.1083e+08, device='cuda:0')
c= tensor(4.1084e+08, device='cuda:0')
c= tensor(4.1084e+08, device='cuda:0')
c= tensor(4.1084e+08, device='cuda:0')
c= tensor(4.1085e+08, device='cuda:0')
c= tensor(4.1085e+08, device='cuda:0')
c= tensor(4.1086e+08, device='cuda:0')
c= tensor(4.1086e+08, device='cuda:0')
c= tensor(4.1086e+08, device='cuda:0')
c= tensor(4.1087e+08, device='cuda:0')
c= tensor(4.1087e+08, device='cuda:0')
c= tensor(4.1088e+08, device='cuda:0')
c= tensor(4.1093e+08, device='cuda:0')
c= tensor(4.1097e+08, device='cuda:0')
c= tensor(4.1097e+08, device='cuda:0')
c= tensor(4.1099e+08, device='cuda:0')
c= tensor(4.1100e+08, device='cuda:0')
c= tensor(4.1101e+08, device='cuda:0')
c= tensor(4.1104e+08, device='cuda:0')
c= tensor(4.1104e+08, device='cuda:0')
c= tensor(4.1105e+08, device='cuda:0')
c= tensor(4.1106e+08, device='cuda:0')
c= tensor(4.1106e+08, device='cuda:0')
c= tensor(4.1107e+08, device='cuda:0')
c= tensor(4.1107e+08, device='cuda:0')
c= tensor(4.1108e+08, device='cuda:0')
c= tensor(4.1110e+08, device='cuda:0')
c= tensor(4.1111e+08, device='cuda:0')
c= tensor(4.1111e+08, device='cuda:0')
c= tensor(4.1112e+08, device='cuda:0')
c= tensor(4.1113e+08, device='cuda:0')
c= tensor(4.1114e+08, device='cuda:0')
c= tensor(4.1115e+08, device='cuda:0')
c= tensor(4.1117e+08, device='cuda:0')
c= tensor(4.1118e+08, device='cuda:0')
c= tensor(4.1119e+08, device='cuda:0')
c= tensor(4.1119e+08, device='cuda:0')
c= tensor(4.1120e+08, device='cuda:0')
c= tensor(4.1121e+08, device='cuda:0')
c= tensor(4.1121e+08, device='cuda:0')
c= tensor(4.1121e+08, device='cuda:0')
c= tensor(4.1122e+08, device='cuda:0')
c= tensor(4.1133e+08, device='cuda:0')
c= tensor(4.1133e+08, device='cuda:0')
c= tensor(4.1133e+08, device='cuda:0')
c= tensor(4.1134e+08, device='cuda:0')
c= tensor(4.1134e+08, device='cuda:0')
c= tensor(4.1135e+08, device='cuda:0')
c= tensor(4.1135e+08, device='cuda:0')
c= tensor(4.1135e+08, device='cuda:0')
c= tensor(4.1135e+08, device='cuda:0')
c= tensor(4.1136e+08, device='cuda:0')
c= tensor(4.1137e+08, device='cuda:0')
c= tensor(4.1139e+08, device='cuda:0')
c= tensor(4.1139e+08, device='cuda:0')
c= tensor(4.1140e+08, device='cuda:0')
c= tensor(4.1140e+08, device='cuda:0')
c= tensor(4.1141e+08, device='cuda:0')
c= tensor(4.1142e+08, device='cuda:0')
c= tensor(4.1142e+08, device='cuda:0')
c= tensor(4.1144e+08, device='cuda:0')
c= tensor(4.1146e+08, device='cuda:0')
c= tensor(4.1146e+08, device='cuda:0')
c= tensor(4.1148e+08, device='cuda:0')
c= tensor(4.1148e+08, device='cuda:0')
c= tensor(4.1149e+08, device='cuda:0')
c= tensor(4.1150e+08, device='cuda:0')
c= tensor(4.1150e+08, device='cuda:0')
c= tensor(4.1151e+08, device='cuda:0')
c= tensor(4.1152e+08, device='cuda:0')
c= tensor(4.1152e+08, device='cuda:0')
c= tensor(4.1152e+08, device='cuda:0')
c= tensor(4.1152e+08, device='cuda:0')
c= tensor(4.1153e+08, device='cuda:0')
c= tensor(4.1153e+08, device='cuda:0')
c= tensor(4.1153e+08, device='cuda:0')
c= tensor(4.1154e+08, device='cuda:0')
c= tensor(4.1155e+08, device='cuda:0')
c= tensor(4.1156e+08, device='cuda:0')
c= tensor(4.1156e+08, device='cuda:0')
c= tensor(4.1156e+08, device='cuda:0')
c= tensor(4.1158e+08, device='cuda:0')
c= tensor(4.1158e+08, device='cuda:0')
c= tensor(4.1160e+08, device='cuda:0')
c= tensor(4.1160e+08, device='cuda:0')
c= tensor(4.1160e+08, device='cuda:0')
c= tensor(4.1161e+08, device='cuda:0')
c= tensor(4.1161e+08, device='cuda:0')
c= tensor(4.1161e+08, device='cuda:0')
c= tensor(4.1161e+08, device='cuda:0')
c= tensor(4.1162e+08, device='cuda:0')
c= tensor(4.1164e+08, device='cuda:0')
c= tensor(4.1164e+08, device='cuda:0')
c= tensor(4.1167e+08, device='cuda:0')
c= tensor(4.1167e+08, device='cuda:0')
c= tensor(4.1167e+08, device='cuda:0')
c= tensor(4.1168e+08, device='cuda:0')
c= tensor(4.1171e+08, device='cuda:0')
c= tensor(4.1171e+08, device='cuda:0')
c= tensor(4.1172e+08, device='cuda:0')
c= tensor(4.1172e+08, device='cuda:0')
c= tensor(4.1172e+08, device='cuda:0')
c= tensor(4.1172e+08, device='cuda:0')
c= tensor(4.1173e+08, device='cuda:0')
c= tensor(4.1173e+08, device='cuda:0')
c= tensor(4.1178e+08, device='cuda:0')
c= tensor(4.1180e+08, device='cuda:0')
c= tensor(4.1180e+08, device='cuda:0')
c= tensor(4.1180e+08, device='cuda:0')
c= tensor(4.1181e+08, device='cuda:0')
c= tensor(4.1181e+08, device='cuda:0')
c= tensor(4.1182e+08, device='cuda:0')
c= tensor(4.1182e+08, device='cuda:0')
c= tensor(4.1183e+08, device='cuda:0')
c= tensor(4.1183e+08, device='cuda:0')
c= tensor(4.1184e+08, device='cuda:0')
c= tensor(4.1187e+08, device='cuda:0')
c= tensor(4.1187e+08, device='cuda:0')
c= tensor(4.1198e+08, device='cuda:0')
c= tensor(4.1198e+08, device='cuda:0')
c= tensor(4.1199e+08, device='cuda:0')
c= tensor(4.1199e+08, device='cuda:0')
c= tensor(4.1199e+08, device='cuda:0')
c= tensor(4.1204e+08, device='cuda:0')
c= tensor(4.1204e+08, device='cuda:0')
c= tensor(4.1205e+08, device='cuda:0')
c= tensor(4.1205e+08, device='cuda:0')
c= tensor(4.1206e+08, device='cuda:0')
c= tensor(4.1206e+08, device='cuda:0')
c= tensor(4.1207e+08, device='cuda:0')
c= tensor(4.1207e+08, device='cuda:0')
c= tensor(4.1208e+08, device='cuda:0')
c= tensor(4.1208e+08, device='cuda:0')
c= tensor(4.1208e+08, device='cuda:0')
c= tensor(4.1208e+08, device='cuda:0')
c= tensor(4.1209e+08, device='cuda:0')
c= tensor(4.1210e+08, device='cuda:0')
c= tensor(4.1210e+08, device='cuda:0')
c= tensor(4.1330e+08, device='cuda:0')
c= tensor(4.1331e+08, device='cuda:0')
c= tensor(4.1332e+08, device='cuda:0')
c= tensor(4.1333e+08, device='cuda:0')
c= tensor(4.1333e+08, device='cuda:0')
c= tensor(4.1334e+08, device='cuda:0')
c= tensor(4.1335e+08, device='cuda:0')
c= tensor(4.1336e+08, device='cuda:0')
c= tensor(4.1336e+08, device='cuda:0')
c= tensor(4.1337e+08, device='cuda:0')
c= tensor(4.1337e+08, device='cuda:0')
c= tensor(4.1338e+08, device='cuda:0')
c= tensor(4.1339e+08, device='cuda:0')
c= tensor(4.1340e+08, device='cuda:0')
c= tensor(4.1341e+08, device='cuda:0')
c= tensor(4.1342e+08, device='cuda:0')
c= tensor(4.1342e+08, device='cuda:0')
c= tensor(4.1342e+08, device='cuda:0')
c= tensor(4.1346e+08, device='cuda:0')
c= tensor(4.1349e+08, device='cuda:0')
c= tensor(4.1350e+08, device='cuda:0')
c= tensor(4.1350e+08, device='cuda:0')
c= tensor(4.1350e+08, device='cuda:0')
c= tensor(4.1350e+08, device='cuda:0')
c= tensor(4.1351e+08, device='cuda:0')
c= tensor(4.1351e+08, device='cuda:0')
c= tensor(4.1352e+08, device='cuda:0')
c= tensor(4.1353e+08, device='cuda:0')
c= tensor(4.1353e+08, device='cuda:0')
c= tensor(4.1354e+08, device='cuda:0')
c= tensor(4.1355e+08, device='cuda:0')
c= tensor(4.1355e+08, device='cuda:0')
c= tensor(4.1356e+08, device='cuda:0')
c= tensor(4.1356e+08, device='cuda:0')
c= tensor(4.1359e+08, device='cuda:0')
c= tensor(4.1362e+08, device='cuda:0')
c= tensor(4.1363e+08, device='cuda:0')
c= tensor(4.1364e+08, device='cuda:0')
c= tensor(4.1364e+08, device='cuda:0')
c= tensor(4.1365e+08, device='cuda:0')
c= tensor(4.1365e+08, device='cuda:0')
c= tensor(4.1365e+08, device='cuda:0')
c= tensor(4.1365e+08, device='cuda:0')
c= tensor(4.1368e+08, device='cuda:0')
c= tensor(4.1368e+08, device='cuda:0')
c= tensor(4.1368e+08, device='cuda:0')
c= tensor(4.1369e+08, device='cuda:0')
c= tensor(4.1370e+08, device='cuda:0')
c= tensor(4.1370e+08, device='cuda:0')
c= tensor(4.1371e+08, device='cuda:0')
c= tensor(4.1372e+08, device='cuda:0')
c= tensor(4.1372e+08, device='cuda:0')
c= tensor(4.1372e+08, device='cuda:0')
c= tensor(4.1373e+08, device='cuda:0')
c= tensor(4.1373e+08, device='cuda:0')
c= tensor(4.1374e+08, device='cuda:0')
c= tensor(4.1374e+08, device='cuda:0')
c= tensor(4.1375e+08, device='cuda:0')
c= tensor(4.1375e+08, device='cuda:0')
c= tensor(4.1376e+08, device='cuda:0')
c= tensor(4.1376e+08, device='cuda:0')
c= tensor(4.1377e+08, device='cuda:0')
c= tensor(4.1379e+08, device='cuda:0')
c= tensor(4.1380e+08, device='cuda:0')
c= tensor(4.1397e+08, device='cuda:0')
c= tensor(4.1538e+08, device='cuda:0')
c= tensor(4.1542e+08, device='cuda:0')
c= tensor(4.1543e+08, device='cuda:0')
c= tensor(4.1543e+08, device='cuda:0')
c= tensor(4.1545e+08, device='cuda:0')
c= tensor(4.1586e+08, device='cuda:0')
c= tensor(4.2385e+08, device='cuda:0')
c= tensor(4.2385e+08, device='cuda:0')
c= tensor(4.2909e+08, device='cuda:0')
c= tensor(4.3009e+08, device='cuda:0')
c= tensor(4.3039e+08, device='cuda:0')
c= tensor(4.3177e+08, device='cuda:0')
c= tensor(4.3178e+08, device='cuda:0')
c= tensor(4.3179e+08, device='cuda:0')
c= tensor(4.3562e+08, device='cuda:0')
c= tensor(5.5623e+08, device='cuda:0')
c= tensor(5.5623e+08, device='cuda:0')
c= tensor(5.5651e+08, device='cuda:0')
c= tensor(5.5686e+08, device='cuda:0')
c= tensor(5.5953e+08, device='cuda:0')
c= tensor(5.6338e+08, device='cuda:0')
c= tensor(5.6546e+08, device='cuda:0')
c= tensor(5.6634e+08, device='cuda:0')
c= tensor(5.6664e+08, device='cuda:0')
c= tensor(5.6665e+08, device='cuda:0')
c= tensor(5.7810e+08, device='cuda:0')
c= tensor(5.7828e+08, device='cuda:0')
c= tensor(5.7828e+08, device='cuda:0')
c= tensor(5.7847e+08, device='cuda:0')
c= tensor(5.8027e+08, device='cuda:0')
c= tensor(5.8505e+08, device='cuda:0')
c= tensor(5.8682e+08, device='cuda:0')
c= tensor(5.8683e+08, device='cuda:0')
c= tensor(5.8699e+08, device='cuda:0')
c= tensor(5.8701e+08, device='cuda:0')
c= tensor(5.8787e+08, device='cuda:0')
c= tensor(5.9312e+08, device='cuda:0')
c= tensor(5.9320e+08, device='cuda:0')
c= tensor(5.9499e+08, device='cuda:0')
c= tensor(5.9499e+08, device='cuda:0')
c= tensor(5.9500e+08, device='cuda:0')
c= tensor(5.9657e+08, device='cuda:0')
c= tensor(5.9742e+08, device='cuda:0')
c= tensor(5.9913e+08, device='cuda:0')
c= tensor(5.9914e+08, device='cuda:0')
c= tensor(6.2199e+08, device='cuda:0')
c= tensor(6.2210e+08, device='cuda:0')
c= tensor(6.2237e+08, device='cuda:0')
c= tensor(6.2342e+08, device='cuda:0')
c= tensor(6.2342e+08, device='cuda:0')
c= tensor(6.2605e+08, device='cuda:0')
c= tensor(6.3717e+08, device='cuda:0')
c= tensor(6.6033e+08, device='cuda:0')
c= tensor(6.6062e+08, device='cuda:0')
c= tensor(6.6081e+08, device='cuda:0')
c= tensor(6.6084e+08, device='cuda:0')
c= tensor(6.6086e+08, device='cuda:0')
c= tensor(6.6459e+08, device='cuda:0')
c= tensor(6.6463e+08, device='cuda:0')
c= tensor(6.6503e+08, device='cuda:0')
c= tensor(6.7171e+08, device='cuda:0')
c= tensor(6.7609e+08, device='cuda:0')
c= tensor(6.7619e+08, device='cuda:0')
c= tensor(6.7620e+08, device='cuda:0')
c= tensor(6.8062e+08, device='cuda:0')
c= tensor(6.9000e+08, device='cuda:0')
c= tensor(6.9084e+08, device='cuda:0')
c= tensor(6.9085e+08, device='cuda:0')
c= tensor(6.9332e+08, device='cuda:0')
c= tensor(6.9340e+08, device='cuda:0')
c= tensor(7.0334e+08, device='cuda:0')
c= tensor(7.0340e+08, device='cuda:0')
c= tensor(7.0613e+08, device='cuda:0')
c= tensor(7.0659e+08, device='cuda:0')
c= tensor(7.1156e+08, device='cuda:0')
c= tensor(7.1242e+08, device='cuda:0')
c= tensor(7.1246e+08, device='cuda:0')
c= tensor(7.1642e+08, device='cuda:0')
c= tensor(7.2174e+08, device='cuda:0')
c= tensor(7.2176e+08, device='cuda:0')
c= tensor(7.2851e+08, device='cuda:0')
c= tensor(7.3533e+08, device='cuda:0')
c= tensor(7.5982e+08, device='cuda:0')
c= tensor(7.6019e+08, device='cuda:0')
c= tensor(7.6020e+08, device='cuda:0')
c= tensor(7.6025e+08, device='cuda:0')
c= tensor(7.6237e+08, device='cuda:0')
c= tensor(7.6263e+08, device='cuda:0')
c= tensor(7.6339e+08, device='cuda:0')
c= tensor(7.6339e+08, device='cuda:0')
c= tensor(7.6443e+08, device='cuda:0')
c= tensor(7.6857e+08, device='cuda:0')
c= tensor(7.6901e+08, device='cuda:0')
c= tensor(7.6906e+08, device='cuda:0')
c= tensor(7.6975e+08, device='cuda:0')
c= tensor(7.6984e+08, device='cuda:0')
c= tensor(7.6987e+08, device='cuda:0')
c= tensor(7.6991e+08, device='cuda:0')
c= tensor(7.6991e+08, device='cuda:0')
c= tensor(7.7050e+08, device='cuda:0')
c= tensor(7.7085e+08, device='cuda:0')
c= tensor(7.7101e+08, device='cuda:0')
c= tensor(7.7163e+08, device='cuda:0')
c= tensor(7.7165e+08, device='cuda:0')
c= tensor(1.3501e+09, device='cuda:0')
c= tensor(1.3501e+09, device='cuda:0')
c= tensor(1.3528e+09, device='cuda:0')
c= tensor(1.3528e+09, device='cuda:0')
c= tensor(1.3528e+09, device='cuda:0')
c= tensor(1.3528e+09, device='cuda:0')
c= tensor(1.3530e+09, device='cuda:0')
c= tensor(1.3530e+09, device='cuda:0')
c= tensor(1.3569e+09, device='cuda:0')
c= tensor(1.3569e+09, device='cuda:0')
c= tensor(1.3569e+09, device='cuda:0')
c= tensor(1.3653e+09, device='cuda:0')
c= tensor(1.3664e+09, device='cuda:0')
c= tensor(1.3669e+09, device='cuda:0')
c= tensor(1.3713e+09, device='cuda:0')
c= tensor(1.3844e+09, device='cuda:0')
c= tensor(1.3844e+09, device='cuda:0')
c= tensor(1.3844e+09, device='cuda:0')
c= tensor(1.3847e+09, device='cuda:0')
c= tensor(1.3847e+09, device='cuda:0')
c= tensor(1.3847e+09, device='cuda:0')
c= tensor(1.3848e+09, device='cuda:0')
c= tensor(1.3848e+09, device='cuda:0')
c= tensor(1.3848e+09, device='cuda:0')
c= tensor(1.3848e+09, device='cuda:0')
c= tensor(1.3848e+09, device='cuda:0')
c= tensor(1.3960e+09, device='cuda:0')
c= tensor(1.3961e+09, device='cuda:0')
c= tensor(1.3977e+09, device='cuda:0')
c= tensor(1.3977e+09, device='cuda:0')
c= tensor(1.3977e+09, device='cuda:0')
c= tensor(1.3983e+09, device='cuda:0')
c= tensor(1.5951e+09, device='cuda:0')
c= tensor(1.6356e+09, device='cuda:0')
c= tensor(1.6357e+09, device='cuda:0')
c= tensor(1.6363e+09, device='cuda:0')
c= tensor(1.6363e+09, device='cuda:0')
c= tensor(1.6364e+09, device='cuda:0')
c= tensor(1.6508e+09, device='cuda:0')
c= tensor(1.6510e+09, device='cuda:0')
c= tensor(1.6510e+09, device='cuda:0')
c= tensor(1.6523e+09, device='cuda:0')
c= tensor(1.7199e+09, device='cuda:0')
c= tensor(1.7201e+09, device='cuda:0')
c= tensor(1.7202e+09, device='cuda:0')
c= tensor(1.7203e+09, device='cuda:0')
c= tensor(1.7204e+09, device='cuda:0')
c= tensor(1.7204e+09, device='cuda:0')
c= tensor(1.7259e+09, device='cuda:0')
c= tensor(1.7261e+09, device='cuda:0')
c= tensor(1.7261e+09, device='cuda:0')
c= tensor(1.7275e+09, device='cuda:0')
c= tensor(1.7275e+09, device='cuda:0')
c= tensor(1.7276e+09, device='cuda:0')
c= tensor(1.7282e+09, device='cuda:0')
c= tensor(1.7309e+09, device='cuda:0')
c= tensor(1.7406e+09, device='cuda:0')
c= tensor(1.7488e+09, device='cuda:0')
c= tensor(1.7569e+09, device='cuda:0')
c= tensor(1.7570e+09, device='cuda:0')
c= tensor(1.7582e+09, device='cuda:0')
c= tensor(1.7616e+09, device='cuda:0')
c= tensor(1.7712e+09, device='cuda:0')
c= tensor(1.7713e+09, device='cuda:0')
c= tensor(1.8094e+09, device='cuda:0')
c= tensor(1.9382e+09, device='cuda:0')
c= tensor(1.9503e+09, device='cuda:0')
c= tensor(1.9547e+09, device='cuda:0')
c= tensor(1.9573e+09, device='cuda:0')
c= tensor(1.9573e+09, device='cuda:0')
c= tensor(1.9573e+09, device='cuda:0')
c= tensor(1.9574e+09, device='cuda:0')
c= tensor(1.9602e+09, device='cuda:0')
c= tensor(1.9624e+09, device='cuda:0')
c= tensor(1.9854e+09, device='cuda:0')
c= tensor(1.9883e+09, device='cuda:0')
c= tensor(1.9892e+09, device='cuda:0')
c= tensor(1.9894e+09, device='cuda:0')
c= tensor(1.9915e+09, device='cuda:0')
c= tensor(1.9915e+09, device='cuda:0')
c= tensor(1.9915e+09, device='cuda:0')
c= tensor(1.9943e+09, device='cuda:0')
c= tensor(1.9946e+09, device='cuda:0')
c= tensor(1.9946e+09, device='cuda:0')
c= tensor(1.9947e+09, device='cuda:0')
c= tensor(2.1037e+09, device='cuda:0')
c= tensor(2.1038e+09, device='cuda:0')
c= tensor(2.1088e+09, device='cuda:0')
c= tensor(2.1088e+09, device='cuda:0')
c= tensor(2.1089e+09, device='cuda:0')
c= tensor(2.1089e+09, device='cuda:0')
c= tensor(2.1089e+09, device='cuda:0')
c= tensor(2.1090e+09, device='cuda:0')
c= tensor(2.1094e+09, device='cuda:0')
c= tensor(2.1095e+09, device='cuda:0')
c= tensor(2.1185e+09, device='cuda:0')
c= tensor(2.1185e+09, device='cuda:0')
c= tensor(2.1205e+09, device='cuda:0')
c= tensor(2.1205e+09, device='cuda:0')
c= tensor(2.1218e+09, device='cuda:0')
c= tensor(2.1218e+09, device='cuda:0')
c= tensor(2.1219e+09, device='cuda:0')
c= tensor(2.1220e+09, device='cuda:0')
c= tensor(2.1221e+09, device='cuda:0')
c= tensor(2.1234e+09, device='cuda:0')
c= tensor(2.2909e+09, device='cuda:0')
c= tensor(2.2910e+09, device='cuda:0')
c= tensor(2.2910e+09, device='cuda:0')
c= tensor(2.2915e+09, device='cuda:0')
c= tensor(2.2916e+09, device='cuda:0')
c= tensor(2.3230e+09, device='cuda:0')
c= tensor(2.3230e+09, device='cuda:0')
c= tensor(2.3251e+09, device='cuda:0')
c= tensor(2.3317e+09, device='cuda:0')
c= tensor(2.3317e+09, device='cuda:0')
c= tensor(2.3654e+09, device='cuda:0')
c= tensor(2.3658e+09, device='cuda:0')
c= tensor(2.4225e+09, device='cuda:0')
c= tensor(2.4226e+09, device='cuda:0')
c= tensor(2.4227e+09, device='cuda:0')
c= tensor(2.4228e+09, device='cuda:0')
c= tensor(2.4228e+09, device='cuda:0')
c= tensor(2.4228e+09, device='cuda:0')
c= tensor(2.4235e+09, device='cuda:0')
c= tensor(2.4235e+09, device='cuda:0')
c= tensor(2.4260e+09, device='cuda:0')
c= tensor(2.4261e+09, device='cuda:0')
c= tensor(2.4261e+09, device='cuda:0')
c= tensor(2.4262e+09, device='cuda:0')
c= tensor(2.4339e+09, device='cuda:0')
c= tensor(2.4380e+09, device='cuda:0')
c= tensor(2.4521e+09, device='cuda:0')
c= tensor(2.4523e+09, device='cuda:0')
c= tensor(2.4523e+09, device='cuda:0')
c= tensor(2.4523e+09, device='cuda:0')
c= tensor(2.4523e+09, device='cuda:0')
c= tensor(2.4824e+09, device='cuda:0')
c= tensor(2.4824e+09, device='cuda:0')
c= tensor(2.4826e+09, device='cuda:0')
c= tensor(2.4836e+09, device='cuda:0')
c= tensor(2.4844e+09, device='cuda:0')
c= tensor(2.4844e+09, device='cuda:0')
c= tensor(2.4845e+09, device='cuda:0')
c= tensor(2.4974e+09, device='cuda:0')
c= tensor(2.4979e+09, device='cuda:0')
c= tensor(2.4980e+09, device='cuda:0')
c= tensor(2.4982e+09, device='cuda:0')
c= tensor(2.4994e+09, device='cuda:0')
c= tensor(2.5023e+09, device='cuda:0')
c= tensor(2.5348e+09, device='cuda:0')
c= tensor(2.5375e+09, device='cuda:0')
c= tensor(2.5375e+09, device='cuda:0')
c= tensor(2.5381e+09, device='cuda:0')
c= tensor(2.5388e+09, device='cuda:0')
c= tensor(2.5388e+09, device='cuda:0')
c= tensor(2.5388e+09, device='cuda:0')
c= tensor(2.5388e+09, device='cuda:0')
c= tensor(2.5408e+09, device='cuda:0')
c= tensor(2.5409e+09, device='cuda:0')
c= tensor(2.5409e+09, device='cuda:0')
c= tensor(2.5410e+09, device='cuda:0')
c= tensor(2.5410e+09, device='cuda:0')
c= tensor(2.5412e+09, device='cuda:0')
c= tensor(2.5412e+09, device='cuda:0')
c= tensor(2.5415e+09, device='cuda:0')
c= tensor(2.5424e+09, device='cuda:0')
c= tensor(2.5425e+09, device='cuda:0')
c= tensor(2.5425e+09, device='cuda:0')
c= tensor(2.5426e+09, device='cuda:0')
c= tensor(2.5428e+09, device='cuda:0')
c= tensor(2.5532e+09, device='cuda:0')
c= tensor(2.5532e+09, device='cuda:0')
c= tensor(2.5532e+09, device='cuda:0')
c= tensor(2.5532e+09, device='cuda:0')
c= tensor(2.5562e+09, device='cuda:0')
c= tensor(2.5736e+09, device='cuda:0')
c= tensor(2.5750e+09, device='cuda:0')
c= tensor(2.5750e+09, device='cuda:0')
c= tensor(2.5807e+09, device='cuda:0')
c= tensor(2.5835e+09, device='cuda:0')
c= tensor(2.5835e+09, device='cuda:0')
c= tensor(2.5836e+09, device='cuda:0')
c= tensor(2.5838e+09, device='cuda:0')
c= tensor(2.5872e+09, device='cuda:0')
c= tensor(2.5932e+09, device='cuda:0')
c= tensor(2.5987e+09, device='cuda:0')
c= tensor(2.5989e+09, device='cuda:0')
c= tensor(2.5989e+09, device='cuda:0')
c= tensor(2.5989e+09, device='cuda:0')
c= tensor(2.5992e+09, device='cuda:0')
c= tensor(2.5992e+09, device='cuda:0')
c= tensor(2.5996e+09, device='cuda:0')
c= tensor(2.6035e+09, device='cuda:0')
c= tensor(2.6392e+09, device='cuda:0')
c= tensor(2.6392e+09, device='cuda:0')
c= tensor(2.6392e+09, device='cuda:0')
c= tensor(2.6393e+09, device='cuda:0')
c= tensor(2.6412e+09, device='cuda:0')
c= tensor(2.6420e+09, device='cuda:0')
c= tensor(2.6422e+09, device='cuda:0')
c= tensor(2.6422e+09, device='cuda:0')
c= tensor(2.6427e+09, device='cuda:0')
c= tensor(2.6427e+09, device='cuda:0')
c= tensor(2.6437e+09, device='cuda:0')
c= tensor(2.6437e+09, device='cuda:0')
c= tensor(2.6438e+09, device='cuda:0')
c= tensor(2.6438e+09, device='cuda:0')
c= tensor(2.6438e+09, device='cuda:0')
c= tensor(2.6438e+09, device='cuda:0')
c= tensor(2.6479e+09, device='cuda:0')
c= tensor(2.6601e+09, device='cuda:0')
c= tensor(2.6630e+09, device='cuda:0')
c= tensor(2.6668e+09, device='cuda:0')
c= tensor(2.6670e+09, device='cuda:0')
c= tensor(2.6671e+09, device='cuda:0')
c= tensor(2.6671e+09, device='cuda:0')
c= tensor(2.6695e+09, device='cuda:0')
c= tensor(2.6699e+09, device='cuda:0')
c= tensor(2.6704e+09, device='cuda:0')
c= tensor(2.6705e+09, device='cuda:0')
c= tensor(2.8266e+09, device='cuda:0')
c= tensor(2.8268e+09, device='cuda:0')
c= tensor(2.8276e+09, device='cuda:0')
c= tensor(2.8332e+09, device='cuda:0')
c= tensor(2.8340e+09, device='cuda:0')
c= tensor(2.8342e+09, device='cuda:0')
c= tensor(2.9035e+09, device='cuda:0')
c= tensor(2.9070e+09, device='cuda:0')
c= tensor(2.9081e+09, device='cuda:0')
c= tensor(2.9083e+09, device='cuda:0')
c= tensor(2.9090e+09, device='cuda:0')
c= tensor(2.9090e+09, device='cuda:0')
c= tensor(2.9169e+09, device='cuda:0')
c= tensor(3.0820e+09, device='cuda:0')
c= tensor(3.0870e+09, device='cuda:0')
c= tensor(3.0922e+09, device='cuda:0')
c= tensor(3.0965e+09, device='cuda:0')
c= tensor(3.0969e+09, device='cuda:0')
c= tensor(3.0972e+09, device='cuda:0')
c= tensor(3.0973e+09, device='cuda:0')
c= tensor(3.0982e+09, device='cuda:0')
c= tensor(3.1155e+09, device='cuda:0')
c= tensor(3.1174e+09, device='cuda:0')
c= tensor(3.1557e+09, device='cuda:0')
c= tensor(3.1619e+09, device='cuda:0')
c= tensor(3.1622e+09, device='cuda:0')
c= tensor(3.1622e+09, device='cuda:0')
c= tensor(3.1631e+09, device='cuda:0')
c= tensor(3.1671e+09, device='cuda:0')
c= tensor(3.1672e+09, device='cuda:0')
c= tensor(3.1931e+09, device='cuda:0')
c= tensor(3.1939e+09, device='cuda:0')
c= tensor(3.1942e+09, device='cuda:0')
c= tensor(3.1972e+09, device='cuda:0')
c= tensor(3.1972e+09, device='cuda:0')
c= tensor(3.1972e+09, device='cuda:0')
c= tensor(3.1972e+09, device='cuda:0')
c= tensor(3.1973e+09, device='cuda:0')
c= tensor(3.1979e+09, device='cuda:0')
c= tensor(4.6791e+09, device='cuda:0')
c= tensor(4.6795e+09, device='cuda:0')
c= tensor(4.6824e+09, device='cuda:0')
c= tensor(4.6824e+09, device='cuda:0')
c= tensor(4.6825e+09, device='cuda:0')
c= tensor(4.6825e+09, device='cuda:0')
c= tensor(4.6947e+09, device='cuda:0')
c= tensor(4.6949e+09, device='cuda:0')
c= tensor(4.7749e+09, device='cuda:0')
c= tensor(4.7749e+09, device='cuda:0')
c= tensor(4.7800e+09, device='cuda:0')
c= tensor(4.7803e+09, device='cuda:0')
c= tensor(4.7857e+09, device='cuda:0')
c= tensor(4.8184e+09, device='cuda:0')
c= tensor(4.8186e+09, device='cuda:0')
c= tensor(4.8186e+09, device='cuda:0')
c= tensor(4.8200e+09, device='cuda:0')
c= tensor(4.8201e+09, device='cuda:0')
c= tensor(4.8204e+09, device='cuda:0')
c= tensor(4.8224e+09, device='cuda:0')
c= tensor(4.8225e+09, device='cuda:0')
c= tensor(4.8236e+09, device='cuda:0')
c= tensor(4.8241e+09, device='cuda:0')
c= tensor(4.8277e+09, device='cuda:0')
c= tensor(4.8356e+09, device='cuda:0')
c= tensor(4.8361e+09, device='cuda:0')
c= tensor(4.8362e+09, device='cuda:0')
c= tensor(4.8408e+09, device='cuda:0')
c= tensor(4.8421e+09, device='cuda:0')
c= tensor(4.8452e+09, device='cuda:0')
c= tensor(4.8454e+09, device='cuda:0')
c= tensor(4.8460e+09, device='cuda:0')
c= tensor(4.8464e+09, device='cuda:0')
c= tensor(4.8596e+09, device='cuda:0')
c= tensor(4.8620e+09, device='cuda:0')
c= tensor(4.8620e+09, device='cuda:0')
c= tensor(4.8620e+09, device='cuda:0')
c= tensor(4.8621e+09, device='cuda:0')
c= tensor(4.8639e+09, device='cuda:0')
c= tensor(4.8664e+09, device='cuda:0')
c= tensor(4.8675e+09, device='cuda:0')
c= tensor(4.8675e+09, device='cuda:0')
c= tensor(4.8677e+09, device='cuda:0')
c= tensor(4.8681e+09, device='cuda:0')
c= tensor(4.8693e+09, device='cuda:0')
c= tensor(4.8697e+09, device='cuda:0')
c= tensor(4.8705e+09, device='cuda:0')
c= tensor(4.8707e+09, device='cuda:0')
c= tensor(4.8712e+09, device='cuda:0')
c= tensor(4.8712e+09, device='cuda:0')
c= tensor(4.8712e+09, device='cuda:0')
c= tensor(4.8766e+09, device='cuda:0')
c= tensor(4.8768e+09, device='cuda:0')
c= tensor(4.8783e+09, device='cuda:0')
c= tensor(4.8783e+09, device='cuda:0')
c= tensor(4.8783e+09, device='cuda:0')
c= tensor(4.8793e+09, device='cuda:0')
c= tensor(4.8798e+09, device='cuda:0')
c= tensor(4.8800e+09, device='cuda:0')
c= tensor(4.8800e+09, device='cuda:0')
c= tensor(4.8800e+09, device='cuda:0')
c= tensor(4.8805e+09, device='cuda:0')
c= tensor(4.8807e+09, device='cuda:0')
c= tensor(4.8825e+09, device='cuda:0')
c= tensor(4.8826e+09, device='cuda:0')
c= tensor(4.8832e+09, device='cuda:0')
c= tensor(4.8837e+09, device='cuda:0')
c= tensor(4.8837e+09, device='cuda:0')
c= tensor(4.8918e+09, device='cuda:0')
c= tensor(4.8923e+09, device='cuda:0')
c= tensor(4.8936e+09, device='cuda:0')
c= tensor(4.8961e+09, device='cuda:0')
c= tensor(4.9002e+09, device='cuda:0')
c= tensor(4.9041e+09, device='cuda:0')
c= tensor(4.9065e+09, device='cuda:0')
c= tensor(4.9065e+09, device='cuda:0')
c= tensor(4.9069e+09, device='cuda:0')
c= tensor(4.9072e+09, device='cuda:0')
c= tensor(4.9077e+09, device='cuda:0')
c= tensor(4.9143e+09, device='cuda:0')
c= tensor(4.9146e+09, device='cuda:0')
c= tensor(4.9154e+09, device='cuda:0')
c= tensor(4.9156e+09, device='cuda:0')
c= tensor(4.9179e+09, device='cuda:0')
c= tensor(4.9179e+09, device='cuda:0')
c= tensor(4.9180e+09, device='cuda:0')
c= tensor(4.9255e+09, device='cuda:0')
c= tensor(4.9430e+09, device='cuda:0')
c= tensor(4.9431e+09, device='cuda:0')
c= tensor(4.9433e+09, device='cuda:0')
c= tensor(4.9434e+09, device='cuda:0')
c= tensor(4.9454e+09, device='cuda:0')
c= tensor(4.9457e+09, device='cuda:0')
c= tensor(4.9458e+09, device='cuda:0')
c= tensor(4.9461e+09, device='cuda:0')
c= tensor(4.9501e+09, device='cuda:0')
c= tensor(4.9502e+09, device='cuda:0')
c= tensor(4.9511e+09, device='cuda:0')
c= tensor(4.9513e+09, device='cuda:0')
memory (bytes)
4463300608
time for making loss 2 is 15.21117115020752
p0 True
it  0 : 1591540224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 44% |
shape of L is 
torch.Size([])
memory (bytes)
4463562752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
memory (bytes)
4464128000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 11% |
error is  46298075000.0
relative error loss 9.350774
shape of L is 
torch.Size([])
memory (bytes)
4683132928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  4% | 11% |
memory (bytes)
4683137024
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  46297830000.0
relative error loss 9.350724
shape of L is 
torch.Size([])
memory (bytes)
4684943360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4685004800
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  46296260000.0
relative error loss 9.350408
shape of L is 
torch.Size([])
memory (bytes)
4685959168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 11% |
memory (bytes)
4685959168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 11% |
error is  46288036000.0
relative error loss 9.348746
shape of L is 
torch.Size([])
memory (bytes)
4687093760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4687151104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  46197735000.0
relative error loss 9.330508
shape of L is 
torch.Size([])
memory (bytes)
4688334848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4688334848
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  45748044000.0
relative error loss 9.239684
shape of L is 
torch.Size([])
memory (bytes)
4690632704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 11% |
memory (bytes)
4690694144
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  43341190000.0
relative error loss 8.753574
shape of L is 
torch.Size([])
memory (bytes)
4692742144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4692742144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  32011198000.0
relative error loss 6.465268
shape of L is 
torch.Size([])
memory (bytes)
4694925312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 11% |
memory (bytes)
4694945792
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  11265021000.0
relative error loss 2.2751844
shape of L is 
torch.Size([])
memory (bytes)
4697018368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 11% |
memory (bytes)
4697018368
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  7124890600.0
relative error loss 1.4390067
time to take a step is 249.56488585472107
it  1 : 1938165760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
4699119616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 11% |
memory (bytes)
4699209728
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  7124890600.0
relative error loss 1.4390067
shape of L is 
torch.Size([])
memory (bytes)
4701302784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
4701302784
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  5473833000.0
relative error loss 1.1055443
shape of L is 
torch.Size([])
memory (bytes)
4703416320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4703416320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 89% | 11% |
error is  6008977000.0
relative error loss 1.2136267
shape of L is 
torch.Size([])
memory (bytes)
4705693696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
4705693696
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  4816960000.0
relative error loss 0.97287637
shape of L is 
torch.Size([])
memory (bytes)
4707725312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4707725312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  4722173400.0
relative error loss 0.95373243
shape of L is 
torch.Size([])
memory (bytes)
4709732352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
4709732352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  4437113300.0
relative error loss 0.8961591
shape of L is 
torch.Size([])
memory (bytes)
4712083456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4712083456
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  4324772000.0
relative error loss 0.87346965
shape of L is 
torch.Size([])
memory (bytes)
4713979904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4713979904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  4239634400.0
relative error loss 0.85627455
shape of L is 
torch.Size([])
memory (bytes)
4716126208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
4716126208
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  4155306500.0
relative error loss 0.8392429
shape of L is 
torch.Size([])
memory (bytes)
4718395392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
4718415872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  3948639700.0
relative error loss 0.79750264
time to take a step is 236.048810005188
it  2 : 2036930560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
4720451584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4720451584
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  3948639700.0
relative error loss 0.79750264
shape of L is 
torch.Size([])
memory (bytes)
4722409472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
4722409472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  5400806400.0
relative error loss 1.0907952
shape of L is 
torch.Size([])
memory (bytes)
4724613120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
4724776960
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  3649806000.0
relative error loss 0.7371475
shape of L is 
torch.Size([])
memory (bytes)
4726853632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
4726853632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  3312992500.0
relative error loss 0.6691216
shape of L is 
torch.Size([])
memory (bytes)
4729016320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4729016320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  3038388700.0
relative error loss 0.6136602
shape of L is 
torch.Size([])
memory (bytes)
4730929152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4731133952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  2854941200.0
relative error loss 0.5766095
shape of L is 
torch.Size([])
memory (bytes)
4733210624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4733272064
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  2650575400.0
relative error loss 0.53533393
shape of L is 
torch.Size([])
memory (bytes)
4735242240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4735242240
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  2498121500.0
relative error loss 0.50454295
shape of L is 
torch.Size([])
memory (bytes)
4737425408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
4737425408
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  2336095200.0
relative error loss 0.47181872
shape of L is 
torch.Size([])
memory (bytes)
4739657728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
4739657728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  2132056000.0
relative error loss 0.43060914
time to take a step is 224.12223267555237
it  3 : 2035980800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
4741726208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4741726208
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  2132056000.0
relative error loss 0.43060914
shape of L is 
torch.Size([])
memory (bytes)
4743696384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4743696384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1993514500.0
relative error loss 0.40262803
shape of L is 
torch.Size([])
memory (bytes)
4746121216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
4746121216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% | 11% |
error is  1835439600.0
relative error loss 0.37070182
shape of L is 
torch.Size([])
memory (bytes)
4748201984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
4748201984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1640701700.0
relative error loss 0.3313708
shape of L is 
torch.Size([])
memory (bytes)
4750262272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4750262272
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1534496800.0
relative error loss 0.3099207
shape of L is 
torch.Size([])
memory (bytes)
4752510976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 11% |
memory (bytes)
4752510976
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1399734000.0
relative error loss 0.2827028
shape of L is 
torch.Size([])
memory (bytes)
4754690048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
4754690048
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1303575000.0
relative error loss 0.26328167
shape of L is 
torch.Size([])
memory (bytes)
4756729856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
4756729856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1243457000.0
relative error loss 0.2511397
shape of L is 
torch.Size([])
memory (bytes)
4758949888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
4758949888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1120009200.0
relative error loss 0.22620709
shape of L is 
torch.Size([])
memory (bytes)
4761055232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
4761116672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1058749950.0
relative error loss 0.21383461
time to take a step is 224.89937925338745
c= tensor(1794.7571, device='cuda:0')
c= tensor(88957.7500, device='cuda:0')
c= tensor(92606.4297, device='cuda:0')
c= tensor(220890.3750, device='cuda:0')
c= tensor(418116.2500, device='cuda:0')
c= tensor(737577., device='cuda:0')
c= tensor(1205132.3750, device='cuda:0')
c= tensor(1483499.3750, device='cuda:0')
c= tensor(1523864.3750, device='cuda:0')
c= tensor(7083574., device='cuda:0')
c= tensor(7106994., device='cuda:0')
c= tensor(16100010., device='cuda:0')
c= tensor(16120583., device='cuda:0')
c= tensor(39408536., device='cuda:0')
c= tensor(39583924., device='cuda:0')
c= tensor(39982432., device='cuda:0')
c= tensor(40767640., device='cuda:0')
c= tensor(40943156., device='cuda:0')
c= tensor(46104900., device='cuda:0')
c= tensor(48683312., device='cuda:0')
c= tensor(49227412., device='cuda:0')
c= tensor(68441416., device='cuda:0')
c= tensor(68465256., device='cuda:0')
c= tensor(69399552., device='cuda:0')
c= tensor(69612808., device='cuda:0')
c= tensor(71224744., device='cuda:0')
c= tensor(72751104., device='cuda:0')
c= tensor(72791416., device='cuda:0')
c= tensor(73372592., device='cuda:0')
c= tensor(3.1460e+08, device='cuda:0')
c= tensor(3.1464e+08, device='cuda:0')
c= tensor(4.0439e+08, device='cuda:0')
c= tensor(4.0488e+08, device='cuda:0')
c= tensor(4.0491e+08, device='cuda:0')
c= tensor(4.0502e+08, device='cuda:0')
c= tensor(4.0829e+08, device='cuda:0')
c= tensor(4.1083e+08, device='cuda:0')
c= tensor(4.1083e+08, device='cuda:0')
c= tensor(4.1084e+08, device='cuda:0')
c= tensor(4.1084e+08, device='cuda:0')
c= tensor(4.1084e+08, device='cuda:0')
c= tensor(4.1085e+08, device='cuda:0')
c= tensor(4.1085e+08, device='cuda:0')
c= tensor(4.1086e+08, device='cuda:0')
c= tensor(4.1086e+08, device='cuda:0')
c= tensor(4.1086e+08, device='cuda:0')
c= tensor(4.1087e+08, device='cuda:0')
c= tensor(4.1087e+08, device='cuda:0')
c= tensor(4.1088e+08, device='cuda:0')
c= tensor(4.1093e+08, device='cuda:0')
c= tensor(4.1097e+08, device='cuda:0')
c= tensor(4.1097e+08, device='cuda:0')
c= tensor(4.1099e+08, device='cuda:0')
c= tensor(4.1100e+08, device='cuda:0')
c= tensor(4.1101e+08, device='cuda:0')
c= tensor(4.1104e+08, device='cuda:0')
c= tensor(4.1104e+08, device='cuda:0')
c= tensor(4.1105e+08, device='cuda:0')
c= tensor(4.1106e+08, device='cuda:0')
c= tensor(4.1106e+08, device='cuda:0')
c= tensor(4.1107e+08, device='cuda:0')
c= tensor(4.1107e+08, device='cuda:0')
c= tensor(4.1108e+08, device='cuda:0')
c= tensor(4.1110e+08, device='cuda:0')
c= tensor(4.1111e+08, device='cuda:0')
c= tensor(4.1111e+08, device='cuda:0')
c= tensor(4.1112e+08, device='cuda:0')
c= tensor(4.1113e+08, device='cuda:0')
c= tensor(4.1114e+08, device='cuda:0')
c= tensor(4.1115e+08, device='cuda:0')
c= tensor(4.1117e+08, device='cuda:0')
c= tensor(4.1118e+08, device='cuda:0')
c= tensor(4.1119e+08, device='cuda:0')
c= tensor(4.1119e+08, device='cuda:0')
c= tensor(4.1120e+08, device='cuda:0')
c= tensor(4.1121e+08, device='cuda:0')
c= tensor(4.1121e+08, device='cuda:0')
c= tensor(4.1121e+08, device='cuda:0')
c= tensor(4.1122e+08, device='cuda:0')
c= tensor(4.1133e+08, device='cuda:0')
c= tensor(4.1133e+08, device='cuda:0')
c= tensor(4.1133e+08, device='cuda:0')
c= tensor(4.1134e+08, device='cuda:0')
c= tensor(4.1134e+08, device='cuda:0')
c= tensor(4.1135e+08, device='cuda:0')
c= tensor(4.1135e+08, device='cuda:0')
c= tensor(4.1135e+08, device='cuda:0')
c= tensor(4.1135e+08, device='cuda:0')
c= tensor(4.1136e+08, device='cuda:0')
c= tensor(4.1137e+08, device='cuda:0')
c= tensor(4.1139e+08, device='cuda:0')
c= tensor(4.1139e+08, device='cuda:0')
c= tensor(4.1140e+08, device='cuda:0')
c= tensor(4.1140e+08, device='cuda:0')
c= tensor(4.1141e+08, device='cuda:0')
c= tensor(4.1142e+08, device='cuda:0')
c= tensor(4.1142e+08, device='cuda:0')
c= tensor(4.1144e+08, device='cuda:0')
c= tensor(4.1146e+08, device='cuda:0')
c= tensor(4.1146e+08, device='cuda:0')
c= tensor(4.1148e+08, device='cuda:0')
c= tensor(4.1148e+08, device='cuda:0')
c= tensor(4.1149e+08, device='cuda:0')
c= tensor(4.1150e+08, device='cuda:0')
c= tensor(4.1150e+08, device='cuda:0')
c= tensor(4.1151e+08, device='cuda:0')
c= tensor(4.1152e+08, device='cuda:0')
c= tensor(4.1152e+08, device='cuda:0')
c= tensor(4.1152e+08, device='cuda:0')
c= tensor(4.1152e+08, device='cuda:0')
c= tensor(4.1153e+08, device='cuda:0')
c= tensor(4.1153e+08, device='cuda:0')
c= tensor(4.1153e+08, device='cuda:0')
c= tensor(4.1154e+08, device='cuda:0')
c= tensor(4.1155e+08, device='cuda:0')
c= tensor(4.1156e+08, device='cuda:0')
c= tensor(4.1156e+08, device='cuda:0')
c= tensor(4.1156e+08, device='cuda:0')
c= tensor(4.1158e+08, device='cuda:0')
c= tensor(4.1158e+08, device='cuda:0')
c= tensor(4.1160e+08, device='cuda:0')
c= tensor(4.1160e+08, device='cuda:0')
c= tensor(4.1160e+08, device='cuda:0')
c= tensor(4.1161e+08, device='cuda:0')
c= tensor(4.1161e+08, device='cuda:0')
c= tensor(4.1161e+08, device='cuda:0')
c= tensor(4.1161e+08, device='cuda:0')
c= tensor(4.1162e+08, device='cuda:0')
c= tensor(4.1164e+08, device='cuda:0')
c= tensor(4.1164e+08, device='cuda:0')
c= tensor(4.1167e+08, device='cuda:0')
c= tensor(4.1167e+08, device='cuda:0')
c= tensor(4.1167e+08, device='cuda:0')
c= tensor(4.1168e+08, device='cuda:0')
c= tensor(4.1171e+08, device='cuda:0')
c= tensor(4.1171e+08, device='cuda:0')
c= tensor(4.1172e+08, device='cuda:0')
c= tensor(4.1172e+08, device='cuda:0')
c= tensor(4.1172e+08, device='cuda:0')
c= tensor(4.1172e+08, device='cuda:0')
c= tensor(4.1173e+08, device='cuda:0')
c= tensor(4.1173e+08, device='cuda:0')
c= tensor(4.1178e+08, device='cuda:0')
c= tensor(4.1180e+08, device='cuda:0')
c= tensor(4.1180e+08, device='cuda:0')
c= tensor(4.1180e+08, device='cuda:0')
c= tensor(4.1181e+08, device='cuda:0')
c= tensor(4.1181e+08, device='cuda:0')
c= tensor(4.1182e+08, device='cuda:0')
c= tensor(4.1182e+08, device='cuda:0')
c= tensor(4.1183e+08, device='cuda:0')
c= tensor(4.1183e+08, device='cuda:0')
c= tensor(4.1184e+08, device='cuda:0')
c= tensor(4.1187e+08, device='cuda:0')
c= tensor(4.1187e+08, device='cuda:0')
c= tensor(4.1198e+08, device='cuda:0')
c= tensor(4.1198e+08, device='cuda:0')
c= tensor(4.1199e+08, device='cuda:0')
c= tensor(4.1199e+08, device='cuda:0')
c= tensor(4.1199e+08, device='cuda:0')
c= tensor(4.1204e+08, device='cuda:0')
c= tensor(4.1204e+08, device='cuda:0')
c= tensor(4.1205e+08, device='cuda:0')
c= tensor(4.1205e+08, device='cuda:0')
c= tensor(4.1206e+08, device='cuda:0')
c= tensor(4.1206e+08, device='cuda:0')
c= tensor(4.1207e+08, device='cuda:0')
c= tensor(4.1207e+08, device='cuda:0')
c= tensor(4.1208e+08, device='cuda:0')
c= tensor(4.1208e+08, device='cuda:0')
c= tensor(4.1208e+08, device='cuda:0')
c= tensor(4.1208e+08, device='cuda:0')
c= tensor(4.1209e+08, device='cuda:0')
c= tensor(4.1210e+08, device='cuda:0')
c= tensor(4.1210e+08, device='cuda:0')
c= tensor(4.1330e+08, device='cuda:0')
c= tensor(4.1331e+08, device='cuda:0')
c= tensor(4.1332e+08, device='cuda:0')
c= tensor(4.1333e+08, device='cuda:0')
c= tensor(4.1333e+08, device='cuda:0')
c= tensor(4.1334e+08, device='cuda:0')
c= tensor(4.1335e+08, device='cuda:0')
c= tensor(4.1336e+08, device='cuda:0')
c= tensor(4.1336e+08, device='cuda:0')
c= tensor(4.1337e+08, device='cuda:0')
c= tensor(4.1337e+08, device='cuda:0')
c= tensor(4.1338e+08, device='cuda:0')
c= tensor(4.1339e+08, device='cuda:0')
c= tensor(4.1340e+08, device='cuda:0')
c= tensor(4.1341e+08, device='cuda:0')
c= tensor(4.1342e+08, device='cuda:0')
c= tensor(4.1342e+08, device='cuda:0')
c= tensor(4.1342e+08, device='cuda:0')
c= tensor(4.1346e+08, device='cuda:0')
c= tensor(4.1349e+08, device='cuda:0')
c= tensor(4.1350e+08, device='cuda:0')
c= tensor(4.1350e+08, device='cuda:0')
c= tensor(4.1350e+08, device='cuda:0')
c= tensor(4.1350e+08, device='cuda:0')
c= tensor(4.1351e+08, device='cuda:0')
c= tensor(4.1351e+08, device='cuda:0')
c= tensor(4.1352e+08, device='cuda:0')
c= tensor(4.1353e+08, device='cuda:0')
c= tensor(4.1353e+08, device='cuda:0')
c= tensor(4.1354e+08, device='cuda:0')
c= tensor(4.1355e+08, device='cuda:0')
c= tensor(4.1355e+08, device='cuda:0')
c= tensor(4.1356e+08, device='cuda:0')
c= tensor(4.1356e+08, device='cuda:0')
c= tensor(4.1359e+08, device='cuda:0')
c= tensor(4.1362e+08, device='cuda:0')
c= tensor(4.1363e+08, device='cuda:0')
c= tensor(4.1364e+08, device='cuda:0')
c= tensor(4.1364e+08, device='cuda:0')
c= tensor(4.1365e+08, device='cuda:0')
c= tensor(4.1365e+08, device='cuda:0')
c= tensor(4.1365e+08, device='cuda:0')
c= tensor(4.1365e+08, device='cuda:0')
c= tensor(4.1368e+08, device='cuda:0')
c= tensor(4.1368e+08, device='cuda:0')
c= tensor(4.1368e+08, device='cuda:0')
c= tensor(4.1369e+08, device='cuda:0')
c= tensor(4.1370e+08, device='cuda:0')
c= tensor(4.1370e+08, device='cuda:0')
c= tensor(4.1371e+08, device='cuda:0')
c= tensor(4.1372e+08, device='cuda:0')
c= tensor(4.1372e+08, device='cuda:0')
c= tensor(4.1372e+08, device='cuda:0')
c= tensor(4.1373e+08, device='cuda:0')
c= tensor(4.1373e+08, device='cuda:0')
c= tensor(4.1374e+08, device='cuda:0')
c= tensor(4.1374e+08, device='cuda:0')
c= tensor(4.1375e+08, device='cuda:0')
c= tensor(4.1375e+08, device='cuda:0')
c= tensor(4.1376e+08, device='cuda:0')
c= tensor(4.1376e+08, device='cuda:0')
c= tensor(4.1377e+08, device='cuda:0')
c= tensor(4.1379e+08, device='cuda:0')
c= tensor(4.1380e+08, device='cuda:0')
c= tensor(4.1397e+08, device='cuda:0')
c= tensor(4.1538e+08, device='cuda:0')
c= tensor(4.1542e+08, device='cuda:0')
c= tensor(4.1543e+08, device='cuda:0')
c= tensor(4.1543e+08, device='cuda:0')
c= tensor(4.1545e+08, device='cuda:0')
c= tensor(4.1586e+08, device='cuda:0')
c= tensor(4.2385e+08, device='cuda:0')
c= tensor(4.2385e+08, device='cuda:0')
c= tensor(4.2909e+08, device='cuda:0')
c= tensor(4.3009e+08, device='cuda:0')
c= tensor(4.3039e+08, device='cuda:0')
c= tensor(4.3177e+08, device='cuda:0')
c= tensor(4.3178e+08, device='cuda:0')
c= tensor(4.3179e+08, device='cuda:0')
c= tensor(4.3562e+08, device='cuda:0')
c= tensor(5.5623e+08, device='cuda:0')
c= tensor(5.5623e+08, device='cuda:0')
c= tensor(5.5651e+08, device='cuda:0')
c= tensor(5.5686e+08, device='cuda:0')
c= tensor(5.5953e+08, device='cuda:0')
c= tensor(5.6338e+08, device='cuda:0')
c= tensor(5.6546e+08, device='cuda:0')
c= tensor(5.6634e+08, device='cuda:0')
c= tensor(5.6664e+08, device='cuda:0')
c= tensor(5.6665e+08, device='cuda:0')
c= tensor(5.7810e+08, device='cuda:0')
c= tensor(5.7828e+08, device='cuda:0')
c= tensor(5.7828e+08, device='cuda:0')
c= tensor(5.7847e+08, device='cuda:0')
c= tensor(5.8027e+08, device='cuda:0')
c= tensor(5.8505e+08, device='cuda:0')
c= tensor(5.8682e+08, device='cuda:0')
c= tensor(5.8683e+08, device='cuda:0')
c= tensor(5.8699e+08, device='cuda:0')
c= tensor(5.8701e+08, device='cuda:0')
c= tensor(5.8787e+08, device='cuda:0')
c= tensor(5.9312e+08, device='cuda:0')
c= tensor(5.9320e+08, device='cuda:0')
c= tensor(5.9499e+08, device='cuda:0')
c= tensor(5.9499e+08, device='cuda:0')
c= tensor(5.9500e+08, device='cuda:0')
c= tensor(5.9657e+08, device='cuda:0')
c= tensor(5.9742e+08, device='cuda:0')
c= tensor(5.9913e+08, device='cuda:0')
c= tensor(5.9914e+08, device='cuda:0')
c= tensor(6.2199e+08, device='cuda:0')
c= tensor(6.2210e+08, device='cuda:0')
c= tensor(6.2237e+08, device='cuda:0')
c= tensor(6.2342e+08, device='cuda:0')
c= tensor(6.2342e+08, device='cuda:0')
c= tensor(6.2605e+08, device='cuda:0')
c= tensor(6.3717e+08, device='cuda:0')
c= tensor(6.6033e+08, device='cuda:0')
c= tensor(6.6062e+08, device='cuda:0')
c= tensor(6.6081e+08, device='cuda:0')
c= tensor(6.6084e+08, device='cuda:0')
c= tensor(6.6086e+08, device='cuda:0')
c= tensor(6.6459e+08, device='cuda:0')
c= tensor(6.6463e+08, device='cuda:0')
c= tensor(6.6503e+08, device='cuda:0')
c= tensor(6.7171e+08, device='cuda:0')
c= tensor(6.7609e+08, device='cuda:0')
c= tensor(6.7619e+08, device='cuda:0')
c= tensor(6.7620e+08, device='cuda:0')
c= tensor(6.8062e+08, device='cuda:0')
c= tensor(6.9000e+08, device='cuda:0')
c= tensor(6.9084e+08, device='cuda:0')
c= tensor(6.9085e+08, device='cuda:0')
c= tensor(6.9332e+08, device='cuda:0')
c= tensor(6.9340e+08, device='cuda:0')
c= tensor(7.0334e+08, device='cuda:0')
c= tensor(7.0340e+08, device='cuda:0')
c= tensor(7.0613e+08, device='cuda:0')
c= tensor(7.0659e+08, device='cuda:0')
c= tensor(7.1156e+08, device='cuda:0')
c= tensor(7.1242e+08, device='cuda:0')
c= tensor(7.1246e+08, device='cuda:0')
c= tensor(7.1642e+08, device='cuda:0')
c= tensor(7.2174e+08, device='cuda:0')
c= tensor(7.2176e+08, device='cuda:0')
c= tensor(7.2851e+08, device='cuda:0')
c= tensor(7.3533e+08, device='cuda:0')
c= tensor(7.5982e+08, device='cuda:0')
c= tensor(7.6019e+08, device='cuda:0')
c= tensor(7.6020e+08, device='cuda:0')
c= tensor(7.6025e+08, device='cuda:0')
c= tensor(7.6237e+08, device='cuda:0')
c= tensor(7.6263e+08, device='cuda:0')
c= tensor(7.6339e+08, device='cuda:0')
c= tensor(7.6339e+08, device='cuda:0')
c= tensor(7.6443e+08, device='cuda:0')
c= tensor(7.6857e+08, device='cuda:0')
c= tensor(7.6901e+08, device='cuda:0')
c= tensor(7.6906e+08, device='cuda:0')
c= tensor(7.6975e+08, device='cuda:0')
c= tensor(7.6984e+08, device='cuda:0')
c= tensor(7.6987e+08, device='cuda:0')
c= tensor(7.6991e+08, device='cuda:0')
c= tensor(7.6991e+08, device='cuda:0')
c= tensor(7.7050e+08, device='cuda:0')
c= tensor(7.7085e+08, device='cuda:0')
c= tensor(7.7101e+08, device='cuda:0')
c= tensor(7.7163e+08, device='cuda:0')
c= tensor(7.7165e+08, device='cuda:0')
c= tensor(1.3501e+09, device='cuda:0')
c= tensor(1.3501e+09, device='cuda:0')
c= tensor(1.3528e+09, device='cuda:0')
c= tensor(1.3528e+09, device='cuda:0')
c= tensor(1.3528e+09, device='cuda:0')
c= tensor(1.3528e+09, device='cuda:0')
c= tensor(1.3530e+09, device='cuda:0')
c= tensor(1.3530e+09, device='cuda:0')
c= tensor(1.3569e+09, device='cuda:0')
c= tensor(1.3569e+09, device='cuda:0')
c= tensor(1.3569e+09, device='cuda:0')
c= tensor(1.3653e+09, device='cuda:0')
c= tensor(1.3664e+09, device='cuda:0')
c= tensor(1.3669e+09, device='cuda:0')
c= tensor(1.3713e+09, device='cuda:0')
c= tensor(1.3844e+09, device='cuda:0')
c= tensor(1.3844e+09, device='cuda:0')
c= tensor(1.3844e+09, device='cuda:0')
c= tensor(1.3847e+09, device='cuda:0')
c= tensor(1.3847e+09, device='cuda:0')
c= tensor(1.3847e+09, device='cuda:0')
c= tensor(1.3848e+09, device='cuda:0')
c= tensor(1.3848e+09, device='cuda:0')
c= tensor(1.3848e+09, device='cuda:0')
c= tensor(1.3848e+09, device='cuda:0')
c= tensor(1.3848e+09, device='cuda:0')
c= tensor(1.3960e+09, device='cuda:0')
c= tensor(1.3961e+09, device='cuda:0')
c= tensor(1.3977e+09, device='cuda:0')
c= tensor(1.3977e+09, device='cuda:0')
c= tensor(1.3977e+09, device='cuda:0')
c= tensor(1.3983e+09, device='cuda:0')
c= tensor(1.5951e+09, device='cuda:0')
c= tensor(1.6356e+09, device='cuda:0')
c= tensor(1.6357e+09, device='cuda:0')
c= tensor(1.6363e+09, device='cuda:0')
c= tensor(1.6363e+09, device='cuda:0')
c= tensor(1.6364e+09, device='cuda:0')
c= tensor(1.6508e+09, device='cuda:0')
c= tensor(1.6510e+09, device='cuda:0')
c= tensor(1.6510e+09, device='cuda:0')
c= tensor(1.6523e+09, device='cuda:0')
c= tensor(1.7199e+09, device='cuda:0')
c= tensor(1.7201e+09, device='cuda:0')
c= tensor(1.7202e+09, device='cuda:0')
c= tensor(1.7203e+09, device='cuda:0')
c= tensor(1.7204e+09, device='cuda:0')
c= tensor(1.7204e+09, device='cuda:0')
c= tensor(1.7259e+09, device='cuda:0')
c= tensor(1.7261e+09, device='cuda:0')
c= tensor(1.7261e+09, device='cuda:0')
c= tensor(1.7275e+09, device='cuda:0')
c= tensor(1.7275e+09, device='cuda:0')
c= tensor(1.7276e+09, device='cuda:0')
c= tensor(1.7282e+09, device='cuda:0')
c= tensor(1.7309e+09, device='cuda:0')
c= tensor(1.7406e+09, device='cuda:0')
c= tensor(1.7488e+09, device='cuda:0')
c= tensor(1.7569e+09, device='cuda:0')
c= tensor(1.7570e+09, device='cuda:0')
c= tensor(1.7582e+09, device='cuda:0')
c= tensor(1.7616e+09, device='cuda:0')
c= tensor(1.7712e+09, device='cuda:0')
c= tensor(1.7713e+09, device='cuda:0')
c= tensor(1.8094e+09, device='cuda:0')
c= tensor(1.9382e+09, device='cuda:0')
c= tensor(1.9503e+09, device='cuda:0')
c= tensor(1.9547e+09, device='cuda:0')
c= tensor(1.9573e+09, device='cuda:0')
c= tensor(1.9573e+09, device='cuda:0')
c= tensor(1.9573e+09, device='cuda:0')
c= tensor(1.9574e+09, device='cuda:0')
c= tensor(1.9602e+09, device='cuda:0')
c= tensor(1.9624e+09, device='cuda:0')
c= tensor(1.9854e+09, device='cuda:0')
c= tensor(1.9883e+09, device='cuda:0')
c= tensor(1.9892e+09, device='cuda:0')
c= tensor(1.9894e+09, device='cuda:0')
c= tensor(1.9915e+09, device='cuda:0')
c= tensor(1.9915e+09, device='cuda:0')
c= tensor(1.9915e+09, device='cuda:0')
c= tensor(1.9943e+09, device='cuda:0')
c= tensor(1.9946e+09, device='cuda:0')
c= tensor(1.9946e+09, device='cuda:0')
c= tensor(1.9947e+09, device='cuda:0')
c= tensor(2.1037e+09, device='cuda:0')
c= tensor(2.1038e+09, device='cuda:0')
c= tensor(2.1088e+09, device='cuda:0')
c= tensor(2.1088e+09, device='cuda:0')
c= tensor(2.1089e+09, device='cuda:0')
c= tensor(2.1089e+09, device='cuda:0')
c= tensor(2.1089e+09, device='cuda:0')
c= tensor(2.1090e+09, device='cuda:0')
c= tensor(2.1094e+09, device='cuda:0')
c= tensor(2.1095e+09, device='cuda:0')
c= tensor(2.1185e+09, device='cuda:0')
c= tensor(2.1185e+09, device='cuda:0')
c= tensor(2.1205e+09, device='cuda:0')
c= tensor(2.1205e+09, device='cuda:0')
c= tensor(2.1218e+09, device='cuda:0')
c= tensor(2.1218e+09, device='cuda:0')
c= tensor(2.1219e+09, device='cuda:0')
c= tensor(2.1220e+09, device='cuda:0')
c= tensor(2.1221e+09, device='cuda:0')
c= tensor(2.1234e+09, device='cuda:0')
c= tensor(2.2909e+09, device='cuda:0')
c= tensor(2.2910e+09, device='cuda:0')
c= tensor(2.2910e+09, device='cuda:0')
c= tensor(2.2915e+09, device='cuda:0')
c= tensor(2.2916e+09, device='cuda:0')
c= tensor(2.3230e+09, device='cuda:0')
c= tensor(2.3230e+09, device='cuda:0')
c= tensor(2.3251e+09, device='cuda:0')
c= tensor(2.3317e+09, device='cuda:0')
c= tensor(2.3317e+09, device='cuda:0')
c= tensor(2.3654e+09, device='cuda:0')
c= tensor(2.3658e+09, device='cuda:0')
c= tensor(2.4225e+09, device='cuda:0')
c= tensor(2.4226e+09, device='cuda:0')
c= tensor(2.4227e+09, device='cuda:0')
c= tensor(2.4228e+09, device='cuda:0')
c= tensor(2.4228e+09, device='cuda:0')
c= tensor(2.4228e+09, device='cuda:0')
c= tensor(2.4235e+09, device='cuda:0')
c= tensor(2.4235e+09, device='cuda:0')
c= tensor(2.4260e+09, device='cuda:0')
c= tensor(2.4261e+09, device='cuda:0')
c= tensor(2.4261e+09, device='cuda:0')
c= tensor(2.4262e+09, device='cuda:0')
c= tensor(2.4339e+09, device='cuda:0')
c= tensor(2.4380e+09, device='cuda:0')
c= tensor(2.4521e+09, device='cuda:0')
c= tensor(2.4523e+09, device='cuda:0')
c= tensor(2.4523e+09, device='cuda:0')
c= tensor(2.4523e+09, device='cuda:0')
c= tensor(2.4523e+09, device='cuda:0')
c= tensor(2.4824e+09, device='cuda:0')
c= tensor(2.4824e+09, device='cuda:0')
c= tensor(2.4826e+09, device='cuda:0')
c= tensor(2.4836e+09, device='cuda:0')
c= tensor(2.4844e+09, device='cuda:0')
c= tensor(2.4844e+09, device='cuda:0')
c= tensor(2.4845e+09, device='cuda:0')
c= tensor(2.4974e+09, device='cuda:0')
c= tensor(2.4979e+09, device='cuda:0')
c= tensor(2.4980e+09, device='cuda:0')
c= tensor(2.4982e+09, device='cuda:0')
c= tensor(2.4994e+09, device='cuda:0')
c= tensor(2.5023e+09, device='cuda:0')
c= tensor(2.5348e+09, device='cuda:0')
c= tensor(2.5375e+09, device='cuda:0')
c= tensor(2.5375e+09, device='cuda:0')
c= tensor(2.5381e+09, device='cuda:0')
c= tensor(2.5388e+09, device='cuda:0')
c= tensor(2.5388e+09, device='cuda:0')
c= tensor(2.5388e+09, device='cuda:0')
c= tensor(2.5388e+09, device='cuda:0')
c= tensor(2.5408e+09, device='cuda:0')
c= tensor(2.5409e+09, device='cuda:0')
c= tensor(2.5409e+09, device='cuda:0')
c= tensor(2.5410e+09, device='cuda:0')
c= tensor(2.5410e+09, device='cuda:0')
c= tensor(2.5412e+09, device='cuda:0')
c= tensor(2.5412e+09, device='cuda:0')
c= tensor(2.5415e+09, device='cuda:0')
c= tensor(2.5424e+09, device='cuda:0')
c= tensor(2.5425e+09, device='cuda:0')
c= tensor(2.5425e+09, device='cuda:0')
c= tensor(2.5426e+09, device='cuda:0')
c= tensor(2.5428e+09, device='cuda:0')
c= tensor(2.5532e+09, device='cuda:0')
c= tensor(2.5532e+09, device='cuda:0')
c= tensor(2.5532e+09, device='cuda:0')
c= tensor(2.5532e+09, device='cuda:0')
c= tensor(2.5562e+09, device='cuda:0')
c= tensor(2.5736e+09, device='cuda:0')
c= tensor(2.5750e+09, device='cuda:0')
c= tensor(2.5750e+09, device='cuda:0')
c= tensor(2.5807e+09, device='cuda:0')
c= tensor(2.5835e+09, device='cuda:0')
c= tensor(2.5835e+09, device='cuda:0')
c= tensor(2.5836e+09, device='cuda:0')
c= tensor(2.5838e+09, device='cuda:0')
c= tensor(2.5872e+09, device='cuda:0')
c= tensor(2.5932e+09, device='cuda:0')
c= tensor(2.5987e+09, device='cuda:0')
c= tensor(2.5989e+09, device='cuda:0')
c= tensor(2.5989e+09, device='cuda:0')
c= tensor(2.5989e+09, device='cuda:0')
c= tensor(2.5992e+09, device='cuda:0')
c= tensor(2.5992e+09, device='cuda:0')
c= tensor(2.5996e+09, device='cuda:0')
c= tensor(2.6035e+09, device='cuda:0')
c= tensor(2.6392e+09, device='cuda:0')
c= tensor(2.6392e+09, device='cuda:0')
c= tensor(2.6392e+09, device='cuda:0')
c= tensor(2.6393e+09, device='cuda:0')
c= tensor(2.6412e+09, device='cuda:0')
c= tensor(2.6420e+09, device='cuda:0')
c= tensor(2.6422e+09, device='cuda:0')
c= tensor(2.6422e+09, device='cuda:0')
c= tensor(2.6427e+09, device='cuda:0')
c= tensor(2.6427e+09, device='cuda:0')
c= tensor(2.6437e+09, device='cuda:0')
c= tensor(2.6437e+09, device='cuda:0')
c= tensor(2.6438e+09, device='cuda:0')
c= tensor(2.6438e+09, device='cuda:0')
c= tensor(2.6438e+09, device='cuda:0')
c= tensor(2.6438e+09, device='cuda:0')
c= tensor(2.6479e+09, device='cuda:0')
c= tensor(2.6601e+09, device='cuda:0')
c= tensor(2.6630e+09, device='cuda:0')
c= tensor(2.6668e+09, device='cuda:0')
c= tensor(2.6670e+09, device='cuda:0')
c= tensor(2.6671e+09, device='cuda:0')
c= tensor(2.6671e+09, device='cuda:0')
c= tensor(2.6695e+09, device='cuda:0')
c= tensor(2.6699e+09, device='cuda:0')
c= tensor(2.6704e+09, device='cuda:0')
c= tensor(2.6705e+09, device='cuda:0')
c= tensor(2.8266e+09, device='cuda:0')
c= tensor(2.8268e+09, device='cuda:0')
c= tensor(2.8276e+09, device='cuda:0')
c= tensor(2.8332e+09, device='cuda:0')
c= tensor(2.8340e+09, device='cuda:0')
c= tensor(2.8342e+09, device='cuda:0')
c= tensor(2.9035e+09, device='cuda:0')
c= tensor(2.9070e+09, device='cuda:0')
c= tensor(2.9081e+09, device='cuda:0')
c= tensor(2.9083e+09, device='cuda:0')
c= tensor(2.9090e+09, device='cuda:0')
c= tensor(2.9090e+09, device='cuda:0')
c= tensor(2.9169e+09, device='cuda:0')
c= tensor(3.0820e+09, device='cuda:0')
c= tensor(3.0870e+09, device='cuda:0')
c= tensor(3.0922e+09, device='cuda:0')
c= tensor(3.0965e+09, device='cuda:0')
c= tensor(3.0969e+09, device='cuda:0')
c= tensor(3.0972e+09, device='cuda:0')
c= tensor(3.0973e+09, device='cuda:0')
c= tensor(3.0982e+09, device='cuda:0')
c= tensor(3.1155e+09, device='cuda:0')
c= tensor(3.1174e+09, device='cuda:0')
c= tensor(3.1557e+09, device='cuda:0')
c= tensor(3.1619e+09, device='cuda:0')
c= tensor(3.1622e+09, device='cuda:0')
c= tensor(3.1622e+09, device='cuda:0')
c= tensor(3.1631e+09, device='cuda:0')
c= tensor(3.1671e+09, device='cuda:0')
c= tensor(3.1672e+09, device='cuda:0')
c= tensor(3.1931e+09, device='cuda:0')
c= tensor(3.1939e+09, device='cuda:0')
c= tensor(3.1942e+09, device='cuda:0')
c= tensor(3.1972e+09, device='cuda:0')
c= tensor(3.1972e+09, device='cuda:0')
c= tensor(3.1972e+09, device='cuda:0')
c= tensor(3.1972e+09, device='cuda:0')
c= tensor(3.1973e+09, device='cuda:0')
c= tensor(3.1979e+09, device='cuda:0')
c= tensor(4.6791e+09, device='cuda:0')
c= tensor(4.6795e+09, device='cuda:0')
c= tensor(4.6824e+09, device='cuda:0')
c= tensor(4.6824e+09, device='cuda:0')
c= tensor(4.6825e+09, device='cuda:0')
c= tensor(4.6825e+09, device='cuda:0')
c= tensor(4.6947e+09, device='cuda:0')
c= tensor(4.6949e+09, device='cuda:0')
c= tensor(4.7749e+09, device='cuda:0')
c= tensor(4.7749e+09, device='cuda:0')
c= tensor(4.7800e+09, device='cuda:0')
c= tensor(4.7803e+09, device='cuda:0')
c= tensor(4.7857e+09, device='cuda:0')
c= tensor(4.8184e+09, device='cuda:0')
c= tensor(4.8186e+09, device='cuda:0')
c= tensor(4.8186e+09, device='cuda:0')
c= tensor(4.8200e+09, device='cuda:0')
c= tensor(4.8201e+09, device='cuda:0')
c= tensor(4.8204e+09, device='cuda:0')
c= tensor(4.8224e+09, device='cuda:0')
c= tensor(4.8225e+09, device='cuda:0')
c= tensor(4.8236e+09, device='cuda:0')
c= tensor(4.8241e+09, device='cuda:0')
c= tensor(4.8277e+09, device='cuda:0')
c= tensor(4.8356e+09, device='cuda:0')
c= tensor(4.8361e+09, device='cuda:0')
c= tensor(4.8362e+09, device='cuda:0')
c= tensor(4.8408e+09, device='cuda:0')
c= tensor(4.8421e+09, device='cuda:0')
c= tensor(4.8452e+09, device='cuda:0')
c= tensor(4.8454e+09, device='cuda:0')
c= tensor(4.8460e+09, device='cuda:0')
c= tensor(4.8464e+09, device='cuda:0')
c= tensor(4.8596e+09, device='cuda:0')
c= tensor(4.8620e+09, device='cuda:0')
c= tensor(4.8620e+09, device='cuda:0')
c= tensor(4.8620e+09, device='cuda:0')
c= tensor(4.8621e+09, device='cuda:0')
c= tensor(4.8639e+09, device='cuda:0')
c= tensor(4.8664e+09, device='cuda:0')
c= tensor(4.8675e+09, device='cuda:0')
c= tensor(4.8675e+09, device='cuda:0')
c= tensor(4.8677e+09, device='cuda:0')
c= tensor(4.8681e+09, device='cuda:0')
c= tensor(4.8693e+09, device='cuda:0')
c= tensor(4.8697e+09, device='cuda:0')
c= tensor(4.8705e+09, device='cuda:0')
c= tensor(4.8707e+09, device='cuda:0')
c= tensor(4.8712e+09, device='cuda:0')
c= tensor(4.8712e+09, device='cuda:0')
c= tensor(4.8712e+09, device='cuda:0')
c= tensor(4.8766e+09, device='cuda:0')
c= tensor(4.8768e+09, device='cuda:0')
c= tensor(4.8783e+09, device='cuda:0')
c= tensor(4.8783e+09, device='cuda:0')
c= tensor(4.8783e+09, device='cuda:0')
c= tensor(4.8793e+09, device='cuda:0')
c= tensor(4.8798e+09, device='cuda:0')
c= tensor(4.8800e+09, device='cuda:0')
c= tensor(4.8800e+09, device='cuda:0')
c= tensor(4.8800e+09, device='cuda:0')
c= tensor(4.8805e+09, device='cuda:0')
c= tensor(4.8807e+09, device='cuda:0')
c= tensor(4.8825e+09, device='cuda:0')
c= tensor(4.8826e+09, device='cuda:0')
c= tensor(4.8832e+09, device='cuda:0')
c= tensor(4.8837e+09, device='cuda:0')
c= tensor(4.8837e+09, device='cuda:0')
c= tensor(4.8918e+09, device='cuda:0')
c= tensor(4.8923e+09, device='cuda:0')
c= tensor(4.8936e+09, device='cuda:0')
c= tensor(4.8961e+09, device='cuda:0')
c= tensor(4.9002e+09, device='cuda:0')
c= tensor(4.9041e+09, device='cuda:0')
c= tensor(4.9065e+09, device='cuda:0')
c= tensor(4.9065e+09, device='cuda:0')
c= tensor(4.9069e+09, device='cuda:0')
c= tensor(4.9072e+09, device='cuda:0')
c= tensor(4.9077e+09, device='cuda:0')
c= tensor(4.9143e+09, device='cuda:0')
c= tensor(4.9146e+09, device='cuda:0')
c= tensor(4.9154e+09, device='cuda:0')
c= tensor(4.9156e+09, device='cuda:0')
c= tensor(4.9179e+09, device='cuda:0')
c= tensor(4.9179e+09, device='cuda:0')
c= tensor(4.9180e+09, device='cuda:0')
c= tensor(4.9255e+09, device='cuda:0')
c= tensor(4.9430e+09, device='cuda:0')
c= tensor(4.9431e+09, device='cuda:0')
c= tensor(4.9433e+09, device='cuda:0')
c= tensor(4.9434e+09, device='cuda:0')
c= tensor(4.9454e+09, device='cuda:0')
c= tensor(4.9457e+09, device='cuda:0')
c= tensor(4.9458e+09, device='cuda:0')
c= tensor(4.9461e+09, device='cuda:0')
c= tensor(4.9501e+09, device='cuda:0')
c= tensor(4.9502e+09, device='cuda:0')
c= tensor(4.9511e+09, device='cuda:0')
c= tensor(4.9513e+09, device='cuda:0')
time to make c is 11.964381694793701
time for making loss is 11.964407205581665
p0 True
it  0 : 1591751680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
4763144192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 11% |
memory (bytes)
4763455488
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1058749950.0
relative error loss 0.21383461
shape of L is 
torch.Size([])
memory (bytes)
4790628352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 11% |
memory (bytes)
4790652928
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  1046313700.0
relative error loss 0.21132289
shape of L is 
torch.Size([])
memory (bytes)
4794220544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
4794220544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  1012385000.0
relative error loss 0.20447034
shape of L is 
torch.Size([])
memory (bytes)
4797423616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
4797464576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  994784800.0
relative error loss 0.20091563
shape of L is 
torch.Size([])
memory (bytes)
4800663552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
4800663552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  976545800.0
relative error loss 0.19723193
shape of L is 
torch.Size([])
memory (bytes)
4803768320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
4803768320
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  966173200.0
relative error loss 0.19513698
shape of L is 
torch.Size([])
memory (bytes)
4807024640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
4807102464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 11% |
error is  957062900.0
relative error loss 0.193297
shape of L is 
torch.Size([])
memory (bytes)
4810194944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
4810321920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  951930400.0
relative error loss 0.19226038
shape of L is 
torch.Size([])
memory (bytes)
4813541376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 11% |
memory (bytes)
4813541376
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  946314750.0
relative error loss 0.1911262
shape of L is 
torch.Size([])
memory (bytes)
4816756736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 11% |
memory (bytes)
4816756736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  941540100.0
relative error loss 0.19016187
time to take a step is 385.59017276763916
it  1 : 2038714368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
4819845120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
4819976192
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  941540100.0
relative error loss 0.19016187
shape of L is 
torch.Size([])
memory (bytes)
4823191552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 11% |
memory (bytes)
4823195648
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  937474560.0
relative error loss 0.18934076
shape of L is 
torch.Size([])
memory (bytes)
4826411008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 11% |
memory (bytes)
4826415104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  935034400.0
relative error loss 0.18884791
shape of L is 
torch.Size([])
memory (bytes)
4829442048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 11% |
memory (bytes)
4829442048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  931880200.0
relative error loss 0.18821086
shape of L is 
torch.Size([])
memory (bytes)
4832796672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
4832833536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  929728500.0
relative error loss 0.1877763
shape of L is 
torch.Size([])
memory (bytes)
4836052992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
4836052992
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  927117800.0
relative error loss 0.18724902
shape of L is 
torch.Size([])
memory (bytes)
4839182336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 11% |
memory (bytes)
4839182336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  925047040.0
relative error loss 0.18683079
shape of L is 
torch.Size([])
memory (bytes)
4842487808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 11% |
memory (bytes)
4842487808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  923385340.0
relative error loss 0.18649517
shape of L is 
torch.Size([])
memory (bytes)
4845703168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 11% |
memory (bytes)
4845703168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  921506300.0
relative error loss 0.18611567
shape of L is 
torch.Size([])
memory (bytes)
4848914432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 11% |
memory (bytes)
4848914432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  919388160.0
relative error loss 0.18568787
time to take a step is 385.8396191596985
it  2 : 2038090752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
4852137984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
4852137984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  919388160.0
relative error loss 0.18568787
shape of L is 
torch.Size([])
memory (bytes)
4855242752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
4855386112
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  918475000.0
relative error loss 0.18550344
shape of L is 
torch.Size([])
memory (bytes)
4858597376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 11% |
memory (bytes)
4858601472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  917058560.0
relative error loss 0.18521735
shape of L is 
torch.Size([])
memory (bytes)
4861833216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
4861833216
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  915954400.0
relative error loss 0.18499435
shape of L is 
torch.Size([])
memory (bytes)
4864819200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
4865028096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 11% |
error is  914677000.0
relative error loss 0.18473636
shape of L is 
torch.Size([])
memory (bytes)
4868161536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
4868239360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  913263360.0
relative error loss 0.18445085
shape of L is 
torch.Size([])
memory (bytes)
4871446528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
4871458816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% | 11% |
error is  912459260.0
relative error loss 0.18428844
shape of L is 
torch.Size([])
memory (bytes)
4874670080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 11% |
memory (bytes)
4874670080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  911840000.0
relative error loss 0.18416338
shape of L is 
torch.Size([])
memory (bytes)
4877864960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
4877864960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  910436100.0
relative error loss 0.18387982
shape of L is 
torch.Size([])
memory (bytes)
4881096704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
4881100800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  910037760.0
relative error loss 0.18379937
time to take a step is 386.991779088974
it  3 : 2038714368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
4884316160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
4884316160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% | 11% |
error is  910037760.0
relative error loss 0.18379937
shape of L is 
torch.Size([])
memory (bytes)
4887429120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
4887429120
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  909204200.0
relative error loss 0.18363102
shape of L is 
torch.Size([])
memory (bytes)
4890693632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
4890738688
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  908770050.0
relative error loss 0.18354334
shape of L is 
torch.Size([])
memory (bytes)
4893958144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
4893958144
| ID | GPU | MEM |
------------------
|  0 |  2% |  0% |
|  1 | 99% | 11% |
error is  907858940.0
relative error loss 0.18335932
shape of L is 
torch.Size([])
memory (bytes)
4897136640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
4897136640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  907280100.0
relative error loss 0.18324241
shape of L is 
torch.Size([])
memory (bytes)
4900397056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
4900397056
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  906855940.0
relative error loss 0.18315674
shape of L is 
torch.Size([])
memory (bytes)
4903608320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
4903608320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  905930500.0
relative error loss 0.18296984
shape of L is 
torch.Size([])
memory (bytes)
4906708992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 11% |
memory (bytes)
4906708992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  906920700.0
relative error loss 0.18316983
shape of L is 
torch.Size([])
memory (bytes)
4910043136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
4910043136
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  905478660.0
relative error loss 0.18287857
shape of L is 
torch.Size([])
memory (bytes)
4913197056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
4913254400
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  904830460.0
relative error loss 0.18274766
time to take a step is 387.33348774909973
it  4 : 2038714368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
4916412416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
4916465664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  904830460.0
relative error loss 0.18274766
shape of L is 
torch.Size([])
memory (bytes)
4919672832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 11% |
memory (bytes)
4919672832
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  904368100.0
relative error loss 0.18265428
shape of L is 
torch.Size([])
memory (bytes)
4922847232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
4922847232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  903904800.0
relative error loss 0.1825607
shape of L is 
torch.Size([])
memory (bytes)
4926107648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 11% |
memory (bytes)
4926107648
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  903462660.0
relative error loss 0.18247141
shape of L is 
torch.Size([])
memory (bytes)
4929327104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
4929327104
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  903179000.0
relative error loss 0.18241411
shape of L is 
torch.Size([])
memory (bytes)
4932419584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
4932419584
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  902849800.0
relative error loss 0.18234763
shape of L is 
torch.Size([])
memory (bytes)
4935757824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 11% |
memory (bytes)
4935757824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  902697500.0
relative error loss 0.18231687
shape of L is 
torch.Size([])
memory (bytes)
4938850304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 11% |
memory (bytes)
4938850304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  902233860.0
relative error loss 0.18222323
shape of L is 
torch.Size([])
memory (bytes)
4942188544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
4942188544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  902084860.0
relative error loss 0.18219313
shape of L is 
torch.Size([])
memory (bytes)
4945408000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 11% |
memory (bytes)
4945408000
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  901883900.0
relative error loss 0.18215255
time to take a step is 368.8743600845337
it  5 : 2038090752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
4948627456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
4948627456
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  901883900.0
relative error loss 0.18215255
shape of L is 
torch.Size([])
memory (bytes)
4951703552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 11% |
memory (bytes)
4951834624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  901602800.0
relative error loss 0.18209578
shape of L is 
torch.Size([])
memory (bytes)
4955058176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
4955058176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  901331200.0
relative error loss 0.18204091
shape of L is 
torch.Size([])
memory (bytes)
4958154752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 11% |
memory (bytes)
4958154752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  901135900.0
relative error loss 0.18200147
shape of L is 
torch.Size([])
memory (bytes)
4961492992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
4961492992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  900851700.0
relative error loss 0.18194407
shape of L is 
torch.Size([])
memory (bytes)
4964589568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 11% |
memory (bytes)
4964589568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  900464640.0
relative error loss 0.1818659
shape of L is 
torch.Size([])
memory (bytes)
4967923712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
4967923712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  900637200.0
relative error loss 0.18190075
shape of L is 
torch.Size([])
memory (bytes)
4971053056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
4971053056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  900253700.0
relative error loss 0.1818233
shape of L is 
torch.Size([])
memory (bytes)
4974223360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
4974354432
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  899980540.0
relative error loss 0.18176813
shape of L is 
torch.Size([])
memory (bytes)
4977561600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
4977561600
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  899685100.0
relative error loss 0.18170846
time to take a step is 288.2749750614166
it  6 : 2038714368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
4980649984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 11% |
memory (bytes)
4980649984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  899685100.0
relative error loss 0.18170846
shape of L is 
torch.Size([])
memory (bytes)
4983992320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 11% |
memory (bytes)
4983992320
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  899468300.0
relative error loss 0.18166466
shape of L is 
torch.Size([])
memory (bytes)
4987125760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
4987125760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  899205100.0
relative error loss 0.18161152
shape of L is 
torch.Size([])
memory (bytes)
4990423040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
4990423040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  898980350.0
relative error loss 0.18156612
shape of L is 
torch.Size([])
memory (bytes)
4993626112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 11% |
memory (bytes)
4993626112
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  898717440.0
relative error loss 0.18151303
shape of L is 
torch.Size([])
memory (bytes)
4996849664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 11% |
memory (bytes)
4996849664
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  898776600.0
relative error loss 0.18152496
shape of L is 
torch.Size([])
memory (bytes)
4999974912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 11% |
memory (bytes)
4999974912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  898488300.0
relative error loss 0.18146674
shape of L is 
torch.Size([])
memory (bytes)
5003284480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5003284480
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  898176800.0
relative error loss 0.18140382
shape of L is 
torch.Size([])
memory (bytes)
5006508032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5006508032
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  897940200.0
relative error loss 0.18135604
shape of L is 
torch.Size([])
memory (bytes)
5009588224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5009715200
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  897650940.0
relative error loss 0.18129762
time to take a step is 289.8646411895752
it  7 : 2038090752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5012926464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5012930560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  897650940.0
relative error loss 0.18129762
shape of L is 
torch.Size([])
memory (bytes)
5016072192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5016072192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  897369340.0
relative error loss 0.18124075
shape of L is 
torch.Size([])
memory (bytes)
5019353088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5019357184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  897108740.0
relative error loss 0.1811881
shape of L is 
torch.Size([])
memory (bytes)
5022494720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5022494720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  896849660.0
relative error loss 0.18113579
shape of L is 
torch.Size([])
memory (bytes)
5025796096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5025796096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% | 11% |
error is  896665100.0
relative error loss 0.1810985
shape of L is 
torch.Size([])
memory (bytes)
5028880384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5028880384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  896411400.0
relative error loss 0.18104728
shape of L is 
torch.Size([])
memory (bytes)
5032226816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5032226816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  896252160.0
relative error loss 0.1810151
shape of L is 
torch.Size([])
memory (bytes)
5035331584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5035331584
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  896098300.0
relative error loss 0.18098404
shape of L is 
torch.Size([])
memory (bytes)
5038657536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5038657536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  895992800.0
relative error loss 0.18096273
shape of L is 
torch.Size([])
memory (bytes)
5041782784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5041782784
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  895897600.0
relative error loss 0.1809435
time to take a step is 290.7464122772217
it  8 : 2038714368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5044994048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5044994048
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  895897600.0
relative error loss 0.1809435
shape of L is 
torch.Size([])
memory (bytes)
5048270848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5048299520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  895809000.0
relative error loss 0.18092561
shape of L is 
torch.Size([])
memory (bytes)
5051428864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5051428864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  895710200.0
relative error loss 0.18090566
shape of L is 
torch.Size([])
memory (bytes)
5054730240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5054730240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  895664900.0
relative error loss 0.1808965
shape of L is 
torch.Size([])
memory (bytes)
5057892352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5057892352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  895599360.0
relative error loss 0.18088326
shape of L is 
torch.Size([])
memory (bytes)
5061165056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5061165056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 11% |
error is  895520000.0
relative error loss 0.18086724
shape of L is 
torch.Size([])
memory (bytes)
5064294400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5064294400
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  895420700.0
relative error loss 0.18084717
shape of L is 
torch.Size([])
memory (bytes)
5067603968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5067603968
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  895328260.0
relative error loss 0.18082851
shape of L is 
torch.Size([])
memory (bytes)
5070757888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5070757888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% | 11% |
error is  895233540.0
relative error loss 0.18080938
shape of L is 
torch.Size([])
memory (bytes)
5074046976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5074046976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  895021300.0
relative error loss 0.18076652
time to take a step is 290.25988578796387
it  9 : 2038714368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5077168128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5077262336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  895021300.0
relative error loss 0.18076652
shape of L is 
torch.Size([])
memory (bytes)
5080440832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5080440832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  894876160.0
relative error loss 0.1807372
shape of L is 
torch.Size([])
memory (bytes)
5083688960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 11% |
memory (bytes)
5083688960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  894719200.0
relative error loss 0.1807055
shape of L is 
torch.Size([])
memory (bytes)
5086711808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5086900224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  894570750.0
relative error loss 0.18067552
shape of L is 
torch.Size([])
memory (bytes)
5090115584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5090115584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  894367200.0
relative error loss 0.18063441
shape of L is 
torch.Size([])
memory (bytes)
5093167104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5093167104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  894122240.0
relative error loss 0.18058494
shape of L is 
torch.Size([])
memory (bytes)
5096550400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5096550400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  893965300.0
relative error loss 0.18055324
shape of L is 
torch.Size([])
memory (bytes)
5099687936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 11% |
memory (bytes)
5099687936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  893849860.0
relative error loss 0.18052992
shape of L is 
torch.Size([])
memory (bytes)
5102993408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5102993408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  893748000.0
relative error loss 0.18050934
shape of L is 
torch.Size([])
memory (bytes)
5106130944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5106130944
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  893565440.0
relative error loss 0.18047248
time to take a step is 289.5382294654846
it  10 : 2038090752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5109428224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 11% |
memory (bytes)
5109428224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  893565440.0
relative error loss 0.18047248
shape of L is 
torch.Size([])
memory (bytes)
5112516608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5112516608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  893398500.0
relative error loss 0.18043877
shape of L is 
torch.Size([])
memory (bytes)
5115854848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5115854848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  893176800.0
relative error loss 0.180394
shape of L is 
torch.Size([])
memory (bytes)
5119074304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5119074304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  893044740.0
relative error loss 0.1803673
shape of L is 
torch.Size([])
memory (bytes)
5122293760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 11% |
memory (bytes)
5122301952
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  892946200.0
relative error loss 0.1803474
shape of L is 
torch.Size([])
memory (bytes)
5125509120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5125509120
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  892917760.0
relative error loss 0.18034166
shape of L is 
torch.Size([])
memory (bytes)
5128630272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 11% |
memory (bytes)
5128630272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  892703000.0
relative error loss 0.18029828
shape of L is 
torch.Size([])
memory (bytes)
5131935744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5131935744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 11% |
error is  892625400.0
relative error loss 0.18028262
shape of L is 
torch.Size([])
memory (bytes)
5135024128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5135155200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% | 11% |
error is  892518140.0
relative error loss 0.18026096
shape of L is 
torch.Size([])
memory (bytes)
5138370560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5138370560
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  892372500.0
relative error loss 0.18023154
time to take a step is 292.0005142688751
it  11 : 2038714368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5141512192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5141512192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 11% |
error is  892372500.0
relative error loss 0.18023154
shape of L is 
torch.Size([])
memory (bytes)
5144797184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5144797184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  892295700.0
relative error loss 0.18021603
shape of L is 
torch.Size([])
memory (bytes)
5147971584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 11% |
memory (bytes)
5147971584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  892134660.0
relative error loss 0.1801835
shape of L is 
torch.Size([])
memory (bytes)
5151227904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5151227904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  892081660.0
relative error loss 0.1801728
shape of L is 
torch.Size([])
memory (bytes)
5154340864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5154340864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  891904500.0
relative error loss 0.18013702
shape of L is 
torch.Size([])
memory (bytes)
5157658624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 11% |
memory (bytes)
5157658624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  891840800.0
relative error loss 0.18012415
shape of L is 
torch.Size([])
memory (bytes)
5160742912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5160742912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  891668000.0
relative error loss 0.18008925
shape of L is 
torch.Size([])
memory (bytes)
5164085248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 11% |
memory (bytes)
5164085248
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  891554800.0
relative error loss 0.18006639
shape of L is 
torch.Size([])
memory (bytes)
5167185920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 11% |
memory (bytes)
5167185920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  891434000.0
relative error loss 0.18004198
shape of L is 
torch.Size([])
memory (bytes)
5170524160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5170524160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  891312400.0
relative error loss 0.18001743
time to take a step is 292.5271999835968
it  12 : 2038090752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5173731328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5173731328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  891312400.0
relative error loss 0.18001743
shape of L is 
torch.Size([])
memory (bytes)
5176946688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 11% |
memory (bytes)
5176946688
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  891202800.0
relative error loss 0.1799953
shape of L is 
torch.Size([])
memory (bytes)
5180088320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 11% |
memory (bytes)
5180088320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  891144960.0
relative error loss 0.17998362
shape of L is 
torch.Size([])
memory (bytes)
5183205376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5183373312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  891054100.0
relative error loss 0.17996526
shape of L is 
torch.Size([])
memory (bytes)
5186584576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5186584576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  890971900.0
relative error loss 0.17994866
shape of L is 
torch.Size([])
memory (bytes)
5189685248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5189685248
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  890889500.0
relative error loss 0.17993201
shape of L is 
torch.Size([])
memory (bytes)
5193023488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5193023488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  890796800.0
relative error loss 0.1799133
shape of L is 
torch.Size([])
memory (bytes)
5196128256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5196128256
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  890736400.0
relative error loss 0.1799011
shape of L is 
torch.Size([])
memory (bytes)
5199454208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 11% |
memory (bytes)
5199454208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  890657000.0
relative error loss 0.17988506
shape of L is 
torch.Size([])
memory (bytes)
5202575360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 11% |
memory (bytes)
5202575360
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  890537700.0
relative error loss 0.17986098
time to take a step is 290.69633650779724
it  13 : 2038714368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5205880832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 11% |
memory (bytes)
5205880832
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  890537700.0
relative error loss 0.17986098
shape of L is 
torch.Size([])
memory (bytes)
5209104384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 11% |
memory (bytes)
5209104384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  890493440.0
relative error loss 0.17985202
shape of L is 
torch.Size([])
memory (bytes)
5212307456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 11% |
memory (bytes)
5212307456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  890415100.0
relative error loss 0.1798362
shape of L is 
torch.Size([])
memory (bytes)
5215539200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 11% |
memory (bytes)
5215539200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  890346500.0
relative error loss 0.17982236
shape of L is 
torch.Size([])
memory (bytes)
5218574336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 11% |
memory (bytes)
5218758656
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  890274300.0
relative error loss 0.17980777
shape of L is 
torch.Size([])
memory (bytes)
5221982208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5221982208
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  890267400.0
relative error loss 0.17980637
shape of L is 
torch.Size([])
memory (bytes)
5225099264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5225099264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  890209000.0
relative error loss 0.17979458
shape of L is 
torch.Size([])
memory (bytes)
5228425216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5228425216
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  890104600.0
relative error loss 0.1797735
shape of L is 
torch.Size([])
memory (bytes)
5231542272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5231542272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  890013440.0
relative error loss 0.17975508
shape of L is 
torch.Size([])
memory (bytes)
5234659328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5234839552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  889906940.0
relative error loss 0.17973357
time to take a step is 289.60724568367004
it  14 : 2038714368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
shape of L is 
torch.Size([])
memory (bytes)
5238050816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 11% |
memory (bytes)
5238050816
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  889906940.0
relative error loss 0.17973357
shape of L is 
torch.Size([])
memory (bytes)
5241155584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5241282560
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  889867500.0
relative error loss 0.17972562
shape of L is 
torch.Size([])
memory (bytes)
5244497920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 11% |
memory (bytes)
5244497920
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  889778400.0
relative error loss 0.17970762
shape of L is 
torch.Size([])
memory (bytes)
5247713280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5247713280
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  889732100.0
relative error loss 0.17969826
shape of L is 
torch.Size([])
memory (bytes)
5250936832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5250936832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  889668600.0
relative error loss 0.17968544
shape of L is 
torch.Size([])
memory (bytes)
5254082560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5254082560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  889605400.0
relative error loss 0.17967266
shape of L is 
torch.Size([])
memory (bytes)
5257375744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5257375744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% | 11% |
error is  889511940.0
relative error loss 0.1796538
shape of L is 
torch.Size([])
memory (bytes)
5260582912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 11% |
memory (bytes)
5260582912
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 11% |
error is  889448200.0
relative error loss 0.17964092
shape of L is 
torch.Size([])
memory (bytes)
5263802368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 11% |
memory (bytes)
5263802368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  889385200.0
relative error loss 0.1796282
shape of L is 
torch.Size([])
memory (bytes)
5267017728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 11% |
memory (bytes)
5267017728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 11% |
error is  889314300.0
relative error loss 0.17961387
time to take a step is 289.30424213409424
sum tnnu_Z after tensor(10482726., device='cuda:0')
shape of features
(4397,)
shape of features
(4397,)
number of orig particles 17586
number of new particles after remove low mass 16509
tnuZ shape should be parts x labs
torch.Size([17586, 702])
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
error without small mass is  1058674000.0
relative error without small mass is  0.21381928
nnu_Z shape should be number of particles by maxV
(17586, 702)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
shape of features
(17586,)
Wed Feb 1 05:56:20 EST 2023
