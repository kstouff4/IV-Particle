Tue Jan 31 16:25:00 EST 2023
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  0% |  0% |
numbers of X: 20100635
numbers of Z: 14695
shape of features
(14695,)
shape of features
(14695,)
ZX	Vol	Parts	Cubes	Eps
Z	0.015057304687693424	14695	14.695	0.10081516905268162
X	0.011489932171263631	402	0.402	0.3057481260827967
X	0.012036143600487895	10892	10.892	0.10338556590366824
X	0.012599550632184707	1084	1.084	0.2265210631680091
X	0.011735480684526251	1577	1.577	0.19523442448099482
X	0.013956946209249808	52639	52.639	0.06424326303927669
X	0.012091503384549732	24243	24.243	0.07930452716647814
X	0.012100877838924424	33632	33.632	0.07112485501356258
X	0.012123413029054186	26782	26.782	0.07678221419382564
X	0.012486416472951052	6241	6.241	0.1260069630546651
X	0.012069076305098413	19628	19.628	0.0850353156058237
X	0.012109638277279343	9229	9.229	0.10947766052966294
X	0.012147650668490549	28862	28.862	0.07494142680437813
X	0.012008187322046579	3756	3.756	0.14731623925655302
X	0.01272242075814652	116037	116.037	0.047862027005476704
X	0.012141637201105313	11986	11.986	0.10043097050360643
X	0.012125125058954868	17588	17.588	0.08833983624875591
X	0.012144669304438634	25904	25.904	0.07768543836561227
X	0.012123785227892	30149	30.149	0.07381111640696414
X	0.012677843969636228	88210	88.21	0.05238125025683698
X	0.012312737163577794	78122	78.122	0.054016580171933254
X	0.012029732689812072	15667	15.667	0.09157073734400517
X	0.01414118810440358	95104	95.104	0.05297801784464129
X	0.0121085040750156	6457	6.457	0.12331613427152507
X	0.012133382643323061	6726	6.726	0.12173300524071495
X	0.012640826075146147	16899	16.899	0.0907760922752768
X	0.01367805464604456	33986	33.986	0.073831454994084
X	0.01247121889539456	44263	44.263	0.06555754556382884
X	0.01215719144441357	8644	8.644	0.11203993095584562
X	0.012134348495436119	86763	86.763	0.051907163968054795
X	0.012579458015690874	631560	631.56	0.027107061305246066
X	0.01210015219787876	3985	3.985	0.1448063151904433
X	0.014971784502854319	479200	479.2	0.03149577162158259
X	0.012074343413298919	3868	3.868	0.14614782374777613
X	0.012124254562487486	10889	10.889	0.1036467486025341
X	0.01211440331062224	20375	20.375	0.08408815111228064
X	0.012148019030439366	198277	198.277	0.03942251932864651
X	0.01301160598327313	36291	36.291	0.07104126008061933
X	0.012083002545975554	845	0.845	0.24272027342754773
X	0.012053805534994801	1780	1.78	0.1891914007980019
X	0.011907182193583193	1595	1.595	0.1954411924883102
X	0.013810443995148762	1542	1.542	0.20767031537669817
X	0.011356623784142773	226	0.226	0.36901747534016494
X	0.011343619969621273	405	0.405	0.30369120521335313
X	0.011901182105505458	1254	1.254	0.21172112258230505
X	0.012016007055127618	360	0.36	0.3219728298234877
X	0.011808613533830399	513	0.513	0.2844639141039399
X	0.01209092421456674	1077	1.077	0.2239140087586909
X	0.011985710405803991	3703	3.703	0.14792333491878226
X	0.011948263415583818	2147	2.147	0.17721109729076764
X	0.01185504175652065	2251	2.251	0.17398395447019915
X	0.014173254006214287	6971	6.971	0.1266848930804096
X	0.011887581055722077	678	0.678	0.2597893036230896
X	0.012032167698154057	3220	3.22	0.15517770867041436
X	0.01212368135710685	2471	2.471	0.1699236869445297
X	0.012088858880581504	2657	2.657	0.16570333572186882
X	0.01196044940933604	1054	1.054	0.22471684520091936
X	0.012024641233426473	869	0.869	0.24007712616143545
X	0.012052279539594896	2143	2.143	0.17783435947658013
X	0.01200661561747413	1070	1.07	0.2238784423046731
X	0.01156560929091289	1383	1.383	0.20297791118108152
X	0.01204412252514351	2917	2.917	0.16042778178633751
X	0.012095081608876112	696	0.696	0.259019933655339
X	0.014026724865165562	2678	2.678	0.17366617347283506
X	0.0120770514447391	6380	6.38	0.12370295644160956
X	0.012007555846260646	2429	2.429	0.17035008978114857
X	0.012054571623579757	1624	1.624	0.19506914732101524
X	0.011868149788150095	1151	1.151	0.21765529242414028
X	0.011956303696564198	940	0.94	0.23342983411499782
X	0.012105759007450898	1962	1.962	0.18341337693536455
X	0.01197393213716129	1802	1.802	0.18800122045353815
X	0.011927227413626859	1905	1.905	0.1843096919713734
X	0.012016236656221703	1849	1.849	0.1866138418942375
X	0.012055966672328946	928	0.928	0.23508126597517792
X	0.011511302907105858	1043	1.043	0.22264522797937158
X	0.012087886278309312	2746	2.746	0.1638890524181056
X	0.0118886751843696	5304	5.304	0.13087096821021182
X	0.01160361568519006	338	0.338	0.32500662367780375
X	0.011729920513499813	654	0.654	0.2617615017690476
X	0.01201913897846266	1715	1.715	0.1913681877802494
X	0.013167538810423414	4329	4.329	0.14489019839405973
X	0.011702957561907187	1162	1.162	0.21595495209569487
X	0.011525636642289125	513	0.513	0.282173258206772
X	0.014128148749355406	1288	1.288	0.22218929548868485
X	0.013327116447575381	689	0.689	0.26843490687719557
X	0.011836335259356446	747	0.747	0.25116871980754596
X	0.01166046421714995	801	0.801	0.2441712021966055
X	0.012020922560380901	703	0.703	0.2576286502094763
X	0.012042703462530646	495	0.495	0.2897607772073908
X	0.012947488768224812	1637	1.637	0.19924111088494664
X	0.012093710021266916	2420	2.42	0.17096795947041993
X	0.012110044046154369	2256	2.256	0.17509303706419244
X	0.01193909889937674	783	0.783	0.24797235272494958
X	0.011842325977756172	885	0.885	0.23740940173243466
X	0.012039777078871694	4453	4.453	0.13931201811201557
X	0.011881629771270568	2644	2.644	0.1650206100583659
X	0.01209139651801651	2917	2.917	0.16063740438376337
X	0.01208174060483185	1544	1.544	0.19853057650981557
X	0.011980034861297497	1631	1.631	0.19438736835591502
X	0.012099449548444805	5866	5.866	0.1272939759001474
X	0.01210107110600025	2732	2.732	0.16422818981487153
X	0.011885008419760854	4592	4.592	0.13729852494244701
X	0.01208993517437664	2734	2.734	0.1641377543475904
X	0.012118984951590878	2183	2.183	0.1770669487446762
X	0.01177709988466642	693	0.693	0.25709978173059755
X	0.012034106347135143	4054	4.054	0.14371762563012092
X	0.011720260267920411	795	0.795	0.2452016429410215
X	0.012039425368659143	1705	1.705	0.19184940584377666
X	0.012017074985780484	708	0.708	0.25699332172347505
X	0.011934840976837688	950	0.95	0.23246862336474364
X	0.011512825387868112	402	0.402	0.3059510548826804
X	0.0117685841358784	2329	2.329	0.17160037898581584
X	0.0119302441416639	434	0.434	0.30180043904489406
X	0.012064747072754966	1047	1.047	0.2258692865249098
X	0.012097639031157677	1514	1.514	0.19992092333538705
X	0.012037478485208258	3290	3.29	0.15409192311243977
X	0.012008442450460954	892	0.892	0.23788877078231088
X	0.01180652670437013	705	0.705	0.2558454388733361
X	0.011692429887590238	334	0.334	0.3271292936159042
X	0.012604591605571524	5154	5.154	0.1347291143393812
X	0.011900004263675174	560	0.56	0.27698294591797606
X	0.012091322169956892	4688	4.688	0.13713934261610455
X	0.011877984005274555	808	0.808	0.24496862157655724
X	0.011653131311551618	1489	1.489	0.1985405927606766
X	0.01208432277124577	3033	3.033	0.1585318893448236
X	0.011741075136933976	1748	1.748	0.18867838331145292
X	0.01207050415244744	1426	1.426	0.20379895905303083
X	0.011812338563211908	798	0.798	0.24553363776114448
X	0.012083168540343265	2113	2.113	0.17882453814605181
X	0.01222132498530224	4576	4.576	0.13874289717503746
X	0.011512176114887302	1295	1.295	0.20715522197794417
X	0.01202071117605072	6727	6.727	0.12134901204596434
X	0.012065391112072544	427	0.427	0.30458219339801673
X	0.012025288767829599	1365	1.365	0.20653212851526795
X	0.011148004069042197	321	0.321	0.32626014789516533
X	0.012136892981448861	539	0.539	0.2823836749015143
X	0.011805750710888642	1879	1.879	0.18452516802244956
X	0.01205883018593204	730	0.73	0.2546795691194683
X	0.01192903019528304	903	0.903	0.23639545867987236
X	0.012033726160782294	1306	1.306	0.2096455503893549
X	0.01164412934730091	1213	1.213	0.2125276336833229
X	0.01198908651844032	1360	1.36	0.20657720414216382
X	0.01154312261892548	858	0.858	0.23783659231318616
X	0.013795051048755011	3105	3.105	0.16439448189158007
X	0.012074685170723543	5493	5.493	0.1300235658758123
X	0.013422455174210497	3179	3.179	0.16162696259562495
X	0.011982536508001245	877	0.877	0.23906521507495132
X	0.01205911711413696	897	0.897	0.2377794710536388
X	0.011924106421426361	642	0.642	0.26482781974465625
X	0.0114417403189752	1060	1.06	0.22100159455414434
X	0.012124533841872396	996	0.996	0.23003922928129367
X	0.012009001649273517	1153	1.153	0.21838653904547703
X	0.012344583884607963	1621	1.621	0.19674233138184877
X	0.01207466322430867	1976	1.976	0.18282238159854242
X	0.012102065636526079	1121	1.121	0.2210130733555611
X	0.011843565541835521	1707	1.707	0.19072881721345927
X	0.012096717591298359	6725	6.725	0.12161629006911197
X	0.013208848686194331	1452	1.452	0.20875302761980538
X	0.012021938417364217	926	0.926	0.23502884739192884
X	0.01202654076346152	1168	1.168	0.21755344339847224
X	0.011091839349143384	185	0.185	0.39139080105244195
X	0.014392933448019249	5705	5.705	0.1361333576167909
X	0.011941095677971645	417	0.417	0.30593984410737307
X	0.012097356167312077	3529	3.529	0.15078087860162198
X	0.011370947245364615	359	0.359	0.3163982446185003
X	0.012007534863881187	1927	1.927	0.18401676032763473
X	0.012106614388625515	2301	2.301	0.1739276772143566
X	0.012045682652206015	547	0.547	0.2802945780810757
X	0.011469935462595999	1003	1.003	0.22529593045554594
X	0.012011681331830113	1585	1.585	0.19642262550238135
X	0.011512992123407527	365	0.365	0.3159597641905941
X	0.012027523826150155	482	0.482	0.292219860584432
X	0.011526668733386924	734	0.734	0.2504201381062965
X	0.01208831906111916	2086	2.086	0.17961828141266198
X	0.012031648211412399	704	0.704	0.25758317301246636
X	0.012119548587039541	2553	2.553	0.1680654888652842
X	0.012097130268535976	1337	1.337	0.20837725693684692
X	0.012056793225653306	1139	1.139	0.2195681602240386
X	0.013260186195649452	2019	2.019	0.1872712464657546
X	0.012121052869026601	1570	1.57	0.19764245100953293
X	0.011887825587376062	816	0.816	0.24423285634927666
X	0.012079614474789346	4129	4.129	0.14302196162134548
X	0.011751162505157042	660	0.66	0.2611233039639142
X	0.01204309953958823	1076	1.076	0.2236876472041932
X	0.01182133981730956	996	0.996	0.22810551619099037
X	0.012620054118698385	2569	2.569	0.1699935216567683
X	0.012096179042275284	2905	2.905	0.16087949335590615
X	0.011753729202384539	1126	1.126	0.21854745998887146
X	0.012068174571969187	1580	1.58	0.1969373844145892
X	0.01413648505349324	2637	2.637	0.17501573767591505
X	0.012077392907894271	5314	5.314	0.13147723082238735
X	0.012016181799759537	1099	1.099	0.22195057066502483
X	0.011934822328658173	516	0.516	0.28491944224434296
X	0.011689571479089651	1142	1.142	0.217125498722881
X	0.012050370806114211	955	0.955	0.23280859673706158
X	0.01180736677002881	1075	1.075	0.22228740874329259
X	0.01204722594116352	1065	1.065	0.22448077302234942
X	0.01208363714988908	1798	1.798	0.18871336675780234
X	0.011824106355404707	631	0.631	0.2656111625738004
X	0.012078894103656621	1785	1.785	0.18914563258328582
X	0.011926503843370299	404	0.404	0.3090608033356274
X	0.012138466764511581	4423	4.423	0.14000674689028345
X	0.01205443201287872	1403	1.403	0.20481559758580145
X	0.012135426196788639	7352	7.352	0.11818159445610894
X	0.012118538756261742	1038	1.038	0.22685636426004224
X	0.012037477046307025	3512	3.512	0.15077418453329747
X	0.01182305341950833	611	0.611	0.26847023762533045
X	0.013600974088062157	1074	1.074	0.2330890708228472
X	0.012113172080659497	1939	1.939	0.18417330966250206
X	0.011690782804880282	838	0.838	0.24073164923418297
X	0.014311151042456553	2928	2.928	0.1697075502176038
X	0.01201466780488975	4704	4.704	0.1366935905888707
X	0.01195744478157906	870	0.87	0.23953724142623412
X	0.011970027419171024	903	0.903	0.2366659605847499
X	0.011727046415274967	2514	2.514	0.16708640832949007
X	0.011721045455956489	422	0.422	0.3028432975520916
X	0.011927079997409154	320	0.32	0.33403711024635696
X	0.011364411035239344	214	0.214	0.37587581831583855
X	0.01161407980030749	574	0.574	0.272494435269593
X	0.01185402474264502	6016	6.016	0.1253677987970504
X	0.012090118535712888	859	0.859	0.24144180267220805
X	0.011811823193312351	596	0.596	0.2706181324351566
X	0.012080603392739822	434	0.434	0.30306303331355533
X	0.012112105168508297	4649	4.649	0.13760050537896526
X	0.011934182341434198	568	0.568	0.27594007487390376
X	0.011981798648741628	1234	1.234	0.21333836012795443
X	0.011939806312935368	2259	2.259	0.1741915232486226
X	0.011727905515713725	1477	1.477	0.19950172463422372
X	0.011752971246104502	825	0.825	0.24241783876397824
X	0.012055224333098387	2671	2.671	0.16525976701718514
X	0.011759357829025482	617	0.617	0.2671157404261579
X	0.01207079897486777	749	0.749	0.2525911129894381
X	0.012068479839988	698	0.698	0.25858245174328126
X	0.011679862484689786	972	0.972	0.22904658529059355
X	0.013650727997113199	1885	1.885	0.1934705291620534
X	0.011697258145914578	1884	1.884	0.1837952959983361
X	0.011900499291756099	1081	1.081	0.22245721521445252
X	0.013929120774495343	2601	2.601	0.17495557419685168
X	0.013917986677168022	1585	1.585	0.20630783264369512
X	0.0125856865373833	557	0.557	0.28270961027248587
X	0.012101542741600618	3846	3.846	0.1465358270097199
X	0.012144033859718737	3122	3.122	0.15726908386285007
X	0.0121509203216841	5070	5.07	0.13382391714072228
X	0.012091299878622298	2222	2.222	0.17589074129017368
X	0.011535692020210472	1500	1.5	0.19738650828734933
X	0.012080507991804226	4220	4.22	0.14198994207195983
X	0.014203817044307784	7867	7.867	0.12176766360043789
X	0.012027185235032123	6237	6.237	0.12446944466549623
X	0.0121212436801049	1342	1.342	0.20825633544459565
X	0.012156247094355773	68198	68.198	0.05627851467996892
X	0.012117951453883504	35999	35.999	0.06956320617803903
X	0.01208225481092721	9395	9.395	0.10874694586491912
X	0.012137096001516511	28229	28.229	0.07547557514110444
X	0.01201974774053112	1121	1.121	0.22051082471768876
X	0.012024748038050977	4760	4.76	0.13619348848679294
X	0.012137496374601521	157937	157.937	0.0425156095141513
X	0.012144899615810677	147358	147.358	0.04351844849694589
X	0.011926552995476548	1331	1.331	0.20770436759121697
X	0.012122005437281189	16489	16.489	0.09025266862612091
X	0.0120725447285283	50187	50.187	0.06219219176661485
X	0.012438327169150741	12244	12.244	0.10052626611588324
X	0.01207048868056341	8928	8.928	0.11057498535091163
X	0.011984712938497026	14791	14.791	0.09322735576175784
X	0.01212488432377761	4881	4.881	0.1354325114610629
X	0.012110739839278257	4957	4.957	0.1346843863744633
X	0.011975768183965766	4055	4.055	0.14347321846416225
X	0.012188758855570409	15096	15.096	0.09311776265287504
X	0.011965299709054071	4158	4.158	0.14223714684081004
X	0.012110694284579039	2539	2.539	0.16833281267077868
X	0.012114365430629059	11473	11.473	0.10182972216319705
X	0.01199076830022708	7234	7.234	0.11834670447014603
X	0.014685803233394638	123739	123.739	0.049143230385864
X	0.012118201495556478	29930	29.93	0.07397934540470211
X	0.011766975819438929	1814	1.814	0.18649872170555643
X	0.012802261463095732	13162	13.162	0.09908051806396533
X	0.012088748911105713	3131	3.131	0.15687946388598933
X	0.012127852386208015	6306	6.306	0.1243588351159587
X	0.01215150767128739	27419	27.419	0.07624174912951641
X	0.01228002635574057	40288	40.288	0.06729888415505898
X	0.01210070844718623	72181	72.181	0.05513947595281942
X	0.01134611345821702	475	0.475	0.2879947057639736
X	0.012127679672692031	1962	1.962	0.18352401625784337
X	0.012141477329528161	32888	32.888	0.07173724431264838
X	0.0121038595860273	30093	30.093	0.07381638916781569
X	0.01212341048839775	15659	15.659	0.0918234477351677
X	0.01198847870674668	1618	1.618	0.19495237020743386
X	0.012145402885650478	105814	105.814	0.048598507424601096
X	0.012142857645902626	4789	4.789	0.13636157922416622
X	0.012134513855220476	9541	9.541	0.10834515503037173
X	0.012746216327763018	40918	40.918	0.0677884383297724
X	0.011983521677157011	813	0.813	0.24518730123578777
X	0.01204919240056256	22371	22.371	0.08136248050092225
X	0.01303990588934393	31218	31.218	0.07475205338611693
X	0.013022593503910116	136522	136.522	0.04569110004921651
X	0.012135474045410417	15681	15.681	0.09181091809922981
X	0.012080093391077368	16417	16.417	0.09028012766594477
X	0.01186013514589546	4365	4.365	0.13954098895813183
X	0.012037347018680996	3063	3.063	0.15780760473320765
X	0.012153431052706632	8255	8.255	0.11376114153114265
X	0.013543546189983918	5682	5.682	0.13358070194508265
X	0.01211567555804893	5787	5.787	0.12792773703284277
X	0.012166433051717504	31455	31.455	0.07286034003132952
X	0.012183454913939757	9908	9.908	0.10713420572952813
X	0.011583077823164435	3417	3.417	0.1502195068147832
X	0.012085975634422083	2560	2.56	0.16775697372706377
X	0.01219865083018539	183480	183.48	0.04051112990076962
X	0.012122039816641918	3967	3.967	0.14511239230831816
X	0.012028012220615319	954	0.954	0.2327457871208613
X	0.012424026929013714	7112	7.112	0.12043615410129331
X	0.012049998988281995	64165	64.165	0.05726592374335244
X	0.012087732454916138	11416	11.416	0.10192411744493339
X	0.012695700158855029	8462	8.462	0.11447954077926982
X	0.011892813263455199	2293	2.293	0.17309858391976332
X	0.011709887660380608	7810	7.81	0.11445477033738607
X	0.012137584777445747	6676	6.676	0.12205024328634553
X	0.012814757146952382	278338	278.338	0.03584093185312697
X	0.012097399787847036	17631	17.631	0.08820063176624467
X	0.012137153438541931	5155	5.155	0.1330340229175348
X	0.012146154748133313	54904	54.904	0.060479975703266936
X	0.012772058462849005	60667	60.667	0.059488925694286
X	0.012163137957818277	3547	3.547	0.15079773371622462
X	0.012093031992176949	47203	47.203	0.06351191390136558
X	0.014166843492883264	69811	69.811	0.058764684976757006
X	0.014644423954193906	245304	245.304	0.03908311954604262
X	0.012121690220858707	5293	5.293	0.1318116396387151
X	0.012098213356899049	810	0.81	0.24627031928440693
X	0.012053880393649118	3630	3.63	0.14919012720792274
X	0.012127486551921722	8947	8.947	0.11067030872598396
X	0.012058473730770937	18184	18.184	0.08720357834397834
X	0.012122017863540583	8308	8.308	0.11342082726456837
X	0.013993590793362655	648	0.648	0.2784740437852244
X	0.01212644660926703	12898	12.898	0.09796487653137913
X	0.012084580991396658	52776	52.776	0.06117843670044527
X	0.01196242809626032	58748	58.748	0.058831341485244604
X	0.012113269178491502	1926	1.926	0.184587248054082
X	0.012756685812166059	67283	67.283	0.05744834263404888
X	0.01212263169534285	4873	4.873	0.13549819184557146
X	0.01211716056424411	7966	7.966	0.11500591745544991
X	0.01205392937546933	6464	6.464	0.12308612467126112
X	0.012078596552448194	1503	1.503	0.20030225175202968
X	0.012241074474056247	85386	85.386	0.052337256946233916
X	0.01214485662454413	19670	19.67	0.08515222702276487
X	0.013180008605282997	6037	6.037	0.12972733785441723
X	0.01243908733353066	10717	10.717	0.10509250936017749
X	0.012042609938755201	2046	2.046	0.18055311968872906
X	0.013093485553821817	169644	169.644	0.0425767253896278
X	0.012028667487788249	8250	8.25	0.11339342079160002
X	0.012155158700623841	94959	94.959	0.0503974040859272
X	0.011972742243287341	1048	1.048	0.22522198581833838
X	0.011970312936760352	1673	1.673	0.19269473758572075
X	0.01191209857520622	1979	1.979	0.18190619715995507
X	0.012163534220666444	10931	10.931	0.10362549759955346
X	0.01383064806166232	3125	3.125	0.16418400153767443
X	0.012128845019968602	4193	4.193	0.14248359407642877
X	0.012326511002461111	3771	3.771	0.148409332499222
X	0.012034351004282785	1973	1.973	0.1827112111119208
X	0.012927595508670885	119141	119.141	0.04769638943608127
X	0.01208219511338343	10819	10.819	0.10374955469243456
X	0.012764106112861917	11051	11.051	0.10492112590927467
X	0.013158784091533378	43256	43.256	0.06725477666380289
X	0.01347552615359712	150141	150.141	0.04477292583471986
X	0.012109835431614336	4578	4.578	0.13829956612741215
X	0.012071439089648426	4511	4.511	0.13883386262998884
X	0.012044725103061142	2233	2.233	0.1753756888247152
X	0.011899838164392154	600	0.6	0.27068442547663657
X	0.01371101165617631	4829	4.829	0.14160244912540446
X	0.012097324338505305	10278	10.278	0.1055828961099857
X	0.012005875863085968	3345	3.345	0.15310847899424876
X	0.012076677561002247	2204	2.204	0.17629715082688122
X	0.012077478560646232	8722	8.722	0.11146026320998224
X	0.012044212471269291	6045	6.045	0.12583285036717765
X	0.01353103883965962	503760	503.76	0.029948091644777155
X	0.011955406030039062	1386	1.386	0.20508490693828615
X	0.013575411137300982	23175	23.175	0.08367153751476211
X	0.011985442644273508	7898	7.898	0.11491559838628511
X	0.011925137247445802	986	0.986	0.22954199773465317
X	0.01215801511963045	56666	56.666	0.05986597233067144
X	0.012185394540287652	103557	103.557	0.049002703953627255
X	0.012185672741725648	248221	248.221	0.036615951980981776
X	0.01214910419057175	20498	20.498	0.08399967181046669
X	0.012131500600065043	11181	11.181	0.10275697058374979
X	0.012078830749001433	1381	1.381	0.20603629852229646
X	0.01400320210800444	39265	39.265	0.07091540460338705
X	0.012140672780905516	366936	366.936	0.03210338377782918
X	0.013415455531523447	23568	23.568	0.08287576778256715
X	0.011999940391694861	8724	8.724	0.11121272373490554
X	0.012092806303956276	34412	34.412	0.07056766176384643
X	0.014918677080158581	143642	143.642	0.04700560247021077
X	0.012008504511242786	24681	24.681	0.07865181056059001
X	0.012000203788133138	4924	4.924	0.1345726668615664
X	0.015032340021788758	4569	4.569	0.1487311071645438
X	0.012119104315577172	11993	11.993	0.10034927288469625
X	0.012408364842836602	6507	6.507	0.12400653786226878
X	0.012776002146777697	195841	195.841	0.04025598454377937
X	0.011975676033548864	7304	7.304	0.11791790594779382
X	0.011867231986782223	1319	1.319	0.20798637176758386
X	0.012124892807492469	9204	9.204	0.10962268437463354
X	0.014120793090274317	18704	18.704	0.0910559224812208
X	0.011676328126960196	795	0.795	0.2448948886393646
X	0.012114360801979779	32063	32.063	0.07229342208661273
X	0.012183690412115998	30484	30.484	0.0736606647844959
X	0.013890823630626568	20713	20.713	0.08753095443020832
X	0.012130853359764236	36020	36.02	0.06957435916242209
X	0.01214513343035202	34799	34.799	0.07040635192880446
X	0.01202736260677527	9493	9.493	0.10820707092418481
X	0.012102538456439313	18266	18.266	0.0871788239396289
X	0.012118995629032497	30192	30.192	0.0737663421552007
X	0.012161432832207162	81589	81.589	0.05302138220721987
X	0.012101986799994299	3131	3.131	0.1569367070667498
X	0.012184914724291883	43681	43.681	0.06533962545333084
X	0.012166856651558106	42069	42.069	0.06613104734188016
X	0.012136645346070946	25980	25.98	0.07759251666761681
X	0.012477270049884708	10506	10.506	0.1058995365339171
X	0.012006611289526719	1939	1.939	0.18363165451254343
X	0.012084833688268199	5312	5.312	0.1315207279255804
X	0.012046584460111427	4569	4.569	0.1381489506966702
X	0.014744354249658222	59428	59.428	0.06283643641962297
X	0.012807555197736457	102079	102.079	0.05006220140791705
X	0.012145866807800295	55858	55.858	0.060133208586587855
X	0.012201138025369915	89620	89.62	0.05144365300370132
X	0.01275506340314147	31492	31.492	0.07398791568807386
X	0.01211463715587588	50018	50.018	0.06233443532558696
X	0.012088239079994921	6269	6.269	0.12446720110365271
X	0.012707652420075233	97520	97.52	0.050697967068173505
X	0.011862382422647654	870	0.87	0.23890077359969442
X	0.012069637863961109	6669	6.669	0.12186467673234891
X	0.012135769557917949	132564	132.564	0.04506927518615787
X	0.012129601990608564	15245	15.245	0.09266300371493755
X	0.012023758478037204	1244	1.244	0.2130132512920647
X	0.012099054874716794	12585	12.585	0.09869596753211231
X	0.013339928870195968	63918	63.918	0.05931667229476824
X	0.012020837693753741	21373	21.373	0.08254480562243358
X	0.01429019402877442	25762	25.762	0.08216487203854825
X	0.012086231295384154	813	0.813	0.24588580173014665
X	0.01199406947069315	2284	2.284	0.17381616093756752
X	0.012040195757338331	2497	2.497	0.16894227385941613
X	0.012643673579839832	29294	29.294	0.07557268736182823
X	0.014117171877123967	20897	20.897	0.0877447726818147
X	0.012104825682964166	18489	18.489	0.08683238018673357
X	0.011907224304577912	1001	1.001	0.22827524581110947
X	0.012135318952267275	40629	40.629	0.06684542477492554
X	0.01199294182388796	4954	4.954	0.13427337595017683
X	0.01397182761820612	13171	13.171	0.10198700141460405
X	0.012141715443663789	9759	9.759	0.10755358991371047
X	0.012083195548841257	30800	30.8	0.07320549739944221
X	0.012034865616744373	1193	1.193	0.21607288327254487
X	0.01204274648168368	24507	24.507	0.07891237802290423
X	0.0121088962508703	14947	14.947	0.09322166194491906
X	0.01202730097959048	10288	10.288	0.10534463139752909
X	0.012131516153098149	15955	15.955	0.0912723902649487
X	0.012574983818685147	123230	123.23	0.04673013067405017
X	0.01210116066752464	5180	5.18	0.13268823660523504
X	0.01269475636621603	7503	7.503	0.11915981162883413
X	0.01212765086286744	67484	67.484	0.056431980164771925
X	0.012129671010053784	9562	9.562	0.10825137650469607
X	0.014269908502101315	225175	225.175	0.03986884665302763
X	0.012031645792391406	4340	4.34	0.14047911723434103
X	0.01232514590170846	70294	70.294	0.055970338998483464
X	0.013829056729439987	183408	183.408	0.042246567409540366
X	0.012016037052057007	2724	2.724	0.16400284623275013
X	0.012094668561154652	57607	57.607	0.05943463101467165
X	0.01218246489092509	9713	9.713	0.10784348817200184
X	0.01463354545967081	75864	75.864	0.05777916675449571
X	0.012034893303517842	7974	7.974	0.11470666970371594
X	0.012015667491497828	18302	18.302	0.08691267500396821
X	0.01197782477056583	1025	1.025	0.22692622922563951
X	0.012122371456417005	2334	2.334	0.17317914000544568
X	0.012027318262690356	2444	2.444	0.17009408078052807
X	0.012123417318188815	11196	11.196	0.10268824266482492
X	0.012090098546664936	7860	7.86	0.11543453688543202
X	0.01293587697317438	59615	59.615	0.06009155503866971
X	0.012104559084230944	9785	9.785	0.10734851677677033
X	0.011945832086753665	2473	2.473	0.16904309276668122
X	0.01204772154886077	5155	5.155	0.13270646598524036
X	0.012726218066782844	3116	3.116	0.15984558200069743
X	0.013268512939501261	2198	2.198	0.18208109426782773
X	0.014199953081787967	77544	77.544	0.05678664834569289
X	0.012751899400854676	22177	22.177	0.08315544534991469
X	0.012105198458021155	1790	1.79	0.18910642926274918
X	0.01209334633524147	2151	2.151	0.17781512254183324
X	0.012021715439215902	2402	2.402	0.17105317781744683
X	0.012469061341703091	243147	243.147	0.03715252045667743
X	0.01204429019025568	4183	4.183	0.14226490625706703
X	0.012118880033577173	7830	7.83	0.11567341963902136
X	0.012001940995107844	33461	33.461	0.07105110836713026
X	0.011851070282886615	10505	10.505	0.10410074534263973
X	0.012079813257121234	3202	3.202	0.15567288151915828
X	0.013212896365166047	6853	6.853	0.12446266587626591
X	0.01224351067552055	30834	30.834	0.07350079424344008
X	0.012382599213275322	30822	30.822	0.07378764790417174
X	0.012024073406299024	27348	27.348	0.07603998384063236
X	0.012140359676394775	5407	5.407	0.13094584664945802
X	0.013895365112976548	4205	4.205	0.14894799248942692
X	0.012086090037526729	21696	21.696	0.08228147171441148
X	0.012487416099927538	6441	6.441	0.12469233607247476
X	0.012178287185249732	41707	41.707	0.06634258961456545
X	0.01206976412758392	1094	1.094	0.22261810778614946
X	0.012162706401628762	63695	63.695	0.05758485714594332
X	0.012134169031351751	1528	1.528	0.19950887914746093
X	0.012121529562706379	21150	21.15	0.08306454384090363
X	0.012010866477696938	7687	7.687	0.11603949212033085
X	0.011927960653721861	1574	1.574	0.1964206557550769
X	0.01203870553219554	6405	6.405	0.12341090938628758
X	0.01208544704831823	26442	26.442	0.0770293334174334
X	0.01183354138486913	8074	8.074	0.11359049609716028
X	0.012073630207191712	18860	18.86	0.08618507433877409
X	0.01212502232091929	3192	3.192	0.15602944176413833
X	0.01279868456509556	119906	119.906	0.04743600870397581
X	0.011951870425431901	996	0.996	0.22894202116578913
X	0.012113564627675391	1662	1.662	0.1938862363740358
X	0.012022403233337614	4303	4.303	0.1408445288575417
X	0.012613800203993721	8938	8.938	0.11216788536757007
X	0.01210119348332703	1323	1.323	0.2091331056772084
X	0.012033766013711259	4706	4.706	0.13674660301699215
X	0.012126944892748138	15228	15.228	0.09269070355890684
X	0.012139512811247796	60594	60.594	0.058513652976183946
X	0.012879039444786184	2724	2.724	0.167838706314411
X	0.011872419597937599	1013	1.013	0.2271485299939911
X	0.011993027983020791	2545	2.545	0.167653910132656
X	0.012120759562049546	122255	122.255	0.04628295387650102
X	0.014257665712290153	445481	445.481	0.031749708869206944
X	0.0120952220601888	8470	8.47	0.11260996200296458
X	0.013863853030021548	3670	3.67	0.15574207639004595
X	0.012142274704591158	107154	107.154	0.04839092110050505
X	0.014512855073795556	63096	63.096	0.06127034750679069
X	0.011436265421683411	1291	1.291	0.2069121505832178
X	0.011986521859351972	4743	4.743	0.13621136727622216
X	0.01213959026349024	20081	20.081	0.08455505262402407
X	0.014394370158959294	234662	234.662	0.03943815894151474
X	0.012564211094321778	227491	227.491	0.03808234369408141
X	0.012177420884018441	70395	70.395	0.05571915237697821
X	0.012129375996139093	7553	7.553	0.11710434017460526
X	0.012438393517074736	4137	4.137	0.14433099000230148
X	0.011798486625043618	2145	2.145	0.17652233734691594
X	0.012066814454078915	33716	33.716	0.0709989942298641
X	0.012057538139635298	5938	5.938	0.12663083881413736
X	0.012084599334100322	9353	9.353	0.10891652429905398
X	0.012089232931060822	64224	64.224	0.05731044731987649
X	0.01215018694276814	215177	215.177	0.03836445537083739
X	0.012123145065565529	2835	2.835	0.1623132895631049
X	0.011523694759445775	918	0.918	0.2324072588086034
X	0.012110144537215652	7825	7.825	0.11567024624127255
X	0.012780127163197728	34184	34.184	0.07203951247746053
X	0.012136459881741826	22077	22.077	0.08191887679764817
X	0.011855699427521002	1765	1.765	0.18868070494963712
X	0.01152571608467586	587	0.587	0.26978011700422616
X	0.012149733035750699	9732	9.732	0.1076766523717451
X	0.012081304616489215	1810	1.81	0.1882832791015339
X	0.013504674486451703	2720	2.72	0.17059713714277933
X	0.012027654722971822	1849	1.849	0.1866729312419944
X	0.01213394255493192	5899	5.899	0.12717678623214485
X	0.013434963676685148	6816	6.816	0.12538212852454012
X	0.012020541648694526	3281	3.281	0.15416031973054367
X	0.0120953238076984	1842	1.842	0.18725896753436147
X	0.012740432267295208	75482	75.482	0.055264586065563205
X	0.012120701161559038	147927	147.927	0.04343369373480408
X	0.012148954240291329	34839	34.839	0.07038677577769761
X	0.012115351108748344	20261	20.261	0.08424776261638986
X	0.012101255011958655	8347	8.347	0.11317921216329906
X	0.01208532522429122	6261	6.261	0.1245101852919072
X	0.014495927481636306	11694	11.694	0.10742226520447866
X	0.012180224068187615	63783	63.783	0.057585982177229725
X	0.013166211721019236	6295	6.295	0.1278856160886542
X	0.012752298888925883	14351	14.351	0.09613955470437058
X	0.0120921337618606	8172	8.172	0.1139527634274673
X	0.014923823633887376	362199	362.199	0.034539094982585006
X	0.012110738364700741	10596	10.596	0.10455453882741528
X	0.0121371908073405	32033	32.033	0.07236138242895317
X	0.013880315445388319	67505	67.505	0.05902297983063241
X	0.01213817372943756	37881	37.881	0.0684295952631329
X	0.01213635741509079	13232	13.232	0.09716002589989878
X	0.012775896619283605	120126	120.126	0.04737888009129341
X	0.012143054922882519	49335	49.335	0.06266969901790663
X	0.012122215957383808	11518	11.518	0.10171889823984473
X	0.012008360910183539	4099	4.099	0.14308757976649591
X	0.012115351748792543	15879	15.879	0.09137715374061585
X	0.012131716560372584	5183	5.183	0.13277419655472797
X	0.012149464079311553	39742	39.742	0.06736523060139561
X	0.014418021947491342	175159	175.159	0.043500182106450166
X	0.012865762753143547	35734	35.734	0.07114067286619065
X	0.012162037562490902	42828	42.828	0.06572937692741604
X	0.012093057925785097	7391	7.391	0.1178359055442077
X	0.012118401400702436	2766	2.766	0.16363054841828506
X	0.014675881370630924	19270	19.27	0.09132173612511242
X	0.01196520947248824	5800	5.8	0.12730069387311463
X	0.013567890089604033	91921	91.921	0.052848482994557626
X	0.013526285126426764	175138	175.138	0.04258592164022804
X	0.012028764633656831	880	0.88	0.23909993148397535
X	0.012198011203971352	377864	377.864	0.031840839047039333
X	0.01248072827156524	9265	9.265	0.11044129780180212
X	0.01212376091402677	27530	27.53	0.07608114686499655
X	0.012032416279760783	4493	4.493	0.1388690538851931
X	0.013859820944219866	21668	21.668	0.08616143648341552
X	0.012146531001050728	79250	79.25	0.053516084485339524
X	0.011939694611477009	1082	1.082	0.2226325458849806
X	0.012241812539698471	313047	313.047	0.033942622407118296
X	0.012763279570044927	10814	10.814	0.10567979747683914
X	0.012559276224826466	15476	15.476	0.09327572436937635
X	0.012122947318047028	7313	7.313	0.11835071376825443
X	0.012103229778694257	6163	6.163	0.1252284591420004
X	0.011408380148813523	1290	1.29	0.20679724942713534
X	0.011917277877272682	7579	7.579	0.1162844704945443
X	0.012124799570450623	4555	4.555	0.1385889897484136
X	0.012144555752461705	16035	16.035	0.09115298383174951
X	0.01495948147797915	15714	15.714	0.09837315431804777
X	0.012128031543372799	13015	13.015	0.09767469155790318
X	0.012142698108082616	124701	124.701	0.04600606483989872
X	0.01329568155047468	2616	2.616	0.17193235887994981
X	0.012097753686994426	12003	12.003	0.10026244950138075
X	0.012030290972177941	2185	2.185	0.17658002071306794
X	0.011960314253516747	2862	2.862	0.16107358336829436
X	0.012072839171961086	54604	54.604	0.0604683779730844
X	0.01372722721024823	622695	622.695	0.028039427871779193
X	0.012050124938715644	13753	13.753	0.09568959187342732
X	0.012771537745554088	91303	91.303	0.051910201481709696
X	0.012444550327660387	4016	4.016	0.14579025696093512
X	0.012112279888091469	62285	62.285	0.0579358624468578
X	0.012117493408568006	73861	73.861	0.05474349696572355
X	0.012107085600155984	15644	15.644	0.09181153925675076
X	0.012093810529166273	2289	2.289	0.17416964068626467
X	0.012688672698865517	62839	62.839	0.05866723810277233
X	0.012055666941315208	8004	8.004	0.11462905720404826
X	0.01211154925300651	41704	41.704	0.06622276794863444
X	0.01241510907673333	114797	114.797	0.047643830063250084
X	0.012134370180802502	6020	6.02	0.12632042635542307
X	0.015014011294743774	14187	14.187	0.101906540028894
X	0.012044080375764885	9905	9.905	0.10673488546923166
X	0.014858404318471833	71062	71.062	0.05935330612257769
X	0.012729410792919147	74187	74.187	0.05556826420546479
X	0.014113918743256966	19498	19.498	0.08978818283483596
X	0.014183012348589272	5553	5.553	0.1366932079780972
X	0.01274647507747025	175370	175.37	0.04173287133618223
X	0.012034812733892277	39023	39.023	0.06756258032883596
X	0.012815869911800074	151048	151.048	0.04394178590173102
X	0.012121365905050398	41882	41.882	0.06614667976467319
X	0.012021955244876401	48675	48.675	0.0627417145039278
X	0.01214439341503596	9706	9.706	0.10775692265684139
X	0.01195987134536048	72499	72.499	0.05484429992815423
X	0.012147483959035971	136914	136.914	0.04460116045023555
X	0.012033529772862905	36237	36.237	0.0692490149862986
X	0.012020147584599818	3917	3.917	0.1453177760642557
X	0.014649746114299644	10079	10.079	0.11327590772012114
X	0.01203051004467312	6531	6.531	0.12258428137278804
X	0.01208614690179225	33037	33.037	0.07152026062990322
X	0.012139975141506379	25950	25.95	0.07762950407725995
X	0.012141776213805418	4383	4.383	0.14044412751795918
X	0.012129577347215483	10944	10.944	0.10348796840083896
X	0.0120452147543719	59850	59.85	0.05860258825228276
X	0.012563555259357697	28214	28.214	0.07636293828180617
X	0.012694173083839041	18199	18.199	0.08868543424288493
X	0.01216064773095742	21554	21.554	0.08263098754783003
X	0.012037259707452893	13068	13.068	0.09729850521029258
X	0.012146885897333443	10865	10.865	0.10378750545507452
X	0.012023820979130415	2073	2.073	0.17967227483953732
X	0.013366286929149526	5292	5.292	0.13618532599034577
X	0.012153842610216719	81093	81.093	0.05311820780141728
X	0.01209267238412977	6684	6.684	0.12185086465429068
X	0.012148417907978248	35347	35.347	0.07004692170816007
X	0.01183477407351168	1152	1.152	0.21738813207549676
X	0.011992452638073537	1402	1.402	0.20451256677436877
X	0.012040326076959254	67080	67.08	0.05640899967288029
X	0.012132596145734566	48993	48.993	0.06279714546341748
X	0.012135072922242758	17153	17.153	0.0891047318558779
X	0.012036804733434624	2499	2.499	0.16888133486442866
X	0.012077491420239788	2010	2.01	0.18179986654171484
X	0.01213223484992827	11342	11.342	0.10227050219630238
X	0.012076904765692599	9763	9.763	0.10734721560667713
X	0.012107227635671168	39732	39.732	0.06729272165623071
X	0.012093567196851854	2893	2.893	0.16109003015340242
X	0.011948435956993672	7770	7.77	0.11542414878059608
X	0.01210561015736744	11337	11.337	0.1022106568180708
X	0.012067831756927652	8769	8.769	0.11123114101428917
X	0.012189131124795622	67616	67.616	0.056490370684237526
X	0.012239294438280313	4190	4.19	0.1429488897219838
X	0.012109480675759544	31250	31.25	0.07290519886579423
X	0.013469463238369986	127777	127.777	0.0472388374046131
X	0.012124645864837013	21913	21.913	0.08209607762797401
X	0.012566258576850644	24143	24.143	0.08043991182463768
X	0.012142898210237582	55386	55.386	0.060298630843294565
X	0.011979157186859962	1011	1.011	0.22797735081418596
X	0.012144340958140981	15317	15.317	0.0925550414822135
X	0.01213181134510885	8197	8.197	0.11396117132187905
X	0.012137758814032054	131026	131.026	0.04524740464952358
X	0.012129205071569038	30921	30.921	0.07320256002114765
X	0.012057427692649067	20715	20.715	0.08349425158276272
X	0.012160898145883201	43942	43.942	0.06516713234172343
X	0.012047881275627807	12027	12.027	0.10005783986746893
X	0.011857597337513001	1258	1.258	0.21123798645909558
X	0.01206593568358272	2179	2.179	0.17691633146156416
X	0.011962484716357635	8362	8.362	0.11267746755124966
X	0.012146075208136918	63644	63.644	0.0575739685581667
X	0.01472096204350618	218007	218.007	0.04072132662836027
X	0.014119370099229969	30600	30.6	0.07727362723431888
X	0.012139922814299398	5781	5.781	0.12805729349064568
X	0.012139311944370975	9189	9.189	0.10972576229906685
X	0.014176796756694438	258897	258.897	0.03797376849346987
X	0.012128082612167254	8532	8.532	0.11243816948443858
X	0.01189943283866336	4639	4.639	0.1368886046297367
X	0.0134958226211416	27962	27.962	0.07844110628640823
X	0.012146236401708239	28690	28.69	0.07508797525698611
X	0.012073836413242889	4305	4.305	0.14102324720220533
X	0.012096387429986564	7191	7.191	0.11892928278076968
X	0.012128200938445018	11837	11.837	0.10081339616805068
time for making epsilon is 1.107351541519165
epsilons are
[0.3057481260827967, 0.10338556590366824, 0.2265210631680091, 0.19523442448099482, 0.06424326303927669, 0.07930452716647814, 0.07112485501356258, 0.07678221419382564, 0.1260069630546651, 0.0850353156058237, 0.10947766052966294, 0.07494142680437813, 0.14731623925655302, 0.047862027005476704, 0.10043097050360643, 0.08833983624875591, 0.07768543836561227, 0.07381111640696414, 0.05238125025683698, 0.054016580171933254, 0.09157073734400517, 0.05297801784464129, 0.12331613427152507, 0.12173300524071495, 0.0907760922752768, 0.073831454994084, 0.06555754556382884, 0.11203993095584562, 0.051907163968054795, 0.027107061305246066, 0.1448063151904433, 0.03149577162158259, 0.14614782374777613, 0.1036467486025341, 0.08408815111228064, 0.03942251932864651, 0.07104126008061933, 0.24272027342754773, 0.1891914007980019, 0.1954411924883102, 0.20767031537669817, 0.36901747534016494, 0.30369120521335313, 0.21172112258230505, 0.3219728298234877, 0.2844639141039399, 0.2239140087586909, 0.14792333491878226, 0.17721109729076764, 0.17398395447019915, 0.1266848930804096, 0.2597893036230896, 0.15517770867041436, 0.1699236869445297, 0.16570333572186882, 0.22471684520091936, 0.24007712616143545, 0.17783435947658013, 0.2238784423046731, 0.20297791118108152, 0.16042778178633751, 0.259019933655339, 0.17366617347283506, 0.12370295644160956, 0.17035008978114857, 0.19506914732101524, 0.21765529242414028, 0.23342983411499782, 0.18341337693536455, 0.18800122045353815, 0.1843096919713734, 0.1866138418942375, 0.23508126597517792, 0.22264522797937158, 0.1638890524181056, 0.13087096821021182, 0.32500662367780375, 0.2617615017690476, 0.1913681877802494, 0.14489019839405973, 0.21595495209569487, 0.282173258206772, 0.22218929548868485, 0.26843490687719557, 0.25116871980754596, 0.2441712021966055, 0.2576286502094763, 0.2897607772073908, 0.19924111088494664, 0.17096795947041993, 0.17509303706419244, 0.24797235272494958, 0.23740940173243466, 0.13931201811201557, 0.1650206100583659, 0.16063740438376337, 0.19853057650981557, 0.19438736835591502, 0.1272939759001474, 0.16422818981487153, 0.13729852494244701, 0.1641377543475904, 0.1770669487446762, 0.25709978173059755, 0.14371762563012092, 0.2452016429410215, 0.19184940584377666, 0.25699332172347505, 0.23246862336474364, 0.3059510548826804, 0.17160037898581584, 0.30180043904489406, 0.2258692865249098, 0.19992092333538705, 0.15409192311243977, 0.23788877078231088, 0.2558454388733361, 0.3271292936159042, 0.1347291143393812, 0.27698294591797606, 0.13713934261610455, 0.24496862157655724, 0.1985405927606766, 0.1585318893448236, 0.18867838331145292, 0.20379895905303083, 0.24553363776114448, 0.17882453814605181, 0.13874289717503746, 0.20715522197794417, 0.12134901204596434, 0.30458219339801673, 0.20653212851526795, 0.32626014789516533, 0.2823836749015143, 0.18452516802244956, 0.2546795691194683, 0.23639545867987236, 0.2096455503893549, 0.2125276336833229, 0.20657720414216382, 0.23783659231318616, 0.16439448189158007, 0.1300235658758123, 0.16162696259562495, 0.23906521507495132, 0.2377794710536388, 0.26482781974465625, 0.22100159455414434, 0.23003922928129367, 0.21838653904547703, 0.19674233138184877, 0.18282238159854242, 0.2210130733555611, 0.19072881721345927, 0.12161629006911197, 0.20875302761980538, 0.23502884739192884, 0.21755344339847224, 0.39139080105244195, 0.1361333576167909, 0.30593984410737307, 0.15078087860162198, 0.3163982446185003, 0.18401676032763473, 0.1739276772143566, 0.2802945780810757, 0.22529593045554594, 0.19642262550238135, 0.3159597641905941, 0.292219860584432, 0.2504201381062965, 0.17961828141266198, 0.25758317301246636, 0.1680654888652842, 0.20837725693684692, 0.2195681602240386, 0.1872712464657546, 0.19764245100953293, 0.24423285634927666, 0.14302196162134548, 0.2611233039639142, 0.2236876472041932, 0.22810551619099037, 0.1699935216567683, 0.16087949335590615, 0.21854745998887146, 0.1969373844145892, 0.17501573767591505, 0.13147723082238735, 0.22195057066502483, 0.28491944224434296, 0.217125498722881, 0.23280859673706158, 0.22228740874329259, 0.22448077302234942, 0.18871336675780234, 0.2656111625738004, 0.18914563258328582, 0.3090608033356274, 0.14000674689028345, 0.20481559758580145, 0.11818159445610894, 0.22685636426004224, 0.15077418453329747, 0.26847023762533045, 0.2330890708228472, 0.18417330966250206, 0.24073164923418297, 0.1697075502176038, 0.1366935905888707, 0.23953724142623412, 0.2366659605847499, 0.16708640832949007, 0.3028432975520916, 0.33403711024635696, 0.37587581831583855, 0.272494435269593, 0.1253677987970504, 0.24144180267220805, 0.2706181324351566, 0.30306303331355533, 0.13760050537896526, 0.27594007487390376, 0.21333836012795443, 0.1741915232486226, 0.19950172463422372, 0.24241783876397824, 0.16525976701718514, 0.2671157404261579, 0.2525911129894381, 0.25858245174328126, 0.22904658529059355, 0.1934705291620534, 0.1837952959983361, 0.22245721521445252, 0.17495557419685168, 0.20630783264369512, 0.28270961027248587, 0.1465358270097199, 0.15726908386285007, 0.13382391714072228, 0.17589074129017368, 0.19738650828734933, 0.14198994207195983, 0.12176766360043789, 0.12446944466549623, 0.20825633544459565, 0.05627851467996892, 0.06956320617803903, 0.10874694586491912, 0.07547557514110444, 0.22051082471768876, 0.13619348848679294, 0.0425156095141513, 0.04351844849694589, 0.20770436759121697, 0.09025266862612091, 0.06219219176661485, 0.10052626611588324, 0.11057498535091163, 0.09322735576175784, 0.1354325114610629, 0.1346843863744633, 0.14347321846416225, 0.09311776265287504, 0.14223714684081004, 0.16833281267077868, 0.10182972216319705, 0.11834670447014603, 0.049143230385864, 0.07397934540470211, 0.18649872170555643, 0.09908051806396533, 0.15687946388598933, 0.1243588351159587, 0.07624174912951641, 0.06729888415505898, 0.05513947595281942, 0.2879947057639736, 0.18352401625784337, 0.07173724431264838, 0.07381638916781569, 0.0918234477351677, 0.19495237020743386, 0.048598507424601096, 0.13636157922416622, 0.10834515503037173, 0.0677884383297724, 0.24518730123578777, 0.08136248050092225, 0.07475205338611693, 0.04569110004921651, 0.09181091809922981, 0.09028012766594477, 0.13954098895813183, 0.15780760473320765, 0.11376114153114265, 0.13358070194508265, 0.12792773703284277, 0.07286034003132952, 0.10713420572952813, 0.1502195068147832, 0.16775697372706377, 0.04051112990076962, 0.14511239230831816, 0.2327457871208613, 0.12043615410129331, 0.05726592374335244, 0.10192411744493339, 0.11447954077926982, 0.17309858391976332, 0.11445477033738607, 0.12205024328634553, 0.03584093185312697, 0.08820063176624467, 0.1330340229175348, 0.060479975703266936, 0.059488925694286, 0.15079773371622462, 0.06351191390136558, 0.058764684976757006, 0.03908311954604262, 0.1318116396387151, 0.24627031928440693, 0.14919012720792274, 0.11067030872598396, 0.08720357834397834, 0.11342082726456837, 0.2784740437852244, 0.09796487653137913, 0.06117843670044527, 0.058831341485244604, 0.184587248054082, 0.05744834263404888, 0.13549819184557146, 0.11500591745544991, 0.12308612467126112, 0.20030225175202968, 0.052337256946233916, 0.08515222702276487, 0.12972733785441723, 0.10509250936017749, 0.18055311968872906, 0.0425767253896278, 0.11339342079160002, 0.0503974040859272, 0.22522198581833838, 0.19269473758572075, 0.18190619715995507, 0.10362549759955346, 0.16418400153767443, 0.14248359407642877, 0.148409332499222, 0.1827112111119208, 0.04769638943608127, 0.10374955469243456, 0.10492112590927467, 0.06725477666380289, 0.04477292583471986, 0.13829956612741215, 0.13883386262998884, 0.1753756888247152, 0.27068442547663657, 0.14160244912540446, 0.1055828961099857, 0.15310847899424876, 0.17629715082688122, 0.11146026320998224, 0.12583285036717765, 0.029948091644777155, 0.20508490693828615, 0.08367153751476211, 0.11491559838628511, 0.22954199773465317, 0.05986597233067144, 0.049002703953627255, 0.036615951980981776, 0.08399967181046669, 0.10275697058374979, 0.20603629852229646, 0.07091540460338705, 0.03210338377782918, 0.08287576778256715, 0.11121272373490554, 0.07056766176384643, 0.04700560247021077, 0.07865181056059001, 0.1345726668615664, 0.1487311071645438, 0.10034927288469625, 0.12400653786226878, 0.04025598454377937, 0.11791790594779382, 0.20798637176758386, 0.10962268437463354, 0.0910559224812208, 0.2448948886393646, 0.07229342208661273, 0.0736606647844959, 0.08753095443020832, 0.06957435916242209, 0.07040635192880446, 0.10820707092418481, 0.0871788239396289, 0.0737663421552007, 0.05302138220721987, 0.1569367070667498, 0.06533962545333084, 0.06613104734188016, 0.07759251666761681, 0.1058995365339171, 0.18363165451254343, 0.1315207279255804, 0.1381489506966702, 0.06283643641962297, 0.05006220140791705, 0.060133208586587855, 0.05144365300370132, 0.07398791568807386, 0.06233443532558696, 0.12446720110365271, 0.050697967068173505, 0.23890077359969442, 0.12186467673234891, 0.04506927518615787, 0.09266300371493755, 0.2130132512920647, 0.09869596753211231, 0.05931667229476824, 0.08254480562243358, 0.08216487203854825, 0.24588580173014665, 0.17381616093756752, 0.16894227385941613, 0.07557268736182823, 0.0877447726818147, 0.08683238018673357, 0.22827524581110947, 0.06684542477492554, 0.13427337595017683, 0.10198700141460405, 0.10755358991371047, 0.07320549739944221, 0.21607288327254487, 0.07891237802290423, 0.09322166194491906, 0.10534463139752909, 0.0912723902649487, 0.04673013067405017, 0.13268823660523504, 0.11915981162883413, 0.056431980164771925, 0.10825137650469607, 0.03986884665302763, 0.14047911723434103, 0.055970338998483464, 0.042246567409540366, 0.16400284623275013, 0.05943463101467165, 0.10784348817200184, 0.05777916675449571, 0.11470666970371594, 0.08691267500396821, 0.22692622922563951, 0.17317914000544568, 0.17009408078052807, 0.10268824266482492, 0.11543453688543202, 0.06009155503866971, 0.10734851677677033, 0.16904309276668122, 0.13270646598524036, 0.15984558200069743, 0.18208109426782773, 0.05678664834569289, 0.08315544534991469, 0.18910642926274918, 0.17781512254183324, 0.17105317781744683, 0.03715252045667743, 0.14226490625706703, 0.11567341963902136, 0.07105110836713026, 0.10410074534263973, 0.15567288151915828, 0.12446266587626591, 0.07350079424344008, 0.07378764790417174, 0.07603998384063236, 0.13094584664945802, 0.14894799248942692, 0.08228147171441148, 0.12469233607247476, 0.06634258961456545, 0.22261810778614946, 0.05758485714594332, 0.19950887914746093, 0.08306454384090363, 0.11603949212033085, 0.1964206557550769, 0.12341090938628758, 0.0770293334174334, 0.11359049609716028, 0.08618507433877409, 0.15602944176413833, 0.04743600870397581, 0.22894202116578913, 0.1938862363740358, 0.1408445288575417, 0.11216788536757007, 0.2091331056772084, 0.13674660301699215, 0.09269070355890684, 0.058513652976183946, 0.167838706314411, 0.2271485299939911, 0.167653910132656, 0.04628295387650102, 0.031749708869206944, 0.11260996200296458, 0.15574207639004595, 0.04839092110050505, 0.06127034750679069, 0.2069121505832178, 0.13621136727622216, 0.08455505262402407, 0.03943815894151474, 0.03808234369408141, 0.05571915237697821, 0.11710434017460526, 0.14433099000230148, 0.17652233734691594, 0.0709989942298641, 0.12663083881413736, 0.10891652429905398, 0.05731044731987649, 0.03836445537083739, 0.1623132895631049, 0.2324072588086034, 0.11567024624127255, 0.07203951247746053, 0.08191887679764817, 0.18868070494963712, 0.26978011700422616, 0.1076766523717451, 0.1882832791015339, 0.17059713714277933, 0.1866729312419944, 0.12717678623214485, 0.12538212852454012, 0.15416031973054367, 0.18725896753436147, 0.055264586065563205, 0.04343369373480408, 0.07038677577769761, 0.08424776261638986, 0.11317921216329906, 0.1245101852919072, 0.10742226520447866, 0.057585982177229725, 0.1278856160886542, 0.09613955470437058, 0.1139527634274673, 0.034539094982585006, 0.10455453882741528, 0.07236138242895317, 0.05902297983063241, 0.0684295952631329, 0.09716002589989878, 0.04737888009129341, 0.06266969901790663, 0.10171889823984473, 0.14308757976649591, 0.09137715374061585, 0.13277419655472797, 0.06736523060139561, 0.043500182106450166, 0.07114067286619065, 0.06572937692741604, 0.1178359055442077, 0.16363054841828506, 0.09132173612511242, 0.12730069387311463, 0.052848482994557626, 0.04258592164022804, 0.23909993148397535, 0.031840839047039333, 0.11044129780180212, 0.07608114686499655, 0.1388690538851931, 0.08616143648341552, 0.053516084485339524, 0.2226325458849806, 0.033942622407118296, 0.10567979747683914, 0.09327572436937635, 0.11835071376825443, 0.1252284591420004, 0.20679724942713534, 0.1162844704945443, 0.1385889897484136, 0.09115298383174951, 0.09837315431804777, 0.09767469155790318, 0.04600606483989872, 0.17193235887994981, 0.10026244950138075, 0.17658002071306794, 0.16107358336829436, 0.0604683779730844, 0.028039427871779193, 0.09568959187342732, 0.051910201481709696, 0.14579025696093512, 0.0579358624468578, 0.05474349696572355, 0.09181153925675076, 0.17416964068626467, 0.05866723810277233, 0.11462905720404826, 0.06622276794863444, 0.047643830063250084, 0.12632042635542307, 0.101906540028894, 0.10673488546923166, 0.05935330612257769, 0.05556826420546479, 0.08978818283483596, 0.1366932079780972, 0.04173287133618223, 0.06756258032883596, 0.04394178590173102, 0.06614667976467319, 0.0627417145039278, 0.10775692265684139, 0.05484429992815423, 0.04460116045023555, 0.0692490149862986, 0.1453177760642557, 0.11327590772012114, 0.12258428137278804, 0.07152026062990322, 0.07762950407725995, 0.14044412751795918, 0.10348796840083896, 0.05860258825228276, 0.07636293828180617, 0.08868543424288493, 0.08263098754783003, 0.09729850521029258, 0.10378750545507452, 0.17967227483953732, 0.13618532599034577, 0.05311820780141728, 0.12185086465429068, 0.07004692170816007, 0.21738813207549676, 0.20451256677436877, 0.05640899967288029, 0.06279714546341748, 0.0891047318558779, 0.16888133486442866, 0.18179986654171484, 0.10227050219630238, 0.10734721560667713, 0.06729272165623071, 0.16109003015340242, 0.11542414878059608, 0.1022106568180708, 0.11123114101428917, 0.056490370684237526, 0.1429488897219838, 0.07290519886579423, 0.0472388374046131, 0.08209607762797401, 0.08043991182463768, 0.060298630843294565, 0.22797735081418596, 0.0925550414822135, 0.11396117132187905, 0.04524740464952358, 0.07320256002114765, 0.08349425158276272, 0.06516713234172343, 0.10005783986746893, 0.21123798645909558, 0.17691633146156416, 0.11267746755124966, 0.0575739685581667, 0.04072132662836027, 0.07727362723431888, 0.12805729349064568, 0.10972576229906685, 0.03797376849346987, 0.11243816948443858, 0.1368886046297367, 0.07844110628640823, 0.07508797525698611, 0.14102324720220533, 0.11892928278076968, 0.10081339616805068]
0.10081516905268162
Making ranges
torch.Size([21743, 2])
We keep 4.50e+06/2.16e+08 =  2% of the original kernel matrix.

torch.Size([1031, 2])
We keep 1.97e+04/1.62e+05 = 12% of the original kernel matrix.

torch.Size([5490, 2])
We keep 3.65e+05/5.91e+06 =  6% of the original kernel matrix.

torch.Size([16096, 2])
We keep 3.60e+06/1.19e+08 =  3% of the original kernel matrix.

torch.Size([18077, 2])
We keep 3.95e+06/1.60e+08 =  2% of the original kernel matrix.

torch.Size([2211, 2])
We keep 9.67e+04/1.18e+06 =  8% of the original kernel matrix.

torch.Size([7443, 2])
We keep 7.24e+05/1.59e+07 =  4% of the original kernel matrix.

torch.Size([3214, 2])
We keep 1.80e+05/2.49e+06 =  7% of the original kernel matrix.

torch.Size([8373, 2])
We keep 9.14e+05/2.32e+07 =  3% of the original kernel matrix.

torch.Size([63030, 2])
We keep 8.80e+07/2.77e+09 =  3% of the original kernel matrix.

torch.Size([34968, 2])
We keep 1.49e+07/7.74e+08 =  1% of the original kernel matrix.

torch.Size([33082, 2])
We keep 1.40e+07/5.88e+08 =  2% of the original kernel matrix.

torch.Size([25609, 2])
We keep 7.53e+06/3.56e+08 =  2% of the original kernel matrix.

torch.Size([46310, 2])
We keep 2.37e+07/1.13e+09 =  2% of the original kernel matrix.

torch.Size([29953, 2])
We keep 9.91e+06/4.94e+08 =  2% of the original kernel matrix.

torch.Size([37187, 2])
We keep 1.52e+07/7.17e+08 =  2% of the original kernel matrix.

torch.Size([27283, 2])
We keep 8.21e+06/3.94e+08 =  2% of the original kernel matrix.

torch.Size([10377, 2])
We keep 1.86e+06/3.90e+07 =  4% of the original kernel matrix.

torch.Size([14392, 2])
We keep 2.47e+06/9.17e+07 =  2% of the original kernel matrix.

torch.Size([20131, 2])
We keep 5.96e+07/3.85e+08 = 15% of the original kernel matrix.

torch.Size([19675, 2])
We keep 6.03e+06/2.88e+08 =  2% of the original kernel matrix.

torch.Size([13110, 2])
We keep 3.13e+06/8.52e+07 =  3% of the original kernel matrix.

torch.Size([16185, 2])
We keep 3.47e+06/1.36e+08 =  2% of the original kernel matrix.

torch.Size([36039, 2])
We keep 2.80e+07/8.33e+08 =  3% of the original kernel matrix.

torch.Size([26278, 2])
We keep 8.80e+06/4.24e+08 =  2% of the original kernel matrix.

torch.Size([6851, 2])
We keep 7.57e+05/1.41e+07 =  5% of the original kernel matrix.

torch.Size([11572, 2])
We keep 1.74e+06/5.52e+07 =  3% of the original kernel matrix.

torch.Size([148371, 2])
We keep 2.26e+08/1.35e+10 =  1% of the original kernel matrix.

torch.Size([56024, 2])
We keep 2.89e+07/1.71e+09 =  1% of the original kernel matrix.

torch.Size([17623, 2])
We keep 4.23e+06/1.44e+08 =  2% of the original kernel matrix.

torch.Size([18265, 2])
We keep 4.07e+06/1.76e+08 =  2% of the original kernel matrix.

torch.Size([23485, 2])
We keep 8.14e+06/3.09e+08 =  2% of the original kernel matrix.

torch.Size([22095, 2])
We keep 5.74e+06/2.58e+08 =  2% of the original kernel matrix.

torch.Size([36589, 2])
We keep 1.49e+07/6.71e+08 =  2% of the original kernel matrix.

torch.Size([27228, 2])
We keep 7.90e+06/3.81e+08 =  2% of the original kernel matrix.

torch.Size([40067, 2])
We keep 2.43e+07/9.09e+08 =  2% of the original kernel matrix.

torch.Size([28046, 2])
We keep 9.08e+06/4.43e+08 =  2% of the original kernel matrix.

torch.Size([114942, 2])
We keep 1.61e+08/7.78e+09 =  2% of the original kernel matrix.

torch.Size([48534, 2])
We keep 2.30e+07/1.30e+09 =  1% of the original kernel matrix.

torch.Size([104634, 2])
We keep 1.07e+08/6.10e+09 =  1% of the original kernel matrix.

torch.Size([46069, 2])
We keep 2.04e+07/1.15e+09 =  1% of the original kernel matrix.

torch.Size([20637, 2])
We keep 9.13e+06/2.45e+08 =  3% of the original kernel matrix.

torch.Size([20360, 2])
We keep 5.34e+06/2.30e+08 =  2% of the original kernel matrix.

torch.Size([112106, 2])
We keep 1.40e+08/9.04e+09 =  1% of the original kernel matrix.

torch.Size([48409, 2])
We keep 2.42e+07/1.40e+09 =  1% of the original kernel matrix.

torch.Size([10599, 2])
We keep 1.86e+06/4.17e+07 =  4% of the original kernel matrix.

torch.Size([14482, 2])
We keep 2.64e+06/9.49e+07 =  2% of the original kernel matrix.

torch.Size([11152, 2])
We keep 1.74e+06/4.52e+07 =  3% of the original kernel matrix.

torch.Size([14962, 2])
We keep 2.65e+06/9.88e+07 =  2% of the original kernel matrix.

torch.Size([16745, 2])
We keep 1.40e+07/2.86e+08 =  4% of the original kernel matrix.

torch.Size([17581, 2])
We keep 5.59e+06/2.48e+08 =  2% of the original kernel matrix.

torch.Size([42081, 2])
We keep 2.51e+07/1.16e+09 =  2% of the original kernel matrix.

torch.Size([28893, 2])
We keep 1.01e+07/4.99e+08 =  2% of the original kernel matrix.

torch.Size([59615, 2])
We keep 5.33e+07/1.96e+09 =  2% of the original kernel matrix.

torch.Size([33942, 2])
We keep 1.27e+07/6.50e+08 =  1% of the original kernel matrix.

torch.Size([13367, 2])
We keep 2.80e+06/7.47e+07 =  3% of the original kernel matrix.

torch.Size([16396, 2])
We keep 3.27e+06/1.27e+08 =  2% of the original kernel matrix.

torch.Size([106515, 2])
We keep 1.49e+08/7.53e+09 =  1% of the original kernel matrix.

torch.Size([46347, 2])
We keep 2.22e+07/1.27e+09 =  1% of the original kernel matrix.

torch.Size([970622, 2])
We keep 5.13e+09/3.99e+11 =  1% of the original kernel matrix.

torch.Size([149223, 2])
We keep 1.35e+08/9.28e+09 =  1% of the original kernel matrix.

torch.Size([6460, 2])
We keep 1.12e+06/1.59e+07 =  7% of the original kernel matrix.

torch.Size([11300, 2])
We keep 1.73e+06/5.86e+07 =  2% of the original kernel matrix.

torch.Size([556810, 2])
We keep 3.11e+09/2.30e+11 =  1% of the original kernel matrix.

torch.Size([114222, 2])
We keep 1.06e+08/7.04e+09 =  1% of the original kernel matrix.

torch.Size([6715, 2])
We keep 8.51e+05/1.50e+07 =  5% of the original kernel matrix.

torch.Size([11447, 2])
We keep 1.79e+06/5.68e+07 =  3% of the original kernel matrix.

torch.Size([15677, 2])
We keep 4.50e+06/1.19e+08 =  3% of the original kernel matrix.

torch.Size([17783, 2])
We keep 3.93e+06/1.60e+08 =  2% of the original kernel matrix.

torch.Size([27179, 2])
We keep 1.37e+07/4.15e+08 =  3% of the original kernel matrix.

torch.Size([23355, 2])
We keep 6.60e+06/2.99e+08 =  2% of the original kernel matrix.

torch.Size([281625, 2])
We keep 5.58e+08/3.93e+10 =  1% of the original kernel matrix.

torch.Size([76949, 2])
We keep 4.65e+07/2.91e+09 =  1% of the original kernel matrix.

torch.Size([47239, 2])
We keep 2.88e+07/1.32e+09 =  2% of the original kernel matrix.

torch.Size([30566, 2])
We keep 1.07e+07/5.33e+08 =  1% of the original kernel matrix.

torch.Size([1905, 2])
We keep 7.14e+04/7.14e+05 = 10% of the original kernel matrix.

torch.Size([6857, 2])
We keep 6.11e+05/1.24e+07 =  4% of the original kernel matrix.

torch.Size([3452, 2])
We keep 2.19e+05/3.17e+06 =  6% of the original kernel matrix.

torch.Size([8526, 2])
We keep 1.02e+06/2.62e+07 =  3% of the original kernel matrix.

torch.Size([3396, 2])
We keep 1.90e+05/2.54e+06 =  7% of the original kernel matrix.

torch.Size([8708, 2])
We keep 9.39e+05/2.34e+07 =  4% of the original kernel matrix.

torch.Size([3010, 2])
We keep 1.74e+05/2.38e+06 =  7% of the original kernel matrix.

torch.Size([8273, 2])
We keep 9.35e+05/2.27e+07 =  4% of the original kernel matrix.

torch.Size([611, 2])
We keep 8.51e+03/5.11e+04 = 16% of the original kernel matrix.

torch.Size([4496, 2])
We keep 2.48e+05/3.32e+06 =  7% of the original kernel matrix.

torch.Size([1051, 2])
We keep 2.13e+04/1.64e+05 = 13% of the original kernel matrix.

torch.Size([5493, 2])
We keep 3.69e+05/5.95e+06 =  6% of the original kernel matrix.

torch.Size([2605, 2])
We keep 1.32e+05/1.57e+06 =  8% of the original kernel matrix.

torch.Size([7637, 2])
We keep 7.95e+05/1.84e+07 =  4% of the original kernel matrix.

torch.Size([1029, 2])
We keep 1.83e+04/1.30e+05 = 14% of the original kernel matrix.

torch.Size([5739, 2])
We keep 3.43e+05/5.29e+06 =  6% of the original kernel matrix.

torch.Size([1152, 2])
We keep 2.84e+04/2.63e+05 = 10% of the original kernel matrix.

torch.Size([5747, 2])
We keep 4.31e+05/7.54e+06 =  5% of the original kernel matrix.

torch.Size([2320, 2])
We keep 9.47e+04/1.16e+06 =  8% of the original kernel matrix.

torch.Size([7429, 2])
We keep 7.15e+05/1.58e+07 =  4% of the original kernel matrix.

torch.Size([6675, 2])
We keep 7.06e+05/1.37e+07 =  5% of the original kernel matrix.

torch.Size([11357, 2])
We keep 1.74e+06/5.44e+07 =  3% of the original kernel matrix.

torch.Size([4254, 2])
We keep 3.12e+05/4.61e+06 =  6% of the original kernel matrix.

torch.Size([9361, 2])
We keep 1.17e+06/3.16e+07 =  3% of the original kernel matrix.

torch.Size([4514, 2])
We keep 3.12e+05/5.07e+06 =  6% of the original kernel matrix.

torch.Size([9571, 2])
We keep 1.17e+06/3.31e+07 =  3% of the original kernel matrix.

torch.Size([9970, 2])
We keep 1.96e+06/4.86e+07 =  4% of the original kernel matrix.

torch.Size([14021, 2])
We keep 2.89e+06/1.02e+08 =  2% of the original kernel matrix.

torch.Size([1583, 2])
We keep 5.14e+04/4.60e+05 = 11% of the original kernel matrix.

torch.Size([6438, 2])
We keep 5.28e+05/9.96e+06 =  5% of the original kernel matrix.

torch.Size([5841, 2])
We keep 6.30e+05/1.04e+07 =  6% of the original kernel matrix.

torch.Size([10855, 2])
We keep 1.52e+06/4.73e+07 =  3% of the original kernel matrix.

torch.Size([4681, 2])
We keep 3.97e+05/6.11e+06 =  6% of the original kernel matrix.

torch.Size([9756, 2])
We keep 1.29e+06/3.63e+07 =  3% of the original kernel matrix.

torch.Size([5139, 2])
We keep 4.25e+05/7.06e+06 =  6% of the original kernel matrix.

torch.Size([10166, 2])
We keep 1.35e+06/3.90e+07 =  3% of the original kernel matrix.

torch.Size([2199, 2])
We keep 9.21e+04/1.11e+06 =  8% of the original kernel matrix.

torch.Size([7205, 2])
We keep 6.92e+05/1.55e+07 =  4% of the original kernel matrix.

torch.Size([1972, 2])
We keep 6.67e+04/7.55e+05 =  8% of the original kernel matrix.

torch.Size([7023, 2])
We keep 6.04e+05/1.28e+07 =  4% of the original kernel matrix.

torch.Size([4409, 2])
We keep 2.84e+05/4.59e+06 =  6% of the original kernel matrix.

torch.Size([9431, 2])
We keep 1.13e+06/3.15e+07 =  3% of the original kernel matrix.

torch.Size([2405, 2])
We keep 9.79e+04/1.14e+06 =  8% of the original kernel matrix.

torch.Size([7607, 2])
We keep 6.98e+05/1.57e+07 =  4% of the original kernel matrix.

torch.Size([2570, 2])
We keep 1.59e+05/1.91e+06 =  8% of the original kernel matrix.

torch.Size([7496, 2])
We keep 8.31e+05/2.03e+07 =  4% of the original kernel matrix.

torch.Size([5475, 2])
We keep 4.95e+05/8.51e+06 =  5% of the original kernel matrix.

torch.Size([10482, 2])
We keep 1.44e+06/4.29e+07 =  3% of the original kernel matrix.

torch.Size([1552, 2])
We keep 4.60e+04/4.84e+05 =  9% of the original kernel matrix.

torch.Size([6434, 2])
We keep 5.24e+05/1.02e+07 =  5% of the original kernel matrix.

torch.Size([4613, 2])
We keep 4.47e+05/7.17e+06 =  6% of the original kernel matrix.

torch.Size([9863, 2])
We keep 1.42e+06/3.94e+07 =  3% of the original kernel matrix.

torch.Size([10450, 2])
We keep 1.58e+06/4.07e+07 =  3% of the original kernel matrix.

torch.Size([14276, 2])
We keep 2.60e+06/9.38e+07 =  2% of the original kernel matrix.

torch.Size([4527, 2])
We keep 3.67e+05/5.90e+06 =  6% of the original kernel matrix.

torch.Size([9690, 2])
We keep 1.28e+06/3.57e+07 =  3% of the original kernel matrix.

torch.Size([3389, 2])
We keep 2.07e+05/2.64e+06 =  7% of the original kernel matrix.

torch.Size([8536, 2])
We keep 9.62e+05/2.39e+07 =  4% of the original kernel matrix.

torch.Size([2358, 2])
We keep 1.06e+05/1.32e+06 =  7% of the original kernel matrix.

torch.Size([7358, 2])
We keep 7.45e+05/1.69e+07 =  4% of the original kernel matrix.

torch.Size([1919, 2])
We keep 1.01e+05/8.84e+05 = 11% of the original kernel matrix.

torch.Size([6641, 2])
We keep 6.65e+05/1.38e+07 =  4% of the original kernel matrix.

torch.Size([4026, 2])
We keep 2.44e+05/3.85e+06 =  6% of the original kernel matrix.

torch.Size([9249, 2])
We keep 1.07e+06/2.88e+07 =  3% of the original kernel matrix.

torch.Size([3600, 2])
We keep 2.19e+05/3.25e+06 =  6% of the original kernel matrix.

torch.Size([8649, 2])
We keep 1.01e+06/2.65e+07 =  3% of the original kernel matrix.

torch.Size([3367, 2])
We keep 3.60e+05/3.63e+06 =  9% of the original kernel matrix.

torch.Size([8341, 2])
We keep 1.08e+06/2.80e+07 =  3% of the original kernel matrix.

torch.Size([3724, 2])
We keep 2.57e+05/3.42e+06 =  7% of the original kernel matrix.

torch.Size([8770, 2])
We keep 1.05e+06/2.72e+07 =  3% of the original kernel matrix.

torch.Size([2004, 2])
We keep 9.45e+04/8.61e+05 = 10% of the original kernel matrix.

torch.Size([6977, 2])
We keep 6.51e+05/1.36e+07 =  4% of the original kernel matrix.

torch.Size([2397, 2])
We keep 8.65e+04/1.09e+06 =  7% of the original kernel matrix.

torch.Size([7499, 2])
We keep 6.75e+05/1.53e+07 =  4% of the original kernel matrix.

torch.Size([5154, 2])
We keep 4.53e+05/7.54e+06 =  6% of the original kernel matrix.

torch.Size([10126, 2])
We keep 1.37e+06/4.04e+07 =  3% of the original kernel matrix.

torch.Size([8331, 2])
We keep 1.68e+06/2.81e+07 =  5% of the original kernel matrix.

torch.Size([12696, 2])
We keep 2.25e+06/7.79e+07 =  2% of the original kernel matrix.

torch.Size([984, 2])
We keep 1.57e+04/1.14e+05 = 13% of the original kernel matrix.

torch.Size([5546, 2])
We keep 3.30e+05/4.97e+06 =  6% of the original kernel matrix.

torch.Size([1641, 2])
We keep 4.40e+04/4.28e+05 = 10% of the original kernel matrix.

torch.Size([6596, 2])
We keep 5.12e+05/9.61e+06 =  5% of the original kernel matrix.

torch.Size([3576, 2])
We keep 2.14e+05/2.94e+06 =  7% of the original kernel matrix.

torch.Size([8816, 2])
We keep 9.81e+05/2.52e+07 =  3% of the original kernel matrix.

torch.Size([6948, 2])
We keep 1.00e+06/1.87e+07 =  5% of the original kernel matrix.

torch.Size([11633, 2])
We keep 2.00e+06/6.36e+07 =  3% of the original kernel matrix.

torch.Size([2569, 2])
We keep 1.10e+05/1.35e+06 =  8% of the original kernel matrix.

torch.Size([7607, 2])
We keep 7.47e+05/1.71e+07 =  4% of the original kernel matrix.

torch.Size([1323, 2])
We keep 3.00e+04/2.63e+05 = 11% of the original kernel matrix.

torch.Size([6017, 2])
We keep 4.24e+05/7.54e+06 =  5% of the original kernel matrix.

torch.Size([2403, 2])
We keep 1.37e+05/1.66e+06 =  8% of the original kernel matrix.

torch.Size([7712, 2])
We keep 8.51e+05/1.89e+07 =  4% of the original kernel matrix.

torch.Size([1410, 2])
We keep 4.87e+04/4.75e+05 = 10% of the original kernel matrix.

torch.Size([6236, 2])
We keep 5.51e+05/1.01e+07 =  5% of the original kernel matrix.

torch.Size([1730, 2])
We keep 5.25e+04/5.58e+05 =  9% of the original kernel matrix.

torch.Size([6548, 2])
We keep 5.49e+05/1.10e+07 =  5% of the original kernel matrix.

torch.Size([1959, 2])
We keep 6.53e+04/6.42e+05 = 10% of the original kernel matrix.

torch.Size([6934, 2])
We keep 5.80e+05/1.18e+07 =  4% of the original kernel matrix.

torch.Size([1637, 2])
We keep 4.76e+04/4.94e+05 =  9% of the original kernel matrix.

torch.Size([6477, 2])
We keep 5.32e+05/1.03e+07 =  5% of the original kernel matrix.

torch.Size([1100, 2])
We keep 2.72e+04/2.45e+05 = 11% of the original kernel matrix.

torch.Size([5604, 2])
We keep 4.22e+05/7.27e+06 =  5% of the original kernel matrix.

torch.Size([3215, 2])
We keep 1.90e+05/2.68e+06 =  7% of the original kernel matrix.

torch.Size([8498, 2])
We keep 9.60e+05/2.41e+07 =  3% of the original kernel matrix.

torch.Size([4676, 2])
We keep 3.76e+05/5.86e+06 =  6% of the original kernel matrix.

torch.Size([9817, 2])
We keep 1.27e+06/3.56e+07 =  3% of the original kernel matrix.

torch.Size([4467, 2])
We keep 3.45e+05/5.09e+06 =  6% of the original kernel matrix.

torch.Size([9498, 2])
We keep 1.20e+06/3.32e+07 =  3% of the original kernel matrix.

torch.Size([1758, 2])
We keep 5.70e+04/6.13e+05 =  9% of the original kernel matrix.

torch.Size([6632, 2])
We keep 5.69e+05/1.15e+07 =  4% of the original kernel matrix.

torch.Size([1941, 2])
We keep 7.22e+04/7.83e+05 =  9% of the original kernel matrix.

torch.Size([6916, 2])
We keep 6.18e+05/1.30e+07 =  4% of the original kernel matrix.

torch.Size([7680, 2])
We keep 9.53e+05/1.98e+07 =  4% of the original kernel matrix.

torch.Size([12202, 2])
We keep 1.97e+06/6.54e+07 =  3% of the original kernel matrix.

torch.Size([4991, 2])
We keep 4.06e+05/6.99e+06 =  5% of the original kernel matrix.

torch.Size([9893, 2])
We keep 1.33e+06/3.89e+07 =  3% of the original kernel matrix.

torch.Size([5426, 2])
We keep 4.92e+05/8.51e+06 =  5% of the original kernel matrix.

torch.Size([10395, 2])
We keep 1.45e+06/4.29e+07 =  3% of the original kernel matrix.

torch.Size([3087, 2])
We keep 1.72e+05/2.38e+06 =  7% of the original kernel matrix.

torch.Size([8301, 2])
We keep 9.17e+05/2.27e+07 =  4% of the original kernel matrix.

torch.Size([3197, 2])
We keep 1.93e+05/2.66e+06 =  7% of the original kernel matrix.

torch.Size([8316, 2])
We keep 9.41e+05/2.40e+07 =  3% of the original kernel matrix.

torch.Size([9857, 2])
We keep 1.38e+06/3.44e+07 =  4% of the original kernel matrix.

torch.Size([13829, 2])
We keep 2.43e+06/8.62e+07 =  2% of the original kernel matrix.

torch.Size([5352, 2])
We keep 4.21e+05/7.46e+06 =  5% of the original kernel matrix.

torch.Size([10377, 2])
We keep 1.36e+06/4.01e+07 =  3% of the original kernel matrix.

torch.Size([7749, 2])
We keep 1.08e+06/2.11e+07 =  5% of the original kernel matrix.

torch.Size([12117, 2])
We keep 2.03e+06/6.75e+07 =  3% of the original kernel matrix.

torch.Size([5373, 2])
We keep 4.33e+05/7.47e+06 =  5% of the original kernel matrix.

torch.Size([10323, 2])
We keep 1.36e+06/4.02e+07 =  3% of the original kernel matrix.

torch.Size([4260, 2])
We keep 3.02e+05/4.77e+06 =  6% of the original kernel matrix.

torch.Size([9281, 2])
We keep 1.17e+06/3.21e+07 =  3% of the original kernel matrix.

torch.Size([1506, 2])
We keep 5.18e+04/4.80e+05 = 10% of the original kernel matrix.

torch.Size([6231, 2])
We keep 5.31e+05/1.02e+07 =  5% of the original kernel matrix.

torch.Size([6883, 2])
We keep 8.55e+05/1.64e+07 =  5% of the original kernel matrix.

torch.Size([11504, 2])
We keep 1.85e+06/5.96e+07 =  3% of the original kernel matrix.

torch.Size([1782, 2])
We keep 5.75e+04/6.32e+05 =  9% of the original kernel matrix.

torch.Size([6734, 2])
We keep 5.77e+05/1.17e+07 =  4% of the original kernel matrix.

torch.Size([3484, 2])
We keep 2.37e+05/2.91e+06 =  8% of the original kernel matrix.

torch.Size([8652, 2])
We keep 9.91e+05/2.51e+07 =  3% of the original kernel matrix.

torch.Size([1557, 2])
We keep 5.21e+04/5.01e+05 = 10% of the original kernel matrix.

torch.Size([6379, 2])
We keep 5.26e+05/1.04e+07 =  5% of the original kernel matrix.

torch.Size([2110, 2])
We keep 8.14e+04/9.02e+05 =  9% of the original kernel matrix.

torch.Size([7096, 2])
We keep 6.55e+05/1.40e+07 =  4% of the original kernel matrix.

torch.Size([986, 2])
We keep 2.20e+04/1.62e+05 = 13% of the original kernel matrix.

torch.Size([5401, 2])
We keep 3.64e+05/5.91e+06 =  6% of the original kernel matrix.

torch.Size([4125, 2])
We keep 3.85e+05/5.42e+06 =  7% of the original kernel matrix.

torch.Size([9045, 2])
We keep 1.22e+06/3.42e+07 =  3% of the original kernel matrix.

torch.Size([1149, 2])
We keep 2.35e+04/1.88e+05 = 12% of the original kernel matrix.

torch.Size([5706, 2])
We keep 3.82e+05/6.38e+06 =  5% of the original kernel matrix.

torch.Size([2072, 2])
We keep 8.99e+04/1.10e+06 =  8% of the original kernel matrix.

torch.Size([7031, 2])
We keep 7.01e+05/1.54e+07 =  4% of the original kernel matrix.

torch.Size([3166, 2])
We keep 1.71e+05/2.29e+06 =  7% of the original kernel matrix.

torch.Size([8373, 2])
We keep 8.90e+05/2.22e+07 =  4% of the original kernel matrix.

torch.Size([5938, 2])
We keep 5.96e+05/1.08e+07 =  5% of the original kernel matrix.

torch.Size([10889, 2])
We keep 1.58e+06/4.83e+07 =  3% of the original kernel matrix.

torch.Size([1725, 2])
We keep 7.72e+04/7.96e+05 =  9% of the original kernel matrix.

torch.Size([6492, 2])
We keep 6.18e+05/1.31e+07 =  4% of the original kernel matrix.

torch.Size([1699, 2])
We keep 4.89e+04/4.97e+05 =  9% of the original kernel matrix.

torch.Size([6595, 2])
We keep 5.26e+05/1.04e+07 =  5% of the original kernel matrix.

torch.Size([869, 2])
We keep 1.64e+04/1.12e+05 = 14% of the original kernel matrix.

torch.Size([5174, 2])
We keep 3.28e+05/4.91e+06 =  6% of the original kernel matrix.

torch.Size([8324, 2])
We keep 1.23e+06/2.66e+07 =  4% of the original kernel matrix.

torch.Size([12788, 2])
We keep 2.22e+06/7.57e+07 =  2% of the original kernel matrix.

torch.Size([1358, 2])
We keep 3.42e+04/3.14e+05 = 10% of the original kernel matrix.

torch.Size([6028, 2])
We keep 4.63e+05/8.23e+06 =  5% of the original kernel matrix.

torch.Size([7997, 2])
We keep 1.33e+06/2.20e+07 =  6% of the original kernel matrix.

torch.Size([12445, 2])
We keep 2.07e+06/6.89e+07 =  3% of the original kernel matrix.

torch.Size([1760, 2])
We keep 6.38e+04/6.53e+05 =  9% of the original kernel matrix.

torch.Size([6559, 2])
We keep 5.85e+05/1.19e+07 =  4% of the original kernel matrix.

torch.Size([3026, 2])
We keep 1.78e+05/2.22e+06 =  8% of the original kernel matrix.

torch.Size([8145, 2])
We keep 8.94e+05/2.19e+07 =  4% of the original kernel matrix.

torch.Size([5639, 2])
We keep 5.22e+05/9.20e+06 =  5% of the original kernel matrix.

torch.Size([10511, 2])
We keep 1.48e+06/4.46e+07 =  3% of the original kernel matrix.

torch.Size([3498, 2])
We keep 2.27e+05/3.06e+06 =  7% of the original kernel matrix.

torch.Size([8592, 2])
We keep 9.94e+05/2.57e+07 =  3% of the original kernel matrix.

torch.Size([2942, 2])
We keep 1.46e+05/2.03e+06 =  7% of the original kernel matrix.

torch.Size([8069, 2])
We keep 8.49e+05/2.10e+07 =  4% of the original kernel matrix.

torch.Size([1910, 2])
We keep 6.61e+04/6.37e+05 = 10% of the original kernel matrix.

torch.Size([6889, 2])
We keep 5.84e+05/1.17e+07 =  4% of the original kernel matrix.

torch.Size([4402, 2])
We keep 3.05e+05/4.46e+06 =  6% of the original kernel matrix.

torch.Size([9485, 2])
We keep 1.15e+06/3.11e+07 =  3% of the original kernel matrix.

torch.Size([8284, 2])
We keep 9.82e+05/2.09e+07 =  4% of the original kernel matrix.

torch.Size([12794, 2])
We keep 2.00e+06/6.72e+07 =  2% of the original kernel matrix.

torch.Size([2734, 2])
We keep 1.27e+05/1.68e+06 =  7% of the original kernel matrix.

torch.Size([7846, 2])
We keep 8.04e+05/1.90e+07 =  4% of the original kernel matrix.

torch.Size([10889, 2])
We keep 1.73e+06/4.53e+07 =  3% of the original kernel matrix.

torch.Size([14687, 2])
We keep 2.70e+06/9.89e+07 =  2% of the original kernel matrix.

torch.Size([1128, 2])
We keep 2.15e+04/1.82e+05 = 11% of the original kernel matrix.

torch.Size([5817, 2])
We keep 3.82e+05/6.27e+06 =  6% of the original kernel matrix.

torch.Size([2921, 2])
We keep 1.67e+05/1.86e+06 =  8% of the original kernel matrix.

torch.Size([8159, 2])
We keep 8.45e+05/2.01e+07 =  4% of the original kernel matrix.

torch.Size([889, 2])
We keep 1.43e+04/1.03e+05 = 13% of the original kernel matrix.

torch.Size([5241, 2])
We keep 3.19e+05/4.72e+06 =  6% of the original kernel matrix.

torch.Size([1326, 2])
We keep 3.67e+04/2.91e+05 = 12% of the original kernel matrix.

torch.Size([6021, 2])
We keep 4.53e+05/7.92e+06 =  5% of the original kernel matrix.

torch.Size([3689, 2])
We keep 2.66e+05/3.53e+06 =  7% of the original kernel matrix.

torch.Size([8721, 2])
We keep 1.05e+06/2.76e+07 =  3% of the original kernel matrix.

torch.Size([1613, 2])
We keep 5.57e+04/5.33e+05 = 10% of the original kernel matrix.

torch.Size([6420, 2])
We keep 5.58e+05/1.07e+07 =  5% of the original kernel matrix.

torch.Size([1953, 2])
We keep 7.20e+04/8.15e+05 =  8% of the original kernel matrix.

torch.Size([6971, 2])
We keep 6.33e+05/1.33e+07 =  4% of the original kernel matrix.

torch.Size([2707, 2])
We keep 1.33e+05/1.71e+06 =  7% of the original kernel matrix.

torch.Size([7829, 2])
We keep 8.16e+05/1.92e+07 =  4% of the original kernel matrix.

torch.Size([2609, 2])
We keep 1.16e+05/1.47e+06 =  7% of the original kernel matrix.

torch.Size([7748, 2])
We keep 7.67e+05/1.78e+07 =  4% of the original kernel matrix.

torch.Size([2834, 2])
We keep 1.42e+05/1.85e+06 =  7% of the original kernel matrix.

torch.Size([7963, 2])
We keep 8.41e+05/2.00e+07 =  4% of the original kernel matrix.

torch.Size([1751, 2])
We keep 7.48e+04/7.36e+05 = 10% of the original kernel matrix.

torch.Size([6566, 2])
We keep 5.96e+05/1.26e+07 =  4% of the original kernel matrix.

torch.Size([5197, 2])
We keep 5.55e+05/9.64e+06 =  5% of the original kernel matrix.

torch.Size([10157, 2])
We keep 1.57e+06/4.56e+07 =  3% of the original kernel matrix.

torch.Size([9324, 2])
We keep 1.23e+06/3.02e+07 =  4% of the original kernel matrix.

torch.Size([13499, 2])
We keep 2.32e+06/8.07e+07 =  2% of the original kernel matrix.

torch.Size([5580, 2])
We keep 5.73e+05/1.01e+07 =  5% of the original kernel matrix.

torch.Size([10604, 2])
We keep 1.57e+06/4.67e+07 =  3% of the original kernel matrix.

torch.Size([1914, 2])
We keep 6.93e+04/7.69e+05 =  9% of the original kernel matrix.

torch.Size([6865, 2])
We keep 6.20e+05/1.29e+07 =  4% of the original kernel matrix.

torch.Size([1958, 2])
We keep 6.92e+04/8.05e+05 =  8% of the original kernel matrix.

torch.Size([6992, 2])
We keep 6.23e+05/1.32e+07 =  4% of the original kernel matrix.

torch.Size([1448, 2])
We keep 4.88e+04/4.12e+05 = 11% of the original kernel matrix.

torch.Size([6210, 2])
We keep 5.07e+05/9.43e+06 =  5% of the original kernel matrix.

torch.Size([2248, 2])
We keep 1.03e+05/1.12e+06 =  9% of the original kernel matrix.

torch.Size([7238, 2])
We keep 6.96e+05/1.56e+07 =  4% of the original kernel matrix.

torch.Size([2209, 2])
We keep 8.94e+04/9.92e+05 =  9% of the original kernel matrix.

torch.Size([7220, 2])
We keep 6.76e+05/1.46e+07 =  4% of the original kernel matrix.

torch.Size([2487, 2])
We keep 1.09e+05/1.33e+06 =  8% of the original kernel matrix.

torch.Size([7532, 2])
We keep 7.50e+05/1.69e+07 =  4% of the original kernel matrix.

torch.Size([3305, 2])
We keep 1.99e+05/2.63e+06 =  7% of the original kernel matrix.

torch.Size([8602, 2])
We keep 9.70e+05/2.38e+07 =  4% of the original kernel matrix.

torch.Size([3985, 2])
We keep 2.50e+05/3.90e+06 =  6% of the original kernel matrix.

torch.Size([9203, 2])
We keep 1.08e+06/2.90e+07 =  3% of the original kernel matrix.

torch.Size([2235, 2])
We keep 1.01e+05/1.26e+06 =  8% of the original kernel matrix.

torch.Size([7257, 2])
We keep 7.18e+05/1.65e+07 =  4% of the original kernel matrix.

torch.Size([3501, 2])
We keep 1.94e+05/2.91e+06 =  6% of the original kernel matrix.

torch.Size([8632, 2])
We keep 9.66e+05/2.51e+07 =  3% of the original kernel matrix.

torch.Size([11019, 2])
We keep 1.67e+06/4.52e+07 =  3% of the original kernel matrix.

torch.Size([14730, 2])
We keep 2.68e+06/9.88e+07 =  2% of the original kernel matrix.

torch.Size([2764, 2])
We keep 1.63e+05/2.11e+06 =  7% of the original kernel matrix.

torch.Size([7968, 2])
We keep 9.07e+05/2.13e+07 =  4% of the original kernel matrix.

torch.Size([2051, 2])
We keep 7.60e+04/8.57e+05 =  8% of the original kernel matrix.

torch.Size([7113, 2])
We keep 6.49e+05/1.36e+07 =  4% of the original kernel matrix.

torch.Size([2637, 2])
We keep 1.16e+05/1.36e+06 =  8% of the original kernel matrix.

torch.Size([7800, 2])
We keep 7.54e+05/1.72e+07 =  4% of the original kernel matrix.

torch.Size([497, 2])
We keep 6.33e+03/3.42e+04 = 18% of the original kernel matrix.

torch.Size([4219, 2])
We keep 2.21e+05/2.72e+06 =  8% of the original kernel matrix.

torch.Size([8672, 2])
We keep 1.50e+06/3.25e+07 =  4% of the original kernel matrix.

torch.Size([13330, 2])
We keep 2.46e+06/8.38e+07 =  2% of the original kernel matrix.

torch.Size([999, 2])
We keep 2.22e+04/1.74e+05 = 12% of the original kernel matrix.

torch.Size([5542, 2])
We keep 3.80e+05/6.13e+06 =  6% of the original kernel matrix.

torch.Size([6314, 2])
We keep 6.90e+05/1.25e+07 =  5% of the original kernel matrix.

torch.Size([11089, 2])
We keep 1.66e+06/5.19e+07 =  3% of the original kernel matrix.

torch.Size([924, 2])
We keep 1.83e+04/1.29e+05 = 14% of the original kernel matrix.

torch.Size([5325, 2])
We keep 3.47e+05/5.28e+06 =  6% of the original kernel matrix.

torch.Size([3598, 2])
We keep 2.63e+05/3.71e+06 =  7% of the original kernel matrix.

torch.Size([8677, 2])
We keep 1.08e+06/2.83e+07 =  3% of the original kernel matrix.

torch.Size([4334, 2])
We keep 3.50e+05/5.29e+06 =  6% of the original kernel matrix.

torch.Size([9429, 2])
We keep 1.23e+06/3.38e+07 =  3% of the original kernel matrix.

torch.Size([1253, 2])
We keep 3.56e+04/2.99e+05 = 11% of the original kernel matrix.

torch.Size([5947, 2])
We keep 4.51e+05/8.04e+06 =  5% of the original kernel matrix.

torch.Size([2108, 2])
We keep 9.56e+04/1.01e+06 =  9% of the original kernel matrix.

torch.Size([7027, 2])
We keep 6.69e+05/1.47e+07 =  4% of the original kernel matrix.

torch.Size([3278, 2])
We keep 1.87e+05/2.51e+06 =  7% of the original kernel matrix.

torch.Size([8542, 2])
We keep 9.38e+05/2.33e+07 =  4% of the original kernel matrix.

torch.Size([941, 2])
We keep 1.72e+04/1.33e+05 = 12% of the original kernel matrix.

torch.Size([5391, 2])
We keep 3.47e+05/5.36e+06 =  6% of the original kernel matrix.

torch.Size([1102, 2])
We keep 2.64e+04/2.32e+05 = 11% of the original kernel matrix.

torch.Size([5566, 2])
We keep 4.17e+05/7.08e+06 =  5% of the original kernel matrix.

torch.Size([1618, 2])
We keep 5.10e+04/5.39e+05 =  9% of the original kernel matrix.

torch.Size([6404, 2])
We keep 5.47e+05/1.08e+07 =  5% of the original kernel matrix.

torch.Size([4288, 2])
We keep 2.89e+05/4.35e+06 =  6% of the original kernel matrix.

torch.Size([9435, 2])
We keep 1.13e+06/3.07e+07 =  3% of the original kernel matrix.

torch.Size([1626, 2])
We keep 5.22e+04/4.96e+05 = 10% of the original kernel matrix.

torch.Size([6472, 2])
We keep 5.33e+05/1.03e+07 =  5% of the original kernel matrix.

torch.Size([4819, 2])
We keep 3.98e+05/6.52e+06 =  6% of the original kernel matrix.

torch.Size([9935, 2])
We keep 1.31e+06/3.75e+07 =  3% of the original kernel matrix.

torch.Size([2784, 2])
We keep 1.31e+05/1.79e+06 =  7% of the original kernel matrix.

torch.Size([8061, 2])
We keep 8.15e+05/1.96e+07 =  4% of the original kernel matrix.

torch.Size([2468, 2])
We keep 1.10e+05/1.30e+06 =  8% of the original kernel matrix.

torch.Size([7567, 2])
We keep 7.50e+05/1.67e+07 =  4% of the original kernel matrix.

torch.Size([3621, 2])
We keep 3.25e+05/4.08e+06 =  7% of the original kernel matrix.

torch.Size([8698, 2])
We keep 1.17e+06/2.97e+07 =  3% of the original kernel matrix.

torch.Size([3279, 2])
We keep 1.90e+05/2.46e+06 =  7% of the original kernel matrix.

torch.Size([8456, 2])
We keep 9.26e+05/2.31e+07 =  4% of the original kernel matrix.

torch.Size([1789, 2])
We keep 6.16e+04/6.66e+05 =  9% of the original kernel matrix.

torch.Size([6663, 2])
We keep 5.89e+05/1.20e+07 =  4% of the original kernel matrix.

torch.Size([7168, 2])
We keep 8.38e+05/1.70e+07 =  4% of the original kernel matrix.

torch.Size([11858, 2])
We keep 1.85e+06/6.07e+07 =  3% of the original kernel matrix.

torch.Size([1525, 2])
We keep 4.48e+04/4.36e+05 = 10% of the original kernel matrix.

torch.Size([6309, 2])
We keep 5.15e+05/9.70e+06 =  5% of the original kernel matrix.

torch.Size([2290, 2])
We keep 9.61e+04/1.16e+06 =  8% of the original kernel matrix.

torch.Size([7361, 2])
We keep 7.13e+05/1.58e+07 =  4% of the original kernel matrix.

torch.Size([2294, 2])
We keep 8.74e+04/9.92e+05 =  8% of the original kernel matrix.

torch.Size([7392, 2])
We keep 6.64e+05/1.46e+07 =  4% of the original kernel matrix.

torch.Size([4892, 2])
We keep 4.19e+05/6.60e+06 =  6% of the original kernel matrix.

torch.Size([9910, 2])
We keep 1.34e+06/3.78e+07 =  3% of the original kernel matrix.

torch.Size([5653, 2])
We keep 4.83e+05/8.44e+06 =  5% of the original kernel matrix.

torch.Size([10670, 2])
We keep 1.45e+06/4.27e+07 =  3% of the original kernel matrix.

torch.Size([2491, 2])
We keep 1.03e+05/1.27e+06 =  8% of the original kernel matrix.

torch.Size([7628, 2])
We keep 7.28e+05/1.65e+07 =  4% of the original kernel matrix.

torch.Size([3226, 2])
We keep 1.83e+05/2.50e+06 =  7% of the original kernel matrix.

torch.Size([8423, 2])
We keep 9.29e+05/2.32e+07 =  4% of the original kernel matrix.

torch.Size([4647, 2])
We keep 4.40e+05/6.95e+06 =  6% of the original kernel matrix.

torch.Size([9825, 2])
We keep 1.39e+06/3.88e+07 =  3% of the original kernel matrix.

torch.Size([8869, 2])
We keep 1.23e+06/2.82e+07 =  4% of the original kernel matrix.

torch.Size([13044, 2])
We keep 2.25e+06/7.81e+07 =  2% of the original kernel matrix.

torch.Size([2352, 2])
We keep 9.63e+04/1.21e+06 =  7% of the original kernel matrix.

torch.Size([7459, 2])
We keep 7.28e+05/1.61e+07 =  4% of the original kernel matrix.

torch.Size([1191, 2])
We keep 3.11e+04/2.66e+05 = 11% of the original kernel matrix.

torch.Size([5770, 2])
We keep 4.44e+05/7.58e+06 =  5% of the original kernel matrix.

torch.Size([2523, 2])
We keep 1.07e+05/1.30e+06 =  8% of the original kernel matrix.

torch.Size([7571, 2])
We keep 7.33e+05/1.68e+07 =  4% of the original kernel matrix.

torch.Size([1999, 2])
We keep 7.59e+04/9.12e+05 =  8% of the original kernel matrix.

torch.Size([7055, 2])
We keep 6.52e+05/1.40e+07 =  4% of the original kernel matrix.

torch.Size([2320, 2])
We keep 9.54e+04/1.16e+06 =  8% of the original kernel matrix.

torch.Size([7323, 2])
We keep 6.84e+05/1.58e+07 =  4% of the original kernel matrix.

torch.Size([2198, 2])
We keep 9.55e+04/1.13e+06 =  8% of the original kernel matrix.

torch.Size([7095, 2])
We keep 7.00e+05/1.57e+07 =  4% of the original kernel matrix.

torch.Size([3498, 2])
We keep 2.18e+05/3.23e+06 =  6% of the original kernel matrix.

torch.Size([8611, 2])
We keep 1.01e+06/2.64e+07 =  3% of the original kernel matrix.

torch.Size([1503, 2])
We keep 4.16e+04/3.98e+05 = 10% of the original kernel matrix.

torch.Size([6365, 2])
We keep 4.91e+05/9.27e+06 =  5% of the original kernel matrix.

torch.Size([3361, 2])
We keep 2.27e+05/3.19e+06 =  7% of the original kernel matrix.

torch.Size([8450, 2])
We keep 1.00e+06/2.62e+07 =  3% of the original kernel matrix.

torch.Size([1075, 2])
We keep 2.14e+04/1.63e+05 = 13% of the original kernel matrix.

torch.Size([5652, 2])
We keep 3.65e+05/5.94e+06 =  6% of the original kernel matrix.

torch.Size([7824, 2])
We keep 9.77e+05/1.96e+07 =  4% of the original kernel matrix.

torch.Size([12339, 2])
We keep 1.95e+06/6.50e+07 =  3% of the original kernel matrix.

torch.Size([3131, 2])
We keep 1.73e+05/1.97e+06 =  8% of the original kernel matrix.

torch.Size([8321, 2])
We keep 8.66e+05/2.06e+07 =  4% of the original kernel matrix.

torch.Size([12069, 2])
We keep 1.88e+06/5.41e+07 =  3% of the original kernel matrix.

torch.Size([15534, 2])
We keep 2.88e+06/1.08e+08 =  2% of the original kernel matrix.

torch.Size([2241, 2])
We keep 9.19e+04/1.08e+06 =  8% of the original kernel matrix.

torch.Size([7270, 2])
We keep 7.02e+05/1.53e+07 =  4% of the original kernel matrix.

torch.Size([6303, 2])
We keep 6.63e+05/1.23e+07 =  5% of the original kernel matrix.

torch.Size([11046, 2])
We keep 1.63e+06/5.16e+07 =  3% of the original kernel matrix.

torch.Size([1406, 2])
We keep 3.84e+04/3.73e+05 = 10% of the original kernel matrix.

torch.Size([6128, 2])
We keep 4.82e+05/8.98e+06 =  5% of the original kernel matrix.

torch.Size([2100, 2])
We keep 1.00e+05/1.15e+06 =  8% of the original kernel matrix.

torch.Size([7455, 2])
We keep 7.29e+05/1.58e+07 =  4% of the original kernel matrix.

torch.Size([3818, 2])
We keep 2.44e+05/3.76e+06 =  6% of the original kernel matrix.

torch.Size([8908, 2])
We keep 1.07e+06/2.85e+07 =  3% of the original kernel matrix.

torch.Size([1849, 2])
We keep 6.25e+04/7.02e+05 =  8% of the original kernel matrix.

torch.Size([6732, 2])
We keep 5.94e+05/1.23e+07 =  4% of the original kernel matrix.

torch.Size([4976, 2])
We keep 5.06e+05/8.57e+06 =  5% of the original kernel matrix.

torch.Size([10161, 2])
We keep 1.52e+06/4.30e+07 =  3% of the original kernel matrix.

torch.Size([8054, 2])
We keep 9.71e+05/2.21e+07 =  4% of the original kernel matrix.

torch.Size([12477, 2])
We keep 2.04e+06/6.91e+07 =  2% of the original kernel matrix.

torch.Size([1845, 2])
We keep 7.78e+04/7.57e+05 = 10% of the original kernel matrix.

torch.Size([6680, 2])
We keep 6.16e+05/1.28e+07 =  4% of the original kernel matrix.

torch.Size([1998, 2])
We keep 7.13e+04/8.15e+05 =  8% of the original kernel matrix.

torch.Size([7017, 2])
We keep 6.26e+05/1.33e+07 =  4% of the original kernel matrix.

torch.Size([4933, 2])
We keep 3.77e+05/6.32e+06 =  5% of the original kernel matrix.

torch.Size([9905, 2])
We keep 1.27e+06/3.69e+07 =  3% of the original kernel matrix.

torch.Size([1074, 2])
We keep 2.52e+04/1.78e+05 = 14% of the original kernel matrix.

torch.Size([5652, 2])
We keep 3.86e+05/6.20e+06 =  6% of the original kernel matrix.

torch.Size([803, 2])
We keep 1.51e+04/1.02e+05 = 14% of the original kernel matrix.

torch.Size([5016, 2])
We keep 3.23e+05/4.70e+06 =  6% of the original kernel matrix.

torch.Size([626, 2])
We keep 7.37e+03/4.58e+04 = 16% of the original kernel matrix.

torch.Size([4746, 2])
We keep 2.40e+05/3.14e+06 =  7% of the original kernel matrix.

torch.Size([1357, 2])
We keep 3.60e+04/3.29e+05 = 10% of the original kernel matrix.

torch.Size([6132, 2])
We keep 4.62e+05/8.43e+06 =  5% of the original kernel matrix.

torch.Size([8877, 2])
We keep 2.09e+06/3.62e+07 =  5% of the original kernel matrix.

torch.Size([13173, 2])
We keep 2.45e+06/8.84e+07 =  2% of the original kernel matrix.

torch.Size([1895, 2])
We keep 6.57e+04/7.38e+05 =  8% of the original kernel matrix.

torch.Size([7000, 2])
We keep 6.08e+05/1.26e+07 =  4% of the original kernel matrix.

torch.Size([1334, 2])
We keep 3.71e+04/3.55e+05 = 10% of the original kernel matrix.

torch.Size([6084, 2])
We keep 4.79e+05/8.76e+06 =  5% of the original kernel matrix.

torch.Size([1160, 2])
We keep 2.58e+04/1.88e+05 = 13% of the original kernel matrix.

torch.Size([5940, 2])
We keep 3.90e+05/6.38e+06 =  6% of the original kernel matrix.

torch.Size([7685, 2])
We keep 1.02e+06/2.16e+07 =  4% of the original kernel matrix.

torch.Size([12217, 2])
We keep 2.04e+06/6.83e+07 =  2% of the original kernel matrix.

torch.Size([1303, 2])
We keep 3.80e+04/3.23e+05 = 11% of the original kernel matrix.

torch.Size([5920, 2])
We keep 4.58e+05/8.35e+06 =  5% of the original kernel matrix.

torch.Size([2383, 2])
We keep 1.19e+05/1.52e+06 =  7% of the original kernel matrix.

torch.Size([7414, 2])
We keep 7.84e+05/1.81e+07 =  4% of the original kernel matrix.

torch.Size([4505, 2])
We keep 3.11e+05/5.10e+06 =  6% of the original kernel matrix.

torch.Size([9557, 2])
We keep 1.19e+06/3.32e+07 =  3% of the original kernel matrix.

torch.Size([3075, 2])
We keep 1.75e+05/2.18e+06 =  8% of the original kernel matrix.

torch.Size([8231, 2])
We keep 8.89e+05/2.17e+07 =  4% of the original kernel matrix.

torch.Size([1876, 2])
We keep 6.53e+04/6.81e+05 =  9% of the original kernel matrix.

torch.Size([6855, 2])
We keep 5.90e+05/1.21e+07 =  4% of the original kernel matrix.

torch.Size([5027, 2])
We keep 4.52e+05/7.13e+06 =  6% of the original kernel matrix.

torch.Size([9920, 2])
We keep 1.35e+06/3.93e+07 =  3% of the original kernel matrix.

torch.Size([1354, 2])
We keep 4.17e+04/3.81e+05 = 10% of the original kernel matrix.

torch.Size([6026, 2])
We keep 4.96e+05/9.07e+06 =  5% of the original kernel matrix.

torch.Size([1627, 2])
We keep 5.38e+04/5.61e+05 =  9% of the original kernel matrix.

torch.Size([6555, 2])
We keep 5.52e+05/1.10e+07 =  5% of the original kernel matrix.

torch.Size([1615, 2])
We keep 4.69e+04/4.87e+05 =  9% of the original kernel matrix.

torch.Size([6463, 2])
We keep 5.23e+05/1.03e+07 =  5% of the original kernel matrix.

torch.Size([2263, 2])
We keep 8.87e+04/9.45e+05 =  9% of the original kernel matrix.

torch.Size([7327, 2])
We keep 6.69e+05/1.43e+07 =  4% of the original kernel matrix.

torch.Size([3547, 2])
We keep 2.45e+05/3.55e+06 =  6% of the original kernel matrix.

torch.Size([8980, 2])
We keep 1.08e+06/2.77e+07 =  3% of the original kernel matrix.

torch.Size([3757, 2])
We keep 2.37e+05/3.55e+06 =  6% of the original kernel matrix.

torch.Size([8835, 2])
We keep 1.04e+06/2.77e+07 =  3% of the original kernel matrix.

torch.Size([2303, 2])
We keep 1.14e+05/1.17e+06 =  9% of the original kernel matrix.

torch.Size([7362, 2])
We keep 7.17e+05/1.59e+07 =  4% of the original kernel matrix.

torch.Size([4553, 2])
We keep 4.17e+05/6.77e+06 =  6% of the original kernel matrix.

torch.Size([9709, 2])
We keep 1.37e+06/3.82e+07 =  3% of the original kernel matrix.

torch.Size([3131, 2])
We keep 1.89e+05/2.51e+06 =  7% of the original kernel matrix.

torch.Size([8695, 2])
We keep 9.60e+05/2.33e+07 =  4% of the original kernel matrix.

torch.Size([1243, 2])
We keep 3.44e+04/3.10e+05 = 11% of the original kernel matrix.

torch.Size([5914, 2])
We keep 4.70e+05/8.19e+06 =  5% of the original kernel matrix.

torch.Size([6621, 2])
We keep 7.09e+05/1.48e+07 =  4% of the original kernel matrix.

torch.Size([11454, 2])
We keep 1.76e+06/5.65e+07 =  3% of the original kernel matrix.

torch.Size([5678, 2])
We keep 5.97e+05/9.75e+06 =  6% of the original kernel matrix.

torch.Size([10785, 2])
We keep 1.48e+06/4.59e+07 =  3% of the original kernel matrix.

torch.Size([8469, 2])
We keep 1.43e+06/2.57e+07 =  5% of the original kernel matrix.

torch.Size([13060, 2])
We keep 2.14e+06/7.45e+07 =  2% of the original kernel matrix.

torch.Size([4369, 2])
We keep 3.51e+05/4.94e+06 =  7% of the original kernel matrix.

torch.Size([9504, 2])
We keep 1.19e+06/3.27e+07 =  3% of the original kernel matrix.

torch.Size([2642, 2])
We keep 2.33e+05/2.25e+06 = 10% of the original kernel matrix.

torch.Size([7473, 2])
We keep 8.85e+05/2.20e+07 =  4% of the original kernel matrix.

torch.Size([7208, 2])
We keep 1.01e+06/1.78e+07 =  5% of the original kernel matrix.

torch.Size([11725, 2])
We keep 1.90e+06/6.20e+07 =  3% of the original kernel matrix.

torch.Size([10549, 2])
We keep 4.81e+06/6.19e+07 =  7% of the original kernel matrix.

torch.Size([14441, 2])
We keep 3.21e+06/1.16e+08 =  2% of the original kernel matrix.

torch.Size([9715, 2])
We keep 2.39e+06/3.89e+07 =  6% of the original kernel matrix.

torch.Size([13889, 2])
We keep 2.51e+06/9.17e+07 =  2% of the original kernel matrix.

torch.Size([2847, 2])
We keep 1.37e+05/1.80e+06 =  7% of the original kernel matrix.

torch.Size([8051, 2])
We keep 8.26e+05/1.97e+07 =  4% of the original kernel matrix.

torch.Size([90237, 2])
We keep 9.02e+07/4.65e+09 =  1% of the original kernel matrix.

torch.Size([42269, 2])
We keep 1.81e+07/1.00e+09 =  1% of the original kernel matrix.

torch.Size([43392, 2])
We keep 3.40e+07/1.30e+09 =  2% of the original kernel matrix.

torch.Size([28528, 2])
We keep 1.05e+07/5.29e+08 =  1% of the original kernel matrix.

torch.Size([13453, 2])
We keep 4.11e+06/8.83e+07 =  4% of the original kernel matrix.

torch.Size([16259, 2])
We keep 3.41e+06/1.38e+08 =  2% of the original kernel matrix.

torch.Size([36758, 2])
We keep 3.17e+07/7.97e+08 =  3% of the original kernel matrix.

torch.Size([27209, 2])
We keep 8.61e+06/4.15e+08 =  2% of the original kernel matrix.

torch.Size([2481, 2])
We keep 1.02e+05/1.26e+06 =  8% of the original kernel matrix.

torch.Size([7638, 2])
We keep 7.18e+05/1.65e+07 =  4% of the original kernel matrix.

torch.Size([7392, 2])
We keep 2.38e+06/2.27e+07 = 10% of the original kernel matrix.

torch.Size([11994, 2])
We keep 2.06e+06/6.99e+07 =  2% of the original kernel matrix.

torch.Size([214907, 2])
We keep 3.82e+08/2.49e+10 =  1% of the original kernel matrix.

torch.Size([67730, 2])
We keep 3.80e+07/2.32e+09 =  1% of the original kernel matrix.

torch.Size([202079, 2])
We keep 3.92e+08/2.17e+10 =  1% of the original kernel matrix.

torch.Size([65957, 2])
We keep 3.58e+07/2.17e+09 =  1% of the original kernel matrix.

torch.Size([2971, 2])
We keep 1.36e+05/1.77e+06 =  7% of the original kernel matrix.

torch.Size([8229, 2])
We keep 8.03e+05/1.96e+07 =  4% of the original kernel matrix.

torch.Size([21814, 2])
We keep 6.97e+06/2.72e+08 =  2% of the original kernel matrix.

torch.Size([21267, 2])
We keep 5.48e+06/2.42e+08 =  2% of the original kernel matrix.

torch.Size([66064, 2])
We keep 8.51e+07/2.52e+09 =  3% of the original kernel matrix.

torch.Size([35928, 2])
We keep 1.40e+07/7.37e+08 =  1% of the original kernel matrix.

torch.Size([16829, 2])
We keep 1.09e+07/1.50e+08 =  7% of the original kernel matrix.

torch.Size([17717, 2])
We keep 4.17e+06/1.80e+08 =  2% of the original kernel matrix.

torch.Size([12631, 2])
We keep 4.24e+06/7.97e+07 =  5% of the original kernel matrix.

torch.Size([15781, 2])
We keep 3.33e+06/1.31e+08 =  2% of the original kernel matrix.

torch.Size([18541, 2])
We keep 8.66e+06/2.19e+08 =  3% of the original kernel matrix.

torch.Size([18745, 2])
We keep 5.07e+06/2.17e+08 =  2% of the original kernel matrix.

torch.Size([7917, 2])
We keep 1.64e+06/2.38e+07 =  6% of the original kernel matrix.

torch.Size([12527, 2])
We keep 2.04e+06/7.17e+07 =  2% of the original kernel matrix.

torch.Size([8464, 2])
We keep 1.84e+06/2.46e+07 =  7% of the original kernel matrix.

torch.Size([12978, 2])
We keep 2.11e+06/7.28e+07 =  2% of the original kernel matrix.

torch.Size([6072, 2])
We keep 2.64e+06/1.64e+07 = 16% of the original kernel matrix.

torch.Size([10878, 2])
We keep 1.91e+06/5.96e+07 =  3% of the original kernel matrix.

torch.Size([18663, 2])
We keep 4.79e+07/2.28e+08 = 21% of the original kernel matrix.

torch.Size([19389, 2])
We keep 4.87e+06/2.22e+08 =  2% of the original kernel matrix.

torch.Size([6976, 2])
We keep 1.69e+06/1.73e+07 =  9% of the original kernel matrix.

torch.Size([11602, 2])
We keep 1.84e+06/6.11e+07 =  3% of the original kernel matrix.

torch.Size([4785, 2])
We keep 3.95e+05/6.45e+06 =  6% of the original kernel matrix.

torch.Size([9763, 2])
We keep 1.31e+06/3.73e+07 =  3% of the original kernel matrix.

torch.Size([15659, 2])
We keep 6.30e+06/1.32e+08 =  4% of the original kernel matrix.

torch.Size([17869, 2])
We keep 4.18e+06/1.69e+08 =  2% of the original kernel matrix.

torch.Size([10387, 2])
We keep 4.36e+06/5.23e+07 =  8% of the original kernel matrix.

torch.Size([14052, 2])
We keep 2.87e+06/1.06e+08 =  2% of the original kernel matrix.

torch.Size([136815, 2])
We keep 3.43e+08/1.53e+10 =  2% of the original kernel matrix.

torch.Size([53662, 2])
We keep 3.14e+07/1.82e+09 =  1% of the original kernel matrix.

torch.Size([41440, 2])
We keep 2.50e+07/8.96e+08 =  2% of the original kernel matrix.

torch.Size([28693, 2])
We keep 9.02e+06/4.40e+08 =  2% of the original kernel matrix.

torch.Size([3546, 2])
We keep 2.69e+05/3.29e+06 =  8% of the original kernel matrix.

torch.Size([8513, 2])
We keep 1.02e+06/2.67e+07 =  3% of the original kernel matrix.

torch.Size([17335, 2])
We keep 5.08e+06/1.73e+08 =  2% of the original kernel matrix.

torch.Size([18996, 2])
We keep 4.69e+06/1.93e+08 =  2% of the original kernel matrix.

torch.Size([5865, 2])
We keep 5.26e+05/9.80e+06 =  5% of the original kernel matrix.

torch.Size([10745, 2])
We keep 1.50e+06/4.60e+07 =  3% of the original kernel matrix.

torch.Size([9766, 2])
We keep 1.77e+06/3.98e+07 =  4% of the original kernel matrix.

torch.Size([13836, 2])
We keep 2.58e+06/9.27e+07 =  2% of the original kernel matrix.

torch.Size([38332, 2])
We keep 2.00e+07/7.52e+08 =  2% of the original kernel matrix.

torch.Size([27685, 2])
We keep 8.17e+06/4.03e+08 =  2% of the original kernel matrix.

torch.Size([42575, 2])
We keep 6.98e+07/1.62e+09 =  4% of the original kernel matrix.

torch.Size([28805, 2])
We keep 1.17e+07/5.92e+08 =  1% of the original kernel matrix.

torch.Size([97397, 2])
We keep 9.12e+07/5.21e+09 =  1% of the original kernel matrix.

torch.Size([44319, 2])
We keep 1.91e+07/1.06e+09 =  1% of the original kernel matrix.

torch.Size([1091, 2])
We keep 2.97e+04/2.26e+05 = 13% of the original kernel matrix.

torch.Size([5494, 2])
We keep 4.04e+05/6.98e+06 =  5% of the original kernel matrix.

torch.Size([3905, 2])
We keep 2.54e+05/3.85e+06 =  6% of the original kernel matrix.

torch.Size([9018, 2])
We keep 1.09e+06/2.88e+07 =  3% of the original kernel matrix.

torch.Size([41868, 2])
We keep 2.76e+07/1.08e+09 =  2% of the original kernel matrix.

torch.Size([28383, 2])
We keep 9.73e+06/4.83e+08 =  2% of the original kernel matrix.

torch.Size([30562, 2])
We keep 6.64e+07/9.06e+08 =  7% of the original kernel matrix.

torch.Size([23558, 2])
We keep 9.18e+06/4.42e+08 =  2% of the original kernel matrix.

torch.Size([20982, 2])
We keep 1.01e+07/2.45e+08 =  4% of the original kernel matrix.

torch.Size([20568, 2])
We keep 5.39e+06/2.30e+08 =  2% of the original kernel matrix.

torch.Size([3218, 2])
We keep 1.95e+05/2.62e+06 =  7% of the original kernel matrix.

torch.Size([8170, 2])
We keep 9.41e+05/2.38e+07 =  3% of the original kernel matrix.

torch.Size([134973, 2])
We keep 1.54e+08/1.12e+10 =  1% of the original kernel matrix.

torch.Size([53037, 2])
We keep 2.63e+07/1.55e+09 =  1% of the original kernel matrix.

torch.Size([8518, 2])
We keep 1.02e+06/2.29e+07 =  4% of the original kernel matrix.

torch.Size([12822, 2])
We keep 2.06e+06/7.04e+07 =  2% of the original kernel matrix.

torch.Size([14399, 2])
We keep 2.80e+06/9.10e+07 =  3% of the original kernel matrix.

torch.Size([17026, 2])
We keep 3.53e+06/1.40e+08 =  2% of the original kernel matrix.

torch.Size([54060, 2])
We keep 3.35e+07/1.67e+09 =  2% of the original kernel matrix.

torch.Size([32525, 2])
We keep 1.18e+07/6.01e+08 =  1% of the original kernel matrix.

torch.Size([1921, 2])
We keep 6.07e+04/6.61e+05 =  9% of the original kernel matrix.

torch.Size([6956, 2])
We keep 5.83e+05/1.19e+07 =  4% of the original kernel matrix.

torch.Size([30034, 2])
We keep 1.61e+07/5.00e+08 =  3% of the original kernel matrix.

torch.Size([24435, 2])
We keep 7.06e+06/3.29e+08 =  2% of the original kernel matrix.

torch.Size([39383, 2])
We keep 2.48e+07/9.75e+08 =  2% of the original kernel matrix.

torch.Size([27918, 2])
We keep 9.37e+06/4.59e+08 =  2% of the original kernel matrix.

torch.Size([162046, 2])
We keep 5.36e+08/1.86e+10 =  2% of the original kernel matrix.

torch.Size([58372, 2])
We keep 3.39e+07/2.01e+09 =  1% of the original kernel matrix.

torch.Size([21658, 2])
We keep 7.09e+06/2.46e+08 =  2% of the original kernel matrix.

torch.Size([21077, 2])
We keep 5.22e+06/2.30e+08 =  2% of the original kernel matrix.

torch.Size([21144, 2])
We keep 7.48e+06/2.70e+08 =  2% of the original kernel matrix.

torch.Size([20594, 2])
We keep 5.45e+06/2.41e+08 =  2% of the original kernel matrix.

torch.Size([6378, 2])
We keep 1.72e+06/1.91e+07 =  9% of the original kernel matrix.

torch.Size([10875, 2])
We keep 1.98e+06/6.41e+07 =  3% of the original kernel matrix.

torch.Size([5769, 2])
We keep 5.22e+05/9.38e+06 =  5% of the original kernel matrix.

torch.Size([10689, 2])
We keep 1.50e+06/4.50e+07 =  3% of the original kernel matrix.

torch.Size([11680, 2])
We keep 2.75e+06/6.81e+07 =  4% of the original kernel matrix.

torch.Size([15227, 2])
We keep 3.01e+06/1.21e+08 =  2% of the original kernel matrix.

torch.Size([8899, 2])
We keep 1.36e+06/3.23e+07 =  4% of the original kernel matrix.

torch.Size([13331, 2])
We keep 2.41e+06/8.35e+07 =  2% of the original kernel matrix.

torch.Size([8752, 2])
We keep 1.76e+06/3.35e+07 =  5% of the original kernel matrix.

torch.Size([13149, 2])
We keep 2.39e+06/8.50e+07 =  2% of the original kernel matrix.

torch.Size([36047, 2])
We keep 7.77e+07/9.89e+08 =  7% of the original kernel matrix.

torch.Size([26805, 2])
We keep 8.63e+06/4.62e+08 =  1% of the original kernel matrix.

torch.Size([14493, 2])
We keep 3.60e+06/9.82e+07 =  3% of the original kernel matrix.

torch.Size([17136, 2])
We keep 3.55e+06/1.46e+08 =  2% of the original kernel matrix.

torch.Size([5682, 2])
We keep 7.36e+05/1.17e+07 =  6% of the original kernel matrix.

torch.Size([10525, 2])
We keep 1.63e+06/5.02e+07 =  3% of the original kernel matrix.

torch.Size([5076, 2])
We keep 3.95e+05/6.55e+06 =  6% of the original kernel matrix.

torch.Size([10173, 2])
We keep 1.31e+06/3.76e+07 =  3% of the original kernel matrix.

torch.Size([148604, 2])
We keep 7.64e+08/3.37e+10 =  2% of the original kernel matrix.

torch.Size([53511, 2])
We keep 4.37e+07/2.70e+09 =  1% of the original kernel matrix.

torch.Size([6804, 2])
We keep 1.21e+06/1.57e+07 =  7% of the original kernel matrix.

torch.Size([11699, 2])
We keep 1.78e+06/5.83e+07 =  3% of the original kernel matrix.

torch.Size([2117, 2])
We keep 8.07e+04/9.10e+05 =  8% of the original kernel matrix.

torch.Size([7251, 2])
We keep 6.59e+05/1.40e+07 =  4% of the original kernel matrix.

torch.Size([10758, 2])
We keep 2.02e+06/5.06e+07 =  3% of the original kernel matrix.

torch.Size([14625, 2])
We keep 2.84e+06/1.05e+08 =  2% of the original kernel matrix.

torch.Size([67860, 2])
We keep 1.37e+08/4.12e+09 =  3% of the original kernel matrix.

torch.Size([36024, 2])
We keep 1.70e+07/9.43e+08 =  1% of the original kernel matrix.

torch.Size([15794, 2])
We keep 5.09e+06/1.30e+08 =  3% of the original kernel matrix.

torch.Size([18095, 2])
We keep 4.08e+06/1.68e+08 =  2% of the original kernel matrix.

torch.Size([10734, 2])
We keep 5.79e+06/7.16e+07 =  8% of the original kernel matrix.

torch.Size([14474, 2])
We keep 3.28e+06/1.24e+08 =  2% of the original kernel matrix.

torch.Size([4003, 2])
We keep 4.56e+05/5.26e+06 =  8% of the original kernel matrix.

torch.Size([9130, 2])
We keep 1.23e+06/3.37e+07 =  3% of the original kernel matrix.

torch.Size([10471, 2])
We keep 3.27e+06/6.10e+07 =  5% of the original kernel matrix.

torch.Size([14112, 2])
We keep 3.01e+06/1.15e+08 =  2% of the original kernel matrix.

torch.Size([10211, 2])
We keep 5.93e+06/4.46e+07 = 13% of the original kernel matrix.

torch.Size([14308, 2])
We keep 2.62e+06/9.81e+07 =  2% of the original kernel matrix.

torch.Size([305652, 2])
We keep 1.49e+09/7.75e+10 =  1% of the original kernel matrix.

torch.Size([78490, 2])
We keep 6.35e+07/4.09e+09 =  1% of the original kernel matrix.

torch.Size([22128, 2])
We keep 7.90e+06/3.11e+08 =  2% of the original kernel matrix.

torch.Size([20892, 2])
We keep 5.79e+06/2.59e+08 =  2% of the original kernel matrix.

torch.Size([8761, 2])
We keep 1.21e+06/2.66e+07 =  4% of the original kernel matrix.

torch.Size([13098, 2])
We keep 2.18e+06/7.58e+07 =  2% of the original kernel matrix.

torch.Size([75941, 2])
We keep 5.01e+07/3.01e+09 =  1% of the original kernel matrix.

torch.Size([38771, 2])
We keep 1.49e+07/8.07e+08 =  1% of the original kernel matrix.

torch.Size([81013, 2])
We keep 5.94e+07/3.68e+09 =  1% of the original kernel matrix.

torch.Size([40340, 2])
We keep 1.63e+07/8.92e+08 =  1% of the original kernel matrix.

torch.Size([5527, 2])
We keep 1.67e+06/1.26e+07 = 13% of the original kernel matrix.

torch.Size([10701, 2])
We keep 1.66e+06/5.21e+07 =  3% of the original kernel matrix.

torch.Size([67482, 2])
We keep 5.22e+07/2.23e+09 =  2% of the original kernel matrix.

torch.Size([35956, 2])
We keep 1.33e+07/6.94e+08 =  1% of the original kernel matrix.

torch.Size([81993, 2])
We keep 7.80e+07/4.87e+09 =  1% of the original kernel matrix.

torch.Size([40741, 2])
We keep 1.87e+07/1.03e+09 =  1% of the original kernel matrix.

torch.Size([299547, 2])
We keep 1.17e+09/6.02e+10 =  1% of the original kernel matrix.

torch.Size([81358, 2])
We keep 5.66e+07/3.60e+09 =  1% of the original kernel matrix.

torch.Size([8606, 2])
We keep 4.34e+06/2.80e+07 = 15% of the original kernel matrix.

torch.Size([13101, 2])
We keep 2.19e+06/7.78e+07 =  2% of the original kernel matrix.

torch.Size([1676, 2])
We keep 7.18e+04/6.56e+05 = 10% of the original kernel matrix.

torch.Size([6467, 2])
We keep 5.81e+05/1.19e+07 =  4% of the original kernel matrix.

torch.Size([6705, 2])
We keep 7.42e+05/1.32e+07 =  5% of the original kernel matrix.

torch.Size([11306, 2])
We keep 1.69e+06/5.33e+07 =  3% of the original kernel matrix.

torch.Size([11019, 2])
We keep 1.09e+07/8.00e+07 = 13% of the original kernel matrix.

torch.Size([14739, 2])
We keep 3.34e+06/1.31e+08 =  2% of the original kernel matrix.

torch.Size([24807, 2])
We keep 1.02e+07/3.31e+08 =  3% of the original kernel matrix.

torch.Size([22357, 2])
We keep 6.01e+06/2.67e+08 =  2% of the original kernel matrix.

torch.Size([12454, 2])
We keep 2.42e+06/6.90e+07 =  3% of the original kernel matrix.

torch.Size([15827, 2])
We keep 3.11e+06/1.22e+08 =  2% of the original kernel matrix.

torch.Size([1433, 2])
We keep 4.37e+04/4.20e+05 = 10% of the original kernel matrix.

torch.Size([6263, 2])
We keep 5.25e+05/9.52e+06 =  5% of the original kernel matrix.

torch.Size([18796, 2])
We keep 4.87e+06/1.66e+08 =  2% of the original kernel matrix.

torch.Size([19787, 2])
We keep 4.43e+06/1.90e+08 =  2% of the original kernel matrix.

torch.Size([74124, 2])
We keep 5.28e+07/2.79e+09 =  1% of the original kernel matrix.

torch.Size([37993, 2])
We keep 1.46e+07/7.76e+08 =  1% of the original kernel matrix.

torch.Size([52642, 2])
We keep 1.52e+08/3.45e+09 =  4% of the original kernel matrix.

torch.Size([30499, 2])
We keep 1.59e+07/8.63e+08 =  1% of the original kernel matrix.

torch.Size([3726, 2])
We keep 3.25e+05/3.71e+06 =  8% of the original kernel matrix.

torch.Size([8835, 2])
We keep 1.06e+06/2.83e+07 =  3% of the original kernel matrix.

torch.Size([84299, 2])
We keep 1.01e+08/4.53e+09 =  2% of the original kernel matrix.

torch.Size([40684, 2])
We keep 1.81e+07/9.89e+08 =  1% of the original kernel matrix.

torch.Size([8070, 2])
We keep 1.22e+06/2.37e+07 =  5% of the original kernel matrix.

torch.Size([12528, 2])
We keep 2.11e+06/7.16e+07 =  2% of the original kernel matrix.

torch.Size([12200, 2])
We keep 2.34e+06/6.35e+07 =  3% of the original kernel matrix.

torch.Size([15487, 2])
We keep 3.06e+06/1.17e+08 =  2% of the original kernel matrix.

torch.Size([10395, 2])
We keep 1.68e+06/4.18e+07 =  4% of the original kernel matrix.

torch.Size([14281, 2])
We keep 2.59e+06/9.50e+07 =  2% of the original kernel matrix.

torch.Size([3253, 2])
We keep 1.67e+05/2.26e+06 =  7% of the original kernel matrix.

torch.Size([8525, 2])
We keep 8.79e+05/2.21e+07 =  3% of the original kernel matrix.

torch.Size([103501, 2])
We keep 1.87e+08/7.29e+09 =  2% of the original kernel matrix.

torch.Size([45512, 2])
We keep 2.22e+07/1.25e+09 =  1% of the original kernel matrix.

torch.Size([27434, 2])
We keep 9.91e+06/3.87e+08 =  2% of the original kernel matrix.

torch.Size([23765, 2])
We keep 6.31e+06/2.89e+08 =  2% of the original kernel matrix.

torch.Size([9464, 2])
We keep 1.78e+06/3.64e+07 =  4% of the original kernel matrix.

torch.Size([13814, 2])
We keep 2.50e+06/8.87e+07 =  2% of the original kernel matrix.

torch.Size([14113, 2])
We keep 4.89e+06/1.15e+08 =  4% of the original kernel matrix.

torch.Size([16897, 2])
We keep 3.84e+06/1.57e+08 =  2% of the original kernel matrix.

torch.Size([3976, 2])
We keep 2.67e+05/4.19e+06 =  6% of the original kernel matrix.

torch.Size([9101, 2])
We keep 1.10e+06/3.01e+07 =  3% of the original kernel matrix.

torch.Size([206104, 2])
We keep 5.71e+08/2.88e+10 =  1% of the original kernel matrix.

torch.Size([66568, 2])
We keep 4.10e+07/2.49e+09 =  1% of the original kernel matrix.

torch.Size([12202, 2])
We keep 2.42e+06/6.81e+07 =  3% of the original kernel matrix.

torch.Size([15538, 2])
We keep 3.18e+06/1.21e+08 =  2% of the original kernel matrix.

torch.Size([126425, 2])
We keep 1.58e+08/9.02e+09 =  1% of the original kernel matrix.

torch.Size([51092, 2])
We keep 2.44e+07/1.40e+09 =  1% of the original kernel matrix.

torch.Size([2261, 2])
We keep 9.47e+04/1.10e+06 =  8% of the original kernel matrix.

torch.Size([7337, 2])
We keep 6.91e+05/1.54e+07 =  4% of the original kernel matrix.

torch.Size([3110, 2])
We keep 2.05e+05/2.80e+06 =  7% of the original kernel matrix.

torch.Size([8127, 2])
We keep 9.69e+05/2.46e+07 =  3% of the original kernel matrix.

torch.Size([3996, 2])
We keep 2.79e+05/3.92e+06 =  7% of the original kernel matrix.

torch.Size([9091, 2])
We keep 1.09e+06/2.91e+07 =  3% of the original kernel matrix.

torch.Size([16391, 2])
We keep 5.73e+06/1.19e+08 =  4% of the original kernel matrix.

torch.Size([18300, 2])
We keep 3.81e+06/1.61e+08 =  2% of the original kernel matrix.

torch.Size([5484, 2])
We keep 5.51e+05/9.77e+06 =  5% of the original kernel matrix.

torch.Size([10558, 2])
We keep 1.55e+06/4.59e+07 =  3% of the original kernel matrix.

torch.Size([7115, 2])
We keep 1.08e+06/1.76e+07 =  6% of the original kernel matrix.

torch.Size([11724, 2])
We keep 1.83e+06/6.16e+07 =  2% of the original kernel matrix.

torch.Size([6707, 2])
We keep 7.48e+05/1.42e+07 =  5% of the original kernel matrix.

torch.Size([11354, 2])
We keep 1.75e+06/5.54e+07 =  3% of the original kernel matrix.

torch.Size([3985, 2])
We keep 2.66e+05/3.89e+06 =  6% of the original kernel matrix.

torch.Size([9140, 2])
We keep 1.09e+06/2.90e+07 =  3% of the original kernel matrix.

torch.Size([149588, 2])
We keep 3.75e+08/1.42e+10 =  2% of the original kernel matrix.

torch.Size([56616, 2])
We keep 3.01e+07/1.75e+09 =  1% of the original kernel matrix.

torch.Size([15640, 2])
We keep 3.86e+06/1.17e+08 =  3% of the original kernel matrix.

torch.Size([17751, 2])
We keep 3.78e+06/1.59e+08 =  2% of the original kernel matrix.

torch.Size([14808, 2])
We keep 4.19e+06/1.22e+08 =  3% of the original kernel matrix.

torch.Size([17387, 2])
We keep 3.98e+06/1.62e+08 =  2% of the original kernel matrix.

torch.Size([57770, 2])
We keep 3.90e+07/1.87e+09 =  2% of the original kernel matrix.

torch.Size([33661, 2])
We keep 1.22e+07/6.36e+08 =  1% of the original kernel matrix.

torch.Size([182751, 2])
We keep 3.16e+08/2.25e+10 =  1% of the original kernel matrix.

torch.Size([63679, 2])
We keep 3.66e+07/2.21e+09 =  1% of the original kernel matrix.

torch.Size([7900, 2])
We keep 9.63e+05/2.10e+07 =  4% of the original kernel matrix.

torch.Size([12411, 2])
We keep 2.01e+06/6.73e+07 =  2% of the original kernel matrix.

torch.Size([8056, 2])
We keep 1.09e+06/2.03e+07 =  5% of the original kernel matrix.

torch.Size([12502, 2])
We keep 1.99e+06/6.63e+07 =  3% of the original kernel matrix.

torch.Size([4314, 2])
We keep 3.73e+05/4.99e+06 =  7% of the original kernel matrix.

torch.Size([9542, 2])
We keep 1.16e+06/3.28e+07 =  3% of the original kernel matrix.

torch.Size([1159, 2])
We keep 4.67e+04/3.60e+05 = 12% of the original kernel matrix.

torch.Size([5578, 2])
We keep 4.90e+05/8.82e+06 =  5% of the original kernel matrix.

torch.Size([7249, 2])
We keep 1.50e+06/2.33e+07 =  6% of the original kernel matrix.

torch.Size([11821, 2])
We keep 2.18e+06/7.10e+07 =  3% of the original kernel matrix.

torch.Size([14326, 2])
We keep 5.17e+06/1.06e+08 =  4% of the original kernel matrix.

torch.Size([17046, 2])
We keep 3.81e+06/1.51e+08 =  2% of the original kernel matrix.

torch.Size([5500, 2])
We keep 8.56e+05/1.12e+07 =  7% of the original kernel matrix.

torch.Size([10268, 2])
We keep 1.60e+06/4.92e+07 =  3% of the original kernel matrix.

torch.Size([4160, 2])
We keep 3.11e+05/4.86e+06 =  6% of the original kernel matrix.

torch.Size([9188, 2])
We keep 1.18e+06/3.24e+07 =  3% of the original kernel matrix.

torch.Size([12890, 2])
We keep 3.24e+06/7.61e+07 =  4% of the original kernel matrix.

torch.Size([16002, 2])
We keep 3.32e+06/1.28e+08 =  2% of the original kernel matrix.

torch.Size([9638, 2])
We keep 1.56e+06/3.65e+07 =  4% of the original kernel matrix.

torch.Size([13746, 2])
We keep 2.50e+06/8.88e+07 =  2% of the original kernel matrix.

torch.Size([671023, 2])
We keep 2.84e+09/2.54e+11 =  1% of the original kernel matrix.

torch.Size([123120, 2])
We keep 1.10e+08/7.40e+09 =  1% of the original kernel matrix.

torch.Size([2707, 2])
We keep 2.18e+05/1.92e+06 = 11% of the original kernel matrix.

torch.Size([7743, 2])
We keep 8.43e+05/2.04e+07 =  4% of the original kernel matrix.

torch.Size([28995, 2])
We keep 1.31e+07/5.37e+08 =  2% of the original kernel matrix.

torch.Size([24384, 2])
We keep 7.47e+06/3.41e+08 =  2% of the original kernel matrix.

torch.Size([11168, 2])
We keep 3.33e+06/6.24e+07 =  5% of the original kernel matrix.

torch.Size([14740, 2])
We keep 3.06e+06/1.16e+08 =  2% of the original kernel matrix.

torch.Size([2055, 2])
We keep 8.15e+04/9.72e+05 =  8% of the original kernel matrix.

torch.Size([7096, 2])
We keep 6.70e+05/1.45e+07 =  4% of the original kernel matrix.

torch.Size([72412, 2])
We keep 7.64e+07/3.21e+09 =  2% of the original kernel matrix.

torch.Size([37436, 2])
We keep 1.54e+07/8.33e+08 =  1% of the original kernel matrix.

torch.Size([112708, 2])
We keep 4.54e+08/1.07e+10 =  4% of the original kernel matrix.

torch.Size([47954, 2])
We keep 2.45e+07/1.52e+09 =  1% of the original kernel matrix.

torch.Size([361897, 2])
We keep 8.27e+08/6.16e+10 =  1% of the original kernel matrix.

torch.Size([88404, 2])
We keep 5.66e+07/3.65e+09 =  1% of the original kernel matrix.

torch.Size([28334, 2])
We keep 1.39e+07/4.20e+08 =  3% of the original kernel matrix.

torch.Size([23930, 2])
We keep 6.48e+06/3.01e+08 =  2% of the original kernel matrix.

torch.Size([16322, 2])
We keep 4.43e+06/1.25e+08 =  3% of the original kernel matrix.

torch.Size([18549, 2])
We keep 4.00e+06/1.64e+08 =  2% of the original kernel matrix.

torch.Size([2967, 2])
We keep 1.58e+05/1.91e+06 =  8% of the original kernel matrix.

torch.Size([8108, 2])
We keep 8.46e+05/2.03e+07 =  4% of the original kernel matrix.

torch.Size([38479, 2])
We keep 5.54e+07/1.54e+09 =  3% of the original kernel matrix.

torch.Size([26935, 2])
We keep 1.16e+07/5.77e+08 =  2% of the original kernel matrix.

torch.Size([434943, 2])
We keep 1.95e+09/1.35e+11 =  1% of the original kernel matrix.

torch.Size([97439, 2])
We keep 8.14e+07/5.39e+09 =  1% of the original kernel matrix.

torch.Size([26396, 2])
We keep 2.59e+07/5.55e+08 =  4% of the original kernel matrix.

torch.Size([23312, 2])
We keep 7.44e+06/3.46e+08 =  2% of the original kernel matrix.

torch.Size([13063, 2])
We keep 3.84e+06/7.61e+07 =  5% of the original kernel matrix.

torch.Size([16083, 2])
We keep 3.38e+06/1.28e+08 =  2% of the original kernel matrix.

torch.Size([39173, 2])
We keep 4.20e+07/1.18e+09 =  3% of the original kernel matrix.

torch.Size([27072, 2])
We keep 1.02e+07/5.06e+08 =  2% of the original kernel matrix.

torch.Size([154025, 2])
We keep 3.64e+08/2.06e+10 =  1% of the original kernel matrix.

torch.Size([57340, 2])
We keep 3.58e+07/2.11e+09 =  1% of the original kernel matrix.

torch.Size([32944, 2])
We keep 1.86e+07/6.09e+08 =  3% of the original kernel matrix.

torch.Size([25204, 2])
We keep 7.77e+06/3.63e+08 =  2% of the original kernel matrix.

torch.Size([7740, 2])
We keep 1.35e+06/2.42e+07 =  5% of the original kernel matrix.

torch.Size([12200, 2])
We keep 2.13e+06/7.24e+07 =  2% of the original kernel matrix.

torch.Size([8020, 2])
We keep 9.93e+05/2.09e+07 =  4% of the original kernel matrix.

torch.Size([13401, 2])
We keep 2.08e+06/6.71e+07 =  3% of the original kernel matrix.

torch.Size([17296, 2])
We keep 4.07e+06/1.44e+08 =  2% of the original kernel matrix.

torch.Size([18167, 2])
We keep 4.06e+06/1.76e+08 =  2% of the original kernel matrix.

torch.Size([9406, 2])
We keep 2.96e+06/4.23e+07 =  6% of the original kernel matrix.

torch.Size([13642, 2])
We keep 2.65e+06/9.56e+07 =  2% of the original kernel matrix.

torch.Size([258187, 2])
We keep 6.84e+08/3.84e+10 =  1% of the original kernel matrix.

torch.Size([74158, 2])
We keep 4.64e+07/2.88e+09 =  1% of the original kernel matrix.

torch.Size([11051, 2])
We keep 3.16e+06/5.33e+07 =  5% of the original kernel matrix.

torch.Size([14670, 2])
We keep 2.91e+06/1.07e+08 =  2% of the original kernel matrix.

torch.Size([2532, 2])
We keep 2.60e+05/1.74e+06 = 14% of the original kernel matrix.

torch.Size([7568, 2])
We keep 8.54e+05/1.94e+07 =  4% of the original kernel matrix.

torch.Size([13805, 2])
We keep 3.87e+06/8.47e+07 =  4% of the original kernel matrix.

torch.Size([16656, 2])
We keep 3.25e+06/1.35e+08 =  2% of the original kernel matrix.

torch.Size([22781, 2])
We keep 1.01e+07/3.50e+08 =  2% of the original kernel matrix.

torch.Size([21945, 2])
We keep 6.27e+06/2.75e+08 =  2% of the original kernel matrix.

torch.Size([1866, 2])
We keep 6.08e+04/6.32e+05 =  9% of the original kernel matrix.

torch.Size([6812, 2])
We keep 5.72e+05/1.17e+07 =  4% of the original kernel matrix.

torch.Size([41517, 2])
We keep 2.89e+07/1.03e+09 =  2% of the original kernel matrix.

torch.Size([28302, 2])
We keep 9.40e+06/4.71e+08 =  1% of the original kernel matrix.

torch.Size([40315, 2])
We keep 2.63e+07/9.29e+08 =  2% of the original kernel matrix.

torch.Size([28291, 2])
We keep 9.10e+06/4.48e+08 =  2% of the original kernel matrix.

torch.Size([25596, 2])
We keep 1.72e+07/4.29e+08 =  4% of the original kernel matrix.

torch.Size([23034, 2])
We keep 6.55e+06/3.04e+08 =  2% of the original kernel matrix.

torch.Size([51006, 2])
We keep 3.54e+07/1.30e+09 =  2% of the original kernel matrix.

torch.Size([31327, 2])
We keep 1.03e+07/5.29e+08 =  1% of the original kernel matrix.

torch.Size([47266, 2])
We keep 3.74e+07/1.21e+09 =  3% of the original kernel matrix.

torch.Size([30037, 2])
We keep 1.01e+07/5.11e+08 =  1% of the original kernel matrix.

torch.Size([13318, 2])
We keep 3.32e+06/9.01e+07 =  3% of the original kernel matrix.

torch.Size([16191, 2])
We keep 3.58e+06/1.39e+08 =  2% of the original kernel matrix.

torch.Size([24336, 2])
We keep 9.70e+06/3.34e+08 =  2% of the original kernel matrix.

torch.Size([22054, 2])
We keep 6.01e+06/2.68e+08 =  2% of the original kernel matrix.

torch.Size([42225, 2])
We keep 2.04e+07/9.12e+08 =  2% of the original kernel matrix.

torch.Size([28805, 2])
We keep 9.00e+06/4.44e+08 =  2% of the original kernel matrix.

torch.Size([109917, 2])
We keep 1.19e+08/6.66e+09 =  1% of the original kernel matrix.

torch.Size([47554, 2])
We keep 2.09e+07/1.20e+09 =  1% of the original kernel matrix.

torch.Size([5656, 2])
We keep 7.56e+05/9.80e+06 =  7% of the original kernel matrix.

torch.Size([10718, 2])
We keep 1.53e+06/4.60e+07 =  3% of the original kernel matrix.

torch.Size([50540, 2])
We keep 1.73e+08/1.91e+09 =  9% of the original kernel matrix.

torch.Size([31893, 2])
We keep 1.19e+07/6.42e+08 =  1% of the original kernel matrix.

torch.Size([54271, 2])
We keep 9.07e+07/1.77e+09 =  5% of the original kernel matrix.

torch.Size([32751, 2])
We keep 1.14e+07/6.18e+08 =  1% of the original kernel matrix.

torch.Size([35268, 2])
We keep 2.19e+07/6.75e+08 =  3% of the original kernel matrix.

torch.Size([26503, 2])
We keep 7.85e+06/3.82e+08 =  2% of the original kernel matrix.

torch.Size([14230, 2])
We keep 7.84e+06/1.10e+08 =  7% of the original kernel matrix.

torch.Size([16933, 2])
We keep 3.64e+06/1.54e+08 =  2% of the original kernel matrix.

torch.Size([3422, 2])
We keep 7.67e+05/3.76e+06 = 20% of the original kernel matrix.

torch.Size([8458, 2])
We keep 9.21e+05/2.85e+07 =  3% of the original kernel matrix.

torch.Size([8550, 2])
We keep 1.36e+06/2.82e+07 =  4% of the original kernel matrix.

torch.Size([12949, 2])
We keep 2.26e+06/7.81e+07 =  2% of the original kernel matrix.

torch.Size([7946, 2])
We keep 1.03e+06/2.09e+07 =  4% of the original kernel matrix.

torch.Size([12500, 2])
We keep 2.04e+06/6.71e+07 =  3% of the original kernel matrix.

torch.Size([59055, 2])
We keep 1.21e+08/3.53e+09 =  3% of the original kernel matrix.

torch.Size([33821, 2])
We keep 1.66e+07/8.73e+08 =  1% of the original kernel matrix.

torch.Size([133116, 2])
We keep 1.96e+08/1.04e+10 =  1% of the original kernel matrix.

torch.Size([52927, 2])
We keep 2.61e+07/1.50e+09 =  1% of the original kernel matrix.

torch.Size([78274, 2])
We keep 5.57e+07/3.12e+09 =  1% of the original kernel matrix.

torch.Size([39419, 2])
We keep 1.51e+07/8.21e+08 =  1% of the original kernel matrix.

torch.Size([106302, 2])
We keep 2.43e+08/8.03e+09 =  3% of the original kernel matrix.

torch.Size([46014, 2])
We keep 2.34e+07/1.32e+09 =  1% of the original kernel matrix.

torch.Size([40809, 2])
We keep 2.49e+07/9.92e+08 =  2% of the original kernel matrix.

torch.Size([28344, 2])
We keep 9.29e+06/4.63e+08 =  2% of the original kernel matrix.

torch.Size([63921, 2])
We keep 8.21e+07/2.50e+09 =  3% of the original kernel matrix.

torch.Size([34991, 2])
We keep 1.39e+07/7.35e+08 =  1% of the original kernel matrix.

torch.Size([8916, 2])
We keep 1.69e+06/3.93e+07 =  4% of the original kernel matrix.

torch.Size([13342, 2])
We keep 2.56e+06/9.21e+07 =  2% of the original kernel matrix.

torch.Size([128937, 2])
We keep 1.44e+08/9.51e+09 =  1% of the original kernel matrix.

torch.Size([51879, 2])
We keep 2.47e+07/1.43e+09 =  1% of the original kernel matrix.

torch.Size([1805, 2])
We keep 6.68e+04/7.57e+05 =  8% of the original kernel matrix.

torch.Size([6781, 2])
We keep 6.05e+05/1.28e+07 =  4% of the original kernel matrix.

torch.Size([10696, 2])
We keep 2.05e+06/4.45e+07 =  4% of the original kernel matrix.

torch.Size([14459, 2])
We keep 2.72e+06/9.80e+07 =  2% of the original kernel matrix.

torch.Size([178044, 2])
We keep 2.46e+08/1.76e+10 =  1% of the original kernel matrix.

torch.Size([61716, 2])
We keep 3.26e+07/1.95e+09 =  1% of the original kernel matrix.

torch.Size([20771, 2])
We keep 6.15e+06/2.32e+08 =  2% of the original kernel matrix.

torch.Size([20538, 2])
We keep 5.13e+06/2.24e+08 =  2% of the original kernel matrix.

torch.Size([2570, 2])
We keep 1.16e+05/1.55e+06 =  7% of the original kernel matrix.

torch.Size([7651, 2])
We keep 7.64e+05/1.83e+07 =  4% of the original kernel matrix.

torch.Size([17455, 2])
We keep 7.47e+06/1.58e+08 =  4% of the original kernel matrix.

torch.Size([18894, 2])
We keep 4.38e+06/1.85e+08 =  2% of the original kernel matrix.

torch.Size([71679, 2])
We keep 9.62e+07/4.09e+09 =  2% of the original kernel matrix.

torch.Size([37675, 2])
We keep 1.72e+07/9.39e+08 =  1% of the original kernel matrix.

torch.Size([28372, 2])
We keep 2.24e+07/4.57e+08 =  4% of the original kernel matrix.

torch.Size([23633, 2])
We keep 6.96e+06/3.14e+08 =  2% of the original kernel matrix.

torch.Size([30888, 2])
We keep 1.59e+07/6.64e+08 =  2% of the original kernel matrix.

torch.Size([25168, 2])
We keep 8.17e+06/3.79e+08 =  2% of the original kernel matrix.

torch.Size([1841, 2])
We keep 6.24e+04/6.61e+05 =  9% of the original kernel matrix.

torch.Size([6808, 2])
We keep 5.88e+05/1.19e+07 =  4% of the original kernel matrix.

torch.Size([4345, 2])
We keep 3.84e+05/5.22e+06 =  7% of the original kernel matrix.

torch.Size([9376, 2])
We keep 1.22e+06/3.36e+07 =  3% of the original kernel matrix.

torch.Size([4635, 2])
We keep 4.46e+05/6.24e+06 =  7% of the original kernel matrix.

torch.Size([9591, 2])
We keep 1.32e+06/3.67e+07 =  3% of the original kernel matrix.

torch.Size([32215, 2])
We keep 4.59e+07/8.58e+08 =  5% of the original kernel matrix.

torch.Size([24590, 2])
We keep 9.15e+06/4.30e+08 =  2% of the original kernel matrix.

torch.Size([25407, 2])
We keep 1.19e+07/4.37e+08 =  2% of the original kernel matrix.

torch.Size([23183, 2])
We keep 6.86e+06/3.07e+08 =  2% of the original kernel matrix.

torch.Size([24725, 2])
We keep 9.92e+06/3.42e+08 =  2% of the original kernel matrix.

torch.Size([22329, 2])
We keep 5.95e+06/2.72e+08 =  2% of the original kernel matrix.

torch.Size([2175, 2])
We keep 8.47e+04/1.00e+06 =  8% of the original kernel matrix.

torch.Size([7185, 2])
We keep 6.78e+05/1.47e+07 =  4% of the original kernel matrix.

torch.Size([56558, 2])
We keep 4.66e+07/1.65e+09 =  2% of the original kernel matrix.

torch.Size([33375, 2])
We keep 1.17e+07/5.97e+08 =  1% of the original kernel matrix.

torch.Size([7615, 2])
We keep 1.47e+06/2.45e+07 =  5% of the original kernel matrix.

torch.Size([12198, 2])
We keep 2.10e+06/7.28e+07 =  2% of the original kernel matrix.

torch.Size([16279, 2])
We keep 5.24e+06/1.73e+08 =  3% of the original kernel matrix.

torch.Size([18754, 2])
We keep 4.67e+06/1.94e+08 =  2% of the original kernel matrix.

torch.Size([11411, 2])
We keep 7.15e+06/9.52e+07 =  7% of the original kernel matrix.

torch.Size([14971, 2])
We keep 3.64e+06/1.43e+08 =  2% of the original kernel matrix.

torch.Size([44495, 2])
We keep 2.13e+07/9.49e+08 =  2% of the original kernel matrix.

torch.Size([29467, 2])
We keep 9.04e+06/4.53e+08 =  1% of the original kernel matrix.

torch.Size([2527, 2])
We keep 1.16e+05/1.42e+06 =  8% of the original kernel matrix.

torch.Size([7612, 2])
We keep 7.66e+05/1.75e+07 =  4% of the original kernel matrix.

torch.Size([32869, 2])
We keep 2.21e+07/6.01e+08 =  3% of the original kernel matrix.

torch.Size([25410, 2])
We keep 7.65e+06/3.60e+08 =  2% of the original kernel matrix.

torch.Size([20889, 2])
We keep 7.41e+06/2.23e+08 =  3% of the original kernel matrix.

torch.Size([20674, 2])
We keep 5.10e+06/2.20e+08 =  2% of the original kernel matrix.

torch.Size([13912, 2])
We keep 6.82e+06/1.06e+08 =  6% of the original kernel matrix.

torch.Size([16720, 2])
We keep 3.87e+06/1.51e+08 =  2% of the original kernel matrix.

torch.Size([22681, 2])
We keep 6.58e+06/2.55e+08 =  2% of the original kernel matrix.

torch.Size([21472, 2])
We keep 5.31e+06/2.34e+08 =  2% of the original kernel matrix.

torch.Size([153119, 2])
We keep 3.38e+08/1.52e+10 =  2% of the original kernel matrix.

torch.Size([56574, 2])
We keep 3.08e+07/1.81e+09 =  1% of the original kernel matrix.

torch.Size([8932, 2])
We keep 1.16e+06/2.68e+07 =  4% of the original kernel matrix.

torch.Size([13204, 2])
We keep 2.21e+06/7.61e+07 =  2% of the original kernel matrix.

torch.Size([9758, 2])
We keep 2.84e+06/5.63e+07 =  5% of the original kernel matrix.

torch.Size([13834, 2])
We keep 2.99e+06/1.10e+08 =  2% of the original kernel matrix.

torch.Size([90854, 2])
We keep 1.01e+08/4.55e+09 =  2% of the original kernel matrix.

torch.Size([42632, 2])
We keep 1.81e+07/9.92e+08 =  1% of the original kernel matrix.

torch.Size([13909, 2])
We keep 3.04e+06/9.14e+07 =  3% of the original kernel matrix.

torch.Size([16678, 2])
We keep 3.57e+06/1.41e+08 =  2% of the original kernel matrix.

torch.Size([273904, 2])
We keep 6.30e+08/5.07e+10 =  1% of the original kernel matrix.

torch.Size([76479, 2])
We keep 5.23e+07/3.31e+09 =  1% of the original kernel matrix.

torch.Size([7365, 2])
We keep 1.31e+06/1.88e+07 =  6% of the original kernel matrix.

torch.Size([11845, 2])
We keep 1.96e+06/6.38e+07 =  3% of the original kernel matrix.

torch.Size([90252, 2])
We keep 9.00e+07/4.94e+09 =  1% of the original kernel matrix.

torch.Size([42386, 2])
We keep 1.86e+07/1.03e+09 =  1% of the original kernel matrix.

torch.Size([220234, 2])
We keep 3.98e+08/3.36e+10 =  1% of the original kernel matrix.

torch.Size([69551, 2])
We keep 4.37e+07/2.70e+09 =  1% of the original kernel matrix.

torch.Size([5017, 2])
We keep 4.80e+05/7.42e+06 =  6% of the original kernel matrix.

torch.Size([10027, 2])
We keep 1.37e+06/4.00e+07 =  3% of the original kernel matrix.

torch.Size([61881, 2])
We keep 9.99e+07/3.32e+09 =  3% of the original kernel matrix.

torch.Size([34145, 2])
We keep 1.58e+07/8.47e+08 =  1% of the original kernel matrix.

torch.Size([11357, 2])
We keep 4.66e+06/9.43e+07 =  4% of the original kernel matrix.

torch.Size([14895, 2])
We keep 3.56e+06/1.43e+08 =  2% of the original kernel matrix.

torch.Size([86886, 2])
We keep 1.19e+08/5.76e+09 =  2% of the original kernel matrix.

torch.Size([41963, 2])
We keep 2.04e+07/1.11e+09 =  1% of the original kernel matrix.

torch.Size([12348, 2])
We keep 2.35e+06/6.36e+07 =  3% of the original kernel matrix.

torch.Size([15642, 2])
We keep 3.08e+06/1.17e+08 =  2% of the original kernel matrix.

torch.Size([23121, 2])
We keep 1.24e+07/3.35e+08 =  3% of the original kernel matrix.

torch.Size([21197, 2])
We keep 6.06e+06/2.69e+08 =  2% of the original kernel matrix.

torch.Size([2090, 2])
We keep 9.21e+04/1.05e+06 =  8% of the original kernel matrix.

torch.Size([7117, 2])
We keep 6.80e+05/1.51e+07 =  4% of the original kernel matrix.

torch.Size([4451, 2])
We keep 3.75e+05/5.45e+06 =  6% of the original kernel matrix.

torch.Size([9563, 2])
We keep 1.23e+06/3.43e+07 =  3% of the original kernel matrix.

torch.Size([4591, 2])
We keep 4.04e+05/5.97e+06 =  6% of the original kernel matrix.

torch.Size([9671, 2])
We keep 1.25e+06/3.59e+07 =  3% of the original kernel matrix.

torch.Size([16486, 2])
We keep 3.73e+06/1.25e+08 =  2% of the original kernel matrix.

torch.Size([18508, 2])
We keep 4.01e+06/1.65e+08 =  2% of the original kernel matrix.

torch.Size([11774, 2])
We keep 2.92e+06/6.18e+07 =  4% of the original kernel matrix.

torch.Size([15110, 2])
We keep 3.08e+06/1.16e+08 =  2% of the original kernel matrix.

torch.Size([75521, 2])
We keep 1.05e+08/3.55e+09 =  2% of the original kernel matrix.

torch.Size([38907, 2])
We keep 1.59e+07/8.76e+08 =  1% of the original kernel matrix.

torch.Size([13975, 2])
We keep 3.01e+06/9.57e+07 =  3% of the original kernel matrix.

torch.Size([16823, 2])
We keep 3.65e+06/1.44e+08 =  2% of the original kernel matrix.

torch.Size([4605, 2])
We keep 4.58e+05/6.12e+06 =  7% of the original kernel matrix.

torch.Size([9592, 2])
We keep 1.29e+06/3.63e+07 =  3% of the original kernel matrix.

torch.Size([8108, 2])
We keep 1.62e+06/2.66e+07 =  6% of the original kernel matrix.

torch.Size([12623, 2])
We keep 2.13e+06/7.58e+07 =  2% of the original kernel matrix.

torch.Size([5615, 2])
We keep 6.69e+05/9.71e+06 =  6% of the original kernel matrix.

torch.Size([10568, 2])
We keep 1.46e+06/4.58e+07 =  3% of the original kernel matrix.

torch.Size([4264, 2])
We keep 3.07e+05/4.83e+06 =  6% of the original kernel matrix.

torch.Size([9747, 2])
We keep 1.19e+06/3.23e+07 =  3% of the original kernel matrix.

torch.Size([92320, 2])
We keep 1.14e+08/6.01e+09 =  1% of the original kernel matrix.

torch.Size([43912, 2])
We keep 2.05e+07/1.14e+09 =  1% of the original kernel matrix.

torch.Size([27063, 2])
We keep 2.24e+07/4.92e+08 =  4% of the original kernel matrix.

torch.Size([23230, 2])
We keep 6.98e+06/3.26e+08 =  2% of the original kernel matrix.

torch.Size([3269, 2])
We keep 2.55e+05/3.20e+06 =  7% of the original kernel matrix.

torch.Size([8347, 2])
We keep 1.02e+06/2.63e+07 =  3% of the original kernel matrix.

torch.Size([4353, 2])
We keep 3.24e+05/4.63e+06 =  7% of the original kernel matrix.

torch.Size([9466, 2])
We keep 1.15e+06/3.16e+07 =  3% of the original kernel matrix.

torch.Size([4225, 2])
We keep 5.35e+05/5.77e+06 =  9% of the original kernel matrix.

torch.Size([9307, 2])
We keep 1.26e+06/3.53e+07 =  3% of the original kernel matrix.

torch.Size([344591, 2])
We keep 1.10e+09/5.91e+10 =  1% of the original kernel matrix.

torch.Size([85669, 2])
We keep 5.57e+07/3.57e+09 =  1% of the original kernel matrix.

torch.Size([7355, 2])
We keep 8.33e+05/1.75e+07 =  4% of the original kernel matrix.

torch.Size([12015, 2])
We keep 1.88e+06/6.15e+07 =  3% of the original kernel matrix.

torch.Size([12158, 2])
We keep 2.24e+06/6.13e+07 =  3% of the original kernel matrix.

torch.Size([15642, 2])
We keep 3.02e+06/1.15e+08 =  2% of the original kernel matrix.

torch.Size([42739, 2])
We keep 2.84e+07/1.12e+09 =  2% of the original kernel matrix.

torch.Size([28466, 2])
We keep 9.89e+06/4.92e+08 =  2% of the original kernel matrix.

torch.Size([10165, 2])
We keep 1.42e+07/1.10e+08 = 12% of the original kernel matrix.

torch.Size([13618, 2])
We keep 3.86e+06/1.54e+08 =  2% of the original kernel matrix.

torch.Size([5567, 2])
We keep 7.57e+05/1.03e+07 =  7% of the original kernel matrix.

torch.Size([10533, 2])
We keep 1.54e+06/4.71e+07 =  3% of the original kernel matrix.

torch.Size([9826, 2])
We keep 2.01e+06/4.70e+07 =  4% of the original kernel matrix.

torch.Size([13878, 2])
We keep 2.82e+06/1.01e+08 =  2% of the original kernel matrix.

torch.Size([39069, 2])
We keep 2.68e+07/9.51e+08 =  2% of the original kernel matrix.

torch.Size([27873, 2])
We keep 8.97e+06/4.53e+08 =  1% of the original kernel matrix.

torch.Size([38359, 2])
We keep 4.92e+07/9.50e+08 =  5% of the original kernel matrix.

torch.Size([27445, 2])
We keep 9.30e+06/4.53e+08 =  2% of the original kernel matrix.

torch.Size([31081, 2])
We keep 2.56e+07/7.48e+08 =  3% of the original kernel matrix.

torch.Size([24068, 2])
We keep 8.42e+06/4.02e+08 =  2% of the original kernel matrix.

torch.Size([9121, 2])
We keep 1.42e+06/2.92e+07 =  4% of the original kernel matrix.

torch.Size([13372, 2])
We keep 2.27e+06/7.95e+07 =  2% of the original kernel matrix.

torch.Size([6894, 2])
We keep 8.77e+05/1.77e+07 =  4% of the original kernel matrix.

torch.Size([11800, 2])
We keep 1.92e+06/6.18e+07 =  3% of the original kernel matrix.

torch.Size([30333, 2])
We keep 1.23e+07/4.71e+08 =  2% of the original kernel matrix.

torch.Size([24691, 2])
We keep 6.81e+06/3.19e+08 =  2% of the original kernel matrix.

torch.Size([8995, 2])
We keep 7.63e+06/4.15e+07 = 18% of the original kernel matrix.

torch.Size([13412, 2])
We keep 2.29e+06/9.47e+07 =  2% of the original kernel matrix.

torch.Size([58187, 2])
We keep 4.27e+07/1.74e+09 =  2% of the original kernel matrix.

torch.Size([33630, 2])
We keep 1.17e+07/6.13e+08 =  1% of the original kernel matrix.

torch.Size([2261, 2])
We keep 1.19e+05/1.20e+06 =  9% of the original kernel matrix.

torch.Size([7386, 2])
We keep 6.84e+05/1.61e+07 =  4% of the original kernel matrix.

torch.Size([87090, 2])
We keep 7.40e+07/4.06e+09 =  1% of the original kernel matrix.

torch.Size([41838, 2])
We keep 1.72e+07/9.36e+08 =  1% of the original kernel matrix.

torch.Size([3139, 2])
We keep 1.78e+05/2.33e+06 =  7% of the original kernel matrix.

torch.Size([8329, 2])
We keep 8.94e+05/2.25e+07 =  3% of the original kernel matrix.

torch.Size([28340, 2])
We keep 1.55e+07/4.47e+08 =  3% of the original kernel matrix.

torch.Size([23764, 2])
We keep 6.72e+06/3.11e+08 =  2% of the original kernel matrix.

torch.Size([10085, 2])
We keep 2.89e+06/5.91e+07 =  4% of the original kernel matrix.

torch.Size([13978, 2])
We keep 2.97e+06/1.13e+08 =  2% of the original kernel matrix.

torch.Size([3353, 2])
We keep 1.99e+05/2.48e+06 =  8% of the original kernel matrix.

torch.Size([8663, 2])
We keep 9.27e+05/2.31e+07 =  4% of the original kernel matrix.

torch.Size([10530, 2])
We keep 1.59e+06/4.10e+07 =  3% of the original kernel matrix.

torch.Size([14422, 2])
We keep 2.58e+06/9.41e+07 =  2% of the original kernel matrix.

torch.Size([34858, 2])
We keep 2.06e+07/6.99e+08 =  2% of the original kernel matrix.

torch.Size([25960, 2])
We keep 8.19e+06/3.89e+08 =  2% of the original kernel matrix.

torch.Size([11549, 2])
We keep 2.83e+06/6.52e+07 =  4% of the original kernel matrix.

torch.Size([14884, 2])
We keep 3.09e+06/1.19e+08 =  2% of the original kernel matrix.

torch.Size([24768, 2])
We keep 1.04e+07/3.56e+08 =  2% of the original kernel matrix.

torch.Size([22348, 2])
We keep 6.20e+06/2.77e+08 =  2% of the original kernel matrix.

torch.Size([4132, 2])
We keep 3.02e+06/1.02e+07 = 29% of the original kernel matrix.

torch.Size([9258, 2])
We keep 1.44e+06/4.69e+07 =  3% of the original kernel matrix.

torch.Size([121126, 2])
We keep 5.29e+08/1.44e+10 =  3% of the original kernel matrix.

torch.Size([49210, 2])
We keep 3.00e+07/1.76e+09 =  1% of the original kernel matrix.

torch.Size([2229, 2])
We keep 9.42e+04/9.92e+05 =  9% of the original kernel matrix.

torch.Size([7326, 2])
We keep 6.75e+05/1.46e+07 =  4% of the original kernel matrix.

torch.Size([3053, 2])
We keep 2.87e+05/2.76e+06 = 10% of the original kernel matrix.

torch.Size([8180, 2])
We keep 9.34e+05/2.44e+07 =  3% of the original kernel matrix.

torch.Size([5944, 2])
We keep 1.99e+06/1.85e+07 = 10% of the original kernel matrix.

torch.Size([10733, 2])
We keep 1.86e+06/6.32e+07 =  2% of the original kernel matrix.

torch.Size([12592, 2])
We keep 3.82e+06/7.99e+07 =  4% of the original kernel matrix.

torch.Size([15780, 2])
We keep 3.44e+06/1.31e+08 =  2% of the original kernel matrix.

torch.Size([2836, 2])
We keep 1.33e+05/1.75e+06 =  7% of the original kernel matrix.

torch.Size([8102, 2])
We keep 8.10e+05/1.94e+07 =  4% of the original kernel matrix.

torch.Size([7992, 2])
We keep 1.04e+06/2.21e+07 =  4% of the original kernel matrix.

torch.Size([12328, 2])
We keep 2.06e+06/6.92e+07 =  2% of the original kernel matrix.

torch.Size([20719, 2])
We keep 6.46e+06/2.32e+08 =  2% of the original kernel matrix.

torch.Size([20591, 2])
We keep 5.15e+06/2.24e+08 =  2% of the original kernel matrix.

torch.Size([71809, 2])
We keep 1.21e+08/3.67e+09 =  3% of the original kernel matrix.

torch.Size([37551, 2])
We keep 1.66e+07/8.90e+08 =  1% of the original kernel matrix.

torch.Size([4911, 2])
We keep 4.52e+05/7.42e+06 =  6% of the original kernel matrix.

torch.Size([9919, 2])
We keep 1.39e+06/4.00e+07 =  3% of the original kernel matrix.

torch.Size([2218, 2])
We keep 9.86e+04/1.03e+06 =  9% of the original kernel matrix.

torch.Size([7145, 2])
We keep 6.83e+05/1.49e+07 =  4% of the original kernel matrix.

torch.Size([4886, 2])
We keep 4.11e+05/6.48e+06 =  6% of the original kernel matrix.

torch.Size([9857, 2])
We keep 1.30e+06/3.74e+07 =  3% of the original kernel matrix.

torch.Size([146103, 2])
We keep 3.54e+08/1.49e+10 =  2% of the original kernel matrix.

torch.Size([55177, 2])
We keep 3.05e+07/1.80e+09 =  1% of the original kernel matrix.

torch.Size([546566, 2])
We keep 2.86e+09/1.98e+11 =  1% of the original kernel matrix.

torch.Size([115244, 2])
We keep 9.83e+07/6.55e+09 =  1% of the original kernel matrix.

torch.Size([12202, 2])
We keep 2.75e+06/7.17e+07 =  3% of the original kernel matrix.

torch.Size([15474, 2])
We keep 3.21e+06/1.24e+08 =  2% of the original kernel matrix.

torch.Size([5724, 2])
We keep 7.70e+05/1.35e+07 =  5% of the original kernel matrix.

torch.Size([10645, 2])
We keep 1.77e+06/5.39e+07 =  3% of the original kernel matrix.

torch.Size([138232, 2])
We keep 2.49e+08/1.15e+10 =  2% of the original kernel matrix.

torch.Size([53662, 2])
We keep 2.72e+07/1.57e+09 =  1% of the original kernel matrix.

torch.Size([75721, 2])
We keep 7.26e+07/3.98e+09 =  1% of the original kernel matrix.

torch.Size([38890, 2])
We keep 1.73e+07/9.27e+08 =  1% of the original kernel matrix.

torch.Size([2851, 2])
We keep 1.29e+05/1.67e+06 =  7% of the original kernel matrix.

torch.Size([7926, 2])
We keep 7.93e+05/1.90e+07 =  4% of the original kernel matrix.

torch.Size([7459, 2])
We keep 1.92e+06/2.25e+07 =  8% of the original kernel matrix.

torch.Size([12031, 2])
We keep 2.05e+06/6.97e+07 =  2% of the original kernel matrix.

torch.Size([26789, 2])
We keep 9.30e+06/4.03e+08 =  2% of the original kernel matrix.

torch.Size([23472, 2])
We keep 6.36e+06/2.95e+08 =  2% of the original kernel matrix.

torch.Size([196145, 2])
We keep 1.09e+09/5.51e+10 =  1% of the original kernel matrix.

torch.Size([61886, 2])
We keep 5.52e+07/3.45e+09 =  1% of the original kernel matrix.

torch.Size([278947, 2])
We keep 9.47e+08/5.18e+10 =  1% of the original kernel matrix.

torch.Size([76499, 2])
We keep 5.32e+07/3.34e+09 =  1% of the original kernel matrix.

torch.Size([97408, 2])
We keep 8.71e+07/4.96e+09 =  1% of the original kernel matrix.

torch.Size([44455, 2])
We keep 1.84e+07/1.03e+09 =  1% of the original kernel matrix.

torch.Size([11725, 2])
We keep 2.19e+06/5.70e+07 =  3% of the original kernel matrix.

torch.Size([15331, 2])
We keep 2.92e+06/1.11e+08 =  2% of the original kernel matrix.

torch.Size([7223, 2])
We keep 8.55e+05/1.71e+07 =  4% of the original kernel matrix.

torch.Size([11901, 2])
We keep 1.89e+06/6.08e+07 =  3% of the original kernel matrix.

torch.Size([3717, 2])
We keep 4.39e+05/4.60e+06 =  9% of the original kernel matrix.

torch.Size([8764, 2])
We keep 1.17e+06/3.15e+07 =  3% of the original kernel matrix.

torch.Size([44691, 2])
We keep 3.40e+07/1.14e+09 =  2% of the original kernel matrix.

torch.Size([29182, 2])
We keep 9.79e+06/4.95e+08 =  1% of the original kernel matrix.

torch.Size([9886, 2])
We keep 1.55e+06/3.53e+07 =  4% of the original kernel matrix.

torch.Size([13948, 2])
We keep 2.47e+06/8.73e+07 =  2% of the original kernel matrix.

torch.Size([9966, 2])
We keep 7.65e+06/8.75e+07 =  8% of the original kernel matrix.

torch.Size([13866, 2])
We keep 3.42e+06/1.37e+08 =  2% of the original kernel matrix.

torch.Size([85738, 2])
We keep 9.78e+07/4.12e+09 =  2% of the original kernel matrix.

torch.Size([41128, 2])
We keep 1.71e+07/9.44e+08 =  1% of the original kernel matrix.

torch.Size([315722, 2])
We keep 6.31e+08/4.63e+10 =  1% of the original kernel matrix.

torch.Size([81680, 2])
We keep 5.02e+07/3.16e+09 =  1% of the original kernel matrix.

torch.Size([5199, 2])
We keep 4.78e+05/8.04e+06 =  5% of the original kernel matrix.

torch.Size([10167, 2])
We keep 1.40e+06/4.17e+07 =  3% of the original kernel matrix.

torch.Size([2073, 2])
We keep 7.84e+04/8.43e+05 =  9% of the original kernel matrix.

torch.Size([7017, 2])
We keep 6.39e+05/1.35e+07 =  4% of the original kernel matrix.

torch.Size([11533, 2])
We keep 5.46e+06/6.12e+07 =  8% of the original kernel matrix.

torch.Size([15126, 2])
We keep 2.80e+06/1.15e+08 =  2% of the original kernel matrix.

torch.Size([45066, 2])
We keep 7.74e+07/1.17e+09 =  6% of the original kernel matrix.

torch.Size([29864, 2])
We keep 9.46e+06/5.02e+08 =  1% of the original kernel matrix.

torch.Size([29389, 2])
We keep 2.01e+07/4.87e+08 =  4% of the original kernel matrix.

torch.Size([24195, 2])
We keep 7.07e+06/3.24e+08 =  2% of the original kernel matrix.

torch.Size([2693, 2])
We keep 4.33e+05/3.12e+06 = 13% of the original kernel matrix.

torch.Size([7465, 2])
We keep 9.77e+05/2.59e+07 =  3% of the original kernel matrix.

torch.Size([1310, 2])
We keep 3.74e+04/3.45e+05 = 10% of the original kernel matrix.

torch.Size([5889, 2])
We keep 4.68e+05/8.63e+06 =  5% of the original kernel matrix.

torch.Size([13944, 2])
We keep 3.94e+06/9.47e+07 =  4% of the original kernel matrix.

torch.Size([16689, 2])
We keep 3.63e+06/1.43e+08 =  2% of the original kernel matrix.

torch.Size([3617, 2])
We keep 2.31e+05/3.28e+06 =  7% of the original kernel matrix.

torch.Size([8803, 2])
We keep 1.02e+06/2.66e+07 =  3% of the original kernel matrix.

torch.Size([4717, 2])
We keep 5.26e+05/7.40e+06 =  7% of the original kernel matrix.

torch.Size([9906, 2])
We keep 1.41e+06/4.00e+07 =  3% of the original kernel matrix.

torch.Size([3746, 2])
We keep 2.33e+05/3.42e+06 =  6% of the original kernel matrix.

torch.Size([8926, 2])
We keep 1.04e+06/2.72e+07 =  3% of the original kernel matrix.

torch.Size([9923, 2])
We keep 1.40e+06/3.48e+07 =  4% of the original kernel matrix.

torch.Size([13951, 2])
We keep 2.43e+06/8.67e+07 =  2% of the original kernel matrix.

torch.Size([10211, 2])
We keep 1.97e+06/4.65e+07 =  4% of the original kernel matrix.

torch.Size([14246, 2])
We keep 2.78e+06/1.00e+08 =  2% of the original kernel matrix.

torch.Size([6162, 2])
We keep 5.53e+05/1.08e+07 =  5% of the original kernel matrix.

torch.Size([11098, 2])
We keep 1.55e+06/4.82e+07 =  3% of the original kernel matrix.

torch.Size([3766, 2])
We keep 2.85e+05/3.39e+06 =  8% of the original kernel matrix.

torch.Size([8798, 2])
We keep 1.04e+06/2.71e+07 =  3% of the original kernel matrix.

torch.Size([98878, 2])
We keep 9.61e+07/5.70e+09 =  1% of the original kernel matrix.

torch.Size([44933, 2])
We keep 2.00e+07/1.11e+09 =  1% of the original kernel matrix.

torch.Size([203589, 2])
We keep 3.36e+08/2.19e+10 =  1% of the original kernel matrix.

torch.Size([66507, 2])
We keep 3.55e+07/2.17e+09 =  1% of the original kernel matrix.

torch.Size([45851, 2])
We keep 4.11e+07/1.21e+09 =  3% of the original kernel matrix.

torch.Size([30021, 2])
We keep 1.03e+07/5.12e+08 =  2% of the original kernel matrix.

torch.Size([24154, 2])
We keep 2.98e+07/4.11e+08 =  7% of the original kernel matrix.

torch.Size([21693, 2])
We keep 6.58e+06/2.98e+08 =  2% of the original kernel matrix.

torch.Size([12719, 2])
We keep 2.47e+06/6.97e+07 =  3% of the original kernel matrix.

torch.Size([15953, 2])
We keep 3.18e+06/1.23e+08 =  2% of the original kernel matrix.

torch.Size([9675, 2])
We keep 2.42e+06/3.92e+07 =  6% of the original kernel matrix.

torch.Size([13733, 2])
We keep 2.59e+06/9.20e+07 =  2% of the original kernel matrix.

torch.Size([14072, 2])
We keep 8.22e+06/1.37e+08 =  6% of the original kernel matrix.

torch.Size([17297, 2])
We keep 4.35e+06/1.72e+08 =  2% of the original kernel matrix.

torch.Size([87847, 2])
We keep 6.68e+07/4.07e+09 =  1% of the original kernel matrix.

torch.Size([42258, 2])
We keep 1.72e+07/9.37e+08 =  1% of the original kernel matrix.

torch.Size([9166, 2])
We keep 2.70e+06/3.96e+07 =  6% of the original kernel matrix.

torch.Size([13478, 2])
We keep 2.62e+06/9.25e+07 =  2% of the original kernel matrix.

torch.Size([19372, 2])
We keep 7.96e+06/2.06e+08 =  3% of the original kernel matrix.

torch.Size([19910, 2])
We keep 4.79e+06/2.11e+08 =  2% of the original kernel matrix.

torch.Size([12335, 2])
We keep 2.69e+06/6.68e+07 =  4% of the original kernel matrix.

torch.Size([15645, 2])
We keep 3.14e+06/1.20e+08 =  2% of the original kernel matrix.

torch.Size([429469, 2])
We keep 1.96e+09/1.31e+11 =  1% of the original kernel matrix.

torch.Size([100797, 2])
We keep 8.20e+07/5.32e+09 =  1% of the original kernel matrix.

torch.Size([14849, 2])
We keep 6.81e+06/1.12e+08 =  6% of the original kernel matrix.

torch.Size([17202, 2])
We keep 3.85e+06/1.56e+08 =  2% of the original kernel matrix.

torch.Size([42251, 2])
We keep 2.66e+07/1.03e+09 =  2% of the original kernel matrix.

torch.Size([28653, 2])
We keep 9.46e+06/4.71e+08 =  2% of the original kernel matrix.

torch.Size([79356, 2])
We keep 1.09e+08/4.56e+09 =  2% of the original kernel matrix.

torch.Size([39876, 2])
We keep 1.79e+07/9.92e+08 =  1% of the original kernel matrix.

torch.Size([53460, 2])
We keep 3.68e+07/1.43e+09 =  2% of the original kernel matrix.

torch.Size([32215, 2])
We keep 1.10e+07/5.57e+08 =  1% of the original kernel matrix.

torch.Size([17967, 2])
We keep 6.31e+06/1.75e+08 =  3% of the original kernel matrix.

torch.Size([19200, 2])
We keep 4.61e+06/1.94e+08 =  2% of the original kernel matrix.

torch.Size([152328, 2])
We keep 2.38e+08/1.44e+10 =  1% of the original kernel matrix.

torch.Size([57262, 2])
We keep 2.97e+07/1.77e+09 =  1% of the original kernel matrix.

torch.Size([67549, 2])
We keep 4.47e+07/2.43e+09 =  1% of the original kernel matrix.

torch.Size([36555, 2])
We keep 1.36e+07/7.25e+08 =  1% of the original kernel matrix.

torch.Size([16304, 2])
We keep 3.85e+06/1.33e+08 =  2% of the original kernel matrix.

torch.Size([18286, 2])
We keep 4.05e+06/1.69e+08 =  2% of the original kernel matrix.

torch.Size([6723, 2])
We keep 1.32e+06/1.68e+07 =  7% of the original kernel matrix.

torch.Size([11475, 2])
We keep 1.86e+06/6.02e+07 =  3% of the original kernel matrix.

torch.Size([21854, 2])
We keep 9.53e+06/2.52e+08 =  3% of the original kernel matrix.

torch.Size([21098, 2])
We keep 5.39e+06/2.33e+08 =  2% of the original kernel matrix.

torch.Size([8099, 2])
We keep 1.47e+06/2.69e+07 =  5% of the original kernel matrix.

torch.Size([12658, 2])
We keep 2.18e+06/7.62e+07 =  2% of the original kernel matrix.

torch.Size([56525, 2])
We keep 3.78e+07/1.58e+09 =  2% of the original kernel matrix.

torch.Size([32935, 2])
We keep 1.13e+07/5.84e+08 =  1% of the original kernel matrix.

torch.Size([201415, 2])
We keep 4.19e+08/3.07e+10 =  1% of the original kernel matrix.

torch.Size([66878, 2])
We keep 4.24e+07/2.57e+09 =  1% of the original kernel matrix.

torch.Size([45207, 2])
We keep 4.06e+07/1.28e+09 =  3% of the original kernel matrix.

torch.Size([29840, 2])
We keep 1.06e+07/5.25e+08 =  2% of the original kernel matrix.

torch.Size([53742, 2])
We keep 4.55e+07/1.83e+09 =  2% of the original kernel matrix.

torch.Size([31965, 2])
We keep 1.22e+07/6.29e+08 =  1% of the original kernel matrix.

torch.Size([11867, 2])
We keep 1.97e+06/5.46e+07 =  3% of the original kernel matrix.

torch.Size([15418, 2])
We keep 2.89e+06/1.09e+08 =  2% of the original kernel matrix.

torch.Size([4917, 2])
We keep 7.33e+05/7.65e+06 =  9% of the original kernel matrix.

torch.Size([10026, 2])
We keep 1.38e+06/4.06e+07 =  3% of the original kernel matrix.

torch.Size([22836, 2])
We keep 9.54e+06/3.71e+08 =  2% of the original kernel matrix.

torch.Size([22309, 2])
We keep 6.41e+06/2.83e+08 =  2% of the original kernel matrix.

torch.Size([8162, 2])
We keep 2.35e+06/3.36e+07 =  6% of the original kernel matrix.

torch.Size([12393, 2])
We keep 2.45e+06/8.52e+07 =  2% of the original kernel matrix.

torch.Size([108227, 2])
We keep 1.78e+08/8.45e+09 =  2% of the original kernel matrix.

torch.Size([48243, 2])
We keep 2.37e+07/1.35e+09 =  1% of the original kernel matrix.

torch.Size([221740, 2])
We keep 3.75e+08/3.07e+10 =  1% of the original kernel matrix.

torch.Size([69736, 2])
We keep 4.17e+07/2.57e+09 =  1% of the original kernel matrix.

torch.Size([1926, 2])
We keep 7.40e+04/7.74e+05 =  9% of the original kernel matrix.

torch.Size([6920, 2])
We keep 6.18e+05/1.29e+07 =  4% of the original kernel matrix.

torch.Size([433071, 2])
We keep 2.70e+09/1.43e+11 =  1% of the original kernel matrix.

torch.Size([97090, 2])
We keep 8.33e+07/5.55e+09 =  1% of the original kernel matrix.

torch.Size([13223, 2])
We keep 8.21e+06/8.58e+07 =  9% of the original kernel matrix.

torch.Size([16386, 2])
We keep 3.18e+06/1.36e+08 =  2% of the original kernel matrix.

torch.Size([37110, 2])
We keep 2.78e+07/7.58e+08 =  3% of the original kernel matrix.

torch.Size([27031, 2])
We keep 8.44e+06/4.05e+08 =  2% of the original kernel matrix.

torch.Size([7132, 2])
We keep 1.38e+06/2.02e+07 =  6% of the original kernel matrix.

torch.Size([11809, 2])
We keep 2.01e+06/6.60e+07 =  3% of the original kernel matrix.

torch.Size([26227, 2])
We keep 1.54e+07/4.70e+08 =  3% of the original kernel matrix.

torch.Size([23236, 2])
We keep 6.96e+06/3.18e+08 =  2% of the original kernel matrix.

torch.Size([109900, 2])
We keep 1.27e+08/6.28e+09 =  2% of the original kernel matrix.

torch.Size([47584, 2])
We keep 2.07e+07/1.16e+09 =  1% of the original kernel matrix.

torch.Size([2351, 2])
We keep 1.00e+05/1.17e+06 =  8% of the original kernel matrix.

torch.Size([7496, 2])
We keep 7.22e+05/1.59e+07 =  4% of the original kernel matrix.

torch.Size([454054, 2])
We keep 1.18e+09/9.80e+10 =  1% of the original kernel matrix.

torch.Size([101048, 2])
We keep 7.10e+07/4.60e+09 =  1% of the original kernel matrix.

torch.Size([14114, 2])
We keep 4.31e+06/1.17e+08 =  3% of the original kernel matrix.

torch.Size([16945, 2])
We keep 3.79e+06/1.59e+08 =  2% of the original kernel matrix.

torch.Size([18113, 2])
We keep 1.56e+07/2.40e+08 =  6% of the original kernel matrix.

torch.Size([18808, 2])
We keep 5.34e+06/2.27e+08 =  2% of the original kernel matrix.

torch.Size([11055, 2])
We keep 2.94e+06/5.35e+07 =  5% of the original kernel matrix.

torch.Size([14874, 2])
We keep 2.89e+06/1.07e+08 =  2% of the original kernel matrix.

torch.Size([10029, 2])
We keep 1.80e+06/3.80e+07 =  4% of the original kernel matrix.

torch.Size([14000, 2])
We keep 2.55e+06/9.06e+07 =  2% of the original kernel matrix.

torch.Size([2685, 2])
We keep 1.64e+05/1.66e+06 =  9% of the original kernel matrix.

torch.Size([7731, 2])
We keep 8.11e+05/1.90e+07 =  4% of the original kernel matrix.

torch.Size([11621, 2])
We keep 2.22e+06/5.74e+07 =  3% of the original kernel matrix.

torch.Size([15049, 2])
We keep 2.96e+06/1.11e+08 =  2% of the original kernel matrix.

torch.Size([7902, 2])
We keep 9.45e+05/2.07e+07 =  4% of the original kernel matrix.

torch.Size([12471, 2])
We keep 1.99e+06/6.69e+07 =  2% of the original kernel matrix.

torch.Size([22320, 2])
We keep 7.21e+06/2.57e+08 =  2% of the original kernel matrix.

torch.Size([21261, 2])
We keep 5.34e+06/2.36e+08 =  2% of the original kernel matrix.

torch.Size([14080, 2])
We keep 1.62e+08/2.47e+08 = 65% of the original kernel matrix.

torch.Size([17177, 2])
We keep 4.00e+06/2.31e+08 =  1% of the original kernel matrix.

torch.Size([16201, 2])
We keep 8.08e+06/1.69e+08 =  4% of the original kernel matrix.

torch.Size([17668, 2])
We keep 4.57e+06/1.91e+08 =  2% of the original kernel matrix.

torch.Size([167042, 2])
We keep 2.35e+08/1.56e+10 =  1% of the original kernel matrix.

torch.Size([60131, 2])
We keep 3.06e+07/1.83e+09 =  1% of the original kernel matrix.

torch.Size([4491, 2])
We keep 4.07e+05/6.84e+06 =  5% of the original kernel matrix.

torch.Size([9570, 2])
We keep 1.36e+06/3.84e+07 =  3% of the original kernel matrix.

torch.Size([17152, 2])
We keep 5.44e+06/1.44e+08 =  3% of the original kernel matrix.

torch.Size([18136, 2])
We keep 4.16e+06/1.76e+08 =  2% of the original kernel matrix.

torch.Size([4124, 2])
We keep 2.97e+05/4.77e+06 =  6% of the original kernel matrix.

torch.Size([9126, 2])
We keep 1.16e+06/3.21e+07 =  3% of the original kernel matrix.

torch.Size([5406, 2])
We keep 1.22e+06/8.19e+06 = 14% of the original kernel matrix.

torch.Size([10506, 2])
We keep 1.26e+06/4.21e+07 =  2% of the original kernel matrix.

torch.Size([64818, 2])
We keep 7.85e+07/2.98e+09 =  2% of the original kernel matrix.

torch.Size([35416, 2])
We keep 1.50e+07/8.02e+08 =  1% of the original kernel matrix.

torch.Size([867471, 2])
We keep 4.47e+09/3.88e+11 =  1% of the original kernel matrix.

torch.Size([140555, 2])
We keep 1.35e+08/9.15e+09 =  1% of the original kernel matrix.

torch.Size([18517, 2])
We keep 7.27e+06/1.89e+08 =  3% of the original kernel matrix.

torch.Size([19199, 2])
We keep 4.76e+06/2.02e+08 =  2% of the original kernel matrix.

torch.Size([119046, 2])
We keep 1.42e+08/8.34e+09 =  1% of the original kernel matrix.

torch.Size([49775, 2])
We keep 2.35e+07/1.34e+09 =  1% of the original kernel matrix.

torch.Size([7226, 2])
We keep 8.11e+05/1.61e+07 =  5% of the original kernel matrix.

torch.Size([11943, 2])
We keep 1.80e+06/5.90e+07 =  3% of the original kernel matrix.

torch.Size([82007, 2])
We keep 7.52e+07/3.88e+09 =  1% of the original kernel matrix.

torch.Size([40209, 2])
We keep 1.69e+07/9.15e+08 =  1% of the original kernel matrix.

torch.Size([86984, 2])
We keep 1.22e+08/5.46e+09 =  2% of the original kernel matrix.

torch.Size([41699, 2])
We keep 1.95e+07/1.09e+09 =  1% of the original kernel matrix.

torch.Size([21177, 2])
We keep 8.96e+06/2.45e+08 =  3% of the original kernel matrix.

torch.Size([20691, 2])
We keep 5.35e+06/2.30e+08 =  2% of the original kernel matrix.

torch.Size([4460, 2])
We keep 4.24e+05/5.24e+06 =  8% of the original kernel matrix.

torch.Size([9593, 2])
We keep 1.20e+06/3.36e+07 =  3% of the original kernel matrix.

torch.Size([82737, 2])
We keep 1.09e+08/3.95e+09 =  2% of the original kernel matrix.

torch.Size([40706, 2])
We keep 1.72e+07/9.23e+08 =  1% of the original kernel matrix.

torch.Size([11696, 2])
We keep 7.35e+06/6.41e+07 = 11% of the original kernel matrix.

torch.Size([15194, 2])
We keep 2.90e+06/1.18e+08 =  2% of the original kernel matrix.

torch.Size([57508, 2])
We keep 4.26e+07/1.74e+09 =  2% of the original kernel matrix.

torch.Size([33168, 2])
We keep 1.19e+07/6.13e+08 =  1% of the original kernel matrix.

torch.Size([147910, 2])
We keep 1.91e+08/1.32e+10 =  1% of the original kernel matrix.

torch.Size([55755, 2])
We keep 2.84e+07/1.69e+09 =  1% of the original kernel matrix.

torch.Size([8287, 2])
We keep 3.48e+06/3.62e+07 =  9% of the original kernel matrix.

torch.Size([12897, 2])
We keep 2.37e+06/8.85e+07 =  2% of the original kernel matrix.

torch.Size([19793, 2])
We keep 5.83e+06/2.01e+08 =  2% of the original kernel matrix.

torch.Size([21992, 2])
We keep 5.00e+06/2.08e+08 =  2% of the original kernel matrix.

torch.Size([13278, 2])
We keep 4.11e+06/9.81e+07 =  4% of the original kernel matrix.

torch.Size([16012, 2])
We keep 3.68e+06/1.46e+08 =  2% of the original kernel matrix.

torch.Size([79435, 2])
We keep 9.51e+07/5.05e+09 =  1% of the original kernel matrix.

torch.Size([39578, 2])
We keep 1.92e+07/1.04e+09 =  1% of the original kernel matrix.

torch.Size([94669, 2])
We keep 1.09e+08/5.50e+09 =  1% of the original kernel matrix.

torch.Size([43797, 2])
We keep 1.93e+07/1.09e+09 =  1% of the original kernel matrix.

torch.Size([23242, 2])
We keep 1.58e+07/3.80e+08 =  4% of the original kernel matrix.

torch.Size([21893, 2])
We keep 6.67e+06/2.87e+08 =  2% of the original kernel matrix.

torch.Size([8152, 2])
We keep 1.45e+06/3.08e+07 =  4% of the original kernel matrix.

torch.Size([12740, 2])
We keep 2.44e+06/8.16e+07 =  2% of the original kernel matrix.

torch.Size([211683, 2])
We keep 4.54e+08/3.08e+10 =  1% of the original kernel matrix.

torch.Size([66794, 2])
We keep 4.20e+07/2.58e+09 =  1% of the original kernel matrix.

torch.Size([52784, 2])
We keep 3.54e+07/1.52e+09 =  2% of the original kernel matrix.

torch.Size([31664, 2])
We keep 1.13e+07/5.73e+08 =  1% of the original kernel matrix.

torch.Size([178667, 2])
We keep 4.06e+08/2.28e+10 =  1% of the original kernel matrix.

torch.Size([62275, 2])
We keep 3.63e+07/2.22e+09 =  1% of the original kernel matrix.

torch.Size([57051, 2])
We keep 4.29e+07/1.75e+09 =  2% of the original kernel matrix.

torch.Size([33044, 2])
We keep 1.19e+07/6.15e+08 =  1% of the original kernel matrix.

torch.Size([58865, 2])
We keep 7.69e+07/2.37e+09 =  3% of the original kernel matrix.

torch.Size([33273, 2])
We keep 1.35e+07/7.15e+08 =  1% of the original kernel matrix.

torch.Size([13295, 2])
We keep 6.73e+06/9.42e+07 =  7% of the original kernel matrix.

torch.Size([16321, 2])
We keep 3.65e+06/1.43e+08 =  2% of the original kernel matrix.

torch.Size([82011, 2])
We keep 2.78e+08/5.26e+09 =  5% of the original kernel matrix.

torch.Size([39902, 2])
We keep 1.93e+07/1.07e+09 =  1% of the original kernel matrix.

torch.Size([159920, 2])
We keep 4.32e+08/1.87e+10 =  2% of the original kernel matrix.

torch.Size([57829, 2])
We keep 3.33e+07/2.01e+09 =  1% of the original kernel matrix.

torch.Size([51979, 2])
We keep 2.81e+07/1.31e+09 =  2% of the original kernel matrix.

torch.Size([31312, 2])
We keep 1.06e+07/5.33e+08 =  1% of the original kernel matrix.

torch.Size([6641, 2])
We keep 8.04e+05/1.53e+07 =  5% of the original kernel matrix.

torch.Size([11433, 2])
We keep 1.79e+06/5.76e+07 =  3% of the original kernel matrix.

torch.Size([12997, 2])
We keep 4.58e+06/1.02e+08 =  4% of the original kernel matrix.

torch.Size([16579, 2])
We keep 3.91e+06/1.48e+08 =  2% of the original kernel matrix.

torch.Size([10170, 2])
We keep 3.26e+06/4.27e+07 =  7% of the original kernel matrix.

torch.Size([14014, 2])
We keep 2.50e+06/9.60e+07 =  2% of the original kernel matrix.

torch.Size([45284, 2])
We keep 2.64e+07/1.09e+09 =  2% of the original kernel matrix.

torch.Size([29486, 2])
We keep 9.75e+06/4.85e+08 =  2% of the original kernel matrix.

torch.Size([36714, 2])
We keep 1.63e+07/6.73e+08 =  2% of the original kernel matrix.

torch.Size([27143, 2])
We keep 7.95e+06/3.81e+08 =  2% of the original kernel matrix.

torch.Size([7708, 2])
We keep 9.08e+05/1.92e+07 =  4% of the original kernel matrix.

torch.Size([12196, 2])
We keep 1.96e+06/6.44e+07 =  3% of the original kernel matrix.

torch.Size([16397, 2])
We keep 3.40e+06/1.20e+08 =  2% of the original kernel matrix.

torch.Size([18302, 2])
We keep 3.92e+06/1.61e+08 =  2% of the original kernel matrix.

torch.Size([79389, 2])
We keep 1.01e+08/3.58e+09 =  2% of the original kernel matrix.

torch.Size([39579, 2])
We keep 1.62e+07/8.79e+08 =  1% of the original kernel matrix.

torch.Size([37200, 2])
We keep 2.00e+07/7.96e+08 =  2% of the original kernel matrix.

torch.Size([27181, 2])
We keep 8.60e+06/4.15e+08 =  2% of the original kernel matrix.

torch.Size([23236, 2])
We keep 1.13e+07/3.31e+08 =  3% of the original kernel matrix.

torch.Size([21777, 2])
We keep 6.07e+06/2.67e+08 =  2% of the original kernel matrix.

torch.Size([29069, 2])
We keep 1.34e+07/4.65e+08 =  2% of the original kernel matrix.

torch.Size([24473, 2])
We keep 6.73e+06/3.17e+08 =  2% of the original kernel matrix.

torch.Size([17220, 2])
We keep 6.68e+06/1.71e+08 =  3% of the original kernel matrix.

torch.Size([18457, 2])
We keep 4.45e+06/1.92e+08 =  2% of the original kernel matrix.

torch.Size([15771, 2])
We keep 4.69e+06/1.18e+08 =  3% of the original kernel matrix.

torch.Size([17972, 2])
We keep 3.83e+06/1.60e+08 =  2% of the original kernel matrix.

torch.Size([3789, 2])
We keep 2.96e+05/4.30e+06 =  6% of the original kernel matrix.

torch.Size([8777, 2])
We keep 1.12e+06/3.05e+07 =  3% of the original kernel matrix.

torch.Size([8319, 2])
We keep 1.33e+06/2.80e+07 =  4% of the original kernel matrix.

torch.Size([12748, 2])
We keep 2.30e+06/7.78e+07 =  2% of the original kernel matrix.

torch.Size([111930, 2])
We keep 1.09e+08/6.58e+09 =  1% of the original kernel matrix.

torch.Size([48007, 2])
We keep 2.08e+07/1.19e+09 =  1% of the original kernel matrix.

torch.Size([10058, 2])
We keep 2.86e+06/4.47e+07 =  6% of the original kernel matrix.

torch.Size([14114, 2])
We keep 2.70e+06/9.82e+07 =  2% of the original kernel matrix.

torch.Size([49958, 2])
We keep 2.72e+07/1.25e+09 =  2% of the original kernel matrix.

torch.Size([31069, 2])
We keep 1.03e+07/5.19e+08 =  1% of the original kernel matrix.

torch.Size([2414, 2])
We keep 1.38e+05/1.33e+06 = 10% of the original kernel matrix.

torch.Size([7540, 2])
We keep 7.50e+05/1.69e+07 =  4% of the original kernel matrix.

torch.Size([2824, 2])
We keep 1.66e+05/1.97e+06 =  8% of the original kernel matrix.

torch.Size([7916, 2])
We keep 8.67e+05/2.06e+07 =  4% of the original kernel matrix.

torch.Size([84194, 2])
We keep 1.32e+08/4.50e+09 =  2% of the original kernel matrix.

torch.Size([40640, 2])
We keep 1.78e+07/9.86e+08 =  1% of the original kernel matrix.

torch.Size([67793, 2])
We keep 4.96e+07/2.40e+09 =  2% of the original kernel matrix.

torch.Size([36136, 2])
We keep 1.36e+07/7.20e+08 =  1% of the original kernel matrix.

torch.Size([23576, 2])
We keep 1.05e+07/2.94e+08 =  3% of the original kernel matrix.

torch.Size([21999, 2])
We keep 5.76e+06/2.52e+08 =  2% of the original kernel matrix.

torch.Size([4690, 2])
We keep 3.79e+05/6.25e+06 =  6% of the original kernel matrix.

torch.Size([9736, 2])
We keep 1.29e+06/3.67e+07 =  3% of the original kernel matrix.

torch.Size([3953, 2])
We keep 2.64e+05/4.04e+06 =  6% of the original kernel matrix.

torch.Size([9040, 2])
We keep 1.10e+06/2.95e+07 =  3% of the original kernel matrix.

torch.Size([16160, 2])
We keep 5.13e+06/1.29e+08 =  3% of the original kernel matrix.

torch.Size([18421, 2])
We keep 4.15e+06/1.67e+08 =  2% of the original kernel matrix.

torch.Size([13942, 2])
We keep 3.69e+06/9.53e+07 =  3% of the original kernel matrix.

torch.Size([16716, 2])
We keep 3.56e+06/1.43e+08 =  2% of the original kernel matrix.

torch.Size([53514, 2])
We keep 4.29e+07/1.58e+09 =  2% of the original kernel matrix.

torch.Size([32167, 2])
We keep 1.15e+07/5.84e+08 =  1% of the original kernel matrix.

torch.Size([4614, 2])
We keep 8.86e+05/8.37e+06 = 10% of the original kernel matrix.

torch.Size([9664, 2])
We keep 1.42e+06/4.25e+07 =  3% of the original kernel matrix.

torch.Size([11289, 2])
We keep 2.54e+06/6.04e+07 =  4% of the original kernel matrix.

torch.Size([14770, 2])
We keep 3.02e+06/1.14e+08 =  2% of the original kernel matrix.

torch.Size([13806, 2])
We keep 6.49e+06/1.29e+08 =  5% of the original kernel matrix.

torch.Size([16584, 2])
We keep 4.13e+06/1.67e+08 =  2% of the original kernel matrix.

torch.Size([12639, 2])
We keep 3.50e+06/7.69e+07 =  4% of the original kernel matrix.

torch.Size([15683, 2])
We keep 3.34e+06/1.29e+08 =  2% of the original kernel matrix.

torch.Size([92829, 2])
We keep 8.30e+07/4.57e+09 =  1% of the original kernel matrix.

torch.Size([43443, 2])
We keep 1.77e+07/9.94e+08 =  1% of the original kernel matrix.

torch.Size([7251, 2])
We keep 1.72e+06/1.76e+07 =  9% of the original kernel matrix.

torch.Size([11975, 2])
We keep 1.84e+06/6.16e+07 =  2% of the original kernel matrix.

torch.Size([41817, 2])
We keep 2.57e+07/9.77e+08 =  2% of the original kernel matrix.

torch.Size([28544, 2])
We keep 9.37e+06/4.59e+08 =  2% of the original kernel matrix.

torch.Size([155799, 2])
We keep 3.08e+08/1.63e+10 =  1% of the original kernel matrix.

torch.Size([57927, 2])
We keep 3.19e+07/1.88e+09 =  1% of the original kernel matrix.

torch.Size([30717, 2])
We keep 1.16e+07/4.80e+08 =  2% of the original kernel matrix.

torch.Size([25158, 2])
We keep 6.98e+06/3.22e+08 =  2% of the original kernel matrix.

torch.Size([32847, 2])
We keep 2.58e+07/5.83e+08 =  4% of the original kernel matrix.

torch.Size([26010, 2])
We keep 7.67e+06/3.55e+08 =  2% of the original kernel matrix.

torch.Size([78598, 2])
We keep 5.33e+07/3.07e+09 =  1% of the original kernel matrix.

torch.Size([39630, 2])
We keep 1.52e+07/8.14e+08 =  1% of the original kernel matrix.

torch.Size([2218, 2])
We keep 8.69e+04/1.02e+06 =  8% of the original kernel matrix.

torch.Size([7291, 2])
We keep 6.87e+05/1.49e+07 =  4% of the original kernel matrix.

torch.Size([20545, 2])
We keep 7.62e+06/2.35e+08 =  3% of the original kernel matrix.

torch.Size([20490, 2])
We keep 5.16e+06/2.25e+08 =  2% of the original kernel matrix.

torch.Size([12339, 2])
We keep 3.00e+06/6.72e+07 =  4% of the original kernel matrix.

torch.Size([15729, 2])
We keep 3.18e+06/1.20e+08 =  2% of the original kernel matrix.

torch.Size([173869, 2])
We keep 2.55e+08/1.72e+10 =  1% of the original kernel matrix.

torch.Size([61651, 2])
We keep 3.21e+07/1.93e+09 =  1% of the original kernel matrix.

torch.Size([39829, 2])
We keep 2.89e+07/9.56e+08 =  3% of the original kernel matrix.

torch.Size([28026, 2])
We keep 9.22e+06/4.54e+08 =  2% of the original kernel matrix.

torch.Size([25963, 2])
We keep 1.58e+07/4.29e+08 =  3% of the original kernel matrix.

torch.Size([22317, 2])
We keep 6.66e+06/3.04e+08 =  2% of the original kernel matrix.

torch.Size([61871, 2])
We keep 5.15e+07/1.93e+09 =  2% of the original kernel matrix.

torch.Size([34878, 2])
We keep 1.23e+07/6.46e+08 =  1% of the original kernel matrix.

torch.Size([17119, 2])
We keep 7.09e+06/1.45e+08 =  4% of the original kernel matrix.

torch.Size([18279, 2])
We keep 4.20e+06/1.77e+08 =  2% of the original kernel matrix.

torch.Size([2534, 2])
We keep 1.79e+05/1.58e+06 = 11% of the original kernel matrix.

torch.Size([7563, 2])
We keep 7.51e+05/1.85e+07 =  4% of the original kernel matrix.

torch.Size([4213, 2])
We keep 3.11e+05/4.75e+06 =  6% of the original kernel matrix.

torch.Size([9375, 2])
We keep 1.17e+06/3.20e+07 =  3% of the original kernel matrix.

torch.Size([12121, 2])
We keep 3.24e+06/6.99e+07 =  4% of the original kernel matrix.

torch.Size([15311, 2])
We keep 3.29e+06/1.23e+08 =  2% of the original kernel matrix.

torch.Size([87391, 2])
We keep 9.84e+07/4.05e+09 =  2% of the original kernel matrix.

torch.Size([41976, 2])
We keep 1.69e+07/9.35e+08 =  1% of the original kernel matrix.

torch.Size([255718, 2])
We keep 7.36e+08/4.75e+10 =  1% of the original kernel matrix.

torch.Size([75885, 2])
We keep 5.12e+07/3.20e+09 =  1% of the original kernel matrix.

torch.Size([32291, 2])
We keep 3.38e+07/9.36e+08 =  3% of the original kernel matrix.

torch.Size([24892, 2])
We keep 9.38e+06/4.50e+08 =  2% of the original kernel matrix.

torch.Size([9227, 2])
We keep 1.42e+06/3.34e+07 =  4% of the original kernel matrix.

torch.Size([13520, 2])
We keep 2.36e+06/8.50e+07 =  2% of the original kernel matrix.

torch.Size([13748, 2])
We keep 2.69e+06/8.44e+07 =  3% of the original kernel matrix.

torch.Size([16726, 2])
We keep 3.41e+06/1.35e+08 =  2% of the original kernel matrix.

torch.Size([307409, 2])
We keep 1.07e+09/6.70e+10 =  1% of the original kernel matrix.

torch.Size([80988, 2])
We keep 6.03e+07/3.80e+09 =  1% of the original kernel matrix.

torch.Size([12859, 2])
We keep 2.82e+06/7.28e+07 =  3% of the original kernel matrix.

torch.Size([16073, 2])
We keep 3.27e+06/1.25e+08 =  2% of the original kernel matrix.

torch.Size([7612, 2])
We keep 1.13e+06/2.15e+07 =  5% of the original kernel matrix.

torch.Size([12117, 2])
We keep 1.99e+06/6.82e+07 =  2% of the original kernel matrix.

torch.Size([35051, 2])
We keep 1.83e+07/7.82e+08 =  2% of the original kernel matrix.

torch.Size([26824, 2])
We keep 8.60e+06/4.11e+08 =  2% of the original kernel matrix.

torch.Size([39403, 2])
We keep 1.75e+07/8.23e+08 =  2% of the original kernel matrix.

torch.Size([27956, 2])
We keep 8.55e+06/4.22e+08 =  2% of the original kernel matrix.

torch.Size([7065, 2])
We keep 1.18e+06/1.85e+07 =  6% of the original kernel matrix.

torch.Size([11623, 2])
We keep 1.91e+06/6.33e+07 =  3% of the original kernel matrix.

torch.Size([11513, 2])
We keep 2.35e+06/5.17e+07 =  4% of the original kernel matrix.

torch.Size([15127, 2])
We keep 2.83e+06/1.06e+08 =  2% of the original kernel matrix.

torch.Size([15814, 2])
We keep 1.06e+07/1.40e+08 =  7% of the original kernel matrix.

torch.Size([17095, 2])
We keep 3.79e+06/1.74e+08 =  2% of the original kernel matrix.

time for making ranges is 2.175949811935425
Sorting X and nu_X
time for sorting X is 0.06942486763000488
Sorting Z and nu_Z
time for sorting Z is 0.0002636909484863281
Starting Optim
sum tnu_Z before tensor(23125180., device='cuda:0')
c= tensor(513.8250, device='cuda:0')
c= tensor(59795.7148, device='cuda:0')
c= tensor(61503.0273, device='cuda:0')
c= tensor(64394.9688, device='cuda:0')
c= tensor(2006032.2500, device='cuda:0')
c= tensor(2255457.2500, device='cuda:0')
c= tensor(2704085.5000, device='cuda:0')
c= tensor(2966089., device='cuda:0')
c= tensor(3046574.2500, device='cuda:0')
c= tensor(8896515., device='cuda:0')
c= tensor(8953334., device='cuda:0')
c= tensor(9802199., device='cuda:0')
c= tensor(9811597., device='cuda:0')
c= tensor(16424948., device='cuda:0')
c= tensor(16535355., device='cuda:0')
c= tensor(16656794., device='cuda:0')
c= tensor(16952710., device='cuda:0')
c= tensor(17638812., device='cuda:0')
c= tensor(21493586., device='cuda:0')
c= tensor(23882112., device='cuda:0')
c= tensor(24052542., device='cuda:0')
c= tensor(28603470., device='cuda:0')
c= tensor(28631404., device='cuda:0')
c= tensor(28660546., device='cuda:0')
c= tensor(29119166., device='cuda:0')
c= tensor(29649816., device='cuda:0')
c= tensor(30781942., device='cuda:0')
c= tensor(30830868., device='cuda:0')
c= tensor(36240400., device='cuda:0')
c= tensor(2.1220e+08, device='cuda:0')
c= tensor(2.1223e+08, device='cuda:0')
c= tensor(3.3510e+08, device='cuda:0')
c= tensor(3.3512e+08, device='cuda:0')
c= tensor(3.3525e+08, device='cuda:0')
c= tensor(3.3559e+08, device='cuda:0')
c= tensor(3.5302e+08, device='cuda:0')
c= tensor(3.5357e+08, device='cuda:0')
c= tensor(3.5357e+08, device='cuda:0')
c= tensor(3.5357e+08, device='cuda:0')
c= tensor(3.5358e+08, device='cuda:0')
c= tensor(3.5358e+08, device='cuda:0')
c= tensor(3.5358e+08, device='cuda:0')
c= tensor(3.5358e+08, device='cuda:0')
c= tensor(3.5358e+08, device='cuda:0')
c= tensor(3.5358e+08, device='cuda:0')
c= tensor(3.5358e+08, device='cuda:0')
c= tensor(3.5358e+08, device='cuda:0')
c= tensor(3.5359e+08, device='cuda:0')
c= tensor(3.5360e+08, device='cuda:0')
c= tensor(3.5360e+08, device='cuda:0')
c= tensor(3.5363e+08, device='cuda:0')
c= tensor(3.5363e+08, device='cuda:0')
c= tensor(3.5364e+08, device='cuda:0')
c= tensor(3.5365e+08, device='cuda:0')
c= tensor(3.5365e+08, device='cuda:0')
c= tensor(3.5365e+08, device='cuda:0')
c= tensor(3.5365e+08, device='cuda:0')
c= tensor(3.5366e+08, device='cuda:0')
c= tensor(3.5366e+08, device='cuda:0')
c= tensor(3.5366e+08, device='cuda:0')
c= tensor(3.5367e+08, device='cuda:0')
c= tensor(3.5367e+08, device='cuda:0')
c= tensor(3.5367e+08, device='cuda:0')
c= tensor(3.5370e+08, device='cuda:0')
c= tensor(3.5370e+08, device='cuda:0')
c= tensor(3.5370e+08, device='cuda:0')
c= tensor(3.5371e+08, device='cuda:0')
c= tensor(3.5371e+08, device='cuda:0')
c= tensor(3.5371e+08, device='cuda:0')
c= tensor(3.5371e+08, device='cuda:0')
c= tensor(3.5372e+08, device='cuda:0')
c= tensor(3.5372e+08, device='cuda:0')
c= tensor(3.5372e+08, device='cuda:0')
c= tensor(3.5373e+08, device='cuda:0')
c= tensor(3.5373e+08, device='cuda:0')
c= tensor(3.5376e+08, device='cuda:0')
c= tensor(3.5376e+08, device='cuda:0')
c= tensor(3.5376e+08, device='cuda:0')
c= tensor(3.5377e+08, device='cuda:0')
c= tensor(3.5378e+08, device='cuda:0')
c= tensor(3.5378e+08, device='cuda:0')
c= tensor(3.5378e+08, device='cuda:0')
c= tensor(3.5378e+08, device='cuda:0')
c= tensor(3.5378e+08, device='cuda:0')
c= tensor(3.5378e+08, device='cuda:0')
c= tensor(3.5378e+08, device='cuda:0')
c= tensor(3.5379e+08, device='cuda:0')
c= tensor(3.5379e+08, device='cuda:0')
c= tensor(3.5379e+08, device='cuda:0')
c= tensor(3.5379e+08, device='cuda:0')
c= tensor(3.5380e+08, device='cuda:0')
c= tensor(3.5380e+08, device='cuda:0')
c= tensor(3.5380e+08, device='cuda:0')
c= tensor(3.5381e+08, device='cuda:0')
c= tensor(3.5382e+08, device='cuda:0')
c= tensor(3.5383e+08, device='cuda:0')
c= tensor(3.5383e+08, device='cuda:0')
c= tensor(3.5383e+08, device='cuda:0')
c= tensor(3.5385e+08, device='cuda:0')
c= tensor(3.5386e+08, device='cuda:0')
c= tensor(3.5387e+08, device='cuda:0')
c= tensor(3.5387e+08, device='cuda:0')
c= tensor(3.5388e+08, device='cuda:0')
c= tensor(3.5388e+08, device='cuda:0')
c= tensor(3.5389e+08, device='cuda:0')
c= tensor(3.5389e+08, device='cuda:0')
c= tensor(3.5390e+08, device='cuda:0')
c= tensor(3.5390e+08, device='cuda:0')
c= tensor(3.5390e+08, device='cuda:0')
c= tensor(3.5390e+08, device='cuda:0')
c= tensor(3.5391e+08, device='cuda:0')
c= tensor(3.5391e+08, device='cuda:0')
c= tensor(3.5391e+08, device='cuda:0')
c= tensor(3.5391e+08, device='cuda:0')
c= tensor(3.5392e+08, device='cuda:0')
c= tensor(3.5392e+08, device='cuda:0')
c= tensor(3.5392e+08, device='cuda:0')
c= tensor(3.5392e+08, device='cuda:0')
c= tensor(3.5394e+08, device='cuda:0')
c= tensor(3.5394e+08, device='cuda:0')
c= tensor(3.5395e+08, device='cuda:0')
c= tensor(3.5396e+08, device='cuda:0')
c= tensor(3.5396e+08, device='cuda:0')
c= tensor(3.5397e+08, device='cuda:0')
c= tensor(3.5397e+08, device='cuda:0')
c= tensor(3.5397e+08, device='cuda:0')
c= tensor(3.5397e+08, device='cuda:0')
c= tensor(3.5398e+08, device='cuda:0')
c= tensor(3.5399e+08, device='cuda:0')
c= tensor(3.5399e+08, device='cuda:0')
c= tensor(3.5402e+08, device='cuda:0')
c= tensor(3.5402e+08, device='cuda:0')
c= tensor(3.5402e+08, device='cuda:0')
c= tensor(3.5402e+08, device='cuda:0')
c= tensor(3.5402e+08, device='cuda:0')
c= tensor(3.5402e+08, device='cuda:0')
c= tensor(3.5403e+08, device='cuda:0')
c= tensor(3.5403e+08, device='cuda:0')
c= tensor(3.5403e+08, device='cuda:0')
c= tensor(3.5403e+08, device='cuda:0')
c= tensor(3.5403e+08, device='cuda:0')
c= tensor(3.5403e+08, device='cuda:0')
c= tensor(3.5405e+08, device='cuda:0')
c= tensor(3.5406e+08, device='cuda:0')
c= tensor(3.5407e+08, device='cuda:0')
c= tensor(3.5407e+08, device='cuda:0')
c= tensor(3.5407e+08, device='cuda:0')
c= tensor(3.5408e+08, device='cuda:0')
c= tensor(3.5408e+08, device='cuda:0')
c= tensor(3.5408e+08, device='cuda:0')
c= tensor(3.5408e+08, device='cuda:0')
c= tensor(3.5408e+08, device='cuda:0')
c= tensor(3.5409e+08, device='cuda:0')
c= tensor(3.5409e+08, device='cuda:0')
c= tensor(3.5409e+08, device='cuda:0')
c= tensor(3.5412e+08, device='cuda:0')
c= tensor(3.5412e+08, device='cuda:0')
c= tensor(3.5412e+08, device='cuda:0')
c= tensor(3.5412e+08, device='cuda:0')
c= tensor(3.5412e+08, device='cuda:0')
c= tensor(3.5414e+08, device='cuda:0')
c= tensor(3.5414e+08, device='cuda:0')
c= tensor(3.5415e+08, device='cuda:0')
c= tensor(3.5415e+08, device='cuda:0')
c= tensor(3.5416e+08, device='cuda:0')
c= tensor(3.5416e+08, device='cuda:0')
c= tensor(3.5417e+08, device='cuda:0')
c= tensor(3.5417e+08, device='cuda:0')
c= tensor(3.5417e+08, device='cuda:0')
c= tensor(3.5417e+08, device='cuda:0')
c= tensor(3.5417e+08, device='cuda:0')
c= tensor(3.5417e+08, device='cuda:0')
c= tensor(3.5418e+08, device='cuda:0')
c= tensor(3.5418e+08, device='cuda:0')
c= tensor(3.5418e+08, device='cuda:0')
c= tensor(3.5418e+08, device='cuda:0')
c= tensor(3.5419e+08, device='cuda:0')
c= tensor(3.5419e+08, device='cuda:0')
c= tensor(3.5419e+08, device='cuda:0')
c= tensor(3.5419e+08, device='cuda:0')
c= tensor(3.5421e+08, device='cuda:0')
c= tensor(3.5421e+08, device='cuda:0')
c= tensor(3.5421e+08, device='cuda:0')
c= tensor(3.5421e+08, device='cuda:0')
c= tensor(3.5421e+08, device='cuda:0')
c= tensor(3.5422e+08, device='cuda:0')
c= tensor(3.5422e+08, device='cuda:0')
c= tensor(3.5423e+08, device='cuda:0')
c= tensor(3.5423e+08, device='cuda:0')
c= tensor(3.5425e+08, device='cuda:0')
c= tensor(3.5425e+08, device='cuda:0')
c= tensor(3.5425e+08, device='cuda:0')
c= tensor(3.5425e+08, device='cuda:0')
c= tensor(3.5425e+08, device='cuda:0')
c= tensor(3.5426e+08, device='cuda:0')
c= tensor(3.5426e+08, device='cuda:0')
c= tensor(3.5426e+08, device='cuda:0')
c= tensor(3.5426e+08, device='cuda:0')
c= tensor(3.5427e+08, device='cuda:0')
c= tensor(3.5427e+08, device='cuda:0')
c= tensor(3.5428e+08, device='cuda:0')
c= tensor(3.5428e+08, device='cuda:0')
c= tensor(3.5431e+08, device='cuda:0')
c= tensor(3.5431e+08, device='cuda:0')
c= tensor(3.5432e+08, device='cuda:0')
c= tensor(3.5432e+08, device='cuda:0')
c= tensor(3.5432e+08, device='cuda:0')
c= tensor(3.5432e+08, device='cuda:0')
c= tensor(3.5433e+08, device='cuda:0')
c= tensor(3.5433e+08, device='cuda:0')
c= tensor(3.5435e+08, device='cuda:0')
c= tensor(3.5435e+08, device='cuda:0')
c= tensor(3.5435e+08, device='cuda:0')
c= tensor(3.5435e+08, device='cuda:0')
c= tensor(3.5435e+08, device='cuda:0')
c= tensor(3.5435e+08, device='cuda:0')
c= tensor(3.5435e+08, device='cuda:0')
c= tensor(3.5436e+08, device='cuda:0')
c= tensor(3.5439e+08, device='cuda:0')
c= tensor(3.5439e+08, device='cuda:0')
c= tensor(3.5439e+08, device='cuda:0')
c= tensor(3.5439e+08, device='cuda:0')
c= tensor(3.5441e+08, device='cuda:0')
c= tensor(3.5441e+08, device='cuda:0')
c= tensor(3.5441e+08, device='cuda:0')
c= tensor(3.5442e+08, device='cuda:0')
c= tensor(3.5442e+08, device='cuda:0')
c= tensor(3.5442e+08, device='cuda:0')
c= tensor(3.5443e+08, device='cuda:0')
c= tensor(3.5443e+08, device='cuda:0')
c= tensor(3.5443e+08, device='cuda:0')
c= tensor(3.5443e+08, device='cuda:0')
c= tensor(3.5443e+08, device='cuda:0')
c= tensor(3.5443e+08, device='cuda:0')
c= tensor(3.5444e+08, device='cuda:0')
c= tensor(3.5444e+08, device='cuda:0')
c= tensor(3.5444e+08, device='cuda:0')
c= tensor(3.5445e+08, device='cuda:0')
c= tensor(3.5445e+08, device='cuda:0')
c= tensor(3.5446e+08, device='cuda:0')
c= tensor(3.5448e+08, device='cuda:0')
c= tensor(3.5455e+08, device='cuda:0')
c= tensor(3.5455e+08, device='cuda:0')
c= tensor(3.5456e+08, device='cuda:0')
c= tensor(3.5457e+08, device='cuda:0')
c= tensor(3.5477e+08, device='cuda:0')
c= tensor(3.5482e+08, device='cuda:0')
c= tensor(3.5482e+08, device='cuda:0')
c= tensor(3.5742e+08, device='cuda:0')
c= tensor(3.5838e+08, device='cuda:0')
c= tensor(3.5844e+08, device='cuda:0')
c= tensor(3.5969e+08, device='cuda:0')
c= tensor(3.5969e+08, device='cuda:0')
c= tensor(3.5974e+08, device='cuda:0')
c= tensor(3.7008e+08, device='cuda:0')
c= tensor(3.8287e+08, device='cuda:0')
c= tensor(3.8287e+08, device='cuda:0')
c= tensor(3.8298e+08, device='cuda:0')
c= tensor(3.8538e+08, device='cuda:0')
c= tensor(3.8561e+08, device='cuda:0')
c= tensor(3.8571e+08, device='cuda:0')
c= tensor(3.8592e+08, device='cuda:0')
c= tensor(3.8599e+08, device='cuda:0')
c= tensor(3.8603e+08, device='cuda:0')
c= tensor(3.8609e+08, device='cuda:0')
c= tensor(3.8982e+08, device='cuda:0')
c= tensor(3.8986e+08, device='cuda:0')
c= tensor(3.8987e+08, device='cuda:0')
c= tensor(3.8997e+08, device='cuda:0')
c= tensor(3.9004e+08, device='cuda:0')
c= tensor(4.0150e+08, device='cuda:0')
c= tensor(4.0197e+08, device='cuda:0')
c= tensor(4.0198e+08, device='cuda:0')
c= tensor(4.0209e+08, device='cuda:0')
c= tensor(4.0210e+08, device='cuda:0')
c= tensor(4.0218e+08, device='cuda:0')
c= tensor(4.0294e+08, device='cuda:0')
c= tensor(4.0835e+08, device='cuda:0')
c= tensor(4.1069e+08, device='cuda:0')
c= tensor(4.1069e+08, device='cuda:0')
c= tensor(4.1069e+08, device='cuda:0')
c= tensor(4.1132e+08, device='cuda:0')
c= tensor(4.1275e+08, device='cuda:0')
c= tensor(4.1293e+08, device='cuda:0')
c= tensor(4.1293e+08, device='cuda:0')
c= tensor(4.2286e+08, device='cuda:0')
c= tensor(4.2288e+08, device='cuda:0')
c= tensor(4.2295e+08, device='cuda:0')
c= tensor(4.2372e+08, device='cuda:0')
c= tensor(4.2372e+08, device='cuda:0')
c= tensor(4.2402e+08, device='cuda:0')
c= tensor(4.2460e+08, device='cuda:0')
c= tensor(4.4192e+08, device='cuda:0')
c= tensor(4.4207e+08, device='cuda:0')
c= tensor(4.4222e+08, device='cuda:0')
c= tensor(4.4223e+08, device='cuda:0')
c= tensor(4.4224e+08, device='cuda:0')
c= tensor(4.4236e+08, device='cuda:0')
c= tensor(4.4238e+08, device='cuda:0')
c= tensor(4.4247e+08, device='cuda:0')
c= tensor(4.4817e+08, device='cuda:0')
c= tensor(4.4828e+08, device='cuda:0')
c= tensor(4.4830e+08, device='cuda:0')
c= tensor(4.4831e+08, device='cuda:0')
c= tensor(4.7720e+08, device='cuda:0')
c= tensor(4.7724e+08, device='cuda:0')
c= tensor(4.7724e+08, device='cuda:0')
c= tensor(4.7727e+08, device='cuda:0')
c= tensor(4.8159e+08, device='cuda:0')
c= tensor(4.8167e+08, device='cuda:0')
c= tensor(4.8179e+08, device='cuda:0')
c= tensor(4.8180e+08, device='cuda:0')
c= tensor(4.8205e+08, device='cuda:0')
c= tensor(4.8232e+08, device='cuda:0')
c= tensor(5.2599e+08, device='cuda:0')
c= tensor(5.2641e+08, device='cuda:0')
c= tensor(5.2644e+08, device='cuda:0')
c= tensor(5.2802e+08, device='cuda:0')
c= tensor(5.2937e+08, device='cuda:0')
c= tensor(5.2940e+08, device='cuda:0')
c= tensor(5.3055e+08, device='cuda:0')
c= tensor(5.3430e+08, device='cuda:0')
c= tensor(5.8450e+08, device='cuda:0')
c= tensor(5.8481e+08, device='cuda:0')
c= tensor(5.8481e+08, device='cuda:0')
c= tensor(5.8481e+08, device='cuda:0')
c= tensor(5.8536e+08, device='cuda:0')
c= tensor(5.8557e+08, device='cuda:0')
c= tensor(5.8561e+08, device='cuda:0')
c= tensor(5.8561e+08, device='cuda:0')
c= tensor(5.8572e+08, device='cuda:0')
c= tensor(5.8677e+08, device='cuda:0')
c= tensor(5.9128e+08, device='cuda:0')
c= tensor(5.9128e+08, device='cuda:0')
c= tensor(5.9491e+08, device='cuda:0')
c= tensor(5.9493e+08, device='cuda:0')
c= tensor(5.9497e+08, device='cuda:0')
c= tensor(5.9500e+08, device='cuda:0')
c= tensor(5.9501e+08, device='cuda:0')
c= tensor(6.0912e+08, device='cuda:0')
c= tensor(6.0936e+08, device='cuda:0')
c= tensor(6.0940e+08, device='cuda:0')
c= tensor(6.0957e+08, device='cuda:0')
c= tensor(6.0958e+08, device='cuda:0')
c= tensor(6.3227e+08, device='cuda:0')
c= tensor(6.3231e+08, device='cuda:0')
c= tensor(6.3788e+08, device='cuda:0')
c= tensor(6.3788e+08, device='cuda:0')
c= tensor(6.3788e+08, device='cuda:0')
c= tensor(6.3788e+08, device='cuda:0')
c= tensor(6.3811e+08, device='cuda:0')
c= tensor(6.3812e+08, device='cuda:0')
c= tensor(6.3813e+08, device='cuda:0')
c= tensor(6.3814e+08, device='cuda:0')
c= tensor(6.3815e+08, device='cuda:0')
c= tensor(6.4769e+08, device='cuda:0')
c= tensor(6.4784e+08, device='cuda:0')
c= tensor(6.4797e+08, device='cuda:0')
c= tensor(6.4892e+08, device='cuda:0')
c= tensor(6.5808e+08, device='cuda:0')
c= tensor(6.5810e+08, device='cuda:0')
c= tensor(6.5811e+08, device='cuda:0')
c= tensor(6.5812e+08, device='cuda:0')
c= tensor(6.5812e+08, device='cuda:0')
c= tensor(6.5814e+08, device='cuda:0')
c= tensor(6.5822e+08, device='cuda:0')
c= tensor(6.5823e+08, device='cuda:0')
c= tensor(6.5824e+08, device='cuda:0')
c= tensor(6.5828e+08, device='cuda:0')
c= tensor(6.5831e+08, device='cuda:0')
c= tensor(7.6348e+08, device='cuda:0')
c= tensor(7.6349e+08, device='cuda:0')
c= tensor(7.6376e+08, device='cuda:0')
c= tensor(7.6387e+08, device='cuda:0')
c= tensor(7.6387e+08, device='cuda:0')
c= tensor(7.6557e+08, device='cuda:0')
c= tensor(7.9911e+08, device='cuda:0')
c= tensor(8.3012e+08, device='cuda:0')
c= tensor(8.3060e+08, device='cuda:0')
c= tensor(8.3071e+08, device='cuda:0')
c= tensor(8.3071e+08, device='cuda:0')
c= tensor(8.3211e+08, device='cuda:0')
c= tensor(9.0562e+08, device='cuda:0')
c= tensor(9.0618e+08, device='cuda:0')
c= tensor(9.0628e+08, device='cuda:0')
c= tensor(9.0769e+08, device='cuda:0')
c= tensor(9.1904e+08, device='cuda:0')
c= tensor(9.1946e+08, device='cuda:0')
c= tensor(9.1949e+08, device='cuda:0')
c= tensor(9.1950e+08, device='cuda:0')
c= tensor(9.1958e+08, device='cuda:0')
c= tensor(9.1967e+08, device='cuda:0')
c= tensor(9.4534e+08, device='cuda:0')
c= tensor(9.4541e+08, device='cuda:0')
c= tensor(9.4542e+08, device='cuda:0')
c= tensor(9.4548e+08, device='cuda:0')
c= tensor(9.4569e+08, device='cuda:0')
c= tensor(9.4569e+08, device='cuda:0')
c= tensor(9.4619e+08, device='cuda:0')
c= tensor(9.4683e+08, device='cuda:0')
c= tensor(9.4720e+08, device='cuda:0')
c= tensor(9.4786e+08, device='cuda:0')
c= tensor(9.4877e+08, device='cuda:0')
c= tensor(9.4884e+08, device='cuda:0')
c= tensor(9.4902e+08, device='cuda:0')
c= tensor(9.4939e+08, device='cuda:0')
c= tensor(9.5218e+08, device='cuda:0')
c= tensor(9.5219e+08, device='cuda:0')
c= tensor(9.6551e+08, device='cuda:0')
c= tensor(9.7168e+08, device='cuda:0')
c= tensor(9.7259e+08, device='cuda:0')
c= tensor(9.7309e+08, device='cuda:0')
c= tensor(9.7319e+08, device='cuda:0')
c= tensor(9.7322e+08, device='cuda:0')
c= tensor(9.7324e+08, device='cuda:0')
c= tensor(9.7809e+08, device='cuda:0')
c= tensor(9.8409e+08, device='cuda:0')
c= tensor(9.8583e+08, device='cuda:0')
c= tensor(9.9363e+08, device='cuda:0')
c= tensor(9.9422e+08, device='cuda:0')
c= tensor(9.9610e+08, device='cuda:0')
c= tensor(9.9620e+08, device='cuda:0')
c= tensor(9.9995e+08, device='cuda:0')
c= tensor(9.9996e+08, device='cuda:0')
c= tensor(9.9998e+08, device='cuda:0')
c= tensor(1.0072e+09, device='cuda:0')
c= tensor(1.0073e+09, device='cuda:0')
c= tensor(1.0073e+09, device='cuda:0')
c= tensor(1.0075e+09, device='cuda:0')
c= tensor(1.0101e+09, device='cuda:0')
c= tensor(1.0105e+09, device='cuda:0')
c= tensor(1.0108e+09, device='cuda:0')
c= tensor(1.0108e+09, device='cuda:0')
c= tensor(1.0108e+09, device='cuda:0')
c= tensor(1.0108e+09, device='cuda:0')
c= tensor(1.0119e+09, device='cuda:0')
c= tensor(1.0120e+09, device='cuda:0')
c= tensor(1.0123e+09, device='cuda:0')
c= tensor(1.0123e+09, device='cuda:0')
c= tensor(1.0134e+09, device='cuda:0')
c= tensor(1.0134e+09, device='cuda:0')
c= tensor(1.0135e+09, device='cuda:0')
c= tensor(1.0138e+09, device='cuda:0')
c= tensor(1.0142e+09, device='cuda:0')
c= tensor(1.0142e+09, device='cuda:0')
c= tensor(1.0147e+09, device='cuda:0')
c= tensor(1.0149e+09, device='cuda:0')
c= tensor(1.0150e+09, device='cuda:0')
c= tensor(1.0152e+09, device='cuda:0')
c= tensor(1.0288e+09, device='cuda:0')
c= tensor(1.0288e+09, device='cuda:0')
c= tensor(1.0289e+09, device='cuda:0')
c= tensor(1.0324e+09, device='cuda:0')
c= tensor(1.0324e+09, device='cuda:0')
c= tensor(1.0535e+09, device='cuda:0')
c= tensor(1.0535e+09, device='cuda:0')
c= tensor(1.0562e+09, device='cuda:0')
c= tensor(1.0662e+09, device='cuda:0')
c= tensor(1.0662e+09, device='cuda:0')
c= tensor(1.0696e+09, device='cuda:0')
c= tensor(1.0699e+09, device='cuda:0')
c= tensor(1.0724e+09, device='cuda:0')
c= tensor(1.0725e+09, device='cuda:0')
c= tensor(1.0727e+09, device='cuda:0')
c= tensor(1.0727e+09, device='cuda:0')
c= tensor(1.0727e+09, device='cuda:0')
c= tensor(1.0727e+09, device='cuda:0')
c= tensor(1.0729e+09, device='cuda:0')
c= tensor(1.0730e+09, device='cuda:0')
c= tensor(1.0767e+09, device='cuda:0')
c= tensor(1.0767e+09, device='cuda:0')
c= tensor(1.0767e+09, device='cuda:0')
c= tensor(1.0768e+09, device='cuda:0')
c= tensor(1.0768e+09, device='cuda:0')
c= tensor(1.0768e+09, device='cuda:0')
c= tensor(1.0799e+09, device='cuda:0')
c= tensor(1.0805e+09, device='cuda:0')
c= tensor(1.0805e+09, device='cuda:0')
c= tensor(1.0805e+09, device='cuda:0')
c= tensor(1.0805e+09, device='cuda:0')
c= tensor(1.1225e+09, device='cuda:0')
c= tensor(1.1225e+09, device='cuda:0')
c= tensor(1.1225e+09, device='cuda:0')
c= tensor(1.1240e+09, device='cuda:0')
c= tensor(1.1245e+09, device='cuda:0')
c= tensor(1.1245e+09, device='cuda:0')
c= tensor(1.1246e+09, device='cuda:0')
c= tensor(1.1257e+09, device='cuda:0')
c= tensor(1.1279e+09, device='cuda:0')
c= tensor(1.1286e+09, device='cuda:0')
c= tensor(1.1286e+09, device='cuda:0')
c= tensor(1.1287e+09, device='cuda:0')
c= tensor(1.1290e+09, device='cuda:0')
c= tensor(1.1296e+09, device='cuda:0')
c= tensor(1.1304e+09, device='cuda:0')
c= tensor(1.1304e+09, device='cuda:0')
c= tensor(1.1324e+09, device='cuda:0')
c= tensor(1.1324e+09, device='cuda:0')
c= tensor(1.1327e+09, device='cuda:0')
c= tensor(1.1328e+09, device='cuda:0')
c= tensor(1.1328e+09, device='cuda:0')
c= tensor(1.1328e+09, device='cuda:0')
c= tensor(1.1333e+09, device='cuda:0')
c= tensor(1.1333e+09, device='cuda:0')
c= tensor(1.1335e+09, device='cuda:0')
c= tensor(1.1337e+09, device='cuda:0')
c= tensor(1.1537e+09, device='cuda:0')
c= tensor(1.1537e+09, device='cuda:0')
c= tensor(1.1537e+09, device='cuda:0')
c= tensor(1.1538e+09, device='cuda:0')
c= tensor(1.1538e+09, device='cuda:0')
c= tensor(1.1538e+09, device='cuda:0')
c= tensor(1.1539e+09, device='cuda:0')
c= tensor(1.1540e+09, device='cuda:0')
c= tensor(1.1575e+09, device='cuda:0')
c= tensor(1.1575e+09, device='cuda:0')
c= tensor(1.1575e+09, device='cuda:0')
c= tensor(1.1576e+09, device='cuda:0')
c= tensor(1.1720e+09, device='cuda:0')
c= tensor(1.2894e+09, device='cuda:0')
c= tensor(1.2895e+09, device='cuda:0')
c= tensor(1.2895e+09, device='cuda:0')
c= tensor(1.2981e+09, device='cuda:0')
c= tensor(1.2994e+09, device='cuda:0')
c= tensor(1.2994e+09, device='cuda:0')
c= tensor(1.2995e+09, device='cuda:0')
c= tensor(1.2997e+09, device='cuda:0')
c= tensor(1.3432e+09, device='cuda:0')
c= tensor(1.3813e+09, device='cuda:0')
c= tensor(1.3836e+09, device='cuda:0')
c= tensor(1.3836e+09, device='cuda:0')
c= tensor(1.3836e+09, device='cuda:0')
c= tensor(1.3836e+09, device='cuda:0')
c= tensor(1.3845e+09, device='cuda:0')
c= tensor(1.3845e+09, device='cuda:0')
c= tensor(1.3847e+09, device='cuda:0')
c= tensor(1.3878e+09, device='cuda:0')
c= tensor(1.4055e+09, device='cuda:0')
c= tensor(1.4055e+09, device='cuda:0')
c= tensor(1.4055e+09, device='cuda:0')
c= tensor(1.4056e+09, device='cuda:0')
c= tensor(1.4075e+09, device='cuda:0')
c= tensor(1.4083e+09, device='cuda:0')
c= tensor(1.4083e+09, device='cuda:0')
c= tensor(1.4083e+09, device='cuda:0')
c= tensor(1.4084e+09, device='cuda:0')
c= tensor(1.4084e+09, device='cuda:0')
c= tensor(1.4084e+09, device='cuda:0')
c= tensor(1.4084e+09, device='cuda:0')
c= tensor(1.4084e+09, device='cuda:0')
c= tensor(1.4085e+09, device='cuda:0')
c= tensor(1.4085e+09, device='cuda:0')
c= tensor(1.4085e+09, device='cuda:0')
c= tensor(1.4110e+09, device='cuda:0')
c= tensor(1.4212e+09, device='cuda:0')
c= tensor(1.4225e+09, device='cuda:0')
c= tensor(1.4231e+09, device='cuda:0')
c= tensor(1.4232e+09, device='cuda:0')
c= tensor(1.4233e+09, device='cuda:0')
c= tensor(1.4234e+09, device='cuda:0')
c= tensor(1.4247e+09, device='cuda:0')
c= tensor(1.4248e+09, device='cuda:0')
c= tensor(1.4250e+09, device='cuda:0')
c= tensor(1.4250e+09, device='cuda:0')
c= tensor(1.4973e+09, device='cuda:0')
c= tensor(1.4974e+09, device='cuda:0')
c= tensor(1.4979e+09, device='cuda:0')
c= tensor(1.5015e+09, device='cuda:0')
c= tensor(1.5027e+09, device='cuda:0')
c= tensor(1.5029e+09, device='cuda:0')
c= tensor(1.5100e+09, device='cuda:0')
c= tensor(1.5119e+09, device='cuda:0')
c= tensor(1.5120e+09, device='cuda:0')
c= tensor(1.5120e+09, device='cuda:0')
c= tensor(1.5122e+09, device='cuda:0')
c= tensor(1.5122e+09, device='cuda:0')
c= tensor(1.5131e+09, device='cuda:0')
c= tensor(1.5244e+09, device='cuda:0')
c= tensor(1.5255e+09, device='cuda:0')
c= tensor(1.5265e+09, device='cuda:0')
c= tensor(1.5265e+09, device='cuda:0')
c= tensor(1.5266e+09, device='cuda:0')
c= tensor(1.5267e+09, device='cuda:0')
c= tensor(1.5268e+09, device='cuda:0')
c= tensor(1.5313e+09, device='cuda:0')
c= tensor(1.5430e+09, device='cuda:0')
c= tensor(1.5430e+09, device='cuda:0')
c= tensor(1.6491e+09, device='cuda:0')
c= tensor(1.6498e+09, device='cuda:0')
c= tensor(1.6504e+09, device='cuda:0')
c= tensor(1.6504e+09, device='cuda:0')
c= tensor(1.6508e+09, device='cuda:0')
c= tensor(1.6536e+09, device='cuda:0')
c= tensor(1.6536e+09, device='cuda:0')
c= tensor(1.6875e+09, device='cuda:0')
c= tensor(1.6877e+09, device='cuda:0')
c= tensor(1.6881e+09, device='cuda:0')
c= tensor(1.6882e+09, device='cuda:0')
c= tensor(1.6882e+09, device='cuda:0')
c= tensor(1.6882e+09, device='cuda:0')
c= tensor(1.6882e+09, device='cuda:0')
c= tensor(1.6882e+09, device='cuda:0')
c= tensor(1.6884e+09, device='cuda:0')
c= tensor(1.7007e+09, device='cuda:0')
c= tensor(1.7014e+09, device='cuda:0')
c= tensor(1.7075e+09, device='cuda:0')
c= tensor(1.7075e+09, device='cuda:0')
c= tensor(1.7076e+09, device='cuda:0')
c= tensor(1.7076e+09, device='cuda:0')
c= tensor(1.7077e+09, device='cuda:0')
c= tensor(1.7102e+09, device='cuda:0')
c= tensor(1.8601e+09, device='cuda:0')
c= tensor(1.8603e+09, device='cuda:0')
c= tensor(1.8640e+09, device='cuda:0')
c= tensor(1.8641e+09, device='cuda:0')
c= tensor(1.8659e+09, device='cuda:0')
c= tensor(1.8702e+09, device='cuda:0')
c= tensor(1.8704e+09, device='cuda:0')
c= tensor(1.8704e+09, device='cuda:0')
c= tensor(1.8727e+09, device='cuda:0')
c= tensor(1.8728e+09, device='cuda:0')
c= tensor(1.8737e+09, device='cuda:0')
c= tensor(1.8788e+09, device='cuda:0')
c= tensor(1.8790e+09, device='cuda:0')
c= tensor(1.8791e+09, device='cuda:0')
c= tensor(1.8792e+09, device='cuda:0')
c= tensor(1.8815e+09, device='cuda:0')
c= tensor(1.8841e+09, device='cuda:0')
c= tensor(1.8844e+09, device='cuda:0')
c= tensor(1.8844e+09, device='cuda:0')
c= tensor(1.9029e+09, device='cuda:0')
c= tensor(1.9036e+09, device='cuda:0')
c= tensor(1.9142e+09, device='cuda:0')
c= tensor(1.9152e+09, device='cuda:0')
c= tensor(1.9173e+09, device='cuda:0')
c= tensor(1.9174e+09, device='cuda:0')
c= tensor(1.9277e+09, device='cuda:0')
c= tensor(1.9480e+09, device='cuda:0')
c= tensor(1.9485e+09, device='cuda:0')
c= tensor(1.9486e+09, device='cuda:0')
c= tensor(1.9486e+09, device='cuda:0')
c= tensor(1.9487e+09, device='cuda:0')
c= tensor(1.9494e+09, device='cuda:0')
c= tensor(1.9496e+09, device='cuda:0')
c= tensor(1.9497e+09, device='cuda:0')
c= tensor(1.9498e+09, device='cuda:0')
c= tensor(1.9534e+09, device='cuda:0')
c= tensor(1.9541e+09, device='cuda:0')
c= tensor(1.9543e+09, device='cuda:0')
c= tensor(1.9549e+09, device='cuda:0')
c= tensor(1.9555e+09, device='cuda:0')
c= tensor(1.9556e+09, device='cuda:0')
c= tensor(1.9556e+09, device='cuda:0')
c= tensor(1.9556e+09, device='cuda:0')
c= tensor(1.9591e+09, device='cuda:0')
c= tensor(1.9592e+09, device='cuda:0')
c= tensor(1.9597e+09, device='cuda:0')
c= tensor(1.9597e+09, device='cuda:0')
c= tensor(1.9597e+09, device='cuda:0')
c= tensor(1.9632e+09, device='cuda:0')
c= tensor(1.9642e+09, device='cuda:0')
c= tensor(1.9644e+09, device='cuda:0')
c= tensor(1.9644e+09, device='cuda:0')
c= tensor(1.9644e+09, device='cuda:0')
c= tensor(1.9645e+09, device='cuda:0')
c= tensor(1.9646e+09, device='cuda:0')
c= tensor(1.9662e+09, device='cuda:0')
c= tensor(1.9662e+09, device='cuda:0')
c= tensor(1.9664e+09, device='cuda:0')
c= tensor(1.9665e+09, device='cuda:0')
c= tensor(1.9666e+09, device='cuda:0')
c= tensor(1.9701e+09, device='cuda:0')
c= tensor(1.9701e+09, device='cuda:0')
c= tensor(1.9711e+09, device='cuda:0')
c= tensor(1.9832e+09, device='cuda:0')
c= tensor(1.9834e+09, device='cuda:0')
c= tensor(1.9839e+09, device='cuda:0')
c= tensor(1.9850e+09, device='cuda:0')
c= tensor(1.9850e+09, device='cuda:0')
c= tensor(1.9851e+09, device='cuda:0')
c= tensor(1.9852e+09, device='cuda:0')
c= tensor(1.9916e+09, device='cuda:0')
c= tensor(1.9924e+09, device='cuda:0')
c= tensor(1.9927e+09, device='cuda:0')
c= tensor(1.9944e+09, device='cuda:0')
c= tensor(1.9947e+09, device='cuda:0')
c= tensor(1.9947e+09, device='cuda:0')
c= tensor(1.9947e+09, device='cuda:0')
c= tensor(1.9948e+09, device='cuda:0')
c= tensor(1.9973e+09, device='cuda:0')
c= tensor(2.0195e+09, device='cuda:0')
c= tensor(2.0207e+09, device='cuda:0')
c= tensor(2.0208e+09, device='cuda:0')
c= tensor(2.0208e+09, device='cuda:0')
c= tensor(2.0527e+09, device='cuda:0')
c= tensor(2.0528e+09, device='cuda:0')
c= tensor(2.0528e+09, device='cuda:0')
c= tensor(2.0531e+09, device='cuda:0')
c= tensor(2.0535e+09, device='cuda:0')
c= tensor(2.0535e+09, device='cuda:0')
c= tensor(2.0535e+09, device='cuda:0')
c= tensor(2.0537e+09, device='cuda:0')
memory (bytes)
3929387008
time for making loss 2 is 14.189844608306885
p0 True
it  0 : 1092926976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
3929714688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  5% |
memory (bytes)
3930394624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% |  6% |
error is  38221337000.0
relative error loss 18.611137
shape of L is 
torch.Size([])
memory (bytes)
4168318976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  4% |  6% |
memory (bytes)
4168417280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  38221080000.0
relative error loss 18.611012
shape of L is 
torch.Size([])
memory (bytes)
4172496896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  6% |
memory (bytes)
4172496896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  38220510000.0
relative error loss 18.610735
shape of L is 
torch.Size([])
memory (bytes)
4174565376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4174565376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  38216800000.0
relative error loss 18.608927
shape of L is 
torch.Size([])
memory (bytes)
4176699392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% |  7% |
memory (bytes)
4176699392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  38196310000.0
relative error loss 18.598951
shape of L is 
torch.Size([])
memory (bytes)
4178862080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4178862080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  38083680000.0
relative error loss 18.544107
shape of L is 
torch.Size([])
memory (bytes)
4180959232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  0% |  7% |
memory (bytes)
4180959232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% |  7% |
error is  37468470000.0
relative error loss 18.244543
shape of L is 
torch.Size([])
memory (bytes)
4183072768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
4183072768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  31228355000.0
relative error loss 15.20604
shape of L is 
torch.Size([])
memory (bytes)
4185223168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4185223168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% |  7% |
error is  8326509600.0
relative error loss 4.054432
shape of L is 
torch.Size([])
memory (bytes)
4187328512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
4187328512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  6411800600.0
relative error loss 3.1221018
shape of L is 
torch.Size([])
memory (bytes)
4189474816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4189474816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  5304089600.0
relative error loss 2.5827234
time to take a step is 269.38145995140076
it  1 : 1384493056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4191604736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4191604736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  5304089600.0
relative error loss 2.5827234
shape of L is 
torch.Size([])
memory (bytes)
4193734656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  7% |
memory (bytes)
4193734656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  2996974000.0
relative error loss 1.4593183
shape of L is 
torch.Size([])
memory (bytes)
4195864576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4195864576
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  2354771200.0
relative error loss 1.14661
shape of L is 
torch.Size([])
memory (bytes)
4198014976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4198014976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  2126176900.0
relative error loss 1.0353005
shape of L is 
torch.Size([])
memory (bytes)
4200144896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4200144896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  2356880000.0
relative error loss 1.1476369
shape of L is 
torch.Size([])
memory (bytes)
4202287104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
4202287104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  2020070900.0
relative error loss 0.9836343
shape of L is 
torch.Size([])
memory (bytes)
4204425216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% |  7% |
memory (bytes)
4204425216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  1931754800.0
relative error loss 0.94063044
shape of L is 
torch.Size([])
memory (bytes)
4206510080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4206510080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  1923162800.0
relative error loss 0.9364467
shape of L is 
torch.Size([])
memory (bytes)
4208635904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
4208635904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  1841766300.0
relative error loss 0.8968123
shape of L is 
torch.Size([])
memory (bytes)
4210749440
| ID | GPU | MEM |
------------------
|  0 | 13% |  0% |
|  1 |  5% |  7% |
memory (bytes)
4210749440
| ID | GPU  | MEM |
-------------------
|  0 |  12% |  0% |
|  1 | 100% |  7% |
error is  1731195900.0
relative error loss 0.8429722
time to take a step is 240.8146026134491
it  2 : 1467020800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4212858880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4212858880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  1731195900.0
relative error loss 0.8429722
shape of L is 
torch.Size([])
memory (bytes)
4214984704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% |  7% |
memory (bytes)
4214984704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  1597125000.0
relative error loss 0.777689
shape of L is 
torch.Size([])
memory (bytes)
4217049088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4217114624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  1474624400.0
relative error loss 0.7180397
shape of L is 
torch.Size([])
memory (bytes)
4219228160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% |  7% |
memory (bytes)
4219228160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  1352461000.0
relative error loss 0.6585546
shape of L is 
torch.Size([])
memory (bytes)
4221353984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
4221353984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  1246491900.0
relative error loss 0.606955
shape of L is 
torch.Size([])
memory (bytes)
4223475712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
4223475712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  1156691700.0
relative error loss 0.56322855
shape of L is 
torch.Size([])
memory (bytes)
4225593344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4225593344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  1000683900.0
relative error loss 0.4872636
shape of L is 
torch.Size([])
memory (bytes)
4227678208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% |  7% |
memory (bytes)
4227678208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  890290800.0
relative error loss 0.4335098
shape of L is 
torch.Size([])
memory (bytes)
4229754880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  7% |
memory (bytes)
4229836800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  792148200.0
relative error loss 0.38572118
shape of L is 
torch.Size([])
memory (bytes)
4231987200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4231987200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  726242400.0
relative error loss 0.35362965
time to take a step is 237.84898710250854
it  3 : 1467700224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4234125312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4234125312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  726242400.0
relative error loss 0.35362965
shape of L is 
torch.Size([])
memory (bytes)
4236267520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4236267520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  644451700.0
relative error loss 0.31380323
shape of L is 
torch.Size([])
memory (bytes)
4238368768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4238368768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  600182900.0
relative error loss 0.2922474
shape of L is 
torch.Size([])
memory (bytes)
4240572416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
4240572416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  536244350.0
relative error loss 0.26111376
shape of L is 
torch.Size([])
memory (bytes)
4242710528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
4242710528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  515410800.0
relative error loss 0.2509693
shape of L is 
torch.Size([])
memory (bytes)
4244852736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4244852736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  468714750.0
relative error loss 0.22823153
shape of L is 
torch.Size([])
memory (bytes)
4246990848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4246990848
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  434824450.0
relative error loss 0.21172932
shape of L is 
torch.Size([])
memory (bytes)
4249145344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4249145344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  418204670.0
relative error loss 0.20363663
shape of L is 
torch.Size([])
memory (bytes)
4251271168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
4251271168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  7% |
error is  384962800.0
relative error loss 0.18745016
shape of L is 
torch.Size([])
memory (bytes)
4253421568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% |  7% |
memory (bytes)
4253421568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  323430400.0
relative error loss 0.15748815
time to take a step is 235.02106261253357
c= tensor(513.8250, device='cuda:0')
c= tensor(59795.7148, device='cuda:0')
c= tensor(61503.0273, device='cuda:0')
c= tensor(64394.9688, device='cuda:0')
c= tensor(2006032.2500, device='cuda:0')
c= tensor(2255457.2500, device='cuda:0')
c= tensor(2704085.5000, device='cuda:0')
c= tensor(2966089., device='cuda:0')
c= tensor(3046574.2500, device='cuda:0')
c= tensor(8896515., device='cuda:0')
c= tensor(8953334., device='cuda:0')
c= tensor(9802199., device='cuda:0')
c= tensor(9811597., device='cuda:0')
c= tensor(16424948., device='cuda:0')
c= tensor(16535355., device='cuda:0')
c= tensor(16656794., device='cuda:0')
c= tensor(16952710., device='cuda:0')
c= tensor(17638812., device='cuda:0')
c= tensor(21493586., device='cuda:0')
c= tensor(23882112., device='cuda:0')
c= tensor(24052542., device='cuda:0')
c= tensor(28603470., device='cuda:0')
c= tensor(28631404., device='cuda:0')
c= tensor(28660546., device='cuda:0')
c= tensor(29119166., device='cuda:0')
c= tensor(29649816., device='cuda:0')
c= tensor(30781942., device='cuda:0')
c= tensor(30830868., device='cuda:0')
c= tensor(36240400., device='cuda:0')
c= tensor(2.1220e+08, device='cuda:0')
c= tensor(2.1223e+08, device='cuda:0')
c= tensor(3.3510e+08, device='cuda:0')
c= tensor(3.3512e+08, device='cuda:0')
c= tensor(3.3525e+08, device='cuda:0')
c= tensor(3.3559e+08, device='cuda:0')
c= tensor(3.5302e+08, device='cuda:0')
c= tensor(3.5357e+08, device='cuda:0')
c= tensor(3.5357e+08, device='cuda:0')
c= tensor(3.5357e+08, device='cuda:0')
c= tensor(3.5358e+08, device='cuda:0')
c= tensor(3.5358e+08, device='cuda:0')
c= tensor(3.5358e+08, device='cuda:0')
c= tensor(3.5358e+08, device='cuda:0')
c= tensor(3.5358e+08, device='cuda:0')
c= tensor(3.5358e+08, device='cuda:0')
c= tensor(3.5358e+08, device='cuda:0')
c= tensor(3.5358e+08, device='cuda:0')
c= tensor(3.5359e+08, device='cuda:0')
c= tensor(3.5360e+08, device='cuda:0')
c= tensor(3.5360e+08, device='cuda:0')
c= tensor(3.5363e+08, device='cuda:0')
c= tensor(3.5363e+08, device='cuda:0')
c= tensor(3.5364e+08, device='cuda:0')
c= tensor(3.5365e+08, device='cuda:0')
c= tensor(3.5365e+08, device='cuda:0')
c= tensor(3.5365e+08, device='cuda:0')
c= tensor(3.5365e+08, device='cuda:0')
c= tensor(3.5366e+08, device='cuda:0')
c= tensor(3.5366e+08, device='cuda:0')
c= tensor(3.5366e+08, device='cuda:0')
c= tensor(3.5367e+08, device='cuda:0')
c= tensor(3.5367e+08, device='cuda:0')
c= tensor(3.5367e+08, device='cuda:0')
c= tensor(3.5370e+08, device='cuda:0')
c= tensor(3.5370e+08, device='cuda:0')
c= tensor(3.5370e+08, device='cuda:0')
c= tensor(3.5371e+08, device='cuda:0')
c= tensor(3.5371e+08, device='cuda:0')
c= tensor(3.5371e+08, device='cuda:0')
c= tensor(3.5371e+08, device='cuda:0')
c= tensor(3.5372e+08, device='cuda:0')
c= tensor(3.5372e+08, device='cuda:0')
c= tensor(3.5372e+08, device='cuda:0')
c= tensor(3.5373e+08, device='cuda:0')
c= tensor(3.5373e+08, device='cuda:0')
c= tensor(3.5376e+08, device='cuda:0')
c= tensor(3.5376e+08, device='cuda:0')
c= tensor(3.5376e+08, device='cuda:0')
c= tensor(3.5377e+08, device='cuda:0')
c= tensor(3.5378e+08, device='cuda:0')
c= tensor(3.5378e+08, device='cuda:0')
c= tensor(3.5378e+08, device='cuda:0')
c= tensor(3.5378e+08, device='cuda:0')
c= tensor(3.5378e+08, device='cuda:0')
c= tensor(3.5378e+08, device='cuda:0')
c= tensor(3.5378e+08, device='cuda:0')
c= tensor(3.5379e+08, device='cuda:0')
c= tensor(3.5379e+08, device='cuda:0')
c= tensor(3.5379e+08, device='cuda:0')
c= tensor(3.5379e+08, device='cuda:0')
c= tensor(3.5380e+08, device='cuda:0')
c= tensor(3.5380e+08, device='cuda:0')
c= tensor(3.5380e+08, device='cuda:0')
c= tensor(3.5381e+08, device='cuda:0')
c= tensor(3.5382e+08, device='cuda:0')
c= tensor(3.5383e+08, device='cuda:0')
c= tensor(3.5383e+08, device='cuda:0')
c= tensor(3.5383e+08, device='cuda:0')
c= tensor(3.5385e+08, device='cuda:0')
c= tensor(3.5386e+08, device='cuda:0')
c= tensor(3.5387e+08, device='cuda:0')
c= tensor(3.5387e+08, device='cuda:0')
c= tensor(3.5388e+08, device='cuda:0')
c= tensor(3.5388e+08, device='cuda:0')
c= tensor(3.5389e+08, device='cuda:0')
c= tensor(3.5389e+08, device='cuda:0')
c= tensor(3.5390e+08, device='cuda:0')
c= tensor(3.5390e+08, device='cuda:0')
c= tensor(3.5390e+08, device='cuda:0')
c= tensor(3.5390e+08, device='cuda:0')
c= tensor(3.5391e+08, device='cuda:0')
c= tensor(3.5391e+08, device='cuda:0')
c= tensor(3.5391e+08, device='cuda:0')
c= tensor(3.5391e+08, device='cuda:0')
c= tensor(3.5392e+08, device='cuda:0')
c= tensor(3.5392e+08, device='cuda:0')
c= tensor(3.5392e+08, device='cuda:0')
c= tensor(3.5392e+08, device='cuda:0')
c= tensor(3.5394e+08, device='cuda:0')
c= tensor(3.5394e+08, device='cuda:0')
c= tensor(3.5395e+08, device='cuda:0')
c= tensor(3.5396e+08, device='cuda:0')
c= tensor(3.5396e+08, device='cuda:0')
c= tensor(3.5397e+08, device='cuda:0')
c= tensor(3.5397e+08, device='cuda:0')
c= tensor(3.5397e+08, device='cuda:0')
c= tensor(3.5397e+08, device='cuda:0')
c= tensor(3.5398e+08, device='cuda:0')
c= tensor(3.5399e+08, device='cuda:0')
c= tensor(3.5399e+08, device='cuda:0')
c= tensor(3.5402e+08, device='cuda:0')
c= tensor(3.5402e+08, device='cuda:0')
c= tensor(3.5402e+08, device='cuda:0')
c= tensor(3.5402e+08, device='cuda:0')
c= tensor(3.5402e+08, device='cuda:0')
c= tensor(3.5402e+08, device='cuda:0')
c= tensor(3.5403e+08, device='cuda:0')
c= tensor(3.5403e+08, device='cuda:0')
c= tensor(3.5403e+08, device='cuda:0')
c= tensor(3.5403e+08, device='cuda:0')
c= tensor(3.5403e+08, device='cuda:0')
c= tensor(3.5403e+08, device='cuda:0')
c= tensor(3.5405e+08, device='cuda:0')
c= tensor(3.5406e+08, device='cuda:0')
c= tensor(3.5407e+08, device='cuda:0')
c= tensor(3.5407e+08, device='cuda:0')
c= tensor(3.5407e+08, device='cuda:0')
c= tensor(3.5408e+08, device='cuda:0')
c= tensor(3.5408e+08, device='cuda:0')
c= tensor(3.5408e+08, device='cuda:0')
c= tensor(3.5408e+08, device='cuda:0')
c= tensor(3.5408e+08, device='cuda:0')
c= tensor(3.5409e+08, device='cuda:0')
c= tensor(3.5409e+08, device='cuda:0')
c= tensor(3.5409e+08, device='cuda:0')
c= tensor(3.5412e+08, device='cuda:0')
c= tensor(3.5412e+08, device='cuda:0')
c= tensor(3.5412e+08, device='cuda:0')
c= tensor(3.5412e+08, device='cuda:0')
c= tensor(3.5412e+08, device='cuda:0')
c= tensor(3.5414e+08, device='cuda:0')
c= tensor(3.5414e+08, device='cuda:0')
c= tensor(3.5415e+08, device='cuda:0')
c= tensor(3.5415e+08, device='cuda:0')
c= tensor(3.5416e+08, device='cuda:0')
c= tensor(3.5416e+08, device='cuda:0')
c= tensor(3.5417e+08, device='cuda:0')
c= tensor(3.5417e+08, device='cuda:0')
c= tensor(3.5417e+08, device='cuda:0')
c= tensor(3.5417e+08, device='cuda:0')
c= tensor(3.5417e+08, device='cuda:0')
c= tensor(3.5417e+08, device='cuda:0')
c= tensor(3.5418e+08, device='cuda:0')
c= tensor(3.5418e+08, device='cuda:0')
c= tensor(3.5418e+08, device='cuda:0')
c= tensor(3.5418e+08, device='cuda:0')
c= tensor(3.5419e+08, device='cuda:0')
c= tensor(3.5419e+08, device='cuda:0')
c= tensor(3.5419e+08, device='cuda:0')
c= tensor(3.5419e+08, device='cuda:0')
c= tensor(3.5421e+08, device='cuda:0')
c= tensor(3.5421e+08, device='cuda:0')
c= tensor(3.5421e+08, device='cuda:0')
c= tensor(3.5421e+08, device='cuda:0')
c= tensor(3.5421e+08, device='cuda:0')
c= tensor(3.5422e+08, device='cuda:0')
c= tensor(3.5422e+08, device='cuda:0')
c= tensor(3.5423e+08, device='cuda:0')
c= tensor(3.5423e+08, device='cuda:0')
c= tensor(3.5425e+08, device='cuda:0')
c= tensor(3.5425e+08, device='cuda:0')
c= tensor(3.5425e+08, device='cuda:0')
c= tensor(3.5425e+08, device='cuda:0')
c= tensor(3.5425e+08, device='cuda:0')
c= tensor(3.5426e+08, device='cuda:0')
c= tensor(3.5426e+08, device='cuda:0')
c= tensor(3.5426e+08, device='cuda:0')
c= tensor(3.5426e+08, device='cuda:0')
c= tensor(3.5427e+08, device='cuda:0')
c= tensor(3.5427e+08, device='cuda:0')
c= tensor(3.5428e+08, device='cuda:0')
c= tensor(3.5428e+08, device='cuda:0')
c= tensor(3.5431e+08, device='cuda:0')
c= tensor(3.5431e+08, device='cuda:0')
c= tensor(3.5432e+08, device='cuda:0')
c= tensor(3.5432e+08, device='cuda:0')
c= tensor(3.5432e+08, device='cuda:0')
c= tensor(3.5432e+08, device='cuda:0')
c= tensor(3.5433e+08, device='cuda:0')
c= tensor(3.5433e+08, device='cuda:0')
c= tensor(3.5435e+08, device='cuda:0')
c= tensor(3.5435e+08, device='cuda:0')
c= tensor(3.5435e+08, device='cuda:0')
c= tensor(3.5435e+08, device='cuda:0')
c= tensor(3.5435e+08, device='cuda:0')
c= tensor(3.5435e+08, device='cuda:0')
c= tensor(3.5435e+08, device='cuda:0')
c= tensor(3.5436e+08, device='cuda:0')
c= tensor(3.5439e+08, device='cuda:0')
c= tensor(3.5439e+08, device='cuda:0')
c= tensor(3.5439e+08, device='cuda:0')
c= tensor(3.5439e+08, device='cuda:0')
c= tensor(3.5441e+08, device='cuda:0')
c= tensor(3.5441e+08, device='cuda:0')
c= tensor(3.5441e+08, device='cuda:0')
c= tensor(3.5442e+08, device='cuda:0')
c= tensor(3.5442e+08, device='cuda:0')
c= tensor(3.5442e+08, device='cuda:0')
c= tensor(3.5443e+08, device='cuda:0')
c= tensor(3.5443e+08, device='cuda:0')
c= tensor(3.5443e+08, device='cuda:0')
c= tensor(3.5443e+08, device='cuda:0')
c= tensor(3.5443e+08, device='cuda:0')
c= tensor(3.5443e+08, device='cuda:0')
c= tensor(3.5444e+08, device='cuda:0')
c= tensor(3.5444e+08, device='cuda:0')
c= tensor(3.5444e+08, device='cuda:0')
c= tensor(3.5445e+08, device='cuda:0')
c= tensor(3.5445e+08, device='cuda:0')
c= tensor(3.5446e+08, device='cuda:0')
c= tensor(3.5448e+08, device='cuda:0')
c= tensor(3.5455e+08, device='cuda:0')
c= tensor(3.5455e+08, device='cuda:0')
c= tensor(3.5456e+08, device='cuda:0')
c= tensor(3.5457e+08, device='cuda:0')
c= tensor(3.5477e+08, device='cuda:0')
c= tensor(3.5482e+08, device='cuda:0')
c= tensor(3.5482e+08, device='cuda:0')
c= tensor(3.5742e+08, device='cuda:0')
c= tensor(3.5838e+08, device='cuda:0')
c= tensor(3.5844e+08, device='cuda:0')
c= tensor(3.5969e+08, device='cuda:0')
c= tensor(3.5969e+08, device='cuda:0')
c= tensor(3.5974e+08, device='cuda:0')
c= tensor(3.7008e+08, device='cuda:0')
c= tensor(3.8287e+08, device='cuda:0')
c= tensor(3.8287e+08, device='cuda:0')
c= tensor(3.8298e+08, device='cuda:0')
c= tensor(3.8538e+08, device='cuda:0')
c= tensor(3.8561e+08, device='cuda:0')
c= tensor(3.8571e+08, device='cuda:0')
c= tensor(3.8592e+08, device='cuda:0')
c= tensor(3.8599e+08, device='cuda:0')
c= tensor(3.8603e+08, device='cuda:0')
c= tensor(3.8609e+08, device='cuda:0')
c= tensor(3.8982e+08, device='cuda:0')
c= tensor(3.8986e+08, device='cuda:0')
c= tensor(3.8987e+08, device='cuda:0')
c= tensor(3.8997e+08, device='cuda:0')
c= tensor(3.9004e+08, device='cuda:0')
c= tensor(4.0150e+08, device='cuda:0')
c= tensor(4.0197e+08, device='cuda:0')
c= tensor(4.0198e+08, device='cuda:0')
c= tensor(4.0209e+08, device='cuda:0')
c= tensor(4.0210e+08, device='cuda:0')
c= tensor(4.0218e+08, device='cuda:0')
c= tensor(4.0294e+08, device='cuda:0')
c= tensor(4.0835e+08, device='cuda:0')
c= tensor(4.1069e+08, device='cuda:0')
c= tensor(4.1069e+08, device='cuda:0')
c= tensor(4.1069e+08, device='cuda:0')
c= tensor(4.1132e+08, device='cuda:0')
c= tensor(4.1275e+08, device='cuda:0')
c= tensor(4.1293e+08, device='cuda:0')
c= tensor(4.1293e+08, device='cuda:0')
c= tensor(4.2286e+08, device='cuda:0')
c= tensor(4.2288e+08, device='cuda:0')
c= tensor(4.2295e+08, device='cuda:0')
c= tensor(4.2372e+08, device='cuda:0')
c= tensor(4.2372e+08, device='cuda:0')
c= tensor(4.2402e+08, device='cuda:0')
c= tensor(4.2460e+08, device='cuda:0')
c= tensor(4.4192e+08, device='cuda:0')
c= tensor(4.4207e+08, device='cuda:0')
c= tensor(4.4222e+08, device='cuda:0')
c= tensor(4.4223e+08, device='cuda:0')
c= tensor(4.4224e+08, device='cuda:0')
c= tensor(4.4236e+08, device='cuda:0')
c= tensor(4.4238e+08, device='cuda:0')
c= tensor(4.4247e+08, device='cuda:0')
c= tensor(4.4817e+08, device='cuda:0')
c= tensor(4.4828e+08, device='cuda:0')
c= tensor(4.4830e+08, device='cuda:0')
c= tensor(4.4831e+08, device='cuda:0')
c= tensor(4.7720e+08, device='cuda:0')
c= tensor(4.7724e+08, device='cuda:0')
c= tensor(4.7724e+08, device='cuda:0')
c= tensor(4.7727e+08, device='cuda:0')
c= tensor(4.8159e+08, device='cuda:0')
c= tensor(4.8167e+08, device='cuda:0')
c= tensor(4.8179e+08, device='cuda:0')
c= tensor(4.8180e+08, device='cuda:0')
c= tensor(4.8205e+08, device='cuda:0')
c= tensor(4.8232e+08, device='cuda:0')
c= tensor(5.2599e+08, device='cuda:0')
c= tensor(5.2641e+08, device='cuda:0')
c= tensor(5.2644e+08, device='cuda:0')
c= tensor(5.2802e+08, device='cuda:0')
c= tensor(5.2937e+08, device='cuda:0')
c= tensor(5.2940e+08, device='cuda:0')
c= tensor(5.3055e+08, device='cuda:0')
c= tensor(5.3430e+08, device='cuda:0')
c= tensor(5.8450e+08, device='cuda:0')
c= tensor(5.8481e+08, device='cuda:0')
c= tensor(5.8481e+08, device='cuda:0')
c= tensor(5.8481e+08, device='cuda:0')
c= tensor(5.8536e+08, device='cuda:0')
c= tensor(5.8557e+08, device='cuda:0')
c= tensor(5.8561e+08, device='cuda:0')
c= tensor(5.8561e+08, device='cuda:0')
c= tensor(5.8572e+08, device='cuda:0')
c= tensor(5.8677e+08, device='cuda:0')
c= tensor(5.9128e+08, device='cuda:0')
c= tensor(5.9128e+08, device='cuda:0')
c= tensor(5.9491e+08, device='cuda:0')
c= tensor(5.9493e+08, device='cuda:0')
c= tensor(5.9497e+08, device='cuda:0')
c= tensor(5.9500e+08, device='cuda:0')
c= tensor(5.9501e+08, device='cuda:0')
c= tensor(6.0912e+08, device='cuda:0')
c= tensor(6.0936e+08, device='cuda:0')
c= tensor(6.0940e+08, device='cuda:0')
c= tensor(6.0957e+08, device='cuda:0')
c= tensor(6.0958e+08, device='cuda:0')
c= tensor(6.3227e+08, device='cuda:0')
c= tensor(6.3231e+08, device='cuda:0')
c= tensor(6.3788e+08, device='cuda:0')
c= tensor(6.3788e+08, device='cuda:0')
c= tensor(6.3788e+08, device='cuda:0')
c= tensor(6.3788e+08, device='cuda:0')
c= tensor(6.3811e+08, device='cuda:0')
c= tensor(6.3812e+08, device='cuda:0')
c= tensor(6.3813e+08, device='cuda:0')
c= tensor(6.3814e+08, device='cuda:0')
c= tensor(6.3815e+08, device='cuda:0')
c= tensor(6.4769e+08, device='cuda:0')
c= tensor(6.4784e+08, device='cuda:0')
c= tensor(6.4797e+08, device='cuda:0')
c= tensor(6.4892e+08, device='cuda:0')
c= tensor(6.5808e+08, device='cuda:0')
c= tensor(6.5810e+08, device='cuda:0')
c= tensor(6.5811e+08, device='cuda:0')
c= tensor(6.5812e+08, device='cuda:0')
c= tensor(6.5812e+08, device='cuda:0')
c= tensor(6.5814e+08, device='cuda:0')
c= tensor(6.5822e+08, device='cuda:0')
c= tensor(6.5823e+08, device='cuda:0')
c= tensor(6.5824e+08, device='cuda:0')
c= tensor(6.5828e+08, device='cuda:0')
c= tensor(6.5831e+08, device='cuda:0')
c= tensor(7.6348e+08, device='cuda:0')
c= tensor(7.6349e+08, device='cuda:0')
c= tensor(7.6376e+08, device='cuda:0')
c= tensor(7.6387e+08, device='cuda:0')
c= tensor(7.6387e+08, device='cuda:0')
c= tensor(7.6557e+08, device='cuda:0')
c= tensor(7.9911e+08, device='cuda:0')
c= tensor(8.3012e+08, device='cuda:0')
c= tensor(8.3060e+08, device='cuda:0')
c= tensor(8.3071e+08, device='cuda:0')
c= tensor(8.3071e+08, device='cuda:0')
c= tensor(8.3211e+08, device='cuda:0')
c= tensor(9.0562e+08, device='cuda:0')
c= tensor(9.0618e+08, device='cuda:0')
c= tensor(9.0628e+08, device='cuda:0')
c= tensor(9.0769e+08, device='cuda:0')
c= tensor(9.1904e+08, device='cuda:0')
c= tensor(9.1946e+08, device='cuda:0')
c= tensor(9.1949e+08, device='cuda:0')
c= tensor(9.1950e+08, device='cuda:0')
c= tensor(9.1958e+08, device='cuda:0')
c= tensor(9.1967e+08, device='cuda:0')
c= tensor(9.4534e+08, device='cuda:0')
c= tensor(9.4541e+08, device='cuda:0')
c= tensor(9.4542e+08, device='cuda:0')
c= tensor(9.4548e+08, device='cuda:0')
c= tensor(9.4569e+08, device='cuda:0')
c= tensor(9.4569e+08, device='cuda:0')
c= tensor(9.4619e+08, device='cuda:0')
c= tensor(9.4683e+08, device='cuda:0')
c= tensor(9.4720e+08, device='cuda:0')
c= tensor(9.4786e+08, device='cuda:0')
c= tensor(9.4877e+08, device='cuda:0')
c= tensor(9.4884e+08, device='cuda:0')
c= tensor(9.4902e+08, device='cuda:0')
c= tensor(9.4939e+08, device='cuda:0')
c= tensor(9.5218e+08, device='cuda:0')
c= tensor(9.5219e+08, device='cuda:0')
c= tensor(9.6551e+08, device='cuda:0')
c= tensor(9.7168e+08, device='cuda:0')
c= tensor(9.7259e+08, device='cuda:0')
c= tensor(9.7309e+08, device='cuda:0')
c= tensor(9.7319e+08, device='cuda:0')
c= tensor(9.7322e+08, device='cuda:0')
c= tensor(9.7324e+08, device='cuda:0')
c= tensor(9.7809e+08, device='cuda:0')
c= tensor(9.8409e+08, device='cuda:0')
c= tensor(9.8583e+08, device='cuda:0')
c= tensor(9.9363e+08, device='cuda:0')
c= tensor(9.9422e+08, device='cuda:0')
c= tensor(9.9610e+08, device='cuda:0')
c= tensor(9.9620e+08, device='cuda:0')
c= tensor(9.9995e+08, device='cuda:0')
c= tensor(9.9996e+08, device='cuda:0')
c= tensor(9.9998e+08, device='cuda:0')
c= tensor(1.0072e+09, device='cuda:0')
c= tensor(1.0073e+09, device='cuda:0')
c= tensor(1.0073e+09, device='cuda:0')
c= tensor(1.0075e+09, device='cuda:0')
c= tensor(1.0101e+09, device='cuda:0')
c= tensor(1.0105e+09, device='cuda:0')
c= tensor(1.0108e+09, device='cuda:0')
c= tensor(1.0108e+09, device='cuda:0')
c= tensor(1.0108e+09, device='cuda:0')
c= tensor(1.0108e+09, device='cuda:0')
c= tensor(1.0119e+09, device='cuda:0')
c= tensor(1.0120e+09, device='cuda:0')
c= tensor(1.0123e+09, device='cuda:0')
c= tensor(1.0123e+09, device='cuda:0')
c= tensor(1.0134e+09, device='cuda:0')
c= tensor(1.0134e+09, device='cuda:0')
c= tensor(1.0135e+09, device='cuda:0')
c= tensor(1.0138e+09, device='cuda:0')
c= tensor(1.0142e+09, device='cuda:0')
c= tensor(1.0142e+09, device='cuda:0')
c= tensor(1.0147e+09, device='cuda:0')
c= tensor(1.0149e+09, device='cuda:0')
c= tensor(1.0150e+09, device='cuda:0')
c= tensor(1.0152e+09, device='cuda:0')
c= tensor(1.0288e+09, device='cuda:0')
c= tensor(1.0288e+09, device='cuda:0')
c= tensor(1.0289e+09, device='cuda:0')
c= tensor(1.0324e+09, device='cuda:0')
c= tensor(1.0324e+09, device='cuda:0')
c= tensor(1.0535e+09, device='cuda:0')
c= tensor(1.0535e+09, device='cuda:0')
c= tensor(1.0562e+09, device='cuda:0')
c= tensor(1.0662e+09, device='cuda:0')
c= tensor(1.0662e+09, device='cuda:0')
c= tensor(1.0696e+09, device='cuda:0')
c= tensor(1.0699e+09, device='cuda:0')
c= tensor(1.0724e+09, device='cuda:0')
c= tensor(1.0725e+09, device='cuda:0')
c= tensor(1.0727e+09, device='cuda:0')
c= tensor(1.0727e+09, device='cuda:0')
c= tensor(1.0727e+09, device='cuda:0')
c= tensor(1.0727e+09, device='cuda:0')
c= tensor(1.0729e+09, device='cuda:0')
c= tensor(1.0730e+09, device='cuda:0')
c= tensor(1.0767e+09, device='cuda:0')
c= tensor(1.0767e+09, device='cuda:0')
c= tensor(1.0767e+09, device='cuda:0')
c= tensor(1.0768e+09, device='cuda:0')
c= tensor(1.0768e+09, device='cuda:0')
c= tensor(1.0768e+09, device='cuda:0')
c= tensor(1.0799e+09, device='cuda:0')
c= tensor(1.0805e+09, device='cuda:0')
c= tensor(1.0805e+09, device='cuda:0')
c= tensor(1.0805e+09, device='cuda:0')
c= tensor(1.0805e+09, device='cuda:0')
c= tensor(1.1225e+09, device='cuda:0')
c= tensor(1.1225e+09, device='cuda:0')
c= tensor(1.1225e+09, device='cuda:0')
c= tensor(1.1240e+09, device='cuda:0')
c= tensor(1.1245e+09, device='cuda:0')
c= tensor(1.1245e+09, device='cuda:0')
c= tensor(1.1246e+09, device='cuda:0')
c= tensor(1.1257e+09, device='cuda:0')
c= tensor(1.1279e+09, device='cuda:0')
c= tensor(1.1286e+09, device='cuda:0')
c= tensor(1.1286e+09, device='cuda:0')
c= tensor(1.1287e+09, device='cuda:0')
c= tensor(1.1290e+09, device='cuda:0')
c= tensor(1.1296e+09, device='cuda:0')
c= tensor(1.1304e+09, device='cuda:0')
c= tensor(1.1304e+09, device='cuda:0')
c= tensor(1.1324e+09, device='cuda:0')
c= tensor(1.1324e+09, device='cuda:0')
c= tensor(1.1327e+09, device='cuda:0')
c= tensor(1.1328e+09, device='cuda:0')
c= tensor(1.1328e+09, device='cuda:0')
c= tensor(1.1328e+09, device='cuda:0')
c= tensor(1.1333e+09, device='cuda:0')
c= tensor(1.1333e+09, device='cuda:0')
c= tensor(1.1335e+09, device='cuda:0')
c= tensor(1.1337e+09, device='cuda:0')
c= tensor(1.1537e+09, device='cuda:0')
c= tensor(1.1537e+09, device='cuda:0')
c= tensor(1.1537e+09, device='cuda:0')
c= tensor(1.1538e+09, device='cuda:0')
c= tensor(1.1538e+09, device='cuda:0')
c= tensor(1.1538e+09, device='cuda:0')
c= tensor(1.1539e+09, device='cuda:0')
c= tensor(1.1540e+09, device='cuda:0')
c= tensor(1.1575e+09, device='cuda:0')
c= tensor(1.1575e+09, device='cuda:0')
c= tensor(1.1575e+09, device='cuda:0')
c= tensor(1.1576e+09, device='cuda:0')
c= tensor(1.1720e+09, device='cuda:0')
c= tensor(1.2894e+09, device='cuda:0')
c= tensor(1.2895e+09, device='cuda:0')
c= tensor(1.2895e+09, device='cuda:0')
c= tensor(1.2981e+09, device='cuda:0')
c= tensor(1.2994e+09, device='cuda:0')
c= tensor(1.2994e+09, device='cuda:0')
c= tensor(1.2995e+09, device='cuda:0')
c= tensor(1.2997e+09, device='cuda:0')
c= tensor(1.3432e+09, device='cuda:0')
c= tensor(1.3813e+09, device='cuda:0')
c= tensor(1.3836e+09, device='cuda:0')
c= tensor(1.3836e+09, device='cuda:0')
c= tensor(1.3836e+09, device='cuda:0')
c= tensor(1.3836e+09, device='cuda:0')
c= tensor(1.3845e+09, device='cuda:0')
c= tensor(1.3845e+09, device='cuda:0')
c= tensor(1.3847e+09, device='cuda:0')
c= tensor(1.3878e+09, device='cuda:0')
c= tensor(1.4055e+09, device='cuda:0')
c= tensor(1.4055e+09, device='cuda:0')
c= tensor(1.4055e+09, device='cuda:0')
c= tensor(1.4056e+09, device='cuda:0')
c= tensor(1.4075e+09, device='cuda:0')
c= tensor(1.4083e+09, device='cuda:0')
c= tensor(1.4083e+09, device='cuda:0')
c= tensor(1.4083e+09, device='cuda:0')
c= tensor(1.4084e+09, device='cuda:0')
c= tensor(1.4084e+09, device='cuda:0')
c= tensor(1.4084e+09, device='cuda:0')
c= tensor(1.4084e+09, device='cuda:0')
c= tensor(1.4084e+09, device='cuda:0')
c= tensor(1.4085e+09, device='cuda:0')
c= tensor(1.4085e+09, device='cuda:0')
c= tensor(1.4085e+09, device='cuda:0')
c= tensor(1.4110e+09, device='cuda:0')
c= tensor(1.4212e+09, device='cuda:0')
c= tensor(1.4225e+09, device='cuda:0')
c= tensor(1.4231e+09, device='cuda:0')
c= tensor(1.4232e+09, device='cuda:0')
c= tensor(1.4233e+09, device='cuda:0')
c= tensor(1.4234e+09, device='cuda:0')
c= tensor(1.4247e+09, device='cuda:0')
c= tensor(1.4248e+09, device='cuda:0')
c= tensor(1.4250e+09, device='cuda:0')
c= tensor(1.4250e+09, device='cuda:0')
c= tensor(1.4973e+09, device='cuda:0')
c= tensor(1.4974e+09, device='cuda:0')
c= tensor(1.4979e+09, device='cuda:0')
c= tensor(1.5015e+09, device='cuda:0')
c= tensor(1.5027e+09, device='cuda:0')
c= tensor(1.5029e+09, device='cuda:0')
c= tensor(1.5100e+09, device='cuda:0')
c= tensor(1.5119e+09, device='cuda:0')
c= tensor(1.5120e+09, device='cuda:0')
c= tensor(1.5120e+09, device='cuda:0')
c= tensor(1.5122e+09, device='cuda:0')
c= tensor(1.5122e+09, device='cuda:0')
c= tensor(1.5131e+09, device='cuda:0')
c= tensor(1.5244e+09, device='cuda:0')
c= tensor(1.5255e+09, device='cuda:0')
c= tensor(1.5265e+09, device='cuda:0')
c= tensor(1.5265e+09, device='cuda:0')
c= tensor(1.5266e+09, device='cuda:0')
c= tensor(1.5267e+09, device='cuda:0')
c= tensor(1.5268e+09, device='cuda:0')
c= tensor(1.5313e+09, device='cuda:0')
c= tensor(1.5430e+09, device='cuda:0')
c= tensor(1.5430e+09, device='cuda:0')
c= tensor(1.6491e+09, device='cuda:0')
c= tensor(1.6498e+09, device='cuda:0')
c= tensor(1.6504e+09, device='cuda:0')
c= tensor(1.6504e+09, device='cuda:0')
c= tensor(1.6508e+09, device='cuda:0')
c= tensor(1.6536e+09, device='cuda:0')
c= tensor(1.6536e+09, device='cuda:0')
c= tensor(1.6875e+09, device='cuda:0')
c= tensor(1.6877e+09, device='cuda:0')
c= tensor(1.6881e+09, device='cuda:0')
c= tensor(1.6882e+09, device='cuda:0')
c= tensor(1.6882e+09, device='cuda:0')
c= tensor(1.6882e+09, device='cuda:0')
c= tensor(1.6882e+09, device='cuda:0')
c= tensor(1.6882e+09, device='cuda:0')
c= tensor(1.6884e+09, device='cuda:0')
c= tensor(1.7007e+09, device='cuda:0')
c= tensor(1.7014e+09, device='cuda:0')
c= tensor(1.7075e+09, device='cuda:0')
c= tensor(1.7075e+09, device='cuda:0')
c= tensor(1.7076e+09, device='cuda:0')
c= tensor(1.7076e+09, device='cuda:0')
c= tensor(1.7077e+09, device='cuda:0')
c= tensor(1.7102e+09, device='cuda:0')
c= tensor(1.8601e+09, device='cuda:0')
c= tensor(1.8603e+09, device='cuda:0')
c= tensor(1.8640e+09, device='cuda:0')
c= tensor(1.8641e+09, device='cuda:0')
c= tensor(1.8659e+09, device='cuda:0')
c= tensor(1.8702e+09, device='cuda:0')
c= tensor(1.8704e+09, device='cuda:0')
c= tensor(1.8704e+09, device='cuda:0')
c= tensor(1.8727e+09, device='cuda:0')
c= tensor(1.8728e+09, device='cuda:0')
c= tensor(1.8737e+09, device='cuda:0')
c= tensor(1.8788e+09, device='cuda:0')
c= tensor(1.8790e+09, device='cuda:0')
c= tensor(1.8791e+09, device='cuda:0')
c= tensor(1.8792e+09, device='cuda:0')
c= tensor(1.8815e+09, device='cuda:0')
c= tensor(1.8841e+09, device='cuda:0')
c= tensor(1.8844e+09, device='cuda:0')
c= tensor(1.8844e+09, device='cuda:0')
c= tensor(1.9029e+09, device='cuda:0')
c= tensor(1.9036e+09, device='cuda:0')
c= tensor(1.9142e+09, device='cuda:0')
c= tensor(1.9152e+09, device='cuda:0')
c= tensor(1.9173e+09, device='cuda:0')
c= tensor(1.9174e+09, device='cuda:0')
c= tensor(1.9277e+09, device='cuda:0')
c= tensor(1.9480e+09, device='cuda:0')
c= tensor(1.9485e+09, device='cuda:0')
c= tensor(1.9486e+09, device='cuda:0')
c= tensor(1.9486e+09, device='cuda:0')
c= tensor(1.9487e+09, device='cuda:0')
c= tensor(1.9494e+09, device='cuda:0')
c= tensor(1.9496e+09, device='cuda:0')
c= tensor(1.9497e+09, device='cuda:0')
c= tensor(1.9498e+09, device='cuda:0')
c= tensor(1.9534e+09, device='cuda:0')
c= tensor(1.9541e+09, device='cuda:0')
c= tensor(1.9543e+09, device='cuda:0')
c= tensor(1.9549e+09, device='cuda:0')
c= tensor(1.9555e+09, device='cuda:0')
c= tensor(1.9556e+09, device='cuda:0')
c= tensor(1.9556e+09, device='cuda:0')
c= tensor(1.9556e+09, device='cuda:0')
c= tensor(1.9591e+09, device='cuda:0')
c= tensor(1.9592e+09, device='cuda:0')
c= tensor(1.9597e+09, device='cuda:0')
c= tensor(1.9597e+09, device='cuda:0')
c= tensor(1.9597e+09, device='cuda:0')
c= tensor(1.9632e+09, device='cuda:0')
c= tensor(1.9642e+09, device='cuda:0')
c= tensor(1.9644e+09, device='cuda:0')
c= tensor(1.9644e+09, device='cuda:0')
c= tensor(1.9644e+09, device='cuda:0')
c= tensor(1.9645e+09, device='cuda:0')
c= tensor(1.9646e+09, device='cuda:0')
c= tensor(1.9662e+09, device='cuda:0')
c= tensor(1.9662e+09, device='cuda:0')
c= tensor(1.9664e+09, device='cuda:0')
c= tensor(1.9665e+09, device='cuda:0')
c= tensor(1.9666e+09, device='cuda:0')
c= tensor(1.9701e+09, device='cuda:0')
c= tensor(1.9701e+09, device='cuda:0')
c= tensor(1.9711e+09, device='cuda:0')
c= tensor(1.9832e+09, device='cuda:0')
c= tensor(1.9834e+09, device='cuda:0')
c= tensor(1.9839e+09, device='cuda:0')
c= tensor(1.9850e+09, device='cuda:0')
c= tensor(1.9850e+09, device='cuda:0')
c= tensor(1.9851e+09, device='cuda:0')
c= tensor(1.9852e+09, device='cuda:0')
c= tensor(1.9916e+09, device='cuda:0')
c= tensor(1.9924e+09, device='cuda:0')
c= tensor(1.9927e+09, device='cuda:0')
c= tensor(1.9944e+09, device='cuda:0')
c= tensor(1.9947e+09, device='cuda:0')
c= tensor(1.9947e+09, device='cuda:0')
c= tensor(1.9947e+09, device='cuda:0')
c= tensor(1.9948e+09, device='cuda:0')
c= tensor(1.9973e+09, device='cuda:0')
c= tensor(2.0195e+09, device='cuda:0')
c= tensor(2.0207e+09, device='cuda:0')
c= tensor(2.0208e+09, device='cuda:0')
c= tensor(2.0208e+09, device='cuda:0')
c= tensor(2.0527e+09, device='cuda:0')
c= tensor(2.0528e+09, device='cuda:0')
c= tensor(2.0528e+09, device='cuda:0')
c= tensor(2.0531e+09, device='cuda:0')
c= tensor(2.0535e+09, device='cuda:0')
c= tensor(2.0535e+09, device='cuda:0')
c= tensor(2.0535e+09, device='cuda:0')
c= tensor(2.0537e+09, device='cuda:0')
time to make c is 12.212625980377197
time for making loss is 12.212661981582642
p0 True
it  0 : 1093783552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  6% |
shape of L is 
torch.Size([])
memory (bytes)
4255592448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  6% |
memory (bytes)
4255793152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  323430400.0
relative error loss 0.15748815
shape of L is 
torch.Size([])
memory (bytes)
4282777600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  6% |
memory (bytes)
4282920960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  6% |
error is  318790270.0
relative error loss 0.15522873
shape of L is 
torch.Size([])
memory (bytes)
4286287872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  6% |
memory (bytes)
4286287872
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  295767420.0
relative error loss 0.1440182
shape of L is 
torch.Size([])
memory (bytes)
4289441792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  7% |
memory (bytes)
4289568768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  288603260.0
relative error loss 0.14052975
shape of L is 
torch.Size([])
memory (bytes)
4292775936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% |  7% |
memory (bytes)
4292775936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% |  7% |
error is  284102000.0
relative error loss 0.13833795
shape of L is 
torch.Size([])
memory (bytes)
4295991296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4295991296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  279359360.0
relative error loss 0.13602862
shape of L is 
torch.Size([])
memory (bytes)
4299206656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% |  7% |
memory (bytes)
4299206656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  276794240.0
relative error loss 0.13477957
shape of L is 
torch.Size([])
memory (bytes)
4302422016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4302422016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  274349820.0
relative error loss 0.13358931
shape of L is 
torch.Size([])
memory (bytes)
4305641472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% |  7% |
memory (bytes)
4305641472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  272615040.0
relative error loss 0.1327446
shape of L is 
torch.Size([])
memory (bytes)
4308852736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4308852736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  271347840.0
relative error loss 0.13212755
time to take a step is 304.7498152256012
it  1 : 1468760576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4312068096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4312068096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  271347840.0
relative error loss 0.13212755
shape of L is 
torch.Size([])
memory (bytes)
4315279360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4315279360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  269789820.0
relative error loss 0.1313689
shape of L is 
torch.Size([])
memory (bytes)
4318404608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4318511104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  268786700.0
relative error loss 0.13088046
shape of L is 
torch.Size([])
memory (bytes)
4321718272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4321730560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  267942020.0
relative error loss 0.13046916
shape of L is 
torch.Size([])
memory (bytes)
4324950016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  7% |
memory (bytes)
4324950016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  267270780.0
relative error loss 0.13014232
shape of L is 
torch.Size([])
memory (bytes)
4328169472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4328173568
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  266706050.0
relative error loss 0.12986733
shape of L is 
torch.Size([])
memory (bytes)
4331397120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% |  7% |
memory (bytes)
4331397120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  266173180.0
relative error loss 0.12960786
shape of L is 
torch.Size([])
memory (bytes)
4334600192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4334600192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  265801090.0
relative error loss 0.12942667
shape of L is 
torch.Size([])
memory (bytes)
4337721344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4337721344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  265417220.0
relative error loss 0.12923975
shape of L is 
torch.Size([])
memory (bytes)
4341039104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% |  7% |
memory (bytes)
4341039104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  264983940.0
relative error loss 0.12902878
time to take a step is 304.4061007499695
it  2 : 1468760576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4344283136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4344283136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  264983940.0
relative error loss 0.12902878
shape of L is 
torch.Size([])
memory (bytes)
4347502592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% |  7% |
memory (bytes)
4347502592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  264748800.0
relative error loss 0.12891428
shape of L is 
torch.Size([])
memory (bytes)
4350750720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4350750720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  264452860.0
relative error loss 0.12877019
shape of L is 
torch.Size([])
memory (bytes)
4353953792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4353953792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  264229890.0
relative error loss 0.1286616
shape of L is 
torch.Size([])
memory (bytes)
4357169152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4357169152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  263912960.0
relative error loss 0.12850729
shape of L is 
torch.Size([])
memory (bytes)
4360388608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4360388608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  263648380.0
relative error loss 0.12837845
shape of L is 
torch.Size([])
memory (bytes)
4363616256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4363616256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  263205500.0
relative error loss 0.1281628
shape of L is 
torch.Size([])
memory (bytes)
4366823424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4366823424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  263006720.0
relative error loss 0.12806602
shape of L is 
torch.Size([])
memory (bytes)
4370038784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4370038784
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  262769280.0
relative error loss 0.1279504
shape of L is 
torch.Size([])
memory (bytes)
4373250048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4373250048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  262470140.0
relative error loss 0.12780474
time to take a step is 293.2051956653595
it  3 : 1469263360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4376367104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4376469504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  262470140.0
relative error loss 0.12780474
shape of L is 
torch.Size([])
memory (bytes)
4379684864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4379684864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  262195460.0
relative error loss 0.12767099
shape of L is 
torch.Size([])
memory (bytes)
4382916608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% |  7% |
memory (bytes)
4382916608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  262054910.0
relative error loss 0.12760255
shape of L is 
torch.Size([])
memory (bytes)
4386136064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4386136064
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  261915260.0
relative error loss 0.12753455
shape of L is 
torch.Size([])
memory (bytes)
4389363712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4389363712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  261676160.0
relative error loss 0.12741812
shape of L is 
torch.Size([])
memory (bytes)
4392583168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% |  7% |
memory (bytes)
4392583168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  261527550.0
relative error loss 0.12734576
shape of L is 
torch.Size([])
memory (bytes)
4395798528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4395798528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  261413500.0
relative error loss 0.12729022
shape of L is 
torch.Size([])
memory (bytes)
4399013888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4399013888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  261321090.0
relative error loss 0.12724523
shape of L is 
torch.Size([])
memory (bytes)
4402237440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4402237440
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  261123070.0
relative error loss 0.1271488
shape of L is 
torch.Size([])
memory (bytes)
4405456896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  5% |  7% |
memory (bytes)
4405456896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  261180670.0
relative error loss 0.12717685
shape of L is 
torch.Size([])
memory (bytes)
4408676352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4408676352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  261040380.0
relative error loss 0.12710854
time to take a step is 328.1670813560486
it  4 : 1468258304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4411904000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
4411904000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% |  7% |
error is  261040380.0
relative error loss 0.12710854
shape of L is 
torch.Size([])
memory (bytes)
4415102976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4415102976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  260923140.0
relative error loss 0.12705144
shape of L is 
torch.Size([])
memory (bytes)
4418347008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4418347008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  260776960.0
relative error loss 0.12698027
shape of L is 
torch.Size([])
memory (bytes)
4421554176
| ID | GPU | MEM |
------------------
|  0 | 26% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4421554176
| ID | GPU | MEM |
------------------
|  0 | 25% |  0% |
|  1 | 99% |  7% |
error is  260680580.0
relative error loss 0.12693334
shape of L is 
torch.Size([])
memory (bytes)
4424646656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4424773632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  260592380.0
relative error loss 0.12689039
shape of L is 
torch.Size([])
memory (bytes)
4427984896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4427984896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% |  7% |
error is  260518020.0
relative error loss 0.12685418
shape of L is 
torch.Size([])
memory (bytes)
4431196160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  7% |
memory (bytes)
4431196160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% |  7% |
error is  260465920.0
relative error loss 0.12682882
shape of L is 
torch.Size([])
memory (bytes)
4434415616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4434415616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% |  7% |
error is  260377220.0
relative error loss 0.12678562
shape of L is 
torch.Size([])
memory (bytes)
4437635072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4437635072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% |  7% |
error is  260305540.0
relative error loss 0.12675072
shape of L is 
torch.Size([])
memory (bytes)
4440862720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4440862720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  260239490.0
relative error loss 0.12671857
time to take a step is 309.3263928890228
it  5 : 1469263360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4444090368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
4444090368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% |  7% |
error is  260239490.0
relative error loss 0.12671857
shape of L is 
torch.Size([])
memory (bytes)
4447305728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% |  7% |
memory (bytes)
4447305728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% |  7% |
error is  260184200.0
relative error loss 0.12669164
shape of L is 
torch.Size([])
memory (bytes)
4450521088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4450521088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% |  7% |
error is  260128380.0
relative error loss 0.12666446
shape of L is 
torch.Size([])
memory (bytes)
4453736448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  7% |
memory (bytes)
4453736448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  260109440.0
relative error loss 0.12665524
shape of L is 
torch.Size([])
memory (bytes)
4456955904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4456955904
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  260047490.0
relative error loss 0.12662508
shape of L is 
torch.Size([])
memory (bytes)
4460183552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  7% |
memory (bytes)
4460183552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  260029950.0
relative error loss 0.12661654
shape of L is 
torch.Size([])
memory (bytes)
4463398912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4463398912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  260004740.0
relative error loss 0.12660426
shape of L is 
torch.Size([])
memory (bytes)
4466630656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4466630656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  259968000.0
relative error loss 0.12658636
shape of L is 
torch.Size([])
memory (bytes)
4469846016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4469846016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  259927170.0
relative error loss 0.12656648
shape of L is 
torch.Size([])
memory (bytes)
4473065472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4473065472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  259888640.0
relative error loss 0.12654772
time to take a step is 292.62317728996277
it  6 : 1468257792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4476284928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4476284928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% |  7% |
error is  259888640.0
relative error loss 0.12654772
shape of L is 
torch.Size([])
memory (bytes)
4479496192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4479496192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% |  7% |
error is  259851400.0
relative error loss 0.12652959
shape of L is 
torch.Size([])
memory (bytes)
4482719744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4482719744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  259759490.0
relative error loss 0.12648484
shape of L is 
torch.Size([])
memory (bytes)
4485939200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% |  7% |
memory (bytes)
4485939200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  259686910.0
relative error loss 0.1264495
shape of L is 
torch.Size([])
memory (bytes)
4489175040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4489175040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% |  7% |
error is  259601540.0
relative error loss 0.12640792
shape of L is 
torch.Size([])
memory (bytes)
4492390400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4492390400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% |  7% |
error is  259546240.0
relative error loss 0.126381
shape of L is 
torch.Size([])
memory (bytes)
4495609856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4495609856
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  259502200.0
relative error loss 0.12635955
shape of L is 
torch.Size([])
memory (bytes)
4498833408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% |  7% |
memory (bytes)
4498833408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  259463800.0
relative error loss 0.12634087
shape of L is 
torch.Size([])
memory (bytes)
4502048768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4502048768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% |  7% |
error is  259436290.0
relative error loss 0.12632746
shape of L is 
torch.Size([])
memory (bytes)
4505268224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4505268224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  259397380.0
relative error loss 0.12630852
time to take a step is 311.7023169994354
it  7 : 1468760576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4508422144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4508487680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  259397380.0
relative error loss 0.12630852
shape of L is 
torch.Size([])
memory (bytes)
4511694848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4511694848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% |  7% |
error is  259338880.0
relative error loss 0.12628002
shape of L is 
torch.Size([])
memory (bytes)
4514930688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% |  7% |
memory (bytes)
4514930688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% |  7% |
error is  259321860.0
relative error loss 0.12627174
shape of L is 
torch.Size([])
memory (bytes)
4518088704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4518088704
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  259262850.0
relative error loss 0.12624301
shape of L is 
torch.Size([])
memory (bytes)
4521365504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4521369600
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  259248000.0
relative error loss 0.12623577
shape of L is 
torch.Size([])
memory (bytes)
4524580864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4524580864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  259205380.0
relative error loss 0.12621503
shape of L is 
torch.Size([])
memory (bytes)
4527804416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4527804416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  259187460.0
relative error loss 0.1262063
shape of L is 
torch.Size([])
memory (bytes)
4530876416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4531036160
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  259154050.0
relative error loss 0.12619002
shape of L is 
torch.Size([])
memory (bytes)
4534255616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4534255616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% |  7% |
error is  259109000.0
relative error loss 0.12616809
shape of L is 
torch.Size([])
memory (bytes)
4537475072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4537475072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  259049090.0
relative error loss 0.12613893
time to take a step is 305.7592570781708
it  8 : 1468760576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4540698624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4540698624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  259049090.0
relative error loss 0.12613893
shape of L is 
torch.Size([])
memory (bytes)
4543905792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% |  7% |
memory (bytes)
4543905792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  259000450.0
relative error loss 0.12611523
shape of L is 
torch.Size([])
memory (bytes)
4547129344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  7% |
memory (bytes)
4547129344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  7% |
error is  258990460.0
relative error loss 0.12611037
shape of L is 
torch.Size([])
memory (bytes)
4550348800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4550348800
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  258930300.0
relative error loss 0.12608108
shape of L is 
torch.Size([])
memory (bytes)
4553564160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4553564160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  258915330.0
relative error loss 0.1260738
shape of L is 
torch.Size([])
memory (bytes)
4556783616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4556787712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% |  7% |
error is  258881540.0
relative error loss 0.12605733
shape of L is 
torch.Size([])
memory (bytes)
4560007168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% |  7% |
memory (bytes)
4560007168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  258887420.0
relative error loss 0.1260602
shape of L is 
torch.Size([])
memory (bytes)
4563218432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4563218432
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  258863740.0
relative error loss 0.12604867
shape of L is 
torch.Size([])
memory (bytes)
4566446080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% |  7% |
memory (bytes)
4566446080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 89% |  7% |
error is  258839940.0
relative error loss 0.12603708
shape of L is 
torch.Size([])
memory (bytes)
4569657344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4569657344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  258801400.0
relative error loss 0.12601832
time to take a step is 305.0835018157959
it  9 : 1469766144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4572868608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4572868608
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  258801400.0
relative error loss 0.12601832
shape of L is 
torch.Size([])
memory (bytes)
4576079872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4576079872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  258786820.0
relative error loss 0.12601121
shape of L is 
torch.Size([])
memory (bytes)
4579307520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4579307520
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  258769400.0
relative error loss 0.12600273
shape of L is 
torch.Size([])
memory (bytes)
4582531072
| ID | GPU | MEM |
------------------
|  0 |  1% |  0% |
|  1 | 13% |  7% |
memory (bytes)
4582531072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% |  7% |
error is  258743680.0
relative error loss 0.12599021
shape of L is 
torch.Size([])
memory (bytes)
4585750528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% |  7% |
memory (bytes)
4585750528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  258727420.0
relative error loss 0.12598228
shape of L is 
torch.Size([])
memory (bytes)
4588957696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4588957696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  258722180.0
relative error loss 0.12597974
shape of L is 
torch.Size([])
memory (bytes)
4592189440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4592189440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  258674560.0
relative error loss 0.12595655
shape of L is 
torch.Size([])
memory (bytes)
4595404800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4595404800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  258659840.0
relative error loss 0.12594938
shape of L is 
torch.Size([])
memory (bytes)
4598628352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4598628352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% |  7% |
error is  258642300.0
relative error loss 0.12594084
shape of L is 
torch.Size([])
memory (bytes)
4601851904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4601851904
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  258618750.0
relative error loss 0.12592937
time to take a step is 302.2294900417328
it  10 : 1469263360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4605067264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4605067264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  258618750.0
relative error loss 0.12592937
shape of L is 
torch.Size([])
memory (bytes)
4608278528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4608278528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  258603260.0
relative error loss 0.12592183
shape of L is 
torch.Size([])
memory (bytes)
4611506176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4611506176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  258571900.0
relative error loss 0.12590656
shape of L is 
torch.Size([])
memory (bytes)
4614721536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  7% |
memory (bytes)
4614729728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% |  7% |
error is  258560260.0
relative error loss 0.1259009
shape of L is 
torch.Size([])
memory (bytes)
4617961472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% |  7% |
memory (bytes)
4617961472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  258529920.0
relative error loss 0.12588613
shape of L is 
torch.Size([])
memory (bytes)
4621168640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% |  7% |
memory (bytes)
4621168640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  258539400.0
relative error loss 0.12589073
shape of L is 
torch.Size([])
memory (bytes)
4624396288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4624396288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  258516600.0
relative error loss 0.12587965
shape of L is 
torch.Size([])
memory (bytes)
4627611648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4627611648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% |  7% |
error is  258496510.0
relative error loss 0.12586986
shape of L is 
torch.Size([])
memory (bytes)
4630839296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4630839296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  258467460.0
relative error loss 0.1258557
shape of L is 
torch.Size([])
memory (bytes)
4634054656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4634054656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  258462600.0
relative error loss 0.12585333
time to take a step is 301.33306860923767
it  11 : 1468760576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4637265920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% |  7% |
memory (bytes)
4637265920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  258462600.0
relative error loss 0.12585333
shape of L is 
torch.Size([])
memory (bytes)
4640485376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  7% |
memory (bytes)
4640485376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  258437250.0
relative error loss 0.12584099
shape of L is 
torch.Size([])
memory (bytes)
4643713024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4643713024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  258427520.0
relative error loss 0.12583625
shape of L is 
torch.Size([])
memory (bytes)
4646920192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4646928384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  258415360.0
relative error loss 0.12583034
shape of L is 
torch.Size([])
memory (bytes)
4650151936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% |  7% |
memory (bytes)
4650151936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  258395000.0
relative error loss 0.12582043
shape of L is 
torch.Size([])
memory (bytes)
4653367296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4653367296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  258369400.0
relative error loss 0.12580796
shape of L is 
torch.Size([])
memory (bytes)
4656582656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4656582656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  258347650.0
relative error loss 0.12579736
shape of L is 
torch.Size([])
memory (bytes)
4659802112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4659802112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  258331400.0
relative error loss 0.12578945
shape of L is 
torch.Size([])
memory (bytes)
4663017472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4663017472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  258314240.0
relative error loss 0.1257811
shape of L is 
torch.Size([])
memory (bytes)
4666245120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4666245120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  258301440.0
relative error loss 0.12577486
time to take a step is 303.4367127418518
it  12 : 1469263360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4669472768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4669472768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  258301440.0
relative error loss 0.12577486
shape of L is 
torch.Size([])
memory (bytes)
4672675840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4672675840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 89% |  7% |
error is  258287230.0
relative error loss 0.12576795
shape of L is 
torch.Size([])
memory (bytes)
4675907584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4675907584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  258259840.0
relative error loss 0.12575461
shape of L is 
torch.Size([])
memory (bytes)
4679131136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4679131136
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  258280060.0
relative error loss 0.12576446
shape of L is 
torch.Size([])
memory (bytes)
4682354688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4682354688
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  258250880.0
relative error loss 0.12575024
shape of L is 
torch.Size([])
memory (bytes)
4685578240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4685578240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  258241150.0
relative error loss 0.1257455
shape of L is 
torch.Size([])
memory (bytes)
4688793600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4688793600
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  258212220.0
relative error loss 0.12573142
shape of L is 
torch.Size([])
memory (bytes)
4692004864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
4692004864
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  258209540.0
relative error loss 0.12573011
shape of L is 
torch.Size([])
memory (bytes)
4695236608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4695236608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  258184960.0
relative error loss 0.12571815
shape of L is 
torch.Size([])
memory (bytes)
4698460160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% |  7% |
memory (bytes)
4698460160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% |  7% |
error is  258174600.0
relative error loss 0.1257131
time to take a step is 300.97143959999084
it  13 : 1470268928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4701679616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4701679616
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  258174600.0
relative error loss 0.1257131
shape of L is 
torch.Size([])
memory (bytes)
4704890880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4704890880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  258161280.0
relative error loss 0.12570661
shape of L is 
torch.Size([])
memory (bytes)
4708110336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4708110336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  258146180.0
relative error loss 0.12569927
shape of L is 
torch.Size([])
memory (bytes)
4711333888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% |  7% |
memory (bytes)
4711333888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  258131330.0
relative error loss 0.12569204
shape of L is 
torch.Size([])
memory (bytes)
4714545152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% |  7% |
memory (bytes)
4714545152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  258120700.0
relative error loss 0.12568685
shape of L is 
torch.Size([])
memory (bytes)
4717776896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  7% |
memory (bytes)
4717776896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  258104060.0
relative error loss 0.12567876
shape of L is 
torch.Size([])
memory (bytes)
4720971776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% |  7% |
memory (bytes)
4720971776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  258096000.0
relative error loss 0.12567483
shape of L is 
torch.Size([])
memory (bytes)
4724211712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4724211712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  258082180.0
relative error loss 0.1256681
shape of L is 
torch.Size([])
memory (bytes)
4727422976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4727422976
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  258076030.0
relative error loss 0.12566511
shape of L is 
torch.Size([])
memory (bytes)
4730638336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4730638336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  258070140.0
relative error loss 0.12566224
time to take a step is 296.62021255493164
it  14 : 1468760576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4733857792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4733857792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  258070140.0
relative error loss 0.12566224
shape of L is 
torch.Size([])
memory (bytes)
4737069056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  7% |
memory (bytes)
4737069056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% |  7% |
error is  258064130.0
relative error loss 0.12565932
shape of L is 
torch.Size([])
memory (bytes)
4740296704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% |  7% |
memory (bytes)
4740296704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  258055680.0
relative error loss 0.1256552
shape of L is 
torch.Size([])
memory (bytes)
4743516160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% |  7% |
memory (bytes)
4743516160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% |  7% |
error is  258053120.0
relative error loss 0.12565395
shape of L is 
torch.Size([])
memory (bytes)
4746735616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4746735616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  258046980.0
relative error loss 0.12565096
shape of L is 
torch.Size([])
memory (bytes)
4749959168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% |  7% |
memory (bytes)
4749959168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  258041730.0
relative error loss 0.12564841
shape of L is 
torch.Size([])
memory (bytes)
4753174528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4753174528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  258033920.0
relative error loss 0.12564461
shape of L is 
torch.Size([])
memory (bytes)
4756398080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4756398080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  258024700.0
relative error loss 0.12564011
shape of L is 
torch.Size([])
memory (bytes)
4759617536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% |  7% |
memory (bytes)
4759617536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  258016770.0
relative error loss 0.12563625
shape of L is 
torch.Size([])
memory (bytes)
4762836992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4762836992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  258002820.0
relative error loss 0.12562945
time to take a step is 300.21789956092834
sum tnnu_Z after tensor(10188737., device='cuda:0')
shape of features
(3674,)
shape of features
(3674,)
number of orig particles 14695
number of new particles after remove low mass 12651
tnuZ shape should be parts x labs
torch.Size([14695, 702])
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
error without small mass is  323401700.0
relative error without small mass is  0.15747418
nnu_Z shape should be number of particles by maxV
(14695, 702)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
shape of features
(14695,)
Tue Jan 31 17:59:01 EST 2023
