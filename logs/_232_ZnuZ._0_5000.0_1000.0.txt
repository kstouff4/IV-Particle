Thu Feb 2 00:04:04 EST 2023
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  0% |  0% |
numbers of X: 23802936
numbers of Z: 17768
shape of features
(17768,)
shape of features
(17768,)
ZX	Vol	Parts	Cubes	Eps
Z	0.023972093983771592	17768	17.768	0.11049835399142147
X	0.021139290079567725	633	0.633	0.32202942958933717
X	0.022625261698983483	15005	15.005	0.11467071941302776
X	0.021812236496862543	2146	2.146	0.21661581451237813
X	0.021920777424169754	1175	1.175	0.26521985214004956
X	0.021814866908537046	71056	71.056	0.06746066282374803
X	0.02186834464191413	29022	29.022	0.09099751319676475
X	0.02191591317393301	45255	45.255	0.07852916880170197
X	0.021963015221168003	32868	32.868	0.08742578920116657
X	0.021850155074606258	3682	3.682	0.18104654712402093
X	0.021934436185996737	29394	29.394	0.0907031970847714
X	0.021844235231052413	5648	5.648	0.1569690580689815
X	0.02176406271765421	53486	53.486	0.07410257215391979
X	0.021899966108552164	4567	4.567	0.16863102392343968
X	0.021963745092630462	135487	135.487	0.05452591004080414
X	0.02191653167035506	23216	23.216	0.09809829912266774
X	0.021877934021498704	34075	34.075	0.08626940735769809
X	0.022089437062460318	50874	50.874	0.0757233096592597
X	0.021897985654655693	38936	38.936	0.08254376572061904
X	0.021959246093790054	96638	96.638	0.06102247678410304
X	0.022319219691638227	94915	94.915	0.061723145703467736
X	0.021788381354818153	5678	5.678	0.15655845491663106
X	0.02329712945109371	226727	226.727	0.04683814899284292
X	0.021936216208214978	13666	13.666	0.11708648466415932
X	0.021886806890421027	7415	7.415	0.14344665216896277
X	0.02160066203948822	89671	89.671	0.06222119527528836
X	0.022228988682615244	49026	49.026	0.07682414719257538
X	0.021881505761938185	23790	23.79	0.09725103994652191
X	0.021810544075966935	29168	29.168	0.09076532130483043
X	0.021820363309004404	19718	19.718	0.10343472679638774
X	0.02220063769278483	1213890	1213.89	0.02634685038311186
X	0.021855000549942653	3761	3.761	0.17978322339553554
X	0.0233144549323675	174889	174.889	0.05108431693759853
X	0.022347364929354196	9703	9.703	0.13206064980186125
X	0.021906765493013913	5261	5.261	0.1608804673334029
X	0.021936127546989148	3508	3.508	0.18423273213485303
X	0.02326413472185747	20440	20.44	0.10440837525586402
X	0.02226033700625614	44384	44.384	0.0794514385919486
X	0.02152693212995687	1341	1.341	0.25226158563634254
X	0.021857552194035738	3663	3.663	0.18137950060730224
X	0.0215715281680367	1406	1.406	0.24848405780967625
X	0.02190504501426375	3625	3.625	0.18214281587453654
X	0.02040951203970538	748	0.748	0.3010534958002316
X	0.020872916763491414	755	0.755	0.3023746624391112
X	0.02158390135877224	2346	2.346	0.20954018029560775
X	0.021325339736936	785	0.785	0.30061369883952943
X	0.02175087609117654	1560	1.56	0.24068593864559668
X	0.02191712221264239	3581	3.581	0.18291938649465836
X	0.021851650703551686	4091	4.091	0.17480409008316383
X	0.02170024933000185	1354	1.354	0.25212467601902055
X	0.021739233440627764	3896	3.896	0.17736792799244674
X	0.021928416576668552	8273	8.273	0.13839319939879
X	0.021154639434215774	1219	1.219	0.2589011466897233
X	0.02190527324304554	1745	1.745	0.232408004192779
X	0.021278193064805197	1487	1.487	0.2427772152142363
X	0.02252653614803309	5678	5.678	0.15830683859650987
X	0.021837085734174557	1466	1.466	0.2460482249571774
X	0.021656825798661856	1900	1.9	0.22505071691422335
X	0.02181221702059976	2037	2.037	0.2204125202139093
X	0.02187076797820801	1976	1.976	0.22285670121025744
X	0.021391307224899132	1325	1.325	0.25273990367195437
X	0.02175894984776325	5137	5.137	0.1617991075544609
X	0.021746705869854232	1213	1.213	0.2617244082570956
X	0.023348772846211067	4923	4.923	0.16801371758899142
X	0.021605022904392188	4523	4.523	0.16841316892169514
X	0.021635480893662757	1283	1.283	0.25643657364103284
X	0.021862244138864232	1546	1.546	0.24182159614949286
X	0.02119879281121498	2137	2.137	0.2148665790495744
X	0.021760705871648328	2532	2.532	0.2048344935866627
X	0.02185529642398676	2586	2.586	0.20369297921693263
X	0.021166077364617	1175	1.175	0.2621405286141619
X	0.021776619421215997	1401	1.401	0.24956524743010394
X	0.021856586408863288	3221	3.221	0.1893203083454891
X	0.021335218946041058	1983	1.983	0.2207622423144148
X	0.021913573440500416	900	0.9	0.2898390503404236
X	0.02192293994297622	1730	1.73	0.2331404141776191
X	0.021854696658053556	3155	3.155	0.1906258558036168
X	0.021925043843522007	1190	1.19	0.2641179005028383
X	0.02128744346341337	744	0.744	0.3058558963528694
X	0.021607936060141206	1976	1.976	0.2219603743483403
X	0.021696064774899496	6544	6.544	0.14911198765837774
X	0.02149237707601425	2304	2.304	0.21050737979791534
X	0.021308618961100805	780	0.78	0.30117591163133384
X	0.02194495148549432	3506	3.506	0.18429246166025715
X	0.021132215664832008	1023	1.023	0.2743825366690124
X	0.021872190600005652	1368	1.368	0.25192352712868665
X	0.02157503816582001	1272	1.272	0.2569339461527385
X	0.021147997780878523	2073	2.073	0.21688190438679567
X	0.020607587360036067	818	0.818	0.29315093383514035
X	0.02184482904411973	2879	2.879	0.19650291404590003
X	0.02180176051526645	1839	1.839	0.22801842691374294
X	0.02186521199408248	3087	3.087	0.19204618646751437
X	0.021863082351387328	1280	1.28	0.2575335455691599
X	0.021819760298488675	818	0.818	0.2987896552757169
X	0.021694136214597884	2099	2.099	0.2178261295183871
X	0.021288087665144208	2364	2.364	0.20804776720932414
X	0.021843060311695688	3083	3.083	0.19206430249006984
X	0.02132959743888842	1955	1.955	0.22179170476727408
X	0.021749105127415423	2070	2.07	0.21902331556922602
X	0.021825256916874782	2916	2.916	0.19560980269726122
X	0.021823040876026913	2358	2.358	0.20995383981237956
X	0.021899767586173256	6095	6.095	0.15316349410274666
X	0.02120905135547619	2284	2.284	0.21018822437459767
X	0.021917130004931076	4148	4.148	0.17417332606577324
X	0.0206239514353236	949	0.949	0.27906276199822466
X	0.02112508573814197	2132	2.132	0.21478490693537722
X	0.02185827858546803	1175	1.175	0.26496755404585554
X	0.02184783398095759	4061	4.061	0.17522327739616553
X	0.0208339369513303	636	0.636	0.3199666713403682
X	0.021691401360853044	1981	1.981	0.2220586583720077
X	0.021828208255244192	981	0.981	0.28126543231484147
X	0.021253084223734283	1261	1.261	0.25639071426789295
X	0.021033241217524555	849	0.849	0.2915185336361539
X	0.021447397127233606	838	0.838	0.294697775926615
X	0.021875725663971484	2831	2.831	0.19770039317352336
X	0.02169828840772648	3191	3.191	0.18945215851023745
X	0.020938704698161253	740	0.74	0.3047235581336067
X	0.02190491466939906	1813	1.813	0.22946401442647701
X	0.02138357524611738	845	0.845	0.2935899624616765
X	0.02190170047428355	5432	5.432	0.15916198909157406
X	0.02089476933683647	1096	1.096	0.2671421041342323
X	0.02193454514405734	7092	7.092	0.14569790329567417
X	0.021813950970834887	2068	2.068	0.2193114296984673
X	0.020971486644137376	1259	1.259	0.2553883973021658
X	0.02193636349124177	1792	1.792	0.23046706769210018
X	0.021440558355390726	1333	1.333	0.2524267140267248
X	0.02207153690167269	795	0.795	0.3027994966498255
X	0.021855011702055956	1729	1.729	0.2329442617535755
X	0.0215187671722719	2006	2.006	0.22054412131004428
X	0.022427446567575778	10329	10.329	0.1294912781889249
X	0.021349637268354326	1655	1.655	0.234529512527749
X	0.02222823959821715	4708	4.708	0.16776043300377166
X	0.02103031535810238	852	0.852	0.29116247124062133
X	0.021817871931577016	2528	2.528	0.2051217782469653
X	0.021032501368979703	614	0.614	0.3247687134653782
X	0.021667521651652547	1818	1.818	0.22842227542752414
X	0.02191633040776596	1857	1.857	0.22767672994896962
X	0.021663215589921465	915	0.915	0.287144570792934
X	0.021468061611321972	1033	1.033	0.27493551080916295
X	0.02156342645479969	1616	1.616	0.23718768619097855
X	0.021809949726017495	2496	2.496	0.20596971172666373
X	0.021797553826329873	3073	3.073	0.1921387965392866
X	0.021413953379867134	540	0.54	0.34101045774200794
X	0.022238145641362093	4898	4.898	0.1655871366944757
X	0.021942316856012185	5340	5.34	0.16016971705243385
X	0.022207872968408014	4663	4.663	0.16824694341818786
X	0.021750425035828296	1580	1.58	0.23966441404925454
X	0.02161122931010833	1756	1.756	0.23087929448908162
X	0.021435933245232464	1702	1.702	0.23266315521145028
X	0.021788223011642995	1488	1.488	0.24464684920092225
X	0.021236327929507335	1440	1.44	0.24522927281946896
X	0.02190525846208249	1484	1.484	0.245304111250162
X	0.022446714791519648	2233	2.233	0.21581817800678885
X	0.021462171389757676	2483	2.483	0.2052259681128614
X	0.021780399308360336	2149	2.149	0.21640957594366778
X	0.021676868439400008	1713	1.713	0.2330306570023477
X	0.0227092609343805	6568	6.568	0.15121328635360545
X	0.021807588127277736	2325	2.325	0.21089270614493466
X	0.021537256443558268	1511	1.511	0.24246104444120856
X	0.021900509560646476	2556	2.556	0.20462771597402013
X	0.021066046876756006	544	0.544	0.33832029346369163
X	0.023356049267161335	43997	43.997	0.08097024239524024
X	0.021464342307820804	1142	1.142	0.2658788718030141
X	0.02160637526676395	3053	3.053	0.19199284176600617
X	0.021305299786180388	717	0.717	0.3097344398763003
X	0.019464745610333473	1824	1.824	0.2201614614249465
X	0.021016714498560008	1021	1.021	0.2740604464322957
X	0.023057225162924938	2264	2.264	0.21675886721139556
X	0.021300536845836937	1901	1.901	0.22377048893650006
X	0.02217485346196651	1875	1.875	0.2278346787671075
X	0.020469207917368355	542	0.542	0.33550622586331535
X	0.021042585202612714	1498	1.498	0.24128431650391807
X	0.021202245741300623	889	0.889	0.2878458910020619
X	0.02179763213834618	4046	4.046	0.17530507149218147
X	0.021628860988632516	1141	1.141	0.2666342908360027
X	0.021935683242956003	3846	3.846	0.1786682137033895
X	0.021741980421823418	2212	2.212	0.21420913750760306
X	0.021807997100917725	2932	2.932	0.19520185401973333
X	0.021383317690218475	1465	1.465	0.2443875933032765
X	0.021629993504093288	2451	2.451	0.2066510863061569
X	0.0217924553083736	1988	1.988	0.22214161656563072
X	0.021810975378239363	2542	2.542	0.20472294187910142
X	0.021186699575715676	1359	1.359	0.24981275977768774
X	0.02170702861209475	1179	1.179	0.2640556864860195
X	0.021384954345374005	1942	1.942	0.2224776391366723
X	0.022524668278208607	5327	5.327	0.16170566535784475
X	0.021933633368610754	3976	3.976	0.1766938368222377
X	0.02166425762457231	1652	1.652	0.23581852730143124
X	0.02164734146357441	2431	2.431	0.20727163328661025
X	0.021949286184309656	5493	5.493	0.15868538651474226
X	0.021922067305272488	9014	9.014	0.13447905574748706
X	0.021325874537270747	1403	1.403	0.24771352890530945
X	0.021258874929199582	404	0.404	0.37473101314089313
X	0.021902626707678962	2589	2.589	0.20376115028287015
X	0.02170976702561555	1125	1.125	0.2682260043588223
X	0.02268666951813337	1160	1.16	0.26942480991992634
X	0.02181196224552636	2234	2.234	0.2137324789199342
X	0.02191043026656033	1364	1.364	0.2523164187821517
X	0.02089905943604276	785	0.785	0.2985971751326146
X	0.021483995140797206	1037	1.037	0.27464946857257855
X	0.021802531012395914	1099	1.099	0.2707092361910641
X	0.022516025517221365	3369	3.369	0.188363567465933
X	0.0215392583180544	1180	1.18	0.2632992239982602
X	0.022363229527367478	5527	5.527	0.1593486366935968
X	0.02188885313172022	2311	2.311	0.21157987388568722
X	0.021447317757265816	1897	1.897	0.22444083982888333
X	0.02042637408813936	752	0.752	0.3006015024366498
X	0.021921912261412803	2587	2.587	0.2038734484094247
X	0.021111453651943727	2261	2.261	0.2105745390079724
X	0.0219120410012592	2004	2.004	0.22195335028942756
X	0.022950164096350985	4960	4.96	0.1666356959120296
X	0.021976430248448926	3255	3.255	0.18900300914535492
X	0.02121491816582948	1279	1.279	0.25502941349536185
X	0.021250801984706722	2097	2.097	0.21640085119250488
X	0.021252364330289767	1139	1.139	0.2652331742054195
X	0.021570369512205503	921	0.921	0.2861097458398286
X	0.021014973794901734	745	0.745	0.3044090268205146
X	0.020044376948411338	783	0.783	0.29472044647389545
X	0.021468100046293206	602	0.602	0.3291539261741869
X	0.021879955478507233	2103	2.103	0.21830769981870868
X	0.02158514763283412	2156	2.156	0.2155272010399533
X	0.020864108658120877	816	0.816	0.29460258891209684
X	0.021285245809948134	1199	1.199	0.2608674115562904
X	0.02145861703151215	2564	2.564	0.20303047752020495
X	0.021229506359341236	957	0.957	0.28098037385676955
X	0.021323577934002205	2903	2.903	0.194388699377229
X	0.022234122290740812	3236	3.236	0.19010950265527163
X	0.021105417029933567	1450	1.45	0.2441604473057759
X	0.021708270079321062	818	0.818	0.2982798876385012
X	0.021418986832603265	3790	3.79	0.17812296045886783
X	0.019564353923457633	589	0.589	0.32145244887886276
X	0.021917094113467876	1106	1.106	0.2706092360179197
X	0.020853787282193604	1234	1.234	0.25661966075370507
X	0.021786079310399768	2548	2.548	0.2044842606421576
X	0.022003621843422333	3631	3.631	0.18231510050600583
X	0.021914635038691378	1751	1.751	0.2321753089410066
X	0.0210763246606222	1022	1.022	0.2742298091076903
X	0.022001874635040927	2578	2.578	0.20435828886486587
X	0.022267279241453775	5303	5.303	0.16133001570896796
X	0.021418564934672098	843	0.843	0.293982129575742
X	0.02192232577637206	3958	3.958	0.17693086977805406
X	0.022089895536186455	11220	11.22	0.12533343285288567
X	0.021953864266521104	6014	6.014	0.1539746351460826
X	0.021876183364815806	4071	4.071	0.1751553810976348
X	0.02066875440557492	2416	2.416	0.20452157332085596
X	0.0218311063120063	10977	10.977	0.12575657287752678
X	0.02144681913279447	26894	26.894	0.09273332450200265
X	0.021769939292804276	56324	56.324	0.07284301437586745
X	0.02215365345046385	2991	2.991	0.19492893719660517
X	0.021865224587437983	26288	26.288	0.09404422506183038
X	0.02113200181659953	49627	49.627	0.07523285428165186
X	0.021561816911463697	42190	42.19	0.0799512533275985
X	0.02332855270349868	216037	216.037	0.047619688784174566
X	0.021723037448356873	1270	1.27	0.25765521868042307
X	0.021471753742350847	13704	13.704	0.11614665923475075
X	0.02182767332830818	50083	50.083	0.07581800803366028
X	0.022023031179591306	218064	218.064	0.04656908517030273
X	0.021963801897638257	2491	2.491	0.20659093607778128
X	0.022152132337086627	20204	20.204	0.1031160003058734
X	0.023276211971495174	94364	94.364	0.06271459960096176
X	0.02200338064429415	164711	164.711	0.05111977527537535
X	0.02189991726203727	39118	39.118	0.08241797578809384
X	0.021529377811066498	40719	40.719	0.08086205827988974
X	0.021943884713512447	15468	15.468	0.11236380822451393
X	0.021979892313734442	7846	7.846	0.14096969242765411
X	0.021850664288640655	8685	8.685	0.13600815228237184
X	0.02208102769477251	44845	44.845	0.07896508091024708
X	0.02149457307512957	4106	4.106	0.17363474204247148
X	0.021983663439215442	3171	3.171	0.1906783015552984
X	0.021794305735651814	66293	66.293	0.06901737786311896
X	0.02172625112274347	20764	20.764	0.10152147639420989
X	0.02197236038846898	252644	252.644	0.04430532312927385
X	0.021879605341820586	64523	64.523	0.06973352900550972
X	0.02161828961151739	952	0.952	0.28317927187582287
X	0.021897740050247364	11329	11.329	0.12456687602387545
X	0.02179688703728474	5237	5.237	0.16085601294036792
X	0.02189370906637537	9620	9.62	0.13153704774849775
X	0.021967693915031894	45590	45.59	0.0783979955304571
X	0.02181189439776001	3761	3.761	0.17966494590729118
X	0.021871985448086734	24412	24.412	0.0964039766813875
X	0.02080958717313789	845	0.845	0.290939207757321
X	0.02191128148122166	3162	3.162	0.19064933911803666
X	0.02181960323657927	14733	14.733	0.11398609793290643
X	0.02182332714605662	44943	44.943	0.07859947311252725
X	0.021825480987906487	27005	27.005	0.09314793124380183
X	0.021781502106725456	2784	2.784	0.19852088313221328
X	0.02197409254392344	159600	159.6	0.05163680397540918
X	0.02201916299637411	8534	8.534	0.137156393193083
X	0.021892170143243045	13364	13.364	0.11788289222323202
X	0.02196200295234001	59747	59.747	0.07163390292284527
X	0.021673080029428495	2321	2.321	0.21057905128112694
X	0.022044940048984624	51781	51.781	0.07522799660056313
X	0.022687047665197062	64743	64.743	0.07050096388461748
X	0.023346800867419432	144971	144.971	0.054406244406133925
X	0.021953381847547034	10083	10.083	0.12960961524914572
X	0.021845962220990542	19404	19.404	0.10403032871519698
X	0.021105751370058004	11811	11.811	0.12134951625684368
X	0.021662697381558894	3316	3.316	0.18693875112648123
X	0.02223677434572996	24008	24.008	0.09747769897949445
X	0.022051650714231773	8248	8.248	0.13879190951189171
X	0.021597647153757938	21463	21.463	0.10020867937599509
X	0.022121260411158587	58020	58.02	0.07251213870829275
X	0.02204490810290561	27272	27.272	0.09315305481656591
X	0.02142402934308778	4172	4.172	0.17252509885052256
X	0.022176042992726793	8591	8.591	0.1371766244901102
X	0.02205529147936099	36957	36.957	0.08419223344324435
X	0.02194752907588907	15115	15.115	0.11323807914926737
X	0.021333835574158532	1928	1.928	0.22283700083676736
X	0.022334857835915812	7947	7.947	0.14112154630422868
X	0.023114700847687877	244883	244.883	0.04553128214144749
X	0.021608610718907604	7407	7.407	0.14288769606673535
X	0.021607285338824084	60004	60.004	0.07114428213473184
X	0.021625474897195532	3787	3.787	0.1787406979408847
X	0.021349072185962006	8745	8.745	0.13465001225850207
X	0.0219179701252836	15407	15.407	0.11246759699671649
X	0.022464095422593425	22333	22.333	0.10019528611533435
X	0.02187123377024916	31217	31.217	0.08881657863270788
X	0.02184571925087526	4330	4.33	0.1715113309898734
X	0.022051413650614374	84398	84.398	0.06392951754270636
X	0.02202974095088268	69944	69.944	0.06803822498027558
X	0.022012621156745474	4279	4.279	0.172627437663233
X	0.021779744449779145	36042	36.042	0.08454370946089909
X	0.022526935145396186	71791	71.791	0.06795332344724354
X	0.023316976259019594	143583	143.583	0.05455774384945972
X	0.022065894816752873	7590	7.59	0.14272276668408132
X	0.02166401299453642	1664	1.664	0.23524940220856336
X	0.021895711697447584	6644	6.644	0.14881377970463996
X	0.022073613805176213	14499	14.499	0.11503900852247859
X	0.021798633356280003	30558	30.558	0.08935141751231532
X	0.021793142154451047	5134	5.134	0.161915339871901
X	0.021672628650952402	2153	2.153	0.21591816795060526
X	0.023202574438177328	62353	62.353	0.07192716456472484
X	0.0221957880453129	91984	91.984	0.062256699419911195
X	0.0216190789033072	141798	141.798	0.05342227276091289
X	0.021652908564861014	3753	3.753	0.17935461629518387
X	0.02182048671030621	11121	11.121	0.12519112175607894
X	0.02187130435578138	9936	9.936	0.13008314665634887
X	0.021737194855190118	3433	3.433	0.18500206030230038
X	0.0219119448214798	6641	6.641	0.14887295715869017
X	0.022043560762830407	3591	3.591	0.183100184200308
X	0.02143108391534023	201092	201.092	0.047411436544583
X	0.022365364204215487	27945	27.945	0.09284473627101222
X	0.022001899490128804	7831	7.831	0.14110670502956826
X	0.0221598051085348	5235	5.235	0.16176444926591668
X	0.0219060387805984	3156	3.156	0.19075486334168767
X	0.02205258429794803	243303	243.303	0.04491975121770943
X	0.021877001788826223	5818	5.818	0.1555027337884944
X	0.02184647386547265	78700	78.7	0.06523322841001879
X	0.02176283199024277	1560	1.56	0.2407300301983119
X	0.021828699242180397	2809	2.809	0.19807304251920407
X	0.021885667522782842	1997	1.997	0.22212319455072427
X	0.021986504249924058	16418	16.418	0.11022462862489692
X	0.02200364384371255	35363	35.363	0.08537194355557302
X	0.02161643075303167	5116	5.116	0.1616656738190868
X	0.021451302240302	4143	4.143	0.17300005567201332
X	0.021818350621209123	3714	3.714	0.18043744647523294
X	0.02309924667263949	118138	118.138	0.05804115784511787
X	0.02175762625959175	17720	17.72	0.1070818662612292
X	0.021821797319880255	9179	9.179	0.1334642804249247
X	0.02320178341887814	33041	33.041	0.0888838235414187
X	0.022305114440318095	25595	25.595	0.0955175352461813
X	0.021827639116140003	3324	3.324	0.18726153682139848
X	0.021872004112137692	3256	3.256	0.18868384927268578
X	0.021885063471030003	6839	6.839	0.14736185130380525
X	0.021817989158589037	762	0.762	0.3059284855169803
X	0.021714534997544326	4053	4.053	0.17498116753804885
X	0.021960637349689248	16414	16.414	0.11019033507683071
X	0.02171765823711516	1565	1.565	0.24030688701877662
X	0.021547772895562818	2955	2.955	0.19391659115934376
X	0.021739662544397617	7209	7.209	0.14447496268529472
X	0.02184098404993479	3376	3.376	0.1863330622100027
X	0.022248699606081113	254950	254.95	0.044355744383377256
X	0.02193363708217111	54793	54.793	0.07369904461118587
X	0.022014007262669524	32473	32.473	0.08784672278035077
X	0.021856802147632207	28053	28.053	0.09201721888659037
X	0.021747141835071836	2968	2.968	0.19422841380419278
X	0.021797840837839005	12721	12.721	0.11966415401549667
X	0.023252296887662102	414375	414.375	0.03828459917537575
X	0.02203153372049578	447876	447.876	0.036640577795140496
X	0.021937569858223257	41112	41.112	0.08110979505337736
X	0.022060673501771003	26265	26.265	0.09435113321087728
X	0.021838238995742405	1783	1.783	0.23050946201450676
X	0.021970934519137248	88458	88.458	0.06285943473186678
X	0.021850955684954456	33483	33.483	0.08673918911281504
X	0.0224372470721955	221584	221.584	0.04660978047280098
X	0.021813895296992392	5214	5.214	0.16113407827252185
X	0.02168831493144526	68832	68.832	0.06804744251759035
X	0.023054315029257626	102598	102.598	0.06079545449046467
X	0.021486440610068845	46105	46.105	0.07753042244040016
X	0.0218139792911312	5500	5.5	0.15829142993503253
X	0.023810609283661098	9235	9.235	0.13712328311891311
X	0.02169719790857985	9775	9.775	0.13044545784992467
X	0.021849885525252216	2975	2.975	0.1943811133567341
X	0.022077207597428344	57556	57.556	0.07265817956495113
X	0.0215539120284104	23996	23.996	0.09648558127061697
X	0.021349817185818003	1857	1.857	0.22569784841486815
X	0.02198006526197644	19994	19.994	0.10320714472829305
X	0.022502061206291304	12040	12.04	0.12317765678926347
X	0.02115327073216774	832	0.832	0.29404788985481206
X	0.022003734818700365	36251	36.251	0.08466920273158678
X	0.022007243276161135	51398	51.398	0.07537138134870007
X	0.021863862516680786	14818	14.818	0.11384459919540527
X	0.021865360863117383	63823	63.823	0.06997235298164728
X	0.02185765985426916	50073	50.073	0.07585776049510605
X	0.0219783270659665	4266	4.266	0.17271282730184556
X	0.021720949974711756	38263	38.263	0.08280055844412051
X	0.02200943987285089	66222	66.222	0.06926846532396762
X	0.022075093091750403	99517	99.517	0.06053433224595519
X	0.021975476069227257	4334	4.334	0.17179735050163908
X	0.02208560904378872	65536	65.536	0.0695894092610078
X	0.022072725394693474	70547	70.547	0.06788791490303481
X	0.022038686386477	19788	19.788	0.10365603706698615
X	0.02325885527997375	19954	19.954	0.10524128031320105
X	0.021624363931395007	2949	2.949	0.19427765664158964
X	0.021852378967681465	7259	7.259	0.14439117228630524
X	0.021650229010194023	4033	4.033	0.17509675230031396
X	0.021691779384525888	26157	26.157	0.0939512181585242
X	0.02197929542002631	114934	114.934	0.057613166891854035
X	0.022031077896663982	78754	78.754	0.06540149987262509
X	0.021907615204127512	253690	253.69	0.044200846351387414
X	0.02214884475231582	22192	22.192	0.09993513690876588
X	0.021910921661834944	71022	71.022	0.06757031072365703
X	0.021608174536247055	9603	9.603	0.13103994941166985
X	0.022287585498290012	74991	74.991	0.06673463545408313
X	0.021252517156029258	1295	1.295	0.25412469417971056
X	0.021940351301804467	9290	9.29	0.13317088555006343
X	0.022004050102922054	116329	116.329	0.05740347847714618
X	0.021906652644869767	9127	9.127	0.13389036449069672
X	0.021799731419285602	2108	2.108	0.21786803285145104
X	0.021899064077877353	7697	7.697	0.14169940368332362
X	0.021821642640914508	22411	22.411	0.09911561222833583
X	0.02168359455848932	16804	16.804	0.10886949333740888
X	0.022418974602754722	65788	65.788	0.06984838254295941
X	0.02161845818159553	8309	8.309	0.13753882801861336
X	0.021686780920572805	15850	15.85	0.1110167753825676
X	0.021615125145042487	2102	2.102	0.21745781211947846
X	0.022393803126032872	2525	2.525	0.20699292002204012
X	0.021966065908154395	14836	14.836	0.11397558218055712
X	0.021904465983746884	26349	26.349	0.09402777890364163
X	0.02183857355729072	3649	3.649	0.1815585900818071
X	0.021982529078509186	69942	69.942	0.0679902340957796
X	0.021573723246541478	2492	2.492	0.20533312733019948
X	0.023270938456056718	15997	15.997	0.11330744612102377
X	0.02192503067935508	2211	2.211	0.21484099532171505
X	0.021964273667836036	66148	66.148	0.06924685326260019
X	0.021422421401568553	1889	1.889	0.2246702318030034
X	0.021855969580981783	19273	19.273	0.10428141577211623
X	0.021900643221261028	9112	9.112	0.13395154271427165
X	0.021900270591797305	14337	14.337	0.11516762173310022
X	0.02185225303388917	22328	22.328	0.0992846553134646
X	0.021953530793479503	169919	169.919	0.05055383944241698
X	0.02184864827505025	4859	4.859	0.16505393832862533
X	0.022723867271209224	10733	10.733	0.1284065087809621
X	0.021772420616001327	44272	44.272	0.07893310577952697
X	0.021729387977906203	4353	4.353	0.17090428132966207
X	0.022006833409543982	239444	239.444	0.045128531223673995
X	0.021367355839695047	2411	2.411	0.20694321446268896
X	0.021588945021092876	68107	68.107	0.06818359406801625
X	0.022156115833521745	177824	177.824	0.04994604268201236
X	0.020945642335249783	1882	1.882	0.2232670777282842
X	0.02176954168852841	101571	101.571	0.0598448136878466
X	0.021995820879115252	11501	11.501	0.12412755195477515
X	0.022009317940104894	56311	56.311	0.07311465719504144
X	0.0219413808300191	5313	5.313	0.160438298131182
X	0.02186211233156118	47127	47.127	0.07741184927467702
X	0.021928682748947523	912	0.912	0.28862851531096784
X	0.021985519367500005	4165	4.165	0.17411675898182155
X	0.021328598571898463	1899	1.899	0.22394727650766452
X	0.021992262691405244	25250	25.25	0.09549988398146454
X	0.021910682318565144	15190	15.19	0.1129881009815199
X	0.021957802022858245	80086	80.086	0.06496469752979767
X	0.0219313753123777	9534	9.534	0.13200698332037503
X	0.021570629886499656	3976	3.976	0.1757136433732337
X	0.02197431706625557	3392	3.392	0.18641742912842343
X	0.023185832136257797	12402	12.402	0.1231904344287374
X	0.0222482070792043	1462	1.462	0.24780831909184606
X	0.02331162833373974	90267	90.267	0.06368168317858681
X	0.021956379369474505	97158	97.158	0.06091076489760585
X	0.02188365285983804	4129	4.129	0.17435121493506422
X	0.02186433010614451	8944	8.944	0.13471050422454708
X	0.021584488645409353	10468	10.468	0.12727976069937363
X	0.023414044280753055	825994	825.994	0.0304906381690434
X	0.021990471706137278	5583	5.583	0.15792672858054
X	0.02188146024005689	14270	14.27	0.11531455018508856
X	0.02175772622570444	80873	80.873	0.06455600571692982
X	0.021419526634288647	25353	25.353	0.09453519059441128
X	0.021680112626815507	3036	3.036	0.19256909479328835
X	0.021128654786666104	2877	2.877	0.19437660622615566
X	0.023295865686593267	157692	157.692	0.052863558892345945
X	0.022017249924063967	27167	27.167	0.09323388824370402
X	0.021700273445150433	44723	44.723	0.0785798917305557
X	0.022056958234267933	64248	64.248	0.07002105812201281
X	0.023601078231088497	10277	10.277	0.1319335239580588
X	0.021876746639292136	21264	21.264	0.10095145695929526
X	0.022023156567043196	75145	75.145	0.06642422446305896
X	0.022261961679940316	49918	49.918	0.07640153509614504
X	0.02207091575784931	4883	4.883	0.1653399782936994
X	0.022312033885765983	38716	38.716	0.08321779927788868
X	0.021819887040581253	2991	2.991	0.1939450478845212
X	0.021934847480363832	16834	16.834	0.10922341857915291
X	0.021356964172809877	16782	16.782	0.10836738679594063
X	0.021857965881842283	4641	4.641	0.16762265881209532
X	0.021956983185945512	42453	42.453	0.08027027691620299
X	0.021868076242848103	16859	16.859	0.1090585174544919
X	0.021462506706852718	3403	3.403	0.18475923906587605
X	0.021903216966123712	18838	18.838	0.10515365453712275
X	0.022022737844215254	6052	6.052	0.1538122044538215
X	0.02190368197818889	58800	58.8	0.07195261789101183
X	0.02158139439546314	48620	48.62	0.07628190244175
X	0.021957851908269713	8291	8.291	0.13835482589834214
X	0.021953586418791606	4723	4.723	0.16688956304564354
X	0.02173738934120177	17644	17.644	0.10720213754615926
X	0.021290460809581066	2261	2.261	0.21116802901102824
X	0.021407713042552956	4581	4.581	0.16718731673078452
X	0.022033345528050585	18327	18.327	0.10633176474869077
X	0.021994959747249156	121113	121.113	0.05662968378281553
X	0.02230636994968199	4706	4.706	0.16798054757150938
X	0.02139096599257926	1192	1.192	0.26180908565714794
X	0.021844967170394133	2531	2.531	0.20512554616174997
X	0.02205175411184754	135663	135.663	0.05457502058497602
X	0.02209168842871896	116474	116.474	0.057455724632565264
X	0.021700497028280474	49591	49.591	0.07591989609643429
X	0.02191803720054154	4699	4.699	0.16708292577777029
X	0.022329727301993948	29653	29.653	0.09097839283342658
X	0.02207552074294548	70536	70.536	0.0678943095940444
X	0.022178010139491805	1883	1.883	0.2275223610754536
X	0.021608605563885545	6331	6.331	0.15056295953552296
X	0.021980950209299722	22503	22.503	0.09922063773408153
X	0.022261336223866616	112093	112.093	0.05834328955154647
X	0.02258874180288139	47695	47.695	0.07794825478921567
X	0.02204785131923443	58852	58.852	0.07208889165322425
X	0.022044793026984175	14241	14.241	0.11567917422516846
X	0.022131401536829125	10376	10.376	0.12872403702029292
X	0.021779572963256204	1892	1.892	0.22579244656472114
X	0.021748624326044477	26155	26.155	0.09403561207946425
X	0.021843605063346393	15010	15.01	0.11332208101586445
X	0.022667421907564732	36917	36.917	0.08499471243731754
X	0.022075680329512657	15528	15.528	0.11244311234485674
X	0.02224635316987453	142481	142.481	0.053847716492005766
X	0.021639248881924755	5200	5.2	0.1608469929368194
X	0.021557952612857964	8625	8.625	0.1357113392833951
X	0.021843586435258404	11707	11.707	0.12310984249939719
X	0.022043592939199205	193958	193.958	0.048438597700842696
X	0.021921465285654855	20197	20.197	0.10276870994814108
X	0.021898432491034597	12192	12.192	0.12155682786767469
X	0.021406904647460017	1512	1.512	0.24191754196815352
X	0.0218797279618172	32663	32.663	0.08749742705238375
X	0.02181950889675094	2126	2.126	0.21731710141407054
X	0.02194431123578474	7932	7.932	0.1403825357591755
X	0.021786903550493206	3092	3.092	0.19171319655755037
X	0.02198399665061445	8645	8.645	0.13649410144549312
X	0.021833621217559853	10822	10.822	0.1263589710279776
X	0.02184891089517868	3055	3.055	0.1926664933318004
X	0.021559436803676207	2108	2.108	0.2170645663864759
X	0.02190773328655398	121843	121.843	0.05644155285262453
X	0.022054178757716567	187410	187.41	0.04900413300118705
X	0.02203299540159572	76320	76.32	0.06609141661366183
X	0.02106579838005496	7298	7.298	0.14238296136496065
X	0.021968903113377614	15773	15.773	0.11167724459066712
X	0.021816222389551394	9010	9.01	0.13428213996461769
X	0.023188941655759802	6805	6.805	0.15048190003019776
X	0.02195712985097771	68747	68.747	0.06835557481618292
X	0.022425581080082566	31377	31.377	0.08940821596185713
X	0.021987377745162483	37575	37.575	0.08364207843786134
X	0.021827123694350366	6144	6.144	0.15258614496320044
X	0.02297542108353867	131389	131.389	0.055920128084301024
X	0.02197302665284769	16812	16.812	0.10933440194686377
X	0.023522880936186152	28806	28.806	0.09346934184629366
X	0.022459587647754423	79355	79.355	0.06565621272050735
X	0.022275010069467572	32232	32.232	0.08841179707620969
X	0.02198235060472945	84715	84.715	0.0637829504833418
X	0.02319736011992843	110479	110.479	0.05943646111415643
X	0.022086845375402012	52802	52.802	0.07478728591993423
X	0.021942060745300942	14156	14.156	0.11572991114798888
X	0.02184425352775774	21040	21.04	0.10125826715177726
X	0.021964145490926377	37436	37.436	0.08371596564139963
X	0.021935982886133278	5106	5.106	0.1625643939814957
X	0.022006303324190797	66254	66.254	0.06925402145079218
X	0.023087738867010855	387143	387.143	0.03906939296858997
X	0.022641796420916994	51757	51.757	0.07591261090813263
X	0.022036915349772264	35738	35.738	0.08511514688850552
X	0.02239871621082044	33477	33.477	0.08746323401981433
X	0.02191904015999593	4703	4.703	0.16703809095074215
X	0.022430871047337913	12725	12.725	0.12079884519116851
X	0.02118959878252835	14407	14.407	0.11372337131173586
X	0.022269845023897436	39288	39.288	0.0827597349719389
X	0.023382709879212576	188563	188.563	0.04986694781551251
X	0.021530638686155044	2400	2.4	0.20778549332973595
X	0.022014643474415445	29656	29.656	0.09054539364361147
X	0.02223179311870622	17560	17.56	0.10818075139422653
X	0.021745208974476484	23567	23.567	0.0973538418422825
X	0.021366528857346023	1669	1.669	0.2339335684633601
X	0.023274335816439864	74018	74.018	0.06800068956821194
X	0.023163909593191742	94364	94.364	0.062613575773033
X	0.021554297325622248	2412	2.412	0.20751628721864343
X	0.023390889049962678	459760	459.76	0.037054291604847245
X	0.02204913686287062	22408	22.408	0.09946329273523721
X	0.022205975205515052	20638	20.638	0.10247094644804569
X	0.02190140465389391	11639	11.639	0.12345786635473305
X	0.021789835879015604	9996	9.996	0.12966096133472785
X	0.02107904614811029	2689	2.689	0.19864915291875634
X	0.02200712951898569	3996	3.996	0.17659536740552895
X	0.021742130991146934	5640	5.64	0.15679817419309403
X	0.022466229615586766	119544	119.544	0.05727970904348076
X	0.0236869662658118	163042	163.042	0.05256983926602403
X	0.021944238973520203	9447	9.447	0.1324368582456672
X	0.021903821265395338	72640	72.64	0.06705760173503603
X	0.021853113250820767	2422	2.422	0.20818354164646344
X	0.02188472688019376	12381	12.381	0.12090988343316035
X	0.021275794766757204	3315	3.315	0.1858378132671003
X	0.021808470112967545	25763	25.763	0.09459675866013813
X	0.021896423298275337	7414	7.414	0.14347410782851505
X	0.022010153829652445	646950	646.95	0.03240291788531786
X	0.021892958374694658	3939	3.939	0.17713572404245942
X	0.022076932031893676	53209	53.209	0.0745849508509535
X	0.02199303754946528	8522	8.522	0.13716644888205254
X	0.02186329515086604	90305	90.305	0.06232581127069516
X	0.022350421824843154	121996	121.996	0.05679542812383541
X	0.02180978262843262	16071	16.071	0.11071406890146235
X	0.02181032785325879	4782	4.782	0.16583805951148237
X	0.022756877002926643	53270	53.27	0.07531416320809646
X	0.02219725811725453	7001	7.001	0.1469083287535546
X	0.021422558306708188	10994	10.994	0.12490271162673351
X	0.02204759545421804	91582	91.582	0.06220859163991041
X	0.022030686519988817	24877	24.877	0.09603068090248175
X	0.02334046704625776	26476	26.476	0.09588537915369932
X	0.021686628104830137	18349	18.349	0.10572877744642445
X	0.02204700462792065	43160	43.16	0.07993851062112327
X	0.021861186449760744	144416	144.416	0.05329495978169634
X	0.02141693062046657	23747	23.747	0.09661609771279249
X	0.022325448512082725	6492	6.492	0.1509410014615138
X	0.022270290083892846	106530	106.53	0.059349625426087904
X	0.02096066851686898	24239	24.239	0.09527160399244854
X	0.021502396424069235	38156	38.156	0.08259897817844322
X	0.02181696091786394	25296	25.296	0.0951876928049418
X	0.022356663121355314	40754	40.754	0.08186135215436445
X	0.02196568939380786	20254	20.254	0.10274121031468854
X	0.021430755377992963	47362	47.362	0.07677193410423852
X	0.02191729723266083	39956	39.956	0.08185935737831403
X	0.021828761249787963	17327	17.327	0.10800285561896424
X	0.021707733825182523	3020	3.02	0.19299046310130297
X	0.02314941686773205	16164	16.164	0.11271898548296914
X	0.021653186020899363	18934	18.934	0.10457466404334594
X	0.02196598944941413	47637	47.637	0.07725657588922963
X	0.022054768031379707	61458	61.458	0.07106264317964749
X	0.02183065627963988	4319	4.319	0.17161735170404593
X	0.022015113586754707	22092	22.092	0.09988385572213347
X	0.021906883721217157	45869	45.869	0.07816645992344834
X	0.021991532382461463	167968	167.968	0.050778084472802465
X	0.022220293077566858	34294	34.294	0.08653209394420142
X	0.02310185006555732	34641	34.641	0.08736794354220481
X	0.02173250420844207	49546	49.546	0.0759801928204072
X	0.021969021025651188	20936	20.936	0.10161839509570701
X	0.0222099543840395	1382	1.382	0.25235575182159503
X	0.021891800912128882	4882	4.882	0.16490275255821957
X	0.022091158704184703	119304	119.304	0.05699732803268086
X	0.02182810196229688	30434	30.434	0.08951290379031134
X	0.02198014612035956	48562	48.562	0.07677939226344926
X	0.021733401517424134	1331	1.331	0.25369771132342317
X	0.02191085337370684	3772	3.772	0.1797611631385479
X	0.022029074195301646	165253	165.253	0.051083694354946065
X	0.021786067515396138	66844	66.844	0.06881854266469675
X	0.021909262938128265	20896	20.896	0.1015909143875685
X	0.021731206516128483	2550	2.55	0.20425900853339912
X	0.02180810049450734	1954	1.954	0.2234761050504203
X	0.02196846932744549	27260	27.26	0.09305891447336168
X	0.02220605224628534	34648	34.648	0.08621795722881873
X	0.021931219520401633	58267	58.267	0.07220158108843996
X	0.02194999869847735	2737	2.737	0.20016427466101847
X	0.02190607412566555	43426	43.426	0.07960461414889652
X	0.021574524139129903	8317	8.317	0.13740151078155435
X	0.021273409485552494	14337	14.337	0.11405813644187371
X	0.02332699940036574	186745	186.745	0.049988483893875287
X	0.021561227031320882	6010	6.01	0.15308512747827135
X	0.021662304779570395	37789	37.789	0.08307042278643446
X	0.022085502417141325	65553	65.553	0.0695832811704521
X	0.02195216997460569	40146	40.146	0.08177333789062409
X	0.023268704664475094	28304	28.304	0.09367883128595154
X	0.021942545266655546	58172	58.172	0.07225329699012065
X	0.02169131956817269	2460	2.46	0.20659364360670343
X	0.021981580855416134	22656	22.656	0.09899772826932889
X	0.022051194906502695	25250	25.25	0.09558511092631813
X	0.021960366434548916	81649	81.649	0.06454999864658564
X	0.02193214378850084	68732	68.732	0.06833460698957683
X	0.02134542806100756	33574	33.574	0.08598722380598461
X	0.022046435932892704	35133	35.133	0.08561326337570714
X	0.0219237279589886	9758	9.758	0.1309738308101179
X	0.021712623203269856	2307	2.307	0.21113240774906297
X	0.021933200793770792	9494	9.494	0.13219578122997308
X	0.021621849168089284	8512	8.512	0.13644377906312824
X	0.021956281073086517	128911	128.911	0.05543145369210636
X	0.022229706009432346	52730	52.73	0.07498228126222431
X	0.022257028882805657	3098	3.098	0.19295761624626132
X	0.021905102601862726	14107	14.107	0.11579866211849979
X	0.021988790070240873	7207	7.207	0.1450381562592719
X	0.022210664659658697	402475	402.475	0.03807216278330174
X	0.021813249029992005	11402	11.402	0.12414039043147819
X	0.021974360912627614	15536	15.536	0.11225155007926907
X	0.02197663049034475	70224	70.224	0.06789302837814441
X	0.02206066803802799	41511	41.511	0.08100002531882097
X	0.0214935745752857	12482	12.482	0.11986015181740206
X	0.021969254411999328	19274	19.274	0.10445947076138297
X	0.021960086142072486	6358	6.358	0.15116033251942954
time for making epsilon is 1.2750556468963623
epsilons are
[0.32202942958933717, 0.11467071941302776, 0.21661581451237813, 0.26521985214004956, 0.06746066282374803, 0.09099751319676475, 0.07852916880170197, 0.08742578920116657, 0.18104654712402093, 0.0907031970847714, 0.1569690580689815, 0.07410257215391979, 0.16863102392343968, 0.05452591004080414, 0.09809829912266774, 0.08626940735769809, 0.0757233096592597, 0.08254376572061904, 0.06102247678410304, 0.061723145703467736, 0.15655845491663106, 0.04683814899284292, 0.11708648466415932, 0.14344665216896277, 0.06222119527528836, 0.07682414719257538, 0.09725103994652191, 0.09076532130483043, 0.10343472679638774, 0.02634685038311186, 0.17978322339553554, 0.05108431693759853, 0.13206064980186125, 0.1608804673334029, 0.18423273213485303, 0.10440837525586402, 0.0794514385919486, 0.25226158563634254, 0.18137950060730224, 0.24848405780967625, 0.18214281587453654, 0.3010534958002316, 0.3023746624391112, 0.20954018029560775, 0.30061369883952943, 0.24068593864559668, 0.18291938649465836, 0.17480409008316383, 0.25212467601902055, 0.17736792799244674, 0.13839319939879, 0.2589011466897233, 0.232408004192779, 0.2427772152142363, 0.15830683859650987, 0.2460482249571774, 0.22505071691422335, 0.2204125202139093, 0.22285670121025744, 0.25273990367195437, 0.1617991075544609, 0.2617244082570956, 0.16801371758899142, 0.16841316892169514, 0.25643657364103284, 0.24182159614949286, 0.2148665790495744, 0.2048344935866627, 0.20369297921693263, 0.2621405286141619, 0.24956524743010394, 0.1893203083454891, 0.2207622423144148, 0.2898390503404236, 0.2331404141776191, 0.1906258558036168, 0.2641179005028383, 0.3058558963528694, 0.2219603743483403, 0.14911198765837774, 0.21050737979791534, 0.30117591163133384, 0.18429246166025715, 0.2743825366690124, 0.25192352712868665, 0.2569339461527385, 0.21688190438679567, 0.29315093383514035, 0.19650291404590003, 0.22801842691374294, 0.19204618646751437, 0.2575335455691599, 0.2987896552757169, 0.2178261295183871, 0.20804776720932414, 0.19206430249006984, 0.22179170476727408, 0.21902331556922602, 0.19560980269726122, 0.20995383981237956, 0.15316349410274666, 0.21018822437459767, 0.17417332606577324, 0.27906276199822466, 0.21478490693537722, 0.26496755404585554, 0.17522327739616553, 0.3199666713403682, 0.2220586583720077, 0.28126543231484147, 0.25639071426789295, 0.2915185336361539, 0.294697775926615, 0.19770039317352336, 0.18945215851023745, 0.3047235581336067, 0.22946401442647701, 0.2935899624616765, 0.15916198909157406, 0.2671421041342323, 0.14569790329567417, 0.2193114296984673, 0.2553883973021658, 0.23046706769210018, 0.2524267140267248, 0.3027994966498255, 0.2329442617535755, 0.22054412131004428, 0.1294912781889249, 0.234529512527749, 0.16776043300377166, 0.29116247124062133, 0.2051217782469653, 0.3247687134653782, 0.22842227542752414, 0.22767672994896962, 0.287144570792934, 0.27493551080916295, 0.23718768619097855, 0.20596971172666373, 0.1921387965392866, 0.34101045774200794, 0.1655871366944757, 0.16016971705243385, 0.16824694341818786, 0.23966441404925454, 0.23087929448908162, 0.23266315521145028, 0.24464684920092225, 0.24522927281946896, 0.245304111250162, 0.21581817800678885, 0.2052259681128614, 0.21640957594366778, 0.2330306570023477, 0.15121328635360545, 0.21089270614493466, 0.24246104444120856, 0.20462771597402013, 0.33832029346369163, 0.08097024239524024, 0.2658788718030141, 0.19199284176600617, 0.3097344398763003, 0.2201614614249465, 0.2740604464322957, 0.21675886721139556, 0.22377048893650006, 0.2278346787671075, 0.33550622586331535, 0.24128431650391807, 0.2878458910020619, 0.17530507149218147, 0.2666342908360027, 0.1786682137033895, 0.21420913750760306, 0.19520185401973333, 0.2443875933032765, 0.2066510863061569, 0.22214161656563072, 0.20472294187910142, 0.24981275977768774, 0.2640556864860195, 0.2224776391366723, 0.16170566535784475, 0.1766938368222377, 0.23581852730143124, 0.20727163328661025, 0.15868538651474226, 0.13447905574748706, 0.24771352890530945, 0.37473101314089313, 0.20376115028287015, 0.2682260043588223, 0.26942480991992634, 0.2137324789199342, 0.2523164187821517, 0.2985971751326146, 0.27464946857257855, 0.2707092361910641, 0.188363567465933, 0.2632992239982602, 0.1593486366935968, 0.21157987388568722, 0.22444083982888333, 0.3006015024366498, 0.2038734484094247, 0.2105745390079724, 0.22195335028942756, 0.1666356959120296, 0.18900300914535492, 0.25502941349536185, 0.21640085119250488, 0.2652331742054195, 0.2861097458398286, 0.3044090268205146, 0.29472044647389545, 0.3291539261741869, 0.21830769981870868, 0.2155272010399533, 0.29460258891209684, 0.2608674115562904, 0.20303047752020495, 0.28098037385676955, 0.194388699377229, 0.19010950265527163, 0.2441604473057759, 0.2982798876385012, 0.17812296045886783, 0.32145244887886276, 0.2706092360179197, 0.25661966075370507, 0.2044842606421576, 0.18231510050600583, 0.2321753089410066, 0.2742298091076903, 0.20435828886486587, 0.16133001570896796, 0.293982129575742, 0.17693086977805406, 0.12533343285288567, 0.1539746351460826, 0.1751553810976348, 0.20452157332085596, 0.12575657287752678, 0.09273332450200265, 0.07284301437586745, 0.19492893719660517, 0.09404422506183038, 0.07523285428165186, 0.0799512533275985, 0.047619688784174566, 0.25765521868042307, 0.11614665923475075, 0.07581800803366028, 0.04656908517030273, 0.20659093607778128, 0.1031160003058734, 0.06271459960096176, 0.05111977527537535, 0.08241797578809384, 0.08086205827988974, 0.11236380822451393, 0.14096969242765411, 0.13600815228237184, 0.07896508091024708, 0.17363474204247148, 0.1906783015552984, 0.06901737786311896, 0.10152147639420989, 0.04430532312927385, 0.06973352900550972, 0.28317927187582287, 0.12456687602387545, 0.16085601294036792, 0.13153704774849775, 0.0783979955304571, 0.17966494590729118, 0.0964039766813875, 0.290939207757321, 0.19064933911803666, 0.11398609793290643, 0.07859947311252725, 0.09314793124380183, 0.19852088313221328, 0.05163680397540918, 0.137156393193083, 0.11788289222323202, 0.07163390292284527, 0.21057905128112694, 0.07522799660056313, 0.07050096388461748, 0.054406244406133925, 0.12960961524914572, 0.10403032871519698, 0.12134951625684368, 0.18693875112648123, 0.09747769897949445, 0.13879190951189171, 0.10020867937599509, 0.07251213870829275, 0.09315305481656591, 0.17252509885052256, 0.1371766244901102, 0.08419223344324435, 0.11323807914926737, 0.22283700083676736, 0.14112154630422868, 0.04553128214144749, 0.14288769606673535, 0.07114428213473184, 0.1787406979408847, 0.13465001225850207, 0.11246759699671649, 0.10019528611533435, 0.08881657863270788, 0.1715113309898734, 0.06392951754270636, 0.06803822498027558, 0.172627437663233, 0.08454370946089909, 0.06795332344724354, 0.05455774384945972, 0.14272276668408132, 0.23524940220856336, 0.14881377970463996, 0.11503900852247859, 0.08935141751231532, 0.161915339871901, 0.21591816795060526, 0.07192716456472484, 0.062256699419911195, 0.05342227276091289, 0.17935461629518387, 0.12519112175607894, 0.13008314665634887, 0.18500206030230038, 0.14887295715869017, 0.183100184200308, 0.047411436544583, 0.09284473627101222, 0.14110670502956826, 0.16176444926591668, 0.19075486334168767, 0.04491975121770943, 0.1555027337884944, 0.06523322841001879, 0.2407300301983119, 0.19807304251920407, 0.22212319455072427, 0.11022462862489692, 0.08537194355557302, 0.1616656738190868, 0.17300005567201332, 0.18043744647523294, 0.05804115784511787, 0.1070818662612292, 0.1334642804249247, 0.0888838235414187, 0.0955175352461813, 0.18726153682139848, 0.18868384927268578, 0.14736185130380525, 0.3059284855169803, 0.17498116753804885, 0.11019033507683071, 0.24030688701877662, 0.19391659115934376, 0.14447496268529472, 0.1863330622100027, 0.044355744383377256, 0.07369904461118587, 0.08784672278035077, 0.09201721888659037, 0.19422841380419278, 0.11966415401549667, 0.03828459917537575, 0.036640577795140496, 0.08110979505337736, 0.09435113321087728, 0.23050946201450676, 0.06285943473186678, 0.08673918911281504, 0.04660978047280098, 0.16113407827252185, 0.06804744251759035, 0.06079545449046467, 0.07753042244040016, 0.15829142993503253, 0.13712328311891311, 0.13044545784992467, 0.1943811133567341, 0.07265817956495113, 0.09648558127061697, 0.22569784841486815, 0.10320714472829305, 0.12317765678926347, 0.29404788985481206, 0.08466920273158678, 0.07537138134870007, 0.11384459919540527, 0.06997235298164728, 0.07585776049510605, 0.17271282730184556, 0.08280055844412051, 0.06926846532396762, 0.06053433224595519, 0.17179735050163908, 0.0695894092610078, 0.06788791490303481, 0.10365603706698615, 0.10524128031320105, 0.19427765664158964, 0.14439117228630524, 0.17509675230031396, 0.0939512181585242, 0.057613166891854035, 0.06540149987262509, 0.044200846351387414, 0.09993513690876588, 0.06757031072365703, 0.13103994941166985, 0.06673463545408313, 0.25412469417971056, 0.13317088555006343, 0.05740347847714618, 0.13389036449069672, 0.21786803285145104, 0.14169940368332362, 0.09911561222833583, 0.10886949333740888, 0.06984838254295941, 0.13753882801861336, 0.1110167753825676, 0.21745781211947846, 0.20699292002204012, 0.11397558218055712, 0.09402777890364163, 0.1815585900818071, 0.0679902340957796, 0.20533312733019948, 0.11330744612102377, 0.21484099532171505, 0.06924685326260019, 0.2246702318030034, 0.10428141577211623, 0.13395154271427165, 0.11516762173310022, 0.0992846553134646, 0.05055383944241698, 0.16505393832862533, 0.1284065087809621, 0.07893310577952697, 0.17090428132966207, 0.045128531223673995, 0.20694321446268896, 0.06818359406801625, 0.04994604268201236, 0.2232670777282842, 0.0598448136878466, 0.12412755195477515, 0.07311465719504144, 0.160438298131182, 0.07741184927467702, 0.28862851531096784, 0.17411675898182155, 0.22394727650766452, 0.09549988398146454, 0.1129881009815199, 0.06496469752979767, 0.13200698332037503, 0.1757136433732337, 0.18641742912842343, 0.1231904344287374, 0.24780831909184606, 0.06368168317858681, 0.06091076489760585, 0.17435121493506422, 0.13471050422454708, 0.12727976069937363, 0.0304906381690434, 0.15792672858054, 0.11531455018508856, 0.06455600571692982, 0.09453519059441128, 0.19256909479328835, 0.19437660622615566, 0.052863558892345945, 0.09323388824370402, 0.0785798917305557, 0.07002105812201281, 0.1319335239580588, 0.10095145695929526, 0.06642422446305896, 0.07640153509614504, 0.1653399782936994, 0.08321779927788868, 0.1939450478845212, 0.10922341857915291, 0.10836738679594063, 0.16762265881209532, 0.08027027691620299, 0.1090585174544919, 0.18475923906587605, 0.10515365453712275, 0.1538122044538215, 0.07195261789101183, 0.07628190244175, 0.13835482589834214, 0.16688956304564354, 0.10720213754615926, 0.21116802901102824, 0.16718731673078452, 0.10633176474869077, 0.05662968378281553, 0.16798054757150938, 0.26180908565714794, 0.20512554616174997, 0.05457502058497602, 0.057455724632565264, 0.07591989609643429, 0.16708292577777029, 0.09097839283342658, 0.0678943095940444, 0.2275223610754536, 0.15056295953552296, 0.09922063773408153, 0.05834328955154647, 0.07794825478921567, 0.07208889165322425, 0.11567917422516846, 0.12872403702029292, 0.22579244656472114, 0.09403561207946425, 0.11332208101586445, 0.08499471243731754, 0.11244311234485674, 0.053847716492005766, 0.1608469929368194, 0.1357113392833951, 0.12310984249939719, 0.048438597700842696, 0.10276870994814108, 0.12155682786767469, 0.24191754196815352, 0.08749742705238375, 0.21731710141407054, 0.1403825357591755, 0.19171319655755037, 0.13649410144549312, 0.1263589710279776, 0.1926664933318004, 0.2170645663864759, 0.05644155285262453, 0.04900413300118705, 0.06609141661366183, 0.14238296136496065, 0.11167724459066712, 0.13428213996461769, 0.15048190003019776, 0.06835557481618292, 0.08940821596185713, 0.08364207843786134, 0.15258614496320044, 0.055920128084301024, 0.10933440194686377, 0.09346934184629366, 0.06565621272050735, 0.08841179707620969, 0.0637829504833418, 0.05943646111415643, 0.07478728591993423, 0.11572991114798888, 0.10125826715177726, 0.08371596564139963, 0.1625643939814957, 0.06925402145079218, 0.03906939296858997, 0.07591261090813263, 0.08511514688850552, 0.08746323401981433, 0.16703809095074215, 0.12079884519116851, 0.11372337131173586, 0.0827597349719389, 0.04986694781551251, 0.20778549332973595, 0.09054539364361147, 0.10818075139422653, 0.0973538418422825, 0.2339335684633601, 0.06800068956821194, 0.062613575773033, 0.20751628721864343, 0.037054291604847245, 0.09946329273523721, 0.10247094644804569, 0.12345786635473305, 0.12966096133472785, 0.19864915291875634, 0.17659536740552895, 0.15679817419309403, 0.05727970904348076, 0.05256983926602403, 0.1324368582456672, 0.06705760173503603, 0.20818354164646344, 0.12090988343316035, 0.1858378132671003, 0.09459675866013813, 0.14347410782851505, 0.03240291788531786, 0.17713572404245942, 0.0745849508509535, 0.13716644888205254, 0.06232581127069516, 0.05679542812383541, 0.11071406890146235, 0.16583805951148237, 0.07531416320809646, 0.1469083287535546, 0.12490271162673351, 0.06220859163991041, 0.09603068090248175, 0.09588537915369932, 0.10572877744642445, 0.07993851062112327, 0.05329495978169634, 0.09661609771279249, 0.1509410014615138, 0.059349625426087904, 0.09527160399244854, 0.08259897817844322, 0.0951876928049418, 0.08186135215436445, 0.10274121031468854, 0.07677193410423852, 0.08185935737831403, 0.10800285561896424, 0.19299046310130297, 0.11271898548296914, 0.10457466404334594, 0.07725657588922963, 0.07106264317964749, 0.17161735170404593, 0.09988385572213347, 0.07816645992344834, 0.050778084472802465, 0.08653209394420142, 0.08736794354220481, 0.0759801928204072, 0.10161839509570701, 0.25235575182159503, 0.16490275255821957, 0.05699732803268086, 0.08951290379031134, 0.07677939226344926, 0.25369771132342317, 0.1797611631385479, 0.051083694354946065, 0.06881854266469675, 0.1015909143875685, 0.20425900853339912, 0.2234761050504203, 0.09305891447336168, 0.08621795722881873, 0.07220158108843996, 0.20016427466101847, 0.07960461414889652, 0.13740151078155435, 0.11405813644187371, 0.049988483893875287, 0.15308512747827135, 0.08307042278643446, 0.0695832811704521, 0.08177333789062409, 0.09367883128595154, 0.07225329699012065, 0.20659364360670343, 0.09899772826932889, 0.09558511092631813, 0.06454999864658564, 0.06833460698957683, 0.08598722380598461, 0.08561326337570714, 0.1309738308101179, 0.21113240774906297, 0.13219578122997308, 0.13644377906312824, 0.05543145369210636, 0.07498228126222431, 0.19295761624626132, 0.11579866211849979, 0.1450381562592719, 0.03807216278330174, 0.12414039043147819, 0.11225155007926907, 0.06789302837814441, 0.08100002531882097, 0.11986015181740206, 0.10445947076138297, 0.15116033251942954]
0.11049835399142147
Making ranges
torch.Size([20423, 2])
We keep 6.50e+06/3.16e+08 =  2% of the original kernel matrix.

torch.Size([1364, 2])
We keep 3.95e+04/4.01e+05 =  9% of the original kernel matrix.

torch.Size([6347, 2])
We keep 6.18e+05/1.12e+07 =  5% of the original kernel matrix.

torch.Size([16061, 2])
We keep 6.53e+06/2.25e+08 =  2% of the original kernel matrix.

torch.Size([18068, 2])
We keep 6.15e+06/2.67e+08 =  2% of the original kernel matrix.

torch.Size([3535, 2])
We keep 2.82e+05/4.61e+06 =  6% of the original kernel matrix.

torch.Size([9030, 2])
We keep 1.42e+06/3.81e+07 =  3% of the original kernel matrix.

torch.Size([1838, 2])
We keep 1.06e+05/1.38e+06 =  7% of the original kernel matrix.

torch.Size([6892, 2])
We keep 9.19e+05/2.09e+07 =  4% of the original kernel matrix.

torch.Size([69681, 2])
We keep 1.17e+08/5.05e+09 =  2% of the original kernel matrix.

torch.Size([36797, 2])
We keep 2.21e+07/1.26e+09 =  1% of the original kernel matrix.

torch.Size([29162, 2])
We keep 1.82e+07/8.42e+08 =  2% of the original kernel matrix.

torch.Size([24609, 2])
We keep 1.02e+07/5.16e+08 =  1% of the original kernel matrix.

torch.Size([44151, 2])
We keep 3.79e+07/2.05e+09 =  1% of the original kernel matrix.

torch.Size([30026, 2])
We keep 1.49e+07/8.04e+08 =  1% of the original kernel matrix.

torch.Size([33008, 2])
We keep 1.91e+07/1.08e+09 =  1% of the original kernel matrix.

torch.Size([26345, 2])
We keep 1.13e+07/5.84e+08 =  1% of the original kernel matrix.

torch.Size([5403, 2])
We keep 8.91e+05/1.36e+07 =  6% of the original kernel matrix.

torch.Size([10498, 2])
We keep 2.00e+06/6.54e+07 =  3% of the original kernel matrix.

torch.Size([23761, 2])
We keep 5.96e+07/8.64e+08 =  6% of the original kernel matrix.

torch.Size([21742, 2])
We keep 9.71e+06/5.22e+08 =  1% of the original kernel matrix.

torch.Size([7559, 2])
We keep 1.40e+06/3.19e+07 =  4% of the original kernel matrix.

torch.Size([11993, 2])
We keep 2.78e+06/1.00e+08 =  2% of the original kernel matrix.

torch.Size([50595, 2])
We keep 7.13e+07/2.86e+09 =  2% of the original kernel matrix.

torch.Size([31535, 2])
We keep 1.68e+07/9.50e+08 =  1% of the original kernel matrix.

torch.Size([6680, 2])
We keep 9.38e+05/2.09e+07 =  4% of the original kernel matrix.

torch.Size([11424, 2])
We keep 2.40e+06/8.11e+07 =  2% of the original kernel matrix.

torch.Size([131063, 2])
We keep 2.38e+08/1.84e+10 =  1% of the original kernel matrix.

torch.Size([51987, 2])
We keep 3.77e+07/2.41e+09 =  1% of the original kernel matrix.

torch.Size([23864, 2])
We keep 1.60e+07/5.39e+08 =  2% of the original kernel matrix.

torch.Size([22068, 2])
We keep 8.48e+06/4.13e+08 =  2% of the original kernel matrix.

torch.Size([34384, 2])
We keep 2.31e+07/1.16e+09 =  1% of the original kernel matrix.

torch.Size([26758, 2])
We keep 1.17e+07/6.05e+08 =  1% of the original kernel matrix.

torch.Size([51940, 2])
We keep 4.86e+07/2.59e+09 =  1% of the original kernel matrix.

torch.Size([32445, 2])
We keep 1.62e+07/9.04e+08 =  1% of the original kernel matrix.

torch.Size([37785, 2])
We keep 3.54e+07/1.52e+09 =  2% of the original kernel matrix.

torch.Size([27691, 2])
We keep 1.32e+07/6.92e+08 =  1% of the original kernel matrix.

torch.Size([99722, 2])
We keep 1.60e+08/9.34e+09 =  1% of the original kernel matrix.

torch.Size([44817, 2])
We keep 2.82e+07/1.72e+09 =  1% of the original kernel matrix.

torch.Size([97113, 2])
We keep 1.43e+08/9.01e+09 =  1% of the original kernel matrix.

torch.Size([44353, 2])
We keep 2.79e+07/1.69e+09 =  1% of the original kernel matrix.

torch.Size([7113, 2])
We keep 2.23e+06/3.22e+07 =  6% of the original kernel matrix.

torch.Size([11746, 2])
We keep 2.80e+06/1.01e+08 =  2% of the original kernel matrix.

torch.Size([204298, 2])
We keep 6.48e+08/5.14e+10 =  1% of the original kernel matrix.

torch.Size([67267, 2])
We keep 5.85e+07/4.03e+09 =  1% of the original kernel matrix.

torch.Size([14949, 2])
We keep 5.88e+06/1.87e+08 =  3% of the original kernel matrix.

torch.Size([17389, 2])
We keep 5.59e+06/2.43e+08 =  2% of the original kernel matrix.

torch.Size([9410, 2])
We keep 1.96e+06/5.50e+07 =  3% of the original kernel matrix.

torch.Size([13314, 2])
We keep 3.42e+06/1.32e+08 =  2% of the original kernel matrix.

torch.Size([70153, 2])
We keep 2.39e+08/8.04e+09 =  2% of the original kernel matrix.

torch.Size([35624, 2])
We keep 2.71e+07/1.59e+09 =  1% of the original kernel matrix.

torch.Size([49910, 2])
We keep 3.76e+07/2.40e+09 =  1% of the original kernel matrix.

torch.Size([31763, 2])
We keep 1.56e+07/8.71e+08 =  1% of the original kernel matrix.

torch.Size([24160, 2])
We keep 2.10e+07/5.66e+08 =  3% of the original kernel matrix.

torch.Size([22232, 2])
We keep 8.65e+06/4.23e+08 =  2% of the original kernel matrix.

torch.Size([27044, 2])
We keep 4.49e+07/8.51e+08 =  5% of the original kernel matrix.

torch.Size([23452, 2])
We keep 1.06e+07/5.18e+08 =  2% of the original kernel matrix.

torch.Size([20391, 2])
We keep 1.23e+07/3.89e+08 =  3% of the original kernel matrix.

torch.Size([20134, 2])
We keep 7.45e+06/3.50e+08 =  2% of the original kernel matrix.

torch.Size([1380724, 2])
We keep 9.63e+09/1.47e+12 =  0% of the original kernel matrix.

torch.Size([182964, 2])
We keep 2.75e+08/2.16e+10 =  1% of the original kernel matrix.

torch.Size([5306, 2])
We keep 1.16e+06/1.41e+07 =  8% of the original kernel matrix.

torch.Size([10317, 2])
We keep 2.05e+06/6.68e+07 =  3% of the original kernel matrix.

torch.Size([156999, 2])
We keep 6.27e+08/3.06e+10 =  2% of the original kernel matrix.

torch.Size([58013, 2])
We keep 4.73e+07/3.11e+09 =  1% of the original kernel matrix.

torch.Size([11122, 2])
We keep 4.82e+06/9.41e+07 =  5% of the original kernel matrix.

torch.Size([14565, 2])
We keep 4.31e+06/1.72e+08 =  2% of the original kernel matrix.

torch.Size([7785, 2])
We keep 1.11e+06/2.77e+07 =  4% of the original kernel matrix.

torch.Size([12323, 2])
We keep 2.61e+06/9.35e+07 =  2% of the original kernel matrix.

torch.Size([5368, 2])
We keep 7.61e+05/1.23e+07 =  6% of the original kernel matrix.

torch.Size([10581, 2])
We keep 1.98e+06/6.23e+07 =  3% of the original kernel matrix.

torch.Size([19134, 2])
We keep 1.87e+07/4.18e+08 =  4% of the original kernel matrix.

torch.Size([19714, 2])
We keep 7.40e+06/3.63e+08 =  2% of the original kernel matrix.

torch.Size([45050, 2])
We keep 3.63e+07/1.97e+09 =  1% of the original kernel matrix.

torch.Size([30459, 2])
We keep 1.44e+07/7.89e+08 =  1% of the original kernel matrix.

torch.Size([2181, 2])
We keep 1.42e+05/1.80e+06 =  7% of the original kernel matrix.

torch.Size([7293, 2])
We keep 1.04e+06/2.38e+07 =  4% of the original kernel matrix.

torch.Size([5569, 2])
We keep 6.71e+05/1.34e+07 =  5% of the original kernel matrix.

torch.Size([10573, 2])
We keep 2.05e+06/6.51e+07 =  3% of the original kernel matrix.

torch.Size([2502, 2])
We keep 1.55e+05/1.98e+06 =  7% of the original kernel matrix.

torch.Size([7768, 2])
We keep 1.06e+06/2.50e+07 =  4% of the original kernel matrix.

torch.Size([5655, 2])
We keep 6.82e+05/1.31e+07 =  5% of the original kernel matrix.

torch.Size([10787, 2])
We keep 2.06e+06/6.44e+07 =  3% of the original kernel matrix.

torch.Size([1294, 2])
We keep 5.76e+04/5.60e+05 = 10% of the original kernel matrix.

torch.Size([5940, 2])
We keep 6.49e+05/1.33e+07 =  4% of the original kernel matrix.

torch.Size([1442, 2])
We keep 6.87e+04/5.70e+05 = 12% of the original kernel matrix.

torch.Size([6241, 2])
We keep 6.69e+05/1.34e+07 =  4% of the original kernel matrix.

torch.Size([3472, 2])
We keep 3.58e+05/5.50e+06 =  6% of the original kernel matrix.

torch.Size([8649, 2])
We keep 1.51e+06/4.17e+07 =  3% of the original kernel matrix.

torch.Size([1465, 2])
We keep 5.58e+04/6.16e+05 =  9% of the original kernel matrix.

torch.Size([6442, 2])
We keep 6.89e+05/1.39e+07 =  4% of the original kernel matrix.

torch.Size([2542, 2])
We keep 1.75e+05/2.43e+06 =  7% of the original kernel matrix.

torch.Size([7838, 2])
We keep 1.13e+06/2.77e+07 =  4% of the original kernel matrix.

torch.Size([5512, 2])
We keep 6.65e+05/1.28e+07 =  5% of the original kernel matrix.

torch.Size([10660, 2])
We keep 2.01e+06/6.36e+07 =  3% of the original kernel matrix.

torch.Size([5881, 2])
We keep 8.68e+05/1.67e+07 =  5% of the original kernel matrix.

torch.Size([10687, 2])
We keep 2.25e+06/7.27e+07 =  3% of the original kernel matrix.

torch.Size([2133, 2])
We keep 1.44e+05/1.83e+06 =  7% of the original kernel matrix.

torch.Size([7243, 2])
We keep 1.04e+06/2.41e+07 =  4% of the original kernel matrix.

torch.Size([5983, 2])
We keep 7.89e+05/1.52e+07 =  5% of the original kernel matrix.

torch.Size([10976, 2])
We keep 2.15e+06/6.92e+07 =  3% of the original kernel matrix.

torch.Size([10871, 2])
We keep 2.69e+06/6.84e+07 =  3% of the original kernel matrix.

torch.Size([14466, 2])
We keep 3.82e+06/1.47e+08 =  2% of the original kernel matrix.

torch.Size([2394, 2])
We keep 1.11e+05/1.49e+06 =  7% of the original kernel matrix.

torch.Size([7665, 2])
We keep 9.38e+05/2.17e+07 =  4% of the original kernel matrix.

torch.Size([2950, 2])
We keep 1.93e+05/3.05e+06 =  6% of the original kernel matrix.

torch.Size([8311, 2])
We keep 1.18e+06/3.10e+07 =  3% of the original kernel matrix.

torch.Size([2590, 2])
We keep 1.50e+05/2.21e+06 =  6% of the original kernel matrix.

torch.Size([7850, 2])
We keep 1.07e+06/2.64e+07 =  4% of the original kernel matrix.

torch.Size([7740, 2])
We keep 1.42e+06/3.22e+07 =  4% of the original kernel matrix.

torch.Size([12070, 2])
We keep 2.85e+06/1.01e+08 =  2% of the original kernel matrix.

torch.Size([2519, 2])
We keep 1.66e+05/2.15e+06 =  7% of the original kernel matrix.

torch.Size([7850, 2])
We keep 1.07e+06/2.60e+07 =  4% of the original kernel matrix.

torch.Size([3312, 2])
We keep 2.37e+05/3.61e+06 =  6% of the original kernel matrix.

torch.Size([8723, 2])
We keep 1.30e+06/3.38e+07 =  3% of the original kernel matrix.

torch.Size([3249, 2])
We keep 2.61e+05/4.15e+06 =  6% of the original kernel matrix.

torch.Size([8433, 2])
We keep 1.34e+06/3.62e+07 =  3% of the original kernel matrix.

torch.Size([3231, 2])
We keep 2.61e+05/3.90e+06 =  6% of the original kernel matrix.

torch.Size([8512, 2])
We keep 1.36e+06/3.51e+07 =  3% of the original kernel matrix.

torch.Size([2173, 2])
We keep 1.32e+05/1.76e+06 =  7% of the original kernel matrix.

torch.Size([7366, 2])
We keep 9.96e+05/2.35e+07 =  4% of the original kernel matrix.

torch.Size([6927, 2])
We keep 1.19e+06/2.64e+07 =  4% of the original kernel matrix.

torch.Size([11551, 2])
We keep 2.64e+06/9.13e+07 =  2% of the original kernel matrix.

torch.Size([2133, 2])
We keep 1.13e+05/1.47e+06 =  7% of the original kernel matrix.

torch.Size([7314, 2])
We keep 9.48e+05/2.16e+07 =  4% of the original kernel matrix.

torch.Size([6798, 2])
We keep 1.13e+06/2.42e+07 =  4% of the original kernel matrix.

torch.Size([11546, 2])
We keep 2.59e+06/8.75e+07 =  2% of the original kernel matrix.

torch.Size([6505, 2])
We keep 9.43e+05/2.05e+07 =  4% of the original kernel matrix.

torch.Size([11245, 2])
We keep 2.38e+06/8.04e+07 =  2% of the original kernel matrix.

torch.Size([2359, 2])
We keep 1.27e+05/1.65e+06 =  7% of the original kernel matrix.

torch.Size([7657, 2])
We keep 9.78e+05/2.28e+07 =  4% of the original kernel matrix.

torch.Size([2630, 2])
We keep 1.63e+05/2.39e+06 =  6% of the original kernel matrix.

torch.Size([7955, 2])
We keep 1.10e+06/2.75e+07 =  3% of the original kernel matrix.

torch.Size([3427, 2])
We keep 3.00e+05/4.57e+06 =  6% of the original kernel matrix.

torch.Size([8650, 2])
We keep 1.41e+06/3.80e+07 =  3% of the original kernel matrix.

torch.Size([3996, 2])
We keep 4.47e+05/6.41e+06 =  6% of the original kernel matrix.

torch.Size([9077, 2])
We keep 1.58e+06/4.50e+07 =  3% of the original kernel matrix.

torch.Size([4094, 2])
We keep 3.68e+05/6.69e+06 =  5% of the original kernel matrix.

torch.Size([9434, 2])
We keep 1.56e+06/4.59e+07 =  3% of the original kernel matrix.

torch.Size([2195, 2])
We keep 1.13e+05/1.38e+06 =  8% of the original kernel matrix.

torch.Size([7400, 2])
We keep 9.33e+05/2.09e+07 =  4% of the original kernel matrix.

torch.Size([2195, 2])
We keep 1.88e+05/1.96e+06 =  9% of the original kernel matrix.

torch.Size([7242, 2])
We keep 1.04e+06/2.49e+07 =  4% of the original kernel matrix.

torch.Size([5072, 2])
We keep 5.67e+05/1.04e+07 =  5% of the original kernel matrix.

torch.Size([10146, 2])
We keep 1.88e+06/5.72e+07 =  3% of the original kernel matrix.

torch.Size([3210, 2])
We keep 2.43e+05/3.93e+06 =  6% of the original kernel matrix.

torch.Size([8382, 2])
We keep 1.29e+06/3.52e+07 =  3% of the original kernel matrix.

torch.Size([1874, 2])
We keep 6.50e+04/8.10e+05 =  8% of the original kernel matrix.

torch.Size([7100, 2])
We keep 7.62e+05/1.60e+07 =  4% of the original kernel matrix.

torch.Size([2856, 2])
We keep 2.07e+05/2.99e+06 =  6% of the original kernel matrix.

torch.Size([8022, 2])
We keep 1.22e+06/3.07e+07 =  3% of the original kernel matrix.

torch.Size([4998, 2])
We keep 5.38e+05/9.95e+06 =  5% of the original kernel matrix.

torch.Size([10220, 2])
We keep 1.83e+06/5.61e+07 =  3% of the original kernel matrix.

torch.Size([1821, 2])
We keep 1.17e+05/1.42e+06 =  8% of the original kernel matrix.

torch.Size([6840, 2])
We keep 9.20e+05/2.11e+07 =  4% of the original kernel matrix.

torch.Size([1582, 2])
We keep 5.41e+04/5.54e+05 =  9% of the original kernel matrix.

torch.Size([6596, 2])
We keep 6.74e+05/1.32e+07 =  5% of the original kernel matrix.

torch.Size([3249, 2])
We keep 2.68e+05/3.90e+06 =  6% of the original kernel matrix.

torch.Size([8449, 2])
We keep 1.31e+06/3.51e+07 =  3% of the original kernel matrix.

torch.Size([8712, 2])
We keep 1.82e+06/4.28e+07 =  4% of the original kernel matrix.

torch.Size([12901, 2])
We keep 3.19e+06/1.16e+08 =  2% of the original kernel matrix.

torch.Size([3660, 2])
We keep 3.28e+05/5.31e+06 =  6% of the original kernel matrix.

torch.Size([8921, 2])
We keep 1.50e+06/4.09e+07 =  3% of the original kernel matrix.

torch.Size([1475, 2])
We keep 5.61e+04/6.08e+05 =  9% of the original kernel matrix.

torch.Size([6451, 2])
We keep 6.99e+05/1.39e+07 =  5% of the original kernel matrix.

torch.Size([5610, 2])
We keep 6.84e+05/1.23e+07 =  5% of the original kernel matrix.

torch.Size([10804, 2])
We keep 1.94e+06/6.23e+07 =  3% of the original kernel matrix.

torch.Size([2039, 2])
We keep 8.80e+04/1.05e+06 =  8% of the original kernel matrix.

torch.Size([7205, 2])
We keep 8.45e+05/1.82e+07 =  4% of the original kernel matrix.

torch.Size([2128, 2])
We keep 1.39e+05/1.87e+06 =  7% of the original kernel matrix.

torch.Size([7201, 2])
We keep 1.03e+06/2.43e+07 =  4% of the original kernel matrix.

torch.Size([2351, 2])
We keep 1.24e+05/1.62e+06 =  7% of the original kernel matrix.

torch.Size([7646, 2])
We keep 9.93e+05/2.26e+07 =  4% of the original kernel matrix.

torch.Size([3524, 2])
We keep 2.70e+05/4.30e+06 =  6% of the original kernel matrix.

torch.Size([8934, 2])
We keep 1.37e+06/3.68e+07 =  3% of the original kernel matrix.

torch.Size([1589, 2])
We keep 6.18e+04/6.69e+05 =  9% of the original kernel matrix.

torch.Size([6605, 2])
We keep 7.30e+05/1.45e+07 =  5% of the original kernel matrix.

torch.Size([4692, 2])
We keep 4.63e+05/8.29e+06 =  5% of the original kernel matrix.

torch.Size([9879, 2])
We keep 1.72e+06/5.12e+07 =  3% of the original kernel matrix.

torch.Size([2896, 2])
We keep 2.26e+05/3.38e+06 =  6% of the original kernel matrix.

torch.Size([8073, 2])
We keep 1.26e+06/3.27e+07 =  3% of the original kernel matrix.

torch.Size([4691, 2])
We keep 5.79e+05/9.53e+06 =  6% of the original kernel matrix.

torch.Size([9881, 2])
We keep 1.86e+06/5.48e+07 =  3% of the original kernel matrix.

torch.Size([2295, 2])
We keep 1.26e+05/1.64e+06 =  7% of the original kernel matrix.

torch.Size([7578, 2])
We keep 9.64e+05/2.27e+07 =  4% of the original kernel matrix.

torch.Size([1428, 2])
We keep 5.93e+04/6.69e+05 =  8% of the original kernel matrix.

torch.Size([6298, 2])
We keep 7.14e+05/1.45e+07 =  4% of the original kernel matrix.

torch.Size([3145, 2])
We keep 2.86e+05/4.41e+06 =  6% of the original kernel matrix.

torch.Size([8494, 2])
We keep 1.40e+06/3.73e+07 =  3% of the original kernel matrix.

torch.Size([3895, 2])
We keep 3.36e+05/5.59e+06 =  6% of the original kernel matrix.

torch.Size([9245, 2])
We keep 1.51e+06/4.20e+07 =  3% of the original kernel matrix.

torch.Size([4767, 2])
We keep 5.17e+05/9.50e+06 =  5% of the original kernel matrix.

torch.Size([9985, 2])
We keep 1.83e+06/5.48e+07 =  3% of the original kernel matrix.

torch.Size([3297, 2])
We keep 2.42e+05/3.82e+06 =  6% of the original kernel matrix.

torch.Size([8566, 2])
We keep 1.30e+06/3.47e+07 =  3% of the original kernel matrix.

torch.Size([3199, 2])
We keep 2.69e+05/4.28e+06 =  6% of the original kernel matrix.

torch.Size([8527, 2])
We keep 1.34e+06/3.68e+07 =  3% of the original kernel matrix.

torch.Size([4601, 2])
We keep 4.97e+05/8.50e+06 =  5% of the original kernel matrix.

torch.Size([9691, 2])
We keep 1.74e+06/5.18e+07 =  3% of the original kernel matrix.

torch.Size([3595, 2])
We keep 3.31e+05/5.56e+06 =  5% of the original kernel matrix.

torch.Size([8782, 2])
We keep 1.49e+06/4.19e+07 =  3% of the original kernel matrix.

torch.Size([8274, 2])
We keep 1.56e+06/3.71e+07 =  4% of the original kernel matrix.

torch.Size([12597, 2])
We keep 3.04e+06/1.08e+08 =  2% of the original kernel matrix.

torch.Size([3527, 2])
We keep 3.52e+05/5.22e+06 =  6% of the original kernel matrix.

torch.Size([8728, 2])
We keep 1.47e+06/4.06e+07 =  3% of the original kernel matrix.

torch.Size([6029, 2])
We keep 8.19e+05/1.72e+07 =  4% of the original kernel matrix.

torch.Size([10804, 2])
We keep 2.23e+06/7.37e+07 =  3% of the original kernel matrix.

torch.Size([1578, 2])
We keep 7.20e+04/9.01e+05 =  7% of the original kernel matrix.

torch.Size([6489, 2])
We keep 7.85e+05/1.69e+07 =  4% of the original kernel matrix.

torch.Size([3530, 2])
We keep 3.03e+05/4.55e+06 =  6% of the original kernel matrix.

torch.Size([8729, 2])
We keep 1.37e+06/3.79e+07 =  3% of the original kernel matrix.

torch.Size([1869, 2])
We keep 1.09e+05/1.38e+06 =  7% of the original kernel matrix.

torch.Size([6873, 2])
We keep 9.17e+05/2.09e+07 =  4% of the original kernel matrix.

torch.Size([6030, 2])
We keep 8.15e+05/1.65e+07 =  4% of the original kernel matrix.

torch.Size([10845, 2])
We keep 2.24e+06/7.22e+07 =  3% of the original kernel matrix.

torch.Size([1214, 2])
We keep 3.96e+04/4.04e+05 =  9% of the original kernel matrix.

torch.Size([6008, 2])
We keep 5.87e+05/1.13e+07 =  5% of the original kernel matrix.

torch.Size([3231, 2])
We keep 2.66e+05/3.92e+06 =  6% of the original kernel matrix.

torch.Size([8493, 2])
We keep 1.34e+06/3.52e+07 =  3% of the original kernel matrix.

torch.Size([1690, 2])
We keep 7.91e+04/9.62e+05 =  8% of the original kernel matrix.

torch.Size([6749, 2])
We keep 8.22e+05/1.74e+07 =  4% of the original kernel matrix.

torch.Size([2266, 2])
We keep 1.21e+05/1.59e+06 =  7% of the original kernel matrix.

torch.Size([7479, 2])
We keep 9.70e+05/2.24e+07 =  4% of the original kernel matrix.

torch.Size([1627, 2])
We keep 6.21e+04/7.21e+05 =  8% of the original kernel matrix.

torch.Size([6630, 2])
We keep 7.29e+05/1.51e+07 =  4% of the original kernel matrix.

torch.Size([1388, 2])
We keep 6.91e+04/7.02e+05 =  9% of the original kernel matrix.

torch.Size([6200, 2])
We keep 7.41e+05/1.49e+07 =  4% of the original kernel matrix.

torch.Size([4600, 2])
We keep 4.48e+05/8.01e+06 =  5% of the original kernel matrix.

torch.Size([9857, 2])
We keep 1.70e+06/5.03e+07 =  3% of the original kernel matrix.

torch.Size([5023, 2])
We keep 5.61e+05/1.02e+07 =  5% of the original kernel matrix.

torch.Size([10156, 2])
We keep 1.88e+06/5.67e+07 =  3% of the original kernel matrix.

torch.Size([1497, 2])
We keep 5.27e+04/5.48e+05 =  9% of the original kernel matrix.

torch.Size([6367, 2])
We keep 6.74e+05/1.31e+07 =  5% of the original kernel matrix.

torch.Size([2835, 2])
We keep 2.22e+05/3.29e+06 =  6% of the original kernel matrix.

torch.Size([8146, 2])
We keep 1.24e+06/3.22e+07 =  3% of the original kernel matrix.

torch.Size([1511, 2])
We keep 6.68e+04/7.14e+05 =  9% of the original kernel matrix.

torch.Size([6432, 2])
We keep 7.63e+05/1.50e+07 =  5% of the original kernel matrix.

torch.Size([7639, 2])
We keep 1.45e+06/2.95e+07 =  4% of the original kernel matrix.

torch.Size([12252, 2])
We keep 2.76e+06/9.65e+07 =  2% of the original kernel matrix.

torch.Size([1891, 2])
We keep 9.52e+04/1.20e+06 =  7% of the original kernel matrix.

torch.Size([6961, 2])
We keep 8.82e+05/1.95e+07 =  4% of the original kernel matrix.

torch.Size([9867, 2])
We keep 1.90e+06/5.03e+07 =  3% of the original kernel matrix.

torch.Size([13735, 2])
We keep 3.37e+06/1.26e+08 =  2% of the original kernel matrix.

torch.Size([3138, 2])
We keep 2.85e+05/4.28e+06 =  6% of the original kernel matrix.

torch.Size([8411, 2])
We keep 1.39e+06/3.67e+07 =  3% of the original kernel matrix.

torch.Size([2251, 2])
We keep 1.21e+05/1.59e+06 =  7% of the original kernel matrix.

torch.Size([7437, 2])
We keep 9.57e+05/2.24e+07 =  4% of the original kernel matrix.

torch.Size([2834, 2])
We keep 2.20e+05/3.21e+06 =  6% of the original kernel matrix.

torch.Size([8128, 2])
We keep 1.25e+06/3.18e+07 =  3% of the original kernel matrix.

torch.Size([2194, 2])
We keep 1.40e+05/1.78e+06 =  7% of the original kernel matrix.

torch.Size([7381, 2])
We keep 1.02e+06/2.37e+07 =  4% of the original kernel matrix.

torch.Size([1475, 2])
We keep 5.80e+04/6.32e+05 =  9% of the original kernel matrix.

torch.Size([6460, 2])
We keep 7.34e+05/1.41e+07 =  5% of the original kernel matrix.

torch.Size([2965, 2])
We keep 2.07e+05/2.99e+06 =  6% of the original kernel matrix.

torch.Size([8302, 2])
We keep 1.21e+06/3.07e+07 =  3% of the original kernel matrix.

torch.Size([3206, 2])
We keep 2.56e+05/4.02e+06 =  6% of the original kernel matrix.

torch.Size([8380, 2])
We keep 1.32e+06/3.56e+07 =  3% of the original kernel matrix.

torch.Size([12333, 2])
We keep 3.64e+06/1.07e+08 =  3% of the original kernel matrix.

torch.Size([15471, 2])
We keep 4.47e+06/1.84e+08 =  2% of the original kernel matrix.

torch.Size([2920, 2])
We keep 1.83e+05/2.74e+06 =  6% of the original kernel matrix.

torch.Size([8221, 2])
We keep 1.18e+06/2.94e+07 =  3% of the original kernel matrix.

torch.Size([6567, 2])
We keep 1.09e+06/2.22e+07 =  4% of the original kernel matrix.

torch.Size([11263, 2])
We keep 2.49e+06/8.37e+07 =  2% of the original kernel matrix.

torch.Size([1679, 2])
We keep 5.95e+04/7.26e+05 =  8% of the original kernel matrix.

torch.Size([6773, 2])
We keep 7.25e+05/1.51e+07 =  4% of the original kernel matrix.

torch.Size([4225, 2])
We keep 3.69e+05/6.39e+06 =  5% of the original kernel matrix.

torch.Size([9530, 2])
We keep 1.58e+06/4.49e+07 =  3% of the original kernel matrix.

torch.Size([1231, 2])
We keep 3.82e+04/3.77e+05 = 10% of the original kernel matrix.

torch.Size([6006, 2])
We keep 5.98e+05/1.09e+07 =  5% of the original kernel matrix.

torch.Size([2815, 2])
We keep 2.28e+05/3.31e+06 =  6% of the original kernel matrix.

torch.Size([8095, 2])
We keep 1.26e+06/3.23e+07 =  3% of the original kernel matrix.

torch.Size([2953, 2])
We keep 2.27e+05/3.45e+06 =  6% of the original kernel matrix.

torch.Size([8206, 2])
We keep 1.27e+06/3.30e+07 =  3% of the original kernel matrix.

torch.Size([1726, 2])
We keep 7.44e+04/8.37e+05 =  8% of the original kernel matrix.

torch.Size([6750, 2])
We keep 7.90e+05/1.63e+07 =  4% of the original kernel matrix.

torch.Size([1995, 2])
We keep 8.82e+04/1.07e+06 =  8% of the original kernel matrix.

torch.Size([7232, 2])
We keep 8.59e+05/1.84e+07 =  4% of the original kernel matrix.

torch.Size([2752, 2])
We keep 1.90e+05/2.61e+06 =  7% of the original kernel matrix.

torch.Size([7999, 2])
We keep 1.17e+06/2.87e+07 =  4% of the original kernel matrix.

torch.Size([4157, 2])
We keep 4.14e+05/6.23e+06 =  6% of the original kernel matrix.

torch.Size([9484, 2])
We keep 1.55e+06/4.43e+07 =  3% of the original kernel matrix.

torch.Size([4552, 2])
We keep 5.40e+05/9.44e+06 =  5% of the original kernel matrix.

torch.Size([9652, 2])
We keep 1.84e+06/5.46e+07 =  3% of the original kernel matrix.

torch.Size([1098, 2])
We keep 3.13e+04/2.92e+05 = 10% of the original kernel matrix.

torch.Size([5813, 2])
We keep 5.53e+05/9.59e+06 =  5% of the original kernel matrix.

torch.Size([7212, 2])
We keep 1.19e+06/2.40e+07 =  4% of the original kernel matrix.

torch.Size([12021, 2])
We keep 2.60e+06/8.70e+07 =  2% of the original kernel matrix.

torch.Size([7837, 2])
We keep 1.31e+06/2.85e+07 =  4% of the original kernel matrix.

torch.Size([12392, 2])
We keep 2.72e+06/9.49e+07 =  2% of the original kernel matrix.

torch.Size([6591, 2])
We keep 1.04e+06/2.17e+07 =  4% of the original kernel matrix.

torch.Size([11255, 2])
We keep 2.48e+06/8.29e+07 =  2% of the original kernel matrix.

torch.Size([2561, 2])
We keep 1.80e+05/2.50e+06 =  7% of the original kernel matrix.

torch.Size([7793, 2])
We keep 1.15e+06/2.81e+07 =  4% of the original kernel matrix.

torch.Size([2917, 2])
We keep 2.12e+05/3.08e+06 =  6% of the original kernel matrix.

torch.Size([8174, 2])
We keep 1.23e+06/3.12e+07 =  3% of the original kernel matrix.

torch.Size([2925, 2])
We keep 1.94e+05/2.90e+06 =  6% of the original kernel matrix.

torch.Size([8285, 2])
We keep 1.18e+06/3.02e+07 =  3% of the original kernel matrix.

torch.Size([2558, 2])
We keep 1.60e+05/2.21e+06 =  7% of the original kernel matrix.

torch.Size([7839, 2])
We keep 1.10e+06/2.64e+07 =  4% of the original kernel matrix.

torch.Size([2596, 2])
We keep 1.41e+05/2.07e+06 =  6% of the original kernel matrix.

torch.Size([7914, 2])
We keep 1.05e+06/2.56e+07 =  4% of the original kernel matrix.

torch.Size([2536, 2])
We keep 1.63e+05/2.20e+06 =  7% of the original kernel matrix.

torch.Size([7841, 2])
We keep 1.11e+06/2.64e+07 =  4% of the original kernel matrix.

torch.Size([3608, 2])
We keep 3.15e+05/4.99e+06 =  6% of the original kernel matrix.

torch.Size([8987, 2])
We keep 1.47e+06/3.97e+07 =  3% of the original kernel matrix.

torch.Size([4158, 2])
We keep 3.54e+05/6.17e+06 =  5% of the original kernel matrix.

torch.Size([9485, 2])
We keep 1.55e+06/4.41e+07 =  3% of the original kernel matrix.

torch.Size([3317, 2])
We keep 2.98e+05/4.62e+06 =  6% of the original kernel matrix.

torch.Size([8661, 2])
We keep 1.38e+06/3.82e+07 =  3% of the original kernel matrix.

torch.Size([2958, 2])
We keep 2.03e+05/2.93e+06 =  6% of the original kernel matrix.

torch.Size([8208, 2])
We keep 1.21e+06/3.04e+07 =  3% of the original kernel matrix.

torch.Size([9167, 2])
We keep 1.80e+06/4.31e+07 =  4% of the original kernel matrix.

torch.Size([13263, 2])
We keep 3.13e+06/1.17e+08 =  2% of the original kernel matrix.

torch.Size([3627, 2])
We keep 3.35e+05/5.41e+06 =  6% of the original kernel matrix.

torch.Size([8885, 2])
We keep 1.49e+06/4.13e+07 =  3% of the original kernel matrix.

torch.Size([2665, 2])
We keep 1.54e+05/2.28e+06 =  6% of the original kernel matrix.

torch.Size([8013, 2])
We keep 1.08e+06/2.68e+07 =  4% of the original kernel matrix.

torch.Size([4107, 2])
We keep 3.86e+05/6.53e+06 =  5% of the original kernel matrix.

torch.Size([9349, 2])
We keep 1.61e+06/4.54e+07 =  3% of the original kernel matrix.

torch.Size([1076, 2])
We keep 3.40e+04/2.96e+05 = 11% of the original kernel matrix.

torch.Size([5706, 2])
We keep 5.63e+05/9.67e+06 =  5% of the original kernel matrix.

torch.Size([40019, 2])
We keep 3.19e+07/1.94e+09 =  1% of the original kernel matrix.

torch.Size([29188, 2])
We keep 1.45e+07/7.82e+08 =  1% of the original kernel matrix.

torch.Size([1937, 2])
We keep 1.04e+05/1.30e+06 =  7% of the original kernel matrix.

torch.Size([7065, 2])
We keep 9.15e+05/2.03e+07 =  4% of the original kernel matrix.

torch.Size([4681, 2])
We keep 5.26e+05/9.32e+06 =  5% of the original kernel matrix.

torch.Size([9870, 2])
We keep 1.81e+06/5.42e+07 =  3% of the original kernel matrix.

torch.Size([1403, 2])
We keep 4.90e+04/5.14e+05 =  9% of the original kernel matrix.

torch.Size([6214, 2])
We keep 6.70e+05/1.27e+07 =  5% of the original kernel matrix.

torch.Size([3091, 2])
We keep 2.36e+05/3.33e+06 =  7% of the original kernel matrix.

torch.Size([8165, 2])
We keep 1.21e+06/3.24e+07 =  3% of the original kernel matrix.

torch.Size([2006, 2])
We keep 8.80e+04/1.04e+06 =  8% of the original kernel matrix.

torch.Size([7177, 2])
We keep 8.53e+05/1.81e+07 =  4% of the original kernel matrix.

torch.Size([3507, 2])
We keep 3.45e+05/5.13e+06 =  6% of the original kernel matrix.

torch.Size([9004, 2])
We keep 1.46e+06/4.02e+07 =  3% of the original kernel matrix.

torch.Size([3234, 2])
We keep 2.27e+05/3.61e+06 =  6% of the original kernel matrix.

torch.Size([8560, 2])
We keep 1.30e+06/3.38e+07 =  3% of the original kernel matrix.

torch.Size([2809, 2])
We keep 2.27e+05/3.52e+06 =  6% of the original kernel matrix.

torch.Size([8012, 2])
We keep 1.29e+06/3.33e+07 =  3% of the original kernel matrix.

torch.Size([986, 2])
We keep 3.17e+04/2.94e+05 = 10% of the original kernel matrix.

torch.Size([5465, 2])
We keep 5.47e+05/9.63e+06 =  5% of the original kernel matrix.

torch.Size([2373, 2])
We keep 1.97e+05/2.24e+06 =  8% of the original kernel matrix.

torch.Size([7405, 2])
We keep 1.11e+06/2.66e+07 =  4% of the original kernel matrix.

torch.Size([1749, 2])
We keep 7.04e+04/7.90e+05 =  8% of the original kernel matrix.

torch.Size([6748, 2])
We keep 7.63e+05/1.58e+07 =  4% of the original kernel matrix.

torch.Size([6064, 2])
We keep 7.79e+05/1.64e+07 =  4% of the original kernel matrix.

torch.Size([10914, 2])
We keep 2.18e+06/7.19e+07 =  3% of the original kernel matrix.

torch.Size([1917, 2])
We keep 9.32e+04/1.30e+06 =  7% of the original kernel matrix.

torch.Size([7013, 2])
We keep 8.65e+05/2.03e+07 =  4% of the original kernel matrix.

torch.Size([5562, 2])
We keep 7.40e+05/1.48e+07 =  5% of the original kernel matrix.

torch.Size([10603, 2])
We keep 2.08e+06/6.83e+07 =  3% of the original kernel matrix.

torch.Size([3520, 2])
We keep 2.95e+05/4.89e+06 =  6% of the original kernel matrix.

torch.Size([8788, 2])
We keep 1.41e+06/3.93e+07 =  3% of the original kernel matrix.

torch.Size([4540, 2])
We keep 4.84e+05/8.60e+06 =  5% of the original kernel matrix.

torch.Size([9669, 2])
We keep 1.76e+06/5.21e+07 =  3% of the original kernel matrix.

torch.Size([2601, 2])
We keep 1.70e+05/2.15e+06 =  7% of the original kernel matrix.

torch.Size([7921, 2])
We keep 1.09e+06/2.60e+07 =  4% of the original kernel matrix.

torch.Size([4164, 2])
We keep 3.74e+05/6.01e+06 =  6% of the original kernel matrix.

torch.Size([9446, 2])
We keep 1.49e+06/4.35e+07 =  3% of the original kernel matrix.

torch.Size([3227, 2])
We keep 2.66e+05/3.95e+06 =  6% of the original kernel matrix.

torch.Size([8409, 2])
We keep 1.33e+06/3.53e+07 =  3% of the original kernel matrix.

torch.Size([4088, 2])
We keep 3.80e+05/6.46e+06 =  5% of the original kernel matrix.

torch.Size([9359, 2])
We keep 1.56e+06/4.52e+07 =  3% of the original kernel matrix.

torch.Size([2231, 2])
We keep 1.33e+05/1.85e+06 =  7% of the original kernel matrix.

torch.Size([7366, 2])
We keep 1.02e+06/2.41e+07 =  4% of the original kernel matrix.

torch.Size([1900, 2])
We keep 1.07e+05/1.39e+06 =  7% of the original kernel matrix.

torch.Size([6996, 2])
We keep 8.94e+05/2.09e+07 =  4% of the original kernel matrix.

torch.Size([3218, 2])
We keep 2.76e+05/3.77e+06 =  7% of the original kernel matrix.

torch.Size([8557, 2])
We keep 1.29e+06/3.45e+07 =  3% of the original kernel matrix.

torch.Size([7054, 2])
We keep 1.28e+06/2.84e+07 =  4% of the original kernel matrix.

torch.Size([11635, 2])
We keep 2.75e+06/9.47e+07 =  2% of the original kernel matrix.

torch.Size([6050, 2])
We keep 8.16e+05/1.58e+07 =  5% of the original kernel matrix.

torch.Size([11028, 2])
We keep 2.20e+06/7.06e+07 =  3% of the original kernel matrix.

torch.Size([2885, 2])
We keep 1.92e+05/2.73e+06 =  7% of the original kernel matrix.

torch.Size([8204, 2])
We keep 1.19e+06/2.94e+07 =  4% of the original kernel matrix.

torch.Size([4138, 2])
We keep 3.54e+05/5.91e+06 =  5% of the original kernel matrix.

torch.Size([9463, 2])
We keep 1.54e+06/4.32e+07 =  3% of the original kernel matrix.

torch.Size([7707, 2])
We keep 1.46e+06/3.02e+07 =  4% of the original kernel matrix.

torch.Size([12191, 2])
We keep 2.79e+06/9.76e+07 =  2% of the original kernel matrix.

torch.Size([11590, 2])
We keep 3.06e+06/8.13e+07 =  3% of the original kernel matrix.

torch.Size([14970, 2])
We keep 4.11e+06/1.60e+08 =  2% of the original kernel matrix.

torch.Size([2445, 2])
We keep 1.51e+05/1.97e+06 =  7% of the original kernel matrix.

torch.Size([7658, 2])
We keep 1.05e+06/2.49e+07 =  4% of the original kernel matrix.

torch.Size([946, 2])
We keep 1.94e+04/1.63e+05 = 11% of the original kernel matrix.

torch.Size([5553, 2])
We keep 4.56e+05/7.18e+06 =  6% of the original kernel matrix.

torch.Size([3999, 2])
We keep 4.13e+05/6.70e+06 =  6% of the original kernel matrix.

torch.Size([9299, 2])
We keep 1.62e+06/4.60e+07 =  3% of the original kernel matrix.

torch.Size([1961, 2])
We keep 9.82e+04/1.27e+06 =  7% of the original kernel matrix.

torch.Size([7099, 2])
We keep 8.68e+05/2.00e+07 =  4% of the original kernel matrix.

torch.Size([1961, 2])
We keep 1.10e+05/1.35e+06 =  8% of the original kernel matrix.

torch.Size([7131, 2])
We keep 9.33e+05/2.06e+07 =  4% of the original kernel matrix.

torch.Size([3718, 2])
We keep 3.10e+05/4.99e+06 =  6% of the original kernel matrix.

torch.Size([8956, 2])
We keep 1.45e+06/3.97e+07 =  3% of the original kernel matrix.

torch.Size([2153, 2])
We keep 1.37e+05/1.86e+06 =  7% of the original kernel matrix.

torch.Size([7334, 2])
We keep 1.04e+06/2.42e+07 =  4% of the original kernel matrix.

torch.Size([1450, 2])
We keep 5.69e+04/6.16e+05 =  9% of the original kernel matrix.

torch.Size([6342, 2])
We keep 7.06e+05/1.39e+07 =  5% of the original kernel matrix.

torch.Size([1983, 2])
We keep 8.76e+04/1.08e+06 =  8% of the original kernel matrix.

torch.Size([7176, 2])
We keep 8.44e+05/1.84e+07 =  4% of the original kernel matrix.

torch.Size([2045, 2])
We keep 9.91e+04/1.21e+06 =  8% of the original kernel matrix.

torch.Size([7314, 2])
We keep 8.85e+05/1.95e+07 =  4% of the original kernel matrix.

torch.Size([5091, 2])
We keep 6.01e+05/1.14e+07 =  5% of the original kernel matrix.

torch.Size([10264, 2])
We keep 1.96e+06/5.99e+07 =  3% of the original kernel matrix.

torch.Size([1900, 2])
We keep 1.04e+05/1.39e+06 =  7% of the original kernel matrix.

torch.Size([6976, 2])
We keep 9.34e+05/2.10e+07 =  4% of the original kernel matrix.

torch.Size([7872, 2])
We keep 1.33e+06/3.05e+07 =  4% of the original kernel matrix.

torch.Size([12337, 2])
We keep 2.79e+06/9.82e+07 =  2% of the original kernel matrix.

torch.Size([3624, 2])
We keep 3.14e+05/5.34e+06 =  5% of the original kernel matrix.

torch.Size([8937, 2])
We keep 1.46e+06/4.11e+07 =  3% of the original kernel matrix.

torch.Size([3343, 2])
We keep 2.46e+05/3.60e+06 =  6% of the original kernel matrix.

torch.Size([8700, 2])
We keep 1.28e+06/3.37e+07 =  3% of the original kernel matrix.

torch.Size([1487, 2])
We keep 5.21e+04/5.66e+05 =  9% of the original kernel matrix.

torch.Size([6505, 2])
We keep 6.86e+05/1.34e+07 =  5% of the original kernel matrix.

torch.Size([4134, 2])
We keep 3.98e+05/6.69e+06 =  5% of the original kernel matrix.

torch.Size([9478, 2])
We keep 1.56e+06/4.60e+07 =  3% of the original kernel matrix.

torch.Size([3525, 2])
We keep 3.13e+05/5.11e+06 =  6% of the original kernel matrix.

torch.Size([8747, 2])
We keep 1.45e+06/4.02e+07 =  3% of the original kernel matrix.

torch.Size([3262, 2])
We keep 2.65e+05/4.02e+06 =  6% of the original kernel matrix.

torch.Size([8560, 2])
We keep 1.31e+06/3.56e+07 =  3% of the original kernel matrix.

torch.Size([6956, 2])
We keep 1.16e+06/2.46e+07 =  4% of the original kernel matrix.

torch.Size([11610, 2])
We keep 2.58e+06/8.81e+07 =  2% of the original kernel matrix.

torch.Size([5009, 2])
We keep 5.91e+05/1.06e+07 =  5% of the original kernel matrix.

torch.Size([10066, 2])
We keep 1.88e+06/5.78e+07 =  3% of the original kernel matrix.

torch.Size([2209, 2])
We keep 1.24e+05/1.64e+06 =  7% of the original kernel matrix.

torch.Size([7357, 2])
We keep 9.46e+05/2.27e+07 =  4% of the original kernel matrix.

torch.Size([3573, 2])
We keep 3.06e+05/4.40e+06 =  6% of the original kernel matrix.

torch.Size([8949, 2])
We keep 1.41e+06/3.73e+07 =  3% of the original kernel matrix.

torch.Size([1882, 2])
We keep 9.75e+04/1.30e+06 =  7% of the original kernel matrix.

torch.Size([7038, 2])
We keep 9.13e+05/2.02e+07 =  4% of the original kernel matrix.

torch.Size([1735, 2])
We keep 7.29e+04/8.48e+05 =  8% of the original kernel matrix.

torch.Size([6734, 2])
We keep 7.82e+05/1.64e+07 =  4% of the original kernel matrix.

torch.Size([1484, 2])
We keep 5.07e+04/5.55e+05 =  9% of the original kernel matrix.

torch.Size([6425, 2])
We keep 6.76e+05/1.32e+07 =  5% of the original kernel matrix.

torch.Size([1415, 2])
We keep 5.88e+04/6.13e+05 =  9% of the original kernel matrix.

torch.Size([6266, 2])
We keep 6.83e+05/1.39e+07 =  4% of the original kernel matrix.

torch.Size([1338, 2])
We keep 3.94e+04/3.62e+05 = 10% of the original kernel matrix.

torch.Size([6249, 2])
We keep 6.00e+05/1.07e+07 =  5% of the original kernel matrix.

torch.Size([3129, 2])
We keep 2.65e+05/4.42e+06 =  5% of the original kernel matrix.

torch.Size([8384, 2])
We keep 1.38e+06/3.74e+07 =  3% of the original kernel matrix.

torch.Size([3690, 2])
We keep 3.04e+05/4.65e+06 =  6% of the original kernel matrix.

torch.Size([9071, 2])
We keep 1.43e+06/3.83e+07 =  3% of the original kernel matrix.

torch.Size([1467, 2])
We keep 6.00e+04/6.66e+05 =  9% of the original kernel matrix.

torch.Size([6334, 2])
We keep 7.16e+05/1.45e+07 =  4% of the original kernel matrix.

torch.Size([2105, 2])
We keep 1.11e+05/1.44e+06 =  7% of the original kernel matrix.

torch.Size([7304, 2])
We keep 9.13e+05/2.13e+07 =  4% of the original kernel matrix.

torch.Size([4013, 2])
We keep 3.92e+05/6.57e+06 =  5% of the original kernel matrix.

torch.Size([9159, 2])
We keep 1.60e+06/4.56e+07 =  3% of the original kernel matrix.

torch.Size([1706, 2])
We keep 7.69e+04/9.16e+05 =  8% of the original kernel matrix.

torch.Size([6709, 2])
We keep 8.05e+05/1.70e+07 =  4% of the original kernel matrix.

torch.Size([4314, 2])
We keep 5.07e+05/8.43e+06 =  6% of the original kernel matrix.

torch.Size([9449, 2])
We keep 1.75e+06/5.16e+07 =  3% of the original kernel matrix.

torch.Size([5083, 2])
We keep 5.93e+05/1.05e+07 =  5% of the original kernel matrix.

torch.Size([10301, 2])
We keep 1.90e+06/5.75e+07 =  3% of the original kernel matrix.

torch.Size([2568, 2])
We keep 1.59e+05/2.10e+06 =  7% of the original kernel matrix.

torch.Size([7817, 2])
We keep 1.09e+06/2.58e+07 =  4% of the original kernel matrix.

torch.Size([1439, 2])
We keep 5.99e+04/6.69e+05 =  8% of the original kernel matrix.

torch.Size([6339, 2])
We keep 7.29e+05/1.45e+07 =  5% of the original kernel matrix.

torch.Size([5934, 2])
We keep 7.42e+05/1.44e+07 =  5% of the original kernel matrix.

torch.Size([10904, 2])
We keep 2.12e+06/6.73e+07 =  3% of the original kernel matrix.

torch.Size([1305, 2])
We keep 3.89e+04/3.47e+05 = 11% of the original kernel matrix.

torch.Size([6085, 2])
We keep 5.84e+05/1.05e+07 =  5% of the original kernel matrix.

torch.Size([2085, 2])
We keep 9.19e+04/1.22e+06 =  7% of the original kernel matrix.

torch.Size([7350, 2])
We keep 8.71e+05/1.97e+07 =  4% of the original kernel matrix.

torch.Size([2222, 2])
We keep 1.22e+05/1.52e+06 =  8% of the original kernel matrix.

torch.Size([7365, 2])
We keep 9.52e+05/2.19e+07 =  4% of the original kernel matrix.

torch.Size([4160, 2])
We keep 4.03e+05/6.49e+06 =  6% of the original kernel matrix.

torch.Size([9394, 2])
We keep 1.59e+06/4.53e+07 =  3% of the original kernel matrix.

torch.Size([5625, 2])
We keep 6.72e+05/1.32e+07 =  5% of the original kernel matrix.

torch.Size([10772, 2])
We keep 2.03e+06/6.45e+07 =  3% of the original kernel matrix.

torch.Size([2985, 2])
We keep 2.12e+05/3.07e+06 =  6% of the original kernel matrix.

torch.Size([8302, 2])
We keep 1.22e+06/3.11e+07 =  3% of the original kernel matrix.

torch.Size([2019, 2])
We keep 8.43e+04/1.04e+06 =  8% of the original kernel matrix.

torch.Size([7241, 2])
We keep 8.36e+05/1.82e+07 =  4% of the original kernel matrix.

torch.Size([4144, 2])
We keep 3.87e+05/6.65e+06 =  5% of the original kernel matrix.

torch.Size([9468, 2])
We keep 1.59e+06/4.58e+07 =  3% of the original kernel matrix.

torch.Size([7260, 2])
We keep 1.31e+06/2.81e+07 =  4% of the original kernel matrix.

torch.Size([11947, 2])
We keep 2.65e+06/9.42e+07 =  2% of the original kernel matrix.

torch.Size([1536, 2])
We keep 6.31e+04/7.11e+05 =  8% of the original kernel matrix.

torch.Size([6524, 2])
We keep 7.41e+05/1.50e+07 =  4% of the original kernel matrix.

torch.Size([5926, 2])
We keep 7.26e+05/1.57e+07 =  4% of the original kernel matrix.

torch.Size([11016, 2])
We keep 2.16e+06/7.03e+07 =  3% of the original kernel matrix.

torch.Size([11589, 2])
We keep 6.76e+06/1.26e+08 =  5% of the original kernel matrix.

torch.Size([15018, 2])
We keep 4.75e+06/1.99e+08 =  2% of the original kernel matrix.

torch.Size([8028, 2])
We keep 1.75e+06/3.62e+07 =  4% of the original kernel matrix.

torch.Size([12681, 2])
We keep 2.69e+06/1.07e+08 =  2% of the original kernel matrix.

torch.Size([5948, 2])
We keep 8.98e+05/1.66e+07 =  5% of the original kernel matrix.

torch.Size([10817, 2])
We keep 2.23e+06/7.23e+07 =  3% of the original kernel matrix.

torch.Size([3100, 2])
We keep 8.60e+05/5.84e+06 = 14% of the original kernel matrix.

torch.Size([8065, 2])
We keep 1.62e+06/4.29e+07 =  3% of the original kernel matrix.

torch.Size([11979, 2])
We keep 8.83e+06/1.20e+08 =  7% of the original kernel matrix.

torch.Size([15337, 2])
We keep 4.89e+06/1.95e+08 =  2% of the original kernel matrix.

torch.Size([26365, 2])
We keep 3.89e+07/7.23e+08 =  5% of the original kernel matrix.

torch.Size([23115, 2])
We keep 9.33e+06/4.78e+08 =  1% of the original kernel matrix.

torch.Size([50020, 2])
We keep 1.11e+08/3.17e+09 =  3% of the original kernel matrix.

torch.Size([31397, 2])
We keep 1.74e+07/1.00e+09 =  1% of the original kernel matrix.

torch.Size([4509, 2])
We keep 4.72e+05/8.95e+06 =  5% of the original kernel matrix.

torch.Size([9717, 2])
We keep 1.77e+06/5.31e+07 =  3% of the original kernel matrix.

torch.Size([26537, 2])
We keep 1.89e+07/6.91e+08 =  2% of the original kernel matrix.

torch.Size([23475, 2])
We keep 9.31e+06/4.67e+08 =  1% of the original kernel matrix.

torch.Size([48661, 2])
We keep 5.64e+07/2.46e+09 =  2% of the original kernel matrix.

torch.Size([30886, 2])
We keep 1.56e+07/8.82e+08 =  1% of the original kernel matrix.

torch.Size([41030, 2])
We keep 4.54e+07/1.78e+09 =  2% of the original kernel matrix.

torch.Size([28796, 2])
We keep 1.41e+07/7.50e+08 =  1% of the original kernel matrix.

torch.Size([195348, 2])
We keep 7.78e+08/4.67e+10 =  1% of the original kernel matrix.

torch.Size([65906, 2])
We keep 5.83e+07/3.84e+09 =  1% of the original kernel matrix.

torch.Size([2365, 2])
We keep 1.19e+05/1.61e+06 =  7% of the original kernel matrix.

torch.Size([7679, 2])
We keep 9.78e+05/2.26e+07 =  4% of the original kernel matrix.

torch.Size([14326, 2])
We keep 1.29e+07/1.88e+08 =  6% of the original kernel matrix.

torch.Size([16649, 2])
We keep 5.68e+06/2.43e+08 =  2% of the original kernel matrix.

torch.Size([49309, 2])
We keep 6.28e+07/2.51e+09 =  2% of the original kernel matrix.

torch.Size([31368, 2])
We keep 1.59e+07/8.90e+08 =  1% of the original kernel matrix.

torch.Size([210743, 2])
We keep 6.89e+08/4.76e+10 =  1% of the original kernel matrix.

torch.Size([68220, 2])
We keep 5.65e+07/3.87e+09 =  1% of the original kernel matrix.

torch.Size([4114, 2])
We keep 3.87e+05/6.21e+06 =  6% of the original kernel matrix.

torch.Size([9487, 2])
We keep 1.52e+06/4.43e+07 =  3% of the original kernel matrix.

torch.Size([20695, 2])
We keep 1.02e+07/4.08e+08 =  2% of the original kernel matrix.

torch.Size([20672, 2])
We keep 7.59e+06/3.59e+08 =  2% of the original kernel matrix.

torch.Size([77480, 2])
We keep 3.34e+08/8.90e+09 =  3% of the original kernel matrix.

torch.Size([39302, 2])
We keep 2.81e+07/1.68e+09 =  1% of the original kernel matrix.

torch.Size([136854, 2])
We keep 7.70e+08/2.71e+10 =  2% of the original kernel matrix.

torch.Size([52882, 2])
We keep 4.66e+07/2.93e+09 =  1% of the original kernel matrix.

torch.Size([35516, 2])
We keep 4.16e+07/1.53e+09 =  2% of the original kernel matrix.

torch.Size([26605, 2])
We keep 1.34e+07/6.95e+08 =  1% of the original kernel matrix.

torch.Size([37608, 2])
We keep 4.97e+07/1.66e+09 =  2% of the original kernel matrix.

torch.Size([27283, 2])
We keep 1.38e+07/7.23e+08 =  1% of the original kernel matrix.

torch.Size([16964, 2])
We keep 9.42e+06/2.39e+08 =  3% of the original kernel matrix.

torch.Size([18446, 2])
We keep 5.88e+06/2.75e+08 =  2% of the original kernel matrix.

torch.Size([10218, 2])
We keep 2.44e+06/6.16e+07 =  3% of the original kernel matrix.

torch.Size([14035, 2])
We keep 3.60e+06/1.39e+08 =  2% of the original kernel matrix.

torch.Size([9457, 2])
We keep 5.14e+06/7.54e+07 =  6% of the original kernel matrix.

torch.Size([13512, 2])
We keep 4.11e+06/1.54e+08 =  2% of the original kernel matrix.

torch.Size([40861, 2])
We keep 7.25e+07/2.01e+09 =  3% of the original kernel matrix.

torch.Size([28813, 2])
We keep 1.31e+07/7.97e+08 =  1% of the original kernel matrix.

torch.Size([5789, 2])
We keep 1.28e+06/1.69e+07 =  7% of the original kernel matrix.

torch.Size([10762, 2])
We keep 2.29e+06/7.30e+07 =  3% of the original kernel matrix.

torch.Size([4657, 2])
We keep 8.50e+05/1.01e+07 =  8% of the original kernel matrix.

torch.Size([9930, 2])
We keep 1.88e+06/5.63e+07 =  3% of the original kernel matrix.

torch.Size([67249, 2])
We keep 9.00e+07/4.39e+09 =  2% of the original kernel matrix.

torch.Size([36269, 2])
We keep 2.08e+07/1.18e+09 =  1% of the original kernel matrix.

torch.Size([20696, 2])
We keep 1.17e+07/4.31e+08 =  2% of the original kernel matrix.

torch.Size([20267, 2])
We keep 7.84e+06/3.69e+08 =  2% of the original kernel matrix.

torch.Size([243133, 2])
We keep 9.85e+08/6.38e+10 =  1% of the original kernel matrix.

torch.Size([73271, 2])
We keep 6.65e+07/4.49e+09 =  1% of the original kernel matrix.

torch.Size([65495, 2])
We keep 6.73e+07/4.16e+09 =  1% of the original kernel matrix.

torch.Size([35812, 2])
We keep 1.99e+07/1.15e+09 =  1% of the original kernel matrix.

torch.Size([1715, 2])
We keep 7.97e+04/9.06e+05 =  8% of the original kernel matrix.

torch.Size([6769, 2])
We keep 8.16e+05/1.69e+07 =  4% of the original kernel matrix.

torch.Size([13808, 2])
We keep 4.22e+06/1.28e+08 =  3% of the original kernel matrix.

torch.Size([16470, 2])
We keep 4.62e+06/2.01e+08 =  2% of the original kernel matrix.

torch.Size([7617, 2])
We keep 1.45e+06/2.74e+07 =  5% of the original kernel matrix.

torch.Size([12185, 2])
We keep 2.65e+06/9.31e+07 =  2% of the original kernel matrix.

torch.Size([11716, 2])
We keep 3.71e+06/9.25e+07 =  4% of the original kernel matrix.

torch.Size([15092, 2])
We keep 4.19e+06/1.71e+08 =  2% of the original kernel matrix.

torch.Size([44240, 2])
We keep 3.80e+07/2.08e+09 =  1% of the original kernel matrix.

torch.Size([29983, 2])
We keep 1.43e+07/8.10e+08 =  1% of the original kernel matrix.

torch.Size([5147, 2])
We keep 9.67e+05/1.41e+07 =  6% of the original kernel matrix.

torch.Size([10169, 2])
We keep 2.16e+06/6.68e+07 =  3% of the original kernel matrix.

torch.Size([23442, 2])
We keep 1.42e+07/5.96e+08 =  2% of the original kernel matrix.

torch.Size([21976, 2])
We keep 8.82e+06/4.34e+08 =  2% of the original kernel matrix.

torch.Size([1745, 2])
We keep 6.66e+04/7.14e+05 =  9% of the original kernel matrix.

torch.Size([6890, 2])
We keep 7.42e+05/1.50e+07 =  4% of the original kernel matrix.

torch.Size([5071, 2])
We keep 5.26e+05/1.00e+07 =  5% of the original kernel matrix.

torch.Size([10337, 2])
We keep 1.84e+06/5.62e+07 =  3% of the original kernel matrix.

torch.Size([15066, 2])
We keep 1.39e+07/2.17e+08 =  6% of the original kernel matrix.

torch.Size([17312, 2])
We keep 5.70e+06/2.62e+08 =  2% of the original kernel matrix.

torch.Size([42195, 2])
We keep 5.16e+07/2.02e+09 =  2% of the original kernel matrix.

torch.Size([29050, 2])
We keep 1.48e+07/7.99e+08 =  1% of the original kernel matrix.

torch.Size([24946, 2])
We keep 1.90e+07/7.29e+08 =  2% of the original kernel matrix.

torch.Size([22375, 2])
We keep 9.61e+06/4.80e+08 =  2% of the original kernel matrix.

torch.Size([4393, 2])
We keep 4.72e+05/7.75e+06 =  6% of the original kernel matrix.

torch.Size([9562, 2])
We keep 1.70e+06/4.95e+07 =  3% of the original kernel matrix.

torch.Size([147676, 2])
We keep 2.73e+08/2.55e+10 =  1% of the original kernel matrix.

torch.Size([56003, 2])
We keep 4.31e+07/2.84e+09 =  1% of the original kernel matrix.

torch.Size([11069, 2])
We keep 2.41e+06/7.28e+07 =  3% of the original kernel matrix.

torch.Size([14681, 2])
We keep 3.76e+06/1.52e+08 =  2% of the original kernel matrix.

torch.Size([15193, 2])
We keep 4.62e+06/1.79e+08 =  2% of the original kernel matrix.

torch.Size([17349, 2])
We keep 5.44e+06/2.37e+08 =  2% of the original kernel matrix.

torch.Size([57959, 2])
We keep 6.24e+07/3.57e+09 =  1% of the original kernel matrix.

torch.Size([34025, 2])
We keep 1.85e+07/1.06e+09 =  1% of the original kernel matrix.

torch.Size([3397, 2])
We keep 3.86e+05/5.39e+06 =  7% of the original kernel matrix.

torch.Size([8611, 2])
We keep 1.51e+06/4.12e+07 =  3% of the original kernel matrix.

torch.Size([50409, 2])
We keep 5.06e+07/2.68e+09 =  1% of the original kernel matrix.

torch.Size([31669, 2])
We keep 1.67e+07/9.20e+08 =  1% of the original kernel matrix.

torch.Size([62785, 2])
We keep 1.07e+08/4.19e+09 =  2% of the original kernel matrix.

torch.Size([35545, 2])
We keep 1.96e+07/1.15e+09 =  1% of the original kernel matrix.

torch.Size([126329, 2])
We keep 5.10e+08/2.10e+10 =  2% of the original kernel matrix.

torch.Size([50966, 2])
We keep 4.09e+07/2.58e+09 =  1% of the original kernel matrix.

torch.Size([11977, 2])
We keep 3.43e+06/1.02e+08 =  3% of the original kernel matrix.

torch.Size([15321, 2])
We keep 4.32e+06/1.79e+08 =  2% of the original kernel matrix.

torch.Size([20297, 2])
We keep 1.03e+07/3.77e+08 =  2% of the original kernel matrix.

torch.Size([20253, 2])
We keep 7.44e+06/3.45e+08 =  2% of the original kernel matrix.

torch.Size([13522, 2])
We keep 5.26e+06/1.39e+08 =  3% of the original kernel matrix.

torch.Size([16077, 2])
We keep 5.17e+06/2.10e+08 =  2% of the original kernel matrix.

torch.Size([4955, 2])
We keep 5.87e+05/1.10e+07 =  5% of the original kernel matrix.

torch.Size([9949, 2])
We keep 1.92e+06/5.89e+07 =  3% of the original kernel matrix.

torch.Size([23684, 2])
We keep 2.33e+07/5.76e+08 =  4% of the original kernel matrix.

torch.Size([21929, 2])
We keep 8.58e+06/4.27e+08 =  2% of the original kernel matrix.

torch.Size([10907, 2])
We keep 2.46e+06/6.80e+07 =  3% of the original kernel matrix.

torch.Size([14459, 2])
We keep 3.71e+06/1.47e+08 =  2% of the original kernel matrix.

torch.Size([20966, 2])
We keep 1.84e+07/4.61e+08 =  3% of the original kernel matrix.

torch.Size([20529, 2])
We keep 8.06e+06/3.81e+08 =  2% of the original kernel matrix.

torch.Size([51582, 2])
We keep 1.40e+08/3.37e+09 =  4% of the original kernel matrix.

torch.Size([32264, 2])
We keep 1.57e+07/1.03e+09 =  1% of the original kernel matrix.

torch.Size([25954, 2])
We keep 9.76e+07/7.44e+08 = 13% of the original kernel matrix.

torch.Size([22931, 2])
We keep 9.41e+06/4.85e+08 =  1% of the original kernel matrix.

torch.Size([5522, 2])
We keep 1.26e+06/1.74e+07 =  7% of the original kernel matrix.

torch.Size([10613, 2])
We keep 2.16e+06/7.41e+07 =  2% of the original kernel matrix.

torch.Size([11030, 2])
We keep 2.64e+06/7.38e+07 =  3% of the original kernel matrix.

torch.Size([14607, 2])
We keep 3.80e+06/1.53e+08 =  2% of the original kernel matrix.

torch.Size([20276, 2])
We keep 4.91e+07/1.37e+09 =  3% of the original kernel matrix.

torch.Size([19444, 2])
We keep 1.15e+07/6.57e+08 =  1% of the original kernel matrix.

torch.Size([16750, 2])
We keep 7.12e+06/2.28e+08 =  3% of the original kernel matrix.

torch.Size([18285, 2])
We keep 5.96e+06/2.69e+08 =  2% of the original kernel matrix.

torch.Size([3239, 2])
We keep 2.32e+05/3.72e+06 =  6% of the original kernel matrix.

torch.Size([8571, 2])
We keep 1.29e+06/3.43e+07 =  3% of the original kernel matrix.

torch.Size([10246, 2])
We keep 2.64e+06/6.32e+07 =  4% of the original kernel matrix.

torch.Size([14087, 2])
We keep 3.72e+06/1.41e+08 =  2% of the original kernel matrix.

torch.Size([212185, 2])
We keep 9.98e+08/6.00e+10 =  1% of the original kernel matrix.

torch.Size([67773, 2])
We keep 6.42e+07/4.35e+09 =  1% of the original kernel matrix.

torch.Size([9870, 2])
We keep 2.92e+06/5.49e+07 =  5% of the original kernel matrix.

torch.Size([13790, 2])
We keep 3.48e+06/1.32e+08 =  2% of the original kernel matrix.

torch.Size([46724, 2])
We keep 1.91e+08/3.60e+09 =  5% of the original kernel matrix.

torch.Size([29549, 2])
We keep 1.83e+07/1.07e+09 =  1% of the original kernel matrix.

torch.Size([5288, 2])
We keep 1.17e+06/1.43e+07 =  8% of the original kernel matrix.

torch.Size([10398, 2])
We keep 2.16e+06/6.73e+07 =  3% of the original kernel matrix.

torch.Size([8646, 2])
We keep 5.23e+06/7.65e+07 =  6% of the original kernel matrix.

torch.Size([12714, 2])
We keep 3.82e+06/1.55e+08 =  2% of the original kernel matrix.

torch.Size([16665, 2])
We keep 1.22e+07/2.37e+08 =  5% of the original kernel matrix.

torch.Size([18280, 2])
We keep 5.58e+06/2.74e+08 =  2% of the original kernel matrix.

torch.Size([16028, 2])
We keep 4.70e+07/4.99e+08 =  9% of the original kernel matrix.

torch.Size([17583, 2])
We keep 7.78e+06/3.97e+08 =  1% of the original kernel matrix.

torch.Size([30997, 2])
We keep 1.81e+07/9.75e+08 =  1% of the original kernel matrix.

torch.Size([24978, 2])
We keep 1.08e+07/5.55e+08 =  1% of the original kernel matrix.

torch.Size([6503, 2])
We keep 8.81e+05/1.87e+07 =  4% of the original kernel matrix.

torch.Size([11391, 2])
We keep 2.33e+06/7.69e+07 =  3% of the original kernel matrix.

torch.Size([86385, 2])
We keep 9.65e+07/7.12e+09 =  1% of the original kernel matrix.

torch.Size([41817, 2])
We keep 2.49e+07/1.50e+09 =  1% of the original kernel matrix.

torch.Size([71742, 2])
We keep 7.66e+07/4.89e+09 =  1% of the original kernel matrix.

torch.Size([37641, 2])
We keep 2.05e+07/1.24e+09 =  1% of the original kernel matrix.

torch.Size([5663, 2])
We keep 1.22e+06/1.83e+07 =  6% of the original kernel matrix.

torch.Size([10782, 2])
We keep 2.25e+06/7.60e+07 =  2% of the original kernel matrix.

torch.Size([36751, 2])
We keep 3.05e+07/1.30e+09 =  2% of the original kernel matrix.

torch.Size([27632, 2])
We keep 1.21e+07/6.40e+08 =  1% of the original kernel matrix.

torch.Size([68690, 2])
We keep 7.57e+07/5.15e+09 =  1% of the original kernel matrix.

torch.Size([37224, 2])
We keep 2.17e+07/1.28e+09 =  1% of the original kernel matrix.

torch.Size([132055, 2])
We keep 2.44e+08/2.06e+10 =  1% of the original kernel matrix.

torch.Size([52611, 2])
We keep 3.90e+07/2.55e+09 =  1% of the original kernel matrix.

torch.Size([9520, 2])
We keep 4.01e+06/5.76e+07 =  6% of the original kernel matrix.

torch.Size([13670, 2])
We keep 3.24e+06/1.35e+08 =  2% of the original kernel matrix.

torch.Size([2516, 2])
We keep 2.55e+05/2.77e+06 =  9% of the original kernel matrix.

torch.Size([7664, 2])
We keep 1.23e+06/2.96e+07 =  4% of the original kernel matrix.

torch.Size([8965, 2])
We keep 1.72e+06/4.41e+07 =  3% of the original kernel matrix.

torch.Size([13038, 2])
We keep 3.21e+06/1.18e+08 =  2% of the original kernel matrix.

torch.Size([13852, 2])
We keep 1.03e+07/2.10e+08 =  4% of the original kernel matrix.

torch.Size([16608, 2])
We keep 5.50e+06/2.58e+08 =  2% of the original kernel matrix.

torch.Size([30456, 2])
We keep 2.24e+07/9.34e+08 =  2% of the original kernel matrix.

torch.Size([24996, 2])
We keep 1.09e+07/5.43e+08 =  2% of the original kernel matrix.

torch.Size([6764, 2])
We keep 1.23e+06/2.64e+07 =  4% of the original kernel matrix.

torch.Size([11431, 2])
We keep 2.60e+06/9.12e+07 =  2% of the original kernel matrix.

torch.Size([3489, 2])
We keep 3.30e+05/4.64e+06 =  7% of the original kernel matrix.

torch.Size([8911, 2])
We keep 1.43e+06/3.83e+07 =  3% of the original kernel matrix.

torch.Size([59363, 2])
We keep 9.59e+07/3.89e+09 =  2% of the original kernel matrix.

torch.Size([34532, 2])
We keep 1.97e+07/1.11e+09 =  1% of the original kernel matrix.

torch.Size([92694, 2])
We keep 1.28e+08/8.46e+09 =  1% of the original kernel matrix.

torch.Size([43183, 2])
We keep 2.71e+07/1.63e+09 =  1% of the original kernel matrix.

torch.Size([106216, 2])
We keep 4.54e+08/2.01e+10 =  2% of the original kernel matrix.

torch.Size([44454, 2])
We keep 4.03e+07/2.52e+09 =  1% of the original kernel matrix.

torch.Size([5295, 2])
We keep 8.33e+05/1.41e+07 =  5% of the original kernel matrix.

torch.Size([10309, 2])
We keep 2.12e+06/6.67e+07 =  3% of the original kernel matrix.

torch.Size([13020, 2])
We keep 4.78e+06/1.24e+08 =  3% of the original kernel matrix.

torch.Size([15918, 2])
We keep 4.72e+06/1.98e+08 =  2% of the original kernel matrix.

torch.Size([12352, 2])
We keep 3.40e+06/9.87e+07 =  3% of the original kernel matrix.

torch.Size([15530, 2])
We keep 4.32e+06/1.77e+08 =  2% of the original kernel matrix.

torch.Size([5381, 2])
We keep 6.25e+05/1.18e+07 =  5% of the original kernel matrix.

torch.Size([10595, 2])
We keep 1.94e+06/6.10e+07 =  3% of the original kernel matrix.

torch.Size([8968, 2])
We keep 1.78e+06/4.41e+07 =  4% of the original kernel matrix.

torch.Size([13143, 2])
We keep 3.21e+06/1.18e+08 =  2% of the original kernel matrix.

torch.Size([5551, 2])
We keep 7.16e+05/1.29e+07 =  5% of the original kernel matrix.

torch.Size([10644, 2])
We keep 2.04e+06/6.38e+07 =  3% of the original kernel matrix.

torch.Size([172079, 2])
We keep 9.49e+08/4.04e+10 =  2% of the original kernel matrix.

torch.Size([60304, 2])
We keep 5.47e+07/3.57e+09 =  1% of the original kernel matrix.

torch.Size([26357, 2])
We keep 1.38e+07/7.81e+08 =  1% of the original kernel matrix.

torch.Size([23514, 2])
We keep 9.79e+06/4.97e+08 =  1% of the original kernel matrix.

torch.Size([10313, 2])
We keep 2.16e+06/6.13e+07 =  3% of the original kernel matrix.

torch.Size([14092, 2])
We keep 3.50e+06/1.39e+08 =  2% of the original kernel matrix.

torch.Size([6962, 2])
We keep 1.48e+06/2.74e+07 =  5% of the original kernel matrix.

torch.Size([11609, 2])
We keep 2.61e+06/9.30e+07 =  2% of the original kernel matrix.

torch.Size([4974, 2])
We keep 1.09e+06/9.96e+06 = 10% of the original kernel matrix.

torch.Size([10245, 2])
We keep 1.78e+06/5.61e+07 =  3% of the original kernel matrix.

torch.Size([235891, 2])
We keep 8.14e+08/5.92e+10 =  1% of the original kernel matrix.

torch.Size([72372, 2])
We keep 6.29e+07/4.32e+09 =  1% of the original kernel matrix.

torch.Size([8331, 2])
We keep 1.55e+06/3.38e+07 =  4% of the original kernel matrix.

torch.Size([12642, 2])
We keep 2.92e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([79872, 2])
We keep 1.05e+08/6.19e+09 =  1% of the original kernel matrix.

torch.Size([39501, 2])
We keep 2.39e+07/1.40e+09 =  1% of the original kernel matrix.

torch.Size([2480, 2])
We keep 2.02e+05/2.43e+06 =  8% of the original kernel matrix.

torch.Size([7667, 2])
We keep 1.07e+06/2.77e+07 =  3% of the original kernel matrix.

torch.Size([4594, 2])
We keep 4.43e+05/7.89e+06 =  5% of the original kernel matrix.

torch.Size([9881, 2])
We keep 1.70e+06/4.99e+07 =  3% of the original kernel matrix.

torch.Size([3295, 2])
We keep 2.41e+05/3.99e+06 =  6% of the original kernel matrix.

torch.Size([8599, 2])
We keep 1.32e+06/3.55e+07 =  3% of the original kernel matrix.

torch.Size([18085, 2])
We keep 1.01e+07/2.70e+08 =  3% of the original kernel matrix.

torch.Size([18795, 2])
We keep 5.86e+06/2.92e+08 =  2% of the original kernel matrix.

torch.Size([27083, 2])
We keep 1.12e+08/1.25e+09 =  8% of the original kernel matrix.

torch.Size([23186, 2])
We keep 1.27e+07/6.28e+08 =  2% of the original kernel matrix.

torch.Size([6605, 2])
We keep 1.53e+06/2.62e+07 =  5% of the original kernel matrix.

torch.Size([11300, 2])
We keep 2.38e+06/9.09e+07 =  2% of the original kernel matrix.

torch.Size([5890, 2])
We keep 8.35e+05/1.72e+07 =  4% of the original kernel matrix.

torch.Size([10712, 2])
We keep 2.22e+06/7.36e+07 =  3% of the original kernel matrix.

torch.Size([5414, 2])
We keep 7.62e+05/1.38e+07 =  5% of the original kernel matrix.

torch.Size([10390, 2])
We keep 2.12e+06/6.60e+07 =  3% of the original kernel matrix.

torch.Size([112459, 2])
We keep 2.73e+08/1.40e+10 =  1% of the original kernel matrix.

torch.Size([48161, 2])
We keep 3.30e+07/2.10e+09 =  1% of the original kernel matrix.

torch.Size([18890, 2])
We keep 8.25e+06/3.14e+08 =  2% of the original kernel matrix.

torch.Size([19460, 2])
We keep 6.71e+06/3.15e+08 =  2% of the original kernel matrix.

torch.Size([11415, 2])
We keep 3.47e+06/8.43e+07 =  4% of the original kernel matrix.

torch.Size([14915, 2])
We keep 3.99e+06/1.63e+08 =  2% of the original kernel matrix.

torch.Size([31780, 2])
We keep 2.56e+07/1.09e+09 =  2% of the original kernel matrix.

torch.Size([25860, 2])
We keep 1.10e+07/5.87e+08 =  1% of the original kernel matrix.

torch.Size([15185, 2])
We keep 5.12e+07/6.55e+08 =  7% of the original kernel matrix.

torch.Size([17110, 2])
We keep 8.76e+06/4.55e+08 =  1% of the original kernel matrix.

torch.Size([5030, 2])
We keep 5.84e+05/1.10e+07 =  5% of the original kernel matrix.

torch.Size([10071, 2])
We keep 1.91e+06/5.91e+07 =  3% of the original kernel matrix.

torch.Size([5013, 2])
We keep 5.77e+05/1.06e+07 =  5% of the original kernel matrix.

torch.Size([10103, 2])
We keep 1.89e+06/5.79e+07 =  3% of the original kernel matrix.

torch.Size([8151, 2])
We keep 5.66e+06/4.68e+07 = 12% of the original kernel matrix.

torch.Size([12559, 2])
We keep 3.21e+06/1.22e+08 =  2% of the original kernel matrix.

torch.Size([1502, 2])
We keep 5.56e+04/5.81e+05 =  9% of the original kernel matrix.

torch.Size([6480, 2])
We keep 6.96e+05/1.35e+07 =  5% of the original kernel matrix.

torch.Size([6114, 2])
We keep 8.26e+05/1.64e+07 =  5% of the original kernel matrix.

torch.Size([10962, 2])
We keep 2.21e+06/7.20e+07 =  3% of the original kernel matrix.

torch.Size([16942, 2])
We keep 8.89e+06/2.69e+08 =  3% of the original kernel matrix.

torch.Size([18008, 2])
We keep 6.37e+06/2.92e+08 =  2% of the original kernel matrix.

torch.Size([2479, 2])
We keep 2.80e+05/2.45e+06 = 11% of the original kernel matrix.

torch.Size([7640, 2])
We keep 1.11e+06/2.78e+07 =  3% of the original kernel matrix.

torch.Size([4362, 2])
We keep 4.92e+05/8.73e+06 =  5% of the original kernel matrix.

torch.Size([9462, 2])
We keep 1.77e+06/5.25e+07 =  3% of the original kernel matrix.

torch.Size([9444, 2])
We keep 2.30e+06/5.20e+07 =  4% of the original kernel matrix.

torch.Size([13341, 2])
We keep 3.47e+06/1.28e+08 =  2% of the original kernel matrix.

torch.Size([4867, 2])
We keep 6.33e+05/1.14e+07 =  5% of the original kernel matrix.

torch.Size([9979, 2])
We keep 1.94e+06/6.00e+07 =  3% of the original kernel matrix.

torch.Size([215834, 2])
We keep 1.18e+09/6.50e+10 =  1% of the original kernel matrix.

torch.Size([67915, 2])
We keep 6.74e+07/4.53e+09 =  1% of the original kernel matrix.

torch.Size([42185, 2])
We keep 2.48e+08/3.00e+09 =  8% of the original kernel matrix.

torch.Size([28332, 2])
We keep 1.78e+07/9.74e+08 =  1% of the original kernel matrix.

torch.Size([31748, 2])
We keep 2.69e+07/1.05e+09 =  2% of the original kernel matrix.

torch.Size([25514, 2])
We keep 1.12e+07/5.77e+08 =  1% of the original kernel matrix.

torch.Size([24618, 2])
We keep 2.69e+07/7.87e+08 =  3% of the original kernel matrix.

torch.Size([22182, 2])
We keep 1.01e+07/4.98e+08 =  2% of the original kernel matrix.

torch.Size([4383, 2])
We keep 5.28e+05/8.81e+06 =  5% of the original kernel matrix.

torch.Size([9545, 2])
We keep 1.78e+06/5.27e+07 =  3% of the original kernel matrix.

torch.Size([13820, 2])
We keep 8.62e+06/1.62e+08 =  5% of the original kernel matrix.

torch.Size([16566, 2])
We keep 5.25e+06/2.26e+08 =  2% of the original kernel matrix.

torch.Size([385683, 2])
We keep 6.29e+09/1.72e+11 =  3% of the original kernel matrix.

torch.Size([92047, 2])
We keep 9.74e+07/7.36e+09 =  1% of the original kernel matrix.

torch.Size([466333, 2])
We keep 1.67e+09/2.01e+11 =  0% of the original kernel matrix.

torch.Size([101798, 2])
We keep 1.08e+08/7.96e+09 =  1% of the original kernel matrix.

torch.Size([39979, 2])
We keep 3.53e+07/1.69e+09 =  2% of the original kernel matrix.

torch.Size([28685, 2])
We keep 1.33e+07/7.30e+08 =  1% of the original kernel matrix.

torch.Size([26600, 2])
We keep 2.42e+07/6.90e+08 =  3% of the original kernel matrix.

torch.Size([23495, 2])
We keep 9.43e+06/4.67e+08 =  2% of the original kernel matrix.

torch.Size([2919, 2])
We keep 2.16e+05/3.18e+06 =  6% of the original kernel matrix.

torch.Size([8211, 2])
We keep 1.25e+06/3.17e+07 =  3% of the original kernel matrix.

torch.Size([66312, 2])
We keep 2.00e+08/7.82e+09 =  2% of the original kernel matrix.

torch.Size([34731, 2])
We keep 2.69e+07/1.57e+09 =  1% of the original kernel matrix.

torch.Size([30682, 2])
We keep 3.75e+07/1.12e+09 =  3% of the original kernel matrix.

torch.Size([24993, 2])
We keep 1.15e+07/5.95e+08 =  1% of the original kernel matrix.

torch.Size([193693, 2])
We keep 7.74e+08/4.91e+10 =  1% of the original kernel matrix.

torch.Size([65437, 2])
We keep 5.84e+07/3.94e+09 =  1% of the original kernel matrix.

torch.Size([7251, 2])
We keep 1.39e+06/2.72e+07 =  5% of the original kernel matrix.

torch.Size([11861, 2])
We keep 2.73e+06/9.26e+07 =  2% of the original kernel matrix.

torch.Size([68632, 2])
We keep 9.03e+07/4.74e+09 =  1% of the original kernel matrix.

torch.Size([36780, 2])
We keep 2.10e+07/1.22e+09 =  1% of the original kernel matrix.

torch.Size([96559, 2])
We keep 1.90e+08/1.05e+10 =  1% of the original kernel matrix.

torch.Size([44199, 2])
We keep 2.93e+07/1.82e+09 =  1% of the original kernel matrix.

torch.Size([43013, 2])
We keep 5.66e+07/2.13e+09 =  2% of the original kernel matrix.

torch.Size([28832, 2])
We keep 1.48e+07/8.19e+08 =  1% of the original kernel matrix.

torch.Size([6953, 2])
We keep 1.72e+06/3.02e+07 =  5% of the original kernel matrix.

torch.Size([11478, 2])
We keep 2.85e+06/9.77e+07 =  2% of the original kernel matrix.

torch.Size([11855, 2])
We keep 3.92e+06/8.53e+07 =  4% of the original kernel matrix.

torch.Size([15575, 2])
We keep 4.22e+06/1.64e+08 =  2% of the original kernel matrix.

torch.Size([12182, 2])
We keep 4.30e+06/9.56e+07 =  4% of the original kernel matrix.

torch.Size([15393, 2])
We keep 4.37e+06/1.74e+08 =  2% of the original kernel matrix.

torch.Size([4370, 2])
We keep 4.95e+05/8.85e+06 =  5% of the original kernel matrix.

torch.Size([9572, 2])
We keep 1.71e+06/5.29e+07 =  3% of the original kernel matrix.

torch.Size([56200, 2])
We keep 6.70e+07/3.31e+09 =  2% of the original kernel matrix.

torch.Size([33386, 2])
We keep 1.79e+07/1.02e+09 =  1% of the original kernel matrix.

torch.Size([21961, 2])
We keep 2.28e+07/5.76e+08 =  3% of the original kernel matrix.

torch.Size([20942, 2])
We keep 8.91e+06/4.26e+08 =  2% of the original kernel matrix.

torch.Size([3064, 2])
We keep 3.14e+05/3.45e+06 =  9% of the original kernel matrix.

torch.Size([8307, 2])
We keep 1.34e+06/3.30e+07 =  4% of the original kernel matrix.

torch.Size([21069, 2])
We keep 1.25e+07/4.00e+08 =  3% of the original kernel matrix.

torch.Size([20891, 2])
We keep 7.16e+06/3.55e+08 =  2% of the original kernel matrix.

torch.Size([13951, 2])
We keep 4.53e+06/1.45e+08 =  3% of the original kernel matrix.

torch.Size([16737, 2])
We keep 5.09e+06/2.14e+08 =  2% of the original kernel matrix.

torch.Size([1394, 2])
We keep 6.10e+04/6.92e+05 =  8% of the original kernel matrix.

torch.Size([6209, 2])
We keep 7.34e+05/1.48e+07 =  4% of the original kernel matrix.

torch.Size([35636, 2])
We keep 4.19e+07/1.31e+09 =  3% of the original kernel matrix.

torch.Size([26986, 2])
We keep 1.20e+07/6.44e+08 =  1% of the original kernel matrix.

torch.Size([52070, 2])
We keep 4.46e+07/2.64e+09 =  1% of the original kernel matrix.

torch.Size([32438, 2])
We keep 1.62e+07/9.13e+08 =  1% of the original kernel matrix.

torch.Size([16414, 2])
We keep 6.16e+06/2.20e+08 =  2% of the original kernel matrix.

torch.Size([18112, 2])
We keep 5.95e+06/2.63e+08 =  2% of the original kernel matrix.

torch.Size([64528, 2])
We keep 1.11e+08/4.07e+09 =  2% of the original kernel matrix.

torch.Size([35513, 2])
We keep 2.01e+07/1.13e+09 =  1% of the original kernel matrix.

torch.Size([49172, 2])
We keep 4.87e+07/2.51e+09 =  1% of the original kernel matrix.

torch.Size([31349, 2])
We keep 1.64e+07/8.90e+08 =  1% of the original kernel matrix.

torch.Size([5827, 2])
We keep 9.28e+05/1.82e+07 =  5% of the original kernel matrix.

torch.Size([10710, 2])
We keep 2.25e+06/7.58e+07 =  2% of the original kernel matrix.

torch.Size([38310, 2])
We keep 3.71e+07/1.46e+09 =  2% of the original kernel matrix.

torch.Size([27962, 2])
We keep 1.27e+07/6.80e+08 =  1% of the original kernel matrix.

torch.Size([69815, 2])
We keep 6.36e+07/4.39e+09 =  1% of the original kernel matrix.

torch.Size([37030, 2])
We keep 2.00e+07/1.18e+09 =  1% of the original kernel matrix.

torch.Size([98720, 2])
We keep 1.34e+08/9.90e+09 =  1% of the original kernel matrix.

torch.Size([44581, 2])
We keep 2.82e+07/1.77e+09 =  1% of the original kernel matrix.

torch.Size([6596, 2])
We keep 8.88e+05/1.88e+07 =  4% of the original kernel matrix.

torch.Size([11429, 2])
We keep 2.28e+06/7.70e+07 =  2% of the original kernel matrix.

torch.Size([57525, 2])
We keep 1.77e+08/4.29e+09 =  4% of the original kernel matrix.

torch.Size([34089, 2])
We keep 1.79e+07/1.16e+09 =  1% of the original kernel matrix.

torch.Size([56963, 2])
We keep 2.09e+08/4.98e+09 =  4% of the original kernel matrix.

torch.Size([33923, 2])
We keep 1.87e+07/1.25e+09 =  1% of the original kernel matrix.

torch.Size([21269, 2])
We keep 1.17e+07/3.92e+08 =  2% of the original kernel matrix.

torch.Size([20834, 2])
We keep 7.00e+06/3.52e+08 =  1% of the original kernel matrix.

torch.Size([15070, 2])
We keep 1.90e+07/3.98e+08 =  4% of the original kernel matrix.

torch.Size([17310, 2])
We keep 6.47e+06/3.55e+08 =  1% of the original kernel matrix.

torch.Size([3824, 2])
We keep 1.55e+06/8.70e+06 = 17% of the original kernel matrix.

torch.Size([8944, 2])
We keep 1.57e+06/5.24e+07 =  3% of the original kernel matrix.

torch.Size([9343, 2])
We keep 2.51e+06/5.27e+07 =  4% of the original kernel matrix.

torch.Size([13586, 2])
We keep 3.49e+06/1.29e+08 =  2% of the original kernel matrix.

torch.Size([5947, 2])
We keep 7.99e+05/1.63e+07 =  4% of the original kernel matrix.

torch.Size([10797, 2])
We keep 2.20e+06/7.17e+07 =  3% of the original kernel matrix.

torch.Size([20792, 2])
We keep 3.46e+07/6.84e+08 =  5% of the original kernel matrix.

torch.Size([19813, 2])
We keep 9.69e+06/4.65e+08 =  2% of the original kernel matrix.

torch.Size([111083, 2])
We keep 1.60e+08/1.32e+10 =  1% of the original kernel matrix.

torch.Size([48086, 2])
We keep 3.25e+07/2.04e+09 =  1% of the original kernel matrix.

torch.Size([80355, 2])
We keep 8.63e+07/6.20e+09 =  1% of the original kernel matrix.

torch.Size([40170, 2])
We keep 2.34e+07/1.40e+09 =  1% of the original kernel matrix.

torch.Size([235367, 2])
We keep 1.20e+09/6.44e+10 =  1% of the original kernel matrix.

torch.Size([71432, 2])
We keep 6.50e+07/4.51e+09 =  1% of the original kernel matrix.

torch.Size([22993, 2])
We keep 2.84e+07/4.92e+08 =  5% of the original kernel matrix.

torch.Size([21763, 2])
We keep 8.00e+06/3.94e+08 =  2% of the original kernel matrix.

torch.Size([70852, 2])
We keep 1.08e+08/5.04e+09 =  2% of the original kernel matrix.

torch.Size([37467, 2])
We keep 2.21e+07/1.26e+09 =  1% of the original kernel matrix.

torch.Size([11083, 2])
We keep 2.95e+06/9.22e+07 =  3% of the original kernel matrix.

torch.Size([14831, 2])
We keep 4.17e+06/1.71e+08 =  2% of the original kernel matrix.

torch.Size([77737, 2])
We keep 9.73e+07/5.62e+09 =  1% of the original kernel matrix.

torch.Size([39123, 2])
We keep 2.27e+07/1.33e+09 =  1% of the original kernel matrix.

torch.Size([2248, 2])
We keep 1.34e+05/1.68e+06 =  8% of the original kernel matrix.

torch.Size([7455, 2])
We keep 9.73e+05/2.30e+07 =  4% of the original kernel matrix.

torch.Size([11037, 2])
We keep 3.63e+06/8.63e+07 =  4% of the original kernel matrix.

torch.Size([14564, 2])
We keep 4.22e+06/1.65e+08 =  2% of the original kernel matrix.

torch.Size([116681, 2])
We keep 2.13e+08/1.35e+10 =  1% of the original kernel matrix.

torch.Size([48804, 2])
We keep 3.29e+07/2.07e+09 =  1% of the original kernel matrix.

torch.Size([11394, 2])
We keep 3.40e+06/8.33e+07 =  4% of the original kernel matrix.

torch.Size([14874, 2])
We keep 3.99e+06/1.62e+08 =  2% of the original kernel matrix.

torch.Size([3080, 2])
We keep 2.83e+05/4.44e+06 =  6% of the original kernel matrix.

torch.Size([8356, 2])
We keep 1.42e+06/3.75e+07 =  3% of the original kernel matrix.

torch.Size([9299, 2])
We keep 2.45e+06/5.92e+07 =  4% of the original kernel matrix.

torch.Size([13528, 2])
We keep 3.57e+06/1.37e+08 =  2% of the original kernel matrix.

torch.Size([22255, 2])
We keep 1.64e+07/5.02e+08 =  3% of the original kernel matrix.

torch.Size([21221, 2])
We keep 8.09e+06/3.98e+08 =  2% of the original kernel matrix.

torch.Size([17170, 2])
We keep 1.03e+07/2.82e+08 =  3% of the original kernel matrix.

torch.Size([18466, 2])
We keep 6.77e+06/2.99e+08 =  2% of the original kernel matrix.

torch.Size([64820, 2])
We keep 1.05e+08/4.33e+09 =  2% of the original kernel matrix.

torch.Size([35865, 2])
We keep 2.02e+07/1.17e+09 =  1% of the original kernel matrix.

torch.Size([9072, 2])
We keep 6.47e+06/6.90e+07 =  9% of the original kernel matrix.

torch.Size([13211, 2])
We keep 3.90e+06/1.48e+08 =  2% of the original kernel matrix.

torch.Size([16069, 2])
We keep 1.09e+07/2.51e+08 =  4% of the original kernel matrix.

torch.Size([17593, 2])
We keep 6.47e+06/2.82e+08 =  2% of the original kernel matrix.

torch.Size([3299, 2])
We keep 2.88e+05/4.42e+06 =  6% of the original kernel matrix.

torch.Size([8643, 2])
We keep 1.36e+06/3.73e+07 =  3% of the original kernel matrix.

torch.Size([4007, 2])
We keep 3.76e+05/6.38e+06 =  5% of the original kernel matrix.

torch.Size([9427, 2])
We keep 1.60e+06/4.49e+07 =  3% of the original kernel matrix.

torch.Size([15818, 2])
We keep 7.40e+06/2.20e+08 =  3% of the original kernel matrix.

torch.Size([17816, 2])
We keep 6.01e+06/2.64e+08 =  2% of the original kernel matrix.

torch.Size([24110, 2])
We keep 3.34e+07/6.94e+08 =  4% of the original kernel matrix.

torch.Size([21991, 2])
We keep 9.35e+06/4.68e+08 =  1% of the original kernel matrix.

torch.Size([5223, 2])
We keep 1.64e+06/1.33e+07 = 12% of the original kernel matrix.

torch.Size([10346, 2])
We keep 1.98e+06/6.48e+07 =  3% of the original kernel matrix.

torch.Size([69749, 2])
We keep 9.71e+07/4.89e+09 =  1% of the original kernel matrix.

torch.Size([37128, 2])
We keep 2.09e+07/1.24e+09 =  1% of the original kernel matrix.

torch.Size([4140, 2])
We keep 3.78e+05/6.21e+06 =  6% of the original kernel matrix.

torch.Size([9438, 2])
We keep 1.55e+06/4.43e+07 =  3% of the original kernel matrix.

torch.Size([17745, 2])
We keep 6.65e+06/2.56e+08 =  2% of the original kernel matrix.

torch.Size([19037, 2])
We keep 6.42e+06/2.84e+08 =  2% of the original kernel matrix.

torch.Size([3352, 2])
We keep 3.15e+05/4.89e+06 =  6% of the original kernel matrix.

torch.Size([8653, 2])
We keep 1.34e+06/3.93e+07 =  3% of the original kernel matrix.

torch.Size([69926, 2])
We keep 6.79e+07/4.38e+09 =  1% of the original kernel matrix.

torch.Size([36914, 2])
We keep 2.03e+07/1.18e+09 =  1% of the original kernel matrix.

torch.Size([3393, 2])
We keep 2.23e+05/3.57e+06 =  6% of the original kernel matrix.

torch.Size([8851, 2])
We keep 1.29e+06/3.36e+07 =  3% of the original kernel matrix.

torch.Size([20078, 2])
We keep 1.70e+07/3.71e+08 =  4% of the original kernel matrix.

torch.Size([20178, 2])
We keep 7.01e+06/3.42e+08 =  2% of the original kernel matrix.

torch.Size([11261, 2])
We keep 3.90e+06/8.30e+07 =  4% of the original kernel matrix.

torch.Size([14808, 2])
We keep 4.12e+06/1.62e+08 =  2% of the original kernel matrix.

torch.Size([15969, 2])
We keep 7.06e+06/2.06e+08 =  3% of the original kernel matrix.

torch.Size([17920, 2])
We keep 5.91e+06/2.55e+08 =  2% of the original kernel matrix.

torch.Size([23436, 2])
We keep 1.32e+07/4.99e+08 =  2% of the original kernel matrix.

torch.Size([21870, 2])
We keep 8.13e+06/3.97e+08 =  2% of the original kernel matrix.

torch.Size([155422, 2])
We keep 5.71e+08/2.89e+10 =  1% of the original kernel matrix.

torch.Size([57458, 2])
We keep 4.54e+07/3.02e+09 =  1% of the original kernel matrix.

torch.Size([7364, 2])
We keep 1.09e+06/2.36e+07 =  4% of the original kernel matrix.

torch.Size([12054, 2])
We keep 2.54e+06/8.63e+07 =  2% of the original kernel matrix.

torch.Size([10147, 2])
We keep 6.57e+06/1.15e+08 =  5% of the original kernel matrix.

torch.Size([13834, 2])
We keep 4.79e+06/1.91e+08 =  2% of the original kernel matrix.

torch.Size([43461, 2])
We keep 4.93e+07/1.96e+09 =  2% of the original kernel matrix.

torch.Size([29639, 2])
We keep 1.47e+07/7.87e+08 =  1% of the original kernel matrix.

torch.Size([6332, 2])
We keep 1.15e+06/1.89e+07 =  6% of the original kernel matrix.

torch.Size([11245, 2])
We keep 2.33e+06/7.73e+07 =  3% of the original kernel matrix.

torch.Size([236368, 2])
We keep 5.67e+08/5.73e+10 =  0% of the original kernel matrix.

torch.Size([72617, 2])
We keep 6.18e+07/4.25e+09 =  1% of the original kernel matrix.

torch.Size([3921, 2])
We keep 3.90e+05/5.81e+06 =  6% of the original kernel matrix.

torch.Size([9303, 2])
We keep 1.56e+06/4.28e+07 =  3% of the original kernel matrix.

torch.Size([67652, 2])
We keep 9.33e+07/4.64e+09 =  2% of the original kernel matrix.

torch.Size([36403, 2])
We keep 2.06e+07/1.21e+09 =  1% of the original kernel matrix.

torch.Size([176317, 2])
We keep 3.24e+08/3.16e+10 =  1% of the original kernel matrix.

torch.Size([61561, 2])
We keep 4.79e+07/3.16e+09 =  1% of the original kernel matrix.

torch.Size([3234, 2])
We keep 2.45e+05/3.54e+06 =  6% of the original kernel matrix.

torch.Size([8552, 2])
We keep 1.30e+06/3.34e+07 =  3% of the original kernel matrix.

torch.Size([83247, 2])
We keep 2.55e+08/1.03e+10 =  2% of the original kernel matrix.

torch.Size([40373, 2])
We keep 2.90e+07/1.80e+09 =  1% of the original kernel matrix.

torch.Size([9566, 2])
We keep 1.06e+07/1.32e+08 =  7% of the original kernel matrix.

torch.Size([13579, 2])
We keep 4.53e+06/2.04e+08 =  2% of the original kernel matrix.

torch.Size([56490, 2])
We keep 7.02e+07/3.17e+09 =  2% of the original kernel matrix.

torch.Size([33528, 2])
We keep 1.75e+07/1.00e+09 =  1% of the original kernel matrix.

torch.Size([7904, 2])
We keep 1.29e+06/2.82e+07 =  4% of the original kernel matrix.

torch.Size([12387, 2])
We keep 2.73e+06/9.44e+07 =  2% of the original kernel matrix.

torch.Size([44102, 2])
We keep 4.67e+07/2.22e+09 =  2% of the original kernel matrix.

torch.Size([29619, 2])
We keep 1.54e+07/8.37e+08 =  1% of the original kernel matrix.

torch.Size([1777, 2])
We keep 7.41e+04/8.32e+05 =  8% of the original kernel matrix.

torch.Size([6929, 2])
We keep 7.40e+05/1.62e+07 =  4% of the original kernel matrix.

torch.Size([5955, 2])
We keep 9.52e+05/1.73e+07 =  5% of the original kernel matrix.

torch.Size([10800, 2])
We keep 2.29e+06/7.40e+07 =  3% of the original kernel matrix.

torch.Size([3231, 2])
We keep 2.40e+05/3.61e+06 =  6% of the original kernel matrix.

torch.Size([8565, 2])
We keep 1.27e+06/3.37e+07 =  3% of the original kernel matrix.

torch.Size([25811, 2])
We keep 2.09e+07/6.38e+08 =  3% of the original kernel matrix.

torch.Size([22861, 2])
We keep 8.80e+06/4.49e+08 =  1% of the original kernel matrix.

torch.Size([16087, 2])
We keep 7.64e+06/2.31e+08 =  3% of the original kernel matrix.

torch.Size([17815, 2])
We keep 6.21e+06/2.70e+08 =  2% of the original kernel matrix.

torch.Size([81592, 2])
We keep 2.22e+08/6.41e+09 =  3% of the original kernel matrix.

torch.Size([40584, 2])
We keep 2.32e+07/1.42e+09 =  1% of the original kernel matrix.

torch.Size([11148, 2])
We keep 3.07e+06/9.09e+07 =  3% of the original kernel matrix.

torch.Size([14573, 2])
We keep 4.21e+06/1.69e+08 =  2% of the original kernel matrix.

torch.Size([5989, 2])
We keep 8.73e+05/1.58e+07 =  5% of the original kernel matrix.

torch.Size([10905, 2])
We keep 2.21e+06/7.06e+07 =  3% of the original kernel matrix.

torch.Size([4774, 2])
We keep 7.74e+05/1.15e+07 =  6% of the original kernel matrix.

torch.Size([9888, 2])
We keep 1.98e+06/6.03e+07 =  3% of the original kernel matrix.

torch.Size([13574, 2])
We keep 1.61e+07/1.54e+08 = 10% of the original kernel matrix.

torch.Size([16515, 2])
We keep 4.87e+06/2.20e+08 =  2% of the original kernel matrix.

torch.Size([2590, 2])
We keep 1.78e+05/2.14e+06 =  8% of the original kernel matrix.

torch.Size([7905, 2])
We keep 1.07e+06/2.60e+07 =  4% of the original kernel matrix.

torch.Size([87879, 2])
We keep 1.10e+08/8.15e+09 =  1% of the original kernel matrix.

torch.Size([41681, 2])
We keep 2.64e+07/1.60e+09 =  1% of the original kernel matrix.

torch.Size([92226, 2])
We keep 2.12e+08/9.44e+09 =  2% of the original kernel matrix.

torch.Size([43092, 2])
We keep 2.87e+07/1.73e+09 =  1% of the original kernel matrix.

torch.Size([5441, 2])
We keep 1.21e+06/1.70e+07 =  7% of the original kernel matrix.

torch.Size([10506, 2])
We keep 2.11e+06/7.34e+07 =  2% of the original kernel matrix.

torch.Size([10989, 2])
We keep 5.19e+06/8.00e+07 =  6% of the original kernel matrix.

torch.Size([14530, 2])
We keep 4.17e+06/1.59e+08 =  2% of the original kernel matrix.

torch.Size([10991, 2])
We keep 7.55e+06/1.10e+08 =  6% of the original kernel matrix.

torch.Size([14490, 2])
We keep 4.39e+06/1.86e+08 =  2% of the original kernel matrix.

torch.Size([826845, 2])
We keep 6.42e+09/6.82e+11 =  0% of the original kernel matrix.

torch.Size([142180, 2])
We keep 1.92e+08/1.47e+10 =  1% of the original kernel matrix.

torch.Size([7709, 2])
We keep 1.36e+06/3.12e+07 =  4% of the original kernel matrix.

torch.Size([12102, 2])
We keep 2.75e+06/9.92e+07 =  2% of the original kernel matrix.

torch.Size([16054, 2])
We keep 6.32e+06/2.04e+08 =  3% of the original kernel matrix.

torch.Size([17909, 2])
We keep 5.50e+06/2.54e+08 =  2% of the original kernel matrix.

torch.Size([72605, 2])
We keep 3.15e+08/6.54e+09 =  4% of the original kernel matrix.

torch.Size([37285, 2])
We keep 2.38e+07/1.44e+09 =  1% of the original kernel matrix.

torch.Size([20337, 2])
We keep 3.83e+07/6.43e+08 =  5% of the original kernel matrix.

torch.Size([19684, 2])
We keep 9.43e+06/4.50e+08 =  2% of the original kernel matrix.

torch.Size([4396, 2])
We keep 6.59e+05/9.22e+06 =  7% of the original kernel matrix.

torch.Size([9485, 2])
We keep 1.84e+06/5.39e+07 =  3% of the original kernel matrix.

torch.Size([4170, 2])
We keep 5.87e+05/8.28e+06 =  7% of the original kernel matrix.

torch.Size([9300, 2])
We keep 1.74e+06/5.11e+07 =  3% of the original kernel matrix.

torch.Size([128970, 2])
We keep 6.38e+08/2.49e+10 =  2% of the original kernel matrix.

torch.Size([51902, 2])
We keep 4.43e+07/2.80e+09 =  1% of the original kernel matrix.

torch.Size([26599, 2])
We keep 1.94e+07/7.38e+08 =  2% of the original kernel matrix.

torch.Size([23415, 2])
We keep 9.64e+06/4.83e+08 =  1% of the original kernel matrix.

torch.Size([36638, 2])
We keep 7.00e+07/2.00e+09 =  3% of the original kernel matrix.

torch.Size([26461, 2])
We keep 1.46e+07/7.95e+08 =  1% of the original kernel matrix.

torch.Size([66291, 2])
We keep 9.38e+07/4.13e+09 =  2% of the original kernel matrix.

torch.Size([35989, 2])
We keep 1.98e+07/1.14e+09 =  1% of the original kernel matrix.

torch.Size([12212, 2])
We keep 3.38e+06/1.06e+08 =  3% of the original kernel matrix.

torch.Size([15568, 2])
We keep 4.40e+06/1.83e+08 =  2% of the original kernel matrix.

torch.Size([21924, 2])
We keep 1.13e+07/4.52e+08 =  2% of the original kernel matrix.

torch.Size([21254, 2])
We keep 7.81e+06/3.78e+08 =  2% of the original kernel matrix.

torch.Size([68091, 2])
We keep 1.62e+08/5.65e+09 =  2% of the original kernel matrix.

torch.Size([36766, 2])
We keep 2.17e+07/1.34e+09 =  1% of the original kernel matrix.

torch.Size([49344, 2])
We keep 4.17e+07/2.49e+09 =  1% of the original kernel matrix.

torch.Size([31728, 2])
We keep 1.61e+07/8.87e+08 =  1% of the original kernel matrix.

torch.Size([6756, 2])
We keep 1.44e+06/2.38e+07 =  6% of the original kernel matrix.

torch.Size([11716, 2])
We keep 2.52e+06/8.68e+07 =  2% of the original kernel matrix.

torch.Size([38594, 2])
We keep 2.56e+07/1.50e+09 =  1% of the original kernel matrix.

torch.Size([28322, 2])
We keep 1.29e+07/6.88e+08 =  1% of the original kernel matrix.

torch.Size([4361, 2])
We keep 4.96e+05/8.95e+06 =  5% of the original kernel matrix.

torch.Size([9523, 2])
We keep 1.69e+06/5.31e+07 =  3% of the original kernel matrix.

torch.Size([17385, 2])
We keep 1.65e+07/2.83e+08 =  5% of the original kernel matrix.

torch.Size([18783, 2])
We keep 6.73e+06/2.99e+08 =  2% of the original kernel matrix.

torch.Size([18617, 2])
We keep 7.67e+06/2.82e+08 =  2% of the original kernel matrix.

torch.Size([19307, 2])
We keep 6.57e+06/2.98e+08 =  2% of the original kernel matrix.

torch.Size([6293, 2])
We keep 1.19e+06/2.15e+07 =  5% of the original kernel matrix.

torch.Size([11158, 2])
We keep 2.38e+06/8.25e+07 =  2% of the original kernel matrix.

torch.Size([35888, 2])
We keep 8.51e+07/1.80e+09 =  4% of the original kernel matrix.

torch.Size([26670, 2])
We keep 1.41e+07/7.54e+08 =  1% of the original kernel matrix.

torch.Size([16929, 2])
We keep 7.40e+06/2.84e+08 =  2% of the original kernel matrix.

torch.Size([18451, 2])
We keep 6.65e+06/3.00e+08 =  2% of the original kernel matrix.

torch.Size([4652, 2])
We keep 9.23e+05/1.16e+07 =  7% of the original kernel matrix.

torch.Size([9591, 2])
We keep 2.01e+06/6.05e+07 =  3% of the original kernel matrix.

torch.Size([19223, 2])
We keep 9.47e+06/3.55e+08 =  2% of the original kernel matrix.

torch.Size([19767, 2])
We keep 7.17e+06/3.35e+08 =  2% of the original kernel matrix.

torch.Size([4549, 2])
We keep 4.43e+06/3.66e+07 = 12% of the original kernel matrix.

torch.Size([9555, 2])
We keep 2.53e+06/1.08e+08 =  2% of the original kernel matrix.

torch.Size([51518, 2])
We keep 1.08e+08/3.46e+09 =  3% of the original kernel matrix.

torch.Size([31650, 2])
We keep 1.85e+07/1.04e+09 =  1% of the original kernel matrix.

torch.Size([39151, 2])
We keep 1.11e+08/2.36e+09 =  4% of the original kernel matrix.

torch.Size([27356, 2])
We keep 1.48e+07/8.64e+08 =  1% of the original kernel matrix.

torch.Size([10040, 2])
We keep 3.80e+06/6.87e+07 =  5% of the original kernel matrix.

torch.Size([13880, 2])
We keep 3.76e+06/1.47e+08 =  2% of the original kernel matrix.

torch.Size([5335, 2])
We keep 2.29e+06/2.23e+07 = 10% of the original kernel matrix.

torch.Size([10157, 2])
We keep 2.51e+06/8.39e+07 =  2% of the original kernel matrix.

torch.Size([17970, 2])
We keep 1.37e+07/3.11e+08 =  4% of the original kernel matrix.

torch.Size([18894, 2])
We keep 6.82e+06/3.13e+08 =  2% of the original kernel matrix.

torch.Size([3601, 2])
We keep 3.16e+05/5.11e+06 =  6% of the original kernel matrix.

torch.Size([8829, 2])
We keep 1.46e+06/4.02e+07 =  3% of the original kernel matrix.

torch.Size([6333, 2])
We keep 1.17e+06/2.10e+07 =  5% of the original kernel matrix.

torch.Size([10983, 2])
We keep 2.44e+06/8.14e+07 =  3% of the original kernel matrix.

torch.Size([20108, 2])
We keep 8.16e+06/3.36e+08 =  2% of the original kernel matrix.

torch.Size([20390, 2])
We keep 7.00e+06/3.26e+08 =  2% of the original kernel matrix.

torch.Size([115158, 2])
We keep 2.50e+08/1.47e+10 =  1% of the original kernel matrix.

torch.Size([48669, 2])
We keep 3.34e+07/2.15e+09 =  1% of the original kernel matrix.

torch.Size([6500, 2])
We keep 1.09e+06/2.21e+07 =  4% of the original kernel matrix.

torch.Size([11249, 2])
We keep 2.49e+06/8.36e+07 =  2% of the original kernel matrix.

torch.Size([2110, 2])
We keep 1.13e+05/1.42e+06 =  7% of the original kernel matrix.

torch.Size([7345, 2])
We keep 9.35e+05/2.12e+07 =  4% of the original kernel matrix.

torch.Size([4141, 2])
We keep 3.96e+05/6.41e+06 =  6% of the original kernel matrix.

torch.Size([9340, 2])
We keep 1.57e+06/4.50e+07 =  3% of the original kernel matrix.

torch.Size([126032, 2])
We keep 3.13e+08/1.84e+10 =  1% of the original kernel matrix.

torch.Size([51005, 2])
We keep 3.79e+07/2.41e+09 =  1% of the original kernel matrix.

torch.Size([115952, 2])
We keep 2.48e+08/1.36e+10 =  1% of the original kernel matrix.

torch.Size([49024, 2])
We keep 3.16e+07/2.07e+09 =  1% of the original kernel matrix.

torch.Size([44018, 2])
We keep 6.46e+07/2.46e+09 =  2% of the original kernel matrix.

torch.Size([29347, 2])
We keep 1.59e+07/8.81e+08 =  1% of the original kernel matrix.

torch.Size([6448, 2])
We keep 1.10e+06/2.21e+07 =  4% of the original kernel matrix.

torch.Size([11155, 2])
We keep 2.47e+06/8.35e+07 =  2% of the original kernel matrix.

torch.Size([28299, 2])
We keep 2.34e+07/8.79e+08 =  2% of the original kernel matrix.

torch.Size([24217, 2])
We keep 1.03e+07/5.27e+08 =  1% of the original kernel matrix.

torch.Size([72872, 2])
We keep 7.39e+07/4.98e+09 =  1% of the original kernel matrix.

torch.Size([37577, 2])
We keep 2.12e+07/1.25e+09 =  1% of the original kernel matrix.

torch.Size([2945, 2])
We keep 2.51e+05/3.55e+06 =  7% of the original kernel matrix.

torch.Size([8161, 2])
We keep 1.32e+06/3.35e+07 =  3% of the original kernel matrix.

torch.Size([8273, 2])
We keep 2.24e+06/4.01e+07 =  5% of the original kernel matrix.

torch.Size([12738, 2])
We keep 2.98e+06/1.12e+08 =  2% of the original kernel matrix.

torch.Size([22848, 2])
We keep 9.87e+06/5.06e+08 =  1% of the original kernel matrix.

torch.Size([21899, 2])
We keep 8.23e+06/4.00e+08 =  2% of the original kernel matrix.

torch.Size([73889, 2])
We keep 3.79e+08/1.26e+10 =  3% of the original kernel matrix.

torch.Size([37319, 2])
We keep 3.18e+07/1.99e+09 =  1% of the original kernel matrix.

torch.Size([43536, 2])
We keep 8.99e+07/2.27e+09 =  3% of the original kernel matrix.

torch.Size([29524, 2])
We keep 1.57e+07/8.47e+08 =  1% of the original kernel matrix.

torch.Size([60245, 2])
We keep 5.71e+07/3.46e+09 =  1% of the original kernel matrix.

torch.Size([34660, 2])
We keep 1.84e+07/1.05e+09 =  1% of the original kernel matrix.

torch.Size([16366, 2])
We keep 5.53e+06/2.03e+08 =  2% of the original kernel matrix.

torch.Size([18083, 2])
We keep 5.70e+06/2.53e+08 =  2% of the original kernel matrix.

torch.Size([11440, 2])
We keep 1.48e+07/1.08e+08 = 13% of the original kernel matrix.

torch.Size([14938, 2])
We keep 4.31e+06/1.84e+08 =  2% of the original kernel matrix.

torch.Size([3294, 2])
We keep 2.27e+05/3.58e+06 =  6% of the original kernel matrix.

torch.Size([8653, 2])
We keep 1.30e+06/3.36e+07 =  3% of the original kernel matrix.

torch.Size([26085, 2])
We keep 1.64e+07/6.84e+08 =  2% of the original kernel matrix.

torch.Size([23185, 2])
We keep 9.47e+06/4.65e+08 =  2% of the original kernel matrix.

torch.Size([17275, 2])
We keep 6.34e+06/2.25e+08 =  2% of the original kernel matrix.

torch.Size([18640, 2])
We keep 5.93e+06/2.67e+08 =  2% of the original kernel matrix.

torch.Size([32930, 2])
We keep 4.44e+07/1.36e+09 =  3% of the original kernel matrix.

torch.Size([25723, 2])
We keep 1.27e+07/6.56e+08 =  1% of the original kernel matrix.

torch.Size([16598, 2])
We keep 7.31e+06/2.41e+08 =  3% of the original kernel matrix.

torch.Size([18145, 2])
We keep 6.08e+06/2.76e+08 =  2% of the original kernel matrix.

torch.Size([140779, 2])
We keep 2.63e+08/2.03e+10 =  1% of the original kernel matrix.

torch.Size([54464, 2])
We keep 3.94e+07/2.53e+09 =  1% of the original kernel matrix.

torch.Size([7269, 2])
We keep 1.83e+06/2.70e+07 =  6% of the original kernel matrix.

torch.Size([11936, 2])
We keep 2.67e+06/9.24e+07 =  2% of the original kernel matrix.

torch.Size([9505, 2])
We keep 5.11e+06/7.44e+07 =  6% of the original kernel matrix.

torch.Size([13389, 2])
We keep 4.02e+06/1.53e+08 =  2% of the original kernel matrix.

torch.Size([13484, 2])
We keep 7.51e+06/1.37e+08 =  5% of the original kernel matrix.

torch.Size([16384, 2])
We keep 4.92e+06/2.08e+08 =  2% of the original kernel matrix.

torch.Size([167652, 2])
We keep 2.33e+09/3.76e+10 =  6% of the original kernel matrix.

torch.Size([60315, 2])
We keep 4.91e+07/3.45e+09 =  1% of the original kernel matrix.

torch.Size([20523, 2])
We keep 9.56e+06/4.08e+08 =  2% of the original kernel matrix.

torch.Size([20461, 2])
We keep 7.54e+06/3.59e+08 =  2% of the original kernel matrix.

torch.Size([13456, 2])
We keep 6.44e+06/1.49e+08 =  4% of the original kernel matrix.

torch.Size([16005, 2])
We keep 5.18e+06/2.17e+08 =  2% of the original kernel matrix.

torch.Size([2590, 2])
We keep 1.62e+05/2.29e+06 =  7% of the original kernel matrix.

torch.Size([7885, 2])
We keep 1.09e+06/2.69e+07 =  4% of the original kernel matrix.

torch.Size([33178, 2])
We keep 2.27e+07/1.07e+09 =  2% of the original kernel matrix.

torch.Size([26203, 2])
We keep 1.12e+07/5.80e+08 =  1% of the original kernel matrix.

torch.Size([3233, 2])
We keep 3.07e+05/4.52e+06 =  6% of the original kernel matrix.

torch.Size([8588, 2])
We keep 1.41e+06/3.78e+07 =  3% of the original kernel matrix.

torch.Size([9289, 2])
We keep 3.31e+06/6.29e+07 =  5% of the original kernel matrix.

torch.Size([13418, 2])
We keep 3.67e+06/1.41e+08 =  2% of the original kernel matrix.

torch.Size([4863, 2])
We keep 5.28e+05/9.56e+06 =  5% of the original kernel matrix.

torch.Size([10006, 2])
We keep 1.83e+06/5.49e+07 =  3% of the original kernel matrix.

torch.Size([11070, 2])
We keep 2.45e+06/7.47e+07 =  3% of the original kernel matrix.

torch.Size([14699, 2])
We keep 3.84e+06/1.54e+08 =  2% of the original kernel matrix.

torch.Size([12142, 2])
We keep 5.45e+06/1.17e+08 =  4% of the original kernel matrix.

torch.Size([15410, 2])
We keep 4.65e+06/1.92e+08 =  2% of the original kernel matrix.

torch.Size([4428, 2])
We keep 5.12e+05/9.33e+06 =  5% of the original kernel matrix.

torch.Size([9518, 2])
We keep 1.81e+06/5.43e+07 =  3% of the original kernel matrix.

torch.Size([3440, 2])
We keep 2.82e+05/4.44e+06 =  6% of the original kernel matrix.

torch.Size([8846, 2])
We keep 1.42e+06/3.75e+07 =  3% of the original kernel matrix.

torch.Size([120421, 2])
We keep 1.98e+08/1.48e+10 =  1% of the original kernel matrix.

torch.Size([50202, 2])
We keep 3.46e+07/2.16e+09 =  1% of the original kernel matrix.

torch.Size([183130, 2])
We keep 4.54e+08/3.51e+10 =  1% of the original kernel matrix.

torch.Size([63240, 2])
We keep 5.07e+07/3.33e+09 =  1% of the original kernel matrix.

torch.Size([74761, 2])
We keep 1.02e+08/5.82e+09 =  1% of the original kernel matrix.

torch.Size([38468, 2])
We keep 2.31e+07/1.36e+09 =  1% of the original kernel matrix.

torch.Size([9512, 2])
We keep 4.19e+06/5.33e+07 =  7% of the original kernel matrix.

torch.Size([13573, 2])
We keep 3.34e+06/1.30e+08 =  2% of the original kernel matrix.

torch.Size([16680, 2])
We keep 6.62e+06/2.49e+08 =  2% of the original kernel matrix.

torch.Size([18316, 2])
We keep 6.24e+06/2.80e+08 =  2% of the original kernel matrix.

torch.Size([11053, 2])
We keep 5.19e+06/8.12e+07 =  6% of the original kernel matrix.

torch.Size([14745, 2])
We keep 4.13e+06/1.60e+08 =  2% of the original kernel matrix.

torch.Size([9045, 2])
We keep 2.10e+06/4.63e+07 =  4% of the original kernel matrix.

torch.Size([13333, 2])
We keep 3.33e+06/1.21e+08 =  2% of the original kernel matrix.

torch.Size([73965, 2])
We keep 6.80e+07/4.73e+09 =  1% of the original kernel matrix.

torch.Size([37894, 2])
We keep 2.08e+07/1.22e+09 =  1% of the original kernel matrix.

torch.Size([28798, 2])
We keep 7.41e+07/9.85e+08 =  7% of the original kernel matrix.

torch.Size([24349, 2])
We keep 1.06e+07/5.58e+08 =  1% of the original kernel matrix.

torch.Size([34420, 2])
We keep 6.75e+07/1.41e+09 =  4% of the original kernel matrix.

torch.Size([26316, 2])
We keep 1.23e+07/6.68e+08 =  1% of the original kernel matrix.

torch.Size([7953, 2])
We keep 1.76e+06/3.77e+07 =  4% of the original kernel matrix.

torch.Size([12224, 2])
We keep 3.04e+06/1.09e+08 =  2% of the original kernel matrix.

torch.Size([114402, 2])
We keep 5.32e+08/1.73e+10 =  3% of the original kernel matrix.

torch.Size([48781, 2])
We keep 3.51e+07/2.33e+09 =  1% of the original kernel matrix.

torch.Size([17422, 2])
We keep 1.12e+07/2.83e+08 =  3% of the original kernel matrix.

torch.Size([18698, 2])
We keep 6.63e+06/2.99e+08 =  2% of the original kernel matrix.

torch.Size([26884, 2])
We keep 2.04e+07/8.30e+08 =  2% of the original kernel matrix.

torch.Size([23805, 2])
We keep 1.02e+07/5.12e+08 =  2% of the original kernel matrix.

torch.Size([80292, 2])
We keep 1.50e+08/6.30e+09 =  2% of the original kernel matrix.

torch.Size([40219, 2])
We keep 2.39e+07/1.41e+09 =  1% of the original kernel matrix.

torch.Size([31063, 2])
We keep 1.94e+07/1.04e+09 =  1% of the original kernel matrix.

torch.Size([25638, 2])
We keep 1.12e+07/5.73e+08 =  1% of the original kernel matrix.

torch.Size([74468, 2])
We keep 3.78e+08/7.18e+09 =  5% of the original kernel matrix.

torch.Size([38690, 2])
We keep 2.45e+07/1.51e+09 =  1% of the original kernel matrix.

torch.Size([103677, 2])
We keep 3.37e+08/1.22e+10 =  2% of the original kernel matrix.

torch.Size([45854, 2])
We keep 3.13e+07/1.96e+09 =  1% of the original kernel matrix.

torch.Size([51471, 2])
We keep 4.40e+07/2.79e+09 =  1% of the original kernel matrix.

torch.Size([32284, 2])
We keep 1.67e+07/9.38e+08 =  1% of the original kernel matrix.

torch.Size([16539, 2])
We keep 5.58e+06/2.00e+08 =  2% of the original kernel matrix.

torch.Size([18169, 2])
We keep 5.58e+06/2.52e+08 =  2% of the original kernel matrix.

torch.Size([21209, 2])
We keep 1.29e+07/4.43e+08 =  2% of the original kernel matrix.

torch.Size([20633, 2])
We keep 7.97e+06/3.74e+08 =  2% of the original kernel matrix.

torch.Size([35587, 2])
We keep 3.24e+07/1.40e+09 =  2% of the original kernel matrix.

torch.Size([27126, 2])
We keep 1.25e+07/6.65e+08 =  1% of the original kernel matrix.

torch.Size([6843, 2])
We keep 1.16e+06/2.61e+07 =  4% of the original kernel matrix.

torch.Size([11570, 2])
We keep 2.56e+06/9.07e+07 =  2% of the original kernel matrix.

torch.Size([69335, 2])
We keep 7.90e+07/4.39e+09 =  1% of the original kernel matrix.

torch.Size([36891, 2])
We keep 2.02e+07/1.18e+09 =  1% of the original kernel matrix.

torch.Size([387862, 2])
We keep 1.57e+09/1.50e+11 =  1% of the original kernel matrix.

torch.Size([92308, 2])
We keep 9.61e+07/6.88e+09 =  1% of the original kernel matrix.

torch.Size([49086, 2])
We keep 9.55e+07/2.68e+09 =  3% of the original kernel matrix.

torch.Size([31514, 2])
We keep 1.63e+07/9.20e+08 =  1% of the original kernel matrix.

torch.Size([30405, 2])
We keep 7.44e+07/1.28e+09 =  5% of the original kernel matrix.

torch.Size([24581, 2])
We keep 1.19e+07/6.35e+08 =  1% of the original kernel matrix.

torch.Size([30986, 2])
We keep 3.14e+07/1.12e+09 =  2% of the original kernel matrix.

torch.Size([25236, 2])
We keep 1.18e+07/5.95e+08 =  1% of the original kernel matrix.

torch.Size([5586, 2])
We keep 1.40e+06/2.21e+07 =  6% of the original kernel matrix.

torch.Size([10624, 2])
We keep 2.36e+06/8.36e+07 =  2% of the original kernel matrix.

torch.Size([14957, 2])
We keep 5.12e+06/1.62e+08 =  3% of the original kernel matrix.

torch.Size([17429, 2])
We keep 5.31e+06/2.26e+08 =  2% of the original kernel matrix.

torch.Size([15037, 2])
We keep 7.55e+06/2.08e+08 =  3% of the original kernel matrix.

torch.Size([17193, 2])
We keep 5.68e+06/2.56e+08 =  2% of the original kernel matrix.

torch.Size([37724, 2])
We keep 4.58e+07/1.54e+09 =  2% of the original kernel matrix.

torch.Size([27985, 2])
We keep 1.33e+07/6.98e+08 =  1% of the original kernel matrix.

torch.Size([174811, 2])
We keep 3.62e+08/3.56e+10 =  1% of the original kernel matrix.

torch.Size([61571, 2])
We keep 5.05e+07/3.35e+09 =  1% of the original kernel matrix.

torch.Size([3562, 2])
We keep 6.89e+05/5.76e+06 = 11% of the original kernel matrix.

torch.Size([8805, 2])
We keep 1.58e+06/4.26e+07 =  3% of the original kernel matrix.

torch.Size([25559, 2])
We keep 6.42e+07/8.79e+08 =  7% of the original kernel matrix.

torch.Size([22521, 2])
We keep 9.84e+06/5.27e+08 =  1% of the original kernel matrix.

torch.Size([19302, 2])
We keep 1.30e+07/3.08e+08 =  4% of the original kernel matrix.

torch.Size([19799, 2])
We keep 6.79e+06/3.12e+08 =  2% of the original kernel matrix.

torch.Size([24386, 2])
We keep 1.25e+07/5.55e+08 =  2% of the original kernel matrix.

torch.Size([22312, 2])
We keep 8.72e+06/4.19e+08 =  2% of the original kernel matrix.

torch.Size([3005, 2])
We keep 1.90e+05/2.79e+06 =  6% of the original kernel matrix.

torch.Size([8320, 2])
We keep 1.17e+06/2.97e+07 =  3% of the original kernel matrix.

torch.Size([71326, 2])
We keep 1.10e+08/5.48e+09 =  2% of the original kernel matrix.

torch.Size([37950, 2])
We keep 2.25e+07/1.32e+09 =  1% of the original kernel matrix.

torch.Size([92589, 2])
We keep 1.26e+08/8.90e+09 =  1% of the original kernel matrix.

torch.Size([42826, 2])
We keep 2.77e+07/1.68e+09 =  1% of the original kernel matrix.

torch.Size([3976, 2])
We keep 3.68e+05/5.82e+06 =  6% of the original kernel matrix.

torch.Size([9318, 2])
We keep 1.54e+06/4.29e+07 =  3% of the original kernel matrix.

torch.Size([462347, 2])
We keep 1.83e+09/2.11e+11 =  0% of the original kernel matrix.

torch.Size([100896, 2])
We keep 1.13e+08/8.17e+09 =  1% of the original kernel matrix.

torch.Size([22016, 2])
We keep 1.41e+07/5.02e+08 =  2% of the original kernel matrix.

torch.Size([21127, 2])
We keep 8.08e+06/3.98e+08 =  2% of the original kernel matrix.

torch.Size([20411, 2])
We keep 1.39e+07/4.26e+08 =  3% of the original kernel matrix.

torch.Size([20452, 2])
We keep 7.79e+06/3.67e+08 =  2% of the original kernel matrix.

torch.Size([12773, 2])
We keep 7.64e+06/1.35e+08 =  5% of the original kernel matrix.

torch.Size([15908, 2])
We keep 5.02e+06/2.07e+08 =  2% of the original kernel matrix.

torch.Size([12224, 2])
We keep 3.62e+06/9.99e+07 =  3% of the original kernel matrix.

torch.Size([15379, 2])
We keep 4.45e+06/1.78e+08 =  2% of the original kernel matrix.

torch.Size([4188, 2])
We keep 6.54e+05/7.23e+06 =  9% of the original kernel matrix.

torch.Size([9468, 2])
We keep 1.62e+06/4.78e+07 =  3% of the original kernel matrix.

torch.Size([5721, 2])
We keep 1.44e+06/1.60e+07 =  8% of the original kernel matrix.

torch.Size([10781, 2])
We keep 2.22e+06/7.10e+07 =  3% of the original kernel matrix.

torch.Size([7413, 2])
We keep 1.30e+06/3.18e+07 =  4% of the original kernel matrix.

torch.Size([11916, 2])
We keep 2.77e+06/1.00e+08 =  2% of the original kernel matrix.

torch.Size([109412, 2])
We keep 2.67e+08/1.43e+10 =  1% of the original kernel matrix.

torch.Size([47003, 2])
We keep 3.41e+07/2.12e+09 =  1% of the original kernel matrix.

torch.Size([72980, 2])
We keep 2.06e+09/2.66e+10 =  7% of the original kernel matrix.

torch.Size([35609, 2])
We keep 4.10e+07/2.90e+09 =  1% of the original kernel matrix.

torch.Size([10020, 2])
We keep 1.05e+07/8.92e+07 = 11% of the original kernel matrix.

torch.Size([14014, 2])
We keep 4.19e+06/1.68e+08 =  2% of the original kernel matrix.

torch.Size([75257, 2])
We keep 8.54e+07/5.28e+09 =  1% of the original kernel matrix.

torch.Size([38068, 2])
We keep 2.21e+07/1.29e+09 =  1% of the original kernel matrix.

torch.Size([3765, 2])
We keep 3.51e+05/5.87e+06 =  5% of the original kernel matrix.

torch.Size([9090, 2])
We keep 1.53e+06/4.30e+07 =  3% of the original kernel matrix.

torch.Size([14674, 2])
We keep 4.66e+06/1.53e+08 =  3% of the original kernel matrix.

torch.Size([17022, 2])
We keep 5.20e+06/2.20e+08 =  2% of the original kernel matrix.

torch.Size([4518, 2])
We keep 6.49e+05/1.10e+07 =  5% of the original kernel matrix.

torch.Size([9556, 2])
We keep 1.92e+06/5.89e+07 =  3% of the original kernel matrix.

torch.Size([18481, 2])
We keep 1.25e+08/6.64e+08 = 18% of the original kernel matrix.

torch.Size([18993, 2])
We keep 8.14e+06/4.58e+08 =  1% of the original kernel matrix.

torch.Size([9122, 2])
We keep 2.75e+06/5.50e+07 =  5% of the original kernel matrix.

torch.Size([13255, 2])
We keep 3.49e+06/1.32e+08 =  2% of the original kernel matrix.

torch.Size([667442, 2])
We keep 3.13e+09/4.19e+11 =  0% of the original kernel matrix.

torch.Size([126639, 2])
We keep 1.53e+08/1.15e+10 =  1% of the original kernel matrix.

torch.Size([5896, 2])
We keep 7.64e+05/1.55e+07 =  4% of the original kernel matrix.

torch.Size([10872, 2])
We keep 2.17e+06/7.00e+07 =  3% of the original kernel matrix.

torch.Size([54878, 2])
We keep 5.49e+07/2.83e+09 =  1% of the original kernel matrix.

torch.Size([33387, 2])
We keep 1.65e+07/9.45e+08 =  1% of the original kernel matrix.

torch.Size([10762, 2])
We keep 3.78e+06/7.26e+07 =  5% of the original kernel matrix.

torch.Size([14485, 2])
We keep 3.62e+06/1.51e+08 =  2% of the original kernel matrix.

torch.Size([90820, 2])
We keep 1.52e+08/8.15e+09 =  1% of the original kernel matrix.

torch.Size([42732, 2])
We keep 2.68e+07/1.60e+09 =  1% of the original kernel matrix.

torch.Size([113992, 2])
We keep 2.58e+08/1.49e+10 =  1% of the original kernel matrix.

torch.Size([48538, 2])
We keep 3.41e+07/2.17e+09 =  1% of the original kernel matrix.

torch.Size([17780, 2])
We keep 6.96e+06/2.58e+08 =  2% of the original kernel matrix.

torch.Size([18445, 2])
We keep 6.24e+06/2.86e+08 =  2% of the original kernel matrix.

torch.Size([6967, 2])
We keep 1.21e+06/2.29e+07 =  5% of the original kernel matrix.

torch.Size([11699, 2])
We keep 2.55e+06/8.50e+07 =  2% of the original kernel matrix.

torch.Size([52154, 2])
We keep 4.59e+07/2.84e+09 =  1% of the original kernel matrix.

torch.Size([32453, 2])
We keep 1.70e+07/9.47e+08 =  1% of the original kernel matrix.

torch.Size([9288, 2])
We keep 2.32e+06/4.90e+07 =  4% of the original kernel matrix.

torch.Size([13368, 2])
We keep 3.32e+06/1.24e+08 =  2% of the original kernel matrix.

torch.Size([11330, 2])
We keep 9.01e+06/1.21e+08 =  7% of the original kernel matrix.

torch.Size([14662, 2])
We keep 4.76e+06/1.95e+08 =  2% of the original kernel matrix.

torch.Size([90751, 2])
We keep 1.31e+08/8.39e+09 =  1% of the original kernel matrix.

torch.Size([42476, 2])
We keep 2.71e+07/1.63e+09 =  1% of the original kernel matrix.

torch.Size([20303, 2])
We keep 4.05e+07/6.19e+08 =  6% of the original kernel matrix.

torch.Size([20117, 2])
We keep 8.11e+06/4.42e+08 =  1% of the original kernel matrix.

torch.Size([26957, 2])
We keep 1.80e+07/7.01e+08 =  2% of the original kernel matrix.

torch.Size([24348, 2])
We keep 9.55e+06/4.70e+08 =  2% of the original kernel matrix.

torch.Size([19112, 2])
We keep 9.99e+06/3.37e+08 =  2% of the original kernel matrix.

torch.Size([19500, 2])
We keep 7.13e+06/3.26e+08 =  2% of the original kernel matrix.

torch.Size([41532, 2])
We keep 3.81e+07/1.86e+09 =  2% of the original kernel matrix.

torch.Size([29148, 2])
We keep 1.38e+07/7.67e+08 =  1% of the original kernel matrix.

torch.Size([128320, 2])
We keep 7.30e+08/2.09e+10 =  3% of the original kernel matrix.

torch.Size([51498, 2])
We keep 3.87e+07/2.57e+09 =  1% of the original kernel matrix.

torch.Size([23164, 2])
We keep 1.65e+07/5.64e+08 =  2% of the original kernel matrix.

torch.Size([21828, 2])
We keep 8.38e+06/4.22e+08 =  1% of the original kernel matrix.

torch.Size([9023, 2])
We keep 1.77e+06/4.21e+07 =  4% of the original kernel matrix.

torch.Size([13138, 2])
We keep 3.16e+06/1.15e+08 =  2% of the original kernel matrix.

torch.Size([92404, 2])
We keep 2.58e+08/1.13e+10 =  2% of the original kernel matrix.

torch.Size([42906, 2])
We keep 3.08e+07/1.89e+09 =  1% of the original kernel matrix.

torch.Size([24436, 2])
We keep 1.36e+07/5.88e+08 =  2% of the original kernel matrix.

torch.Size([22341, 2])
We keep 8.72e+06/4.31e+08 =  2% of the original kernel matrix.

torch.Size([36869, 2])
We keep 5.04e+07/1.46e+09 =  3% of the original kernel matrix.

torch.Size([27488, 2])
We keep 1.27e+07/6.78e+08 =  1% of the original kernel matrix.

torch.Size([24465, 2])
We keep 1.95e+07/6.40e+08 =  3% of the original kernel matrix.

torch.Size([22281, 2])
We keep 9.31e+06/4.49e+08 =  2% of the original kernel matrix.

torch.Size([37183, 2])
We keep 4.53e+07/1.66e+09 =  2% of the original kernel matrix.

torch.Size([27098, 2])
We keep 1.38e+07/7.24e+08 =  1% of the original kernel matrix.

torch.Size([20275, 2])
We keep 1.34e+07/4.10e+08 =  3% of the original kernel matrix.

torch.Size([20310, 2])
We keep 7.33e+06/3.60e+08 =  2% of the original kernel matrix.

torch.Size([37594, 2])
We keep 1.26e+08/2.24e+09 =  5% of the original kernel matrix.

torch.Size([26635, 2])
We keep 1.52e+07/8.42e+08 =  1% of the original kernel matrix.

torch.Size([37103, 2])
We keep 5.63e+07/1.60e+09 =  3% of the original kernel matrix.

torch.Size([27271, 2])
We keep 1.39e+07/7.10e+08 =  1% of the original kernel matrix.

torch.Size([18857, 2])
We keep 8.53e+06/3.00e+08 =  2% of the original kernel matrix.

torch.Size([19659, 2])
We keep 6.74e+06/3.08e+08 =  2% of the original kernel matrix.

torch.Size([4303, 2])
We keep 5.21e+05/9.12e+06 =  5% of the original kernel matrix.

torch.Size([9369, 2])
We keep 1.80e+06/5.37e+07 =  3% of the original kernel matrix.

torch.Size([17163, 2])
We keep 1.04e+07/2.61e+08 =  3% of the original kernel matrix.

torch.Size([18583, 2])
We keep 6.55e+06/2.87e+08 =  2% of the original kernel matrix.

torch.Size([18498, 2])
We keep 1.70e+07/3.58e+08 =  4% of the original kernel matrix.

torch.Size([19255, 2])
We keep 7.02e+06/3.36e+08 =  2% of the original kernel matrix.

torch.Size([45182, 2])
We keep 5.53e+07/2.27e+09 =  2% of the original kernel matrix.

torch.Size([30059, 2])
We keep 1.52e+07/8.46e+08 =  1% of the original kernel matrix.

torch.Size([62670, 2])
We keep 1.10e+08/3.78e+09 =  2% of the original kernel matrix.

torch.Size([35374, 2])
We keep 1.85e+07/1.09e+09 =  1% of the original kernel matrix.

torch.Size([6619, 2])
We keep 9.54e+05/1.87e+07 =  5% of the original kernel matrix.

torch.Size([11381, 2])
We keep 2.32e+06/7.67e+07 =  3% of the original kernel matrix.

torch.Size([22405, 2])
We keep 3.26e+07/4.88e+08 =  6% of the original kernel matrix.

torch.Size([21362, 2])
We keep 8.04e+06/3.93e+08 =  2% of the original kernel matrix.

torch.Size([44626, 2])
We keep 4.95e+07/2.10e+09 =  2% of the original kernel matrix.

torch.Size([29977, 2])
We keep 1.51e+07/8.15e+08 =  1% of the original kernel matrix.

torch.Size([161217, 2])
We keep 1.07e+09/2.82e+10 =  3% of the original kernel matrix.

torch.Size([58693, 2])
We keep 4.48e+07/2.98e+09 =  1% of the original kernel matrix.

torch.Size([32347, 2])
We keep 2.48e+07/1.18e+09 =  2% of the original kernel matrix.

torch.Size([25934, 2])
We keep 1.18e+07/6.09e+08 =  1% of the original kernel matrix.

torch.Size([33043, 2])
We keep 2.54e+07/1.20e+09 =  2% of the original kernel matrix.

torch.Size([26515, 2])
We keep 1.16e+07/6.16e+08 =  1% of the original kernel matrix.

torch.Size([37138, 2])
We keep 1.09e+08/2.45e+09 =  4% of the original kernel matrix.

torch.Size([26618, 2])
We keep 1.58e+07/8.80e+08 =  1% of the original kernel matrix.

torch.Size([22184, 2])
We keep 1.12e+07/4.38e+08 =  2% of the original kernel matrix.

torch.Size([21426, 2])
We keep 7.51e+06/3.72e+08 =  2% of the original kernel matrix.

torch.Size([2236, 2])
We keep 1.41e+05/1.91e+06 =  7% of the original kernel matrix.

torch.Size([7419, 2])
We keep 1.01e+06/2.46e+07 =  4% of the original kernel matrix.

torch.Size([7170, 2])
We keep 1.13e+06/2.38e+07 =  4% of the original kernel matrix.

torch.Size([11921, 2])
We keep 2.56e+06/8.67e+07 =  2% of the original kernel matrix.

torch.Size([115360, 2])
We keep 1.78e+08/1.42e+10 =  1% of the original kernel matrix.

torch.Size([49216, 2])
We keep 3.34e+07/2.12e+09 =  1% of the original kernel matrix.

torch.Size([28263, 2])
We keep 2.30e+07/9.26e+08 =  2% of the original kernel matrix.

torch.Size([23825, 2])
We keep 1.08e+07/5.41e+08 =  2% of the original kernel matrix.

torch.Size([49808, 2])
We keep 3.94e+07/2.36e+09 =  1% of the original kernel matrix.

torch.Size([31868, 2])
We keep 1.54e+07/8.63e+08 =  1% of the original kernel matrix.

torch.Size([2299, 2])
We keep 1.32e+05/1.77e+06 =  7% of the original kernel matrix.

torch.Size([7575, 2])
We keep 1.01e+06/2.36e+07 =  4% of the original kernel matrix.

torch.Size([5281, 2])
We keep 9.57e+05/1.42e+07 =  6% of the original kernel matrix.

torch.Size([10320, 2])
We keep 2.08e+06/6.70e+07 =  3% of the original kernel matrix.

torch.Size([155915, 2])
We keep 4.50e+08/2.73e+10 =  1% of the original kernel matrix.

torch.Size([57413, 2])
We keep 4.58e+07/2.94e+09 =  1% of the original kernel matrix.

torch.Size([65081, 2])
We keep 1.41e+08/4.47e+09 =  3% of the original kernel matrix.

torch.Size([35726, 2])
We keep 2.07e+07/1.19e+09 =  1% of the original kernel matrix.

torch.Size([22148, 2])
We keep 9.98e+06/4.37e+08 =  2% of the original kernel matrix.

torch.Size([21465, 2])
We keep 7.81e+06/3.71e+08 =  2% of the original kernel matrix.

torch.Size([4095, 2])
We keep 3.59e+05/6.50e+06 =  5% of the original kernel matrix.

torch.Size([9355, 2])
We keep 1.57e+06/4.53e+07 =  3% of the original kernel matrix.

torch.Size([3321, 2])
We keep 2.44e+05/3.82e+06 =  6% of the original kernel matrix.

torch.Size([8661, 2])
We keep 1.33e+06/3.47e+07 =  3% of the original kernel matrix.

torch.Size([26076, 2])
We keep 1.84e+07/7.43e+08 =  2% of the original kernel matrix.

torch.Size([23291, 2])
We keep 9.74e+06/4.84e+08 =  2% of the original kernel matrix.

torch.Size([34058, 2])
We keep 2.95e+07/1.20e+09 =  2% of the original kernel matrix.

torch.Size([26425, 2])
We keep 1.19e+07/6.16e+08 =  1% of the original kernel matrix.

torch.Size([56820, 2])
We keep 1.09e+08/3.40e+09 =  3% of the original kernel matrix.

torch.Size([33639, 2])
We keep 1.85e+07/1.04e+09 =  1% of the original kernel matrix.

torch.Size([3839, 2])
We keep 4.28e+05/7.49e+06 =  5% of the original kernel matrix.

torch.Size([9149, 2])
We keep 1.60e+06/4.86e+07 =  3% of the original kernel matrix.

torch.Size([32852, 2])
We keep 1.26e+08/1.89e+09 =  6% of the original kernel matrix.

torch.Size([25109, 2])
We keep 1.50e+07/7.72e+08 =  1% of the original kernel matrix.

torch.Size([9976, 2])
We keep 3.14e+06/6.92e+07 =  4% of the original kernel matrix.

torch.Size([13830, 2])
We keep 3.85e+06/1.48e+08 =  2% of the original kernel matrix.

torch.Size([14707, 2])
We keep 1.22e+07/2.06e+08 =  5% of the original kernel matrix.

torch.Size([16870, 2])
We keep 5.86e+06/2.55e+08 =  2% of the original kernel matrix.

torch.Size([172993, 2])
We keep 4.90e+08/3.49e+10 =  1% of the original kernel matrix.

torch.Size([61195, 2])
We keep 4.76e+07/3.32e+09 =  1% of the original kernel matrix.

torch.Size([7709, 2])
We keep 3.02e+06/3.61e+07 =  8% of the original kernel matrix.

torch.Size([12364, 2])
We keep 2.96e+06/1.07e+08 =  2% of the original kernel matrix.

torch.Size([37389, 2])
We keep 4.47e+07/1.43e+09 =  3% of the original kernel matrix.

torch.Size([27444, 2])
We keep 1.22e+07/6.71e+08 =  1% of the original kernel matrix.

torch.Size([67965, 2])
We keep 7.24e+07/4.30e+09 =  1% of the original kernel matrix.

torch.Size([36630, 2])
We keep 1.98e+07/1.16e+09 =  1% of the original kernel matrix.

torch.Size([40559, 2])
We keep 3.49e+07/1.61e+09 =  2% of the original kernel matrix.

torch.Size([28894, 2])
We keep 1.34e+07/7.13e+08 =  1% of the original kernel matrix.

torch.Size([27388, 2])
We keep 1.74e+07/8.01e+08 =  2% of the original kernel matrix.

torch.Size([24212, 2])
We keep 9.97e+06/5.03e+08 =  1% of the original kernel matrix.

torch.Size([60965, 2])
We keep 5.13e+07/3.38e+09 =  1% of the original kernel matrix.

torch.Size([34833, 2])
We keep 1.79e+07/1.03e+09 =  1% of the original kernel matrix.

torch.Size([4046, 2])
We keep 3.58e+05/6.05e+06 =  5% of the original kernel matrix.

torch.Size([9437, 2])
We keep 1.54e+06/4.37e+07 =  3% of the original kernel matrix.

torch.Size([22953, 2])
We keep 1.18e+07/5.13e+08 =  2% of the original kernel matrix.

torch.Size([21883, 2])
We keep 8.35e+06/4.03e+08 =  2% of the original kernel matrix.

torch.Size([25444, 2])
We keep 1.46e+07/6.38e+08 =  2% of the original kernel matrix.

torch.Size([23186, 2])
We keep 9.25e+06/4.49e+08 =  2% of the original kernel matrix.

torch.Size([84736, 2])
We keep 1.35e+08/6.67e+09 =  2% of the original kernel matrix.

torch.Size([40728, 2])
We keep 2.42e+07/1.45e+09 =  1% of the original kernel matrix.

torch.Size([69901, 2])
We keep 8.65e+07/4.72e+09 =  1% of the original kernel matrix.

torch.Size([37416, 2])
We keep 2.06e+07/1.22e+09 =  1% of the original kernel matrix.

torch.Size([30986, 2])
We keep 3.30e+07/1.13e+09 =  2% of the original kernel matrix.

torch.Size([24785, 2])
We keep 1.16e+07/5.97e+08 =  1% of the original kernel matrix.

torch.Size([35179, 2])
We keep 2.20e+07/1.23e+09 =  1% of the original kernel matrix.

torch.Size([27163, 2])
We keep 1.18e+07/6.24e+08 =  1% of the original kernel matrix.

torch.Size([11874, 2])
We keep 3.21e+06/9.52e+07 =  3% of the original kernel matrix.

torch.Size([15227, 2])
We keep 4.25e+06/1.73e+08 =  2% of the original kernel matrix.

torch.Size([3274, 2])
We keep 7.84e+05/5.32e+06 = 14% of the original kernel matrix.

torch.Size([8527, 2])
We keep 1.36e+06/4.10e+07 =  3% of the original kernel matrix.

torch.Size([10967, 2])
We keep 5.89e+06/9.01e+07 =  6% of the original kernel matrix.

torch.Size([14615, 2])
We keep 4.13e+06/1.69e+08 =  2% of the original kernel matrix.

torch.Size([10652, 2])
We keep 2.94e+06/7.25e+07 =  4% of the original kernel matrix.

torch.Size([14349, 2])
We keep 3.94e+06/1.51e+08 =  2% of the original kernel matrix.

torch.Size([127385, 2])
We keep 2.43e+08/1.66e+10 =  1% of the original kernel matrix.

torch.Size([51427, 2])
We keep 3.60e+07/2.29e+09 =  1% of the original kernel matrix.

torch.Size([51042, 2])
We keep 8.74e+07/2.78e+09 =  3% of the original kernel matrix.

torch.Size([32255, 2])
We keep 1.62e+07/9.37e+08 =  1% of the original kernel matrix.

torch.Size([4231, 2])
We keep 7.38e+05/9.60e+06 =  7% of the original kernel matrix.

torch.Size([9335, 2])
We keep 1.87e+06/5.50e+07 =  3% of the original kernel matrix.

torch.Size([16340, 2])
We keep 6.52e+06/1.99e+08 =  3% of the original kernel matrix.

torch.Size([18084, 2])
We keep 5.60e+06/2.51e+08 =  2% of the original kernel matrix.

torch.Size([9770, 2])
We keep 1.85e+06/5.19e+07 =  3% of the original kernel matrix.

torch.Size([13750, 2])
We keep 3.32e+06/1.28e+08 =  2% of the original kernel matrix.

torch.Size([403639, 2])
We keep 1.84e+09/1.62e+11 =  1% of the original kernel matrix.

torch.Size([94484, 2])
We keep 1.01e+08/7.15e+09 =  1% of the original kernel matrix.

torch.Size([13919, 2])
We keep 4.15e+06/1.30e+08 =  3% of the original kernel matrix.

torch.Size([16535, 2])
We keep 4.78e+06/2.03e+08 =  2% of the original kernel matrix.

torch.Size([16380, 2])
We keep 6.97e+06/2.41e+08 =  2% of the original kernel matrix.

torch.Size([18208, 2])
We keep 6.28e+06/2.76e+08 =  2% of the original kernel matrix.

torch.Size([71288, 2])
We keep 1.03e+08/4.93e+09 =  2% of the original kernel matrix.

torch.Size([37417, 2])
We keep 2.15e+07/1.25e+09 =  1% of the original kernel matrix.

torch.Size([39851, 2])
We keep 2.80e+07/1.72e+09 =  1% of the original kernel matrix.

torch.Size([28772, 2])
We keep 1.37e+07/7.38e+08 =  1% of the original kernel matrix.

torch.Size([13921, 2])
We keep 5.60e+06/1.56e+08 =  3% of the original kernel matrix.

torch.Size([16557, 2])
We keep 5.19e+06/2.22e+08 =  2% of the original kernel matrix.

torch.Size([19277, 2])
We keep 9.55e+06/3.71e+08 =  2% of the original kernel matrix.

torch.Size([19957, 2])
We keep 7.43e+06/3.42e+08 =  2% of the original kernel matrix.

torch.Size([8753, 2])
We keep 2.85e+06/4.04e+07 =  7% of the original kernel matrix.

torch.Size([13055, 2])
We keep 3.03e+06/1.13e+08 =  2% of the original kernel matrix.

time for making ranges is 2.204561948776245
Sorting X and nu_X
time for sorting X is 0.07488751411437988
Sorting Z and nu_Z
time for sorting Z is 0.00027060508728027344
Starting Optim
sum tnu_Z before tensor(29514944., device='cuda:0')
c= tensor(753.2037, device='cuda:0')
c= tensor(85643.1562, device='cuda:0')
c= tensor(89745.8125, device='cuda:0')
c= tensor(91436.1797, device='cuda:0')
c= tensor(2095463.8750, device='cuda:0')
c= tensor(2386062.5000, device='cuda:0')
c= tensor(3036223.5000, device='cuda:0')
c= tensor(3337504., device='cuda:0')
c= tensor(3354898.2500, device='cuda:0')
c= tensor(11764611., device='cuda:0')
c= tensor(11781018., device='cuda:0')
c= tensor(13947819., device='cuda:0')
c= tensor(13957505., device='cuda:0')
c= tensor(19681200., device='cuda:0')
c= tensor(19998520., device='cuda:0')
c= tensor(20330606., device='cuda:0')
c= tensor(21265592., device='cuda:0')
c= tensor(22136060., device='cuda:0')
c= tensor(25507368., device='cuda:0')
c= tensor(28239880., device='cuda:0')
c= tensor(28263014., device='cuda:0')
c= tensor(53609176., device='cuda:0')
c= tensor(53679304., device='cuda:0')
c= tensor(53709216., device='cuda:0')
c= tensor(60089956., device='cuda:0')
c= tensor(60910960., device='cuda:0')
c= tensor(61281200., device='cuda:0')
c= tensor(62330676., device='cuda:0')
c= tensor(62804188., device='cuda:0')
c= tensor(4.1051e+08, device='cuda:0')
c= tensor(4.1052e+08, device='cuda:0')
c= tensor(4.2785e+08, device='cuda:0')
c= tensor(4.2791e+08, device='cuda:0')
c= tensor(4.2792e+08, device='cuda:0')
c= tensor(4.2793e+08, device='cuda:0')
c= tensor(4.2827e+08, device='cuda:0')
c= tensor(4.2893e+08, device='cuda:0')
c= tensor(4.2893e+08, device='cuda:0')
c= tensor(4.2894e+08, device='cuda:0')
c= tensor(4.2894e+08, device='cuda:0')
c= tensor(4.2895e+08, device='cuda:0')
c= tensor(4.2895e+08, device='cuda:0')
c= tensor(4.2895e+08, device='cuda:0')
c= tensor(4.2895e+08, device='cuda:0')
c= tensor(4.2895e+08, device='cuda:0')
c= tensor(4.2896e+08, device='cuda:0')
c= tensor(4.2896e+08, device='cuda:0')
c= tensor(4.2897e+08, device='cuda:0')
c= tensor(4.2898e+08, device='cuda:0')
c= tensor(4.2899e+08, device='cuda:0')
c= tensor(4.2902e+08, device='cuda:0')
c= tensor(4.2902e+08, device='cuda:0')
c= tensor(4.2902e+08, device='cuda:0')
c= tensor(4.2902e+08, device='cuda:0')
c= tensor(4.2904e+08, device='cuda:0')
c= tensor(4.2904e+08, device='cuda:0')
c= tensor(4.2904e+08, device='cuda:0')
c= tensor(4.2904e+08, device='cuda:0')
c= tensor(4.2905e+08, device='cuda:0')
c= tensor(4.2905e+08, device='cuda:0')
c= tensor(4.2906e+08, device='cuda:0')
c= tensor(4.2907e+08, device='cuda:0')
c= tensor(4.2908e+08, device='cuda:0')
c= tensor(4.2909e+08, device='cuda:0')
c= tensor(4.2909e+08, device='cuda:0')
c= tensor(4.2909e+08, device='cuda:0')
c= tensor(4.2909e+08, device='cuda:0')
c= tensor(4.2910e+08, device='cuda:0')
c= tensor(4.2911e+08, device='cuda:0')
c= tensor(4.2911e+08, device='cuda:0')
c= tensor(4.2911e+08, device='cuda:0')
c= tensor(4.2912e+08, device='cuda:0')
c= tensor(4.2912e+08, device='cuda:0')
c= tensor(4.2912e+08, device='cuda:0')
c= tensor(4.2912e+08, device='cuda:0')
c= tensor(4.2913e+08, device='cuda:0')
c= tensor(4.2913e+08, device='cuda:0')
c= tensor(4.2913e+08, device='cuda:0')
c= tensor(4.2914e+08, device='cuda:0')
c= tensor(4.2916e+08, device='cuda:0')
c= tensor(4.2916e+08, device='cuda:0')
c= tensor(4.2916e+08, device='cuda:0')
c= tensor(4.2917e+08, device='cuda:0')
c= tensor(4.2917e+08, device='cuda:0')
c= tensor(4.2917e+08, device='cuda:0')
c= tensor(4.2917e+08, device='cuda:0')
c= tensor(4.2918e+08, device='cuda:0')
c= tensor(4.2918e+08, device='cuda:0')
c= tensor(4.2918e+08, device='cuda:0')
c= tensor(4.2919e+08, device='cuda:0')
c= tensor(4.2919e+08, device='cuda:0')
c= tensor(4.2919e+08, device='cuda:0')
c= tensor(4.2919e+08, device='cuda:0')
c= tensor(4.2920e+08, device='cuda:0')
c= tensor(4.2920e+08, device='cuda:0')
c= tensor(4.2921e+08, device='cuda:0')
c= tensor(4.2921e+08, device='cuda:0')
c= tensor(4.2921e+08, device='cuda:0')
c= tensor(4.2922e+08, device='cuda:0')
c= tensor(4.2922e+08, device='cuda:0')
c= tensor(4.2924e+08, device='cuda:0')
c= tensor(4.2925e+08, device='cuda:0')
c= tensor(4.2925e+08, device='cuda:0')
c= tensor(4.2926e+08, device='cuda:0')
c= tensor(4.2926e+08, device='cuda:0')
c= tensor(4.2926e+08, device='cuda:0')
c= tensor(4.2927e+08, device='cuda:0')
c= tensor(4.2927e+08, device='cuda:0')
c= tensor(4.2927e+08, device='cuda:0')
c= tensor(4.2927e+08, device='cuda:0')
c= tensor(4.2928e+08, device='cuda:0')
c= tensor(4.2928e+08, device='cuda:0')
c= tensor(4.2928e+08, device='cuda:0')
c= tensor(4.2928e+08, device='cuda:0')
c= tensor(4.2929e+08, device='cuda:0')
c= tensor(4.2929e+08, device='cuda:0')
c= tensor(4.2929e+08, device='cuda:0')
c= tensor(4.2929e+08, device='cuda:0')
c= tensor(4.2931e+08, device='cuda:0')
c= tensor(4.2931e+08, device='cuda:0')
c= tensor(4.2933e+08, device='cuda:0')
c= tensor(4.2934e+08, device='cuda:0')
c= tensor(4.2934e+08, device='cuda:0')
c= tensor(4.2934e+08, device='cuda:0')
c= tensor(4.2934e+08, device='cuda:0')
c= tensor(4.2934e+08, device='cuda:0')
c= tensor(4.2935e+08, device='cuda:0')
c= tensor(4.2935e+08, device='cuda:0')
c= tensor(4.2939e+08, device='cuda:0')
c= tensor(4.2939e+08, device='cuda:0')
c= tensor(4.2940e+08, device='cuda:0')
c= tensor(4.2940e+08, device='cuda:0')
c= tensor(4.2941e+08, device='cuda:0')
c= tensor(4.2941e+08, device='cuda:0')
c= tensor(4.2941e+08, device='cuda:0')
c= tensor(4.2941e+08, device='cuda:0')
c= tensor(4.2942e+08, device='cuda:0')
c= tensor(4.2942e+08, device='cuda:0')
c= tensor(4.2942e+08, device='cuda:0')
c= tensor(4.2943e+08, device='cuda:0')
c= tensor(4.2943e+08, device='cuda:0')
c= tensor(4.2943e+08, device='cuda:0')
c= tensor(4.2944e+08, device='cuda:0')
c= tensor(4.2946e+08, device='cuda:0')
c= tensor(4.2947e+08, device='cuda:0')
c= tensor(4.2947e+08, device='cuda:0')
c= tensor(4.2947e+08, device='cuda:0')
c= tensor(4.2948e+08, device='cuda:0')
c= tensor(4.2948e+08, device='cuda:0')
c= tensor(4.2948e+08, device='cuda:0')
c= tensor(4.2948e+08, device='cuda:0')
c= tensor(4.2949e+08, device='cuda:0')
c= tensor(4.2949e+08, device='cuda:0')
c= tensor(4.2950e+08, device='cuda:0')
c= tensor(4.2950e+08, device='cuda:0')
c= tensor(4.2952e+08, device='cuda:0')
c= tensor(4.2952e+08, device='cuda:0')
c= tensor(4.2952e+08, device='cuda:0')
c= tensor(4.2953e+08, device='cuda:0')
c= tensor(4.2953e+08, device='cuda:0')
c= tensor(4.3000e+08, device='cuda:0')
c= tensor(4.3001e+08, device='cuda:0')
c= tensor(4.3001e+08, device='cuda:0')
c= tensor(4.3001e+08, device='cuda:0')
c= tensor(4.3001e+08, device='cuda:0')
c= tensor(4.3002e+08, device='cuda:0')
c= tensor(4.3002e+08, device='cuda:0')
c= tensor(4.3003e+08, device='cuda:0')
c= tensor(4.3003e+08, device='cuda:0')
c= tensor(4.3003e+08, device='cuda:0')
c= tensor(4.3003e+08, device='cuda:0')
c= tensor(4.3003e+08, device='cuda:0')
c= tensor(4.3004e+08, device='cuda:0')
c= tensor(4.3004e+08, device='cuda:0')
c= tensor(4.3005e+08, device='cuda:0')
c= tensor(4.3006e+08, device='cuda:0')
c= tensor(4.3006e+08, device='cuda:0')
c= tensor(4.3007e+08, device='cuda:0')
c= tensor(4.3007e+08, device='cuda:0')
c= tensor(4.3007e+08, device='cuda:0')
c= tensor(4.3008e+08, device='cuda:0')
c= tensor(4.3008e+08, device='cuda:0')
c= tensor(4.3008e+08, device='cuda:0')
c= tensor(4.3008e+08, device='cuda:0')
c= tensor(4.3010e+08, device='cuda:0')
c= tensor(4.3011e+08, device='cuda:0')
c= tensor(4.3011e+08, device='cuda:0')
c= tensor(4.3011e+08, device='cuda:0')
c= tensor(4.3013e+08, device='cuda:0')
c= tensor(4.3016e+08, device='cuda:0')
c= tensor(4.3017e+08, device='cuda:0')
c= tensor(4.3017e+08, device='cuda:0')
c= tensor(4.3017e+08, device='cuda:0')
c= tensor(4.3017e+08, device='cuda:0')
c= tensor(4.3017e+08, device='cuda:0')
c= tensor(4.3018e+08, device='cuda:0')
c= tensor(4.3018e+08, device='cuda:0')
c= tensor(4.3018e+08, device='cuda:0')
c= tensor(4.3018e+08, device='cuda:0')
c= tensor(4.3018e+08, device='cuda:0')
c= tensor(4.3019e+08, device='cuda:0')
c= tensor(4.3019e+08, device='cuda:0')
c= tensor(4.3021e+08, device='cuda:0')
c= tensor(4.3021e+08, device='cuda:0')
c= tensor(4.3021e+08, device='cuda:0')
c= tensor(4.3021e+08, device='cuda:0')
c= tensor(4.3022e+08, device='cuda:0')
c= tensor(4.3022e+08, device='cuda:0')
c= tensor(4.3022e+08, device='cuda:0')
c= tensor(4.3024e+08, device='cuda:0')
c= tensor(4.3024e+08, device='cuda:0')
c= tensor(4.3025e+08, device='cuda:0')
c= tensor(4.3025e+08, device='cuda:0')
c= tensor(4.3025e+08, device='cuda:0')
c= tensor(4.3025e+08, device='cuda:0')
c= tensor(4.3025e+08, device='cuda:0')
c= tensor(4.3025e+08, device='cuda:0')
c= tensor(4.3025e+08, device='cuda:0')
c= tensor(4.3026e+08, device='cuda:0')
c= tensor(4.3026e+08, device='cuda:0')
c= tensor(4.3026e+08, device='cuda:0')
c= tensor(4.3026e+08, device='cuda:0')
c= tensor(4.3027e+08, device='cuda:0')
c= tensor(4.3027e+08, device='cuda:0')
c= tensor(4.3028e+08, device='cuda:0')
c= tensor(4.3028e+08, device='cuda:0')
c= tensor(4.3028e+08, device='cuda:0')
c= tensor(4.3029e+08, device='cuda:0')
c= tensor(4.3029e+08, device='cuda:0')
c= tensor(4.3029e+08, device='cuda:0')
c= tensor(4.3030e+08, device='cuda:0')
c= tensor(4.3030e+08, device='cuda:0')
c= tensor(4.3030e+08, device='cuda:0')
c= tensor(4.3031e+08, device='cuda:0')
c= tensor(4.3031e+08, device='cuda:0')
c= tensor(4.3031e+08, device='cuda:0')
c= tensor(4.3032e+08, device='cuda:0')
c= tensor(4.3034e+08, device='cuda:0')
c= tensor(4.3034e+08, device='cuda:0')
c= tensor(4.3036e+08, device='cuda:0')
c= tensor(4.3053e+08, device='cuda:0')
c= tensor(4.3060e+08, device='cuda:0')
c= tensor(4.3061e+08, device='cuda:0')
c= tensor(4.3062e+08, device='cuda:0')
c= tensor(4.3074e+08, device='cuda:0')
c= tensor(4.3193e+08, device='cuda:0')
c= tensor(4.3611e+08, device='cuda:0')
c= tensor(4.3612e+08, device='cuda:0')
c= tensor(4.3653e+08, device='cuda:0')
c= tensor(4.3781e+08, device='cuda:0')
c= tensor(4.3862e+08, device='cuda:0')
c= tensor(4.6199e+08, device='cuda:0')
c= tensor(4.6199e+08, device='cuda:0')
c= tensor(4.6227e+08, device='cuda:0')
c= tensor(4.6339e+08, device='cuda:0')
c= tensor(4.8612e+08, device='cuda:0')
c= tensor(4.8612e+08, device='cuda:0')
c= tensor(4.8628e+08, device='cuda:0')
c= tensor(5.0798e+08, device='cuda:0')
c= tensor(5.3084e+08, device='cuda:0')
c= tensor(5.3177e+08, device='cuda:0')
c= tensor(5.3291e+08, device='cuda:0')
c= tensor(5.3321e+08, device='cuda:0')
c= tensor(5.3326e+08, device='cuda:0')
c= tensor(5.3336e+08, device='cuda:0')
c= tensor(5.3812e+08, device='cuda:0')
c= tensor(5.3814e+08, device='cuda:0')
c= tensor(5.3816e+08, device='cuda:0')
c= tensor(5.3973e+08, device='cuda:0')
c= tensor(5.3993e+08, device='cuda:0')
c= tensor(5.6804e+08, device='cuda:0')
c= tensor(5.6927e+08, device='cuda:0')
c= tensor(5.6927e+08, device='cuda:0')
c= tensor(5.6935e+08, device='cuda:0')
c= tensor(5.6937e+08, device='cuda:0')
c= tensor(5.6947e+08, device='cuda:0')
c= tensor(5.7084e+08, device='cuda:0')
c= tensor(5.7096e+08, device='cuda:0')
c= tensor(5.7124e+08, device='cuda:0')
c= tensor(5.7124e+08, device='cuda:0')
c= tensor(5.7125e+08, device='cuda:0')
c= tensor(5.7153e+08, device='cuda:0')
c= tensor(5.7262e+08, device='cuda:0')
c= tensor(5.7300e+08, device='cuda:0')
c= tensor(5.7301e+08, device='cuda:0')
c= tensor(5.9009e+08, device='cuda:0')
c= tensor(5.9013e+08, device='cuda:0')
c= tensor(5.9023e+08, device='cuda:0')
c= tensor(5.9157e+08, device='cuda:0')
c= tensor(5.9158e+08, device='cuda:0')
c= tensor(5.9246e+08, device='cuda:0')
c= tensor(5.9480e+08, device='cuda:0')
c= tensor(6.0911e+08, device='cuda:0')
c= tensor(6.0916e+08, device='cuda:0')
c= tensor(6.0933e+08, device='cuda:0')
c= tensor(6.0939e+08, device='cuda:0')
c= tensor(6.0940e+08, device='cuda:0')
c= tensor(6.1020e+08, device='cuda:0')
c= tensor(6.1024e+08, device='cuda:0')
c= tensor(6.1068e+08, device='cuda:0')
c= tensor(6.2219e+08, device='cuda:0')
c= tensor(6.2893e+08, device='cuda:0')
c= tensor(6.2899e+08, device='cuda:0')
c= tensor(6.2903e+08, device='cuda:0')
c= tensor(6.3155e+08, device='cuda:0')
c= tensor(6.3176e+08, device='cuda:0')
c= tensor(6.3176e+08, device='cuda:0')
c= tensor(6.3179e+08, device='cuda:0')
c= tensor(6.6207e+08, device='cuda:0')
c= tensor(6.6210e+08, device='cuda:0')
c= tensor(6.6561e+08, device='cuda:0')
c= tensor(6.6564e+08, device='cuda:0')
c= tensor(6.6604e+08, device='cuda:0')
c= tensor(6.6681e+08, device='cuda:0')
c= tensor(6.6849e+08, device='cuda:0')
c= tensor(6.6930e+08, device='cuda:0')
c= tensor(6.6931e+08, device='cuda:0')
c= tensor(6.7226e+08, device='cuda:0')
c= tensor(6.7444e+08, device='cuda:0')
c= tensor(6.7447e+08, device='cuda:0')
c= tensor(6.7491e+08, device='cuda:0')
c= tensor(6.7853e+08, device='cuda:0')
c= tensor(6.8396e+08, device='cuda:0')
c= tensor(6.8425e+08, device='cuda:0')
c= tensor(6.8426e+08, device='cuda:0')
c= tensor(6.8427e+08, device='cuda:0')
c= tensor(6.8501e+08, device='cuda:0')
c= tensor(6.8544e+08, device='cuda:0')
c= tensor(6.8546e+08, device='cuda:0')
c= tensor(6.8546e+08, device='cuda:0')
c= tensor(6.8727e+08, device='cuda:0')
c= tensor(6.8974e+08, device='cuda:0')
c= tensor(7.0246e+08, device='cuda:0')
c= tensor(7.0247e+08, device='cuda:0')
c= tensor(7.0258e+08, device='cuda:0')
c= tensor(7.0264e+08, device='cuda:0')
c= tensor(7.0265e+08, device='cuda:0')
c= tensor(7.0267e+08, device='cuda:0')
c= tensor(7.0268e+08, device='cuda:0')
c= tensor(7.3528e+08, device='cuda:0')
c= tensor(7.3555e+08, device='cuda:0')
c= tensor(7.3559e+08, device='cuda:0')
c= tensor(7.3562e+08, device='cuda:0')
c= tensor(7.3563e+08, device='cuda:0')
c= tensor(7.6566e+08, device='cuda:0')
c= tensor(7.6567e+08, device='cuda:0')
c= tensor(7.6785e+08, device='cuda:0')
c= tensor(7.6786e+08, device='cuda:0')
c= tensor(7.6786e+08, device='cuda:0')
c= tensor(7.6787e+08, device='cuda:0')
c= tensor(7.6833e+08, device='cuda:0')
c= tensor(7.7009e+08, device='cuda:0')
c= tensor(7.7014e+08, device='cuda:0')
c= tensor(7.7015e+08, device='cuda:0')
c= tensor(7.7015e+08, device='cuda:0')
c= tensor(7.7665e+08, device='cuda:0')
c= tensor(7.7687e+08, device='cuda:0')
c= tensor(7.7696e+08, device='cuda:0')
c= tensor(7.7773e+08, device='cuda:0')
c= tensor(7.7845e+08, device='cuda:0')
c= tensor(7.7846e+08, device='cuda:0')
c= tensor(7.7846e+08, device='cuda:0')
c= tensor(7.7886e+08, device='cuda:0')
c= tensor(7.7886e+08, device='cuda:0')
c= tensor(7.7887e+08, device='cuda:0')
c= tensor(7.7899e+08, device='cuda:0')
c= tensor(7.7900e+08, device='cuda:0')
c= tensor(7.7900e+08, device='cuda:0')
c= tensor(7.7902e+08, device='cuda:0')
c= tensor(7.7903e+08, device='cuda:0')
c= tensor(8.1633e+08, device='cuda:0')
c= tensor(8.2253e+08, device='cuda:0')
c= tensor(8.2309e+08, device='cuda:0')
c= tensor(8.2365e+08, device='cuda:0')
c= tensor(8.2366e+08, device='cuda:0')
c= tensor(8.2376e+08, device='cuda:0')
c= tensor(1.1430e+09, device='cuda:0')
c= tensor(1.2044e+09, device='cuda:0')
c= tensor(1.2055e+09, device='cuda:0')
c= tensor(1.2060e+09, device='cuda:0')
c= tensor(1.2060e+09, device='cuda:0')
c= tensor(1.2119e+09, device='cuda:0')
c= tensor(1.2128e+09, device='cuda:0')
c= tensor(1.2378e+09, device='cuda:0')
c= tensor(1.2378e+09, device='cuda:0')
c= tensor(1.2403e+09, device='cuda:0')
c= tensor(1.2452e+09, device='cuda:0')
c= tensor(1.2463e+09, device='cuda:0')
c= tensor(1.2463e+09, device='cuda:0')
c= tensor(1.2464e+09, device='cuda:0')
c= tensor(1.2464e+09, device='cuda:0')
c= tensor(1.2464e+09, device='cuda:0')
c= tensor(1.2479e+09, device='cuda:0')
c= tensor(1.2483e+09, device='cuda:0')
c= tensor(1.2483e+09, device='cuda:0')
c= tensor(1.2486e+09, device='cuda:0')
c= tensor(1.2486e+09, device='cuda:0')
c= tensor(1.2486e+09, device='cuda:0')
c= tensor(1.2494e+09, device='cuda:0')
c= tensor(1.2504e+09, device='cuda:0')
c= tensor(1.2505e+09, device='cuda:0')
c= tensor(1.2525e+09, device='cuda:0')
c= tensor(1.2534e+09, device='cuda:0')
c= tensor(1.2535e+09, device='cuda:0')
c= tensor(1.2542e+09, device='cuda:0')
c= tensor(1.2554e+09, device='cuda:0')
c= tensor(1.2587e+09, device='cuda:0')
c= tensor(1.2587e+09, device='cuda:0')
c= tensor(1.2737e+09, device='cuda:0')
c= tensor(1.2914e+09, device='cuda:0')
c= tensor(1.2920e+09, device='cuda:0')
c= tensor(1.2932e+09, device='cuda:0')
c= tensor(1.2932e+09, device='cuda:0')
c= tensor(1.2933e+09, device='cuda:0')
c= tensor(1.2933e+09, device='cuda:0')
c= tensor(1.2939e+09, device='cuda:0')
c= tensor(1.2973e+09, device='cuda:0')
c= tensor(1.2999e+09, device='cuda:0')
c= tensor(1.3366e+09, device='cuda:0')
c= tensor(1.3370e+09, device='cuda:0')
c= tensor(1.3390e+09, device='cuda:0')
c= tensor(1.3392e+09, device='cuda:0')
c= tensor(1.3410e+09, device='cuda:0')
c= tensor(1.3410e+09, device='cuda:0')
c= tensor(1.3410e+09, device='cuda:0')
c= tensor(1.3462e+09, device='cuda:0')
c= tensor(1.3462e+09, device='cuda:0')
c= tensor(1.3462e+09, device='cuda:0')
c= tensor(1.3463e+09, device='cuda:0')
c= tensor(1.3467e+09, device='cuda:0')
c= tensor(1.3469e+09, device='cuda:0')
c= tensor(1.3487e+09, device='cuda:0')
c= tensor(1.3488e+09, device='cuda:0')
c= tensor(1.3490e+09, device='cuda:0')
c= tensor(1.3490e+09, device='cuda:0')
c= tensor(1.3490e+09, device='cuda:0')
c= tensor(1.3491e+09, device='cuda:0')
c= tensor(1.3498e+09, device='cuda:0')
c= tensor(1.3499e+09, device='cuda:0')
c= tensor(1.3520e+09, device='cuda:0')
c= tensor(1.3520e+09, device='cuda:0')
c= tensor(1.3521e+09, device='cuda:0')
c= tensor(1.3521e+09, device='cuda:0')
c= tensor(1.3532e+09, device='cuda:0')
c= tensor(1.3532e+09, device='cuda:0')
c= tensor(1.3536e+09, device='cuda:0')
c= tensor(1.3536e+09, device='cuda:0')
c= tensor(1.3538e+09, device='cuda:0')
c= tensor(1.3541e+09, device='cuda:0')
c= tensor(1.3748e+09, device='cuda:0')
c= tensor(1.3748e+09, device='cuda:0')
c= tensor(1.3749e+09, device='cuda:0')
c= tensor(1.3761e+09, device='cuda:0')
c= tensor(1.3761e+09, device='cuda:0')
c= tensor(1.3947e+09, device='cuda:0')
c= tensor(1.3947e+09, device='cuda:0')
c= tensor(1.3965e+09, device='cuda:0')
c= tensor(1.4036e+09, device='cuda:0')
c= tensor(1.4037e+09, device='cuda:0')
c= tensor(1.4119e+09, device='cuda:0')
c= tensor(1.4123e+09, device='cuda:0')
c= tensor(1.4138e+09, device='cuda:0')
c= tensor(1.4139e+09, device='cuda:0')
c= tensor(1.4147e+09, device='cuda:0')
c= tensor(1.4147e+09, device='cuda:0')
c= tensor(1.4148e+09, device='cuda:0')
c= tensor(1.4148e+09, device='cuda:0')
c= tensor(1.4157e+09, device='cuda:0')
c= tensor(1.4158e+09, device='cuda:0')
c= tensor(1.4201e+09, device='cuda:0')
c= tensor(1.4202e+09, device='cuda:0')
c= tensor(1.4202e+09, device='cuda:0')
c= tensor(1.4202e+09, device='cuda:0')
c= tensor(1.4206e+09, device='cuda:0')
c= tensor(1.4206e+09, device='cuda:0')
c= tensor(1.4228e+09, device='cuda:0')
c= tensor(1.4270e+09, device='cuda:0')
c= tensor(1.4271e+09, device='cuda:0')
c= tensor(1.4272e+09, device='cuda:0')
c= tensor(1.4273e+09, device='cuda:0')
c= tensor(1.6988e+09, device='cuda:0')
c= tensor(1.6988e+09, device='cuda:0')
c= tensor(1.6990e+09, device='cuda:0')
c= tensor(1.7047e+09, device='cuda:0')
c= tensor(1.7056e+09, device='cuda:0')
c= tensor(1.7056e+09, device='cuda:0')
c= tensor(1.7056e+09, device='cuda:0')
c= tensor(1.7257e+09, device='cuda:0')
c= tensor(1.7262e+09, device='cuda:0')
c= tensor(1.7280e+09, device='cuda:0')
c= tensor(1.7294e+09, device='cuda:0')
c= tensor(1.7295e+09, device='cuda:0')
c= tensor(1.7297e+09, device='cuda:0')
c= tensor(1.7333e+09, device='cuda:0')
c= tensor(1.7341e+09, device='cuda:0')
c= tensor(1.7341e+09, device='cuda:0')
c= tensor(1.7348e+09, device='cuda:0')
c= tensor(1.7348e+09, device='cuda:0')
c= tensor(1.7351e+09, device='cuda:0')
c= tensor(1.7352e+09, device='cuda:0')
c= tensor(1.7353e+09, device='cuda:0')
c= tensor(1.7381e+09, device='cuda:0')
c= tensor(1.7382e+09, device='cuda:0')
c= tensor(1.7382e+09, device='cuda:0')
c= tensor(1.7383e+09, device='cuda:0')
c= tensor(1.7385e+09, device='cuda:0')
c= tensor(1.7409e+09, device='cuda:0')
c= tensor(1.7445e+09, device='cuda:0')
c= tensor(1.7446e+09, device='cuda:0')
c= tensor(1.7446e+09, device='cuda:0')
c= tensor(1.7449e+09, device='cuda:0')
c= tensor(1.7449e+09, device='cuda:0')
c= tensor(1.7449e+09, device='cuda:0')
c= tensor(1.7450e+09, device='cuda:0')
c= tensor(1.7569e+09, device='cuda:0')
c= tensor(1.7569e+09, device='cuda:0')
c= tensor(1.7569e+09, device='cuda:0')
c= tensor(1.7569e+09, device='cuda:0')
c= tensor(1.7658e+09, device='cuda:0')
c= tensor(1.7727e+09, device='cuda:0')
c= tensor(1.7750e+09, device='cuda:0')
c= tensor(1.7751e+09, device='cuda:0')
c= tensor(1.7755e+09, device='cuda:0')
c= tensor(1.7767e+09, device='cuda:0')
c= tensor(1.7767e+09, device='cuda:0')
c= tensor(1.7768e+09, device='cuda:0')
c= tensor(1.7770e+09, device='cuda:0')
c= tensor(1.7964e+09, device='cuda:0')
c= tensor(1.7989e+09, device='cuda:0')
c= tensor(1.7999e+09, device='cuda:0')
c= tensor(1.8000e+09, device='cuda:0')
c= tensor(1.8004e+09, device='cuda:0')
c= tensor(1.8004e+09, device='cuda:0')
c= tensor(1.8008e+09, device='cuda:0')
c= tensor(1.8009e+09, device='cuda:0')
c= tensor(1.8023e+09, device='cuda:0')
c= tensor(1.8024e+09, device='cuda:0')
c= tensor(1.8078e+09, device='cuda:0')
c= tensor(1.8078e+09, device='cuda:0')
c= tensor(1.8079e+09, device='cuda:0')
c= tensor(1.8081e+09, device='cuda:0')
c= tensor(1.8850e+09, device='cuda:0')
c= tensor(1.8851e+09, device='cuda:0')
c= tensor(1.8852e+09, device='cuda:0')
c= tensor(1.8852e+09, device='cuda:0')
c= tensor(1.8856e+09, device='cuda:0')
c= tensor(1.8856e+09, device='cuda:0')
c= tensor(1.8857e+09, device='cuda:0')
c= tensor(1.8857e+09, device='cuda:0')
c= tensor(1.8857e+09, device='cuda:0')
c= tensor(1.8858e+09, device='cuda:0')
c= tensor(1.8858e+09, device='cuda:0')
c= tensor(1.8859e+09, device='cuda:0')
c= tensor(1.8904e+09, device='cuda:0')
c= tensor(1.9027e+09, device='cuda:0')
c= tensor(1.9048e+09, device='cuda:0')
c= tensor(1.9049e+09, device='cuda:0')
c= tensor(1.9050e+09, device='cuda:0')
c= tensor(1.9051e+09, device='cuda:0')
c= tensor(1.9051e+09, device='cuda:0')
c= tensor(1.9062e+09, device='cuda:0')
c= tensor(1.9074e+09, device='cuda:0')
c= tensor(1.9087e+09, device='cuda:0')
c= tensor(1.9087e+09, device='cuda:0')
c= tensor(1.9224e+09, device='cuda:0')
c= tensor(1.9225e+09, device='cuda:0')
c= tensor(1.9229e+09, device='cuda:0')
c= tensor(1.9287e+09, device='cuda:0')
c= tensor(1.9291e+09, device='cuda:0')
c= tensor(1.9366e+09, device='cuda:0')
c= tensor(1.9440e+09, device='cuda:0')
c= tensor(1.9453e+09, device='cuda:0')
c= tensor(1.9454e+09, device='cuda:0')
c= tensor(1.9457e+09, device='cuda:0')
c= tensor(1.9462e+09, device='cuda:0')
c= tensor(1.9463e+09, device='cuda:0')
c= tensor(1.9476e+09, device='cuda:0')
c= tensor(1.9891e+09, device='cuda:0')
c= tensor(1.9920e+09, device='cuda:0')
c= tensor(1.9932e+09, device='cuda:0')
c= tensor(1.9937e+09, device='cuda:0')
c= tensor(1.9937e+09, device='cuda:0')
c= tensor(1.9938e+09, device='cuda:0')
c= tensor(1.9940e+09, device='cuda:0')
c= tensor(1.9947e+09, device='cuda:0')
c= tensor(2.0042e+09, device='cuda:0')
c= tensor(2.0042e+09, device='cuda:0')
c= tensor(2.0076e+09, device='cuda:0')
c= tensor(2.0080e+09, device='cuda:0')
c= tensor(2.0083e+09, device='cuda:0')
c= tensor(2.0083e+09, device='cuda:0')
c= tensor(2.0105e+09, device='cuda:0')
c= tensor(2.0130e+09, device='cuda:0')
c= tensor(2.0130e+09, device='cuda:0')
c= tensor(2.0596e+09, device='cuda:0')
c= tensor(2.0602e+09, device='cuda:0')
c= tensor(2.0604e+09, device='cuda:0')
c= tensor(2.0605e+09, device='cuda:0')
c= tensor(2.0606e+09, device='cuda:0')
c= tensor(2.0606e+09, device='cuda:0')
c= tensor(2.0606e+09, device='cuda:0')
c= tensor(2.0606e+09, device='cuda:0')
c= tensor(2.0666e+09, device='cuda:0')
c= tensor(2.1385e+09, device='cuda:0')
c= tensor(2.1387e+09, device='cuda:0')
c= tensor(2.1401e+09, device='cuda:0')
c= tensor(2.1401e+09, device='cuda:0')
c= tensor(2.1402e+09, device='cuda:0')
c= tensor(2.1402e+09, device='cuda:0')
c= tensor(2.1451e+09, device='cuda:0')
c= tensor(2.1452e+09, device='cuda:0')
c= tensor(2.2507e+09, device='cuda:0')
c= tensor(2.2507e+09, device='cuda:0')
c= tensor(2.2519e+09, device='cuda:0')
c= tensor(2.2520e+09, device='cuda:0')
c= tensor(2.2553e+09, device='cuda:0')
c= tensor(2.2637e+09, device='cuda:0')
c= tensor(2.2638e+09, device='cuda:0')
c= tensor(2.2638e+09, device='cuda:0')
c= tensor(2.2647e+09, device='cuda:0')
c= tensor(2.2648e+09, device='cuda:0')
c= tensor(2.2649e+09, device='cuda:0')
c= tensor(2.2675e+09, device='cuda:0')
c= tensor(2.2703e+09, device='cuda:0')
c= tensor(2.2706e+09, device='cuda:0')
c= tensor(2.2708e+09, device='cuda:0')
c= tensor(2.2717e+09, device='cuda:0')
c= tensor(2.2874e+09, device='cuda:0')
c= tensor(2.2877e+09, device='cuda:0')
c= tensor(2.2877e+09, device='cuda:0')
c= tensor(2.2950e+09, device='cuda:0')
c= tensor(2.2952e+09, device='cuda:0')
c= tensor(2.2960e+09, device='cuda:0')
c= tensor(2.2964e+09, device='cuda:0')
c= tensor(2.2974e+09, device='cuda:0')
c= tensor(2.2977e+09, device='cuda:0')
c= tensor(2.3022e+09, device='cuda:0')
c= tensor(2.3032e+09, device='cuda:0')
c= tensor(2.3033e+09, device='cuda:0')
c= tensor(2.3033e+09, device='cuda:0')
c= tensor(2.3037e+09, device='cuda:0')
c= tensor(2.3042e+09, device='cuda:0')
c= tensor(2.3054e+09, device='cuda:0')
c= tensor(2.3073e+09, device='cuda:0')
c= tensor(2.3074e+09, device='cuda:0')
c= tensor(2.3080e+09, device='cuda:0')
c= tensor(2.3089e+09, device='cuda:0')
c= tensor(2.3332e+09, device='cuda:0')
c= tensor(2.3336e+09, device='cuda:0')
c= tensor(2.3344e+09, device='cuda:0')
c= tensor(2.3397e+09, device='cuda:0')
c= tensor(2.3401e+09, device='cuda:0')
c= tensor(2.3401e+09, device='cuda:0')
c= tensor(2.3402e+09, device='cuda:0')
c= tensor(2.3448e+09, device='cuda:0')
c= tensor(2.3454e+09, device='cuda:0')
c= tensor(2.3461e+09, device='cuda:0')
c= tensor(2.3461e+09, device='cuda:0')
c= tensor(2.3461e+09, device='cuda:0')
c= tensor(2.3568e+09, device='cuda:0')
c= tensor(2.3594e+09, device='cuda:0')
c= tensor(2.3596e+09, device='cuda:0')
c= tensor(2.3596e+09, device='cuda:0')
c= tensor(2.3596e+09, device='cuda:0')
c= tensor(2.3599e+09, device='cuda:0')
c= tensor(2.3604e+09, device='cuda:0')
c= tensor(2.3627e+09, device='cuda:0')
c= tensor(2.3627e+09, device='cuda:0')
c= tensor(2.3728e+09, device='cuda:0')
c= tensor(2.3728e+09, device='cuda:0')
c= tensor(2.3730e+09, device='cuda:0')
c= tensor(2.3939e+09, device='cuda:0')
c= tensor(2.3940e+09, device='cuda:0')
c= tensor(2.3949e+09, device='cuda:0')
c= tensor(2.3963e+09, device='cuda:0')
c= tensor(2.3968e+09, device='cuda:0')
c= tensor(2.3971e+09, device='cuda:0')
c= tensor(2.3981e+09, device='cuda:0')
c= tensor(2.3981e+09, device='cuda:0')
c= tensor(2.3982e+09, device='cuda:0')
c= tensor(2.3984e+09, device='cuda:0')
c= tensor(2.4006e+09, device='cuda:0')
c= tensor(2.4028e+09, device='cuda:0')
c= tensor(2.4034e+09, device='cuda:0')
c= tensor(2.4040e+09, device='cuda:0')
c= tensor(2.4041e+09, device='cuda:0')
c= tensor(2.4041e+09, device='cuda:0')
c= tensor(2.4042e+09, device='cuda:0')
c= tensor(2.4043e+09, device='cuda:0')
c= tensor(2.4098e+09, device='cuda:0')
c= tensor(2.4121e+09, device='cuda:0')
c= tensor(2.4121e+09, device='cuda:0')
c= tensor(2.4123e+09, device='cuda:0')
c= tensor(2.4123e+09, device='cuda:0')
c= tensor(2.4659e+09, device='cuda:0')
c= tensor(2.4660e+09, device='cuda:0')
c= tensor(2.4661e+09, device='cuda:0')
c= tensor(2.4680e+09, device='cuda:0')
c= tensor(2.4685e+09, device='cuda:0')
c= tensor(2.4686e+09, device='cuda:0')
c= tensor(2.4687e+09, device='cuda:0')
c= tensor(2.4688e+09, device='cuda:0')
memory (bytes)
4146573312
time for making loss 2 is 14.715425968170166
p0 True
it  0 : 1124083712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 19% |
shape of L is 
torch.Size([])
memory (bytes)
4146900992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  5% |
memory (bytes)
4147470336
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  51163620000.0
relative error loss 20.724472
shape of L is 
torch.Size([])
memory (bytes)
4367958016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  4% |  6% |
memory (bytes)
4368064512
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  51163345000.0
relative error loss 20.72436
shape of L is 
torch.Size([])
memory (bytes)
4369920000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4369920000
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  51161973000.0
relative error loss 20.723804
shape of L is 
torch.Size([])
memory (bytes)
4370956288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4371156992
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  51154550000.0
relative error loss 20.720798
shape of L is 
torch.Size([])
memory (bytes)
4373192704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% |  7% |
memory (bytes)
4373274624
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  51113670000.0
relative error loss 20.704239
shape of L is 
torch.Size([])
memory (bytes)
4375408640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4375408640
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  50889023000.0
relative error loss 20.613243
shape of L is 
torch.Size([])
memory (bytes)
4377473024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4377473024
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  48471503000.0
relative error loss 19.633995
shape of L is 
torch.Size([])
memory (bytes)
4379426816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4379656192
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  37743096000.0
relative error loss 15.288319
shape of L is 
torch.Size([])
memory (bytes)
4381790208
| ID | GPU | MEM |
------------------
|  0 | 27% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4381790208
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  10400332000.0
relative error loss 4.2127857
shape of L is 
torch.Size([])
memory (bytes)
4383846400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4383907840
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  5293477000.0
relative error loss 2.1441898
time to take a step is 245.00790786743164
it  1 : 1475530240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4385996800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
4386058240
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  5293477000.0
relative error loss 2.1441898
shape of L is 
torch.Size([])
memory (bytes)
4388118528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% |  7% |
memory (bytes)
4388118528
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  3241868300.0
relative error loss 1.3131597
shape of L is 
torch.Size([])
memory (bytes)
4390240256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4390301696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 89% |  8% |
error is  2735043000.0
relative error loss 1.1078638
shape of L is 
torch.Size([])
memory (bytes)
4392378368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
4392443904
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  2372528000.0
relative error loss 0.9610225
shape of L is 
torch.Size([])
memory (bytes)
4394508288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4394569728
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  2280578300.0
relative error loss 0.9237771
shape of L is 
torch.Size([])
memory (bytes)
4396625920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4396687360
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  2175539700.0
relative error loss 0.8812299
shape of L is 
torch.Size([])
memory (bytes)
4398739456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4398776320
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  2101462000.0
relative error loss 0.85122377
shape of L is 
torch.Size([])
memory (bytes)
4400881664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4400947200
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  2038264400.0
relative error loss 0.8256248
shape of L is 
torch.Size([])
memory (bytes)
4402995200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% |  7% |
memory (bytes)
4402995200
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  2000559600.0
relative error loss 0.81035197
shape of L is 
torch.Size([])
memory (bytes)
4405149696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  8% |
memory (bytes)
4405174272
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  1966835800.0
relative error loss 0.7966917
time to take a step is 231.17831158638
it  2 : 1577072640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4407197696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4407279616
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  1966835800.0
relative error loss 0.7966917
shape of L is 
torch.Size([])
memory (bytes)
4409323520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4409384960
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  1891743200.0
relative error loss 0.7662745
shape of L is 
torch.Size([])
memory (bytes)
4411461632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4411523072
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  1808236300.0
relative error loss 0.732449
shape of L is 
torch.Size([])
memory (bytes)
4413493248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4413493248
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  1732181800.0
relative error loss 0.70164216
shape of L is 
torch.Size([])
memory (bytes)
4415754240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4415754240
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  1625615900.0
relative error loss 0.6584763
shape of L is 
torch.Size([])
memory (bytes)
4417814528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% |  7% |
memory (bytes)
4417814528
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  1510278700.0
relative error loss 0.61175746
shape of L is 
torch.Size([])
memory (bytes)
4419825664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4420018176
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  1391998100.0
relative error loss 0.5638464
shape of L is 
torch.Size([])
memory (bytes)
4422074368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
4422074368
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  1282773800.0
relative error loss 0.5196037
shape of L is 
torch.Size([])
memory (bytes)
4424138752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4424138752
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  1177697900.0
relative error loss 0.47704142
shape of L is 
torch.Size([])
memory (bytes)
4426297344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4426297344
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  1074583800.0
relative error loss 0.43527377
time to take a step is 230.1959547996521
it  3 : 1577072128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4428521472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
4428521472
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  1074583800.0
relative error loss 0.43527377
shape of L is 
torch.Size([])
memory (bytes)
4430589952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% |  7% |
memory (bytes)
4430589952
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  986754560.0
relative error loss 0.39969742
shape of L is 
torch.Size([])
memory (bytes)
4432687104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
4432822272
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  897660000.0
relative error loss 0.36360854
shape of L is 
torch.Size([])
memory (bytes)
4434907136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4434907136
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  828239100.0
relative error loss 0.33548874
shape of L is 
torch.Size([])
memory (bytes)
4436959232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% |  7% |
memory (bytes)
4436959232
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  764519940.0
relative error loss 0.30967847
shape of L is 
torch.Size([])
memory (bytes)
4439126016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% |  7% |
memory (bytes)
4439126016
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  888150140.0
relative error loss 0.35975644
shape of L is 
torch.Size([])
memory (bytes)
4441329664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% |  8% |
memory (bytes)
4441395200
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  727694600.0
relative error loss 0.2947619
shape of L is 
torch.Size([])
memory (bytes)
4443295744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
4443529216
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  683360500.0
relative error loss 0.27680382
shape of L is 
torch.Size([])
memory (bytes)
4445630464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4445630464
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  634678660.0
relative error loss 0.2570846
shape of L is 
torch.Size([])
memory (bytes)
4447813632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4447813632
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  587261200.0
relative error loss 0.23787758
time to take a step is 230.71192860603333
c= tensor(753.2037, device='cuda:0')
c= tensor(85643.1562, device='cuda:0')
c= tensor(89745.8125, device='cuda:0')
c= tensor(91436.1797, device='cuda:0')
c= tensor(2095463.8750, device='cuda:0')
c= tensor(2386062.5000, device='cuda:0')
c= tensor(3036223.5000, device='cuda:0')
c= tensor(3337504., device='cuda:0')
c= tensor(3354898.2500, device='cuda:0')
c= tensor(11764611., device='cuda:0')
c= tensor(11781018., device='cuda:0')
c= tensor(13947819., device='cuda:0')
c= tensor(13957505., device='cuda:0')
c= tensor(19681200., device='cuda:0')
c= tensor(19998520., device='cuda:0')
c= tensor(20330606., device='cuda:0')
c= tensor(21265592., device='cuda:0')
c= tensor(22136060., device='cuda:0')
c= tensor(25507368., device='cuda:0')
c= tensor(28239880., device='cuda:0')
c= tensor(28263014., device='cuda:0')
c= tensor(53609176., device='cuda:0')
c= tensor(53679304., device='cuda:0')
c= tensor(53709216., device='cuda:0')
c= tensor(60089956., device='cuda:0')
c= tensor(60910960., device='cuda:0')
c= tensor(61281200., device='cuda:0')
c= tensor(62330676., device='cuda:0')
c= tensor(62804188., device='cuda:0')
c= tensor(4.1051e+08, device='cuda:0')
c= tensor(4.1052e+08, device='cuda:0')
c= tensor(4.2785e+08, device='cuda:0')
c= tensor(4.2791e+08, device='cuda:0')
c= tensor(4.2792e+08, device='cuda:0')
c= tensor(4.2793e+08, device='cuda:0')
c= tensor(4.2827e+08, device='cuda:0')
c= tensor(4.2893e+08, device='cuda:0')
c= tensor(4.2893e+08, device='cuda:0')
c= tensor(4.2894e+08, device='cuda:0')
c= tensor(4.2894e+08, device='cuda:0')
c= tensor(4.2895e+08, device='cuda:0')
c= tensor(4.2895e+08, device='cuda:0')
c= tensor(4.2895e+08, device='cuda:0')
c= tensor(4.2895e+08, device='cuda:0')
c= tensor(4.2895e+08, device='cuda:0')
c= tensor(4.2896e+08, device='cuda:0')
c= tensor(4.2896e+08, device='cuda:0')
c= tensor(4.2897e+08, device='cuda:0')
c= tensor(4.2898e+08, device='cuda:0')
c= tensor(4.2899e+08, device='cuda:0')
c= tensor(4.2902e+08, device='cuda:0')
c= tensor(4.2902e+08, device='cuda:0')
c= tensor(4.2902e+08, device='cuda:0')
c= tensor(4.2902e+08, device='cuda:0')
c= tensor(4.2904e+08, device='cuda:0')
c= tensor(4.2904e+08, device='cuda:0')
c= tensor(4.2904e+08, device='cuda:0')
c= tensor(4.2904e+08, device='cuda:0')
c= tensor(4.2905e+08, device='cuda:0')
c= tensor(4.2905e+08, device='cuda:0')
c= tensor(4.2906e+08, device='cuda:0')
c= tensor(4.2907e+08, device='cuda:0')
c= tensor(4.2908e+08, device='cuda:0')
c= tensor(4.2909e+08, device='cuda:0')
c= tensor(4.2909e+08, device='cuda:0')
c= tensor(4.2909e+08, device='cuda:0')
c= tensor(4.2909e+08, device='cuda:0')
c= tensor(4.2910e+08, device='cuda:0')
c= tensor(4.2911e+08, device='cuda:0')
c= tensor(4.2911e+08, device='cuda:0')
c= tensor(4.2911e+08, device='cuda:0')
c= tensor(4.2912e+08, device='cuda:0')
c= tensor(4.2912e+08, device='cuda:0')
c= tensor(4.2912e+08, device='cuda:0')
c= tensor(4.2912e+08, device='cuda:0')
c= tensor(4.2913e+08, device='cuda:0')
c= tensor(4.2913e+08, device='cuda:0')
c= tensor(4.2913e+08, device='cuda:0')
c= tensor(4.2914e+08, device='cuda:0')
c= tensor(4.2916e+08, device='cuda:0')
c= tensor(4.2916e+08, device='cuda:0')
c= tensor(4.2916e+08, device='cuda:0')
c= tensor(4.2917e+08, device='cuda:0')
c= tensor(4.2917e+08, device='cuda:0')
c= tensor(4.2917e+08, device='cuda:0')
c= tensor(4.2917e+08, device='cuda:0')
c= tensor(4.2918e+08, device='cuda:0')
c= tensor(4.2918e+08, device='cuda:0')
c= tensor(4.2918e+08, device='cuda:0')
c= tensor(4.2919e+08, device='cuda:0')
c= tensor(4.2919e+08, device='cuda:0')
c= tensor(4.2919e+08, device='cuda:0')
c= tensor(4.2919e+08, device='cuda:0')
c= tensor(4.2920e+08, device='cuda:0')
c= tensor(4.2920e+08, device='cuda:0')
c= tensor(4.2921e+08, device='cuda:0')
c= tensor(4.2921e+08, device='cuda:0')
c= tensor(4.2921e+08, device='cuda:0')
c= tensor(4.2922e+08, device='cuda:0')
c= tensor(4.2922e+08, device='cuda:0')
c= tensor(4.2924e+08, device='cuda:0')
c= tensor(4.2925e+08, device='cuda:0')
c= tensor(4.2925e+08, device='cuda:0')
c= tensor(4.2926e+08, device='cuda:0')
c= tensor(4.2926e+08, device='cuda:0')
c= tensor(4.2926e+08, device='cuda:0')
c= tensor(4.2927e+08, device='cuda:0')
c= tensor(4.2927e+08, device='cuda:0')
c= tensor(4.2927e+08, device='cuda:0')
c= tensor(4.2927e+08, device='cuda:0')
c= tensor(4.2928e+08, device='cuda:0')
c= tensor(4.2928e+08, device='cuda:0')
c= tensor(4.2928e+08, device='cuda:0')
c= tensor(4.2928e+08, device='cuda:0')
c= tensor(4.2929e+08, device='cuda:0')
c= tensor(4.2929e+08, device='cuda:0')
c= tensor(4.2929e+08, device='cuda:0')
c= tensor(4.2929e+08, device='cuda:0')
c= tensor(4.2931e+08, device='cuda:0')
c= tensor(4.2931e+08, device='cuda:0')
c= tensor(4.2933e+08, device='cuda:0')
c= tensor(4.2934e+08, device='cuda:0')
c= tensor(4.2934e+08, device='cuda:0')
c= tensor(4.2934e+08, device='cuda:0')
c= tensor(4.2934e+08, device='cuda:0')
c= tensor(4.2934e+08, device='cuda:0')
c= tensor(4.2935e+08, device='cuda:0')
c= tensor(4.2935e+08, device='cuda:0')
c= tensor(4.2939e+08, device='cuda:0')
c= tensor(4.2939e+08, device='cuda:0')
c= tensor(4.2940e+08, device='cuda:0')
c= tensor(4.2940e+08, device='cuda:0')
c= tensor(4.2941e+08, device='cuda:0')
c= tensor(4.2941e+08, device='cuda:0')
c= tensor(4.2941e+08, device='cuda:0')
c= tensor(4.2941e+08, device='cuda:0')
c= tensor(4.2942e+08, device='cuda:0')
c= tensor(4.2942e+08, device='cuda:0')
c= tensor(4.2942e+08, device='cuda:0')
c= tensor(4.2943e+08, device='cuda:0')
c= tensor(4.2943e+08, device='cuda:0')
c= tensor(4.2943e+08, device='cuda:0')
c= tensor(4.2944e+08, device='cuda:0')
c= tensor(4.2946e+08, device='cuda:0')
c= tensor(4.2947e+08, device='cuda:0')
c= tensor(4.2947e+08, device='cuda:0')
c= tensor(4.2947e+08, device='cuda:0')
c= tensor(4.2948e+08, device='cuda:0')
c= tensor(4.2948e+08, device='cuda:0')
c= tensor(4.2948e+08, device='cuda:0')
c= tensor(4.2948e+08, device='cuda:0')
c= tensor(4.2949e+08, device='cuda:0')
c= tensor(4.2949e+08, device='cuda:0')
c= tensor(4.2950e+08, device='cuda:0')
c= tensor(4.2950e+08, device='cuda:0')
c= tensor(4.2952e+08, device='cuda:0')
c= tensor(4.2952e+08, device='cuda:0')
c= tensor(4.2952e+08, device='cuda:0')
c= tensor(4.2953e+08, device='cuda:0')
c= tensor(4.2953e+08, device='cuda:0')
c= tensor(4.3000e+08, device='cuda:0')
c= tensor(4.3001e+08, device='cuda:0')
c= tensor(4.3001e+08, device='cuda:0')
c= tensor(4.3001e+08, device='cuda:0')
c= tensor(4.3001e+08, device='cuda:0')
c= tensor(4.3002e+08, device='cuda:0')
c= tensor(4.3002e+08, device='cuda:0')
c= tensor(4.3003e+08, device='cuda:0')
c= tensor(4.3003e+08, device='cuda:0')
c= tensor(4.3003e+08, device='cuda:0')
c= tensor(4.3003e+08, device='cuda:0')
c= tensor(4.3003e+08, device='cuda:0')
c= tensor(4.3004e+08, device='cuda:0')
c= tensor(4.3004e+08, device='cuda:0')
c= tensor(4.3005e+08, device='cuda:0')
c= tensor(4.3006e+08, device='cuda:0')
c= tensor(4.3006e+08, device='cuda:0')
c= tensor(4.3007e+08, device='cuda:0')
c= tensor(4.3007e+08, device='cuda:0')
c= tensor(4.3007e+08, device='cuda:0')
c= tensor(4.3008e+08, device='cuda:0')
c= tensor(4.3008e+08, device='cuda:0')
c= tensor(4.3008e+08, device='cuda:0')
c= tensor(4.3008e+08, device='cuda:0')
c= tensor(4.3010e+08, device='cuda:0')
c= tensor(4.3011e+08, device='cuda:0')
c= tensor(4.3011e+08, device='cuda:0')
c= tensor(4.3011e+08, device='cuda:0')
c= tensor(4.3013e+08, device='cuda:0')
c= tensor(4.3016e+08, device='cuda:0')
c= tensor(4.3017e+08, device='cuda:0')
c= tensor(4.3017e+08, device='cuda:0')
c= tensor(4.3017e+08, device='cuda:0')
c= tensor(4.3017e+08, device='cuda:0')
c= tensor(4.3017e+08, device='cuda:0')
c= tensor(4.3018e+08, device='cuda:0')
c= tensor(4.3018e+08, device='cuda:0')
c= tensor(4.3018e+08, device='cuda:0')
c= tensor(4.3018e+08, device='cuda:0')
c= tensor(4.3018e+08, device='cuda:0')
c= tensor(4.3019e+08, device='cuda:0')
c= tensor(4.3019e+08, device='cuda:0')
c= tensor(4.3021e+08, device='cuda:0')
c= tensor(4.3021e+08, device='cuda:0')
c= tensor(4.3021e+08, device='cuda:0')
c= tensor(4.3021e+08, device='cuda:0')
c= tensor(4.3022e+08, device='cuda:0')
c= tensor(4.3022e+08, device='cuda:0')
c= tensor(4.3022e+08, device='cuda:0')
c= tensor(4.3024e+08, device='cuda:0')
c= tensor(4.3024e+08, device='cuda:0')
c= tensor(4.3025e+08, device='cuda:0')
c= tensor(4.3025e+08, device='cuda:0')
c= tensor(4.3025e+08, device='cuda:0')
c= tensor(4.3025e+08, device='cuda:0')
c= tensor(4.3025e+08, device='cuda:0')
c= tensor(4.3025e+08, device='cuda:0')
c= tensor(4.3025e+08, device='cuda:0')
c= tensor(4.3026e+08, device='cuda:0')
c= tensor(4.3026e+08, device='cuda:0')
c= tensor(4.3026e+08, device='cuda:0')
c= tensor(4.3026e+08, device='cuda:0')
c= tensor(4.3027e+08, device='cuda:0')
c= tensor(4.3027e+08, device='cuda:0')
c= tensor(4.3028e+08, device='cuda:0')
c= tensor(4.3028e+08, device='cuda:0')
c= tensor(4.3028e+08, device='cuda:0')
c= tensor(4.3029e+08, device='cuda:0')
c= tensor(4.3029e+08, device='cuda:0')
c= tensor(4.3029e+08, device='cuda:0')
c= tensor(4.3030e+08, device='cuda:0')
c= tensor(4.3030e+08, device='cuda:0')
c= tensor(4.3030e+08, device='cuda:0')
c= tensor(4.3031e+08, device='cuda:0')
c= tensor(4.3031e+08, device='cuda:0')
c= tensor(4.3031e+08, device='cuda:0')
c= tensor(4.3032e+08, device='cuda:0')
c= tensor(4.3034e+08, device='cuda:0')
c= tensor(4.3034e+08, device='cuda:0')
c= tensor(4.3036e+08, device='cuda:0')
c= tensor(4.3053e+08, device='cuda:0')
c= tensor(4.3060e+08, device='cuda:0')
c= tensor(4.3061e+08, device='cuda:0')
c= tensor(4.3062e+08, device='cuda:0')
c= tensor(4.3074e+08, device='cuda:0')
c= tensor(4.3193e+08, device='cuda:0')
c= tensor(4.3611e+08, device='cuda:0')
c= tensor(4.3612e+08, device='cuda:0')
c= tensor(4.3653e+08, device='cuda:0')
c= tensor(4.3781e+08, device='cuda:0')
c= tensor(4.3862e+08, device='cuda:0')
c= tensor(4.6199e+08, device='cuda:0')
c= tensor(4.6199e+08, device='cuda:0')
c= tensor(4.6227e+08, device='cuda:0')
c= tensor(4.6339e+08, device='cuda:0')
c= tensor(4.8612e+08, device='cuda:0')
c= tensor(4.8612e+08, device='cuda:0')
c= tensor(4.8628e+08, device='cuda:0')
c= tensor(5.0798e+08, device='cuda:0')
c= tensor(5.3084e+08, device='cuda:0')
c= tensor(5.3177e+08, device='cuda:0')
c= tensor(5.3291e+08, device='cuda:0')
c= tensor(5.3321e+08, device='cuda:0')
c= tensor(5.3326e+08, device='cuda:0')
c= tensor(5.3336e+08, device='cuda:0')
c= tensor(5.3812e+08, device='cuda:0')
c= tensor(5.3814e+08, device='cuda:0')
c= tensor(5.3816e+08, device='cuda:0')
c= tensor(5.3973e+08, device='cuda:0')
c= tensor(5.3993e+08, device='cuda:0')
c= tensor(5.6804e+08, device='cuda:0')
c= tensor(5.6927e+08, device='cuda:0')
c= tensor(5.6927e+08, device='cuda:0')
c= tensor(5.6935e+08, device='cuda:0')
c= tensor(5.6937e+08, device='cuda:0')
c= tensor(5.6947e+08, device='cuda:0')
c= tensor(5.7084e+08, device='cuda:0')
c= tensor(5.7096e+08, device='cuda:0')
c= tensor(5.7124e+08, device='cuda:0')
c= tensor(5.7124e+08, device='cuda:0')
c= tensor(5.7125e+08, device='cuda:0')
c= tensor(5.7153e+08, device='cuda:0')
c= tensor(5.7262e+08, device='cuda:0')
c= tensor(5.7300e+08, device='cuda:0')
c= tensor(5.7301e+08, device='cuda:0')
c= tensor(5.9009e+08, device='cuda:0')
c= tensor(5.9013e+08, device='cuda:0')
c= tensor(5.9023e+08, device='cuda:0')
c= tensor(5.9157e+08, device='cuda:0')
c= tensor(5.9158e+08, device='cuda:0')
c= tensor(5.9246e+08, device='cuda:0')
c= tensor(5.9480e+08, device='cuda:0')
c= tensor(6.0911e+08, device='cuda:0')
c= tensor(6.0916e+08, device='cuda:0')
c= tensor(6.0933e+08, device='cuda:0')
c= tensor(6.0939e+08, device='cuda:0')
c= tensor(6.0940e+08, device='cuda:0')
c= tensor(6.1020e+08, device='cuda:0')
c= tensor(6.1024e+08, device='cuda:0')
c= tensor(6.1068e+08, device='cuda:0')
c= tensor(6.2219e+08, device='cuda:0')
c= tensor(6.2893e+08, device='cuda:0')
c= tensor(6.2899e+08, device='cuda:0')
c= tensor(6.2903e+08, device='cuda:0')
c= tensor(6.3155e+08, device='cuda:0')
c= tensor(6.3176e+08, device='cuda:0')
c= tensor(6.3176e+08, device='cuda:0')
c= tensor(6.3179e+08, device='cuda:0')
c= tensor(6.6207e+08, device='cuda:0')
c= tensor(6.6210e+08, device='cuda:0')
c= tensor(6.6561e+08, device='cuda:0')
c= tensor(6.6564e+08, device='cuda:0')
c= tensor(6.6604e+08, device='cuda:0')
c= tensor(6.6681e+08, device='cuda:0')
c= tensor(6.6849e+08, device='cuda:0')
c= tensor(6.6930e+08, device='cuda:0')
c= tensor(6.6931e+08, device='cuda:0')
c= tensor(6.7226e+08, device='cuda:0')
c= tensor(6.7444e+08, device='cuda:0')
c= tensor(6.7447e+08, device='cuda:0')
c= tensor(6.7491e+08, device='cuda:0')
c= tensor(6.7853e+08, device='cuda:0')
c= tensor(6.8396e+08, device='cuda:0')
c= tensor(6.8425e+08, device='cuda:0')
c= tensor(6.8426e+08, device='cuda:0')
c= tensor(6.8427e+08, device='cuda:0')
c= tensor(6.8501e+08, device='cuda:0')
c= tensor(6.8544e+08, device='cuda:0')
c= tensor(6.8546e+08, device='cuda:0')
c= tensor(6.8546e+08, device='cuda:0')
c= tensor(6.8727e+08, device='cuda:0')
c= tensor(6.8974e+08, device='cuda:0')
c= tensor(7.0246e+08, device='cuda:0')
c= tensor(7.0247e+08, device='cuda:0')
c= tensor(7.0258e+08, device='cuda:0')
c= tensor(7.0264e+08, device='cuda:0')
c= tensor(7.0265e+08, device='cuda:0')
c= tensor(7.0267e+08, device='cuda:0')
c= tensor(7.0268e+08, device='cuda:0')
c= tensor(7.3528e+08, device='cuda:0')
c= tensor(7.3555e+08, device='cuda:0')
c= tensor(7.3559e+08, device='cuda:0')
c= tensor(7.3562e+08, device='cuda:0')
c= tensor(7.3563e+08, device='cuda:0')
c= tensor(7.6566e+08, device='cuda:0')
c= tensor(7.6567e+08, device='cuda:0')
c= tensor(7.6785e+08, device='cuda:0')
c= tensor(7.6786e+08, device='cuda:0')
c= tensor(7.6786e+08, device='cuda:0')
c= tensor(7.6787e+08, device='cuda:0')
c= tensor(7.6833e+08, device='cuda:0')
c= tensor(7.7009e+08, device='cuda:0')
c= tensor(7.7014e+08, device='cuda:0')
c= tensor(7.7015e+08, device='cuda:0')
c= tensor(7.7015e+08, device='cuda:0')
c= tensor(7.7665e+08, device='cuda:0')
c= tensor(7.7687e+08, device='cuda:0')
c= tensor(7.7696e+08, device='cuda:0')
c= tensor(7.7773e+08, device='cuda:0')
c= tensor(7.7845e+08, device='cuda:0')
c= tensor(7.7846e+08, device='cuda:0')
c= tensor(7.7846e+08, device='cuda:0')
c= tensor(7.7886e+08, device='cuda:0')
c= tensor(7.7886e+08, device='cuda:0')
c= tensor(7.7887e+08, device='cuda:0')
c= tensor(7.7899e+08, device='cuda:0')
c= tensor(7.7900e+08, device='cuda:0')
c= tensor(7.7900e+08, device='cuda:0')
c= tensor(7.7902e+08, device='cuda:0')
c= tensor(7.7903e+08, device='cuda:0')
c= tensor(8.1633e+08, device='cuda:0')
c= tensor(8.2253e+08, device='cuda:0')
c= tensor(8.2309e+08, device='cuda:0')
c= tensor(8.2365e+08, device='cuda:0')
c= tensor(8.2366e+08, device='cuda:0')
c= tensor(8.2376e+08, device='cuda:0')
c= tensor(1.1430e+09, device='cuda:0')
c= tensor(1.2044e+09, device='cuda:0')
c= tensor(1.2055e+09, device='cuda:0')
c= tensor(1.2060e+09, device='cuda:0')
c= tensor(1.2060e+09, device='cuda:0')
c= tensor(1.2119e+09, device='cuda:0')
c= tensor(1.2128e+09, device='cuda:0')
c= tensor(1.2378e+09, device='cuda:0')
c= tensor(1.2378e+09, device='cuda:0')
c= tensor(1.2403e+09, device='cuda:0')
c= tensor(1.2452e+09, device='cuda:0')
c= tensor(1.2463e+09, device='cuda:0')
c= tensor(1.2463e+09, device='cuda:0')
c= tensor(1.2464e+09, device='cuda:0')
c= tensor(1.2464e+09, device='cuda:0')
c= tensor(1.2464e+09, device='cuda:0')
c= tensor(1.2479e+09, device='cuda:0')
c= tensor(1.2483e+09, device='cuda:0')
c= tensor(1.2483e+09, device='cuda:0')
c= tensor(1.2486e+09, device='cuda:0')
c= tensor(1.2486e+09, device='cuda:0')
c= tensor(1.2486e+09, device='cuda:0')
c= tensor(1.2494e+09, device='cuda:0')
c= tensor(1.2504e+09, device='cuda:0')
c= tensor(1.2505e+09, device='cuda:0')
c= tensor(1.2525e+09, device='cuda:0')
c= tensor(1.2534e+09, device='cuda:0')
c= tensor(1.2535e+09, device='cuda:0')
c= tensor(1.2542e+09, device='cuda:0')
c= tensor(1.2554e+09, device='cuda:0')
c= tensor(1.2587e+09, device='cuda:0')
c= tensor(1.2587e+09, device='cuda:0')
c= tensor(1.2737e+09, device='cuda:0')
c= tensor(1.2914e+09, device='cuda:0')
c= tensor(1.2920e+09, device='cuda:0')
c= tensor(1.2932e+09, device='cuda:0')
c= tensor(1.2932e+09, device='cuda:0')
c= tensor(1.2933e+09, device='cuda:0')
c= tensor(1.2933e+09, device='cuda:0')
c= tensor(1.2939e+09, device='cuda:0')
c= tensor(1.2973e+09, device='cuda:0')
c= tensor(1.2999e+09, device='cuda:0')
c= tensor(1.3366e+09, device='cuda:0')
c= tensor(1.3370e+09, device='cuda:0')
c= tensor(1.3390e+09, device='cuda:0')
c= tensor(1.3392e+09, device='cuda:0')
c= tensor(1.3410e+09, device='cuda:0')
c= tensor(1.3410e+09, device='cuda:0')
c= tensor(1.3410e+09, device='cuda:0')
c= tensor(1.3462e+09, device='cuda:0')
c= tensor(1.3462e+09, device='cuda:0')
c= tensor(1.3462e+09, device='cuda:0')
c= tensor(1.3463e+09, device='cuda:0')
c= tensor(1.3467e+09, device='cuda:0')
c= tensor(1.3469e+09, device='cuda:0')
c= tensor(1.3487e+09, device='cuda:0')
c= tensor(1.3488e+09, device='cuda:0')
c= tensor(1.3490e+09, device='cuda:0')
c= tensor(1.3490e+09, device='cuda:0')
c= tensor(1.3490e+09, device='cuda:0')
c= tensor(1.3491e+09, device='cuda:0')
c= tensor(1.3498e+09, device='cuda:0')
c= tensor(1.3499e+09, device='cuda:0')
c= tensor(1.3520e+09, device='cuda:0')
c= tensor(1.3520e+09, device='cuda:0')
c= tensor(1.3521e+09, device='cuda:0')
c= tensor(1.3521e+09, device='cuda:0')
c= tensor(1.3532e+09, device='cuda:0')
c= tensor(1.3532e+09, device='cuda:0')
c= tensor(1.3536e+09, device='cuda:0')
c= tensor(1.3536e+09, device='cuda:0')
c= tensor(1.3538e+09, device='cuda:0')
c= tensor(1.3541e+09, device='cuda:0')
c= tensor(1.3748e+09, device='cuda:0')
c= tensor(1.3748e+09, device='cuda:0')
c= tensor(1.3749e+09, device='cuda:0')
c= tensor(1.3761e+09, device='cuda:0')
c= tensor(1.3761e+09, device='cuda:0')
c= tensor(1.3947e+09, device='cuda:0')
c= tensor(1.3947e+09, device='cuda:0')
c= tensor(1.3965e+09, device='cuda:0')
c= tensor(1.4036e+09, device='cuda:0')
c= tensor(1.4037e+09, device='cuda:0')
c= tensor(1.4119e+09, device='cuda:0')
c= tensor(1.4123e+09, device='cuda:0')
c= tensor(1.4138e+09, device='cuda:0')
c= tensor(1.4139e+09, device='cuda:0')
c= tensor(1.4147e+09, device='cuda:0')
c= tensor(1.4147e+09, device='cuda:0')
c= tensor(1.4148e+09, device='cuda:0')
c= tensor(1.4148e+09, device='cuda:0')
c= tensor(1.4157e+09, device='cuda:0')
c= tensor(1.4158e+09, device='cuda:0')
c= tensor(1.4201e+09, device='cuda:0')
c= tensor(1.4202e+09, device='cuda:0')
c= tensor(1.4202e+09, device='cuda:0')
c= tensor(1.4202e+09, device='cuda:0')
c= tensor(1.4206e+09, device='cuda:0')
c= tensor(1.4206e+09, device='cuda:0')
c= tensor(1.4228e+09, device='cuda:0')
c= tensor(1.4270e+09, device='cuda:0')
c= tensor(1.4271e+09, device='cuda:0')
c= tensor(1.4272e+09, device='cuda:0')
c= tensor(1.4273e+09, device='cuda:0')
c= tensor(1.6988e+09, device='cuda:0')
c= tensor(1.6988e+09, device='cuda:0')
c= tensor(1.6990e+09, device='cuda:0')
c= tensor(1.7047e+09, device='cuda:0')
c= tensor(1.7056e+09, device='cuda:0')
c= tensor(1.7056e+09, device='cuda:0')
c= tensor(1.7056e+09, device='cuda:0')
c= tensor(1.7257e+09, device='cuda:0')
c= tensor(1.7262e+09, device='cuda:0')
c= tensor(1.7280e+09, device='cuda:0')
c= tensor(1.7294e+09, device='cuda:0')
c= tensor(1.7295e+09, device='cuda:0')
c= tensor(1.7297e+09, device='cuda:0')
c= tensor(1.7333e+09, device='cuda:0')
c= tensor(1.7341e+09, device='cuda:0')
c= tensor(1.7341e+09, device='cuda:0')
c= tensor(1.7348e+09, device='cuda:0')
c= tensor(1.7348e+09, device='cuda:0')
c= tensor(1.7351e+09, device='cuda:0')
c= tensor(1.7352e+09, device='cuda:0')
c= tensor(1.7353e+09, device='cuda:0')
c= tensor(1.7381e+09, device='cuda:0')
c= tensor(1.7382e+09, device='cuda:0')
c= tensor(1.7382e+09, device='cuda:0')
c= tensor(1.7383e+09, device='cuda:0')
c= tensor(1.7385e+09, device='cuda:0')
c= tensor(1.7409e+09, device='cuda:0')
c= tensor(1.7445e+09, device='cuda:0')
c= tensor(1.7446e+09, device='cuda:0')
c= tensor(1.7446e+09, device='cuda:0')
c= tensor(1.7449e+09, device='cuda:0')
c= tensor(1.7449e+09, device='cuda:0')
c= tensor(1.7449e+09, device='cuda:0')
c= tensor(1.7450e+09, device='cuda:0')
c= tensor(1.7569e+09, device='cuda:0')
c= tensor(1.7569e+09, device='cuda:0')
c= tensor(1.7569e+09, device='cuda:0')
c= tensor(1.7569e+09, device='cuda:0')
c= tensor(1.7658e+09, device='cuda:0')
c= tensor(1.7727e+09, device='cuda:0')
c= tensor(1.7750e+09, device='cuda:0')
c= tensor(1.7751e+09, device='cuda:0')
c= tensor(1.7755e+09, device='cuda:0')
c= tensor(1.7767e+09, device='cuda:0')
c= tensor(1.7767e+09, device='cuda:0')
c= tensor(1.7768e+09, device='cuda:0')
c= tensor(1.7770e+09, device='cuda:0')
c= tensor(1.7964e+09, device='cuda:0')
c= tensor(1.7989e+09, device='cuda:0')
c= tensor(1.7999e+09, device='cuda:0')
c= tensor(1.8000e+09, device='cuda:0')
c= tensor(1.8004e+09, device='cuda:0')
c= tensor(1.8004e+09, device='cuda:0')
c= tensor(1.8008e+09, device='cuda:0')
c= tensor(1.8009e+09, device='cuda:0')
c= tensor(1.8023e+09, device='cuda:0')
c= tensor(1.8024e+09, device='cuda:0')
c= tensor(1.8078e+09, device='cuda:0')
c= tensor(1.8078e+09, device='cuda:0')
c= tensor(1.8079e+09, device='cuda:0')
c= tensor(1.8081e+09, device='cuda:0')
c= tensor(1.8850e+09, device='cuda:0')
c= tensor(1.8851e+09, device='cuda:0')
c= tensor(1.8852e+09, device='cuda:0')
c= tensor(1.8852e+09, device='cuda:0')
c= tensor(1.8856e+09, device='cuda:0')
c= tensor(1.8856e+09, device='cuda:0')
c= tensor(1.8857e+09, device='cuda:0')
c= tensor(1.8857e+09, device='cuda:0')
c= tensor(1.8857e+09, device='cuda:0')
c= tensor(1.8858e+09, device='cuda:0')
c= tensor(1.8858e+09, device='cuda:0')
c= tensor(1.8859e+09, device='cuda:0')
c= tensor(1.8904e+09, device='cuda:0')
c= tensor(1.9027e+09, device='cuda:0')
c= tensor(1.9048e+09, device='cuda:0')
c= tensor(1.9049e+09, device='cuda:0')
c= tensor(1.9050e+09, device='cuda:0')
c= tensor(1.9051e+09, device='cuda:0')
c= tensor(1.9051e+09, device='cuda:0')
c= tensor(1.9062e+09, device='cuda:0')
c= tensor(1.9074e+09, device='cuda:0')
c= tensor(1.9087e+09, device='cuda:0')
c= tensor(1.9087e+09, device='cuda:0')
c= tensor(1.9224e+09, device='cuda:0')
c= tensor(1.9225e+09, device='cuda:0')
c= tensor(1.9229e+09, device='cuda:0')
c= tensor(1.9287e+09, device='cuda:0')
c= tensor(1.9291e+09, device='cuda:0')
c= tensor(1.9366e+09, device='cuda:0')
c= tensor(1.9440e+09, device='cuda:0')
c= tensor(1.9453e+09, device='cuda:0')
c= tensor(1.9454e+09, device='cuda:0')
c= tensor(1.9457e+09, device='cuda:0')
c= tensor(1.9462e+09, device='cuda:0')
c= tensor(1.9463e+09, device='cuda:0')
c= tensor(1.9476e+09, device='cuda:0')
c= tensor(1.9891e+09, device='cuda:0')
c= tensor(1.9920e+09, device='cuda:0')
c= tensor(1.9932e+09, device='cuda:0')
c= tensor(1.9937e+09, device='cuda:0')
c= tensor(1.9937e+09, device='cuda:0')
c= tensor(1.9938e+09, device='cuda:0')
c= tensor(1.9940e+09, device='cuda:0')
c= tensor(1.9947e+09, device='cuda:0')
c= tensor(2.0042e+09, device='cuda:0')
c= tensor(2.0042e+09, device='cuda:0')
c= tensor(2.0076e+09, device='cuda:0')
c= tensor(2.0080e+09, device='cuda:0')
c= tensor(2.0083e+09, device='cuda:0')
c= tensor(2.0083e+09, device='cuda:0')
c= tensor(2.0105e+09, device='cuda:0')
c= tensor(2.0130e+09, device='cuda:0')
c= tensor(2.0130e+09, device='cuda:0')
c= tensor(2.0596e+09, device='cuda:0')
c= tensor(2.0602e+09, device='cuda:0')
c= tensor(2.0604e+09, device='cuda:0')
c= tensor(2.0605e+09, device='cuda:0')
c= tensor(2.0606e+09, device='cuda:0')
c= tensor(2.0606e+09, device='cuda:0')
c= tensor(2.0606e+09, device='cuda:0')
c= tensor(2.0606e+09, device='cuda:0')
c= tensor(2.0666e+09, device='cuda:0')
c= tensor(2.1385e+09, device='cuda:0')
c= tensor(2.1387e+09, device='cuda:0')
c= tensor(2.1401e+09, device='cuda:0')
c= tensor(2.1401e+09, device='cuda:0')
c= tensor(2.1402e+09, device='cuda:0')
c= tensor(2.1402e+09, device='cuda:0')
c= tensor(2.1451e+09, device='cuda:0')
c= tensor(2.1452e+09, device='cuda:0')
c= tensor(2.2507e+09, device='cuda:0')
c= tensor(2.2507e+09, device='cuda:0')
c= tensor(2.2519e+09, device='cuda:0')
c= tensor(2.2520e+09, device='cuda:0')
c= tensor(2.2553e+09, device='cuda:0')
c= tensor(2.2637e+09, device='cuda:0')
c= tensor(2.2638e+09, device='cuda:0')
c= tensor(2.2638e+09, device='cuda:0')
c= tensor(2.2647e+09, device='cuda:0')
c= tensor(2.2648e+09, device='cuda:0')
c= tensor(2.2649e+09, device='cuda:0')
c= tensor(2.2675e+09, device='cuda:0')
c= tensor(2.2703e+09, device='cuda:0')
c= tensor(2.2706e+09, device='cuda:0')
c= tensor(2.2708e+09, device='cuda:0')
c= tensor(2.2717e+09, device='cuda:0')
c= tensor(2.2874e+09, device='cuda:0')
c= tensor(2.2877e+09, device='cuda:0')
c= tensor(2.2877e+09, device='cuda:0')
c= tensor(2.2950e+09, device='cuda:0')
c= tensor(2.2952e+09, device='cuda:0')
c= tensor(2.2960e+09, device='cuda:0')
c= tensor(2.2964e+09, device='cuda:0')
c= tensor(2.2974e+09, device='cuda:0')
c= tensor(2.2977e+09, device='cuda:0')
c= tensor(2.3022e+09, device='cuda:0')
c= tensor(2.3032e+09, device='cuda:0')
c= tensor(2.3033e+09, device='cuda:0')
c= tensor(2.3033e+09, device='cuda:0')
c= tensor(2.3037e+09, device='cuda:0')
c= tensor(2.3042e+09, device='cuda:0')
c= tensor(2.3054e+09, device='cuda:0')
c= tensor(2.3073e+09, device='cuda:0')
c= tensor(2.3074e+09, device='cuda:0')
c= tensor(2.3080e+09, device='cuda:0')
c= tensor(2.3089e+09, device='cuda:0')
c= tensor(2.3332e+09, device='cuda:0')
c= tensor(2.3336e+09, device='cuda:0')
c= tensor(2.3344e+09, device='cuda:0')
c= tensor(2.3397e+09, device='cuda:0')
c= tensor(2.3401e+09, device='cuda:0')
c= tensor(2.3401e+09, device='cuda:0')
c= tensor(2.3402e+09, device='cuda:0')
c= tensor(2.3448e+09, device='cuda:0')
c= tensor(2.3454e+09, device='cuda:0')
c= tensor(2.3461e+09, device='cuda:0')
c= tensor(2.3461e+09, device='cuda:0')
c= tensor(2.3461e+09, device='cuda:0')
c= tensor(2.3568e+09, device='cuda:0')
c= tensor(2.3594e+09, device='cuda:0')
c= tensor(2.3596e+09, device='cuda:0')
c= tensor(2.3596e+09, device='cuda:0')
c= tensor(2.3596e+09, device='cuda:0')
c= tensor(2.3599e+09, device='cuda:0')
c= tensor(2.3604e+09, device='cuda:0')
c= tensor(2.3627e+09, device='cuda:0')
c= tensor(2.3627e+09, device='cuda:0')
c= tensor(2.3728e+09, device='cuda:0')
c= tensor(2.3728e+09, device='cuda:0')
c= tensor(2.3730e+09, device='cuda:0')
c= tensor(2.3939e+09, device='cuda:0')
c= tensor(2.3940e+09, device='cuda:0')
c= tensor(2.3949e+09, device='cuda:0')
c= tensor(2.3963e+09, device='cuda:0')
c= tensor(2.3968e+09, device='cuda:0')
c= tensor(2.3971e+09, device='cuda:0')
c= tensor(2.3981e+09, device='cuda:0')
c= tensor(2.3981e+09, device='cuda:0')
c= tensor(2.3982e+09, device='cuda:0')
c= tensor(2.3984e+09, device='cuda:0')
c= tensor(2.4006e+09, device='cuda:0')
c= tensor(2.4028e+09, device='cuda:0')
c= tensor(2.4034e+09, device='cuda:0')
c= tensor(2.4040e+09, device='cuda:0')
c= tensor(2.4041e+09, device='cuda:0')
c= tensor(2.4041e+09, device='cuda:0')
c= tensor(2.4042e+09, device='cuda:0')
c= tensor(2.4043e+09, device='cuda:0')
c= tensor(2.4098e+09, device='cuda:0')
c= tensor(2.4121e+09, device='cuda:0')
c= tensor(2.4121e+09, device='cuda:0')
c= tensor(2.4123e+09, device='cuda:0')
c= tensor(2.4123e+09, device='cuda:0')
c= tensor(2.4659e+09, device='cuda:0')
c= tensor(2.4660e+09, device='cuda:0')
c= tensor(2.4661e+09, device='cuda:0')
c= tensor(2.4680e+09, device='cuda:0')
c= tensor(2.4685e+09, device='cuda:0')
c= tensor(2.4686e+09, device='cuda:0')
c= tensor(2.4687e+09, device='cuda:0')
c= tensor(2.4688e+09, device='cuda:0')
time to make c is 11.337916135787964
time for making loss is 11.337965250015259
p0 True
it  0 : 1124736000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  6% |
shape of L is 
torch.Size([])
memory (bytes)
4449918976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  6% |
memory (bytes)
4450185216
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  6% |
error is  587261200.0
relative error loss 0.23787758
shape of L is 
torch.Size([])
memory (bytes)
4477112320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  6% |
memory (bytes)
4477136896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% |  7% |
error is  580182660.0
relative error loss 0.23501033
shape of L is 
torch.Size([])
memory (bytes)
4480659456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4480774144
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  564704000.0
relative error loss 0.2287405
shape of L is 
torch.Size([])
memory (bytes)
4483739648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4483973120
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  557562240.0
relative error loss 0.22584763
shape of L is 
torch.Size([])
memory (bytes)
4487172096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4487172096
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  551205250.0
relative error loss 0.22327265
shape of L is 
torch.Size([])
memory (bytes)
4490235904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  7% |
memory (bytes)
4490235904
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  544620800.0
relative error loss 0.22060554
shape of L is 
torch.Size([])
memory (bytes)
4493508608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4493586432
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  541586800.0
relative error loss 0.2193766
shape of L is 
torch.Size([])
memory (bytes)
4496560128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% |  7% |
memory (bytes)
4496797696
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  536758800.0
relative error loss 0.21742094
shape of L is 
torch.Size([])
memory (bytes)
4499943424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4500017152
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  534101380.0
relative error loss 0.21634452
shape of L is 
torch.Size([])
memory (bytes)
4503126016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  7% |
memory (bytes)
4503126016
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  532018560.0
relative error loss 0.21550085
time to take a step is 296.8542392253876
it  1 : 1577498624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4506378240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% |  7% |
memory (bytes)
4506451968
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  532018560.0
relative error loss 0.21550085
shape of L is 
torch.Size([])
memory (bytes)
4509536256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% |  7% |
memory (bytes)
4509663232
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  529383040.0
relative error loss 0.2144333
shape of L is 
torch.Size([])
memory (bytes)
4512690176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4512878592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  8% |
error is  526482430.0
relative error loss 0.21325837
shape of L is 
torch.Size([])
memory (bytes)
4516110336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  7% |
memory (bytes)
4516110336
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  525396220.0
relative error loss 0.21281838
shape of L is 
torch.Size([])
memory (bytes)
4519243776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4519317504
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  524331140.0
relative error loss 0.21238697
shape of L is 
torch.Size([])
memory (bytes)
4522553344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4522553344
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  522060160.0
relative error loss 0.21146707
shape of L is 
torch.Size([])
memory (bytes)
4525690880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% |  7% |
memory (bytes)
4525768704
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  520596600.0
relative error loss 0.21087424
shape of L is 
torch.Size([])
memory (bytes)
4528979968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4528979968
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  518934140.0
relative error loss 0.21020083
shape of L is 
torch.Size([])
memory (bytes)
4532097024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% |  7% |
memory (bytes)
4532097024
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  517993340.0
relative error loss 0.20981975
shape of L is 
torch.Size([])
memory (bytes)
4535316480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4535410688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  8% |
error is  517009660.0
relative error loss 0.2094213
time to take a step is 310.34516954421997
it  2 : 1577498624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4538621952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4538621952
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  517009660.0
relative error loss 0.2094213
shape of L is 
torch.Size([])
memory (bytes)
4541763584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% |  7% |
memory (bytes)
4541841408
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  516332540.0
relative error loss 0.20914702
shape of L is 
torch.Size([])
memory (bytes)
4544954368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  7% |
memory (bytes)
4544954368
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  515421440.0
relative error loss 0.20877798
shape of L is 
torch.Size([])
memory (bytes)
4548296704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4548296704
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  514833920.0
relative error loss 0.20854
shape of L is 
torch.Size([])
memory (bytes)
4551270400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4551512064
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  514349570.0
relative error loss 0.2083438
shape of L is 
torch.Size([])
memory (bytes)
4554715136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4554715136
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  513399680.0
relative error loss 0.20795904
shape of L is 
torch.Size([])
memory (bytes)
4557811712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4557811712
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  512916220.0
relative error loss 0.20776321
shape of L is 
torch.Size([])
memory (bytes)
4561154048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4561154048
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  512411650.0
relative error loss 0.20755883
shape of L is 
torch.Size([])
memory (bytes)
4564312064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% |  7% |
memory (bytes)
4564312064
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  512108800.0
relative error loss 0.20743614
shape of L is 
torch.Size([])
memory (bytes)
4567543808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4567592960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  511485950.0
relative error loss 0.20718385
time to take a step is 307.59369349479675
it  3 : 1577498624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4570787840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4570787840
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  511485950.0
relative error loss 0.20718385
shape of L is 
torch.Size([])
memory (bytes)
4574023680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4574023680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% |  8% |
error is  511094660.0
relative error loss 0.20702535
shape of L is 
torch.Size([])
memory (bytes)
4577189888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4577189888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% |  8% |
error is  510531460.0
relative error loss 0.20679723
shape of L is 
torch.Size([])
memory (bytes)
4580274176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4580478976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  510141570.0
relative error loss 0.20663929
shape of L is 
torch.Size([])
memory (bytes)
4583620608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4583698432
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  509869700.0
relative error loss 0.20652917
shape of L is 
torch.Size([])
memory (bytes)
4586909696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% |  7% |
memory (bytes)
4586909696
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  509531780.0
relative error loss 0.20639229
shape of L is 
torch.Size([])
memory (bytes)
4590108672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  7% |
memory (bytes)
4590125056
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  509050240.0
relative error loss 0.20619723
shape of L is 
torch.Size([])
memory (bytes)
4593217536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  7% |
memory (bytes)
4593217536
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  508864130.0
relative error loss 0.20612185
shape of L is 
torch.Size([])
memory (bytes)
4596543488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4596551680
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  508623100.0
relative error loss 0.20602421
shape of L is 
torch.Size([])
memory (bytes)
4599746560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  7% |
memory (bytes)
4599746560
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  508239600.0
relative error loss 0.20586888
time to take a step is 294.7395339012146
it  4 : 1577498624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4602896384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4602896384
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  508239600.0
relative error loss 0.20586888
shape of L is 
torch.Size([])
memory (bytes)
4606189568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% |  7% |
memory (bytes)
4606189568
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  507881100.0
relative error loss 0.20572366
shape of L is 
torch.Size([])
memory (bytes)
4609368064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  7% |
memory (bytes)
4609368064
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  507634700.0
relative error loss 0.20562385
shape of L is 
torch.Size([])
memory (bytes)
4612636672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  7% |
memory (bytes)
4612636672
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  507444350.0
relative error loss 0.20554675
shape of L is 
torch.Size([])
memory (bytes)
4615823360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4615823360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% |  8% |
error is  507012220.0
relative error loss 0.20537171
shape of L is 
torch.Size([])
memory (bytes)
4619071488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  7% |
memory (bytes)
4619071488
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  506934660.0
relative error loss 0.2053403
shape of L is 
torch.Size([])
memory (bytes)
4622209024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  7% |
memory (bytes)
4622209024
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  506677630.0
relative error loss 0.20523618
shape of L is 
torch.Size([])
memory (bytes)
4625338368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% |  7% |
memory (bytes)
4625506304
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  506517250.0
relative error loss 0.20517121
shape of L is 
torch.Size([])
memory (bytes)
4628647936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  7% |
memory (bytes)
4628721664
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  506361100.0
relative error loss 0.20510796
shape of L is 
torch.Size([])
memory (bytes)
4631863296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4631863296
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  506136580.0
relative error loss 0.20501702
time to take a step is 290.00809121131897
it  5 : 1577498624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4635111424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4635168768
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  506136580.0
relative error loss 0.20501702
shape of L is 
torch.Size([])
memory (bytes)
4638269440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  7% |
memory (bytes)
4638269440
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  505907070.0
relative error loss 0.20492406
shape of L is 
torch.Size([])
memory (bytes)
4641562624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4641587200
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  505767300.0
relative error loss 0.20486744
shape of L is 
torch.Size([])
memory (bytes)
4644732928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% |  7% |
memory (bytes)
4644732928
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  505561340.0
relative error loss 0.20478402
shape of L is 
torch.Size([])
memory (bytes)
4647911424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4647911424
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  505455740.0
relative error loss 0.20474124
shape of L is 
torch.Size([])
memory (bytes)
4651167744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  7% |
memory (bytes)
4651241472
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  505302900.0
relative error loss 0.20467934
shape of L is 
torch.Size([])
memory (bytes)
4654338048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4654456832
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  505222800.0
relative error loss 0.20464689
shape of L is 
torch.Size([])
memory (bytes)
4657668096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% |  7% |
memory (bytes)
4657668096
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  505156860.0
relative error loss 0.20462018
shape of L is 
torch.Size([])
memory (bytes)
4660752384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4660752384
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  505020030.0
relative error loss 0.20456475
shape of L is 
torch.Size([])
memory (bytes)
4664066048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4664107008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% |  8% |
error is  504919040.0
relative error loss 0.20452385
time to take a step is 292.2752273082733
it  6 : 1577498624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4667244544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4667322368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% |  7% |
error is  504919040.0
relative error loss 0.20452385
shape of L is 
torch.Size([])
memory (bytes)
4670435328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  7% |
memory (bytes)
4670537728
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  504861700.0
relative error loss 0.20450062
shape of L is 
torch.Size([])
memory (bytes)
4673728512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4673777664
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  504807680.0
relative error loss 0.20447874
shape of L is 
torch.Size([])
memory (bytes)
4676894720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  7% |
memory (bytes)
4677001216
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  504670080.0
relative error loss 0.204423
shape of L is 
torch.Size([])
memory (bytes)
4680200192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4680200192
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  504543740.0
relative error loss 0.20437182
shape of L is 
torch.Size([])
memory (bytes)
4683333632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  7% |
memory (bytes)
4683333632
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  504391800.0
relative error loss 0.20431028
shape of L is 
torch.Size([])
memory (bytes)
4686536704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4686639104
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  504311940.0
relative error loss 0.20427793
shape of L is 
torch.Size([])
memory (bytes)
4689854464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  7% |
memory (bytes)
4689854464
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  504185860.0
relative error loss 0.20422685
shape of L is 
torch.Size([])
memory (bytes)
4693065728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  7% |
memory (bytes)
4693065728
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  504045440.0
relative error loss 0.20416997
shape of L is 
torch.Size([])
memory (bytes)
4696223744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  7% |
memory (bytes)
4696293376
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  503883650.0
relative error loss 0.20410444
time to take a step is 295.88963747024536
it  7 : 1577498624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4699504640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% |  7% |
memory (bytes)
4699504640
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  503883650.0
relative error loss 0.20410444
shape of L is 
torch.Size([])
memory (bytes)
4702715904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  7% |
memory (bytes)
4702715904
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  503697400.0
relative error loss 0.20402901
shape of L is 
torch.Size([])
memory (bytes)
4705837056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% |  7% |
memory (bytes)
4705837056
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  503605500.0
relative error loss 0.20399179
shape of L is 
torch.Size([])
memory (bytes)
4709122048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4709122048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% |  8% |
error is  503493250.0
relative error loss 0.2039463
shape of L is 
torch.Size([])
memory (bytes)
4712382464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  7% |
memory (bytes)
4712382464
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  503336200.0
relative error loss 0.2038827
shape of L is 
torch.Size([])
memory (bytes)
4715520000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4715520000
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  503283970.0
relative error loss 0.20386153
shape of L is 
torch.Size([])
memory (bytes)
4718751744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4718825472
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  503183870.0
relative error loss 0.20382099
shape of L is 
torch.Size([])
memory (bytes)
4721889280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4722036736
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  503096320.0
relative error loss 0.20378552
shape of L is 
torch.Size([])
memory (bytes)
4725252096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4725252096
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  503021950.0
relative error loss 0.20375541
shape of L is 
torch.Size([])
memory (bytes)
4728344576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4728344576
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  502980100.0
relative error loss 0.20373845
time to take a step is 305.70032787323
it  8 : 1577498624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4731654144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4731686912
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  502980100.0
relative error loss 0.20373845
shape of L is 
torch.Size([])
memory (bytes)
4734734336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4734898176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  502814720.0
relative error loss 0.20367146
shape of L is 
torch.Size([])
memory (bytes)
4738048000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4738121728
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  502717200.0
relative error loss 0.20363195
shape of L is 
torch.Size([])
memory (bytes)
4741259264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% |  7% |
memory (bytes)
4741259264
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  502629500.0
relative error loss 0.20359644
shape of L is 
torch.Size([])
memory (bytes)
4744568832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  7% |
memory (bytes)
4744568832
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  502495600.0
relative error loss 0.2035422
shape of L is 
torch.Size([])
memory (bytes)
4747669504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4747669504
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  502576770.0
relative error loss 0.20357507
shape of L is 
torch.Size([])
memory (bytes)
4750925824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% |  8% |
memory (bytes)
4751003648
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  502421500.0
relative error loss 0.20351219
shape of L is 
torch.Size([])
memory (bytes)
4754214912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% |  7% |
memory (bytes)
4754214912
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  502340860.0
relative error loss 0.20347951
shape of L is 
torch.Size([])
memory (bytes)
4757368832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% |  7% |
memory (bytes)
4757434368
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  502279680.0
relative error loss 0.20345473
shape of L is 
torch.Size([])
memory (bytes)
4760567808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% |  7% |
memory (bytes)
4760645632
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  502206340.0
relative error loss 0.20342503
time to take a step is 282.2905583381653
it  9 : 1577498624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4763848704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4763848704
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  502206340.0
relative error loss 0.20342503
shape of L is 
torch.Size([])
memory (bytes)
4767068160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% |  7% |
memory (bytes)
4767068160
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  502132100.0
relative error loss 0.20339495
shape of L is 
torch.Size([])
memory (bytes)
4770222080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  7% |
memory (bytes)
4770222080
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  502097660.0
relative error loss 0.203381
shape of L is 
torch.Size([])
memory (bytes)
4773412864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  7% |
memory (bytes)
4773519360
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  502057860.0
relative error loss 0.20336488
shape of L is 
torch.Size([])
memory (bytes)
4776738816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  7% |
memory (bytes)
4776738816
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  501989250.0
relative error loss 0.20333709
shape of L is 
torch.Size([])
memory (bytes)
4779778048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4779945984
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  501952260.0
relative error loss 0.20332211
shape of L is 
torch.Size([])
memory (bytes)
4783091712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4783165440
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  501912200.0
relative error loss 0.20330589
shape of L is 
torch.Size([])
memory (bytes)
4786380800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% |  7% |
memory (bytes)
4786380800
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  501882000.0
relative error loss 0.20329365
shape of L is 
torch.Size([])
memory (bytes)
4789592064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4789592064
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  501837060.0
relative error loss 0.20327544
shape of L is 
torch.Size([])
memory (bytes)
4792733696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4792811520
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  501802750.0
relative error loss 0.20326155
time to take a step is 307.63433933258057
it  10 : 1577498624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4796026880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4796026880
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  501802750.0
relative error loss 0.20326155
shape of L is 
torch.Size([])
memory (bytes)
4799238144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4799238144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% |  8% |
error is  501756300.0
relative error loss 0.20324273
shape of L is 
torch.Size([])
memory (bytes)
4802322432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4802469888
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  501715070.0
relative error loss 0.20322603
shape of L is 
torch.Size([])
memory (bytes)
4805681152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% |  8% |
memory (bytes)
4805685248
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  501684220.0
relative error loss 0.20321354
shape of L is 
torch.Size([])
memory (bytes)
4808863744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  8% |
memory (bytes)
4808863744
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  501616260.0
relative error loss 0.203186
shape of L is 
torch.Size([])
memory (bytes)
4812124160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4812124160
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  501565700.0
relative error loss 0.20316553
shape of L is 
torch.Size([])
memory (bytes)
4815183872
| ID | GPU | MEM |
------------------
|  0 |  2% |  0% |
|  1 | 23% |  8% |
memory (bytes)
4815339520
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  501530100.0
relative error loss 0.20315112
shape of L is 
torch.Size([])
memory (bytes)
4818530304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4818563072
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  501499260.0
relative error loss 0.20313862
shape of L is 
torch.Size([])
memory (bytes)
4821712896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4821786624
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  501448700.0
relative error loss 0.20311815
shape of L is 
torch.Size([])
memory (bytes)
4824997888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4824997888
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  501431300.0
relative error loss 0.20311108
time to take a step is 306.4309606552124
it  11 : 1577498624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4828225536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  7% |
memory (bytes)
4828225536
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  501431300.0
relative error loss 0.20311108
shape of L is 
torch.Size([])
memory (bytes)
4831326208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4831326208
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  501375600.0
relative error loss 0.20308854
shape of L is 
torch.Size([])
memory (bytes)
4834672640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% |  8% |
memory (bytes)
4834672640
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  501341440.0
relative error loss 0.2030747
shape of L is 
torch.Size([])
memory (bytes)
4837765120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  8% |
memory (bytes)
4837892096
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  501301000.0
relative error loss 0.2030583
shape of L is 
torch.Size([])
memory (bytes)
4841119744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% |  8% |
memory (bytes)
4841119744
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  501240830.0
relative error loss 0.20303394
shape of L is 
torch.Size([])
memory (bytes)
4844253184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  8% |
memory (bytes)
4844331008
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  501242880.0
relative error loss 0.20303477
shape of L is 
torch.Size([])
memory (bytes)
4847538176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4847538176
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  501203330.0
relative error loss 0.20301874
shape of L is 
torch.Size([])
memory (bytes)
4850700288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% |  8% |
memory (bytes)
4850774016
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  501161200.0
relative error loss 0.2030017
shape of L is 
torch.Size([])
memory (bytes)
4853977088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  8% |
memory (bytes)
4853977088
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  501125250.0
relative error loss 0.20298712
shape of L is 
torch.Size([])
memory (bytes)
4857192448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  8% |
memory (bytes)
4857192448
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  501065600.0
relative error loss 0.20296296
time to take a step is 290.9030055999756
it  12 : 1577498624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4860305408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4860416000
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  501065600.0
relative error loss 0.20296296
shape of L is 
torch.Size([])
memory (bytes)
4863627264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  7% |
memory (bytes)
4863627264
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  501052670.0
relative error loss 0.20295772
shape of L is 
torch.Size([])
memory (bytes)
4866859008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4866859008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% |  8% |
error is  500989700.0
relative error loss 0.20293221
shape of L is 
torch.Size([])
memory (bytes)
4870049792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  8% |
memory (bytes)
4870049792
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  500966270.0
relative error loss 0.20292273
shape of L is 
torch.Size([])
memory (bytes)
4873293824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% |  8% |
memory (bytes)
4873293824
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  500919300.0
relative error loss 0.2029037
shape of L is 
torch.Size([])
memory (bytes)
4876378112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  8% |
memory (bytes)
4876517376
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  500860670.0
relative error loss 0.20287995
shape of L is 
torch.Size([])
memory (bytes)
4879724544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% |  8% |
memory (bytes)
4879724544
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  500919680.0
relative error loss 0.20290385
shape of L is 
torch.Size([])
memory (bytes)
4882878464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  8% |
memory (bytes)
4882878464
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  500828540.0
relative error loss 0.20286694
shape of L is 
torch.Size([])
memory (bytes)
4886069248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  8% |
memory (bytes)
4886159360
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  500778100.0
relative error loss 0.20284651
shape of L is 
torch.Size([])
memory (bytes)
4889378816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  8% |
memory (bytes)
4889378816
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  500739330.0
relative error loss 0.20283079
time to take a step is 290.1949324607849
it  13 : 1577498624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4892520448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  7% |
memory (bytes)
4892520448
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  500739330.0
relative error loss 0.20283079
shape of L is 
torch.Size([])
memory (bytes)
4895817728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4895817728
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  500660860.0
relative error loss 0.20279902
shape of L is 
torch.Size([])
memory (bytes)
4898951168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  8% |
memory (bytes)
4898951168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% |  8% |
error is  500608900.0
relative error loss 0.20277797
shape of L is 
torch.Size([])
memory (bytes)
4902252544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4902252544
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  500571260.0
relative error loss 0.20276272
shape of L is 
torch.Size([])
memory (bytes)
4905472000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  8% |
memory (bytes)
4905472000
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  500539900.0
relative error loss 0.20275001
shape of L is 
torch.Size([])
memory (bytes)
4908531712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4908683264
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  500496000.0
relative error loss 0.20273224
shape of L is 
torch.Size([])
memory (bytes)
4911894528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% |  8% |
memory (bytes)
4911894528
| ID | GPU  | MEM |
-------------------
|  0 |   6% |  0% |
|  1 | 100% |  8% |
error is  500451460.0
relative error loss 0.20271419
shape of L is 
torch.Size([])
memory (bytes)
4915113984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  8% |
memory (bytes)
4915113984
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  500414340.0
relative error loss 0.20269915
shape of L is 
torch.Size([])
memory (bytes)
4918267904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4918341632
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  500398080.0
relative error loss 0.20269257
shape of L is 
torch.Size([])
memory (bytes)
4921552896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  8% |
memory (bytes)
4921552896
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  500372480.0
relative error loss 0.2026822
time to take a step is 292.41342854499817
it  14 : 1577498624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4924657664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4924764160
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  500372480.0
relative error loss 0.2026822
shape of L is 
torch.Size([])
memory (bytes)
4927934464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  7% |
memory (bytes)
4927987712
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  500348540.0
relative error loss 0.20267251
shape of L is 
torch.Size([])
memory (bytes)
4931104768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% |  8% |
memory (bytes)
4931104768
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  500311940.0
relative error loss 0.20265768
shape of L is 
torch.Size([])
memory (bytes)
4934430720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4934430720
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  500213500.0
relative error loss 0.20261781
shape of L is 
torch.Size([])
memory (bytes)
4937609216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  8% |
memory (bytes)
4937609216
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  500201730.0
relative error loss 0.20261304
shape of L is 
torch.Size([])
memory (bytes)
4940865536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4940865536
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  500155650.0
relative error loss 0.20259437
shape of L is 
torch.Size([])
memory (bytes)
4944023552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4944093184
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  500128400.0
relative error loss 0.20258333
shape of L is 
torch.Size([])
memory (bytes)
4947197952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% |  8% |
memory (bytes)
4947304448
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  500099330.0
relative error loss 0.20257156
shape of L is 
torch.Size([])
memory (bytes)
4950519808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  8% |
memory (bytes)
4950523904
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  500049020.0
relative error loss 0.20255119
shape of L is 
torch.Size([])
memory (bytes)
4953669632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4953669632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  500039800.0
relative error loss 0.20254745
time to take a step is 293.21963262557983
sum tnnu_Z after tensor(8184377., device='cuda:0')
shape of features
(4442,)
shape of features
(4442,)
number of orig particles 17768
number of new particles after remove low mass 16491
tnuZ shape should be parts x labs
torch.Size([17768, 702])
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
error without small mass is  587228400.0
relative error without small mass is  0.2378643
nnu_Z shape should be number of particles by maxV
(17768, 702)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
shape of features
(17768,)
Thu Feb 2 01:35:46 EST 2023
