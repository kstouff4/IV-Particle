Tue Jan 31 14:58:14 EST 2023
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  0% |  0% |
numbers of X: 33623890
numbers of Z: 20842
shape of features
(20842,)
shape of features
(20842,)
ZX	Vol	Parts	Cubes	Eps
Z	0.01900942688320141	20842	20.842	0.09697874150626114
X	0.016568942136944483	829	0.829	0.27138140312786
X	0.017072396063941637	17014	17.014	0.10011427722532766
X	0.017046769137403682	1430	1.43	0.2284381743657149
X	0.01680305963467785	3536	3.536	0.16812231859909346
X	0.017385866197038413	73354	73.354	0.061885776423058265
X	0.017090253162505713	48155	48.155	0.07080039863717008
X	0.017572936592389073	47664	47.664	0.07170530466122133
X	0.017132515342145577	41421	41.421	0.07450753290651163
X	0.016989758925986885	20112	20.112	0.0945316674992286
X	0.017119475900734482	35722	35.722	0.07825610585594868
X	0.018434587149872013	16800	16.8	0.1031433844988471
X	0.017119394037761443	46320	46.32	0.07176399586459314
X	0.01793075479282086	5534	5.534	0.14797436066275851
X	0.01714293977111865	185563	185.563	0.045206196804098986
X	0.017036291865761267	17130	17.13	0.09981731946013274
X	0.01710631379552742	27846	27.846	0.0850089277971336
X	0.017127712884822253	42193	42.193	0.07404339487541392
X	0.017104286522941055	41850	41.85	0.0742112648221239
X	0.017545775969075795	111175	111.175	0.05404080552104195
X	0.017108784122731514	113974	113.974	0.05314606552900647
X	0.01714530425470813	25426	25.426	0.08769114038058556
X	0.017547824003953105	153234	153.234	0.04856119631879859
X	0.017079780890878166	10629	10.629	0.11712869268198958
X	0.017217567400749516	12171	12.171	0.1122574509644716
X	0.01744863916932	16646	16.646	0.10158210867242812
X	0.017879269718901348	50760	50.76	0.0706224404119963
X	0.017454113088556922	61594	61.594	0.06568302194694628
X	0.017223853556980695	14152	14.152	0.10676711348703749
X	0.017972548680765397	145789	145.789	0.049769311999837744
X	0.01808455132254452	863830	863.83	0.02756088455302964
X	0.017018214522734453	10582	10.582	0.11716073266582214
X	0.017957249359941075	792996	792.996	0.028291519421305325
X	0.016896689598750625	6103	6.103	0.14041691222836689
X	0.017079231389021534	20829	20.829	0.09359801407178817
X	0.017094066513040514	35139	35.139	0.07864757395884468
X	0.018049724025847657	342389	342.389	0.037495883184581566
X	0.017643874708743023	58182	58.182	0.0671843994937573
X	0.016274578660490365	1134	1.134	0.2430145254869766
X	0.01693207560685183	2779	2.779	0.18264476017859105
X	0.016844155404940022	2671	2.671	0.1847531211152075
X	0.01681930887096579	2758	2.758	0.1826997535028621
X	0.015465326083713728	690	0.69	0.281948635344732
X	0.015981994822682085	795	0.795	0.2719075618600581
X	0.01670677020206309	1863	1.863	0.20775901680390826
X	0.017088367814804134	779	0.779	0.2799331784013424
X	0.016638034726167053	1021	1.021	0.25352770675780345
X	0.016954430360050825	1821	1.821	0.21037352580467514
X	0.01701827635745868	5869	5.869	0.14259922135240044
X	0.016989769480030167	3720	3.72	0.16591341995609743
X	0.017624605282822836	5657	5.657	0.14605319893160812
X	0.017522868039711138	10165	10.165	0.11990370487895351
X	0.01599483466699348	867	0.867	0.2642328967714189
X	0.017006378656097203	6634	6.634	0.13686071119992665
X	0.01667695952525145	4542	4.542	0.15427299645982634
X	0.016904302931491453	3623	3.623	0.1670999465335345
X	0.01627195967120468	2699	2.699	0.1820031864586887
X	0.016444742182119637	1562	1.562	0.21917045699179408
X	0.01698700746907248	4470	4.47	0.15605216019007892
X	0.01711514794342037	2464	2.464	0.19080070922239
X	0.01591088864142059	2202	2.202	0.1933269386236795
X	0.017319483227927843	5750	5.75	0.1444183442847551
X	0.01711223335911487	1320	1.32	0.23491509321863793
X	0.01718204639180354	3273	3.273	0.1737972741530132
X	0.016949860459477058	10805	10.805	0.11619313030667196
X	0.016955481502029073	4654	4.654	0.1538725848773174
X	0.01744869581384835	2191	2.191	0.19969791537450604
X	0.01672993667524928	2744	2.744	0.18268521333777965
X	0.01639357089987854	1727	1.727	0.21173553076098603
X	0.017115294848320504	3247	3.247	0.17403397197373102
X	0.016788794832987848	3295	3.295	0.17207642219500083
X	0.01686035978527087	2986	2.986	0.17807065034433334
X	0.01835997934113566	3387	3.387	0.17566517142706267
X	0.016952869356942495	1937	1.937	0.20608095812904287
X	0.01665815633194318	2187	2.187	0.19675512347887167
X	0.016728121947634813	4437	4.437	0.15563929408854588
X	0.016957809672011995	11200	11.2	0.1148287343363664
X	0.016014137674575227	705	0.705	0.2832075412847517
X	0.016908296499160192	891	0.891	0.26673134215769356
X	0.016944573296930006	2814	2.814	0.1819291042539166
X	0.01655957293610796	9113	9.113	0.12202886934689956
X	0.016160181691632462	1676	1.676	0.21284203781396305
X	0.015736818475476123	1124	1.124	0.2410184049638228
X	0.016633927115028126	1415	1.415	0.22737678324126084
X	0.01636787702481119	1402	1.402	0.22685487332390245
X	0.01655666161776542	1188	1.188	0.24064976619105874
X	0.016961064033648224	1157	1.157	0.2447409143755581
X	0.01633809833488866	1659	1.659	0.21434747563701467
X	0.015886496988634802	876	0.876	0.26272900003344
X	0.016907832741414316	2355	2.355	0.19291485780356527
X	0.0170827269419908	4438	4.438	0.1567195948890839
X	0.016994669075834325	4958	4.958	0.15077710370005973
X	0.01706010381280117	1629	1.629	0.21878633592209504
X	0.01696695119540716	1337	1.337	0.23325133304907192
X	0.016902007924879905	9790	9.79	0.11996425732694366
X	0.018294731332829554	3556	3.556	0.17263205004590587
X	0.017018169399345445	4554	4.554	0.15518149756823946
X	0.01721972395170396	2114	2.114	0.20120575601403656
X	0.0168893018480032	2372	2.372	0.19238254734564758
X	0.01706312342918594	7106	7.106	0.13390941405999754
X	0.01708197110857811	4540	4.54	0.1555347289577542
X	0.01693434501891042	9987	9.987	0.11924617335834602
X	0.017198184870496465	5457	5.457	0.1466142623750852
X	0.0175198001294863	4202	4.202	0.16095001929339112
X	0.016646095452645202	1078	1.078	0.2490182671158037
X	0.016990017016278374	7304	7.304	0.13249851669831017
X	0.016415862347343078	1408	1.408	0.22675333452853572
X	0.01843153459002924	2914	2.914	0.18493707426189698
X	0.01655032822683504	1135	1.135	0.2443075657871243
X	0.017071306455214767	1846	1.846	0.20989963690434435
X	0.017197078386962952	627	0.627	0.3015752593674204
X	0.016755678082658147	2604	2.604	0.1859974582228809
X	0.01666166239432749	997	0.997	0.25566683235754245
X	0.016420107481614828	1769	1.769	0.2101596747028335
X	0.01711698933513966	2211	2.211	0.19782425817477892
X	0.017410708627082896	5310	5.31	0.14856185338823955
X	0.016075375019683542	2143	2.143	0.19575505276978555
X	0.017277671334858014	1653	1.653	0.21864394341542046
X	0.01643691121527517	643	0.643	0.2945798785562794
X	0.01742259809606268	7248	7.248	0.13395684528737647
X	0.01682465601969249	777	0.777	0.2787244443478522
X	0.01706399095331089	7571	7.571	0.13111200395805966
X	0.016561283212422194	1300	1.3	0.23355197485112011
X	0.01695006181502612	2073	2.073	0.2014608497393238
X	0.01711123151340816	4473	4.473	0.1563966536352151
X	0.017089226973515313	3611	3.611	0.16789263421193754
X	0.016782184934786826	2173	2.173	0.19766491593536872
X	0.016932208270900133	980	0.98	0.2585210088428229
X	0.017511038867309054	3464	3.464	0.1716239959669898
X	0.017566212851212657	5660	5.66	0.1458659427360323
X	0.016625029096575587	2203	2.203	0.1961474375711802
X	0.016934846950091814	10121	10.121	0.11871874138623563
X	0.01704392330504901	872	0.872	0.2693711179992838
X	0.017093454515880315	2845	2.845	0.18179524650952372
X	0.016468940001168206	907	0.907	0.262836769145911
X	0.01677804306579409	911	0.911	0.26408332287562575
X	0.018130647563131815	2321	2.321	0.19841723430265187
X	0.016661450657419686	1523	1.523	0.22199215951709317
X	0.01682186052121796	1368	1.368	0.23081426364357618
X	0.016552170524673275	2350	2.35	0.19168834003624696
X	0.0170676452609076	1793	1.793	0.21193260323522944
X	0.01689840595544705	1911	1.911	0.20678943693411214
X	0.016681169832990336	1463	1.463	0.22507506934459204
X	0.016158784203642872	5373	5.373	0.14434351609905866
X	0.017102311174051782	10531	10.531	0.1175425379810691
X	0.0170294765502036	3347	3.347	0.17199475193431268
X	0.01667517685467038	1554	1.554	0.2205666318679976
X	0.016961455683267946	1437	1.437	0.22768554065298022
X	0.01662008517999286	1036	1.036	0.2522074028594759
X	0.016785807977147914	1644	1.644	0.21694358493598231
X	0.016814451546342	1482	1.482	0.22470436081379072
X	0.016789460103126288	1882	1.882	0.2073985443425031
X	0.0168466557701131	2660	2.66	0.18501659687364844
X	0.016994190740665598	2766	2.766	0.1831538719275414
X	0.017020362317568146	3685	3.685	0.1665368846526429
X	0.01704349329708236	2546	2.546	0.18846617000379148
X	0.017529861882904178	8059	8.059	0.12956840507667122
X	0.01671218256789295	2840	2.84	0.18053924021315387
X	0.016698672538432255	1635	1.635	0.21696419220711785
X	0.01729176200840551	2001	2.001	0.20520946445809612
X	0.016031895823590176	517	0.517	0.3141700489327912
X	0.017275935378640125	7621	7.621	0.13136405211266078
X	0.01624610465007532	809	0.809	0.27181092485227
X	0.01725973169595603	6937	6.937	0.1355046239391676
X	0.015688389163765772	446	0.446	0.3276532637944314
X	0.016961495662290167	3462	3.462	0.1698422341543527
X	0.01691291212842812	3497	3.497	0.16911193288602977
X	0.017070844521620433	1292	1.292	0.23640908625718618
X	0.01682525626765798	2381	2.381	0.19189666661048693
X	0.016860694052078268	2145	2.145	0.19883036837297696
X	0.017001821469039606	705	0.705	0.2889141271000654
X	0.016164271823946835	632	0.632	0.2946318474328512
X	0.01637440937788216	1578	1.578	0.21811534229427917
X	0.017039369698197724	3427	3.427	0.1706788889500412
X	0.016982689938067078	1328	1.328	0.23384933280787334
X	0.017305896789040178	4925	4.925	0.15202999859350536
X	0.01706079292039834	1856	1.856	0.20947896200717928
X	0.016815461839746398	2071	2.071	0.20099082537187524
X	0.016933937250878703	3250	3.25	0.17336371107868845
X	0.016957240120683628	3584	3.584	0.16787874850650306
X	0.016730292453494836	1701	1.701	0.21425603963162015
X	0.017299512714993796	6090	6.09	0.14162465313916112
X	0.017024897704529036	1115	1.115	0.24808654709526468
X	0.016944707812239276	2728	2.728	0.18382161307665898
X	0.0169345951915061	1452	1.452	0.2267789470065067
X	0.016712779964903685	3764	3.764	0.16436134766493354
X	0.016996389310739712	5352	5.352	0.14698742727556974
X	0.016650652563454348	1787	1.787	0.21042737396881384
X	0.01703215882985806	2465	2.465	0.19046605717499426
X	0.017879105871222236	4078	4.078	0.16366891053706775
X	0.016856431910494064	9243	9.243	0.12217553023791175
X	0.016921904852109582	1873	1.873	0.20827500671150592
X	0.016989023790876072	904	0.904	0.2658683649205833
X	0.016524183113920614	2351	2.351	0.19155307287830511
X	0.016988841241788975	2341	2.341	0.19360687782149166
X	0.016579255012019314	2959	2.959	0.17761242702621294
X	0.01698935982754396	1455	1.455	0.22686700641873983
X	0.016950653500369502	3075	3.075	0.1766500543474202
X	0.016870672725449072	1329	1.329	0.23327550587005522
X	0.016398510829320007	4392	4.392	0.15513651332900916
X	0.016630494845262776	701	0.701	0.28733975557965763
X	0.017754664747270364	6953	6.953	0.13668265795815984
X	0.0169403778831424	1987	1.987	0.20428747577582365
X	0.017263751840814124	12089	12.089	0.11261120413227324
X	0.01680397353048123	1708	1.708	0.2142765806666077
X	0.017012024761764683	6682	6.682	0.13654731924065844
X	0.016340003570944978	1309	1.309	0.23197303265084582
X	0.016868832202209073	1451	1.451	0.22653702974507064
X	0.01727377683022691	2868	2.868	0.18194329371489024
X	0.01648478563482725	1927	1.927	0.2045190953886099
X	0.016544218227275538	3903	3.903	0.16183876894965632
X	0.01695935026295072	4152	4.152	0.15985171068365234
X	0.016553232405873915	1878	1.878	0.20656769726915636
X	0.016921683517185204	2036	2.036	0.20256073531361998
X	0.016922139454383266	4977	4.977	0.15037046509333657
X	0.016963618735565327	631	0.631	0.29956866307173124
X	0.016196215155891036	496	0.496	0.31962736437896927
X	0.017084227415711997	398	0.398	0.35013653044001386
X	0.017160417745484373	1482	1.482	0.22623504697222335
X	0.016719387733178246	10951	10.951	0.11514776960554328
X	0.016881414037135473	1390	1.39	0.2298606729262197
X	0.016543734094287593	1404	1.404	0.22755627246671667
X	0.017065904047593523	759	0.759	0.2822468960318815
X	0.016787019265710097	8032	8.032	0.12785468002635905
X	0.016617916010560112	1194	1.194	0.24054190399805953
X	0.01687340499089238	1714	1.714	0.2143206269092621
X	0.016765028891150835	2600	2.6	0.18612740370465974
X	0.016737729886628857	2218	2.218	0.19614549493196112
X	0.01704390930986136	1725	1.725	0.21458204098998523
X	0.017256280357266823	3823	3.823	0.16526502868104273
X	0.015859969130671104	1045	1.045	0.24758748165693784
X	0.01674009289168104	1058	1.058	0.25104836577164874
X	0.016708838660529593	1525	1.525	0.22210524130548104
X	0.017302450402456485	1474	1.474	0.22726717304191094
X	0.017738246155805677	2540	2.54	0.19114325919366135
X	0.01711598448293461	2771	2.771	0.18347988225897802
X	0.016863061600624518	1768	1.768	0.2120726786647705
X	0.0184655396309544	6246	6.246	0.1435225057174538
X	0.017133765834025035	2725	2.725	0.18457042270250762
X	0.017005605809023074	1253	1.253	0.2385321992151761
X	0.01699831610215132	6918	6.918	0.13494033412026463
X	0.017526266312920938	10752	10.752	0.1176883254926339
X	0.017599501321731686	7086	7.086	0.1354252787292808
X	0.01732003618883777	4417	4.417	0.157691081581249
X	0.016796952706512065	2248	2.248	0.19549908578523462
X	0.017017019922599887	6621	6.621	0.13697878398132787
X	0.016968883595463738	9652	9.652	0.12069204677671588
X	0.017041525238126243	21452	21.452	0.09261476404181426
X	0.017243195713627785	2411	2.411	0.19266678438793036
X	0.017125716295436733	128892	128.892	0.05102788806640379
X	0.01698468044158191	50874	50.874	0.06937247113067331
X	0.0163962365032363	30152	30.152	0.08162239800502663
X	0.017405900686818052	33337	33.337	0.08052362705257507
X	0.01645358499340377	1606	1.606	0.21718925353258978
X	0.01704860734652942	6838	6.838	0.13559799584980778
X	0.017487309858401773	290386	290.386	0.03919666437983343
X	0.01714593204246694	224425	224.425	0.04243230473947582
X	0.01758903386410796	2803	2.803	0.18444758037631606
X	0.01709086958463729	27080	27.08	0.08577719364300043
X	0.017625773029938212	88698	88.698	0.05835500563739444
X	0.016993325614780626	19598	19.598	0.09535765048151618
X	0.01691928154887867	19166	19.166	0.09592905020751154
X	0.016805990354938707	23369	23.369	0.08959311993654913
X	0.01737964421061276	10846	10.846	0.11701919166361303
X	0.017058899036768535	11332	11.332	0.11460796758838668
X	0.016786262335494	3544	3.544	0.16793972238002658
X	0.01753672816005909	44169	44.169	0.07349825041178175
X	0.017070753430969348	9302	9.302	0.12243120334844494
X	0.01695345634959364	3885	3.885	0.16341393067996415
X	0.01702684638759504	17489	17.489	0.09911127887558739
X	0.016185033974347767	15305	15.305	0.10188105350355833
X	0.018444322809919565	191490	191.49	0.04583939444483197
X	0.017081445123489233	45119	45.119	0.0723416452341542
X	0.016955822788256452	3195	3.195	0.17442792006313984
X	0.01696512400006766	31703	31.703	0.08118682529196934
X	0.017857176375444716	5964	5.964	0.1441313267730522
X	0.017120599649081712	10329	10.329	0.11834599484306274
X	0.01714565746370918	42035	42.035	0.07416193120197748
X	0.01740033210042788	36793	36.793	0.07791076086020174
X	0.017331988680435714	98543	98.543	0.05602831208182234
X	0.01565700744410396	802	0.802	0.2692638988416585
X	0.016748872694905433	4228	4.228	0.15822807129212405
X	0.017123580883204315	58587	58.587	0.06636377921248632
X	0.016980560486253047	37320	37.32	0.07691365715052412
X	0.01711877662795245	22399	22.399	0.09142845167472681
X	0.017231940781705153	2842	2.842	0.18234899467092464
X	0.017124229256822394	162537	162.537	0.04723017969971141
X	0.0171173562609255	10441	10.441	0.11791386219226066
X	0.01710297733145774	15690	15.69	0.10291600644809522
X	0.0172175167533944	68546	68.546	0.0630951725733229
X	0.017016411472038695	1869	1.869	0.2088107688279358
X	0.016994759719311286	41078	41.078	0.07451354851435069
X	0.017936862964354386	66628	66.628	0.06456986886150115
X	0.01758336761434852	229923	229.923	0.04244630393678885
X	0.018322632866387134	35263	35.263	0.08039385869690036
X	0.016934030928621058	31830	31.83	0.08102914132672942
X	0.01617510633458029	7842	7.842	0.12729365857171082
X	0.01710747721341591	4543	4.543	0.15557784246938672
X	0.01759188970123418	31308	31.308	0.08251870226822874
X	0.017084328382859296	6927	6.927	0.13510898798322554
X	0.01708752908697211	9902	9.902	0.11994591127219936
X	0.017507746197346147	43024	43.024	0.07410368741576641
X	0.017146370883708816	15511	15.511	0.10339768259203445
X	0.01705417000914642	6747	6.747	0.13621971248628148
X	0.017015244437930482	4055	4.055	0.16129309590982094
X	0.01735384126224582	264441	264.441	0.04033563916760852
X	0.017143804843623517	6644	6.644	0.1371594905965918
X	0.016924317328037407	4267	4.267	0.15829341147788367
X	0.018031955653768562	11143	11.143	0.11740328081640468
X	0.01745698138401472	104514	104.514	0.055072090173217535
X	0.017089164244134297	19655	19.655	0.09544413768228205
X	0.01722595030976685	13272	13.272	0.1090809568866882
X	0.017089143379757282	4749	4.749	0.15324019924828586
X	0.016718073992815323	13359	13.359	0.10776327077592976
X	0.017125606612650064	9499	9.499	0.12170899940811909
X	0.0175317578387732	453839	453.839	0.03380450258194947
X	0.017075889499410628	30255	30.255	0.08264096747835754
X	0.017228361275835175	10146	10.146	0.11930254385114009
X	0.017118685264000004	90291	90.291	0.05744805304934825
X	0.017150415210094968	105424	105.424	0.05458982677222717
X	0.01714431916838878	5010	5.01	0.15069337943645547
X	0.017065008226098466	74217	74.217	0.0612633915232432
X	0.018457107966654583	121754	121.754	0.05332040893646733
X	0.017966163938865107	371246	371.246	0.036441635377132485
X	0.017038926476896084	9609	9.609	0.12103789026953095
X	0.016932115425500837	1897	1.897	0.20743464693873948
X	0.017029264148775574	8350	8.35	0.12681475943715992
X	0.0171439079586533	13137	13.137	0.10927929272193695
X	0.016989339845392044	24268	24.268	0.0887934313321826
X	0.0171155173116476	19041	19.041	0.09650876520650567
X	0.01647032391793895	1432	1.432	0.22572848883181193
X	0.01755296455820115	25255	25.255	0.08857973316498338
X	0.017103710335128062	90546	90.546	0.057377332581991784
X	0.016463628039393517	77427	77.427	0.05968671875031218
X	0.017229081220226598	5161	5.161	0.14945464584028226
X	0.016984719017573032	108529	108.529	0.05388950425480006
X	0.017285752629502545	8672	8.672	0.12585090458358783
X	0.0169425054421038	14374	14.374	0.10563309333713763
X	0.017103003264152164	11637	11.637	0.1136956323415599
X	0.017082570106922233	2612	2.612	0.18700792570609737
X	0.01715677579745433	130408	130.408	0.0508600926753674
X	0.017151366389276786	32587	32.587	0.0807392630930653
X	0.01751874803660667	11358	11.358	0.11554036469514498
X	0.01737821022217071	19763	19.763	0.09580409188895182
X	0.016941872290372258	4237	4.237	0.1587209715620606
X	0.017564517638976776	237052	237.052	0.04200144203338619
X	0.01686845782428368	17590	17.59	0.09861353185549693
X	0.017110843983896874	139674	139.674	0.049665173111714186
X	0.016943119497630547	1857	1.857	0.20895871555475978
X	0.016975075394903078	3980	3.98	0.16217212716729498
X	0.016988032356951364	3917	3.917	0.1630784228376371
X	0.0171540788111748	17356	17.356	0.09961068344139107
X	0.01694486507924985	4174	4.174	0.15952492783081768
X	0.016873247205905707	9713	9.713	0.12021216723272303
X	0.016793308805055866	5575	5.575	0.1444210397896237
X	0.01696192481086341	4383	4.383	0.15700056706479015
X	0.018085844501593968	144988	144.988	0.049965347377987045
X	0.017321521165586204	21911	21.911	0.09246441152127262
X	0.01812028942088173	23460	23.46	0.09175133120664612
X	0.01748715845240942	87176	87.176	0.05853838937881282
X	0.01788219238330178	275371	275.371	0.04019458234930542
X	0.01722438222548067	6809	6.809	0.13625531122371376
X	0.01708463142606989	10049	10.049	0.11935141967362403
X	0.01708918378469967	6363	6.363	0.13900159992190544
X	0.016586181531542524	917	0.917	0.26249781508419445
X	0.017211686948066768	9242	9.242	0.1230323042958236
X	0.017102988774695578	16282	16.282	0.10165328554954302
X	0.016092542607418294	7119	7.119	0.13124070608017938
X	0.01704163091569991	4842	4.842	0.15211152604179246
X	0.016761252258147223	17815	17.815	0.09798814703639497
X	0.017094235451844568	12797	12.797	0.1101320788912791
X	0.017405781813290416	751875	751.875	0.028500291162775627
X	0.016915828913889396	3745	3.745	0.16530290557030766
X	0.01760125381605136	40381	40.381	0.07582088430498019
X	0.017014738393678706	13824	13.824	0.10716768524307871
X	0.01751726918403386	1768	1.768	0.21478043337828656
X	0.017112926264594548	105736	105.736	0.05449631467748103
X	0.017160262716827968	180668	180.668	0.04562619285413645
X	0.017377568159916427	371920	371.92	0.0360174660777117
X	0.017136841798208145	32304	32.304	0.08095148660824081
X	0.017115510423981378	20969	20.969	0.09345532351179042
X	0.017044480086253633	2638	2.638	0.18625287662831774
X	0.01708698636442614	38606	38.606	0.07620852490829237
X	0.01712191463641176	738411	738.411	0.028515745056171264
X	0.01749561687370639	33560	33.56	0.08048268116904514
X	0.016899314185788373	9960	9.96	0.11927147239640586
X	0.01699522717061565	51980	51.98	0.06889117090809442
X	0.017811355668307226	243969	243.969	0.04179465808001311
X	0.016599028498289748	43818	43.818	0.07235635927328764
X	0.01694939780720021	10011	10.011	0.11918609856353568
X	0.0182531372296329	3290	3.29	0.1770301376245852
X	0.017104259613347557	22553	22.553	0.09119408145230544
X	0.017044737264636747	8900	8.9	0.12418432131847669
X	0.01721787448501596	275372	275.372	0.0396905014890506
X	0.01746142384955909	10254	10.254	0.11941588834317193
X	0.0166973373826794	1560	1.56	0.2203810297699838
X	0.017327767249185656	27890	27.89	0.08532927005569456
X	0.0188064215640353	29932	29.932	0.08564923273657936
X	0.017007995914625075	1492	1.492	0.22505816483925284
X	0.017533963003163566	65078	65.078	0.0645875230321631
X	0.017413405275223608	49567	49.567	0.07056083169234756
X	0.01765840835177117	66867	66.867	0.06415737289865392
X	0.01744205904950249	96444	96.444	0.05655107252872602
X	0.017506774867205872	72612	72.612	0.062239474327464774
X	0.017131582315187587	15898	15.898	0.10252229706812271
X	0.017089949788300605	43231	43.231	0.07339196264230596
X	0.017162170673240152	54141	54.141	0.06818392240270743
X	0.01750133783495369	178732	178.732	0.046091726660273634
X	0.01692142933004059	5461	5.461	0.14578794781730198
X	0.01715244563323074	70956	70.956	0.062293917323367906
X	0.017407170824988473	62322	62.322	0.0653675593328276
X	0.017146952434865345	44476	44.476	0.07278140032011844
X	0.017155864021482305	18151	18.151	0.09813803234500662
X	0.016859004546243308	13342	13.342	0.10811111102548394
X	0.017089008935353065	9835	9.835	0.12022113880242588
X	0.016920747346655374	7378	7.378	0.13187433925188088
X	0.016650816810415834	81520	81.52	0.058892111265665986
X	0.01795404753151968	140217	140.217	0.05040271140594239
X	0.017154102427515226	93217	93.217	0.056879743028028944
X	0.017190485990942438	119515	119.515	0.05239484738050893
X	0.018571452821180678	64504	64.504	0.06603179109062583
X	0.018170794480057033	74695	74.695	0.062425322535179774
X	0.017070931336245893	9560	9.56	0.12132019724968887
X	0.01738402590471752	180583	180.583	0.04583084159355781
X	0.016127042206216337	1181	1.181	0.2390203850251168
X	0.016763483772741318	11456	11.456	0.1135299442191528
X	0.017571174222468015	182769	182.769	0.0458106108795469
X	0.017104698255697916	31111	31.111	0.0819219878746057
X	0.016889500541553287	2010	2.01	0.20330134402893107
X	0.017123644983615953	15658	15.658	0.1030275349193214
X	0.01776422763876373	110126	110.126	0.054435910108532735
X	0.01719866046773645	23458	23.458	0.09017120643257734
X	0.017844045153629022	45876	45.876	0.07299659287131498
X	0.016246984015275093	1442	1.442	0.22418290761946574
X	0.017085308235401647	4052	4.052	0.16155402918579054
X	0.016965865093211033	3153	3.153	0.17523358506710512
X	0.016998808354010606	28628	28.628	0.08405092352687504
X	0.017900256909927286	34218	34.218	0.08057522879870258
X	0.017037296063839576	51096	51.096	0.069343313330299
X	0.01700894960197062	2025	2.025	0.2032750869902219
X	0.017140606444730516	60566	60.566	0.06565469131052203
X	0.0168469825011999	9736	9.736	0.12005507402013191
X	0.017110536557126764	28484	28.484	0.08437637900672991
X	0.01714355180500798	15983	15.983	0.10236405973158923
X	0.01713051268113847	86344	86.344	0.05832383404539209
X	0.016696314735939282	2442	2.442	0.18979800991467355
X	0.01722750588313079	48913	48.913	0.07062081124929957
X	0.017115511637000938	21162	21.162	0.09317034965835588
X	0.017066125160124467	13574	13.574	0.10793004558769767
X	0.017539002816419066	26418	26.418	0.08723718717529406
X	0.01719454395569648	194724	194.724	0.044530447532266836
X	0.017138285632326722	10800	10.8	0.11664009742434049
X	0.0168548114818791	9093	9.093	0.1228397440191486
X	0.01731938071013199	93440	93.44	0.057016406620834716
X	0.01688589634937492	19036	19.036	0.09608364596443286
X	0.017184027980505515	387505	387.505	0.035395595242286436
X	0.016856409640909686	6432	6.432	0.13787102796821116
X	0.01713278611431859	116801	116.801	0.05273842358307584
X	0.01805053302933401	288639	288.639	0.03969279427817137
X	0.017452606738089394	5883	5.883	0.14368799501747204
X	0.01703202638675029	75399	75.399	0.06090228375883252
X	0.017086973674505925	13843	13.843	0.10727000737380307
X	0.01761763952424356	157676	157.676	0.048164535378139535
X	0.016994543313420634	16366	16.366	0.10126413342308205
X	0.017028192811091757	21233	21.233	0.09290784779220904
X	0.016955660199966673	1602	1.602	0.21955874056770175
X	0.017024442619880308	3872	3.872	0.16382462573416107
X	0.017542846410186233	4708	4.708	0.15503194737980916
X	0.01712969225894347	18711	18.711	0.09709961784750537
X	0.016829144252693367	20440	20.44	0.09372606425749577
X	0.017567802991458752	105113	105.113	0.05508334943649401
X	0.017091925696098315	15980	15.98	0.10226760226205531
X	0.01654603696408514	3875	3.875	0.1622335834665003
X	0.017082932739528517	9410	9.41	0.12199001063618906
X	0.01862782699945989	19916	19.916	0.09779576000601242
X	0.017643029940350167	9322	9.322	0.12369572512445659
X	0.017616982260406096	136372	136.372	0.05055166166994215
X	0.017086436170624688	32672	32.672	0.08056725913505124
X	0.017110435444079475	3095	3.095	0.17682084821689872
X	0.017454826240536965	4923	4.923	0.1524855058100348
X	0.016249958412671597	4898	4.898	0.14914584454520663
X	0.017575147983623304	433150	433.15	0.03436266914336383
X	0.01712624421857024	7070	7.07	0.13430151330433915
X	0.01759151644053383	15363	15.363	0.10461864303157015
X	0.016998649717162483	46194	46.194	0.07165991437373906
X	0.01650547827394184	13220	13.22	0.10767929894285677
X	0.017130600147899955	5935	5.935	0.1423805342820216
X	0.018284191877771746	9076	9.076	0.1262972112295912
X	0.01754342763761754	81214	81.214	0.06000137224343948
X	0.017061560199254432	50756	50.756	0.06953078267181381
X	0.016492415031705156	29707	29.707	0.08218801402251795
X	0.017126186444853773	9289	9.289	0.1226207312161004
X	0.017134557277248195	20590	20.59	0.09406011679220126
X	0.017094226063707998	43599	43.599	0.07319099280979184
X	0.017377865235760123	44441	44.441	0.0731258399198022
X	0.017129912156421942	68989	68.989	0.06285286739815592
X	0.017062927174138647	3168	3.168	0.1752895884655749
X	0.01715206190230615	89473	89.473	0.05766001867162982
X	0.017130772605277542	3313	3.313	0.17292263238322564
X	0.01738434584117375	41518	41.518	0.07481246700391428
X	0.016980260971967514	13177	13.177	0.10882013781578341
X	0.017082602927818408	3218	3.218	0.1744439862008469
X	0.017342266273499527	15193	15.193	0.10450909911988075
X	0.016927038384388547	45129	45.129	0.07211768150961874
X	0.0169061577785646	17264	17.264	0.09930424872264106
X	0.017566322369892754	40077	40.077	0.07596179429024079
X	0.01713688604958528	5639	5.639	0.14484714454072437
X	0.017009240287542957	120375	120.375	0.05208542418632896
X	0.016915159846191997	2382	2.382	0.19221094607462536
X	0.01715164731981152	3120	3.12	0.17648877199519894
X	0.017086045009011626	7153	7.153	0.13367528140053003
X	0.016981841827193114	16033	16.033	0.10193500133872146
X	0.016797346360440064	2717	2.717	0.18353420417636312
X	0.017080642756914913	8647	8.647	0.12547183519039637
X	0.01727741383274787	22741	22.741	0.09124793800342525
X	0.017494491424006695	80952	80.952	0.060010131664858524
X	0.017435888439990894	4427	4.427	0.15792280643480774
X	0.01694632105132503	2182	2.182	0.19803421350245928
X	0.017004043966356172	4729	4.729	0.15320077716126965
X	0.01711918338272806	178401	178.401	0.04578205188801133
X	0.01811028861032857	687950	687.95	0.029747889901397484
X	0.01683213711517553	19083	19.083	0.09590270788836026
X	0.017847372636914598	6180	6.18	0.14240610414212976
X	0.01710518886351746	152373	152.373	0.04823992439664564
X	0.018037043082092218	111175	111.175	0.05454053597703418
X	0.01630843201079203	2169	2.169	0.19590741995763847
X	0.01746636032853241	7476	7.476	0.1326918882269974
X	0.01713488400030381	31325	31.325	0.08178306129871948
X	0.01752856960734709	355505	355.505	0.03666910033255684
X	0.01755293444174196	471409	471.409	0.03339262961394526
X	0.01712964983449413	116989	116.989	0.05270694189938449
X	0.017368500446617304	16038	16.038	0.10269217959788851
X	0.01708648415135928	6004	6.004	0.1417111453594002
X	0.01673805108352374	3795	3.795	0.16399516202758363
X	0.01689395767744683	57135	57.135	0.0666207815724075
X	0.017249001561885264	11960	11.96	0.11298243418341973
X	0.017038158805062705	16026	16.026	0.10206241517362777
X	0.018060047857936967	108532	108.532	0.05500308264976654
X	0.017515683694557117	327005	327.005	0.037695620821189144
X	0.0162285117494233	4766	4.766	0.1504440046805505
X	0.016773129798122354	1654	1.654	0.2164509688970938
X	0.01709852314639566	19455	19.455	0.095787562476115
X	0.01713560084620549	77496	77.496	0.060470006714796055
X	0.01692733704992706	32877	32.877	0.08014913138249045
X	0.01708627531571424	2456	2.456	0.19090018349383028
X	0.016868645364134332	1836	1.836	0.20944460452652225
X	0.017065941249761067	14372	14.372	0.10589391687164477
X	0.017055293684730705	3333	3.333	0.17232222604401531
X	0.01753599687415517	11336	11.336	0.11565299236897583
X	0.017087154631892847	3305	3.305	0.1729150370002431
X	0.017083734721873824	9963	9.963	0.11969175352060392
X	0.018213660820921428	9648	9.648	0.12359094415944887
X	0.016928150042254203	7083	7.083	0.13369979847237787
X	0.016650704508118296	2991	2.991	0.17723061052570308
X	0.017602482900768355	121978	121.978	0.05245225223099331
X	0.017183855605499662	266679	266.679	0.040090727045640255
X	0.017566104331281317	59333	59.333	0.0666489098220702
X	0.016978038624865827	24552	24.552	0.0884301199168249
X	0.017011435172454414	13014	13.014	0.10933940780223683
X	0.01748747864153075	10500	10.5	0.1185348258560016
X	0.017213951636246633	16351	16.351	0.10172915098730663
X	0.017133111995811266	114307	114.307	0.053119560812147755
X	0.01666216807242176	10364	10.364	0.1171479524340553
X	0.017581586534217236	26581	26.581	0.08712890359528808
X	0.017105876520039382	14754	14.754	0.10505382053642191
X	0.01819165240099773	681024	681.024	0.029893028118360032
X	0.01701796821805827	11849	11.849	0.1128258644258707
X	0.017076438763412746	57150	57.15	0.06685394249220956
X	0.017951980155678365	143312	143.312	0.05003531069386049
X	0.017590903886592485	58411	58.411	0.06702927233546857
X	0.01712115369999631	18317	18.317	0.09777464029044529
X	0.01881598803276861	243686	243.686	0.04258259751559129
X	0.017273771481584658	93143	93.143	0.057026797429188036
X	0.01713254457162011	21663	21.663	0.09247714713194725
X	0.016713595673642487	5939	5.939	0.14118401302492534
X	0.017073890859962543	20613	20.613	0.09391402034773333
X	0.017097720943955654	9936	9.936	0.11983275588941633
X	0.017250353737124854	59472	59.472	0.06619550308407991
X	0.017988755841050124	316368	316.368	0.03845352813313418
X	0.017289507713555065	58371	58.371	0.06665946616966259
X	0.01712685852489052	81979	81.979	0.05933692432397233
X	0.017682542353367192	14007	14.007	0.10807697803503698
X	0.01747852553668285	5377	5.377	0.1481340576116021
X	0.018027262188186295	17005	17.005	0.10196498185636234
X	0.01688579968516963	6448	6.448	0.1378369119362518
X	0.01779746130749257	192096	192.096	0.045249443796585194
X	0.017784684441882217	315056	315.056	0.038360665800129555
X	0.016989271477862545	4047	4.047	0.16131714129258815
X	0.017519291895368837	681244	681.244	0.029516936848153808
X	0.01752301352926293	49813	49.813	0.0705919804399462
X	0.01711483754507548	34148	34.148	0.07943328123784212
X	0.0169741749911235	7852	7.852	0.12930127124392693
X	0.01762529692371378	39121	39.121	0.07666117970904007
X	0.017125146648434113	122859	122.859	0.05184924550337798
X	0.017074029129594808	2255	2.255	0.19636458192462553
X	0.018020050326021365	503954	503.954	0.032944913198445176
X	0.017147170893377955	18041	18.041	0.09832047240379967
X	0.01786971928877397	23485	23.485	0.09129403210310791
X	0.017137599258261756	9272	9.272	0.1227228747830868
X	0.017051931180572962	14699	14.699	0.10507399878253983
X	0.016975687196476352	1840	1.84	0.20973448646969634
X	0.016911283456489134	12495	12.495	0.11061485036315065
X	0.017558664041162287	10401	10.401	0.11907085103031906
X	0.017251778045796657	30019	30.019	0.08314048374457918
X	0.018905454090247187	384998	384.998	0.03661925854277627
X	0.01673947686536468	15383	15.383	0.10285694329815004
X	0.017125624824136453	215197	215.197	0.04301336350592661
X	0.01706690453792886	5434	5.434	0.14644629038004653
X	0.016941559414584917	16508	16.508	0.10086789886720605
X	0.016684812515146023	4020	4.02	0.16070587704103978
X	0.017328810663905292	28198	28.198	0.08501916137705469
X	0.01699087467533195	88930	88.93	0.057595578939201364
X	0.01739843405372245	911067	911.067	0.02672924027879592
X	0.017079775872958194	18103	18.103	0.09807926561531073
X	0.01759534784326991	121859	121.859	0.052462230227208875
X	0.017820531699302738	9292	9.292	0.12424258692548952
X	0.017327522222266772	107678	107.678	0.05439218367146542
X	0.017485029860200892	106171	106.171	0.054813410333637284
X	0.01699029513885534	27493	27.493	0.08517776957738153
X	0.017025414117755346	5353	5.353	0.14706189130600675
X	0.017907275172666527	90719	90.719	0.058225123401495764
X	0.017544110945130927	21931	21.931	0.09283056136391842
X	0.016801012433629733	70346	70.346	0.06204384180448765
X	0.018156505402245813	265162	265.162	0.040911021713255565
X	0.017125244042907887	9410	9.41	0.12209064325533205
X	0.018153756631541843	24816	24.816	0.0901041739469144
X	0.016983707634563194	16440	16.44	0.10109047347036702
X	0.017064232766997457	130854	130.854	0.05071073686524727
X	0.017417642642973342	156607	156.607	0.048090512116174655
X	0.016995832885234066	26178	26.178	0.08659018408551176
X	0.018265739511942523	8269	8.269	0.13023511243690608
X	0.017432669149919023	253974	253.974	0.040944120148154114
X	0.017006872342416664	66851	66.851	0.06336345471264161
X	0.017618581998623775	306555	306.555	0.0385911318246061
X	0.016955218247999847	90988	90.988	0.057118012674910824
X	0.01709963778441457	67868	67.868	0.06315978165010032
X	0.017284687460152357	19762	19.762	0.09563353559419589
X	0.016813797989077988	82132	82.132	0.058936518225009976
X	0.017427517400881058	246781	246.781	0.041334047584321014
X	0.017480181473209136	55982	55.982	0.06784208328429998
X	0.016761292542131778	6040	6.04	0.14052605544441032
X	0.018337056125235715	13087	13.087	0.11189993577926473
X	0.017334724219869434	17754	17.754	0.09920652567639349
X	0.017207958989814157	68141	68.141	0.0632082286588938
X	0.017149941336977847	42515	42.515	0.07388792750684349
X	0.01708782523781987	7619	7.619	0.13089697394392225
X	0.01711387094248378	18541	18.541	0.09736548531076059
X	0.017403787821803015	92011	92.011	0.057403011403648424
X	0.017110440821047845	40898	40.898	0.07479163763896893
X	0.01710211078785837	30278	30.278	0.08266230482165433
X	0.017157192264647514	37615	37.615	0.076977130421694
X	0.017472330999721928	19661	19.661	0.0961424271299842
X	0.017140678591413883	18331	18.331	0.0977868863748355
X	0.016942904715944347	3422	3.422	0.1704391217794022
X	0.016971226656184	6599	6.599	0.13700771872976272
X	0.0171461959706277	146741	146.741	0.04888835634373327
X	0.017112691320674795	9064	9.064	0.12359459343775177
X	0.017147702827134445	58857	58.857	0.06629324463899634
X	0.017013316329997565	1917	1.917	0.207040649479889
X	0.016942815096433347	2354	2.354	0.19307514555316238
X	0.017008622544096356	104256	104.256	0.054641510327714686
X	0.017119454939373703	77503	77.503	0.060449188243511774
X	0.01711626844084047	25985	25.985	0.08700858835603963
X	0.01707956366662263	3634	3.634	0.1675060957585434
X	0.016259713661073014	4553	4.553	0.15285221891640385
X	0.016971195362456032	18639	18.639	0.09692369035996101
X	0.017062206515346182	20345	20.345	0.09430307045244934
X	0.017111522658931993	73803	73.803	0.061433446473953206
X	0.017018014093706738	4545	4.545	0.15528338769501368
X	0.016911414705631934	10364	10.364	0.11772919607536135
X	0.017058939431048203	18333	18.333	0.09762764866852976
X	0.01782834411647902	18600	18.6	0.09859752679269619
X	0.017373263568721032	102521	102.521	0.05533792344624877
X	0.017057414692298378	7447	7.447	0.13181877903927897
X	0.017092727427234645	40416	40.416	0.07506186418996463
X	0.017245518833713405	186488	186.488	0.045221169638537645
X	0.017144093669874173	31047	31.047	0.08204112928469917
X	0.0175442395715345	33845	33.845	0.08033041493061442
X	0.017550229822016775	82822	82.822	0.05961822358358061
X	0.01747119362065908	2056	2.056	0.20406404392862096
X	0.01709730661993026	26315	26.315	0.08661134349394964
X	0.0171274151569164	12758	12.758	0.11031546730006084
X	0.017828224554224708	223397	223.397	0.04305367746255991
X	0.017072427467312128	44604	44.604	0.0726062192961846
X	0.017369687521446405	34058	34.058	0.07989586226139947
X	0.017292212914571697	69074	69.074	0.0630248744685992
X	0.017090051709085514	25515	25.515	0.08749487224697174
X	0.017362243666123114	2230	2.23	0.19819848385017985
X	0.016994338884124716	3892	3.892	0.16344707295874877
X	0.017234487578015285	14843	14.843	0.10510552906139478
X	0.017149975107779977	85352	85.352	0.05857108429574277
X	0.01792154365094914	370741	370.741	0.03642796683007481
X	0.017066515145378866	47075	47.075	0.07130470277742931
X	0.017125961665965903	10191	10.191	0.11889019936898054
X	0.017572346900868764	18386	18.386	0.09850255694842859
X	0.017908997399430582	407598	407.598	0.03528686503343556
X	0.01703069642895588	17696	17.696	0.09873074922649185
X	0.016922316902364166	9029	9.029	0.12329346468528873
X	0.01833182053057416	54452	54.452	0.0695660354381647
X	0.017498582089908742	54360	54.36	0.06853427197585221
X	0.01748993799729505	7159	7.159	0.1346827480683131
X	0.01712977722091129	11443	11.443	0.11439418248244301
X	0.017109124658967984	25603	25.603	0.08742701382821698
time for making epsilon is 1.809877872467041
epsilons are
[0.27138140312786, 0.10011427722532766, 0.2284381743657149, 0.16812231859909346, 0.061885776423058265, 0.07080039863717008, 0.07170530466122133, 0.07450753290651163, 0.0945316674992286, 0.07825610585594868, 0.1031433844988471, 0.07176399586459314, 0.14797436066275851, 0.045206196804098986, 0.09981731946013274, 0.0850089277971336, 0.07404339487541392, 0.0742112648221239, 0.05404080552104195, 0.05314606552900647, 0.08769114038058556, 0.04856119631879859, 0.11712869268198958, 0.1122574509644716, 0.10158210867242812, 0.0706224404119963, 0.06568302194694628, 0.10676711348703749, 0.049769311999837744, 0.02756088455302964, 0.11716073266582214, 0.028291519421305325, 0.14041691222836689, 0.09359801407178817, 0.07864757395884468, 0.037495883184581566, 0.0671843994937573, 0.2430145254869766, 0.18264476017859105, 0.1847531211152075, 0.1826997535028621, 0.281948635344732, 0.2719075618600581, 0.20775901680390826, 0.2799331784013424, 0.25352770675780345, 0.21037352580467514, 0.14259922135240044, 0.16591341995609743, 0.14605319893160812, 0.11990370487895351, 0.2642328967714189, 0.13686071119992665, 0.15427299645982634, 0.1670999465335345, 0.1820031864586887, 0.21917045699179408, 0.15605216019007892, 0.19080070922239, 0.1933269386236795, 0.1444183442847551, 0.23491509321863793, 0.1737972741530132, 0.11619313030667196, 0.1538725848773174, 0.19969791537450604, 0.18268521333777965, 0.21173553076098603, 0.17403397197373102, 0.17207642219500083, 0.17807065034433334, 0.17566517142706267, 0.20608095812904287, 0.19675512347887167, 0.15563929408854588, 0.1148287343363664, 0.2832075412847517, 0.26673134215769356, 0.1819291042539166, 0.12202886934689956, 0.21284203781396305, 0.2410184049638228, 0.22737678324126084, 0.22685487332390245, 0.24064976619105874, 0.2447409143755581, 0.21434747563701467, 0.26272900003344, 0.19291485780356527, 0.1567195948890839, 0.15077710370005973, 0.21878633592209504, 0.23325133304907192, 0.11996425732694366, 0.17263205004590587, 0.15518149756823946, 0.20120575601403656, 0.19238254734564758, 0.13390941405999754, 0.1555347289577542, 0.11924617335834602, 0.1466142623750852, 0.16095001929339112, 0.2490182671158037, 0.13249851669831017, 0.22675333452853572, 0.18493707426189698, 0.2443075657871243, 0.20989963690434435, 0.3015752593674204, 0.1859974582228809, 0.25566683235754245, 0.2101596747028335, 0.19782425817477892, 0.14856185338823955, 0.19575505276978555, 0.21864394341542046, 0.2945798785562794, 0.13395684528737647, 0.2787244443478522, 0.13111200395805966, 0.23355197485112011, 0.2014608497393238, 0.1563966536352151, 0.16789263421193754, 0.19766491593536872, 0.2585210088428229, 0.1716239959669898, 0.1458659427360323, 0.1961474375711802, 0.11871874138623563, 0.2693711179992838, 0.18179524650952372, 0.262836769145911, 0.26408332287562575, 0.19841723430265187, 0.22199215951709317, 0.23081426364357618, 0.19168834003624696, 0.21193260323522944, 0.20678943693411214, 0.22507506934459204, 0.14434351609905866, 0.1175425379810691, 0.17199475193431268, 0.2205666318679976, 0.22768554065298022, 0.2522074028594759, 0.21694358493598231, 0.22470436081379072, 0.2073985443425031, 0.18501659687364844, 0.1831538719275414, 0.1665368846526429, 0.18846617000379148, 0.12956840507667122, 0.18053924021315387, 0.21696419220711785, 0.20520946445809612, 0.3141700489327912, 0.13136405211266078, 0.27181092485227, 0.1355046239391676, 0.3276532637944314, 0.1698422341543527, 0.16911193288602977, 0.23640908625718618, 0.19189666661048693, 0.19883036837297696, 0.2889141271000654, 0.2946318474328512, 0.21811534229427917, 0.1706788889500412, 0.23384933280787334, 0.15202999859350536, 0.20947896200717928, 0.20099082537187524, 0.17336371107868845, 0.16787874850650306, 0.21425603963162015, 0.14162465313916112, 0.24808654709526468, 0.18382161307665898, 0.2267789470065067, 0.16436134766493354, 0.14698742727556974, 0.21042737396881384, 0.19046605717499426, 0.16366891053706775, 0.12217553023791175, 0.20827500671150592, 0.2658683649205833, 0.19155307287830511, 0.19360687782149166, 0.17761242702621294, 0.22686700641873983, 0.1766500543474202, 0.23327550587005522, 0.15513651332900916, 0.28733975557965763, 0.13668265795815984, 0.20428747577582365, 0.11261120413227324, 0.2142765806666077, 0.13654731924065844, 0.23197303265084582, 0.22653702974507064, 0.18194329371489024, 0.2045190953886099, 0.16183876894965632, 0.15985171068365234, 0.20656769726915636, 0.20256073531361998, 0.15037046509333657, 0.29956866307173124, 0.31962736437896927, 0.35013653044001386, 0.22623504697222335, 0.11514776960554328, 0.2298606729262197, 0.22755627246671667, 0.2822468960318815, 0.12785468002635905, 0.24054190399805953, 0.2143206269092621, 0.18612740370465974, 0.19614549493196112, 0.21458204098998523, 0.16526502868104273, 0.24758748165693784, 0.25104836577164874, 0.22210524130548104, 0.22726717304191094, 0.19114325919366135, 0.18347988225897802, 0.2120726786647705, 0.1435225057174538, 0.18457042270250762, 0.2385321992151761, 0.13494033412026463, 0.1176883254926339, 0.1354252787292808, 0.157691081581249, 0.19549908578523462, 0.13697878398132787, 0.12069204677671588, 0.09261476404181426, 0.19266678438793036, 0.05102788806640379, 0.06937247113067331, 0.08162239800502663, 0.08052362705257507, 0.21718925353258978, 0.13559799584980778, 0.03919666437983343, 0.04243230473947582, 0.18444758037631606, 0.08577719364300043, 0.05835500563739444, 0.09535765048151618, 0.09592905020751154, 0.08959311993654913, 0.11701919166361303, 0.11460796758838668, 0.16793972238002658, 0.07349825041178175, 0.12243120334844494, 0.16341393067996415, 0.09911127887558739, 0.10188105350355833, 0.04583939444483197, 0.0723416452341542, 0.17442792006313984, 0.08118682529196934, 0.1441313267730522, 0.11834599484306274, 0.07416193120197748, 0.07791076086020174, 0.05602831208182234, 0.2692638988416585, 0.15822807129212405, 0.06636377921248632, 0.07691365715052412, 0.09142845167472681, 0.18234899467092464, 0.04723017969971141, 0.11791386219226066, 0.10291600644809522, 0.0630951725733229, 0.2088107688279358, 0.07451354851435069, 0.06456986886150115, 0.04244630393678885, 0.08039385869690036, 0.08102914132672942, 0.12729365857171082, 0.15557784246938672, 0.08251870226822874, 0.13510898798322554, 0.11994591127219936, 0.07410368741576641, 0.10339768259203445, 0.13621971248628148, 0.16129309590982094, 0.04033563916760852, 0.1371594905965918, 0.15829341147788367, 0.11740328081640468, 0.055072090173217535, 0.09544413768228205, 0.1090809568866882, 0.15324019924828586, 0.10776327077592976, 0.12170899940811909, 0.03380450258194947, 0.08264096747835754, 0.11930254385114009, 0.05744805304934825, 0.05458982677222717, 0.15069337943645547, 0.0612633915232432, 0.05332040893646733, 0.036441635377132485, 0.12103789026953095, 0.20743464693873948, 0.12681475943715992, 0.10927929272193695, 0.0887934313321826, 0.09650876520650567, 0.22572848883181193, 0.08857973316498338, 0.057377332581991784, 0.05968671875031218, 0.14945464584028226, 0.05388950425480006, 0.12585090458358783, 0.10563309333713763, 0.1136956323415599, 0.18700792570609737, 0.0508600926753674, 0.0807392630930653, 0.11554036469514498, 0.09580409188895182, 0.1587209715620606, 0.04200144203338619, 0.09861353185549693, 0.049665173111714186, 0.20895871555475978, 0.16217212716729498, 0.1630784228376371, 0.09961068344139107, 0.15952492783081768, 0.12021216723272303, 0.1444210397896237, 0.15700056706479015, 0.049965347377987045, 0.09246441152127262, 0.09175133120664612, 0.05853838937881282, 0.04019458234930542, 0.13625531122371376, 0.11935141967362403, 0.13900159992190544, 0.26249781508419445, 0.1230323042958236, 0.10165328554954302, 0.13124070608017938, 0.15211152604179246, 0.09798814703639497, 0.1101320788912791, 0.028500291162775627, 0.16530290557030766, 0.07582088430498019, 0.10716768524307871, 0.21478043337828656, 0.05449631467748103, 0.04562619285413645, 0.0360174660777117, 0.08095148660824081, 0.09345532351179042, 0.18625287662831774, 0.07620852490829237, 0.028515745056171264, 0.08048268116904514, 0.11927147239640586, 0.06889117090809442, 0.04179465808001311, 0.07235635927328764, 0.11918609856353568, 0.1770301376245852, 0.09119408145230544, 0.12418432131847669, 0.0396905014890506, 0.11941588834317193, 0.2203810297699838, 0.08532927005569456, 0.08564923273657936, 0.22505816483925284, 0.0645875230321631, 0.07056083169234756, 0.06415737289865392, 0.05655107252872602, 0.062239474327464774, 0.10252229706812271, 0.07339196264230596, 0.06818392240270743, 0.046091726660273634, 0.14578794781730198, 0.062293917323367906, 0.0653675593328276, 0.07278140032011844, 0.09813803234500662, 0.10811111102548394, 0.12022113880242588, 0.13187433925188088, 0.058892111265665986, 0.05040271140594239, 0.056879743028028944, 0.05239484738050893, 0.06603179109062583, 0.062425322535179774, 0.12132019724968887, 0.04583084159355781, 0.2390203850251168, 0.1135299442191528, 0.0458106108795469, 0.0819219878746057, 0.20330134402893107, 0.1030275349193214, 0.054435910108532735, 0.09017120643257734, 0.07299659287131498, 0.22418290761946574, 0.16155402918579054, 0.17523358506710512, 0.08405092352687504, 0.08057522879870258, 0.069343313330299, 0.2032750869902219, 0.06565469131052203, 0.12005507402013191, 0.08437637900672991, 0.10236405973158923, 0.05832383404539209, 0.18979800991467355, 0.07062081124929957, 0.09317034965835588, 0.10793004558769767, 0.08723718717529406, 0.044530447532266836, 0.11664009742434049, 0.1228397440191486, 0.057016406620834716, 0.09608364596443286, 0.035395595242286436, 0.13787102796821116, 0.05273842358307584, 0.03969279427817137, 0.14368799501747204, 0.06090228375883252, 0.10727000737380307, 0.048164535378139535, 0.10126413342308205, 0.09290784779220904, 0.21955874056770175, 0.16382462573416107, 0.15503194737980916, 0.09709961784750537, 0.09372606425749577, 0.05508334943649401, 0.10226760226205531, 0.1622335834665003, 0.12199001063618906, 0.09779576000601242, 0.12369572512445659, 0.05055166166994215, 0.08056725913505124, 0.17682084821689872, 0.1524855058100348, 0.14914584454520663, 0.03436266914336383, 0.13430151330433915, 0.10461864303157015, 0.07165991437373906, 0.10767929894285677, 0.1423805342820216, 0.1262972112295912, 0.06000137224343948, 0.06953078267181381, 0.08218801402251795, 0.1226207312161004, 0.09406011679220126, 0.07319099280979184, 0.0731258399198022, 0.06285286739815592, 0.1752895884655749, 0.05766001867162982, 0.17292263238322564, 0.07481246700391428, 0.10882013781578341, 0.1744439862008469, 0.10450909911988075, 0.07211768150961874, 0.09930424872264106, 0.07596179429024079, 0.14484714454072437, 0.05208542418632896, 0.19221094607462536, 0.17648877199519894, 0.13367528140053003, 0.10193500133872146, 0.18353420417636312, 0.12547183519039637, 0.09124793800342525, 0.060010131664858524, 0.15792280643480774, 0.19803421350245928, 0.15320077716126965, 0.04578205188801133, 0.029747889901397484, 0.09590270788836026, 0.14240610414212976, 0.04823992439664564, 0.05454053597703418, 0.19590741995763847, 0.1326918882269974, 0.08178306129871948, 0.03666910033255684, 0.03339262961394526, 0.05270694189938449, 0.10269217959788851, 0.1417111453594002, 0.16399516202758363, 0.0666207815724075, 0.11298243418341973, 0.10206241517362777, 0.05500308264976654, 0.037695620821189144, 0.1504440046805505, 0.2164509688970938, 0.095787562476115, 0.060470006714796055, 0.08014913138249045, 0.19090018349383028, 0.20944460452652225, 0.10589391687164477, 0.17232222604401531, 0.11565299236897583, 0.1729150370002431, 0.11969175352060392, 0.12359094415944887, 0.13369979847237787, 0.17723061052570308, 0.05245225223099331, 0.040090727045640255, 0.0666489098220702, 0.0884301199168249, 0.10933940780223683, 0.1185348258560016, 0.10172915098730663, 0.053119560812147755, 0.1171479524340553, 0.08712890359528808, 0.10505382053642191, 0.029893028118360032, 0.1128258644258707, 0.06685394249220956, 0.05003531069386049, 0.06702927233546857, 0.09777464029044529, 0.04258259751559129, 0.057026797429188036, 0.09247714713194725, 0.14118401302492534, 0.09391402034773333, 0.11983275588941633, 0.06619550308407991, 0.03845352813313418, 0.06665946616966259, 0.05933692432397233, 0.10807697803503698, 0.1481340576116021, 0.10196498185636234, 0.1378369119362518, 0.045249443796585194, 0.038360665800129555, 0.16131714129258815, 0.029516936848153808, 0.0705919804399462, 0.07943328123784212, 0.12930127124392693, 0.07666117970904007, 0.05184924550337798, 0.19636458192462553, 0.032944913198445176, 0.09832047240379967, 0.09129403210310791, 0.1227228747830868, 0.10507399878253983, 0.20973448646969634, 0.11061485036315065, 0.11907085103031906, 0.08314048374457918, 0.03661925854277627, 0.10285694329815004, 0.04301336350592661, 0.14644629038004653, 0.10086789886720605, 0.16070587704103978, 0.08501916137705469, 0.057595578939201364, 0.02672924027879592, 0.09807926561531073, 0.052462230227208875, 0.12424258692548952, 0.05439218367146542, 0.054813410333637284, 0.08517776957738153, 0.14706189130600675, 0.058225123401495764, 0.09283056136391842, 0.06204384180448765, 0.040911021713255565, 0.12209064325533205, 0.0901041739469144, 0.10109047347036702, 0.05071073686524727, 0.048090512116174655, 0.08659018408551176, 0.13023511243690608, 0.040944120148154114, 0.06336345471264161, 0.0385911318246061, 0.057118012674910824, 0.06315978165010032, 0.09563353559419589, 0.058936518225009976, 0.041334047584321014, 0.06784208328429998, 0.14052605544441032, 0.11189993577926473, 0.09920652567639349, 0.0632082286588938, 0.07388792750684349, 0.13089697394392225, 0.09736548531076059, 0.057403011403648424, 0.07479163763896893, 0.08266230482165433, 0.076977130421694, 0.0961424271299842, 0.0977868863748355, 0.1704391217794022, 0.13700771872976272, 0.04888835634373327, 0.12359459343775177, 0.06629324463899634, 0.207040649479889, 0.19307514555316238, 0.054641510327714686, 0.060449188243511774, 0.08700858835603963, 0.1675060957585434, 0.15285221891640385, 0.09692369035996101, 0.09430307045244934, 0.061433446473953206, 0.15528338769501368, 0.11772919607536135, 0.09762764866852976, 0.09859752679269619, 0.05533792344624877, 0.13181877903927897, 0.07506186418996463, 0.045221169638537645, 0.08204112928469917, 0.08033041493061442, 0.05961822358358061, 0.20406404392862096, 0.08661134349394964, 0.11031546730006084, 0.04305367746255991, 0.0726062192961846, 0.07989586226139947, 0.0630248744685992, 0.08749487224697174, 0.19819848385017985, 0.16344707295874877, 0.10510552906139478, 0.05857108429574277, 0.03642796683007481, 0.07130470277742931, 0.11889019936898054, 0.09850255694842859, 0.03528686503343556, 0.09873074922649185, 0.12329346468528873, 0.0695660354381647, 0.06853427197585221, 0.1346827480683131, 0.11439418248244301, 0.08742701382821698]
0.09697874150626114
Making ranges
torch.Size([30214, 2])
We keep 6.30e+06/4.34e+08 =  1% of the original kernel matrix.

torch.Size([2103, 2])
We keep 5.44e+04/6.87e+05 =  7% of the original kernel matrix.

torch.Size([9457, 2])
We keep 6.80e+05/1.73e+07 =  3% of the original kernel matrix.

torch.Size([26155, 2])
We keep 6.07e+06/2.89e+08 =  2% of the original kernel matrix.

torch.Size([29412, 2])
We keep 6.02e+06/3.55e+08 =  1% of the original kernel matrix.

torch.Size([3439, 2])
We keep 1.28e+05/2.04e+06 =  6% of the original kernel matrix.

torch.Size([11548, 2])
We keep 9.77e+05/2.98e+07 =  3% of the original kernel matrix.

torch.Size([7198, 2])
We keep 7.42e+05/1.25e+07 =  5% of the original kernel matrix.

torch.Size([15363, 2])
We keep 1.84e+06/7.37e+07 =  2% of the original kernel matrix.

torch.Size([110159, 2])
We keep 9.72e+07/5.38e+09 =  1% of the original kernel matrix.

torch.Size([58942, 2])
We keep 2.02e+07/1.53e+09 =  1% of the original kernel matrix.

torch.Size([75149, 2])
We keep 3.39e+07/2.32e+09 =  1% of the original kernel matrix.

torch.Size([48423, 2])
We keep 1.40e+07/1.00e+09 =  1% of the original kernel matrix.

torch.Size([71077, 2])
We keep 3.20e+07/2.27e+09 =  1% of the original kernel matrix.

torch.Size([47220, 2])
We keep 1.40e+07/9.93e+08 =  1% of the original kernel matrix.

torch.Size([63951, 2])
We keep 2.35e+07/1.72e+09 =  1% of the original kernel matrix.

torch.Size([45220, 2])
We keep 1.23e+07/8.63e+08 =  1% of the original kernel matrix.

torch.Size([28862, 2])
We keep 2.53e+07/4.04e+08 =  6% of the original kernel matrix.

torch.Size([30563, 2])
We keep 6.95e+06/4.19e+08 =  1% of the original kernel matrix.

torch.Size([45886, 2])
We keep 1.51e+08/1.28e+09 = 11% of the original kernel matrix.

torch.Size([38089, 2])
We keep 1.09e+07/7.45e+08 =  1% of the original kernel matrix.

torch.Size([23625, 2])
We keep 6.75e+06/2.82e+08 =  2% of the original kernel matrix.

torch.Size([27668, 2])
We keep 6.03e+06/3.50e+08 =  1% of the original kernel matrix.

torch.Size([64227, 2])
We keep 4.42e+07/2.15e+09 =  2% of the original kernel matrix.

torch.Size([44758, 2])
We keep 1.35e+07/9.65e+08 =  1% of the original kernel matrix.

torch.Size([10445, 2])
We keep 1.06e+06/3.06e+07 =  3% of the original kernel matrix.

torch.Size([18118, 2])
We keep 2.54e+06/1.15e+08 =  2% of the original kernel matrix.

torch.Size([275279, 2])
We keep 3.62e+08/3.44e+10 =  1% of the original kernel matrix.

torch.Size([97910, 2])
We keep 4.49e+07/3.87e+09 =  1% of the original kernel matrix.

torch.Size([26836, 2])
We keep 5.50e+06/2.93e+08 =  1% of the original kernel matrix.

torch.Size([29561, 2])
We keep 6.00e+06/3.57e+08 =  1% of the original kernel matrix.

torch.Size([41351, 2])
We keep 1.30e+07/7.75e+08 =  1% of the original kernel matrix.

torch.Size([37130, 2])
We keep 8.85e+06/5.80e+08 =  1% of the original kernel matrix.

torch.Size([64822, 2])
We keep 2.62e+07/1.78e+09 =  1% of the original kernel matrix.

torch.Size([45515, 2])
We keep 1.25e+07/8.79e+08 =  1% of the original kernel matrix.

torch.Size([60154, 2])
We keep 3.12e+07/1.75e+09 =  1% of the original kernel matrix.

torch.Size([43658, 2])
We keep 1.26e+07/8.72e+08 =  1% of the original kernel matrix.

torch.Size([162320, 2])
We keep 1.76e+08/1.24e+10 =  1% of the original kernel matrix.

torch.Size([72996, 2])
We keep 2.90e+07/2.32e+09 =  1% of the original kernel matrix.

torch.Size([168148, 2])
We keep 1.56e+08/1.30e+10 =  1% of the original kernel matrix.

torch.Size([74568, 2])
We keep 2.94e+07/2.38e+09 =  1% of the original kernel matrix.

torch.Size([36586, 2])
We keep 1.34e+07/6.46e+08 =  2% of the original kernel matrix.

torch.Size([34197, 2])
We keep 8.22e+06/5.30e+08 =  1% of the original kernel matrix.

torch.Size([220841, 2])
We keep 2.17e+08/2.35e+10 =  0% of the original kernel matrix.

torch.Size([87453, 2])
We keep 3.72e+07/3.19e+09 =  1% of the original kernel matrix.

torch.Size([17909, 2])
We keep 3.05e+06/1.13e+08 =  2% of the original kernel matrix.

torch.Size([23956, 2])
We keep 4.17e+06/2.22e+08 =  1% of the original kernel matrix.

torch.Size([20017, 2])
We keep 4.03e+06/1.48e+08 =  2% of the original kernel matrix.

torch.Size([25440, 2])
We keep 4.45e+06/2.54e+08 =  1% of the original kernel matrix.

torch.Size([20128, 2])
We keep 1.14e+07/2.77e+08 =  4% of the original kernel matrix.

torch.Size([24612, 2])
We keep 5.93e+06/3.47e+08 =  1% of the original kernel matrix.

torch.Size([76113, 2])
We keep 3.50e+07/2.58e+09 =  1% of the original kernel matrix.

torch.Size([49179, 2])
We keep 1.46e+07/1.06e+09 =  1% of the original kernel matrix.

torch.Size([92161, 2])
We keep 6.97e+07/3.79e+09 =  1% of the original kernel matrix.

torch.Size([53587, 2])
We keep 1.73e+07/1.28e+09 =  1% of the original kernel matrix.

torch.Size([22983, 2])
We keep 4.38e+06/2.00e+08 =  2% of the original kernel matrix.

torch.Size([27548, 2])
We keep 5.20e+06/2.95e+08 =  1% of the original kernel matrix.

torch.Size([175014, 2])
We keep 2.87e+08/2.13e+10 =  1% of the original kernel matrix.

torch.Size([75599, 2])
We keep 3.65e+07/3.04e+09 =  1% of the original kernel matrix.

torch.Size([1421020, 2])
We keep 5.72e+09/7.46e+11 =  0% of the original kernel matrix.

torch.Size([227945, 2])
We keep 1.81e+08/1.80e+10 =  1% of the original kernel matrix.

torch.Size([16774, 2])
We keep 4.30e+06/1.12e+08 =  3% of the original kernel matrix.

torch.Size([23035, 2])
We keep 4.06e+06/2.21e+08 =  1% of the original kernel matrix.

torch.Size([1246063, 2])
We keep 5.16e+09/6.29e+11 =  0% of the original kernel matrix.

torch.Size([214373, 2])
We keep 1.68e+08/1.65e+10 =  1% of the original kernel matrix.

torch.Size([11453, 2])
We keep 1.39e+06/3.72e+07 =  3% of the original kernel matrix.

torch.Size([19083, 2])
We keep 2.74e+06/1.27e+08 =  2% of the original kernel matrix.

torch.Size([30840, 2])
We keep 1.10e+07/4.34e+08 =  2% of the original kernel matrix.

torch.Size([31873, 2])
We keep 7.01e+06/4.34e+08 =  1% of the original kernel matrix.

torch.Size([50995, 2])
We keep 2.59e+07/1.23e+09 =  2% of the original kernel matrix.

torch.Size([40280, 2])
We keep 1.09e+07/7.32e+08 =  1% of the original kernel matrix.

torch.Size([512679, 2])
We keep 1.14e+09/1.17e+11 =  0% of the original kernel matrix.

torch.Size([132282, 2])
We keep 7.81e+07/7.14e+09 =  1% of the original kernel matrix.

torch.Size([89679, 2])
We keep 5.31e+07/3.39e+09 =  1% of the original kernel matrix.

torch.Size([53058, 2])
We keep 1.65e+07/1.21e+09 =  1% of the original kernel matrix.

torch.Size([2764, 2])
We keep 9.34e+04/1.29e+06 =  7% of the original kernel matrix.

torch.Size([10488, 2])
We keep 8.32e+05/2.36e+07 =  3% of the original kernel matrix.

torch.Size([6006, 2])
We keep 3.54e+05/7.72e+06 =  4% of the original kernel matrix.

torch.Size([14161, 2])
We keep 1.54e+06/5.79e+07 =  2% of the original kernel matrix.

torch.Size([6013, 2])
We keep 3.49e+05/7.13e+06 =  4% of the original kernel matrix.

torch.Size([14248, 2])
We keep 1.51e+06/5.57e+07 =  2% of the original kernel matrix.

torch.Size([6141, 2])
We keep 3.30e+05/7.61e+06 =  4% of the original kernel matrix.

torch.Size([14435, 2])
We keep 1.51e+06/5.75e+07 =  2% of the original kernel matrix.

torch.Size([1572, 2])
We keep 1.01e+05/4.76e+05 = 21% of the original kernel matrix.

torch.Size([8285, 2])
We keep 6.01e+05/1.44e+07 =  4% of the original kernel matrix.

torch.Size([2074, 2])
We keep 4.86e+04/6.32e+05 =  7% of the original kernel matrix.

torch.Size([9418, 2])
We keep 6.56e+05/1.66e+07 =  3% of the original kernel matrix.

torch.Size([4292, 2])
We keep 1.87e+05/3.47e+06 =  5% of the original kernel matrix.

torch.Size([12427, 2])
We keep 1.16e+06/3.88e+07 =  2% of the original kernel matrix.

torch.Size([1973, 2])
We keep 4.59e+04/6.07e+05 =  7% of the original kernel matrix.

torch.Size([9320, 2])
We keep 6.47e+05/1.62e+07 =  3% of the original kernel matrix.

torch.Size([2473, 2])
We keep 6.76e+04/1.04e+06 =  6% of the original kernel matrix.

torch.Size([10083, 2])
We keep 7.65e+05/2.13e+07 =  3% of the original kernel matrix.

torch.Size([3964, 2])
We keep 1.81e+05/3.32e+06 =  5% of the original kernel matrix.

torch.Size([11927, 2])
We keep 1.14e+06/3.80e+07 =  3% of the original kernel matrix.

torch.Size([10904, 2])
We keep 1.19e+06/3.44e+07 =  3% of the original kernel matrix.

torch.Size([18307, 2])
We keep 2.67e+06/1.22e+08 =  2% of the original kernel matrix.

torch.Size([7465, 2])
We keep 5.91e+05/1.38e+07 =  4% of the original kernel matrix.

torch.Size([15502, 2])
We keep 1.90e+06/7.75e+07 =  2% of the original kernel matrix.

torch.Size([10781, 2])
We keep 1.49e+06/3.20e+07 =  4% of the original kernel matrix.

torch.Size([18482, 2])
We keep 2.62e+06/1.18e+08 =  2% of the original kernel matrix.

torch.Size([17303, 2])
We keep 2.70e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([23456, 2])
We keep 4.06e+06/2.12e+08 =  1% of the original kernel matrix.

torch.Size([2157, 2])
We keep 5.44e+04/7.52e+05 =  7% of the original kernel matrix.

torch.Size([9527, 2])
We keep 6.91e+05/1.81e+07 =  3% of the original kernel matrix.

torch.Size([11385, 2])
We keep 2.43e+06/4.40e+07 =  5% of the original kernel matrix.

torch.Size([18943, 2])
We keep 2.85e+06/1.38e+08 =  2% of the original kernel matrix.

torch.Size([8676, 2])
We keep 1.00e+06/2.06e+07 =  4% of the original kernel matrix.

torch.Size([16622, 2])
We keep 2.18e+06/9.47e+07 =  2% of the original kernel matrix.

torch.Size([7702, 2])
We keep 5.59e+05/1.31e+07 =  4% of the original kernel matrix.

torch.Size([15851, 2])
We keep 1.87e+06/7.55e+07 =  2% of the original kernel matrix.

torch.Size([5633, 2])
We keep 4.57e+05/7.28e+06 =  6% of the original kernel matrix.

torch.Size([13720, 2])
We keep 1.49e+06/5.63e+07 =  2% of the original kernel matrix.

torch.Size([3521, 2])
We keep 1.41e+05/2.44e+06 =  5% of the original kernel matrix.

torch.Size([11500, 2])
We keep 1.02e+06/3.26e+07 =  3% of the original kernel matrix.

torch.Size([9181, 2])
We keep 7.44e+05/2.00e+07 =  3% of the original kernel matrix.

torch.Size([17039, 2])
We keep 2.16e+06/9.32e+07 =  2% of the original kernel matrix.

torch.Size([5304, 2])
We keep 2.99e+05/6.07e+06 =  4% of the original kernel matrix.

torch.Size([13502, 2])
We keep 1.42e+06/5.14e+07 =  2% of the original kernel matrix.

torch.Size([4724, 2])
We keep 2.72e+05/4.85e+06 =  5% of the original kernel matrix.

torch.Size([12665, 2])
We keep 1.29e+06/4.59e+07 =  2% of the original kernel matrix.

torch.Size([11111, 2])
We keep 1.19e+06/3.31e+07 =  3% of the original kernel matrix.

torch.Size([18628, 2])
We keep 2.60e+06/1.20e+08 =  2% of the original kernel matrix.

torch.Size([2899, 2])
We keep 1.04e+05/1.74e+06 =  5% of the original kernel matrix.

torch.Size([10715, 2])
We keep 9.23e+05/2.75e+07 =  3% of the original kernel matrix.

torch.Size([6701, 2])
We keep 4.78e+05/1.07e+07 =  4% of the original kernel matrix.

torch.Size([14813, 2])
We keep 1.74e+06/6.82e+07 =  2% of the original kernel matrix.

torch.Size([18732, 2])
We keep 2.84e+06/1.17e+08 =  2% of the original kernel matrix.

torch.Size([24431, 2])
We keep 4.21e+06/2.25e+08 =  1% of the original kernel matrix.

torch.Size([8676, 2])
We keep 9.03e+05/2.17e+07 =  4% of the original kernel matrix.

torch.Size([16661, 2])
We keep 2.22e+06/9.70e+07 =  2% of the original kernel matrix.

torch.Size([4657, 2])
We keep 2.44e+05/4.80e+06 =  5% of the original kernel matrix.

torch.Size([12926, 2])
We keep 1.33e+06/4.57e+07 =  2% of the original kernel matrix.

torch.Size([5814, 2])
We keep 3.44e+05/7.53e+06 =  4% of the original kernel matrix.

torch.Size([13923, 2])
We keep 1.52e+06/5.72e+07 =  2% of the original kernel matrix.

torch.Size([3901, 2])
We keep 1.78e+05/2.98e+06 =  5% of the original kernel matrix.

torch.Size([11792, 2])
We keep 1.09e+06/3.60e+07 =  3% of the original kernel matrix.

torch.Size([6756, 2])
We keep 4.44e+05/1.05e+07 =  4% of the original kernel matrix.

torch.Size([14926, 2])
We keep 1.71e+06/6.77e+07 =  2% of the original kernel matrix.

torch.Size([7305, 2])
We keep 4.60e+05/1.09e+07 =  4% of the original kernel matrix.

torch.Size([15443, 2])
We keep 1.73e+06/6.87e+07 =  2% of the original kernel matrix.

torch.Size([5961, 2])
We keep 5.38e+05/8.92e+06 =  6% of the original kernel matrix.

torch.Size([14036, 2])
We keep 1.62e+06/6.22e+07 =  2% of the original kernel matrix.

torch.Size([6629, 2])
We keep 6.65e+05/1.15e+07 =  5% of the original kernel matrix.

torch.Size([14722, 2])
We keep 1.83e+06/7.06e+07 =  2% of the original kernel matrix.

torch.Size([4465, 2])
We keep 2.13e+05/3.75e+06 =  5% of the original kernel matrix.

torch.Size([12631, 2])
We keep 1.20e+06/4.04e+07 =  2% of the original kernel matrix.

torch.Size([4806, 2])
We keep 3.06e+05/4.78e+06 =  6% of the original kernel matrix.

torch.Size([12887, 2])
We keep 1.30e+06/4.56e+07 =  2% of the original kernel matrix.

torch.Size([8884, 2])
We keep 7.57e+05/1.97e+07 =  3% of the original kernel matrix.

torch.Size([16732, 2])
We keep 2.15e+06/9.25e+07 =  2% of the original kernel matrix.

torch.Size([17702, 2])
We keep 3.75e+06/1.25e+08 =  2% of the original kernel matrix.

torch.Size([23802, 2])
We keep 4.32e+06/2.33e+08 =  1% of the original kernel matrix.

torch.Size([1838, 2])
We keep 4.05e+04/4.97e+05 =  8% of the original kernel matrix.

torch.Size([9039, 2])
We keep 5.99e+05/1.47e+07 =  4% of the original kernel matrix.

torch.Size([2071, 2])
We keep 5.90e+04/7.94e+05 =  7% of the original kernel matrix.

torch.Size([9386, 2])
We keep 7.10e+05/1.86e+07 =  3% of the original kernel matrix.

torch.Size([6035, 2])
We keep 3.91e+05/7.92e+06 =  4% of the original kernel matrix.

torch.Size([14220, 2])
We keep 1.54e+06/5.86e+07 =  2% of the original kernel matrix.

torch.Size([15813, 2])
We keep 2.22e+06/8.30e+07 =  2% of the original kernel matrix.

torch.Size([22102, 2])
We keep 3.65e+06/1.90e+08 =  1% of the original kernel matrix.

torch.Size([3945, 2])
We keep 1.60e+05/2.81e+06 =  5% of the original kernel matrix.

torch.Size([11893, 2])
We keep 1.08e+06/3.49e+07 =  3% of the original kernel matrix.

torch.Size([2603, 2])
We keep 8.91e+04/1.26e+06 =  7% of the original kernel matrix.

torch.Size([10091, 2])
We keep 8.07e+05/2.34e+07 =  3% of the original kernel matrix.

torch.Size([3398, 2])
We keep 1.20e+05/2.00e+06 =  6% of the original kernel matrix.

torch.Size([11402, 2])
We keep 9.55e+05/2.95e+07 =  3% of the original kernel matrix.

torch.Size([3521, 2])
We keep 1.15e+05/1.97e+06 =  5% of the original kernel matrix.

torch.Size([11473, 2])
We keep 9.46e+05/2.92e+07 =  3% of the original kernel matrix.

torch.Size([2855, 2])
We keep 9.28e+04/1.41e+06 =  6% of the original kernel matrix.

torch.Size([10635, 2])
We keep 8.60e+05/2.48e+07 =  3% of the original kernel matrix.

torch.Size([2988, 2])
We keep 8.73e+04/1.34e+06 =  6% of the original kernel matrix.

torch.Size([11007, 2])
We keep 8.31e+05/2.41e+07 =  3% of the original kernel matrix.

torch.Size([3940, 2])
We keep 1.56e+05/2.75e+06 =  5% of the original kernel matrix.

torch.Size([12049, 2])
We keep 1.06e+06/3.46e+07 =  3% of the original kernel matrix.

torch.Size([2236, 2])
We keep 5.82e+04/7.67e+05 =  7% of the original kernel matrix.

torch.Size([9667, 2])
We keep 6.89e+05/1.83e+07 =  3% of the original kernel matrix.

torch.Size([5195, 2])
We keep 2.63e+05/5.55e+06 =  4% of the original kernel matrix.

torch.Size([13456, 2])
We keep 1.37e+06/4.91e+07 =  2% of the original kernel matrix.

torch.Size([9188, 2])
We keep 7.77e+05/1.97e+07 =  3% of the original kernel matrix.

torch.Size([17065, 2])
We keep 2.16e+06/9.25e+07 =  2% of the original kernel matrix.

torch.Size([10190, 2])
We keep 9.10e+05/2.46e+07 =  3% of the original kernel matrix.

torch.Size([17859, 2])
We keep 2.34e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([3773, 2])
We keep 1.47e+05/2.65e+06 =  5% of the original kernel matrix.

torch.Size([11867, 2])
We keep 1.07e+06/3.40e+07 =  3% of the original kernel matrix.

torch.Size([3100, 2])
We keep 1.11e+05/1.79e+06 =  6% of the original kernel matrix.

torch.Size([11000, 2])
We keep 9.24e+05/2.79e+07 =  3% of the original kernel matrix.

torch.Size([16361, 2])
We keep 2.71e+06/9.58e+07 =  2% of the original kernel matrix.

torch.Size([22656, 2])
We keep 3.92e+06/2.04e+08 =  1% of the original kernel matrix.

torch.Size([6770, 2])
We keep 5.53e+05/1.26e+07 =  4% of the original kernel matrix.

torch.Size([14872, 2])
We keep 1.89e+06/7.41e+07 =  2% of the original kernel matrix.

torch.Size([8993, 2])
We keep 8.75e+05/2.07e+07 =  4% of the original kernel matrix.

torch.Size([16858, 2])
We keep 2.22e+06/9.49e+07 =  2% of the original kernel matrix.

torch.Size([4591, 2])
We keep 2.40e+05/4.47e+06 =  5% of the original kernel matrix.

torch.Size([12794, 2])
We keep 1.28e+06/4.41e+07 =  2% of the original kernel matrix.

torch.Size([4967, 2])
We keep 4.11e+05/5.63e+06 =  7% of the original kernel matrix.

torch.Size([13262, 2])
We keep 1.39e+06/4.94e+07 =  2% of the original kernel matrix.

torch.Size([13178, 2])
We keep 1.52e+06/5.05e+07 =  3% of the original kernel matrix.

torch.Size([20235, 2])
We keep 3.07e+06/1.48e+08 =  2% of the original kernel matrix.

torch.Size([9480, 2])
We keep 7.72e+05/2.06e+07 =  3% of the original kernel matrix.

torch.Size([17375, 2])
We keep 2.19e+06/9.46e+07 =  2% of the original kernel matrix.

torch.Size([17455, 2])
We keep 2.54e+06/9.97e+07 =  2% of the original kernel matrix.

torch.Size([23477, 2])
We keep 3.95e+06/2.08e+08 =  1% of the original kernel matrix.

torch.Size([10995, 2])
We keep 1.01e+06/2.98e+07 =  3% of the original kernel matrix.

torch.Size([18605, 2])
We keep 2.51e+06/1.14e+08 =  2% of the original kernel matrix.

torch.Size([8600, 2])
We keep 6.86e+05/1.77e+07 =  3% of the original kernel matrix.

torch.Size([16625, 2])
We keep 2.08e+06/8.76e+07 =  2% of the original kernel matrix.

torch.Size([2499, 2])
We keep 7.91e+04/1.16e+06 =  6% of the original kernel matrix.

torch.Size([10044, 2])
We keep 8.07e+05/2.25e+07 =  3% of the original kernel matrix.

torch.Size([13099, 2])
We keep 1.81e+06/5.33e+07 =  3% of the original kernel matrix.

torch.Size([20147, 2])
We keep 3.13e+06/1.52e+08 =  2% of the original kernel matrix.

torch.Size([3445, 2])
We keep 1.18e+05/1.98e+06 =  5% of the original kernel matrix.

torch.Size([11458, 2])
We keep 9.35e+05/2.93e+07 =  3% of the original kernel matrix.

torch.Size([6191, 2])
We keep 3.94e+05/8.49e+06 =  4% of the original kernel matrix.

torch.Size([14398, 2])
We keep 1.63e+06/6.07e+07 =  2% of the original kernel matrix.

torch.Size([2639, 2])
We keep 1.26e+05/1.29e+06 =  9% of the original kernel matrix.

torch.Size([10250, 2])
We keep 8.01e+05/2.37e+07 =  3% of the original kernel matrix.

torch.Size([3912, 2])
We keep 1.89e+05/3.41e+06 =  5% of the original kernel matrix.

torch.Size([11991, 2])
We keep 1.17e+06/3.85e+07 =  3% of the original kernel matrix.

torch.Size([1627, 2])
We keep 3.29e+04/3.93e+05 =  8% of the original kernel matrix.

torch.Size([8757, 2])
We keep 5.62e+05/1.31e+07 =  4% of the original kernel matrix.

torch.Size([5446, 2])
We keep 3.67e+05/6.78e+06 =  5% of the original kernel matrix.

torch.Size([13526, 2])
We keep 1.47e+06/5.43e+07 =  2% of the original kernel matrix.

torch.Size([2549, 2])
We keep 8.13e+04/9.94e+05 =  8% of the original kernel matrix.

torch.Size([10325, 2])
We keep 7.64e+05/2.08e+07 =  3% of the original kernel matrix.

torch.Size([3776, 2])
We keep 1.74e+05/3.13e+06 =  5% of the original kernel matrix.

torch.Size([11630, 2])
We keep 1.11e+06/3.69e+07 =  3% of the original kernel matrix.

torch.Size([5106, 2])
We keep 2.52e+05/4.89e+06 =  5% of the original kernel matrix.

torch.Size([13363, 2])
We keep 1.30e+06/4.61e+07 =  2% of the original kernel matrix.

torch.Size([9935, 2])
We keep 1.03e+06/2.82e+07 =  3% of the original kernel matrix.

torch.Size([17691, 2])
We keep 2.48e+06/1.11e+08 =  2% of the original kernel matrix.

torch.Size([4602, 2])
We keep 2.80e+05/4.59e+06 =  6% of the original kernel matrix.

torch.Size([12641, 2])
We keep 1.27e+06/4.47e+07 =  2% of the original kernel matrix.

torch.Size([3759, 2])
We keep 1.75e+05/2.73e+06 =  6% of the original kernel matrix.

torch.Size([12004, 2])
We keep 1.09e+06/3.45e+07 =  3% of the original kernel matrix.

torch.Size([1780, 2])
We keep 3.60e+04/4.13e+05 =  8% of the original kernel matrix.

torch.Size([8957, 2])
We keep 5.72e+05/1.34e+07 =  4% of the original kernel matrix.

torch.Size([13023, 2])
We keep 1.70e+06/5.25e+07 =  3% of the original kernel matrix.

torch.Size([20316, 2])
We keep 3.14e+06/1.51e+08 =  2% of the original kernel matrix.

torch.Size([1972, 2])
We keep 4.72e+04/6.04e+05 =  7% of the original kernel matrix.

torch.Size([9305, 2])
We keep 6.58e+05/1.62e+07 =  4% of the original kernel matrix.

torch.Size([14344, 2])
We keep 1.97e+06/5.73e+07 =  3% of the original kernel matrix.

torch.Size([21228, 2])
We keep 3.21e+06/1.58e+08 =  2% of the original kernel matrix.

torch.Size([3169, 2])
We keep 1.04e+05/1.69e+06 =  6% of the original kernel matrix.

torch.Size([11071, 2])
We keep 9.05e+05/2.71e+07 =  3% of the original kernel matrix.

torch.Size([4213, 2])
We keep 2.37e+05/4.30e+06 =  5% of the original kernel matrix.

torch.Size([12204, 2])
We keep 1.26e+06/4.32e+07 =  2% of the original kernel matrix.

torch.Size([9111, 2])
We keep 8.03e+05/2.00e+07 =  4% of the original kernel matrix.

torch.Size([17096, 2])
We keep 2.18e+06/9.32e+07 =  2% of the original kernel matrix.

torch.Size([7135, 2])
We keep 6.16e+05/1.30e+07 =  4% of the original kernel matrix.

torch.Size([15073, 2])
We keep 1.86e+06/7.53e+07 =  2% of the original kernel matrix.

torch.Size([4815, 2])
We keep 2.46e+05/4.72e+06 =  5% of the original kernel matrix.

torch.Size([12986, 2])
We keep 1.30e+06/4.53e+07 =  2% of the original kernel matrix.

torch.Size([2381, 2])
We keep 7.08e+04/9.60e+05 =  7% of the original kernel matrix.

torch.Size([10099, 2])
We keep 7.68e+05/2.04e+07 =  3% of the original kernel matrix.

torch.Size([7591, 2])
We keep 5.09e+05/1.20e+07 =  4% of the original kernel matrix.

torch.Size([15786, 2])
We keep 1.82e+06/7.22e+07 =  2% of the original kernel matrix.

torch.Size([11054, 2])
We keep 1.13e+06/3.20e+07 =  3% of the original kernel matrix.

torch.Size([18729, 2])
We keep 2.58e+06/1.18e+08 =  2% of the original kernel matrix.

torch.Size([4868, 2])
We keep 2.47e+05/4.85e+06 =  5% of the original kernel matrix.

torch.Size([12932, 2])
We keep 1.29e+06/4.59e+07 =  2% of the original kernel matrix.

torch.Size([17509, 2])
We keep 2.60e+06/1.02e+08 =  2% of the original kernel matrix.

torch.Size([23578, 2])
We keep 3.99e+06/2.11e+08 =  1% of the original kernel matrix.

torch.Size([2167, 2])
We keep 5.46e+04/7.60e+05 =  7% of the original kernel matrix.

torch.Size([9703, 2])
We keep 6.79e+05/1.82e+07 =  3% of the original kernel matrix.

torch.Size([5961, 2])
We keep 4.40e+05/8.09e+06 =  5% of the original kernel matrix.

torch.Size([14203, 2])
We keep 1.56e+06/5.93e+07 =  2% of the original kernel matrix.

torch.Size([2140, 2])
We keep 7.82e+04/8.23e+05 =  9% of the original kernel matrix.

torch.Size([9412, 2])
We keep 7.22e+05/1.89e+07 =  3% of the original kernel matrix.

torch.Size([2082, 2])
We keep 5.84e+04/8.30e+05 =  7% of the original kernel matrix.

torch.Size([9295, 2])
We keep 7.05e+05/1.90e+07 =  3% of the original kernel matrix.

torch.Size([4881, 2])
We keep 2.90e+05/5.39e+06 =  5% of the original kernel matrix.

torch.Size([13141, 2])
We keep 1.38e+06/4.84e+07 =  2% of the original kernel matrix.

torch.Size([3453, 2])
We keep 1.35e+05/2.32e+06 =  5% of the original kernel matrix.

torch.Size([11330, 2])
We keep 1.01e+06/3.17e+07 =  3% of the original kernel matrix.

torch.Size([3189, 2])
We keep 1.20e+05/1.87e+06 =  6% of the original kernel matrix.

torch.Size([11058, 2])
We keep 9.51e+05/2.85e+07 =  3% of the original kernel matrix.

torch.Size([5017, 2])
We keep 2.84e+05/5.52e+06 =  5% of the original kernel matrix.

torch.Size([13151, 2])
We keep 1.37e+06/4.90e+07 =  2% of the original kernel matrix.

torch.Size([3938, 2])
We keep 1.78e+05/3.21e+06 =  5% of the original kernel matrix.

torch.Size([12041, 2])
We keep 1.14e+06/3.74e+07 =  3% of the original kernel matrix.

torch.Size([4284, 2])
We keep 1.94e+05/3.65e+06 =  5% of the original kernel matrix.

torch.Size([12384, 2])
We keep 1.20e+06/3.98e+07 =  3% of the original kernel matrix.

torch.Size([3119, 2])
We keep 1.47e+05/2.14e+06 =  6% of the original kernel matrix.

torch.Size([10790, 2])
We keep 9.96e+05/3.05e+07 =  3% of the original kernel matrix.

torch.Size([10475, 2])
We keep 9.86e+05/2.89e+07 =  3% of the original kernel matrix.

torch.Size([17956, 2])
We keep 2.47e+06/1.12e+08 =  2% of the original kernel matrix.

torch.Size([17009, 2])
We keep 2.64e+06/1.11e+08 =  2% of the original kernel matrix.

torch.Size([23145, 2])
We keep 4.12e+06/2.19e+08 =  1% of the original kernel matrix.

torch.Size([7216, 2])
We keep 5.00e+05/1.12e+07 =  4% of the original kernel matrix.

torch.Size([15480, 2])
We keep 1.77e+06/6.98e+07 =  2% of the original kernel matrix.

torch.Size([3563, 2])
We keep 1.43e+05/2.41e+06 =  5% of the original kernel matrix.

torch.Size([11515, 2])
We keep 1.03e+06/3.24e+07 =  3% of the original kernel matrix.

torch.Size([3462, 2])
We keep 1.28e+05/2.06e+06 =  6% of the original kernel matrix.

torch.Size([11421, 2])
We keep 9.71e+05/2.99e+07 =  3% of the original kernel matrix.

torch.Size([2339, 2])
We keep 7.94e+04/1.07e+06 =  7% of the original kernel matrix.

torch.Size([9778, 2])
We keep 7.77e+05/2.16e+07 =  3% of the original kernel matrix.

torch.Size([3822, 2])
We keep 1.66e+05/2.70e+06 =  6% of the original kernel matrix.

torch.Size([11887, 2])
We keep 1.07e+06/3.43e+07 =  3% of the original kernel matrix.

torch.Size([3443, 2])
We keep 1.34e+05/2.20e+06 =  6% of the original kernel matrix.

torch.Size([11369, 2])
We keep 9.91e+05/3.09e+07 =  3% of the original kernel matrix.

torch.Size([4316, 2])
We keep 1.93e+05/3.54e+06 =  5% of the original kernel matrix.

torch.Size([12462, 2])
We keep 1.19e+06/3.92e+07 =  3% of the original kernel matrix.

torch.Size([6036, 2])
We keep 3.34e+05/7.08e+06 =  4% of the original kernel matrix.

torch.Size([14305, 2])
We keep 1.50e+06/5.54e+07 =  2% of the original kernel matrix.

torch.Size([5905, 2])
We keep 3.56e+05/7.65e+06 =  4% of the original kernel matrix.

torch.Size([14043, 2])
We keep 1.54e+06/5.76e+07 =  2% of the original kernel matrix.

torch.Size([6834, 2])
We keep 1.13e+06/1.36e+07 =  8% of the original kernel matrix.

torch.Size([14887, 2])
We keep 1.90e+06/7.68e+07 =  2% of the original kernel matrix.

torch.Size([5407, 2])
We keep 3.13e+05/6.48e+06 =  4% of the original kernel matrix.

torch.Size([13593, 2])
We keep 1.45e+06/5.31e+07 =  2% of the original kernel matrix.

torch.Size([13745, 2])
We keep 1.94e+06/6.49e+07 =  2% of the original kernel matrix.

torch.Size([20638, 2])
We keep 3.39e+06/1.68e+08 =  2% of the original kernel matrix.

torch.Size([6183, 2])
We keep 3.77e+05/8.07e+06 =  4% of the original kernel matrix.

torch.Size([14362, 2])
We keep 1.56e+06/5.92e+07 =  2% of the original kernel matrix.

torch.Size([3968, 2])
We keep 1.47e+05/2.67e+06 =  5% of the original kernel matrix.

torch.Size([12161, 2])
We keep 1.06e+06/3.41e+07 =  3% of the original kernel matrix.

torch.Size([4413, 2])
We keep 2.09e+05/4.00e+06 =  5% of the original kernel matrix.

torch.Size([12520, 2])
We keep 1.24e+06/4.17e+07 =  2% of the original kernel matrix.

torch.Size([1352, 2])
We keep 2.53e+04/2.67e+05 =  9% of the original kernel matrix.

torch.Size([8109, 2])
We keep 4.90e+05/1.08e+07 =  4% of the original kernel matrix.

torch.Size([14026, 2])
We keep 1.83e+06/5.81e+07 =  3% of the original kernel matrix.

torch.Size([20871, 2])
We keep 3.22e+06/1.59e+08 =  2% of the original kernel matrix.

torch.Size([2128, 2])
We keep 5.12e+04/6.54e+05 =  7% of the original kernel matrix.

torch.Size([9640, 2])
We keep 6.63e+05/1.69e+07 =  3% of the original kernel matrix.

torch.Size([12395, 2])
We keep 1.85e+06/4.81e+07 =  3% of the original kernel matrix.

torch.Size([19581, 2])
We keep 2.98e+06/1.45e+08 =  2% of the original kernel matrix.

torch.Size([1369, 2])
We keep 1.90e+04/1.99e+05 =  9% of the original kernel matrix.

torch.Size([8160, 2])
We keep 4.50e+05/9.30e+06 =  4% of the original kernel matrix.

torch.Size([7069, 2])
We keep 5.19e+05/1.20e+07 =  4% of the original kernel matrix.

torch.Size([15216, 2])
We keep 1.81e+06/7.22e+07 =  2% of the original kernel matrix.

torch.Size([7193, 2])
We keep 5.31e+05/1.22e+07 =  4% of the original kernel matrix.

torch.Size([15329, 2])
We keep 1.82e+06/7.29e+07 =  2% of the original kernel matrix.

torch.Size([2984, 2])
We keep 1.02e+05/1.67e+06 =  6% of the original kernel matrix.

torch.Size([10821, 2])
We keep 8.70e+05/2.69e+07 =  3% of the original kernel matrix.

torch.Size([5348, 2])
We keep 3.04e+05/5.67e+06 =  5% of the original kernel matrix.

torch.Size([13502, 2])
We keep 1.38e+06/4.96e+07 =  2% of the original kernel matrix.

torch.Size([4777, 2])
We keep 2.35e+05/4.60e+06 =  5% of the original kernel matrix.

torch.Size([12909, 2])
We keep 1.28e+06/4.47e+07 =  2% of the original kernel matrix.

torch.Size([1815, 2])
We keep 3.88e+04/4.97e+05 =  7% of the original kernel matrix.

torch.Size([9145, 2])
We keep 6.05e+05/1.47e+07 =  4% of the original kernel matrix.

torch.Size([1651, 2])
We keep 3.61e+04/3.99e+05 =  9% of the original kernel matrix.

torch.Size([8704, 2])
We keep 5.74e+05/1.32e+07 =  4% of the original kernel matrix.

torch.Size([3731, 2])
We keep 1.55e+05/2.49e+06 =  6% of the original kernel matrix.

torch.Size([11554, 2])
We keep 1.02e+06/3.29e+07 =  3% of the original kernel matrix.

torch.Size([7548, 2])
We keep 5.03e+05/1.17e+07 =  4% of the original kernel matrix.

torch.Size([15721, 2])
We keep 1.77e+06/7.14e+07 =  2% of the original kernel matrix.

torch.Size([3175, 2])
We keep 1.13e+05/1.76e+06 =  6% of the original kernel matrix.

torch.Size([11130, 2])
We keep 9.22e+05/2.77e+07 =  3% of the original kernel matrix.

torch.Size([9735, 2])
We keep 9.20e+05/2.43e+07 =  3% of the original kernel matrix.

torch.Size([17699, 2])
We keep 2.31e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([4073, 2])
We keep 1.95e+05/3.44e+06 =  5% of the original kernel matrix.

torch.Size([12198, 2])
We keep 1.16e+06/3.87e+07 =  2% of the original kernel matrix.

torch.Size([4582, 2])
We keep 2.33e+05/4.29e+06 =  5% of the original kernel matrix.

torch.Size([12668, 2])
We keep 1.25e+06/4.32e+07 =  2% of the original kernel matrix.

torch.Size([6645, 2])
We keep 5.18e+05/1.06e+07 =  4% of the original kernel matrix.

torch.Size([14616, 2])
We keep 1.74e+06/6.77e+07 =  2% of the original kernel matrix.

torch.Size([7315, 2])
We keep 7.16e+05/1.28e+07 =  5% of the original kernel matrix.

torch.Size([15387, 2])
We keep 1.86e+06/7.47e+07 =  2% of the original kernel matrix.

torch.Size([3853, 2])
We keep 1.62e+05/2.89e+06 =  5% of the original kernel matrix.

torch.Size([11771, 2])
We keep 1.10e+06/3.55e+07 =  3% of the original kernel matrix.

torch.Size([11585, 2])
We keep 1.27e+06/3.71e+07 =  3% of the original kernel matrix.

torch.Size([19064, 2])
We keep 2.73e+06/1.27e+08 =  2% of the original kernel matrix.

torch.Size([2658, 2])
We keep 8.25e+04/1.24e+06 =  6% of the original kernel matrix.

torch.Size([10317, 2])
We keep 8.28e+05/2.32e+07 =  3% of the original kernel matrix.

torch.Size([5735, 2])
We keep 4.13e+05/7.44e+06 =  5% of the original kernel matrix.

torch.Size([13955, 2])
We keep 1.53e+06/5.69e+07 =  2% of the original kernel matrix.

torch.Size([3511, 2])
We keep 1.26e+05/2.11e+06 =  5% of the original kernel matrix.

torch.Size([11708, 2])
We keep 9.80e+05/3.03e+07 =  3% of the original kernel matrix.

torch.Size([8171, 2])
We keep 5.98e+05/1.42e+07 =  4% of the original kernel matrix.

torch.Size([16278, 2])
We keep 1.91e+06/7.84e+07 =  2% of the original kernel matrix.

torch.Size([10731, 2])
We keep 1.01e+06/2.86e+07 =  3% of the original kernel matrix.

torch.Size([18386, 2])
We keep 2.48e+06/1.12e+08 =  2% of the original kernel matrix.

torch.Size([3750, 2])
We keep 1.83e+05/3.19e+06 =  5% of the original kernel matrix.

torch.Size([11607, 2])
We keep 1.12e+06/3.72e+07 =  3% of the original kernel matrix.

torch.Size([5169, 2])
We keep 3.39e+05/6.08e+06 =  5% of the original kernel matrix.

torch.Size([13300, 2])
We keep 1.42e+06/5.14e+07 =  2% of the original kernel matrix.

torch.Size([8497, 2])
We keep 6.53e+05/1.66e+07 =  3% of the original kernel matrix.

torch.Size([16752, 2])
We keep 2.05e+06/8.50e+07 =  2% of the original kernel matrix.

torch.Size([15985, 2])
We keep 2.36e+06/8.54e+07 =  2% of the original kernel matrix.

torch.Size([22434, 2])
We keep 3.71e+06/1.93e+08 =  1% of the original kernel matrix.

torch.Size([4208, 2])
We keep 1.92e+05/3.51e+06 =  5% of the original kernel matrix.

torch.Size([12283, 2])
We keep 1.18e+06/3.90e+07 =  3% of the original kernel matrix.

torch.Size([2107, 2])
We keep 6.00e+04/8.17e+05 =  7% of the original kernel matrix.

torch.Size([9445, 2])
We keep 7.20e+05/1.88e+07 =  3% of the original kernel matrix.

torch.Size([5334, 2])
We keep 2.66e+05/5.53e+06 =  4% of the original kernel matrix.

torch.Size([13546, 2])
We keep 1.35e+06/4.90e+07 =  2% of the original kernel matrix.

torch.Size([5034, 2])
We keep 2.94e+05/5.48e+06 =  5% of the original kernel matrix.

torch.Size([13360, 2])
We keep 1.35e+06/4.88e+07 =  2% of the original kernel matrix.

torch.Size([6350, 2])
We keep 5.29e+05/8.76e+06 =  6% of the original kernel matrix.

torch.Size([14498, 2])
We keep 1.62e+06/6.17e+07 =  2% of the original kernel matrix.

torch.Size([3307, 2])
We keep 1.25e+05/2.12e+06 =  5% of the original kernel matrix.

torch.Size([11162, 2])
We keep 9.79e+05/3.03e+07 =  3% of the original kernel matrix.

torch.Size([6545, 2])
We keep 4.61e+05/9.46e+06 =  4% of the original kernel matrix.

torch.Size([14711, 2])
We keep 1.66e+06/6.41e+07 =  2% of the original kernel matrix.

torch.Size([3123, 2])
We keep 1.01e+05/1.77e+06 =  5% of the original kernel matrix.

torch.Size([11074, 2])
We keep 9.02e+05/2.77e+07 =  3% of the original kernel matrix.

torch.Size([8750, 2])
We keep 9.12e+05/1.93e+07 =  4% of the original kernel matrix.

torch.Size([16472, 2])
We keep 2.12e+06/9.15e+07 =  2% of the original kernel matrix.

torch.Size([1814, 2])
We keep 3.75e+04/4.91e+05 =  7% of the original kernel matrix.

torch.Size([9118, 2])
We keep 5.92e+05/1.46e+07 =  4% of the original kernel matrix.

torch.Size([12800, 2])
We keep 1.62e+06/4.83e+07 =  3% of the original kernel matrix.

torch.Size([20107, 2])
We keep 3.02e+06/1.45e+08 =  2% of the original kernel matrix.

torch.Size([4543, 2])
We keep 2.21e+05/3.95e+06 =  5% of the original kernel matrix.

torch.Size([12768, 2])
We keep 1.23e+06/4.14e+07 =  2% of the original kernel matrix.

torch.Size([20141, 2])
We keep 3.19e+06/1.46e+08 =  2% of the original kernel matrix.

torch.Size([25735, 2])
We keep 4.59e+06/2.52e+08 =  1% of the original kernel matrix.

torch.Size([4207, 2])
We keep 1.63e+05/2.92e+06 =  5% of the original kernel matrix.

torch.Size([12403, 2])
We keep 1.10e+06/3.56e+07 =  3% of the original kernel matrix.

torch.Size([12028, 2])
We keep 2.26e+06/4.46e+07 =  5% of the original kernel matrix.

torch.Size([19399, 2])
We keep 2.89e+06/1.39e+08 =  2% of the original kernel matrix.

torch.Size([3015, 2])
We keep 1.03e+05/1.71e+06 =  6% of the original kernel matrix.

torch.Size([10822, 2])
We keep 9.05e+05/2.73e+07 =  3% of the original kernel matrix.

torch.Size([3346, 2])
We keep 1.32e+05/2.11e+06 =  6% of the original kernel matrix.

torch.Size([11342, 2])
We keep 9.79e+05/3.02e+07 =  3% of the original kernel matrix.

torch.Size([5961, 2])
We keep 3.81e+05/8.23e+06 =  4% of the original kernel matrix.

torch.Size([14121, 2])
We keep 1.58e+06/5.98e+07 =  2% of the original kernel matrix.

torch.Size([4419, 2])
We keep 1.94e+05/3.71e+06 =  5% of the original kernel matrix.

torch.Size([12544, 2])
We keep 1.17e+06/4.02e+07 =  2% of the original kernel matrix.

torch.Size([8385, 2])
We keep 5.89e+05/1.52e+07 =  3% of the original kernel matrix.

torch.Size([16431, 2])
We keep 1.94e+06/8.13e+07 =  2% of the original kernel matrix.

torch.Size([8181, 2])
We keep 6.57e+05/1.72e+07 =  3% of the original kernel matrix.

torch.Size([16084, 2])
We keep 2.06e+06/8.65e+07 =  2% of the original kernel matrix.

torch.Size([4369, 2])
We keep 2.58e+05/3.53e+06 =  7% of the original kernel matrix.

torch.Size([12438, 2])
We keep 1.18e+06/3.91e+07 =  3% of the original kernel matrix.

torch.Size([4667, 2])
We keep 2.08e+05/4.15e+06 =  5% of the original kernel matrix.

torch.Size([12839, 2])
We keep 1.23e+06/4.24e+07 =  2% of the original kernel matrix.

torch.Size([9882, 2])
We keep 9.02e+05/2.48e+07 =  3% of the original kernel matrix.

torch.Size([17709, 2])
We keep 2.34e+06/1.04e+08 =  2% of the original kernel matrix.

torch.Size([1634, 2])
We keep 3.44e+04/3.98e+05 =  8% of the original kernel matrix.

torch.Size([8783, 2])
We keep 5.67e+05/1.32e+07 =  4% of the original kernel matrix.

torch.Size([1331, 2])
We keep 2.47e+04/2.46e+05 = 10% of the original kernel matrix.

torch.Size([7987, 2])
We keep 4.75e+05/1.03e+07 =  4% of the original kernel matrix.

torch.Size([1215, 2])
We keep 1.53e+04/1.58e+05 =  9% of the original kernel matrix.

torch.Size([7929, 2])
We keep 4.16e+05/8.30e+06 =  5% of the original kernel matrix.

torch.Size([3459, 2])
We keep 1.38e+05/2.20e+06 =  6% of the original kernel matrix.

torch.Size([11479, 2])
We keep 1.01e+06/3.09e+07 =  3% of the original kernel matrix.

torch.Size([16792, 2])
We keep 4.40e+06/1.20e+08 =  3% of the original kernel matrix.

torch.Size([23038, 2])
We keep 4.20e+06/2.28e+08 =  1% of the original kernel matrix.

torch.Size([3397, 2])
We keep 1.19e+05/1.93e+06 =  6% of the original kernel matrix.

torch.Size([11375, 2])
We keep 9.40e+05/2.90e+07 =  3% of the original kernel matrix.

torch.Size([3170, 2])
We keep 1.97e+05/1.97e+06 =  9% of the original kernel matrix.

torch.Size([10756, 2])
We keep 9.60e+05/2.93e+07 =  3% of the original kernel matrix.

torch.Size([1834, 2])
We keep 4.43e+04/5.76e+05 =  7% of the original kernel matrix.

torch.Size([9049, 2])
We keep 6.32e+05/1.58e+07 =  3% of the original kernel matrix.

torch.Size([14207, 2])
We keep 2.20e+06/6.45e+07 =  3% of the original kernel matrix.

torch.Size([21008, 2])
We keep 3.34e+06/1.67e+08 =  1% of the original kernel matrix.

torch.Size([2681, 2])
We keep 1.50e+05/1.43e+06 = 10% of the original kernel matrix.

torch.Size([10305, 2])
We keep 8.59e+05/2.49e+07 =  3% of the original kernel matrix.

torch.Size([3826, 2])
We keep 1.81e+05/2.94e+06 =  6% of the original kernel matrix.

torch.Size([11825, 2])
We keep 1.11e+06/3.57e+07 =  3% of the original kernel matrix.

torch.Size([5753, 2])
We keep 3.20e+05/6.76e+06 =  4% of the original kernel matrix.

torch.Size([13939, 2])
We keep 1.47e+06/5.42e+07 =  2% of the original kernel matrix.

torch.Size([4920, 2])
We keep 2.72e+05/4.92e+06 =  5% of the original kernel matrix.

torch.Size([13037, 2])
We keep 1.31e+06/4.62e+07 =  2% of the original kernel matrix.

torch.Size([4019, 2])
We keep 1.74e+05/2.98e+06 =  5% of the original kernel matrix.

torch.Size([12108, 2])
We keep 1.11e+06/3.60e+07 =  3% of the original kernel matrix.

torch.Size([7897, 2])
We keep 6.20e+05/1.46e+07 =  4% of the original kernel matrix.

torch.Size([15835, 2])
We keep 1.95e+06/7.97e+07 =  2% of the original kernel matrix.

torch.Size([2730, 2])
We keep 7.30e+04/1.09e+06 =  6% of the original kernel matrix.

torch.Size([10545, 2])
We keep 7.76e+05/2.18e+07 =  3% of the original kernel matrix.

torch.Size([2483, 2])
We keep 7.25e+04/1.12e+06 =  6% of the original kernel matrix.

torch.Size([10239, 2])
We keep 7.98e+05/2.21e+07 =  3% of the original kernel matrix.

torch.Size([3437, 2])
We keep 1.76e+05/2.33e+06 =  7% of the original kernel matrix.

torch.Size([11350, 2])
We keep 1.02e+06/3.18e+07 =  3% of the original kernel matrix.

torch.Size([3457, 2])
We keep 1.31e+05/2.17e+06 =  6% of the original kernel matrix.

torch.Size([11453, 2])
We keep 9.99e+05/3.07e+07 =  3% of the original kernel matrix.

torch.Size([5531, 2])
We keep 3.00e+05/6.45e+06 =  4% of the original kernel matrix.

torch.Size([13947, 2])
We keep 1.46e+06/5.29e+07 =  2% of the original kernel matrix.

torch.Size([5764, 2])
We keep 3.80e+05/7.68e+06 =  4% of the original kernel matrix.

torch.Size([13826, 2])
We keep 1.54e+06/5.78e+07 =  2% of the original kernel matrix.

torch.Size([4000, 2])
We keep 1.82e+05/3.13e+06 =  5% of the original kernel matrix.

torch.Size([12108, 2])
We keep 1.12e+06/3.68e+07 =  3% of the original kernel matrix.

torch.Size([11447, 2])
We keep 1.29e+06/3.90e+07 =  3% of the original kernel matrix.

torch.Size([19025, 2])
We keep 2.83e+06/1.30e+08 =  2% of the original kernel matrix.

torch.Size([6126, 2])
We keep 3.34e+05/7.43e+06 =  4% of the original kernel matrix.

torch.Size([14550, 2])
We keep 1.48e+06/5.68e+07 =  2% of the original kernel matrix.

torch.Size([3080, 2])
We keep 9.88e+04/1.57e+06 =  6% of the original kernel matrix.

torch.Size([11024, 2])
We keep 8.93e+05/2.61e+07 =  3% of the original kernel matrix.

torch.Size([12504, 2])
We keep 1.45e+06/4.79e+07 =  3% of the original kernel matrix.

torch.Size([19966, 2])
We keep 2.97e+06/1.44e+08 =  2% of the original kernel matrix.

torch.Size([15921, 2])
We keep 1.14e+07/1.16e+08 =  9% of the original kernel matrix.

torch.Size([22532, 2])
We keep 4.09e+06/2.24e+08 =  1% of the original kernel matrix.

torch.Size([12639, 2])
We keep 1.93e+06/5.02e+07 =  3% of the original kernel matrix.

torch.Size([20074, 2])
We keep 2.95e+06/1.48e+08 =  1% of the original kernel matrix.

torch.Size([8676, 2])
We keep 7.95e+05/1.95e+07 =  4% of the original kernel matrix.

torch.Size([16562, 2])
We keep 2.15e+06/9.21e+07 =  2% of the original kernel matrix.

torch.Size([4248, 2])
We keep 4.02e+05/5.05e+06 =  7% of the original kernel matrix.

torch.Size([12079, 2])
We keep 1.32e+06/4.69e+07 =  2% of the original kernel matrix.

torch.Size([12260, 2])
We keep 1.53e+06/4.38e+07 =  3% of the original kernel matrix.

torch.Size([19508, 2])
We keep 2.93e+06/1.38e+08 =  2% of the original kernel matrix.

torch.Size([16332, 2])
We keep 4.44e+06/9.32e+07 =  4% of the original kernel matrix.

torch.Size([22722, 2])
We keep 3.90e+06/2.01e+08 =  1% of the original kernel matrix.

torch.Size([30702, 2])
We keep 2.41e+07/4.60e+08 =  5% of the original kernel matrix.

torch.Size([31580, 2])
We keep 7.07e+06/4.47e+08 =  1% of the original kernel matrix.

torch.Size([5423, 2])
We keep 2.77e+05/5.81e+06 =  4% of the original kernel matrix.

torch.Size([13774, 2])
We keep 1.39e+06/5.03e+07 =  2% of the original kernel matrix.

torch.Size([183390, 2])
We keep 2.06e+08/1.66e+10 =  1% of the original kernel matrix.

torch.Size([77847, 2])
We keep 3.26e+07/2.69e+09 =  1% of the original kernel matrix.

torch.Size([67429, 2])
We keep 4.90e+07/2.59e+09 =  1% of the original kernel matrix.

torch.Size([45426, 2])
We keep 1.46e+07/1.06e+09 =  1% of the original kernel matrix.

torch.Size([41089, 2])
We keep 2.52e+07/9.09e+08 =  2% of the original kernel matrix.

torch.Size([35947, 2])
We keep 9.34e+06/6.28e+08 =  1% of the original kernel matrix.

torch.Size([47573, 2])
We keep 3.12e+07/1.11e+09 =  2% of the original kernel matrix.

torch.Size([39279, 2])
We keep 1.05e+07/6.95e+08 =  1% of the original kernel matrix.

torch.Size([3955, 2])
We keep 1.44e+05/2.58e+06 =  5% of the original kernel matrix.

torch.Size([12097, 2])
We keep 1.04e+06/3.35e+07 =  3% of the original kernel matrix.

torch.Size([12101, 2])
We keep 3.05e+06/4.68e+07 =  6% of the original kernel matrix.

torch.Size([19422, 2])
We keep 2.93e+06/1.43e+08 =  2% of the original kernel matrix.

torch.Size([436573, 2])
We keep 8.65e+08/8.43e+10 =  1% of the original kernel matrix.

torch.Size([121264, 2])
We keep 6.72e+07/6.05e+09 =  1% of the original kernel matrix.

torch.Size([336416, 2])
We keep 5.28e+08/5.04e+10 =  1% of the original kernel matrix.

torch.Size([107954, 2])
We keep 5.27e+07/4.68e+09 =  1% of the original kernel matrix.

torch.Size([6090, 2])
We keep 3.68e+05/7.86e+06 =  4% of the original kernel matrix.

torch.Size([14400, 2])
We keep 1.52e+06/5.84e+07 =  2% of the original kernel matrix.

torch.Size([40591, 2])
We keep 1.20e+07/7.33e+08 =  1% of the original kernel matrix.

torch.Size([36839, 2])
We keep 8.69e+06/5.64e+08 =  1% of the original kernel matrix.

torch.Size([117016, 2])
We keep 1.90e+08/7.87e+09 =  2% of the original kernel matrix.

torch.Size([61300, 2])
We keep 2.40e+07/1.85e+09 =  1% of the original kernel matrix.

torch.Size([28987, 2])
We keep 1.42e+07/3.84e+08 =  3% of the original kernel matrix.

torch.Size([31184, 2])
We keep 6.91e+06/4.08e+08 =  1% of the original kernel matrix.

torch.Size([28207, 2])
We keep 1.26e+07/3.67e+08 =  3% of the original kernel matrix.

torch.Size([30378, 2])
We keep 6.69e+06/3.99e+08 =  1% of the original kernel matrix.

torch.Size([28534, 2])
We keep 1.80e+07/5.46e+08 =  3% of the original kernel matrix.

torch.Size([29137, 2])
We keep 7.80e+06/4.87e+08 =  1% of the original kernel matrix.

torch.Size([17679, 2])
We keep 5.04e+06/1.18e+08 =  4% of the original kernel matrix.

torch.Size([23900, 2])
We keep 4.04e+06/2.26e+08 =  1% of the original kernel matrix.

torch.Size([18522, 2])
We keep 5.62e+06/1.28e+08 =  4% of the original kernel matrix.

torch.Size([24453, 2])
We keep 4.25e+06/2.36e+08 =  1% of the original kernel matrix.

torch.Size([6303, 2])
We keep 1.52e+06/1.26e+07 = 12% of the original kernel matrix.

torch.Size([14413, 2])
We keep 1.86e+06/7.39e+07 =  2% of the original kernel matrix.

torch.Size([50707, 2])
We keep 1.58e+08/1.95e+09 =  8% of the original kernel matrix.

torch.Size([39982, 2])
We keep 1.25e+07/9.21e+08 =  1% of the original kernel matrix.

torch.Size([14963, 2])
We keep 7.14e+06/8.65e+07 =  8% of the original kernel matrix.

torch.Size([21508, 2])
We keep 3.62e+06/1.94e+08 =  1% of the original kernel matrix.

torch.Size([7920, 2])
We keep 6.38e+05/1.51e+07 =  4% of the original kernel matrix.

torch.Size([15966, 2])
We keep 1.96e+06/8.10e+07 =  2% of the original kernel matrix.

torch.Size([26493, 2])
We keep 9.05e+06/3.06e+08 =  2% of the original kernel matrix.

torch.Size([29817, 2])
We keep 6.21e+06/3.65e+08 =  1% of the original kernel matrix.

torch.Size([22345, 2])
We keep 1.12e+07/2.34e+08 =  4% of the original kernel matrix.

torch.Size([26443, 2])
We keep 5.58e+06/3.19e+08 =  1% of the original kernel matrix.

torch.Size([258520, 2])
We keep 4.00e+08/3.67e+10 =  1% of the original kernel matrix.

torch.Size([94882, 2])
We keep 4.62e+07/3.99e+09 =  1% of the original kernel matrix.

torch.Size([68890, 2])
We keep 3.31e+07/2.04e+09 =  1% of the original kernel matrix.

torch.Size([46717, 2])
We keep 1.33e+07/9.40e+08 =  1% of the original kernel matrix.

torch.Size([6465, 2])
We keep 4.77e+05/1.02e+07 =  4% of the original kernel matrix.

torch.Size([14571, 2])
We keep 1.70e+06/6.66e+07 =  2% of the original kernel matrix.

torch.Size([47263, 2])
We keep 1.87e+07/1.01e+09 =  1% of the original kernel matrix.

torch.Size([39143, 2])
We keep 9.91e+06/6.61e+08 =  1% of the original kernel matrix.

torch.Size([10708, 2])
We keep 1.26e+06/3.56e+07 =  3% of the original kernel matrix.

torch.Size([18340, 2])
We keep 2.72e+06/1.24e+08 =  2% of the original kernel matrix.

torch.Size([17462, 2])
We keep 3.21e+06/1.07e+08 =  3% of the original kernel matrix.

torch.Size([23711, 2])
We keep 4.03e+06/2.15e+08 =  1% of the original kernel matrix.

torch.Size([62897, 2])
We keep 2.90e+07/1.77e+09 =  1% of the original kernel matrix.

torch.Size([44982, 2])
We keep 1.23e+07/8.76e+08 =  1% of the original kernel matrix.

torch.Size([44515, 2])
We keep 4.27e+07/1.35e+09 =  3% of the original kernel matrix.

torch.Size([37543, 2])
We keep 1.15e+07/7.67e+08 =  1% of the original kernel matrix.

torch.Size([140861, 2])
We keep 1.29e+08/9.71e+09 =  1% of the original kernel matrix.

torch.Size([67706, 2])
We keep 2.60e+07/2.05e+09 =  1% of the original kernel matrix.

torch.Size([2037, 2])
We keep 5.16e+04/6.43e+05 =  8% of the original kernel matrix.

torch.Size([9216, 2])
We keep 6.49e+05/1.67e+07 =  3% of the original kernel matrix.

torch.Size([8850, 2])
We keep 7.03e+05/1.79e+07 =  3% of the original kernel matrix.

torch.Size([16665, 2])
We keep 2.05e+06/8.81e+07 =  2% of the original kernel matrix.

torch.Size([79888, 2])
We keep 6.10e+07/3.43e+09 =  1% of the original kernel matrix.

torch.Size([49488, 2])
We keep 1.66e+07/1.22e+09 =  1% of the original kernel matrix.

torch.Size([45904, 2])
We keep 5.77e+07/1.39e+09 =  4% of the original kernel matrix.

torch.Size([37579, 2])
We keep 1.14e+07/7.78e+08 =  1% of the original kernel matrix.

torch.Size([32081, 2])
We keep 1.31e+07/5.02e+08 =  2% of the original kernel matrix.

torch.Size([31912, 2])
We keep 7.47e+06/4.67e+08 =  1% of the original kernel matrix.

torch.Size([5821, 2])
We keep 4.05e+05/8.08e+06 =  5% of the original kernel matrix.

torch.Size([13893, 2])
We keep 1.57e+06/5.92e+07 =  2% of the original kernel matrix.

torch.Size([221648, 2])
We keep 2.33e+08/2.64e+10 =  0% of the original kernel matrix.

torch.Size([86555, 2])
We keep 3.96e+07/3.39e+09 =  1% of the original kernel matrix.

torch.Size([17896, 2])
We keep 2.58e+06/1.09e+08 =  2% of the original kernel matrix.

torch.Size([23985, 2])
We keep 4.04e+06/2.18e+08 =  1% of the original kernel matrix.

torch.Size([24588, 2])
We keep 4.64e+06/2.46e+08 =  1% of the original kernel matrix.

torch.Size([28343, 2])
We keep 5.59e+06/3.27e+08 =  1% of the original kernel matrix.

torch.Size([102434, 2])
We keep 7.09e+07/4.70e+09 =  1% of the original kernel matrix.

torch.Size([56913, 2])
We keep 1.90e+07/1.43e+09 =  1% of the original kernel matrix.

torch.Size([3977, 2])
We keep 1.90e+05/3.49e+06 =  5% of the original kernel matrix.

torch.Size([11889, 2])
We keep 1.17e+06/3.90e+07 =  3% of the original kernel matrix.

torch.Size([62574, 2])
We keep 2.92e+07/1.69e+09 =  1% of the original kernel matrix.

torch.Size([44563, 2])
We keep 1.23e+07/8.56e+08 =  1% of the original kernel matrix.

torch.Size([93414, 2])
We keep 9.21e+07/4.44e+09 =  2% of the original kernel matrix.

torch.Size([54077, 2])
We keep 1.83e+07/1.39e+09 =  1% of the original kernel matrix.

torch.Size([312890, 2])
We keep 7.59e+08/5.29e+10 =  1% of the original kernel matrix.

torch.Size([103071, 2])
We keep 5.41e+07/4.79e+09 =  1% of the original kernel matrix.

torch.Size([48200, 2])
We keep 2.23e+07/1.24e+09 =  1% of the original kernel matrix.

torch.Size([39330, 2])
We keep 1.09e+07/7.35e+08 =  1% of the original kernel matrix.

torch.Size([47844, 2])
We keep 1.82e+07/1.01e+09 =  1% of the original kernel matrix.

torch.Size([39214, 2])
We keep 9.91e+06/6.63e+08 =  1% of the original kernel matrix.

torch.Size([13343, 2])
We keep 2.94e+06/6.15e+07 =  4% of the original kernel matrix.

torch.Size([20237, 2])
We keep 3.28e+06/1.63e+08 =  2% of the original kernel matrix.

torch.Size([9442, 2])
We keep 7.59e+05/2.06e+07 =  3% of the original kernel matrix.

torch.Size([17269, 2])
We keep 2.19e+06/9.47e+07 =  2% of the original kernel matrix.

torch.Size([40422, 2])
We keep 3.75e+07/9.80e+08 =  3% of the original kernel matrix.

torch.Size([36116, 2])
We keep 9.56e+06/6.53e+08 =  1% of the original kernel matrix.

torch.Size([13001, 2])
We keep 1.44e+06/4.80e+07 =  2% of the original kernel matrix.

torch.Size([20149, 2])
We keep 2.98e+06/1.44e+08 =  2% of the original kernel matrix.

torch.Size([16265, 2])
We keep 3.14e+06/9.80e+07 =  3% of the original kernel matrix.

torch.Size([22850, 2])
We keep 3.85e+06/2.06e+08 =  1% of the original kernel matrix.

torch.Size([55040, 2])
We keep 1.07e+08/1.85e+09 =  5% of the original kernel matrix.

torch.Size([42075, 2])
We keep 1.17e+07/8.97e+08 =  1% of the original kernel matrix.

torch.Size([24574, 2])
We keep 6.10e+06/2.41e+08 =  2% of the original kernel matrix.

torch.Size([28264, 2])
We keep 5.30e+06/3.23e+08 =  1% of the original kernel matrix.

torch.Size([11263, 2])
We keep 2.34e+06/4.55e+07 =  5% of the original kernel matrix.

torch.Size([18725, 2])
We keep 2.99e+06/1.41e+08 =  2% of the original kernel matrix.

torch.Size([8536, 2])
We keep 7.07e+05/1.64e+07 =  4% of the original kernel matrix.

torch.Size([16525, 2])
We keep 2.00e+06/8.45e+07 =  2% of the original kernel matrix.

torch.Size([220995, 2])
We keep 1.17e+09/6.99e+10 =  1% of the original kernel matrix.

torch.Size([81304, 2])
We keep 6.19e+07/5.51e+09 =  1% of the original kernel matrix.

torch.Size([12220, 2])
We keep 1.80e+06/4.41e+07 =  4% of the original kernel matrix.

torch.Size([19729, 2])
We keep 2.85e+06/1.38e+08 =  2% of the original kernel matrix.

torch.Size([7293, 2])
We keep 4.58e+06/1.82e+07 = 25% of the original kernel matrix.

torch.Size([15485, 2])
We keep 2.08e+06/8.89e+07 =  2% of the original kernel matrix.

torch.Size([17708, 2])
We keep 3.26e+06/1.24e+08 =  2% of the original kernel matrix.

torch.Size([23862, 2])
We keep 4.36e+06/2.32e+08 =  1% of the original kernel matrix.

torch.Size([120385, 2])
We keep 2.41e+08/1.09e+10 =  2% of the original kernel matrix.

torch.Size([61845, 2])
We keep 2.71e+07/2.18e+09 =  1% of the original kernel matrix.

torch.Size([28787, 2])
We keep 8.63e+06/3.86e+08 =  2% of the original kernel matrix.

torch.Size([31483, 2])
We keep 6.76e+06/4.10e+08 =  1% of the original kernel matrix.

torch.Size([18319, 2])
We keep 7.84e+06/1.76e+08 =  4% of the original kernel matrix.

torch.Size([24015, 2])
We keep 4.93e+06/2.77e+08 =  1% of the original kernel matrix.

torch.Size([7883, 2])
We keep 1.73e+06/2.26e+07 =  7% of the original kernel matrix.

torch.Size([16066, 2])
We keep 2.28e+06/9.90e+07 =  2% of the original kernel matrix.

torch.Size([17532, 2])
We keep 9.99e+06/1.78e+08 =  5% of the original kernel matrix.

torch.Size([23042, 2])
We keep 5.05e+06/2.78e+08 =  1% of the original kernel matrix.

torch.Size([15704, 2])
We keep 9.31e+06/9.02e+07 = 10% of the original kernel matrix.

torch.Size([22552, 2])
We keep 3.69e+06/1.98e+08 =  1% of the original kernel matrix.

torch.Size([541717, 2])
We keep 2.53e+09/2.06e+11 =  1% of the original kernel matrix.

torch.Size([134086, 2])
We keep 1.00e+08/9.46e+09 =  1% of the original kernel matrix.

torch.Size([39942, 2])
We keep 1.52e+07/9.15e+08 =  1% of the original kernel matrix.

torch.Size([35621, 2])
We keep 9.50e+06/6.31e+08 =  1% of the original kernel matrix.

torch.Size([17794, 2])
We keep 2.65e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([23901, 2])
We keep 3.96e+06/2.11e+08 =  1% of the original kernel matrix.

torch.Size([137091, 2])
We keep 8.72e+07/8.15e+09 =  1% of the original kernel matrix.

torch.Size([66915, 2])
We keep 2.38e+07/1.88e+09 =  1% of the original kernel matrix.

torch.Size([158456, 2])
We keep 1.12e+08/1.11e+10 =  1% of the original kernel matrix.

torch.Size([72214, 2])
We keep 2.70e+07/2.20e+09 =  1% of the original kernel matrix.

torch.Size([8529, 2])
We keep 2.93e+06/2.51e+07 = 11% of the original kernel matrix.

torch.Size([16718, 2])
We keep 2.38e+06/1.04e+08 =  2% of the original kernel matrix.

torch.Size([113396, 2])
We keep 9.21e+07/5.51e+09 =  1% of the original kernel matrix.

torch.Size([59840, 2])
We keep 2.03e+07/1.55e+09 =  1% of the original kernel matrix.

torch.Size([169137, 2])
We keep 1.44e+08/1.48e+10 =  0% of the original kernel matrix.

torch.Size([75495, 2])
We keep 3.11e+07/2.54e+09 =  1% of the original kernel matrix.

torch.Size([581487, 2])
We keep 1.61e+09/1.38e+11 =  1% of the original kernel matrix.

torch.Size([142574, 2])
We keep 8.32e+07/7.74e+09 =  1% of the original kernel matrix.

torch.Size([15999, 2])
We keep 9.32e+06/9.23e+07 = 10% of the original kernel matrix.

torch.Size([22638, 2])
We keep 3.74e+06/2.00e+08 =  1% of the original kernel matrix.

torch.Size([4069, 2])
We keep 2.53e+05/3.60e+06 =  7% of the original kernel matrix.

torch.Size([12225, 2])
We keep 1.18e+06/3.95e+07 =  2% of the original kernel matrix.

torch.Size([15213, 2])
We keep 1.98e+06/6.97e+07 =  2% of the original kernel matrix.

torch.Size([21851, 2])
We keep 3.46e+06/1.74e+08 =  1% of the original kernel matrix.

torch.Size([16603, 2])
We keep 1.52e+07/1.73e+08 =  8% of the original kernel matrix.

torch.Size([22992, 2])
We keep 4.75e+06/2.74e+08 =  1% of the original kernel matrix.

torch.Size([35500, 2])
We keep 1.21e+07/5.89e+08 =  2% of the original kernel matrix.

torch.Size([34255, 2])
We keep 8.02e+06/5.06e+08 =  1% of the original kernel matrix.

torch.Size([29596, 2])
We keep 9.49e+06/3.63e+08 =  2% of the original kernel matrix.

torch.Size([30251, 2])
We keep 6.21e+06/3.97e+08 =  1% of the original kernel matrix.

torch.Size([3564, 2])
We keep 1.19e+05/2.05e+06 =  5% of the original kernel matrix.

torch.Size([11625, 2])
We keep 9.60e+05/2.98e+07 =  3% of the original kernel matrix.

torch.Size([36465, 2])
We keep 1.50e+07/6.38e+08 =  2% of the original kernel matrix.

torch.Size([35062, 2])
We keep 8.30e+06/5.26e+08 =  1% of the original kernel matrix.

torch.Size([135716, 2])
We keep 9.21e+07/8.20e+09 =  1% of the original kernel matrix.

torch.Size([66370, 2])
We keep 2.40e+07/1.89e+09 =  1% of the original kernel matrix.

torch.Size([65786, 2])
We keep 1.80e+08/5.99e+09 =  2% of the original kernel matrix.

torch.Size([42008, 2])
We keep 2.07e+07/1.61e+09 =  1% of the original kernel matrix.

torch.Size([9498, 2])
We keep 1.68e+06/2.66e+07 =  6% of the original kernel matrix.

torch.Size([17354, 2])
We keep 2.37e+06/1.08e+08 =  2% of the original kernel matrix.

torch.Size([153897, 2])
We keep 2.18e+08/1.18e+10 =  1% of the original kernel matrix.

torch.Size([70517, 2])
We keep 2.79e+07/2.26e+09 =  1% of the original kernel matrix.

torch.Size([15264, 2])
We keep 2.23e+06/7.52e+07 =  2% of the original kernel matrix.

torch.Size([21942, 2])
We keep 3.53e+06/1.81e+08 =  1% of the original kernel matrix.

torch.Size([21693, 2])
We keep 5.07e+06/2.07e+08 =  2% of the original kernel matrix.

torch.Size([26454, 2])
We keep 5.21e+06/3.00e+08 =  1% of the original kernel matrix.

torch.Size([19322, 2])
We keep 3.13e+06/1.35e+08 =  2% of the original kernel matrix.

torch.Size([24931, 2])
We keep 4.43e+06/2.43e+08 =  1% of the original kernel matrix.

torch.Size([5762, 2])
We keep 3.21e+05/6.82e+06 =  4% of the original kernel matrix.

torch.Size([14095, 2])
We keep 1.46e+06/5.44e+07 =  2% of the original kernel matrix.

torch.Size([160435, 2])
We keep 3.68e+08/1.70e+10 =  2% of the original kernel matrix.

torch.Size([71583, 2])
We keep 3.28e+07/2.72e+09 =  1% of the original kernel matrix.

torch.Size([50088, 2])
We keep 1.74e+07/1.06e+09 =  1% of the original kernel matrix.

torch.Size([40392, 2])
We keep 9.99e+06/6.79e+08 =  1% of the original kernel matrix.

torch.Size([18689, 2])
We keep 4.67e+06/1.29e+08 =  3% of the original kernel matrix.

torch.Size([24643, 2])
We keep 4.38e+06/2.37e+08 =  1% of the original kernel matrix.

torch.Size([26704, 2])
We keep 1.14e+07/3.91e+08 =  2% of the original kernel matrix.

torch.Size([29461, 2])
We keep 6.77e+06/4.12e+08 =  1% of the original kernel matrix.

torch.Size([8497, 2])
We keep 1.17e+06/1.80e+07 =  6% of the original kernel matrix.

torch.Size([16510, 2])
We keep 2.09e+06/8.83e+07 =  2% of the original kernel matrix.

torch.Size([331187, 2])
We keep 6.65e+08/5.62e+10 =  1% of the original kernel matrix.

torch.Size([107026, 2])
We keep 5.55e+07/4.94e+09 =  1% of the original kernel matrix.

torch.Size([26721, 2])
We keep 5.91e+06/3.09e+08 =  1% of the original kernel matrix.

torch.Size([29959, 2])
We keep 6.30e+06/3.67e+08 =  1% of the original kernel matrix.

torch.Size([203007, 2])
We keep 2.56e+08/1.95e+10 =  1% of the original kernel matrix.

torch.Size([82298, 2])
We keep 3.50e+07/2.91e+09 =  1% of the original kernel matrix.

torch.Size([4130, 2])
We keep 1.85e+05/3.45e+06 =  5% of the original kernel matrix.

torch.Size([12208, 2])
We keep 1.16e+06/3.87e+07 =  2% of the original kernel matrix.

torch.Size([8380, 2])
We keep 6.29e+05/1.58e+07 =  3% of the original kernel matrix.

torch.Size([16377, 2])
We keep 1.99e+06/8.30e+07 =  2% of the original kernel matrix.

torch.Size([8125, 2])
We keep 6.94e+05/1.53e+07 =  4% of the original kernel matrix.

torch.Size([16281, 2])
We keep 1.94e+06/8.16e+07 =  2% of the original kernel matrix.

torch.Size([26507, 2])
We keep 8.53e+06/3.01e+08 =  2% of the original kernel matrix.

torch.Size([29724, 2])
We keep 5.84e+06/3.62e+08 =  1% of the original kernel matrix.

torch.Size([8150, 2])
We keep 7.22e+05/1.74e+07 =  4% of the original kernel matrix.

torch.Size([16073, 2])
We keep 2.06e+06/8.70e+07 =  2% of the original kernel matrix.

torch.Size([16379, 2])
We keep 3.73e+06/9.43e+07 =  3% of the original kernel matrix.

torch.Size([22725, 2])
We keep 3.71e+06/2.02e+08 =  1% of the original kernel matrix.

torch.Size([10599, 2])
We keep 1.11e+06/3.11e+07 =  3% of the original kernel matrix.

torch.Size([18015, 2])
We keep 2.55e+06/1.16e+08 =  2% of the original kernel matrix.

torch.Size([9051, 2])
We keep 7.63e+05/1.92e+07 =  3% of the original kernel matrix.

torch.Size([16976, 2])
We keep 2.15e+06/9.14e+07 =  2% of the original kernel matrix.

torch.Size([201851, 2])
We keep 3.27e+08/2.10e+10 =  1% of the original kernel matrix.

torch.Size([82447, 2])
We keep 3.66e+07/3.02e+09 =  1% of the original kernel matrix.

torch.Size([32286, 2])
We keep 9.78e+06/4.80e+08 =  2% of the original kernel matrix.

torch.Size([32431, 2])
We keep 7.28e+06/4.57e+08 =  1% of the original kernel matrix.

torch.Size([32221, 2])
We keep 1.11e+07/5.50e+08 =  2% of the original kernel matrix.

torch.Size([32370, 2])
We keep 7.76e+06/4.89e+08 =  1% of the original kernel matrix.

torch.Size([129878, 2])
We keep 1.02e+08/7.60e+09 =  1% of the original kernel matrix.

torch.Size([65056, 2])
We keep 2.33e+07/1.82e+09 =  1% of the original kernel matrix.

torch.Size([414803, 2])
We keep 6.97e+08/7.58e+10 =  0% of the original kernel matrix.

torch.Size([119609, 2])
We keep 6.40e+07/5.74e+09 =  1% of the original kernel matrix.

torch.Size([12424, 2])
We keep 1.52e+06/4.64e+07 =  3% of the original kernel matrix.

torch.Size([19592, 2])
We keep 2.97e+06/1.42e+08 =  2% of the original kernel matrix.

torch.Size([17643, 2])
We keep 2.88e+06/1.01e+08 =  2% of the original kernel matrix.

torch.Size([23743, 2])
We keep 4.00e+06/2.09e+08 =  1% of the original kernel matrix.

torch.Size([10970, 2])
We keep 4.16e+06/4.05e+07 = 10% of the original kernel matrix.

torch.Size([18686, 2])
We keep 2.86e+06/1.33e+08 =  2% of the original kernel matrix.

torch.Size([2308, 2])
We keep 6.77e+04/8.41e+05 =  8% of the original kernel matrix.

torch.Size([9697, 2])
We keep 7.21e+05/1.91e+07 =  3% of the original kernel matrix.

torch.Size([15728, 2])
We keep 2.80e+06/8.54e+07 =  3% of the original kernel matrix.

torch.Size([22216, 2])
We keep 3.75e+06/1.93e+08 =  1% of the original kernel matrix.

torch.Size([24369, 2])
We keep 8.24e+06/2.65e+08 =  3% of the original kernel matrix.

torch.Size([28146, 2])
We keep 5.81e+06/3.39e+08 =  1% of the original kernel matrix.

torch.Size([12479, 2])
We keep 2.36e+06/5.07e+07 =  4% of the original kernel matrix.

torch.Size([19487, 2])
We keep 2.97e+06/1.48e+08 =  2% of the original kernel matrix.

torch.Size([9810, 2])
We keep 8.63e+05/2.34e+07 =  3% of the original kernel matrix.

torch.Size([17565, 2])
We keep 2.31e+06/1.01e+08 =  2% of the original kernel matrix.

torch.Size([27459, 2])
We keep 7.95e+06/3.17e+08 =  2% of the original kernel matrix.

torch.Size([30527, 2])
We keep 6.31e+06/3.71e+08 =  1% of the original kernel matrix.

torch.Size([19845, 2])
We keep 4.66e+06/1.64e+08 =  2% of the original kernel matrix.

torch.Size([25179, 2])
We keep 4.78e+06/2.67e+08 =  1% of the original kernel matrix.

torch.Size([1173542, 2])
We keep 4.54e+09/5.65e+11 =  0% of the original kernel matrix.

torch.Size([204289, 2])
We keep 1.60e+08/1.57e+10 =  1% of the original kernel matrix.

torch.Size([6155, 2])
We keep 1.54e+06/1.40e+07 = 10% of the original kernel matrix.

torch.Size([13990, 2])
We keep 1.94e+06/7.81e+07 =  2% of the original kernel matrix.

torch.Size([59451, 2])
We keep 3.33e+07/1.63e+09 =  2% of the original kernel matrix.

torch.Size([43726, 2])
We keep 1.22e+07/8.42e+08 =  1% of the original kernel matrix.

torch.Size([19508, 2])
We keep 7.93e+06/1.91e+08 =  4% of the original kernel matrix.

torch.Size([24633, 2])
We keep 4.98e+06/2.88e+08 =  1% of the original kernel matrix.

torch.Size([3980, 2])
We keep 2.09e+05/3.13e+06 =  6% of the original kernel matrix.

torch.Size([12150, 2])
We keep 1.13e+06/3.68e+07 =  3% of the original kernel matrix.

torch.Size([148421, 2])
We keep 1.46e+08/1.12e+10 =  1% of the original kernel matrix.

torch.Size([69358, 2])
We keep 2.74e+07/2.20e+09 =  1% of the original kernel matrix.

torch.Size([211667, 2])
We keep 9.53e+08/3.26e+10 =  2% of the original kernel matrix.

torch.Size([84046, 2])
We keep 4.05e+07/3.77e+09 =  1% of the original kernel matrix.

torch.Size([584206, 2])
We keep 1.14e+09/1.38e+11 =  0% of the original kernel matrix.

torch.Size([142770, 2])
We keep 8.26e+07/7.75e+09 =  1% of the original kernel matrix.

torch.Size([48871, 2])
We keep 2.39e+07/1.04e+09 =  2% of the original kernel matrix.

torch.Size([39939, 2])
We keep 9.90e+06/6.73e+08 =  1% of the original kernel matrix.

torch.Size([31952, 2])
We keep 1.01e+07/4.40e+08 =  2% of the original kernel matrix.

torch.Size([32616, 2])
We keep 6.98e+06/4.37e+08 =  1% of the original kernel matrix.

torch.Size([5880, 2])
We keep 3.47e+05/6.96e+06 =  4% of the original kernel matrix.

torch.Size([14113, 2])
We keep 1.50e+06/5.50e+07 =  2% of the original kernel matrix.

torch.Size([48572, 2])
We keep 4.36e+07/1.49e+09 =  2% of the original kernel matrix.

torch.Size([38730, 2])
We keep 1.18e+07/8.05e+08 =  1% of the original kernel matrix.

torch.Size([1037288, 2])
We keep 5.53e+09/5.45e+11 =  1% of the original kernel matrix.

torch.Size([189838, 2])
We keep 1.57e+08/1.54e+10 =  1% of the original kernel matrix.

torch.Size([46758, 2])
We keep 4.10e+07/1.13e+09 =  3% of the original kernel matrix.

torch.Size([39087, 2])
We keep 1.04e+07/6.99e+08 =  1% of the original kernel matrix.

torch.Size([17101, 2])
We keep 3.17e+06/9.92e+07 =  3% of the original kernel matrix.

torch.Size([23101, 2])
We keep 3.96e+06/2.08e+08 =  1% of the original kernel matrix.

torch.Size([65041, 2])
We keep 7.28e+07/2.70e+09 =  2% of the original kernel matrix.

torch.Size([44700, 2])
We keep 1.51e+07/1.08e+09 =  1% of the original kernel matrix.

torch.Size([342514, 2])
We keep 6.22e+08/5.95e+10 =  1% of the original kernel matrix.

torch.Size([108075, 2])
We keep 5.77e+07/5.08e+09 =  1% of the original kernel matrix.

torch.Size([63820, 2])
We keep 3.71e+07/1.92e+09 =  1% of the original kernel matrix.

torch.Size([44004, 2])
We keep 1.30e+07/9.13e+08 =  1% of the original kernel matrix.

torch.Size([16583, 2])
We keep 3.29e+06/1.00e+08 =  3% of the original kernel matrix.

torch.Size([22758, 2])
We keep 3.96e+06/2.09e+08 =  1% of the original kernel matrix.

torch.Size([6717, 2])
We keep 5.08e+05/1.08e+07 =  4% of the original kernel matrix.

torch.Size([15042, 2])
We keep 1.75e+06/6.86e+07 =  2% of the original kernel matrix.

torch.Size([34161, 2])
We keep 8.67e+06/5.09e+08 =  1% of the original kernel matrix.

torch.Size([33677, 2])
We keep 7.47e+06/4.70e+08 =  1% of the original kernel matrix.

torch.Size([15344, 2])
We keep 4.21e+06/7.92e+07 =  5% of the original kernel matrix.

torch.Size([22063, 2])
We keep 3.62e+06/1.85e+08 =  1% of the original kernel matrix.

torch.Size([412533, 2])
We keep 1.02e+09/7.58e+10 =  1% of the original kernel matrix.

torch.Size([118736, 2])
We keep 6.41e+07/5.74e+09 =  1% of the original kernel matrix.

torch.Size([17052, 2])
We keep 3.42e+06/1.05e+08 =  3% of the original kernel matrix.

torch.Size([23138, 2])
We keep 4.07e+06/2.14e+08 =  1% of the original kernel matrix.

torch.Size([3214, 2])
We keep 2.04e+05/2.43e+06 =  8% of the original kernel matrix.

torch.Size([10917, 2])
We keep 1.05e+06/3.25e+07 =  3% of the original kernel matrix.

torch.Size([41428, 2])
We keep 2.33e+07/7.78e+08 =  2% of the original kernel matrix.

torch.Size([37054, 2])
We keep 8.65e+06/5.81e+08 =  1% of the original kernel matrix.

torch.Size([41728, 2])
We keep 1.72e+07/8.96e+08 =  1% of the original kernel matrix.

torch.Size([37692, 2])
We keep 9.64e+06/6.24e+08 =  1% of the original kernel matrix.

torch.Size([3354, 2])
We keep 1.54e+05/2.23e+06 =  6% of the original kernel matrix.

torch.Size([11317, 2])
We keep 1.02e+06/3.11e+07 =  3% of the original kernel matrix.

torch.Size([94039, 2])
We keep 7.66e+07/4.24e+09 =  1% of the original kernel matrix.

torch.Size([53561, 2])
We keep 1.80e+07/1.36e+09 =  1% of the original kernel matrix.

torch.Size([72101, 2])
We keep 6.09e+07/2.46e+09 =  2% of the original kernel matrix.

torch.Size([47925, 2])
We keep 1.45e+07/1.03e+09 =  1% of the original kernel matrix.

torch.Size([98324, 2])
We keep 1.31e+08/4.47e+09 =  2% of the original kernel matrix.

torch.Size([55717, 2])
We keep 1.81e+07/1.39e+09 =  1% of the original kernel matrix.

torch.Size([140464, 2])
We keep 1.39e+08/9.30e+09 =  1% of the original kernel matrix.

torch.Size([67721, 2])
We keep 2.54e+07/2.01e+09 =  1% of the original kernel matrix.

torch.Size([101510, 2])
We keep 1.01e+08/5.27e+09 =  1% of the original kernel matrix.

torch.Size([56269, 2])
We keep 2.00e+07/1.51e+09 =  1% of the original kernel matrix.

torch.Size([23146, 2])
We keep 6.08e+06/2.53e+08 =  2% of the original kernel matrix.

torch.Size([27224, 2])
We keep 5.75e+06/3.31e+08 =  1% of the original kernel matrix.

torch.Size([63706, 2])
We keep 3.26e+07/1.87e+09 =  1% of the original kernel matrix.

torch.Size([44722, 2])
We keep 1.28e+07/9.01e+08 =  1% of the original kernel matrix.

torch.Size([84580, 2])
We keep 3.90e+07/2.93e+09 =  1% of the original kernel matrix.

torch.Size([51405, 2])
We keep 1.54e+07/1.13e+09 =  1% of the original kernel matrix.

torch.Size([258395, 2])
We keep 3.21e+08/3.19e+10 =  1% of the original kernel matrix.

torch.Size([95102, 2])
We keep 4.31e+07/3.73e+09 =  1% of the original kernel matrix.

torch.Size([10557, 2])
We keep 1.34e+06/2.98e+07 =  4% of the original kernel matrix.

torch.Size([18318, 2])
We keep 2.50e+06/1.14e+08 =  2% of the original kernel matrix.

torch.Size([81942, 2])
We keep 3.03e+08/5.03e+09 =  6% of the original kernel matrix.

torch.Size([51495, 2])
We keep 1.83e+07/1.48e+09 =  1% of the original kernel matrix.

torch.Size([87524, 2])
We keep 1.17e+08/3.88e+09 =  3% of the original kernel matrix.

torch.Size([52773, 2])
We keep 1.67e+07/1.30e+09 =  1% of the original kernel matrix.

torch.Size([63064, 2])
We keep 4.28e+07/1.98e+09 =  2% of the original kernel matrix.

torch.Size([44785, 2])
We keep 1.30e+07/9.27e+08 =  1% of the original kernel matrix.

torch.Size([26024, 2])
We keep 9.01e+06/3.29e+08 =  2% of the original kernel matrix.

torch.Size([29543, 2])
We keep 6.19e+06/3.78e+08 =  1% of the original kernel matrix.

torch.Size([15184, 2])
We keep 3.52e+07/1.78e+08 = 19% of the original kernel matrix.

torch.Size([21366, 2])
We keep 5.06e+06/2.78e+08 =  1% of the original kernel matrix.

torch.Size([15702, 2])
We keep 2.66e+06/9.67e+07 =  2% of the original kernel matrix.

torch.Size([22207, 2])
We keep 3.91e+06/2.05e+08 =  1% of the original kernel matrix.

torch.Size([13767, 2])
We keep 1.66e+06/5.44e+07 =  3% of the original kernel matrix.

torch.Size([20765, 2])
We keep 3.15e+06/1.54e+08 =  2% of the original kernel matrix.

torch.Size([102034, 2])
We keep 1.88e+08/6.65e+09 =  2% of the original kernel matrix.

torch.Size([55707, 2])
We keep 2.20e+07/1.70e+09 =  1% of the original kernel matrix.

torch.Size([200528, 2])
We keep 2.55e+08/1.97e+10 =  1% of the original kernel matrix.

torch.Size([81975, 2])
We keep 3.52e+07/2.92e+09 =  1% of the original kernel matrix.

torch.Size([139281, 2])
We keep 9.34e+07/8.69e+09 =  1% of the original kernel matrix.

torch.Size([67351, 2])
We keep 2.45e+07/1.94e+09 =  1% of the original kernel matrix.

torch.Size([161225, 2])
We keep 2.71e+08/1.43e+10 =  1% of the original kernel matrix.

torch.Size([72454, 2])
We keep 3.09e+07/2.49e+09 =  1% of the original kernel matrix.

torch.Size([89229, 2])
We keep 7.16e+07/4.16e+09 =  1% of the original kernel matrix.

torch.Size([52915, 2])
We keep 1.80e+07/1.34e+09 =  1% of the original kernel matrix.

torch.Size([99190, 2])
We keep 1.11e+08/5.58e+09 =  1% of the original kernel matrix.

torch.Size([55789, 2])
We keep 2.07e+07/1.56e+09 =  1% of the original kernel matrix.

torch.Size([13928, 2])
We keep 2.43e+06/9.14e+07 =  2% of the original kernel matrix.

torch.Size([21033, 2])
We keep 3.81e+06/1.99e+08 =  1% of the original kernel matrix.

torch.Size([260802, 2])
We keep 3.15e+08/3.26e+10 =  0% of the original kernel matrix.

torch.Size([95648, 2])
We keep 4.38e+07/3.76e+09 =  1% of the original kernel matrix.

torch.Size([2901, 2])
We keep 8.96e+04/1.39e+06 =  6% of the original kernel matrix.

torch.Size([10731, 2])
We keep 8.52e+05/2.46e+07 =  3% of the original kernel matrix.

torch.Size([18810, 2])
We keep 3.66e+06/1.31e+08 =  2% of the original kernel matrix.

torch.Size([24383, 2])
We keep 4.42e+06/2.39e+08 =  1% of the original kernel matrix.

torch.Size([260682, 2])
We keep 3.14e+08/3.34e+10 =  0% of the original kernel matrix.

torch.Size([94645, 2])
We keep 4.46e+07/3.81e+09 =  1% of the original kernel matrix.

torch.Size([46606, 2])
We keep 1.62e+07/9.68e+08 =  1% of the original kernel matrix.

torch.Size([38962, 2])
We keep 9.78e+06/6.48e+08 =  1% of the original kernel matrix.

torch.Size([4520, 2])
We keep 2.20e+05/4.04e+06 =  5% of the original kernel matrix.

torch.Size([12634, 2])
We keep 1.21e+06/4.19e+07 =  2% of the original kernel matrix.

torch.Size([23622, 2])
We keep 6.21e+06/2.45e+08 =  2% of the original kernel matrix.

torch.Size([27614, 2])
We keep 5.53e+06/3.26e+08 =  1% of the original kernel matrix.

torch.Size([136597, 2])
We keep 1.82e+08/1.21e+10 =  1% of the original kernel matrix.

torch.Size([66522, 2])
We keep 2.84e+07/2.30e+09 =  1% of the original kernel matrix.

torch.Size([34079, 2])
We keep 1.61e+07/5.50e+08 =  2% of the original kernel matrix.

torch.Size([33374, 2])
We keep 7.89e+06/4.89e+08 =  1% of the original kernel matrix.

torch.Size([66964, 2])
We keep 3.35e+07/2.10e+09 =  1% of the original kernel matrix.

torch.Size([46339, 2])
We keep 1.35e+07/9.56e+08 =  1% of the original kernel matrix.

torch.Size([3450, 2])
We keep 1.31e+05/2.08e+06 =  6% of the original kernel matrix.

torch.Size([11355, 2])
We keep 9.60e+05/3.01e+07 =  3% of the original kernel matrix.

torch.Size([8238, 2])
We keep 6.91e+05/1.64e+07 =  4% of the original kernel matrix.

torch.Size([16183, 2])
We keep 2.03e+06/8.45e+07 =  2% of the original kernel matrix.

torch.Size([6559, 2])
We keep 5.20e+05/9.94e+06 =  5% of the original kernel matrix.

torch.Size([14740, 2])
We keep 1.70e+06/6.57e+07 =  2% of the original kernel matrix.

torch.Size([33688, 2])
We keep 3.40e+07/8.20e+08 =  4% of the original kernel matrix.

torch.Size([31922, 2])
We keep 9.34e+06/5.97e+08 =  1% of the original kernel matrix.

torch.Size([49603, 2])
We keep 1.95e+07/1.17e+09 =  1% of the original kernel matrix.

torch.Size([40315, 2])
We keep 1.06e+07/7.13e+08 =  1% of the original kernel matrix.

torch.Size([76204, 2])
We keep 8.88e+07/2.61e+09 =  3% of the original kernel matrix.

torch.Size([48440, 2])
We keep 1.49e+07/1.06e+09 =  1% of the original kernel matrix.

torch.Size([4505, 2])
We keep 2.15e+05/4.10e+06 =  5% of the original kernel matrix.

torch.Size([12582, 2])
We keep 1.23e+06/4.22e+07 =  2% of the original kernel matrix.

torch.Size([90299, 2])
We keep 6.16e+07/3.67e+09 =  1% of the original kernel matrix.

torch.Size([53446, 2])
We keep 1.68e+07/1.26e+09 =  1% of the original kernel matrix.

torch.Size([14214, 2])
We keep 3.63e+06/9.48e+07 =  3% of the original kernel matrix.

torch.Size([20999, 2])
We keep 3.90e+06/2.03e+08 =  1% of the original kernel matrix.

torch.Size([43058, 2])
We keep 1.43e+07/8.11e+08 =  1% of the original kernel matrix.

torch.Size([37984, 2])
We keep 8.94e+06/5.94e+08 =  1% of the original kernel matrix.

torch.Size([18493, 2])
We keep 2.39e+07/2.55e+08 =  9% of the original kernel matrix.

torch.Size([23881, 2])
We keep 5.92e+06/3.33e+08 =  1% of the original kernel matrix.

torch.Size([130295, 2])
We keep 8.79e+07/7.46e+09 =  1% of the original kernel matrix.

torch.Size([64762, 2])
We keep 2.29e+07/1.80e+09 =  1% of the original kernel matrix.

torch.Size([4916, 2])
We keep 5.54e+05/5.96e+06 =  9% of the original kernel matrix.

torch.Size([12958, 2])
We keep 1.43e+06/5.09e+07 =  2% of the original kernel matrix.

torch.Size([70882, 2])
We keep 6.33e+07/2.39e+09 =  2% of the original kernel matrix.

torch.Size([46930, 2])
We keep 1.43e+07/1.02e+09 =  1% of the original kernel matrix.

torch.Size([31802, 2])
We keep 8.49e+06/4.48e+08 =  1% of the original kernel matrix.

torch.Size([32723, 2])
We keep 7.17e+06/4.41e+08 =  1% of the original kernel matrix.

torch.Size([20462, 2])
We keep 8.48e+06/1.84e+08 =  4% of the original kernel matrix.

torch.Size([25653, 2])
We keep 5.11e+06/2.83e+08 =  1% of the original kernel matrix.

torch.Size([38421, 2])
We keep 1.19e+07/6.98e+08 =  1% of the original kernel matrix.

torch.Size([35368, 2])
We keep 8.54e+06/5.51e+08 =  1% of the original kernel matrix.

torch.Size([275789, 2])
We keep 5.05e+08/3.79e+10 =  1% of the original kernel matrix.

torch.Size([96936, 2])
We keep 4.64e+07/4.06e+09 =  1% of the original kernel matrix.

torch.Size([18972, 2])
We keep 2.68e+06/1.17e+08 =  2% of the original kernel matrix.

torch.Size([24767, 2])
We keep 4.18e+06/2.25e+08 =  1% of the original kernel matrix.

torch.Size([13823, 2])
We keep 3.55e+06/8.27e+07 =  4% of the original kernel matrix.

torch.Size([20721, 2])
We keep 3.68e+06/1.90e+08 =  1% of the original kernel matrix.

torch.Size([133731, 2])
We keep 1.50e+08/8.73e+09 =  1% of the original kernel matrix.

torch.Size([65728, 2])
We keep 2.48e+07/1.95e+09 =  1% of the original kernel matrix.

torch.Size([28844, 2])
We keep 6.96e+06/3.62e+08 =  1% of the original kernel matrix.

torch.Size([30799, 2])
We keep 6.59e+06/3.97e+08 =  1% of the original kernel matrix.

torch.Size([618319, 2])
We keep 1.13e+09/1.50e+11 =  0% of the original kernel matrix.

torch.Size([147438, 2])
We keep 8.61e+07/8.08e+09 =  1% of the original kernel matrix.

torch.Size([11857, 2])
We keep 1.60e+06/4.14e+07 =  3% of the original kernel matrix.

torch.Size([19249, 2])
We keep 2.87e+06/1.34e+08 =  2% of the original kernel matrix.

torch.Size([167127, 2])
We keep 1.66e+08/1.36e+10 =  1% of the original kernel matrix.

torch.Size([73903, 2])
We keep 3.01e+07/2.43e+09 =  1% of the original kernel matrix.

torch.Size([438451, 2])
We keep 6.36e+08/8.33e+10 =  0% of the original kernel matrix.

torch.Size([122318, 2])
We keep 6.66e+07/6.02e+09 =  1% of the original kernel matrix.

torch.Size([10627, 2])
We keep 1.41e+06/3.46e+07 =  4% of the original kernel matrix.

torch.Size([18034, 2])
We keep 2.67e+06/1.23e+08 =  2% of the original kernel matrix.

torch.Size([90900, 2])
We keep 1.10e+08/5.69e+09 =  1% of the original kernel matrix.

torch.Size([52890, 2])
We keep 2.04e+07/1.57e+09 =  1% of the original kernel matrix.

torch.Size([16076, 2])
We keep 7.23e+06/1.92e+08 =  3% of the original kernel matrix.

torch.Size([22316, 2])
We keep 5.15e+06/2.89e+08 =  1% of the original kernel matrix.

torch.Size([225418, 2])
We keep 3.66e+08/2.49e+10 =  1% of the original kernel matrix.

torch.Size([87385, 2])
We keep 3.86e+07/3.29e+09 =  1% of the original kernel matrix.

torch.Size([25085, 2])
We keep 6.32e+06/2.68e+08 =  2% of the original kernel matrix.

torch.Size([28793, 2])
We keep 5.79e+06/3.41e+08 =  1% of the original kernel matrix.

torch.Size([30459, 2])
We keep 1.09e+07/4.51e+08 =  2% of the original kernel matrix.

torch.Size([31415, 2])
We keep 7.24e+06/4.43e+08 =  1% of the original kernel matrix.

torch.Size([3365, 2])
We keep 1.68e+05/2.57e+06 =  6% of the original kernel matrix.

torch.Size([11150, 2])
We keep 1.06e+06/3.34e+07 =  3% of the original kernel matrix.

torch.Size([7730, 2])
We keep 7.10e+05/1.50e+07 =  4% of the original kernel matrix.

torch.Size([15868, 2])
We keep 1.94e+06/8.07e+07 =  2% of the original kernel matrix.

torch.Size([8780, 2])
We keep 1.01e+06/2.22e+07 =  4% of the original kernel matrix.

torch.Size([16762, 2])
We keep 2.24e+06/9.81e+07 =  2% of the original kernel matrix.

torch.Size([28195, 2])
We keep 7.25e+06/3.50e+08 =  2% of the original kernel matrix.

torch.Size([28991, 2])
We keep 6.03e+06/3.90e+08 =  1% of the original kernel matrix.

torch.Size([30273, 2])
We keep 2.23e+07/4.18e+08 =  5% of the original kernel matrix.

torch.Size([31199, 2])
We keep 7.02e+06/4.26e+08 =  1% of the original kernel matrix.

torch.Size([151139, 2])
We keep 1.95e+08/1.10e+10 =  1% of the original kernel matrix.

torch.Size([70281, 2])
We keep 2.71e+07/2.19e+09 =  1% of the original kernel matrix.

torch.Size([24832, 2])
We keep 5.18e+06/2.55e+08 =  2% of the original kernel matrix.

torch.Size([28758, 2])
We keep 5.77e+06/3.33e+08 =  1% of the original kernel matrix.

torch.Size([8212, 2])
We keep 6.88e+05/1.50e+07 =  4% of the original kernel matrix.

torch.Size([16152, 2])
We keep 1.95e+06/8.08e+07 =  2% of the original kernel matrix.

torch.Size([15662, 2])
We keep 2.86e+06/8.85e+07 =  3% of the original kernel matrix.

torch.Size([22126, 2])
We keep 3.70e+06/1.96e+08 =  1% of the original kernel matrix.

torch.Size([25631, 2])
We keep 3.95e+07/3.97e+08 =  9% of the original kernel matrix.

torch.Size([28709, 2])
We keep 6.84e+06/4.15e+08 =  1% of the original kernel matrix.

torch.Size([15515, 2])
We keep 8.03e+06/8.69e+07 =  9% of the original kernel matrix.

torch.Size([22248, 2])
We keep 3.72e+06/1.94e+08 =  1% of the original kernel matrix.

torch.Size([198833, 2])
We keep 2.42e+08/1.86e+10 =  1% of the original kernel matrix.

torch.Size([81959, 2])
We keep 3.37e+07/2.84e+09 =  1% of the original kernel matrix.

torch.Size([45397, 2])
We keep 3.84e+07/1.07e+09 =  3% of the original kernel matrix.

torch.Size([38165, 2])
We keep 1.00e+07/6.81e+08 =  1% of the original kernel matrix.

torch.Size([6108, 2])
We keep 5.67e+05/9.58e+06 =  5% of the original kernel matrix.

torch.Size([14373, 2])
We keep 1.68e+06/6.45e+07 =  2% of the original kernel matrix.

torch.Size([9869, 2])
We keep 9.12e+05/2.42e+07 =  3% of the original kernel matrix.

torch.Size([17616, 2])
We keep 2.32e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([8478, 2])
We keep 1.23e+06/2.40e+07 =  5% of the original kernel matrix.

torch.Size([16385, 2])
We keep 2.28e+06/1.02e+08 =  2% of the original kernel matrix.

torch.Size([669528, 2])
We keep 2.24e+09/1.88e+11 =  1% of the original kernel matrix.

torch.Size([154325, 2])
We keep 9.51e+07/9.03e+09 =  1% of the original kernel matrix.

torch.Size([12949, 2])
We keep 1.58e+06/5.00e+07 =  3% of the original kernel matrix.

torch.Size([20097, 2])
We keep 3.06e+06/1.47e+08 =  2% of the original kernel matrix.

torch.Size([23867, 2])
We keep 5.50e+06/2.36e+08 =  2% of the original kernel matrix.

torch.Size([28141, 2])
We keep 5.55e+06/3.20e+08 =  1% of the original kernel matrix.

torch.Size([66554, 2])
We keep 3.58e+07/2.13e+09 =  1% of the original kernel matrix.

torch.Size([45376, 2])
We keep 1.35e+07/9.63e+08 =  1% of the original kernel matrix.

torch.Size([14872, 2])
We keep 2.16e+07/1.75e+08 = 12% of the original kernel matrix.

torch.Size([21130, 2])
We keep 5.01e+06/2.76e+08 =  1% of the original kernel matrix.

torch.Size([10957, 2])
We keep 1.65e+06/3.52e+07 =  4% of the original kernel matrix.

torch.Size([18587, 2])
We keep 2.69e+06/1.24e+08 =  2% of the original kernel matrix.

torch.Size([14551, 2])
We keep 2.71e+06/8.24e+07 =  3% of the original kernel matrix.

torch.Size([21346, 2])
We keep 3.77e+06/1.89e+08 =  1% of the original kernel matrix.

torch.Size([101434, 2])
We keep 2.39e+08/6.60e+09 =  3% of the original kernel matrix.

torch.Size([56695, 2])
We keep 2.20e+07/1.69e+09 =  1% of the original kernel matrix.

torch.Size([71823, 2])
We keep 1.15e+08/2.58e+09 =  4% of the original kernel matrix.

torch.Size([47618, 2])
We keep 1.48e+07/1.06e+09 =  1% of the original kernel matrix.

torch.Size([38928, 2])
We keep 2.14e+07/8.83e+08 =  2% of the original kernel matrix.

torch.Size([34530, 2])
We keep 9.31e+06/6.19e+08 =  1% of the original kernel matrix.

torch.Size([16423, 2])
We keep 2.68e+06/8.63e+07 =  3% of the original kernel matrix.

torch.Size([22883, 2])
We keep 3.71e+06/1.94e+08 =  1% of the original kernel matrix.

torch.Size([29386, 2])
We keep 8.44e+07/4.24e+08 = 19% of the original kernel matrix.

torch.Size([30911, 2])
We keep 7.00e+06/4.29e+08 =  1% of the original kernel matrix.

torch.Size([65731, 2])
We keep 3.27e+07/1.90e+09 =  1% of the original kernel matrix.

torch.Size([45733, 2])
We keep 1.29e+07/9.09e+08 =  1% of the original kernel matrix.

torch.Size([37313, 2])
We keep 2.50e+08/1.98e+09 = 12% of the original kernel matrix.

torch.Size([33358, 2])
We keep 1.34e+07/9.26e+08 =  1% of the original kernel matrix.

torch.Size([107245, 2])
We keep 7.90e+07/4.76e+09 =  1% of the original kernel matrix.

torch.Size([58151, 2])
We keep 1.87e+07/1.44e+09 =  1% of the original kernel matrix.

torch.Size([6274, 2])
We keep 5.57e+05/1.00e+07 =  5% of the original kernel matrix.

torch.Size([14598, 2])
We keep 1.62e+06/6.60e+07 =  2% of the original kernel matrix.

torch.Size([128195, 2])
We keep 1.25e+08/8.01e+09 =  1% of the original kernel matrix.

torch.Size([64529, 2])
We keep 2.39e+07/1.86e+09 =  1% of the original kernel matrix.

torch.Size([6618, 2])
We keep 6.98e+05/1.10e+07 =  6% of the original kernel matrix.

torch.Size([14792, 2])
We keep 1.73e+06/6.90e+07 =  2% of the original kernel matrix.

torch.Size([58848, 2])
We keep 4.89e+07/1.72e+09 =  2% of the original kernel matrix.

torch.Size([43121, 2])
We keep 1.24e+07/8.65e+08 =  1% of the original kernel matrix.

torch.Size([18140, 2])
We keep 5.70e+06/1.74e+08 =  3% of the original kernel matrix.

torch.Size([23830, 2])
We keep 4.81e+06/2.75e+08 =  1% of the original kernel matrix.

torch.Size([6681, 2])
We keep 4.52e+05/1.04e+07 =  4% of the original kernel matrix.

torch.Size([14925, 2])
We keep 1.70e+06/6.71e+07 =  2% of the original kernel matrix.

torch.Size([23969, 2])
We keep 9.89e+06/2.31e+08 =  4% of the original kernel matrix.

torch.Size([28037, 2])
We keep 5.52e+06/3.17e+08 =  1% of the original kernel matrix.

torch.Size([64673, 2])
We keep 4.03e+07/2.04e+09 =  1% of the original kernel matrix.

torch.Size([44577, 2])
We keep 1.34e+07/9.41e+08 =  1% of the original kernel matrix.

torch.Size([23264, 2])
We keep 8.39e+06/2.98e+08 =  2% of the original kernel matrix.

torch.Size([26952, 2])
We keep 6.23e+06/3.60e+08 =  1% of the original kernel matrix.

torch.Size([57592, 2])
We keep 3.38e+07/1.61e+09 =  2% of the original kernel matrix.

torch.Size([42664, 2])
We keep 1.22e+07/8.35e+08 =  1% of the original kernel matrix.

torch.Size([8311, 2])
We keep 4.30e+06/3.18e+07 = 13% of the original kernel matrix.

torch.Size([16080, 2])
We keep 2.45e+06/1.18e+08 =  2% of the original kernel matrix.

torch.Size([133062, 2])
We keep 4.39e+08/1.45e+10 =  3% of the original kernel matrix.

torch.Size([64780, 2])
We keep 3.09e+07/2.51e+09 =  1% of the original kernel matrix.

torch.Size([4960, 2])
We keep 4.52e+05/5.67e+06 =  7% of the original kernel matrix.

torch.Size([13103, 2])
We keep 1.39e+06/4.96e+07 =  2% of the original kernel matrix.

torch.Size([5441, 2])
We keep 7.48e+05/9.73e+06 =  7% of the original kernel matrix.

torch.Size([13407, 2])
We keep 1.63e+06/6.50e+07 =  2% of the original kernel matrix.

torch.Size([10116, 2])
We keep 3.46e+06/5.12e+07 =  6% of the original kernel matrix.

torch.Size([17702, 2])
We keep 2.98e+06/1.49e+08 =  2% of the original kernel matrix.

torch.Size([23188, 2])
We keep 8.02e+06/2.57e+08 =  3% of the original kernel matrix.

torch.Size([27234, 2])
We keep 5.75e+06/3.34e+08 =  1% of the original kernel matrix.

torch.Size([6002, 2])
We keep 3.64e+05/7.38e+06 =  4% of the original kernel matrix.

torch.Size([14194, 2])
We keep 1.50e+06/5.66e+07 =  2% of the original kernel matrix.

torch.Size([15440, 2])
We keep 2.15e+06/7.48e+07 =  2% of the original kernel matrix.

torch.Size([22040, 2])
We keep 3.55e+06/1.80e+08 =  1% of the original kernel matrix.

torch.Size([34467, 2])
We keep 9.58e+06/5.17e+08 =  1% of the original kernel matrix.

torch.Size([33984, 2])
We keep 7.52e+06/4.74e+08 =  1% of the original kernel matrix.

torch.Size([108274, 2])
We keep 1.57e+08/6.55e+09 =  2% of the original kernel matrix.

torch.Size([58795, 2])
We keep 2.19e+07/1.69e+09 =  1% of the original kernel matrix.

torch.Size([8914, 2])
We keep 7.75e+05/1.96e+07 =  3% of the original kernel matrix.

torch.Size([16902, 2])
We keep 2.15e+06/9.23e+07 =  2% of the original kernel matrix.

torch.Size([4841, 2])
We keep 2.77e+05/4.76e+06 =  5% of the original kernel matrix.

torch.Size([12965, 2])
We keep 1.28e+06/4.55e+07 =  2% of the original kernel matrix.

torch.Size([9005, 2])
We keep 9.50e+05/2.24e+07 =  4% of the original kernel matrix.

torch.Size([16728, 2])
We keep 2.26e+06/9.86e+07 =  2% of the original kernel matrix.

torch.Size([223951, 2])
We keep 5.60e+08/3.18e+10 =  1% of the original kernel matrix.

torch.Size([86833, 2])
We keep 4.34e+07/3.72e+09 =  1% of the original kernel matrix.

torch.Size([1045786, 2])
We keep 4.56e+09/4.73e+11 =  0% of the original kernel matrix.

torch.Size([196534, 2])
We keep 1.48e+08/1.43e+10 =  1% of the original kernel matrix.

torch.Size([27850, 2])
We keep 1.02e+07/3.64e+08 =  2% of the original kernel matrix.

torch.Size([30137, 2])
We keep 6.70e+06/3.98e+08 =  1% of the original kernel matrix.

torch.Size([11100, 2])
We keep 1.39e+06/3.82e+07 =  3% of the original kernel matrix.

torch.Size([18626, 2])
We keep 2.80e+06/1.29e+08 =  2% of the original kernel matrix.

torch.Size([204464, 2])
We keep 3.89e+08/2.32e+10 =  1% of the original kernel matrix.

torch.Size([82468, 2])
We keep 3.79e+07/3.18e+09 =  1% of the original kernel matrix.

torch.Size([158318, 2])
We keep 1.30e+08/1.24e+10 =  1% of the original kernel matrix.

torch.Size([72769, 2])
We keep 2.87e+07/2.32e+09 =  1% of the original kernel matrix.

torch.Size([4939, 2])
We keep 2.32e+05/4.70e+06 =  4% of the original kernel matrix.

torch.Size([13088, 2])
We keep 1.27e+06/4.52e+07 =  2% of the original kernel matrix.

torch.Size([12293, 2])
We keep 3.19e+06/5.59e+07 =  5% of the original kernel matrix.

torch.Size([19745, 2])
We keep 3.19e+06/1.56e+08 =  2% of the original kernel matrix.

torch.Size([46635, 2])
We keep 1.40e+07/9.81e+08 =  1% of the original kernel matrix.

torch.Size([39340, 2])
We keep 9.70e+06/6.53e+08 =  1% of the original kernel matrix.

torch.Size([346635, 2])
We keep 2.04e+09/1.26e+11 =  1% of the original kernel matrix.

torch.Size([102304, 2])
We keep 8.14e+07/7.41e+09 =  1% of the original kernel matrix.

torch.Size([601564, 2])
We keep 2.48e+09/2.22e+11 =  1% of the original kernel matrix.

torch.Size([144630, 2])
We keep 1.04e+08/9.83e+09 =  1% of the original kernel matrix.

torch.Size([172764, 2])
We keep 1.53e+08/1.37e+10 =  1% of the original kernel matrix.

torch.Size([75776, 2])
We keep 2.98e+07/2.44e+09 =  1% of the original kernel matrix.

torch.Size([24709, 2])
We keep 6.04e+06/2.57e+08 =  2% of the original kernel matrix.

torch.Size([28786, 2])
We keep 5.72e+06/3.34e+08 =  1% of the original kernel matrix.

torch.Size([11449, 2])
We keep 1.27e+06/3.60e+07 =  3% of the original kernel matrix.

torch.Size([18938, 2])
We keep 2.68e+06/1.25e+08 =  2% of the original kernel matrix.

torch.Size([7183, 2])
We keep 1.18e+06/1.44e+07 =  8% of the original kernel matrix.

torch.Size([15415, 2])
We keep 1.98e+06/7.91e+07 =  2% of the original kernel matrix.

torch.Size([81356, 2])
We keep 6.40e+07/3.26e+09 =  1% of the original kernel matrix.

torch.Size([49936, 2])
We keep 1.60e+07/1.19e+09 =  1% of the original kernel matrix.

torch.Size([20178, 2])
We keep 3.50e+06/1.43e+08 =  2% of the original kernel matrix.

torch.Size([25466, 2])
We keep 4.55e+06/2.49e+08 =  1% of the original kernel matrix.

torch.Size([17704, 2])
We keep 1.43e+07/2.57e+08 =  5% of the original kernel matrix.

torch.Size([23349, 2])
We keep 5.57e+06/3.34e+08 =  1% of the original kernel matrix.

torch.Size([144151, 2])
We keep 2.06e+08/1.18e+10 =  1% of the original kernel matrix.

torch.Size([68324, 2])
We keep 2.82e+07/2.26e+09 =  1% of the original kernel matrix.

torch.Size([513495, 2])
We keep 9.02e+08/1.07e+11 =  0% of the original kernel matrix.

torch.Size([132410, 2])
We keep 7.44e+07/6.82e+09 =  1% of the original kernel matrix.

torch.Size([9690, 2])
We keep 9.25e+05/2.27e+07 =  4% of the original kernel matrix.

torch.Size([17389, 2])
We keep 2.25e+06/9.93e+07 =  2% of the original kernel matrix.

torch.Size([3918, 2])
We keep 1.59e+05/2.74e+06 =  5% of the original kernel matrix.

torch.Size([12008, 2])
We keep 1.07e+06/3.45e+07 =  3% of the original kernel matrix.

torch.Size([28268, 2])
We keep 2.66e+07/3.78e+08 =  7% of the original kernel matrix.

torch.Size([30704, 2])
We keep 6.44e+06/4.05e+08 =  1% of the original kernel matrix.

torch.Size([110589, 2])
We keep 3.37e+08/6.01e+09 =  5% of the original kernel matrix.

torch.Size([59708, 2])
We keep 2.01e+07/1.62e+09 =  1% of the original kernel matrix.

torch.Size([47918, 2])
We keep 3.44e+07/1.08e+09 =  3% of the original kernel matrix.

torch.Size([39203, 2])
We keep 1.03e+07/6.85e+08 =  1% of the original kernel matrix.

torch.Size([4578, 2])
We keep 5.53e+05/6.03e+06 =  9% of the original kernel matrix.

torch.Size([12665, 2])
We keep 1.39e+06/5.12e+07 =  2% of the original kernel matrix.

torch.Size([3965, 2])
We keep 2.57e+05/3.37e+06 =  7% of the original kernel matrix.

torch.Size([11917, 2])
We keep 1.17e+06/3.83e+07 =  3% of the original kernel matrix.

torch.Size([22561, 2])
We keep 5.26e+06/2.07e+08 =  2% of the original kernel matrix.

torch.Size([26962, 2])
We keep 5.21e+06/3.00e+08 =  1% of the original kernel matrix.

torch.Size([7051, 2])
We keep 5.15e+05/1.11e+07 =  4% of the original kernel matrix.

torch.Size([15279, 2])
We keep 1.76e+06/6.95e+07 =  2% of the original kernel matrix.

torch.Size([16720, 2])
We keep 1.76e+07/1.29e+08 = 13% of the original kernel matrix.

torch.Size([22885, 2])
We keep 4.28e+06/2.36e+08 =  1% of the original kernel matrix.

torch.Size([6848, 2])
We keep 4.82e+05/1.09e+07 =  4% of the original kernel matrix.

torch.Size([14978, 2])
We keep 1.74e+06/6.89e+07 =  2% of the original kernel matrix.

torch.Size([17817, 2])
We keep 2.46e+06/9.93e+07 =  2% of the original kernel matrix.

torch.Size([23915, 2])
We keep 3.92e+06/2.08e+08 =  1% of the original kernel matrix.

torch.Size([16192, 2])
We keep 2.78e+06/9.31e+07 =  2% of the original kernel matrix.

torch.Size([22721, 2])
We keep 3.87e+06/2.01e+08 =  1% of the original kernel matrix.

torch.Size([13131, 2])
We keep 1.47e+06/5.02e+07 =  2% of the original kernel matrix.

torch.Size([20351, 2])
We keep 3.04e+06/1.48e+08 =  2% of the original kernel matrix.

torch.Size([6425, 2])
We keep 4.59e+05/8.95e+06 =  5% of the original kernel matrix.

torch.Size([14512, 2])
We keep 1.61e+06/6.23e+07 =  2% of the original kernel matrix.

torch.Size([176314, 2])
We keep 1.78e+08/1.49e+10 =  1% of the original kernel matrix.

torch.Size([76630, 2])
We keep 3.12e+07/2.54e+09 =  1% of the original kernel matrix.

torch.Size([415095, 2])
We keep 7.52e+08/7.11e+10 =  1% of the original kernel matrix.

torch.Size([119116, 2])
We keep 6.19e+07/5.56e+09 =  1% of the original kernel matrix.

torch.Size([83029, 2])
We keep 8.60e+07/3.52e+09 =  2% of the original kernel matrix.

torch.Size([51472, 2])
We keep 1.68e+07/1.24e+09 =  1% of the original kernel matrix.

torch.Size([30125, 2])
We keep 3.16e+07/6.03e+08 =  5% of the original kernel matrix.

torch.Size([30829, 2])
We keep 8.14e+06/5.12e+08 =  1% of the original kernel matrix.

torch.Size([21743, 2])
We keep 3.59e+06/1.69e+08 =  2% of the original kernel matrix.

torch.Size([26725, 2])
We keep 4.81e+06/2.71e+08 =  1% of the original kernel matrix.

torch.Size([17174, 2])
We keep 4.44e+06/1.10e+08 =  4% of the original kernel matrix.

torch.Size([23240, 2])
We keep 4.20e+06/2.19e+08 =  1% of the original kernel matrix.

torch.Size([24657, 2])
We keep 9.49e+06/2.67e+08 =  3% of the original kernel matrix.

torch.Size([28322, 2])
We keep 5.85e+06/3.41e+08 =  1% of the original kernel matrix.

torch.Size([169426, 2])
We keep 1.38e+08/1.31e+10 =  1% of the original kernel matrix.

torch.Size([74747, 2])
We keep 2.94e+07/2.38e+09 =  1% of the original kernel matrix.

torch.Size([15944, 2])
We keep 5.13e+06/1.07e+08 =  4% of the original kernel matrix.

torch.Size([22277, 2])
We keep 4.04e+06/2.16e+08 =  1% of the original kernel matrix.

torch.Size([37297, 2])
We keep 2.53e+07/7.07e+08 =  3% of the original kernel matrix.

torch.Size([34876, 2])
We keep 8.36e+06/5.54e+08 =  1% of the original kernel matrix.

torch.Size([22284, 2])
We keep 5.35e+06/2.18e+08 =  2% of the original kernel matrix.

torch.Size([26905, 2])
We keep 5.41e+06/3.08e+08 =  1% of the original kernel matrix.

torch.Size([1054043, 2])
We keep 4.01e+09/4.64e+11 =  0% of the original kernel matrix.

torch.Size([198413, 2])
We keep 1.46e+08/1.42e+10 =  1% of the original kernel matrix.

torch.Size([19643, 2])
We keep 5.47e+06/1.40e+08 =  3% of the original kernel matrix.

torch.Size([25095, 2])
We keep 4.54e+06/2.47e+08 =  1% of the original kernel matrix.

torch.Size([85151, 2])
We keep 5.15e+07/3.27e+09 =  1% of the original kernel matrix.

torch.Size([51434, 2])
We keep 1.62e+07/1.19e+09 =  1% of the original kernel matrix.

torch.Size([191087, 2])
We keep 3.30e+08/2.05e+10 =  1% of the original kernel matrix.

torch.Size([80085, 2])
We keep 3.55e+07/2.99e+09 =  1% of the original kernel matrix.

torch.Size([89417, 2])
We keep 7.30e+07/3.41e+09 =  2% of the original kernel matrix.

torch.Size([53090, 2])
We keep 1.65e+07/1.22e+09 =  1% of the original kernel matrix.

torch.Size([27647, 2])
We keep 9.46e+06/3.36e+08 =  2% of the original kernel matrix.

torch.Size([30090, 2])
We keep 6.42e+06/3.82e+08 =  1% of the original kernel matrix.

torch.Size([327018, 2])
We keep 7.38e+08/5.94e+10 =  1% of the original kernel matrix.

torch.Size([106508, 2])
We keep 5.75e+07/5.08e+09 =  1% of the original kernel matrix.

torch.Size([134762, 2])
We keep 9.10e+07/8.68e+09 =  1% of the original kernel matrix.

torch.Size([66099, 2])
We keep 2.45e+07/1.94e+09 =  1% of the original kernel matrix.

torch.Size([32763, 2])
We keep 8.76e+06/4.69e+08 =  1% of the original kernel matrix.

torch.Size([33231, 2])
We keep 7.23e+06/4.52e+08 =  1% of the original kernel matrix.

torch.Size([10658, 2])
We keep 1.62e+06/3.53e+07 =  4% of the original kernel matrix.

torch.Size([18435, 2])
We keep 2.68e+06/1.24e+08 =  2% of the original kernel matrix.

torch.Size([30655, 2])
We keep 1.06e+07/4.25e+08 =  2% of the original kernel matrix.

torch.Size([31854, 2])
We keep 7.07e+06/4.30e+08 =  1% of the original kernel matrix.

torch.Size([16339, 2])
We keep 4.08e+06/9.87e+07 =  4% of the original kernel matrix.

torch.Size([22827, 2])
We keep 3.90e+06/2.07e+08 =  1% of the original kernel matrix.

torch.Size([94120, 2])
We keep 4.66e+07/3.54e+09 =  1% of the original kernel matrix.

torch.Size([54159, 2])
We keep 1.67e+07/1.24e+09 =  1% of the original kernel matrix.

torch.Size([483001, 2])
We keep 9.82e+08/1.00e+11 =  0% of the original kernel matrix.

torch.Size([128568, 2])
We keep 7.27e+07/6.59e+09 =  1% of the original kernel matrix.

torch.Size([85104, 2])
We keep 1.12e+08/3.41e+09 =  3% of the original kernel matrix.

torch.Size([51784, 2])
We keep 1.67e+07/1.22e+09 =  1% of the original kernel matrix.

torch.Size([110474, 2])
We keep 1.05e+08/6.72e+09 =  1% of the original kernel matrix.

torch.Size([58736, 2])
We keep 2.22e+07/1.71e+09 =  1% of the original kernel matrix.

torch.Size([21331, 2])
We keep 4.41e+06/1.96e+08 =  2% of the original kernel matrix.

torch.Size([26432, 2])
We keep 5.20e+06/2.92e+08 =  1% of the original kernel matrix.

torch.Size([9074, 2])
We keep 2.35e+06/2.89e+07 =  8% of the original kernel matrix.

torch.Size([17128, 2])
We keep 2.42e+06/1.12e+08 =  2% of the original kernel matrix.

torch.Size([24561, 2])
We keep 5.97e+06/2.89e+08 =  2% of the original kernel matrix.

torch.Size([28552, 2])
We keep 6.10e+06/3.54e+08 =  1% of the original kernel matrix.

torch.Size([10284, 2])
We keep 2.03e+06/4.16e+07 =  4% of the original kernel matrix.

torch.Size([17830, 2])
We keep 2.88e+06/1.34e+08 =  2% of the original kernel matrix.

torch.Size([243100, 2])
We keep 5.12e+08/3.69e+10 =  1% of the original kernel matrix.

torch.Size([92029, 2])
We keep 4.63e+07/4.00e+09 =  1% of the original kernel matrix.

torch.Size([482519, 2])
We keep 7.44e+08/9.93e+10 =  0% of the original kernel matrix.

torch.Size([128871, 2])
We keep 7.17e+07/6.57e+09 =  1% of the original kernel matrix.

torch.Size([6761, 2])
We keep 3.54e+06/1.64e+07 = 21% of the original kernel matrix.

torch.Size([14869, 2])
We keep 1.95e+06/8.43e+07 =  2% of the original kernel matrix.

torch.Size([813906, 2])
We keep 6.47e+09/4.64e+11 =  1% of the original kernel matrix.

torch.Size([166404, 2])
We keep 1.45e+08/1.42e+10 =  1% of the original kernel matrix.

torch.Size([63459, 2])
We keep 3.18e+08/2.48e+09 = 12% of the original kernel matrix.

torch.Size([44991, 2])
We keep 1.47e+07/1.04e+09 =  1% of the original kernel matrix.

torch.Size([50637, 2])
We keep 2.25e+07/1.17e+09 =  1% of the original kernel matrix.

torch.Size([40306, 2])
We keep 1.06e+07/7.12e+08 =  1% of the original kernel matrix.

torch.Size([12401, 2])
We keep 2.24e+06/6.17e+07 =  3% of the original kernel matrix.

torch.Size([19553, 2])
We keep 3.34e+06/1.64e+08 =  2% of the original kernel matrix.

torch.Size([56118, 2])
We keep 3.21e+07/1.53e+09 =  2% of the original kernel matrix.

torch.Size([42531, 2])
We keep 1.17e+07/8.15e+08 =  1% of the original kernel matrix.

torch.Size([184895, 2])
We keep 1.76e+08/1.51e+10 =  1% of the original kernel matrix.

torch.Size([78476, 2])
We keep 3.12e+07/2.56e+09 =  1% of the original kernel matrix.

torch.Size([5176, 2])
We keep 2.80e+05/5.09e+06 =  5% of the original kernel matrix.

torch.Size([13505, 2])
We keep 1.32e+06/4.70e+07 =  2% of the original kernel matrix.

torch.Size([754739, 2])
We keep 1.72e+09/2.54e+11 =  0% of the original kernel matrix.

torch.Size([167391, 2])
We keep 1.11e+08/1.05e+10 =  1% of the original kernel matrix.

torch.Size([26833, 2])
We keep 7.58e+06/3.25e+08 =  2% of the original kernel matrix.

torch.Size([29819, 2])
We keep 6.11e+06/3.76e+08 =  1% of the original kernel matrix.

torch.Size([30353, 2])
We keep 2.07e+07/5.52e+08 =  3% of the original kernel matrix.

torch.Size([30979, 2])
We keep 7.83e+06/4.89e+08 =  1% of the original kernel matrix.

torch.Size([14711, 2])
We keep 3.06e+06/8.60e+07 =  3% of the original kernel matrix.

torch.Size([21646, 2])
We keep 3.73e+06/1.93e+08 =  1% of the original kernel matrix.

torch.Size([22397, 2])
We keep 4.93e+06/2.16e+08 =  2% of the original kernel matrix.

torch.Size([27185, 2])
We keep 5.37e+06/3.06e+08 =  1% of the original kernel matrix.

torch.Size([3981, 2])
We keep 1.99e+05/3.39e+06 =  5% of the original kernel matrix.

torch.Size([12030, 2])
We keep 1.16e+06/3.83e+07 =  3% of the original kernel matrix.

torch.Size([19213, 2])
We keep 4.17e+06/1.56e+08 =  2% of the original kernel matrix.

torch.Size([24683, 2])
We keep 4.71e+06/2.60e+08 =  1% of the original kernel matrix.

torch.Size([17184, 2])
We keep 3.57e+06/1.08e+08 =  3% of the original kernel matrix.

torch.Size([23323, 2])
We keep 4.09e+06/2.17e+08 =  1% of the original kernel matrix.

torch.Size([42807, 2])
We keep 1.80e+07/9.01e+08 =  1% of the original kernel matrix.

torch.Size([37201, 2])
We keep 9.63e+06/6.26e+08 =  1% of the original kernel matrix.

torch.Size([208703, 2])
We keep 1.05e+10/1.48e+11 =  7% of the original kernel matrix.

torch.Size([71248, 2])
We keep 8.75e+07/8.02e+09 =  1% of the original kernel matrix.

torch.Size([20948, 2])
We keep 8.05e+06/2.37e+08 =  3% of the original kernel matrix.

torch.Size([25459, 2])
We keep 5.62e+06/3.21e+08 =  1% of the original kernel matrix.

torch.Size([322917, 2])
We keep 4.21e+08/4.63e+10 =  0% of the original kernel matrix.

torch.Size([106028, 2])
We keep 5.07e+07/4.49e+09 =  1% of the original kernel matrix.

torch.Size([10689, 2])
We keep 1.09e+06/2.95e+07 =  3% of the original kernel matrix.

torch.Size([18442, 2])
We keep 2.50e+06/1.13e+08 =  2% of the original kernel matrix.

torch.Size([25424, 2])
We keep 6.10e+06/2.73e+08 =  2% of the original kernel matrix.

torch.Size([29092, 2])
We keep 5.92e+06/3.44e+08 =  1% of the original kernel matrix.

torch.Size([7823, 2])
We keep 6.65e+05/1.62e+07 =  4% of the original kernel matrix.

torch.Size([15688, 2])
We keep 1.98e+06/8.38e+07 =  2% of the original kernel matrix.

torch.Size([30482, 2])
We keep 1.22e+08/7.95e+08 = 15% of the original kernel matrix.

torch.Size([30916, 2])
We keep 9.18e+06/5.88e+08 =  1% of the original kernel matrix.

torch.Size([108993, 2])
We keep 1.43e+08/7.91e+09 =  1% of the original kernel matrix.

torch.Size([58328, 2])
We keep 2.39e+07/1.85e+09 =  1% of the original kernel matrix.

torch.Size([1553094, 2])
We keep 5.67e+09/8.30e+11 =  0% of the original kernel matrix.

torch.Size([240017, 2])
We keep 1.91e+08/1.90e+10 =  1% of the original kernel matrix.

torch.Size([25917, 2])
We keep 1.10e+07/3.28e+08 =  3% of the original kernel matrix.

torch.Size([29221, 2])
We keep 6.42e+06/3.77e+08 =  1% of the original kernel matrix.

torch.Size([177503, 2])
We keep 1.69e+08/1.48e+10 =  1% of the original kernel matrix.

torch.Size([76832, 2])
We keep 3.12e+07/2.54e+09 =  1% of the original kernel matrix.

torch.Size([15953, 2])
We keep 4.41e+06/8.63e+07 =  5% of the original kernel matrix.

torch.Size([22626, 2])
We keep 3.57e+06/1.94e+08 =  1% of the original kernel matrix.

torch.Size([153561, 2])
We keep 1.42e+08/1.16e+10 =  1% of the original kernel matrix.

torch.Size([70830, 2])
We keep 2.80e+07/2.24e+09 =  1% of the original kernel matrix.

torch.Size([131347, 2])
We keep 1.56e+08/1.13e+10 =  1% of the original kernel matrix.

torch.Size([65255, 2])
We keep 2.74e+07/2.21e+09 =  1% of the original kernel matrix.

torch.Size([40064, 2])
We keep 1.99e+07/7.56e+08 =  2% of the original kernel matrix.

torch.Size([35921, 2])
We keep 8.78e+06/5.73e+08 =  1% of the original kernel matrix.

torch.Size([10569, 2])
We keep 1.20e+06/2.87e+07 =  4% of the original kernel matrix.

torch.Size([18201, 2])
We keep 2.43e+06/1.12e+08 =  2% of the original kernel matrix.

torch.Size([133253, 2])
We keep 1.24e+08/8.23e+09 =  1% of the original kernel matrix.

torch.Size([65834, 2])
We keep 2.41e+07/1.89e+09 =  1% of the original kernel matrix.

torch.Size([29871, 2])
We keep 2.55e+07/4.81e+08 =  5% of the original kernel matrix.

torch.Size([31167, 2])
We keep 7.15e+06/4.57e+08 =  1% of the original kernel matrix.

torch.Size([101601, 2])
We keep 8.81e+07/4.95e+09 =  1% of the original kernel matrix.

torch.Size([55763, 2])
We keep 1.91e+07/1.47e+09 =  1% of the original kernel matrix.

torch.Size([370140, 2])
We keep 7.28e+08/7.03e+10 =  1% of the original kernel matrix.

torch.Size([111822, 2])
We keep 6.17e+07/5.53e+09 =  1% of the original kernel matrix.

torch.Size([13739, 2])
We keep 4.57e+06/8.85e+07 =  5% of the original kernel matrix.

torch.Size([21077, 2])
We keep 3.53e+06/1.96e+08 =  1% of the original kernel matrix.

torch.Size([34384, 2])
We keep 2.33e+07/6.16e+08 =  3% of the original kernel matrix.

torch.Size([33810, 2])
We keep 8.24e+06/5.17e+08 =  1% of the original kernel matrix.

torch.Size([24242, 2])
We keep 6.64e+06/2.70e+08 =  2% of the original kernel matrix.

torch.Size([27678, 2])
We keep 5.79e+06/3.43e+08 =  1% of the original kernel matrix.

torch.Size([191254, 2])
We keep 1.95e+08/1.71e+10 =  1% of the original kernel matrix.

torch.Size([79546, 2])
We keep 3.30e+07/2.73e+09 =  1% of the original kernel matrix.

torch.Size([220410, 2])
We keep 2.91e+08/2.45e+10 =  1% of the original kernel matrix.

torch.Size([86282, 2])
We keep 3.80e+07/3.26e+09 =  1% of the original kernel matrix.

torch.Size([38654, 2])
We keep 1.45e+07/6.85e+08 =  2% of the original kernel matrix.

torch.Size([35811, 2])
We keep 8.51e+06/5.46e+08 =  1% of the original kernel matrix.

torch.Size([14081, 2])
We keep 2.20e+06/6.84e+07 =  3% of the original kernel matrix.

torch.Size([21113, 2])
We keep 3.51e+06/1.72e+08 =  2% of the original kernel matrix.

torch.Size([342751, 2])
We keep 6.91e+08/6.45e+10 =  1% of the original kernel matrix.

torch.Size([107057, 2])
We keep 5.95e+07/5.29e+09 =  1% of the original kernel matrix.

torch.Size([97425, 2])
We keep 7.41e+07/4.47e+09 =  1% of the original kernel matrix.

torch.Size([54824, 2])
We keep 1.87e+07/1.39e+09 =  1% of the original kernel matrix.

torch.Size([407658, 2])
We keep 1.35e+09/9.40e+10 =  1% of the original kernel matrix.

torch.Size([116756, 2])
We keep 6.97e+07/6.39e+09 =  1% of the original kernel matrix.

torch.Size([129742, 2])
We keep 1.21e+08/8.28e+09 =  1% of the original kernel matrix.

torch.Size([64256, 2])
We keep 2.40e+07/1.90e+09 =  1% of the original kernel matrix.

torch.Size([87233, 2])
We keep 8.93e+07/4.61e+09 =  1% of the original kernel matrix.

torch.Size([51405, 2])
We keep 1.88e+07/1.41e+09 =  1% of the original kernel matrix.

torch.Size([27354, 2])
We keep 2.13e+07/3.91e+08 =  5% of the original kernel matrix.

torch.Size([30256, 2])
We keep 6.95e+06/4.12e+08 =  1% of the original kernel matrix.

torch.Size([105762, 2])
We keep 2.59e+08/6.75e+09 =  3% of the original kernel matrix.

torch.Size([56803, 2])
We keep 2.23e+07/1.71e+09 =  1% of the original kernel matrix.

torch.Size([310926, 2])
We keep 1.12e+09/6.09e+10 =  1% of the original kernel matrix.

torch.Size([101804, 2])
We keep 5.83e+07/5.14e+09 =  1% of the original kernel matrix.

torch.Size([83886, 2])
We keep 4.76e+07/3.13e+09 =  1% of the original kernel matrix.

torch.Size([50734, 2])
We keep 1.61e+07/1.17e+09 =  1% of the original kernel matrix.

torch.Size([11038, 2])
We keep 1.45e+06/3.65e+07 =  3% of the original kernel matrix.

torch.Size([18540, 2])
We keep 2.72e+06/1.26e+08 =  2% of the original kernel matrix.

torch.Size([19274, 2])
We keep 7.12e+06/1.71e+08 =  4% of the original kernel matrix.

torch.Size([24914, 2])
We keep 5.04e+06/2.73e+08 =  1% of the original kernel matrix.

torch.Size([25554, 2])
We keep 1.97e+07/3.15e+08 =  6% of the original kernel matrix.

torch.Size([28667, 2])
We keep 6.15e+06/3.70e+08 =  1% of the original kernel matrix.

torch.Size([98743, 2])
We keep 1.26e+08/4.64e+09 =  2% of the original kernel matrix.

torch.Size([55439, 2])
We keep 1.91e+07/1.42e+09 =  1% of the original kernel matrix.

torch.Size([65245, 2])
We keep 2.76e+07/1.81e+09 =  1% of the original kernel matrix.

torch.Size([45704, 2])
We keep 1.26e+07/8.86e+08 =  1% of the original kernel matrix.

torch.Size([14068, 2])
We keep 1.72e+06/5.80e+07 =  2% of the original kernel matrix.

torch.Size([20938, 2])
We keep 3.21e+06/1.59e+08 =  2% of the original kernel matrix.

torch.Size([28656, 2])
We keep 5.90e+06/3.44e+08 =  1% of the original kernel matrix.

torch.Size([29610, 2])
We keep 6.13e+06/3.86e+08 =  1% of the original kernel matrix.

torch.Size([124896, 2])
We keep 2.33e+08/8.47e+09 =  2% of the original kernel matrix.

torch.Size([63014, 2])
We keep 2.46e+07/1.92e+09 =  1% of the original kernel matrix.

torch.Size([63614, 2])
We keep 2.71e+07/1.67e+09 =  1% of the original kernel matrix.

torch.Size([45285, 2])
We keep 1.21e+07/8.52e+08 =  1% of the original kernel matrix.

torch.Size([44994, 2])
We keep 1.91e+07/9.17e+08 =  2% of the original kernel matrix.

torch.Size([38431, 2])
We keep 9.71e+06/6.31e+08 =  1% of the original kernel matrix.

torch.Size([56913, 2])
We keep 2.43e+07/1.41e+09 =  1% of the original kernel matrix.

torch.Size([42925, 2])
We keep 1.13e+07/7.84e+08 =  1% of the original kernel matrix.

torch.Size([26979, 2])
We keep 1.18e+07/3.87e+08 =  3% of the original kernel matrix.

torch.Size([29293, 2])
We keep 6.82e+06/4.10e+08 =  1% of the original kernel matrix.

torch.Size([28224, 2])
We keep 7.10e+06/3.36e+08 =  2% of the original kernel matrix.

torch.Size([30743, 2])
We keep 6.28e+06/3.82e+08 =  1% of the original kernel matrix.

torch.Size([6528, 2])
We keep 6.05e+05/1.17e+07 =  5% of the original kernel matrix.

torch.Size([14565, 2])
We keep 1.81e+06/7.13e+07 =  2% of the original kernel matrix.

torch.Size([12115, 2])
We keep 1.70e+06/4.35e+07 =  3% of the original kernel matrix.

torch.Size([19586, 2])
We keep 2.92e+06/1.38e+08 =  2% of the original kernel matrix.

torch.Size([221479, 2])
We keep 2.10e+08/2.15e+10 =  0% of the original kernel matrix.

torch.Size([86553, 2])
We keep 3.60e+07/3.06e+09 =  1% of the original kernel matrix.

torch.Size([15213, 2])
We keep 3.46e+06/8.22e+07 =  4% of the original kernel matrix.

torch.Size([21950, 2])
We keep 3.67e+06/1.89e+08 =  1% of the original kernel matrix.

torch.Size([93448, 2])
We keep 4.67e+07/3.46e+09 =  1% of the original kernel matrix.

torch.Size([54016, 2])
We keep 1.66e+07/1.23e+09 =  1% of the original kernel matrix.

torch.Size([4464, 2])
We keep 2.16e+05/3.67e+06 =  5% of the original kernel matrix.

torch.Size([12744, 2])
We keep 1.18e+06/4.00e+07 =  2% of the original kernel matrix.

torch.Size([5201, 2])
We keep 2.78e+05/5.54e+06 =  5% of the original kernel matrix.

torch.Size([13369, 2])
We keep 1.38e+06/4.91e+07 =  2% of the original kernel matrix.

torch.Size([136406, 2])
We keep 2.27e+08/1.09e+10 =  2% of the original kernel matrix.

torch.Size([66063, 2])
We keep 2.71e+07/2.17e+09 =  1% of the original kernel matrix.

torch.Size([115455, 2])
We keep 7.90e+07/6.01e+09 =  1% of the original kernel matrix.

torch.Size([60361, 2])
We keep 2.10e+07/1.62e+09 =  1% of the original kernel matrix.

torch.Size([37802, 2])
We keep 1.53e+07/6.75e+08 =  2% of the original kernel matrix.

torch.Size([35413, 2])
We keep 8.48e+06/5.42e+08 =  1% of the original kernel matrix.

torch.Size([7531, 2])
We keep 5.76e+05/1.32e+07 =  4% of the original kernel matrix.

torch.Size([15681, 2])
We keep 1.86e+06/7.57e+07 =  2% of the original kernel matrix.

torch.Size([9097, 2])
We keep 8.95e+05/2.07e+07 =  4% of the original kernel matrix.

torch.Size([16949, 2])
We keep 2.17e+06/9.49e+07 =  2% of the original kernel matrix.

torch.Size([28382, 2])
We keep 7.80e+06/3.47e+08 =  2% of the original kernel matrix.

torch.Size([28846, 2])
We keep 5.93e+06/3.88e+08 =  1% of the original kernel matrix.

torch.Size([31515, 2])
We keep 1.27e+07/4.14e+08 =  3% of the original kernel matrix.

torch.Size([32191, 2])
We keep 6.81e+06/4.24e+08 =  1% of the original kernel matrix.

torch.Size([107861, 2])
We keep 1.01e+08/5.45e+09 =  1% of the original kernel matrix.

torch.Size([58451, 2])
We keep 2.03e+07/1.54e+09 =  1% of the original kernel matrix.

torch.Size([8023, 2])
We keep 1.11e+06/2.07e+07 =  5% of the original kernel matrix.

torch.Size([16234, 2])
We keep 2.20e+06/9.47e+07 =  2% of the original kernel matrix.

torch.Size([15312, 2])
We keep 3.59e+06/1.07e+08 =  3% of the original kernel matrix.

torch.Size([21761, 2])
We keep 4.09e+06/2.16e+08 =  1% of the original kernel matrix.

torch.Size([24749, 2])
We keep 9.60e+06/3.36e+08 =  2% of the original kernel matrix.

torch.Size([27567, 2])
We keep 6.26e+06/3.82e+08 =  1% of the original kernel matrix.

torch.Size([25819, 2])
We keep 1.28e+07/3.46e+08 =  3% of the original kernel matrix.

torch.Size([29140, 2])
We keep 6.71e+06/3.88e+08 =  1% of the original kernel matrix.

torch.Size([148478, 2])
We keep 1.53e+08/1.05e+10 =  1% of the original kernel matrix.

torch.Size([69879, 2])
We keep 2.62e+07/2.14e+09 =  1% of the original kernel matrix.

torch.Size([13389, 2])
We keep 2.75e+06/5.55e+07 =  4% of the original kernel matrix.

torch.Size([20607, 2])
We keep 3.03e+06/1.55e+08 =  1% of the original kernel matrix.

torch.Size([58988, 2])
We keep 2.78e+07/1.63e+09 =  1% of the original kernel matrix.

torch.Size([43226, 2])
We keep 1.21e+07/8.42e+08 =  1% of the original kernel matrix.

torch.Size([271546, 2])
We keep 4.99e+08/3.48e+10 =  1% of the original kernel matrix.

torch.Size([97315, 2])
We keep 4.55e+07/3.89e+09 =  1% of the original kernel matrix.

torch.Size([46577, 2])
We keep 1.71e+07/9.64e+08 =  1% of the original kernel matrix.

torch.Size([39218, 2])
We keep 9.83e+06/6.47e+08 =  1% of the original kernel matrix.

torch.Size([49589, 2])
We keep 2.95e+07/1.15e+09 =  2% of the original kernel matrix.

torch.Size([40267, 2])
We keep 1.06e+07/7.05e+08 =  1% of the original kernel matrix.

torch.Size([123611, 2])
We keep 7.97e+07/6.86e+09 =  1% of the original kernel matrix.

torch.Size([63017, 2])
We keep 2.23e+07/1.73e+09 =  1% of the original kernel matrix.

torch.Size([4636, 2])
We keep 2.15e+05/4.23e+06 =  5% of the original kernel matrix.

torch.Size([12912, 2])
We keep 1.26e+06/4.29e+07 =  2% of the original kernel matrix.

torch.Size([39090, 2])
We keep 1.43e+07/6.92e+08 =  2% of the original kernel matrix.

torch.Size([36111, 2])
We keep 8.56e+06/5.48e+08 =  1% of the original kernel matrix.

torch.Size([21037, 2])
We keep 4.26e+06/1.63e+08 =  2% of the original kernel matrix.

torch.Size([26128, 2])
We keep 4.76e+06/2.66e+08 =  1% of the original kernel matrix.

torch.Size([311620, 2])
We keep 5.37e+08/4.99e+10 =  1% of the original kernel matrix.

torch.Size([104236, 2])
We keep 5.33e+07/4.66e+09 =  1% of the original kernel matrix.

torch.Size([63027, 2])
We keep 3.67e+07/1.99e+09 =  1% of the original kernel matrix.

torch.Size([44758, 2])
We keep 1.30e+07/9.30e+08 =  1% of the original kernel matrix.

torch.Size([44342, 2])
We keep 2.84e+07/1.16e+09 =  2% of the original kernel matrix.

torch.Size([37230, 2])
We keep 1.06e+07/7.10e+08 =  1% of the original kernel matrix.

torch.Size([104496, 2])
We keep 8.91e+07/4.77e+09 =  1% of the original kernel matrix.

torch.Size([57551, 2])
We keep 1.90e+07/1.44e+09 =  1% of the original kernel matrix.

torch.Size([37505, 2])
We keep 2.85e+07/6.51e+08 =  4% of the original kernel matrix.

torch.Size([34793, 2])
We keep 8.12e+06/5.32e+08 =  1% of the original kernel matrix.

torch.Size([4349, 2])
We keep 4.67e+05/4.97e+06 =  9% of the original kernel matrix.

torch.Size([12326, 2])
We keep 1.26e+06/4.65e+07 =  2% of the original kernel matrix.

torch.Size([8155, 2])
We keep 6.31e+05/1.51e+07 =  4% of the original kernel matrix.

torch.Size([16203, 2])
We keep 1.95e+06/8.11e+07 =  2% of the original kernel matrix.

torch.Size([21806, 2])
We keep 6.45e+06/2.20e+08 =  2% of the original kernel matrix.

torch.Size([26362, 2])
We keep 5.52e+06/3.09e+08 =  1% of the original kernel matrix.

torch.Size([128674, 2])
We keep 1.21e+08/7.28e+09 =  1% of the original kernel matrix.

torch.Size([64446, 2])
We keep 2.24e+07/1.78e+09 =  1% of the original kernel matrix.

torch.Size([565935, 2])
We keep 1.25e+09/1.37e+11 =  0% of the original kernel matrix.

torch.Size([140889, 2])
We keep 8.30e+07/7.73e+09 =  1% of the original kernel matrix.

torch.Size([59023, 2])
We keep 6.45e+07/2.22e+09 =  2% of the original kernel matrix.

torch.Size([42304, 2])
We keep 1.38e+07/9.81e+08 =  1% of the original kernel matrix.

torch.Size([17363, 2])
We keep 2.78e+06/1.04e+08 =  2% of the original kernel matrix.

torch.Size([23644, 2])
We keep 3.94e+06/2.12e+08 =  1% of the original kernel matrix.

torch.Size([28065, 2])
We keep 6.91e+06/3.38e+08 =  2% of the original kernel matrix.

torch.Size([30986, 2])
We keep 6.48e+06/3.83e+08 =  1% of the original kernel matrix.

torch.Size([590769, 2])
We keep 1.70e+09/1.66e+11 =  1% of the original kernel matrix.

torch.Size([143776, 2])
We keep 9.17e+07/8.50e+09 =  1% of the original kernel matrix.

torch.Size([27872, 2])
We keep 6.46e+06/3.13e+08 =  2% of the original kernel matrix.

torch.Size([30985, 2])
We keep 6.26e+06/3.69e+08 =  1% of the original kernel matrix.

torch.Size([15608, 2])
We keep 2.62e+06/8.15e+07 =  3% of the original kernel matrix.

torch.Size([22208, 2])
We keep 3.57e+06/1.88e+08 =  1% of the original kernel matrix.

torch.Size([80071, 2])
We keep 4.80e+07/2.97e+09 =  1% of the original kernel matrix.

torch.Size([50338, 2])
We keep 1.56e+07/1.13e+09 =  1% of the original kernel matrix.

torch.Size([83990, 2])
We keep 4.73e+07/2.96e+09 =  1% of the original kernel matrix.

torch.Size([51218, 2])
We keep 1.54e+07/1.13e+09 =  1% of the original kernel matrix.

torch.Size([11727, 2])
We keep 2.35e+06/5.13e+07 =  4% of the original kernel matrix.

torch.Size([19164, 2])
We keep 3.06e+06/1.49e+08 =  2% of the original kernel matrix.

torch.Size([19117, 2])
We keep 3.65e+06/1.31e+08 =  2% of the original kernel matrix.

torch.Size([24840, 2])
We keep 4.36e+06/2.38e+08 =  1% of the original kernel matrix.

torch.Size([33764, 2])
We keep 3.39e+07/6.56e+08 =  5% of the original kernel matrix.

torch.Size([33007, 2])
We keep 7.99e+06/5.34e+08 =  1% of the original kernel matrix.

time for making ranges is 3.5329365730285645
Sorting X and nu_X
time for sorting X is 0.08303046226501465
Sorting Z and nu_Z
time for sorting Z is 0.0002613067626953125
Starting Optim
sum tnu_Z before tensor(35168600., device='cuda:0')
c= tensor(1108.4108, device='cuda:0')
c= tensor(100674.7266, device='cuda:0')
c= tensor(103160.1953, device='cuda:0')
c= tensor(115709.3516, device='cuda:0')
c= tensor(2439110.2500, device='cuda:0')
c= tensor(3064290.2500, device='cuda:0')
c= tensor(3658738.2500, device='cuda:0')
c= tensor(4070134., device='cuda:0')
c= tensor(4936970., device='cuda:0')
c= tensor(18009818., device='cuda:0')
c= tensor(18125408., device='cuda:0')
c= tensor(19910068., device='cuda:0')
c= tensor(19924056., device='cuda:0')
c= tensor(31246916., device='cuda:0')
c= tensor(31395698., device='cuda:0')
c= tensor(31593544., device='cuda:0')
c= tensor(32110246., device='cuda:0')
c= tensor(33005720., device='cuda:0')
c= tensor(37353856., device='cuda:0')
c= tensor(40892984., device='cuda:0')
c= tensor(41147064., device='cuda:0')
c= tensor(48806400., device='cuda:0')
c= tensor(48856480., device='cuda:0')
c= tensor(48934244., device='cuda:0')
c= tensor(49311476., device='cuda:0')
c= tensor(50094312., device='cuda:0')
c= tensor(51906604., device='cuda:0')
c= tensor(51975432., device='cuda:0')
c= tensor(61196960., device='cuda:0')
c= tensor(2.5843e+08, device='cuda:0')
c= tensor(2.5853e+08, device='cuda:0')
c= tensor(4.5985e+08, device='cuda:0')
c= tensor(4.5988e+08, device='cuda:0')
c= tensor(4.6019e+08, device='cuda:0')
c= tensor(4.6091e+08, device='cuda:0')
c= tensor(4.9574e+08, device='cuda:0')
c= tensor(4.9684e+08, device='cuda:0')
c= tensor(4.9684e+08, device='cuda:0')
c= tensor(4.9685e+08, device='cuda:0')
c= tensor(4.9685e+08, device='cuda:0')
c= tensor(4.9686e+08, device='cuda:0')
c= tensor(4.9686e+08, device='cuda:0')
c= tensor(4.9686e+08, device='cuda:0')
c= tensor(4.9686e+08, device='cuda:0')
c= tensor(4.9686e+08, device='cuda:0')
c= tensor(4.9687e+08, device='cuda:0')
c= tensor(4.9687e+08, device='cuda:0')
c= tensor(4.9688e+08, device='cuda:0')
c= tensor(4.9689e+08, device='cuda:0')
c= tensor(4.9691e+08, device='cuda:0')
c= tensor(4.9695e+08, device='cuda:0')
c= tensor(4.9695e+08, device='cuda:0')
c= tensor(4.9701e+08, device='cuda:0')
c= tensor(4.9702e+08, device='cuda:0')
c= tensor(4.9703e+08, device='cuda:0')
c= tensor(4.9704e+08, device='cuda:0')
c= tensor(4.9704e+08, device='cuda:0')
c= tensor(4.9705e+08, device='cuda:0')
c= tensor(4.9705e+08, device='cuda:0')
c= tensor(4.9706e+08, device='cuda:0')
c= tensor(4.9707e+08, device='cuda:0')
c= tensor(4.9707e+08, device='cuda:0')
c= tensor(4.9708e+08, device='cuda:0')
c= tensor(4.9712e+08, device='cuda:0')
c= tensor(4.9713e+08, device='cuda:0')
c= tensor(4.9714e+08, device='cuda:0')
c= tensor(4.9714e+08, device='cuda:0')
c= tensor(4.9714e+08, device='cuda:0')
c= tensor(4.9715e+08, device='cuda:0')
c= tensor(4.9716e+08, device='cuda:0')
c= tensor(4.9717e+08, device='cuda:0')
c= tensor(4.9718e+08, device='cuda:0')
c= tensor(4.9718e+08, device='cuda:0')
c= tensor(4.9718e+08, device='cuda:0')
c= tensor(4.9719e+08, device='cuda:0')
c= tensor(4.9726e+08, device='cuda:0')
c= tensor(4.9726e+08, device='cuda:0')
c= tensor(4.9726e+08, device='cuda:0')
c= tensor(4.9726e+08, device='cuda:0')
c= tensor(4.9730e+08, device='cuda:0')
c= tensor(4.9730e+08, device='cuda:0')
c= tensor(4.9730e+08, device='cuda:0')
c= tensor(4.9730e+08, device='cuda:0')
c= tensor(4.9731e+08, device='cuda:0')
c= tensor(4.9731e+08, device='cuda:0')
c= tensor(4.9731e+08, device='cuda:0')
c= tensor(4.9731e+08, device='cuda:0')
c= tensor(4.9731e+08, device='cuda:0')
c= tensor(4.9732e+08, device='cuda:0')
c= tensor(4.9733e+08, device='cuda:0')
c= tensor(4.9734e+08, device='cuda:0')
c= tensor(4.9734e+08, device='cuda:0')
c= tensor(4.9734e+08, device='cuda:0')
c= tensor(4.9738e+08, device='cuda:0')
c= tensor(4.9739e+08, device='cuda:0')
c= tensor(4.9740e+08, device='cuda:0')
c= tensor(4.9740e+08, device='cuda:0')
c= tensor(4.9741e+08, device='cuda:0')
c= tensor(4.9743e+08, device='cuda:0')
c= tensor(4.9743e+08, device='cuda:0')
c= tensor(4.9747e+08, device='cuda:0')
c= tensor(4.9749e+08, device='cuda:0')
c= tensor(4.9749e+08, device='cuda:0')
c= tensor(4.9750e+08, device='cuda:0')
c= tensor(4.9753e+08, device='cuda:0')
c= tensor(4.9753e+08, device='cuda:0')
c= tensor(4.9753e+08, device='cuda:0')
c= tensor(4.9754e+08, device='cuda:0')
c= tensor(4.9754e+08, device='cuda:0')
c= tensor(4.9754e+08, device='cuda:0')
c= tensor(4.9754e+08, device='cuda:0')
c= tensor(4.9755e+08, device='cuda:0')
c= tensor(4.9755e+08, device='cuda:0')
c= tensor(4.9755e+08, device='cuda:0')
c= tensor(4.9757e+08, device='cuda:0')
c= tensor(4.9757e+08, device='cuda:0')
c= tensor(4.9757e+08, device='cuda:0')
c= tensor(4.9757e+08, device='cuda:0')
c= tensor(4.9760e+08, device='cuda:0')
c= tensor(4.9760e+08, device='cuda:0')
c= tensor(4.9762e+08, device='cuda:0')
c= tensor(4.9762e+08, device='cuda:0')
c= tensor(4.9763e+08, device='cuda:0')
c= tensor(4.9764e+08, device='cuda:0')
c= tensor(4.9765e+08, device='cuda:0')
c= tensor(4.9765e+08, device='cuda:0')
c= tensor(4.9765e+08, device='cuda:0')
c= tensor(4.9766e+08, device='cuda:0')
c= tensor(4.9767e+08, device='cuda:0')
c= tensor(4.9768e+08, device='cuda:0')
c= tensor(4.9772e+08, device='cuda:0')
c= tensor(4.9772e+08, device='cuda:0')
c= tensor(4.9772e+08, device='cuda:0')
c= tensor(4.9772e+08, device='cuda:0')
c= tensor(4.9772e+08, device='cuda:0')
c= tensor(4.9773e+08, device='cuda:0')
c= tensor(4.9773e+08, device='cuda:0')
c= tensor(4.9773e+08, device='cuda:0')
c= tensor(4.9774e+08, device='cuda:0')
c= tensor(4.9774e+08, device='cuda:0')
c= tensor(4.9774e+08, device='cuda:0')
c= tensor(4.9774e+08, device='cuda:0')
c= tensor(4.9776e+08, device='cuda:0')
c= tensor(4.9780e+08, device='cuda:0')
c= tensor(4.9780e+08, device='cuda:0')
c= tensor(4.9781e+08, device='cuda:0')
c= tensor(4.9781e+08, device='cuda:0')
c= tensor(4.9781e+08, device='cuda:0')
c= tensor(4.9781e+08, device='cuda:0')
c= tensor(4.9781e+08, device='cuda:0')
c= tensor(4.9782e+08, device='cuda:0')
c= tensor(4.9782e+08, device='cuda:0')
c= tensor(4.9783e+08, device='cuda:0')
c= tensor(4.9784e+08, device='cuda:0')
c= tensor(4.9785e+08, device='cuda:0')
c= tensor(4.9787e+08, device='cuda:0')
c= tensor(4.9788e+08, device='cuda:0')
c= tensor(4.9788e+08, device='cuda:0')
c= tensor(4.9788e+08, device='cuda:0')
c= tensor(4.9788e+08, device='cuda:0')
c= tensor(4.9792e+08, device='cuda:0')
c= tensor(4.9792e+08, device='cuda:0')
c= tensor(4.9794e+08, device='cuda:0')
c= tensor(4.9794e+08, device='cuda:0')
c= tensor(4.9795e+08, device='cuda:0')
c= tensor(4.9796e+08, device='cuda:0')
c= tensor(4.9796e+08, device='cuda:0')
c= tensor(4.9796e+08, device='cuda:0')
c= tensor(4.9797e+08, device='cuda:0')
c= tensor(4.9797e+08, device='cuda:0')
c= tensor(4.9797e+08, device='cuda:0')
c= tensor(4.9797e+08, device='cuda:0')
c= tensor(4.9798e+08, device='cuda:0')
c= tensor(4.9798e+08, device='cuda:0')
c= tensor(4.9799e+08, device='cuda:0')
c= tensor(4.9800e+08, device='cuda:0')
c= tensor(4.9800e+08, device='cuda:0')
c= tensor(4.9800e+08, device='cuda:0')
c= tensor(4.9801e+08, device='cuda:0')
c= tensor(4.9802e+08, device='cuda:0')
c= tensor(4.9803e+08, device='cuda:0')
c= tensor(4.9803e+08, device='cuda:0')
c= tensor(4.9804e+08, device='cuda:0')
c= tensor(4.9804e+08, device='cuda:0')
c= tensor(4.9805e+08, device='cuda:0')
c= tensor(4.9806e+08, device='cuda:0')
c= tensor(4.9807e+08, device='cuda:0')
c= tensor(4.9807e+08, device='cuda:0')
c= tensor(4.9808e+08, device='cuda:0')
c= tensor(4.9811e+08, device='cuda:0')
c= tensor(4.9812e+08, device='cuda:0')
c= tensor(4.9812e+08, device='cuda:0')
c= tensor(4.9812e+08, device='cuda:0')
c= tensor(4.9813e+08, device='cuda:0')
c= tensor(4.9813e+08, device='cuda:0')
c= tensor(4.9814e+08, device='cuda:0')
c= tensor(4.9814e+08, device='cuda:0')
c= tensor(4.9814e+08, device='cuda:0')
c= tensor(4.9816e+08, device='cuda:0')
c= tensor(4.9816e+08, device='cuda:0')
c= tensor(4.9818e+08, device='cuda:0')
c= tensor(4.9818e+08, device='cuda:0')
c= tensor(4.9822e+08, device='cuda:0')
c= tensor(4.9823e+08, device='cuda:0')
c= tensor(4.9827e+08, device='cuda:0')
c= tensor(4.9828e+08, device='cuda:0')
c= tensor(4.9828e+08, device='cuda:0')
c= tensor(4.9828e+08, device='cuda:0')
c= tensor(4.9829e+08, device='cuda:0')
c= tensor(4.9830e+08, device='cuda:0')
c= tensor(4.9830e+08, device='cuda:0')
c= tensor(4.9831e+08, device='cuda:0')
c= tensor(4.9831e+08, device='cuda:0')
c= tensor(4.9832e+08, device='cuda:0')
c= tensor(4.9832e+08, device='cuda:0')
c= tensor(4.9832e+08, device='cuda:0')
c= tensor(4.9832e+08, device='cuda:0')
c= tensor(4.9833e+08, device='cuda:0')
c= tensor(4.9839e+08, device='cuda:0')
c= tensor(4.9839e+08, device='cuda:0')
c= tensor(4.9840e+08, device='cuda:0')
c= tensor(4.9840e+08, device='cuda:0')
c= tensor(4.9844e+08, device='cuda:0')
c= tensor(4.9844e+08, device='cuda:0')
c= tensor(4.9844e+08, device='cuda:0')
c= tensor(4.9845e+08, device='cuda:0')
c= tensor(4.9845e+08, device='cuda:0')
c= tensor(4.9845e+08, device='cuda:0')
c= tensor(4.9846e+08, device='cuda:0')
c= tensor(4.9846e+08, device='cuda:0')
c= tensor(4.9846e+08, device='cuda:0')
c= tensor(4.9847e+08, device='cuda:0')
c= tensor(4.9847e+08, device='cuda:0')
c= tensor(4.9847e+08, device='cuda:0')
c= tensor(4.9848e+08, device='cuda:0')
c= tensor(4.9848e+08, device='cuda:0')
c= tensor(4.9850e+08, device='cuda:0')
c= tensor(4.9850e+08, device='cuda:0')
c= tensor(4.9851e+08, device='cuda:0')
c= tensor(4.9854e+08, device='cuda:0')
c= tensor(4.9879e+08, device='cuda:0')
c= tensor(4.9888e+08, device='cuda:0')
c= tensor(4.9889e+08, device='cuda:0')
c= tensor(4.9890e+08, device='cuda:0')
c= tensor(4.9892e+08, device='cuda:0')
c= tensor(4.9908e+08, device='cuda:0')
c= tensor(4.9964e+08, device='cuda:0')
c= tensor(4.9965e+08, device='cuda:0')
c= tensor(5.0576e+08, device='cuda:0')
c= tensor(5.0713e+08, device='cuda:0')
c= tensor(5.0755e+08, device='cuda:0')
c= tensor(5.0885e+08, device='cuda:0')
c= tensor(5.0885e+08, device='cuda:0')
c= tensor(5.0891e+08, device='cuda:0')
c= tensor(5.3180e+08, device='cuda:0')
c= tensor(5.5057e+08, device='cuda:0')
c= tensor(5.5058e+08, device='cuda:0')
c= tensor(5.5077e+08, device='cuda:0')
c= tensor(5.5673e+08, device='cuda:0')
c= tensor(5.5708e+08, device='cuda:0')
c= tensor(5.5736e+08, device='cuda:0')
c= tensor(5.5776e+08, device='cuda:0')
c= tensor(5.5793e+08, device='cuda:0')
c= tensor(5.5810e+08, device='cuda:0')
c= tensor(5.5814e+08, device='cuda:0')
c= tensor(5.6471e+08, device='cuda:0')
c= tensor(5.6496e+08, device='cuda:0')
c= tensor(5.6497e+08, device='cuda:0')
c= tensor(5.6512e+08, device='cuda:0')
c= tensor(5.6533e+08, device='cuda:0')
c= tensor(5.7956e+08, device='cuda:0')
c= tensor(5.8024e+08, device='cuda:0')
c= tensor(5.8025e+08, device='cuda:0')
c= tensor(5.8072e+08, device='cuda:0')
c= tensor(5.8073e+08, device='cuda:0')
c= tensor(5.8089e+08, device='cuda:0')
c= tensor(5.8190e+08, device='cuda:0')
c= tensor(5.8579e+08, device='cuda:0')
c= tensor(5.8908e+08, device='cuda:0')
c= tensor(5.8908e+08, device='cuda:0')
c= tensor(5.8909e+08, device='cuda:0')
c= tensor(5.9048e+08, device='cuda:0')
c= tensor(5.9192e+08, device='cuda:0')
c= tensor(5.9219e+08, device='cuda:0')
c= tensor(5.9219e+08, device='cuda:0')
c= tensor(6.0775e+08, device='cuda:0')
c= tensor(6.0779e+08, device='cuda:0')
c= tensor(6.0790e+08, device='cuda:0')
c= tensor(6.0965e+08, device='cuda:0')
c= tensor(6.0965e+08, device='cuda:0')
c= tensor(6.1029e+08, device='cuda:0')
c= tensor(6.1250e+08, device='cuda:0')
c= tensor(6.3932e+08, device='cuda:0')
c= tensor(6.3979e+08, device='cuda:0')
c= tensor(6.4020e+08, device='cuda:0')
c= tensor(6.4023e+08, device='cuda:0')
c= tensor(6.4024e+08, device='cuda:0')
c= tensor(6.4104e+08, device='cuda:0')
c= tensor(6.4106e+08, device='cuda:0')
c= tensor(6.4120e+08, device='cuda:0')
c= tensor(6.4882e+08, device='cuda:0')
c= tensor(6.4902e+08, device='cuda:0')
c= tensor(6.4908e+08, device='cuda:0')
c= tensor(6.4909e+08, device='cuda:0')
c= tensor(6.9126e+08, device='cuda:0')
c= tensor(6.9132e+08, device='cuda:0')
c= tensor(6.9141e+08, device='cuda:0')
c= tensor(6.9147e+08, device='cuda:0')
c= tensor(6.9879e+08, device='cuda:0')
c= tensor(6.9893e+08, device='cuda:0')
c= tensor(6.9912e+08, device='cuda:0')
c= tensor(6.9918e+08, device='cuda:0')
c= tensor(6.9990e+08, device='cuda:0')
c= tensor(7.0033e+08, device='cuda:0')
c= tensor(7.7802e+08, device='cuda:0')
c= tensor(7.7888e+08, device='cuda:0')
c= tensor(7.7894e+08, device='cuda:0')
c= tensor(7.8199e+08, device='cuda:0')
c= tensor(7.8469e+08, device='cuda:0')
c= tensor(7.8474e+08, device='cuda:0')
c= tensor(7.8690e+08, device='cuda:0')
c= tensor(7.9436e+08, device='cuda:0')
c= tensor(8.5772e+08, device='cuda:0')
c= tensor(8.5841e+08, device='cuda:0')
c= tensor(8.5842e+08, device='cuda:0')
c= tensor(8.5844e+08, device='cuda:0')
c= tensor(8.5920e+08, device='cuda:0')
c= tensor(8.5946e+08, device='cuda:0')
c= tensor(8.5962e+08, device='cuda:0')
c= tensor(8.5962e+08, device='cuda:0')
c= tensor(8.6002e+08, device='cuda:0')
c= tensor(8.6202e+08, device='cuda:0')
c= tensor(8.6785e+08, device='cuda:0')
c= tensor(8.6788e+08, device='cuda:0')
c= tensor(8.7719e+08, device='cuda:0')
c= tensor(8.7723e+08, device='cuda:0')
c= tensor(8.7733e+08, device='cuda:0')
c= tensor(8.7739e+08, device='cuda:0')
c= tensor(8.7739e+08, device='cuda:0')
c= tensor(8.9968e+08, device='cuda:0')
c= tensor(9.0018e+08, device='cuda:0')
c= tensor(9.0030e+08, device='cuda:0')
c= tensor(9.0060e+08, device='cuda:0')
c= tensor(9.0062e+08, device='cuda:0')
c= tensor(9.3144e+08, device='cuda:0')
c= tensor(9.3154e+08, device='cuda:0')
c= tensor(9.4150e+08, device='cuda:0')
c= tensor(9.4150e+08, device='cuda:0')
c= tensor(9.4151e+08, device='cuda:0')
c= tensor(9.4152e+08, device='cuda:0')
c= tensor(9.4186e+08, device='cuda:0')
c= tensor(9.4187e+08, device='cuda:0')
c= tensor(9.4196e+08, device='cuda:0')
c= tensor(9.4198e+08, device='cuda:0')
c= tensor(9.4199e+08, device='cuda:0')
c= tensor(9.5022e+08, device='cuda:0')
c= tensor(9.5057e+08, device='cuda:0')
c= tensor(9.5093e+08, device='cuda:0')
c= tensor(9.5346e+08, device='cuda:0')
c= tensor(9.7360e+08, device='cuda:0')
c= tensor(9.7362e+08, device='cuda:0')
c= tensor(9.7368e+08, device='cuda:0')
c= tensor(9.7374e+08, device='cuda:0')
c= tensor(9.7374e+08, device='cuda:0')
c= tensor(9.7377e+08, device='cuda:0')
c= tensor(9.7395e+08, device='cuda:0')
c= tensor(9.7399e+08, device='cuda:0')
c= tensor(9.7400e+08, device='cuda:0')
c= tensor(9.7411e+08, device='cuda:0')
c= tensor(9.7420e+08, device='cuda:0')
c= tensor(1.1392e+09, device='cuda:0')
c= tensor(1.1392e+09, device='cuda:0')
c= tensor(1.1399e+09, device='cuda:0')
c= tensor(1.1402e+09, device='cuda:0')
c= tensor(1.1402e+09, device='cuda:0')
c= tensor(1.1437e+09, device='cuda:0')
c= tensor(1.2167e+09, device='cuda:0')
c= tensor(1.2597e+09, device='cuda:0')
c= tensor(1.2604e+09, device='cuda:0')
c= tensor(1.2607e+09, device='cuda:0')
c= tensor(1.2607e+09, device='cuda:0')
c= tensor(1.2618e+09, device='cuda:0')
c= tensor(1.4584e+09, device='cuda:0')
c= tensor(1.4591e+09, device='cuda:0')
c= tensor(1.4592e+09, device='cuda:0')
c= tensor(1.4619e+09, device='cuda:0')
c= tensor(1.4824e+09, device='cuda:0')
c= tensor(1.4834e+09, device='cuda:0')
c= tensor(1.4835e+09, device='cuda:0')
c= tensor(1.4835e+09, device='cuda:0')
c= tensor(1.4836e+09, device='cuda:0')
c= tensor(1.4837e+09, device='cuda:0')
c= tensor(1.5212e+09, device='cuda:0')
c= tensor(1.5213e+09, device='cuda:0')
c= tensor(1.5213e+09, device='cuda:0')
c= tensor(1.5216e+09, device='cuda:0')
c= tensor(1.5220e+09, device='cuda:0')
c= tensor(1.5220e+09, device='cuda:0')
c= tensor(1.5234e+09, device='cuda:0')
c= tensor(1.5249e+09, device='cuda:0')
c= tensor(1.5279e+09, device='cuda:0')
c= tensor(1.5309e+09, device='cuda:0')
c= tensor(1.5333e+09, device='cuda:0')
c= tensor(1.5334e+09, device='cuda:0')
c= tensor(1.5341e+09, device='cuda:0')
c= tensor(1.5349e+09, device='cuda:0')
c= tensor(1.5433e+09, device='cuda:0')
c= tensor(1.5434e+09, device='cuda:0')
c= tensor(1.5635e+09, device='cuda:0')
c= tensor(1.5706e+09, device='cuda:0')
c= tensor(1.5724e+09, device='cuda:0')
c= tensor(1.5728e+09, device='cuda:0')
c= tensor(1.5740e+09, device='cuda:0')
c= tensor(1.5741e+09, device='cuda:0')
c= tensor(1.5741e+09, device='cuda:0')
c= tensor(1.5817e+09, device='cuda:0')
c= tensor(1.5896e+09, device='cuda:0')
c= tensor(1.5928e+09, device='cuda:0')
c= tensor(1.6025e+09, device='cuda:0')
c= tensor(1.6042e+09, device='cuda:0')
c= tensor(1.6069e+09, device='cuda:0')
c= tensor(1.6071e+09, device='cuda:0')
c= tensor(1.6155e+09, device='cuda:0')
c= tensor(1.6155e+09, device='cuda:0')
c= tensor(1.6155e+09, device='cuda:0')
c= tensor(1.6246e+09, device='cuda:0')
c= tensor(1.6249e+09, device='cuda:0')
c= tensor(1.6249e+09, device='cuda:0')
c= tensor(1.6251e+09, device='cuda:0')
c= tensor(1.6304e+09, device='cuda:0')
c= tensor(1.6307e+09, device='cuda:0')
c= tensor(1.6314e+09, device='cuda:0')
c= tensor(1.6314e+09, device='cuda:0')
c= tensor(1.6314e+09, device='cuda:0')
c= tensor(1.6314e+09, device='cuda:0')
c= tensor(1.6323e+09, device='cuda:0')
c= tensor(1.6326e+09, device='cuda:0')
c= tensor(1.6346e+09, device='cuda:0')
c= tensor(1.6346e+09, device='cuda:0')
c= tensor(1.6363e+09, device='cuda:0')
c= tensor(1.6363e+09, device='cuda:0')
c= tensor(1.6366e+09, device='cuda:0')
c= tensor(1.6376e+09, device='cuda:0')
c= tensor(1.6393e+09, device='cuda:0')
c= tensor(1.6394e+09, device='cuda:0')
c= tensor(1.6410e+09, device='cuda:0')
c= tensor(1.6412e+09, device='cuda:0')
c= tensor(1.6413e+09, device='cuda:0')
c= tensor(1.6417e+09, device='cuda:0')
c= tensor(1.6661e+09, device='cuda:0')
c= tensor(1.6661e+09, device='cuda:0')
c= tensor(1.6662e+09, device='cuda:0')
c= tensor(1.6717e+09, device='cuda:0')
c= tensor(1.6718e+09, device='cuda:0')
c= tensor(1.7128e+09, device='cuda:0')
c= tensor(1.7128e+09, device='cuda:0')
c= tensor(1.7178e+09, device='cuda:0')
c= tensor(1.7336e+09, device='cuda:0')
c= tensor(1.7336e+09, device='cuda:0')
c= tensor(1.7379e+09, device='cuda:0')
c= tensor(1.7384e+09, device='cuda:0')
c= tensor(1.7478e+09, device='cuda:0')
c= tensor(1.7479e+09, device='cuda:0')
c= tensor(1.7481e+09, device='cuda:0')
c= tensor(1.7481e+09, device='cuda:0')
c= tensor(1.7482e+09, device='cuda:0')
c= tensor(1.7482e+09, device='cuda:0')
c= tensor(1.7486e+09, device='cuda:0')
c= tensor(1.7493e+09, device='cuda:0')
c= tensor(1.7566e+09, device='cuda:0')
c= tensor(1.7567e+09, device='cuda:0')
c= tensor(1.7567e+09, device='cuda:0')
c= tensor(1.7567e+09, device='cuda:0')
c= tensor(1.7574e+09, device='cuda:0')
c= tensor(1.7575e+09, device='cuda:0')
c= tensor(1.7658e+09, device='cuda:0')
c= tensor(1.7667e+09, device='cuda:0')
c= tensor(1.7667e+09, device='cuda:0')
c= tensor(1.7667e+09, device='cuda:0')
c= tensor(1.7667e+09, device='cuda:0')
c= tensor(1.8504e+09, device='cuda:0')
c= tensor(1.8504e+09, device='cuda:0')
c= tensor(1.8505e+09, device='cuda:0')
c= tensor(1.8526e+09, device='cuda:0')
c= tensor(1.8533e+09, device='cuda:0')
c= tensor(1.8533e+09, device='cuda:0')
c= tensor(1.8534e+09, device='cuda:0')
c= tensor(1.8620e+09, device='cuda:0')
c= tensor(1.8673e+09, device='cuda:0')
c= tensor(1.8680e+09, device='cuda:0')
c= tensor(1.8680e+09, device='cuda:0')
c= tensor(1.8696e+09, device='cuda:0')
c= tensor(1.8705e+09, device='cuda:0')
c= tensor(1.8765e+09, device='cuda:0')
c= tensor(1.8781e+09, device='cuda:0')
c= tensor(1.8781e+09, device='cuda:0')
c= tensor(1.8812e+09, device='cuda:0')
c= tensor(1.8812e+09, device='cuda:0')
c= tensor(1.8825e+09, device='cuda:0')
c= tensor(1.8826e+09, device='cuda:0')
c= tensor(1.8826e+09, device='cuda:0')
c= tensor(1.8829e+09, device='cuda:0')
c= tensor(1.8838e+09, device='cuda:0')
c= tensor(1.8840e+09, device='cuda:0')
c= tensor(1.8846e+09, device='cuda:0')
c= tensor(1.8848e+09, device='cuda:0')
c= tensor(1.9021e+09, device='cuda:0')
c= tensor(1.9021e+09, device='cuda:0')
c= tensor(1.9021e+09, device='cuda:0')
c= tensor(1.9022e+09, device='cuda:0')
c= tensor(1.9024e+09, device='cuda:0')
c= tensor(1.9024e+09, device='cuda:0')
c= tensor(1.9024e+09, device='cuda:0')
c= tensor(1.9026e+09, device='cuda:0')
c= tensor(1.9076e+09, device='cuda:0')
c= tensor(1.9076e+09, device='cuda:0')
c= tensor(1.9076e+09, device='cuda:0')
c= tensor(1.9076e+09, device='cuda:0')
c= tensor(1.9325e+09, device='cuda:0')
c= tensor(2.1038e+09, device='cuda:0')
c= tensor(2.1040e+09, device='cuda:0')
c= tensor(2.1040e+09, device='cuda:0')
c= tensor(2.1184e+09, device='cuda:0')
c= tensor(2.1210e+09, device='cuda:0')
c= tensor(2.1210e+09, device='cuda:0')
c= tensor(2.1211e+09, device='cuda:0')
c= tensor(2.1214e+09, device='cuda:0')
c= tensor(2.2034e+09, device='cuda:0')
c= tensor(2.3103e+09, device='cuda:0')
c= tensor(2.3141e+09, device='cuda:0')
c= tensor(2.3142e+09, device='cuda:0')
c= tensor(2.3142e+09, device='cuda:0')
c= tensor(2.3142e+09, device='cuda:0')
c= tensor(2.3161e+09, device='cuda:0')
c= tensor(2.3161e+09, device='cuda:0')
c= tensor(2.3165e+09, device='cuda:0')
c= tensor(2.3225e+09, device='cuda:0')
c= tensor(2.3487e+09, device='cuda:0')
c= tensor(2.3488e+09, device='cuda:0')
c= tensor(2.3488e+09, device='cuda:0')
c= tensor(2.3492e+09, device='cuda:0')
c= tensor(2.3572e+09, device='cuda:0')
c= tensor(2.3585e+09, device='cuda:0')
c= tensor(2.3585e+09, device='cuda:0')
c= tensor(2.3585e+09, device='cuda:0')
c= tensor(2.3586e+09, device='cuda:0')
c= tensor(2.3586e+09, device='cuda:0')
c= tensor(2.3589e+09, device='cuda:0')
c= tensor(2.3590e+09, device='cuda:0')
c= tensor(2.3590e+09, device='cuda:0')
c= tensor(2.3591e+09, device='cuda:0')
c= tensor(2.3591e+09, device='cuda:0')
c= tensor(2.3591e+09, device='cuda:0')
c= tensor(2.3639e+09, device='cuda:0')
c= tensor(2.3874e+09, device='cuda:0')
c= tensor(2.3902e+09, device='cuda:0')
c= tensor(2.3911e+09, device='cuda:0')
c= tensor(2.3912e+09, device='cuda:0')
c= tensor(2.3913e+09, device='cuda:0')
c= tensor(2.3914e+09, device='cuda:0')
c= tensor(2.3943e+09, device='cuda:0')
c= tensor(2.3945e+09, device='cuda:0')
c= tensor(2.3949e+09, device='cuda:0')
c= tensor(2.3951e+09, device='cuda:0')
c= tensor(2.5514e+09, device='cuda:0')
c= tensor(2.5515e+09, device='cuda:0')
c= tensor(2.5525e+09, device='cuda:0')
c= tensor(2.5631e+09, device='cuda:0')
c= tensor(2.5656e+09, device='cuda:0')
c= tensor(2.5658e+09, device='cuda:0')
c= tensor(2.5873e+09, device='cuda:0')
c= tensor(2.5918e+09, device='cuda:0')
c= tensor(2.5919e+09, device='cuda:0')
c= tensor(2.5920e+09, device='cuda:0')
c= tensor(2.5922e+09, device='cuda:0')
c= tensor(2.5923e+09, device='cuda:0')
c= tensor(2.5932e+09, device='cuda:0')
c= tensor(2.6203e+09, device='cuda:0')
c= tensor(2.6241e+09, device='cuda:0')
c= tensor(2.6266e+09, device='cuda:0')
c= tensor(2.6267e+09, device='cuda:0')
c= tensor(2.6267e+09, device='cuda:0')
c= tensor(2.6268e+09, device='cuda:0')
c= tensor(2.6269e+09, device='cuda:0')
c= tensor(2.6408e+09, device='cuda:0')
c= tensor(2.6650e+09, device='cuda:0')
c= tensor(2.6650e+09, device='cuda:0')
c= tensor(2.8940e+09, device='cuda:0')
c= tensor(2.9038e+09, device='cuda:0')
c= tensor(2.9044e+09, device='cuda:0')
c= tensor(2.9044e+09, device='cuda:0')
c= tensor(2.9052e+09, device='cuda:0')
c= tensor(2.9094e+09, device='cuda:0')
c= tensor(2.9094e+09, device='cuda:0')
c= tensor(2.9614e+09, device='cuda:0')
c= tensor(2.9618e+09, device='cuda:0')
c= tensor(2.9623e+09, device='cuda:0')
c= tensor(2.9624e+09, device='cuda:0')
c= tensor(2.9625e+09, device='cuda:0')
c= tensor(2.9625e+09, device='cuda:0')
c= tensor(2.9625e+09, device='cuda:0')
c= tensor(2.9626e+09, device='cuda:0')
c= tensor(2.9632e+09, device='cuda:0')
c= tensor(3.2964e+09, device='cuda:0')
c= tensor(3.2974e+09, device='cuda:0')
c= tensor(3.3087e+09, device='cuda:0')
c= tensor(3.3087e+09, device='cuda:0')
c= tensor(3.3088e+09, device='cuda:0')
c= tensor(3.3089e+09, device='cuda:0')
c= tensor(3.3124e+09, device='cuda:0')
c= tensor(3.3166e+09, device='cuda:0')
c= tensor(3.5176e+09, device='cuda:0')
c= tensor(3.5180e+09, device='cuda:0')
c= tensor(3.5223e+09, device='cuda:0')
c= tensor(3.5225e+09, device='cuda:0')
c= tensor(3.5262e+09, device='cuda:0')
c= tensor(3.5324e+09, device='cuda:0')
c= tensor(3.5329e+09, device='cuda:0')
c= tensor(3.5329e+09, device='cuda:0')
c= tensor(3.5357e+09, device='cuda:0')
c= tensor(3.5363e+09, device='cuda:0')
c= tensor(3.5383e+09, device='cuda:0')
c= tensor(3.5583e+09, device='cuda:0')
c= tensor(3.5587e+09, device='cuda:0')
c= tensor(3.5592e+09, device='cuda:0')
c= tensor(3.5594e+09, device='cuda:0')
c= tensor(3.5644e+09, device='cuda:0')
c= tensor(3.5720e+09, device='cuda:0')
c= tensor(3.5722e+09, device='cuda:0')
c= tensor(3.5723e+09, device='cuda:0')
c= tensor(3.5988e+09, device='cuda:0')
c= tensor(3.6003e+09, device='cuda:0')
c= tensor(3.6340e+09, device='cuda:0')
c= tensor(3.6370e+09, device='cuda:0')
c= tensor(3.6394e+09, device='cuda:0')
c= tensor(3.6398e+09, device='cuda:0')
c= tensor(3.6498e+09, device='cuda:0')
c= tensor(3.7001e+09, device='cuda:0')
c= tensor(3.7010e+09, device='cuda:0')
c= tensor(3.7010e+09, device='cuda:0')
c= tensor(3.7012e+09, device='cuda:0')
c= tensor(3.7017e+09, device='cuda:0')
c= tensor(3.7049e+09, device='cuda:0')
c= tensor(3.7055e+09, device='cuda:0')
c= tensor(3.7055e+09, device='cuda:0')
c= tensor(3.7057e+09, device='cuda:0')
c= tensor(3.7141e+09, device='cuda:0')
c= tensor(3.7150e+09, device='cuda:0')
c= tensor(3.7154e+09, device='cuda:0')
c= tensor(3.7162e+09, device='cuda:0')
c= tensor(3.7172e+09, device='cuda:0')
c= tensor(3.7173e+09, device='cuda:0')
c= tensor(3.7173e+09, device='cuda:0')
c= tensor(3.7174e+09, device='cuda:0')
c= tensor(3.7244e+09, device='cuda:0')
c= tensor(3.7245e+09, device='cuda:0')
c= tensor(3.7254e+09, device='cuda:0')
c= tensor(3.7254e+09, device='cuda:0')
c= tensor(3.7254e+09, device='cuda:0')
c= tensor(3.7318e+09, device='cuda:0')
c= tensor(3.7336e+09, device='cuda:0')
c= tensor(3.7340e+09, device='cuda:0')
c= tensor(3.7340e+09, device='cuda:0')
c= tensor(3.7340e+09, device='cuda:0')
c= tensor(3.7341e+09, device='cuda:0')
c= tensor(3.7344e+09, device='cuda:0')
c= tensor(3.7382e+09, device='cuda:0')
c= tensor(3.7382e+09, device='cuda:0')
c= tensor(3.7384e+09, device='cuda:0')
c= tensor(3.7386e+09, device='cuda:0')
c= tensor(3.7390e+09, device='cuda:0')
c= tensor(3.7445e+09, device='cuda:0')
c= tensor(3.7447e+09, device='cuda:0')
c= tensor(3.7464e+09, device='cuda:0')
c= tensor(3.7661e+09, device='cuda:0')
c= tensor(3.7665e+09, device='cuda:0')
c= tensor(3.7671e+09, device='cuda:0')
c= tensor(3.7688e+09, device='cuda:0')
c= tensor(3.7688e+09, device='cuda:0')
c= tensor(3.7690e+09, device='cuda:0')
c= tensor(3.7691e+09, device='cuda:0')
c= tensor(3.7826e+09, device='cuda:0')
c= tensor(3.7836e+09, device='cuda:0')
c= tensor(3.7842e+09, device='cuda:0')
c= tensor(3.7870e+09, device='cuda:0')
c= tensor(3.7882e+09, device='cuda:0')
c= tensor(3.7883e+09, device='cuda:0')
c= tensor(3.7883e+09, device='cuda:0')
c= tensor(3.7885e+09, device='cuda:0')
c= tensor(3.7916e+09, device='cuda:0')
c= tensor(3.8308e+09, device='cuda:0')
c= tensor(3.8330e+09, device='cuda:0')
c= tensor(3.8330e+09, device='cuda:0')
c= tensor(3.8331e+09, device='cuda:0')
c= tensor(3.8868e+09, device='cuda:0')
c= tensor(3.8869e+09, device='cuda:0')
c= tensor(3.8870e+09, device='cuda:0')
c= tensor(3.8879e+09, device='cuda:0')
c= tensor(3.8889e+09, device='cuda:0')
c= tensor(3.8889e+09, device='cuda:0')
c= tensor(3.8890e+09, device='cuda:0')
c= tensor(3.8895e+09, device='cuda:0')
memory (bytes)
4678152192
time for making loss 2 is 13.049672603607178
p0 True
it  0 : 1908014080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 35% |
shape of L is 
torch.Size([])
memory (bytes)
4678483968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 15% |
memory (bytes)
4679094272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 17% |
error is  62323044000.0
relative error loss 16.023527
shape of L is 
torch.Size([])
memory (bytes)
4849815552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 17% |
memory (bytes)
4849930240
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  62322766000.0
relative error loss 16.023457
shape of L is 
torch.Size([])
memory (bytes)
4854370304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 17% |
memory (bytes)
4854370304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  62321890000.0
relative error loss 16.02323
shape of L is 
torch.Size([])
memory (bytes)
4856492032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 17% |
memory (bytes)
4856492032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  62316700000.0
relative error loss 16.021896
shape of L is 
torch.Size([])
memory (bytes)
4858621952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 17% |
memory (bytes)
4858621952
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  62288085000.0
relative error loss 16.01454
shape of L is 
torch.Size([])
memory (bytes)
4860592128
| ID | GPU | MEM |
------------------
|  0 |  2% |  0% |
|  1 |  7% | 17% |
memory (bytes)
4860592128
| ID | GPU | MEM |
------------------
|  0 |  2% |  0% |
|  1 | 99% | 17% |
error is  61974106000.0
relative error loss 15.933814
shape of L is 
torch.Size([])
memory (bytes)
4862869504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 17% |
memory (bytes)
4862889984
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  60422160000.0
relative error loss 15.5348015
shape of L is 
torch.Size([])
memory (bytes)
4865007616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 17% |
memory (bytes)
4865036288
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  52442540000.0
relative error loss 13.483206
shape of L is 
torch.Size([])
memory (bytes)
4867174400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 17% |
memory (bytes)
4867190784
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  14172813000.0
relative error loss 3.6438923
shape of L is 
torch.Size([])
memory (bytes)
4869320704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 17% |
memory (bytes)
4869320704
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  7655091000.0
relative error loss 1.9681574
time to take a step is 216.71404457092285
it  1 : 2317884928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
4871430144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 17% |
memory (bytes)
4871446528
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  7655091000.0
relative error loss 1.9681574
shape of L is 
torch.Size([])
memory (bytes)
4873551872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 17% |
memory (bytes)
4873572352
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  8304852000.0
relative error loss 2.1352139
shape of L is 
torch.Size([])
memory (bytes)
4875730944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 17% |
memory (bytes)
4875767808
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  5661993000.0
relative error loss 1.4557232
shape of L is 
torch.Size([])
memory (bytes)
4877877248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 17% |
memory (bytes)
4877877248
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  4319520300.0
relative error loss 1.1105676
shape of L is 
torch.Size([])
memory (bytes)
4880019456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 17% |
memory (bytes)
4880019456
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  4000593000.0
relative error loss 1.0285699
shape of L is 
torch.Size([])
memory (bytes)
4882141184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 17% |
memory (bytes)
4882157568
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3733286100.0
relative error loss 0.9598442
shape of L is 
torch.Size([])
memory (bytes)
4884287488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 17% |
memory (bytes)
4884307968
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3703218700.0
relative error loss 0.95211375
shape of L is 
torch.Size([])
memory (bytes)
4886347776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 17% |
memory (bytes)
4886347776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  3898406400.0
relative error loss 1.0022973
shape of L is 
torch.Size([])
memory (bytes)
4888526848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 17% |
memory (bytes)
4888526848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 17% |
error is  3543095600.0
relative error loss 0.91094536
shape of L is 
torch.Size([])
memory (bytes)
4890628096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 17% |
memory (bytes)
4890648576
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3489393700.0
relative error loss 0.89713836
time to take a step is 214.74212861061096
it  2 : 2434739712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
4892733440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 17% |
memory (bytes)
4892749824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  3489393700.0
relative error loss 0.89713836
shape of L is 
torch.Size([])
memory (bytes)
4894879744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 17% |
memory (bytes)
4894879744
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3813969200.0
relative error loss 0.98058814
shape of L is 
torch.Size([])
memory (bytes)
4896980992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 17% |
memory (bytes)
4897001472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  3325585700.0
relative error loss 0.8550226
shape of L is 
torch.Size([])
memory (bytes)
4899110912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 17% |
memory (bytes)
4899131392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  3140789200.0
relative error loss 0.8075107
shape of L is 
torch.Size([])
memory (bytes)
4901220352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 17% |
memory (bytes)
4901240832
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3164484000.0
relative error loss 0.8136027
shape of L is 
torch.Size([])
memory (bytes)
4903354368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 17% |
memory (bytes)
4903374848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% | 17% |
error is  2973091800.0
relative error loss 0.76439494
shape of L is 
torch.Size([])
memory (bytes)
4905463808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 17% |
memory (bytes)
4905480192
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2750952400.0
relative error loss 0.7072819
shape of L is 
torch.Size([])
memory (bytes)
4907585536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 17% |
memory (bytes)
4907606016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 17% |
error is  2643206700.0
relative error loss 0.67958
shape of L is 
torch.Size([])
memory (bytes)
4909731840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
4909731840
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2461864700.0
relative error loss 0.6329562
shape of L is 
torch.Size([])
memory (bytes)
4911865856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 17% |
memory (bytes)
4911865856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% | 17% |
error is  2140292400.0
relative error loss 0.55027854
time to take a step is 212.34593534469604
it  3 : 2434739712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
4913991680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 17% |
memory (bytes)
4914012160
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2140292400.0
relative error loss 0.55027854
shape of L is 
torch.Size([])
memory (bytes)
4916129792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 17% |
memory (bytes)
4916150272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 17% |
error is  1898676400.0
relative error loss 0.488158
shape of L is 
torch.Size([])
memory (bytes)
4918280192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
4918280192
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2096286200.0
relative error loss 0.53896433
shape of L is 
torch.Size([])
memory (bytes)
4920332288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 17% |
memory (bytes)
4920430592
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  1707375600.0
relative error loss 0.43897372
shape of L is 
torch.Size([])
memory (bytes)
4922556416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 17% |
memory (bytes)
4922576896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1538646300.0
relative error loss 0.3955927
shape of L is 
torch.Size([])
memory (bytes)
4924694528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 17% |
memory (bytes)
4924715008
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  1338603500.0
relative error loss 0.34416083
shape of L is 
torch.Size([])
memory (bytes)
4926857216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 17% |
memory (bytes)
4926857216
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  1179756500.0
relative error loss 0.30332056
shape of L is 
torch.Size([])
memory (bytes)
4928995328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 17% |
memory (bytes)
4928995328
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  1079462900.0
relative error loss 0.27753463
shape of L is 
torch.Size([])
memory (bytes)
4931104768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 17% |
memory (bytes)
4931121152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% | 17% |
error is  1003680500.0
relative error loss 0.25805065
shape of L is 
torch.Size([])
memory (bytes)
4933189632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 17% |
memory (bytes)
4933267456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  922868500.0
relative error loss 0.23727353
time to take a step is 214.0843644142151
c= tensor(1108.4108, device='cuda:0')
c= tensor(100674.7266, device='cuda:0')
c= tensor(103160.1953, device='cuda:0')
c= tensor(115709.3516, device='cuda:0')
c= tensor(2439110.2500, device='cuda:0')
c= tensor(3064290.2500, device='cuda:0')
c= tensor(3658738.2500, device='cuda:0')
c= tensor(4070134., device='cuda:0')
c= tensor(4936970., device='cuda:0')
c= tensor(18009818., device='cuda:0')
c= tensor(18125408., device='cuda:0')
c= tensor(19910068., device='cuda:0')
c= tensor(19924056., device='cuda:0')
c= tensor(31246916., device='cuda:0')
c= tensor(31395698., device='cuda:0')
c= tensor(31593544., device='cuda:0')
c= tensor(32110246., device='cuda:0')
c= tensor(33005720., device='cuda:0')
c= tensor(37353856., device='cuda:0')
c= tensor(40892984., device='cuda:0')
c= tensor(41147064., device='cuda:0')
c= tensor(48806400., device='cuda:0')
c= tensor(48856480., device='cuda:0')
c= tensor(48934244., device='cuda:0')
c= tensor(49311476., device='cuda:0')
c= tensor(50094312., device='cuda:0')
c= tensor(51906604., device='cuda:0')
c= tensor(51975432., device='cuda:0')
c= tensor(61196960., device='cuda:0')
c= tensor(2.5843e+08, device='cuda:0')
c= tensor(2.5853e+08, device='cuda:0')
c= tensor(4.5985e+08, device='cuda:0')
c= tensor(4.5988e+08, device='cuda:0')
c= tensor(4.6019e+08, device='cuda:0')
c= tensor(4.6091e+08, device='cuda:0')
c= tensor(4.9574e+08, device='cuda:0')
c= tensor(4.9684e+08, device='cuda:0')
c= tensor(4.9684e+08, device='cuda:0')
c= tensor(4.9685e+08, device='cuda:0')
c= tensor(4.9685e+08, device='cuda:0')
c= tensor(4.9686e+08, device='cuda:0')
c= tensor(4.9686e+08, device='cuda:0')
c= tensor(4.9686e+08, device='cuda:0')
c= tensor(4.9686e+08, device='cuda:0')
c= tensor(4.9686e+08, device='cuda:0')
c= tensor(4.9687e+08, device='cuda:0')
c= tensor(4.9687e+08, device='cuda:0')
c= tensor(4.9688e+08, device='cuda:0')
c= tensor(4.9689e+08, device='cuda:0')
c= tensor(4.9691e+08, device='cuda:0')
c= tensor(4.9695e+08, device='cuda:0')
c= tensor(4.9695e+08, device='cuda:0')
c= tensor(4.9701e+08, device='cuda:0')
c= tensor(4.9702e+08, device='cuda:0')
c= tensor(4.9703e+08, device='cuda:0')
c= tensor(4.9704e+08, device='cuda:0')
c= tensor(4.9704e+08, device='cuda:0')
c= tensor(4.9705e+08, device='cuda:0')
c= tensor(4.9705e+08, device='cuda:0')
c= tensor(4.9706e+08, device='cuda:0')
c= tensor(4.9707e+08, device='cuda:0')
c= tensor(4.9707e+08, device='cuda:0')
c= tensor(4.9708e+08, device='cuda:0')
c= tensor(4.9712e+08, device='cuda:0')
c= tensor(4.9713e+08, device='cuda:0')
c= tensor(4.9714e+08, device='cuda:0')
c= tensor(4.9714e+08, device='cuda:0')
c= tensor(4.9714e+08, device='cuda:0')
c= tensor(4.9715e+08, device='cuda:0')
c= tensor(4.9716e+08, device='cuda:0')
c= tensor(4.9717e+08, device='cuda:0')
c= tensor(4.9718e+08, device='cuda:0')
c= tensor(4.9718e+08, device='cuda:0')
c= tensor(4.9718e+08, device='cuda:0')
c= tensor(4.9719e+08, device='cuda:0')
c= tensor(4.9726e+08, device='cuda:0')
c= tensor(4.9726e+08, device='cuda:0')
c= tensor(4.9726e+08, device='cuda:0')
c= tensor(4.9726e+08, device='cuda:0')
c= tensor(4.9730e+08, device='cuda:0')
c= tensor(4.9730e+08, device='cuda:0')
c= tensor(4.9730e+08, device='cuda:0')
c= tensor(4.9730e+08, device='cuda:0')
c= tensor(4.9731e+08, device='cuda:0')
c= tensor(4.9731e+08, device='cuda:0')
c= tensor(4.9731e+08, device='cuda:0')
c= tensor(4.9731e+08, device='cuda:0')
c= tensor(4.9731e+08, device='cuda:0')
c= tensor(4.9732e+08, device='cuda:0')
c= tensor(4.9733e+08, device='cuda:0')
c= tensor(4.9734e+08, device='cuda:0')
c= tensor(4.9734e+08, device='cuda:0')
c= tensor(4.9734e+08, device='cuda:0')
c= tensor(4.9738e+08, device='cuda:0')
c= tensor(4.9739e+08, device='cuda:0')
c= tensor(4.9740e+08, device='cuda:0')
c= tensor(4.9740e+08, device='cuda:0')
c= tensor(4.9741e+08, device='cuda:0')
c= tensor(4.9743e+08, device='cuda:0')
c= tensor(4.9743e+08, device='cuda:0')
c= tensor(4.9747e+08, device='cuda:0')
c= tensor(4.9749e+08, device='cuda:0')
c= tensor(4.9749e+08, device='cuda:0')
c= tensor(4.9750e+08, device='cuda:0')
c= tensor(4.9753e+08, device='cuda:0')
c= tensor(4.9753e+08, device='cuda:0')
c= tensor(4.9753e+08, device='cuda:0')
c= tensor(4.9754e+08, device='cuda:0')
c= tensor(4.9754e+08, device='cuda:0')
c= tensor(4.9754e+08, device='cuda:0')
c= tensor(4.9754e+08, device='cuda:0')
c= tensor(4.9755e+08, device='cuda:0')
c= tensor(4.9755e+08, device='cuda:0')
c= tensor(4.9755e+08, device='cuda:0')
c= tensor(4.9757e+08, device='cuda:0')
c= tensor(4.9757e+08, device='cuda:0')
c= tensor(4.9757e+08, device='cuda:0')
c= tensor(4.9757e+08, device='cuda:0')
c= tensor(4.9760e+08, device='cuda:0')
c= tensor(4.9760e+08, device='cuda:0')
c= tensor(4.9762e+08, device='cuda:0')
c= tensor(4.9762e+08, device='cuda:0')
c= tensor(4.9763e+08, device='cuda:0')
c= tensor(4.9764e+08, device='cuda:0')
c= tensor(4.9765e+08, device='cuda:0')
c= tensor(4.9765e+08, device='cuda:0')
c= tensor(4.9765e+08, device='cuda:0')
c= tensor(4.9766e+08, device='cuda:0')
c= tensor(4.9767e+08, device='cuda:0')
c= tensor(4.9768e+08, device='cuda:0')
c= tensor(4.9772e+08, device='cuda:0')
c= tensor(4.9772e+08, device='cuda:0')
c= tensor(4.9772e+08, device='cuda:0')
c= tensor(4.9772e+08, device='cuda:0')
c= tensor(4.9772e+08, device='cuda:0')
c= tensor(4.9773e+08, device='cuda:0')
c= tensor(4.9773e+08, device='cuda:0')
c= tensor(4.9773e+08, device='cuda:0')
c= tensor(4.9774e+08, device='cuda:0')
c= tensor(4.9774e+08, device='cuda:0')
c= tensor(4.9774e+08, device='cuda:0')
c= tensor(4.9774e+08, device='cuda:0')
c= tensor(4.9776e+08, device='cuda:0')
c= tensor(4.9780e+08, device='cuda:0')
c= tensor(4.9780e+08, device='cuda:0')
c= tensor(4.9781e+08, device='cuda:0')
c= tensor(4.9781e+08, device='cuda:0')
c= tensor(4.9781e+08, device='cuda:0')
c= tensor(4.9781e+08, device='cuda:0')
c= tensor(4.9781e+08, device='cuda:0')
c= tensor(4.9782e+08, device='cuda:0')
c= tensor(4.9782e+08, device='cuda:0')
c= tensor(4.9783e+08, device='cuda:0')
c= tensor(4.9784e+08, device='cuda:0')
c= tensor(4.9785e+08, device='cuda:0')
c= tensor(4.9787e+08, device='cuda:0')
c= tensor(4.9788e+08, device='cuda:0')
c= tensor(4.9788e+08, device='cuda:0')
c= tensor(4.9788e+08, device='cuda:0')
c= tensor(4.9788e+08, device='cuda:0')
c= tensor(4.9792e+08, device='cuda:0')
c= tensor(4.9792e+08, device='cuda:0')
c= tensor(4.9794e+08, device='cuda:0')
c= tensor(4.9794e+08, device='cuda:0')
c= tensor(4.9795e+08, device='cuda:0')
c= tensor(4.9796e+08, device='cuda:0')
c= tensor(4.9796e+08, device='cuda:0')
c= tensor(4.9796e+08, device='cuda:0')
c= tensor(4.9797e+08, device='cuda:0')
c= tensor(4.9797e+08, device='cuda:0')
c= tensor(4.9797e+08, device='cuda:0')
c= tensor(4.9797e+08, device='cuda:0')
c= tensor(4.9798e+08, device='cuda:0')
c= tensor(4.9798e+08, device='cuda:0')
c= tensor(4.9799e+08, device='cuda:0')
c= tensor(4.9800e+08, device='cuda:0')
c= tensor(4.9800e+08, device='cuda:0')
c= tensor(4.9800e+08, device='cuda:0')
c= tensor(4.9801e+08, device='cuda:0')
c= tensor(4.9802e+08, device='cuda:0')
c= tensor(4.9803e+08, device='cuda:0')
c= tensor(4.9803e+08, device='cuda:0')
c= tensor(4.9804e+08, device='cuda:0')
c= tensor(4.9804e+08, device='cuda:0')
c= tensor(4.9805e+08, device='cuda:0')
c= tensor(4.9806e+08, device='cuda:0')
c= tensor(4.9807e+08, device='cuda:0')
c= tensor(4.9807e+08, device='cuda:0')
c= tensor(4.9808e+08, device='cuda:0')
c= tensor(4.9811e+08, device='cuda:0')
c= tensor(4.9812e+08, device='cuda:0')
c= tensor(4.9812e+08, device='cuda:0')
c= tensor(4.9812e+08, device='cuda:0')
c= tensor(4.9813e+08, device='cuda:0')
c= tensor(4.9813e+08, device='cuda:0')
c= tensor(4.9814e+08, device='cuda:0')
c= tensor(4.9814e+08, device='cuda:0')
c= tensor(4.9814e+08, device='cuda:0')
c= tensor(4.9816e+08, device='cuda:0')
c= tensor(4.9816e+08, device='cuda:0')
c= tensor(4.9818e+08, device='cuda:0')
c= tensor(4.9818e+08, device='cuda:0')
c= tensor(4.9822e+08, device='cuda:0')
c= tensor(4.9823e+08, device='cuda:0')
c= tensor(4.9827e+08, device='cuda:0')
c= tensor(4.9828e+08, device='cuda:0')
c= tensor(4.9828e+08, device='cuda:0')
c= tensor(4.9828e+08, device='cuda:0')
c= tensor(4.9829e+08, device='cuda:0')
c= tensor(4.9830e+08, device='cuda:0')
c= tensor(4.9830e+08, device='cuda:0')
c= tensor(4.9831e+08, device='cuda:0')
c= tensor(4.9831e+08, device='cuda:0')
c= tensor(4.9832e+08, device='cuda:0')
c= tensor(4.9832e+08, device='cuda:0')
c= tensor(4.9832e+08, device='cuda:0')
c= tensor(4.9832e+08, device='cuda:0')
c= tensor(4.9833e+08, device='cuda:0')
c= tensor(4.9839e+08, device='cuda:0')
c= tensor(4.9839e+08, device='cuda:0')
c= tensor(4.9840e+08, device='cuda:0')
c= tensor(4.9840e+08, device='cuda:0')
c= tensor(4.9844e+08, device='cuda:0')
c= tensor(4.9844e+08, device='cuda:0')
c= tensor(4.9844e+08, device='cuda:0')
c= tensor(4.9845e+08, device='cuda:0')
c= tensor(4.9845e+08, device='cuda:0')
c= tensor(4.9845e+08, device='cuda:0')
c= tensor(4.9846e+08, device='cuda:0')
c= tensor(4.9846e+08, device='cuda:0')
c= tensor(4.9846e+08, device='cuda:0')
c= tensor(4.9847e+08, device='cuda:0')
c= tensor(4.9847e+08, device='cuda:0')
c= tensor(4.9847e+08, device='cuda:0')
c= tensor(4.9848e+08, device='cuda:0')
c= tensor(4.9848e+08, device='cuda:0')
c= tensor(4.9850e+08, device='cuda:0')
c= tensor(4.9850e+08, device='cuda:0')
c= tensor(4.9851e+08, device='cuda:0')
c= tensor(4.9854e+08, device='cuda:0')
c= tensor(4.9879e+08, device='cuda:0')
c= tensor(4.9888e+08, device='cuda:0')
c= tensor(4.9889e+08, device='cuda:0')
c= tensor(4.9890e+08, device='cuda:0')
c= tensor(4.9892e+08, device='cuda:0')
c= tensor(4.9908e+08, device='cuda:0')
c= tensor(4.9964e+08, device='cuda:0')
c= tensor(4.9965e+08, device='cuda:0')
c= tensor(5.0576e+08, device='cuda:0')
c= tensor(5.0713e+08, device='cuda:0')
c= tensor(5.0755e+08, device='cuda:0')
c= tensor(5.0885e+08, device='cuda:0')
c= tensor(5.0885e+08, device='cuda:0')
c= tensor(5.0891e+08, device='cuda:0')
c= tensor(5.3180e+08, device='cuda:0')
c= tensor(5.5057e+08, device='cuda:0')
c= tensor(5.5058e+08, device='cuda:0')
c= tensor(5.5077e+08, device='cuda:0')
c= tensor(5.5673e+08, device='cuda:0')
c= tensor(5.5708e+08, device='cuda:0')
c= tensor(5.5736e+08, device='cuda:0')
c= tensor(5.5776e+08, device='cuda:0')
c= tensor(5.5793e+08, device='cuda:0')
c= tensor(5.5810e+08, device='cuda:0')
c= tensor(5.5814e+08, device='cuda:0')
c= tensor(5.6471e+08, device='cuda:0')
c= tensor(5.6496e+08, device='cuda:0')
c= tensor(5.6497e+08, device='cuda:0')
c= tensor(5.6512e+08, device='cuda:0')
c= tensor(5.6533e+08, device='cuda:0')
c= tensor(5.7956e+08, device='cuda:0')
c= tensor(5.8024e+08, device='cuda:0')
c= tensor(5.8025e+08, device='cuda:0')
c= tensor(5.8072e+08, device='cuda:0')
c= tensor(5.8073e+08, device='cuda:0')
c= tensor(5.8089e+08, device='cuda:0')
c= tensor(5.8190e+08, device='cuda:0')
c= tensor(5.8579e+08, device='cuda:0')
c= tensor(5.8908e+08, device='cuda:0')
c= tensor(5.8908e+08, device='cuda:0')
c= tensor(5.8909e+08, device='cuda:0')
c= tensor(5.9048e+08, device='cuda:0')
c= tensor(5.9192e+08, device='cuda:0')
c= tensor(5.9219e+08, device='cuda:0')
c= tensor(5.9219e+08, device='cuda:0')
c= tensor(6.0775e+08, device='cuda:0')
c= tensor(6.0779e+08, device='cuda:0')
c= tensor(6.0790e+08, device='cuda:0')
c= tensor(6.0965e+08, device='cuda:0')
c= tensor(6.0965e+08, device='cuda:0')
c= tensor(6.1029e+08, device='cuda:0')
c= tensor(6.1250e+08, device='cuda:0')
c= tensor(6.3932e+08, device='cuda:0')
c= tensor(6.3979e+08, device='cuda:0')
c= tensor(6.4020e+08, device='cuda:0')
c= tensor(6.4023e+08, device='cuda:0')
c= tensor(6.4024e+08, device='cuda:0')
c= tensor(6.4104e+08, device='cuda:0')
c= tensor(6.4106e+08, device='cuda:0')
c= tensor(6.4120e+08, device='cuda:0')
c= tensor(6.4882e+08, device='cuda:0')
c= tensor(6.4902e+08, device='cuda:0')
c= tensor(6.4908e+08, device='cuda:0')
c= tensor(6.4909e+08, device='cuda:0')
c= tensor(6.9126e+08, device='cuda:0')
c= tensor(6.9132e+08, device='cuda:0')
c= tensor(6.9141e+08, device='cuda:0')
c= tensor(6.9147e+08, device='cuda:0')
c= tensor(6.9879e+08, device='cuda:0')
c= tensor(6.9893e+08, device='cuda:0')
c= tensor(6.9912e+08, device='cuda:0')
c= tensor(6.9918e+08, device='cuda:0')
c= tensor(6.9990e+08, device='cuda:0')
c= tensor(7.0033e+08, device='cuda:0')
c= tensor(7.7802e+08, device='cuda:0')
c= tensor(7.7888e+08, device='cuda:0')
c= tensor(7.7894e+08, device='cuda:0')
c= tensor(7.8199e+08, device='cuda:0')
c= tensor(7.8469e+08, device='cuda:0')
c= tensor(7.8474e+08, device='cuda:0')
c= tensor(7.8690e+08, device='cuda:0')
c= tensor(7.9436e+08, device='cuda:0')
c= tensor(8.5772e+08, device='cuda:0')
c= tensor(8.5841e+08, device='cuda:0')
c= tensor(8.5842e+08, device='cuda:0')
c= tensor(8.5844e+08, device='cuda:0')
c= tensor(8.5920e+08, device='cuda:0')
c= tensor(8.5946e+08, device='cuda:0')
c= tensor(8.5962e+08, device='cuda:0')
c= tensor(8.5962e+08, device='cuda:0')
c= tensor(8.6002e+08, device='cuda:0')
c= tensor(8.6202e+08, device='cuda:0')
c= tensor(8.6785e+08, device='cuda:0')
c= tensor(8.6788e+08, device='cuda:0')
c= tensor(8.7719e+08, device='cuda:0')
c= tensor(8.7723e+08, device='cuda:0')
c= tensor(8.7733e+08, device='cuda:0')
c= tensor(8.7739e+08, device='cuda:0')
c= tensor(8.7739e+08, device='cuda:0')
c= tensor(8.9968e+08, device='cuda:0')
c= tensor(9.0018e+08, device='cuda:0')
c= tensor(9.0030e+08, device='cuda:0')
c= tensor(9.0060e+08, device='cuda:0')
c= tensor(9.0062e+08, device='cuda:0')
c= tensor(9.3144e+08, device='cuda:0')
c= tensor(9.3154e+08, device='cuda:0')
c= tensor(9.4150e+08, device='cuda:0')
c= tensor(9.4150e+08, device='cuda:0')
c= tensor(9.4151e+08, device='cuda:0')
c= tensor(9.4152e+08, device='cuda:0')
c= tensor(9.4186e+08, device='cuda:0')
c= tensor(9.4187e+08, device='cuda:0')
c= tensor(9.4196e+08, device='cuda:0')
c= tensor(9.4198e+08, device='cuda:0')
c= tensor(9.4199e+08, device='cuda:0')
c= tensor(9.5022e+08, device='cuda:0')
c= tensor(9.5057e+08, device='cuda:0')
c= tensor(9.5093e+08, device='cuda:0')
c= tensor(9.5346e+08, device='cuda:0')
c= tensor(9.7360e+08, device='cuda:0')
c= tensor(9.7362e+08, device='cuda:0')
c= tensor(9.7368e+08, device='cuda:0')
c= tensor(9.7374e+08, device='cuda:0')
c= tensor(9.7374e+08, device='cuda:0')
c= tensor(9.7377e+08, device='cuda:0')
c= tensor(9.7395e+08, device='cuda:0')
c= tensor(9.7399e+08, device='cuda:0')
c= tensor(9.7400e+08, device='cuda:0')
c= tensor(9.7411e+08, device='cuda:0')
c= tensor(9.7420e+08, device='cuda:0')
c= tensor(1.1392e+09, device='cuda:0')
c= tensor(1.1392e+09, device='cuda:0')
c= tensor(1.1399e+09, device='cuda:0')
c= tensor(1.1402e+09, device='cuda:0')
c= tensor(1.1402e+09, device='cuda:0')
c= tensor(1.1437e+09, device='cuda:0')
c= tensor(1.2167e+09, device='cuda:0')
c= tensor(1.2597e+09, device='cuda:0')
c= tensor(1.2604e+09, device='cuda:0')
c= tensor(1.2607e+09, device='cuda:0')
c= tensor(1.2607e+09, device='cuda:0')
c= tensor(1.2618e+09, device='cuda:0')
c= tensor(1.4584e+09, device='cuda:0')
c= tensor(1.4591e+09, device='cuda:0')
c= tensor(1.4592e+09, device='cuda:0')
c= tensor(1.4619e+09, device='cuda:0')
c= tensor(1.4824e+09, device='cuda:0')
c= tensor(1.4834e+09, device='cuda:0')
c= tensor(1.4835e+09, device='cuda:0')
c= tensor(1.4835e+09, device='cuda:0')
c= tensor(1.4836e+09, device='cuda:0')
c= tensor(1.4837e+09, device='cuda:0')
c= tensor(1.5212e+09, device='cuda:0')
c= tensor(1.5213e+09, device='cuda:0')
c= tensor(1.5213e+09, device='cuda:0')
c= tensor(1.5216e+09, device='cuda:0')
c= tensor(1.5220e+09, device='cuda:0')
c= tensor(1.5220e+09, device='cuda:0')
c= tensor(1.5234e+09, device='cuda:0')
c= tensor(1.5249e+09, device='cuda:0')
c= tensor(1.5279e+09, device='cuda:0')
c= tensor(1.5309e+09, device='cuda:0')
c= tensor(1.5333e+09, device='cuda:0')
c= tensor(1.5334e+09, device='cuda:0')
c= tensor(1.5341e+09, device='cuda:0')
c= tensor(1.5349e+09, device='cuda:0')
c= tensor(1.5433e+09, device='cuda:0')
c= tensor(1.5434e+09, device='cuda:0')
c= tensor(1.5635e+09, device='cuda:0')
c= tensor(1.5706e+09, device='cuda:0')
c= tensor(1.5724e+09, device='cuda:0')
c= tensor(1.5728e+09, device='cuda:0')
c= tensor(1.5740e+09, device='cuda:0')
c= tensor(1.5741e+09, device='cuda:0')
c= tensor(1.5741e+09, device='cuda:0')
c= tensor(1.5817e+09, device='cuda:0')
c= tensor(1.5896e+09, device='cuda:0')
c= tensor(1.5928e+09, device='cuda:0')
c= tensor(1.6025e+09, device='cuda:0')
c= tensor(1.6042e+09, device='cuda:0')
c= tensor(1.6069e+09, device='cuda:0')
c= tensor(1.6071e+09, device='cuda:0')
c= tensor(1.6155e+09, device='cuda:0')
c= tensor(1.6155e+09, device='cuda:0')
c= tensor(1.6155e+09, device='cuda:0')
c= tensor(1.6246e+09, device='cuda:0')
c= tensor(1.6249e+09, device='cuda:0')
c= tensor(1.6249e+09, device='cuda:0')
c= tensor(1.6251e+09, device='cuda:0')
c= tensor(1.6304e+09, device='cuda:0')
c= tensor(1.6307e+09, device='cuda:0')
c= tensor(1.6314e+09, device='cuda:0')
c= tensor(1.6314e+09, device='cuda:0')
c= tensor(1.6314e+09, device='cuda:0')
c= tensor(1.6314e+09, device='cuda:0')
c= tensor(1.6323e+09, device='cuda:0')
c= tensor(1.6326e+09, device='cuda:0')
c= tensor(1.6346e+09, device='cuda:0')
c= tensor(1.6346e+09, device='cuda:0')
c= tensor(1.6363e+09, device='cuda:0')
c= tensor(1.6363e+09, device='cuda:0')
c= tensor(1.6366e+09, device='cuda:0')
c= tensor(1.6376e+09, device='cuda:0')
c= tensor(1.6393e+09, device='cuda:0')
c= tensor(1.6394e+09, device='cuda:0')
c= tensor(1.6410e+09, device='cuda:0')
c= tensor(1.6412e+09, device='cuda:0')
c= tensor(1.6413e+09, device='cuda:0')
c= tensor(1.6417e+09, device='cuda:0')
c= tensor(1.6661e+09, device='cuda:0')
c= tensor(1.6661e+09, device='cuda:0')
c= tensor(1.6662e+09, device='cuda:0')
c= tensor(1.6717e+09, device='cuda:0')
c= tensor(1.6718e+09, device='cuda:0')
c= tensor(1.7128e+09, device='cuda:0')
c= tensor(1.7128e+09, device='cuda:0')
c= tensor(1.7178e+09, device='cuda:0')
c= tensor(1.7336e+09, device='cuda:0')
c= tensor(1.7336e+09, device='cuda:0')
c= tensor(1.7379e+09, device='cuda:0')
c= tensor(1.7384e+09, device='cuda:0')
c= tensor(1.7478e+09, device='cuda:0')
c= tensor(1.7479e+09, device='cuda:0')
c= tensor(1.7481e+09, device='cuda:0')
c= tensor(1.7481e+09, device='cuda:0')
c= tensor(1.7482e+09, device='cuda:0')
c= tensor(1.7482e+09, device='cuda:0')
c= tensor(1.7486e+09, device='cuda:0')
c= tensor(1.7493e+09, device='cuda:0')
c= tensor(1.7566e+09, device='cuda:0')
c= tensor(1.7567e+09, device='cuda:0')
c= tensor(1.7567e+09, device='cuda:0')
c= tensor(1.7567e+09, device='cuda:0')
c= tensor(1.7574e+09, device='cuda:0')
c= tensor(1.7575e+09, device='cuda:0')
c= tensor(1.7658e+09, device='cuda:0')
c= tensor(1.7667e+09, device='cuda:0')
c= tensor(1.7667e+09, device='cuda:0')
c= tensor(1.7667e+09, device='cuda:0')
c= tensor(1.7667e+09, device='cuda:0')
c= tensor(1.8504e+09, device='cuda:0')
c= tensor(1.8504e+09, device='cuda:0')
c= tensor(1.8505e+09, device='cuda:0')
c= tensor(1.8526e+09, device='cuda:0')
c= tensor(1.8533e+09, device='cuda:0')
c= tensor(1.8533e+09, device='cuda:0')
c= tensor(1.8534e+09, device='cuda:0')
c= tensor(1.8620e+09, device='cuda:0')
c= tensor(1.8673e+09, device='cuda:0')
c= tensor(1.8680e+09, device='cuda:0')
c= tensor(1.8680e+09, device='cuda:0')
c= tensor(1.8696e+09, device='cuda:0')
c= tensor(1.8705e+09, device='cuda:0')
c= tensor(1.8765e+09, device='cuda:0')
c= tensor(1.8781e+09, device='cuda:0')
c= tensor(1.8781e+09, device='cuda:0')
c= tensor(1.8812e+09, device='cuda:0')
c= tensor(1.8812e+09, device='cuda:0')
c= tensor(1.8825e+09, device='cuda:0')
c= tensor(1.8826e+09, device='cuda:0')
c= tensor(1.8826e+09, device='cuda:0')
c= tensor(1.8829e+09, device='cuda:0')
c= tensor(1.8838e+09, device='cuda:0')
c= tensor(1.8840e+09, device='cuda:0')
c= tensor(1.8846e+09, device='cuda:0')
c= tensor(1.8848e+09, device='cuda:0')
c= tensor(1.9021e+09, device='cuda:0')
c= tensor(1.9021e+09, device='cuda:0')
c= tensor(1.9021e+09, device='cuda:0')
c= tensor(1.9022e+09, device='cuda:0')
c= tensor(1.9024e+09, device='cuda:0')
c= tensor(1.9024e+09, device='cuda:0')
c= tensor(1.9024e+09, device='cuda:0')
c= tensor(1.9026e+09, device='cuda:0')
c= tensor(1.9076e+09, device='cuda:0')
c= tensor(1.9076e+09, device='cuda:0')
c= tensor(1.9076e+09, device='cuda:0')
c= tensor(1.9076e+09, device='cuda:0')
c= tensor(1.9325e+09, device='cuda:0')
c= tensor(2.1038e+09, device='cuda:0')
c= tensor(2.1040e+09, device='cuda:0')
c= tensor(2.1040e+09, device='cuda:0')
c= tensor(2.1184e+09, device='cuda:0')
c= tensor(2.1210e+09, device='cuda:0')
c= tensor(2.1210e+09, device='cuda:0')
c= tensor(2.1211e+09, device='cuda:0')
c= tensor(2.1214e+09, device='cuda:0')
c= tensor(2.2034e+09, device='cuda:0')
c= tensor(2.3103e+09, device='cuda:0')
c= tensor(2.3141e+09, device='cuda:0')
c= tensor(2.3142e+09, device='cuda:0')
c= tensor(2.3142e+09, device='cuda:0')
c= tensor(2.3142e+09, device='cuda:0')
c= tensor(2.3161e+09, device='cuda:0')
c= tensor(2.3161e+09, device='cuda:0')
c= tensor(2.3165e+09, device='cuda:0')
c= tensor(2.3225e+09, device='cuda:0')
c= tensor(2.3487e+09, device='cuda:0')
c= tensor(2.3488e+09, device='cuda:0')
c= tensor(2.3488e+09, device='cuda:0')
c= tensor(2.3492e+09, device='cuda:0')
c= tensor(2.3572e+09, device='cuda:0')
c= tensor(2.3585e+09, device='cuda:0')
c= tensor(2.3585e+09, device='cuda:0')
c= tensor(2.3585e+09, device='cuda:0')
c= tensor(2.3586e+09, device='cuda:0')
c= tensor(2.3586e+09, device='cuda:0')
c= tensor(2.3589e+09, device='cuda:0')
c= tensor(2.3590e+09, device='cuda:0')
c= tensor(2.3590e+09, device='cuda:0')
c= tensor(2.3591e+09, device='cuda:0')
c= tensor(2.3591e+09, device='cuda:0')
c= tensor(2.3591e+09, device='cuda:0')
c= tensor(2.3639e+09, device='cuda:0')
c= tensor(2.3874e+09, device='cuda:0')
c= tensor(2.3902e+09, device='cuda:0')
c= tensor(2.3911e+09, device='cuda:0')
c= tensor(2.3912e+09, device='cuda:0')
c= tensor(2.3913e+09, device='cuda:0')
c= tensor(2.3914e+09, device='cuda:0')
c= tensor(2.3943e+09, device='cuda:0')
c= tensor(2.3945e+09, device='cuda:0')
c= tensor(2.3949e+09, device='cuda:0')
c= tensor(2.3951e+09, device='cuda:0')
c= tensor(2.5514e+09, device='cuda:0')
c= tensor(2.5515e+09, device='cuda:0')
c= tensor(2.5525e+09, device='cuda:0')
c= tensor(2.5631e+09, device='cuda:0')
c= tensor(2.5656e+09, device='cuda:0')
c= tensor(2.5658e+09, device='cuda:0')
c= tensor(2.5873e+09, device='cuda:0')
c= tensor(2.5918e+09, device='cuda:0')
c= tensor(2.5919e+09, device='cuda:0')
c= tensor(2.5920e+09, device='cuda:0')
c= tensor(2.5922e+09, device='cuda:0')
c= tensor(2.5923e+09, device='cuda:0')
c= tensor(2.5932e+09, device='cuda:0')
c= tensor(2.6203e+09, device='cuda:0')
c= tensor(2.6241e+09, device='cuda:0')
c= tensor(2.6266e+09, device='cuda:0')
c= tensor(2.6267e+09, device='cuda:0')
c= tensor(2.6267e+09, device='cuda:0')
c= tensor(2.6268e+09, device='cuda:0')
c= tensor(2.6269e+09, device='cuda:0')
c= tensor(2.6408e+09, device='cuda:0')
c= tensor(2.6650e+09, device='cuda:0')
c= tensor(2.6650e+09, device='cuda:0')
c= tensor(2.8940e+09, device='cuda:0')
c= tensor(2.9038e+09, device='cuda:0')
c= tensor(2.9044e+09, device='cuda:0')
c= tensor(2.9044e+09, device='cuda:0')
c= tensor(2.9052e+09, device='cuda:0')
c= tensor(2.9094e+09, device='cuda:0')
c= tensor(2.9094e+09, device='cuda:0')
c= tensor(2.9614e+09, device='cuda:0')
c= tensor(2.9618e+09, device='cuda:0')
c= tensor(2.9623e+09, device='cuda:0')
c= tensor(2.9624e+09, device='cuda:0')
c= tensor(2.9625e+09, device='cuda:0')
c= tensor(2.9625e+09, device='cuda:0')
c= tensor(2.9625e+09, device='cuda:0')
c= tensor(2.9626e+09, device='cuda:0')
c= tensor(2.9632e+09, device='cuda:0')
c= tensor(3.2964e+09, device='cuda:0')
c= tensor(3.2974e+09, device='cuda:0')
c= tensor(3.3087e+09, device='cuda:0')
c= tensor(3.3087e+09, device='cuda:0')
c= tensor(3.3088e+09, device='cuda:0')
c= tensor(3.3089e+09, device='cuda:0')
c= tensor(3.3124e+09, device='cuda:0')
c= tensor(3.3166e+09, device='cuda:0')
c= tensor(3.5176e+09, device='cuda:0')
c= tensor(3.5180e+09, device='cuda:0')
c= tensor(3.5223e+09, device='cuda:0')
c= tensor(3.5225e+09, device='cuda:0')
c= tensor(3.5262e+09, device='cuda:0')
c= tensor(3.5324e+09, device='cuda:0')
c= tensor(3.5329e+09, device='cuda:0')
c= tensor(3.5329e+09, device='cuda:0')
c= tensor(3.5357e+09, device='cuda:0')
c= tensor(3.5363e+09, device='cuda:0')
c= tensor(3.5383e+09, device='cuda:0')
c= tensor(3.5583e+09, device='cuda:0')
c= tensor(3.5587e+09, device='cuda:0')
c= tensor(3.5592e+09, device='cuda:0')
c= tensor(3.5594e+09, device='cuda:0')
c= tensor(3.5644e+09, device='cuda:0')
c= tensor(3.5720e+09, device='cuda:0')
c= tensor(3.5722e+09, device='cuda:0')
c= tensor(3.5723e+09, device='cuda:0')
c= tensor(3.5988e+09, device='cuda:0')
c= tensor(3.6003e+09, device='cuda:0')
c= tensor(3.6340e+09, device='cuda:0')
c= tensor(3.6370e+09, device='cuda:0')
c= tensor(3.6394e+09, device='cuda:0')
c= tensor(3.6398e+09, device='cuda:0')
c= tensor(3.6498e+09, device='cuda:0')
c= tensor(3.7001e+09, device='cuda:0')
c= tensor(3.7010e+09, device='cuda:0')
c= tensor(3.7010e+09, device='cuda:0')
c= tensor(3.7012e+09, device='cuda:0')
c= tensor(3.7017e+09, device='cuda:0')
c= tensor(3.7049e+09, device='cuda:0')
c= tensor(3.7055e+09, device='cuda:0')
c= tensor(3.7055e+09, device='cuda:0')
c= tensor(3.7057e+09, device='cuda:0')
c= tensor(3.7141e+09, device='cuda:0')
c= tensor(3.7150e+09, device='cuda:0')
c= tensor(3.7154e+09, device='cuda:0')
c= tensor(3.7162e+09, device='cuda:0')
c= tensor(3.7172e+09, device='cuda:0')
c= tensor(3.7173e+09, device='cuda:0')
c= tensor(3.7173e+09, device='cuda:0')
c= tensor(3.7174e+09, device='cuda:0')
c= tensor(3.7244e+09, device='cuda:0')
c= tensor(3.7245e+09, device='cuda:0')
c= tensor(3.7254e+09, device='cuda:0')
c= tensor(3.7254e+09, device='cuda:0')
c= tensor(3.7254e+09, device='cuda:0')
c= tensor(3.7318e+09, device='cuda:0')
c= tensor(3.7336e+09, device='cuda:0')
c= tensor(3.7340e+09, device='cuda:0')
c= tensor(3.7340e+09, device='cuda:0')
c= tensor(3.7340e+09, device='cuda:0')
c= tensor(3.7341e+09, device='cuda:0')
c= tensor(3.7344e+09, device='cuda:0')
c= tensor(3.7382e+09, device='cuda:0')
c= tensor(3.7382e+09, device='cuda:0')
c= tensor(3.7384e+09, device='cuda:0')
c= tensor(3.7386e+09, device='cuda:0')
c= tensor(3.7390e+09, device='cuda:0')
c= tensor(3.7445e+09, device='cuda:0')
c= tensor(3.7447e+09, device='cuda:0')
c= tensor(3.7464e+09, device='cuda:0')
c= tensor(3.7661e+09, device='cuda:0')
c= tensor(3.7665e+09, device='cuda:0')
c= tensor(3.7671e+09, device='cuda:0')
c= tensor(3.7688e+09, device='cuda:0')
c= tensor(3.7688e+09, device='cuda:0')
c= tensor(3.7690e+09, device='cuda:0')
c= tensor(3.7691e+09, device='cuda:0')
c= tensor(3.7826e+09, device='cuda:0')
c= tensor(3.7836e+09, device='cuda:0')
c= tensor(3.7842e+09, device='cuda:0')
c= tensor(3.7870e+09, device='cuda:0')
c= tensor(3.7882e+09, device='cuda:0')
c= tensor(3.7883e+09, device='cuda:0')
c= tensor(3.7883e+09, device='cuda:0')
c= tensor(3.7885e+09, device='cuda:0')
c= tensor(3.7916e+09, device='cuda:0')
c= tensor(3.8308e+09, device='cuda:0')
c= tensor(3.8330e+09, device='cuda:0')
c= tensor(3.8330e+09, device='cuda:0')
c= tensor(3.8331e+09, device='cuda:0')
c= tensor(3.8868e+09, device='cuda:0')
c= tensor(3.8869e+09, device='cuda:0')
c= tensor(3.8870e+09, device='cuda:0')
c= tensor(3.8879e+09, device='cuda:0')
c= tensor(3.8889e+09, device='cuda:0')
c= tensor(3.8889e+09, device='cuda:0')
c= tensor(3.8890e+09, device='cuda:0')
c= tensor(3.8895e+09, device='cuda:0')
time to make c is 10.916803121566772
time for making loss is 10.916888952255249
p0 True
it  0 : 1908264448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
4935442432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 17% |
memory (bytes)
4935598080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  922868500.0
relative error loss 0.23727353
shape of L is 
torch.Size([])
memory (bytes)
4962222080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
4962304000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  916417540.0
relative error loss 0.23561496
shape of L is 
torch.Size([])
memory (bytes)
4965945344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
4965945344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  882502140.0
relative error loss 0.22689517
shape of L is 
torch.Size([])
memory (bytes)
4969144320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 17% |
memory (bytes)
4969144320
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  870701060.0
relative error loss 0.22386105
shape of L is 
torch.Size([])
memory (bytes)
4972347392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 17% |
memory (bytes)
4972347392
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  861057540.0
relative error loss 0.22138166
shape of L is 
torch.Size([])
memory (bytes)
4975546368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 17% |
memory (bytes)
4975546368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  852370700.0
relative error loss 0.21914823
shape of L is 
torch.Size([])
memory (bytes)
4978720768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 17% |
memory (bytes)
4978757632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  846119940.0
relative error loss 0.21754114
shape of L is 
torch.Size([])
memory (bytes)
4981989376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 17% |
memory (bytes)
4981989376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  841194240.0
relative error loss 0.21627472
shape of L is 
torch.Size([])
memory (bytes)
4985200640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
4985200640
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  838016260.0
relative error loss 0.21545765
shape of L is 
torch.Size([])
memory (bytes)
4988379136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
4988411904
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  834892000.0
relative error loss 0.21465439
time to take a step is 284.74741435050964
it  1 : 2437238784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
4991594496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
4991623168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  834892000.0
relative error loss 0.21465439
shape of L is 
torch.Size([])
memory (bytes)
4994818048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
4994838528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 17% |
error is  832169700.0
relative error loss 0.21395448
shape of L is 
torch.Size([])
memory (bytes)
4998049792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 17% |
memory (bytes)
4998049792
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  829544450.0
relative error loss 0.2132795
shape of L is 
torch.Size([])
memory (bytes)
5001224192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 17% |
memory (bytes)
5001277440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  827960060.0
relative error loss 0.21287215
shape of L is 
torch.Size([])
memory (bytes)
5004460032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5004460032
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  825760000.0
relative error loss 0.21230651
shape of L is 
torch.Size([])
memory (bytes)
5007683584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 17% |
memory (bytes)
5007724544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  824365600.0
relative error loss 0.21194799
shape of L is 
torch.Size([])
memory (bytes)
5010931712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
5010939904
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  823135740.0
relative error loss 0.2116318
shape of L is 
torch.Size([])
memory (bytes)
5014126592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 17% |
memory (bytes)
5014163456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  821844740.0
relative error loss 0.21129988
shape of L is 
torch.Size([])
memory (bytes)
5017378816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
5017378816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  820854800.0
relative error loss 0.21104535
shape of L is 
torch.Size([])
memory (bytes)
5020598272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5020598272
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  820228600.0
relative error loss 0.21088436
time to take a step is 274.80194091796875
it  2 : 2437238784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
5023776768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5023813632
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  820228600.0
relative error loss 0.21088436
shape of L is 
torch.Size([])
memory (bytes)
5027033088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 17% |
memory (bytes)
5027033088
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  818444540.0
relative error loss 0.21042567
shape of L is 
torch.Size([])
memory (bytes)
5030248448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5030252544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  817875700.0
relative error loss 0.21027942
shape of L is 
torch.Size([])
memory (bytes)
5033431040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5033467904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% | 17% |
error is  817118700.0
relative error loss 0.2100848
shape of L is 
torch.Size([])
memory (bytes)
5036642304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5036683264
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  816113150.0
relative error loss 0.20982626
shape of L is 
torch.Size([])
memory (bytes)
5039898624
| ID | GPU | MEM |
------------------
|  0 |  1% |  0% |
|  1 | 25% | 17% |
memory (bytes)
5039898624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  815633660.0
relative error loss 0.20970298
shape of L is 
torch.Size([])
memory (bytes)
5043134464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 17% |
memory (bytes)
5043134464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  814905600.0
relative error loss 0.2095158
shape of L is 
torch.Size([])
memory (bytes)
5046329344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
5046345728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  814618900.0
relative error loss 0.20944208
shape of L is 
torch.Size([])
memory (bytes)
5049548800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 17% |
memory (bytes)
5049548800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  813833500.0
relative error loss 0.20924014
shape of L is 
torch.Size([])
memory (bytes)
5052731392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
5052768256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  813724400.0
relative error loss 0.20921211
time to take a step is 280.68017411231995
it  3 : 2437238784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
5055959040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 17% |
memory (bytes)
5055959040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  813724400.0
relative error loss 0.20921211
shape of L is 
torch.Size([])
memory (bytes)
5059158016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 17% |
memory (bytes)
5059194880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  813054200.0
relative error loss 0.20903979
shape of L is 
torch.Size([])
memory (bytes)
5062246400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5062246400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  812715800.0
relative error loss 0.20895278
shape of L is 
torch.Size([])
memory (bytes)
5065613312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 17% |
memory (bytes)
5065613312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% | 17% |
error is  812250100.0
relative error loss 0.20883305
shape of L is 
torch.Size([])
memory (bytes)
5068804096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 17% |
memory (bytes)
5068845056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  811811300.0
relative error loss 0.20872024
shape of L is 
torch.Size([])
memory (bytes)
5072011264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5072052224
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  811384300.0
relative error loss 0.20861046
shape of L is 
torch.Size([])
memory (bytes)
5075226624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 17% |
memory (bytes)
5075226624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  811067140.0
relative error loss 0.2085289
shape of L is 
torch.Size([])
memory (bytes)
5078454272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 17% |
memory (bytes)
5078454272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  810684700.0
relative error loss 0.20843057
shape of L is 
torch.Size([])
memory (bytes)
5081661440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5081698304
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  810413060.0
relative error loss 0.20836075
shape of L is 
torch.Size([])
memory (bytes)
5084872704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5084913664
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  810093060.0
relative error loss 0.20827846
time to take a step is 277.03978419303894
it  4 : 2437238784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
5088096256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 17% |
memory (bytes)
5088133120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  810093060.0
relative error loss 0.20827846
shape of L is 
torch.Size([])
memory (bytes)
5091299328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 17% |
memory (bytes)
5091336192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  809849340.0
relative error loss 0.2082158
shape of L is 
torch.Size([])
memory (bytes)
5094518784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 17% |
memory (bytes)
5094518784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  809530900.0
relative error loss 0.20813394
shape of L is 
torch.Size([])
memory (bytes)
5097742336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 17% |
memory (bytes)
5097783296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  809519900.0
relative error loss 0.2081311
shape of L is 
torch.Size([])
memory (bytes)
5100793856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5100994560
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  809334800.0
relative error loss 0.20808351
shape of L is 
torch.Size([])
memory (bytes)
5104218112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5104218112
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  809085440.0
relative error loss 0.2080194
shape of L is 
torch.Size([])
memory (bytes)
5107429376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5107433472
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  808824300.0
relative error loss 0.20795228
shape of L is 
torch.Size([])
memory (bytes)
5110652928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 17% |
memory (bytes)
5110652928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  808591360.0
relative error loss 0.20789237
shape of L is 
torch.Size([])
memory (bytes)
5113835520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
5113868288
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  808362500.0
relative error loss 0.20783353
shape of L is 
torch.Size([])
memory (bytes)
5117038592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 17% |
memory (bytes)
5117079552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  808193300.0
relative error loss 0.20779003
time to take a step is 272.8248116970062
it  5 : 2437238784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
5120241664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 17% |
memory (bytes)
5120241664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  808193300.0
relative error loss 0.20779003
shape of L is 
torch.Size([])
memory (bytes)
5123489792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5123493888
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  808016100.0
relative error loss 0.20774448
shape of L is 
torch.Size([])
memory (bytes)
5126680576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 17% |
memory (bytes)
5126721536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  807864300.0
relative error loss 0.20770545
shape of L is 
torch.Size([])
memory (bytes)
5129908224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 17% |
memory (bytes)
5129936896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  807689200.0
relative error loss 0.20766044
shape of L is 
torch.Size([])
memory (bytes)
5133148160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
5133148160
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  807599360.0
relative error loss 0.20763732
shape of L is 
torch.Size([])
memory (bytes)
5136318464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 17% |
memory (bytes)
5136359424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  807506200.0
relative error loss 0.20761336
shape of L is 
torch.Size([])
memory (bytes)
5139533824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 17% |
memory (bytes)
5139533824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  807399700.0
relative error loss 0.20758599
shape of L is 
torch.Size([])
memory (bytes)
5142790144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 17% |
memory (bytes)
5142790144
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  807279900.0
relative error loss 0.20755519
shape of L is 
torch.Size([])
memory (bytes)
5145997312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5145997312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  807240450.0
relative error loss 0.20754506
shape of L is 
torch.Size([])
memory (bytes)
5149188096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 17% |
memory (bytes)
5149224960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% | 17% |
error is  807187700.0
relative error loss 0.2075315
time to take a step is 282.8441216945648
it  6 : 2437238784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
5152440320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 17% |
memory (bytes)
5152440320
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  807187700.0
relative error loss 0.2075315
shape of L is 
torch.Size([])
memory (bytes)
5155655680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 17% |
memory (bytes)
5155655680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  807095550.0
relative error loss 0.20750779
shape of L is 
torch.Size([])
memory (bytes)
5158830080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5158871040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 17% |
error is  807025900.0
relative error loss 0.2074899
shape of L is 
torch.Size([])
memory (bytes)
5162045440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
5162086400
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  806883840.0
relative error loss 0.20745337
shape of L is 
torch.Size([])
memory (bytes)
5165301760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 17% |
memory (bytes)
5165305856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  806781200.0
relative error loss 0.20742697
shape of L is 
torch.Size([])
memory (bytes)
5168476160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 17% |
memory (bytes)
5168513024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  806606850.0
relative error loss 0.20738214
shape of L is 
torch.Size([])
memory (bytes)
5171716096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 17% |
memory (bytes)
5171724288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  806415100.0
relative error loss 0.20733285
shape of L is 
torch.Size([])
memory (bytes)
5174943744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 17% |
memory (bytes)
5174947840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  806264800.0
relative error loss 0.20729421
shape of L is 
torch.Size([])
memory (bytes)
5178122240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5178159104
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  806105600.0
relative error loss 0.20725328
shape of L is 
torch.Size([])
memory (bytes)
5181386752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
5181386752
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  806012160.0
relative error loss 0.20722926
time to take a step is 283.868834733963
it  7 : 2437238784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
5184565248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 17% |
memory (bytes)
5184565248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% | 17% |
error is  806012160.0
relative error loss 0.20722926
shape of L is 
torch.Size([])
memory (bytes)
5187813376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 17% |
memory (bytes)
5187813376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% | 17% |
error is  805855000.0
relative error loss 0.20718884
shape of L is 
torch.Size([])
memory (bytes)
5190991872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 17% |
memory (bytes)
5191032832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  805764350.0
relative error loss 0.20716554
shape of L is 
torch.Size([])
memory (bytes)
5194248192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 17% |
memory (bytes)
5194248192
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  805565950.0
relative error loss 0.20711453
shape of L is 
torch.Size([])
memory (bytes)
5197426688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5197463552
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  805489900.0
relative error loss 0.20709498
shape of L is 
torch.Size([])
memory (bytes)
5200683008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 17% |
memory (bytes)
5200683008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  805391360.0
relative error loss 0.20706964
shape of L is 
torch.Size([])
memory (bytes)
5203898368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 17% |
memory (bytes)
5203898368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  805380350.0
relative error loss 0.2070668
shape of L is 
torch.Size([])
memory (bytes)
5207109632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 17% |
memory (bytes)
5207109632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  805317600.0
relative error loss 0.20705068
shape of L is 
torch.Size([])
memory (bytes)
5210333184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 17% |
memory (bytes)
5210333184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 17% |
error is  805207550.0
relative error loss 0.20702238
shape of L is 
torch.Size([])
memory (bytes)
5213540352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 17% |
memory (bytes)
5213540352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  805079040.0
relative error loss 0.20698935
time to take a step is 285.56667017936707
it  8 : 2437238784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
5216632832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 17% |
memory (bytes)
5216632832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  805079040.0
relative error loss 0.20698935
shape of L is 
torch.Size([])
memory (bytes)
5219975168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5219975168
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  804977150.0
relative error loss 0.20696315
shape of L is 
torch.Size([])
memory (bytes)
5223161856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 17% |
memory (bytes)
5223198720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  804867300.0
relative error loss 0.20693491
shape of L is 
torch.Size([])
memory (bytes)
5226418176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 17% |
memory (bytes)
5226418176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  804770300.0
relative error loss 0.20690997
shape of L is 
torch.Size([])
memory (bytes)
5229596672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5229625344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% | 17% |
error is  804669440.0
relative error loss 0.20688403
shape of L is 
torch.Size([])
memory (bytes)
5232844800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 17% |
memory (bytes)
5232844800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  804654100.0
relative error loss 0.20688008
shape of L is 
torch.Size([])
memory (bytes)
5236056064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 17% |
memory (bytes)
5236056064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  804562400.0
relative error loss 0.20685652
shape of L is 
torch.Size([])
memory (bytes)
5239279616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
5239283712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  804519200.0
relative error loss 0.2068454
shape of L is 
torch.Size([])
memory (bytes)
5242499072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 17% |
memory (bytes)
5242499072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  804471550.0
relative error loss 0.20683315
shape of L is 
torch.Size([])
memory (bytes)
5245693952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 17% |
memory (bytes)
5245693952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  804409600.0
relative error loss 0.20681722
time to take a step is 283.42833971977234
it  9 : 2437238784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
5248925696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
5248925696
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  804409600.0
relative error loss 0.20681722
shape of L is 
torch.Size([])
memory (bytes)
5252132864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
5252136960
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  804341500.0
relative error loss 0.20679972
shape of L is 
torch.Size([])
memory (bytes)
5255237632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 17% |
memory (bytes)
5255364608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% | 17% |
error is  804295940.0
relative error loss 0.206788
shape of L is 
torch.Size([])
memory (bytes)
5258530816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 17% |
memory (bytes)
5258571776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 17% |
error is  804237060.0
relative error loss 0.20677286
shape of L is 
torch.Size([])
memory (bytes)
5261766656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5261807616
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  804193540.0
relative error loss 0.20676167
shape of L is 
torch.Size([])
memory (bytes)
5264977920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5265014784
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  804136700.0
relative error loss 0.20674707
shape of L is 
torch.Size([])
memory (bytes)
5268238336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 17% |
memory (bytes)
5268238336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  804088600.0
relative error loss 0.20673469
shape of L is 
torch.Size([])
memory (bytes)
5271453696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5271453696
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  804053500.0
relative error loss 0.20672567
shape of L is 
torch.Size([])
memory (bytes)
5274624000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5274660864
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  803976700.0
relative error loss 0.20670593
shape of L is 
torch.Size([])
memory (bytes)
5277839360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5277880320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  803913200.0
relative error loss 0.20668961
time to take a step is 284.2863435745239
it  10 : 2437238784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
5281099776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 17% |
memory (bytes)
5281099776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% | 17% |
error is  803913200.0
relative error loss 0.20668961
shape of L is 
torch.Size([])
memory (bytes)
5284155392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5284302848
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  803879400.0
relative error loss 0.20668091
shape of L is 
torch.Size([])
memory (bytes)
5287522304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
5287526400
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  803838200.0
relative error loss 0.20667031
shape of L is 
torch.Size([])
memory (bytes)
5290598400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 17% |
memory (bytes)
5290598400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  803761400.0
relative error loss 0.20665057
shape of L is 
torch.Size([])
memory (bytes)
5293944832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5293948928
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  803902460.0
relative error loss 0.20668684
shape of L is 
torch.Size([])
memory (bytes)
5297115136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 17% |
memory (bytes)
5297115136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  803714560.0
relative error loss 0.20663853
shape of L is 
torch.Size([])
memory (bytes)
5300240384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5300379648
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  803640300.0
relative error loss 0.20661944
shape of L is 
torch.Size([])
memory (bytes)
5303562240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5303562240
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  803587300.0
relative error loss 0.20660582
shape of L is 
torch.Size([])
memory (bytes)
5306675200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5306818560
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  803520000.0
relative error loss 0.2065885
shape of L is 
torch.Size([])
memory (bytes)
5310046208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 17% |
memory (bytes)
5310046208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  803463400.0
relative error loss 0.20657396
time to take a step is 274.968878030777
it  11 : 2437238784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
5313101824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 17% |
memory (bytes)
5313269760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  803463400.0
relative error loss 0.20657396
shape of L is 
torch.Size([])
memory (bytes)
5316456448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5316485120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% | 17% |
error is  803428600.0
relative error loss 0.20656501
shape of L is 
torch.Size([])
memory (bytes)
5319565312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5319565312
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  803373300.0
relative error loss 0.20655079
shape of L is 
torch.Size([])
memory (bytes)
5322928128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 17% |
memory (bytes)
5322928128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 17% |
error is  803366140.0
relative error loss 0.20654894
shape of L is 
torch.Size([])
memory (bytes)
5326131200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5326139392
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  803304960.0
relative error loss 0.20653322
shape of L is 
torch.Size([])
memory (bytes)
5329371136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 17% |
memory (bytes)
5329371136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% | 17% |
error is  803271400.0
relative error loss 0.2065246
shape of L is 
torch.Size([])
memory (bytes)
5332541440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 17% |
memory (bytes)
5332582400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  803228700.0
relative error loss 0.2065136
shape of L is 
torch.Size([])
memory (bytes)
5335773184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
5335805952
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  803169300.0
relative error loss 0.20649834
shape of L is 
torch.Size([])
memory (bytes)
5338996736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
5339025408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  803089150.0
relative error loss 0.20647773
shape of L is 
torch.Size([])
memory (bytes)
5342232576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 17% |
memory (bytes)
5342232576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  802990100.0
relative error loss 0.20645227
time to take a step is 281.5266830921173
it  12 : 2437238784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
5345443840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 17% |
memory (bytes)
5345443840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  802990100.0
relative error loss 0.20645227
shape of L is 
torch.Size([])
memory (bytes)
5348663296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5348663296
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  802939900.0
relative error loss 0.20643936
shape of L is 
torch.Size([])
memory (bytes)
5351895040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 17% |
memory (bytes)
5351895040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  802877950.0
relative error loss 0.20642343
shape of L is 
torch.Size([])
memory (bytes)
5355081728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 17% |
memory (bytes)
5355081728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 89% | 17% |
error is  802826000.0
relative error loss 0.20641007
shape of L is 
torch.Size([])
memory (bytes)
5358333952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 17% |
memory (bytes)
5358333952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  802786560.0
relative error loss 0.20639993
shape of L is 
torch.Size([])
memory (bytes)
5361553408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 17% |
memory (bytes)
5361553408
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  802716160.0
relative error loss 0.20638184
shape of L is 
torch.Size([])
memory (bytes)
5364760576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 17% |
memory (bytes)
5364764672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  802685950.0
relative error loss 0.20637406
shape of L is 
torch.Size([])
memory (bytes)
5367992320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5367992320
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  802663200.0
relative error loss 0.20636821
shape of L is 
torch.Size([])
memory (bytes)
5371203584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5371203584
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  802621950.0
relative error loss 0.20635761
shape of L is 
torch.Size([])
memory (bytes)
5374423040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 17% |
memory (bytes)
5374427136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  802584800.0
relative error loss 0.20634808
time to take a step is 287.1385977268219
it  13 : 2437238784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
5377630208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 17% |
memory (bytes)
5377630208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  802584800.0
relative error loss 0.20634808
shape of L is 
torch.Size([])
memory (bytes)
5380845568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 17% |
memory (bytes)
5380845568
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  802572540.0
relative error loss 0.20634492
shape of L is 
torch.Size([])
memory (bytes)
5384065024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5384065024
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  802522600.0
relative error loss 0.20633207
shape of L is 
torch.Size([])
memory (bytes)
5387288576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 17% |
memory (bytes)
5387288576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  802495740.0
relative error loss 0.20632516
shape of L is 
torch.Size([])
memory (bytes)
5390508032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 17% |
memory (bytes)
5390508032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  802471700.0
relative error loss 0.20631897
shape of L is 
torch.Size([])
memory (bytes)
5393715200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
5393715200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 17% |
error is  802435600.0
relative error loss 0.20630969
shape of L is 
torch.Size([])
memory (bytes)
5396942848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
5396942848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 17% |
error is  802382850.0
relative error loss 0.20629615
shape of L is 
torch.Size([])
memory (bytes)
5400145920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 17% |
memory (bytes)
5400145920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  802329340.0
relative error loss 0.20628238
shape of L is 
torch.Size([])
memory (bytes)
5403365376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5403365376
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  802274300.0
relative error loss 0.20626824
shape of L is 
torch.Size([])
memory (bytes)
5406588928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 17% |
memory (bytes)
5406588928
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  802222850.0
relative error loss 0.206255
time to take a step is 285.1898543834686
it  14 : 2437238784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
5409804288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 17% |
memory (bytes)
5409804288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  802222850.0
relative error loss 0.206255
shape of L is 
torch.Size([])
memory (bytes)
5413019648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
5413019648
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  802175200.0
relative error loss 0.20624276
shape of L is 
torch.Size([])
memory (bytes)
5416230912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 17% |
memory (bytes)
5416230912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  802148600.0
relative error loss 0.20623592
shape of L is 
torch.Size([])
memory (bytes)
5419438080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 17% |
memory (bytes)
5419438080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  802114050.0
relative error loss 0.20622703
shape of L is 
torch.Size([])
memory (bytes)
5422649344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5422649344
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  802078460.0
relative error loss 0.20621789
shape of L is 
torch.Size([])
memory (bytes)
5425831936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% | 17% |
memory (bytes)
5425868800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  802061060.0
relative error loss 0.2062134
shape of L is 
torch.Size([])
memory (bytes)
5429088256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 17% |
memory (bytes)
5429088256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  802023940.0
relative error loss 0.20620386
shape of L is 
torch.Size([])
memory (bytes)
5432274944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
5432311808
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  802019600.0
relative error loss 0.20620275
shape of L is 
torch.Size([])
memory (bytes)
5435478016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
5435518976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  801990660.0
relative error loss 0.20619531
shape of L is 
torch.Size([])
memory (bytes)
5438701568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 17% |
memory (bytes)
5438701568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  801978400.0
relative error loss 0.20619215
time to take a step is 278.4436357021332
sum tnnu_Z after tensor(12170234., device='cuda:0')
shape of features
(5211,)
shape of features
(5211,)
number of orig particles 20842
number of new particles after remove low mass 19467
tnuZ shape should be parts x labs
torch.Size([20842, 702])
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
error without small mass is  922579460.0
relative error without small mass is  0.23719922
nnu_Z shape should be number of particles by maxV
(20842, 702)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
shape of features
(20842,)
Tue Jan 31 16:25:00 EST 2023
