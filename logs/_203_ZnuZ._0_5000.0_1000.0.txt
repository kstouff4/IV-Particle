Tue Jan 31 07:17:28 EST 2023
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  0% |  0% |
numbers of X: 9632026
numbers of Z: 13246
shape of features
(13246,)
shape of features
(13246,)
ZX	Vol	Parts	Cubes	Eps
Z	0.012001906859754151	13246	13.246	0.09676578715705995
X	0.00794463150657826	165	0.165	0.3638004830252055
X	0.009276297962252199	4898	4.898	0.12372343520392871
X	0.01184280295908659	5468	5.468	0.1293824386247739
X	0.00909460633326528	2203	2.203	0.1604192454661956
X	0.00924227244415104	2302	2.302	0.15893680645379393
X	0.00933899729602044	7256	7.256	0.10877631074383502
X	0.009807903376718122	16699	16.699	0.08374558191675707
X	0.00945273815763581	11723	11.723	0.09307640936053029
X	0.008977958121108276	5428	5.428	0.11826218864032874
X	0.00935253501971375	5588	5.588	0.11872922925453122
X	0.009244644535044996	4648	4.648	0.12575966325896074
X	0.010057145122020335	11012	11.012	0.09702183956651572
X	0.009132445843403409	1253	1.253	0.19388550618572545
X	0.00937638055911785	62987	62.987	0.05299823834584343
X	0.009333746773936199	6023	6.023	0.1157215877399135
X	0.009249174339308441	5516	5.516	0.11880267953145483
X	0.009527596380392918	9423	9.423	0.10036864317400544
X	0.00936427311963008	14079	14.079	0.08729058840724617
X	0.009829118044953988	30642	30.642	0.06845404996961964
X	0.00936121214273151	32269	32.269	0.06619860841602512
X	0.008971269309703008	2472	2.472	0.15367450015482942
X	0.010048452617266128	42408	42.408	0.06188002292414477
X	0.009253802319813575	2731	2.731	0.1501986995813257
X	0.009480737803450224	6645	6.645	0.11257683903432462
X	0.00876574190007971	591	0.591	0.24569730871747086
X	0.009364316432753875	11964	11.964	0.092158039098395
X	0.009926661579977241	13725	13.725	0.08976291639185682
X	0.009244243472963468	1545	1.545	0.18154360861962238
X	0.011650627338164951	63524	63.524	0.056815978588801976
X	0.009505335152293022	195158	195.158	0.036519685197006026
X	0.0092483342501271	1960	1.96	0.16772680800783651
X	0.010612593321575787	374711	374.711	0.03048187608621813
X	0.00933291297736714	2221	2.221	0.1613700323033625
X	0.00902738666777536	2614	2.614	0.1511537132502638
X	0.009017073660578528	1843	1.843	0.16976445389549236
X	0.00938446484862784	94223	94.223	0.04635367748092426
X	0.00983325236584958	13911	13.911	0.0890798775858049
X	0.0075369245688727445	630	0.63	0.2287096765913277
X	0.008649841689566107	493	0.493	0.25984861912026813
X	0.00833954310559626	1306	1.306	0.18552382667618925
X	0.00960282319701755	1373	1.373	0.19123885372153687
X	0.008540748075098735	546	0.546	0.2500927428811193
X	0.008795830556225053	184	0.184	0.36292733289801743
X	0.008273957393717439	524	0.524	0.2508768941037049
X	0.008381737928679195	100	0.1	0.43763430636709694
X	0.008505691102379932	247	0.247	0.32534006650095193
X	0.008454041913454762	381	0.381	0.2810044130250083
X	0.008797776795903336	2976	2.976	0.143520301334856
X	0.008121597992405919	293	0.293	0.30263881152102384
X	0.008535921570990182	1310	1.31	0.18677828073757344
X	0.009225874464858256	4014	4.014	0.13197045810722244
X	0.008350853876746753	226	0.226	0.3330740632232199
X	0.009290314396213827	1178	1.178	0.1990497823112905
X	0.00930355816011593	1009	1.009	0.20969398363382494
X	0.009112475102504259	1539	1.539	0.18091139048585073
X	0.008691591071315898	595	0.595	0.24445228798579127
X	0.010691715505673258	8335	8.335	0.10865438634075945
X	0.008295429888003726	406	0.406	0.273382653477972
X	0.009006688359254215	648	0.648	0.2404344585987537
X	0.008517205960986238	450	0.45	0.26649856726169635
X	0.00914134207368542	572	0.572	0.25188635623733663
X	0.009465140356374814	179	0.179	0.3753398497459197
X	0.009174135659687997	712	0.712	0.23443820530491435
X	0.009118624106462827	1208	1.208	0.19616467045477423
X	0.009212108196592402	677	0.677	0.23873904584584765
X	0.008814170795402991	816	0.816	0.2210533500482956
X	0.00894903089036167	446	0.446	0.27173591945193826
X	0.008482209191288784	375	0.375	0.2828085729879952
X	0.009185358204104252	382	0.382	0.28863184453789376
X	0.008723867278002218	394	0.394	0.28080463754232415
X	0.008389639772829359	699	0.699	0.2289577662424195
X	0.008393014874345999	454	0.454	0.26441578623563283
X	0.008879586203727587	316	0.316	0.30401980891851216
X	0.008630693756770835	351	0.351	0.2907900419505624
X	0.008607576666708479	1552	1.552	0.17700942618765836
X	0.00904217771805638	2620	2.62	0.1511206857850938
X	0.00885992251650732	145	0.145	0.3938709711463687
X	0.008802970287795782	3383	3.383	0.13754424151142663
X	0.0089657208900896	513	0.513	0.2595111067851536
X	0.010414809760882094	2668	2.668	0.1574545344121074
X	0.009274536077171228	1255	1.255	0.1947823000011448
X	0.008376815128842415	244	0.244	0.3250097339709783
X	0.00921344605385712	562	0.562	0.25403599694582135
X	0.008410125570906781	250	0.25	0.32281533496882314
X	0.008448530274312901	300	0.3	0.30424256228329444
X	0.0075950192330914235	5551	5.551	0.11101607880031661
X	0.00884777389634423	392	0.392	0.28260682738266407
X	0.0086593455970527	212	0.212	0.3443893694742776
X	0.00882749029613118	328	0.328	0.29967772863575326
X	0.009355187789118428	8624	8.624	0.10274986357741864
X	0.008488440302578682	234	0.234	0.33103253547422373
X	0.008455832888220155	207	0.207	0.3443991059253896
X	0.008552345124400071	622	0.622	0.23956954672919048
X	0.008742411300905964	3586	3.586	0.1345881994765671
X	0.008920087694375496	758	0.758	0.22745734880134233
X	0.009057998443384374	898	0.898	0.21606543766930597
X	0.011660102109412793	3736	3.736	0.14613867386501636
X	0.009003577889606975	963	0.963	0.21066688789055507
X	0.00882653655998716	610	0.61	0.24368015550426583
X	0.008909254788245941	1789	1.789	0.17076951547168226
X	0.009037389471237299	2101	2.101	0.16263186741919397
X	0.00876974087782056	678	0.678	0.2347392372925599
X	0.008589964314275403	438	0.438	0.26967400377200956
X	0.007265852282907106	356	0.356	0.2732827308078312
X	0.008800613999320714	1995	1.995	0.16400484987151495
X	0.009054707352357647	366	0.366	0.2913831570009361
X	0.00907093989710547	526	0.526	0.25835815932054557
X	0.007505836703241797	941	0.941	0.19980353274146176
X	0.008768372672114019	1836	1.836	0.16840259300422575
X	0.008755166465381338	148	0.148	0.3896436151981403
X	0.010295670390128568	5437	5.437	0.12371767955997882
X	0.00827174384882768	272	0.272	0.31213518335727336
X	0.009981916686247618	647	0.647	0.24894489224523475
X	0.008952398412408889	424	0.424	0.2763913931725955
X	0.008581203507321113	2634	2.634	0.14824412020490832
X	0.008852195120344273	1763	1.763	0.17123776676979946
X	0.007607689213594822	2756	2.756	0.1402785225534546
X	0.008127324247671722	90	0.09	0.44864382272184167
X	0.00896220721119639	2958	2.958	0.1447012458698869
X	0.008117830146851966	194	0.194	0.34717310747273267
X	0.009202563260239603	739	0.739	0.23178649292645837
X	0.008437997141320857	771	0.771	0.22202148269782898
X	0.008263118564909374	426	0.426	0.268685849763829
X	0.011703279182386917	7868	7.868	0.11415135508435895
X	0.008113305682031549	477	0.477	0.2571736606156324
X	0.009673435105056827	574	0.574	0.25638322729423824
X	0.008867250344267268	640	0.64	0.2401797187613022
X	0.009061170058144339	206	0.206	0.35299808845885083
X	0.009567440509512342	1499	1.499	0.18549464611588254
X	0.008911431591966149	318	0.318	0.30374335516450285
X	0.009065470607597702	1154	1.154	0.19879014406258683
X	0.00926196862350284	808	0.808	0.22547443891847724
X	0.008780252674082969	334	0.334	0.2973401223075031
X	0.008258473690449453	190	0.19	0.3515998891959326
X	0.008561583535321255	217	0.217	0.3404328455093806
X	0.009327106674654681	14048	14.048	0.08723902739396158
X	0.008664252505301997	179	0.179	0.3644399876135699
X	0.007397239471819055	392	0.392	0.2662325818156657
X	0.008894839384650638	653	0.653	0.23882235489592502
X	0.009055146492204041	378	0.378	0.2882711807280111
X	0.00905507169186158	336	0.336	0.2998132842984146
X	0.00805095514236444	383	0.383	0.27598334723571655
X	0.008838408473821497	1881	1.881	0.1674923944104505
X	0.009172817596751285	1595	1.595	0.17916260018242114
X	0.008598741116268285	352	0.352	0.29015545295178025
X	0.007468192678197162	474	0.474	0.25069502997304977
X	0.009014950319427107	790	0.79	0.22513624353040954
X	0.008851127379894588	337	0.337	0.29725070729203873
X	0.008621066416521838	256	0.256	0.3229289708555203
X	0.008766229517337562	536	0.536	0.2538337402457634
X	0.008677907484000118	806	0.806	0.22081402695457497
X	0.009281428612280509	1718	1.718	0.1754677159962553
X	0.009011391482826253	608	0.608	0.24563830540903098
X	0.009277827489762967	1066	1.066	0.20569782335775758
X	0.009264621421227519	260	0.26	0.3290673368895854
X	0.009178817304172972	4836	4.836	0.12381316092114003
X	0.00907044926114845	318	0.318	0.3055394032713153
X	0.00877485828289248	871	0.871	0.2159769076783701
X	0.008680030308961795	315	0.315	0.30204401129877234
X	0.008147348585697264	122	0.122	0.405712905385689
X	0.011020892962266219	1932	1.932	0.17867750827848172
X	0.0077756119661283206	127	0.127	0.39413374459172335
X	0.008367799388259222	1327	1.327	0.18474815598780336
X	0.008674873041846203	306	0.306	0.30491624792291994
X	0.00855370107367431	684	0.684	0.23211292391080152
X	0.011233023849652801	2890	2.89	0.15722910637158177
X	0.008856377346061166	907	0.907	0.21373859082573934
X	0.008722377749991992	282	0.282	0.3139028515817345
X	0.009960976799734973	930	0.93	0.22043116367836563
X	0.008550411507820321	73	0.073	0.4892769075866468
X	0.008586082304123095	159	0.159	0.3779775227062425
X	0.008615476032201428	348	0.348	0.2914517572889905
X	0.00919647127666025	671	0.671	0.23931296792465265
X	0.00831781251196045	187	0.187	0.3543147020432669
X	0.008891955069117302	2254	2.254	0.15800874219220334
X	0.009233000287135652	424	0.424	0.2792494528339774
X	0.008668630710124189	533	0.533	0.2533617777630249
X	0.008668405888239186	987	0.987	0.20631969887107868
X	0.009036753147623202	580	0.58	0.2497630227726182
X	0.008471751837101645	247	0.247	0.3249067675248386
X	0.010451649916524237	7167	7.167	0.11340072121753467
X	0.008669221493955841	234	0.234	0.33336608431136333
X	0.009150630028840818	626	0.626	0.2445075202534581
X	0.00847358795911098	207	0.207	0.34463998721015726
X	0.00893509826010261	583	0.583	0.24839542502603107
X	0.009212890413588777	1571	1.571	0.18033218301790518
X	0.008708210179601395	255	0.255	0.32443640021142395
X	0.008612233735957599	644	0.644	0.23736133734990988
X	0.009363104496638616	12318	12.318	0.09126268115748841
X	0.008798040014934562	1230	1.23	0.19267569778593352
X	0.008518865636460269	224	0.224	0.33628790682115695
X	0.008370934025244631	439	0.439	0.26715899410876903
X	0.008684917584782218	545	0.545	0.2516458270811524
X	0.008392242251996565	694	0.694	0.22953003017532755
X	0.008437580035814895	481	0.481	0.25983098698235413
X	0.008675419461805509	947	0.947	0.20924102289755403
X	0.008469215802663386	1208	1.208	0.191392723141659
X	0.008758807255682477	294	0.294	0.31000298787923497
X	0.008439114121462041	759	0.759	0.22319529121535678
X	0.00869597684763338	229	0.229	0.3361199555886711
X	0.011789604367320494	7034	7.034	0.11878609964836881
X	0.00847050529094663	224	0.224	0.33565034654402515
X	0.009035207316449023	850	0.85	0.2198735889931162
X	0.010522931567505781	604	0.604	0.25923876022526654
X	0.008722402059572669	2045	2.045	0.16217388838644592
X	0.008821675595230763	133	0.133	0.404793143226951
X	0.008768845758301018	1717	1.717	0.17220960818322983
X	0.009118182392641703	1524	1.524	0.18154087990082302
X	0.008796961027905854	372	0.372	0.28703173051254316
X	0.008731791434394775	751	0.751	0.22654499519128243
X	0.008807260360746862	921	0.921	0.21225620822186225
X	0.008590392463813051	406	0.406	0.2765852339241354
X	0.00900260330623992	283	0.283	0.3168549961041198
X	0.008665672424368168	409	0.409	0.2767109338350287
X	0.009132457865910527	216	0.216	0.34837312318004793
X	0.007913889791367151	493	0.493	0.25225959428313444
X	0.009214364605289746	103	0.103	0.44724214607189167
X	0.00826853282149797	144	0.144	0.38579386833873297
X	0.008885078119489615	3113	3.113	0.14184935867856316
X	0.008436283468413545	185	0.185	0.35726719911018706
X	0.008373276781561452	223	0.223	0.334860196732068
X	0.009089159080259328	212	0.212	0.3499956241303678
X	0.008793544952844288	722	0.722	0.23007826663676334
X	0.008597673493704954	322	0.322	0.29888791733185577
X	0.00800860054896326	908	0.908	0.20661265953538838
X	0.00878429338291601	399	0.399	0.28027088109405235
X	0.007420337234530893	549	0.549	0.23820475850679684
X	0.008550666139511892	372	0.372	0.28432758958982574
X	0.00880781228296512	392	0.392	0.2821807137928577
X	0.008335380905033728	376	0.376	0.2809177633525909
X	0.008194205834395313	501	0.501	0.25383785023662125
X	0.00851816890692984	1115	1.115	0.19695051768491634
X	0.009111645073950583	1122	1.122	0.20100243215322858
X	0.011505547364880669	4735	4.735	0.13444018469254174
X	0.0084597873724816	261	0.261	0.3188401318903645
X	0.009015001689269274	269	0.269	0.3224070358642755
X	0.009072135702359487	512	0.512	0.2607032982525766
X	0.009355030960421952	1132	1.132	0.20217755216976052
X	0.008150546927531022	146	0.146	0.3821888083914971
X	0.00912888324308344	2935	2.935	0.1459720976568041
X	0.00926642725868517	8066	8.066	0.10473327909383644
X	0.00932888122292096	799	0.799	0.2268615672796951
X	0.008804084975122028	1177	1.177	0.19557015843567246
X	0.008901841321946907	390	0.39	0.283664560312628
X	0.008900726251046457	1929	1.929	0.16648089635485122
X	0.00925821946314222	2414	2.414	0.1565296936272576
X	0.009291327246473715	31276	31.276	0.06672504120820959
X	0.009284209843735682	414	0.414	0.2819994116727059
X	0.009329866022999548	18302	18.302	0.07988384348610596
X	0.009343237014517187	9559	9.559	0.09924187664744413
X	0.010949095400102605	4197	4.197	0.13766168789123986
X	0.009368099658086082	3072	3.072	0.1450140550378984
X	0.0090289546578126	219	0.219	0.3454600135659713
X	0.00834279315454734	238	0.238	0.32727400137577944
X	0.009292753391685879	101615	101.615	0.04505353408563778
X	0.011207874368480508	44987	44.987	0.0629233844346537
X	0.009784762546256045	840	0.84	0.22668546189327282
X	0.009512165628915359	6667	6.667	0.11257699438248552
X	0.00998189889431699	18514	18.514	0.08138998511915252
X	0.009119874555544591	4481	4.481	0.12672762822781064
X	0.009281969801702785	8337	8.337	0.1036438291832675
X	0.009075909610699909	2823	2.823	0.14759096124813403
X	0.00936485806891749	4492	4.492	0.12774791403567765
X	0.009339077179555018	4586	4.586	0.12675253039586565
X	0.008594397665441742	286	0.286	0.3108969564764934
X	0.011094636684141844	38244	38.244	0.06619877076240874
X	0.009292713176355683	564	0.564	0.2544609492156188
X	0.008945983143228016	894	0.894	0.2154915205198261
X	0.009077676506197922	2454	2.454	0.1546559790269494
X	0.01128480143059594	9608	9.608	0.105508385075184
X	0.009848696876726007	47450	47.45	0.05920806273832529
X	0.009341479514844835	12358	12.358	0.09109387113554694
X	0.008124449525137432	188	0.188	0.35092316034175675
X	0.009172453375865102	5215	5.215	0.12071013437332859
X	0.008846356154982422	1246	1.246	0.1921979683493818
X	0.009348704299992239	4792	4.792	0.1249524774275834
X	0.00937207386947522	15590	15.59	0.08439755447893048
X	0.008978794621151406	6470	6.47	0.11154186944842433
X	0.009141467911891124	8159	8.159	0.10386271444743178
X	0.0084496649329032	80	0.08	0.4726973491920182
X	0.00902535551310246	733	0.733	0.2309156926092209
X	0.011151120800205425	10252	10.252	0.10284187223026271
X	0.009196616337541815	7058	7.058	0.10922334471512994
X	0.0097644437405182	8541	8.541	0.1045633756396173
X	0.008903278830070145	689	0.689	0.23466244115814597
X	0.00935450275185631	52870	52.87	0.056139748868257136
X	0.00934209631029082	6591	6.591	0.11233049591358522
X	0.009285148434527251	4691	4.691	0.12555706041783532
X	0.009372972860744507	32671	32.671	0.06595357033279449
X	0.008932066364236555	499	0.499	0.2615875870551342
X	0.00921370356606655	7588	7.588	0.10668474031818015
X	0.01065613463720233	31522	31.522	0.0696618993519586
X	0.009341348388905771	98399	98.399	0.04561835580776191
X	0.009828923806775556	5003	5.003	0.12524444959567815
X	0.009095023860454002	5620	5.62	0.1174056982377871
X	0.008712975560219816	966	0.966	0.20815960565566757
X	0.009233659683097856	1183	1.183	0.19836408339805098
X	0.009375067982809826	21881	21.881	0.07538803436393172
X	0.009318268586327911	1595	1.595	0.18010461677991163
X	0.0093160795714977	4109	4.109	0.13137085184895494
X	0.009982189079710636	12346	12.346	0.0931607922322496
X	0.009369550821025125	5158	5.158	0.12201479028202593
X	0.009150564590567603	3821	3.821	0.13378998890943028
X	0.011679621463208974	6640	6.64	0.12071286642334536
X	0.009841430321324496	96527	96.527	0.04671644690480105
X	0.009335171375106852	1334	1.334	0.19127380111007442
X	0.009208642753604158	2409	2.409	0.15635782058423545
X	0.009128919936346623	2064	2.064	0.1641483676891421
X	0.009318694377764686	22706	22.706	0.07431414377427858
X	0.009341953007736275	1750	1.75	0.1747698619636105
X	0.009383330671882859	10634	10.634	0.09591505682642018
X	0.009151059342771082	1475	1.475	0.1837496203512032
X	0.008859528715045233	6934	6.934	0.10851145684964929
X	0.009360949047631998	2618	2.618	0.1529149967086415
X	0.009535003778480124	64531	64.531	0.052866938105933875
X	0.00935479437407075	10580	10.58	0.09598046714873792
X	0.009224606372204004	1321	1.321	0.19113857411661853
X	0.00936806811023432	28509	28.509	0.06900639691999139
X	0.009376027959598613	24808	24.808	0.0723006797085399
X	0.009490810056358318	747	0.747	0.23334270423549872
X	0.009345873414556057	13881	13.881	0.0876461909631103
X	0.00935857673893609	30504	30.504	0.06744519342726879
X	0.009952330876264452	67421	67.421	0.05284978263866548
X	0.011668109064002303	3120	3.12	0.15522033936828203
X	0.008646834574655836	1016	1.016	0.2041680656485937
X	0.00928393759997355	1661	1.661	0.17746832900001508
X	0.009346539801293186	1955	1.955	0.16846177254226338
X	0.009163593057856339	2927	2.927	0.14628990482780935
X	0.00932533603227262	2395	2.395	0.15732086447177043
X	0.008819170393513799	189	0.189	0.36001614437464613
X	0.009369066587763243	12814	12.814	0.09008874466299176
X	0.011534903057314768	24422	24.422	0.07787730379982684
X	0.009092127922671188	3316	3.316	0.13996421257432112
X	0.009197515650756808	1067	1.067	0.20503847923107968
X	0.009144228946998102	7319	7.319	0.10770398723498092
X	0.009266948707611529	1595	1.595	0.17977336899273624
X	0.00940387911762816	2247	2.247	0.16115161237908704
X	0.009799143823734924	2450	2.45	0.1587354822897286
X	0.009304249244701456	476	0.476	0.26937552114971125
X	0.00914324702111476	17627	17.627	0.08034780487616756
X	0.009344115645131507	6356	6.356	0.11370636394567768
X	0.009331477683771884	3393	3.393	0.14010560851500692
X	0.009260700614582479	17693	17.693	0.08058993192678993
X	0.00920722627065654	862	0.862	0.22022866701435578
X	0.009353757718744459	58542	58.542	0.054263298177375005
X	0.00912255472294076	1405	1.405	0.18655778826226982
X	0.009228293749185438	11698	11.698	0.0923995659422722
X	0.008760481091753921	1099	1.099	0.19976071695447753
X	0.008878603994545155	720	0.72	0.23103125103874209
X	0.008477455186390948	463	0.463	0.26356930853647303
X	0.009349580441266452	2263	2.263	0.16046088734830266
X	0.009259125838999639	636	0.636	0.24417673950659505
X	0.009082847615004599	6407	6.407	0.11233689988673265
X	0.008871843536026417	778	0.778	0.2250840697278148
X	0.00885476194879362	933	0.933	0.21172155845901408
X	0.00936793060845308	39639	39.639	0.061826357168729494
X	0.009359361459262687	10797	10.797	0.09534861146169173
X	0.009341627083455813	8405	8.405	0.10358453866852675
X	0.009360725894611534	27188	27.188	0.07008806804878158
X	0.009397983152919981	81974	81.974	0.04857947811566929
X	0.009759092358335308	1172	1.172	0.2026876428700937
X	0.009121227362684172	830	0.83	0.22232675523105622
X	0.009125501518265269	2992	2.992	0.14502127448417726
X	0.008156526311083179	498	0.498	0.25395609058551866
X	0.008788102285657578	979	0.979	0.2078280377213176
X	0.009415784709469072	2664	2.664	0.1523260217106234
X	0.008720369208035999	1159	1.159	0.1959523512224278
X	0.009653017882187684	649	0.649	0.24592695809634976
X	0.008835613635073873	564	0.564	0.25021837784211076
X	0.009293899625583022	1870	1.87	0.17065491090714174
X	0.009522710788872926	100875	100.875	0.04553293887931064
X	0.008998673115705132	1395	1.395	0.18615216410645102
X	0.009741993907539425	22224	22.224	0.07596412535082084
X	0.0097028409449552	1786	1.786	0.17579471689723636
X	0.008781800502342207	1064	1.064	0.20209115825396798
X	0.009012251517804899	6176	6.176	0.113424969752685
X	0.009415159313870283	28445	28.445	0.06917363187016444
X	0.00984141199202995	76785	76.785	0.050418935813131255
X	0.009335865687647738	2417	2.417	0.15690109754067674
X	0.009370618254235144	7401	7.401	0.10818306473959967
X	0.009775841717618552	728	0.728	0.23768818478404746
X	0.008951257349480116	1751	1.751	0.172265892711865
X	0.009370074173836652	255845	255.845	0.03320896687240676
X	0.010624424280230502	4276	4.276	0.13544279367200165
X	0.009155028614239804	1628	1.628	0.17782866514013093
X	0.009301331872008975	9682	9.682	0.09867186888633464
X	0.009729962901009623	81084	81.084	0.04932404361766247
X	0.009101688441935828	7672	7.672	0.10586143831107618
X	0.010095243138416397	1418	1.418	0.19237421061554572
X	0.008780412177127288	1640	1.64	0.17494049529673583
X	0.009372021768258356	2262	2.262	0.16061282811963845
X	0.009301116342980215	758	0.758	0.23065097266290388
X	0.010541560687488221	26397	26.397	0.07364050876839999
X	0.008908040434376199	2884	2.884	0.1456338423308195
X	0.008943784637251463	653	0.653	0.23925960638135624
X	0.0098086717471296	4650	4.65	0.12824853593954827
X	0.00917697676734778	3607	3.607	0.13651629608715868
X	0.008642742073696605	566	0.566	0.24809139841920297
X	0.009270436936407373	18309	18.309	0.07970370802961613
X	0.00933523138125315	11866	11.866	0.09231527779759671
X	0.01135307312331607	30096	30.096	0.07225505217287
X	0.010457804235615868	22884	22.884	0.07702579502013235
X	0.009256065524019232	38622	38.622	0.06211510451108184
X	0.008928062633387763	4656	4.656	0.12423617893400828
X	0.00924957496020863	8370	8.37	0.10338688238662039
X	0.00954264523560301	14444	14.444	0.08709510884418571
X	0.009377185619110443	17959	17.959	0.08052488639718892
X	0.009206765202520915	614	0.614	0.2465923307382354
X	0.00938623116827459	16880	16.88	0.0822317629744065
X	0.009379261203886224	23642	23.642	0.07347870143861322
X	0.009368713703302929	13659	13.659	0.08819020542044219
X	0.009345167259997015	12568	12.568	0.09059557148820283
X	0.008995012298767632	8241	8.241	0.10296128381659804
X	0.008811898362756882	4584	4.584	0.12433926336822322
X	0.009225500994886811	826	0.826	0.2235304217690926
X	0.00887723401752846	8639	8.639	0.1009108965936506
X	0.009936939175383602	26353	26.353	0.07224495471046215
X	0.00937662721127732	23248	23.248	0.07388455881101763
X	0.009335058807540245	46777	46.777	0.05843793992587221
X	0.009846563503465269	38027	38.027	0.06373786917920428
X	0.009323978609007981	12162	12.162	0.09152338829653152
X	0.009896461154654404	3261	3.261	0.1447803787774588
X	0.011275022568228757	66160	66.16	0.05544222373574086
X	0.00886151274458918	3572	3.572	0.13537302659417963
X	0.009233605800435576	1564	1.564	0.18073608073065617
X	0.009476167835535626	54227	54.227	0.05590779814435386
X	0.009211015709299877	2358	2.358	0.15749057698077465
X	0.008777588755587323	430	0.43	0.2732974574274145
X	0.009360538187349296	5136	5.136	0.12214956783204135
X	0.01050678230920844	54441	54.441	0.05778935936985354
X	0.009225442329569886	4461	4.461	0.1274045656810368
X	0.009369303443428952	13289	13.289	0.08900307286379738
X	0.008946587292188158	255	0.255	0.3273701477643841
X	0.00902248410633091	352	0.352	0.2948454888278099
X	0.00874274106374218	1203	1.203	0.19369897811153455
X	0.00917963199857808	2676	2.676	0.15081564206390288
X	0.009780371265928425	5992	5.992	0.11774103399826344
X	0.009018488523486612	11755	11.755	0.09154550845923186
X	0.008808484569598748	420	0.42	0.27577229656551244
X	0.009368337732609153	17176	17.176	0.08170466163993441
X	0.00889084674080043	910	0.91	0.21378008425231942
X	0.010351139942279153	11748	11.748	0.09586823844834105
X	0.008828872012833349	1383	1.383	0.18550730394219983
X	0.009494247553745916	15418	15.418	0.08507673932830835
X	0.008813771368279896	1385	1.385	0.18531219591617934
X	0.00902870415425526	3656	3.656	0.1351677481092732
X	0.00932928697928818	3018	3.018	0.1456721082876976
X	0.009304797970635478	4016	4.016	0.13232373410953382
X	0.0093386966578315	11676	11.676	0.09282480825955212
X	0.01072140352567403	27074	27.074	0.0734344318492676
X	0.00903773199247912	980	0.98	0.20970613092757348
X	0.009163833703018	1724	1.724	0.17452057932679937
X	0.00932946585881679	23627	23.627	0.07336395449423554
X	0.009273249237262398	3087	3.087	0.14428851630777537
X	0.010676959981972368	133856	133.856	0.043046375589589556
X	0.007879435852546197	241	0.241	0.31976139300944356
X	0.009236758494699763	18303	18.303	0.07961577059848005
X	0.009911137710364129	49487	49.487	0.05850739940085808
X	0.0085637810329575	779	0.779	0.22235284923279305
X	0.009039543328332497	17676	17.676	0.07996884668137846
X	0.009339482255956794	4716	4.716	0.12557880180317055
X	0.009813415133124441	73644	73.644	0.05107726102780436
X	0.009222002158865759	2229	2.229	0.16053573361705142
X	0.008638029140020672	2451	2.451	0.15217983814014258
X	0.00890306934378477	765	0.765	0.2266171500927576
X	0.009011946592792051	433	0.433	0.27507021758724376
X	0.0092645352004806	4933	4.933	0.12337793774532203
X	0.00932994606895931	4335	4.335	0.12911102942499825
X	0.00909132538012064	3449	3.449	0.13813741850900366
X	0.009283875590447565	20646	20.646	0.07661217576080405
X	0.009340989250005717	4914	4.914	0.12387563869970593
X	0.00895010453259482	246	0.246	0.331358157633616
X	0.009172731576785689	2879	2.879	0.14714731434474654
X	0.009354626049184633	29381	29.381	0.06828415707719303
X	0.009186444171694619	8485	8.485	0.10268299587360646
X	0.0094838823211585	44004	44.004	0.05995581812302665
X	0.009121974930441548	2112	2.112	0.1628539587680532
X	0.00934217275376574	1205	1.205	0.19791875437189044
X	0.008797577509204997	259	0.259	0.32385786305493053
X	0.008897975073534459	1501	1.501	0.18098260326930424
X	0.011626026459655983	38955	38.955	0.06682762590180337
X	0.009223383889971185	2716	2.716	0.15030964177826706
X	0.00901680558561436	5086	5.086	0.1210297445513592
X	0.009133991652409326	9803	9.803	0.09767135271170999
X	0.009191429779150335	2741	2.741	0.14967801420353397
X	0.009092617212777631	784	0.784	0.2263554463404462
X	0.008610195519627219	2122	2.122	0.15949823369161747
X	0.00978753452896046	30912	30.912	0.06815777442023428
X	0.009354254394872157	3712	3.712	0.13608191212269716
X	0.00892103340095307	1603	1.603	0.17721230766617815
X	0.009343065970202822	1826	1.826	0.172317564347311
X	0.009360163566726254	41458	41.458	0.06089173709347816
X	0.009342843857498336	24204	24.204	0.07281105360256482
X	0.00950650964339521	30836	30.836	0.06755451059278939
X	0.009787631622707382	18983	18.983	0.08018705357862417
X	0.009764088615684377	933	0.933	0.2187341987001227
X	0.009374303950288794	12263	12.263	0.09143534352200806
X	0.00913456098127989	1815	1.815	0.17137087598461945
X	0.00945447605984224	5203	5.203	0.12202848332770207
X	0.0092524889384449	2149	2.149	0.1626824557785311
X	0.009274410100206812	754	0.754	0.2308367670949848
X	0.009321267634732127	3583	3.583	0.13753378331990723
X	0.008803833476393874	4259	4.259	0.12738590976826292
X	0.008181592254990103	436	0.436	0.26573603306107746
X	0.009084794961600539	3591	3.591	0.1362594322231812
X	0.009357402864820473	1371	1.371	0.18968775253129705
X	0.009358694713933073	23932	23.932	0.07312716923425157
X	0.008789230342684296	525	0.525	0.25581765543282936
X	0.009273464906389188	2422	2.422	0.15644293845167986
X	0.00973513580338544	359	0.359	0.3004340750556935
X	0.009258712631549145	2925	2.925	0.14682778418945233
X	0.009160078976701973	1718	1.718	0.17469964381861067
X	0.009085911997248059	2247	2.247	0.15931444363017025
X	0.009363360238097695	4244	4.244	0.1301823536167075
X	0.01160347062947757	11241	11.241	0.10106349658331219
X	0.009197773281104792	681	0.681	0.2381470445096529
X	0.008382342188819999	218	0.218	0.3375226825772589
X	0.008788195755591933	404	0.404	0.27915116994983136
X	0.009355704155132217	28919	28.919	0.06864850276274136
X	0.010750800523315132	239279	239.279	0.0355504748705216
X	0.009305090391897974	4441	4.441	0.12796168229237262
X	0.009129431610349664	1101	1.101	0.20240386759657014
X	0.009342413842101405	12861	12.861	0.08989346503004747
X	0.009346045482697298	17274	17.274	0.08148512291288872
X	0.008435382156298242	1521	1.521	0.17700757387165522
X	0.010595040805858678	9232	9.232	0.10469733150001323
X	0.009338630714281216	5139	5.139	0.12203044494454396
X	0.009367460570939174	91219	91.219	0.0468287094174153
X	0.009362816266529584	63644	63.644	0.052789758835094305
X	0.01046603704611026	59608	59.608	0.055996288071819646
X	0.009293120859009742	2085	2.085	0.16457044443635138
X	0.00936839663909566	1366	1.366	0.1899932580023785
X	0.009100714409396918	506	0.506	0.262004242106395
X	0.009358068120562321	9024	9.024	0.10121907679891207
X	0.009137668689991567	1758	1.758	0.17322298265368907
X	0.009023017463147548	1994	1.994	0.16540255528819992
X	0.01050348292295558	33398	33.398	0.06800449571008786
X	0.010360696192776497	99821	99.821	0.04699543504055917
X	0.00897365615625219	2410	2.41	0.15499491871691948
X	0.008549418173156093	212	0.212	0.34292585730109953
X	0.009298579804528665	1789	1.789	0.17322161927023624
X	0.009499552091484042	9786	9.786	0.0990146155882422
X	0.00935685258468541	5326	5.326	0.12066358056443549
X	0.008767183402402862	156	0.156	0.38304091350124003
X	0.008944190464617603	329	0.329	0.30068727340469387
X	0.009334819783873597	3048	3.048	0.14522130164501057
X	0.008652933499789678	506	0.506	0.2576346565157401
X	0.008821757807299777	14252	14.252	0.08522357971420166
X	0.00927549448130297	1027	1.027	0.20825185373156288
X	0.009359563279059	2830	2.83	0.1489897471488964
X	0.009167240601771636	2364	2.364	0.15710754872022958
X	0.008860125107261304	643	0.643	0.23974135767924332
X	0.008777473516841449	722	0.722	0.22993801439761413
X	0.009520584411770772	41896	41.896	0.06102348633148713
X	0.009377745076205205	64018	64.018	0.05271474461728935
X	0.00934838288511523	16753	16.753	0.08232807630520197
X	0.009360685961752909	3917	3.917	0.13369588322972023
X	0.009342420707415972	4080	4.08	0.13180536429826184
X	0.009124510670154904	1103	1.103	0.2022451071408431
X	0.009882324683414835	4093	4.093	0.13415466875906365
X	0.009498764236973652	17326	17.326	0.08184451876738662
X	0.009256842704256066	3403	3.403	0.13959407277102515
X	0.009246115293094803	3977	3.977	0.1324751961338476
X	0.008979911601980788	6446	6.446	0.11168476124016298
X	0.01195398717579367	196202	196.202	0.03934924934321302
X	0.009444422385233412	3381	3.381	0.1408348310249945
X	0.009342808691487698	12696	12.696	0.09028248701834712
X	0.009365279315955695	75611	75.611	0.04984771450402612
X	0.010767696340528722	11070	11.07	0.09908130686428093
X	0.009362239891808875	3632	3.632	0.13711279221236936
X	0.010278743468494977	52086	52.086	0.05822010802224908
X	0.009340200650969086	15287	15.287	0.08485510597091676
X	0.009239894726660888	5514	5.514	0.11877729224159761
X	0.009328616891647197	1283	1.283	0.19372999494879609
X	0.00935941810123056	4481	4.481	0.1278275999505995
X	0.009329980646736076	2472	2.472	0.15569598934428117
X	0.009337713303215388	13635	13.635	0.08814448224864625
X	0.01194563735502841	130280	130.28	0.045093144943069915
X	0.011586089702939003	22038	22.038	0.08070892621145415
X	0.009858801825280246	26200	26.2	0.07219508219222451
X	0.009368521453115728	3534	3.534	0.13839960060077666
X	0.009357262209191388	3386	3.386	0.14033110616257793
X	0.009355311774190186	5073	5.073	0.12263030290627965
X	0.008935836647027155	1159	1.159	0.19755313608574154
X	0.010485436456022672	123789	123.789	0.0439172178035708
X	0.011107433347816569	44980	44.98	0.06273810859127336
X	0.011279085110455531	5645	5.645	0.1259514898931185
X	0.009386031478449882	329387	329.387	0.030543948857957148
X	0.01038232375064272	50084	50.084	0.05918321044429921
X	0.0092145456143568	6092	6.092	0.11479016404486218
X	0.009249591210624257	503	0.503	0.26394788402159247
X	0.009330471195053088	4591	4.591	0.12666756673555732
X	0.009990030094407137	15031	15.031	0.08726892787708321
X	0.008781884105465246	2063	2.063	0.16206758393736231
X	0.009909280041629795	153009	153.009	0.04015827429013477
X	0.01020164288811604	4026	4.026	0.1363323064475041
X	0.008758380761983126	4603	4.603	0.12391606552576731
X	0.009332865390067141	1038	1.038	0.2079405633131888
X	0.008954995925448631	1476	1.476	0.18238663539563751
X	0.009260472398881892	265	0.265	0.3269357574193352
X	0.009245109331657377	2951	2.951	0.14632356735219862
X	0.011114997411133372	3799	3.799	0.14302546138960484
X	0.00936877934578327	3348	3.348	0.14091779979032426
X	0.011856232478442471	523189	523.189	0.028298453404120374
X	0.008611395931551746	3847	3.847	0.13081286487902807
X	0.009495212595928303	14436	14.436	0.08696662304062448
X	0.008986452780494383	2701	2.701	0.14928674106771336
X	0.008889761379909182	4037	4.037	0.1300999465418614
X	0.008304739908852986	1581	1.581	0.17383266509476147
X	0.009237769150057338	18076	18.076	0.07995057487195081
X	0.009041358302036857	19465	19.465	0.07744494769030276
X	0.009379789424459398	115400	115.4	0.04331740449988791
X	0.009343127020661241	3226	3.226	0.14254192843833113
X	0.00936753196840896	44982	44.982	0.059273714077095474
X	0.009499299631268665	4216	4.216	0.13109828029744658
X	0.009198685015982184	22826	22.826	0.07386385641383104
X	0.009353421058733723	39430	39.43	0.06190340946371883
X	0.009354132436510357	2820	2.82	0.14913679373059738
X	0.009253357102747784	1116	1.116	0.2024005598036309
X	0.009370767879035099	15141	15.141	0.08521974040378423
X	0.009141376318268288	1060	1.06	0.205069894254618
X	0.00916526611045785	7479	7.479	0.10701233701759061
X	0.009137103432316064	41076	41.076	0.06059081562508089
X	0.009211413260422487	1724	1.724	0.17482210095465647
X	0.01174451684897727	15251	15.251	0.09165980832603478
X	0.00857227784726692	2380	2.38	0.15328738919648083
X	0.009220775339589461	23598	23.598	0.07310786165234981
X	0.009519820553685866	42729	42.729	0.060622709687218224
X	0.011390342398913442	3186	3.186	0.1529080450981843
X	0.009272493699583696	1904	1.904	0.1695026663713251
X	0.009815903613866347	79264	79.264	0.049844598272609
X	0.009148149531927923	15900	15.9	0.08317229976500262
X	0.009304892181762137	105771	105.771	0.04447489297006916
X	0.009329113061849574	8841	8.841	0.10180747241097013
X	0.009356659287290013	12615	12.615	0.09051999476808911
X	0.009217767663983379	8602	8.602	0.10233136637678879
X	0.00911608751519335	11734	11.734	0.0919293382701935
X	0.009224438111487264	51103	51.103	0.056515101046698706
X	0.009262537928391348	6514	6.514	0.11245033365243257
X	0.009226762693982215	1140	1.14	0.2007774033544636
X	0.00901749732413404	1678	1.678	0.17515855558835278
X	0.009310816441044378	4554	4.554	0.12692044306519099
X	0.00933254399934795	15249	15.249	0.08490232055563655
X	0.011644316219630161	9423	9.423	0.10731034717486918
X	0.009257457869309585	2367	2.367	0.15755462708239884
X	0.009324966758811984	6553	6.553	0.11247837709420483
X	0.009305312235650018	6420	6.42	0.11317017151097027
X	0.012005225259090889	389909	389.909	0.0313426479436992
X	0.009409738456351274	10071	10.071	0.09776160405973951
X	0.009342532216571124	16659	16.659	0.08246542318663207
X	0.009048338433929694	4438	4.438	0.12680231411432233
X	0.009839818639911447	3129	3.129	0.1465077201623872
X	0.009111397164519239	589	0.589	0.24916663917906542
X	0.00917426800418961	2584	2.584	0.15255495021361007
X	0.011919416678538523	29038	29.038	0.07431835708690117
X	0.009301800195165941	1855	1.855	0.17116213597937505
X	0.009372932684836428	11330	11.33	0.09387469292266615
X	0.009011457955560062	217	0.217	0.3462941132937807
X	0.008996003179074199	1331	1.331	0.18907053289179318
X	0.00921580994114689	6511	6.511	0.11227815601315512
X	0.009336012623802377	12126	12.126	0.09165326813244587
X	0.009285610735728617	3850	3.85	0.13410652671834233
X	0.009118784291771582	1022	1.022	0.20740951798804355
X	0.008933172198255672	977	0.977	0.20910787114451335
X	0.009177797786745275	4903	4.903	0.12324203583694146
X	0.00938717589341037	4861	4.861	0.12452879284773968
X	0.009340207896780599	24455	24.455	0.07255426695375969
X	0.009346530463286925	982	0.982	0.21192375770629154
X	0.009037459740695034	3284	3.284	0.14013536285939743
X	0.009000479787195521	1753	1.753	0.17251541806133402
X	0.008547483259906221	2140	2.14	0.15866269562245072
X	0.010517163287155185	27474	27.474	0.0726092985017792
X	0.009216018796329675	777	0.777	0.22805561926603313
X	0.00939994801940991	10944	10.944	0.09505678521822912
X	0.009377818078279379	62224	62.224	0.05321670233561333
X	0.009373262662540327	6405	6.405	0.11353346286062728
X	0.00981933018631715	19785	19.785	0.07917388144356242
X	0.0095139039546669	16134	16.134	0.08385691929240528
X	0.008900614456011811	370	0.37	0.2886729486972348
X	0.009338658314421048	6080	6.08	0.11537905265726785
X	0.009981406347871098	2919	2.919	0.15065580849198637
X	0.011540821984314545	45608	45.608	0.06325068569904897
X	0.009995324345572425	19118	19.118	0.08055975647555118
X	0.00897741546709979	2857	2.857	0.14646944232698877
X	0.009352589247439794	17200	17.2	0.0816208546756739
X	0.009172497544796647	3395	3.395	0.13927802781617568
X	0.008986697122688403	2105	2.105	0.16222433464643263
X	0.00930284865266216	663	0.663	0.24119454188233153
X	0.011541927003232628	9410	9.41	0.10704414063879256
X	0.009816474306713596	25405	25.405	0.07283589228791788
X	0.011972322316887185	151591	151.591	0.04290438299740805
X	0.00895263235390185	8412	8.412	0.10209798291221048
X	0.009351115404057154	3081	3.081	0.1447851119097025
X	0.009334002209408433	7238	7.238	0.10884699411298179
X	0.011754018556694232	27690	27.69	0.07515441041060428
X	0.009244670820723848	2624	2.624	0.15216305076255365
X	0.009186345094767073	2047	2.047	0.16494594288075148
X	0.009361671521217049	9943	9.943	0.09801186498454453
X	0.009378911509921824	23729	23.729	0.07338787862054173
X	0.011066220360107331	5990	5.99	0.12270365439667624
X	0.009816771279457806	4205	4.205	0.13265823785668215
X	0.009299848549703796	722	0.722	0.23441184843760848
time for making epsilon is 0.5389726161956787
epsilons are
[0.3638004830252055, 0.12372343520392871, 0.1293824386247739, 0.1604192454661956, 0.15893680645379393, 0.10877631074383502, 0.08374558191675707, 0.09307640936053029, 0.11826218864032874, 0.11872922925453122, 0.12575966325896074, 0.09702183956651572, 0.19388550618572545, 0.05299823834584343, 0.1157215877399135, 0.11880267953145483, 0.10036864317400544, 0.08729058840724617, 0.06845404996961964, 0.06619860841602512, 0.15367450015482942, 0.06188002292414477, 0.1501986995813257, 0.11257683903432462, 0.24569730871747086, 0.092158039098395, 0.08976291639185682, 0.18154360861962238, 0.056815978588801976, 0.036519685197006026, 0.16772680800783651, 0.03048187608621813, 0.1613700323033625, 0.1511537132502638, 0.16976445389549236, 0.04635367748092426, 0.0890798775858049, 0.2287096765913277, 0.25984861912026813, 0.18552382667618925, 0.19123885372153687, 0.2500927428811193, 0.36292733289801743, 0.2508768941037049, 0.43763430636709694, 0.32534006650095193, 0.2810044130250083, 0.143520301334856, 0.30263881152102384, 0.18677828073757344, 0.13197045810722244, 0.3330740632232199, 0.1990497823112905, 0.20969398363382494, 0.18091139048585073, 0.24445228798579127, 0.10865438634075945, 0.273382653477972, 0.2404344585987537, 0.26649856726169635, 0.25188635623733663, 0.3753398497459197, 0.23443820530491435, 0.19616467045477423, 0.23873904584584765, 0.2210533500482956, 0.27173591945193826, 0.2828085729879952, 0.28863184453789376, 0.28080463754232415, 0.2289577662424195, 0.26441578623563283, 0.30401980891851216, 0.2907900419505624, 0.17700942618765836, 0.1511206857850938, 0.3938709711463687, 0.13754424151142663, 0.2595111067851536, 0.1574545344121074, 0.1947823000011448, 0.3250097339709783, 0.25403599694582135, 0.32281533496882314, 0.30424256228329444, 0.11101607880031661, 0.28260682738266407, 0.3443893694742776, 0.29967772863575326, 0.10274986357741864, 0.33103253547422373, 0.3443991059253896, 0.23956954672919048, 0.1345881994765671, 0.22745734880134233, 0.21606543766930597, 0.14613867386501636, 0.21066688789055507, 0.24368015550426583, 0.17076951547168226, 0.16263186741919397, 0.2347392372925599, 0.26967400377200956, 0.2732827308078312, 0.16400484987151495, 0.2913831570009361, 0.25835815932054557, 0.19980353274146176, 0.16840259300422575, 0.3896436151981403, 0.12371767955997882, 0.31213518335727336, 0.24894489224523475, 0.2763913931725955, 0.14824412020490832, 0.17123776676979946, 0.1402785225534546, 0.44864382272184167, 0.1447012458698869, 0.34717310747273267, 0.23178649292645837, 0.22202148269782898, 0.268685849763829, 0.11415135508435895, 0.2571736606156324, 0.25638322729423824, 0.2401797187613022, 0.35299808845885083, 0.18549464611588254, 0.30374335516450285, 0.19879014406258683, 0.22547443891847724, 0.2973401223075031, 0.3515998891959326, 0.3404328455093806, 0.08723902739396158, 0.3644399876135699, 0.2662325818156657, 0.23882235489592502, 0.2882711807280111, 0.2998132842984146, 0.27598334723571655, 0.1674923944104505, 0.17916260018242114, 0.29015545295178025, 0.25069502997304977, 0.22513624353040954, 0.29725070729203873, 0.3229289708555203, 0.2538337402457634, 0.22081402695457497, 0.1754677159962553, 0.24563830540903098, 0.20569782335775758, 0.3290673368895854, 0.12381316092114003, 0.3055394032713153, 0.2159769076783701, 0.30204401129877234, 0.405712905385689, 0.17867750827848172, 0.39413374459172335, 0.18474815598780336, 0.30491624792291994, 0.23211292391080152, 0.15722910637158177, 0.21373859082573934, 0.3139028515817345, 0.22043116367836563, 0.4892769075866468, 0.3779775227062425, 0.2914517572889905, 0.23931296792465265, 0.3543147020432669, 0.15800874219220334, 0.2792494528339774, 0.2533617777630249, 0.20631969887107868, 0.2497630227726182, 0.3249067675248386, 0.11340072121753467, 0.33336608431136333, 0.2445075202534581, 0.34463998721015726, 0.24839542502603107, 0.18033218301790518, 0.32443640021142395, 0.23736133734990988, 0.09126268115748841, 0.19267569778593352, 0.33628790682115695, 0.26715899410876903, 0.2516458270811524, 0.22953003017532755, 0.25983098698235413, 0.20924102289755403, 0.191392723141659, 0.31000298787923497, 0.22319529121535678, 0.3361199555886711, 0.11878609964836881, 0.33565034654402515, 0.2198735889931162, 0.25923876022526654, 0.16217388838644592, 0.404793143226951, 0.17220960818322983, 0.18154087990082302, 0.28703173051254316, 0.22654499519128243, 0.21225620822186225, 0.2765852339241354, 0.3168549961041198, 0.2767109338350287, 0.34837312318004793, 0.25225959428313444, 0.44724214607189167, 0.38579386833873297, 0.14184935867856316, 0.35726719911018706, 0.334860196732068, 0.3499956241303678, 0.23007826663676334, 0.29888791733185577, 0.20661265953538838, 0.28027088109405235, 0.23820475850679684, 0.28432758958982574, 0.2821807137928577, 0.2809177633525909, 0.25383785023662125, 0.19695051768491634, 0.20100243215322858, 0.13444018469254174, 0.3188401318903645, 0.3224070358642755, 0.2607032982525766, 0.20217755216976052, 0.3821888083914971, 0.1459720976568041, 0.10473327909383644, 0.2268615672796951, 0.19557015843567246, 0.283664560312628, 0.16648089635485122, 0.1565296936272576, 0.06672504120820959, 0.2819994116727059, 0.07988384348610596, 0.09924187664744413, 0.13766168789123986, 0.1450140550378984, 0.3454600135659713, 0.32727400137577944, 0.04505353408563778, 0.0629233844346537, 0.22668546189327282, 0.11257699438248552, 0.08138998511915252, 0.12672762822781064, 0.1036438291832675, 0.14759096124813403, 0.12774791403567765, 0.12675253039586565, 0.3108969564764934, 0.06619877076240874, 0.2544609492156188, 0.2154915205198261, 0.1546559790269494, 0.105508385075184, 0.05920806273832529, 0.09109387113554694, 0.35092316034175675, 0.12071013437332859, 0.1921979683493818, 0.1249524774275834, 0.08439755447893048, 0.11154186944842433, 0.10386271444743178, 0.4726973491920182, 0.2309156926092209, 0.10284187223026271, 0.10922334471512994, 0.1045633756396173, 0.23466244115814597, 0.056139748868257136, 0.11233049591358522, 0.12555706041783532, 0.06595357033279449, 0.2615875870551342, 0.10668474031818015, 0.0696618993519586, 0.04561835580776191, 0.12524444959567815, 0.1174056982377871, 0.20815960565566757, 0.19836408339805098, 0.07538803436393172, 0.18010461677991163, 0.13137085184895494, 0.0931607922322496, 0.12201479028202593, 0.13378998890943028, 0.12071286642334536, 0.04671644690480105, 0.19127380111007442, 0.15635782058423545, 0.1641483676891421, 0.07431414377427858, 0.1747698619636105, 0.09591505682642018, 0.1837496203512032, 0.10851145684964929, 0.1529149967086415, 0.052866938105933875, 0.09598046714873792, 0.19113857411661853, 0.06900639691999139, 0.0723006797085399, 0.23334270423549872, 0.0876461909631103, 0.06744519342726879, 0.05284978263866548, 0.15522033936828203, 0.2041680656485937, 0.17746832900001508, 0.16846177254226338, 0.14628990482780935, 0.15732086447177043, 0.36001614437464613, 0.09008874466299176, 0.07787730379982684, 0.13996421257432112, 0.20503847923107968, 0.10770398723498092, 0.17977336899273624, 0.16115161237908704, 0.1587354822897286, 0.26937552114971125, 0.08034780487616756, 0.11370636394567768, 0.14010560851500692, 0.08058993192678993, 0.22022866701435578, 0.054263298177375005, 0.18655778826226982, 0.0923995659422722, 0.19976071695447753, 0.23103125103874209, 0.26356930853647303, 0.16046088734830266, 0.24417673950659505, 0.11233689988673265, 0.2250840697278148, 0.21172155845901408, 0.061826357168729494, 0.09534861146169173, 0.10358453866852675, 0.07008806804878158, 0.04857947811566929, 0.2026876428700937, 0.22232675523105622, 0.14502127448417726, 0.25395609058551866, 0.2078280377213176, 0.1523260217106234, 0.1959523512224278, 0.24592695809634976, 0.25021837784211076, 0.17065491090714174, 0.04553293887931064, 0.18615216410645102, 0.07596412535082084, 0.17579471689723636, 0.20209115825396798, 0.113424969752685, 0.06917363187016444, 0.050418935813131255, 0.15690109754067674, 0.10818306473959967, 0.23768818478404746, 0.172265892711865, 0.03320896687240676, 0.13544279367200165, 0.17782866514013093, 0.09867186888633464, 0.04932404361766247, 0.10586143831107618, 0.19237421061554572, 0.17494049529673583, 0.16061282811963845, 0.23065097266290388, 0.07364050876839999, 0.1456338423308195, 0.23925960638135624, 0.12824853593954827, 0.13651629608715868, 0.24809139841920297, 0.07970370802961613, 0.09231527779759671, 0.07225505217287, 0.07702579502013235, 0.06211510451108184, 0.12423617893400828, 0.10338688238662039, 0.08709510884418571, 0.08052488639718892, 0.2465923307382354, 0.0822317629744065, 0.07347870143861322, 0.08819020542044219, 0.09059557148820283, 0.10296128381659804, 0.12433926336822322, 0.2235304217690926, 0.1009108965936506, 0.07224495471046215, 0.07388455881101763, 0.05843793992587221, 0.06373786917920428, 0.09152338829653152, 0.1447803787774588, 0.05544222373574086, 0.13537302659417963, 0.18073608073065617, 0.05590779814435386, 0.15749057698077465, 0.2732974574274145, 0.12214956783204135, 0.05778935936985354, 0.1274045656810368, 0.08900307286379738, 0.3273701477643841, 0.2948454888278099, 0.19369897811153455, 0.15081564206390288, 0.11774103399826344, 0.09154550845923186, 0.27577229656551244, 0.08170466163993441, 0.21378008425231942, 0.09586823844834105, 0.18550730394219983, 0.08507673932830835, 0.18531219591617934, 0.1351677481092732, 0.1456721082876976, 0.13232373410953382, 0.09282480825955212, 0.0734344318492676, 0.20970613092757348, 0.17452057932679937, 0.07336395449423554, 0.14428851630777537, 0.043046375589589556, 0.31976139300944356, 0.07961577059848005, 0.05850739940085808, 0.22235284923279305, 0.07996884668137846, 0.12557880180317055, 0.05107726102780436, 0.16053573361705142, 0.15217983814014258, 0.2266171500927576, 0.27507021758724376, 0.12337793774532203, 0.12911102942499825, 0.13813741850900366, 0.07661217576080405, 0.12387563869970593, 0.331358157633616, 0.14714731434474654, 0.06828415707719303, 0.10268299587360646, 0.05995581812302665, 0.1628539587680532, 0.19791875437189044, 0.32385786305493053, 0.18098260326930424, 0.06682762590180337, 0.15030964177826706, 0.1210297445513592, 0.09767135271170999, 0.14967801420353397, 0.2263554463404462, 0.15949823369161747, 0.06815777442023428, 0.13608191212269716, 0.17721230766617815, 0.172317564347311, 0.06089173709347816, 0.07281105360256482, 0.06755451059278939, 0.08018705357862417, 0.2187341987001227, 0.09143534352200806, 0.17137087598461945, 0.12202848332770207, 0.1626824557785311, 0.2308367670949848, 0.13753378331990723, 0.12738590976826292, 0.26573603306107746, 0.1362594322231812, 0.18968775253129705, 0.07312716923425157, 0.25581765543282936, 0.15644293845167986, 0.3004340750556935, 0.14682778418945233, 0.17469964381861067, 0.15931444363017025, 0.1301823536167075, 0.10106349658331219, 0.2381470445096529, 0.3375226825772589, 0.27915116994983136, 0.06864850276274136, 0.0355504748705216, 0.12796168229237262, 0.20240386759657014, 0.08989346503004747, 0.08148512291288872, 0.17700757387165522, 0.10469733150001323, 0.12203044494454396, 0.0468287094174153, 0.052789758835094305, 0.055996288071819646, 0.16457044443635138, 0.1899932580023785, 0.262004242106395, 0.10121907679891207, 0.17322298265368907, 0.16540255528819992, 0.06800449571008786, 0.04699543504055917, 0.15499491871691948, 0.34292585730109953, 0.17322161927023624, 0.0990146155882422, 0.12066358056443549, 0.38304091350124003, 0.30068727340469387, 0.14522130164501057, 0.2576346565157401, 0.08522357971420166, 0.20825185373156288, 0.1489897471488964, 0.15710754872022958, 0.23974135767924332, 0.22993801439761413, 0.06102348633148713, 0.05271474461728935, 0.08232807630520197, 0.13369588322972023, 0.13180536429826184, 0.2022451071408431, 0.13415466875906365, 0.08184451876738662, 0.13959407277102515, 0.1324751961338476, 0.11168476124016298, 0.03934924934321302, 0.1408348310249945, 0.09028248701834712, 0.04984771450402612, 0.09908130686428093, 0.13711279221236936, 0.05822010802224908, 0.08485510597091676, 0.11877729224159761, 0.19372999494879609, 0.1278275999505995, 0.15569598934428117, 0.08814448224864625, 0.045093144943069915, 0.08070892621145415, 0.07219508219222451, 0.13839960060077666, 0.14033110616257793, 0.12263030290627965, 0.19755313608574154, 0.0439172178035708, 0.06273810859127336, 0.1259514898931185, 0.030543948857957148, 0.05918321044429921, 0.11479016404486218, 0.26394788402159247, 0.12666756673555732, 0.08726892787708321, 0.16206758393736231, 0.04015827429013477, 0.1363323064475041, 0.12391606552576731, 0.2079405633131888, 0.18238663539563751, 0.3269357574193352, 0.14632356735219862, 0.14302546138960484, 0.14091779979032426, 0.028298453404120374, 0.13081286487902807, 0.08696662304062448, 0.14928674106771336, 0.1300999465418614, 0.17383266509476147, 0.07995057487195081, 0.07744494769030276, 0.04331740449988791, 0.14254192843833113, 0.059273714077095474, 0.13109828029744658, 0.07386385641383104, 0.06190340946371883, 0.14913679373059738, 0.2024005598036309, 0.08521974040378423, 0.205069894254618, 0.10701233701759061, 0.06059081562508089, 0.17482210095465647, 0.09165980832603478, 0.15328738919648083, 0.07310786165234981, 0.060622709687218224, 0.1529080450981843, 0.1695026663713251, 0.049844598272609, 0.08317229976500262, 0.04447489297006916, 0.10180747241097013, 0.09051999476808911, 0.10233136637678879, 0.0919293382701935, 0.056515101046698706, 0.11245033365243257, 0.2007774033544636, 0.17515855558835278, 0.12692044306519099, 0.08490232055563655, 0.10731034717486918, 0.15755462708239884, 0.11247837709420483, 0.11317017151097027, 0.0313426479436992, 0.09776160405973951, 0.08246542318663207, 0.12680231411432233, 0.1465077201623872, 0.24916663917906542, 0.15255495021361007, 0.07431835708690117, 0.17116213597937505, 0.09387469292266615, 0.3462941132937807, 0.18907053289179318, 0.11227815601315512, 0.09165326813244587, 0.13410652671834233, 0.20740951798804355, 0.20910787114451335, 0.12324203583694146, 0.12452879284773968, 0.07255426695375969, 0.21192375770629154, 0.14013536285939743, 0.17251541806133402, 0.15866269562245072, 0.0726092985017792, 0.22805561926603313, 0.09505678521822912, 0.05321670233561333, 0.11353346286062728, 0.07917388144356242, 0.08385691929240528, 0.2886729486972348, 0.11537905265726785, 0.15065580849198637, 0.06325068569904897, 0.08055975647555118, 0.14646944232698877, 0.0816208546756739, 0.13927802781617568, 0.16222433464643263, 0.24119454188233153, 0.10704414063879256, 0.07283589228791788, 0.04290438299740805, 0.10209798291221048, 0.1447851119097025, 0.10884699411298179, 0.07515441041060428, 0.15216305076255365, 0.16494594288075148, 0.09801186498454453, 0.07338787862054173, 0.12270365439667624, 0.13265823785668215, 0.23441184843760848]
0.09676578715705995
Making ranges
torch.Size([20879, 2])
We keep 3.86e+06/1.75e+08 =  2% of the original kernel matrix.

torch.Size([542, 2])
We keep 5.09e+03/2.72e+04 = 18% of the original kernel matrix.

torch.Size([4166, 2])
We keep 1.78e+05/2.19e+06 =  8% of the original kernel matrix.

torch.Size([9386, 2])
We keep 1.12e+06/2.40e+07 =  4% of the original kernel matrix.

torch.Size([13445, 2])
We keep 1.97e+06/6.49e+07 =  3% of the original kernel matrix.

torch.Size([8112, 2])
We keep 1.55e+06/2.99e+07 =  5% of the original kernel matrix.

torch.Size([12459, 2])
We keep 2.24e+06/7.24e+07 =  3% of the original kernel matrix.

torch.Size([4582, 2])
We keep 3.34e+05/4.85e+06 =  6% of the original kernel matrix.

torch.Size([9498, 2])
We keep 1.04e+06/2.92e+07 =  3% of the original kernel matrix.

torch.Size([4916, 2])
We keep 3.35e+05/5.30e+06 =  6% of the original kernel matrix.

torch.Size([9779, 2])
We keep 1.10e+06/3.05e+07 =  3% of the original kernel matrix.

torch.Size([12046, 2])
We keep 1.97e+06/5.26e+07 =  3% of the original kernel matrix.

torch.Size([15405, 2])
We keep 2.64e+06/9.61e+07 =  2% of the original kernel matrix.

torch.Size([24237, 2])
We keep 7.45e+06/2.79e+08 =  2% of the original kernel matrix.

torch.Size([21956, 2])
We keep 5.22e+06/2.21e+08 =  2% of the original kernel matrix.

torch.Size([19164, 2])
We keep 3.80e+06/1.37e+08 =  2% of the original kernel matrix.

torch.Size([19723, 2])
We keep 3.86e+06/1.55e+08 =  2% of the original kernel matrix.

torch.Size([8333, 2])
We keep 4.43e+06/2.95e+07 = 15% of the original kernel matrix.

torch.Size([12567, 2])
We keep 2.18e+06/7.19e+07 =  3% of the original kernel matrix.

torch.Size([7377, 2])
We keep 2.75e+06/3.12e+07 =  8% of the original kernel matrix.

torch.Size([11946, 2])
We keep 2.13e+06/7.40e+07 =  2% of the original kernel matrix.

torch.Size([7924, 2])
We keep 1.14e+06/2.16e+07 =  5% of the original kernel matrix.

torch.Size([12193, 2])
We keep 1.86e+06/6.16e+07 =  3% of the original kernel matrix.

torch.Size([15165, 2])
We keep 5.04e+06/1.21e+08 =  4% of the original kernel matrix.

torch.Size([16416, 2])
We keep 3.59e+06/1.46e+08 =  2% of the original kernel matrix.

torch.Size([2977, 2])
We keep 1.24e+05/1.57e+06 =  7% of the original kernel matrix.

torch.Size([7997, 2])
We keep 7.16e+05/1.66e+07 =  4% of the original kernel matrix.

torch.Size([99064, 2])
We keep 1.01e+08/3.97e+09 =  2% of the original kernel matrix.

torch.Size([44637, 2])
We keep 1.59e+07/8.34e+08 =  1% of the original kernel matrix.

torch.Size([10737, 2])
We keep 1.34e+06/3.63e+07 =  3% of the original kernel matrix.

torch.Size([14440, 2])
We keep 2.27e+06/7.98e+07 =  2% of the original kernel matrix.

torch.Size([10367, 2])
We keep 1.58e+06/3.04e+07 =  5% of the original kernel matrix.

torch.Size([14228, 2])
We keep 2.10e+06/7.31e+07 =  2% of the original kernel matrix.

torch.Size([14714, 2])
We keep 3.28e+06/8.88e+07 =  3% of the original kernel matrix.

torch.Size([17246, 2])
We keep 3.25e+06/1.25e+08 =  2% of the original kernel matrix.

torch.Size([21768, 2])
We keep 7.04e+06/1.98e+08 =  3% of the original kernel matrix.

torch.Size([20438, 2])
We keep 4.54e+06/1.86e+08 =  2% of the original kernel matrix.

torch.Size([46446, 2])
We keep 2.77e+07/9.39e+08 =  2% of the original kernel matrix.

torch.Size([29538, 2])
We keep 8.67e+06/4.06e+08 =  2% of the original kernel matrix.

torch.Size([53772, 2])
We keep 2.57e+07/1.04e+09 =  2% of the original kernel matrix.

torch.Size([31875, 2])
We keep 8.88e+06/4.27e+08 =  2% of the original kernel matrix.

torch.Size([4885, 2])
We keep 4.86e+05/6.11e+06 =  7% of the original kernel matrix.

torch.Size([9581, 2])
We keep 1.15e+06/3.27e+07 =  3% of the original kernel matrix.

torch.Size([64910, 2])
We keep 3.74e+07/1.80e+09 =  2% of the original kernel matrix.

torch.Size([35384, 2])
We keep 1.12e+07/5.62e+08 =  1% of the original kernel matrix.

torch.Size([5750, 2])
We keep 4.74e+05/7.46e+06 =  6% of the original kernel matrix.

torch.Size([10576, 2])
We keep 1.26e+06/3.62e+07 =  3% of the original kernel matrix.

torch.Size([11104, 2])
We keep 4.24e+06/4.42e+07 =  9% of the original kernel matrix.

torch.Size([14759, 2])
We keep 2.47e+06/8.80e+07 =  2% of the original kernel matrix.

torch.Size([1588, 2])
We keep 4.26e+04/3.49e+05 = 12% of the original kernel matrix.

torch.Size([6157, 2])
We keep 4.24e+05/7.83e+06 =  5% of the original kernel matrix.

torch.Size([19330, 2])
We keep 3.87e+06/1.43e+08 =  2% of the original kernel matrix.

torch.Size([19820, 2])
We keep 3.89e+06/1.58e+08 =  2% of the original kernel matrix.

torch.Size([20753, 2])
We keep 5.57e+06/1.88e+08 =  2% of the original kernel matrix.

torch.Size([20349, 2])
We keep 4.43e+06/1.82e+08 =  2% of the original kernel matrix.

torch.Size([3539, 2])
We keep 1.94e+05/2.39e+06 =  8% of the original kernel matrix.

torch.Size([8549, 2])
We keep 8.24e+05/2.05e+07 =  4% of the original kernel matrix.

torch.Size([63464, 2])
We keep 1.15e+08/4.04e+09 =  2% of the original kernel matrix.

torch.Size([34439, 2])
We keep 1.65e+07/8.41e+08 =  1% of the original kernel matrix.

torch.Size([329867, 2])
We keep 5.01e+08/3.81e+10 =  1% of the original kernel matrix.

torch.Size([83170, 2])
We keep 4.30e+07/2.59e+09 =  1% of the original kernel matrix.

torch.Size([3849, 2])
We keep 3.14e+05/3.84e+06 =  8% of the original kernel matrix.

torch.Size([8724, 2])
We keep 9.86e+05/2.60e+07 =  3% of the original kernel matrix.

torch.Size([544304, 2])
We keep 1.83e+09/1.40e+11 =  1% of the original kernel matrix.

torch.Size([109414, 2])
We keep 7.93e+07/4.96e+09 =  1% of the original kernel matrix.

torch.Size([4550, 2])
We keep 3.34e+05/4.93e+06 =  6% of the original kernel matrix.

torch.Size([9374, 2])
We keep 1.09e+06/2.94e+07 =  3% of the original kernel matrix.

torch.Size([5445, 2])
We keep 4.57e+05/6.83e+06 =  6% of the original kernel matrix.

torch.Size([10151, 2])
We keep 1.19e+06/3.46e+07 =  3% of the original kernel matrix.

torch.Size([3984, 2])
We keep 2.44e+05/3.40e+06 =  7% of the original kernel matrix.

torch.Size([8896, 2])
We keep 9.40e+05/2.44e+07 =  3% of the original kernel matrix.

torch.Size([142151, 2])
We keep 1.63e+08/8.88e+09 =  1% of the original kernel matrix.

torch.Size([54136, 2])
We keep 2.25e+07/1.25e+09 =  1% of the original kernel matrix.

torch.Size([20923, 2])
We keep 6.13e+06/1.94e+08 =  3% of the original kernel matrix.

torch.Size([20564, 2])
We keep 4.52e+06/1.84e+08 =  2% of the original kernel matrix.

torch.Size([1785, 2])
We keep 4.07e+04/3.97e+05 = 10% of the original kernel matrix.

torch.Size([6342, 2])
We keep 4.23e+05/8.34e+06 =  5% of the original kernel matrix.

torch.Size([1297, 2])
We keep 3.08e+04/2.43e+05 = 12% of the original kernel matrix.

torch.Size([5773, 2])
We keep 3.73e+05/6.53e+06 =  5% of the original kernel matrix.

torch.Size([3163, 2])
We keep 1.70e+05/1.71e+06 =  9% of the original kernel matrix.

torch.Size([7988, 2])
We keep 7.37e+05/1.73e+07 =  4% of the original kernel matrix.

torch.Size([2787, 2])
We keep 1.70e+05/1.89e+06 =  9% of the original kernel matrix.

torch.Size([7586, 2])
We keep 7.76e+05/1.82e+07 =  4% of the original kernel matrix.

torch.Size([1199, 2])
We keep 5.19e+04/2.98e+05 = 17% of the original kernel matrix.

torch.Size([5304, 2])
We keep 4.06e+05/7.23e+06 =  5% of the original kernel matrix.

torch.Size([512, 2])
We keep 8.38e+03/3.39e+04 = 24% of the original kernel matrix.

torch.Size([3853, 2])
We keep 2.05e+05/2.44e+06 =  8% of the original kernel matrix.

torch.Size([1359, 2])
We keep 3.41e+04/2.75e+05 = 12% of the original kernel matrix.

torch.Size([5731, 2])
We keep 3.95e+05/6.94e+06 =  5% of the original kernel matrix.

torch.Size([246, 2])
We keep 3.01e+03/1.00e+04 = 30% of the original kernel matrix.

torch.Size([3093, 2])
We keep 1.35e+05/1.32e+06 = 10% of the original kernel matrix.

torch.Size([663, 2])
We keep 1.26e+04/6.10e+04 = 20% of the original kernel matrix.

torch.Size([4412, 2])
We keep 2.41e+05/3.27e+06 =  7% of the original kernel matrix.

torch.Size([1120, 2])
We keep 2.26e+04/1.45e+05 = 15% of the original kernel matrix.

torch.Size([5370, 2])
We keep 3.21e+05/5.05e+06 =  6% of the original kernel matrix.

torch.Size([5993, 2])
We keep 5.86e+05/8.86e+06 =  6% of the original kernel matrix.

torch.Size([10631, 2])
We keep 1.35e+06/3.94e+07 =  3% of the original kernel matrix.

torch.Size([835, 2])
We keep 1.24e+04/8.58e+04 = 14% of the original kernel matrix.

torch.Size([4828, 2])
We keep 2.59e+05/3.88e+06 =  6% of the original kernel matrix.

torch.Size([2686, 2])
We keep 2.10e+05/1.72e+06 = 12% of the original kernel matrix.

torch.Size([7357, 2])
We keep 7.28e+05/1.74e+07 =  4% of the original kernel matrix.

torch.Size([7237, 2])
We keep 8.36e+05/1.61e+07 =  5% of the original kernel matrix.

torch.Size([11743, 2])
We keep 1.68e+06/5.32e+07 =  3% of the original kernel matrix.

torch.Size([698, 2])
We keep 9.74e+03/5.11e+04 = 19% of the original kernel matrix.

torch.Size([4543, 2])
We keep 2.23e+05/2.99e+06 =  7% of the original kernel matrix.

torch.Size([2581, 2])
We keep 1.16e+05/1.39e+06 =  8% of the original kernel matrix.

torch.Size([7467, 2])
We keep 6.77e+05/1.56e+07 =  4% of the original kernel matrix.

torch.Size([2306, 2])
We keep 9.73e+04/1.02e+06 =  9% of the original kernel matrix.

torch.Size([7059, 2])
We keep 6.17e+05/1.34e+07 =  4% of the original kernel matrix.

torch.Size([3672, 2])
We keep 1.89e+05/2.37e+06 =  7% of the original kernel matrix.

torch.Size([8673, 2])
We keep 8.41e+05/2.04e+07 =  4% of the original kernel matrix.

torch.Size([1357, 2])
We keep 3.98e+04/3.54e+05 = 11% of the original kernel matrix.

torch.Size([5644, 2])
We keep 4.12e+05/7.88e+06 =  5% of the original kernel matrix.

torch.Size([11381, 2])
We keep 2.57e+06/6.95e+07 =  3% of the original kernel matrix.

torch.Size([14655, 2])
We keep 3.02e+06/1.10e+08 =  2% of the original kernel matrix.

torch.Size([1119, 2])
We keep 2.21e+04/1.65e+05 = 13% of the original kernel matrix.

torch.Size([5382, 2])
We keep 3.28e+05/5.38e+06 =  6% of the original kernel matrix.

torch.Size([1501, 2])
We keep 4.93e+04/4.20e+05 = 11% of the original kernel matrix.

torch.Size([5961, 2])
We keep 4.51e+05/8.58e+06 =  5% of the original kernel matrix.

torch.Size([1196, 2])
We keep 2.63e+04/2.02e+05 = 12% of the original kernel matrix.

torch.Size([5457, 2])
We keep 3.46e+05/5.96e+06 =  5% of the original kernel matrix.

torch.Size([1392, 2])
We keep 3.83e+04/3.27e+05 = 11% of the original kernel matrix.

torch.Size([5929, 2])
We keep 4.32e+05/7.58e+06 =  5% of the original kernel matrix.

torch.Size([426, 2])
We keep 6.74e+03/3.20e+04 = 21% of the original kernel matrix.

torch.Size([3883, 2])
We keep 1.96e+05/2.37e+06 =  8% of the original kernel matrix.

torch.Size([1588, 2])
We keep 7.17e+04/5.07e+05 = 14% of the original kernel matrix.

torch.Size([6097, 2])
We keep 4.89e+05/9.43e+06 =  5% of the original kernel matrix.

torch.Size([2874, 2])
We keep 1.29e+05/1.46e+06 =  8% of the original kernel matrix.

torch.Size([7719, 2])
We keep 6.98e+05/1.60e+07 =  4% of the original kernel matrix.

torch.Size([1664, 2])
We keep 5.07e+04/4.58e+05 = 11% of the original kernel matrix.

torch.Size([6307, 2])
We keep 4.66e+05/8.97e+06 =  5% of the original kernel matrix.

torch.Size([2008, 2])
We keep 7.65e+04/6.66e+05 = 11% of the original kernel matrix.

torch.Size([6684, 2])
We keep 5.29e+05/1.08e+07 =  4% of the original kernel matrix.

torch.Size([1087, 2])
We keep 3.16e+04/1.99e+05 = 15% of the original kernel matrix.

torch.Size([5429, 2])
We keep 3.43e+05/5.91e+06 =  5% of the original kernel matrix.

torch.Size([948, 2])
We keep 2.61e+04/1.41e+05 = 18% of the original kernel matrix.

torch.Size([5048, 2])
We keep 3.16e+05/4.97e+06 =  6% of the original kernel matrix.

torch.Size([1003, 2])
We keep 1.74e+04/1.46e+05 = 11% of the original kernel matrix.

torch.Size([5352, 2])
We keep 3.09e+05/5.06e+06 =  6% of the original kernel matrix.

torch.Size([1116, 2])
We keep 2.31e+04/1.55e+05 = 14% of the original kernel matrix.

torch.Size([5440, 2])
We keep 3.28e+05/5.22e+06 =  6% of the original kernel matrix.

torch.Size([1888, 2])
We keep 5.25e+04/4.89e+05 = 10% of the original kernel matrix.

torch.Size([6506, 2])
We keep 4.80e+05/9.26e+06 =  5% of the original kernel matrix.

torch.Size([1129, 2])
We keep 3.12e+04/2.06e+05 = 15% of the original kernel matrix.

torch.Size([5338, 2])
We keep 3.60e+05/6.01e+06 =  5% of the original kernel matrix.

torch.Size([735, 2])
We keep 1.59e+04/9.99e+04 = 15% of the original kernel matrix.

torch.Size([4524, 2])
We keep 2.85e+05/4.19e+06 =  6% of the original kernel matrix.

torch.Size([937, 2])
We keep 1.78e+04/1.23e+05 = 14% of the original kernel matrix.

torch.Size([5052, 2])
We keep 3.05e+05/4.65e+06 =  6% of the original kernel matrix.

torch.Size([3232, 2])
We keep 1.87e+05/2.41e+06 =  7% of the original kernel matrix.

torch.Size([7968, 2])
We keep 8.27e+05/2.06e+07 =  4% of the original kernel matrix.

torch.Size([4919, 2])
We keep 4.97e+05/6.86e+06 =  7% of the original kernel matrix.

torch.Size([9523, 2])
We keep 1.21e+06/3.47e+07 =  3% of the original kernel matrix.

torch.Size([395, 2])
We keep 4.85e+03/2.10e+04 = 23% of the original kernel matrix.

torch.Size([3646, 2])
We keep 1.69e+05/1.92e+06 =  8% of the original kernel matrix.

torch.Size([6561, 2])
We keep 6.29e+05/1.14e+07 =  5% of the original kernel matrix.

torch.Size([10905, 2])
We keep 1.46e+06/4.48e+07 =  3% of the original kernel matrix.

torch.Size([1413, 2])
We keep 3.16e+04/2.63e+05 = 12% of the original kernel matrix.

torch.Size([5965, 2])
We keep 3.93e+05/6.80e+06 =  5% of the original kernel matrix.

torch.Size([4949, 2])
We keep 4.81e+05/7.12e+06 =  6% of the original kernel matrix.

torch.Size([9608, 2])
We keep 1.28e+06/3.53e+07 =  3% of the original kernel matrix.

torch.Size([2794, 2])
We keep 1.39e+05/1.58e+06 =  8% of the original kernel matrix.

torch.Size([7520, 2])
We keep 7.22e+05/1.66e+07 =  4% of the original kernel matrix.

torch.Size([706, 2])
We keep 9.86e+03/5.95e+04 = 16% of the original kernel matrix.

torch.Size([4591, 2])
We keep 2.37e+05/3.23e+06 =  7% of the original kernel matrix.

torch.Size([1167, 2])
We keep 5.31e+04/3.16e+05 = 16% of the original kernel matrix.

torch.Size([5359, 2])
We keep 4.20e+05/7.44e+06 =  5% of the original kernel matrix.

torch.Size([777, 2])
We keep 1.10e+04/6.25e+04 = 17% of the original kernel matrix.

torch.Size([4739, 2])
We keep 2.45e+05/3.31e+06 =  7% of the original kernel matrix.

torch.Size([719, 2])
We keep 1.39e+04/9.00e+04 = 15% of the original kernel matrix.

torch.Size([4512, 2])
We keep 2.73e+05/3.97e+06 =  6% of the original kernel matrix.

torch.Size([11399, 2])
We keep 1.22e+06/3.08e+07 =  3% of the original kernel matrix.

torch.Size([14523, 2])
We keep 2.07e+06/7.35e+07 =  2% of the original kernel matrix.

torch.Size([1003, 2])
We keep 2.29e+04/1.54e+05 = 14% of the original kernel matrix.

torch.Size([5116, 2])
We keep 3.29e+05/5.19e+06 =  6% of the original kernel matrix.

torch.Size([653, 2])
We keep 8.45e+03/4.49e+04 = 18% of the original kernel matrix.

torch.Size([4494, 2])
We keep 2.17e+05/2.81e+06 =  7% of the original kernel matrix.

torch.Size([853, 2])
We keep 1.55e+04/1.08e+05 = 14% of the original kernel matrix.

torch.Size([4864, 2])
We keep 2.87e+05/4.34e+06 =  6% of the original kernel matrix.

torch.Size([13819, 2])
We keep 2.49e+06/7.44e+07 =  3% of the original kernel matrix.

torch.Size([16492, 2])
We keep 3.02e+06/1.14e+08 =  2% of the original kernel matrix.

torch.Size([628, 2])
We keep 8.77e+03/5.48e+04 = 16% of the original kernel matrix.

torch.Size([4359, 2])
We keep 2.27e+05/3.10e+06 =  7% of the original kernel matrix.

torch.Size([682, 2])
We keep 7.33e+03/4.28e+04 = 17% of the original kernel matrix.

torch.Size([4730, 2])
We keep 2.17e+05/2.74e+06 =  7% of the original kernel matrix.

torch.Size([1663, 2])
We keep 4.56e+04/3.87e+05 = 11% of the original kernel matrix.

torch.Size([6227, 2])
We keep 4.39e+05/8.24e+06 =  5% of the original kernel matrix.

torch.Size([6127, 2])
We keep 8.46e+05/1.29e+07 =  6% of the original kernel matrix.

torch.Size([10528, 2])
We keep 1.50e+06/4.75e+07 =  3% of the original kernel matrix.

torch.Size([1924, 2])
We keep 5.95e+04/5.75e+05 = 10% of the original kernel matrix.

torch.Size([6598, 2])
We keep 5.11e+05/1.00e+07 =  5% of the original kernel matrix.

torch.Size([2127, 2])
We keep 7.66e+04/8.06e+05 =  9% of the original kernel matrix.

torch.Size([6861, 2])
We keep 5.65e+05/1.19e+07 =  4% of the original kernel matrix.

torch.Size([6083, 2])
We keep 8.24e+05/1.40e+07 =  5% of the original kernel matrix.

torch.Size([10598, 2])
We keep 1.69e+06/4.95e+07 =  3% of the original kernel matrix.

torch.Size([1848, 2])
We keep 1.45e+05/9.27e+05 = 15% of the original kernel matrix.

torch.Size([6230, 2])
We keep 6.11e+05/1.28e+07 =  4% of the original kernel matrix.

torch.Size([1516, 2])
We keep 5.07e+04/3.72e+05 = 13% of the original kernel matrix.

torch.Size([6053, 2])
We keep 4.29e+05/8.08e+06 =  5% of the original kernel matrix.

torch.Size([4006, 2])
We keep 2.48e+05/3.20e+06 =  7% of the original kernel matrix.

torch.Size([8795, 2])
We keep 9.16e+05/2.37e+07 =  3% of the original kernel matrix.

torch.Size([4376, 2])
We keep 3.12e+05/4.41e+06 =  7% of the original kernel matrix.

torch.Size([9163, 2])
We keep 1.03e+06/2.78e+07 =  3% of the original kernel matrix.

torch.Size([1536, 2])
We keep 7.91e+04/4.60e+05 = 17% of the original kernel matrix.

torch.Size([5943, 2])
We keep 4.71e+05/8.98e+06 =  5% of the original kernel matrix.

torch.Size([1198, 2])
We keep 2.48e+04/1.92e+05 = 12% of the original kernel matrix.

torch.Size([5518, 2])
We keep 3.35e+05/5.80e+06 =  5% of the original kernel matrix.

torch.Size([1006, 2])
We keep 1.65e+04/1.27e+05 = 13% of the original kernel matrix.

torch.Size([4946, 2])
We keep 2.91e+05/4.72e+06 =  6% of the original kernel matrix.

torch.Size([3438, 2])
We keep 3.56e+05/3.98e+06 =  8% of the original kernel matrix.

torch.Size([7929, 2])
We keep 9.77e+05/2.64e+07 =  3% of the original kernel matrix.

torch.Size([910, 2])
We keep 2.20e+04/1.34e+05 = 16% of the original kernel matrix.

torch.Size([4953, 2])
We keep 3.07e+05/4.85e+06 =  6% of the original kernel matrix.

torch.Size([1434, 2])
We keep 3.24e+04/2.77e+05 = 11% of the original kernel matrix.

torch.Size([6099, 2])
We keep 3.99e+05/6.97e+06 =  5% of the original kernel matrix.

torch.Size([2414, 2])
We keep 8.50e+04/8.85e+05 =  9% of the original kernel matrix.

torch.Size([7014, 2])
We keep 5.40e+05/1.25e+07 =  4% of the original kernel matrix.

torch.Size([3891, 2])
We keep 2.50e+05/3.37e+06 =  7% of the original kernel matrix.

torch.Size([8621, 2])
We keep 9.33e+05/2.43e+07 =  3% of the original kernel matrix.

torch.Size([372, 2])
We keep 5.50e+03/2.19e+04 = 25% of the original kernel matrix.

torch.Size([3582, 2])
We keep 1.75e+05/1.96e+06 =  8% of the original kernel matrix.

torch.Size([8830, 2])
We keep 1.35e+06/2.96e+07 =  4% of the original kernel matrix.

torch.Size([12719, 2])
We keep 2.17e+06/7.20e+07 =  3% of the original kernel matrix.

torch.Size([700, 2])
We keep 1.37e+04/7.40e+04 = 18% of the original kernel matrix.

torch.Size([4384, 2])
We keep 2.59e+05/3.60e+06 =  7% of the original kernel matrix.

torch.Size([1494, 2])
We keep 4.76e+04/4.19e+05 = 11% of the original kernel matrix.

torch.Size([6130, 2])
We keep 4.75e+05/8.57e+06 =  5% of the original kernel matrix.

torch.Size([1084, 2])
We keep 2.62e+04/1.80e+05 = 14% of the original kernel matrix.

torch.Size([5314, 2])
We keep 3.39e+05/5.62e+06 =  6% of the original kernel matrix.

torch.Size([5295, 2])
We keep 5.20e+05/6.94e+06 =  7% of the original kernel matrix.

torch.Size([9853, 2])
We keep 1.22e+06/3.49e+07 =  3% of the original kernel matrix.

torch.Size([3748, 2])
We keep 2.40e+05/3.11e+06 =  7% of the original kernel matrix.

torch.Size([8562, 2])
We keep 8.85e+05/2.34e+07 =  3% of the original kernel matrix.

torch.Size([6391, 2])
We keep 4.04e+05/7.60e+06 =  5% of the original kernel matrix.

torch.Size([10719, 2])
We keep 1.21e+06/3.65e+07 =  3% of the original kernel matrix.

torch.Size([276, 2])
We keep 2.04e+03/8.10e+03 = 25% of the original kernel matrix.

torch.Size([3372, 2])
We keep 1.24e+05/1.19e+06 = 10% of the original kernel matrix.

torch.Size([5415, 2])
We keep 6.05e+05/8.75e+06 =  6% of the original kernel matrix.

torch.Size([10043, 2])
We keep 1.32e+06/3.92e+07 =  3% of the original kernel matrix.

torch.Size([622, 2])
We keep 6.63e+03/3.76e+04 = 17% of the original kernel matrix.

torch.Size([4458, 2])
We keep 2.04e+05/2.57e+06 =  7% of the original kernel matrix.

torch.Size([1855, 2])
We keep 5.74e+04/5.46e+05 = 10% of the original kernel matrix.

torch.Size([6688, 2])
We keep 4.98e+05/9.79e+06 =  5% of the original kernel matrix.

torch.Size([1744, 2])
We keep 7.46e+04/5.94e+05 = 12% of the original kernel matrix.

torch.Size([6264, 2])
We keep 5.09e+05/1.02e+07 =  4% of the original kernel matrix.

torch.Size([1114, 2])
We keep 2.47e+04/1.81e+05 = 13% of the original kernel matrix.

torch.Size([5324, 2])
We keep 3.33e+05/5.64e+06 =  5% of the original kernel matrix.

torch.Size([10444, 2])
We keep 2.44e+06/6.19e+07 =  3% of the original kernel matrix.

torch.Size([14001, 2])
We keep 2.94e+06/1.04e+08 =  2% of the original kernel matrix.

torch.Size([1170, 2])
We keep 3.08e+04/2.28e+05 = 13% of the original kernel matrix.

torch.Size([5292, 2])
We keep 3.50e+05/6.32e+06 =  5% of the original kernel matrix.

torch.Size([1274, 2])
We keep 3.73e+04/3.29e+05 = 11% of the original kernel matrix.

torch.Size([5722, 2])
We keep 4.26e+05/7.60e+06 =  5% of the original kernel matrix.

torch.Size([1499, 2])
We keep 4.69e+04/4.10e+05 = 11% of the original kernel matrix.

torch.Size([6046, 2])
We keep 4.45e+05/8.48e+06 =  5% of the original kernel matrix.

torch.Size([582, 2])
We keep 7.26e+03/4.24e+04 = 17% of the original kernel matrix.

torch.Size([4399, 2])
We keep 2.20e+05/2.73e+06 =  8% of the original kernel matrix.

torch.Size([3363, 2])
We keep 1.78e+05/2.25e+06 =  7% of the original kernel matrix.

torch.Size([8444, 2])
We keep 8.26e+05/1.99e+07 =  4% of the original kernel matrix.

torch.Size([876, 2])
We keep 1.46e+04/1.01e+05 = 14% of the original kernel matrix.

torch.Size([5063, 2])
We keep 2.86e+05/4.21e+06 =  6% of the original kernel matrix.

torch.Size([2511, 2])
We keep 1.24e+05/1.33e+06 =  9% of the original kernel matrix.

torch.Size([7309, 2])
We keep 6.68e+05/1.53e+07 =  4% of the original kernel matrix.

torch.Size([1702, 2])
We keep 8.00e+04/6.53e+05 = 12% of the original kernel matrix.

torch.Size([6201, 2])
We keep 5.34e+05/1.07e+07 =  4% of the original kernel matrix.

torch.Size([874, 2])
We keep 2.06e+04/1.12e+05 = 18% of the original kernel matrix.

torch.Size([4922, 2])
We keep 2.94e+05/4.42e+06 =  6% of the original kernel matrix.

torch.Size([600, 2])
We keep 6.25e+03/3.61e+04 = 17% of the original kernel matrix.

torch.Size([4449, 2])
We keep 2.01e+05/2.52e+06 =  7% of the original kernel matrix.

torch.Size([589, 2])
We keep 8.89e+03/4.71e+04 = 18% of the original kernel matrix.

torch.Size([4214, 2])
We keep 2.21e+05/2.87e+06 =  7% of the original kernel matrix.

torch.Size([20077, 2])
We keep 5.72e+06/1.97e+08 =  2% of the original kernel matrix.

torch.Size([19408, 2])
We keep 4.45e+06/1.86e+08 =  2% of the original kernel matrix.

torch.Size([569, 2])
We keep 6.46e+03/3.20e+04 = 20% of the original kernel matrix.

torch.Size([4297, 2])
We keep 1.95e+05/2.37e+06 =  8% of the original kernel matrix.

torch.Size([1158, 2])
We keep 1.94e+04/1.54e+05 = 12% of the original kernel matrix.

torch.Size([5322, 2])
We keep 3.09e+05/5.19e+06 =  5% of the original kernel matrix.

torch.Size([1358, 2])
We keep 7.79e+04/4.26e+05 = 18% of the original kernel matrix.

torch.Size([5650, 2])
We keep 4.48e+05/8.65e+06 =  5% of the original kernel matrix.

torch.Size([1037, 2])
We keep 2.11e+04/1.43e+05 = 14% of the original kernel matrix.

torch.Size([5278, 2])
We keep 3.23e+05/5.01e+06 =  6% of the original kernel matrix.

torch.Size([831, 2])
We keep 1.66e+04/1.13e+05 = 14% of the original kernel matrix.

torch.Size([4841, 2])
We keep 2.98e+05/4.45e+06 =  6% of the original kernel matrix.

torch.Size([953, 2])
We keep 2.08e+04/1.47e+05 = 14% of the original kernel matrix.

torch.Size([4933, 2])
We keep 3.16e+05/5.07e+06 =  6% of the original kernel matrix.

torch.Size([3777, 2])
We keep 2.43e+05/3.54e+06 =  6% of the original kernel matrix.

torch.Size([8525, 2])
We keep 9.25e+05/2.49e+07 =  3% of the original kernel matrix.

torch.Size([3440, 2])
We keep 2.08e+05/2.54e+06 =  8% of the original kernel matrix.

torch.Size([8268, 2])
We keep 8.58e+05/2.11e+07 =  4% of the original kernel matrix.

torch.Size([1024, 2])
We keep 1.72e+04/1.24e+05 = 13% of the original kernel matrix.

torch.Size([5346, 2])
We keep 3.01e+05/4.66e+06 =  6% of the original kernel matrix.

torch.Size([1332, 2])
We keep 2.71e+04/2.25e+05 = 12% of the original kernel matrix.

torch.Size([5623, 2])
We keep 3.55e+05/6.28e+06 =  5% of the original kernel matrix.

torch.Size([1714, 2])
We keep 7.46e+04/6.24e+05 = 11% of the original kernel matrix.

torch.Size([6354, 2])
We keep 5.19e+05/1.05e+07 =  4% of the original kernel matrix.

torch.Size([784, 2])
We keep 2.02e+04/1.14e+05 = 17% of the original kernel matrix.

torch.Size([4687, 2])
We keep 2.91e+05/4.46e+06 =  6% of the original kernel matrix.

torch.Size([808, 2])
We keep 1.08e+04/6.55e+04 = 16% of the original kernel matrix.

torch.Size([4911, 2])
We keep 2.43e+05/3.39e+06 =  7% of the original kernel matrix.

torch.Size([1288, 2])
We keep 3.85e+04/2.87e+05 = 13% of the original kernel matrix.

torch.Size([5572, 2])
We keep 4.05e+05/7.10e+06 =  5% of the original kernel matrix.

torch.Size([1914, 2])
We keep 8.20e+04/6.50e+05 = 12% of the original kernel matrix.

torch.Size([6542, 2])
We keep 5.24e+05/1.07e+07 =  4% of the original kernel matrix.

torch.Size([3409, 2])
We keep 2.28e+05/2.95e+06 =  7% of the original kernel matrix.

torch.Size([8196, 2])
We keep 9.11e+05/2.28e+07 =  4% of the original kernel matrix.

torch.Size([1560, 2])
We keep 3.96e+04/3.70e+05 = 10% of the original kernel matrix.

torch.Size([6149, 2])
We keep 4.36e+05/8.05e+06 =  5% of the original kernel matrix.

torch.Size([1839, 2])
We keep 1.89e+05/1.14e+06 = 16% of the original kernel matrix.

torch.Size([6311, 2])
We keep 6.56e+05/1.41e+07 =  4% of the original kernel matrix.

torch.Size([694, 2])
We keep 1.12e+04/6.76e+04 = 16% of the original kernel matrix.

torch.Size([4599, 2])
We keep 2.49e+05/3.44e+06 =  7% of the original kernel matrix.

torch.Size([8361, 2])
We keep 1.58e+06/2.34e+07 =  6% of the original kernel matrix.

torch.Size([12463, 2])
We keep 1.95e+06/6.41e+07 =  3% of the original kernel matrix.

torch.Size([793, 2])
We keep 1.36e+04/1.01e+05 = 13% of the original kernel matrix.

torch.Size([4892, 2])
We keep 2.84e+05/4.21e+06 =  6% of the original kernel matrix.

torch.Size([2137, 2])
We keep 7.39e+04/7.59e+05 =  9% of the original kernel matrix.

torch.Size([6910, 2])
We keep 5.61e+05/1.15e+07 =  4% of the original kernel matrix.

torch.Size([878, 2])
We keep 1.61e+04/9.92e+04 = 16% of the original kernel matrix.

torch.Size([4964, 2])
We keep 2.83e+05/4.17e+06 =  6% of the original kernel matrix.

torch.Size([337, 2])
We keep 3.22e+03/1.49e+04 = 21% of the original kernel matrix.

torch.Size([3548, 2])
We keep 1.53e+05/1.62e+06 =  9% of the original kernel matrix.

torch.Size([3517, 2])
We keep 3.25e+05/3.73e+06 =  8% of the original kernel matrix.

torch.Size([8511, 2])
We keep 1.05e+06/2.56e+07 =  4% of the original kernel matrix.

torch.Size([370, 2])
We keep 4.20e+03/1.61e+04 = 26% of the original kernel matrix.

torch.Size([3589, 2])
We keep 1.53e+05/1.68e+06 =  9% of the original kernel matrix.

torch.Size([2574, 2])
We keep 2.03e+05/1.76e+06 = 11% of the original kernel matrix.

torch.Size([7115, 2])
We keep 7.31e+05/1.76e+07 =  4% of the original kernel matrix.

torch.Size([827, 2])
We keep 1.65e+04/9.36e+04 = 17% of the original kernel matrix.

torch.Size([4781, 2])
We keep 2.85e+05/4.05e+06 =  7% of the original kernel matrix.

torch.Size([1662, 2])
We keep 4.81e+04/4.68e+05 = 10% of the original kernel matrix.

torch.Size([6223, 2])
We keep 4.65e+05/9.06e+06 =  5% of the original kernel matrix.

torch.Size([5119, 2])
We keep 5.35e+05/8.35e+06 =  6% of the original kernel matrix.

torch.Size([9777, 2])
We keep 1.38e+06/3.83e+07 =  3% of the original kernel matrix.

torch.Size([2099, 2])
We keep 9.84e+04/8.23e+05 = 11% of the original kernel matrix.

torch.Size([6886, 2])
We keep 5.77e+05/1.20e+07 =  4% of the original kernel matrix.

torch.Size([704, 2])
We keep 1.51e+04/7.95e+04 = 18% of the original kernel matrix.

torch.Size([4487, 2])
We keep 2.60e+05/3.74e+06 =  6% of the original kernel matrix.

torch.Size([1899, 2])
We keep 9.44e+04/8.65e+05 = 10% of the original kernel matrix.

torch.Size([6513, 2])
We keep 6.08e+05/1.23e+07 =  4% of the original kernel matrix.

torch.Size([270, 2])
We keep 1.50e+03/5.33e+03 = 28% of the original kernel matrix.

torch.Size([3328, 2])
We keep 1.11e+05/9.67e+05 = 11% of the original kernel matrix.

torch.Size([444, 2])
We keep 4.41e+03/2.53e+04 = 17% of the original kernel matrix.

torch.Size([3970, 2])
We keep 1.79e+05/2.11e+06 =  8% of the original kernel matrix.

torch.Size([855, 2])
We keep 1.92e+04/1.21e+05 = 15% of the original kernel matrix.

torch.Size([4868, 2])
We keep 2.96e+05/4.61e+06 =  6% of the original kernel matrix.

torch.Size([1625, 2])
We keep 4.96e+04/4.50e+05 = 11% of the original kernel matrix.

torch.Size([6291, 2])
We keep 4.74e+05/8.89e+06 =  5% of the original kernel matrix.

torch.Size([596, 2])
We keep 8.53e+03/3.50e+04 = 24% of the original kernel matrix.

torch.Size([4385, 2])
We keep 2.02e+05/2.48e+06 =  8% of the original kernel matrix.

torch.Size([4419, 2])
We keep 3.99e+05/5.08e+06 =  7% of the original kernel matrix.

torch.Size([9117, 2])
We keep 1.09e+06/2.99e+07 =  3% of the original kernel matrix.

torch.Size([1089, 2])
We keep 2.22e+04/1.80e+05 = 12% of the original kernel matrix.

torch.Size([5420, 2])
We keep 3.41e+05/5.62e+06 =  6% of the original kernel matrix.

torch.Size([1226, 2])
We keep 3.45e+04/2.84e+05 = 12% of the original kernel matrix.

torch.Size([5491, 2])
We keep 3.95e+05/7.06e+06 =  5% of the original kernel matrix.

torch.Size([2460, 2])
We keep 1.06e+05/9.74e+05 = 10% of the original kernel matrix.

torch.Size([7204, 2])
We keep 5.93e+05/1.31e+07 =  4% of the original kernel matrix.

torch.Size([1215, 2])
We keep 4.87e+04/3.36e+05 = 14% of the original kernel matrix.

torch.Size([5425, 2])
We keep 4.18e+05/7.68e+06 =  5% of the original kernel matrix.

torch.Size([714, 2])
We keep 1.06e+04/6.10e+04 = 17% of the original kernel matrix.

torch.Size([4603, 2])
We keep 2.38e+05/3.27e+06 =  7% of the original kernel matrix.

torch.Size([10924, 2])
We keep 2.01e+06/5.14e+07 =  3% of the original kernel matrix.

torch.Size([14327, 2])
We keep 2.68e+06/9.49e+07 =  2% of the original kernel matrix.

torch.Size([735, 2])
We keep 9.19e+03/5.48e+04 = 16% of the original kernel matrix.

torch.Size([4751, 2])
We keep 2.34e+05/3.10e+06 =  7% of the original kernel matrix.

torch.Size([1353, 2])
We keep 5.45e+04/3.92e+05 = 13% of the original kernel matrix.

torch.Size([5686, 2])
We keep 4.51e+05/8.29e+06 =  5% of the original kernel matrix.

torch.Size([573, 2])
We keep 8.32e+03/4.28e+04 = 19% of the original kernel matrix.

torch.Size([4273, 2])
We keep 2.15e+05/2.74e+06 =  7% of the original kernel matrix.

torch.Size([1528, 2])
We keep 3.79e+04/3.40e+05 = 11% of the original kernel matrix.

torch.Size([6171, 2])
We keep 4.26e+05/7.72e+06 =  5% of the original kernel matrix.

torch.Size([3462, 2])
We keep 2.06e+05/2.47e+06 =  8% of the original kernel matrix.

torch.Size([8393, 2])
We keep 8.43e+05/2.08e+07 =  4% of the original kernel matrix.

torch.Size([727, 2])
We keep 1.23e+04/6.50e+04 = 18% of the original kernel matrix.

torch.Size([4571, 2])
We keep 2.46e+05/3.38e+06 =  7% of the original kernel matrix.

torch.Size([1534, 2])
We keep 5.27e+04/4.15e+05 = 12% of the original kernel matrix.

torch.Size([5983, 2])
We keep 4.49e+05/8.53e+06 =  5% of the original kernel matrix.

torch.Size([17831, 2])
We keep 4.64e+06/1.52e+08 =  3% of the original kernel matrix.

torch.Size([18731, 2])
We keep 4.06e+06/1.63e+08 =  2% of the original kernel matrix.

torch.Size([2754, 2])
We keep 1.31e+05/1.51e+06 =  8% of the original kernel matrix.

torch.Size([7607, 2])
We keep 7.04e+05/1.63e+07 =  4% of the original kernel matrix.

torch.Size([616, 2])
We keep 8.49e+03/5.02e+04 = 16% of the original kernel matrix.

torch.Size([4367, 2])
We keep 2.22e+05/2.97e+06 =  7% of the original kernel matrix.

torch.Size([1127, 2])
We keep 2.48e+04/1.93e+05 = 12% of the original kernel matrix.

torch.Size([5325, 2])
We keep 3.40e+05/5.81e+06 =  5% of the original kernel matrix.

torch.Size([1235, 2])
We keep 3.35e+04/2.97e+05 = 11% of the original kernel matrix.

torch.Size([5482, 2])
We keep 3.95e+05/7.22e+06 =  5% of the original kernel matrix.

torch.Size([1751, 2])
We keep 4.71e+04/4.82e+05 =  9% of the original kernel matrix.

torch.Size([6309, 2])
We keep 4.52e+05/9.19e+06 =  4% of the original kernel matrix.

torch.Size([965, 2])
We keep 4.11e+04/2.31e+05 = 17% of the original kernel matrix.

torch.Size([4845, 2])
We keep 3.68e+05/6.37e+06 =  5% of the original kernel matrix.

torch.Size([2423, 2])
We keep 8.03e+04/8.97e+05 =  8% of the original kernel matrix.

torch.Size([7340, 2])
We keep 5.85e+05/1.25e+07 =  4% of the original kernel matrix.

torch.Size([2870, 2])
We keep 1.40e+05/1.46e+06 =  9% of the original kernel matrix.

torch.Size([7553, 2])
We keep 6.91e+05/1.60e+07 =  4% of the original kernel matrix.

torch.Size([651, 2])
We keep 2.39e+04/8.64e+04 = 27% of the original kernel matrix.

torch.Size([4248, 2])
We keep 2.81e+05/3.89e+06 =  7% of the original kernel matrix.

torch.Size([1549, 2])
We keep 7.25e+04/5.76e+05 = 12% of the original kernel matrix.

torch.Size([5934, 2])
We keep 4.91e+05/1.01e+07 =  4% of the original kernel matrix.

torch.Size([662, 2])
We keep 9.36e+03/5.24e+04 = 17% of the original kernel matrix.

torch.Size([4601, 2])
We keep 2.34e+05/3.03e+06 =  7% of the original kernel matrix.

torch.Size([9793, 2])
We keep 2.06e+06/4.95e+07 =  4% of the original kernel matrix.

torch.Size([13599, 2])
We keep 2.72e+06/9.32e+07 =  2% of the original kernel matrix.

torch.Size([599, 2])
We keep 9.01e+03/5.02e+04 = 17% of the original kernel matrix.

torch.Size([4361, 2])
We keep 2.26e+05/2.97e+06 =  7% of the original kernel matrix.

torch.Size([2161, 2])
We keep 7.97e+04/7.22e+05 = 11% of the original kernel matrix.

torch.Size([7012, 2])
We keep 5.51e+05/1.13e+07 =  4% of the original kernel matrix.

torch.Size([1363, 2])
We keep 4.42e+04/3.65e+05 = 12% of the original kernel matrix.

torch.Size([5909, 2])
We keep 4.61e+05/8.00e+06 =  5% of the original kernel matrix.

torch.Size([4308, 2])
We keep 3.17e+05/4.18e+06 =  7% of the original kernel matrix.

torch.Size([9008, 2])
We keep 9.91e+05/2.71e+07 =  3% of the original kernel matrix.

torch.Size([421, 2])
We keep 3.88e+03/1.77e+04 = 21% of the original kernel matrix.

torch.Size([3887, 2])
We keep 1.60e+05/1.76e+06 =  9% of the original kernel matrix.

torch.Size([3722, 2])
We keep 2.19e+05/2.95e+06 =  7% of the original kernel matrix.

torch.Size([8559, 2])
We keep 8.99e+05/2.27e+07 =  3% of the original kernel matrix.

torch.Size([3281, 2])
We keep 2.04e+05/2.32e+06 =  8% of the original kernel matrix.

torch.Size([8143, 2])
We keep 8.27e+05/2.02e+07 =  4% of the original kernel matrix.

torch.Size([939, 2])
We keep 2.52e+04/1.38e+05 = 18% of the original kernel matrix.

torch.Size([5047, 2])
We keep 3.15e+05/4.93e+06 =  6% of the original kernel matrix.

torch.Size([2044, 2])
We keep 5.73e+04/5.64e+05 = 10% of the original kernel matrix.

torch.Size([6816, 2])
We keep 4.94e+05/9.95e+06 =  4% of the original kernel matrix.

torch.Size([2105, 2])
We keep 9.83e+04/8.48e+05 = 11% of the original kernel matrix.

torch.Size([6679, 2])
We keep 5.85e+05/1.22e+07 =  4% of the original kernel matrix.

torch.Size([968, 2])
We keep 3.04e+04/1.65e+05 = 18% of the original kernel matrix.

torch.Size([5029, 2])
We keep 3.29e+05/5.38e+06 =  6% of the original kernel matrix.

torch.Size([828, 2])
We keep 1.26e+04/8.01e+04 = 15% of the original kernel matrix.

torch.Size([4911, 2])
We keep 2.64e+05/3.75e+06 =  7% of the original kernel matrix.

torch.Size([1085, 2])
We keep 2.22e+04/1.67e+05 = 13% of the original kernel matrix.

torch.Size([5298, 2])
We keep 3.26e+05/5.42e+06 =  6% of the original kernel matrix.

torch.Size([577, 2])
We keep 8.60e+03/4.67e+04 = 18% of the original kernel matrix.

torch.Size([4252, 2])
We keep 2.21e+05/2.86e+06 =  7% of the original kernel matrix.

torch.Size([1207, 2])
We keep 3.59e+04/2.43e+05 = 14% of the original kernel matrix.

torch.Size([5324, 2])
We keep 3.72e+05/6.53e+06 =  5% of the original kernel matrix.

torch.Size([274, 2])
We keep 2.87e+03/1.06e+04 = 27% of the original kernel matrix.

torch.Size([3327, 2])
We keep 1.38e+05/1.36e+06 = 10% of the original kernel matrix.

torch.Size([440, 2])
We keep 4.46e+03/2.07e+04 = 21% of the original kernel matrix.

torch.Size([3935, 2])
We keep 1.75e+05/1.91e+06 =  9% of the original kernel matrix.

torch.Size([5455, 2])
We keep 6.82e+05/9.69e+06 =  7% of the original kernel matrix.

torch.Size([9980, 2])
We keep 1.37e+06/4.12e+07 =  3% of the original kernel matrix.

torch.Size([663, 2])
We keep 6.53e+03/3.42e+04 = 19% of the original kernel matrix.

torch.Size([4642, 2])
We keep 1.98e+05/2.45e+06 =  8% of the original kernel matrix.

torch.Size([417, 2])
We keep 1.58e+04/4.97e+04 = 31% of the original kernel matrix.

torch.Size([3427, 2])
We keep 2.28e+05/2.95e+06 =  7% of the original kernel matrix.

torch.Size([602, 2])
We keep 8.59e+03/4.49e+04 = 19% of the original kernel matrix.

torch.Size([4378, 2])
We keep 2.24e+05/2.81e+06 =  7% of the original kernel matrix.

torch.Size([1850, 2])
We keep 5.24e+04/5.21e+05 = 10% of the original kernel matrix.

torch.Size([6492, 2])
We keep 4.83e+05/9.56e+06 =  5% of the original kernel matrix.

torch.Size([574, 2])
We keep 2.37e+04/1.04e+05 = 22% of the original kernel matrix.

torch.Size([3799, 2])
We keep 2.89e+05/4.27e+06 =  6% of the original kernel matrix.

torch.Size([2315, 2])
We keep 7.84e+04/8.24e+05 =  9% of the original kernel matrix.

torch.Size([6922, 2])
We keep 5.62e+05/1.20e+07 =  4% of the original kernel matrix.

torch.Size([990, 2])
We keep 2.57e+04/1.59e+05 = 16% of the original kernel matrix.

torch.Size([5052, 2])
We keep 3.34e+05/5.29e+06 =  6% of the original kernel matrix.

torch.Size([1520, 2])
We keep 3.24e+04/3.01e+05 = 10% of the original kernel matrix.

torch.Size([5943, 2])
We keep 3.87e+05/7.27e+06 =  5% of the original kernel matrix.

torch.Size([1061, 2])
We keep 1.98e+04/1.38e+05 = 14% of the original kernel matrix.

torch.Size([5294, 2])
We keep 3.10e+05/4.93e+06 =  6% of the original kernel matrix.

torch.Size([1030, 2])
We keep 2.03e+04/1.54e+05 = 13% of the original kernel matrix.

torch.Size([5244, 2])
We keep 3.21e+05/5.19e+06 =  6% of the original kernel matrix.

torch.Size([985, 2])
We keep 1.96e+04/1.41e+05 = 13% of the original kernel matrix.

torch.Size([5050, 2])
We keep 3.16e+05/4.98e+06 =  6% of the original kernel matrix.

torch.Size([1257, 2])
We keep 3.51e+04/2.51e+05 = 13% of the original kernel matrix.

torch.Size([5456, 2])
We keep 3.76e+05/6.64e+06 =  5% of the original kernel matrix.

torch.Size([2653, 2])
We keep 1.12e+05/1.24e+06 =  9% of the original kernel matrix.

torch.Size([7316, 2])
We keep 6.45e+05/1.48e+07 =  4% of the original kernel matrix.

torch.Size([2350, 2])
We keep 1.39e+05/1.26e+06 = 11% of the original kernel matrix.

torch.Size([7149, 2])
We keep 6.73e+05/1.49e+07 =  4% of the original kernel matrix.

torch.Size([7385, 2])
We keep 1.19e+06/2.24e+07 =  5% of the original kernel matrix.

torch.Size([11850, 2])
We keep 2.01e+06/6.27e+07 =  3% of the original kernel matrix.

torch.Size([804, 2])
We keep 1.11e+04/6.81e+04 = 16% of the original kernel matrix.

torch.Size([4770, 2])
We keep 2.43e+05/3.46e+06 =  7% of the original kernel matrix.

torch.Size([803, 2])
We keep 1.21e+04/7.24e+04 = 16% of the original kernel matrix.

torch.Size([4895, 2])
We keep 2.57e+05/3.56e+06 =  7% of the original kernel matrix.

torch.Size([1419, 2])
We keep 3.25e+04/2.62e+05 = 12% of the original kernel matrix.

torch.Size([5969, 2])
We keep 3.96e+05/6.78e+06 =  5% of the original kernel matrix.

torch.Size([2388, 2])
We keep 1.10e+05/1.28e+06 =  8% of the original kernel matrix.

torch.Size([7238, 2])
We keep 6.76e+05/1.50e+07 =  4% of the original kernel matrix.

torch.Size([435, 2])
We keep 4.39e+03/2.13e+04 = 20% of the original kernel matrix.

torch.Size([3810, 2])
We keep 1.64e+05/1.93e+06 =  8% of the original kernel matrix.

torch.Size([5705, 2])
We keep 8.97e+05/8.61e+06 = 10% of the original kernel matrix.

torch.Size([10509, 2])
We keep 1.32e+06/3.89e+07 =  3% of the original kernel matrix.

torch.Size([8939, 2])
We keep 7.05e+06/6.51e+07 = 10% of the original kernel matrix.

torch.Size([12782, 2])
We keep 2.92e+06/1.07e+08 =  2% of the original kernel matrix.

torch.Size([1905, 2])
We keep 6.63e+04/6.38e+05 = 10% of the original kernel matrix.

torch.Size([6730, 2])
We keep 4.85e+05/1.06e+07 =  4% of the original kernel matrix.

torch.Size([2690, 2])
We keep 1.44e+05/1.39e+06 = 10% of the original kernel matrix.

torch.Size([7496, 2])
We keep 6.86e+05/1.56e+07 =  4% of the original kernel matrix.

torch.Size([899, 2])
We keep 2.47e+04/1.52e+05 = 16% of the original kernel matrix.

torch.Size([4721, 2])
We keep 3.12e+05/5.17e+06 =  6% of the original kernel matrix.

torch.Size([4266, 2])
We keep 2.69e+05/3.72e+06 =  7% of the original kernel matrix.

torch.Size([9073, 2])
We keep 9.74e+05/2.56e+07 =  3% of the original kernel matrix.

torch.Size([5010, 2])
We keep 4.02e+05/5.83e+06 =  6% of the original kernel matrix.

torch.Size([9754, 2])
We keep 1.16e+06/3.20e+07 =  3% of the original kernel matrix.

torch.Size([25944, 2])
We keep 9.24e+07/9.78e+08 =  9% of the original kernel matrix.

torch.Size([20995, 2])
We keep 8.81e+06/4.14e+08 =  2% of the original kernel matrix.

torch.Size([1070, 2])
We keep 2.31e+04/1.71e+05 = 13% of the original kernel matrix.

torch.Size([5366, 2])
We keep 3.35e+05/5.48e+06 =  6% of the original kernel matrix.

torch.Size([27652, 2])
We keep 9.12e+06/3.35e+08 =  2% of the original kernel matrix.

torch.Size([22944, 2])
We keep 5.56e+06/2.42e+08 =  2% of the original kernel matrix.

torch.Size([14107, 2])
We keep 3.78e+06/9.14e+07 =  4% of the original kernel matrix.

torch.Size([16611, 2])
We keep 3.43e+06/1.27e+08 =  2% of the original kernel matrix.

torch.Size([6799, 2])
We keep 1.02e+06/1.76e+07 =  5% of the original kernel matrix.

torch.Size([11389, 2])
We keep 1.84e+06/5.56e+07 =  3% of the original kernel matrix.

torch.Size([6091, 2])
We keep 5.19e+05/9.44e+06 =  5% of the original kernel matrix.

torch.Size([10871, 2])
We keep 1.37e+06/4.07e+07 =  3% of the original kernel matrix.

torch.Size([628, 2])
We keep 9.01e+03/4.80e+04 = 18% of the original kernel matrix.

torch.Size([4410, 2])
We keep 2.21e+05/2.90e+06 =  7% of the original kernel matrix.

torch.Size([618, 2])
We keep 1.19e+04/5.66e+04 = 21% of the original kernel matrix.

torch.Size([4218, 2])
We keep 2.37e+05/3.15e+06 =  7% of the original kernel matrix.

torch.Size([155033, 2])
We keep 1.78e+08/1.03e+10 =  1% of the original kernel matrix.

torch.Size([57004, 2])
We keep 2.40e+07/1.35e+09 =  1% of the original kernel matrix.

torch.Size([61307, 2])
We keep 5.15e+07/2.02e+09 =  2% of the original kernel matrix.

torch.Size([34403, 2])
We keep 1.22e+07/5.96e+08 =  2% of the original kernel matrix.

torch.Size([1544, 2])
We keep 2.05e+05/7.06e+05 = 29% of the original kernel matrix.

torch.Size([5902, 2])
We keep 5.64e+05/1.11e+07 =  5% of the original kernel matrix.

torch.Size([11372, 2])
We keep 1.72e+06/4.44e+07 =  3% of the original kernel matrix.

torch.Size([15043, 2])
We keep 2.45e+06/8.83e+07 =  2% of the original kernel matrix.

torch.Size([20385, 2])
We keep 3.00e+07/3.43e+08 =  8% of the original kernel matrix.

torch.Size([19345, 2])
We keep 5.51e+06/2.45e+08 =  2% of the original kernel matrix.

torch.Size([7835, 2])
We keep 2.26e+06/2.01e+07 = 11% of the original kernel matrix.

torch.Size([12202, 2])
We keep 1.85e+06/5.94e+07 =  3% of the original kernel matrix.

torch.Size([8784, 2])
We keep 5.72e+06/6.95e+07 =  8% of the original kernel matrix.

torch.Size([12529, 2])
We keep 2.99e+06/1.10e+08 =  2% of the original kernel matrix.

torch.Size([4507, 2])
We keep 8.16e+05/7.97e+06 = 10% of the original kernel matrix.

torch.Size([9123, 2])
We keep 1.21e+06/3.74e+07 =  3% of the original kernel matrix.

torch.Size([5821, 2])
We keep 2.48e+06/2.02e+07 = 12% of the original kernel matrix.

torch.Size([10537, 2])
We keep 1.79e+06/5.95e+07 =  3% of the original kernel matrix.

torch.Size([8000, 2])
We keep 1.94e+06/2.10e+07 =  9% of the original kernel matrix.

torch.Size([12530, 2])
We keep 1.89e+06/6.07e+07 =  3% of the original kernel matrix.

torch.Size([794, 2])
We keep 1.35e+04/8.18e+04 = 16% of the original kernel matrix.

torch.Size([4858, 2])
We keep 2.65e+05/3.79e+06 =  6% of the original kernel matrix.

torch.Size([27248, 2])
We keep 9.14e+07/1.46e+09 =  6% of the original kernel matrix.

torch.Size([20852, 2])
We keep 1.08e+07/5.07e+08 =  2% of the original kernel matrix.

torch.Size([1300, 2])
We keep 3.81e+04/3.18e+05 = 11% of the original kernel matrix.

torch.Size([5646, 2])
We keep 4.15e+05/7.47e+06 =  5% of the original kernel matrix.

torch.Size([1828, 2])
We keep 1.29e+05/7.99e+05 = 16% of the original kernel matrix.

torch.Size([6356, 2])
We keep 5.62e+05/1.18e+07 =  4% of the original kernel matrix.

torch.Size([5261, 2])
We keep 4.43e+05/6.02e+06 =  7% of the original kernel matrix.

torch.Size([10034, 2])
We keep 1.17e+06/3.25e+07 =  3% of the original kernel matrix.

torch.Size([12850, 2])
We keep 3.50e+06/9.23e+07 =  3% of the original kernel matrix.

torch.Size([15769, 2])
We keep 3.41e+06/1.27e+08 =  2% of the original kernel matrix.

torch.Size([68410, 2])
We keep 6.89e+07/2.25e+09 =  3% of the original kernel matrix.

torch.Size([36334, 2])
We keep 1.27e+07/6.29e+08 =  2% of the original kernel matrix.

torch.Size([19838, 2])
We keep 4.59e+06/1.53e+08 =  3% of the original kernel matrix.

torch.Size([20029, 2])
We keep 4.05e+06/1.64e+08 =  2% of the original kernel matrix.

torch.Size([608, 2])
We keep 6.31e+03/3.53e+04 = 17% of the original kernel matrix.

torch.Size([4315, 2])
We keep 1.95e+05/2.49e+06 =  7% of the original kernel matrix.

torch.Size([9119, 2])
We keep 1.33e+06/2.72e+07 =  4% of the original kernel matrix.

torch.Size([13059, 2])
We keep 2.04e+06/6.91e+07 =  2% of the original kernel matrix.

torch.Size([2693, 2])
We keep 1.63e+05/1.55e+06 = 10% of the original kernel matrix.

torch.Size([7470, 2])
We keep 7.11e+05/1.65e+07 =  4% of the original kernel matrix.

torch.Size([6646, 2])
We keep 1.60e+06/2.30e+07 =  6% of the original kernel matrix.

torch.Size([11222, 2])
We keep 1.89e+06/6.35e+07 =  2% of the original kernel matrix.

torch.Size([25061, 2])
We keep 5.85e+06/2.43e+08 =  2% of the original kernel matrix.

torch.Size([22305, 2])
We keep 4.80e+06/2.07e+08 =  2% of the original kernel matrix.

torch.Size([7788, 2])
We keep 2.54e+06/4.19e+07 =  6% of the original kernel matrix.

torch.Size([11717, 2])
We keep 2.37e+06/8.57e+07 =  2% of the original kernel matrix.

torch.Size([13082, 2])
We keep 3.05e+06/6.66e+07 =  4% of the original kernel matrix.

torch.Size([15925, 2])
We keep 2.94e+06/1.08e+08 =  2% of the original kernel matrix.

torch.Size([246, 2])
We keep 1.72e+03/6.40e+03 = 26% of the original kernel matrix.

torch.Size([3338, 2])
We keep 1.22e+05/1.06e+06 = 11% of the original kernel matrix.

torch.Size([1742, 2])
We keep 5.64e+04/5.37e+05 = 10% of the original kernel matrix.

torch.Size([6422, 2])
We keep 4.93e+05/9.71e+06 =  5% of the original kernel matrix.

torch.Size([13962, 2])
We keep 3.85e+06/1.05e+08 =  3% of the original kernel matrix.

torch.Size([16668, 2])
We keep 3.64e+06/1.36e+08 =  2% of the original kernel matrix.

torch.Size([10215, 2])
We keep 3.22e+06/4.98e+07 =  6% of the original kernel matrix.

torch.Size([13923, 2])
We keep 2.62e+06/9.35e+07 =  2% of the original kernel matrix.

torch.Size([10840, 2])
We keep 5.18e+06/7.29e+07 =  7% of the original kernel matrix.

torch.Size([14104, 2])
We keep 3.05e+06/1.13e+08 =  2% of the original kernel matrix.

torch.Size([1569, 2])
We keep 5.15e+04/4.75e+05 = 10% of the original kernel matrix.

torch.Size([6115, 2])
We keep 4.78e+05/9.13e+06 =  5% of the original kernel matrix.

torch.Size([73039, 2])
We keep 4.75e+07/2.80e+09 =  1% of the original kernel matrix.

torch.Size([38137, 2])
We keep 1.35e+07/7.00e+08 =  1% of the original kernel matrix.

torch.Size([10975, 2])
We keep 1.62e+06/4.34e+07 =  3% of the original kernel matrix.

torch.Size([14632, 2])
We keep 2.46e+06/8.73e+07 =  2% of the original kernel matrix.

torch.Size([8565, 2])
We keep 9.45e+05/2.20e+07 =  4% of the original kernel matrix.

torch.Size([12854, 2])
We keep 1.88e+06/6.21e+07 =  3% of the original kernel matrix.

torch.Size([49079, 2])
We keep 2.93e+07/1.07e+09 =  2% of the original kernel matrix.

torch.Size([30415, 2])
We keep 9.04e+06/4.33e+08 =  2% of the original kernel matrix.

torch.Size([1188, 2])
We keep 3.13e+04/2.49e+05 = 12% of the original kernel matrix.

torch.Size([5485, 2])
We keep 3.84e+05/6.61e+06 =  5% of the original kernel matrix.

torch.Size([12778, 2])
We keep 2.64e+06/5.76e+07 =  4% of the original kernel matrix.

torch.Size([15795, 2])
We keep 2.73e+06/1.01e+08 =  2% of the original kernel matrix.

torch.Size([41700, 2])
We keep 5.44e+07/9.94e+08 =  5% of the original kernel matrix.

torch.Size([28066, 2])
We keep 9.13e+06/4.18e+08 =  2% of the original kernel matrix.

torch.Size([141510, 2])
We keep 2.65e+08/9.68e+09 =  2% of the original kernel matrix.

torch.Size([53416, 2])
We keep 2.34e+07/1.30e+09 =  1% of the original kernel matrix.

torch.Size([8341, 2])
We keep 1.25e+06/2.50e+07 =  5% of the original kernel matrix.

torch.Size([12519, 2])
We keep 2.01e+06/6.63e+07 =  3% of the original kernel matrix.

torch.Size([10020, 2])
We keep 1.40e+06/3.16e+07 =  4% of the original kernel matrix.

torch.Size([13831, 2])
We keep 2.18e+06/7.44e+07 =  2% of the original kernel matrix.

torch.Size([2321, 2])
We keep 1.27e+05/9.33e+05 = 13% of the original kernel matrix.

torch.Size([6979, 2])
We keep 6.04e+05/1.28e+07 =  4% of the original kernel matrix.

torch.Size([2877, 2])
We keep 1.12e+05/1.40e+06 =  8% of the original kernel matrix.

torch.Size([7919, 2])
We keep 6.92e+05/1.57e+07 =  4% of the original kernel matrix.

torch.Size([28929, 2])
We keep 2.19e+07/4.79e+08 =  4% of the original kernel matrix.

torch.Size([23149, 2])
We keep 6.22e+06/2.90e+08 =  2% of the original kernel matrix.

torch.Size([3374, 2])
We keep 1.92e+05/2.54e+06 =  7% of the original kernel matrix.

torch.Size([8240, 2])
We keep 8.18e+05/2.11e+07 =  3% of the original kernel matrix.

torch.Size([7143, 2])
We keep 9.73e+05/1.69e+07 =  5% of the original kernel matrix.

torch.Size([11805, 2])
We keep 1.72e+06/5.44e+07 =  3% of the original kernel matrix.

torch.Size([16252, 2])
We keep 7.60e+06/1.52e+08 =  4% of the original kernel matrix.

torch.Size([17926, 2])
We keep 3.67e+06/1.64e+08 =  2% of the original kernel matrix.

torch.Size([8961, 2])
We keep 2.08e+06/2.66e+07 =  7% of the original kernel matrix.

torch.Size([13261, 2])
We keep 2.03e+06/6.83e+07 =  2% of the original kernel matrix.

torch.Size([5923, 2])
We keep 2.01e+06/1.46e+07 = 13% of the original kernel matrix.

torch.Size([10620, 2])
We keep 1.63e+06/5.06e+07 =  3% of the original kernel matrix.

torch.Size([9710, 2])
We keep 1.99e+06/4.41e+07 =  4% of the original kernel matrix.

torch.Size([13783, 2])
We keep 2.64e+06/8.80e+07 =  2% of the original kernel matrix.

torch.Size([91935, 2])
We keep 2.33e+08/9.32e+09 =  2% of the original kernel matrix.

torch.Size([41310, 2])
We keep 2.29e+07/1.28e+09 =  1% of the original kernel matrix.

torch.Size([2662, 2])
We keep 1.80e+05/1.78e+06 = 10% of the original kernel matrix.

torch.Size([7666, 2])
We keep 7.21e+05/1.77e+07 =  4% of the original kernel matrix.

torch.Size([4812, 2])
We keep 8.18e+05/5.80e+06 = 14% of the original kernel matrix.

torch.Size([9768, 2])
We keep 1.15e+06/3.19e+07 =  3% of the original kernel matrix.

torch.Size([4394, 2])
We keep 3.62e+05/4.26e+06 =  8% of the original kernel matrix.

torch.Size([9340, 2])
We keep 1.04e+06/2.73e+07 =  3% of the original kernel matrix.

torch.Size([23475, 2])
We keep 2.50e+07/5.16e+08 =  4% of the original kernel matrix.

torch.Size([20639, 2])
We keep 6.79e+06/3.01e+08 =  2% of the original kernel matrix.

torch.Size([3911, 2])
We keep 2.25e+05/3.06e+06 =  7% of the original kernel matrix.

torch.Size([8876, 2])
We keep 9.06e+05/2.32e+07 =  3% of the original kernel matrix.

torch.Size([14813, 2])
We keep 9.48e+06/1.13e+08 =  8% of the original kernel matrix.

torch.Size([16660, 2])
We keep 3.42e+06/1.41e+08 =  2% of the original kernel matrix.

torch.Size([2299, 2])
We keep 5.24e+05/2.18e+06 = 24% of the original kernel matrix.

torch.Size([6885, 2])
We keep 8.35e+05/1.95e+07 =  4% of the original kernel matrix.

torch.Size([8479, 2])
We keep 4.49e+06/4.81e+07 =  9% of the original kernel matrix.

torch.Size([12239, 2])
We keep 2.62e+06/9.18e+07 =  2% of the original kernel matrix.

torch.Size([4952, 2])
We keep 4.41e+05/6.85e+06 =  6% of the original kernel matrix.

torch.Size([9978, 2])
We keep 1.19e+06/3.47e+07 =  3% of the original kernel matrix.

torch.Size([76565, 2])
We keep 1.31e+08/4.16e+09 =  3% of the original kernel matrix.

torch.Size([37355, 2])
We keep 1.61e+07/8.55e+08 =  1% of the original kernel matrix.

torch.Size([13163, 2])
We keep 3.49e+06/1.12e+08 =  3% of the original kernel matrix.

torch.Size([15799, 2])
We keep 3.54e+06/1.40e+08 =  2% of the original kernel matrix.

torch.Size([2964, 2])
We keep 1.39e+05/1.75e+06 =  7% of the original kernel matrix.

torch.Size([7978, 2])
We keep 7.51e+05/1.75e+07 =  4% of the original kernel matrix.

torch.Size([46705, 2])
We keep 1.76e+07/8.13e+08 =  2% of the original kernel matrix.

torch.Size([30070, 2])
We keep 8.05e+06/3.78e+08 =  2% of the original kernel matrix.

torch.Size([40874, 2])
We keep 1.40e+07/6.15e+08 =  2% of the original kernel matrix.

torch.Size([28047, 2])
We keep 7.16e+06/3.29e+08 =  2% of the original kernel matrix.

torch.Size([1671, 2])
We keep 6.17e+04/5.58e+05 = 11% of the original kernel matrix.

torch.Size([6383, 2])
We keep 5.02e+05/9.89e+06 =  5% of the original kernel matrix.

torch.Size([21890, 2])
We keep 8.22e+06/1.93e+08 =  4% of the original kernel matrix.

torch.Size([20863, 2])
We keep 4.39e+06/1.84e+08 =  2% of the original kernel matrix.

torch.Size([47321, 2])
We keep 1.95e+07/9.30e+08 =  2% of the original kernel matrix.

torch.Size([30249, 2])
We keep 8.48e+06/4.04e+08 =  2% of the original kernel matrix.

torch.Size([100236, 2])
We keep 1.34e+08/4.55e+09 =  2% of the original kernel matrix.

torch.Size([44887, 2])
We keep 1.69e+07/8.93e+08 =  1% of the original kernel matrix.

torch.Size([4690, 2])
We keep 1.86e+06/9.73e+06 = 19% of the original kernel matrix.

torch.Size([9692, 2])
We keep 1.43e+06/4.13e+07 =  3% of the original kernel matrix.

torch.Size([2353, 2])
We keep 1.12e+05/1.03e+06 = 10% of the original kernel matrix.

torch.Size([7157, 2])
We keep 6.18e+05/1.35e+07 =  4% of the original kernel matrix.

torch.Size([3642, 2])
We keep 2.15e+05/2.76e+06 =  7% of the original kernel matrix.

torch.Size([8650, 2])
We keep 8.69e+05/2.20e+07 =  3% of the original kernel matrix.

torch.Size([3744, 2])
We keep 2.74e+05/3.82e+06 =  7% of the original kernel matrix.

torch.Size([8627, 2])
We keep 9.75e+05/2.59e+07 =  3% of the original kernel matrix.

torch.Size([6034, 2])
We keep 4.78e+05/8.57e+06 =  5% of the original kernel matrix.

torch.Size([10824, 2])
We keep 1.32e+06/3.88e+07 =  3% of the original kernel matrix.

torch.Size([5259, 2])
We keep 4.01e+05/5.74e+06 =  6% of the original kernel matrix.

torch.Size([10194, 2])
We keep 1.11e+06/3.17e+07 =  3% of the original kernel matrix.

torch.Size([547, 2])
We keep 7.44e+03/3.57e+04 = 20% of the original kernel matrix.

torch.Size([4229, 2])
We keep 2.04e+05/2.50e+06 =  8% of the original kernel matrix.

torch.Size([20516, 2])
We keep 6.42e+06/1.64e+08 =  3% of the original kernel matrix.

torch.Size([20336, 2])
We keep 4.20e+06/1.70e+08 =  2% of the original kernel matrix.

torch.Size([31685, 2])
We keep 1.50e+07/5.96e+08 =  2% of the original kernel matrix.

torch.Size([24937, 2])
We keep 7.34e+06/3.23e+08 =  2% of the original kernel matrix.

torch.Size([2188, 2])
We keep 2.34e+06/1.10e+07 = 21% of the original kernel matrix.

torch.Size([5827, 2])
We keep 1.47e+06/4.39e+07 =  3% of the original kernel matrix.

torch.Size([2319, 2])
We keep 1.62e+05/1.14e+06 = 14% of the original kernel matrix.

torch.Size([7054, 2])
We keep 6.62e+05/1.41e+07 =  4% of the original kernel matrix.

torch.Size([11956, 2])
We keep 2.69e+06/5.36e+07 =  5% of the original kernel matrix.

torch.Size([15053, 2])
We keep 2.66e+06/9.69e+07 =  2% of the original kernel matrix.

torch.Size([3350, 2])
We keep 2.15e+05/2.54e+06 =  8% of the original kernel matrix.

torch.Size([8126, 2])
We keep 8.45e+05/2.11e+07 =  4% of the original kernel matrix.

torch.Size([3807, 2])
We keep 4.82e+05/5.05e+06 =  9% of the original kernel matrix.

torch.Size([8463, 2])
We keep 1.10e+06/2.98e+07 =  3% of the original kernel matrix.

torch.Size([4838, 2])
We keep 3.95e+05/6.00e+06 =  6% of the original kernel matrix.

torch.Size([9734, 2])
We keep 1.18e+06/3.25e+07 =  3% of the original kernel matrix.

torch.Size([1278, 2])
We keep 2.77e+04/2.27e+05 = 12% of the original kernel matrix.

torch.Size([5851, 2])
We keep 3.63e+05/6.31e+06 =  5% of the original kernel matrix.

torch.Size([22917, 2])
We keep 1.33e+07/3.11e+08 =  4% of the original kernel matrix.

torch.Size([20615, 2])
We keep 5.32e+06/2.33e+08 =  2% of the original kernel matrix.

torch.Size([11642, 2])
We keep 1.44e+06/4.04e+07 =  3% of the original kernel matrix.

torch.Size([15161, 2])
We keep 2.35e+06/8.42e+07 =  2% of the original kernel matrix.

torch.Size([6880, 2])
We keep 8.24e+05/1.15e+07 =  7% of the original kernel matrix.

torch.Size([11554, 2])
We keep 1.49e+06/4.49e+07 =  3% of the original kernel matrix.

torch.Size([23557, 2])
We keep 1.22e+07/3.13e+08 =  3% of the original kernel matrix.

torch.Size([20912, 2])
We keep 5.45e+06/2.34e+08 =  2% of the original kernel matrix.

torch.Size([1888, 2])
We keep 1.03e+05/7.43e+05 = 13% of the original kernel matrix.

torch.Size([6608, 2])
We keep 5.59e+05/1.14e+07 =  4% of the original kernel matrix.

torch.Size([85264, 2])
We keep 7.76e+07/3.43e+09 =  2% of the original kernel matrix.

torch.Size([41157, 2])
We keep 1.48e+07/7.75e+08 =  1% of the original kernel matrix.

torch.Size([2880, 2])
We keep 1.88e+05/1.97e+06 =  9% of the original kernel matrix.

torch.Size([7679, 2])
We keep 7.83e+05/1.86e+07 =  4% of the original kernel matrix.

torch.Size([19068, 2])
We keep 4.31e+06/1.37e+08 =  3% of the original kernel matrix.

torch.Size([19393, 2])
We keep 3.90e+06/1.55e+08 =  2% of the original kernel matrix.

torch.Size([2450, 2])
We keep 1.18e+05/1.21e+06 =  9% of the original kernel matrix.

torch.Size([7086, 2])
We keep 6.39e+05/1.46e+07 =  4% of the original kernel matrix.

torch.Size([1725, 2])
We keep 5.69e+04/5.18e+05 = 10% of the original kernel matrix.

torch.Size([6251, 2])
We keep 4.91e+05/9.54e+06 =  5% of the original kernel matrix.

torch.Size([1221, 2])
We keep 2.56e+04/2.14e+05 = 11% of the original kernel matrix.

torch.Size([5563, 2])
We keep 3.61e+05/6.13e+06 =  5% of the original kernel matrix.

torch.Size([4878, 2])
We keep 3.11e+05/5.12e+06 =  6% of the original kernel matrix.

torch.Size([9906, 2])
We keep 1.04e+06/3.00e+07 =  3% of the original kernel matrix.

torch.Size([1597, 2])
We keep 4.60e+04/4.04e+05 = 11% of the original kernel matrix.

torch.Size([6160, 2])
We keep 4.48e+05/8.42e+06 =  5% of the original kernel matrix.

torch.Size([10043, 2])
We keep 6.54e+06/4.10e+07 = 15% of the original kernel matrix.

torch.Size([14019, 2])
We keep 2.43e+06/8.49e+07 =  2% of the original kernel matrix.

torch.Size([1952, 2])
We keep 6.64e+04/6.05e+05 = 10% of the original kernel matrix.

torch.Size([6629, 2])
We keep 5.28e+05/1.03e+07 =  5% of the original kernel matrix.

torch.Size([2197, 2])
We keep 8.51e+04/8.70e+05 =  9% of the original kernel matrix.

torch.Size([6915, 2])
We keep 5.85e+05/1.24e+07 =  4% of the original kernel matrix.

torch.Size([63571, 2])
We keep 3.55e+07/1.57e+09 =  2% of the original kernel matrix.

torch.Size([34760, 2])
We keep 1.06e+07/5.25e+08 =  2% of the original kernel matrix.

torch.Size([16957, 2])
We keep 3.83e+06/1.17e+08 =  3% of the original kernel matrix.

torch.Size([18447, 2])
We keep 3.69e+06/1.43e+08 =  2% of the original kernel matrix.

torch.Size([13702, 2])
We keep 2.63e+06/7.06e+07 =  3% of the original kernel matrix.

torch.Size([16389, 2])
We keep 3.00e+06/1.11e+08 =  2% of the original kernel matrix.

torch.Size([44291, 2])
We keep 1.85e+07/7.39e+08 =  2% of the original kernel matrix.

torch.Size([29018, 2])
We keep 7.77e+06/3.60e+08 =  2% of the original kernel matrix.

torch.Size([127602, 2])
We keep 1.09e+08/6.72e+09 =  1% of the original kernel matrix.

torch.Size([51254, 2])
We keep 1.99e+07/1.09e+09 =  1% of the original kernel matrix.

torch.Size([2566, 2])
We keep 1.32e+05/1.37e+06 =  9% of the original kernel matrix.

torch.Size([7514, 2])
We keep 7.02e+05/1.55e+07 =  4% of the original kernel matrix.

torch.Size([2118, 2])
We keep 6.48e+04/6.89e+05 =  9% of the original kernel matrix.

torch.Size([7045, 2])
We keep 5.27e+05/1.10e+07 =  4% of the original kernel matrix.

torch.Size([4308, 2])
We keep 1.08e+06/8.95e+06 = 12% of the original kernel matrix.

torch.Size([8942, 2])
We keep 1.35e+06/3.96e+07 =  3% of the original kernel matrix.

torch.Size([1331, 2])
We keep 2.99e+04/2.48e+05 = 12% of the original kernel matrix.

torch.Size([5752, 2])
We keep 3.81e+05/6.60e+06 =  5% of the original kernel matrix.

torch.Size([2488, 2])
We keep 8.99e+04/9.58e+05 =  9% of the original kernel matrix.

torch.Size([7238, 2])
We keep 6.05e+05/1.30e+07 =  4% of the original kernel matrix.

torch.Size([5306, 2])
We keep 6.56e+05/7.10e+06 =  9% of the original kernel matrix.

torch.Size([10150, 2])
We keep 1.25e+06/3.53e+07 =  3% of the original kernel matrix.

torch.Size([2655, 2])
We keep 1.27e+05/1.34e+06 =  9% of the original kernel matrix.

torch.Size([7384, 2])
We keep 6.69e+05/1.54e+07 =  4% of the original kernel matrix.

torch.Size([1651, 2])
We keep 5.11e+04/4.21e+05 = 12% of the original kernel matrix.

torch.Size([6297, 2])
We keep 4.70e+05/8.60e+06 =  5% of the original kernel matrix.

torch.Size([1313, 2])
We keep 3.80e+04/3.18e+05 = 11% of the original kernel matrix.

torch.Size([5718, 2])
We keep 4.13e+05/7.47e+06 =  5% of the original kernel matrix.

torch.Size([3659, 2])
We keep 2.95e+05/3.50e+06 =  8% of the original kernel matrix.

torch.Size([8469, 2])
We keep 9.55e+05/2.48e+07 =  3% of the original kernel matrix.

torch.Size([144644, 2])
We keep 2.20e+08/1.02e+10 =  2% of the original kernel matrix.

torch.Size([54253, 2])
We keep 2.40e+07/1.34e+09 =  1% of the original kernel matrix.

torch.Size([1703, 2])
We keep 3.22e+05/1.95e+06 = 16% of the original kernel matrix.

torch.Size([5483, 2])
We keep 7.88e+05/1.85e+07 =  4% of the original kernel matrix.

torch.Size([30301, 2])
We keep 3.35e+07/4.94e+08 =  6% of the original kernel matrix.

torch.Size([23828, 2])
We keep 6.68e+06/2.94e+08 =  2% of the original kernel matrix.

torch.Size([3098, 2])
We keep 2.75e+05/3.19e+06 =  8% of the original kernel matrix.

torch.Size([7856, 2])
We keep 9.31e+05/2.37e+07 =  3% of the original kernel matrix.

torch.Size([2119, 2])
We keep 1.31e+05/1.13e+06 = 11% of the original kernel matrix.

torch.Size([6659, 2])
We keep 6.52e+05/1.41e+07 =  4% of the original kernel matrix.

torch.Size([10780, 2])
We keep 1.65e+06/3.81e+07 =  4% of the original kernel matrix.

torch.Size([14355, 2])
We keep 2.32e+06/8.18e+07 =  2% of the original kernel matrix.

torch.Size([37152, 2])
We keep 4.44e+07/8.09e+08 =  5% of the original kernel matrix.

torch.Size([26663, 2])
We keep 7.26e+06/3.77e+08 =  1% of the original kernel matrix.

torch.Size([115685, 2])
We keep 9.07e+07/5.90e+09 =  1% of the original kernel matrix.

torch.Size([48706, 2])
We keep 1.86e+07/1.02e+09 =  1% of the original kernel matrix.

torch.Size([4990, 2])
We keep 3.53e+05/5.84e+06 =  6% of the original kernel matrix.

torch.Size([10067, 2])
We keep 1.12e+06/3.20e+07 =  3% of the original kernel matrix.

torch.Size([11886, 2])
We keep 3.41e+06/5.48e+07 =  6% of the original kernel matrix.

torch.Size([15303, 2])
We keep 2.74e+06/9.80e+07 =  2% of the original kernel matrix.

torch.Size([1723, 2])
We keep 5.21e+04/5.30e+05 =  9% of the original kernel matrix.

torch.Size([6546, 2])
We keep 4.96e+05/9.64e+06 =  5% of the original kernel matrix.

torch.Size([3226, 2])
We keep 3.28e+05/3.07e+06 = 10% of the original kernel matrix.

torch.Size([7895, 2])
We keep 9.25e+05/2.32e+07 =  3% of the original kernel matrix.

torch.Size([313695, 2])
We keep 1.27e+09/6.55e+10 =  1% of the original kernel matrix.

torch.Size([78996, 2])
We keep 5.49e+07/3.39e+09 =  1% of the original kernel matrix.

torch.Size([6598, 2])
We keep 1.63e+06/1.83e+07 =  8% of the original kernel matrix.

torch.Size([11426, 2])
We keep 1.83e+06/5.66e+07 =  3% of the original kernel matrix.

torch.Size([3658, 2])
We keep 2.00e+05/2.65e+06 =  7% of the original kernel matrix.

torch.Size([8595, 2])
We keep 8.59e+05/2.16e+07 =  3% of the original kernel matrix.

torch.Size([13128, 2])
We keep 5.30e+06/9.37e+07 =  5% of the original kernel matrix.

torch.Size([16013, 2])
We keep 3.42e+06/1.28e+08 =  2% of the original kernel matrix.

torch.Size([119268, 2])
We keep 1.23e+08/6.57e+09 =  1% of the original kernel matrix.

torch.Size([49050, 2])
We keep 2.00e+07/1.07e+09 =  1% of the original kernel matrix.

torch.Size([12645, 2])
We keep 2.21e+06/5.89e+07 =  3% of the original kernel matrix.

torch.Size([15635, 2])
We keep 2.78e+06/1.02e+08 =  2% of the original kernel matrix.

torch.Size([2831, 2])
We keep 2.27e+05/2.01e+06 = 11% of the original kernel matrix.

torch.Size([7699, 2])
We keep 8.26e+05/1.88e+07 =  4% of the original kernel matrix.

torch.Size([2898, 2])
We keep 5.14e+05/2.69e+06 = 19% of the original kernel matrix.

torch.Size([7598, 2])
We keep 8.69e+05/2.17e+07 =  4% of the original kernel matrix.

torch.Size([4559, 2])
We keep 3.15e+05/5.12e+06 =  6% of the original kernel matrix.

torch.Size([9455, 2])
We keep 1.09e+06/3.00e+07 =  3% of the original kernel matrix.

torch.Size([1334, 2])
We keep 1.25e+05/5.75e+05 = 21% of the original kernel matrix.

torch.Size([5461, 2])
We keep 4.86e+05/1.00e+07 =  4% of the original kernel matrix.

torch.Size([35486, 2])
We keep 2.38e+07/6.97e+08 =  3% of the original kernel matrix.

torch.Size([25841, 2])
We keep 7.65e+06/3.50e+08 =  2% of the original kernel matrix.

torch.Size([5597, 2])
We keep 8.96e+05/8.32e+06 = 10% of the original kernel matrix.

torch.Size([10212, 2])
We keep 1.30e+06/3.82e+07 =  3% of the original kernel matrix.

torch.Size([1573, 2])
We keep 4.53e+04/4.26e+05 = 10% of the original kernel matrix.

torch.Size([6124, 2])
We keep 4.58e+05/8.65e+06 =  5% of the original kernel matrix.

torch.Size([8454, 2])
We keep 1.10e+06/2.16e+07 =  5% of the original kernel matrix.

torch.Size([12846, 2])
We keep 1.84e+06/6.16e+07 =  2% of the original kernel matrix.

torch.Size([6811, 2])
We keep 7.74e+05/1.30e+07 =  5% of the original kernel matrix.

torch.Size([11377, 2])
We keep 1.54e+06/4.78e+07 =  3% of the original kernel matrix.

torch.Size([1495, 2])
We keep 3.69e+04/3.20e+05 = 11% of the original kernel matrix.

torch.Size([5934, 2])
We keep 4.06e+05/7.50e+06 =  5% of the original kernel matrix.

torch.Size([29007, 2])
We keep 9.68e+06/3.35e+08 =  2% of the original kernel matrix.

torch.Size([23857, 2])
We keep 5.64e+06/2.43e+08 =  2% of the original kernel matrix.

torch.Size([18405, 2])
We keep 7.51e+06/1.41e+08 =  5% of the original kernel matrix.

torch.Size([19040, 2])
We keep 3.99e+06/1.57e+08 =  2% of the original kernel matrix.

torch.Size([31433, 2])
We keep 6.29e+07/9.06e+08 =  6% of the original kernel matrix.

torch.Size([24159, 2])
We keep 8.82e+06/3.99e+08 =  2% of the original kernel matrix.

torch.Size([32470, 2])
We keep 1.67e+07/5.24e+08 =  3% of the original kernel matrix.

torch.Size([25043, 2])
We keep 6.85e+06/3.03e+08 =  2% of the original kernel matrix.

torch.Size([60150, 2])
We keep 4.23e+07/1.49e+09 =  2% of the original kernel matrix.

torch.Size([33596, 2])
We keep 1.04e+07/5.12e+08 =  2% of the original kernel matrix.

torch.Size([8339, 2])
We keep 1.30e+06/2.17e+07 =  5% of the original kernel matrix.

torch.Size([12514, 2])
We keep 1.85e+06/6.17e+07 =  3% of the original kernel matrix.

torch.Size([13146, 2])
We keep 3.14e+06/7.01e+07 =  4% of the original kernel matrix.

torch.Size([15959, 2])
We keep 3.01e+06/1.11e+08 =  2% of the original kernel matrix.

torch.Size([22737, 2])
We keep 5.85e+06/2.09e+08 =  2% of the original kernel matrix.

torch.Size([21362, 2])
We keep 4.53e+06/1.91e+08 =  2% of the original kernel matrix.

torch.Size([27569, 2])
We keep 8.57e+06/3.23e+08 =  2% of the original kernel matrix.

torch.Size([23344, 2])
We keep 5.43e+06/2.38e+08 =  2% of the original kernel matrix.

torch.Size([1587, 2])
We keep 4.89e+04/3.77e+05 = 12% of the original kernel matrix.

torch.Size([6215, 2])
We keep 4.37e+05/8.13e+06 =  5% of the original kernel matrix.

torch.Size([19316, 2])
We keep 1.04e+07/2.85e+08 =  3% of the original kernel matrix.

torch.Size([19306, 2])
We keep 4.74e+06/2.24e+08 =  2% of the original kernel matrix.

torch.Size([33854, 2])
We keep 1.84e+07/5.59e+08 =  3% of the original kernel matrix.

torch.Size([25574, 2])
We keep 6.62e+06/3.13e+08 =  2% of the original kernel matrix.

torch.Size([21195, 2])
We keep 8.05e+06/1.87e+08 =  4% of the original kernel matrix.

torch.Size([20436, 2])
We keep 4.28e+06/1.81e+08 =  2% of the original kernel matrix.

torch.Size([18874, 2])
We keep 7.37e+06/1.58e+08 =  4% of the original kernel matrix.

torch.Size([19413, 2])
We keep 4.05e+06/1.66e+08 =  2% of the original kernel matrix.

torch.Size([10791, 2])
We keep 1.04e+07/6.79e+07 = 15% of the original kernel matrix.

torch.Size([14185, 2])
We keep 2.98e+06/1.09e+08 =  2% of the original kernel matrix.

torch.Size([8886, 2])
We keep 9.62e+05/2.10e+07 =  4% of the original kernel matrix.

torch.Size([12882, 2])
We keep 1.84e+06/6.07e+07 =  3% of the original kernel matrix.

torch.Size([1890, 2])
We keep 7.20e+04/6.82e+05 = 10% of the original kernel matrix.

torch.Size([6571, 2])
We keep 5.44e+05/1.09e+07 =  4% of the original kernel matrix.

torch.Size([10202, 2])
We keep 5.57e+06/7.46e+07 =  7% of the original kernel matrix.

torch.Size([13628, 2])
We keep 2.96e+06/1.14e+08 =  2% of the original kernel matrix.

torch.Size([41169, 2])
We keep 2.06e+07/6.94e+08 =  2% of the original kernel matrix.

torch.Size([28209, 2])
We keep 7.60e+06/3.49e+08 =  2% of the original kernel matrix.

torch.Size([37647, 2])
We keep 1.23e+07/5.40e+08 =  2% of the original kernel matrix.

torch.Size([27000, 2])
We keep 6.71e+06/3.08e+08 =  2% of the original kernel matrix.

torch.Size([54542, 2])
We keep 8.35e+07/2.19e+09 =  3% of the original kernel matrix.

torch.Size([31992, 2])
We keep 1.24e+07/6.20e+08 =  2% of the original kernel matrix.

torch.Size([52034, 2])
We keep 5.01e+07/1.45e+09 =  3% of the original kernel matrix.

torch.Size([31024, 2])
We keep 1.04e+07/5.04e+08 =  2% of the original kernel matrix.

torch.Size([17266, 2])
We keep 6.08e+06/1.48e+08 =  4% of the original kernel matrix.

torch.Size([18234, 2])
We keep 3.92e+06/1.61e+08 =  2% of the original kernel matrix.

torch.Size([5495, 2])
We keep 5.74e+05/1.06e+07 =  5% of the original kernel matrix.

torch.Size([10438, 2])
We keep 1.46e+06/4.32e+07 =  3% of the original kernel matrix.

torch.Size([88295, 2])
We keep 9.63e+07/4.38e+09 =  2% of the original kernel matrix.

torch.Size([41885, 2])
We keep 1.68e+07/8.76e+08 =  1% of the original kernel matrix.

torch.Size([7056, 2])
We keep 7.48e+05/1.28e+07 =  5% of the original kernel matrix.

torch.Size([11479, 2])
We keep 1.53e+06/4.73e+07 =  3% of the original kernel matrix.

torch.Size([3121, 2])
We keep 2.53e+05/2.45e+06 = 10% of the original kernel matrix.

torch.Size([7778, 2])
We keep 8.15e+05/2.07e+07 =  3% of the original kernel matrix.

torch.Size([83960, 2])
We keep 5.87e+07/2.94e+09 =  1% of the original kernel matrix.

torch.Size([40653, 2])
We keep 1.40e+07/7.18e+08 =  1% of the original kernel matrix.

torch.Size([5296, 2])
We keep 3.40e+05/5.56e+06 =  6% of the original kernel matrix.

torch.Size([10184, 2])
We keep 1.10e+06/3.12e+07 =  3% of the original kernel matrix.

torch.Size([991, 2])
We keep 2.50e+04/1.85e+05 = 13% of the original kernel matrix.

torch.Size([5046, 2])
We keep 3.41e+05/5.70e+06 =  5% of the original kernel matrix.

torch.Size([8916, 2])
We keep 1.09e+06/2.64e+07 =  4% of the original kernel matrix.

torch.Size([13225, 2])
We keep 1.99e+06/6.80e+07 =  2% of the original kernel matrix.

torch.Size([72288, 2])
We keep 7.76e+07/2.96e+09 =  2% of the original kernel matrix.

torch.Size([37566, 2])
We keep 1.41e+07/7.21e+08 =  1% of the original kernel matrix.

torch.Size([8374, 2])
We keep 1.07e+06/1.99e+07 =  5% of the original kernel matrix.

torch.Size([12571, 2])
We keep 1.84e+06/5.91e+07 =  3% of the original kernel matrix.

torch.Size([20418, 2])
We keep 8.10e+06/1.77e+08 =  4% of the original kernel matrix.

torch.Size([20214, 2])
We keep 4.30e+06/1.76e+08 =  2% of the original kernel matrix.

torch.Size([780, 2])
We keep 1.13e+04/6.50e+04 = 17% of the original kernel matrix.

torch.Size([4710, 2])
We keep 2.40e+05/3.38e+06 =  7% of the original kernel matrix.

torch.Size([1073, 2])
We keep 1.74e+04/1.24e+05 = 14% of the original kernel matrix.

torch.Size([5394, 2])
We keep 3.01e+05/4.66e+06 =  6% of the original kernel matrix.

torch.Size([2777, 2])
We keep 1.34e+05/1.45e+06 =  9% of the original kernel matrix.

torch.Size([7551, 2])
We keep 6.95e+05/1.59e+07 =  4% of the original kernel matrix.

torch.Size([3359, 2])
We keep 6.82e+05/7.16e+06 =  9% of the original kernel matrix.

torch.Size([7762, 2])
We keep 1.23e+06/3.54e+07 =  3% of the original kernel matrix.

torch.Size([10698, 2])
We keep 1.60e+06/3.59e+07 =  4% of the original kernel matrix.

torch.Size([14425, 2])
We keep 2.29e+06/7.94e+07 =  2% of the original kernel matrix.

torch.Size([18235, 2])
We keep 7.06e+06/1.38e+08 =  5% of the original kernel matrix.

torch.Size([18955, 2])
We keep 3.91e+06/1.56e+08 =  2% of the original kernel matrix.

torch.Size([1107, 2])
We keep 2.29e+04/1.76e+05 = 12% of the original kernel matrix.

torch.Size([5454, 2])
We keep 3.36e+05/5.56e+06 =  6% of the original kernel matrix.

torch.Size([25619, 2])
We keep 1.02e+07/2.95e+08 =  3% of the original kernel matrix.

torch.Size([22377, 2])
We keep 5.29e+06/2.28e+08 =  2% of the original kernel matrix.

torch.Size([2071, 2])
We keep 9.63e+04/8.28e+05 = 11% of the original kernel matrix.

torch.Size([6794, 2])
We keep 5.71e+05/1.21e+07 =  4% of the original kernel matrix.

torch.Size([17307, 2])
We keep 5.12e+06/1.38e+08 =  3% of the original kernel matrix.

torch.Size([18528, 2])
We keep 3.89e+06/1.56e+08 =  2% of the original kernel matrix.

torch.Size([2939, 2])
We keep 1.74e+05/1.91e+06 =  9% of the original kernel matrix.

torch.Size([7715, 2])
We keep 7.31e+05/1.83e+07 =  3% of the original kernel matrix.

torch.Size([24386, 2])
We keep 6.62e+06/2.38e+08 =  2% of the original kernel matrix.

torch.Size([22192, 2])
We keep 4.85e+06/2.04e+08 =  2% of the original kernel matrix.

torch.Size([2526, 2])
We keep 2.69e+05/1.92e+06 = 14% of the original kernel matrix.

torch.Size([7218, 2])
We keep 7.78e+05/1.83e+07 =  4% of the original kernel matrix.

torch.Size([7086, 2])
We keep 7.81e+05/1.34e+07 =  5% of the original kernel matrix.

torch.Size([11675, 2])
We keep 1.53e+06/4.84e+07 =  3% of the original kernel matrix.

torch.Size([6384, 2])
We keep 5.14e+05/9.11e+06 =  5% of the original kernel matrix.

torch.Size([11050, 2])
We keep 1.35e+06/4.00e+07 =  3% of the original kernel matrix.

torch.Size([6229, 2])
We keep 2.23e+06/1.61e+07 = 13% of the original kernel matrix.

torch.Size([10810, 2])
We keep 1.70e+06/5.32e+07 =  3% of the original kernel matrix.

torch.Size([19111, 2])
We keep 4.55e+06/1.36e+08 =  3% of the original kernel matrix.

torch.Size([19440, 2])
We keep 3.83e+06/1.55e+08 =  2% of the original kernel matrix.

torch.Size([34092, 2])
We keep 2.72e+07/7.33e+08 =  3% of the original kernel matrix.

torch.Size([25293, 2])
We keep 7.95e+06/3.59e+08 =  2% of the original kernel matrix.

torch.Size([2324, 2])
We keep 1.23e+05/9.60e+05 = 12% of the original kernel matrix.

torch.Size([7187, 2])
We keep 6.11e+05/1.30e+07 =  4% of the original kernel matrix.

torch.Size([2867, 2])
We keep 3.78e+05/2.97e+06 = 12% of the original kernel matrix.

torch.Size([7546, 2])
We keep 9.07e+05/2.28e+07 =  3% of the original kernel matrix.

torch.Size([35152, 2])
We keep 2.58e+07/5.58e+08 =  4% of the original kernel matrix.

torch.Size([25887, 2])
We keep 6.98e+06/3.13e+08 =  2% of the original kernel matrix.

torch.Size([5739, 2])
We keep 5.57e+05/9.53e+06 =  5% of the original kernel matrix.

torch.Size([10252, 2])
We keep 1.38e+06/4.09e+07 =  3% of the original kernel matrix.

torch.Size([189201, 2])
We keep 2.44e+08/1.79e+10 =  1% of the original kernel matrix.

torch.Size([63007, 2])
We keep 3.08e+07/1.77e+09 =  1% of the original kernel matrix.

torch.Size([772, 2])
We keep 9.39e+03/5.81e+04 = 16% of the original kernel matrix.

torch.Size([4667, 2])
We keep 2.26e+05/3.19e+06 =  7% of the original kernel matrix.

torch.Size([28618, 2])
We keep 9.87e+06/3.35e+08 =  2% of the original kernel matrix.

torch.Size([23427, 2])
We keep 5.60e+06/2.42e+08 =  2% of the original kernel matrix.

torch.Size([76279, 2])
We keep 4.29e+07/2.45e+09 =  1% of the original kernel matrix.

torch.Size([38766, 2])
We keep 1.29e+07/6.56e+08 =  1% of the original kernel matrix.

torch.Size([2030, 2])
We keep 6.02e+04/6.07e+05 =  9% of the original kernel matrix.

torch.Size([6747, 2])
We keep 5.07e+05/1.03e+07 =  4% of the original kernel matrix.

torch.Size([21198, 2])
We keep 1.28e+07/3.12e+08 =  4% of the original kernel matrix.

torch.Size([19691, 2])
We keep 5.44e+06/2.34e+08 =  2% of the original kernel matrix.

torch.Size([5920, 2])
We keep 1.50e+06/2.22e+07 =  6% of the original kernel matrix.

torch.Size([10446, 2])
We keep 1.94e+06/6.25e+07 =  3% of the original kernel matrix.

torch.Size([109513, 2])
We keep 1.53e+08/5.42e+09 =  2% of the original kernel matrix.

torch.Size([46805, 2])
We keep 1.85e+07/9.75e+08 =  1% of the original kernel matrix.

torch.Size([4459, 2])
We keep 3.64e+05/4.97e+06 =  7% of the original kernel matrix.

torch.Size([9298, 2])
We keep 1.09e+06/2.95e+07 =  3% of the original kernel matrix.

torch.Size([4929, 2])
We keep 5.05e+05/6.01e+06 =  8% of the original kernel matrix.

torch.Size([9610, 2])
We keep 1.16e+06/3.25e+07 =  3% of the original kernel matrix.

torch.Size([1818, 2])
We keep 6.88e+04/5.85e+05 = 11% of the original kernel matrix.

torch.Size([6508, 2])
We keep 4.89e+05/1.01e+07 =  4% of the original kernel matrix.

torch.Size([1103, 2])
We keep 2.35e+04/1.87e+05 = 12% of the original kernel matrix.

torch.Size([5437, 2])
We keep 3.52e+05/5.74e+06 =  6% of the original kernel matrix.

torch.Size([8709, 2])
We keep 1.60e+06/2.43e+07 =  6% of the original kernel matrix.

torch.Size([12828, 2])
We keep 2.01e+06/6.53e+07 =  3% of the original kernel matrix.

torch.Size([7946, 2])
We keep 9.48e+05/1.88e+07 =  5% of the original kernel matrix.

torch.Size([12479, 2])
We keep 1.78e+06/5.74e+07 =  3% of the original kernel matrix.

torch.Size([6085, 2])
We keep 1.81e+06/1.19e+07 = 15% of the original kernel matrix.

torch.Size([10759, 2])
We keep 1.56e+06/4.57e+07 =  3% of the original kernel matrix.

torch.Size([32067, 2])
We keep 1.41e+07/4.26e+08 =  3% of the original kernel matrix.

torch.Size([24703, 2])
We keep 6.11e+06/2.73e+08 =  2% of the original kernel matrix.

torch.Size([9207, 2])
We keep 1.07e+06/2.41e+07 =  4% of the original kernel matrix.

torch.Size([13353, 2])
We keep 1.96e+06/6.51e+07 =  3% of the original kernel matrix.

torch.Size([696, 2])
We keep 1.11e+04/6.05e+04 = 18% of the original kernel matrix.

torch.Size([4463, 2])
We keep 2.45e+05/3.26e+06 =  7% of the original kernel matrix.

torch.Size([5969, 2])
We keep 5.36e+05/8.29e+06 =  6% of the original kernel matrix.

torch.Size([10738, 2])
We keep 1.31e+06/3.81e+07 =  3% of the original kernel matrix.

torch.Size([26891, 2])
We keep 5.25e+07/8.63e+08 =  6% of the original kernel matrix.

torch.Size([20901, 2])
We keep 8.37e+06/3.89e+08 =  2% of the original kernel matrix.

torch.Size([8036, 2])
We keep 8.26e+06/7.20e+07 = 11% of the original kernel matrix.

torch.Size([12003, 2])
We keep 3.01e+06/1.12e+08 =  2% of the original kernel matrix.

torch.Size([71514, 2])
We keep 4.58e+07/1.94e+09 =  2% of the original kernel matrix.

torch.Size([37321, 2])
We keep 1.16e+07/5.83e+08 =  1% of the original kernel matrix.

torch.Size([4801, 2])
We keep 3.43e+05/4.46e+06 =  7% of the original kernel matrix.

torch.Size([9797, 2])
We keep 1.04e+06/2.80e+07 =  3% of the original kernel matrix.

torch.Size([2634, 2])
We keep 1.49e+05/1.45e+06 = 10% of the original kernel matrix.

torch.Size([7588, 2])
We keep 7.02e+05/1.60e+07 =  4% of the original kernel matrix.

torch.Size([687, 2])
We keep 1.46e+04/6.71e+04 = 21% of the original kernel matrix.

torch.Size([4362, 2])
We keep 2.47e+05/3.43e+06 =  7% of the original kernel matrix.

torch.Size([2466, 2])
We keep 4.37e+05/2.25e+06 = 19% of the original kernel matrix.

torch.Size([7056, 2])
We keep 8.34e+05/1.99e+07 =  4% of the original kernel matrix.

torch.Size([51012, 2])
We keep 5.08e+07/1.52e+09 =  3% of the original kernel matrix.

torch.Size([31203, 2])
We keep 1.07e+07/5.16e+08 =  2% of the original kernel matrix.

torch.Size([5119, 2])
We keep 4.98e+05/7.38e+06 =  6% of the original kernel matrix.

torch.Size([9868, 2])
We keep 1.25e+06/3.60e+07 =  3% of the original kernel matrix.

torch.Size([9745, 2])
We keep 1.16e+06/2.59e+07 =  4% of the original kernel matrix.

torch.Size([13699, 2])
We keep 2.01e+06/6.74e+07 =  2% of the original kernel matrix.

torch.Size([14513, 2])
We keep 3.92e+06/9.61e+07 =  4% of the original kernel matrix.

torch.Size([16554, 2])
We keep 3.36e+06/1.30e+08 =  2% of the original kernel matrix.

torch.Size([4973, 2])
We keep 7.54e+05/7.51e+06 = 10% of the original kernel matrix.

torch.Size([9875, 2])
We keep 1.25e+06/3.63e+07 =  3% of the original kernel matrix.

torch.Size([1718, 2])
We keep 6.34e+04/6.15e+05 = 10% of the original kernel matrix.

torch.Size([6407, 2])
We keep 5.10e+05/1.04e+07 =  4% of the original kernel matrix.

torch.Size([4492, 2])
We keep 3.13e+05/4.50e+06 =  6% of the original kernel matrix.

torch.Size([9196, 2])
We keep 1.03e+06/2.81e+07 =  3% of the original kernel matrix.

torch.Size([45478, 2])
We keep 3.60e+07/9.56e+08 =  3% of the original kernel matrix.

torch.Size([29357, 2])
We keep 8.75e+06/4.09e+08 =  2% of the original kernel matrix.

torch.Size([7405, 2])
We keep 7.65e+05/1.38e+07 =  5% of the original kernel matrix.

torch.Size([11958, 2])
We keep 1.56e+06/4.92e+07 =  3% of the original kernel matrix.

torch.Size([3344, 2])
We keep 2.18e+05/2.57e+06 =  8% of the original kernel matrix.

torch.Size([8280, 2])
We keep 8.67e+05/2.12e+07 =  4% of the original kernel matrix.

torch.Size([3748, 2])
We keep 3.07e+05/3.33e+06 =  9% of the original kernel matrix.

torch.Size([8877, 2])
We keep 9.29e+05/2.42e+07 =  3% of the original kernel matrix.

torch.Size([28677, 2])
We keep 2.78e+08/1.72e+09 = 16% of the original kernel matrix.

torch.Size([21104, 2])
We keep 1.12e+07/5.49e+08 =  2% of the original kernel matrix.

torch.Size([39299, 2])
We keep 1.56e+07/5.86e+08 =  2% of the original kernel matrix.

torch.Size([27452, 2])
We keep 7.08e+06/3.21e+08 =  2% of the original kernel matrix.

torch.Size([25805, 2])
We keep 6.54e+07/9.51e+08 =  6% of the original kernel matrix.

torch.Size([20015, 2])
We keep 8.71e+06/4.08e+08 =  2% of the original kernel matrix.

torch.Size([29129, 2])
We keep 1.01e+07/3.60e+08 =  2% of the original kernel matrix.

torch.Size([24069, 2])
We keep 5.69e+06/2.51e+08 =  2% of the original kernel matrix.

torch.Size([2026, 2])
We keep 9.53e+04/8.70e+05 = 10% of the original kernel matrix.

torch.Size([6768, 2])
We keep 6.11e+05/1.24e+07 =  4% of the original kernel matrix.

torch.Size([18621, 2])
We keep 5.26e+06/1.50e+08 =  3% of the original kernel matrix.

torch.Size([19256, 2])
We keep 3.96e+06/1.62e+08 =  2% of the original kernel matrix.

torch.Size([2915, 2])
We keep 4.39e+05/3.29e+06 = 13% of the original kernel matrix.

torch.Size([7624, 2])
We keep 8.64e+05/2.40e+07 =  3% of the original kernel matrix.

torch.Size([8818, 2])
We keep 1.43e+06/2.71e+07 =  5% of the original kernel matrix.

torch.Size([12810, 2])
We keep 2.06e+06/6.89e+07 =  2% of the original kernel matrix.

torch.Size([4324, 2])
We keep 3.13e+05/4.62e+06 =  6% of the original kernel matrix.

torch.Size([9306, 2])
We keep 1.07e+06/2.85e+07 =  3% of the original kernel matrix.

torch.Size([1925, 2])
We keep 5.46e+04/5.69e+05 =  9% of the original kernel matrix.

torch.Size([6809, 2])
We keep 4.97e+05/9.99e+06 =  4% of the original kernel matrix.

torch.Size([6286, 2])
We keep 1.19e+06/1.28e+07 =  9% of the original kernel matrix.

torch.Size([10898, 2])
We keep 1.57e+06/4.75e+07 =  3% of the original kernel matrix.

torch.Size([7667, 2])
We keep 1.17e+06/1.81e+07 =  6% of the original kernel matrix.

torch.Size([11925, 2])
We keep 1.74e+06/5.64e+07 =  3% of the original kernel matrix.

torch.Size([1131, 2])
We keep 2.71e+04/1.90e+05 = 14% of the original kernel matrix.

torch.Size([5272, 2])
We keep 3.40e+05/5.78e+06 =  5% of the original kernel matrix.

torch.Size([6801, 2])
We keep 8.67e+05/1.29e+07 =  6% of the original kernel matrix.

torch.Size([11242, 2])
We keep 1.54e+06/4.76e+07 =  3% of the original kernel matrix.

torch.Size([2316, 2])
We keep 2.92e+05/1.88e+06 = 15% of the original kernel matrix.

torch.Size([7087, 2])
We keep 7.30e+05/1.82e+07 =  4% of the original kernel matrix.

torch.Size([25281, 2])
We keep 4.63e+07/5.73e+08 =  8% of the original kernel matrix.

torch.Size([21254, 2])
We keep 7.05e+06/3.17e+08 =  2% of the original kernel matrix.

torch.Size([1063, 2])
We keep 5.40e+04/2.76e+05 = 19% of the original kernel matrix.

torch.Size([4980, 2])
We keep 4.01e+05/6.95e+06 =  5% of the original kernel matrix.

torch.Size([3936, 2])
We keep 5.38e+05/5.87e+06 =  9% of the original kernel matrix.

torch.Size([8334, 2])
We keep 1.13e+06/3.21e+07 =  3% of the original kernel matrix.

torch.Size([810, 2])
We keep 2.07e+04/1.29e+05 = 16% of the original kernel matrix.

torch.Size([4824, 2])
We keep 2.92e+05/4.76e+06 =  6% of the original kernel matrix.

torch.Size([5572, 2])
We keep 7.68e+05/8.56e+06 =  8% of the original kernel matrix.

torch.Size([10438, 2])
We keep 1.36e+06/3.87e+07 =  3% of the original kernel matrix.

torch.Size([3467, 2])
We keep 2.13e+05/2.95e+06 =  7% of the original kernel matrix.

torch.Size([8270, 2])
We keep 8.75e+05/2.28e+07 =  3% of the original kernel matrix.

torch.Size([4818, 2])
We keep 3.37e+05/5.05e+06 =  6% of the original kernel matrix.

torch.Size([9705, 2])
We keep 1.08e+06/2.98e+07 =  3% of the original kernel matrix.

torch.Size([8366, 2])
We keep 8.35e+05/1.80e+07 =  4% of the original kernel matrix.

torch.Size([12737, 2])
We keep 1.74e+06/5.62e+07 =  3% of the original kernel matrix.

torch.Size([14468, 2])
We keep 5.69e+06/1.26e+08 =  4% of the original kernel matrix.

torch.Size([16972, 2])
We keep 3.89e+06/1.49e+08 =  2% of the original kernel matrix.

torch.Size([1568, 2])
We keep 6.32e+04/4.64e+05 = 13% of the original kernel matrix.

torch.Size([6140, 2])
We keep 4.77e+05/9.02e+06 =  5% of the original kernel matrix.

torch.Size([631, 2])
We keep 8.30e+03/4.75e+04 = 17% of the original kernel matrix.

torch.Size([4439, 2])
We keep 2.22e+05/2.89e+06 =  7% of the original kernel matrix.

torch.Size([857, 2])
We keep 3.13e+04/1.63e+05 = 19% of the original kernel matrix.

torch.Size([4688, 2])
We keep 3.35e+05/5.35e+06 =  6% of the original kernel matrix.

torch.Size([41021, 2])
We keep 2.91e+07/8.36e+08 =  3% of the original kernel matrix.

torch.Size([27998, 2])
We keep 8.14e+06/3.83e+08 =  2% of the original kernel matrix.

torch.Size([334464, 2])
We keep 1.14e+09/5.73e+10 =  1% of the original kernel matrix.

torch.Size([83911, 2])
We keep 5.27e+07/3.17e+09 =  1% of the original kernel matrix.

torch.Size([7638, 2])
We keep 1.28e+06/1.97e+07 =  6% of the original kernel matrix.

torch.Size([11979, 2])
We keep 1.83e+06/5.88e+07 =  3% of the original kernel matrix.

torch.Size([2234, 2])
We keep 1.18e+05/1.21e+06 =  9% of the original kernel matrix.

torch.Size([6821, 2])
We keep 6.62e+05/1.46e+07 =  4% of the original kernel matrix.

torch.Size([17089, 2])
We keep 9.83e+06/1.65e+08 =  5% of the original kernel matrix.

torch.Size([17967, 2])
We keep 4.19e+06/1.70e+08 =  2% of the original kernel matrix.

torch.Size([27510, 2])
We keep 7.35e+06/2.98e+08 =  2% of the original kernel matrix.

torch.Size([23485, 2])
We keep 5.29e+06/2.29e+08 =  2% of the original kernel matrix.

torch.Size([3257, 2])
We keep 1.99e+05/2.31e+06 =  8% of the original kernel matrix.

torch.Size([7918, 2])
We keep 8.00e+05/2.01e+07 =  3% of the original kernel matrix.

torch.Size([13154, 2])
We keep 2.99e+06/8.52e+07 =  3% of the original kernel matrix.

torch.Size([16051, 2])
We keep 3.26e+06/1.22e+08 =  2% of the original kernel matrix.

torch.Size([9522, 2])
We keep 1.04e+06/2.64e+07 =  3% of the original kernel matrix.

torch.Size([13671, 2])
We keep 2.00e+06/6.81e+07 =  2% of the original kernel matrix.

torch.Size([91752, 2])
We keep 2.47e+08/8.32e+09 =  2% of the original kernel matrix.

torch.Size([40074, 2])
We keep 2.18e+07/1.21e+09 =  1% of the original kernel matrix.

torch.Size([68989, 2])
We keep 1.27e+08/4.05e+09 =  3% of the original kernel matrix.

torch.Size([34657, 2])
We keep 1.59e+07/8.43e+08 =  1% of the original kernel matrix.

torch.Size([83174, 2])
We keep 6.90e+07/3.55e+09 =  1% of the original kernel matrix.

torch.Size([40473, 2])
We keep 1.53e+07/7.90e+08 =  1% of the original kernel matrix.

torch.Size([4663, 2])
We keep 3.65e+05/4.35e+06 =  8% of the original kernel matrix.

torch.Size([9706, 2])
We keep 1.03e+06/2.76e+07 =  3% of the original kernel matrix.

torch.Size([2720, 2])
We keep 2.04e+05/1.87e+06 = 10% of the original kernel matrix.

torch.Size([7607, 2])
We keep 7.79e+05/1.81e+07 =  4% of the original kernel matrix.

torch.Size([1263, 2])
We keep 3.21e+04/2.56e+05 = 12% of the original kernel matrix.

torch.Size([5573, 2])
We keep 3.90e+05/6.70e+06 =  5% of the original kernel matrix.

torch.Size([12928, 2])
We keep 3.37e+06/8.14e+07 =  4% of the original kernel matrix.

torch.Size([15792, 2])
We keep 3.10e+06/1.20e+08 =  2% of the original kernel matrix.

torch.Size([3901, 2])
We keep 2.33e+05/3.09e+06 =  7% of the original kernel matrix.

torch.Size([8975, 2])
We keep 9.32e+05/2.33e+07 =  4% of the original kernel matrix.

torch.Size([2868, 2])
We keep 4.63e+05/3.98e+06 = 11% of the original kernel matrix.

torch.Size([7552, 2])
We keep 9.85e+05/2.64e+07 =  3% of the original kernel matrix.

torch.Size([45801, 2])
We keep 4.16e+07/1.12e+09 =  3% of the original kernel matrix.

torch.Size([29295, 2])
We keep 9.27e+06/4.42e+08 =  2% of the original kernel matrix.

torch.Size([144515, 2])
We keep 1.47e+08/9.96e+09 =  1% of the original kernel matrix.

torch.Size([55292, 2])
We keep 2.38e+07/1.32e+09 =  1% of the original kernel matrix.

torch.Size([4776, 2])
We keep 4.05e+05/5.81e+06 =  6% of the original kernel matrix.

torch.Size([9409, 2])
We keep 1.13e+06/3.19e+07 =  3% of the original kernel matrix.

torch.Size([592, 2])
We keep 8.55e+03/4.49e+04 = 19% of the original kernel matrix.

torch.Size([4247, 2])
We keep 2.16e+05/2.81e+06 =  7% of the original kernel matrix.

torch.Size([3671, 2])
We keep 4.33e+05/3.20e+06 = 13% of the original kernel matrix.

torch.Size([8695, 2])
We keep 9.45e+05/2.37e+07 =  3% of the original kernel matrix.

torch.Size([16357, 2])
We keep 3.56e+06/9.58e+07 =  3% of the original kernel matrix.

torch.Size([18283, 2])
We keep 3.37e+06/1.30e+08 =  2% of the original kernel matrix.

torch.Size([9276, 2])
We keep 1.44e+06/2.84e+07 =  5% of the original kernel matrix.

torch.Size([13384, 2])
We keep 2.09e+06/7.05e+07 =  2% of the original kernel matrix.

torch.Size([442, 2])
We keep 6.13e+03/2.43e+04 = 25% of the original kernel matrix.

torch.Size([3858, 2])
We keep 1.86e+05/2.07e+06 =  8% of the original kernel matrix.

torch.Size([816, 2])
We keep 1.76e+04/1.08e+05 = 16% of the original kernel matrix.

torch.Size([4886, 2])
We keep 2.93e+05/4.36e+06 =  6% of the original kernel matrix.

torch.Size([5818, 2])
We keep 5.86e+05/9.29e+06 =  6% of the original kernel matrix.

torch.Size([10620, 2])
We keep 1.38e+06/4.04e+07 =  3% of the original kernel matrix.

torch.Size([1420, 2])
We keep 3.22e+04/2.56e+05 = 12% of the original kernel matrix.

torch.Size([5935, 2])
We keep 3.82e+05/6.70e+06 =  5% of the original kernel matrix.

torch.Size([12479, 2])
We keep 2.08e+07/2.03e+08 = 10% of the original kernel matrix.

torch.Size([14325, 2])
We keep 4.62e+06/1.89e+08 =  2% of the original kernel matrix.

torch.Size([2517, 2])
We keep 1.07e+05/1.05e+06 = 10% of the original kernel matrix.

torch.Size([7469, 2])
We keep 6.35e+05/1.36e+07 =  4% of the original kernel matrix.

torch.Size([5945, 2])
We keep 4.33e+05/8.01e+06 =  5% of the original kernel matrix.

torch.Size([10834, 2])
We keep 1.28e+06/3.75e+07 =  3% of the original kernel matrix.

torch.Size([4967, 2])
We keep 3.67e+05/5.59e+06 =  6% of the original kernel matrix.

torch.Size([9782, 2])
We keep 1.12e+06/3.13e+07 =  3% of the original kernel matrix.

torch.Size([1640, 2])
We keep 4.73e+04/4.13e+05 = 11% of the original kernel matrix.

torch.Size([6197, 2])
We keep 4.49e+05/8.52e+06 =  5% of the original kernel matrix.

torch.Size([1881, 2])
We keep 5.64e+04/5.21e+05 = 10% of the original kernel matrix.

torch.Size([6595, 2])
We keep 4.92e+05/9.56e+06 =  5% of the original kernel matrix.

torch.Size([66510, 2])
We keep 3.55e+07/1.76e+09 =  2% of the original kernel matrix.

torch.Size([35979, 2])
We keep 1.10e+07/5.55e+08 =  1% of the original kernel matrix.

torch.Size([103052, 2])
We keep 9.94e+07/4.10e+09 =  2% of the original kernel matrix.

torch.Size([45605, 2])
We keep 1.63e+07/8.48e+08 =  1% of the original kernel matrix.

torch.Size([23596, 2])
We keep 2.37e+07/2.81e+08 =  8% of the original kernel matrix.

torch.Size([21341, 2])
We keep 5.26e+06/2.22e+08 =  2% of the original kernel matrix.

torch.Size([7068, 2])
We keep 1.21e+06/1.53e+07 =  7% of the original kernel matrix.

torch.Size([11685, 2])
We keep 1.66e+06/5.19e+07 =  3% of the original kernel matrix.

torch.Size([7940, 2])
We keep 7.65e+05/1.66e+07 =  4% of the original kernel matrix.

torch.Size([12416, 2])
We keep 1.69e+06/5.40e+07 =  3% of the original kernel matrix.

torch.Size([2298, 2])
We keep 1.55e+05/1.22e+06 = 12% of the original kernel matrix.

torch.Size([7066, 2])
We keep 6.53e+05/1.46e+07 =  4% of the original kernel matrix.

torch.Size([7393, 2])
We keep 9.54e+05/1.68e+07 =  5% of the original kernel matrix.

torch.Size([12037, 2])
We keep 1.76e+06/5.42e+07 =  3% of the original kernel matrix.

torch.Size([26792, 2])
We keep 7.87e+06/3.00e+08 =  2% of the original kernel matrix.

torch.Size([23123, 2])
We keep 5.32e+06/2.30e+08 =  2% of the original kernel matrix.

torch.Size([5983, 2])
We keep 1.19e+06/1.16e+07 = 10% of the original kernel matrix.

torch.Size([10690, 2])
We keep 1.47e+06/4.51e+07 =  3% of the original kernel matrix.

torch.Size([7592, 2])
We keep 8.90e+05/1.58e+07 =  5% of the original kernel matrix.

torch.Size([12149, 2])
We keep 1.67e+06/5.27e+07 =  3% of the original kernel matrix.

torch.Size([11040, 2])
We keep 1.86e+06/4.16e+07 =  4% of the original kernel matrix.

torch.Size([14622, 2])
We keep 2.41e+06/8.54e+07 =  2% of the original kernel matrix.

torch.Size([254279, 2])
We keep 6.03e+08/3.85e+10 =  1% of the original kernel matrix.

torch.Size([72909, 2])
We keep 4.48e+07/2.60e+09 =  1% of the original kernel matrix.

torch.Size([6939, 2])
We keep 6.52e+05/1.14e+07 =  5% of the original kernel matrix.

torch.Size([11586, 2])
We keep 1.48e+06/4.48e+07 =  3% of the original kernel matrix.

torch.Size([19554, 2])
We keep 5.12e+06/1.61e+08 =  3% of the original kernel matrix.

torch.Size([19714, 2])
We keep 4.16e+06/1.68e+08 =  2% of the original kernel matrix.

torch.Size([96763, 2])
We keep 1.56e+08/5.72e+09 =  2% of the original kernel matrix.

torch.Size([43885, 2])
We keep 1.86e+07/1.00e+09 =  1% of the original kernel matrix.

torch.Size([16333, 2])
We keep 3.57e+06/1.23e+08 =  2% of the original kernel matrix.

torch.Size([18508, 2])
We keep 3.78e+06/1.47e+08 =  2% of the original kernel matrix.

torch.Size([6750, 2])
We keep 8.52e+05/1.32e+07 =  6% of the original kernel matrix.

torch.Size([11460, 2])
We keep 1.52e+06/4.81e+07 =  3% of the original kernel matrix.

torch.Size([71153, 2])
We keep 6.43e+07/2.71e+09 =  2% of the original kernel matrix.

torch.Size([36843, 2])
We keep 1.36e+07/6.90e+08 =  1% of the original kernel matrix.

torch.Size([24833, 2])
We keep 6.12e+06/2.34e+08 =  2% of the original kernel matrix.

torch.Size([22133, 2])
We keep 4.76e+06/2.02e+08 =  2% of the original kernel matrix.

torch.Size([10282, 2])
We keep 1.54e+06/3.04e+07 =  5% of the original kernel matrix.

torch.Size([14122, 2])
We keep 2.09e+06/7.30e+07 =  2% of the original kernel matrix.

torch.Size([2948, 2])
We keep 1.70e+05/1.65e+06 = 10% of the original kernel matrix.

torch.Size([7925, 2])
We keep 7.42e+05/1.70e+07 =  4% of the original kernel matrix.

torch.Size([7815, 2])
We keep 1.38e+06/2.01e+07 =  6% of the original kernel matrix.

torch.Size([12359, 2])
We keep 1.85e+06/5.94e+07 =  3% of the original kernel matrix.

torch.Size([4806, 2])
We keep 5.57e+05/6.11e+06 =  9% of the original kernel matrix.

torch.Size([9738, 2])
We keep 1.17e+06/3.27e+07 =  3% of the original kernel matrix.

torch.Size([22349, 2])
We keep 5.75e+06/1.86e+08 =  3% of the original kernel matrix.

torch.Size([21395, 2])
We keep 4.31e+06/1.81e+08 =  2% of the original kernel matrix.

torch.Size([155426, 2])
We keep 3.51e+08/1.70e+10 =  2% of the original kernel matrix.

torch.Size([57857, 2])
We keep 3.13e+07/1.73e+09 =  1% of the original kernel matrix.

torch.Size([26193, 2])
We keep 5.01e+07/4.86e+08 = 10% of the original kernel matrix.

torch.Size([22639, 2])
We keep 6.92e+06/2.92e+08 =  2% of the original kernel matrix.

torch.Size([38797, 2])
We keep 1.86e+07/6.86e+08 =  2% of the original kernel matrix.

torch.Size([27049, 2])
We keep 7.68e+06/3.47e+08 =  2% of the original kernel matrix.

torch.Size([6186, 2])
We keep 8.48e+05/1.25e+07 =  6% of the original kernel matrix.

torch.Size([10821, 2])
We keep 1.46e+06/4.68e+07 =  3% of the original kernel matrix.

torch.Size([4985, 2])
We keep 1.65e+06/1.15e+07 = 14% of the original kernel matrix.

torch.Size([9638, 2])
We keep 1.50e+06/4.49e+07 =  3% of the original kernel matrix.

torch.Size([8396, 2])
We keep 1.91e+06/2.57e+07 =  7% of the original kernel matrix.

torch.Size([12499, 2])
We keep 2.03e+06/6.72e+07 =  3% of the original kernel matrix.

torch.Size([2353, 2])
We keep 1.94e+05/1.34e+06 = 14% of the original kernel matrix.

torch.Size([7043, 2])
We keep 6.80e+05/1.54e+07 =  4% of the original kernel matrix.

torch.Size([134539, 2])
We keep 3.74e+08/1.53e+10 =  2% of the original kernel matrix.

torch.Size([51377, 2])
We keep 2.89e+07/1.64e+09 =  1% of the original kernel matrix.

torch.Size([59853, 2])
We keep 4.12e+07/2.02e+09 =  2% of the original kernel matrix.

torch.Size([33753, 2])
We keep 1.21e+07/5.96e+08 =  2% of the original kernel matrix.

torch.Size([8349, 2])
We keep 1.94e+06/3.19e+07 =  6% of the original kernel matrix.

torch.Size([12854, 2])
We keep 2.28e+06/7.48e+07 =  3% of the original kernel matrix.

torch.Size([393077, 2])
We keep 2.17e+09/1.08e+11 =  2% of the original kernel matrix.

torch.Size([87214, 2])
We keep 6.93e+07/4.36e+09 =  1% of the original kernel matrix.

torch.Size([47410, 2])
We keep 2.18e+08/2.51e+09 =  8% of the original kernel matrix.

torch.Size([29790, 2])
We keep 1.36e+07/6.63e+08 =  2% of the original kernel matrix.

torch.Size([10772, 2])
We keep 1.42e+06/3.71e+07 =  3% of the original kernel matrix.

torch.Size([14470, 2])
We keep 2.31e+06/8.07e+07 =  2% of the original kernel matrix.

torch.Size([1229, 2])
We keep 3.45e+04/2.53e+05 = 13% of the original kernel matrix.

torch.Size([5640, 2])
We keep 3.92e+05/6.66e+06 =  5% of the original kernel matrix.

torch.Size([8469, 2])
We keep 1.32e+06/2.11e+07 =  6% of the original kernel matrix.

torch.Size([12832, 2])
We keep 1.86e+06/6.08e+07 =  3% of the original kernel matrix.

torch.Size([22239, 2])
We keep 6.66e+06/2.26e+08 =  2% of the original kernel matrix.

torch.Size([21206, 2])
We keep 4.82e+06/1.99e+08 =  2% of the original kernel matrix.

torch.Size([4465, 2])
We keep 3.17e+05/4.26e+06 =  7% of the original kernel matrix.

torch.Size([9320, 2])
We keep 1.02e+06/2.73e+07 =  3% of the original kernel matrix.

torch.Size([238671, 2])
We keep 3.36e+08/2.34e+10 =  1% of the original kernel matrix.

torch.Size([70302, 2])
We keep 3.48e+07/2.03e+09 =  1% of the original kernel matrix.

torch.Size([6733, 2])
We keep 8.95e+05/1.62e+07 =  5% of the original kernel matrix.

torch.Size([11684, 2])
We keep 1.63e+06/5.33e+07 =  3% of the original kernel matrix.

torch.Size([6738, 2])
We keep 1.72e+06/2.12e+07 =  8% of the original kernel matrix.

torch.Size([11178, 2])
We keep 1.87e+06/6.10e+07 =  3% of the original kernel matrix.

torch.Size([2274, 2])
We keep 9.58e+04/1.08e+06 =  8% of the original kernel matrix.

torch.Size([7109, 2])
We keep 6.01e+05/1.37e+07 =  4% of the original kernel matrix.

torch.Size([3419, 2])
We keep 1.69e+05/2.18e+06 =  7% of the original kernel matrix.

torch.Size([8329, 2])
We keep 7.99e+05/1.96e+07 =  4% of the original kernel matrix.

torch.Size([682, 2])
We keep 1.25e+04/7.02e+04 = 17% of the original kernel matrix.

torch.Size([4519, 2])
We keep 2.59e+05/3.51e+06 =  7% of the original kernel matrix.

torch.Size([4130, 2])
We keep 7.97e+05/8.71e+06 =  9% of the original kernel matrix.

torch.Size([8230, 2])
We keep 1.32e+06/3.91e+07 =  3% of the original kernel matrix.

torch.Size([6320, 2])
We keep 1.25e+06/1.44e+07 =  8% of the original kernel matrix.

torch.Size([11013, 2])
We keep 1.71e+06/5.03e+07 =  3% of the original kernel matrix.

torch.Size([6493, 2])
We keep 6.51e+05/1.12e+07 =  5% of the original kernel matrix.

torch.Size([11069, 2])
We keep 1.48e+06/4.43e+07 =  3% of the original kernel matrix.

torch.Size([386446, 2])
We keep 8.13e+09/2.74e+11 =  2% of the original kernel matrix.

torch.Size([78510, 2])
We keep 1.11e+08/6.93e+09 =  1% of the original kernel matrix.

torch.Size([6738, 2])
We keep 8.76e+05/1.48e+07 =  5% of the original kernel matrix.

torch.Size([11089, 2])
We keep 1.61e+06/5.10e+07 =  3% of the original kernel matrix.

torch.Size([22801, 2])
We keep 5.35e+06/2.08e+08 =  2% of the original kernel matrix.

torch.Size([21529, 2])
We keep 4.53e+06/1.91e+08 =  2% of the original kernel matrix.

torch.Size([5564, 2])
We keep 4.45e+05/7.30e+06 =  6% of the original kernel matrix.

torch.Size([10347, 2])
We keep 1.24e+06/3.58e+07 =  3% of the original kernel matrix.

torch.Size([7739, 2])
We keep 8.40e+05/1.63e+07 =  5% of the original kernel matrix.

torch.Size([11982, 2])
We keep 1.69e+06/5.35e+07 =  3% of the original kernel matrix.

torch.Size([3312, 2])
We keep 2.68e+05/2.50e+06 = 10% of the original kernel matrix.

torch.Size([8080, 2])
We keep 8.25e+05/2.09e+07 =  3% of the original kernel matrix.

torch.Size([18603, 2])
We keep 2.97e+07/3.27e+08 =  9% of the original kernel matrix.

torch.Size([18249, 2])
We keep 5.59e+06/2.39e+08 =  2% of the original kernel matrix.

torch.Size([23054, 2])
We keep 1.58e+07/3.79e+08 =  4% of the original kernel matrix.

torch.Size([20094, 2])
We keep 5.83e+06/2.58e+08 =  2% of the original kernel matrix.

torch.Size([184399, 2])
We keep 1.87e+08/1.33e+10 =  1% of the original kernel matrix.

torch.Size([62459, 2])
We keep 2.68e+07/1.53e+09 =  1% of the original kernel matrix.

torch.Size([5932, 2])
We keep 9.78e+05/1.04e+07 =  9% of the original kernel matrix.

torch.Size([10539, 2])
We keep 1.41e+06/4.27e+07 =  3% of the original kernel matrix.

torch.Size([71519, 2])
We keep 4.35e+07/2.02e+09 =  2% of the original kernel matrix.

torch.Size([37341, 2])
We keep 1.18e+07/5.96e+08 =  1% of the original kernel matrix.

torch.Size([6262, 2])
We keep 2.32e+06/1.78e+07 = 13% of the original kernel matrix.

torch.Size([10993, 2])
We keep 1.75e+06/5.58e+07 =  3% of the original kernel matrix.

torch.Size([35297, 2])
We keep 1.49e+07/5.21e+08 =  2% of the original kernel matrix.

torch.Size([25715, 2])
We keep 6.69e+06/3.02e+08 =  2% of the original kernel matrix.

torch.Size([48660, 2])
We keep 4.06e+07/1.55e+09 =  2% of the original kernel matrix.

torch.Size([30670, 2])
We keep 1.07e+07/5.22e+08 =  2% of the original kernel matrix.

torch.Size([5713, 2])
We keep 4.75e+05/7.95e+06 =  5% of the original kernel matrix.

torch.Size([10593, 2])
We keep 1.29e+06/3.74e+07 =  3% of the original kernel matrix.

torch.Size([2331, 2])
We keep 1.64e+05/1.25e+06 = 13% of the original kernel matrix.

torch.Size([7068, 2])
We keep 6.70e+05/1.48e+07 =  4% of the original kernel matrix.

torch.Size([24581, 2])
We keep 6.13e+06/2.29e+08 =  2% of the original kernel matrix.

torch.Size([22202, 2])
We keep 4.75e+06/2.01e+08 =  2% of the original kernel matrix.

torch.Size([2416, 2])
We keep 1.11e+05/1.12e+06 =  9% of the original kernel matrix.

torch.Size([7340, 2])
We keep 6.39e+05/1.40e+07 =  4% of the original kernel matrix.

torch.Size([11701, 2])
We keep 2.64e+06/5.59e+07 =  4% of the original kernel matrix.

torch.Size([15050, 2])
We keep 2.69e+06/9.91e+07 =  2% of the original kernel matrix.

torch.Size([60413, 2])
We keep 5.34e+07/1.69e+09 =  3% of the original kernel matrix.

torch.Size([33618, 2])
We keep 1.09e+07/5.44e+08 =  1% of the original kernel matrix.

torch.Size([3475, 2])
We keep 2.54e+05/2.97e+06 =  8% of the original kernel matrix.

torch.Size([8417, 2])
We keep 9.09e+05/2.28e+07 =  3% of the original kernel matrix.

torch.Size([18245, 2])
We keep 1.22e+07/2.33e+08 =  5% of the original kernel matrix.

torch.Size([19432, 2])
We keep 5.09e+06/2.02e+08 =  2% of the original kernel matrix.

torch.Size([4830, 2])
We keep 4.15e+05/5.66e+06 =  7% of the original kernel matrix.

torch.Size([9543, 2])
We keep 1.13e+06/3.15e+07 =  3% of the original kernel matrix.

torch.Size([37951, 2])
We keep 1.55e+07/5.57e+08 =  2% of the original kernel matrix.

torch.Size([26938, 2])
We keep 6.85e+06/3.13e+08 =  2% of the original kernel matrix.

torch.Size([66146, 2])
We keep 5.37e+07/1.83e+09 =  2% of the original kernel matrix.

torch.Size([35537, 2])
We keep 1.14e+07/5.66e+08 =  2% of the original kernel matrix.

torch.Size([5446, 2])
We keep 7.62e+05/1.02e+07 =  7% of the original kernel matrix.

torch.Size([10387, 2])
We keep 1.50e+06/4.22e+07 =  3% of the original kernel matrix.

torch.Size([3788, 2])
We keep 3.17e+05/3.63e+06 =  8% of the original kernel matrix.

torch.Size([8656, 2])
We keep 9.58e+05/2.52e+07 =  3% of the original kernel matrix.

torch.Size([96578, 2])
We keep 1.64e+08/6.28e+09 =  2% of the original kernel matrix.

torch.Size([43073, 2])
We keep 1.94e+07/1.05e+09 =  1% of the original kernel matrix.

torch.Size([22862, 2])
We keep 8.51e+06/2.53e+08 =  3% of the original kernel matrix.

torch.Size([20795, 2])
We keep 4.85e+06/2.11e+08 =  2% of the original kernel matrix.

torch.Size([124190, 2])
We keep 2.91e+08/1.12e+10 =  2% of the original kernel matrix.

torch.Size([48939, 2])
We keep 2.47e+07/1.40e+09 =  1% of the original kernel matrix.

torch.Size([12921, 2])
We keep 4.08e+06/7.82e+07 =  5% of the original kernel matrix.

torch.Size([15439, 2])
We keep 3.08e+06/1.17e+08 =  2% of the original kernel matrix.

torch.Size([16843, 2])
We keep 7.64e+06/1.59e+08 =  4% of the original kernel matrix.

torch.Size([17787, 2])
We keep 4.20e+06/1.67e+08 =  2% of the original kernel matrix.

torch.Size([11013, 2])
We keep 6.33e+06/7.40e+07 =  8% of the original kernel matrix.

torch.Size([14470, 2])
We keep 3.06e+06/1.14e+08 =  2% of the original kernel matrix.

torch.Size([15475, 2])
We keep 5.66e+06/1.38e+08 =  4% of the original kernel matrix.

torch.Size([17013, 2])
We keep 3.86e+06/1.55e+08 =  2% of the original kernel matrix.

torch.Size([61464, 2])
We keep 7.41e+07/2.61e+09 =  2% of the original kernel matrix.

torch.Size([32933, 2])
We keep 1.31e+07/6.77e+08 =  1% of the original kernel matrix.

torch.Size([11157, 2])
We keep 1.92e+06/4.24e+07 =  4% of the original kernel matrix.

torch.Size([14775, 2])
We keep 2.46e+06/8.63e+07 =  2% of the original kernel matrix.

torch.Size([2226, 2])
We keep 1.12e+05/1.30e+06 =  8% of the original kernel matrix.

torch.Size([6813, 2])
We keep 6.70e+05/1.51e+07 =  4% of the original kernel matrix.

torch.Size([3202, 2])
We keep 2.48e+05/2.82e+06 =  8% of the original kernel matrix.

torch.Size([8036, 2])
We keep 8.67e+05/2.22e+07 =  3% of the original kernel matrix.

torch.Size([7055, 2])
We keep 2.08e+06/2.07e+07 = 10% of the original kernel matrix.

torch.Size([11570, 2])
We keep 1.83e+06/6.03e+07 =  3% of the original kernel matrix.

torch.Size([22953, 2])
We keep 1.52e+07/2.33e+08 =  6% of the original kernel matrix.

torch.Size([21208, 2])
We keep 4.90e+06/2.02e+08 =  2% of the original kernel matrix.

torch.Size([13412, 2])
We keep 3.22e+06/8.88e+07 =  3% of the original kernel matrix.

torch.Size([16427, 2])
We keep 3.41e+06/1.25e+08 =  2% of the original kernel matrix.

torch.Size([5243, 2])
We keep 3.62e+05/5.60e+06 =  6% of the original kernel matrix.

torch.Size([10129, 2])
We keep 1.13e+06/3.14e+07 =  3% of the original kernel matrix.

torch.Size([11260, 2])
We keep 1.51e+06/4.29e+07 =  3% of the original kernel matrix.

torch.Size([14891, 2])
We keep 2.43e+06/8.68e+07 =  2% of the original kernel matrix.

torch.Size([10678, 2])
We keep 2.49e+06/4.12e+07 =  6% of the original kernel matrix.

torch.Size([14374, 2])
We keep 2.41e+06/8.50e+07 =  2% of the original kernel matrix.

torch.Size([528610, 2])
We keep 1.75e+09/1.52e+11 =  1% of the original kernel matrix.

torch.Size([113735, 2])
We keep 8.30e+07/5.16e+09 =  1% of the original kernel matrix.

torch.Size([15695, 2])
We keep 4.40e+06/1.01e+08 =  4% of the original kernel matrix.

torch.Size([17601, 2])
We keep 3.46e+06/1.33e+08 =  2% of the original kernel matrix.

torch.Size([26908, 2])
We keep 7.18e+06/2.78e+08 =  2% of the original kernel matrix.

torch.Size([23256, 2])
We keep 5.13e+06/2.21e+08 =  2% of the original kernel matrix.

torch.Size([6435, 2])
We keep 1.26e+06/1.97e+07 =  6% of the original kernel matrix.

torch.Size([10729, 2])
We keep 1.75e+06/5.88e+07 =  2% of the original kernel matrix.

torch.Size([5948, 2])
We keep 6.83e+05/9.79e+06 =  6% of the original kernel matrix.

torch.Size([10900, 2])
We keep 1.35e+06/4.14e+07 =  3% of the original kernel matrix.

torch.Size([1333, 2])
We keep 4.03e+04/3.47e+05 = 11% of the original kernel matrix.

torch.Size([5696, 2])
We keep 4.27e+05/7.80e+06 =  5% of the original kernel matrix.

torch.Size([5224, 2])
We keep 4.69e+05/6.68e+06 =  7% of the original kernel matrix.

torch.Size([9869, 2])
We keep 1.21e+06/3.42e+07 =  3% of the original kernel matrix.

torch.Size([35846, 2])
We keep 1.95e+07/8.43e+08 =  2% of the original kernel matrix.

torch.Size([26394, 2])
We keep 8.47e+06/3.85e+08 =  2% of the original kernel matrix.

torch.Size([3854, 2])
We keep 3.40e+05/3.44e+06 =  9% of the original kernel matrix.

torch.Size([8919, 2])
We keep 9.63e+05/2.46e+07 =  3% of the original kernel matrix.

torch.Size([18397, 2])
We keep 3.47e+06/1.28e+08 =  2% of the original kernel matrix.

torch.Size([19404, 2])
We keep 3.73e+06/1.50e+08 =  2% of the original kernel matrix.

torch.Size([668, 2])
We keep 7.72e+03/4.71e+04 = 16% of the original kernel matrix.

torch.Size([4677, 2])
We keep 2.13e+05/2.87e+06 =  7% of the original kernel matrix.

torch.Size([2896, 2])
We keep 1.51e+05/1.77e+06 =  8% of the original kernel matrix.

torch.Size([7662, 2])
We keep 7.47e+05/1.76e+07 =  4% of the original kernel matrix.

torch.Size([10636, 2])
We keep 1.80e+06/4.24e+07 =  4% of the original kernel matrix.

torch.Size([14472, 2])
We keep 2.44e+06/8.62e+07 =  2% of the original kernel matrix.

torch.Size([18667, 2])
We keep 4.75e+06/1.47e+08 =  3% of the original kernel matrix.

torch.Size([19240, 2])
We keep 4.01e+06/1.61e+08 =  2% of the original kernel matrix.

torch.Size([7376, 2])
We keep 8.19e+05/1.48e+07 =  5% of the original kernel matrix.

torch.Size([11914, 2])
We keep 1.63e+06/5.10e+07 =  3% of the original kernel matrix.

torch.Size([2455, 2])
We keep 9.42e+04/1.04e+06 =  9% of the original kernel matrix.

torch.Size([7311, 2])
We keep 6.20e+05/1.35e+07 =  4% of the original kernel matrix.

torch.Size([2055, 2])
We keep 1.27e+05/9.55e+05 = 13% of the original kernel matrix.

torch.Size([6692, 2])
We keep 6.12e+05/1.29e+07 =  4% of the original kernel matrix.

torch.Size([9007, 2])
We keep 1.72e+06/2.40e+07 =  7% of the original kernel matrix.

torch.Size([13267, 2])
We keep 1.99e+06/6.49e+07 =  3% of the original kernel matrix.

torch.Size([9050, 2])
We keep 1.64e+06/2.36e+07 =  6% of the original kernel matrix.

torch.Size([13288, 2])
We keep 1.97e+06/6.44e+07 =  3% of the original kernel matrix.

torch.Size([38384, 2])
We keep 1.66e+07/5.98e+08 =  2% of the original kernel matrix.

torch.Size([27112, 2])
We keep 7.08e+06/3.24e+08 =  2% of the original kernel matrix.

torch.Size([2217, 2])
We keep 9.51e+04/9.64e+05 =  9% of the original kernel matrix.

torch.Size([6996, 2])
We keep 5.99e+05/1.30e+07 =  4% of the original kernel matrix.

torch.Size([5427, 2])
We keep 7.14e+05/1.08e+07 =  6% of the original kernel matrix.

torch.Size([10049, 2])
We keep 1.44e+06/4.35e+07 =  3% of the original kernel matrix.

torch.Size([2863, 2])
We keep 4.74e+05/3.07e+06 = 15% of the original kernel matrix.

torch.Size([7553, 2])
We keep 9.04e+05/2.32e+07 =  3% of the original kernel matrix.

torch.Size([4008, 2])
We keep 7.45e+05/4.58e+06 = 16% of the original kernel matrix.

torch.Size([8596, 2])
We keep 9.99e+05/2.83e+07 =  3% of the original kernel matrix.

torch.Size([37427, 2])
We keep 3.31e+07/7.55e+08 =  4% of the original kernel matrix.

torch.Size([26908, 2])
We keep 7.88e+06/3.64e+08 =  2% of the original kernel matrix.

torch.Size([1795, 2])
We keep 6.23e+04/6.04e+05 = 10% of the original kernel matrix.

torch.Size([6511, 2])
We keep 5.22e+05/1.03e+07 =  5% of the original kernel matrix.

torch.Size([16254, 2])
We keep 4.28e+06/1.20e+08 =  3% of the original kernel matrix.

torch.Size([17869, 2])
We keep 3.68e+06/1.45e+08 =  2% of the original kernel matrix.

torch.Size([98205, 2])
We keep 9.69e+07/3.87e+09 =  2% of the original kernel matrix.

torch.Size([44303, 2])
We keep 1.57e+07/8.24e+08 =  1% of the original kernel matrix.

torch.Size([11440, 2])
We keep 1.85e+06/4.10e+07 =  4% of the original kernel matrix.

torch.Size([15044, 2])
We keep 2.45e+06/8.48e+07 =  2% of the original kernel matrix.

torch.Size([29606, 2])
We keep 1.77e+07/3.91e+08 =  4% of the original kernel matrix.

torch.Size([24124, 2])
We keep 6.01e+06/2.62e+08 =  2% of the original kernel matrix.

torch.Size([24847, 2])
We keep 6.54e+06/2.60e+08 =  2% of the original kernel matrix.

torch.Size([22365, 2])
We keep 5.00e+06/2.14e+08 =  2% of the original kernel matrix.

torch.Size([984, 2])
We keep 1.84e+04/1.37e+05 = 13% of the original kernel matrix.

torch.Size([5274, 2])
We keep 3.09e+05/4.90e+06 =  6% of the original kernel matrix.

torch.Size([10909, 2])
We keep 1.72e+06/3.70e+07 =  4% of the original kernel matrix.

torch.Size([14555, 2])
We keep 2.35e+06/8.05e+07 =  2% of the original kernel matrix.

torch.Size([5817, 2])
We keep 4.82e+05/8.52e+06 =  5% of the original kernel matrix.

torch.Size([10653, 2])
We keep 1.33e+06/3.87e+07 =  3% of the original kernel matrix.

torch.Size([60307, 2])
We keep 6.32e+07/2.08e+09 =  3% of the original kernel matrix.

torch.Size([33766, 2])
We keep 1.23e+07/6.04e+08 =  2% of the original kernel matrix.

torch.Size([27148, 2])
We keep 1.20e+07/3.65e+08 =  3% of the original kernel matrix.

torch.Size([23046, 2])
We keep 5.88e+06/2.53e+08 =  2% of the original kernel matrix.

torch.Size([5712, 2])
We keep 6.13e+05/8.16e+06 =  7% of the original kernel matrix.

torch.Size([10520, 2])
We keep 1.31e+06/3.78e+07 =  3% of the original kernel matrix.

torch.Size([27060, 2])
We keep 1.15e+07/2.96e+08 =  3% of the original kernel matrix.

torch.Size([23094, 2])
We keep 5.30e+06/2.28e+08 =  2% of the original kernel matrix.

torch.Size([6449, 2])
We keep 7.09e+05/1.15e+07 =  6% of the original kernel matrix.

torch.Size([11084, 2])
We keep 1.43e+06/4.50e+07 =  3% of the original kernel matrix.

torch.Size([2008, 2])
We keep 1.12e+06/4.43e+06 = 25% of the original kernel matrix.

torch.Size([6041, 2])
We keep 1.00e+06/2.79e+07 =  3% of the original kernel matrix.

torch.Size([1745, 2])
We keep 4.59e+04/4.40e+05 = 10% of the original kernel matrix.

torch.Size([6540, 2])
We keep 4.62e+05/8.78e+06 =  5% of the original kernel matrix.

torch.Size([12546, 2])
We keep 3.27e+06/8.85e+07 =  3% of the original kernel matrix.

torch.Size([15685, 2])
We keep 3.41e+06/1.25e+08 =  2% of the original kernel matrix.

torch.Size([39636, 2])
We keep 2.05e+07/6.45e+08 =  3% of the original kernel matrix.

torch.Size([27670, 2])
We keep 7.37e+06/3.37e+08 =  2% of the original kernel matrix.

torch.Size([189278, 2])
We keep 3.91e+08/2.30e+10 =  1% of the original kernel matrix.

torch.Size([63223, 2])
We keep 3.56e+07/2.01e+09 =  1% of the original kernel matrix.

torch.Size([12624, 2])
We keep 3.73e+06/7.08e+07 =  5% of the original kernel matrix.

torch.Size([15308, 2])
We keep 2.94e+06/1.11e+08 =  2% of the original kernel matrix.

torch.Size([5748, 2])
We keep 7.04e+05/9.49e+06 =  7% of the original kernel matrix.

torch.Size([10571, 2])
We keep 1.38e+06/4.08e+07 =  3% of the original kernel matrix.

torch.Size([11901, 2])
We keep 2.21e+06/5.24e+07 =  4% of the original kernel matrix.

torch.Size([15333, 2])
We keep 2.61e+06/9.59e+07 =  2% of the original kernel matrix.

torch.Size([35008, 2])
We keep 2.01e+07/7.67e+08 =  2% of the original kernel matrix.

torch.Size([26275, 2])
We keep 8.13e+06/3.67e+08 =  2% of the original kernel matrix.

torch.Size([5630, 2])
We keep 4.49e+05/6.89e+06 =  6% of the original kernel matrix.

torch.Size([10439, 2])
We keep 1.23e+06/3.48e+07 =  3% of the original kernel matrix.

torch.Size([4202, 2])
We keep 2.99e+05/4.19e+06 =  7% of the original kernel matrix.

torch.Size([9200, 2])
We keep 1.01e+06/2.71e+07 =  3% of the original kernel matrix.

torch.Size([15369, 2])
We keep 3.41e+06/9.89e+07 =  3% of the original kernel matrix.

torch.Size([17716, 2])
We keep 3.43e+06/1.32e+08 =  2% of the original kernel matrix.

torch.Size([39048, 2])
We keep 1.79e+07/5.63e+08 =  3% of the original kernel matrix.

torch.Size([27523, 2])
We keep 6.90e+06/3.14e+08 =  2% of the original kernel matrix.

torch.Size([8918, 2])
We keep 1.73e+06/3.59e+07 =  4% of the original kernel matrix.

torch.Size([13008, 2])
We keep 2.38e+06/7.93e+07 =  2% of the original kernel matrix.

torch.Size([7433, 2])
We keep 1.88e+06/1.77e+07 = 10% of the original kernel matrix.

torch.Size([12051, 2])
We keep 1.76e+06/5.57e+07 =  3% of the original kernel matrix.

torch.Size([1680, 2])
We keep 5.23e+04/5.21e+05 = 10% of the original kernel matrix.

torch.Size([6489, 2])
We keep 4.80e+05/9.56e+06 =  5% of the original kernel matrix.

time for making ranges is 1.708531379699707
Sorting X and nu_X
time for sorting X is 0.05404782295227051
Sorting Z and nu_Z
time for sorting Z is 0.0002617835998535156
Starting Optim
sum tnu_Z before tensor(11829296., device='cuda:0')
c= tensor(190.1308, device='cuda:0')
c= tensor(15633.1621, device='cuda:0')
c= tensor(38432.6562, device='cuda:0')
c= tensor(44321.8008, device='cuda:0')
c= tensor(49400.7461, device='cuda:0')
c= tensor(80713.9453, device='cuda:0')
c= tensor(224191.5000, device='cuda:0')
c= tensor(288510.7500, device='cuda:0')
c= tensor(398134.7500, device='cuda:0')
c= tensor(968765.0625, device='cuda:0')
c= tensor(985791.9375, device='cuda:0')
c= tensor(1154587., device='cuda:0')
c= tensor(1156698.5000, device='cuda:0')
c= tensor(3988494., device='cuda:0')
c= tensor(4024894., device='cuda:0')
c= tensor(4045535.7500, device='cuda:0')
c= tensor(4105888., device='cuda:0')
c= tensor(4326323.5000, device='cuda:0')
c= tensor(5007712.5000, device='cuda:0')
c= tensor(5538882., device='cuda:0')
c= tensor(5545727.5000, device='cuda:0')
c= tensor(6708141., device='cuda:0')
c= tensor(6715035.5000, device='cuda:0')
c= tensor(6799173.5000, device='cuda:0')
c= tensor(6800368.5000, device='cuda:0')
c= tensor(6886912.5000, device='cuda:0')
c= tensor(7022523.5000, device='cuda:0')
c= tensor(7025429., device='cuda:0')
c= tensor(10056090., device='cuda:0')
c= tensor(26079410., device='cuda:0')
c= tensor(26085374., device='cuda:0')
c= tensor(97984928., device='cuda:0')
c= tensor(97990952., device='cuda:0')
c= tensor(97997528., device='cuda:0')
c= tensor(98001160., device='cuda:0')
c= tensor(1.0243e+08, device='cuda:0')
c= tensor(1.0254e+08, device='cuda:0')
c= tensor(1.0254e+08, device='cuda:0')
c= tensor(1.0254e+08, device='cuda:0')
c= tensor(1.0255e+08, device='cuda:0')
c= tensor(1.0255e+08, device='cuda:0')
c= tensor(1.0255e+08, device='cuda:0')
c= tensor(1.0255e+08, device='cuda:0')
c= tensor(1.0255e+08, device='cuda:0')
c= tensor(1.0255e+08, device='cuda:0')
c= tensor(1.0255e+08, device='cuda:0')
c= tensor(1.0255e+08, device='cuda:0')
c= tensor(1.0256e+08, device='cuda:0')
c= tensor(1.0256e+08, device='cuda:0')
c= tensor(1.0256e+08, device='cuda:0')
c= tensor(1.0257e+08, device='cuda:0')
c= tensor(1.0258e+08, device='cuda:0')
c= tensor(1.0258e+08, device='cuda:0')
c= tensor(1.0258e+08, device='cuda:0')
c= tensor(1.0258e+08, device='cuda:0')
c= tensor(1.0258e+08, device='cuda:0')
c= tensor(1.0263e+08, device='cuda:0')
c= tensor(1.0263e+08, device='cuda:0')
c= tensor(1.0263e+08, device='cuda:0')
c= tensor(1.0263e+08, device='cuda:0')
c= tensor(1.0263e+08, device='cuda:0')
c= tensor(1.0263e+08, device='cuda:0')
c= tensor(1.0263e+08, device='cuda:0')
c= tensor(1.0263e+08, device='cuda:0')
c= tensor(1.0263e+08, device='cuda:0')
c= tensor(1.0264e+08, device='cuda:0')
c= tensor(1.0264e+08, device='cuda:0')
c= tensor(1.0264e+08, device='cuda:0')
c= tensor(1.0264e+08, device='cuda:0')
c= tensor(1.0264e+08, device='cuda:0')
c= tensor(1.0264e+08, device='cuda:0')
c= tensor(1.0264e+08, device='cuda:0')
c= tensor(1.0264e+08, device='cuda:0')
c= tensor(1.0264e+08, device='cuda:0')
c= tensor(1.0264e+08, device='cuda:0')
c= tensor(1.0265e+08, device='cuda:0')
c= tensor(1.0265e+08, device='cuda:0')
c= tensor(1.0266e+08, device='cuda:0')
c= tensor(1.0266e+08, device='cuda:0')
c= tensor(1.0266e+08, device='cuda:0')
c= tensor(1.0267e+08, device='cuda:0')
c= tensor(1.0267e+08, device='cuda:0')
c= tensor(1.0267e+08, device='cuda:0')
c= tensor(1.0267e+08, device='cuda:0')
c= tensor(1.0267e+08, device='cuda:0')
c= tensor(1.0269e+08, device='cuda:0')
c= tensor(1.0269e+08, device='cuda:0')
c= tensor(1.0269e+08, device='cuda:0')
c= tensor(1.0269e+08, device='cuda:0')
c= tensor(1.0273e+08, device='cuda:0')
c= tensor(1.0273e+08, device='cuda:0')
c= tensor(1.0273e+08, device='cuda:0')
c= tensor(1.0273e+08, device='cuda:0')
c= tensor(1.0275e+08, device='cuda:0')
c= tensor(1.0275e+08, device='cuda:0')
c= tensor(1.0275e+08, device='cuda:0')
c= tensor(1.0276e+08, device='cuda:0')
c= tensor(1.0276e+08, device='cuda:0')
c= tensor(1.0276e+08, device='cuda:0')
c= tensor(1.0276e+08, device='cuda:0')
c= tensor(1.0277e+08, device='cuda:0')
c= tensor(1.0277e+08, device='cuda:0')
c= tensor(1.0277e+08, device='cuda:0')
c= tensor(1.0277e+08, device='cuda:0')
c= tensor(1.0278e+08, device='cuda:0')
c= tensor(1.0278e+08, device='cuda:0')
c= tensor(1.0278e+08, device='cuda:0')
c= tensor(1.0278e+08, device='cuda:0')
c= tensor(1.0278e+08, device='cuda:0')
c= tensor(1.0278e+08, device='cuda:0')
c= tensor(1.0280e+08, device='cuda:0')
c= tensor(1.0280e+08, device='cuda:0')
c= tensor(1.0280e+08, device='cuda:0')
c= tensor(1.0280e+08, device='cuda:0')
c= tensor(1.0281e+08, device='cuda:0')
c= tensor(1.0281e+08, device='cuda:0')
c= tensor(1.0282e+08, device='cuda:0')
c= tensor(1.0282e+08, device='cuda:0')
c= tensor(1.0283e+08, device='cuda:0')
c= tensor(1.0283e+08, device='cuda:0')
c= tensor(1.0283e+08, device='cuda:0')
c= tensor(1.0283e+08, device='cuda:0')
c= tensor(1.0283e+08, device='cuda:0')
c= tensor(1.0287e+08, device='cuda:0')
c= tensor(1.0287e+08, device='cuda:0')
c= tensor(1.0287e+08, device='cuda:0')
c= tensor(1.0288e+08, device='cuda:0')
c= tensor(1.0288e+08, device='cuda:0')
c= tensor(1.0288e+08, device='cuda:0')
c= tensor(1.0288e+08, device='cuda:0')
c= tensor(1.0288e+08, device='cuda:0')
c= tensor(1.0288e+08, device='cuda:0')
c= tensor(1.0288e+08, device='cuda:0')
c= tensor(1.0288e+08, device='cuda:0')
c= tensor(1.0288e+08, device='cuda:0')
c= tensor(1.0298e+08, device='cuda:0')
c= tensor(1.0298e+08, device='cuda:0')
c= tensor(1.0298e+08, device='cuda:0')
c= tensor(1.0298e+08, device='cuda:0')
c= tensor(1.0298e+08, device='cuda:0')
c= tensor(1.0298e+08, device='cuda:0')
c= tensor(1.0298e+08, device='cuda:0')
c= tensor(1.0299e+08, device='cuda:0')
c= tensor(1.0299e+08, device='cuda:0')
c= tensor(1.0299e+08, device='cuda:0')
c= tensor(1.0299e+08, device='cuda:0')
c= tensor(1.0299e+08, device='cuda:0')
c= tensor(1.0299e+08, device='cuda:0')
c= tensor(1.0299e+08, device='cuda:0')
c= tensor(1.0299e+08, device='cuda:0')
c= tensor(1.0300e+08, device='cuda:0')
c= tensor(1.0300e+08, device='cuda:0')
c= tensor(1.0300e+08, device='cuda:0')
c= tensor(1.0300e+08, device='cuda:0')
c= tensor(1.0300e+08, device='cuda:0')
c= tensor(1.0303e+08, device='cuda:0')
c= tensor(1.0303e+08, device='cuda:0')
c= tensor(1.0303e+08, device='cuda:0')
c= tensor(1.0303e+08, device='cuda:0')
c= tensor(1.0303e+08, device='cuda:0')
c= tensor(1.0303e+08, device='cuda:0')
c= tensor(1.0303e+08, device='cuda:0')
c= tensor(1.0304e+08, device='cuda:0')
c= tensor(1.0304e+08, device='cuda:0')
c= tensor(1.0304e+08, device='cuda:0')
c= tensor(1.0304e+08, device='cuda:0')
c= tensor(1.0304e+08, device='cuda:0')
c= tensor(1.0304e+08, device='cuda:0')
c= tensor(1.0305e+08, device='cuda:0')
c= tensor(1.0305e+08, device='cuda:0')
c= tensor(1.0305e+08, device='cuda:0')
c= tensor(1.0305e+08, device='cuda:0')
c= tensor(1.0305e+08, device='cuda:0')
c= tensor(1.0305e+08, device='cuda:0')
c= tensor(1.0305e+08, device='cuda:0')
c= tensor(1.0305e+08, device='cuda:0')
c= tensor(1.0305e+08, device='cuda:0')
c= tensor(1.0306e+08, device='cuda:0')
c= tensor(1.0306e+08, device='cuda:0')
c= tensor(1.0306e+08, device='cuda:0')
c= tensor(1.0309e+08, device='cuda:0')
c= tensor(1.0309e+08, device='cuda:0')
c= tensor(1.0309e+08, device='cuda:0')
c= tensor(1.0309e+08, device='cuda:0')
c= tensor(1.0309e+08, device='cuda:0')
c= tensor(1.0309e+08, device='cuda:0')
c= tensor(1.0310e+08, device='cuda:0')
c= tensor(1.0310e+08, device='cuda:0')
c= tensor(1.0318e+08, device='cuda:0')
c= tensor(1.0319e+08, device='cuda:0')
c= tensor(1.0319e+08, device='cuda:0')
c= tensor(1.0319e+08, device='cuda:0')
c= tensor(1.0319e+08, device='cuda:0')
c= tensor(1.0319e+08, device='cuda:0')
c= tensor(1.0319e+08, device='cuda:0')
c= tensor(1.0319e+08, device='cuda:0')
c= tensor(1.0319e+08, device='cuda:0')
c= tensor(1.0319e+08, device='cuda:0')
c= tensor(1.0319e+08, device='cuda:0')
c= tensor(1.0319e+08, device='cuda:0')
c= tensor(1.0322e+08, device='cuda:0')
c= tensor(1.0322e+08, device='cuda:0')
c= tensor(1.0322e+08, device='cuda:0')
c= tensor(1.0322e+08, device='cuda:0')
c= tensor(1.0323e+08, device='cuda:0')
c= tensor(1.0323e+08, device='cuda:0')
c= tensor(1.0323e+08, device='cuda:0')
c= tensor(1.0324e+08, device='cuda:0')
c= tensor(1.0324e+08, device='cuda:0')
c= tensor(1.0324e+08, device='cuda:0')
c= tensor(1.0324e+08, device='cuda:0')
c= tensor(1.0324e+08, device='cuda:0')
c= tensor(1.0324e+08, device='cuda:0')
c= tensor(1.0324e+08, device='cuda:0')
c= tensor(1.0324e+08, device='cuda:0')
c= tensor(1.0324e+08, device='cuda:0')
c= tensor(1.0324e+08, device='cuda:0')
c= tensor(1.0324e+08, device='cuda:0')
c= tensor(1.0325e+08, device='cuda:0')
c= tensor(1.0325e+08, device='cuda:0')
c= tensor(1.0325e+08, device='cuda:0')
c= tensor(1.0325e+08, device='cuda:0')
c= tensor(1.0325e+08, device='cuda:0')
c= tensor(1.0325e+08, device='cuda:0')
c= tensor(1.0326e+08, device='cuda:0')
c= tensor(1.0326e+08, device='cuda:0')
c= tensor(1.0326e+08, device='cuda:0')
c= tensor(1.0326e+08, device='cuda:0')
c= tensor(1.0326e+08, device='cuda:0')
c= tensor(1.0326e+08, device='cuda:0')
c= tensor(1.0326e+08, device='cuda:0')
c= tensor(1.0326e+08, device='cuda:0')
c= tensor(1.0326e+08, device='cuda:0')
c= tensor(1.0328e+08, device='cuda:0')
c= tensor(1.0328e+08, device='cuda:0')
c= tensor(1.0328e+08, device='cuda:0')
c= tensor(1.0328e+08, device='cuda:0')
c= tensor(1.0328e+08, device='cuda:0')
c= tensor(1.0328e+08, device='cuda:0')
c= tensor(1.0330e+08, device='cuda:0')
c= tensor(1.0343e+08, device='cuda:0')
c= tensor(1.0343e+08, device='cuda:0')
c= tensor(1.0343e+08, device='cuda:0')
c= tensor(1.0343e+08, device='cuda:0')
c= tensor(1.0343e+08, device='cuda:0')
c= tensor(1.0344e+08, device='cuda:0')
c= tensor(1.0552e+08, device='cuda:0')
c= tensor(1.0552e+08, device='cuda:0')
c= tensor(1.0570e+08, device='cuda:0')
c= tensor(1.0581e+08, device='cuda:0')
c= tensor(1.0582e+08, device='cuda:0')
c= tensor(1.0585e+08, device='cuda:0')
c= tensor(1.0585e+08, device='cuda:0')
c= tensor(1.0585e+08, device='cuda:0')
c= tensor(1.1058e+08, device='cuda:0')
c= tensor(1.1216e+08, device='cuda:0')
c= tensor(1.1216e+08, device='cuda:0')
c= tensor(1.1219e+08, device='cuda:0')
c= tensor(1.1292e+08, device='cuda:0')
c= tensor(1.1296e+08, device='cuda:0')
c= tensor(1.1308e+08, device='cuda:0')
c= tensor(1.1310e+08, device='cuda:0')
c= tensor(1.1315e+08, device='cuda:0')
c= tensor(1.1318e+08, device='cuda:0')
c= tensor(1.1319e+08, device='cuda:0')
c= tensor(1.1570e+08, device='cuda:0')
c= tensor(1.1570e+08, device='cuda:0')
c= tensor(1.1570e+08, device='cuda:0')
c= tensor(1.1571e+08, device='cuda:0')
c= tensor(1.1577e+08, device='cuda:0')
c= tensor(1.1754e+08, device='cuda:0')
c= tensor(1.1764e+08, device='cuda:0')
c= tensor(1.1764e+08, device='cuda:0')
c= tensor(1.1766e+08, device='cuda:0')
c= tensor(1.1766e+08, device='cuda:0')
c= tensor(1.1773e+08, device='cuda:0')
c= tensor(1.1793e+08, device='cuda:0')
c= tensor(1.1826e+08, device='cuda:0')
c= tensor(1.1833e+08, device='cuda:0')
c= tensor(1.1833e+08, device='cuda:0')
c= tensor(1.1833e+08, device='cuda:0')
c= tensor(1.1840e+08, device='cuda:0')
c= tensor(1.1847e+08, device='cuda:0')
c= tensor(1.1856e+08, device='cuda:0')
c= tensor(1.1856e+08, device='cuda:0')
c= tensor(1.2167e+08, device='cuda:0')
c= tensor(1.2170e+08, device='cuda:0')
c= tensor(1.2172e+08, device='cuda:0')
c= tensor(1.2243e+08, device='cuda:0')
c= tensor(1.2243e+08, device='cuda:0')
c= tensor(1.2248e+08, device='cuda:0')
c= tensor(1.2353e+08, device='cuda:0')
c= tensor(1.3272e+08, device='cuda:0')
c= tensor(1.3274e+08, device='cuda:0')
c= tensor(1.3276e+08, device='cuda:0')
c= tensor(1.3277e+08, device='cuda:0')
c= tensor(1.3277e+08, device='cuda:0')
c= tensor(1.3325e+08, device='cuda:0')
c= tensor(1.3326e+08, device='cuda:0')
c= tensor(1.3331e+08, device='cuda:0')
c= tensor(1.3411e+08, device='cuda:0')
c= tensor(1.3417e+08, device='cuda:0')
c= tensor(1.3422e+08, device='cuda:0')
c= tensor(1.3424e+08, device='cuda:0')
c= tensor(1.4187e+08, device='cuda:0')
c= tensor(1.4188e+08, device='cuda:0')
c= tensor(1.4190e+08, device='cuda:0')
c= tensor(1.4190e+08, device='cuda:0')
c= tensor(1.4279e+08, device='cuda:0')
c= tensor(1.4280e+08, device='cuda:0')
c= tensor(1.4298e+08, device='cuda:0')
c= tensor(1.4299e+08, device='cuda:0')
c= tensor(1.4331e+08, device='cuda:0')
c= tensor(1.4334e+08, device='cuda:0')
c= tensor(1.4637e+08, device='cuda:0')
c= tensor(1.4658e+08, device='cuda:0')
c= tensor(1.4658e+08, device='cuda:0')
c= tensor(1.4707e+08, device='cuda:0')
c= tensor(1.4733e+08, device='cuda:0')
c= tensor(1.4734e+08, device='cuda:0')
c= tensor(1.4749e+08, device='cuda:0')
c= tensor(1.4841e+08, device='cuda:0')
c= tensor(1.5268e+08, device='cuda:0')
c= tensor(1.5271e+08, device='cuda:0')
c= tensor(1.5271e+08, device='cuda:0')
c= tensor(1.5272e+08, device='cuda:0')
c= tensor(1.5273e+08, device='cuda:0')
c= tensor(1.5274e+08, device='cuda:0')
c= tensor(1.5274e+08, device='cuda:0')
c= tensor(1.5274e+08, device='cuda:0')
c= tensor(1.5292e+08, device='cuda:0')
c= tensor(1.5320e+08, device='cuda:0')
c= tensor(1.5324e+08, device='cuda:0')
c= tensor(1.5325e+08, device='cuda:0')
c= tensor(1.5332e+08, device='cuda:0')
c= tensor(1.5332e+08, device='cuda:0')
c= tensor(1.5333e+08, device='cuda:0')
c= tensor(1.5333e+08, device='cuda:0')
c= tensor(1.5334e+08, device='cuda:0')
c= tensor(1.5583e+08, device='cuda:0')
c= tensor(1.5586e+08, device='cuda:0')
c= tensor(1.5587e+08, device='cuda:0')
c= tensor(1.5618e+08, device='cuda:0')
c= tensor(1.5618e+08, device='cuda:0')
c= tensor(1.5946e+08, device='cuda:0')
c= tensor(1.5946e+08, device='cuda:0')
c= tensor(1.5957e+08, device='cuda:0')
c= tensor(1.5957e+08, device='cuda:0')
c= tensor(1.5957e+08, device='cuda:0')
c= tensor(1.5957e+08, device='cuda:0')
c= tensor(1.5958e+08, device='cuda:0')
c= tensor(1.5958e+08, device='cuda:0')
c= tensor(1.5969e+08, device='cuda:0')
c= tensor(1.5969e+08, device='cuda:0')
c= tensor(1.5969e+08, device='cuda:0')
c= tensor(1.6047e+08, device='cuda:0')
c= tensor(1.6059e+08, device='cuda:0')
c= tensor(1.6065e+08, device='cuda:0')
c= tensor(1.6104e+08, device='cuda:0')
c= tensor(1.6377e+08, device='cuda:0')
c= tensor(1.6378e+08, device='cuda:0')
c= tensor(1.6378e+08, device='cuda:0')
c= tensor(1.6379e+08, device='cuda:0')
c= tensor(1.6380e+08, device='cuda:0')
c= tensor(1.6380e+08, device='cuda:0')
c= tensor(1.6380e+08, device='cuda:0')
c= tensor(1.6381e+08, device='cuda:0')
c= tensor(1.6381e+08, device='cuda:0')
c= tensor(1.6381e+08, device='cuda:0')
c= tensor(1.6381e+08, device='cuda:0')
c= tensor(1.7090e+08, device='cuda:0')
c= tensor(1.7090e+08, device='cuda:0')
c= tensor(1.7164e+08, device='cuda:0')
c= tensor(1.7164e+08, device='cuda:0')
c= tensor(1.7165e+08, device='cuda:0')
c= tensor(1.7167e+08, device='cuda:0')
c= tensor(1.7477e+08, device='cuda:0')
c= tensor(1.7783e+08, device='cuda:0')
c= tensor(1.7784e+08, device='cuda:0')
c= tensor(1.7790e+08, device='cuda:0')
c= tensor(1.7790e+08, device='cuda:0')
c= tensor(1.7791e+08, device='cuda:0')
c= tensor(2.2140e+08, device='cuda:0')
c= tensor(2.2143e+08, device='cuda:0')
c= tensor(2.2143e+08, device='cuda:0')
c= tensor(2.2163e+08, device='cuda:0')
c= tensor(2.2525e+08, device='cuda:0')
c= tensor(2.2528e+08, device='cuda:0')
c= tensor(2.2529e+08, device='cuda:0')
c= tensor(2.2529e+08, device='cuda:0')
c= tensor(2.2530e+08, device='cuda:0')
c= tensor(2.2530e+08, device='cuda:0')
c= tensor(2.2592e+08, device='cuda:0')
c= tensor(2.2593e+08, device='cuda:0')
c= tensor(2.2594e+08, device='cuda:0')
c= tensor(2.2595e+08, device='cuda:0')
c= tensor(2.2596e+08, device='cuda:0')
c= tensor(2.2596e+08, device='cuda:0')
c= tensor(2.2615e+08, device='cuda:0')
c= tensor(2.2631e+08, device='cuda:0')
c= tensor(2.2756e+08, device='cuda:0')
c= tensor(2.2786e+08, device='cuda:0')
c= tensor(2.2894e+08, device='cuda:0')
c= tensor(2.2896e+08, device='cuda:0')
c= tensor(2.2902e+08, device='cuda:0')
c= tensor(2.2913e+08, device='cuda:0')
c= tensor(2.2930e+08, device='cuda:0')
c= tensor(2.2930e+08, device='cuda:0')
c= tensor(2.3090e+08, device='cuda:0')
c= tensor(2.3184e+08, device='cuda:0')
c= tensor(2.3220e+08, device='cuda:0')
c= tensor(2.3236e+08, device='cuda:0')
c= tensor(2.3269e+08, device='cuda:0')
c= tensor(2.3271e+08, device='cuda:0')
c= tensor(2.3271e+08, device='cuda:0')
c= tensor(2.3291e+08, device='cuda:0')
c= tensor(2.3343e+08, device='cuda:0')
c= tensor(2.3380e+08, device='cuda:0')
c= tensor(2.3678e+08, device='cuda:0')
c= tensor(2.3798e+08, device='cuda:0')
c= tensor(2.3815e+08, device='cuda:0')
c= tensor(2.3818e+08, device='cuda:0')
c= tensor(2.4038e+08, device='cuda:0')
c= tensor(2.4039e+08, device='cuda:0')
c= tensor(2.4040e+08, device='cuda:0')
c= tensor(2.4197e+08, device='cuda:0')
c= tensor(2.4198e+08, device='cuda:0')
c= tensor(2.4198e+08, device='cuda:0')
c= tensor(2.4201e+08, device='cuda:0')
c= tensor(2.4452e+08, device='cuda:0')
c= tensor(2.4454e+08, device='cuda:0')
c= tensor(2.4468e+08, device='cuda:0')
c= tensor(2.4468e+08, device='cuda:0')
c= tensor(2.4468e+08, device='cuda:0')
c= tensor(2.4468e+08, device='cuda:0')
c= tensor(2.4472e+08, device='cuda:0')
c= tensor(2.4474e+08, device='cuda:0')
c= tensor(2.4488e+08, device='cuda:0')
c= tensor(2.4488e+08, device='cuda:0')
c= tensor(2.4513e+08, device='cuda:0')
c= tensor(2.4513e+08, device='cuda:0')
c= tensor(2.4523e+08, device='cuda:0')
c= tensor(2.4524e+08, device='cuda:0')
c= tensor(2.4535e+08, device='cuda:0')
c= tensor(2.4536e+08, device='cuda:0')
c= tensor(2.4537e+08, device='cuda:0')
c= tensor(2.4538e+08, device='cuda:0')
c= tensor(2.4541e+08, device='cuda:0')
c= tensor(2.4554e+08, device='cuda:0')
c= tensor(2.4643e+08, device='cuda:0')
c= tensor(2.4643e+08, device='cuda:0')
c= tensor(2.4644e+08, device='cuda:0')
c= tensor(2.4726e+08, device='cuda:0')
c= tensor(2.4727e+08, device='cuda:0')
c= tensor(2.5553e+08, device='cuda:0')
c= tensor(2.5553e+08, device='cuda:0')
c= tensor(2.5575e+08, device='cuda:0')
c= tensor(2.5662e+08, device='cuda:0')
c= tensor(2.5662e+08, device='cuda:0')
c= tensor(2.5709e+08, device='cuda:0')
c= tensor(2.5717e+08, device='cuda:0')
c= tensor(2.6078e+08, device='cuda:0')
c= tensor(2.6078e+08, device='cuda:0')
c= tensor(2.6079e+08, device='cuda:0')
c= tensor(2.6079e+08, device='cuda:0')
c= tensor(2.6079e+08, device='cuda:0')
c= tensor(2.6082e+08, device='cuda:0')
c= tensor(2.6085e+08, device='cuda:0')
c= tensor(2.6089e+08, device='cuda:0')
c= tensor(2.6130e+08, device='cuda:0')
c= tensor(2.6132e+08, device='cuda:0')
c= tensor(2.6132e+08, device='cuda:0')
c= tensor(2.6133e+08, device='cuda:0')
c= tensor(2.6236e+08, device='cuda:0')
c= tensor(2.6248e+08, device='cuda:0')
c= tensor(2.6347e+08, device='cuda:0')
c= tensor(2.6348e+08, device='cuda:0')
c= tensor(2.6348e+08, device='cuda:0')
c= tensor(2.6348e+08, device='cuda:0')
c= tensor(2.6349e+08, device='cuda:0')
c= tensor(2.6461e+08, device='cuda:0')
c= tensor(2.6462e+08, device='cuda:0')
c= tensor(2.6464e+08, device='cuda:0')
c= tensor(2.6495e+08, device='cuda:0')
c= tensor(2.6497e+08, device='cuda:0')
c= tensor(2.6497e+08, device='cuda:0')
c= tensor(2.6498e+08, device='cuda:0')
c= tensor(2.6618e+08, device='cuda:0')
c= tensor(2.6619e+08, device='cuda:0')
c= tensor(2.6620e+08, device='cuda:0')
c= tensor(2.6620e+08, device='cuda:0')
c= tensor(2.7348e+08, device='cuda:0')
c= tensor(2.7385e+08, device='cuda:0')
c= tensor(2.7544e+08, device='cuda:0')
c= tensor(2.7564e+08, device='cuda:0')
c= tensor(2.7564e+08, device='cuda:0')
c= tensor(2.7574e+08, device='cuda:0')
c= tensor(2.7575e+08, device='cuda:0')
c= tensor(2.7577e+08, device='cuda:0')
c= tensor(2.7578e+08, device='cuda:0')
c= tensor(2.7578e+08, device='cuda:0')
c= tensor(2.7582e+08, device='cuda:0')
c= tensor(2.7584e+08, device='cuda:0')
c= tensor(2.7584e+08, device='cuda:0')
c= tensor(2.7585e+08, device='cuda:0')
c= tensor(2.7586e+08, device='cuda:0')
c= tensor(2.7752e+08, device='cuda:0')
c= tensor(2.7752e+08, device='cuda:0')
c= tensor(2.7753e+08, device='cuda:0')
c= tensor(2.7753e+08, device='cuda:0')
c= tensor(2.7755e+08, device='cuda:0')
c= tensor(2.7755e+08, device='cuda:0')
c= tensor(2.7755e+08, device='cuda:0')
c= tensor(2.7757e+08, device='cuda:0')
c= tensor(2.7772e+08, device='cuda:0')
c= tensor(2.7772e+08, device='cuda:0')
c= tensor(2.7772e+08, device='cuda:0')
c= tensor(2.7772e+08, device='cuda:0')
c= tensor(2.7876e+08, device='cuda:0')
c= tensor(3.1634e+08, device='cuda:0')
c= tensor(3.1636e+08, device='cuda:0')
c= tensor(3.1637e+08, device='cuda:0')
c= tensor(3.1654e+08, device='cuda:0')
c= tensor(3.1667e+08, device='cuda:0')
c= tensor(3.1667e+08, device='cuda:0')
c= tensor(3.1672e+08, device='cuda:0')
c= tensor(3.1674e+08, device='cuda:0')
c= tensor(3.2581e+08, device='cuda:0')
c= tensor(3.2973e+08, device='cuda:0')
c= tensor(3.3140e+08, device='cuda:0')
c= tensor(3.3140e+08, device='cuda:0')
c= tensor(3.3141e+08, device='cuda:0')
c= tensor(3.3141e+08, device='cuda:0')
c= tensor(3.3148e+08, device='cuda:0')
c= tensor(3.3149e+08, device='cuda:0')
c= tensor(3.3151e+08, device='cuda:0')
c= tensor(3.3227e+08, device='cuda:0')
c= tensor(3.3596e+08, device='cuda:0')
c= tensor(3.3596e+08, device='cuda:0')
c= tensor(3.3596e+08, device='cuda:0')
c= tensor(3.3598e+08, device='cuda:0')
c= tensor(3.3606e+08, device='cuda:0')
c= tensor(3.3610e+08, device='cuda:0')
c= tensor(3.3610e+08, device='cuda:0')
c= tensor(3.3610e+08, device='cuda:0')
c= tensor(3.3611e+08, device='cuda:0')
c= tensor(3.3611e+08, device='cuda:0')
c= tensor(3.3650e+08, device='cuda:0')
c= tensor(3.3650e+08, device='cuda:0')
c= tensor(3.3651e+08, device='cuda:0')
c= tensor(3.3651e+08, device='cuda:0')
c= tensor(3.3651e+08, device='cuda:0')
c= tensor(3.3652e+08, device='cuda:0')
c= tensor(3.3746e+08, device='cuda:0')
c= tensor(3.4013e+08, device='cuda:0')
c= tensor(3.4058e+08, device='cuda:0')
c= tensor(3.4060e+08, device='cuda:0')
c= tensor(3.4062e+08, device='cuda:0')
c= tensor(3.4062e+08, device='cuda:0')
c= tensor(3.4063e+08, device='cuda:0')
c= tensor(3.4077e+08, device='cuda:0')
c= tensor(3.4079e+08, device='cuda:0')
c= tensor(3.4081e+08, device='cuda:0')
c= tensor(3.4084e+08, device='cuda:0')
c= tensor(3.5948e+08, device='cuda:0')
c= tensor(3.5949e+08, device='cuda:0')
c= tensor(3.5958e+08, device='cuda:0')
c= tensor(3.6423e+08, device='cuda:0')
c= tensor(3.6430e+08, device='cuda:0')
c= tensor(3.6432e+08, device='cuda:0')
c= tensor(3.6587e+08, device='cuda:0')
c= tensor(3.6612e+08, device='cuda:0')
c= tensor(3.6614e+08, device='cuda:0')
c= tensor(3.6614e+08, device='cuda:0')
c= tensor(3.6617e+08, device='cuda:0')
c= tensor(3.6618e+08, device='cuda:0')
c= tensor(3.6629e+08, device='cuda:0')
c= tensor(3.7540e+08, device='cuda:0')
c= tensor(3.7711e+08, device='cuda:0')
c= tensor(3.7750e+08, device='cuda:0')
c= tensor(3.7751e+08, device='cuda:0')
c= tensor(3.7753e+08, device='cuda:0')
c= tensor(3.7757e+08, device='cuda:0')
c= tensor(3.7757e+08, device='cuda:0')
c= tensor(3.8807e+08, device='cuda:0')
c= tensor(3.8898e+08, device='cuda:0')
c= tensor(3.8901e+08, device='cuda:0')
c= tensor(4.6976e+08, device='cuda:0')
c= tensor(4.7676e+08, device='cuda:0')
c= tensor(4.7679e+08, device='cuda:0')
c= tensor(4.7680e+08, device='cuda:0')
c= tensor(4.7682e+08, device='cuda:0')
c= tensor(4.7693e+08, device='cuda:0')
c= tensor(4.7694e+08, device='cuda:0')
c= tensor(4.8624e+08, device='cuda:0')
c= tensor(4.8630e+08, device='cuda:0')
c= tensor(4.8633e+08, device='cuda:0')
c= tensor(4.8634e+08, device='cuda:0')
c= tensor(4.8634e+08, device='cuda:0')
c= tensor(4.8634e+08, device='cuda:0')
c= tensor(4.8635e+08, device='cuda:0')
c= tensor(4.8637e+08, device='cuda:0')
c= tensor(4.8638e+08, device='cuda:0')
c= tensor(7.8381e+08, device='cuda:0')
c= tensor(7.8386e+08, device='cuda:0')
c= tensor(7.8396e+08, device='cuda:0')
c= tensor(7.8397e+08, device='cuda:0')
c= tensor(7.8398e+08, device='cuda:0')
c= tensor(7.8398e+08, device='cuda:0')
c= tensor(7.8481e+08, device='cuda:0')
c= tensor(7.8516e+08, device='cuda:0')
c= tensor(7.9084e+08, device='cuda:0')
c= tensor(7.9086e+08, device='cuda:0')
c= tensor(7.9196e+08, device='cuda:0')
c= tensor(7.9200e+08, device='cuda:0')
c= tensor(7.9232e+08, device='cuda:0')
c= tensor(7.9404e+08, device='cuda:0')
c= tensor(7.9405e+08, device='cuda:0')
c= tensor(7.9405e+08, device='cuda:0')
c= tensor(7.9420e+08, device='cuda:0')
c= tensor(7.9420e+08, device='cuda:0')
c= tensor(7.9424e+08, device='cuda:0')
c= tensor(7.9537e+08, device='cuda:0')
c= tensor(7.9538e+08, device='cuda:0')
c= tensor(7.9561e+08, device='cuda:0')
c= tensor(7.9562e+08, device='cuda:0')
c= tensor(7.9591e+08, device='cuda:0')
c= tensor(7.9704e+08, device='cuda:0')
c= tensor(7.9705e+08, device='cuda:0')
c= tensor(7.9705e+08, device='cuda:0')
c= tensor(8.0267e+08, device='cuda:0')
c= tensor(8.0283e+08, device='cuda:0')
c= tensor(8.1018e+08, device='cuda:0')
c= tensor(8.1026e+08, device='cuda:0')
c= tensor(8.1043e+08, device='cuda:0')
c= tensor(8.1054e+08, device='cuda:0')
c= tensor(8.1083e+08, device='cuda:0')
c= tensor(8.1271e+08, device='cuda:0')
c= tensor(8.1274e+08, device='cuda:0')
c= tensor(8.1274e+08, device='cuda:0')
c= tensor(8.1275e+08, device='cuda:0')
c= tensor(8.1279e+08, device='cuda:0')
c= tensor(8.1316e+08, device='cuda:0')
c= tensor(8.1321e+08, device='cuda:0')
c= tensor(8.1322e+08, device='cuda:0')
c= tensor(8.1326e+08, device='cuda:0')
c= tensor(8.1332e+08, device='cuda:0')
c= tensor(8.7307e+08, device='cuda:0')
c= tensor(8.7316e+08, device='cuda:0')
c= tensor(8.7340e+08, device='cuda:0')
c= tensor(8.7352e+08, device='cuda:0')
c= tensor(8.7354e+08, device='cuda:0')
c= tensor(8.7354e+08, device='cuda:0')
c= tensor(8.7355e+08, device='cuda:0')
c= tensor(8.7393e+08, device='cuda:0')
c= tensor(8.7393e+08, device='cuda:0')
c= tensor(8.7400e+08, device='cuda:0')
c= tensor(8.7400e+08, device='cuda:0')
c= tensor(8.7400e+08, device='cuda:0')
c= tensor(8.7405e+08, device='cuda:0')
c= tensor(8.7415e+08, device='cuda:0')
c= tensor(8.7416e+08, device='cuda:0')
c= tensor(8.7417e+08, device='cuda:0')
c= tensor(8.7417e+08, device='cuda:0')
c= tensor(8.7419e+08, device='cuda:0')
c= tensor(8.7422e+08, device='cuda:0')
c= tensor(8.7464e+08, device='cuda:0')
c= tensor(8.7464e+08, device='cuda:0')
c= tensor(8.7468e+08, device='cuda:0')
c= tensor(8.7469e+08, device='cuda:0')
c= tensor(8.7470e+08, device='cuda:0')
c= tensor(8.7546e+08, device='cuda:0')
c= tensor(8.7547e+08, device='cuda:0')
c= tensor(8.7591e+08, device='cuda:0')
c= tensor(8.7883e+08, device='cuda:0')
c= tensor(8.7886e+08, device='cuda:0')
c= tensor(8.7921e+08, device='cuda:0')
c= tensor(8.7934e+08, device='cuda:0')
c= tensor(8.7934e+08, device='cuda:0')
c= tensor(8.7937e+08, device='cuda:0')
c= tensor(8.7937e+08, device='cuda:0')
c= tensor(8.8044e+08, device='cuda:0')
c= tensor(8.8083e+08, device='cuda:0')
c= tensor(8.8084e+08, device='cuda:0')
c= tensor(8.8121e+08, device='cuda:0')
c= tensor(8.8122e+08, device='cuda:0')
c= tensor(8.8129e+08, device='cuda:0')
c= tensor(8.8130e+08, device='cuda:0')
c= tensor(8.8135e+08, device='cuda:0')
c= tensor(8.8191e+08, device='cuda:0')
c= tensor(8.9378e+08, device='cuda:0')
c= tensor(8.9388e+08, device='cuda:0')
c= tensor(8.9389e+08, device='cuda:0')
c= tensor(8.9393e+08, device='cuda:0')
c= tensor(8.9428e+08, device='cuda:0')
c= tensor(8.9428e+08, device='cuda:0')
c= tensor(8.9429e+08, device='cuda:0')
c= tensor(8.9435e+08, device='cuda:0')
c= tensor(8.9471e+08, device='cuda:0')
c= tensor(8.9473e+08, device='cuda:0')
c= tensor(8.9476e+08, device='cuda:0')
c= tensor(8.9476e+08, device='cuda:0')
memory (bytes)
3385049088
time for making loss 2 is 16.06557607650757
p0 True
it  0 : 626225152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  5% | 12% |
shape of L is 
torch.Size([])
memory (bytes)
3385253888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  5% |  5% |
memory (bytes)
3385847808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% |  7% |
error is  13321071000.0
relative error loss 14.887814
shape of L is 
torch.Size([])
memory (bytes)
3659550720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  7% |
memory (bytes)
3659554816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  13320911000.0
relative error loss 14.887635
shape of L is 
torch.Size([])
memory (bytes)
3663765504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% |  7% |
memory (bytes)
3663765504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  13320188000.0
relative error loss 14.886827
shape of L is 
torch.Size([])
memory (bytes)
3665846272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
3665887232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  13312252000.0
relative error loss 14.877957
shape of L is 
torch.Size([])
memory (bytes)
3667914752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
3667951616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  13272586000.0
relative error loss 14.833627
shape of L is 
torch.Size([])
memory (bytes)
3670040576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
3670040576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% |  7% |
error is  12844769000.0
relative error loss 14.355493
shape of L is 
torch.Size([])
memory (bytes)
3672174592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% |  7% |
memory (bytes)
3672174592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  10920806000.0
relative error loss 12.205245
shape of L is 
torch.Size([])
memory (bytes)
3674218496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
3674222592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  7% |
error is  3710389200.0
relative error loss 4.1467824
shape of L is 
torch.Size([])
memory (bytes)
3676418048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
3676418048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  1861883900.0
relative error loss 2.0808673
shape of L is 
torch.Size([])
memory (bytes)
3678326784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
3678326784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  1191410200.0
relative error loss 1.3315365
time to take a step is 256.7571179866791
it  1 : 961539584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
3680559104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
3680559104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  7% |
error is  1191410200.0
relative error loss 1.3315365
shape of L is 
torch.Size([])
memory (bytes)
3682779136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  0% |  7% |
memory (bytes)
3682779136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  965049600.0
relative error loss 1.0785528
shape of L is 
torch.Size([])
memory (bytes)
3684724736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
3684724736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  921059600.0
relative error loss 1.029389
shape of L is 
torch.Size([])
memory (bytes)
3687047168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
3687079936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  810121900.0
relative error loss 0.9054035
shape of L is 
torch.Size([])
memory (bytes)
3689140224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% |  7% |
memory (bytes)
3689140224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  805071800.0
relative error loss 0.8997594
shape of L is 
torch.Size([])
memory (bytes)
3691114496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
3691114496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  754959360.0
relative error loss 0.8437531
shape of L is 
torch.Size([])
memory (bytes)
3693408256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
3693408256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  676323650.0
relative error loss 0.75586873
shape of L is 
torch.Size([])
memory (bytes)
3695562752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
3695562752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  621873800.0
relative error loss 0.6950148
shape of L is 
torch.Size([])
memory (bytes)
3697606656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
3697606656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  691595900.0
relative error loss 0.7729372
shape of L is 
torch.Size([])
memory (bytes)
3699585024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
3699585024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  584021700.0
relative error loss 0.65271074
time to take a step is 245.63426160812378
it  2 : 961811456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
3701940224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
3701985280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  584021700.0
relative error loss 0.65271074
shape of L is 
torch.Size([])
memory (bytes)
3704020992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
3704020992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  551451000.0
relative error loss 0.6163093
shape of L is 
torch.Size([])
memory (bytes)
3706109952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
3706109952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  523999000.0
relative error loss 0.58562857
shape of L is 
torch.Size([])
memory (bytes)
3708092416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% |  7% |
memory (bytes)
3708092416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  481426100.0
relative error loss 0.5380485
shape of L is 
torch.Size([])
memory (bytes)
3710418944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
3710418944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  427757660.0
relative error loss 0.4780679
shape of L is 
torch.Size([])
memory (bytes)
3712274432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  4% |  7% |
memory (bytes)
3712540672
| ID | GPU | MEM |
------------------
|  0 |  1% |  0% |
|  1 | 99% |  7% |
error is  390898020.0
relative error loss 0.43687305
shape of L is 
torch.Size([])
memory (bytes)
3714629632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% |  7% |
memory (bytes)
3714670592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  389885980.0
relative error loss 0.435742
shape of L is 
torch.Size([])
memory (bytes)
3716759552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
3716759552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  357361860.0
relative error loss 0.39939258
shape of L is 
torch.Size([])
memory (bytes)
3718938624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
3718938624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  323567420.0
relative error loss 0.36162344
shape of L is 
torch.Size([])
memory (bytes)
3721064448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
3721064448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  290961660.0
relative error loss 0.3251828
time to take a step is 249.22429180145264
it  3 : 961257472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
3723153408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
3723153408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  290961660.0
relative error loss 0.3251828
shape of L is 
torch.Size([])
memory (bytes)
3725271040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  7% |
memory (bytes)
3725271040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  250200060.0
relative error loss 0.27962705
shape of L is 
torch.Size([])
memory (bytes)
3727384576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
3727384576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  224218560.0
relative error loss 0.2505898
shape of L is 
torch.Size([])
memory (bytes)
3729580032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
3729580032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  205528060.0
relative error loss 0.22970103
shape of L is 
torch.Size([])
memory (bytes)
3731537920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% |  7% |
memory (bytes)
3731537920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  183264900.0
relative error loss 0.2048194
shape of L is 
torch.Size([])
memory (bytes)
3733766144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
3733766144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  166531330.0
relative error loss 0.18611772
shape of L is 
torch.Size([])
memory (bytes)
3736023040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  5% |  7% |
memory (bytes)
3736023040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  151271870.0
relative error loss 0.16906355
shape of L is 
torch.Size([])
memory (bytes)
3738075136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% |  7% |
memory (bytes)
3738210304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  136154300.0
relative error loss 0.15216795
shape of L is 
torch.Size([])
memory (bytes)
3740278784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
3740278784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  125272770.0
relative error loss 0.14000659
shape of L is 
torch.Size([])
memory (bytes)
3742461952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  7% |
memory (bytes)
3742507008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  114888700.0
relative error loss 0.1284012
time to take a step is 257.54765701293945
c= tensor(190.1308, device='cuda:0')
c= tensor(15633.1621, device='cuda:0')
c= tensor(38432.6562, device='cuda:0')
c= tensor(44321.8008, device='cuda:0')
c= tensor(49400.7461, device='cuda:0')
c= tensor(80713.9453, device='cuda:0')
c= tensor(224191.5000, device='cuda:0')
c= tensor(288510.7500, device='cuda:0')
c= tensor(398134.7500, device='cuda:0')
c= tensor(968765.0625, device='cuda:0')
c= tensor(985791.9375, device='cuda:0')
c= tensor(1154587., device='cuda:0')
c= tensor(1156698.5000, device='cuda:0')
c= tensor(3988494., device='cuda:0')
c= tensor(4024894., device='cuda:0')
c= tensor(4045535.7500, device='cuda:0')
c= tensor(4105888., device='cuda:0')
c= tensor(4326323.5000, device='cuda:0')
c= tensor(5007712.5000, device='cuda:0')
c= tensor(5538882., device='cuda:0')
c= tensor(5545727.5000, device='cuda:0')
c= tensor(6708141., device='cuda:0')
c= tensor(6715035.5000, device='cuda:0')
c= tensor(6799173.5000, device='cuda:0')
c= tensor(6800368.5000, device='cuda:0')
c= tensor(6886912.5000, device='cuda:0')
c= tensor(7022523.5000, device='cuda:0')
c= tensor(7025429., device='cuda:0')
c= tensor(10056090., device='cuda:0')
c= tensor(26079410., device='cuda:0')
c= tensor(26085374., device='cuda:0')
c= tensor(97984928., device='cuda:0')
c= tensor(97990952., device='cuda:0')
c= tensor(97997528., device='cuda:0')
c= tensor(98001160., device='cuda:0')
c= tensor(1.0243e+08, device='cuda:0')
c= tensor(1.0254e+08, device='cuda:0')
c= tensor(1.0254e+08, device='cuda:0')
c= tensor(1.0254e+08, device='cuda:0')
c= tensor(1.0255e+08, device='cuda:0')
c= tensor(1.0255e+08, device='cuda:0')
c= tensor(1.0255e+08, device='cuda:0')
c= tensor(1.0255e+08, device='cuda:0')
c= tensor(1.0255e+08, device='cuda:0')
c= tensor(1.0255e+08, device='cuda:0')
c= tensor(1.0255e+08, device='cuda:0')
c= tensor(1.0255e+08, device='cuda:0')
c= tensor(1.0256e+08, device='cuda:0')
c= tensor(1.0256e+08, device='cuda:0')
c= tensor(1.0256e+08, device='cuda:0')
c= tensor(1.0257e+08, device='cuda:0')
c= tensor(1.0258e+08, device='cuda:0')
c= tensor(1.0258e+08, device='cuda:0')
c= tensor(1.0258e+08, device='cuda:0')
c= tensor(1.0258e+08, device='cuda:0')
c= tensor(1.0258e+08, device='cuda:0')
c= tensor(1.0263e+08, device='cuda:0')
c= tensor(1.0263e+08, device='cuda:0')
c= tensor(1.0263e+08, device='cuda:0')
c= tensor(1.0263e+08, device='cuda:0')
c= tensor(1.0263e+08, device='cuda:0')
c= tensor(1.0263e+08, device='cuda:0')
c= tensor(1.0263e+08, device='cuda:0')
c= tensor(1.0263e+08, device='cuda:0')
c= tensor(1.0263e+08, device='cuda:0')
c= tensor(1.0264e+08, device='cuda:0')
c= tensor(1.0264e+08, device='cuda:0')
c= tensor(1.0264e+08, device='cuda:0')
c= tensor(1.0264e+08, device='cuda:0')
c= tensor(1.0264e+08, device='cuda:0')
c= tensor(1.0264e+08, device='cuda:0')
c= tensor(1.0264e+08, device='cuda:0')
c= tensor(1.0264e+08, device='cuda:0')
c= tensor(1.0264e+08, device='cuda:0')
c= tensor(1.0264e+08, device='cuda:0')
c= tensor(1.0265e+08, device='cuda:0')
c= tensor(1.0265e+08, device='cuda:0')
c= tensor(1.0266e+08, device='cuda:0')
c= tensor(1.0266e+08, device='cuda:0')
c= tensor(1.0266e+08, device='cuda:0')
c= tensor(1.0267e+08, device='cuda:0')
c= tensor(1.0267e+08, device='cuda:0')
c= tensor(1.0267e+08, device='cuda:0')
c= tensor(1.0267e+08, device='cuda:0')
c= tensor(1.0267e+08, device='cuda:0')
c= tensor(1.0269e+08, device='cuda:0')
c= tensor(1.0269e+08, device='cuda:0')
c= tensor(1.0269e+08, device='cuda:0')
c= tensor(1.0269e+08, device='cuda:0')
c= tensor(1.0273e+08, device='cuda:0')
c= tensor(1.0273e+08, device='cuda:0')
c= tensor(1.0273e+08, device='cuda:0')
c= tensor(1.0273e+08, device='cuda:0')
c= tensor(1.0275e+08, device='cuda:0')
c= tensor(1.0275e+08, device='cuda:0')
c= tensor(1.0275e+08, device='cuda:0')
c= tensor(1.0276e+08, device='cuda:0')
c= tensor(1.0276e+08, device='cuda:0')
c= tensor(1.0276e+08, device='cuda:0')
c= tensor(1.0276e+08, device='cuda:0')
c= tensor(1.0277e+08, device='cuda:0')
c= tensor(1.0277e+08, device='cuda:0')
c= tensor(1.0277e+08, device='cuda:0')
c= tensor(1.0277e+08, device='cuda:0')
c= tensor(1.0278e+08, device='cuda:0')
c= tensor(1.0278e+08, device='cuda:0')
c= tensor(1.0278e+08, device='cuda:0')
c= tensor(1.0278e+08, device='cuda:0')
c= tensor(1.0278e+08, device='cuda:0')
c= tensor(1.0278e+08, device='cuda:0')
c= tensor(1.0280e+08, device='cuda:0')
c= tensor(1.0280e+08, device='cuda:0')
c= tensor(1.0280e+08, device='cuda:0')
c= tensor(1.0280e+08, device='cuda:0')
c= tensor(1.0281e+08, device='cuda:0')
c= tensor(1.0281e+08, device='cuda:0')
c= tensor(1.0282e+08, device='cuda:0')
c= tensor(1.0282e+08, device='cuda:0')
c= tensor(1.0283e+08, device='cuda:0')
c= tensor(1.0283e+08, device='cuda:0')
c= tensor(1.0283e+08, device='cuda:0')
c= tensor(1.0283e+08, device='cuda:0')
c= tensor(1.0283e+08, device='cuda:0')
c= tensor(1.0287e+08, device='cuda:0')
c= tensor(1.0287e+08, device='cuda:0')
c= tensor(1.0287e+08, device='cuda:0')
c= tensor(1.0288e+08, device='cuda:0')
c= tensor(1.0288e+08, device='cuda:0')
c= tensor(1.0288e+08, device='cuda:0')
c= tensor(1.0288e+08, device='cuda:0')
c= tensor(1.0288e+08, device='cuda:0')
c= tensor(1.0288e+08, device='cuda:0')
c= tensor(1.0288e+08, device='cuda:0')
c= tensor(1.0288e+08, device='cuda:0')
c= tensor(1.0288e+08, device='cuda:0')
c= tensor(1.0298e+08, device='cuda:0')
c= tensor(1.0298e+08, device='cuda:0')
c= tensor(1.0298e+08, device='cuda:0')
c= tensor(1.0298e+08, device='cuda:0')
c= tensor(1.0298e+08, device='cuda:0')
c= tensor(1.0298e+08, device='cuda:0')
c= tensor(1.0298e+08, device='cuda:0')
c= tensor(1.0299e+08, device='cuda:0')
c= tensor(1.0299e+08, device='cuda:0')
c= tensor(1.0299e+08, device='cuda:0')
c= tensor(1.0299e+08, device='cuda:0')
c= tensor(1.0299e+08, device='cuda:0')
c= tensor(1.0299e+08, device='cuda:0')
c= tensor(1.0299e+08, device='cuda:0')
c= tensor(1.0299e+08, device='cuda:0')
c= tensor(1.0300e+08, device='cuda:0')
c= tensor(1.0300e+08, device='cuda:0')
c= tensor(1.0300e+08, device='cuda:0')
c= tensor(1.0300e+08, device='cuda:0')
c= tensor(1.0300e+08, device='cuda:0')
c= tensor(1.0303e+08, device='cuda:0')
c= tensor(1.0303e+08, device='cuda:0')
c= tensor(1.0303e+08, device='cuda:0')
c= tensor(1.0303e+08, device='cuda:0')
c= tensor(1.0303e+08, device='cuda:0')
c= tensor(1.0303e+08, device='cuda:0')
c= tensor(1.0303e+08, device='cuda:0')
c= tensor(1.0304e+08, device='cuda:0')
c= tensor(1.0304e+08, device='cuda:0')
c= tensor(1.0304e+08, device='cuda:0')
c= tensor(1.0304e+08, device='cuda:0')
c= tensor(1.0304e+08, device='cuda:0')
c= tensor(1.0304e+08, device='cuda:0')
c= tensor(1.0305e+08, device='cuda:0')
c= tensor(1.0305e+08, device='cuda:0')
c= tensor(1.0305e+08, device='cuda:0')
c= tensor(1.0305e+08, device='cuda:0')
c= tensor(1.0305e+08, device='cuda:0')
c= tensor(1.0305e+08, device='cuda:0')
c= tensor(1.0305e+08, device='cuda:0')
c= tensor(1.0305e+08, device='cuda:0')
c= tensor(1.0305e+08, device='cuda:0')
c= tensor(1.0306e+08, device='cuda:0')
c= tensor(1.0306e+08, device='cuda:0')
c= tensor(1.0306e+08, device='cuda:0')
c= tensor(1.0309e+08, device='cuda:0')
c= tensor(1.0309e+08, device='cuda:0')
c= tensor(1.0309e+08, device='cuda:0')
c= tensor(1.0309e+08, device='cuda:0')
c= tensor(1.0309e+08, device='cuda:0')
c= tensor(1.0309e+08, device='cuda:0')
c= tensor(1.0310e+08, device='cuda:0')
c= tensor(1.0310e+08, device='cuda:0')
c= tensor(1.0318e+08, device='cuda:0')
c= tensor(1.0319e+08, device='cuda:0')
c= tensor(1.0319e+08, device='cuda:0')
c= tensor(1.0319e+08, device='cuda:0')
c= tensor(1.0319e+08, device='cuda:0')
c= tensor(1.0319e+08, device='cuda:0')
c= tensor(1.0319e+08, device='cuda:0')
c= tensor(1.0319e+08, device='cuda:0')
c= tensor(1.0319e+08, device='cuda:0')
c= tensor(1.0319e+08, device='cuda:0')
c= tensor(1.0319e+08, device='cuda:0')
c= tensor(1.0319e+08, device='cuda:0')
c= tensor(1.0322e+08, device='cuda:0')
c= tensor(1.0322e+08, device='cuda:0')
c= tensor(1.0322e+08, device='cuda:0')
c= tensor(1.0322e+08, device='cuda:0')
c= tensor(1.0323e+08, device='cuda:0')
c= tensor(1.0323e+08, device='cuda:0')
c= tensor(1.0323e+08, device='cuda:0')
c= tensor(1.0324e+08, device='cuda:0')
c= tensor(1.0324e+08, device='cuda:0')
c= tensor(1.0324e+08, device='cuda:0')
c= tensor(1.0324e+08, device='cuda:0')
c= tensor(1.0324e+08, device='cuda:0')
c= tensor(1.0324e+08, device='cuda:0')
c= tensor(1.0324e+08, device='cuda:0')
c= tensor(1.0324e+08, device='cuda:0')
c= tensor(1.0324e+08, device='cuda:0')
c= tensor(1.0324e+08, device='cuda:0')
c= tensor(1.0324e+08, device='cuda:0')
c= tensor(1.0325e+08, device='cuda:0')
c= tensor(1.0325e+08, device='cuda:0')
c= tensor(1.0325e+08, device='cuda:0')
c= tensor(1.0325e+08, device='cuda:0')
c= tensor(1.0325e+08, device='cuda:0')
c= tensor(1.0325e+08, device='cuda:0')
c= tensor(1.0326e+08, device='cuda:0')
c= tensor(1.0326e+08, device='cuda:0')
c= tensor(1.0326e+08, device='cuda:0')
c= tensor(1.0326e+08, device='cuda:0')
c= tensor(1.0326e+08, device='cuda:0')
c= tensor(1.0326e+08, device='cuda:0')
c= tensor(1.0326e+08, device='cuda:0')
c= tensor(1.0326e+08, device='cuda:0')
c= tensor(1.0326e+08, device='cuda:0')
c= tensor(1.0328e+08, device='cuda:0')
c= tensor(1.0328e+08, device='cuda:0')
c= tensor(1.0328e+08, device='cuda:0')
c= tensor(1.0328e+08, device='cuda:0')
c= tensor(1.0328e+08, device='cuda:0')
c= tensor(1.0328e+08, device='cuda:0')
c= tensor(1.0330e+08, device='cuda:0')
c= tensor(1.0343e+08, device='cuda:0')
c= tensor(1.0343e+08, device='cuda:0')
c= tensor(1.0343e+08, device='cuda:0')
c= tensor(1.0343e+08, device='cuda:0')
c= tensor(1.0343e+08, device='cuda:0')
c= tensor(1.0344e+08, device='cuda:0')
c= tensor(1.0552e+08, device='cuda:0')
c= tensor(1.0552e+08, device='cuda:0')
c= tensor(1.0570e+08, device='cuda:0')
c= tensor(1.0581e+08, device='cuda:0')
c= tensor(1.0582e+08, device='cuda:0')
c= tensor(1.0585e+08, device='cuda:0')
c= tensor(1.0585e+08, device='cuda:0')
c= tensor(1.0585e+08, device='cuda:0')
c= tensor(1.1058e+08, device='cuda:0')
c= tensor(1.1216e+08, device='cuda:0')
c= tensor(1.1216e+08, device='cuda:0')
c= tensor(1.1219e+08, device='cuda:0')
c= tensor(1.1292e+08, device='cuda:0')
c= tensor(1.1296e+08, device='cuda:0')
c= tensor(1.1308e+08, device='cuda:0')
c= tensor(1.1310e+08, device='cuda:0')
c= tensor(1.1315e+08, device='cuda:0')
c= tensor(1.1318e+08, device='cuda:0')
c= tensor(1.1319e+08, device='cuda:0')
c= tensor(1.1570e+08, device='cuda:0')
c= tensor(1.1570e+08, device='cuda:0')
c= tensor(1.1570e+08, device='cuda:0')
c= tensor(1.1571e+08, device='cuda:0')
c= tensor(1.1577e+08, device='cuda:0')
c= tensor(1.1754e+08, device='cuda:0')
c= tensor(1.1764e+08, device='cuda:0')
c= tensor(1.1764e+08, device='cuda:0')
c= tensor(1.1766e+08, device='cuda:0')
c= tensor(1.1766e+08, device='cuda:0')
c= tensor(1.1773e+08, device='cuda:0')
c= tensor(1.1793e+08, device='cuda:0')
c= tensor(1.1826e+08, device='cuda:0')
c= tensor(1.1833e+08, device='cuda:0')
c= tensor(1.1833e+08, device='cuda:0')
c= tensor(1.1833e+08, device='cuda:0')
c= tensor(1.1840e+08, device='cuda:0')
c= tensor(1.1847e+08, device='cuda:0')
c= tensor(1.1856e+08, device='cuda:0')
c= tensor(1.1856e+08, device='cuda:0')
c= tensor(1.2167e+08, device='cuda:0')
c= tensor(1.2170e+08, device='cuda:0')
c= tensor(1.2172e+08, device='cuda:0')
c= tensor(1.2243e+08, device='cuda:0')
c= tensor(1.2243e+08, device='cuda:0')
c= tensor(1.2248e+08, device='cuda:0')
c= tensor(1.2353e+08, device='cuda:0')
c= tensor(1.3272e+08, device='cuda:0')
c= tensor(1.3274e+08, device='cuda:0')
c= tensor(1.3276e+08, device='cuda:0')
c= tensor(1.3277e+08, device='cuda:0')
c= tensor(1.3277e+08, device='cuda:0')
c= tensor(1.3325e+08, device='cuda:0')
c= tensor(1.3326e+08, device='cuda:0')
c= tensor(1.3331e+08, device='cuda:0')
c= tensor(1.3411e+08, device='cuda:0')
c= tensor(1.3417e+08, device='cuda:0')
c= tensor(1.3422e+08, device='cuda:0')
c= tensor(1.3424e+08, device='cuda:0')
c= tensor(1.4187e+08, device='cuda:0')
c= tensor(1.4188e+08, device='cuda:0')
c= tensor(1.4190e+08, device='cuda:0')
c= tensor(1.4190e+08, device='cuda:0')
c= tensor(1.4279e+08, device='cuda:0')
c= tensor(1.4280e+08, device='cuda:0')
c= tensor(1.4298e+08, device='cuda:0')
c= tensor(1.4299e+08, device='cuda:0')
c= tensor(1.4331e+08, device='cuda:0')
c= tensor(1.4334e+08, device='cuda:0')
c= tensor(1.4637e+08, device='cuda:0')
c= tensor(1.4658e+08, device='cuda:0')
c= tensor(1.4658e+08, device='cuda:0')
c= tensor(1.4707e+08, device='cuda:0')
c= tensor(1.4733e+08, device='cuda:0')
c= tensor(1.4734e+08, device='cuda:0')
c= tensor(1.4749e+08, device='cuda:0')
c= tensor(1.4841e+08, device='cuda:0')
c= tensor(1.5268e+08, device='cuda:0')
c= tensor(1.5271e+08, device='cuda:0')
c= tensor(1.5271e+08, device='cuda:0')
c= tensor(1.5272e+08, device='cuda:0')
c= tensor(1.5273e+08, device='cuda:0')
c= tensor(1.5274e+08, device='cuda:0')
c= tensor(1.5274e+08, device='cuda:0')
c= tensor(1.5274e+08, device='cuda:0')
c= tensor(1.5292e+08, device='cuda:0')
c= tensor(1.5320e+08, device='cuda:0')
c= tensor(1.5324e+08, device='cuda:0')
c= tensor(1.5325e+08, device='cuda:0')
c= tensor(1.5332e+08, device='cuda:0')
c= tensor(1.5332e+08, device='cuda:0')
c= tensor(1.5333e+08, device='cuda:0')
c= tensor(1.5333e+08, device='cuda:0')
c= tensor(1.5334e+08, device='cuda:0')
c= tensor(1.5583e+08, device='cuda:0')
c= tensor(1.5586e+08, device='cuda:0')
c= tensor(1.5587e+08, device='cuda:0')
c= tensor(1.5618e+08, device='cuda:0')
c= tensor(1.5618e+08, device='cuda:0')
c= tensor(1.5946e+08, device='cuda:0')
c= tensor(1.5946e+08, device='cuda:0')
c= tensor(1.5957e+08, device='cuda:0')
c= tensor(1.5957e+08, device='cuda:0')
c= tensor(1.5957e+08, device='cuda:0')
c= tensor(1.5957e+08, device='cuda:0')
c= tensor(1.5958e+08, device='cuda:0')
c= tensor(1.5958e+08, device='cuda:0')
c= tensor(1.5969e+08, device='cuda:0')
c= tensor(1.5969e+08, device='cuda:0')
c= tensor(1.5969e+08, device='cuda:0')
c= tensor(1.6047e+08, device='cuda:0')
c= tensor(1.6059e+08, device='cuda:0')
c= tensor(1.6065e+08, device='cuda:0')
c= tensor(1.6104e+08, device='cuda:0')
c= tensor(1.6377e+08, device='cuda:0')
c= tensor(1.6378e+08, device='cuda:0')
c= tensor(1.6378e+08, device='cuda:0')
c= tensor(1.6379e+08, device='cuda:0')
c= tensor(1.6380e+08, device='cuda:0')
c= tensor(1.6380e+08, device='cuda:0')
c= tensor(1.6380e+08, device='cuda:0')
c= tensor(1.6381e+08, device='cuda:0')
c= tensor(1.6381e+08, device='cuda:0')
c= tensor(1.6381e+08, device='cuda:0')
c= tensor(1.6381e+08, device='cuda:0')
c= tensor(1.7090e+08, device='cuda:0')
c= tensor(1.7090e+08, device='cuda:0')
c= tensor(1.7164e+08, device='cuda:0')
c= tensor(1.7164e+08, device='cuda:0')
c= tensor(1.7165e+08, device='cuda:0')
c= tensor(1.7167e+08, device='cuda:0')
c= tensor(1.7477e+08, device='cuda:0')
c= tensor(1.7783e+08, device='cuda:0')
c= tensor(1.7784e+08, device='cuda:0')
c= tensor(1.7790e+08, device='cuda:0')
c= tensor(1.7790e+08, device='cuda:0')
c= tensor(1.7791e+08, device='cuda:0')
c= tensor(2.2140e+08, device='cuda:0')
c= tensor(2.2143e+08, device='cuda:0')
c= tensor(2.2143e+08, device='cuda:0')
c= tensor(2.2163e+08, device='cuda:0')
c= tensor(2.2525e+08, device='cuda:0')
c= tensor(2.2528e+08, device='cuda:0')
c= tensor(2.2529e+08, device='cuda:0')
c= tensor(2.2529e+08, device='cuda:0')
c= tensor(2.2530e+08, device='cuda:0')
c= tensor(2.2530e+08, device='cuda:0')
c= tensor(2.2592e+08, device='cuda:0')
c= tensor(2.2593e+08, device='cuda:0')
c= tensor(2.2594e+08, device='cuda:0')
c= tensor(2.2595e+08, device='cuda:0')
c= tensor(2.2596e+08, device='cuda:0')
c= tensor(2.2596e+08, device='cuda:0')
c= tensor(2.2615e+08, device='cuda:0')
c= tensor(2.2631e+08, device='cuda:0')
c= tensor(2.2756e+08, device='cuda:0')
c= tensor(2.2786e+08, device='cuda:0')
c= tensor(2.2894e+08, device='cuda:0')
c= tensor(2.2896e+08, device='cuda:0')
c= tensor(2.2902e+08, device='cuda:0')
c= tensor(2.2913e+08, device='cuda:0')
c= tensor(2.2930e+08, device='cuda:0')
c= tensor(2.2930e+08, device='cuda:0')
c= tensor(2.3090e+08, device='cuda:0')
c= tensor(2.3184e+08, device='cuda:0')
c= tensor(2.3220e+08, device='cuda:0')
c= tensor(2.3236e+08, device='cuda:0')
c= tensor(2.3269e+08, device='cuda:0')
c= tensor(2.3271e+08, device='cuda:0')
c= tensor(2.3271e+08, device='cuda:0')
c= tensor(2.3291e+08, device='cuda:0')
c= tensor(2.3343e+08, device='cuda:0')
c= tensor(2.3380e+08, device='cuda:0')
c= tensor(2.3678e+08, device='cuda:0')
c= tensor(2.3798e+08, device='cuda:0')
c= tensor(2.3815e+08, device='cuda:0')
c= tensor(2.3818e+08, device='cuda:0')
c= tensor(2.4038e+08, device='cuda:0')
c= tensor(2.4039e+08, device='cuda:0')
c= tensor(2.4040e+08, device='cuda:0')
c= tensor(2.4197e+08, device='cuda:0')
c= tensor(2.4198e+08, device='cuda:0')
c= tensor(2.4198e+08, device='cuda:0')
c= tensor(2.4201e+08, device='cuda:0')
c= tensor(2.4452e+08, device='cuda:0')
c= tensor(2.4454e+08, device='cuda:0')
c= tensor(2.4468e+08, device='cuda:0')
c= tensor(2.4468e+08, device='cuda:0')
c= tensor(2.4468e+08, device='cuda:0')
c= tensor(2.4468e+08, device='cuda:0')
c= tensor(2.4472e+08, device='cuda:0')
c= tensor(2.4474e+08, device='cuda:0')
c= tensor(2.4488e+08, device='cuda:0')
c= tensor(2.4488e+08, device='cuda:0')
c= tensor(2.4513e+08, device='cuda:0')
c= tensor(2.4513e+08, device='cuda:0')
c= tensor(2.4523e+08, device='cuda:0')
c= tensor(2.4524e+08, device='cuda:0')
c= tensor(2.4535e+08, device='cuda:0')
c= tensor(2.4536e+08, device='cuda:0')
c= tensor(2.4537e+08, device='cuda:0')
c= tensor(2.4538e+08, device='cuda:0')
c= tensor(2.4541e+08, device='cuda:0')
c= tensor(2.4554e+08, device='cuda:0')
c= tensor(2.4643e+08, device='cuda:0')
c= tensor(2.4643e+08, device='cuda:0')
c= tensor(2.4644e+08, device='cuda:0')
c= tensor(2.4726e+08, device='cuda:0')
c= tensor(2.4727e+08, device='cuda:0')
c= tensor(2.5553e+08, device='cuda:0')
c= tensor(2.5553e+08, device='cuda:0')
c= tensor(2.5575e+08, device='cuda:0')
c= tensor(2.5662e+08, device='cuda:0')
c= tensor(2.5662e+08, device='cuda:0')
c= tensor(2.5709e+08, device='cuda:0')
c= tensor(2.5717e+08, device='cuda:0')
c= tensor(2.6078e+08, device='cuda:0')
c= tensor(2.6078e+08, device='cuda:0')
c= tensor(2.6079e+08, device='cuda:0')
c= tensor(2.6079e+08, device='cuda:0')
c= tensor(2.6079e+08, device='cuda:0')
c= tensor(2.6082e+08, device='cuda:0')
c= tensor(2.6085e+08, device='cuda:0')
c= tensor(2.6089e+08, device='cuda:0')
c= tensor(2.6130e+08, device='cuda:0')
c= tensor(2.6132e+08, device='cuda:0')
c= tensor(2.6132e+08, device='cuda:0')
c= tensor(2.6133e+08, device='cuda:0')
c= tensor(2.6236e+08, device='cuda:0')
c= tensor(2.6248e+08, device='cuda:0')
c= tensor(2.6347e+08, device='cuda:0')
c= tensor(2.6348e+08, device='cuda:0')
c= tensor(2.6348e+08, device='cuda:0')
c= tensor(2.6348e+08, device='cuda:0')
c= tensor(2.6349e+08, device='cuda:0')
c= tensor(2.6461e+08, device='cuda:0')
c= tensor(2.6462e+08, device='cuda:0')
c= tensor(2.6464e+08, device='cuda:0')
c= tensor(2.6495e+08, device='cuda:0')
c= tensor(2.6497e+08, device='cuda:0')
c= tensor(2.6497e+08, device='cuda:0')
c= tensor(2.6498e+08, device='cuda:0')
c= tensor(2.6618e+08, device='cuda:0')
c= tensor(2.6619e+08, device='cuda:0')
c= tensor(2.6620e+08, device='cuda:0')
c= tensor(2.6620e+08, device='cuda:0')
c= tensor(2.7348e+08, device='cuda:0')
c= tensor(2.7385e+08, device='cuda:0')
c= tensor(2.7544e+08, device='cuda:0')
c= tensor(2.7564e+08, device='cuda:0')
c= tensor(2.7564e+08, device='cuda:0')
c= tensor(2.7574e+08, device='cuda:0')
c= tensor(2.7575e+08, device='cuda:0')
c= tensor(2.7577e+08, device='cuda:0')
c= tensor(2.7578e+08, device='cuda:0')
c= tensor(2.7578e+08, device='cuda:0')
c= tensor(2.7582e+08, device='cuda:0')
c= tensor(2.7584e+08, device='cuda:0')
c= tensor(2.7584e+08, device='cuda:0')
c= tensor(2.7585e+08, device='cuda:0')
c= tensor(2.7586e+08, device='cuda:0')
c= tensor(2.7752e+08, device='cuda:0')
c= tensor(2.7752e+08, device='cuda:0')
c= tensor(2.7753e+08, device='cuda:0')
c= tensor(2.7753e+08, device='cuda:0')
c= tensor(2.7755e+08, device='cuda:0')
c= tensor(2.7755e+08, device='cuda:0')
c= tensor(2.7755e+08, device='cuda:0')
c= tensor(2.7757e+08, device='cuda:0')
c= tensor(2.7772e+08, device='cuda:0')
c= tensor(2.7772e+08, device='cuda:0')
c= tensor(2.7772e+08, device='cuda:0')
c= tensor(2.7772e+08, device='cuda:0')
c= tensor(2.7876e+08, device='cuda:0')
c= tensor(3.1634e+08, device='cuda:0')
c= tensor(3.1636e+08, device='cuda:0')
c= tensor(3.1637e+08, device='cuda:0')
c= tensor(3.1654e+08, device='cuda:0')
c= tensor(3.1667e+08, device='cuda:0')
c= tensor(3.1667e+08, device='cuda:0')
c= tensor(3.1672e+08, device='cuda:0')
c= tensor(3.1674e+08, device='cuda:0')
c= tensor(3.2581e+08, device='cuda:0')
c= tensor(3.2973e+08, device='cuda:0')
c= tensor(3.3140e+08, device='cuda:0')
c= tensor(3.3140e+08, device='cuda:0')
c= tensor(3.3141e+08, device='cuda:0')
c= tensor(3.3141e+08, device='cuda:0')
c= tensor(3.3148e+08, device='cuda:0')
c= tensor(3.3149e+08, device='cuda:0')
c= tensor(3.3151e+08, device='cuda:0')
c= tensor(3.3227e+08, device='cuda:0')
c= tensor(3.3596e+08, device='cuda:0')
c= tensor(3.3596e+08, device='cuda:0')
c= tensor(3.3596e+08, device='cuda:0')
c= tensor(3.3598e+08, device='cuda:0')
c= tensor(3.3606e+08, device='cuda:0')
c= tensor(3.3610e+08, device='cuda:0')
c= tensor(3.3610e+08, device='cuda:0')
c= tensor(3.3610e+08, device='cuda:0')
c= tensor(3.3611e+08, device='cuda:0')
c= tensor(3.3611e+08, device='cuda:0')
c= tensor(3.3650e+08, device='cuda:0')
c= tensor(3.3650e+08, device='cuda:0')
c= tensor(3.3651e+08, device='cuda:0')
c= tensor(3.3651e+08, device='cuda:0')
c= tensor(3.3651e+08, device='cuda:0')
c= tensor(3.3652e+08, device='cuda:0')
c= tensor(3.3746e+08, device='cuda:0')
c= tensor(3.4013e+08, device='cuda:0')
c= tensor(3.4058e+08, device='cuda:0')
c= tensor(3.4060e+08, device='cuda:0')
c= tensor(3.4062e+08, device='cuda:0')
c= tensor(3.4062e+08, device='cuda:0')
c= tensor(3.4063e+08, device='cuda:0')
c= tensor(3.4077e+08, device='cuda:0')
c= tensor(3.4079e+08, device='cuda:0')
c= tensor(3.4081e+08, device='cuda:0')
c= tensor(3.4084e+08, device='cuda:0')
c= tensor(3.5948e+08, device='cuda:0')
c= tensor(3.5949e+08, device='cuda:0')
c= tensor(3.5958e+08, device='cuda:0')
c= tensor(3.6423e+08, device='cuda:0')
c= tensor(3.6430e+08, device='cuda:0')
c= tensor(3.6432e+08, device='cuda:0')
c= tensor(3.6587e+08, device='cuda:0')
c= tensor(3.6612e+08, device='cuda:0')
c= tensor(3.6614e+08, device='cuda:0')
c= tensor(3.6614e+08, device='cuda:0')
c= tensor(3.6617e+08, device='cuda:0')
c= tensor(3.6618e+08, device='cuda:0')
c= tensor(3.6629e+08, device='cuda:0')
c= tensor(3.7540e+08, device='cuda:0')
c= tensor(3.7711e+08, device='cuda:0')
c= tensor(3.7750e+08, device='cuda:0')
c= tensor(3.7751e+08, device='cuda:0')
c= tensor(3.7753e+08, device='cuda:0')
c= tensor(3.7757e+08, device='cuda:0')
c= tensor(3.7757e+08, device='cuda:0')
c= tensor(3.8807e+08, device='cuda:0')
c= tensor(3.8898e+08, device='cuda:0')
c= tensor(3.8901e+08, device='cuda:0')
c= tensor(4.6976e+08, device='cuda:0')
c= tensor(4.7676e+08, device='cuda:0')
c= tensor(4.7679e+08, device='cuda:0')
c= tensor(4.7680e+08, device='cuda:0')
c= tensor(4.7682e+08, device='cuda:0')
c= tensor(4.7693e+08, device='cuda:0')
c= tensor(4.7694e+08, device='cuda:0')
c= tensor(4.8624e+08, device='cuda:0')
c= tensor(4.8630e+08, device='cuda:0')
c= tensor(4.8633e+08, device='cuda:0')
c= tensor(4.8634e+08, device='cuda:0')
c= tensor(4.8634e+08, device='cuda:0')
c= tensor(4.8634e+08, device='cuda:0')
c= tensor(4.8635e+08, device='cuda:0')
c= tensor(4.8637e+08, device='cuda:0')
c= tensor(4.8638e+08, device='cuda:0')
c= tensor(7.8381e+08, device='cuda:0')
c= tensor(7.8386e+08, device='cuda:0')
c= tensor(7.8396e+08, device='cuda:0')
c= tensor(7.8397e+08, device='cuda:0')
c= tensor(7.8398e+08, device='cuda:0')
c= tensor(7.8398e+08, device='cuda:0')
c= tensor(7.8481e+08, device='cuda:0')
c= tensor(7.8516e+08, device='cuda:0')
c= tensor(7.9084e+08, device='cuda:0')
c= tensor(7.9086e+08, device='cuda:0')
c= tensor(7.9196e+08, device='cuda:0')
c= tensor(7.9200e+08, device='cuda:0')
c= tensor(7.9232e+08, device='cuda:0')
c= tensor(7.9404e+08, device='cuda:0')
c= tensor(7.9405e+08, device='cuda:0')
c= tensor(7.9405e+08, device='cuda:0')
c= tensor(7.9420e+08, device='cuda:0')
c= tensor(7.9420e+08, device='cuda:0')
c= tensor(7.9424e+08, device='cuda:0')
c= tensor(7.9537e+08, device='cuda:0')
c= tensor(7.9538e+08, device='cuda:0')
c= tensor(7.9561e+08, device='cuda:0')
c= tensor(7.9562e+08, device='cuda:0')
c= tensor(7.9591e+08, device='cuda:0')
c= tensor(7.9704e+08, device='cuda:0')
c= tensor(7.9705e+08, device='cuda:0')
c= tensor(7.9705e+08, device='cuda:0')
c= tensor(8.0267e+08, device='cuda:0')
c= tensor(8.0283e+08, device='cuda:0')
c= tensor(8.1018e+08, device='cuda:0')
c= tensor(8.1026e+08, device='cuda:0')
c= tensor(8.1043e+08, device='cuda:0')
c= tensor(8.1054e+08, device='cuda:0')
c= tensor(8.1083e+08, device='cuda:0')
c= tensor(8.1271e+08, device='cuda:0')
c= tensor(8.1274e+08, device='cuda:0')
c= tensor(8.1274e+08, device='cuda:0')
c= tensor(8.1275e+08, device='cuda:0')
c= tensor(8.1279e+08, device='cuda:0')
c= tensor(8.1316e+08, device='cuda:0')
c= tensor(8.1321e+08, device='cuda:0')
c= tensor(8.1322e+08, device='cuda:0')
c= tensor(8.1326e+08, device='cuda:0')
c= tensor(8.1332e+08, device='cuda:0')
c= tensor(8.7307e+08, device='cuda:0')
c= tensor(8.7316e+08, device='cuda:0')
c= tensor(8.7340e+08, device='cuda:0')
c= tensor(8.7352e+08, device='cuda:0')
c= tensor(8.7354e+08, device='cuda:0')
c= tensor(8.7354e+08, device='cuda:0')
c= tensor(8.7355e+08, device='cuda:0')
c= tensor(8.7393e+08, device='cuda:0')
c= tensor(8.7393e+08, device='cuda:0')
c= tensor(8.7400e+08, device='cuda:0')
c= tensor(8.7400e+08, device='cuda:0')
c= tensor(8.7400e+08, device='cuda:0')
c= tensor(8.7405e+08, device='cuda:0')
c= tensor(8.7415e+08, device='cuda:0')
c= tensor(8.7416e+08, device='cuda:0')
c= tensor(8.7417e+08, device='cuda:0')
c= tensor(8.7417e+08, device='cuda:0')
c= tensor(8.7419e+08, device='cuda:0')
c= tensor(8.7422e+08, device='cuda:0')
c= tensor(8.7464e+08, device='cuda:0')
c= tensor(8.7464e+08, device='cuda:0')
c= tensor(8.7468e+08, device='cuda:0')
c= tensor(8.7469e+08, device='cuda:0')
c= tensor(8.7470e+08, device='cuda:0')
c= tensor(8.7546e+08, device='cuda:0')
c= tensor(8.7547e+08, device='cuda:0')
c= tensor(8.7591e+08, device='cuda:0')
c= tensor(8.7883e+08, device='cuda:0')
c= tensor(8.7886e+08, device='cuda:0')
c= tensor(8.7921e+08, device='cuda:0')
c= tensor(8.7934e+08, device='cuda:0')
c= tensor(8.7934e+08, device='cuda:0')
c= tensor(8.7937e+08, device='cuda:0')
c= tensor(8.7937e+08, device='cuda:0')
c= tensor(8.8044e+08, device='cuda:0')
c= tensor(8.8083e+08, device='cuda:0')
c= tensor(8.8084e+08, device='cuda:0')
c= tensor(8.8121e+08, device='cuda:0')
c= tensor(8.8122e+08, device='cuda:0')
c= tensor(8.8129e+08, device='cuda:0')
c= tensor(8.8130e+08, device='cuda:0')
c= tensor(8.8135e+08, device='cuda:0')
c= tensor(8.8191e+08, device='cuda:0')
c= tensor(8.9378e+08, device='cuda:0')
c= tensor(8.9388e+08, device='cuda:0')
c= tensor(8.9389e+08, device='cuda:0')
c= tensor(8.9393e+08, device='cuda:0')
c= tensor(8.9428e+08, device='cuda:0')
c= tensor(8.9428e+08, device='cuda:0')
c= tensor(8.9429e+08, device='cuda:0')
c= tensor(8.9435e+08, device='cuda:0')
c= tensor(8.9471e+08, device='cuda:0')
c= tensor(8.9473e+08, device='cuda:0')
c= tensor(8.9476e+08, device='cuda:0')
c= tensor(8.9476e+08, device='cuda:0')
time to make c is 15.426233530044556
time for making loss is 15.4262535572052
p0 True
it  0 : 626384384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
3744407552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
3744751616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  114888700.0
relative error loss 0.1284012
shape of L is 
torch.Size([])
memory (bytes)
3771879424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
3771899904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  112351620.0
relative error loss 0.12556572
shape of L is 
torch.Size([])
memory (bytes)
3775508480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
3775508480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  103631230.0
relative error loss 0.11581971
shape of L is 
torch.Size([])
memory (bytes)
3778678784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
3778760704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  100963580.0
relative error loss 0.112838306
shape of L is 
torch.Size([])
memory (bytes)
3781902336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
3781902336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  98964030.0
relative error loss 0.11060358
shape of L is 
torch.Size([])
memory (bytes)
3785093120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
3785093120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  97410370.0
relative error loss 0.10886718
shape of L is 
torch.Size([])
memory (bytes)
3788423168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
3788423168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  96265220.0
relative error loss 0.107587345
shape of L is 
torch.Size([])
memory (bytes)
3791429632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
3791429632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  95460740.0
relative error loss 0.106688246
shape of L is 
torch.Size([])
memory (bytes)
3794857984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% |  7% |
memory (bytes)
3794857984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  94745600.0
relative error loss 0.105889
shape of L is 
torch.Size([])
memory (bytes)
3797925888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
3798077440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  94266620.0
relative error loss 0.10535369
time to take a step is 318.3258125782013
it  1 : 962573824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
3801145344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
3801145344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  94266620.0
relative error loss 0.10535369
shape of L is 
torch.Size([])
memory (bytes)
3804516352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
3804516352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  93834050.0
relative error loss 0.10487024
shape of L is 
torch.Size([])
memory (bytes)
3807707136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
3807707136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  93419460.0
relative error loss 0.104406886
shape of L is 
torch.Size([])
memory (bytes)
3810938880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
3810988032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% |  7% |
error is  93092220.0
relative error loss 0.10404117
shape of L is 
torch.Size([])
memory (bytes)
3814125568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% |  7% |
memory (bytes)
3814219776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  92796030.0
relative error loss 0.10371014
shape of L is 
torch.Size([])
memory (bytes)
3817312256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
3817312256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  92549760.0
relative error loss 0.1034349
shape of L is 
torch.Size([])
memory (bytes)
3820584960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
3820679168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  92354300.0
relative error loss 0.103216454
shape of L is 
torch.Size([])
memory (bytes)
3823742976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
3823902720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  92148160.0
relative error loss 0.10298607
shape of L is 
torch.Size([])
memory (bytes)
3827126272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
3827130368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  92049920.0
relative error loss 0.10287627
shape of L is 
torch.Size([])
memory (bytes)
3830337536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
3830337536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  91883390.0
relative error loss 0.10269015
time to take a step is 319.5360891819
it  2 : 962573824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
3833581568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
3833581568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  91883390.0
relative error loss 0.10269015
shape of L is 
torch.Size([])
memory (bytes)
3836719104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
3836813312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  91782210.0
relative error loss 0.10257707
shape of L is 
torch.Size([])
memory (bytes)
3839885312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
3840045056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  91675390.0
relative error loss 0.102457695
shape of L is 
torch.Size([])
memory (bytes)
3843264512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
3843264512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  91597950.0
relative error loss 0.10237114
shape of L is 
torch.Size([])
memory (bytes)
3846492160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
3846492160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  91380420.0
relative error loss 0.10212802
shape of L is 
torch.Size([])
memory (bytes)
3849621504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
3849719808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  91286780.0
relative error loss 0.10202338
shape of L is 
torch.Size([])
memory (bytes)
3852931072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
3852931072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  91181760.0
relative error loss 0.101906
shape of L is 
torch.Size([])
memory (bytes)
3855994880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
3856154624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  91027580.0
relative error loss 0.10173369
shape of L is 
torch.Size([])
memory (bytes)
3859382272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
3859382272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  90973310.0
relative error loss 0.10167304
shape of L is 
torch.Size([])
memory (bytes)
3862503424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
3862503424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  90853310.0
relative error loss 0.10153893
time to take a step is 377.82931208610535
it  3 : 962573824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
3865743360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
3865833472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  90853310.0
relative error loss 0.10153893
shape of L is 
torch.Size([])
memory (bytes)
3869044736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
3869044736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  90811070.0
relative error loss 0.10149172
shape of L is 
torch.Size([])
memory (bytes)
3872288768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
3872292864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  90707710.0
relative error loss 0.1013762
shape of L is 
torch.Size([])
memory (bytes)
3875426304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
3875426304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  90700740.0
relative error loss 0.101368405
shape of L is 
torch.Size([])
memory (bytes)
3878739968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  7% |
memory (bytes)
3878744064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  90658750.0
relative error loss 0.10132148
shape of L is 
torch.Size([])
memory (bytes)
3881967616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
3881967616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  90592380.0
relative error loss 0.10124731
shape of L is 
torch.Size([])
memory (bytes)
3885096960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% |  7% |
memory (bytes)
3885191168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  90501950.0
relative error loss 0.101146236
shape of L is 
torch.Size([])
memory (bytes)
3888418816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% |  7% |
memory (bytes)
3888418816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  90474110.0
relative error loss 0.10111512
shape of L is 
torch.Size([])
memory (bytes)
3891597312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
3891597312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  90425090.0
relative error loss 0.10106034
shape of L is 
torch.Size([])
memory (bytes)
3894870016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
3894870016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  90391740.0
relative error loss 0.10102307
time to take a step is 402.02948117256165
it  4 : 962573824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
3898003456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
3898097664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  90391740.0
relative error loss 0.10102307
shape of L is 
torch.Size([])
memory (bytes)
3901329408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
3901329408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  90363580.0
relative error loss 0.1009916
shape of L is 
torch.Size([])
memory (bytes)
3904557056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
3904557056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  90299580.0
relative error loss 0.100920066
shape of L is 
torch.Size([])
memory (bytes)
3907780608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
3907780608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  90255940.0
relative error loss 0.10087129
shape of L is 
torch.Size([])
memory (bytes)
3911008256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
3911008256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  90212420.0
relative error loss 0.10082265
shape of L is 
torch.Size([])
memory (bytes)
3914227712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  7% |
memory (bytes)
3914227712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  90181380.0
relative error loss 0.10078796
shape of L is 
torch.Size([])
memory (bytes)
3917418496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
3917447168
| ID | GPU | MEM |
------------------
|  0 |  2% |  0% |
|  1 | 99% |  7% |
error is  90132030.0
relative error loss 0.10073281
shape of L is 
torch.Size([])
memory (bytes)
3920674816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  5% |  7% |
memory (bytes)
3920674816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  90145540.0
relative error loss 0.100747906
shape of L is 
torch.Size([])
memory (bytes)
3923894272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
3923894272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  90103620.0
relative error loss 0.10070106
shape of L is 
torch.Size([])
memory (bytes)
3927117824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  5% |  7% |
memory (bytes)
3927117824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  90065020.0
relative error loss 0.100657925
time to take a step is 415.0061264038086
it  5 : 962573824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
3930288128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
3930288128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  90065020.0
relative error loss 0.100657925
shape of L is 
torch.Size([])
memory (bytes)
3933552640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
3933552640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  90018110.0
relative error loss 0.100605495
shape of L is 
torch.Size([])
memory (bytes)
3936694272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% |  7% |
memory (bytes)
3936694272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89996030.0
relative error loss 0.10058082
shape of L is 
torch.Size([])
memory (bytes)
3939917824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
3940012032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89966400.0
relative error loss 0.1005477
shape of L is 
torch.Size([])
memory (bytes)
3943129088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  7% |
memory (bytes)
3943247872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89948540.0
relative error loss 0.10052774
shape of L is 
torch.Size([])
memory (bytes)
3946414080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
3946471424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89931780.0
relative error loss 0.100509
shape of L is 
torch.Size([])
memory (bytes)
3949699072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
3949699072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89899650.0
relative error loss 0.1004731
shape of L is 
torch.Size([])
memory (bytes)
3952918528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
3952918528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89868290.0
relative error loss 0.10043805
shape of L is 
torch.Size([])
memory (bytes)
3956142080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
3956142080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89851260.0
relative error loss 0.10041902
shape of L is 
torch.Size([])
memory (bytes)
3959238656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
3959369728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89826050.0
relative error loss 0.10039084
time to take a step is 436.49081587791443
it  6 : 962573824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  5% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
3962589184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  5% |  7% |
memory (bytes)
3962593280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89826050.0
relative error loss 0.10039084
shape of L is 
torch.Size([])
memory (bytes)
3965771776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
3965771776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89793470.0
relative error loss 0.10035443
shape of L is 
torch.Size([])
memory (bytes)
3969044480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  7% |
memory (bytes)
3969044480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89763970.0
relative error loss 0.10032146
shape of L is 
torch.Size([])
memory (bytes)
3972190208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
3972190208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89744450.0
relative error loss 0.10029964
shape of L is 
torch.Size([])
memory (bytes)
3975503872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
3975503872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89729410.0
relative error loss 0.10028283
shape of L is 
torch.Size([])
memory (bytes)
3978731520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
3978731520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89711040.0
relative error loss 0.10026231
shape of L is 
torch.Size([])
memory (bytes)
3981950976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
3981950976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89689860.0
relative error loss 0.10023863
shape of L is 
torch.Size([])
memory (bytes)
3985174528
| ID | GPU | MEM |
------------------
|  0 |  4% |  0% |
|  1 |  7% |  7% |
memory (bytes)
3985174528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89671300.0
relative error loss 0.100217886
shape of L is 
torch.Size([])
memory (bytes)
3988410368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
3988410368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89661500.0
relative error loss 0.10020694
shape of L is 
torch.Size([])
memory (bytes)
3991629824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
3991629824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89631550.0
relative error loss 0.100173466
time to take a step is 414.52228927612305
it  7 : 962573824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
3994763264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
3994845184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89631550.0
relative error loss 0.100173466
shape of L is 
torch.Size([])
memory (bytes)
3998068736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
3998068736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89623740.0
relative error loss 0.10016474
shape of L is 
torch.Size([])
memory (bytes)
4001210368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
4001210368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89606980.0
relative error loss 0.100146
shape of L is 
torch.Size([])
memory (bytes)
4004528128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
4004528128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89598080.0
relative error loss 0.10013606
shape of L is 
torch.Size([])
memory (bytes)
4007665664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
4007665664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89581820.0
relative error loss 0.10011789
shape of L is 
torch.Size([])
memory (bytes)
4010962944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
4010991616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89565950.0
relative error loss 0.10010015
shape of L is 
torch.Size([])
memory (bytes)
4014129152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
4014223360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89555580.0
relative error loss 0.10008857
shape of L is 
torch.Size([])
memory (bytes)
4017442816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
4017442816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89560830.0
relative error loss 0.10009443
shape of L is 
torch.Size([])
memory (bytes)
4020678656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4020678656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89546110.0
relative error loss 0.10007798
shape of L is 
torch.Size([])
memory (bytes)
4023816192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4023816192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89530050.0
relative error loss 0.10006002
time to take a step is 402.7226982116699
it  8 : 962573824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4027150336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4027150336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89530050.0
relative error loss 0.10006002
shape of L is 
torch.Size([])
memory (bytes)
4030267392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4030361600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89516670.0
relative error loss 0.10004508
shape of L is 
torch.Size([])
memory (bytes)
4033581056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4033581056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89502850.0
relative error loss 0.100029625
shape of L is 
torch.Size([])
memory (bytes)
4036718592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
4036718592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89488900.0
relative error loss 0.10001403
shape of L is 
torch.Size([])
memory (bytes)
4039942144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% |  7% |
memory (bytes)
4040032256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89474620.0
relative error loss 0.09999809
shape of L is 
torch.Size([])
memory (bytes)
4043264000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4043264000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89460740.0
relative error loss 0.09998256
shape of L is 
torch.Size([])
memory (bytes)
4046487552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4046487552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89452990.0
relative error loss 0.09997391
shape of L is 
torch.Size([])
memory (bytes)
4049715200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
4049715200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89443580.0
relative error loss 0.09996339
shape of L is 
torch.Size([])
memory (bytes)
4052938752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
4052938752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89438530.0
relative error loss 0.09995774
shape of L is 
torch.Size([])
memory (bytes)
4056059904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
4056162304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89433020.0
relative error loss 0.09995159
time to take a step is 403.2397971153259
it  9 : 962573824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4059398144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
4059398144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89433020.0
relative error loss 0.09995159
shape of L is 
torch.Size([])
memory (bytes)
4062613504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
4062613504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89425410.0
relative error loss 0.09994308
shape of L is 
torch.Size([])
memory (bytes)
4065841152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
4065841152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89419260.0
relative error loss 0.09993621
shape of L is 
torch.Size([])
memory (bytes)
4068962304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
4069056512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89416580.0
relative error loss 0.09993321
shape of L is 
torch.Size([])
memory (bytes)
4072284160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  7% |
memory (bytes)
4072284160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89412990.0
relative error loss 0.0999292
shape of L is 
torch.Size([])
memory (bytes)
4075409408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4075409408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89405950.0
relative error loss 0.09992133
shape of L is 
torch.Size([])
memory (bytes)
4078718976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4078735360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89404800.0
relative error loss 0.09992005
shape of L is 
torch.Size([])
memory (bytes)
4081963008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4081963008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89391940.0
relative error loss 0.09990567
shape of L is 
torch.Size([])
memory (bytes)
4085182464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% |  7% |
memory (bytes)
4085182464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89387970.0
relative error loss 0.09990124
shape of L is 
torch.Size([])
memory (bytes)
4088414208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4088414208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89380100.0
relative error loss 0.09989244
time to take a step is 376.7493829727173
it  10 : 962573824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4091551744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4091645952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89380100.0
relative error loss 0.09989244
shape of L is 
torch.Size([])
memory (bytes)
4094820352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4094857216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89372800.0
relative error loss 0.09988429
shape of L is 
torch.Size([])
memory (bytes)
4098007040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4098007040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  7% |
error is  89363010.0
relative error loss 0.09987334
shape of L is 
torch.Size([])
memory (bytes)
4101275648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4101320704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% |  7% |
error is  89355520.0
relative error loss 0.09986497
shape of L is 
torch.Size([])
memory (bytes)
4104466432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4104466432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  7% |
error is  89347200.0
relative error loss 0.09985567
shape of L is 
torch.Size([])
memory (bytes)
4107771904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% |  7% |
memory (bytes)
4107771904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89340030.0
relative error loss 0.09984766
shape of L is 
torch.Size([])
memory (bytes)
4110913536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4110913536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89333250.0
relative error loss 0.09984008
shape of L is 
torch.Size([])
memory (bytes)
4114219008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4114227200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89325570.0
relative error loss 0.0998315
shape of L is 
torch.Size([])
memory (bytes)
4117438464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4117438464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89320510.0
relative error loss 0.099825844
shape of L is 
torch.Size([])
memory (bytes)
4120571904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4120571904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89315580.0
relative error loss 0.09982034
time to take a step is 330.3628463745117
it  11 : 962573824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4123889664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4123889664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% |  7% |
error is  89315580.0
relative error loss 0.09982034
shape of L is 
torch.Size([])
memory (bytes)
4127035392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4127035392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89311740.0
relative error loss 0.09981605
shape of L is 
torch.Size([])
memory (bytes)
4130336768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4130336768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89308350.0
relative error loss 0.099812254
shape of L is 
torch.Size([])
memory (bytes)
4133494784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% |  7% |
memory (bytes)
4133494784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89304830.0
relative error loss 0.09980832
shape of L is 
torch.Size([])
memory (bytes)
4136800256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% |  7% |
memory (bytes)
4136800256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89299390.0
relative error loss 0.09980224
shape of L is 
torch.Size([])
memory (bytes)
4140015616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4140015616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89296700.0
relative error loss 0.09979924
shape of L is 
torch.Size([])
memory (bytes)
4143247360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4143247360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89291140.0
relative error loss 0.09979302
shape of L is 
torch.Size([])
memory (bytes)
4146376704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4146470912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89286460.0
relative error loss 0.099787794
shape of L is 
torch.Size([])
memory (bytes)
4149694464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
4149694464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89281410.0
relative error loss 0.09978214
shape of L is 
torch.Size([])
memory (bytes)
4152922112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  7% |
memory (bytes)
4152922112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89278660.0
relative error loss 0.09977907
time to take a step is 330.5313997268677
it  12 : 962573824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4156059648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  7% |
memory (bytes)
4156153856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89278660.0
relative error loss 0.09977907
shape of L is 
torch.Size([])
memory (bytes)
4159377408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  7% |
memory (bytes)
4159377408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89268990.0
relative error loss 0.099768266
shape of L is 
torch.Size([])
memory (bytes)
4162584576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4162584576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89265020.0
relative error loss 0.09976383
shape of L is 
torch.Size([])
memory (bytes)
4165828608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4165828608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89260290.0
relative error loss 0.099758536
shape of L is 
torch.Size([])
memory (bytes)
4168884224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4169043968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89252670.0
relative error loss 0.09975003
shape of L is 
torch.Size([])
memory (bytes)
4172201984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% |  7% |
memory (bytes)
4172296192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89246400.0
relative error loss 0.099743016
shape of L is 
torch.Size([])
memory (bytes)
4175515648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% |  7% |
memory (bytes)
4175515648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89241020.0
relative error loss 0.09973701
shape of L is 
torch.Size([])
memory (bytes)
4178644992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4178739200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89236990.0
relative error loss 0.0997325
shape of L is 
torch.Size([])
memory (bytes)
4181958656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% |  7% |
memory (bytes)
4181962752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89230530.0
relative error loss 0.099725276
shape of L is 
torch.Size([])
memory (bytes)
4185186304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4185186304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89232060.0
relative error loss 0.099727
shape of L is 
torch.Size([])
memory (bytes)
4188270592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4188409856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89226240.0
relative error loss 0.099720486
time to take a step is 333.66715383529663
it  13 : 962574336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4191649792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4191649792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89226240.0
relative error loss 0.099720486
shape of L is 
torch.Size([])
memory (bytes)
4194852864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4194852864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89220860.0
relative error loss 0.09971448
shape of L is 
torch.Size([])
memory (bytes)
4197937152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4198100992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89212860.0
relative error loss 0.09970554
shape of L is 
torch.Size([])
memory (bytes)
4201328640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4201328640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89206400.0
relative error loss 0.09969831
shape of L is 
torch.Size([])
memory (bytes)
4204335104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4204335104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89199490.0
relative error loss 0.09969059
shape of L is 
torch.Size([])
memory (bytes)
4207771648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% |  7% |
memory (bytes)
4207771648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89193220.0
relative error loss 0.099683575
shape of L is 
torch.Size([])
memory (bytes)
4210909184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4210909184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89187710.0
relative error loss 0.09967743
shape of L is 
torch.Size([])
memory (bytes)
4214067200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  7% |
memory (bytes)
4214222848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89182910.0
relative error loss 0.099672064
shape of L is 
torch.Size([])
memory (bytes)
4217458688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4217458688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89178370.0
relative error loss 0.09966698
shape of L is 
torch.Size([])
memory (bytes)
4220489728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% |  7% |
memory (bytes)
4220674048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89173760.0
relative error loss 0.099661835
time to take a step is 299.0560882091522
it  14 : 962573824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
shape of L is 
torch.Size([])
memory (bytes)
4223897600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% |  7% |
memory (bytes)
4223897600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89173760.0
relative error loss 0.099661835
shape of L is 
torch.Size([])
memory (bytes)
4227133440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4227133440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89170050.0
relative error loss 0.099657685
shape of L is 
torch.Size([])
memory (bytes)
4230299648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% |  7% |
memory (bytes)
4230299648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89165630.0
relative error loss 0.09965275
shape of L is 
torch.Size([])
memory (bytes)
4233588736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4233588736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89162880.0
relative error loss 0.099649675
shape of L is 
torch.Size([])
memory (bytes)
4236623872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% |  7% |
memory (bytes)
4236816384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89161150.0
relative error loss 0.099647745
shape of L is 
torch.Size([])
memory (bytes)
4239945728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4239945728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89159040.0
relative error loss 0.09964538
shape of L is 
torch.Size([])
memory (bytes)
4243267584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4243267584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89156100.0
relative error loss 0.09964209
shape of L is 
torch.Size([])
memory (bytes)
4246450176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
4246450176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89153340.0
relative error loss 0.09963901
shape of L is 
torch.Size([])
memory (bytes)
4249722880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  7% |
memory (bytes)
4249722880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89151940.0
relative error loss 0.09963744
shape of L is 
torch.Size([])
memory (bytes)
4252946432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% |  7% |
memory (bytes)
4252946432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  7% |
error is  89149760.0
relative error loss 0.09963501
time to take a step is 304.89808773994446
sum tnnu_Z after tensor(4674291., device='cuda:0')
shape of features
(3312,)
shape of features
(3312,)
number of orig particles 13246
number of new particles after remove low mass 11707
tnuZ shape should be parts x labs
torch.Size([13246, 702])
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
error without small mass is  114875790.0
relative error without small mass is  0.12838678
nnu_Z shape should be number of particles by maxV
(13246, 702)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
shape of features
(13246,)
Tue Jan 31 09:06:41 EST 2023
