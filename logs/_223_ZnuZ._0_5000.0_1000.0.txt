Wed Feb 1 10:39:47 EST 2023
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  0% |  0% |
numbers of X: 18207393
numbers of Z: 23627
shape of features
(23627,)
shape of features
(23627,)
ZX	Vol	Parts	Cubes	Eps
Z	0.02398736238699213	23627	23.627	0.10050584170009942
X	0.020689611215285793	364	0.364	0.3844887603456323
X	0.021031477858194723	6683	6.683	0.14654334819853085
X	0.02367242313391338	2675	2.675	0.2068424355443081
X	0.020311852071200242	863	0.863	0.2865802406838935
X	0.021317260752340063	19643	19.643	0.10276404570639672
X	0.02143977179769678	12565	12.565	0.11949577635653673
X	0.021506309947564778	40530	40.53	0.0809586204186733
X	0.02147061918801615	23507	23.507	0.09702473247730942
X	0.021222904107051428	3659	3.659	0.17967216491596938
X	0.02147374278330006	9755	9.755	0.13008488109982297
X	0.02136793195541862	3876	3.876	0.17665512492094446
X	0.02106711848285618	54195	54.195	0.07298192355671344
X	0.02052409574791518	3159	3.159	0.18659706226374412
X	0.0214678922351835	108660	108.66	0.05824248573021189
X	0.02143869150560117	15227	15.227	0.11207996218560616
X	0.021359636749581093	18413	18.413	0.10507267038163651
X	0.02149657038750698	30100	30.1	0.08938560016739548
X	0.021273085161161383	46228	46.228	0.07720435363641567
X	0.02148851104703671	115240	115.24	0.05713045612661157
X	0.022261594038904448	90353	90.353	0.0626909080651872
X	0.02108277451872881	3061	3.061	0.19026320642893702
X	0.021522907140571205	117364	117.364	0.05681399687909652
X	0.021346946081124318	9748	9.748	0.1298594056627384
X	0.021293794219590875	5612	5.612	0.1559714573699171
X	0.021077428119900567	32584	32.584	0.08648439266592035
X	0.02144751807974326	47438	47.438	0.07675091421867136
X	0.02146990927208262	33198	33.198	0.08647809143368013
X	0.021205079787918007	6246	6.246	0.15029549980680068
X	0.021462947522141458	32315	32.315	0.08724925644005431
X	0.02153726122881383	836374	836.374	0.02952999118749178
X	0.021389865839561113	2924	2.924	0.19412295669206459
X	0.02208285631040037	155834	155.834	0.05213519585487446
X	0.020990681774723883	4109	4.109	0.172225240992339
X	0.021085104912777628	3469	3.469	0.18249759915541103
X	0.02096795219180778	2479	2.479	0.20374791052131108
X	0.021921880953477104	30648	30.648	0.08943178354446298
X	0.021978064823052472	40125	40.125	0.08181974748975722
X	0.020330199407651687	1156	1.156	0.26005264066431605
X	0.020647459739461502	4953	4.953	0.16094077932734083
X	0.019720798045628405	916	0.916	0.27819095410361516
X	0.021230959112273434	2290	2.29	0.21007677604212485
X	0.018240669373231606	565	0.565	0.31841787610992045
X	0.018329919863447015	549	0.549	0.3220050928745049
X	0.021031496481962945	1636	1.636	0.23425864421878662
X	0.021084726051786472	333	0.333	0.3985730313945395
X	0.020602597330584886	334	0.334	0.3951165028253722
X	0.02147786931496722	3043	3.043	0.19182111645753136
X	0.020652880965554413	2413	2.413	0.20455390434730264
X	0.020233388469340256	787	0.787	0.29514219022488053
X	0.02116091851473226	2317	2.317	0.2090272115325903
X	0.02122757740206164	3877	3.877	0.17625233423484515
X	0.02056325593871114	2159	2.159	0.21197260947870689
X	0.020167783048683236	1671	1.671	0.2293826524253005
X	0.021790229057886603	1056	1.056	0.2742832535650031
X	0.021078141249686027	5825	5.825	0.15352488954598978
X	0.021438406583462286	1583	1.583	0.23836209623196236
X	0.020948747897210747	1142	1.142	0.26373270478268634
X	0.02116290861160374	2926	2.926	0.19338985075471787
X	0.020797304670770254	1324	1.324	0.25044153461693536
X	0.019647786972993766	1189	1.189	0.25470878764397675
X	0.02142215730284434	3596	3.596	0.18127914338650084
X	0.020520248307940817	622	0.622	0.3207234702316966
X	0.021718382880104532	6476	6.476	0.14968336861182546
X	0.02088518125658164	1694	1.694	0.23101576296701457
X	0.019876431487849688	748	0.748	0.298409244224643
X	0.020746890883120473	1477	1.477	0.24128158428573152
X	0.02041594416531249	851	0.851	0.2884119846581148
X	0.02045132969762562	1581	1.581	0.23474511316117544
X	0.02118161923827452	1359	1.359	0.24979279073111452
X	0.02010499624029607	870	0.87	0.2848360237849549
X	0.020649114280520065	1316	1.316	0.2503505286759975
X	0.0205337585708698	2673	2.673	0.19731336246337702
X	0.02136171602852165	948	0.948	0.28245066400839447
X	0.021259189598201753	799	0.799	0.29853824490194186
X	0.020762148737243764	1304	1.304	0.2515735059561906
X	0.020877592984126565	2674	2.674	0.1983838623085524
X	0.02099654952489316	1175	1.175	0.26143878790057673
X	0.019286883648571982	505	0.505	0.33676237275615317
X	0.021328068413626704	1684	1.684	0.23309689008919265
X	0.023061575791716422	4000	4.0	0.17931212718253273
X	0.020313054844317443	1630	1.63	0.23184400951874778
X	0.019764347133045174	316	0.316	0.3969463353191744
X	0.021402636955969185	4474	4.474	0.16849632574013695
X	0.018786083144188906	282	0.282	0.4053800549966198
X	0.020686869918250285	1052	1.052	0.26991458211265146
X	0.020759104702680366	567	0.567	0.3320538500923844
X	0.02059040959960762	1254	1.254	0.25416894318736993
X	0.018760393911498235	483	0.483	0.33866080842240426
X	0.020956168229400097	2251	2.251	0.21036757163312772
X	0.020775917533145068	2417	2.417	0.2048461727060445
X	0.019600285965101206	628	0.628	0.31484798726065927
X	0.019820503344843463	608	0.608	0.31945059398790265
X	0.019601984102964345	463	0.463	0.3485296101796515
X	0.020688036587138227	1812	1.812	0.22517509816745204
X	0.020182243665710357	1173	1.173	0.25816090669812347
X	0.0209391842529425	2657	2.657	0.1990013708026037
X	0.020901280387610074	2834	2.834	0.19465146728928281
X	0.020490923859317573	2005	2.005	0.21701131945798674
X	0.02092688610561706	1377	1.377	0.2476986027707181
X	0.02028333789254782	1630	1.63	0.23173089573072087
X	0.021370003406089244	7854	7.854	0.13960615461298254
X	0.020198624388403856	996	0.996	0.27270147370309683
X	0.020475759209245343	1873	1.873	0.22193923195231643
X	0.020908680332130697	954	0.954	0.2798504971193256
X	0.019500267352960465	1509	1.509	0.23466614661405027
X	0.019501507589209956	1088	1.088	0.2617056489226939
X	0.021246862401380982	2801	2.801	0.19648401868446397
X	0.020921466222666076	410	0.41	0.37091074978081356
X	0.019230356458849088	1176	1.176	0.2538204857588362
X	0.020139756578634153	939	0.939	0.27784091994689797
X	0.020785008642788403	1150	1.15	0.26243240253164674
X	0.020252099854072957	644	0.644	0.31564195955785096
X	0.020647835616600607	472	0.472	0.3523526253931379
X	0.02136694475015352	1790	1.79	0.22854047833148353
X	0.019805176388090893	1984	1.984	0.21531734478328843
X	0.018550678944879464	453	0.453	0.3446835329346183
X	0.020906601080686746	770	0.77	0.3005598064981482
X	0.020390066694711276	564	0.564	0.3306586189034935
X	0.021393209311820312	2704	2.704	0.19926135734557293
X	0.0197222268945328	626	0.626	0.3158352061743354
X	0.021127175584986942	5599	5.599	0.1556838840826442
X	0.02077446480303795	1260	1.26	0.2545187410394612
X	0.01880022943393672	604	0.604	0.31456379590539174
X	0.020863745604752484	1671	1.671	0.23199143054213456
X	0.02083730299463401	1187	1.187	0.2598944045050604
X	0.019689632656110005	627	0.627	0.315493215400937
X	0.02139614971765941	1440	1.44	0.2458429236297947
X	0.019434345950455543	768	0.768	0.29358660472102904
X	0.021899622273202482	13275	13.275	0.11815915181281748
X	0.02105760311496	1629	1.629	0.23469073796608125
X	0.020942174609155807	2657	2.657	0.19901084357945387
X	0.021098176230676044	514	0.514	0.34495353422588004
X	0.020993898481723235	1391	1.391	0.24712801886179434
X	0.020200416311665432	485	0.485	0.34663535822072705
X	0.020975030133196366	1405	1.405	0.24623063763114497
X	0.020816944807498728	920	0.92	0.28284144964908386
X	0.018998680665101075	399	0.399	0.3624517345560412
X	0.02034157499485996	939	0.939	0.278765908754863
X	0.020294927403988722	694	0.694	0.30808887089747067
X	0.021292509136455747	1252	1.252	0.2571624202805803
X	0.020955619561987413	2038	2.038	0.21745301918960483
X	0.020029949090423427	549	0.549	0.33166718163192543
X	0.02090251426228257	4038	4.038	0.17298591294981291
X	0.020531357952317284	5195	5.195	0.15810443078240655
X	0.02148328213727464	5003	5.003	0.16253867298214916
X	0.020519626797266415	675	0.675	0.31209625805923596
X	0.02026803532840433	910	0.91	0.2813563708183181
X	0.01968757228933536	1090	1.09	0.26237466028749473
X	0.020113181727606247	608	0.608	0.3210153006211179
X	0.020938353288263278	871	0.871	0.28860783885722874
X	0.01948970212914485	1986	1.986	0.21409604887027447
X	0.02282680220181303	4388	4.388	0.17327110787518393
X	0.021423973411770245	2584	2.584	0.20239626964742075
X	0.020786682604679228	1830	1.83	0.22479054056374564
X	0.020931485177902356	1093	1.093	0.26754281617619347
X	0.021150365492196448	6068	6.068	0.15162032906080855
X	0.020938717702327934	956	0.956	0.27978906040806234
X	0.020480446527482092	679	0.679	0.31128382112990305
X	0.020077660189735964	1576	1.576	0.23355309606073865
X	0.02107703584604292	276	0.276	0.42426136413506804
X	0.021421700945960895	1417	1.417	0.24726427142274332
X	0.02020116452090655	516	0.516	0.3395540394331517
X	0.020375379677945903	2232	2.232	0.20899561459932275
X	0.020808009251504556	546	0.546	0.3365213557198546
X	0.01859758587218379	1017	1.017	0.26345829278211447
X	0.018623440978859797	839	0.839	0.2810386099360884
X	0.021417317402682243	1109	1.109	0.2682941329251794
X	0.020918059436646402	943	0.943	0.28097643633284203
X	0.021046500154333823	1724	1.724	0.23025772449283102
X	0.0209863618358922	281	0.281	0.421124277461675
X	0.01997428546269389	739	0.739	0.3001066400952608
X	0.01798012591347325	737	0.737	0.29002921489295935
X	0.021569291190309946	2245	2.245	0.2125885367756106
X	0.019686518990641166	660	0.66	0.31012849018578154
X	0.021150113387242205	1848	1.848	0.2253565806099107
X	0.021706983737016457	1355	1.355	0.2520887096198568
X	0.021106422499396083	1786	1.786	0.22777762358332543
X	0.020637077391802528	2673	2.673	0.19764374664611384
X	0.02054802840712348	1745	1.745	0.22750532979724006
X	0.02023061537019575	956	0.956	0.2765988692260004
X	0.020433426615790642	1636	1.636	0.2320167296791488
X	0.020945316981427365	1297	1.297	0.2527642527387009
X	0.019935502505782767	695	0.695	0.3061123096769661
X	0.021366111581732548	1028	1.028	0.2749439234867221
X	0.02137326410746775	5256	5.256	0.15961431260751244
X	0.020903409223510878	2238	2.238	0.21059708078026557
X	0.020489286932744426	1405	1.405	0.24431502338815264
X	0.020800713982608052	1441	1.441	0.2434845565496576
X	0.02098061467796061	5222	5.222	0.15897437030382047
X	0.021123031857045744	5713	5.713	0.15463127730744933
X	0.018889476673646252	551	0.551	0.32485491534906974
X	0.018133794652358006	278	0.278	0.4025451815422898
X	0.020299088144178568	958	0.958	0.2767177429998683
X	0.020143637864732045	502	0.502	0.34235609392614186
X	0.020274094609529288	756	0.756	0.29932286337112674
X	0.020943606328934626	1918	1.918	0.2218541826998158
X	0.019747580896762533	1051	1.051	0.2658502431486148
X	0.020483217862578364	442	0.442	0.359190883094035
X	0.020910362469389158	1281	1.281	0.253670993876246
X	0.020263739596294666	520	0.52	0.3390304848482045
X	0.02119698839642724	2382	2.382	0.20722594794692417
X	0.02080035840178704	1169	1.169	0.26106707438730686
X	0.02137589275225544	3341	3.341	0.185644717271884
X	0.022386738756926747	3971	3.971	0.1779769041417749
X	0.020932588338959	1582	1.582	0.23652233139152978
X	0.017600761061096346	270	0.27	0.4024598500233505
X	0.022670259211046505	3192	3.192	0.19221968788273006
X	0.02072628638828718	1637	1.637	0.23307244842701982
X	0.020820116450545084	809	0.809	0.29524204912617297
X	0.022219290234031738	6277	6.277	0.15240270816747523
X	0.021615408666707612	4112	4.112	0.1738748553155915
X	0.019861628827892584	790	0.79	0.2929516331295217
X	0.019738306830345758	636	0.636	0.3142564933083118
X	0.020618263993376698	441	0.441	0.3602504252726587
X	0.02006336037608011	401	0.401	0.368484960533561
X	0.020210719680186902	313	0.313	0.4011860410274786
X	0.020933547979790496	377	0.377	0.3815052311498745
X	0.018578121920595596	295	0.295	0.3978566280654981
X	0.021040153820057063	2324	2.324	0.20841913811309132
X	0.02053903823377377	1011	1.011	0.2728619525807985
X	0.020639313214756334	416	0.416	0.36745201820524176
X	0.020516340033151628	898	0.898	0.28375347176814986
X	0.021239659115748197	1377	1.377	0.24892653889859162
X	0.020245526422506505	682	0.682	0.30963370754262437
X	0.021410280165098685	3063	3.063	0.19120171548532064
X	0.02091310555120605	1684	1.684	0.23157525311478283
X	0.01970162516038871	790	0.79	0.29216284633963147
X	0.018171024941950276	224	0.224	0.4328895832580438
X	0.02024038045167249	3729	3.729	0.17574194532524268
X	0.020642305698158055	326	0.326	0.3985781940939668
X	0.020914595766628352	623	0.623	0.3225921607383288
X	0.01985546830846715	975	0.975	0.2730812233126915
X	0.020886822753974507	1209	1.209	0.25851278524138416
X	0.021073868811049643	4396	4.396	0.1686147369989481
X	0.02096328824988674	1524	1.524	0.23960257421833664
X	0.01946367366968586	398	0.398	0.36569069862368264
X	0.020898374908459114	741	0.741	0.3043907396900718
X	0.021377571443097605	3319	3.319	0.1860588667258747
X	0.020006969727001264	425	0.425	0.36107503469414487
X	0.021365690334734597	3573	3.573	0.18150752622660646
X	0.021453586520144147	11574	11.574	0.12283971760050558
X	0.021471152958706	2275	2.275	0.2113284600161368
X	0.020742454001999495	2555	2.555	0.20098165142108243
X	0.018760628103924794	421	0.421	0.3545317160976281
X	0.021157790030970153	3584	3.584	0.18073160301981453
X	0.021737388031039143	15168	15.168	0.11274388934153991
X	0.02141322428154754	41864	41.864	0.07997368627853274
X	0.021276243245892616	1667	1.667	0.2336969883472119
X	0.021477406030083822	19964	19.964	0.10246560031171545
X	0.02117564751229776	47901	47.901	0.0761781999628018
X	0.020974282828824853	5087	5.087	0.16035227558373166
X	0.021955336605109378	223734	223.734	0.04612496371735808
X	0.01966752445794	797	0.797	0.2911368145128546
X	0.02108664781269549	5384	5.384	0.1576282900809297
X	0.02143172444503502	38644	38.644	0.08215960597344889
X	0.021875059936652463	223321	223.321	0.04609706017144524
X	0.021446952732549394	1253	1.253	0.2577140966276639
X	0.021252741341236815	12939	12.939	0.11798806567358039
X	0.02142011865337933	9715	9.715	0.13015465013393387
X	0.021374907066860575	71701	71.701	0.06680254488111285
X	0.021359160789356565	25113	25.113	0.09474621355497469
X	0.02209122564803661	20347	20.347	0.1027794940695826
X	0.021499297632737138	9572	9.572	0.13096056116850074
X	0.021362011113404328	7035	7.035	0.14480804098908256
X	0.020241449359433707	8663	8.663	0.13269597932013533
X	0.021530610347433348	38258	38.258	0.082561585819589
X	0.020524475908805954	1306	1.306	0.25048187165895663
X	0.020908636050413643	1274	1.274	0.2541277502188512
X	0.021150338697549193	30068	30.068	0.08893463526643453
X	0.021140524972327133	12395	12.395	0.11947849558482576
X	0.021304711133933556	161710	161.71	0.05088380633159198
X	0.021325825697366737	44005	44.005	0.07854790568463606
X	0.02006005765019253	436	0.436	0.3583289490722759
X	0.020943802965498026	4218	4.218	0.1706015565714961
X	0.020936698964618417	3158	3.158	0.18785900928400562
X	0.021142783891809312	7974	7.974	0.138408250711909
X	0.02148985363801562	36099	36.099	0.08412261452572711
X	0.02157412587127991	17468	17.468	0.10729101199009437
X	0.021037305773552544	25091	25.091	0.09429544542086422
X	0.019903071820993167	505	0.505	0.3403112012532416
X	0.0209169845797163	2776	2.776	0.196046851289973
X	0.021469949050984007	11009	11.009	0.12493795143953094
X	0.021413829115130064	39252	39.252	0.0817104334204246
X	0.020609689840214714	13873	13.873	0.11410386531919296
X	0.020357376127510683	2003	2.003	0.21661088854361898
X	0.021467525550200588	153214	153.214	0.05193911107288564
X	0.02133176755636688	3459	3.459	0.18338286786230684
X	0.02139939488075196	15154	15.154	0.11219101314804161
X	0.021496613542112585	50484	50.484	0.0752324889983896
X	0.02029502007526823	1408	1.408	0.2433673296397511
X	0.021451790552212892	17133	17.133	0.10778129012049514
X	0.022208464265036323	43465	43.465	0.07994530208962183
X	0.022148281222417913	93073	93.073	0.06196866005493898
X	0.020637779263604134	5398	5.398	0.15636637240450904
X	0.02071465685015642	9236	9.236	0.13089747617779118
X	0.02002020428215796	3183	3.183	0.18459104226833756
X	0.02113562366768226	2759	2.759	0.19713078696691963
X	0.021891440679910248	18061	18.061	0.10662135160297936
X	0.021348269728231856	4299	4.299	0.1706076150996282
X	0.021505154533582878	26148	26.148	0.09369175278364802
X	0.021532108375075566	38023	38.023	0.08273324507805524
X	0.021508615309234806	10580	10.58	0.12668025477274739
X	0.02117442773301071	3407	3.407	0.1838568645123606
X	0.021396474560795752	4874	4.874	0.1637390371692374
X	0.021527961551936255	27329	27.329	0.09235485330635204
X	0.021494395814434037	8070	8.07	0.1386172362947701
X	0.020895186944444186	2008	2.008	0.21832033254855507
X	0.021394329499040927	9552	9.552	0.13083826871644375
X	0.02144332138657128	273850	273.85	0.042781870602023556
X	0.020799289215982272	3671	3.671	0.17827401126114606
X	0.020658304045688806	41901	41.901	0.07899933321932077
X	0.021075927652711872	3710	3.71	0.178431254882985
X	0.019273221941832255	5911	5.911	0.14828519333162649
X	0.02150135485729444	6774	6.774	0.14696261741101463
X	0.021099327246039374	13911	13.911	0.11489560574204981
X	0.021380982697483748	31896	31.896	0.0875179520502559
X	0.021214511070751563	2683	2.683	0.19922207145812498
X	0.021314046612895647	66294	66.294	0.0685063079430054
X	0.021528833064814127	56947	56.947	0.07230747787519391
X	0.021450472780933417	2096	2.096	0.21711101789130824
X	0.021345415214795122	23843	23.843	0.096378738191584
X	0.021529147131786833	59341	59.341	0.0713220780524981
X	0.02309824852115258	147371	147.371	0.0539165804007854
X	0.021225704042737235	11149	11.149	0.12393921173052388
X	0.018730957033027528	2349	2.349	0.19978320851855358
X	0.0214384836700538	3297	3.297	0.1866487299848312
X	0.021471043863071675	5164	5.164	0.16080108513920816
X	0.021186063408089387	18738	18.738	0.10417791894117884
X	0.021155813107400667	3126	3.126	0.18915315201361388
X	0.01977173567023538	1307	1.307	0.2473183854766344
X	0.02145467128715828	43092	43.092	0.07925774975956386
X	0.021476908761241295	88242	88.242	0.062435578941254156
X	0.019540297504433927	60342	60.342	0.06867054440145745
X	0.02035673250687518	1493	1.493	0.23889996570233557
X	0.0212891708854217	6559	6.559	0.14806089571163397
X	0.02212946766854898	12676	12.676	0.12041009863406761
X	0.0213660455206542	5886	5.886	0.15368611437324498
X	0.02144924191939596	5726	5.726	0.15530551137491735
X	0.021359422757009403	2298	2.298	0.21025507908607108
X	0.020783926291724198	100347	100.347	0.05916629847995259
X	0.021473704206817042	20398	20.398	0.10172783294200431
X	0.021424364412370306	5851	5.851	0.15413192987513552
X	0.02095313797693056	5212	5.212	0.15900650383161294
X	0.019500241698832098	1887	1.887	0.2178157925525554
X	0.021515778833798203	202328	202.328	0.04737694225789941
X	0.02107147178243947	3307	3.307	0.18539025081094
X	0.021380516546332436	58642	58.642	0.071439182026555
X	0.02087332704484895	730	0.73	0.30578976079280523
X	0.020247851356901728	1003	1.003	0.27228643362911575
X	0.021422153597238678	3112	3.112	0.1902280313766475
X	0.021490928054302645	11106	11.106	0.12461371220402069
X	0.021873014337210433	11654	11.654	0.12335153017794878
X	0.021058912386372117	7815	7.815	0.13915612236247116
X	0.02052249785044981	2780	2.78	0.19471308646212676
X	0.021129946884133423	2223	2.223	0.21182943810418375
X	0.021796887434646223	169494	169.494	0.05047543272028982
X	0.021985308099253052	15124	15.124	0.11328053614419901
X	0.021351299210510308	7741	7.741	0.14024124106765637
X	0.022071362029890434	37969	37.969	0.08345774737850037
X	0.022010598736790298	28145	28.145	0.09213193683242946
X	0.02074537768149933	2336	2.336	0.20708541013377862
X	0.020938413864416	2031	2.031	0.2176429574866216
X	0.02109662010888091	5511	5.511	0.15643268173272187
X	0.019353363250955605	231	0.231	0.4375706218193173
X	0.020784643791293425	1360	1.36	0.24816158779164088
X	0.021439956147790457	10660	10.66	0.12622795975204362
X	0.020036377095089174	1670	1.67	0.228929051302447
X	0.02032625074731707	1281	1.281	0.2512866305527332
X	0.02064020645146148	1724	1.724	0.22876641083246813
X	0.02110016248797841	2931	2.931	0.19308861189234627
X	0.021387790668909782	239664	239.664	0.044687669572983334
X	0.021268772460226053	5609	5.609	0.1559381323219768
X	0.021182661332486984	19804	19.804	0.10226865281921384
X	0.01981047823749048	23200	23.2	0.09487147621122603
X	0.02144842176672345	1424	1.424	0.24696104297609905
X	0.020924422304828803	5536	5.536	0.15577070962401754
X	0.02201009007763617	253476	253.476	0.04428211188987081
X	0.02150554642224774	341956	341.956	0.039767359114205744
X	0.02148978445219991	22788	22.788	0.09806377695616526
X	0.021471849260719853	12018	12.018	0.12134235422118128
X	0.01999475389385691	1124	1.124	0.2610457241023148
X	0.021013945682568088	53305	53.305	0.07332406817748273
X	0.021384291368102364	32457	32.457	0.08701527476756292
X	0.022058387769361	77810	77.81	0.06569204533128234
X	0.02006956623092132	5215	5.215	0.15670922317330865
X	0.021433562453885377	59769	59.769	0.07104597145784532
X	0.02204398308006492	81212	81.212	0.06474754224873241
X	0.020913085521333707	25039	25.039	0.09417458413818104
X	0.020907413106675626	2578	2.578	0.2009119624889596
X	0.023782163659606375	8201	8.201	0.14260284297893336
X	0.0211156794453293	3649	3.649	0.17953277029223985
X	0.020756154085727396	1734	1.734	0.2287525431261387
X	0.021504025798923063	55933	55.933	0.07271386333455591
X	0.020043842884674132	13772	13.772	0.11332559082340962
X	0.0196227354913317	502	0.502	0.33937924370999817
X	0.021437159875274482	10343	10.343	0.12749903783325509
X	0.021464732578193955	5162	5.162	0.16080609065801146
X	0.01969054703424166	495	0.495	0.3413638496911207
X	0.021090624858562425	30596	30.596	0.08833678684532464
X	0.021518987476885406	60599	60.599	0.07081394539622834
X	0.021434701279461003	13498	13.498	0.11666735103685165
X	0.02130591087347203	37374	37.374	0.08291698618298159
X	0.021440535016333355	42090	42.09	0.07986421566212709
X	0.020846838911516542	3764	3.764	0.17692812099324742
X	0.020820987848297008	13386	13.386	0.11586444082225755
X	0.021495246336720883	30669	30.669	0.08882753361735737
X	0.021502365003541347	77585	77.585	0.06519828245640957
X	0.020942400674057334	2187	2.187	0.21235340401430203
X	0.02154499258299136	38353	38.353	0.08251172554458656
X	0.02154334932619521	58610	58.61	0.07163311450575176
X	0.02153167283359582	10946	10.946	0.12529702982763388
X	0.021524948590160655	6081	6.081	0.1524014292139763
X	0.022448299551635194	18224	18.224	0.10719632927994223
X	0.021904662222793782	6131	6.131	0.15287451121927834
X	0.020802102107487123	2055	2.055	0.21632089238767419
X	0.02133720938263068	12985	12.985	0.11800449617874363
X	0.02177861155905717	82093	82.093	0.06425516168240032
X	0.021488477129692214	48047	48.047	0.07647387922518106
X	0.02146982691388972	303752	303.752	0.04134628231492677
X	0.021421750472031858	26223	26.223	0.09348118171151315
X	0.02128783749997523	51726	51.726	0.07438309021238883
X	0.021329393519726778	10228	10.228	0.1277603018413882
X	0.021509370647028816	55970	55.97	0.07270385939821333
X	0.019766241137148807	588	0.588	0.3227371213850911
X	0.020956673274377623	4940	4.94	0.16188196392775878
X	0.021473366497356178	126721	126.721	0.055337200145389766
X	0.0213716632751118	8797	8.797	0.13443156163059028
X	0.019568505777475962	1541	1.541	0.23330193564584384
X	0.02149176909011118	8551	8.551	0.13596227404623068
X	0.021879510669020538	17891	17.891	0.10693855950274128
X	0.02142668808697975	21510	21.51	0.09987072725011824
X	0.021441015925228332	26959	26.959	0.09265037758208036
X	0.020915857033779624	6358	6.358	0.14872536622584828
X	0.020969930240326708	19277	19.277	0.10284563076832355
X	0.02122607535139061	1327	1.327	0.251960769625048
X	0.021376889179685207	8573	8.573	0.13560339435191474
X	0.02146320426460038	12371	12.371	0.12016092160860983
X	0.02147329532837808	19218	19.218	0.10376801346770154
X	0.020119394820087674	876	0.876	0.2842520479691989
X	0.021541392347902685	82936	82.936	0.06380336172411732
X	0.020158250621427804	793	0.793	0.29403106548542257
X	0.021250910241745472	10105	10.105	0.12811905884437966
X	0.02073777551265051	3211	3.211	0.18622613124704565
X	0.021241368395528693	12490	12.49	0.11936399640523408
X	0.021137276432513955	1889	1.889	0.2236689452365997
X	0.020880169030506088	9576	9.576	0.1296731191775556
X	0.021286565657301103	7415	7.415	0.14212314668196943
X	0.02141684297834879	11683	11.683	0.12238654985706673
X	0.02143788265666473	20813	20.813	0.1009909370236743
X	0.021470900340846692	120062	120.062	0.05633974723004464
X	0.020944752648845388	1609	1.609	0.23523741440917886
X	0.01990394812044786	4002	4.002	0.17069496684052873
X	0.021347934519026848	69827	69.827	0.06736652716027626
X	0.020567031048154813	2609	2.609	0.1990211240963361
X	0.02149032693651322	256551	256.551	0.04375453810582898
X	0.020718506189554765	736	0.736	0.3042006995021695
X	0.022072665829311187	50868	50.868	0.07570711707267527
X	0.02192455798494049	146464	146.464	0.053096592620067495
X	0.02085943292771592	1256	1.256	0.2551355361196198
X	0.02127067427537359	107655	107.655	0.05824370818562566
X	0.021411262716786294	6220	6.22	0.15099085066012824
X	0.021373569863509165	79682	79.682	0.06449196549487544
X	0.021008737822667746	4039	4.039	0.17326414615990907
X	0.020265409223476916	41036	41.036	0.07904300676262947
X	0.020073582865395533	385	0.385	0.3735838171100186
X	0.020421520586195067	2235	2.235	0.20905963268766622
X	0.02145246857061855	2632	2.632	0.20124748568103937
X	0.021515233613990447	19212	19.212	0.10384633234649218
X	0.020359412924025624	9415	9.415	0.12931482272472897
X	0.021443879197358424	51874	51.874	0.07449341282988003
X	0.02094778237332859	7112	7.112	0.14334485240482417
X	0.020630979717595338	1094	1.094	0.2661751515658089
X	0.02124032761966372	2422	2.422	0.20621916504944915
X	0.02140629614487368	15498	15.498	0.11136668462164084
X	0.020437660616587004	1452	1.452	0.2414468380031893
X	0.022427764467593484	71466	71.466	0.06795616109152662
X	0.021539311791410522	34557	34.557	0.08542112037806521
X	0.021483886221833986	5118	5.118	0.16131355196904879
X	0.019801863915728583	1068	1.068	0.2646741983152682
X	0.020603733158537894	5619	5.619	0.1542039960477194
X	0.021580838295630452	341809	341.809	0.03981942082974945
X	0.021324880990664114	4705	4.705	0.1654914887246141
X	0.02133933981801721	7379	7.379	0.14247144300201625
X	0.021258669474201172	40806	40.806	0.08046444463675168
X	0.021060693516734216	14330	14.33	0.11369520766637177
X	0.020607343726966838	1343	1.343	0.24849367225013824
X	0.019219865500998398	1334	1.334	0.24333140936093978
X	0.021600052288773885	114114	114.114	0.057416752428541176
X	0.021498122797386422	7864	7.864	0.13982527371090783
X	0.021192952936279243	19869	19.869	0.10217355114132791
X	0.021485783551736155	16695	16.695	0.10877312129144988
X	0.021482368740147213	8862	8.862	0.13433323506652434
X	0.021159913667861463	25306	25.306	0.09420995490571939
X	0.021517841163460918	73489	73.489	0.06640367569455682
X	0.021531105070013715	53223	53.223	0.07395865250313903
X	0.02134604506488788	1926	1.926	0.2229566240187439
X	0.02153582165442039	28691	28.691	0.09088075371521578
X	0.021462036649657304	3802	3.802	0.17805449415050636
X	0.021352276625788823	6337	6.337	0.14991790111813363
X	0.02138707018565212	14245	14.245	0.11450636116796169
X	0.021436658979065656	2238	2.238	0.21237285509112264
X	0.021252661642381473	32720	32.72	0.08660308130621344
X	0.020788779276475357	6348	6.348	0.14850144856801709
X	0.02066063711216573	1149	1.149	0.26198387430773085
X	0.0210653747335978	7030	7.03	0.14416880148794095
X	0.021485202829360835	2783	2.783	0.19764026030393828
X	0.02132164798596269	40631	40.631	0.08065929453835226
X	0.021043321210127603	40033	40.033	0.08070467927640064
X	0.021419765168590132	5369	5.369	0.15860143061528467
X	0.02068034552422855	4360	4.36	0.16801849894003631
X	0.021401885104347103	10775	10.775	0.12570278983026056
X	0.020055377183826224	1317	1.317	0.24786489361511665
X	0.02098887199256935	2703	2.703	0.19802241425773875
X	0.021498042671372352	13386	13.386	0.11710695805152255
X	0.021527362249410154	70127	70.127	0.06745826796147311
X	0.02094833979168951	2208	2.208	0.21169804475734838
X	0.021225806538860702	959	0.959	0.280768596762008
X	0.019851036770820403	1210	1.21	0.25409684608078564
X	0.021465859330569124	118203	118.203	0.056629134834933675
X	0.022258758747915407	187923	187.923	0.0491104222838248
X	0.021206705767867984	14118	14.118	0.11452492369355396
X	0.01927730779971875	2298	2.298	0.2031883416608234
X	0.021512094519984065	16464	16.464	0.1093240670885131
X	0.023624819901418053	37565	37.565	0.08567649758328699
X	0.021195077300313506	1240	1.24	0.2575952041814561
X	0.02144917207128913	5329	5.329	0.1590699963134374
X	0.02142525263908363	10622	10.622	0.12634940997488434
X	0.021817857810964725	46260	46.26	0.07783988032447291
X	0.02135640600792702	18943	18.943	0.104078207309759
X	0.021506660940172034	65405	65.405	0.06902196928688674
X	0.021359914466840046	5152	5.152	0.1606477146200217
X	0.02096921354722106	1400	1.4	0.24650063115553866
X	0.01919949226467562	1301	1.301	0.24528490133198658
X	0.020604946113474253	14593	14.593	0.11218693943046303
X	0.020997771618345477	8342	8.342	0.136029888089724
X	0.021434143476304082	27256	27.256	0.09230275652567058
X	0.021121901950394838	29209	29.209	0.08975778178647767
X	0.0220676112707239	153474	153.474	0.052389007503899056
X	0.020275349704537926	3536	3.536	0.1789858502265504
X	0.019515579509281275	3041	3.041	0.18583250334431076
X	0.021436554694681455	13457	13.457	0.11678908197240427
X	0.021818559497831812	204551	204.551	0.04742508912854875
X	0.02103822555703128	17681	17.681	0.10596621635140678
X	0.020812186338864155	3767	3.767	0.1767830796038239
X	0.020059899872212673	768	0.768	0.2967033935765534
X	0.021406191492913954	23971	23.971	0.09629810732048082
X	0.020716362572145326	653	0.653	0.3165678261860649
X	0.021331277836897346	6483	6.483	0.1487351503915287
X	0.020653603726304304	1628	1.628	0.2332278881686107
X	0.021491481262383393	6761	6.761	0.14703423664963958
X	0.020700951158647143	3869	3.869	0.1749029724554805
X	0.0212711426477018	1953	1.953	0.22166452382323257
X	0.02041166093816169	840	0.84	0.2896452116073493
X	0.02201349911142458	116775	116.775	0.057338509778941886
X	0.02187755520804491	206443	206.443	0.04732233972148462
X	0.021521783146002234	62881	62.881	0.06994976672023195
X	0.019911529430627333	19898	19.898	0.10002265950579334
X	0.021530809007891963	13398	13.398	0.11713143349044773
X	0.020928660568679074	4907	4.907	0.16217172097391222
X	0.0212632478597254	5414	5.414	0.15777461051573277
X	0.021766523061378745	27083	27.083	0.09297455260113889
X	0.023906641171031754	25759	25.759	0.09754309272101773
X	0.02147511816670375	55117	55.117	0.07303818914086432
X	0.021069736653113097	4213	4.213	0.17101041048518795
X	0.022881283950680023	146326	146.326	0.05387487761587655
X	0.021439124656428245	10996	10.996	0.12492732461551916
X	0.022687521097014187	14029	14.029	0.11737801937623477
X	0.02315544590161703	79344	79.344	0.06633046153905837
X	0.022056743629723593	31356	31.356	0.08893517825077121
X	0.021483992114871592	100607	100.607	0.05977169822382502
X	0.021990063795964573	67931	67.931	0.06866248131292116
X	0.021438061979731138	24779	24.779	0.09528705739503728
X	0.021478699887238033	21605	21.605	0.09980475657376588
X	0.020459089965079235	8456	8.456	0.1342477799721339
X	0.021451262902329186	32570	32.57	0.08700516513796219
X	0.021458508747635423	5708	5.708	0.1554909765149912
X	0.021457242686983236	61946	61.946	0.07022960325891735
X	0.02238420239280663	389160	389.16	0.038601526250737435
X	0.021824454850434254	33168	33.168	0.08697772416792125
X	0.020528221343152964	23564	23.564	0.09550675788784052
X	0.020921658532469512	15754	15.754	0.10991790007430476
X	0.021307636376762057	3039	3.039	0.19139688216903775
X	0.0234606002372587	18247	18.247	0.10873830252079097
X	0.020264461575381202	9558	9.558	0.12846633892335124
X	0.02200358394005292	39871	39.871	0.08202484757816102
X	0.021854052935582593	92504	92.504	0.061819263406910165
X	0.021191162021314564	2060	2.06	0.2174849373812105
X	0.02145234678451646	73697	73.697	0.06627376851652807
X	0.02146164935872	51613	51.613	0.07463937554458526
X	0.02141491676323298	19684	19.684	0.10284922214425331
X	0.021269884755240755	1424	1.424	0.24627389691732315
X	0.02142820514657282	40812	40.812	0.08067382323118696
X	0.02147451345860331	52343	52.343	0.07430560493856299
X	0.02047809501495657	1780	1.78	0.22574762179392924
X	0.02181436845007865	333611	333.611	0.04028723180531392
X	0.021506910155964527	31052	31.052	0.08847681513704073
X	0.020975471963343126	9225	9.225	0.13149678120503192
X	0.0214805139225167	3115	3.115	0.1903394772494182
X	0.020018790839875702	2060	2.06	0.21339793237843271
X	0.019674787215794298	930	0.93	0.27657252322000264
X	0.020997970701617914	6330	6.33	0.14913899257558236
X	0.02108630646814172	3983	3.983	0.17428636117727261
X	0.021028879462795733	38075	38.075	0.08204625073087604
X	0.02318893135777972	368374	368.374	0.03977992149339465
X	0.021636436079720225	3960	3.96	0.1761287204527898
X	0.02134130016019321	25882	25.882	0.09377225223421524
X	0.02052896021905992	1266	1.266	0.2531110366183084
X	0.020950524688264114	7414	7.414	0.14137765508358993
X	0.021269493527498105	4513	4.513	0.16766044132807226
X	0.021278919166285003	48302	48.302	0.07609009921776574
X	0.020718667196695018	11577	11.577	0.12141022399807802
X	0.0222417887718204	447491	447.491	0.03676730538207441
X	0.021466235737585004	2364	2.364	0.20862650088000703
X	0.0215181524083176	48124	48.124	0.07646823867658807
X	0.021425959777839832	5476	5.476	0.15757680187621165
X	0.021463700193528577	54909	54.909	0.07311733487749335
X	0.0213554529194144	165220	165.22	0.05056096911707201
X	0.02096411069665101	9089	9.089	0.13212554151557587
X	0.021055415393840447	1928	1.928	0.221863364551016
X	0.022848918611736976	40274	40.274	0.08278410572052805
X	0.02103448719202089	7561	7.561	0.14064281927550462
X	0.02098881351148819	12600	12.6	0.1185420539521701
X	0.020818518497010746	47053	47.053	0.07619985895146716
X	0.02129125214924644	11037	11.037	0.12448491303296973
X	0.023978879956407743	19817	19.817	0.1065606828211086
X	0.019995269321370048	7862	7.862	0.13649960864293498
X	0.02374981449653085	34893	34.893	0.08796447272306733
X	0.02147125496452753	105595	105.595	0.05880370517863863
X	0.020927581413910278	8890	8.89	0.13302678435696225
X	0.020826478916464188	2285	2.285	0.2088862638741236
X	0.02139764601970478	62200	62.2	0.07006888468063052
X	0.020286038560529632	17179	17.179	0.10569792356108235
X	0.021183094859871212	63211	63.211	0.06945959552821823
X	0.021308388709483282	9621	9.621	0.13034972881818155
X	0.020443903592438244	36373	36.373	0.08252678005014766
X	0.021076335481750526	13232	13.232	0.11678575432256931
X	0.0210904421242897	44976	44.976	0.07769056597053306
X	0.021208158279565712	18383	18.383	0.10488068687035493
X	0.020835033767178258	9015	9.015	0.13221360869083393
X	0.019634285774991808	1531	1.531	0.2340704763950228
X	0.021217370986439513	10406	10.406	0.12680486504600366
X	0.020821675265861487	11833	11.833	0.12072770219744376
X	0.02142281969058112	47740	47.74	0.0765593209206933
X	0.021506719879849225	57511	57.511	0.07204564820631405
X	0.021186494356580165	3067	3.067	0.19045035005226688
X	0.02144245339406126	14173	14.173	0.11479885661576245
X	0.021084216235328978	22563	22.563	0.09776578072033269
X	0.021447898305603983	121057	121.057	0.05616489416858331
X	0.022095734252123838	23170	23.17	0.09842999613575763
X	0.021494086278373518	31101	31.101	0.0884127457383933
X	0.02214914505536584	23005	23.005	0.09874419567217003
X	0.021483598961202634	13714	13.714	0.11613977059587
X	0.021189971192581196	979	0.979	0.27868647443656186
X	0.02121627225985466	4104	4.104	0.17291018518830553
X	0.021527802705676568	50733	50.733	0.07514551297178215
X	0.020513070091093887	32874	32.874	0.08545274885393815
X	0.021508392361961903	33766	33.766	0.08604179157500443
X	0.020983962660915824	1059	1.059	0.2706016964229265
X	0.021523338584372248	2783	2.783	0.19775712686816363
X	0.021266815061010313	85072	85.072	0.06299488253886094
X	0.02145950702699885	30792	30.792	0.08865990895379042
X	0.021496356328648308	11433	11.433	0.12342454919738638
X	0.020863017352917002	1187	1.187	0.260001268473167
X	0.021262797853097516	647	0.647	0.320311140078267
X	0.02144235856156245	17204	17.204	0.10761703936239127
X	0.02115467209452769	29881	29.881	0.08912585794412263
X	0.021426095981353796	28028	28.028	0.09143594949565383
X	0.021471727736844907	2045	2.045	0.21897333179020612
X	0.02091731491957038	96030	96.03	0.06016810524949389
X	0.02113666527660848	7961	7.961	0.13847018804869005
X	0.020764218689394924	3868	3.868	0.1750960610267634
X	0.021975088352133142	140430	140.43	0.053887774070770104
X	0.021516768301857593	13606	13.606	0.11650615210780231
X	0.0214674386123988	55691	55.691	0.07277771308035655
X	0.02148461261027775	58619	58.619	0.07156429115936448
X	0.02139054049841627	25717	25.717	0.09404459009931376
X	0.022057135800308283	39517	39.517	0.08233573814998729
X	0.02190510646475935	71180	71.18	0.06751430399577667
X	0.02030622889013194	1062	1.062	0.26740421800094033
X	0.021126617816748666	25434	25.434	0.09400229058569103
X	0.021489913770203022	8542	8.542	0.13600609413463444
X	0.021276848719498753	111594	111.594	0.057555763622167216
X	0.021436814625290214	95715	95.715	0.060728620550529855
X	0.02114060122983436	15881	15.881	0.11000518680142454
X	0.02145455082449527	23142	23.142	0.09750783470042196
X	0.020974643470895678	5328	5.328	0.15789805864924106
X	0.02007553002558062	3173	3.173	0.18495481815098241
X	0.021162457390219998	12090	12.09	0.12051652443617122
X	0.020599840324893377	5418	5.418	0.15607797356946929
X	0.02149347485178416	117786	117.786	0.05672018691998014
X	0.022082078838446786	74674	74.674	0.06662289701784857
X	0.020695217319450322	2262	2.262	0.2091506074290879
X	0.021475082670912617	6863	6.863	0.14626495498306039
X	0.02148309610673194	5999	5.999	0.15299339071701212
X	0.021496186534588877	174043	174.043	0.04980064341466553
X	0.021345954914647736	6210	6.21	0.1509181000850234
X	0.020675659003411933	10713	10.713	0.1245038416221641
X	0.021406013508859727	44006	44.006	0.07864563685061703
X	0.0215600053130415	40415	40.415	0.08110272132603107
X	0.020505610475539886	6094	6.094	0.14985000132600973
X	0.021343418385589764	16171	16.171	0.10969218822434462
X	0.021418005663855676	11498	11.498	0.12304167696105543
time for making epsilon is 0.997586727142334
epsilons are
[0.3844887603456323, 0.14654334819853085, 0.2068424355443081, 0.2865802406838935, 0.10276404570639672, 0.11949577635653673, 0.0809586204186733, 0.09702473247730942, 0.17967216491596938, 0.13008488109982297, 0.17665512492094446, 0.07298192355671344, 0.18659706226374412, 0.05824248573021189, 0.11207996218560616, 0.10507267038163651, 0.08938560016739548, 0.07720435363641567, 0.05713045612661157, 0.0626909080651872, 0.19026320642893702, 0.05681399687909652, 0.1298594056627384, 0.1559714573699171, 0.08648439266592035, 0.07675091421867136, 0.08647809143368013, 0.15029549980680068, 0.08724925644005431, 0.02952999118749178, 0.19412295669206459, 0.05213519585487446, 0.172225240992339, 0.18249759915541103, 0.20374791052131108, 0.08943178354446298, 0.08181974748975722, 0.26005264066431605, 0.16094077932734083, 0.27819095410361516, 0.21007677604212485, 0.31841787610992045, 0.3220050928745049, 0.23425864421878662, 0.3985730313945395, 0.3951165028253722, 0.19182111645753136, 0.20455390434730264, 0.29514219022488053, 0.2090272115325903, 0.17625233423484515, 0.21197260947870689, 0.2293826524253005, 0.2742832535650031, 0.15352488954598978, 0.23836209623196236, 0.26373270478268634, 0.19338985075471787, 0.25044153461693536, 0.25470878764397675, 0.18127914338650084, 0.3207234702316966, 0.14968336861182546, 0.23101576296701457, 0.298409244224643, 0.24128158428573152, 0.2884119846581148, 0.23474511316117544, 0.24979279073111452, 0.2848360237849549, 0.2503505286759975, 0.19731336246337702, 0.28245066400839447, 0.29853824490194186, 0.2515735059561906, 0.1983838623085524, 0.26143878790057673, 0.33676237275615317, 0.23309689008919265, 0.17931212718253273, 0.23184400951874778, 0.3969463353191744, 0.16849632574013695, 0.4053800549966198, 0.26991458211265146, 0.3320538500923844, 0.25416894318736993, 0.33866080842240426, 0.21036757163312772, 0.2048461727060445, 0.31484798726065927, 0.31945059398790265, 0.3485296101796515, 0.22517509816745204, 0.25816090669812347, 0.1990013708026037, 0.19465146728928281, 0.21701131945798674, 0.2476986027707181, 0.23173089573072087, 0.13960615461298254, 0.27270147370309683, 0.22193923195231643, 0.2798504971193256, 0.23466614661405027, 0.2617056489226939, 0.19648401868446397, 0.37091074978081356, 0.2538204857588362, 0.27784091994689797, 0.26243240253164674, 0.31564195955785096, 0.3523526253931379, 0.22854047833148353, 0.21531734478328843, 0.3446835329346183, 0.3005598064981482, 0.3306586189034935, 0.19926135734557293, 0.3158352061743354, 0.1556838840826442, 0.2545187410394612, 0.31456379590539174, 0.23199143054213456, 0.2598944045050604, 0.315493215400937, 0.2458429236297947, 0.29358660472102904, 0.11815915181281748, 0.23469073796608125, 0.19901084357945387, 0.34495353422588004, 0.24712801886179434, 0.34663535822072705, 0.24623063763114497, 0.28284144964908386, 0.3624517345560412, 0.278765908754863, 0.30808887089747067, 0.2571624202805803, 0.21745301918960483, 0.33166718163192543, 0.17298591294981291, 0.15810443078240655, 0.16253867298214916, 0.31209625805923596, 0.2813563708183181, 0.26237466028749473, 0.3210153006211179, 0.28860783885722874, 0.21409604887027447, 0.17327110787518393, 0.20239626964742075, 0.22479054056374564, 0.26754281617619347, 0.15162032906080855, 0.27978906040806234, 0.31128382112990305, 0.23355309606073865, 0.42426136413506804, 0.24726427142274332, 0.3395540394331517, 0.20899561459932275, 0.3365213557198546, 0.26345829278211447, 0.2810386099360884, 0.2682941329251794, 0.28097643633284203, 0.23025772449283102, 0.421124277461675, 0.3001066400952608, 0.29002921489295935, 0.2125885367756106, 0.31012849018578154, 0.2253565806099107, 0.2520887096198568, 0.22777762358332543, 0.19764374664611384, 0.22750532979724006, 0.2765988692260004, 0.2320167296791488, 0.2527642527387009, 0.3061123096769661, 0.2749439234867221, 0.15961431260751244, 0.21059708078026557, 0.24431502338815264, 0.2434845565496576, 0.15897437030382047, 0.15463127730744933, 0.32485491534906974, 0.4025451815422898, 0.2767177429998683, 0.34235609392614186, 0.29932286337112674, 0.2218541826998158, 0.2658502431486148, 0.359190883094035, 0.253670993876246, 0.3390304848482045, 0.20722594794692417, 0.26106707438730686, 0.185644717271884, 0.1779769041417749, 0.23652233139152978, 0.4024598500233505, 0.19221968788273006, 0.23307244842701982, 0.29524204912617297, 0.15240270816747523, 0.1738748553155915, 0.2929516331295217, 0.3142564933083118, 0.3602504252726587, 0.368484960533561, 0.4011860410274786, 0.3815052311498745, 0.3978566280654981, 0.20841913811309132, 0.2728619525807985, 0.36745201820524176, 0.28375347176814986, 0.24892653889859162, 0.30963370754262437, 0.19120171548532064, 0.23157525311478283, 0.29216284633963147, 0.4328895832580438, 0.17574194532524268, 0.3985781940939668, 0.3225921607383288, 0.2730812233126915, 0.25851278524138416, 0.1686147369989481, 0.23960257421833664, 0.36569069862368264, 0.3043907396900718, 0.1860588667258747, 0.36107503469414487, 0.18150752622660646, 0.12283971760050558, 0.2113284600161368, 0.20098165142108243, 0.3545317160976281, 0.18073160301981453, 0.11274388934153991, 0.07997368627853274, 0.2336969883472119, 0.10246560031171545, 0.0761781999628018, 0.16035227558373166, 0.04612496371735808, 0.2911368145128546, 0.1576282900809297, 0.08215960597344889, 0.04609706017144524, 0.2577140966276639, 0.11798806567358039, 0.13015465013393387, 0.06680254488111285, 0.09474621355497469, 0.1027794940695826, 0.13096056116850074, 0.14480804098908256, 0.13269597932013533, 0.082561585819589, 0.25048187165895663, 0.2541277502188512, 0.08893463526643453, 0.11947849558482576, 0.05088380633159198, 0.07854790568463606, 0.3583289490722759, 0.1706015565714961, 0.18785900928400562, 0.138408250711909, 0.08412261452572711, 0.10729101199009437, 0.09429544542086422, 0.3403112012532416, 0.196046851289973, 0.12493795143953094, 0.0817104334204246, 0.11410386531919296, 0.21661088854361898, 0.05193911107288564, 0.18338286786230684, 0.11219101314804161, 0.0752324889983896, 0.2433673296397511, 0.10778129012049514, 0.07994530208962183, 0.06196866005493898, 0.15636637240450904, 0.13089747617779118, 0.18459104226833756, 0.19713078696691963, 0.10662135160297936, 0.1706076150996282, 0.09369175278364802, 0.08273324507805524, 0.12668025477274739, 0.1838568645123606, 0.1637390371692374, 0.09235485330635204, 0.1386172362947701, 0.21832033254855507, 0.13083826871644375, 0.042781870602023556, 0.17827401126114606, 0.07899933321932077, 0.178431254882985, 0.14828519333162649, 0.14696261741101463, 0.11489560574204981, 0.0875179520502559, 0.19922207145812498, 0.0685063079430054, 0.07230747787519391, 0.21711101789130824, 0.096378738191584, 0.0713220780524981, 0.0539165804007854, 0.12393921173052388, 0.19978320851855358, 0.1866487299848312, 0.16080108513920816, 0.10417791894117884, 0.18915315201361388, 0.2473183854766344, 0.07925774975956386, 0.062435578941254156, 0.06867054440145745, 0.23889996570233557, 0.14806089571163397, 0.12041009863406761, 0.15368611437324498, 0.15530551137491735, 0.21025507908607108, 0.05916629847995259, 0.10172783294200431, 0.15413192987513552, 0.15900650383161294, 0.2178157925525554, 0.04737694225789941, 0.18539025081094, 0.071439182026555, 0.30578976079280523, 0.27228643362911575, 0.1902280313766475, 0.12461371220402069, 0.12335153017794878, 0.13915612236247116, 0.19471308646212676, 0.21182943810418375, 0.05047543272028982, 0.11328053614419901, 0.14024124106765637, 0.08345774737850037, 0.09213193683242946, 0.20708541013377862, 0.2176429574866216, 0.15643268173272187, 0.4375706218193173, 0.24816158779164088, 0.12622795975204362, 0.228929051302447, 0.2512866305527332, 0.22876641083246813, 0.19308861189234627, 0.044687669572983334, 0.1559381323219768, 0.10226865281921384, 0.09487147621122603, 0.24696104297609905, 0.15577070962401754, 0.04428211188987081, 0.039767359114205744, 0.09806377695616526, 0.12134235422118128, 0.2610457241023148, 0.07332406817748273, 0.08701527476756292, 0.06569204533128234, 0.15670922317330865, 0.07104597145784532, 0.06474754224873241, 0.09417458413818104, 0.2009119624889596, 0.14260284297893336, 0.17953277029223985, 0.2287525431261387, 0.07271386333455591, 0.11332559082340962, 0.33937924370999817, 0.12749903783325509, 0.16080609065801146, 0.3413638496911207, 0.08833678684532464, 0.07081394539622834, 0.11666735103685165, 0.08291698618298159, 0.07986421566212709, 0.17692812099324742, 0.11586444082225755, 0.08882753361735737, 0.06519828245640957, 0.21235340401430203, 0.08251172554458656, 0.07163311450575176, 0.12529702982763388, 0.1524014292139763, 0.10719632927994223, 0.15287451121927834, 0.21632089238767419, 0.11800449617874363, 0.06425516168240032, 0.07647387922518106, 0.04134628231492677, 0.09348118171151315, 0.07438309021238883, 0.1277603018413882, 0.07270385939821333, 0.3227371213850911, 0.16188196392775878, 0.055337200145389766, 0.13443156163059028, 0.23330193564584384, 0.13596227404623068, 0.10693855950274128, 0.09987072725011824, 0.09265037758208036, 0.14872536622584828, 0.10284563076832355, 0.251960769625048, 0.13560339435191474, 0.12016092160860983, 0.10376801346770154, 0.2842520479691989, 0.06380336172411732, 0.29403106548542257, 0.12811905884437966, 0.18622613124704565, 0.11936399640523408, 0.2236689452365997, 0.1296731191775556, 0.14212314668196943, 0.12238654985706673, 0.1009909370236743, 0.05633974723004464, 0.23523741440917886, 0.17069496684052873, 0.06736652716027626, 0.1990211240963361, 0.04375453810582898, 0.3042006995021695, 0.07570711707267527, 0.053096592620067495, 0.2551355361196198, 0.05824370818562566, 0.15099085066012824, 0.06449196549487544, 0.17326414615990907, 0.07904300676262947, 0.3735838171100186, 0.20905963268766622, 0.20124748568103937, 0.10384633234649218, 0.12931482272472897, 0.07449341282988003, 0.14334485240482417, 0.2661751515658089, 0.20621916504944915, 0.11136668462164084, 0.2414468380031893, 0.06795616109152662, 0.08542112037806521, 0.16131355196904879, 0.2646741983152682, 0.1542039960477194, 0.03981942082974945, 0.1654914887246141, 0.14247144300201625, 0.08046444463675168, 0.11369520766637177, 0.24849367225013824, 0.24333140936093978, 0.057416752428541176, 0.13982527371090783, 0.10217355114132791, 0.10877312129144988, 0.13433323506652434, 0.09420995490571939, 0.06640367569455682, 0.07395865250313903, 0.2229566240187439, 0.09088075371521578, 0.17805449415050636, 0.14991790111813363, 0.11450636116796169, 0.21237285509112264, 0.08660308130621344, 0.14850144856801709, 0.26198387430773085, 0.14416880148794095, 0.19764026030393828, 0.08065929453835226, 0.08070467927640064, 0.15860143061528467, 0.16801849894003631, 0.12570278983026056, 0.24786489361511665, 0.19802241425773875, 0.11710695805152255, 0.06745826796147311, 0.21169804475734838, 0.280768596762008, 0.25409684608078564, 0.056629134834933675, 0.0491104222838248, 0.11452492369355396, 0.2031883416608234, 0.1093240670885131, 0.08567649758328699, 0.2575952041814561, 0.1590699963134374, 0.12634940997488434, 0.07783988032447291, 0.104078207309759, 0.06902196928688674, 0.1606477146200217, 0.24650063115553866, 0.24528490133198658, 0.11218693943046303, 0.136029888089724, 0.09230275652567058, 0.08975778178647767, 0.052389007503899056, 0.1789858502265504, 0.18583250334431076, 0.11678908197240427, 0.04742508912854875, 0.10596621635140678, 0.1767830796038239, 0.2967033935765534, 0.09629810732048082, 0.3165678261860649, 0.1487351503915287, 0.2332278881686107, 0.14703423664963958, 0.1749029724554805, 0.22166452382323257, 0.2896452116073493, 0.057338509778941886, 0.04732233972148462, 0.06994976672023195, 0.10002265950579334, 0.11713143349044773, 0.16217172097391222, 0.15777461051573277, 0.09297455260113889, 0.09754309272101773, 0.07303818914086432, 0.17101041048518795, 0.05387487761587655, 0.12492732461551916, 0.11737801937623477, 0.06633046153905837, 0.08893517825077121, 0.05977169822382502, 0.06866248131292116, 0.09528705739503728, 0.09980475657376588, 0.1342477799721339, 0.08700516513796219, 0.1554909765149912, 0.07022960325891735, 0.038601526250737435, 0.08697772416792125, 0.09550675788784052, 0.10991790007430476, 0.19139688216903775, 0.10873830252079097, 0.12846633892335124, 0.08202484757816102, 0.061819263406910165, 0.2174849373812105, 0.06627376851652807, 0.07463937554458526, 0.10284922214425331, 0.24627389691732315, 0.08067382323118696, 0.07430560493856299, 0.22574762179392924, 0.04028723180531392, 0.08847681513704073, 0.13149678120503192, 0.1903394772494182, 0.21339793237843271, 0.27657252322000264, 0.14913899257558236, 0.17428636117727261, 0.08204625073087604, 0.03977992149339465, 0.1761287204527898, 0.09377225223421524, 0.2531110366183084, 0.14137765508358993, 0.16766044132807226, 0.07609009921776574, 0.12141022399807802, 0.03676730538207441, 0.20862650088000703, 0.07646823867658807, 0.15757680187621165, 0.07311733487749335, 0.05056096911707201, 0.13212554151557587, 0.221863364551016, 0.08278410572052805, 0.14064281927550462, 0.1185420539521701, 0.07619985895146716, 0.12448491303296973, 0.1065606828211086, 0.13649960864293498, 0.08796447272306733, 0.05880370517863863, 0.13302678435696225, 0.2088862638741236, 0.07006888468063052, 0.10569792356108235, 0.06945959552821823, 0.13034972881818155, 0.08252678005014766, 0.11678575432256931, 0.07769056597053306, 0.10488068687035493, 0.13221360869083393, 0.2340704763950228, 0.12680486504600366, 0.12072770219744376, 0.0765593209206933, 0.07204564820631405, 0.19045035005226688, 0.11479885661576245, 0.09776578072033269, 0.05616489416858331, 0.09842999613575763, 0.0884127457383933, 0.09874419567217003, 0.11613977059587, 0.27868647443656186, 0.17291018518830553, 0.07514551297178215, 0.08545274885393815, 0.08604179157500443, 0.2706016964229265, 0.19775712686816363, 0.06299488253886094, 0.08865990895379042, 0.12342454919738638, 0.260001268473167, 0.320311140078267, 0.10761703936239127, 0.08912585794412263, 0.09143594949565383, 0.21897333179020612, 0.06016810524949389, 0.13847018804869005, 0.1750960610267634, 0.053887774070770104, 0.11650615210780231, 0.07277771308035655, 0.07156429115936448, 0.09404459009931376, 0.08233573814998729, 0.06751430399577667, 0.26740421800094033, 0.09400229058569103, 0.13600609413463444, 0.057555763622167216, 0.060728620550529855, 0.11000518680142454, 0.09750783470042196, 0.15789805864924106, 0.18495481815098241, 0.12051652443617122, 0.15607797356946929, 0.05672018691998014, 0.06662289701784857, 0.2091506074290879, 0.14626495498306039, 0.15299339071701212, 0.04980064341466553, 0.1509181000850234, 0.1245038416221641, 0.07864563685061703, 0.08110272132603107, 0.14985000132600973, 0.10969218822434462, 0.12304167696105543]
0.10050584170009942
Making ranges
torch.Size([31878, 2])
We keep 7.55e+06/5.58e+08 =  1% of the original kernel matrix.

torch.Size([893, 2])
We keep 1.37e+04/1.32e+05 = 10% of the original kernel matrix.

torch.Size([7244, 2])
We keep 4.43e+05/8.60e+06 =  5% of the original kernel matrix.

torch.Size([12095, 2])
We keep 1.65e+06/4.47e+07 =  3% of the original kernel matrix.

torch.Size([19751, 2])
We keep 3.18e+06/1.58e+08 =  2% of the original kernel matrix.

torch.Size([5237, 2])
We keep 3.18e+05/7.16e+06 =  4% of the original kernel matrix.

torch.Size([14049, 2])
We keep 1.66e+06/6.32e+07 =  2% of the original kernel matrix.

torch.Size([2119, 2])
We keep 8.07e+04/7.45e+05 = 10% of the original kernel matrix.

torch.Size([9723, 2])
We keep 7.53e+05/2.04e+07 =  3% of the original kernel matrix.

torch.Size([26742, 2])
We keep 1.14e+07/3.86e+08 =  2% of the original kernel matrix.

torch.Size([30472, 2])
We keep 7.48e+06/4.64e+08 =  1% of the original kernel matrix.

torch.Size([19834, 2])
We keep 3.78e+06/1.58e+08 =  2% of the original kernel matrix.

torch.Size([25760, 2])
We keep 5.12e+06/2.97e+08 =  1% of the original kernel matrix.

torch.Size([55884, 2])
We keep 2.27e+07/1.64e+09 =  1% of the original kernel matrix.

torch.Size([43950, 2])
We keep 1.30e+07/9.58e+08 =  1% of the original kernel matrix.

torch.Size([33009, 2])
We keep 8.28e+06/5.53e+08 =  1% of the original kernel matrix.

torch.Size([34398, 2])
We keep 8.32e+06/5.55e+08 =  1% of the original kernel matrix.

torch.Size([6427, 2])
We keep 1.32e+06/1.34e+07 =  9% of the original kernel matrix.

torch.Size([15087, 2])
We keep 2.02e+06/8.65e+07 =  2% of the original kernel matrix.

torch.Size([9883, 2])
We keep 6.44e+06/9.52e+07 =  6% of the original kernel matrix.

torch.Size([18021, 2])
We keep 3.96e+06/2.30e+08 =  1% of the original kernel matrix.

torch.Size([7432, 2])
We keep 7.37e+05/1.50e+07 =  4% of the original kernel matrix.

torch.Size([16088, 2])
We keep 2.10e+06/9.16e+07 =  2% of the original kernel matrix.

torch.Size([71356, 2])
We keep 4.78e+07/2.94e+09 =  1% of the original kernel matrix.

torch.Size([48532, 2])
We keep 1.61e+07/1.28e+09 =  1% of the original kernel matrix.

torch.Size([6750, 2])
We keep 4.08e+05/9.98e+06 =  4% of the original kernel matrix.

torch.Size([15433, 2])
We keep 1.80e+06/7.46e+07 =  2% of the original kernel matrix.

torch.Size([150747, 2])
We keep 1.45e+08/1.18e+10 =  1% of the original kernel matrix.

torch.Size([71649, 2])
We keep 2.99e+07/2.57e+09 =  1% of the original kernel matrix.

torch.Size([22863, 2])
We keep 4.43e+06/2.32e+08 =  1% of the original kernel matrix.

torch.Size([27617, 2])
We keep 5.85e+06/3.60e+08 =  1% of the original kernel matrix.

torch.Size([27041, 2])
We keep 6.46e+06/3.39e+08 =  1% of the original kernel matrix.

torch.Size([30692, 2])
We keep 6.88e+06/4.35e+08 =  1% of the original kernel matrix.

torch.Size([41695, 2])
We keep 1.38e+07/9.06e+08 =  1% of the original kernel matrix.

torch.Size([38289, 2])
We keep 1.01e+07/7.11e+08 =  1% of the original kernel matrix.

torch.Size([61052, 2])
We keep 4.15e+07/2.14e+09 =  1% of the original kernel matrix.

torch.Size([45115, 2])
We keep 1.47e+07/1.09e+09 =  1% of the original kernel matrix.

torch.Size([156018, 2])
We keep 1.50e+08/1.33e+10 =  1% of the original kernel matrix.

torch.Size([72902, 2])
We keep 3.11e+07/2.72e+09 =  1% of the original kernel matrix.

torch.Size([121740, 2])
We keep 1.02e+08/8.16e+09 =  1% of the original kernel matrix.

torch.Size([63458, 2])
We keep 2.57e+07/2.13e+09 =  1% of the original kernel matrix.

torch.Size([5425, 2])
We keep 6.48e+05/9.37e+06 =  6% of the original kernel matrix.

torch.Size([13777, 2])
We keep 1.78e+06/7.23e+07 =  2% of the original kernel matrix.

torch.Size([166454, 2])
We keep 1.31e+08/1.38e+10 =  0% of the original kernel matrix.

torch.Size([75713, 2])
We keep 3.05e+07/2.77e+09 =  1% of the original kernel matrix.

torch.Size([16429, 2])
We keep 2.78e+06/9.50e+07 =  2% of the original kernel matrix.

torch.Size([23020, 2])
We keep 4.16e+06/2.30e+08 =  1% of the original kernel matrix.

torch.Size([10558, 2])
We keep 9.69e+05/3.15e+07 =  3% of the original kernel matrix.

torch.Size([18739, 2])
We keep 2.75e+06/1.33e+08 =  2% of the original kernel matrix.

torch.Size([26470, 2])
We keep 4.87e+07/1.06e+09 =  4% of the original kernel matrix.

torch.Size([26884, 2])
We keep 1.12e+07/7.70e+08 =  1% of the original kernel matrix.

torch.Size([67597, 2])
We keep 2.74e+07/2.25e+09 =  1% of the original kernel matrix.

torch.Size([47948, 2])
We keep 1.46e+07/1.12e+09 =  1% of the original kernel matrix.

torch.Size([38461, 2])
We keep 2.68e+07/1.10e+09 =  2% of the original kernel matrix.

torch.Size([35696, 2])
We keep 1.12e+07/7.84e+08 =  1% of the original kernel matrix.

torch.Size([10531, 2])
We keep 2.47e+06/3.90e+07 =  6% of the original kernel matrix.

torch.Size([18510, 2])
We keep 3.05e+06/1.48e+08 =  2% of the original kernel matrix.

torch.Size([37519, 2])
We keep 3.19e+07/1.04e+09 =  3% of the original kernel matrix.

torch.Size([35128, 2])
We keep 1.07e+07/7.64e+08 =  1% of the original kernel matrix.

torch.Size([1281094, 2])
We keep 3.84e+09/7.00e+11 =  0% of the original kernel matrix.

torch.Size([222853, 2])
We keep 1.83e+08/1.98e+10 =  0% of the original kernel matrix.

torch.Size([5972, 2])
We keep 5.19e+05/8.55e+06 =  6% of the original kernel matrix.

torch.Size([14686, 2])
We keep 1.72e+06/6.91e+07 =  2% of the original kernel matrix.

torch.Size([188818, 2])
We keep 6.15e+08/2.43e+10 =  2% of the original kernel matrix.

torch.Size([81245, 2])
We keep 4.06e+07/3.68e+09 =  1% of the original kernel matrix.

torch.Size([7582, 2])
We keep 8.08e+05/1.69e+07 =  4% of the original kernel matrix.

torch.Size([16082, 2])
We keep 2.20e+06/9.71e+07 =  2% of the original kernel matrix.

torch.Size([7492, 2])
We keep 4.64e+05/1.20e+07 =  3% of the original kernel matrix.

torch.Size([16199, 2])
We keep 1.93e+06/8.20e+07 =  2% of the original kernel matrix.

torch.Size([4864, 2])
We keep 3.91e+05/6.15e+06 =  6% of the original kernel matrix.

torch.Size([13349, 2])
We keep 1.55e+06/5.86e+07 =  2% of the original kernel matrix.

torch.Size([35782, 2])
We keep 4.41e+07/9.39e+08 =  4% of the original kernel matrix.

torch.Size([34812, 2])
We keep 1.02e+07/7.24e+08 =  1% of the original kernel matrix.

torch.Size([55292, 2])
We keep 2.30e+07/1.61e+09 =  1% of the original kernel matrix.

torch.Size([43979, 2])
We keep 1.29e+07/9.48e+08 =  1% of the original kernel matrix.

torch.Size([2722, 2])
We keep 9.39e+04/1.34e+06 =  7% of the original kernel matrix.

torch.Size([10802, 2])
We keep 9.28e+05/2.73e+07 =  3% of the original kernel matrix.

torch.Size([10079, 2])
We keep 8.61e+05/2.45e+07 =  3% of the original kernel matrix.

torch.Size([18164, 2])
We keep 2.50e+06/1.17e+08 =  2% of the original kernel matrix.

torch.Size([2120, 2])
We keep 6.59e+04/8.39e+05 =  7% of the original kernel matrix.

torch.Size([9790, 2])
We keep 7.83e+05/2.16e+07 =  3% of the original kernel matrix.

torch.Size([4745, 2])
We keep 2.76e+05/5.24e+06 =  5% of the original kernel matrix.

torch.Size([13351, 2])
We keep 1.47e+06/5.41e+07 =  2% of the original kernel matrix.

torch.Size([1117, 2])
We keep 4.82e+04/3.19e+05 = 15% of the original kernel matrix.

torch.Size([7197, 2])
We keep 5.38e+05/1.33e+07 =  4% of the original kernel matrix.

torch.Size([1438, 2])
We keep 3.27e+04/3.01e+05 = 10% of the original kernel matrix.

torch.Size([8346, 2])
We keep 5.18e+05/1.30e+07 =  3% of the original kernel matrix.

torch.Size([3881, 2])
We keep 1.46e+05/2.68e+06 =  5% of the original kernel matrix.

torch.Size([12355, 2])
We keep 1.14e+06/3.87e+07 =  2% of the original kernel matrix.

torch.Size([1054, 2])
We keep 1.35e+04/1.11e+05 = 12% of the original kernel matrix.

torch.Size([7954, 2])
We keep 4.18e+05/7.87e+06 =  5% of the original kernel matrix.

torch.Size([961, 2])
We keep 1.28e+04/1.12e+05 = 11% of the original kernel matrix.

torch.Size([7631, 2])
We keep 4.23e+05/7.89e+06 =  5% of the original kernel matrix.

torch.Size([6113, 2])
We keep 4.70e+05/9.26e+06 =  5% of the original kernel matrix.

torch.Size([14685, 2])
We keep 1.80e+06/7.19e+07 =  2% of the original kernel matrix.

torch.Size([5114, 2])
We keep 2.98e+05/5.82e+06 =  5% of the original kernel matrix.

torch.Size([13685, 2])
We keep 1.53e+06/5.70e+07 =  2% of the original kernel matrix.

torch.Size([1824, 2])
We keep 4.64e+04/6.19e+05 =  7% of the original kernel matrix.

torch.Size([9266, 2])
We keep 7.14e+05/1.86e+07 =  3% of the original kernel matrix.

torch.Size([4379, 2])
We keep 6.56e+05/5.37e+06 = 12% of the original kernel matrix.

torch.Size([12846, 2])
We keep 1.44e+06/5.47e+07 =  2% of the original kernel matrix.

torch.Size([7835, 2])
We keep 5.88e+05/1.50e+07 =  3% of the original kernel matrix.

torch.Size([16276, 2])
We keep 2.12e+06/9.16e+07 =  2% of the original kernel matrix.

torch.Size([4792, 2])
We keep 2.77e+05/4.66e+06 =  5% of the original kernel matrix.

torch.Size([13352, 2])
We keep 1.38e+06/5.10e+07 =  2% of the original kernel matrix.

torch.Size([3835, 2])
We keep 1.53e+05/2.79e+06 =  5% of the original kernel matrix.

torch.Size([12295, 2])
We keep 1.14e+06/3.95e+07 =  2% of the original kernel matrix.

torch.Size([2466, 2])
We keep 7.61e+04/1.12e+06 =  6% of the original kernel matrix.

torch.Size([10483, 2])
We keep 8.73e+05/2.50e+07 =  3% of the original kernel matrix.

torch.Size([11116, 2])
We keep 1.20e+06/3.39e+07 =  3% of the original kernel matrix.

torch.Size([19050, 2])
We keep 2.87e+06/1.38e+08 =  2% of the original kernel matrix.

torch.Size([3325, 2])
We keep 2.15e+05/2.51e+06 =  8% of the original kernel matrix.

torch.Size([11619, 2])
We keep 1.14e+06/3.74e+07 =  3% of the original kernel matrix.

torch.Size([2502, 2])
We keep 8.78e+04/1.30e+06 =  6% of the original kernel matrix.

torch.Size([10543, 2])
We keep 9.14e+05/2.70e+07 =  3% of the original kernel matrix.

torch.Size([6152, 2])
We keep 3.79e+05/8.56e+06 =  4% of the original kernel matrix.

torch.Size([14751, 2])
We keep 1.72e+06/6.91e+07 =  2% of the original kernel matrix.

torch.Size([2947, 2])
We keep 1.06e+05/1.75e+06 =  6% of the original kernel matrix.

torch.Size([11128, 2])
We keep 1.01e+06/3.13e+07 =  3% of the original kernel matrix.

torch.Size([2734, 2])
We keep 9.56e+04/1.41e+06 =  6% of the original kernel matrix.

torch.Size([10624, 2])
We keep 9.20e+05/2.81e+07 =  3% of the original kernel matrix.

torch.Size([7216, 2])
We keep 5.59e+05/1.29e+07 =  4% of the original kernel matrix.

torch.Size([15838, 2])
We keep 2.01e+06/8.50e+07 =  2% of the original kernel matrix.

torch.Size([1626, 2])
We keep 3.23e+04/3.87e+05 =  8% of the original kernel matrix.

torch.Size([9003, 2])
We keep 6.14e+05/1.47e+07 =  4% of the original kernel matrix.

torch.Size([12154, 2])
We keep 1.32e+06/4.19e+07 =  3% of the original kernel matrix.

torch.Size([19884, 2])
We keep 3.08e+06/1.53e+08 =  2% of the original kernel matrix.

torch.Size([3904, 2])
We keep 1.62e+05/2.87e+06 =  5% of the original kernel matrix.

torch.Size([12457, 2])
We keep 1.17e+06/4.00e+07 =  2% of the original kernel matrix.

torch.Size([1803, 2])
We keep 4.09e+04/5.60e+05 =  7% of the original kernel matrix.

torch.Size([9254, 2])
We keep 6.79e+05/1.77e+07 =  3% of the original kernel matrix.

torch.Size([3391, 2])
We keep 1.23e+05/2.18e+06 =  5% of the original kernel matrix.

torch.Size([11814, 2])
We keep 1.09e+06/3.49e+07 =  3% of the original kernel matrix.

torch.Size([2093, 2])
We keep 5.43e+04/7.24e+05 =  7% of the original kernel matrix.

torch.Size([9893, 2])
We keep 7.56e+05/2.01e+07 =  3% of the original kernel matrix.

torch.Size([3552, 2])
We keep 1.39e+05/2.50e+06 =  5% of the original kernel matrix.

torch.Size([11916, 2])
We keep 1.11e+06/3.74e+07 =  2% of the original kernel matrix.

torch.Size([3132, 2])
We keep 1.13e+05/1.85e+06 =  6% of the original kernel matrix.

torch.Size([11496, 2])
We keep 1.01e+06/3.21e+07 =  3% of the original kernel matrix.

torch.Size([2223, 2])
We keep 5.86e+04/7.57e+05 =  7% of the original kernel matrix.

torch.Size([10041, 2])
We keep 7.57e+05/2.06e+07 =  3% of the original kernel matrix.

torch.Size([2748, 2])
We keep 1.27e+05/1.73e+06 =  7% of the original kernel matrix.

torch.Size([10703, 2])
We keep 9.66e+05/3.11e+07 =  3% of the original kernel matrix.

torch.Size([5841, 2])
We keep 3.19e+05/7.14e+06 =  4% of the original kernel matrix.

torch.Size([14505, 2])
We keep 1.59e+06/6.32e+07 =  2% of the original kernel matrix.

torch.Size([2192, 2])
We keep 6.87e+04/8.99e+05 =  7% of the original kernel matrix.

torch.Size([10063, 2])
We keep 8.08e+05/2.24e+07 =  3% of the original kernel matrix.

torch.Size([1959, 2])
We keep 5.27e+04/6.38e+05 =  8% of the original kernel matrix.

torch.Size([9611, 2])
We keep 7.08e+05/1.89e+07 =  3% of the original kernel matrix.

torch.Size([2861, 2])
We keep 1.04e+05/1.70e+06 =  6% of the original kernel matrix.

torch.Size([10961, 2])
We keep 9.98e+05/3.08e+07 =  3% of the original kernel matrix.

torch.Size([5537, 2])
We keep 3.23e+05/7.15e+06 =  4% of the original kernel matrix.

torch.Size([14172, 2])
We keep 1.62e+06/6.32e+07 =  2% of the original kernel matrix.

torch.Size([2572, 2])
We keep 1.04e+05/1.38e+06 =  7% of the original kernel matrix.

torch.Size([10538, 2])
We keep 9.33e+05/2.78e+07 =  3% of the original kernel matrix.

torch.Size([1257, 2])
We keep 2.64e+04/2.55e+05 = 10% of the original kernel matrix.

torch.Size([8102, 2])
We keep 5.32e+05/1.19e+07 =  4% of the original kernel matrix.

torch.Size([3651, 2])
We keep 1.95e+05/2.84e+06 =  6% of the original kernel matrix.

torch.Size([12023, 2])
We keep 1.18e+06/3.98e+07 =  2% of the original kernel matrix.

torch.Size([7635, 2])
We keep 6.21e+05/1.60e+07 =  3% of the original kernel matrix.

torch.Size([16194, 2])
We keep 2.18e+06/9.45e+07 =  2% of the original kernel matrix.

torch.Size([3914, 2])
We keep 1.47e+05/2.66e+06 =  5% of the original kernel matrix.

torch.Size([12459, 2])
We keep 1.16e+06/3.85e+07 =  3% of the original kernel matrix.

torch.Size([1036, 2])
We keep 1.32e+04/9.99e+04 = 13% of the original kernel matrix.

torch.Size([7797, 2])
We keep 3.99e+05/7.47e+06 =  5% of the original kernel matrix.

torch.Size([8961, 2])
We keep 7.61e+05/2.00e+07 =  3% of the original kernel matrix.

torch.Size([17299, 2])
We keep 2.33e+06/1.06e+08 =  2% of the original kernel matrix.

torch.Size([844, 2])
We keep 9.77e+03/7.95e+04 = 12% of the original kernel matrix.

torch.Size([7170, 2])
We keep 3.74e+05/6.66e+06 =  5% of the original kernel matrix.

torch.Size([2528, 2])
We keep 7.33e+04/1.11e+06 =  6% of the original kernel matrix.

torch.Size([10511, 2])
We keep 8.55e+05/2.49e+07 =  3% of the original kernel matrix.

torch.Size([1499, 2])
We keep 2.82e+04/3.21e+05 =  8% of the original kernel matrix.

torch.Size([8807, 2])
We keep 5.79e+05/1.34e+07 =  4% of the original kernel matrix.

torch.Size([3081, 2])
We keep 9.65e+04/1.57e+06 =  6% of the original kernel matrix.

torch.Size([11459, 2])
We keep 9.68e+05/2.96e+07 =  3% of the original kernel matrix.

torch.Size([1266, 2])
We keep 2.39e+04/2.33e+05 = 10% of the original kernel matrix.

torch.Size([8146, 2])
We keep 5.17e+05/1.14e+07 =  4% of the original kernel matrix.

torch.Size([4820, 2])
We keep 2.50e+05/5.07e+06 =  4% of the original kernel matrix.

torch.Size([13394, 2])
We keep 1.45e+06/5.32e+07 =  2% of the original kernel matrix.

torch.Size([5235, 2])
We keep 2.95e+05/5.84e+06 =  5% of the original kernel matrix.

torch.Size([13858, 2])
We keep 1.52e+06/5.71e+07 =  2% of the original kernel matrix.

torch.Size([1683, 2])
We keep 3.66e+04/3.94e+05 =  9% of the original kernel matrix.

torch.Size([9079, 2])
We keep 6.24e+05/1.48e+07 =  4% of the original kernel matrix.

torch.Size([1609, 2])
We keep 3.25e+04/3.70e+05 =  8% of the original kernel matrix.

torch.Size([8933, 2])
We keep 6.04e+05/1.44e+07 =  4% of the original kernel matrix.

torch.Size([1400, 2])
We keep 1.89e+04/2.14e+05 =  8% of the original kernel matrix.

torch.Size([8635, 2])
We keep 4.95e+05/1.09e+07 =  4% of the original kernel matrix.

torch.Size([3899, 2])
We keep 1.73e+05/3.28e+06 =  5% of the original kernel matrix.

torch.Size([12392, 2])
We keep 1.23e+06/4.28e+07 =  2% of the original kernel matrix.

torch.Size([2852, 2])
We keep 8.82e+04/1.38e+06 =  6% of the original kernel matrix.

torch.Size([10904, 2])
We keep 9.25e+05/2.77e+07 =  3% of the original kernel matrix.

torch.Size([5425, 2])
We keep 3.32e+05/7.06e+06 =  4% of the original kernel matrix.

torch.Size([13998, 2])
We keep 1.63e+06/6.28e+07 =  2% of the original kernel matrix.

torch.Size([6149, 2])
We keep 3.47e+05/8.03e+06 =  4% of the original kernel matrix.

torch.Size([14852, 2])
We keep 1.68e+06/6.70e+07 =  2% of the original kernel matrix.

torch.Size([4189, 2])
We keep 3.43e+05/4.02e+06 =  8% of the original kernel matrix.

torch.Size([12710, 2])
We keep 1.32e+06/4.74e+07 =  2% of the original kernel matrix.

torch.Size([3120, 2])
We keep 1.11e+05/1.90e+06 =  5% of the original kernel matrix.

torch.Size([11339, 2])
We keep 1.04e+06/3.25e+07 =  3% of the original kernel matrix.

torch.Size([3903, 2])
We keep 1.41e+05/2.66e+06 =  5% of the original kernel matrix.

torch.Size([12455, 2])
We keep 1.15e+06/3.85e+07 =  2% of the original kernel matrix.

torch.Size([13446, 2])
We keep 1.82e+06/6.17e+07 =  2% of the original kernel matrix.

torch.Size([20783, 2])
We keep 3.56e+06/1.86e+08 =  1% of the original kernel matrix.

torch.Size([2465, 2])
We keep 6.49e+04/9.92e+05 =  6% of the original kernel matrix.

torch.Size([10555, 2])
We keep 8.15e+05/2.35e+07 =  3% of the original kernel matrix.

torch.Size([4457, 2])
We keep 1.95e+05/3.51e+06 =  5% of the original kernel matrix.

torch.Size([13196, 2])
We keep 1.26e+06/4.43e+07 =  2% of the original kernel matrix.

torch.Size([2266, 2])
We keep 6.27e+04/9.10e+05 =  6% of the original kernel matrix.

torch.Size([10151, 2])
We keep 8.19e+05/2.25e+07 =  3% of the original kernel matrix.

torch.Size([3566, 2])
We keep 1.53e+05/2.28e+06 =  6% of the original kernel matrix.

torch.Size([11911, 2])
We keep 1.04e+06/3.57e+07 =  2% of the original kernel matrix.

torch.Size([2686, 2])
We keep 7.49e+04/1.18e+06 =  6% of the original kernel matrix.

torch.Size([10726, 2])
We keep 8.61e+05/2.57e+07 =  3% of the original kernel matrix.

torch.Size([5932, 2])
We keep 3.46e+05/7.85e+06 =  4% of the original kernel matrix.

torch.Size([14707, 2])
We keep 1.68e+06/6.62e+07 =  2% of the original kernel matrix.

torch.Size([1131, 2])
We keep 1.76e+04/1.68e+05 = 10% of the original kernel matrix.

torch.Size([7821, 2])
We keep 4.63e+05/9.69e+06 =  4% of the original kernel matrix.

torch.Size([2856, 2])
We keep 8.91e+04/1.38e+06 =  6% of the original kernel matrix.

torch.Size([10893, 2])
We keep 9.02e+05/2.78e+07 =  3% of the original kernel matrix.

torch.Size([2098, 2])
We keep 6.43e+04/8.82e+05 =  7% of the original kernel matrix.

torch.Size([9631, 2])
We keep 7.93e+05/2.22e+07 =  3% of the original kernel matrix.

torch.Size([2453, 2])
We keep 1.05e+05/1.32e+06 =  7% of the original kernel matrix.

torch.Size([10277, 2])
We keep 9.26e+05/2.72e+07 =  3% of the original kernel matrix.

torch.Size([1656, 2])
We keep 3.39e+04/4.15e+05 =  8% of the original kernel matrix.

torch.Size([9041, 2])
We keep 6.18e+05/1.52e+07 =  4% of the original kernel matrix.

torch.Size([1163, 2])
We keep 2.29e+04/2.23e+05 = 10% of the original kernel matrix.

torch.Size([7843, 2])
We keep 5.17e+05/1.12e+07 =  4% of the original kernel matrix.

torch.Size([3743, 2])
We keep 1.75e+05/3.20e+06 =  5% of the original kernel matrix.

torch.Size([12233, 2])
We keep 1.24e+06/4.23e+07 =  2% of the original kernel matrix.

torch.Size([4323, 2])
We keep 2.29e+05/3.94e+06 =  5% of the original kernel matrix.

torch.Size([12642, 2])
We keep 1.30e+06/4.69e+07 =  2% of the original kernel matrix.

torch.Size([1328, 2])
We keep 2.34e+04/2.05e+05 = 11% of the original kernel matrix.

torch.Size([8162, 2])
We keep 4.84e+05/1.07e+07 =  4% of the original kernel matrix.

torch.Size([1823, 2])
We keep 4.54e+04/5.93e+05 =  7% of the original kernel matrix.

torch.Size([9429, 2])
We keep 7.03e+05/1.82e+07 =  3% of the original kernel matrix.

torch.Size([1507, 2])
We keep 2.67e+04/3.18e+05 =  8% of the original kernel matrix.

torch.Size([8758, 2])
We keep 5.73e+05/1.33e+07 =  4% of the original kernel matrix.

torch.Size([5485, 2])
We keep 3.76e+05/7.31e+06 =  5% of the original kernel matrix.

torch.Size([14111, 2])
We keep 1.65e+06/6.39e+07 =  2% of the original kernel matrix.

torch.Size([1654, 2])
We keep 3.25e+04/3.92e+05 =  8% of the original kernel matrix.

torch.Size([9087, 2])
We keep 6.17e+05/1.48e+07 =  4% of the original kernel matrix.

torch.Size([10861, 2])
We keep 1.04e+06/3.13e+07 =  3% of the original kernel matrix.

torch.Size([18615, 2])
We keep 2.75e+06/1.32e+08 =  2% of the original kernel matrix.

torch.Size([2921, 2])
We keep 1.00e+05/1.59e+06 =  6% of the original kernel matrix.

torch.Size([10997, 2])
We keep 9.83e+05/2.98e+07 =  3% of the original kernel matrix.

torch.Size([1516, 2])
We keep 3.00e+04/3.65e+05 =  8% of the original kernel matrix.

torch.Size([8642, 2])
We keep 5.88e+05/1.43e+07 =  4% of the original kernel matrix.

torch.Size([3806, 2])
We keep 1.56e+05/2.79e+06 =  5% of the original kernel matrix.

torch.Size([12252, 2])
We keep 1.18e+06/3.95e+07 =  2% of the original kernel matrix.

torch.Size([2758, 2])
We keep 8.52e+04/1.41e+06 =  6% of the original kernel matrix.

torch.Size([10952, 2])
We keep 9.28e+05/2.80e+07 =  3% of the original kernel matrix.

torch.Size([1655, 2])
We keep 3.19e+04/3.93e+05 =  8% of the original kernel matrix.

torch.Size([9174, 2])
We keep 6.07e+05/1.48e+07 =  4% of the original kernel matrix.

torch.Size([3211, 2])
We keep 1.28e+05/2.07e+06 =  6% of the original kernel matrix.

torch.Size([11484, 2])
We keep 1.09e+06/3.40e+07 =  3% of the original kernel matrix.

torch.Size([2021, 2])
We keep 4.36e+04/5.90e+05 =  7% of the original kernel matrix.

torch.Size([9779, 2])
We keep 6.88e+05/1.81e+07 =  3% of the original kernel matrix.

torch.Size([20696, 2])
We keep 3.83e+06/1.76e+08 =  2% of the original kernel matrix.

torch.Size([26410, 2])
We keep 5.30e+06/3.14e+08 =  1% of the original kernel matrix.

torch.Size([3612, 2])
We keep 1.59e+05/2.65e+06 =  5% of the original kernel matrix.

torch.Size([12017, 2])
We keep 1.16e+06/3.85e+07 =  3% of the original kernel matrix.

torch.Size([5409, 2])
We keep 3.52e+05/7.06e+06 =  4% of the original kernel matrix.

torch.Size([13894, 2])
We keep 1.62e+06/6.28e+07 =  2% of the original kernel matrix.

torch.Size([1430, 2])
We keep 2.35e+04/2.64e+05 =  8% of the original kernel matrix.

torch.Size([8770, 2])
We keep 5.32e+05/1.21e+07 =  4% of the original kernel matrix.

torch.Size([3195, 2])
We keep 1.16e+05/1.93e+06 =  5% of the original kernel matrix.

torch.Size([11488, 2])
We keep 1.04e+06/3.29e+07 =  3% of the original kernel matrix.

torch.Size([1269, 2])
We keep 2.72e+04/2.35e+05 = 11% of the original kernel matrix.

torch.Size([8143, 2])
We keep 5.10e+05/1.15e+07 =  4% of the original kernel matrix.

torch.Size([3243, 2])
We keep 1.31e+05/1.97e+06 =  6% of the original kernel matrix.

torch.Size([11612, 2])
We keep 1.06e+06/3.32e+07 =  3% of the original kernel matrix.

torch.Size([2228, 2])
We keep 6.12e+04/8.46e+05 =  7% of the original kernel matrix.

torch.Size([10109, 2])
We keep 7.98e+05/2.17e+07 =  3% of the original kernel matrix.

torch.Size([1131, 2])
We keep 1.71e+04/1.59e+05 = 10% of the original kernel matrix.

torch.Size([7968, 2])
We keep 4.62e+05/9.43e+06 =  4% of the original kernel matrix.

torch.Size([2162, 2])
We keep 6.23e+04/8.82e+05 =  7% of the original kernel matrix.

torch.Size([10002, 2])
We keep 7.94e+05/2.22e+07 =  3% of the original kernel matrix.

torch.Size([1810, 2])
We keep 4.29e+04/4.82e+05 =  8% of the original kernel matrix.

torch.Size([9266, 2])
We keep 6.69e+05/1.64e+07 =  4% of the original kernel matrix.

torch.Size([2629, 2])
We keep 1.19e+05/1.57e+06 =  7% of the original kernel matrix.

torch.Size([10490, 2])
We keep 9.94e+05/2.96e+07 =  3% of the original kernel matrix.

torch.Size([4242, 2])
We keep 2.18e+05/4.15e+06 =  5% of the original kernel matrix.

torch.Size([12756, 2])
We keep 1.37e+06/4.82e+07 =  2% of the original kernel matrix.

torch.Size([1463, 2])
We keep 2.75e+04/3.01e+05 =  9% of the original kernel matrix.

torch.Size([8700, 2])
We keep 5.60e+05/1.30e+07 =  4% of the original kernel matrix.

torch.Size([8029, 2])
We keep 6.38e+05/1.63e+07 =  3% of the original kernel matrix.

torch.Size([16452, 2])
We keep 2.19e+06/9.54e+07 =  2% of the original kernel matrix.

torch.Size([10210, 2])
We keep 9.50e+05/2.70e+07 =  3% of the original kernel matrix.

torch.Size([18310, 2])
We keep 2.59e+06/1.23e+08 =  2% of the original kernel matrix.

torch.Size([9796, 2])
We keep 8.79e+05/2.50e+07 =  3% of the original kernel matrix.

torch.Size([17876, 2])
We keep 2.55e+06/1.18e+08 =  2% of the original kernel matrix.

torch.Size([1679, 2])
We keep 3.67e+04/4.56e+05 =  8% of the original kernel matrix.

torch.Size([9079, 2])
We keep 6.55e+05/1.59e+07 =  4% of the original kernel matrix.

torch.Size([2315, 2])
We keep 5.36e+04/8.28e+05 =  6% of the original kernel matrix.

torch.Size([10383, 2])
We keep 7.75e+05/2.15e+07 =  3% of the original kernel matrix.

torch.Size([2538, 2])
We keep 8.45e+04/1.19e+06 =  7% of the original kernel matrix.

torch.Size([10489, 2])
We keep 8.57e+05/2.58e+07 =  3% of the original kernel matrix.

torch.Size([1555, 2])
We keep 3.05e+04/3.70e+05 =  8% of the original kernel matrix.

torch.Size([8698, 2])
We keep 5.94e+05/1.44e+07 =  4% of the original kernel matrix.

torch.Size([2155, 2])
We keep 5.64e+04/7.59e+05 =  7% of the original kernel matrix.

torch.Size([9962, 2])
We keep 7.75e+05/2.06e+07 =  3% of the original kernel matrix.

torch.Size([4406, 2])
We keep 2.38e+05/3.94e+06 =  6% of the original kernel matrix.

torch.Size([12808, 2])
We keep 1.31e+06/4.69e+07 =  2% of the original kernel matrix.

torch.Size([8302, 2])
We keep 7.40e+05/1.93e+07 =  3% of the original kernel matrix.

torch.Size([16746, 2])
We keep 2.36e+06/1.04e+08 =  2% of the original kernel matrix.

torch.Size([5633, 2])
We keep 2.90e+05/6.68e+06 =  4% of the original kernel matrix.

torch.Size([14442, 2])
We keep 1.57e+06/6.11e+07 =  2% of the original kernel matrix.

torch.Size([3535, 2])
We keep 5.25e+05/3.35e+06 = 15% of the original kernel matrix.

torch.Size([11633, 2])
We keep 1.25e+06/4.32e+07 =  2% of the original kernel matrix.

torch.Size([2564, 2])
We keep 7.64e+04/1.19e+06 =  6% of the original kernel matrix.

torch.Size([10678, 2])
We keep 8.82e+05/2.58e+07 =  3% of the original kernel matrix.

torch.Size([11447, 2])
We keep 1.28e+06/3.68e+07 =  3% of the original kernel matrix.

torch.Size([19321, 2])
We keep 2.89e+06/1.43e+08 =  2% of the original kernel matrix.

torch.Size([2265, 2])
We keep 6.10e+04/9.14e+05 =  6% of the original kernel matrix.

torch.Size([10234, 2])
We keep 8.16e+05/2.26e+07 =  3% of the original kernel matrix.

torch.Size([1610, 2])
We keep 3.98e+04/4.61e+05 =  8% of the original kernel matrix.

torch.Size([8867, 2])
We keep 6.51e+05/1.60e+07 =  4% of the original kernel matrix.

torch.Size([3702, 2])
We keep 1.33e+05/2.48e+06 =  5% of the original kernel matrix.

torch.Size([12149, 2])
We keep 1.10e+06/3.72e+07 =  2% of the original kernel matrix.

torch.Size([675, 2])
We keep 1.03e+04/7.62e+04 = 13% of the original kernel matrix.

torch.Size([6489, 2])
We keep 3.75e+05/6.52e+06 =  5% of the original kernel matrix.

torch.Size([3211, 2])
We keep 1.40e+05/2.01e+06 =  6% of the original kernel matrix.

torch.Size([11610, 2])
We keep 1.07e+06/3.35e+07 =  3% of the original kernel matrix.

torch.Size([1331, 2])
We keep 2.77e+04/2.66e+05 = 10% of the original kernel matrix.

torch.Size([8307, 2])
We keep 5.51e+05/1.22e+07 =  4% of the original kernel matrix.

torch.Size([4783, 2])
We keep 2.51e+05/4.98e+06 =  5% of the original kernel matrix.

torch.Size([13354, 2])
We keep 1.42e+06/5.27e+07 =  2% of the original kernel matrix.

torch.Size([1498, 2])
We keep 2.75e+04/2.98e+05 =  9% of the original kernel matrix.

torch.Size([8843, 2])
We keep 5.74e+05/1.29e+07 =  4% of the original kernel matrix.

torch.Size([2434, 2])
We keep 7.45e+04/1.03e+06 =  7% of the original kernel matrix.

torch.Size([10086, 2])
We keep 8.15e+05/2.40e+07 =  3% of the original kernel matrix.

torch.Size([2186, 2])
We keep 4.86e+04/7.04e+05 =  6% of the original kernel matrix.

torch.Size([10011, 2])
We keep 7.27e+05/1.98e+07 =  3% of the original kernel matrix.

torch.Size([2571, 2])
We keep 7.88e+04/1.23e+06 =  6% of the original kernel matrix.

torch.Size([10769, 2])
We keep 8.78e+05/2.62e+07 =  3% of the original kernel matrix.

torch.Size([2290, 2])
We keep 6.34e+04/8.89e+05 =  7% of the original kernel matrix.

torch.Size([10082, 2])
We keep 8.07e+05/2.23e+07 =  3% of the original kernel matrix.

torch.Size([3999, 2])
We keep 1.58e+05/2.97e+06 =  5% of the original kernel matrix.

torch.Size([12601, 2])
We keep 1.20e+06/4.07e+07 =  2% of the original kernel matrix.

torch.Size([823, 2])
We keep 1.05e+04/7.90e+04 = 13% of the original kernel matrix.

torch.Size([7187, 2])
We keep 3.85e+05/6.64e+06 =  5% of the original kernel matrix.

torch.Size([1774, 2])
We keep 5.01e+04/5.46e+05 =  9% of the original kernel matrix.

torch.Size([9111, 2])
We keep 6.94e+05/1.75e+07 =  3% of the original kernel matrix.

torch.Size([1845, 2])
We keep 4.13e+04/5.43e+05 =  7% of the original kernel matrix.

torch.Size([9061, 2])
We keep 6.56e+05/1.74e+07 =  3% of the original kernel matrix.

torch.Size([4819, 2])
We keep 2.37e+05/5.04e+06 =  4% of the original kernel matrix.

torch.Size([13470, 2])
We keep 1.43e+06/5.30e+07 =  2% of the original kernel matrix.

torch.Size([1786, 2])
We keep 3.47e+04/4.36e+05 =  7% of the original kernel matrix.

torch.Size([9287, 2])
We keep 6.16e+05/1.56e+07 =  3% of the original kernel matrix.

torch.Size([3985, 2])
We keep 1.73e+05/3.42e+06 =  5% of the original kernel matrix.

torch.Size([12553, 2])
We keep 1.24e+06/4.37e+07 =  2% of the original kernel matrix.

torch.Size([2991, 2])
We keep 1.09e+05/1.84e+06 =  5% of the original kernel matrix.

torch.Size([11325, 2])
We keep 1.04e+06/3.20e+07 =  3% of the original kernel matrix.

torch.Size([3731, 2])
We keep 1.78e+05/3.19e+06 =  5% of the original kernel matrix.

torch.Size([12058, 2])
We keep 1.23e+06/4.22e+07 =  2% of the original kernel matrix.

torch.Size([5698, 2])
We keep 3.45e+05/7.14e+06 =  4% of the original kernel matrix.

torch.Size([14290, 2])
We keep 1.64e+06/6.32e+07 =  2% of the original kernel matrix.

torch.Size([3523, 2])
We keep 2.54e+05/3.05e+06 =  8% of the original kernel matrix.

torch.Size([11654, 2])
We keep 1.20e+06/4.12e+07 =  2% of the original kernel matrix.

torch.Size([2105, 2])
We keep 7.24e+04/9.14e+05 =  7% of the original kernel matrix.

torch.Size([9714, 2])
We keep 7.85e+05/2.26e+07 =  3% of the original kernel matrix.

torch.Size([3930, 2])
We keep 1.51e+05/2.68e+06 =  5% of the original kernel matrix.

torch.Size([12522, 2])
We keep 1.15e+06/3.87e+07 =  2% of the original kernel matrix.

torch.Size([2996, 2])
We keep 1.01e+05/1.68e+06 =  5% of the original kernel matrix.

torch.Size([11261, 2])
We keep 9.88e+05/3.06e+07 =  3% of the original kernel matrix.

torch.Size([1740, 2])
We keep 5.16e+04/4.83e+05 = 10% of the original kernel matrix.

torch.Size([8960, 2])
We keep 6.29e+05/1.64e+07 =  3% of the original kernel matrix.

torch.Size([2512, 2])
We keep 7.45e+04/1.06e+06 =  7% of the original kernel matrix.

torch.Size([10596, 2])
We keep 8.39e+05/2.43e+07 =  3% of the original kernel matrix.

torch.Size([10499, 2])
We keep 9.23e+05/2.76e+07 =  3% of the original kernel matrix.

torch.Size([18572, 2])
We keep 2.64e+06/1.24e+08 =  2% of the original kernel matrix.

torch.Size([4689, 2])
We keep 2.45e+05/5.01e+06 =  4% of the original kernel matrix.

torch.Size([13279, 2])
We keep 1.45e+06/5.29e+07 =  2% of the original kernel matrix.

torch.Size([3422, 2])
We keep 1.11e+05/1.97e+06 =  5% of the original kernel matrix.

torch.Size([11855, 2])
We keep 1.04e+06/3.32e+07 =  3% of the original kernel matrix.

torch.Size([3272, 2])
We keep 1.32e+05/2.08e+06 =  6% of the original kernel matrix.

torch.Size([11564, 2])
We keep 1.07e+06/3.40e+07 =  3% of the original kernel matrix.

torch.Size([10413, 2])
We keep 1.01e+06/2.73e+07 =  3% of the original kernel matrix.

torch.Size([18559, 2])
We keep 2.62e+06/1.23e+08 =  2% of the original kernel matrix.

torch.Size([10782, 2])
We keep 1.14e+06/3.26e+07 =  3% of the original kernel matrix.

torch.Size([18671, 2])
We keep 2.82e+06/1.35e+08 =  2% of the original kernel matrix.

torch.Size([1444, 2])
We keep 2.54e+04/3.04e+05 =  8% of the original kernel matrix.

torch.Size([8452, 2])
We keep 5.56e+05/1.30e+07 =  4% of the original kernel matrix.

torch.Size([807, 2])
We keep 9.07e+03/7.73e+04 = 11% of the original kernel matrix.

torch.Size([6896, 2])
We keep 3.57e+05/6.57e+06 =  5% of the original kernel matrix.

torch.Size([2250, 2])
We keep 6.45e+04/9.18e+05 =  7% of the original kernel matrix.

torch.Size([10047, 2])
We keep 8.13e+05/2.26e+07 =  3% of the original kernel matrix.

torch.Size([1464, 2])
We keep 2.33e+04/2.52e+05 =  9% of the original kernel matrix.

torch.Size([8748, 2])
We keep 5.29e+05/1.19e+07 =  4% of the original kernel matrix.

torch.Size([1715, 2])
We keep 7.44e+04/5.72e+05 = 13% of the original kernel matrix.

torch.Size([8827, 2])
We keep 6.78e+05/1.79e+07 =  3% of the original kernel matrix.

torch.Size([4437, 2])
We keep 1.89e+05/3.68e+06 =  5% of the original kernel matrix.

torch.Size([13110, 2])
We keep 1.29e+06/4.53e+07 =  2% of the original kernel matrix.

torch.Size([2444, 2])
We keep 7.69e+04/1.10e+06 =  6% of the original kernel matrix.

torch.Size([10378, 2])
We keep 8.55e+05/2.48e+07 =  3% of the original kernel matrix.

torch.Size([1152, 2])
We keep 1.96e+04/1.95e+05 = 10% of the original kernel matrix.

torch.Size([8108, 2])
We keep 4.93e+05/1.04e+07 =  4% of the original kernel matrix.

torch.Size([3137, 2])
We keep 9.64e+04/1.64e+06 =  5% of the original kernel matrix.

torch.Size([11509, 2])
We keep 9.66e+05/3.03e+07 =  3% of the original kernel matrix.

torch.Size([1468, 2])
We keep 2.28e+04/2.70e+05 =  8% of the original kernel matrix.

torch.Size([8866, 2])
We keep 5.29e+05/1.23e+07 =  4% of the original kernel matrix.

torch.Size([5147, 2])
We keep 2.54e+05/5.67e+06 =  4% of the original kernel matrix.

torch.Size([13943, 2])
We keep 1.49e+06/5.63e+07 =  2% of the original kernel matrix.

torch.Size([2757, 2])
We keep 8.46e+04/1.37e+06 =  6% of the original kernel matrix.

torch.Size([10909, 2])
We keep 9.28e+05/2.76e+07 =  3% of the original kernel matrix.

torch.Size([6742, 2])
We keep 4.53e+05/1.12e+07 =  4% of the original kernel matrix.

torch.Size([15294, 2])
We keep 1.90e+06/7.89e+07 =  2% of the original kernel matrix.

torch.Size([7824, 2])
We keep 6.34e+05/1.58e+07 =  4% of the original kernel matrix.

torch.Size([16425, 2])
We keep 2.15e+06/9.38e+07 =  2% of the original kernel matrix.

torch.Size([3501, 2])
We keep 1.44e+05/2.50e+06 =  5% of the original kernel matrix.

torch.Size([11859, 2])
We keep 1.13e+06/3.74e+07 =  3% of the original kernel matrix.

torch.Size([882, 2])
We keep 9.46e+03/7.29e+04 = 12% of the original kernel matrix.

torch.Size([7138, 2])
We keep 3.55e+05/6.38e+06 =  5% of the original kernel matrix.

torch.Size([6329, 2])
We keep 4.35e+05/1.02e+07 =  4% of the original kernel matrix.

torch.Size([15110, 2])
We keep 1.88e+06/7.54e+07 =  2% of the original kernel matrix.

torch.Size([3817, 2])
We keep 1.54e+05/2.68e+06 =  5% of the original kernel matrix.

torch.Size([12215, 2])
We keep 1.16e+06/3.87e+07 =  3% of the original kernel matrix.

torch.Size([1879, 2])
We keep 4.88e+04/6.54e+05 =  7% of the original kernel matrix.

torch.Size([9424, 2])
We keep 7.26e+05/1.91e+07 =  3% of the original kernel matrix.

torch.Size([11384, 2])
We keep 1.28e+06/3.94e+07 =  3% of the original kernel matrix.

torch.Size([19292, 2])
We keep 3.00e+06/1.48e+08 =  2% of the original kernel matrix.

torch.Size([8118, 2])
We keep 6.69e+05/1.69e+07 =  3% of the original kernel matrix.

torch.Size([16542, 2])
We keep 2.22e+06/9.72e+07 =  2% of the original kernel matrix.

torch.Size([1781, 2])
We keep 6.03e+04/6.24e+05 =  9% of the original kernel matrix.

torch.Size([9073, 2])
We keep 7.07e+05/1.87e+07 =  3% of the original kernel matrix.

torch.Size([1572, 2])
We keep 3.47e+04/4.04e+05 =  8% of the original kernel matrix.

torch.Size([8810, 2])
We keep 6.26e+05/1.50e+07 =  4% of the original kernel matrix.

torch.Size([1200, 2])
We keep 1.93e+04/1.94e+05 =  9% of the original kernel matrix.

torch.Size([8173, 2])
We keep 4.98e+05/1.04e+07 =  4% of the original kernel matrix.

torch.Size([1235, 2])
We keep 1.55e+04/1.61e+05 =  9% of the original kernel matrix.

torch.Size([8355, 2])
We keep 4.55e+05/9.47e+06 =  4% of the original kernel matrix.

torch.Size([986, 2])
We keep 1.21e+04/9.80e+04 = 12% of the original kernel matrix.

torch.Size([7603, 2])
We keep 3.90e+05/7.40e+06 =  5% of the original kernel matrix.

torch.Size([1008, 2])
We keep 1.49e+04/1.42e+05 = 10% of the original kernel matrix.

torch.Size([7663, 2])
We keep 4.32e+05/8.91e+06 =  4% of the original kernel matrix.

torch.Size([818, 2])
We keep 1.04e+04/8.70e+04 = 11% of the original kernel matrix.

torch.Size([6947, 2])
We keep 3.72e+05/6.97e+06 =  5% of the original kernel matrix.

torch.Size([4883, 2])
We keep 2.65e+05/5.40e+06 =  4% of the original kernel matrix.

torch.Size([13466, 2])
We keep 1.45e+06/5.49e+07 =  2% of the original kernel matrix.

torch.Size([2458, 2])
We keep 7.10e+04/1.02e+06 =  6% of the original kernel matrix.

torch.Size([10416, 2])
We keep 8.31e+05/2.39e+07 =  3% of the original kernel matrix.

torch.Size([1193, 2])
We keep 2.00e+04/1.73e+05 = 11% of the original kernel matrix.

torch.Size([8180, 2])
We keep 4.64e+05/9.83e+06 =  4% of the original kernel matrix.

torch.Size([2055, 2])
We keep 6.14e+04/8.06e+05 =  7% of the original kernel matrix.

torch.Size([9699, 2])
We keep 7.68e+05/2.12e+07 =  3% of the original kernel matrix.

torch.Size([2984, 2])
We keep 1.14e+05/1.90e+06 =  5% of the original kernel matrix.

torch.Size([11157, 2])
We keep 1.03e+06/3.25e+07 =  3% of the original kernel matrix.

torch.Size([1745, 2])
We keep 4.89e+04/4.65e+05 = 10% of the original kernel matrix.

torch.Size([9125, 2])
We keep 6.43e+05/1.61e+07 =  3% of the original kernel matrix.

torch.Size([6101, 2])
We keep 4.28e+05/9.38e+06 =  4% of the original kernel matrix.

torch.Size([14657, 2])
We keep 1.78e+06/7.24e+07 =  2% of the original kernel matrix.

torch.Size([3787, 2])
We keep 1.54e+05/2.84e+06 =  5% of the original kernel matrix.

torch.Size([12248, 2])
We keep 1.19e+06/3.98e+07 =  2% of the original kernel matrix.

torch.Size([1990, 2])
We keep 4.77e+04/6.24e+05 =  7% of the original kernel matrix.

torch.Size([9519, 2])
We keep 7.14e+05/1.87e+07 =  3% of the original kernel matrix.

torch.Size([624, 2])
We keep 7.50e+03/5.02e+04 = 14% of the original kernel matrix.

torch.Size([6206, 2])
We keep 3.20e+05/5.29e+06 =  6% of the original kernel matrix.

torch.Size([7718, 2])
We keep 5.54e+05/1.39e+07 =  3% of the original kernel matrix.

torch.Size([16196, 2])
We keep 2.05e+06/8.81e+07 =  2% of the original kernel matrix.

torch.Size([967, 2])
We keep 1.32e+04/1.06e+05 = 12% of the original kernel matrix.

torch.Size([7606, 2])
We keep 4.10e+05/7.70e+06 =  5% of the original kernel matrix.

torch.Size([1661, 2])
We keep 3.22e+04/3.88e+05 =  8% of the original kernel matrix.

torch.Size([9158, 2])
We keep 6.17e+05/1.47e+07 =  4% of the original kernel matrix.

torch.Size([2338, 2])
We keep 6.91e+04/9.51e+05 =  7% of the original kernel matrix.

torch.Size([10068, 2])
We keep 8.07e+05/2.30e+07 =  3% of the original kernel matrix.

torch.Size([2788, 2])
We keep 1.00e+05/1.46e+06 =  6% of the original kernel matrix.

torch.Size([10844, 2])
We keep 9.56e+05/2.86e+07 =  3% of the original kernel matrix.

torch.Size([8581, 2])
We keep 7.67e+05/1.93e+07 =  3% of the original kernel matrix.

torch.Size([16980, 2])
We keep 2.32e+06/1.04e+08 =  2% of the original kernel matrix.

torch.Size([3377, 2])
We keep 1.39e+05/2.32e+06 =  5% of the original kernel matrix.

torch.Size([11716, 2])
We keep 1.12e+06/3.60e+07 =  3% of the original kernel matrix.

torch.Size([1019, 2])
We keep 1.76e+04/1.58e+05 = 11% of the original kernel matrix.

torch.Size([7575, 2])
We keep 4.59e+05/9.40e+06 =  4% of the original kernel matrix.

torch.Size([1923, 2])
We keep 4.54e+04/5.49e+05 =  8% of the original kernel matrix.

torch.Size([9546, 2])
We keep 6.93e+05/1.75e+07 =  3% of the original kernel matrix.

torch.Size([6764, 2])
We keep 4.47e+05/1.10e+07 =  4% of the original kernel matrix.

torch.Size([15585, 2])
We keep 1.84e+06/7.84e+07 =  2% of the original kernel matrix.

torch.Size([1057, 2])
We keep 2.17e+04/1.81e+05 = 12% of the original kernel matrix.

torch.Size([7610, 2])
We keep 4.78e+05/1.00e+07 =  4% of the original kernel matrix.

torch.Size([7116, 2])
We keep 4.80e+05/1.28e+07 =  3% of the original kernel matrix.

torch.Size([15913, 2])
We keep 1.98e+06/8.44e+07 =  2% of the original kernel matrix.

torch.Size([13760, 2])
We keep 8.82e+06/1.34e+08 =  6% of the original kernel matrix.

torch.Size([20907, 2])
We keep 4.85e+06/2.73e+08 =  1% of the original kernel matrix.

torch.Size([4795, 2])
We keep 2.74e+05/5.18e+06 =  5% of the original kernel matrix.

torch.Size([13511, 2])
We keep 1.36e+06/5.38e+07 =  2% of the original kernel matrix.

torch.Size([5373, 2])
We keep 3.04e+05/6.53e+06 =  4% of the original kernel matrix.

torch.Size([14040, 2])
We keep 1.56e+06/6.04e+07 =  2% of the original kernel matrix.

torch.Size([978, 2])
We keep 2.67e+04/1.77e+05 = 15% of the original kernel matrix.

torch.Size([7056, 2])
We keep 4.80e+05/9.95e+06 =  4% of the original kernel matrix.

torch.Size([7433, 2])
We keep 5.23e+05/1.28e+07 =  4% of the original kernel matrix.

torch.Size([16019, 2])
We keep 2.01e+06/8.47e+07 =  2% of the original kernel matrix.

torch.Size([22516, 2])
We keep 7.13e+06/2.30e+08 =  3% of the original kernel matrix.

torch.Size([27560, 2])
We keep 5.95e+06/3.58e+08 =  1% of the original kernel matrix.

torch.Size([43483, 2])
We keep 7.81e+07/1.75e+09 =  4% of the original kernel matrix.

torch.Size([37353, 2])
We keep 1.32e+07/9.89e+08 =  1% of the original kernel matrix.

torch.Size([3882, 2])
We keep 1.43e+05/2.78e+06 =  5% of the original kernel matrix.

torch.Size([12558, 2])
We keep 1.17e+06/3.94e+07 =  2% of the original kernel matrix.

torch.Size([26981, 2])
We keep 1.21e+07/3.99e+08 =  3% of the original kernel matrix.

torch.Size([30380, 2])
We keep 7.30e+06/4.72e+08 =  1% of the original kernel matrix.

torch.Size([60217, 2])
We keep 4.33e+07/2.29e+09 =  1% of the original kernel matrix.

torch.Size([44287, 2])
We keep 1.48e+07/1.13e+09 =  1% of the original kernel matrix.

torch.Size([9181, 2])
We keep 1.22e+06/2.59e+07 =  4% of the original kernel matrix.

torch.Size([17381, 2])
We keep 2.61e+06/1.20e+08 =  2% of the original kernel matrix.

torch.Size([236886, 2])
We keep 1.02e+09/5.01e+10 =  2% of the original kernel matrix.

torch.Size([91017, 2])
We keep 5.67e+07/5.29e+09 =  1% of the original kernel matrix.

torch.Size([2013, 2])
We keep 5.09e+04/6.35e+05 =  8% of the original kernel matrix.

torch.Size([9579, 2])
We keep 7.01e+05/1.88e+07 =  3% of the original kernel matrix.

torch.Size([8740, 2])
We keep 2.62e+06/2.90e+07 =  9% of the original kernel matrix.

torch.Size([17034, 2])
We keep 2.75e+06/1.27e+08 =  2% of the original kernel matrix.

torch.Size([42101, 2])
We keep 5.03e+07/1.49e+09 =  3% of the original kernel matrix.

torch.Size([36726, 2])
We keep 1.23e+07/9.13e+08 =  1% of the original kernel matrix.

torch.Size([298670, 2])
We keep 4.95e+08/4.99e+10 =  0% of the original kernel matrix.

torch.Size([104414, 2])
We keep 5.47e+07/5.28e+09 =  1% of the original kernel matrix.

torch.Size([2982, 2])
We keep 9.18e+04/1.57e+06 =  5% of the original kernel matrix.

torch.Size([11427, 2])
We keep 9.60e+05/2.96e+07 =  3% of the original kernel matrix.

torch.Size([20309, 2])
We keep 3.92e+06/1.67e+08 =  2% of the original kernel matrix.

torch.Size([25991, 2])
We keep 5.19e+06/3.06e+08 =  1% of the original kernel matrix.

torch.Size([14232, 2])
We keep 5.27e+06/9.44e+07 =  5% of the original kernel matrix.

torch.Size([21671, 2])
We keep 4.27e+06/2.30e+08 =  1% of the original kernel matrix.

torch.Size([75355, 2])
We keep 1.67e+08/5.14e+09 =  3% of the original kernel matrix.

torch.Size([48676, 2])
We keep 2.13e+07/1.69e+09 =  1% of the original kernel matrix.

torch.Size([30231, 2])
We keep 1.65e+07/6.31e+08 =  2% of the original kernel matrix.

torch.Size([31652, 2])
We keep 8.63e+06/5.93e+08 =  1% of the original kernel matrix.

torch.Size([24403, 2])
We keep 1.12e+07/4.14e+08 =  2% of the original kernel matrix.

torch.Size([28528, 2])
We keep 7.59e+06/4.81e+08 =  1% of the original kernel matrix.

torch.Size([14871, 2])
We keep 4.32e+06/9.16e+07 =  4% of the original kernel matrix.

torch.Size([22453, 2])
We keep 3.96e+06/2.26e+08 =  1% of the original kernel matrix.

torch.Size([12786, 2])
We keep 1.78e+06/4.95e+07 =  3% of the original kernel matrix.

torch.Size([20527, 2])
We keep 3.23e+06/1.66e+08 =  1% of the original kernel matrix.

torch.Size([10780, 2])
We keep 4.86e+06/7.50e+07 =  6% of the original kernel matrix.

torch.Size([18209, 2])
We keep 3.88e+06/2.05e+08 =  1% of the original kernel matrix.

torch.Size([37642, 2])
We keep 7.24e+07/1.46e+09 =  4% of the original kernel matrix.

torch.Size([34614, 2])
We keep 1.22e+07/9.04e+08 =  1% of the original kernel matrix.

torch.Size([2857, 2])
We keep 1.54e+05/1.71e+06 =  9% of the original kernel matrix.

torch.Size([10981, 2])
We keep 9.94e+05/3.09e+07 =  3% of the original kernel matrix.

torch.Size([2866, 2])
We keep 1.26e+05/1.62e+06 =  7% of the original kernel matrix.

torch.Size([10989, 2])
We keep 9.93e+05/3.01e+07 =  3% of the original kernel matrix.

torch.Size([40331, 2])
We keep 2.05e+07/9.04e+08 =  2% of the original kernel matrix.

torch.Size([37048, 2])
We keep 1.03e+07/7.10e+08 =  1% of the original kernel matrix.

torch.Size([18010, 2])
We keep 4.57e+06/1.54e+08 =  2% of the original kernel matrix.

torch.Size([24017, 2])
We keep 5.04e+06/2.93e+08 =  1% of the original kernel matrix.

torch.Size([209916, 2])
We keep 3.41e+08/2.62e+10 =  1% of the original kernel matrix.

torch.Size([85457, 2])
We keep 4.21e+07/3.82e+09 =  1% of the original kernel matrix.

torch.Size([61618, 2])
We keep 2.45e+07/1.94e+09 =  1% of the original kernel matrix.

torch.Size([45943, 2])
We keep 1.37e+07/1.04e+09 =  1% of the original kernel matrix.

torch.Size([1057, 2])
We keep 1.95e+04/1.90e+05 = 10% of the original kernel matrix.

torch.Size([7603, 2])
We keep 4.89e+05/1.03e+07 =  4% of the original kernel matrix.

torch.Size([8382, 2])
We keep 7.06e+05/1.78e+07 =  3% of the original kernel matrix.

torch.Size([16852, 2])
We keep 2.23e+06/9.97e+07 =  2% of the original kernel matrix.

torch.Size([6218, 2])
We keep 5.18e+05/9.97e+06 =  5% of the original kernel matrix.

torch.Size([14818, 2])
We keep 1.81e+06/7.46e+07 =  2% of the original kernel matrix.

torch.Size([11832, 2])
We keep 2.36e+06/6.36e+07 =  3% of the original kernel matrix.

torch.Size([19627, 2])
We keep 3.58e+06/1.88e+08 =  1% of the original kernel matrix.

torch.Size([49597, 2])
We keep 1.83e+07/1.30e+09 =  1% of the original kernel matrix.

torch.Size([41263, 2])
We keep 1.15e+07/8.53e+08 =  1% of the original kernel matrix.

torch.Size([18161, 2])
We keep 2.45e+07/3.05e+08 =  8% of the original kernel matrix.

torch.Size([24064, 2])
We keep 6.51e+06/4.13e+08 =  1% of the original kernel matrix.

torch.Size([34852, 2])
We keep 1.08e+07/6.30e+08 =  1% of the original kernel matrix.

torch.Size([34543, 2])
We keep 8.58e+06/5.93e+08 =  1% of the original kernel matrix.

torch.Size([1412, 2])
We keep 2.37e+04/2.55e+05 =  9% of the original kernel matrix.

torch.Size([8568, 2])
We keep 5.18e+05/1.19e+07 =  4% of the original kernel matrix.

torch.Size([5776, 2])
We keep 3.57e+05/7.71e+06 =  4% of the original kernel matrix.

torch.Size([14425, 2])
We keep 1.65e+06/6.56e+07 =  2% of the original kernel matrix.

torch.Size([13954, 2])
We keep 1.27e+07/1.21e+08 = 10% of the original kernel matrix.

torch.Size([21268, 2])
We keep 4.64e+06/2.60e+08 =  1% of the original kernel matrix.

torch.Size([47564, 2])
We keep 3.05e+07/1.54e+09 =  1% of the original kernel matrix.

torch.Size([39389, 2])
We keep 1.27e+07/9.27e+08 =  1% of the original kernel matrix.

torch.Size([20748, 2])
We keep 5.07e+06/1.92e+08 =  2% of the original kernel matrix.

torch.Size([26151, 2])
We keep 5.52e+06/3.28e+08 =  1% of the original kernel matrix.

torch.Size([4196, 2])
We keep 2.43e+05/4.01e+06 =  6% of the original kernel matrix.

torch.Size([12592, 2])
We keep 1.32e+06/4.73e+07 =  2% of the original kernel matrix.

torch.Size([178823, 2])
We keep 2.01e+08/2.35e+10 =  0% of the original kernel matrix.

torch.Size([78981, 2])
We keep 3.96e+07/3.62e+09 =  1% of the original kernel matrix.

torch.Size([7135, 2])
We keep 4.54e+05/1.20e+07 =  3% of the original kernel matrix.

torch.Size([15886, 2])
We keep 1.91e+06/8.17e+07 =  2% of the original kernel matrix.

torch.Size([23090, 2])
We keep 4.58e+06/2.30e+08 =  1% of the original kernel matrix.

torch.Size([27875, 2])
We keep 5.83e+06/3.58e+08 =  1% of the original kernel matrix.

torch.Size([64950, 2])
We keep 4.28e+07/2.55e+09 =  1% of the original kernel matrix.

torch.Size([46316, 2])
We keep 1.55e+07/1.19e+09 =  1% of the original kernel matrix.

torch.Size([3324, 2])
We keep 1.15e+05/1.98e+06 =  5% of the original kernel matrix.

torch.Size([11753, 2])
We keep 1.05e+06/3.33e+07 =  3% of the original kernel matrix.

torch.Size([24423, 2])
We keep 6.05e+06/2.94e+08 =  2% of the original kernel matrix.

torch.Size([28636, 2])
We keep 6.51e+06/4.05e+08 =  1% of the original kernel matrix.

torch.Size([53434, 2])
We keep 4.00e+07/1.89e+09 =  2% of the original kernel matrix.

torch.Size([42393, 2])
We keep 1.34e+07/1.03e+09 =  1% of the original kernel matrix.

torch.Size([112089, 2])
We keep 1.33e+08/8.66e+09 =  1% of the original kernel matrix.

torch.Size([61041, 2])
We keep 2.61e+07/2.20e+09 =  1% of the original kernel matrix.

torch.Size([9842, 2])
We keep 1.12e+06/2.91e+07 =  3% of the original kernel matrix.

torch.Size([17917, 2])
We keep 2.66e+06/1.28e+08 =  2% of the original kernel matrix.

torch.Size([14986, 2])
We keep 2.80e+06/8.53e+07 =  3% of the original kernel matrix.

torch.Size([21952, 2])
We keep 4.04e+06/2.18e+08 =  1% of the original kernel matrix.

torch.Size([5854, 2])
We keep 6.76e+05/1.01e+07 =  6% of the original kernel matrix.

torch.Size([14060, 2])
We keep 1.88e+06/7.52e+07 =  2% of the original kernel matrix.

torch.Size([5902, 2])
We keep 3.44e+05/7.61e+06 =  4% of the original kernel matrix.

torch.Size([14677, 2])
We keep 1.65e+06/6.52e+07 =  2% of the original kernel matrix.

torch.Size([23310, 2])
We keep 9.35e+06/3.26e+08 =  2% of the original kernel matrix.

torch.Size([27832, 2])
We keep 6.77e+06/4.27e+08 =  1% of the original kernel matrix.

torch.Size([8548, 2])
We keep 7.35e+05/1.85e+07 =  3% of the original kernel matrix.

torch.Size([17017, 2])
We keep 2.26e+06/1.02e+08 =  2% of the original kernel matrix.

torch.Size([31414, 2])
We keep 2.08e+07/6.84e+08 =  3% of the original kernel matrix.

torch.Size([32331, 2])
We keep 9.23e+06/6.18e+08 =  1% of the original kernel matrix.

torch.Size([40489, 2])
We keep 7.49e+07/1.45e+09 =  5% of the original kernel matrix.

torch.Size([36906, 2])
We keep 1.07e+07/8.98e+08 =  1% of the original kernel matrix.

torch.Size([15508, 2])
We keep 5.21e+06/1.12e+08 =  4% of the original kernel matrix.

torch.Size([22722, 2])
We keep 4.39e+06/2.50e+08 =  1% of the original kernel matrix.

torch.Size([5011, 2])
We keep 7.46e+05/1.16e+07 =  6% of the original kernel matrix.

torch.Size([13198, 2])
We keep 1.93e+06/8.05e+07 =  2% of the original kernel matrix.

torch.Size([9519, 2])
We keep 8.84e+05/2.38e+07 =  3% of the original kernel matrix.

torch.Size([17861, 2])
We keep 2.47e+06/1.15e+08 =  2% of the original kernel matrix.

torch.Size([15731, 2])
We keep 5.06e+07/7.47e+08 =  6% of the original kernel matrix.

torch.Size([20801, 2])
We keep 9.11e+06/6.46e+08 =  1% of the original kernel matrix.

torch.Size([12544, 2])
We keep 2.37e+06/6.51e+07 =  3% of the original kernel matrix.

torch.Size([20320, 2])
We keep 3.62e+06/1.91e+08 =  1% of the original kernel matrix.

torch.Size([3975, 2])
We keep 2.90e+05/4.03e+06 =  7% of the original kernel matrix.

torch.Size([12341, 2])
We keep 1.32e+06/4.74e+07 =  2% of the original kernel matrix.

torch.Size([16177, 2])
We keep 2.51e+06/9.12e+07 =  2% of the original kernel matrix.

torch.Size([22979, 2])
We keep 4.13e+06/2.26e+08 =  1% of the original kernel matrix.

torch.Size([311855, 2])
We keep 1.06e+09/7.50e+10 =  1% of the original kernel matrix.

torch.Size([103892, 2])
We keep 6.68e+07/6.47e+09 =  1% of the original kernel matrix.

torch.Size([7145, 2])
We keep 6.98e+05/1.35e+07 =  5% of the original kernel matrix.

torch.Size([15634, 2])
We keep 2.06e+06/8.67e+07 =  2% of the original kernel matrix.

torch.Size([47406, 2])
We keep 5.87e+07/1.76e+09 =  3% of the original kernel matrix.

torch.Size([39057, 2])
We keep 1.27e+07/9.90e+08 =  1% of the original kernel matrix.

torch.Size([6089, 2])
We keep 1.03e+06/1.38e+07 =  7% of the original kernel matrix.

torch.Size([14675, 2])
We keep 2.08e+06/8.77e+07 =  2% of the original kernel matrix.

torch.Size([7395, 2])
We keep 3.86e+06/3.49e+07 = 11% of the original kernel matrix.

torch.Size([15024, 2])
We keep 2.86e+06/1.40e+08 =  2% of the original kernel matrix.

torch.Size([11362, 2])
We keep 1.97e+06/4.59e+07 =  4% of the original kernel matrix.

torch.Size([19551, 2])
We keep 2.98e+06/1.60e+08 =  1% of the original kernel matrix.

torch.Size([11819, 2])
We keep 2.65e+07/1.94e+08 = 13% of the original kernel matrix.

torch.Size([19234, 2])
We keep 5.45e+06/3.29e+08 =  1% of the original kernel matrix.

torch.Size([39869, 2])
We keep 1.43e+07/1.02e+09 =  1% of the original kernel matrix.

torch.Size([36409, 2])
We keep 1.05e+07/7.54e+08 =  1% of the original kernel matrix.

torch.Size([5639, 2])
We keep 3.24e+05/7.20e+06 =  4% of the original kernel matrix.

torch.Size([14316, 2])
We keep 1.64e+06/6.34e+07 =  2% of the original kernel matrix.

torch.Size([94468, 2])
We keep 4.88e+07/4.39e+09 =  1% of the original kernel matrix.

torch.Size([56164, 2])
We keep 1.92e+07/1.57e+09 =  1% of the original kernel matrix.

torch.Size([81881, 2])
We keep 3.89e+07/3.24e+09 =  1% of the original kernel matrix.

torch.Size([52271, 2])
We keep 1.69e+07/1.35e+09 =  1% of the original kernel matrix.

torch.Size([4025, 2])
We keep 2.38e+05/4.39e+06 =  5% of the original kernel matrix.

torch.Size([12382, 2])
We keep 1.36e+06/4.95e+07 =  2% of the original kernel matrix.

torch.Size([32624, 2])
We keep 1.02e+07/5.68e+08 =  1% of the original kernel matrix.

torch.Size([33920, 2])
We keep 8.39e+06/5.63e+08 =  1% of the original kernel matrix.

torch.Size([83613, 2])
We keep 3.99e+07/3.52e+09 =  1% of the original kernel matrix.

torch.Size([53230, 2])
We keep 1.76e+07/1.40e+09 =  1% of the original kernel matrix.

torch.Size([188330, 2])
We keep 2.02e+08/2.17e+10 =  0% of the original kernel matrix.

torch.Size([81584, 2])
We keep 3.85e+07/3.48e+09 =  1% of the original kernel matrix.

torch.Size([16478, 2])
We keep 1.07e+07/1.24e+08 =  8% of the original kernel matrix.

torch.Size([23343, 2])
We keep 4.16e+06/2.63e+08 =  1% of the original kernel matrix.

torch.Size([3431, 2])
We keep 1.25e+06/5.52e+06 = 22% of the original kernel matrix.

torch.Size([11060, 2])
We keep 1.48e+06/5.55e+07 =  2% of the original kernel matrix.

torch.Size([6687, 2])
We keep 4.57e+05/1.09e+07 =  4% of the original kernel matrix.

torch.Size([15196, 2])
We keep 1.90e+06/7.79e+07 =  2% of the original kernel matrix.

torch.Size([6295, 2])
We keep 1.32e+06/2.67e+07 =  4% of the original kernel matrix.

torch.Size([14906, 2])
We keep 2.46e+06/1.22e+08 =  2% of the original kernel matrix.

torch.Size([27507, 2])
We keep 9.39e+06/3.51e+08 =  2% of the original kernel matrix.

torch.Size([30643, 2])
We keep 6.99e+06/4.43e+08 =  1% of the original kernel matrix.

torch.Size([6411, 2])
We keep 4.14e+05/9.77e+06 =  4% of the original kernel matrix.

torch.Size([15176, 2])
We keep 1.78e+06/7.39e+07 =  2% of the original kernel matrix.

torch.Size([3183, 2])
We keep 1.12e+05/1.71e+06 =  6% of the original kernel matrix.

torch.Size([11426, 2])
We keep 9.92e+05/3.09e+07 =  3% of the original kernel matrix.

torch.Size([60165, 2])
We keep 3.08e+07/1.86e+09 =  1% of the original kernel matrix.

torch.Size([45456, 2])
We keep 1.35e+07/1.02e+09 =  1% of the original kernel matrix.

torch.Size([123534, 2])
We keep 8.95e+07/7.79e+09 =  1% of the original kernel matrix.

torch.Size([63765, 2])
We keep 2.49e+07/2.08e+09 =  1% of the original kernel matrix.

torch.Size([43581, 2])
We keep 1.83e+08/3.64e+09 =  5% of the original kernel matrix.

torch.Size([31663, 2])
We keep 1.84e+07/1.43e+09 =  1% of the original kernel matrix.

torch.Size([3104, 2])
We keep 1.31e+05/2.23e+06 =  5% of the original kernel matrix.

torch.Size([11193, 2])
We keep 1.09e+06/3.53e+07 =  3% of the original kernel matrix.

torch.Size([10249, 2])
We keep 1.99e+06/4.30e+07 =  4% of the original kernel matrix.

torch.Size([18300, 2])
We keep 3.06e+06/1.55e+08 =  1% of the original kernel matrix.

torch.Size([19998, 2])
We keep 4.48e+06/1.61e+08 =  2% of the original kernel matrix.

torch.Size([25913, 2])
We keep 5.18e+06/2.99e+08 =  1% of the original kernel matrix.

torch.Size([11113, 2])
We keep 1.25e+06/3.46e+07 =  3% of the original kernel matrix.

torch.Size([19043, 2])
We keep 2.78e+06/1.39e+08 =  2% of the original kernel matrix.

torch.Size([10712, 2])
We keep 1.18e+06/3.28e+07 =  3% of the original kernel matrix.

torch.Size([18761, 2])
We keep 2.83e+06/1.35e+08 =  2% of the original kernel matrix.

torch.Size([4703, 2])
We keep 4.10e+05/5.28e+06 =  7% of the original kernel matrix.

torch.Size([13357, 2])
We keep 1.49e+06/5.43e+07 =  2% of the original kernel matrix.

torch.Size([104015, 2])
We keep 2.24e+08/1.01e+10 =  2% of the original kernel matrix.

torch.Size([56826, 2])
We keep 2.81e+07/2.37e+09 =  1% of the original kernel matrix.

torch.Size([29535, 2])
We keep 6.87e+06/4.16e+08 =  1% of the original kernel matrix.

torch.Size([32587, 2])
We keep 7.51e+06/4.82e+08 =  1% of the original kernel matrix.

torch.Size([10707, 2])
We keep 1.38e+06/3.42e+07 =  4% of the original kernel matrix.

torch.Size([18938, 2])
We keep 2.85e+06/1.38e+08 =  2% of the original kernel matrix.

torch.Size([9541, 2])
We keep 1.95e+06/2.72e+07 =  7% of the original kernel matrix.

torch.Size([18033, 2])
We keep 2.59e+06/1.23e+08 =  2% of the original kernel matrix.

torch.Size([3556, 2])
We keep 3.58e+05/3.56e+06 = 10% of the original kernel matrix.

torch.Size([11572, 2])
We keep 1.23e+06/4.46e+07 =  2% of the original kernel matrix.

torch.Size([258814, 2])
We keep 4.26e+08/4.09e+10 =  1% of the original kernel matrix.

torch.Size([96145, 2])
We keep 5.01e+07/4.78e+09 =  1% of the original kernel matrix.

torch.Size([6749, 2])
We keep 4.74e+05/1.09e+07 =  4% of the original kernel matrix.

torch.Size([15277, 2])
We keep 1.89e+06/7.81e+07 =  2% of the original kernel matrix.

torch.Size([82159, 2])
We keep 5.62e+07/3.44e+09 =  1% of the original kernel matrix.

torch.Size([52206, 2])
We keep 1.77e+07/1.39e+09 =  1% of the original kernel matrix.

torch.Size([1762, 2])
We keep 4.55e+04/5.33e+05 =  8% of the original kernel matrix.

torch.Size([9157, 2])
We keep 6.61e+05/1.72e+07 =  3% of the original kernel matrix.

torch.Size([2434, 2])
We keep 6.79e+04/1.01e+06 =  6% of the original kernel matrix.

torch.Size([10328, 2])
We keep 8.37e+05/2.37e+07 =  3% of the original kernel matrix.

torch.Size([6484, 2])
We keep 3.98e+05/9.68e+06 =  4% of the original kernel matrix.

torch.Size([15108, 2])
We keep 1.80e+06/7.35e+07 =  2% of the original kernel matrix.

torch.Size([17177, 2])
We keep 6.34e+06/1.23e+08 =  5% of the original kernel matrix.

torch.Size([24061, 2])
We keep 4.46e+06/2.62e+08 =  1% of the original kernel matrix.

torch.Size([14027, 2])
We keep 1.02e+07/1.36e+08 =  7% of the original kernel matrix.

torch.Size([21271, 2])
We keep 4.99e+06/2.75e+08 =  1% of the original kernel matrix.

torch.Size([12803, 2])
We keep 3.00e+06/6.11e+07 =  4% of the original kernel matrix.

torch.Size([20426, 2])
We keep 3.57e+06/1.85e+08 =  1% of the original kernel matrix.

torch.Size([5943, 2])
We keep 3.54e+05/7.73e+06 =  4% of the original kernel matrix.

torch.Size([14497, 2])
We keep 1.66e+06/6.57e+07 =  2% of the original kernel matrix.

torch.Size([4747, 2])
We keep 2.70e+05/4.94e+06 =  5% of the original kernel matrix.

torch.Size([13326, 2])
We keep 1.44e+06/5.25e+07 =  2% of the original kernel matrix.

torch.Size([227607, 2])
We keep 3.45e+08/2.87e+10 =  1% of the original kernel matrix.

torch.Size([89432, 2])
We keep 4.33e+07/4.00e+09 =  1% of the original kernel matrix.

torch.Size([22303, 2])
We keep 5.77e+06/2.29e+08 =  2% of the original kernel matrix.

torch.Size([27467, 2])
We keep 5.90e+06/3.57e+08 =  1% of the original kernel matrix.

torch.Size([13728, 2])
We keep 2.36e+06/5.99e+07 =  3% of the original kernel matrix.

torch.Size([21338, 2])
We keep 3.49e+06/1.83e+08 =  1% of the original kernel matrix.

torch.Size([50405, 2])
We keep 3.31e+07/1.44e+09 =  2% of the original kernel matrix.

torch.Size([41866, 2])
We keep 1.23e+07/8.97e+08 =  1% of the original kernel matrix.

torch.Size([27685, 2])
We keep 2.26e+07/7.92e+08 =  2% of the original kernel matrix.

torch.Size([29722, 2])
We keep 9.57e+06/6.65e+08 =  1% of the original kernel matrix.

torch.Size([5083, 2])
We keep 2.64e+05/5.46e+06 =  4% of the original kernel matrix.

torch.Size([13660, 2])
We keep 1.46e+06/5.52e+07 =  2% of the original kernel matrix.

torch.Size([4402, 2])
We keep 2.06e+05/4.12e+06 =  4% of the original kernel matrix.

torch.Size([12886, 2])
We keep 1.33e+06/4.80e+07 =  2% of the original kernel matrix.

torch.Size([8130, 2])
We keep 3.67e+06/3.04e+07 = 12% of the original kernel matrix.

torch.Size([16452, 2])
We keep 2.75e+06/1.30e+08 =  2% of the original kernel matrix.

torch.Size([699, 2])
We keep 6.96e+03/5.34e+04 = 13% of the original kernel matrix.

torch.Size([6703, 2])
We keep 3.25e+05/5.46e+06 =  5% of the original kernel matrix.

torch.Size([3052, 2])
We keep 1.08e+05/1.85e+06 =  5% of the original kernel matrix.

torch.Size([11214, 2])
We keep 1.02e+06/3.21e+07 =  3% of the original kernel matrix.

torch.Size([14892, 2])
We keep 4.14e+06/1.14e+08 =  3% of the original kernel matrix.

torch.Size([21996, 2])
We keep 4.58e+06/2.52e+08 =  1% of the original kernel matrix.

torch.Size([3502, 2])
We keep 1.88e+05/2.79e+06 =  6% of the original kernel matrix.

torch.Size([11709, 2])
We keep 1.18e+06/3.95e+07 =  2% of the original kernel matrix.

torch.Size([2985, 2])
We keep 1.02e+05/1.64e+06 =  6% of the original kernel matrix.

torch.Size([11246, 2])
We keep 9.87e+05/3.03e+07 =  3% of the original kernel matrix.

torch.Size([3708, 2])
We keep 1.94e+05/2.97e+06 =  6% of the original kernel matrix.

torch.Size([11939, 2])
We keep 1.22e+06/4.07e+07 =  2% of the original kernel matrix.

torch.Size([5845, 2])
We keep 4.61e+05/8.59e+06 =  5% of the original kernel matrix.

torch.Size([14523, 2])
We keep 1.74e+06/6.93e+07 =  2% of the original kernel matrix.

torch.Size([252778, 2])
We keep 8.78e+08/5.74e+10 =  1% of the original kernel matrix.

torch.Size([92261, 2])
We keep 5.99e+07/5.66e+09 =  1% of the original kernel matrix.

torch.Size([8743, 2])
We keep 1.69e+06/3.15e+07 =  5% of the original kernel matrix.

torch.Size([17007, 2])
We keep 2.81e+06/1.33e+08 =  2% of the original kernel matrix.

torch.Size([27847, 2])
We keep 8.27e+06/3.92e+08 =  2% of the original kernel matrix.

torch.Size([31003, 2])
We keep 7.21e+06/4.68e+08 =  1% of the original kernel matrix.

torch.Size([21846, 2])
We keep 1.79e+07/5.38e+08 =  3% of the original kernel matrix.

torch.Size([25163, 2])
We keep 8.30e+06/5.48e+08 =  1% of the original kernel matrix.

torch.Size([3210, 2])
We keep 1.21e+05/2.03e+06 =  5% of the original kernel matrix.

torch.Size([11517, 2])
We keep 1.06e+06/3.36e+07 =  3% of the original kernel matrix.

torch.Size([9894, 2])
We keep 1.59e+06/3.06e+07 =  5% of the original kernel matrix.

torch.Size([18091, 2])
We keep 2.75e+06/1.31e+08 =  2% of the original kernel matrix.

torch.Size([283441, 2])
We keep 1.33e+09/6.43e+10 =  2% of the original kernel matrix.

torch.Size([99603, 2])
We keep 5.77e+07/5.99e+09 =  0% of the original kernel matrix.

torch.Size([489490, 2])
We keep 8.06e+08/1.17e+11 =  0% of the original kernel matrix.

torch.Size([133662, 2])
We keep 8.07e+07/8.08e+09 =  0% of the original kernel matrix.

torch.Size([29297, 2])
We keep 2.12e+07/5.19e+08 =  4% of the original kernel matrix.

torch.Size([31312, 2])
We keep 8.31e+06/5.38e+08 =  1% of the original kernel matrix.

torch.Size([18770, 2])
We keep 4.14e+06/1.44e+08 =  2% of the original kernel matrix.

torch.Size([24889, 2])
We keep 4.92e+06/2.84e+08 =  1% of the original kernel matrix.

torch.Size([2662, 2])
We keep 8.17e+04/1.26e+06 =  6% of the original kernel matrix.

torch.Size([10686, 2])
We keep 8.95e+05/2.66e+07 =  3% of the original kernel matrix.

torch.Size([42655, 2])
We keep 1.20e+08/2.84e+09 =  4% of the original kernel matrix.

torch.Size([33537, 2])
We keep 1.67e+07/1.26e+09 =  1% of the original kernel matrix.

torch.Size([33954, 2])
We keep 7.21e+07/1.05e+09 =  6% of the original kernel matrix.

torch.Size([32715, 2])
We keep 1.05e+07/7.67e+08 =  1% of the original kernel matrix.

torch.Size([87875, 2])
We keep 1.60e+08/6.05e+09 =  2% of the original kernel matrix.

torch.Size([53160, 2])
We keep 2.28e+07/1.84e+09 =  1% of the original kernel matrix.

torch.Size([9822, 2])
We keep 1.27e+06/2.72e+07 =  4% of the original kernel matrix.

torch.Size([17909, 2])
We keep 2.60e+06/1.23e+08 =  2% of the original kernel matrix.

torch.Size([78843, 2])
We keep 5.29e+07/3.57e+09 =  1% of the original kernel matrix.

torch.Size([51038, 2])
We keep 1.81e+07/1.41e+09 =  1% of the original kernel matrix.

torch.Size([105539, 2])
We keep 1.34e+08/6.60e+09 =  2% of the original kernel matrix.

torch.Size([58831, 2])
We keep 2.31e+07/1.92e+09 =  1% of the original kernel matrix.

torch.Size([32619, 2])
We keep 1.19e+07/6.27e+08 =  1% of the original kernel matrix.

torch.Size([32518, 2])
We keep 8.64e+06/5.92e+08 =  1% of the original kernel matrix.

torch.Size([4608, 2])
We keep 3.75e+05/6.65e+06 =  5% of the original kernel matrix.

torch.Size([12745, 2])
We keep 1.62e+06/6.09e+07 =  2% of the original kernel matrix.

torch.Size([14343, 2])
We keep 1.93e+06/6.73e+07 =  2% of the original kernel matrix.

torch.Size([21753, 2])
We keep 3.72e+06/1.94e+08 =  1% of the original kernel matrix.

torch.Size([6987, 2])
We keep 5.10e+05/1.33e+07 =  3% of the original kernel matrix.

torch.Size([15574, 2])
We keep 2.02e+06/8.62e+07 =  2% of the original kernel matrix.

torch.Size([3784, 2])
We keep 1.62e+05/3.01e+06 =  5% of the original kernel matrix.

torch.Size([12238, 2])
We keep 1.20e+06/4.10e+07 =  2% of the original kernel matrix.

torch.Size([73821, 2])
We keep 5.00e+07/3.13e+09 =  1% of the original kernel matrix.

torch.Size([49387, 2])
We keep 1.68e+07/1.32e+09 =  1% of the original kernel matrix.

torch.Size([18719, 2])
We keep 5.94e+06/1.90e+08 =  3% of the original kernel matrix.

torch.Size([24316, 2])
We keep 5.49e+06/3.25e+08 =  1% of the original kernel matrix.

torch.Size([1391, 2])
We keep 2.48e+04/2.52e+05 =  9% of the original kernel matrix.

torch.Size([8562, 2])
We keep 5.38e+05/1.19e+07 =  4% of the original kernel matrix.

torch.Size([17325, 2])
We keep 2.75e+06/1.07e+08 =  2% of the original kernel matrix.

torch.Size([23907, 2])
We keep 4.19e+06/2.44e+08 =  1% of the original kernel matrix.

torch.Size([10061, 2])
We keep 9.53e+05/2.66e+07 =  3% of the original kernel matrix.

torch.Size([18339, 2])
We keep 2.60e+06/1.22e+08 =  2% of the original kernel matrix.

torch.Size([1357, 2])
We keep 2.44e+04/2.45e+05 =  9% of the original kernel matrix.

torch.Size([8438, 2])
We keep 5.31e+05/1.17e+07 =  4% of the original kernel matrix.

torch.Size([38691, 2])
We keep 4.39e+07/9.36e+08 =  4% of the original kernel matrix.

torch.Size([36168, 2])
We keep 1.02e+07/7.23e+08 =  1% of the original kernel matrix.

torch.Size([83463, 2])
We keep 5.16e+07/3.67e+09 =  1% of the original kernel matrix.

torch.Size([52726, 2])
We keep 1.77e+07/1.43e+09 =  1% of the original kernel matrix.

torch.Size([21153, 2])
We keep 5.25e+06/1.82e+08 =  2% of the original kernel matrix.

torch.Size([26666, 2])
We keep 5.32e+06/3.19e+08 =  1% of the original kernel matrix.

torch.Size([47776, 2])
We keep 4.24e+07/1.40e+09 =  3% of the original kernel matrix.

torch.Size([40125, 2])
We keep 1.19e+07/8.83e+08 =  1% of the original kernel matrix.

torch.Size([54865, 2])
We keep 2.85e+07/1.77e+09 =  1% of the original kernel matrix.

torch.Size([42691, 2])
We keep 1.32e+07/9.94e+08 =  1% of the original kernel matrix.

torch.Size([6431, 2])
We keep 8.90e+05/1.42e+07 =  6% of the original kernel matrix.

torch.Size([14789, 2])
We keep 2.09e+06/8.89e+07 =  2% of the original kernel matrix.

torch.Size([20218, 2])
We keep 4.78e+06/1.79e+08 =  2% of the original kernel matrix.

torch.Size([25668, 2])
We keep 5.36e+06/3.16e+08 =  1% of the original kernel matrix.

torch.Size([42277, 2])
We keep 1.29e+07/9.41e+08 =  1% of the original kernel matrix.

torch.Size([38545, 2])
We keep 1.01e+07/7.25e+08 =  1% of the original kernel matrix.

torch.Size([110338, 2])
We keep 7.99e+07/6.02e+09 =  1% of the original kernel matrix.

torch.Size([60067, 2])
We keep 2.21e+07/1.83e+09 =  1% of the original kernel matrix.

torch.Size([4818, 2])
We keep 2.28e+05/4.78e+06 =  4% of the original kernel matrix.

torch.Size([13454, 2])
We keep 1.40e+06/5.17e+07 =  2% of the original kernel matrix.

torch.Size([41947, 2])
We keep 6.89e+07/1.47e+09 =  4% of the original kernel matrix.

torch.Size([37501, 2])
We keep 1.12e+07/9.06e+08 =  1% of the original kernel matrix.

torch.Size([61586, 2])
We keep 1.27e+08/3.44e+09 =  3% of the original kernel matrix.

torch.Size([45108, 2])
We keep 1.57e+07/1.38e+09 =  1% of the original kernel matrix.

torch.Size([17010, 2])
We keep 3.54e+06/1.20e+08 =  2% of the original kernel matrix.

torch.Size([23827, 2])
We keep 4.38e+06/2.59e+08 =  1% of the original kernel matrix.

torch.Size([8751, 2])
We keep 2.43e+06/3.70e+07 =  6% of the original kernel matrix.

torch.Size([17207, 2])
We keep 2.68e+06/1.44e+08 =  1% of the original kernel matrix.

torch.Size([10086, 2])
We keep 7.94e+07/3.32e+08 = 23% of the original kernel matrix.

torch.Size([17439, 2])
We keep 7.09e+06/4.31e+08 =  1% of the original kernel matrix.

torch.Size([10876, 2])
We keep 1.43e+06/3.76e+07 =  3% of the original kernel matrix.

torch.Size([19054, 2])
We keep 2.99e+06/1.45e+08 =  2% of the original kernel matrix.

torch.Size([4350, 2])
We keep 2.21e+05/4.22e+06 =  5% of the original kernel matrix.

torch.Size([12784, 2])
We keep 1.36e+06/4.86e+07 =  2% of the original kernel matrix.

torch.Size([13828, 2])
We keep 1.05e+07/1.69e+08 =  6% of the original kernel matrix.

torch.Size([20584, 2])
We keep 5.27e+06/3.07e+08 =  1% of the original kernel matrix.

torch.Size([117777, 2])
We keep 7.47e+07/6.74e+09 =  1% of the original kernel matrix.

torch.Size([62119, 2])
We keep 2.33e+07/1.94e+09 =  1% of the original kernel matrix.

torch.Size([67015, 2])
We keep 2.97e+07/2.31e+09 =  1% of the original kernel matrix.

torch.Size([47581, 2])
We keep 1.48e+07/1.14e+09 =  1% of the original kernel matrix.

torch.Size([394599, 2])
We keep 1.04e+09/9.23e+10 =  1% of the original kernel matrix.

torch.Size([117951, 2])
We keep 7.27e+07/7.18e+09 =  1% of the original kernel matrix.

torch.Size([32355, 2])
We keep 2.48e+07/6.88e+08 =  3% of the original kernel matrix.

torch.Size([33016, 2])
We keep 9.15e+06/6.20e+08 =  1% of the original kernel matrix.

torch.Size([66984, 2])
We keep 5.05e+07/2.68e+09 =  1% of the original kernel matrix.

torch.Size([47072, 2])
We keep 1.58e+07/1.22e+09 =  1% of the original kernel matrix.

torch.Size([13864, 2])
We keep 2.54e+06/1.05e+08 =  2% of the original kernel matrix.

torch.Size([21481, 2])
We keep 4.29e+06/2.42e+08 =  1% of the original kernel matrix.

torch.Size([76335, 2])
We keep 5.05e+07/3.13e+09 =  1% of the original kernel matrix.

torch.Size([50494, 2])
We keep 1.69e+07/1.32e+09 =  1% of the original kernel matrix.

torch.Size([1605, 2])
We keep 3.10e+04/3.46e+05 =  8% of the original kernel matrix.

torch.Size([8861, 2])
We keep 5.84e+05/1.39e+07 =  4% of the original kernel matrix.

torch.Size([8466, 2])
We keep 1.16e+06/2.44e+07 =  4% of the original kernel matrix.

torch.Size([16520, 2])
We keep 2.48e+06/1.17e+08 =  2% of the original kernel matrix.

torch.Size([172714, 2])
We keep 2.11e+08/1.61e+10 =  1% of the original kernel matrix.

torch.Size([77184, 2])
We keep 3.43e+07/2.99e+09 =  1% of the original kernel matrix.

torch.Size([14848, 2])
We keep 2.35e+06/7.74e+07 =  3% of the original kernel matrix.

torch.Size([21936, 2])
We keep 3.80e+06/2.08e+08 =  1% of the original kernel matrix.

torch.Size([3766, 2])
We keep 1.37e+05/2.37e+06 =  5% of the original kernel matrix.

torch.Size([12164, 2])
We keep 1.08e+06/3.64e+07 =  2% of the original kernel matrix.

torch.Size([11903, 2])
We keep 3.13e+06/7.31e+07 =  4% of the original kernel matrix.

torch.Size([19874, 2])
We keep 3.77e+06/2.02e+08 =  1% of the original kernel matrix.

torch.Size([22641, 2])
We keep 1.63e+07/3.20e+08 =  5% of the original kernel matrix.

torch.Size([27407, 2])
We keep 6.71e+06/4.23e+08 =  1% of the original kernel matrix.

torch.Size([26240, 2])
We keep 2.97e+07/4.63e+08 =  6% of the original kernel matrix.

torch.Size([29017, 2])
We keep 7.62e+06/5.08e+08 =  1% of the original kernel matrix.

torch.Size([36925, 2])
We keep 1.28e+07/7.27e+08 =  1% of the original kernel matrix.

torch.Size([35861, 2])
We keep 9.33e+06/6.37e+08 =  1% of the original kernel matrix.

torch.Size([7719, 2])
We keep 3.31e+06/4.04e+07 =  8% of the original kernel matrix.

torch.Size([15534, 2])
We keep 3.09e+06/1.50e+08 =  2% of the original kernel matrix.

torch.Size([20303, 2])
We keep 1.57e+07/3.72e+08 =  4% of the original kernel matrix.

torch.Size([25416, 2])
We keep 7.42e+06/4.55e+08 =  1% of the original kernel matrix.

torch.Size([2953, 2])
We keep 1.20e+05/1.76e+06 =  6% of the original kernel matrix.

torch.Size([11167, 2])
We keep 1.01e+06/3.14e+07 =  3% of the original kernel matrix.

torch.Size([13230, 2])
We keep 5.27e+06/7.35e+07 =  7% of the original kernel matrix.

torch.Size([21079, 2])
We keep 3.83e+06/2.03e+08 =  1% of the original kernel matrix.

torch.Size([19816, 2])
We keep 3.55e+06/1.53e+08 =  2% of the original kernel matrix.

torch.Size([25656, 2])
We keep 5.01e+06/2.92e+08 =  1% of the original kernel matrix.

torch.Size([21500, 2])
We keep 3.37e+07/3.69e+08 =  9% of the original kernel matrix.

torch.Size([26258, 2])
We keep 6.93e+06/4.54e+08 =  1% of the original kernel matrix.

torch.Size([2238, 2])
We keep 5.32e+04/7.67e+05 =  6% of the original kernel matrix.

torch.Size([10168, 2])
We keep 7.47e+05/2.07e+07 =  3% of the original kernel matrix.

torch.Size([116138, 2])
We keep 9.56e+07/6.88e+09 =  1% of the original kernel matrix.

torch.Size([61938, 2])
We keep 2.31e+07/1.96e+09 =  1% of the original kernel matrix.

torch.Size([1787, 2])
We keep 5.25e+04/6.29e+05 =  8% of the original kernel matrix.

torch.Size([9156, 2])
We keep 7.22e+05/1.87e+07 =  3% of the original kernel matrix.

torch.Size([17438, 2])
We keep 2.85e+06/1.02e+08 =  2% of the original kernel matrix.

torch.Size([24054, 2])
We keep 4.16e+06/2.39e+08 =  1% of the original kernel matrix.

torch.Size([4965, 2])
We keep 1.19e+06/1.03e+07 = 11% of the original kernel matrix.

torch.Size([13236, 2])
We keep 1.84e+06/7.59e+07 =  2% of the original kernel matrix.

torch.Size([19942, 2])
We keep 3.37e+06/1.56e+08 =  2% of the original kernel matrix.

torch.Size([25830, 2])
We keep 5.01e+06/2.95e+08 =  1% of the original kernel matrix.

torch.Size([4290, 2])
We keep 1.87e+05/3.57e+06 =  5% of the original kernel matrix.

torch.Size([12929, 2])
We keep 1.29e+06/4.46e+07 =  2% of the original kernel matrix.

torch.Size([15180, 2])
We keep 4.05e+06/9.17e+07 =  4% of the original kernel matrix.

torch.Size([22230, 2])
We keep 4.06e+06/2.26e+08 =  1% of the original kernel matrix.

torch.Size([12173, 2])
We keep 2.08e+06/5.50e+07 =  3% of the original kernel matrix.

torch.Size([19883, 2])
We keep 3.41e+06/1.75e+08 =  1% of the original kernel matrix.

torch.Size([18205, 2])
We keep 4.18e+06/1.36e+08 =  3% of the original kernel matrix.

torch.Size([24672, 2])
We keep 4.86e+06/2.76e+08 =  1% of the original kernel matrix.

torch.Size([29454, 2])
We keep 8.51e+06/4.33e+08 =  1% of the original kernel matrix.

torch.Size([31131, 2])
We keep 7.28e+06/4.92e+08 =  1% of the original kernel matrix.

torch.Size([145081, 2])
We keep 2.14e+08/1.44e+10 =  1% of the original kernel matrix.

torch.Size([70215, 2])
We keep 3.18e+07/2.84e+09 =  1% of the original kernel matrix.

torch.Size([3735, 2])
We keep 1.48e+05/2.59e+06 =  5% of the original kernel matrix.

torch.Size([12211, 2])
We keep 1.15e+06/3.80e+07 =  3% of the original kernel matrix.

torch.Size([5317, 2])
We keep 1.08e+06/1.60e+07 =  6% of the original kernel matrix.

torch.Size([12725, 2])
We keep 2.13e+06/9.46e+07 =  2% of the original kernel matrix.

torch.Size([86474, 2])
We keep 9.23e+07/4.88e+09 =  1% of the original kernel matrix.

torch.Size([52713, 2])
We keep 2.07e+07/1.65e+09 =  1% of the original kernel matrix.

torch.Size([5500, 2])
We keep 3.32e+05/6.81e+06 =  4% of the original kernel matrix.

torch.Size([14175, 2])
We keep 1.58e+06/6.16e+07 =  2% of the original kernel matrix.

torch.Size([356082, 2])
We keep 5.04e+08/6.58e+10 =  0% of the original kernel matrix.

torch.Size([114304, 2])
We keep 6.28e+07/6.06e+09 =  1% of the original kernel matrix.

torch.Size([1673, 2])
We keep 5.35e+04/5.42e+05 =  9% of the original kernel matrix.

torch.Size([9006, 2])
We keep 6.94e+05/1.74e+07 =  3% of the original kernel matrix.

torch.Size([66730, 2])
We keep 3.73e+07/2.59e+09 =  1% of the original kernel matrix.

torch.Size([46982, 2])
We keep 1.57e+07/1.20e+09 =  1% of the original kernel matrix.

torch.Size([201331, 2])
We keep 1.86e+08/2.15e+10 =  0% of the original kernel matrix.

torch.Size([84055, 2])
We keep 3.84e+07/3.46e+09 =  1% of the original kernel matrix.

torch.Size([2883, 2])
We keep 9.98e+04/1.58e+06 =  6% of the original kernel matrix.

torch.Size([11068, 2])
We keep 9.67e+05/2.97e+07 =  3% of the original kernel matrix.

torch.Size([120497, 2])
We keep 1.81e+08/1.16e+10 =  1% of the original kernel matrix.

torch.Size([62597, 2])
We keep 2.87e+07/2.54e+09 =  1% of the original kernel matrix.

torch.Size([6475, 2])
We keep 2.67e+06/3.87e+07 =  6% of the original kernel matrix.

torch.Size([14027, 2])
We keep 2.90e+06/1.47e+08 =  1% of the original kernel matrix.

torch.Size([107342, 2])
We keep 1.41e+08/6.35e+09 =  2% of the original kernel matrix.

torch.Size([59221, 2])
We keep 2.29e+07/1.88e+09 =  1% of the original kernel matrix.

torch.Size([8075, 2])
We keep 6.66e+05/1.63e+07 =  4% of the original kernel matrix.

torch.Size([16500, 2])
We keep 2.19e+06/9.54e+07 =  2% of the original kernel matrix.

torch.Size([46010, 2])
We keep 4.74e+07/1.68e+09 =  2% of the original kernel matrix.

torch.Size([37745, 2])
We keep 1.33e+07/9.70e+08 =  1% of the original kernel matrix.

torch.Size([1187, 2])
We keep 1.60e+04/1.48e+05 = 10% of the original kernel matrix.

torch.Size([8120, 2])
We keep 4.40e+05/9.10e+06 =  4% of the original kernel matrix.

torch.Size([4821, 2])
We keep 2.48e+05/5.00e+06 =  4% of the original kernel matrix.

torch.Size([13476, 2])
We keep 1.44e+06/5.28e+07 =  2% of the original kernel matrix.

torch.Size([5170, 2])
We keep 4.81e+05/6.93e+06 =  6% of the original kernel matrix.

torch.Size([13788, 2])
We keep 1.61e+06/6.22e+07 =  2% of the original kernel matrix.

torch.Size([26210, 2])
We keep 8.61e+06/3.69e+08 =  2% of the original kernel matrix.

torch.Size([29585, 2])
We keep 6.99e+06/4.54e+08 =  1% of the original kernel matrix.

torch.Size([12658, 2])
We keep 4.05e+06/8.86e+07 =  4% of the original kernel matrix.

torch.Size([19906, 2])
We keep 4.10e+06/2.22e+08 =  1% of the original kernel matrix.

torch.Size([65452, 2])
We keep 8.23e+07/2.69e+09 =  3% of the original kernel matrix.

torch.Size([46464, 2])
We keep 1.60e+07/1.23e+09 =  1% of the original kernel matrix.

torch.Size([12612, 2])
We keep 1.57e+06/5.06e+07 =  3% of the original kernel matrix.

torch.Size([20150, 2])
We keep 3.30e+06/1.68e+08 =  1% of the original kernel matrix.

torch.Size([2556, 2])
We keep 8.23e+04/1.20e+06 =  6% of the original kernel matrix.

torch.Size([10578, 2])
We keep 8.95e+05/2.58e+07 =  3% of the original kernel matrix.

torch.Size([4598, 2])
We keep 4.67e+05/5.87e+06 =  7% of the original kernel matrix.

torch.Size([13005, 2])
We keep 1.51e+06/5.72e+07 =  2% of the original kernel matrix.

torch.Size([21788, 2])
We keep 2.30e+07/2.40e+08 =  9% of the original kernel matrix.

torch.Size([27173, 2])
We keep 5.88e+06/3.66e+08 =  1% of the original kernel matrix.

torch.Size([3106, 2])
We keep 1.91e+05/2.11e+06 =  9% of the original kernel matrix.

torch.Size([11225, 2])
We keep 1.00e+06/3.43e+07 =  2% of the original kernel matrix.

torch.Size([98505, 2])
We keep 5.75e+07/5.11e+09 =  1% of the original kernel matrix.

torch.Size([56899, 2])
We keep 2.06e+07/1.69e+09 =  1% of the original kernel matrix.

torch.Size([40256, 2])
We keep 3.75e+07/1.19e+09 =  3% of the original kernel matrix.

torch.Size([36751, 2])
We keep 1.16e+07/8.16e+08 =  1% of the original kernel matrix.

torch.Size([8308, 2])
We keep 1.33e+06/2.62e+07 =  5% of the original kernel matrix.

torch.Size([16718, 2])
We keep 2.53e+06/1.21e+08 =  2% of the original kernel matrix.

torch.Size([2557, 2])
We keep 7.51e+04/1.14e+06 =  6% of the original kernel matrix.

torch.Size([10510, 2])
We keep 8.67e+05/2.52e+07 =  3% of the original kernel matrix.

torch.Size([7525, 2])
We keep 3.25e+06/3.16e+07 = 10% of the original kernel matrix.

torch.Size([15447, 2])
We keep 2.76e+06/1.33e+08 =  2% of the original kernel matrix.

torch.Size([481872, 2])
We keep 1.45e+09/1.17e+11 =  1% of the original kernel matrix.

torch.Size([132136, 2])
We keep 8.17e+07/8.08e+09 =  1% of the original kernel matrix.

torch.Size([9212, 2])
We keep 8.07e+05/2.21e+07 =  3% of the original kernel matrix.

torch.Size([17638, 2])
We keep 2.40e+06/1.11e+08 =  2% of the original kernel matrix.

torch.Size([12900, 2])
We keep 2.61e+06/5.44e+07 =  4% of the original kernel matrix.

torch.Size([20588, 2])
We keep 3.42e+06/1.74e+08 =  1% of the original kernel matrix.

torch.Size([43264, 2])
We keep 4.01e+07/1.67e+09 =  2% of the original kernel matrix.

torch.Size([36134, 2])
We keep 1.32e+07/9.64e+08 =  1% of the original kernel matrix.

torch.Size([15412, 2])
We keep 1.08e+07/2.05e+08 =  5% of the original kernel matrix.

torch.Size([21824, 2])
We keep 5.75e+06/3.39e+08 =  1% of the original kernel matrix.

torch.Size([2654, 2])
We keep 1.25e+05/1.80e+06 =  6% of the original kernel matrix.

torch.Size([10456, 2])
We keep 1.02e+06/3.17e+07 =  3% of the original kernel matrix.

torch.Size([3198, 2])
We keep 1.15e+05/1.78e+06 =  6% of the original kernel matrix.

torch.Size([11179, 2])
We keep 9.91e+05/3.15e+07 =  3% of the original kernel matrix.

torch.Size([117998, 2])
We keep 3.88e+08/1.30e+10 =  2% of the original kernel matrix.

torch.Size([61934, 2])
We keep 3.03e+07/2.70e+09 =  1% of the original kernel matrix.

torch.Size([13455, 2])
We keep 2.07e+06/6.18e+07 =  3% of the original kernel matrix.

torch.Size([21031, 2])
We keep 3.58e+06/1.86e+08 =  1% of the original kernel matrix.

torch.Size([22868, 2])
We keep 1.23e+07/3.95e+08 =  3% of the original kernel matrix.

torch.Size([27075, 2])
We keep 7.48e+06/4.69e+08 =  1% of the original kernel matrix.

torch.Size([24628, 2])
We keep 7.19e+06/2.79e+08 =  2% of the original kernel matrix.

torch.Size([29032, 2])
We keep 6.32e+06/3.94e+08 =  1% of the original kernel matrix.

torch.Size([14645, 2])
We keep 3.74e+06/7.85e+07 =  4% of the original kernel matrix.

torch.Size([21896, 2])
We keep 3.64e+06/2.09e+08 =  1% of the original kernel matrix.

torch.Size([35436, 2])
We keep 1.72e+07/6.40e+08 =  2% of the original kernel matrix.

torch.Size([34829, 2])
We keep 8.69e+06/5.98e+08 =  1% of the original kernel matrix.

torch.Size([90600, 2])
We keep 1.40e+08/5.40e+09 =  2% of the original kernel matrix.

torch.Size([54006, 2])
We keep 2.12e+07/1.74e+09 =  1% of the original kernel matrix.

torch.Size([73374, 2])
We keep 3.90e+07/2.83e+09 =  1% of the original kernel matrix.

torch.Size([49548, 2])
We keep 1.63e+07/1.26e+09 =  1% of the original kernel matrix.

torch.Size([4136, 2])
We keep 2.74e+05/3.71e+06 =  7% of the original kernel matrix.

torch.Size([12776, 2])
We keep 1.27e+06/4.55e+07 =  2% of the original kernel matrix.

torch.Size([40116, 2])
We keep 1.27e+07/8.23e+08 =  1% of the original kernel matrix.

torch.Size([37030, 2])
We keep 9.70e+06/6.78e+08 =  1% of the original kernel matrix.

torch.Size([7516, 2])
We keep 6.89e+05/1.45e+07 =  4% of the original kernel matrix.

torch.Size([15969, 2])
We keep 2.10e+06/8.98e+07 =  2% of the original kernel matrix.

torch.Size([11077, 2])
We keep 1.83e+06/4.02e+07 =  4% of the original kernel matrix.

torch.Size([19114, 2])
We keep 3.08e+06/1.50e+08 =  2% of the original kernel matrix.

torch.Size([20897, 2])
We keep 5.14e+06/2.03e+08 =  2% of the original kernel matrix.

torch.Size([26335, 2])
We keep 5.63e+06/3.37e+08 =  1% of the original kernel matrix.

torch.Size([4905, 2])
We keep 2.45e+05/5.01e+06 =  4% of the original kernel matrix.

torch.Size([13699, 2])
We keep 1.37e+06/5.29e+07 =  2% of the original kernel matrix.

torch.Size([27214, 2])
We keep 3.74e+07/1.07e+09 =  3% of the original kernel matrix.

torch.Size([28462, 2])
We keep 1.10e+07/7.73e+08 =  1% of the original kernel matrix.

torch.Size([11615, 2])
We keep 1.35e+06/4.03e+07 =  3% of the original kernel matrix.

torch.Size([19255, 2])
We keep 3.03e+06/1.50e+08 =  2% of the original kernel matrix.

torch.Size([2432, 2])
We keep 9.90e+04/1.32e+06 =  7% of the original kernel matrix.

torch.Size([10183, 2])
We keep 9.26e+05/2.71e+07 =  3% of the original kernel matrix.

torch.Size([12406, 2])
We keep 1.57e+06/4.94e+07 =  3% of the original kernel matrix.

torch.Size([19968, 2])
We keep 3.30e+06/1.66e+08 =  1% of the original kernel matrix.

torch.Size([2230, 2])
We keep 1.67e+06/7.75e+06 = 21% of the original kernel matrix.

torch.Size([8788, 2])
We keep 1.40e+06/6.58e+07 =  2% of the original kernel matrix.

torch.Size([39009, 2])
We keep 4.65e+07/1.65e+09 =  2% of the original kernel matrix.

torch.Size([34055, 2])
We keep 1.33e+07/9.60e+08 =  1% of the original kernel matrix.

torch.Size([28746, 2])
We keep 9.35e+07/1.60e+09 =  5% of the original kernel matrix.

torch.Size([28249, 2])
We keep 1.32e+07/9.46e+08 =  1% of the original kernel matrix.

torch.Size([9350, 2])
We keep 1.10e+06/2.88e+07 =  3% of the original kernel matrix.

torch.Size([17490, 2])
We keep 2.57e+06/1.27e+08 =  2% of the original kernel matrix.

torch.Size([6495, 2])
We keep 1.43e+06/1.90e+07 =  7% of the original kernel matrix.

torch.Size([14488, 2])
We keep 2.19e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([17356, 2])
We keep 3.43e+06/1.16e+08 =  2% of the original kernel matrix.

torch.Size([23945, 2])
We keep 4.56e+06/2.55e+08 =  1% of the original kernel matrix.

torch.Size([3083, 2])
We keep 1.07e+05/1.73e+06 =  6% of the original kernel matrix.

torch.Size([11275, 2])
We keep 9.94e+05/3.11e+07 =  3% of the original kernel matrix.

torch.Size([5821, 2])
We keep 3.36e+05/7.31e+06 =  4% of the original kernel matrix.

torch.Size([14567, 2])
We keep 1.64e+06/6.39e+07 =  2% of the original kernel matrix.

torch.Size([21202, 2])
We keep 3.98e+06/1.79e+08 =  2% of the original kernel matrix.

torch.Size([26712, 2])
We keep 5.28e+06/3.16e+08 =  1% of the original kernel matrix.

torch.Size([91048, 2])
We keep 9.82e+07/4.92e+09 =  1% of the original kernel matrix.

torch.Size([55135, 2])
We keep 2.06e+07/1.66e+09 =  1% of the original kernel matrix.

torch.Size([4865, 2])
We keep 2.52e+05/4.88e+06 =  5% of the original kernel matrix.

torch.Size([13459, 2])
We keep 1.43e+06/5.22e+07 =  2% of the original kernel matrix.

torch.Size([2191, 2])
We keep 7.63e+04/9.20e+05 =  8% of the original kernel matrix.

torch.Size([9997, 2])
We keep 8.23e+05/2.27e+07 =  3% of the original kernel matrix.

torch.Size([2901, 2])
We keep 1.06e+05/1.46e+06 =  7% of the original kernel matrix.

torch.Size([11059, 2])
We keep 9.18e+05/2.86e+07 =  3% of the original kernel matrix.

torch.Size([147614, 2])
We keep 2.05e+08/1.40e+10 =  1% of the original kernel matrix.

torch.Size([70824, 2])
We keep 3.25e+07/2.79e+09 =  1% of the original kernel matrix.

torch.Size([243744, 2])
We keep 4.33e+08/3.53e+10 =  1% of the original kernel matrix.

torch.Size([93651, 2])
We keep 4.65e+07/4.44e+09 =  1% of the original kernel matrix.

torch.Size([18144, 2])
We keep 8.89e+06/1.99e+08 =  4% of the original kernel matrix.

torch.Size([24049, 2])
We keep 5.67e+06/3.34e+08 =  1% of the original kernel matrix.

torch.Size([5259, 2])
We keep 2.56e+05/5.28e+06 =  4% of the original kernel matrix.

torch.Size([13834, 2])
We keep 1.44e+06/5.43e+07 =  2% of the original kernel matrix.

torch.Size([20072, 2])
We keep 8.18e+06/2.71e+08 =  3% of the original kernel matrix.

torch.Size([25578, 2])
We keep 6.39e+06/3.89e+08 =  1% of the original kernel matrix.

torch.Size([47072, 2])
We keep 1.87e+07/1.41e+09 =  1% of the original kernel matrix.

torch.Size([40963, 2])
We keep 1.23e+07/8.88e+08 =  1% of the original kernel matrix.

torch.Size([3076, 2])
We keep 9.42e+04/1.54e+06 =  6% of the original kernel matrix.

torch.Size([11435, 2])
We keep 9.61e+05/2.93e+07 =  3% of the original kernel matrix.

torch.Size([9881, 2])
We keep 1.07e+06/2.84e+07 =  3% of the original kernel matrix.

torch.Size([18391, 2])
We keep 2.57e+06/1.26e+08 =  2% of the original kernel matrix.

torch.Size([18097, 2])
We keep 2.60e+06/1.13e+08 =  2% of the original kernel matrix.

torch.Size([24367, 2])
We keep 4.42e+06/2.51e+08 =  1% of the original kernel matrix.

torch.Size([33966, 2])
We keep 2.47e+08/2.14e+09 = 11% of the original kernel matrix.

torch.Size([32288, 2])
We keep 1.45e+07/1.09e+09 =  1% of the original kernel matrix.

torch.Size([21747, 2])
We keep 5.12e+07/3.59e+08 = 14% of the original kernel matrix.

torch.Size([26482, 2])
We keep 7.08e+06/4.48e+08 =  1% of the original kernel matrix.

torch.Size([92174, 2])
We keep 6.57e+07/4.28e+09 =  1% of the original kernel matrix.

torch.Size([55179, 2])
We keep 1.93e+07/1.55e+09 =  1% of the original kernel matrix.

torch.Size([10113, 2])
We keep 9.94e+05/2.65e+07 =  3% of the original kernel matrix.

torch.Size([18171, 2])
We keep 2.60e+06/1.22e+08 =  2% of the original kernel matrix.

torch.Size([3231, 2])
We keep 1.13e+05/1.96e+06 =  5% of the original kernel matrix.

torch.Size([11564, 2])
We keep 1.03e+06/3.31e+07 =  3% of the original kernel matrix.

torch.Size([3323, 2])
We keep 9.72e+04/1.69e+06 =  5% of the original kernel matrix.

torch.Size([11656, 2])
We keep 9.66e+05/3.07e+07 =  3% of the original kernel matrix.

torch.Size([19550, 2])
We keep 6.20e+06/2.13e+08 =  2% of the original kernel matrix.

torch.Size([25297, 2])
We keep 5.61e+06/3.45e+08 =  1% of the original kernel matrix.

torch.Size([14519, 2])
We keep 2.28e+06/6.96e+07 =  3% of the original kernel matrix.

torch.Size([21801, 2])
We keep 3.72e+06/1.97e+08 =  1% of the original kernel matrix.

torch.Size([29990, 2])
We keep 1.90e+07/7.43e+08 =  2% of the original kernel matrix.

torch.Size([30563, 2])
We keep 9.46e+06/6.44e+08 =  1% of the original kernel matrix.

torch.Size([39005, 2])
We keep 2.22e+07/8.53e+08 =  2% of the original kernel matrix.

torch.Size([36500, 2])
We keep 9.65e+06/6.90e+08 =  1% of the original kernel matrix.

torch.Size([206683, 2])
We keep 2.21e+08/2.36e+10 =  0% of the original kernel matrix.

torch.Size([85162, 2])
We keep 3.98e+07/3.63e+09 =  1% of the original kernel matrix.

torch.Size([6963, 2])
We keep 7.86e+05/1.25e+07 =  6% of the original kernel matrix.

torch.Size([15424, 2])
We keep 1.96e+06/8.35e+07 =  2% of the original kernel matrix.

torch.Size([5084, 2])
We keep 1.20e+06/9.25e+06 = 12% of the original kernel matrix.

torch.Size([13362, 2])
We keep 1.79e+06/7.18e+07 =  2% of the original kernel matrix.

torch.Size([21062, 2])
We keep 5.56e+06/1.81e+08 =  3% of the original kernel matrix.

torch.Size([26575, 2])
We keep 5.36e+06/3.18e+08 =  1% of the original kernel matrix.

torch.Size([225100, 2])
We keep 1.70e+09/4.18e+10 =  4% of the original kernel matrix.

torch.Size([89397, 2])
We keep 4.87e+07/4.83e+09 =  1% of the original kernel matrix.

torch.Size([26461, 2])
We keep 6.60e+06/3.13e+08 =  2% of the original kernel matrix.

torch.Size([30050, 2])
We keep 6.35e+06/4.18e+08 =  1% of the original kernel matrix.

torch.Size([6273, 2])
We keep 1.25e+06/1.42e+07 =  8% of the original kernel matrix.

torch.Size([14380, 2])
We keep 2.15e+06/8.90e+07 =  2% of the original kernel matrix.

torch.Size([1745, 2])
We keep 5.21e+04/5.90e+05 =  8% of the original kernel matrix.

torch.Size([8986, 2])
We keep 6.87e+05/1.81e+07 =  3% of the original kernel matrix.

torch.Size([33526, 2])
We keep 1.19e+07/5.75e+08 =  2% of the original kernel matrix.

torch.Size([33992, 2])
We keep 8.27e+06/5.66e+08 =  1% of the original kernel matrix.

torch.Size([1652, 2])
We keep 3.70e+04/4.26e+05 =  8% of the original kernel matrix.

torch.Size([9056, 2])
We keep 6.31e+05/1.54e+07 =  4% of the original kernel matrix.

torch.Size([9156, 2])
We keep 3.98e+06/4.20e+07 =  9% of the original kernel matrix.

torch.Size([17204, 2])
We keep 3.09e+06/1.53e+08 =  2% of the original kernel matrix.

torch.Size([3737, 2])
We keep 1.52e+05/2.65e+06 =  5% of the original kernel matrix.

torch.Size([12217, 2])
We keep 1.16e+06/3.85e+07 =  3% of the original kernel matrix.

torch.Size([12327, 2])
We keep 1.28e+06/4.57e+07 =  2% of the original kernel matrix.

torch.Size([20113, 2])
We keep 3.11e+06/1.60e+08 =  1% of the original kernel matrix.

torch.Size([7509, 2])
We keep 6.47e+05/1.50e+07 =  4% of the original kernel matrix.

torch.Size([15990, 2])
We keep 2.12e+06/9.14e+07 =  2% of the original kernel matrix.

torch.Size([4463, 2])
We keep 1.88e+05/3.81e+06 =  4% of the original kernel matrix.

torch.Size([13167, 2])
We keep 1.31e+06/4.61e+07 =  2% of the original kernel matrix.

torch.Size([2127, 2])
We keep 5.22e+04/7.06e+05 =  7% of the original kernel matrix.

torch.Size([9945, 2])
We keep 7.51e+05/1.98e+07 =  3% of the original kernel matrix.

torch.Size([159081, 2])
We keep 1.44e+08/1.36e+10 =  1% of the original kernel matrix.

torch.Size([73888, 2])
We keep 3.16e+07/2.76e+09 =  1% of the original kernel matrix.

torch.Size([270898, 2])
We keep 5.92e+08/4.26e+10 =  1% of the original kernel matrix.

torch.Size([99212, 2])
We keep 5.25e+07/4.88e+09 =  1% of the original kernel matrix.

torch.Size([82685, 2])
We keep 6.48e+07/3.95e+09 =  1% of the original kernel matrix.

torch.Size([52194, 2])
We keep 1.89e+07/1.49e+09 =  1% of the original kernel matrix.

torch.Size([19181, 2])
We keep 4.44e+07/3.96e+08 = 11% of the original kernel matrix.

torch.Size([23878, 2])
We keep 6.78e+06/4.70e+08 =  1% of the original kernel matrix.

torch.Size([20971, 2])
We keep 3.73e+06/1.80e+08 =  2% of the original kernel matrix.

torch.Size([26548, 2])
We keep 5.23e+06/3.17e+08 =  1% of the original kernel matrix.

torch.Size([8476, 2])
We keep 1.41e+06/2.41e+07 =  5% of the original kernel matrix.

torch.Size([16971, 2])
We keep 2.55e+06/1.16e+08 =  2% of the original kernel matrix.

torch.Size([10054, 2])
We keep 1.27e+06/2.93e+07 =  4% of the original kernel matrix.

torch.Size([18243, 2])
We keep 2.74e+06/1.28e+08 =  2% of the original kernel matrix.

torch.Size([37048, 2])
We keep 1.11e+07/7.33e+08 =  1% of the original kernel matrix.

torch.Size([36421, 2])
We keep 9.29e+06/6.40e+08 =  1% of the original kernel matrix.

torch.Size([34656, 2])
We keep 1.81e+07/6.64e+08 =  2% of the original kernel matrix.

torch.Size([35478, 2])
We keep 8.91e+06/6.09e+08 =  1% of the original kernel matrix.

torch.Size([61067, 2])
We keep 9.90e+07/3.04e+09 =  3% of the original kernel matrix.

torch.Size([44658, 2])
We keep 1.65e+07/1.30e+09 =  1% of the original kernel matrix.

torch.Size([8055, 2])
We keep 1.09e+06/1.77e+07 =  6% of the original kernel matrix.

torch.Size([16487, 2])
We keep 2.26e+06/9.95e+07 =  2% of the original kernel matrix.

torch.Size([169566, 2])
We keep 4.16e+08/2.14e+10 =  1% of the original kernel matrix.

torch.Size([76846, 2])
We keep 3.80e+07/3.46e+09 =  1% of the original kernel matrix.

torch.Size([17454, 2])
We keep 3.87e+06/1.21e+08 =  3% of the original kernel matrix.

torch.Size([24004, 2])
We keep 4.56e+06/2.60e+08 =  1% of the original kernel matrix.

torch.Size([21108, 2])
We keep 4.82e+06/1.97e+08 =  2% of the original kernel matrix.

torch.Size([26819, 2])
We keep 5.57e+06/3.31e+08 =  1% of the original kernel matrix.

torch.Size([102308, 2])
We keep 1.24e+08/6.30e+09 =  1% of the original kernel matrix.

torch.Size([58442, 2])
We keep 2.29e+07/1.87e+09 =  1% of the original kernel matrix.

torch.Size([42513, 2])
We keep 1.52e+07/9.83e+08 =  1% of the original kernel matrix.

torch.Size([38627, 2])
We keep 1.05e+07/7.41e+08 =  1% of the original kernel matrix.

torch.Size([106225, 2])
We keep 2.93e+08/1.01e+10 =  2% of the original kernel matrix.

torch.Size([58540, 2])
We keep 2.74e+07/2.38e+09 =  1% of the original kernel matrix.

torch.Size([91605, 2])
We keep 8.62e+07/4.61e+09 =  1% of the original kernel matrix.

torch.Size([54968, 2])
We keep 1.97e+07/1.61e+09 =  1% of the original kernel matrix.

torch.Size([34519, 2])
We keep 9.49e+06/6.14e+08 =  1% of the original kernel matrix.

torch.Size([34546, 2])
We keep 8.59e+06/5.85e+08 =  1% of the original kernel matrix.

torch.Size([30873, 2])
We keep 1.28e+07/4.67e+08 =  2% of the original kernel matrix.

torch.Size([32327, 2])
We keep 7.54e+06/5.10e+08 =  1% of the original kernel matrix.

torch.Size([13662, 2])
We keep 2.55e+06/7.15e+07 =  3% of the original kernel matrix.

torch.Size([20917, 2])
We keep 3.78e+06/2.00e+08 =  1% of the original kernel matrix.

torch.Size([42885, 2])
We keep 1.98e+07/1.06e+09 =  1% of the original kernel matrix.

torch.Size([38476, 2])
We keep 1.07e+07/7.70e+08 =  1% of the original kernel matrix.

torch.Size([9578, 2])
We keep 1.93e+06/3.26e+07 =  5% of the original kernel matrix.

torch.Size([18128, 2])
We keep 2.71e+06/1.35e+08 =  2% of the original kernel matrix.

torch.Size([88872, 2])
We keep 9.01e+07/3.84e+09 =  2% of the original kernel matrix.

torch.Size([54102, 2])
We keep 1.83e+07/1.46e+09 =  1% of the original kernel matrix.

torch.Size([549109, 2])
We keep 1.21e+09/1.51e+11 =  0% of the original kernel matrix.

torch.Size([141294, 2])
We keep 9.17e+07/9.19e+09 =  0% of the original kernel matrix.

torch.Size([42606, 2])
We keep 4.78e+07/1.10e+09 =  4% of the original kernel matrix.

torch.Size([38244, 2])
We keep 1.09e+07/7.84e+08 =  1% of the original kernel matrix.

torch.Size([22187, 2])
We keep 2.65e+07/5.55e+08 =  4% of the original kernel matrix.

torch.Size([26151, 2])
We keep 8.29e+06/5.57e+08 =  1% of the original kernel matrix.

torch.Size([18968, 2])
We keep 9.09e+06/2.48e+08 =  3% of the original kernel matrix.

torch.Size([24658, 2])
We keep 6.18e+06/3.72e+08 =  1% of the original kernel matrix.

torch.Size([4998, 2])
We keep 6.02e+05/9.24e+06 =  6% of the original kernel matrix.

torch.Size([13283, 2])
We keep 1.69e+06/7.18e+07 =  2% of the original kernel matrix.

torch.Size([25945, 2])
We keep 6.03e+06/3.33e+08 =  1% of the original kernel matrix.

torch.Size([30291, 2])
We keep 6.90e+06/4.31e+08 =  1% of the original kernel matrix.

torch.Size([11733, 2])
We keep 6.70e+06/9.14e+07 =  7% of the original kernel matrix.

torch.Size([19093, 2])
We keep 4.21e+06/2.26e+08 =  1% of the original kernel matrix.

torch.Size([48439, 2])
We keep 6.00e+07/1.59e+09 =  3% of the original kernel matrix.

torch.Size([40430, 2])
We keep 1.28e+07/9.42e+08 =  1% of the original kernel matrix.

torch.Size([130934, 2])
We keep 8.88e+07/8.56e+09 =  1% of the original kernel matrix.

torch.Size([66343, 2])
We keep 2.58e+07/2.19e+09 =  1% of the original kernel matrix.

torch.Size([3703, 2])
We keep 3.68e+05/4.24e+06 =  8% of the original kernel matrix.

torch.Size([11763, 2])
We keep 1.37e+06/4.87e+07 =  2% of the original kernel matrix.

torch.Size([69255, 2])
We keep 4.16e+08/5.43e+09 =  7% of the original kernel matrix.

torch.Size([46369, 2])
We keep 2.01e+07/1.74e+09 =  1% of the original kernel matrix.

torch.Size([57126, 2])
We keep 2.76e+08/2.66e+09 = 10% of the original kernel matrix.

torch.Size([43212, 2])
We keep 1.50e+07/1.22e+09 =  1% of the original kernel matrix.

torch.Size([28149, 2])
We keep 7.91e+06/3.87e+08 =  2% of the original kernel matrix.

torch.Size([30935, 2])
We keep 7.29e+06/4.65e+08 =  1% of the original kernel matrix.

torch.Size([3162, 2])
We keep 1.20e+05/2.03e+06 =  5% of the original kernel matrix.

torch.Size([11439, 2])
We keep 1.04e+06/3.36e+07 =  3% of the original kernel matrix.

torch.Size([55940, 2])
We keep 2.72e+07/1.67e+09 =  1% of the original kernel matrix.

torch.Size([43757, 2])
We keep 1.30e+07/9.64e+08 =  1% of the original kernel matrix.

torch.Size([73973, 2])
We keep 3.44e+07/2.74e+09 =  1% of the original kernel matrix.

torch.Size([49792, 2])
We keep 1.59e+07/1.24e+09 =  1% of the original kernel matrix.

torch.Size([3854, 2])
We keep 1.76e+05/3.17e+06 =  5% of the original kernel matrix.

torch.Size([12270, 2])
We keep 1.22e+06/4.21e+07 =  2% of the original kernel matrix.

torch.Size([474936, 2])
We keep 7.79e+08/1.11e+11 =  0% of the original kernel matrix.

torch.Size([130795, 2])
We keep 7.94e+07/7.88e+09 =  1% of the original kernel matrix.

torch.Size([36582, 2])
We keep 2.84e+07/9.64e+08 =  2% of the original kernel matrix.

torch.Size([35011, 2])
We keep 1.03e+07/7.34e+08 =  1% of the original kernel matrix.

torch.Size([15737, 2])
We keep 2.76e+06/8.51e+07 =  3% of the original kernel matrix.

torch.Size([22602, 2])
We keep 4.04e+06/2.18e+08 =  1% of the original kernel matrix.

torch.Size([6203, 2])
We keep 4.77e+05/9.70e+06 =  4% of the original kernel matrix.

torch.Size([14936, 2])
We keep 1.76e+06/7.36e+07 =  2% of the original kernel matrix.

torch.Size([4601, 2])
We keep 2.25e+05/4.24e+06 =  5% of the original kernel matrix.

torch.Size([13114, 2])
We keep 1.35e+06/4.87e+07 =  2% of the original kernel matrix.

torch.Size([2169, 2])
We keep 6.29e+04/8.65e+05 =  7% of the original kernel matrix.

torch.Size([9889, 2])
We keep 7.79e+05/2.20e+07 =  3% of the original kernel matrix.

torch.Size([10370, 2])
We keep 2.68e+06/4.01e+07 =  6% of the original kernel matrix.

torch.Size([18461, 2])
We keep 3.05e+06/1.50e+08 =  2% of the original kernel matrix.

torch.Size([7763, 2])
We keep 7.77e+05/1.59e+07 =  4% of the original kernel matrix.

torch.Size([16508, 2])
We keep 2.15e+06/9.41e+07 =  2% of the original kernel matrix.

torch.Size([49263, 2])
We keep 2.83e+07/1.45e+09 =  1% of the original kernel matrix.

torch.Size([40456, 2])
We keep 1.24e+07/9.00e+08 =  1% of the original kernel matrix.

torch.Size([211143, 2])
We keep 8.76e+09/1.36e+11 =  6% of the original kernel matrix.

torch.Size([72943, 2])
We keep 8.57e+07/8.70e+09 =  0% of the original kernel matrix.

torch.Size([6971, 2])
We keep 1.02e+06/1.57e+07 =  6% of the original kernel matrix.

torch.Size([15663, 2])
We keep 2.18e+06/9.36e+07 =  2% of the original kernel matrix.

torch.Size([35684, 2])
We keep 1.12e+07/6.70e+08 =  1% of the original kernel matrix.

torch.Size([35325, 2])
We keep 9.01e+06/6.12e+08 =  1% of the original kernel matrix.

torch.Size([2989, 2])
We keep 1.02e+05/1.60e+06 =  6% of the original kernel matrix.

torch.Size([11327, 2])
We keep 9.65e+05/2.99e+07 =  3% of the original kernel matrix.

torch.Size([13179, 2])
We keep 1.77e+06/5.50e+07 =  3% of the original kernel matrix.

torch.Size([20596, 2])
We keep 3.35e+06/1.75e+08 =  1% of the original kernel matrix.

torch.Size([8535, 2])
We keep 8.31e+05/2.04e+07 =  4% of the original kernel matrix.

torch.Size([16925, 2])
We keep 2.38e+06/1.07e+08 =  2% of the original kernel matrix.

torch.Size([39262, 2])
We keep 2.38e+08/2.33e+09 = 10% of the original kernel matrix.

torch.Size([34457, 2])
We keep 1.50e+07/1.14e+09 =  1% of the original kernel matrix.

torch.Size([15817, 2])
We keep 8.14e+06/1.34e+08 =  6% of the original kernel matrix.

torch.Size([22389, 2])
We keep 4.67e+06/2.74e+08 =  1% of the original kernel matrix.

torch.Size([639650, 2])
We keep 1.47e+09/2.00e+11 =  0% of the original kernel matrix.

torch.Size([153120, 2])
We keep 1.04e+08/1.06e+10 =  0% of the original kernel matrix.

torch.Size([4733, 2])
We keep 2.68e+05/5.59e+06 =  4% of the original kernel matrix.

torch.Size([13351, 2])
We keep 1.50e+06/5.59e+07 =  2% of the original kernel matrix.

torch.Size([67050, 2])
We keep 3.92e+07/2.32e+09 =  1% of the original kernel matrix.

torch.Size([47833, 2])
We keep 1.48e+07/1.14e+09 =  1% of the original kernel matrix.

torch.Size([9801, 2])
We keep 1.73e+06/3.00e+07 =  5% of the original kernel matrix.

torch.Size([18282, 2])
We keep 2.69e+06/1.29e+08 =  2% of the original kernel matrix.

torch.Size([72699, 2])
We keep 4.65e+07/3.01e+09 =  1% of the original kernel matrix.

torch.Size([49015, 2])
We keep 1.68e+07/1.30e+09 =  1% of the original kernel matrix.

torch.Size([202375, 2])
We keep 2.98e+08/2.73e+10 =  1% of the original kernel matrix.

torch.Size([84190, 2])
We keep 4.22e+07/3.90e+09 =  1% of the original kernel matrix.

torch.Size([15548, 2])
We keep 2.39e+06/8.26e+07 =  2% of the original kernel matrix.

torch.Size([22424, 2])
We keep 3.93e+06/2.15e+08 =  1% of the original kernel matrix.

torch.Size([4074, 2])
We keep 2.25e+05/3.72e+06 =  6% of the original kernel matrix.

torch.Size([12648, 2])
We keep 1.31e+06/4.56e+07 =  2% of the original kernel matrix.

torch.Size([52182, 2])
We keep 2.21e+07/1.62e+09 =  1% of the original kernel matrix.

torch.Size([42492, 2])
We keep 1.29e+07/9.52e+08 =  1% of the original kernel matrix.

torch.Size([11228, 2])
We keep 3.46e+06/5.72e+07 =  6% of the original kernel matrix.

torch.Size([19109, 2])
We keep 3.48e+06/1.79e+08 =  1% of the original kernel matrix.

torch.Size([14503, 2])
We keep 6.83e+06/1.59e+08 =  4% of the original kernel matrix.

torch.Size([20801, 2])
We keep 5.14e+06/2.98e+08 =  1% of the original kernel matrix.

torch.Size([65079, 2])
We keep 3.18e+07/2.21e+09 =  1% of the original kernel matrix.

torch.Size([46614, 2])
We keep 1.46e+07/1.11e+09 =  1% of the original kernel matrix.

torch.Size([9737, 2])
We keep 7.92e+06/1.22e+08 =  6% of the original kernel matrix.

torch.Size([17392, 2])
We keep 4.16e+06/2.61e+08 =  1% of the original kernel matrix.

torch.Size([28093, 2])
We keep 7.27e+06/3.93e+08 =  1% of the original kernel matrix.

torch.Size([31891, 2])
We keep 7.34e+06/4.68e+08 =  1% of the original kernel matrix.

torch.Size([14097, 2])
We keep 2.00e+06/6.18e+07 =  3% of the original kernel matrix.

torch.Size([21371, 2])
We keep 3.44e+06/1.86e+08 =  1% of the original kernel matrix.

torch.Size([43566, 2])
We keep 2.09e+07/1.22e+09 =  1% of the original kernel matrix.

torch.Size([39393, 2])
We keep 1.16e+07/8.24e+08 =  1% of the original kernel matrix.

torch.Size([122015, 2])
We keep 2.48e+08/1.12e+10 =  2% of the original kernel matrix.

torch.Size([63165, 2])
We keep 2.87e+07/2.49e+09 =  1% of the original kernel matrix.

torch.Size([15315, 2])
We keep 2.62e+06/7.90e+07 =  3% of the original kernel matrix.

torch.Size([22411, 2])
We keep 3.88e+06/2.10e+08 =  1% of the original kernel matrix.

torch.Size([4597, 2])
We keep 2.85e+05/5.22e+06 =  5% of the original kernel matrix.

torch.Size([13091, 2])
We keep 1.45e+06/5.40e+07 =  2% of the original kernel matrix.

torch.Size([69413, 2])
We keep 8.65e+07/3.87e+09 =  2% of the original kernel matrix.

torch.Size([46956, 2])
We keep 1.86e+07/1.47e+09 =  1% of the original kernel matrix.

torch.Size([24795, 2])
We keep 5.89e+06/2.95e+08 =  1% of the original kernel matrix.

torch.Size([28702, 2])
We keep 6.39e+06/4.06e+08 =  1% of the original kernel matrix.

torch.Size([85782, 2])
We keep 1.29e+08/4.00e+09 =  3% of the original kernel matrix.

torch.Size([52605, 2])
We keep 1.82e+07/1.49e+09 =  1% of the original kernel matrix.

torch.Size([14090, 2])
We keep 4.35e+06/9.26e+07 =  4% of the original kernel matrix.

torch.Size([21357, 2])
We keep 3.92e+06/2.27e+08 =  1% of the original kernel matrix.

torch.Size([41993, 2])
We keep 7.62e+07/1.32e+09 =  5% of the original kernel matrix.

torch.Size([36306, 2])
We keep 1.19e+07/8.59e+08 =  1% of the original kernel matrix.

torch.Size([19187, 2])
We keep 5.66e+06/1.75e+08 =  3% of the original kernel matrix.

torch.Size([25096, 2])
We keep 5.26e+06/3.13e+08 =  1% of the original kernel matrix.

torch.Size([42493, 2])
We keep 7.06e+07/2.02e+09 =  3% of the original kernel matrix.

torch.Size([35213, 2])
We keep 1.40e+07/1.06e+09 =  1% of the original kernel matrix.

torch.Size([23821, 2])
We keep 1.20e+07/3.38e+08 =  3% of the original kernel matrix.

torch.Size([28237, 2])
We keep 6.77e+06/4.34e+08 =  1% of the original kernel matrix.

torch.Size([14919, 2])
We keep 2.93e+06/8.13e+07 =  3% of the original kernel matrix.

torch.Size([21987, 2])
We keep 3.95e+06/2.13e+08 =  1% of the original kernel matrix.

torch.Size([3748, 2])
We keep 1.36e+05/2.34e+06 =  5% of the original kernel matrix.

torch.Size([12227, 2])
We keep 1.09e+06/3.62e+07 =  3% of the original kernel matrix.

torch.Size([17129, 2])
We keep 3.41e+06/1.08e+08 =  3% of the original kernel matrix.

torch.Size([23804, 2])
We keep 4.46e+06/2.46e+08 =  1% of the original kernel matrix.

torch.Size([16754, 2])
We keep 1.18e+07/1.40e+08 =  8% of the original kernel matrix.

torch.Size([23180, 2])
We keep 4.83e+06/2.80e+08 =  1% of the original kernel matrix.

torch.Size([61616, 2])
We keep 8.48e+07/2.28e+09 =  3% of the original kernel matrix.

torch.Size([45038, 2])
We keep 1.49e+07/1.13e+09 =  1% of the original kernel matrix.

torch.Size([80733, 2])
We keep 5.49e+07/3.31e+09 =  1% of the original kernel matrix.

torch.Size([51809, 2])
We keep 1.70e+07/1.36e+09 =  1% of the original kernel matrix.

torch.Size([6062, 2])
We keep 4.37e+05/9.41e+06 =  4% of the original kernel matrix.

torch.Size([14625, 2])
We keep 1.81e+06/7.25e+07 =  2% of the original kernel matrix.

torch.Size([21369, 2])
We keep 4.00e+06/2.01e+08 =  1% of the original kernel matrix.

torch.Size([26755, 2])
We keep 5.50e+06/3.35e+08 =  1% of the original kernel matrix.

torch.Size([29563, 2])
We keep 1.34e+07/5.09e+08 =  2% of the original kernel matrix.

torch.Size([31466, 2])
We keep 8.22e+06/5.33e+08 =  1% of the original kernel matrix.

torch.Size([162011, 2])
We keep 3.09e+08/1.47e+10 =  2% of the original kernel matrix.

torch.Size([74664, 2])
We keep 3.22e+07/2.86e+09 =  1% of the original kernel matrix.

torch.Size([31551, 2])
We keep 1.14e+07/5.37e+08 =  2% of the original kernel matrix.

torch.Size([33106, 2])
We keep 8.31e+06/5.47e+08 =  1% of the original kernel matrix.

torch.Size([42961, 2])
We keep 1.44e+07/9.67e+08 =  1% of the original kernel matrix.

torch.Size([39012, 2])
We keep 1.01e+07/7.35e+08 =  1% of the original kernel matrix.

torch.Size([25626, 2])
We keep 1.97e+07/5.29e+08 =  3% of the original kernel matrix.

torch.Size([29319, 2])
We keep 8.39e+06/5.44e+08 =  1% of the original kernel matrix.

torch.Size([21524, 2])
We keep 4.68e+06/1.88e+08 =  2% of the original kernel matrix.

torch.Size([26821, 2])
We keep 5.29e+06/3.24e+08 =  1% of the original kernel matrix.

torch.Size([2179, 2])
We keep 7.07e+04/9.58e+05 =  7% of the original kernel matrix.

torch.Size([9966, 2])
We keep 8.25e+05/2.31e+07 =  3% of the original kernel matrix.

torch.Size([8079, 2])
We keep 6.40e+05/1.68e+07 =  3% of the original kernel matrix.

torch.Size([16489, 2])
We keep 2.16e+06/9.70e+07 =  2% of the original kernel matrix.

torch.Size([70272, 2])
We keep 3.21e+07/2.57e+09 =  1% of the original kernel matrix.

torch.Size([48749, 2])
We keep 1.54e+07/1.20e+09 =  1% of the original kernel matrix.

torch.Size([41817, 2])
We keep 2.13e+07/1.08e+09 =  1% of the original kernel matrix.

torch.Size([36915, 2])
We keep 1.09e+07/7.77e+08 =  1% of the original kernel matrix.

torch.Size([47071, 2])
We keep 1.83e+07/1.14e+09 =  1% of the original kernel matrix.

torch.Size([40703, 2])
We keep 1.11e+07/7.98e+08 =  1% of the original kernel matrix.

torch.Size([2570, 2])
We keep 7.19e+04/1.12e+06 =  6% of the original kernel matrix.

torch.Size([10733, 2])
We keep 8.68e+05/2.50e+07 =  3% of the original kernel matrix.

torch.Size([5792, 2])
We keep 3.51e+05/7.75e+06 =  4% of the original kernel matrix.

torch.Size([14565, 2])
We keep 1.69e+06/6.58e+07 =  2% of the original kernel matrix.

torch.Size([104293, 2])
We keep 1.55e+08/7.24e+09 =  2% of the original kernel matrix.

torch.Size([58147, 2])
We keep 2.46e+07/2.01e+09 =  1% of the original kernel matrix.

torch.Size([41060, 2])
We keep 1.73e+07/9.48e+08 =  1% of the original kernel matrix.

torch.Size([37482, 2])
We keep 1.04e+07/7.28e+08 =  1% of the original kernel matrix.

torch.Size([18954, 2])
We keep 3.29e+06/1.31e+08 =  2% of the original kernel matrix.

torch.Size([25129, 2])
We keep 4.72e+06/2.70e+08 =  1% of the original kernel matrix.

torch.Size([2756, 2])
We keep 8.59e+04/1.41e+06 =  6% of the original kernel matrix.

torch.Size([10997, 2])
We keep 9.31e+05/2.80e+07 =  3% of the original kernel matrix.

torch.Size([1569, 2])
We keep 3.84e+04/4.19e+05 =  9% of the original kernel matrix.

torch.Size([8800, 2])
We keep 6.42e+05/1.53e+07 =  4% of the original kernel matrix.

torch.Size([24921, 2])
We keep 7.58e+06/2.96e+08 =  2% of the original kernel matrix.

torch.Size([29114, 2])
We keep 6.51e+06/4.06e+08 =  1% of the original kernel matrix.

torch.Size([39433, 2])
We keep 2.01e+07/8.93e+08 =  2% of the original kernel matrix.

torch.Size([36554, 2])
We keep 1.01e+07/7.06e+08 =  1% of the original kernel matrix.

torch.Size([34373, 2])
We keep 2.50e+07/7.86e+08 =  3% of the original kernel matrix.

torch.Size([33962, 2])
We keep 9.67e+06/6.62e+08 =  1% of the original kernel matrix.

torch.Size([3839, 2])
We keep 2.73e+05/4.18e+06 =  6% of the original kernel matrix.

torch.Size([12206, 2])
We keep 1.30e+06/4.83e+07 =  2% of the original kernel matrix.

torch.Size([60835, 2])
We keep 6.08e+08/9.22e+09 =  6% of the original kernel matrix.

torch.Size([42199, 2])
We keep 2.74e+07/2.27e+09 =  1% of the original kernel matrix.

torch.Size([11531, 2])
We keep 3.18e+06/6.34e+07 =  5% of the original kernel matrix.

torch.Size([18996, 2])
We keep 3.67e+06/1.88e+08 =  1% of the original kernel matrix.

torch.Size([7079, 2])
We keep 7.90e+05/1.50e+07 =  5% of the original kernel matrix.

torch.Size([15322, 2])
We keep 2.17e+06/9.14e+07 =  2% of the original kernel matrix.

torch.Size([174341, 2])
We keep 2.76e+08/1.97e+10 =  1% of the original kernel matrix.

torch.Size([77520, 2])
We keep 3.69e+07/3.32e+09 =  1% of the original kernel matrix.

torch.Size([13196, 2])
We keep 1.55e+07/1.85e+08 =  8% of the original kernel matrix.

torch.Size([20381, 2])
We keep 5.40e+06/3.21e+08 =  1% of the original kernel matrix.

torch.Size([72563, 2])
We keep 5.12e+07/3.10e+09 =  1% of the original kernel matrix.

torch.Size([48787, 2])
We keep 1.67e+07/1.32e+09 =  1% of the original kernel matrix.

torch.Size([84274, 2])
We keep 5.58e+07/3.44e+09 =  1% of the original kernel matrix.

torch.Size([52963, 2])
We keep 1.76e+07/1.38e+09 =  1% of the original kernel matrix.

torch.Size([35867, 2])
We keep 1.10e+07/6.61e+08 =  1% of the original kernel matrix.

torch.Size([35316, 2])
We keep 8.94e+06/6.08e+08 =  1% of the original kernel matrix.

torch.Size([53058, 2])
We keep 3.38e+07/1.56e+09 =  2% of the original kernel matrix.

torch.Size([43091, 2])
We keep 1.27e+07/9.34e+08 =  1% of the original kernel matrix.

torch.Size([102375, 2])
We keep 6.13e+07/5.07e+09 =  1% of the original kernel matrix.

torch.Size([57977, 2])
We keep 2.03e+07/1.68e+09 =  1% of the original kernel matrix.

torch.Size([2230, 2])
We keep 8.90e+04/1.13e+06 =  7% of the original kernel matrix.

torch.Size([9861, 2])
We keep 8.68e+05/2.51e+07 =  3% of the original kernel matrix.

torch.Size([34617, 2])
We keep 2.83e+07/6.47e+08 =  4% of the original kernel matrix.

torch.Size([34431, 2])
We keep 8.75e+06/6.01e+08 =  1% of the original kernel matrix.

torch.Size([14873, 2])
We keep 2.35e+06/7.30e+07 =  3% of the original kernel matrix.

torch.Size([22054, 2])
We keep 3.80e+06/2.02e+08 =  1% of the original kernel matrix.

torch.Size([155181, 2])
We keep 1.63e+08/1.25e+10 =  1% of the original kernel matrix.

torch.Size([73014, 2])
We keep 3.03e+07/2.64e+09 =  1% of the original kernel matrix.

torch.Size([127147, 2])
We keep 1.15e+08/9.16e+09 =  1% of the original kernel matrix.

torch.Size([65424, 2])
We keep 2.63e+07/2.26e+09 =  1% of the original kernel matrix.

torch.Size([19096, 2])
We keep 7.59e+06/2.52e+08 =  3% of the original kernel matrix.

torch.Size([24610, 2])
We keep 6.23e+06/3.75e+08 =  1% of the original kernel matrix.

torch.Size([32605, 2])
We keep 8.55e+06/5.36e+08 =  1% of the original kernel matrix.

torch.Size([33751, 2])
We keep 8.17e+06/5.47e+08 =  1% of the original kernel matrix.

torch.Size([9834, 2])
We keep 9.22e+05/2.84e+07 =  3% of the original kernel matrix.

torch.Size([18252, 2])
We keep 2.61e+06/1.26e+08 =  2% of the original kernel matrix.

torch.Size([4127, 2])
We keep 1.62e+06/1.01e+07 = 16% of the original kernel matrix.

torch.Size([12007, 2])
We keep 1.84e+06/7.50e+07 =  2% of the original kernel matrix.

torch.Size([14849, 2])
We keep 3.74e+07/1.46e+08 = 25% of the original kernel matrix.

torch.Size([21959, 2])
We keep 5.06e+06/2.86e+08 =  1% of the original kernel matrix.

torch.Size([9788, 2])
We keep 1.10e+06/2.94e+07 =  3% of the original kernel matrix.

torch.Size([17701, 2])
We keep 2.71e+06/1.28e+08 =  2% of the original kernel matrix.

torch.Size([163368, 2])
We keep 1.57e+08/1.39e+10 =  1% of the original kernel matrix.

torch.Size([75051, 2])
We keep 3.19e+07/2.78e+09 =  1% of the original kernel matrix.

torch.Size([97994, 2])
We keep 2.11e+08/5.58e+09 =  3% of the original kernel matrix.

torch.Size([57356, 2])
We keep 2.14e+07/1.76e+09 =  1% of the original kernel matrix.

torch.Size([4067, 2])
We keep 3.85e+05/5.12e+06 =  7% of the original kernel matrix.

torch.Size([12253, 2])
We keep 1.44e+06/5.34e+07 =  2% of the original kernel matrix.

torch.Size([11929, 2])
We keep 1.58e+06/4.71e+07 =  3% of the original kernel matrix.

torch.Size([19847, 2])
We keep 3.15e+06/1.62e+08 =  1% of the original kernel matrix.

torch.Size([11195, 2])
We keep 1.15e+06/3.60e+07 =  3% of the original kernel matrix.

torch.Size([19218, 2])
We keep 2.87e+06/1.42e+08 =  2% of the original kernel matrix.

torch.Size([222535, 2])
We keep 3.50e+08/3.03e+10 =  1% of the original kernel matrix.

torch.Size([88704, 2])
We keep 4.59e+07/4.11e+09 =  1% of the original kernel matrix.

torch.Size([11672, 2])
We keep 1.27e+06/3.86e+07 =  3% of the original kernel matrix.

torch.Size([19390, 2])
We keep 2.91e+06/1.47e+08 =  1% of the original kernel matrix.

torch.Size([17188, 2])
We keep 3.79e+06/1.15e+08 =  3% of the original kernel matrix.

torch.Size([23648, 2])
We keep 4.51e+06/2.53e+08 =  1% of the original kernel matrix.

torch.Size([59067, 2])
We keep 3.48e+07/1.94e+09 =  1% of the original kernel matrix.

torch.Size([44839, 2])
We keep 1.40e+07/1.04e+09 =  1% of the original kernel matrix.

torch.Size([55986, 2])
We keep 2.24e+07/1.63e+09 =  1% of the original kernel matrix.

torch.Size([44097, 2])
We keep 1.28e+07/9.55e+08 =  1% of the original kernel matrix.

torch.Size([10799, 2])
We keep 1.45e+06/3.71e+07 =  3% of the original kernel matrix.

torch.Size([18698, 2])
We keep 2.96e+06/1.44e+08 =  2% of the original kernel matrix.

torch.Size([24202, 2])
We keep 6.89e+06/2.62e+08 =  2% of the original kernel matrix.

torch.Size([28794, 2])
We keep 6.27e+06/3.82e+08 =  1% of the original kernel matrix.

torch.Size([17780, 2])
We keep 5.15e+06/1.32e+08 =  3% of the original kernel matrix.

torch.Size([24218, 2])
We keep 4.76e+06/2.72e+08 =  1% of the original kernel matrix.

time for making ranges is 2.721248149871826
Sorting X and nu_X
time for sorting X is 0.06587862968444824
Sorting Z and nu_Z
time for sorting Z is 0.0002608299255371094
Starting Optim
sum tnu_Z before tensor(21131536., device='cuda:0')
c= tensor(445.5976, device='cuda:0')
c= tensor(20172.1465, device='cuda:0')
c= tensor(24394.1113, device='cuda:0')
c= tensor(25883.4277, device='cuda:0')
c= tensor(180515.2344, device='cuda:0')
c= tensor(230156.5469, device='cuda:0')
c= tensor(637871.9375, device='cuda:0')
c= tensor(762217.3750, device='cuda:0')
c= tensor(784626.3125, device='cuda:0')
c= tensor(1733544., device='cuda:0')
c= tensor(1742662.6250, device='cuda:0')
c= tensor(2958325.5000, device='cuda:0')
c= tensor(2963403.2500, device='cuda:0')
c= tensor(6436498., device='cuda:0')
c= tensor(6548359., device='cuda:0')
c= tensor(6631861., device='cuda:0')
c= tensor(6880960., device='cuda:0')
c= tensor(7858309., device='cuda:0')
c= tensor(11343056., device='cuda:0')
c= tensor(13377967., device='cuda:0')
c= tensor(13386474., device='cuda:0')
c= tensor(17693160., device='cuda:0')
c= tensor(17725560., device='cuda:0')
c= tensor(17742658., device='cuda:0')
c= tensor(19032146., device='cuda:0')
c= tensor(19609614., device='cuda:0')
c= tensor(20112764., device='cuda:0')
c= tensor(20150220., device='cuda:0')
c= tensor(21077712., device='cuda:0')
c= tensor(1.4992e+08, device='cuda:0')
c= tensor(1.4993e+08, device='cuda:0')
c= tensor(1.6726e+08, device='cuda:0')
c= tensor(1.6727e+08, device='cuda:0')
c= tensor(1.6728e+08, device='cuda:0')
c= tensor(1.6728e+08, device='cuda:0')
c= tensor(1.6798e+08, device='cuda:0')
c= tensor(1.6840e+08, device='cuda:0')
c= tensor(1.6841e+08, device='cuda:0')
c= tensor(1.6842e+08, device='cuda:0')
c= tensor(1.6842e+08, device='cuda:0')
c= tensor(1.6842e+08, device='cuda:0')
c= tensor(1.6842e+08, device='cuda:0')
c= tensor(1.6842e+08, device='cuda:0')
c= tensor(1.6842e+08, device='cuda:0')
c= tensor(1.6842e+08, device='cuda:0')
c= tensor(1.6843e+08, device='cuda:0')
c= tensor(1.6843e+08, device='cuda:0')
c= tensor(1.6843e+08, device='cuda:0')
c= tensor(1.6844e+08, device='cuda:0')
c= tensor(1.6844e+08, device='cuda:0')
c= tensor(1.6845e+08, device='cuda:0')
c= tensor(1.6845e+08, device='cuda:0')
c= tensor(1.6846e+08, device='cuda:0')
c= tensor(1.6846e+08, device='cuda:0')
c= tensor(1.6847e+08, device='cuda:0')
c= tensor(1.6847e+08, device='cuda:0')
c= tensor(1.6847e+08, device='cuda:0')
c= tensor(1.6848e+08, device='cuda:0')
c= tensor(1.6848e+08, device='cuda:0')
c= tensor(1.6848e+08, device='cuda:0')
c= tensor(1.6849e+08, device='cuda:0')
c= tensor(1.6849e+08, device='cuda:0')
c= tensor(1.6850e+08, device='cuda:0')
c= tensor(1.6851e+08, device='cuda:0')
c= tensor(1.6851e+08, device='cuda:0')
c= tensor(1.6851e+08, device='cuda:0')
c= tensor(1.6851e+08, device='cuda:0')
c= tensor(1.6851e+08, device='cuda:0')
c= tensor(1.6851e+08, device='cuda:0')
c= tensor(1.6852e+08, device='cuda:0')
c= tensor(1.6852e+08, device='cuda:0')
c= tensor(1.6852e+08, device='cuda:0')
c= tensor(1.6852e+08, device='cuda:0')
c= tensor(1.6852e+08, device='cuda:0')
c= tensor(1.6853e+08, device='cuda:0')
c= tensor(1.6853e+08, device='cuda:0')
c= tensor(1.6853e+08, device='cuda:0')
c= tensor(1.6853e+08, device='cuda:0')
c= tensor(1.6854e+08, device='cuda:0')
c= tensor(1.6854e+08, device='cuda:0')
c= tensor(1.6854e+08, device='cuda:0')
c= tensor(1.6854e+08, device='cuda:0')
c= tensor(1.6855e+08, device='cuda:0')
c= tensor(1.6855e+08, device='cuda:0')
c= tensor(1.6856e+08, device='cuda:0')
c= tensor(1.6856e+08, device='cuda:0')
c= tensor(1.6856e+08, device='cuda:0')
c= tensor(1.6856e+08, device='cuda:0')
c= tensor(1.6856e+08, device='cuda:0')
c= tensor(1.6857e+08, device='cuda:0')
c= tensor(1.6857e+08, device='cuda:0')
c= tensor(1.6857e+08, device='cuda:0')
c= tensor(1.6857e+08, device='cuda:0')
c= tensor(1.6857e+08, device='cuda:0')
c= tensor(1.6857e+08, device='cuda:0')
c= tensor(1.6858e+08, device='cuda:0')
c= tensor(1.6858e+08, device='cuda:0')
c= tensor(1.6858e+08, device='cuda:0')
c= tensor(1.6859e+08, device='cuda:0')
c= tensor(1.6859e+08, device='cuda:0')
c= tensor(1.6862e+08, device='cuda:0')
c= tensor(1.6862e+08, device='cuda:0')
c= tensor(1.6862e+08, device='cuda:0')
c= tensor(1.6862e+08, device='cuda:0')
c= tensor(1.6862e+08, device='cuda:0')
c= tensor(1.6863e+08, device='cuda:0')
c= tensor(1.6863e+08, device='cuda:0')
c= tensor(1.6863e+08, device='cuda:0')
c= tensor(1.6863e+08, device='cuda:0')
c= tensor(1.6863e+08, device='cuda:0')
c= tensor(1.6863e+08, device='cuda:0')
c= tensor(1.6864e+08, device='cuda:0')
c= tensor(1.6864e+08, device='cuda:0')
c= tensor(1.6864e+08, device='cuda:0')
c= tensor(1.6864e+08, device='cuda:0')
c= tensor(1.6864e+08, device='cuda:0')
c= tensor(1.6864e+08, device='cuda:0')
c= tensor(1.6864e+08, device='cuda:0')
c= tensor(1.6865e+08, device='cuda:0')
c= tensor(1.6865e+08, device='cuda:0')
c= tensor(1.6866e+08, device='cuda:0')
c= tensor(1.6866e+08, device='cuda:0')
c= tensor(1.6866e+08, device='cuda:0')
c= tensor(1.6867e+08, device='cuda:0')
c= tensor(1.6867e+08, device='cuda:0')
c= tensor(1.6867e+08, device='cuda:0')
c= tensor(1.6867e+08, device='cuda:0')
c= tensor(1.6867e+08, device='cuda:0')
c= tensor(1.6872e+08, device='cuda:0')
c= tensor(1.6872e+08, device='cuda:0')
c= tensor(1.6872e+08, device='cuda:0')
c= tensor(1.6872e+08, device='cuda:0')
c= tensor(1.6873e+08, device='cuda:0')
c= tensor(1.6873e+08, device='cuda:0')
c= tensor(1.6873e+08, device='cuda:0')
c= tensor(1.6873e+08, device='cuda:0')
c= tensor(1.6873e+08, device='cuda:0')
c= tensor(1.6873e+08, device='cuda:0')
c= tensor(1.6873e+08, device='cuda:0')
c= tensor(1.6873e+08, device='cuda:0')
c= tensor(1.6874e+08, device='cuda:0')
c= tensor(1.6874e+08, device='cuda:0')
c= tensor(1.6875e+08, device='cuda:0')
c= tensor(1.6876e+08, device='cuda:0')
c= tensor(1.6877e+08, device='cuda:0')
c= tensor(1.6877e+08, device='cuda:0')
c= tensor(1.6877e+08, device='cuda:0')
c= tensor(1.6877e+08, device='cuda:0')
c= tensor(1.6877e+08, device='cuda:0')
c= tensor(1.6877e+08, device='cuda:0')
c= tensor(1.6878e+08, device='cuda:0')
c= tensor(1.6878e+08, device='cuda:0')
c= tensor(1.6879e+08, device='cuda:0')
c= tensor(1.6879e+08, device='cuda:0')
c= tensor(1.6880e+08, device='cuda:0')
c= tensor(1.6881e+08, device='cuda:0')
c= tensor(1.6881e+08, device='cuda:0')
c= tensor(1.6881e+08, device='cuda:0')
c= tensor(1.6882e+08, device='cuda:0')
c= tensor(1.6882e+08, device='cuda:0')
c= tensor(1.6882e+08, device='cuda:0')
c= tensor(1.6882e+08, device='cuda:0')
c= tensor(1.6882e+08, device='cuda:0')
c= tensor(1.6882e+08, device='cuda:0')
c= tensor(1.6882e+08, device='cuda:0')
c= tensor(1.6882e+08, device='cuda:0')
c= tensor(1.6883e+08, device='cuda:0')
c= tensor(1.6883e+08, device='cuda:0')
c= tensor(1.6883e+08, device='cuda:0')
c= tensor(1.6883e+08, device='cuda:0')
c= tensor(1.6883e+08, device='cuda:0')
c= tensor(1.6883e+08, device='cuda:0')
c= tensor(1.6884e+08, device='cuda:0')
c= tensor(1.6884e+08, device='cuda:0')
c= tensor(1.6884e+08, device='cuda:0')
c= tensor(1.6884e+08, device='cuda:0')
c= tensor(1.6884e+08, device='cuda:0')
c= tensor(1.6885e+08, device='cuda:0')
c= tensor(1.6885e+08, device='cuda:0')
c= tensor(1.6885e+08, device='cuda:0')
c= tensor(1.6886e+08, device='cuda:0')
c= tensor(1.6886e+08, device='cuda:0')
c= tensor(1.6886e+08, device='cuda:0')
c= tensor(1.6886e+08, device='cuda:0')
c= tensor(1.6887e+08, device='cuda:0')
c= tensor(1.6887e+08, device='cuda:0')
c= tensor(1.6887e+08, device='cuda:0')
c= tensor(1.6888e+08, device='cuda:0')
c= tensor(1.6889e+08, device='cuda:0')
c= tensor(1.6890e+08, device='cuda:0')
c= tensor(1.6890e+08, device='cuda:0')
c= tensor(1.6890e+08, device='cuda:0')
c= tensor(1.6890e+08, device='cuda:0')
c= tensor(1.6890e+08, device='cuda:0')
c= tensor(1.6890e+08, device='cuda:0')
c= tensor(1.6891e+08, device='cuda:0')
c= tensor(1.6891e+08, device='cuda:0')
c= tensor(1.6891e+08, device='cuda:0')
c= tensor(1.6891e+08, device='cuda:0')
c= tensor(1.6891e+08, device='cuda:0')
c= tensor(1.6891e+08, device='cuda:0')
c= tensor(1.6892e+08, device='cuda:0')
c= tensor(1.6892e+08, device='cuda:0')
c= tensor(1.6893e+08, device='cuda:0')
c= tensor(1.6893e+08, device='cuda:0')
c= tensor(1.6893e+08, device='cuda:0')
c= tensor(1.6894e+08, device='cuda:0')
c= tensor(1.6894e+08, device='cuda:0')
c= tensor(1.6894e+08, device='cuda:0')
c= tensor(1.6895e+08, device='cuda:0')
c= tensor(1.6896e+08, device='cuda:0')
c= tensor(1.6896e+08, device='cuda:0')
c= tensor(1.6896e+08, device='cuda:0')
c= tensor(1.6896e+08, device='cuda:0')
c= tensor(1.6896e+08, device='cuda:0')
c= tensor(1.6896e+08, device='cuda:0')
c= tensor(1.6897e+08, device='cuda:0')
c= tensor(1.6897e+08, device='cuda:0')
c= tensor(1.6897e+08, device='cuda:0')
c= tensor(1.6897e+08, device='cuda:0')
c= tensor(1.6897e+08, device='cuda:0')
c= tensor(1.6897e+08, device='cuda:0')
c= tensor(1.6897e+08, device='cuda:0')
c= tensor(1.6898e+08, device='cuda:0')
c= tensor(1.6898e+08, device='cuda:0')
c= tensor(1.6898e+08, device='cuda:0')
c= tensor(1.6898e+08, device='cuda:0')
c= tensor(1.6898e+08, device='cuda:0')
c= tensor(1.6899e+08, device='cuda:0')
c= tensor(1.6899e+08, device='cuda:0')
c= tensor(1.6899e+08, device='cuda:0')
c= tensor(1.6899e+08, device='cuda:0')
c= tensor(1.6900e+08, device='cuda:0')
c= tensor(1.6900e+08, device='cuda:0')
c= tensor(1.6901e+08, device='cuda:0')
c= tensor(1.6901e+08, device='cuda:0')
c= tensor(1.6901e+08, device='cuda:0')
c= tensor(1.6902e+08, device='cuda:0')
c= tensor(1.6902e+08, device='cuda:0')
c= tensor(1.6903e+08, device='cuda:0')
c= tensor(1.6920e+08, device='cuda:0')
c= tensor(1.6921e+08, device='cuda:0')
c= tensor(1.6921e+08, device='cuda:0')
c= tensor(1.6922e+08, device='cuda:0')
c= tensor(1.6922e+08, device='cuda:0')
c= tensor(1.6948e+08, device='cuda:0')
c= tensor(1.7156e+08, device='cuda:0')
c= tensor(1.7156e+08, device='cuda:0')
c= tensor(1.7178e+08, device='cuda:0')
c= tensor(1.7278e+08, device='cuda:0')
c= tensor(1.7279e+08, device='cuda:0')
c= tensor(2.0586e+08, device='cuda:0')
c= tensor(2.0586e+08, device='cuda:0')
c= tensor(2.0590e+08, device='cuda:0')
c= tensor(2.0673e+08, device='cuda:0')
c= tensor(2.2290e+08, device='cuda:0')
c= tensor(2.2290e+08, device='cuda:0')
c= tensor(2.2295e+08, device='cuda:0')
c= tensor(2.2310e+08, device='cuda:0')
c= tensor(2.2675e+08, device='cuda:0')
c= tensor(2.2726e+08, device='cuda:0')
c= tensor(2.2749e+08, device='cuda:0')
c= tensor(2.2762e+08, device='cuda:0')
c= tensor(2.2765e+08, device='cuda:0')
c= tensor(2.2774e+08, device='cuda:0')
c= tensor(2.3014e+08, device='cuda:0')
c= tensor(2.3014e+08, device='cuda:0')
c= tensor(2.3014e+08, device='cuda:0')
c= tensor(2.3048e+08, device='cuda:0')
c= tensor(2.3056e+08, device='cuda:0')
c= tensor(2.4112e+08, device='cuda:0')
c= tensor(2.4157e+08, device='cuda:0')
c= tensor(2.4157e+08, device='cuda:0')
c= tensor(2.4159e+08, device='cuda:0')
c= tensor(2.4159e+08, device='cuda:0')
c= tensor(2.4166e+08, device='cuda:0')
c= tensor(2.4233e+08, device='cuda:0')
c= tensor(2.4404e+08, device='cuda:0')
c= tensor(2.4426e+08, device='cuda:0')
c= tensor(2.4426e+08, device='cuda:0')
c= tensor(2.4427e+08, device='cuda:0')
c= tensor(2.4446e+08, device='cuda:0')
c= tensor(2.4518e+08, device='cuda:0')
c= tensor(2.4526e+08, device='cuda:0')
c= tensor(2.4527e+08, device='cuda:0')
c= tensor(2.5753e+08, device='cuda:0')
c= tensor(2.5754e+08, device='cuda:0')
c= tensor(2.5764e+08, device='cuda:0')
c= tensor(2.5859e+08, device='cuda:0')
c= tensor(2.5859e+08, device='cuda:0')
c= tensor(2.5869e+08, device='cuda:0')
c= tensor(2.5964e+08, device='cuda:0')
c= tensor(2.6387e+08, device='cuda:0')
c= tensor(2.6388e+08, device='cuda:0')
c= tensor(2.6393e+08, device='cuda:0')
c= tensor(2.6393e+08, device='cuda:0')
c= tensor(2.6394e+08, device='cuda:0')
c= tensor(2.6425e+08, device='cuda:0')
c= tensor(2.6427e+08, device='cuda:0')
c= tensor(2.6481e+08, device='cuda:0')
c= tensor(2.7196e+08, device='cuda:0')
c= tensor(2.7209e+08, device='cuda:0')
c= tensor(2.7211e+08, device='cuda:0')
c= tensor(2.7212e+08, device='cuda:0')
c= tensor(2.7367e+08, device='cuda:0')
c= tensor(2.7373e+08, device='cuda:0')
c= tensor(2.7374e+08, device='cuda:0')
c= tensor(2.7377e+08, device='cuda:0')
c= tensor(3.0732e+08, device='cuda:0')
c= tensor(3.0733e+08, device='cuda:0')
c= tensor(3.0853e+08, device='cuda:0')
c= tensor(3.0857e+08, device='cuda:0')
c= tensor(3.0876e+08, device='cuda:0')
c= tensor(3.0888e+08, device='cuda:0')
c= tensor(3.0938e+08, device='cuda:0')
c= tensor(3.1010e+08, device='cuda:0')
c= tensor(3.1011e+08, device='cuda:0')
c= tensor(3.1159e+08, device='cuda:0')
c= tensor(3.1247e+08, device='cuda:0')
c= tensor(3.1247e+08, device='cuda:0')
c= tensor(3.1262e+08, device='cuda:0')
c= tensor(3.1415e+08, device='cuda:0')
c= tensor(3.1898e+08, device='cuda:0')
c= tensor(3.1944e+08, device='cuda:0')
c= tensor(3.1947e+08, device='cuda:0')
c= tensor(3.1947e+08, device='cuda:0')
c= tensor(3.1956e+08, device='cuda:0')
c= tensor(3.1975e+08, device='cuda:0')
c= tensor(3.1976e+08, device='cuda:0')
c= tensor(3.1976e+08, device='cuda:0')
c= tensor(3.2039e+08, device='cuda:0')
c= tensor(3.2219e+08, device='cuda:0')
c= tensor(3.2679e+08, device='cuda:0')
c= tensor(3.2679e+08, device='cuda:0')
c= tensor(3.2682e+08, device='cuda:0')
c= tensor(3.2688e+08, device='cuda:0')
c= tensor(3.2690e+08, device='cuda:0')
c= tensor(3.2691e+08, device='cuda:0')
c= tensor(3.2692e+08, device='cuda:0')
c= tensor(3.3910e+08, device='cuda:0')
c= tensor(3.3922e+08, device='cuda:0')
c= tensor(3.3925e+08, device='cuda:0')
c= tensor(3.3928e+08, device='cuda:0')
c= tensor(3.3929e+08, device='cuda:0')
c= tensor(3.5416e+08, device='cuda:0')
c= tensor(3.5417e+08, device='cuda:0')
c= tensor(3.5530e+08, device='cuda:0')
c= tensor(3.5530e+08, device='cuda:0')
c= tensor(3.5530e+08, device='cuda:0')
c= tensor(3.5530e+08, device='cuda:0')
c= tensor(3.5550e+08, device='cuda:0')
c= tensor(3.5564e+08, device='cuda:0')
c= tensor(3.5569e+08, device='cuda:0')
c= tensor(3.5569e+08, device='cuda:0')
c= tensor(3.5570e+08, device='cuda:0')
c= tensor(3.6427e+08, device='cuda:0')
c= tensor(3.6440e+08, device='cuda:0')
c= tensor(3.6444e+08, device='cuda:0')
c= tensor(3.6522e+08, device='cuda:0')
c= tensor(3.6556e+08, device='cuda:0')
c= tensor(3.6557e+08, device='cuda:0')
c= tensor(3.6557e+08, device='cuda:0')
c= tensor(3.6563e+08, device='cuda:0')
c= tensor(3.6563e+08, device='cuda:0')
c= tensor(3.6563e+08, device='cuda:0')
c= tensor(3.6568e+08, device='cuda:0')
c= tensor(3.6568e+08, device='cuda:0')
c= tensor(3.6569e+08, device='cuda:0')
c= tensor(3.6569e+08, device='cuda:0')
c= tensor(3.6569e+08, device='cuda:0')
c= tensor(3.9337e+08, device='cuda:0')
c= tensor(3.9342e+08, device='cuda:0')
c= tensor(3.9360e+08, device='cuda:0')
c= tensor(3.9403e+08, device='cuda:0')
c= tensor(3.9403e+08, device='cuda:0')
c= tensor(3.9405e+08, device='cuda:0')
c= tensor(4.7204e+08, device='cuda:0')
c= tensor(4.9992e+08, device='cuda:0')
c= tensor(5.0044e+08, device='cuda:0')
c= tensor(5.0051e+08, device='cuda:0')
c= tensor(5.0051e+08, device='cuda:0')
c= tensor(5.0354e+08, device='cuda:0')
c= tensor(5.0504e+08, device='cuda:0')
c= tensor(5.0859e+08, device='cuda:0')
c= tensor(5.0862e+08, device='cuda:0')
c= tensor(5.1013e+08, device='cuda:0')
c= tensor(5.1331e+08, device='cuda:0')
c= tensor(5.1353e+08, device='cuda:0')
c= tensor(5.1354e+08, device='cuda:0')
c= tensor(5.1356e+08, device='cuda:0')
c= tensor(5.1357e+08, device='cuda:0')
c= tensor(5.1357e+08, device='cuda:0')
c= tensor(5.1468e+08, device='cuda:0')
c= tensor(5.1481e+08, device='cuda:0')
c= tensor(5.1481e+08, device='cuda:0')
c= tensor(5.1485e+08, device='cuda:0')
c= tensor(5.1486e+08, device='cuda:0')
c= tensor(5.1486e+08, device='cuda:0')
c= tensor(5.1555e+08, device='cuda:0')
c= tensor(5.1676e+08, device='cuda:0')
c= tensor(5.1684e+08, device='cuda:0')
c= tensor(5.1766e+08, device='cuda:0')
c= tensor(5.1830e+08, device='cuda:0')
c= tensor(5.1831e+08, device='cuda:0')
c= tensor(5.1839e+08, device='cuda:0')
c= tensor(5.1861e+08, device='cuda:0')
c= tensor(5.2024e+08, device='cuda:0')
c= tensor(5.2024e+08, device='cuda:0')
c= tensor(5.2773e+08, device='cuda:0')
c= tensor(5.3880e+08, device='cuda:0')
c= tensor(5.3896e+08, device='cuda:0')
c= tensor(5.3911e+08, device='cuda:0')
c= tensor(5.4147e+08, device='cuda:0')
c= tensor(5.4149e+08, device='cuda:0')
c= tensor(5.4149e+08, device='cuda:0')
c= tensor(5.4170e+08, device='cuda:0')
c= tensor(5.4302e+08, device='cuda:0')
c= tensor(5.4389e+08, device='cuda:0')
c= tensor(5.7834e+08, device='cuda:0')
c= tensor(5.7894e+08, device='cuda:0')
c= tensor(5.8009e+08, device='cuda:0')
c= tensor(5.8025e+08, device='cuda:0')
c= tensor(5.8116e+08, device='cuda:0')
c= tensor(5.8116e+08, device='cuda:0')
c= tensor(5.8118e+08, device='cuda:0')
c= tensor(5.8696e+08, device='cuda:0')
c= tensor(5.8699e+08, device='cuda:0')
c= tensor(5.8700e+08, device='cuda:0')
c= tensor(5.8710e+08, device='cuda:0')
c= tensor(5.8747e+08, device='cuda:0')
c= tensor(5.8805e+08, device='cuda:0')
c= tensor(5.8827e+08, device='cuda:0')
c= tensor(5.8832e+08, device='cuda:0')
c= tensor(5.8857e+08, device='cuda:0')
c= tensor(5.8857e+08, device='cuda:0')
c= tensor(5.8868e+08, device='cuda:0')
c= tensor(5.8873e+08, device='cuda:0')
c= tensor(5.8933e+08, device='cuda:0')
c= tensor(5.8933e+08, device='cuda:0')
c= tensor(5.9138e+08, device='cuda:0')
c= tensor(5.9138e+08, device='cuda:0')
c= tensor(5.9144e+08, device='cuda:0')
c= tensor(5.9146e+08, device='cuda:0')
c= tensor(5.9150e+08, device='cuda:0')
c= tensor(5.9150e+08, device='cuda:0')
c= tensor(5.9157e+08, device='cuda:0')
c= tensor(5.9160e+08, device='cuda:0')
c= tensor(5.9167e+08, device='cuda:0')
c= tensor(5.9190e+08, device='cuda:0')
c= tensor(5.9871e+08, device='cuda:0')
c= tensor(5.9871e+08, device='cuda:0')
c= tensor(5.9874e+08, device='cuda:0')
c= tensor(6.0107e+08, device='cuda:0')
c= tensor(6.0107e+08, device='cuda:0')
c= tensor(6.1774e+08, device='cuda:0')
c= tensor(6.1774e+08, device='cuda:0')
c= tensor(6.1849e+08, device='cuda:0')
c= tensor(6.2237e+08, device='cuda:0')
c= tensor(6.2238e+08, device='cuda:0')
c= tensor(6.2799e+08, device='cuda:0')
c= tensor(6.2810e+08, device='cuda:0')
c= tensor(6.3096e+08, device='cuda:0')
c= tensor(6.3097e+08, device='cuda:0')
c= tensor(6.3181e+08, device='cuda:0')
c= tensor(6.3181e+08, device='cuda:0')
c= tensor(6.3181e+08, device='cuda:0')
c= tensor(6.3182e+08, device='cuda:0')
c= tensor(6.3218e+08, device='cuda:0')
c= tensor(6.3224e+08, device='cuda:0')
c= tensor(6.3374e+08, device='cuda:0')
c= tensor(6.3375e+08, device='cuda:0')
c= tensor(6.3376e+08, device='cuda:0')
c= tensor(6.3377e+08, device='cuda:0')
c= tensor(6.3416e+08, device='cuda:0')
c= tensor(6.3416e+08, device='cuda:0')
c= tensor(6.3522e+08, device='cuda:0')
c= tensor(6.3583e+08, device='cuda:0')
c= tensor(6.3586e+08, device='cuda:0')
c= tensor(6.3586e+08, device='cuda:0')
c= tensor(6.3592e+08, device='cuda:0')
c= tensor(6.8496e+08, device='cuda:0')
c= tensor(6.8497e+08, device='cuda:0')
c= tensor(6.8501e+08, device='cuda:0')
c= tensor(6.8603e+08, device='cuda:0')
c= tensor(6.8639e+08, device='cuda:0')
c= tensor(6.8639e+08, device='cuda:0')
c= tensor(6.8639e+08, device='cuda:0')
c= tensor(6.9952e+08, device='cuda:0')
c= tensor(6.9956e+08, device='cuda:0')
c= tensor(6.9991e+08, device='cuda:0')
c= tensor(7.0002e+08, device='cuda:0')
c= tensor(7.0012e+08, device='cuda:0')
c= tensor(7.0048e+08, device='cuda:0')
c= tensor(7.0374e+08, device='cuda:0')
c= tensor(7.0444e+08, device='cuda:0')
c= tensor(7.0444e+08, device='cuda:0')
c= tensor(7.0478e+08, device='cuda:0')
c= tensor(7.0479e+08, device='cuda:0')
c= tensor(7.0483e+08, device='cuda:0')
c= tensor(7.0491e+08, device='cuda:0')
c= tensor(7.0492e+08, device='cuda:0')
c= tensor(7.0605e+08, device='cuda:0')
c= tensor(7.0607e+08, device='cuda:0')
c= tensor(7.0607e+08, device='cuda:0')
c= tensor(7.0609e+08, device='cuda:0')
c= tensor(7.0619e+08, device='cuda:0')
c= tensor(7.0746e+08, device='cuda:0')
c= tensor(7.0933e+08, device='cuda:0')
c= tensor(7.0935e+08, device='cuda:0')
c= tensor(7.0937e+08, device='cuda:0')
c= tensor(7.0943e+08, device='cuda:0')
c= tensor(7.0943e+08, device='cuda:0')
c= tensor(7.0943e+08, device='cuda:0')
c= tensor(7.0949e+08, device='cuda:0')
c= tensor(7.1277e+08, device='cuda:0')
c= tensor(7.1277e+08, device='cuda:0')
c= tensor(7.1277e+08, device='cuda:0')
c= tensor(7.1278e+08, device='cuda:0')
c= tensor(7.1853e+08, device='cuda:0')
c= tensor(7.3150e+08, device='cuda:0')
c= tensor(7.3168e+08, device='cuda:0')
c= tensor(7.3168e+08, device='cuda:0')
c= tensor(7.3181e+08, device='cuda:0')
c= tensor(7.3209e+08, device='cuda:0')
c= tensor(7.3209e+08, device='cuda:0')
c= tensor(7.3214e+08, device='cuda:0')
c= tensor(7.3218e+08, device='cuda:0')
c= tensor(7.3931e+08, device='cuda:0')
c= tensor(7.4026e+08, device='cuda:0')
c= tensor(7.4161e+08, device='cuda:0')
c= tensor(7.4162e+08, device='cuda:0')
c= tensor(7.4162e+08, device='cuda:0')
c= tensor(7.4162e+08, device='cuda:0')
c= tensor(7.4173e+08, device='cuda:0')
c= tensor(7.4177e+08, device='cuda:0')
c= tensor(7.4238e+08, device='cuda:0')
c= tensor(7.4290e+08, device='cuda:0')
c= tensor(7.4768e+08, device='cuda:0')
c= tensor(7.4769e+08, device='cuda:0')
c= tensor(7.4771e+08, device='cuda:0')
c= tensor(7.4780e+08, device='cuda:0')
c= tensor(8.0543e+08, device='cuda:0')
c= tensor(8.0563e+08, device='cuda:0')
c= tensor(8.0564e+08, device='cuda:0')
c= tensor(8.0564e+08, device='cuda:0')
c= tensor(8.0585e+08, device='cuda:0')
c= tensor(8.0585e+08, device='cuda:0')
c= tensor(8.0591e+08, device='cuda:0')
c= tensor(8.0591e+08, device='cuda:0')
c= tensor(8.0593e+08, device='cuda:0')
c= tensor(8.0595e+08, device='cuda:0')
c= tensor(8.0595e+08, device='cuda:0')
c= tensor(8.0595e+08, device='cuda:0')
c= tensor(8.0936e+08, device='cuda:0')
c= tensor(8.2811e+08, device='cuda:0')
c= tensor(8.2946e+08, device='cuda:0')
c= tensor(8.3024e+08, device='cuda:0')
c= tensor(8.3034e+08, device='cuda:0')
c= tensor(8.3036e+08, device='cuda:0')
c= tensor(8.3038e+08, device='cuda:0')
c= tensor(8.3054e+08, device='cuda:0')
c= tensor(8.3083e+08, device='cuda:0')
c= tensor(8.3291e+08, device='cuda:0')
c= tensor(8.3292e+08, device='cuda:0')
c= tensor(8.4448e+08, device='cuda:0')
c= tensor(8.4454e+08, device='cuda:0')
c= tensor(8.4461e+08, device='cuda:0')
c= tensor(8.4875e+08, device='cuda:0')
c= tensor(8.4903e+08, device='cuda:0')
c= tensor(8.5594e+08, device='cuda:0')
c= tensor(8.5785e+08, device='cuda:0')
c= tensor(8.5810e+08, device='cuda:0')
c= tensor(8.5832e+08, device='cuda:0')
c= tensor(8.5836e+08, device='cuda:0')
c= tensor(8.5872e+08, device='cuda:0')
c= tensor(8.5875e+08, device='cuda:0')
c= tensor(8.6065e+08, device='cuda:0')
c= tensor(8.9158e+08, device='cuda:0')
c= tensor(8.9298e+08, device='cuda:0')
c= tensor(8.9344e+08, device='cuda:0')
c= tensor(8.9356e+08, device='cuda:0')
c= tensor(8.9358e+08, device='cuda:0')
c= tensor(8.9367e+08, device='cuda:0')
c= tensor(8.9380e+08, device='cuda:0')
c= tensor(8.9485e+08, device='cuda:0')
c= tensor(8.9681e+08, device='cuda:0')
c= tensor(8.9681e+08, device='cuda:0')
c= tensor(9.0709e+08, device='cuda:0')
c= tensor(9.1821e+08, device='cuda:0')
c= tensor(9.1839e+08, device='cuda:0')
c= tensor(9.1839e+08, device='cuda:0')
c= tensor(9.1898e+08, device='cuda:0')
c= tensor(9.1961e+08, device='cuda:0')
c= tensor(9.1961e+08, device='cuda:0')
c= tensor(9.3953e+08, device='cuda:0')
c= tensor(9.4026e+08, device='cuda:0')
c= tensor(9.4030e+08, device='cuda:0')
c= tensor(9.4032e+08, device='cuda:0')
c= tensor(9.4032e+08, device='cuda:0')
c= tensor(9.4032e+08, device='cuda:0')
c= tensor(9.4035e+08, device='cuda:0')
c= tensor(9.4037e+08, device='cuda:0')
c= tensor(9.4090e+08, device='cuda:0')
c= tensor(1.2164e+09, device='cuda:0')
c= tensor(1.2164e+09, device='cuda:0')
c= tensor(1.2166e+09, device='cuda:0')
c= tensor(1.2166e+09, device='cuda:0')
c= tensor(1.2166e+09, device='cuda:0')
c= tensor(1.2166e+09, device='cuda:0')
c= tensor(1.2229e+09, device='cuda:0')
c= tensor(1.2230e+09, device='cuda:0')
c= tensor(1.2671e+09, device='cuda:0')
c= tensor(1.2672e+09, device='cuda:0')
c= tensor(1.2680e+09, device='cuda:0')
c= tensor(1.2680e+09, device='cuda:0')
c= tensor(1.2691e+09, device='cuda:0')
c= tensor(1.2790e+09, device='cuda:0')
c= tensor(1.2790e+09, device='cuda:0')
c= tensor(1.2790e+09, device='cuda:0')
c= tensor(1.2794e+09, device='cuda:0')
c= tensor(1.2795e+09, device='cuda:0')
c= tensor(1.2796e+09, device='cuda:0')
c= tensor(1.2802e+09, device='cuda:0')
c= tensor(1.2808e+09, device='cuda:0')
c= tensor(1.2809e+09, device='cuda:0')
c= tensor(1.2809e+09, device='cuda:0')
c= tensor(1.2814e+09, device='cuda:0')
c= tensor(1.2864e+09, device='cuda:0')
c= tensor(1.2864e+09, device='cuda:0')
c= tensor(1.2864e+09, device='cuda:0')
c= tensor(1.2899e+09, device='cuda:0')
c= tensor(1.2900e+09, device='cuda:0')
c= tensor(1.2922e+09, device='cuda:0')
c= tensor(1.2923e+09, device='cuda:0')
c= tensor(1.2942e+09, device='cuda:0')
c= tensor(1.2943e+09, device='cuda:0')
c= tensor(1.2971e+09, device='cuda:0')
c= tensor(1.2974e+09, device='cuda:0')
c= tensor(1.2974e+09, device='cuda:0')
c= tensor(1.2974e+09, device='cuda:0')
c= tensor(1.2975e+09, device='cuda:0')
c= tensor(1.2977e+09, device='cuda:0')
c= tensor(1.2996e+09, device='cuda:0')
c= tensor(1.3006e+09, device='cuda:0')
c= tensor(1.3006e+09, device='cuda:0')
c= tensor(1.3007e+09, device='cuda:0')
c= tensor(1.3009e+09, device='cuda:0')
c= tensor(1.3078e+09, device='cuda:0')
c= tensor(1.3080e+09, device='cuda:0')
c= tensor(1.3086e+09, device='cuda:0')
c= tensor(1.3095e+09, device='cuda:0')
c= tensor(1.3096e+09, device='cuda:0')
c= tensor(1.3096e+09, device='cuda:0')
c= tensor(1.3097e+09, device='cuda:0')
c= tensor(1.3103e+09, device='cuda:0')
c= tensor(1.3108e+09, device='cuda:0')
c= tensor(1.3111e+09, device='cuda:0')
c= tensor(1.3111e+09, device='cuda:0')
c= tensor(1.3111e+09, device='cuda:0')
c= tensor(1.3143e+09, device='cuda:0')
c= tensor(1.3146e+09, device='cuda:0')
c= tensor(1.3146e+09, device='cuda:0')
c= tensor(1.3146e+09, device='cuda:0')
c= tensor(1.3146e+09, device='cuda:0')
c= tensor(1.3148e+09, device='cuda:0')
c= tensor(1.3151e+09, device='cuda:0')
c= tensor(1.3157e+09, device='cuda:0')
c= tensor(1.3157e+09, device='cuda:0')
c= tensor(1.3354e+09, device='cuda:0')
c= tensor(1.3354e+09, device='cuda:0')
c= tensor(1.3354e+09, device='cuda:0')
c= tensor(1.3451e+09, device='cuda:0')
c= tensor(1.3456e+09, device='cuda:0')
c= tensor(1.3468e+09, device='cuda:0')
c= tensor(1.3480e+09, device='cuda:0')
c= tensor(1.3481e+09, device='cuda:0')
c= tensor(1.3488e+09, device='cuda:0')
c= tensor(1.3500e+09, device='cuda:0')
c= tensor(1.3500e+09, device='cuda:0')
c= tensor(1.3506e+09, device='cuda:0')
c= tensor(1.3506e+09, device='cuda:0')
c= tensor(1.3538e+09, device='cuda:0')
c= tensor(1.3567e+09, device='cuda:0')
c= tensor(1.3568e+09, device='cuda:0')
c= tensor(1.3571e+09, device='cuda:0')
c= tensor(1.3571e+09, device='cuda:0')
c= tensor(1.3573e+09, device='cuda:0')
c= tensor(1.3582e+09, device='cuda:0')
c= tensor(1.3582e+09, device='cuda:0')
c= tensor(1.3622e+09, device='cuda:0')
c= tensor(1.3664e+09, device='cuda:0')
c= tensor(1.3665e+09, device='cuda:0')
c= tensor(1.3665e+09, device='cuda:0')
c= tensor(1.3665e+09, device='cuda:0')
c= tensor(1.3753e+09, device='cuda:0')
c= tensor(1.3753e+09, device='cuda:0')
c= tensor(1.3754e+09, device='cuda:0')
c= tensor(1.3759e+09, device='cuda:0')
c= tensor(1.3763e+09, device='cuda:0')
c= tensor(1.3764e+09, device='cuda:0')
c= tensor(1.3765e+09, device='cuda:0')
c= tensor(1.3765e+09, device='cuda:0')
memory (bytes)
3876179968
time for making loss 2 is 15.072057485580444
p0 True
it  0 : 1138440192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 21% |
shape of L is 
torch.Size([])
memory (bytes)
3876507648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
3877011456
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  19545924000.0
relative error loss 14.19923
shape of L is 
torch.Size([])
memory (bytes)
4131909632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  5% |  8% |
memory (bytes)
4131913728
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  19545820000.0
relative error loss 14.199155
shape of L is 
torch.Size([])
memory (bytes)
4133892096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  8% |
memory (bytes)
4133892096
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  19544875000.0
relative error loss 14.198468
shape of L is 
torch.Size([])
memory (bytes)
4135890944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  8% |
memory (bytes)
4136001536
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  19539638000.0
relative error loss 14.194664
shape of L is 
torch.Size([])
memory (bytes)
4138082304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  8% |
memory (bytes)
4138123264
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  19483202000.0
relative error loss 14.153666
shape of L is 
torch.Size([])
memory (bytes)
4140355584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  8% |
memory (bytes)
4140380160
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  19202777000.0
relative error loss 13.94995
shape of L is 
torch.Size([])
memory (bytes)
4142432256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% |  8% |
memory (bytes)
4142477312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  17719170000.0
relative error loss 12.872176
shape of L is 
torch.Size([])
memory (bytes)
4144558080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  8% |
memory (bytes)
4144603136
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  11183200000.0
relative error loss 8.12409
shape of L is 
torch.Size([])
memory (bytes)
4146716672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  5% |  8% |
memory (bytes)
4146716672
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  3878848800.0
relative error loss 2.8178084
shape of L is 
torch.Size([])
memory (bytes)
4148813824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% |  8% |
memory (bytes)
4148813824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  2219881000.0
relative error loss 1.6126432
time to take a step is 289.84730315208435
it  1 : 1603621376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  5% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4150919168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  5% |  8% |
memory (bytes)
4150976512
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  2219881000.0
relative error loss 1.6126432
shape of L is 
torch.Size([])
memory (bytes)
4153106432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% |  8% |
memory (bytes)
4153106432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% |  8% |
error is  1581732500.0
relative error loss 1.1490571
shape of L is 
torch.Size([])
memory (bytes)
4154986496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  5% |  8% |
memory (bytes)
4155244544
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  2137478100.0
relative error loss 1.5527813
shape of L is 
torch.Size([])
memory (bytes)
4157349888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  5% |  8% |
memory (bytes)
4157407232
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  1327731700.0
relative error loss 0.9645371
shape of L is 
torch.Size([])
memory (bytes)
4159528960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  5% |  8% |
memory (bytes)
4159528960
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  1266442100.0
relative error loss 0.92001295
shape of L is 
torch.Size([])
memory (bytes)
4161617920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  5% |  8% |
memory (bytes)
4161658880
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  1135110100.0
relative error loss 0.82460624
shape of L is 
torch.Size([])
memory (bytes)
4163674112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  8% |
memory (bytes)
4163674112
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  1095061500.0
relative error loss 0.79551274
shape of L is 
torch.Size([])
memory (bytes)
4165713920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% |  8% |
memory (bytes)
4165713920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  1063898000.0
relative error loss 0.7728738
shape of L is 
torch.Size([])
memory (bytes)
4167925760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% |  8% |
memory (bytes)
4167925760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  8% |
error is  1041901000.0
relative error loss 0.756894
shape of L is 
torch.Size([])
memory (bytes)
4170129408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% |  8% |
memory (bytes)
4170129408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  998838140.0
relative error loss 0.7256108
time to take a step is 283.70025849342346
it  2 : 1737076224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4172238848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% |  8% |
memory (bytes)
4172238848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  998838140.0
relative error loss 0.7256108
shape of L is 
torch.Size([])
memory (bytes)
4174151680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  8% |
memory (bytes)
4174151680
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  959440100.0
relative error loss 0.6969899
shape of L is 
torch.Size([])
memory (bytes)
4176461824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  8% |
memory (bytes)
4176461824
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  905172160.0
relative error loss 0.65756667
shape of L is 
torch.Size([])
memory (bytes)
4178501632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
4178501632
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  867998460.0
relative error loss 0.63056165
shape of L is 
torch.Size([])
memory (bytes)
4180705280
| ID | GPU | MEM |
------------------
|  0 |  1% |  0% |
|  1 | 25% |  8% |
memory (bytes)
4180705280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  999700100.0
relative error loss 0.72623694
shape of L is 
torch.Size([])
memory (bytes)
4182704128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  8% |
memory (bytes)
4182704128
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  805828200.0
relative error loss 0.5853978
shape of L is 
torch.Size([])
memory (bytes)
4184973312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  8% |
memory (bytes)
4185018368
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  750727100.0
relative error loss 0.5453693
shape of L is 
torch.Size([])
memory (bytes)
4186951680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4186951680
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  678793700.0
relative error loss 0.49311298
shape of L is 
torch.Size([])
memory (bytes)
4189003776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  8% |
memory (bytes)
4189265920
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  617227840.0
relative error loss 0.44838813
shape of L is 
torch.Size([])
memory (bytes)
4191166464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  8% |
memory (bytes)
4191166464
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  572266600.0
relative error loss 0.41572586
time to take a step is 230.45741891860962
it  3 : 1736312320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4193497088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  8% |
memory (bytes)
4193542144
| ID | GPU  | MEM |
-------------------
|  0 |   1% |  0% |
|  1 | 100% |  8% |
error is  572266600.0
relative error loss 0.41572586
shape of L is 
torch.Size([])
memory (bytes)
4195651584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% |  8% |
memory (bytes)
4195651584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% |  8% |
error is  534626620.0
relative error loss 0.38838208
shape of L is 
torch.Size([])
memory (bytes)
4197658624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% |  8% |
memory (bytes)
4197658624
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  496125380.0
relative error loss 0.36041266
shape of L is 
torch.Size([])
memory (bytes)
4199956480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
4199976960
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  442467400.0
relative error loss 0.32143256
shape of L is 
torch.Size([])
memory (bytes)
4202016768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  8% |
memory (bytes)
4202016768
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  426431300.0
relative error loss 0.30978307
shape of L is 
torch.Size([])
memory (bytes)
4204216320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  8% |
memory (bytes)
4204216320
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  411225730.0
relative error loss 0.2987369
shape of L is 
torch.Size([])
memory (bytes)
4206276608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  9% |
memory (bytes)
4206276608
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  383181440.0
relative error loss 0.278364
shape of L is 
torch.Size([])
memory (bytes)
4208525312
| ID | GPU | MEM |
------------------
|  0 |  2% |  0% |
|  1 |  8% |  9% |
memory (bytes)
4208525312
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  359450500.0
relative error loss 0.26112455
shape of L is 
torch.Size([])
memory (bytes)
4210491392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  9% |
memory (bytes)
4210491392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  507617020.0
relative error loss 0.36876082
shape of L is 
torch.Size([])
memory (bytes)
4212518912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  9% |
memory (bytes)
4212776960
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  335803780.0
relative error loss 0.24394627
time to take a step is 230.53360033035278
c= tensor(445.5976, device='cuda:0')
c= tensor(20172.1465, device='cuda:0')
c= tensor(24394.1113, device='cuda:0')
c= tensor(25883.4277, device='cuda:0')
c= tensor(180515.2344, device='cuda:0')
c= tensor(230156.5469, device='cuda:0')
c= tensor(637871.9375, device='cuda:0')
c= tensor(762217.3750, device='cuda:0')
c= tensor(784626.3125, device='cuda:0')
c= tensor(1733544., device='cuda:0')
c= tensor(1742662.6250, device='cuda:0')
c= tensor(2958325.5000, device='cuda:0')
c= tensor(2963403.2500, device='cuda:0')
c= tensor(6436498., device='cuda:0')
c= tensor(6548359., device='cuda:0')
c= tensor(6631861., device='cuda:0')
c= tensor(6880960., device='cuda:0')
c= tensor(7858309., device='cuda:0')
c= tensor(11343056., device='cuda:0')
c= tensor(13377967., device='cuda:0')
c= tensor(13386474., device='cuda:0')
c= tensor(17693160., device='cuda:0')
c= tensor(17725560., device='cuda:0')
c= tensor(17742658., device='cuda:0')
c= tensor(19032146., device='cuda:0')
c= tensor(19609614., device='cuda:0')
c= tensor(20112764., device='cuda:0')
c= tensor(20150220., device='cuda:0')
c= tensor(21077712., device='cuda:0')
c= tensor(1.4992e+08, device='cuda:0')
c= tensor(1.4993e+08, device='cuda:0')
c= tensor(1.6726e+08, device='cuda:0')
c= tensor(1.6727e+08, device='cuda:0')
c= tensor(1.6728e+08, device='cuda:0')
c= tensor(1.6728e+08, device='cuda:0')
c= tensor(1.6798e+08, device='cuda:0')
c= tensor(1.6840e+08, device='cuda:0')
c= tensor(1.6841e+08, device='cuda:0')
c= tensor(1.6842e+08, device='cuda:0')
c= tensor(1.6842e+08, device='cuda:0')
c= tensor(1.6842e+08, device='cuda:0')
c= tensor(1.6842e+08, device='cuda:0')
c= tensor(1.6842e+08, device='cuda:0')
c= tensor(1.6842e+08, device='cuda:0')
c= tensor(1.6842e+08, device='cuda:0')
c= tensor(1.6843e+08, device='cuda:0')
c= tensor(1.6843e+08, device='cuda:0')
c= tensor(1.6843e+08, device='cuda:0')
c= tensor(1.6844e+08, device='cuda:0')
c= tensor(1.6844e+08, device='cuda:0')
c= tensor(1.6845e+08, device='cuda:0')
c= tensor(1.6845e+08, device='cuda:0')
c= tensor(1.6846e+08, device='cuda:0')
c= tensor(1.6846e+08, device='cuda:0')
c= tensor(1.6847e+08, device='cuda:0')
c= tensor(1.6847e+08, device='cuda:0')
c= tensor(1.6847e+08, device='cuda:0')
c= tensor(1.6848e+08, device='cuda:0')
c= tensor(1.6848e+08, device='cuda:0')
c= tensor(1.6848e+08, device='cuda:0')
c= tensor(1.6849e+08, device='cuda:0')
c= tensor(1.6849e+08, device='cuda:0')
c= tensor(1.6850e+08, device='cuda:0')
c= tensor(1.6851e+08, device='cuda:0')
c= tensor(1.6851e+08, device='cuda:0')
c= tensor(1.6851e+08, device='cuda:0')
c= tensor(1.6851e+08, device='cuda:0')
c= tensor(1.6851e+08, device='cuda:0')
c= tensor(1.6851e+08, device='cuda:0')
c= tensor(1.6852e+08, device='cuda:0')
c= tensor(1.6852e+08, device='cuda:0')
c= tensor(1.6852e+08, device='cuda:0')
c= tensor(1.6852e+08, device='cuda:0')
c= tensor(1.6852e+08, device='cuda:0')
c= tensor(1.6853e+08, device='cuda:0')
c= tensor(1.6853e+08, device='cuda:0')
c= tensor(1.6853e+08, device='cuda:0')
c= tensor(1.6853e+08, device='cuda:0')
c= tensor(1.6854e+08, device='cuda:0')
c= tensor(1.6854e+08, device='cuda:0')
c= tensor(1.6854e+08, device='cuda:0')
c= tensor(1.6854e+08, device='cuda:0')
c= tensor(1.6855e+08, device='cuda:0')
c= tensor(1.6855e+08, device='cuda:0')
c= tensor(1.6856e+08, device='cuda:0')
c= tensor(1.6856e+08, device='cuda:0')
c= tensor(1.6856e+08, device='cuda:0')
c= tensor(1.6856e+08, device='cuda:0')
c= tensor(1.6856e+08, device='cuda:0')
c= tensor(1.6857e+08, device='cuda:0')
c= tensor(1.6857e+08, device='cuda:0')
c= tensor(1.6857e+08, device='cuda:0')
c= tensor(1.6857e+08, device='cuda:0')
c= tensor(1.6857e+08, device='cuda:0')
c= tensor(1.6857e+08, device='cuda:0')
c= tensor(1.6858e+08, device='cuda:0')
c= tensor(1.6858e+08, device='cuda:0')
c= tensor(1.6858e+08, device='cuda:0')
c= tensor(1.6859e+08, device='cuda:0')
c= tensor(1.6859e+08, device='cuda:0')
c= tensor(1.6862e+08, device='cuda:0')
c= tensor(1.6862e+08, device='cuda:0')
c= tensor(1.6862e+08, device='cuda:0')
c= tensor(1.6862e+08, device='cuda:0')
c= tensor(1.6862e+08, device='cuda:0')
c= tensor(1.6863e+08, device='cuda:0')
c= tensor(1.6863e+08, device='cuda:0')
c= tensor(1.6863e+08, device='cuda:0')
c= tensor(1.6863e+08, device='cuda:0')
c= tensor(1.6863e+08, device='cuda:0')
c= tensor(1.6863e+08, device='cuda:0')
c= tensor(1.6864e+08, device='cuda:0')
c= tensor(1.6864e+08, device='cuda:0')
c= tensor(1.6864e+08, device='cuda:0')
c= tensor(1.6864e+08, device='cuda:0')
c= tensor(1.6864e+08, device='cuda:0')
c= tensor(1.6864e+08, device='cuda:0')
c= tensor(1.6864e+08, device='cuda:0')
c= tensor(1.6865e+08, device='cuda:0')
c= tensor(1.6865e+08, device='cuda:0')
c= tensor(1.6866e+08, device='cuda:0')
c= tensor(1.6866e+08, device='cuda:0')
c= tensor(1.6866e+08, device='cuda:0')
c= tensor(1.6867e+08, device='cuda:0')
c= tensor(1.6867e+08, device='cuda:0')
c= tensor(1.6867e+08, device='cuda:0')
c= tensor(1.6867e+08, device='cuda:0')
c= tensor(1.6867e+08, device='cuda:0')
c= tensor(1.6872e+08, device='cuda:0')
c= tensor(1.6872e+08, device='cuda:0')
c= tensor(1.6872e+08, device='cuda:0')
c= tensor(1.6872e+08, device='cuda:0')
c= tensor(1.6873e+08, device='cuda:0')
c= tensor(1.6873e+08, device='cuda:0')
c= tensor(1.6873e+08, device='cuda:0')
c= tensor(1.6873e+08, device='cuda:0')
c= tensor(1.6873e+08, device='cuda:0')
c= tensor(1.6873e+08, device='cuda:0')
c= tensor(1.6873e+08, device='cuda:0')
c= tensor(1.6873e+08, device='cuda:0')
c= tensor(1.6874e+08, device='cuda:0')
c= tensor(1.6874e+08, device='cuda:0')
c= tensor(1.6875e+08, device='cuda:0')
c= tensor(1.6876e+08, device='cuda:0')
c= tensor(1.6877e+08, device='cuda:0')
c= tensor(1.6877e+08, device='cuda:0')
c= tensor(1.6877e+08, device='cuda:0')
c= tensor(1.6877e+08, device='cuda:0')
c= tensor(1.6877e+08, device='cuda:0')
c= tensor(1.6877e+08, device='cuda:0')
c= tensor(1.6878e+08, device='cuda:0')
c= tensor(1.6878e+08, device='cuda:0')
c= tensor(1.6879e+08, device='cuda:0')
c= tensor(1.6879e+08, device='cuda:0')
c= tensor(1.6880e+08, device='cuda:0')
c= tensor(1.6881e+08, device='cuda:0')
c= tensor(1.6881e+08, device='cuda:0')
c= tensor(1.6881e+08, device='cuda:0')
c= tensor(1.6882e+08, device='cuda:0')
c= tensor(1.6882e+08, device='cuda:0')
c= tensor(1.6882e+08, device='cuda:0')
c= tensor(1.6882e+08, device='cuda:0')
c= tensor(1.6882e+08, device='cuda:0')
c= tensor(1.6882e+08, device='cuda:0')
c= tensor(1.6882e+08, device='cuda:0')
c= tensor(1.6882e+08, device='cuda:0')
c= tensor(1.6883e+08, device='cuda:0')
c= tensor(1.6883e+08, device='cuda:0')
c= tensor(1.6883e+08, device='cuda:0')
c= tensor(1.6883e+08, device='cuda:0')
c= tensor(1.6883e+08, device='cuda:0')
c= tensor(1.6883e+08, device='cuda:0')
c= tensor(1.6884e+08, device='cuda:0')
c= tensor(1.6884e+08, device='cuda:0')
c= tensor(1.6884e+08, device='cuda:0')
c= tensor(1.6884e+08, device='cuda:0')
c= tensor(1.6884e+08, device='cuda:0')
c= tensor(1.6885e+08, device='cuda:0')
c= tensor(1.6885e+08, device='cuda:0')
c= tensor(1.6885e+08, device='cuda:0')
c= tensor(1.6886e+08, device='cuda:0')
c= tensor(1.6886e+08, device='cuda:0')
c= tensor(1.6886e+08, device='cuda:0')
c= tensor(1.6886e+08, device='cuda:0')
c= tensor(1.6887e+08, device='cuda:0')
c= tensor(1.6887e+08, device='cuda:0')
c= tensor(1.6887e+08, device='cuda:0')
c= tensor(1.6888e+08, device='cuda:0')
c= tensor(1.6889e+08, device='cuda:0')
c= tensor(1.6890e+08, device='cuda:0')
c= tensor(1.6890e+08, device='cuda:0')
c= tensor(1.6890e+08, device='cuda:0')
c= tensor(1.6890e+08, device='cuda:0')
c= tensor(1.6890e+08, device='cuda:0')
c= tensor(1.6890e+08, device='cuda:0')
c= tensor(1.6891e+08, device='cuda:0')
c= tensor(1.6891e+08, device='cuda:0')
c= tensor(1.6891e+08, device='cuda:0')
c= tensor(1.6891e+08, device='cuda:0')
c= tensor(1.6891e+08, device='cuda:0')
c= tensor(1.6891e+08, device='cuda:0')
c= tensor(1.6892e+08, device='cuda:0')
c= tensor(1.6892e+08, device='cuda:0')
c= tensor(1.6893e+08, device='cuda:0')
c= tensor(1.6893e+08, device='cuda:0')
c= tensor(1.6893e+08, device='cuda:0')
c= tensor(1.6894e+08, device='cuda:0')
c= tensor(1.6894e+08, device='cuda:0')
c= tensor(1.6894e+08, device='cuda:0')
c= tensor(1.6895e+08, device='cuda:0')
c= tensor(1.6896e+08, device='cuda:0')
c= tensor(1.6896e+08, device='cuda:0')
c= tensor(1.6896e+08, device='cuda:0')
c= tensor(1.6896e+08, device='cuda:0')
c= tensor(1.6896e+08, device='cuda:0')
c= tensor(1.6896e+08, device='cuda:0')
c= tensor(1.6897e+08, device='cuda:0')
c= tensor(1.6897e+08, device='cuda:0')
c= tensor(1.6897e+08, device='cuda:0')
c= tensor(1.6897e+08, device='cuda:0')
c= tensor(1.6897e+08, device='cuda:0')
c= tensor(1.6897e+08, device='cuda:0')
c= tensor(1.6897e+08, device='cuda:0')
c= tensor(1.6898e+08, device='cuda:0')
c= tensor(1.6898e+08, device='cuda:0')
c= tensor(1.6898e+08, device='cuda:0')
c= tensor(1.6898e+08, device='cuda:0')
c= tensor(1.6898e+08, device='cuda:0')
c= tensor(1.6899e+08, device='cuda:0')
c= tensor(1.6899e+08, device='cuda:0')
c= tensor(1.6899e+08, device='cuda:0')
c= tensor(1.6899e+08, device='cuda:0')
c= tensor(1.6900e+08, device='cuda:0')
c= tensor(1.6900e+08, device='cuda:0')
c= tensor(1.6901e+08, device='cuda:0')
c= tensor(1.6901e+08, device='cuda:0')
c= tensor(1.6901e+08, device='cuda:0')
c= tensor(1.6902e+08, device='cuda:0')
c= tensor(1.6902e+08, device='cuda:0')
c= tensor(1.6903e+08, device='cuda:0')
c= tensor(1.6920e+08, device='cuda:0')
c= tensor(1.6921e+08, device='cuda:0')
c= tensor(1.6921e+08, device='cuda:0')
c= tensor(1.6922e+08, device='cuda:0')
c= tensor(1.6922e+08, device='cuda:0')
c= tensor(1.6948e+08, device='cuda:0')
c= tensor(1.7156e+08, device='cuda:0')
c= tensor(1.7156e+08, device='cuda:0')
c= tensor(1.7178e+08, device='cuda:0')
c= tensor(1.7278e+08, device='cuda:0')
c= tensor(1.7279e+08, device='cuda:0')
c= tensor(2.0586e+08, device='cuda:0')
c= tensor(2.0586e+08, device='cuda:0')
c= tensor(2.0590e+08, device='cuda:0')
c= tensor(2.0673e+08, device='cuda:0')
c= tensor(2.2290e+08, device='cuda:0')
c= tensor(2.2290e+08, device='cuda:0')
c= tensor(2.2295e+08, device='cuda:0')
c= tensor(2.2310e+08, device='cuda:0')
c= tensor(2.2675e+08, device='cuda:0')
c= tensor(2.2726e+08, device='cuda:0')
c= tensor(2.2749e+08, device='cuda:0')
c= tensor(2.2762e+08, device='cuda:0')
c= tensor(2.2765e+08, device='cuda:0')
c= tensor(2.2774e+08, device='cuda:0')
c= tensor(2.3014e+08, device='cuda:0')
c= tensor(2.3014e+08, device='cuda:0')
c= tensor(2.3014e+08, device='cuda:0')
c= tensor(2.3048e+08, device='cuda:0')
c= tensor(2.3056e+08, device='cuda:0')
c= tensor(2.4112e+08, device='cuda:0')
c= tensor(2.4157e+08, device='cuda:0')
c= tensor(2.4157e+08, device='cuda:0')
c= tensor(2.4159e+08, device='cuda:0')
c= tensor(2.4159e+08, device='cuda:0')
c= tensor(2.4166e+08, device='cuda:0')
c= tensor(2.4233e+08, device='cuda:0')
c= tensor(2.4404e+08, device='cuda:0')
c= tensor(2.4426e+08, device='cuda:0')
c= tensor(2.4426e+08, device='cuda:0')
c= tensor(2.4427e+08, device='cuda:0')
c= tensor(2.4446e+08, device='cuda:0')
c= tensor(2.4518e+08, device='cuda:0')
c= tensor(2.4526e+08, device='cuda:0')
c= tensor(2.4527e+08, device='cuda:0')
c= tensor(2.5753e+08, device='cuda:0')
c= tensor(2.5754e+08, device='cuda:0')
c= tensor(2.5764e+08, device='cuda:0')
c= tensor(2.5859e+08, device='cuda:0')
c= tensor(2.5859e+08, device='cuda:0')
c= tensor(2.5869e+08, device='cuda:0')
c= tensor(2.5964e+08, device='cuda:0')
c= tensor(2.6387e+08, device='cuda:0')
c= tensor(2.6388e+08, device='cuda:0')
c= tensor(2.6393e+08, device='cuda:0')
c= tensor(2.6393e+08, device='cuda:0')
c= tensor(2.6394e+08, device='cuda:0')
c= tensor(2.6425e+08, device='cuda:0')
c= tensor(2.6427e+08, device='cuda:0')
c= tensor(2.6481e+08, device='cuda:0')
c= tensor(2.7196e+08, device='cuda:0')
c= tensor(2.7209e+08, device='cuda:0')
c= tensor(2.7211e+08, device='cuda:0')
c= tensor(2.7212e+08, device='cuda:0')
c= tensor(2.7367e+08, device='cuda:0')
c= tensor(2.7373e+08, device='cuda:0')
c= tensor(2.7374e+08, device='cuda:0')
c= tensor(2.7377e+08, device='cuda:0')
c= tensor(3.0732e+08, device='cuda:0')
c= tensor(3.0733e+08, device='cuda:0')
c= tensor(3.0853e+08, device='cuda:0')
c= tensor(3.0857e+08, device='cuda:0')
c= tensor(3.0876e+08, device='cuda:0')
c= tensor(3.0888e+08, device='cuda:0')
c= tensor(3.0938e+08, device='cuda:0')
c= tensor(3.1010e+08, device='cuda:0')
c= tensor(3.1011e+08, device='cuda:0')
c= tensor(3.1159e+08, device='cuda:0')
c= tensor(3.1247e+08, device='cuda:0')
c= tensor(3.1247e+08, device='cuda:0')
c= tensor(3.1262e+08, device='cuda:0')
c= tensor(3.1415e+08, device='cuda:0')
c= tensor(3.1898e+08, device='cuda:0')
c= tensor(3.1944e+08, device='cuda:0')
c= tensor(3.1947e+08, device='cuda:0')
c= tensor(3.1947e+08, device='cuda:0')
c= tensor(3.1956e+08, device='cuda:0')
c= tensor(3.1975e+08, device='cuda:0')
c= tensor(3.1976e+08, device='cuda:0')
c= tensor(3.1976e+08, device='cuda:0')
c= tensor(3.2039e+08, device='cuda:0')
c= tensor(3.2219e+08, device='cuda:0')
c= tensor(3.2679e+08, device='cuda:0')
c= tensor(3.2679e+08, device='cuda:0')
c= tensor(3.2682e+08, device='cuda:0')
c= tensor(3.2688e+08, device='cuda:0')
c= tensor(3.2690e+08, device='cuda:0')
c= tensor(3.2691e+08, device='cuda:0')
c= tensor(3.2692e+08, device='cuda:0')
c= tensor(3.3910e+08, device='cuda:0')
c= tensor(3.3922e+08, device='cuda:0')
c= tensor(3.3925e+08, device='cuda:0')
c= tensor(3.3928e+08, device='cuda:0')
c= tensor(3.3929e+08, device='cuda:0')
c= tensor(3.5416e+08, device='cuda:0')
c= tensor(3.5417e+08, device='cuda:0')
c= tensor(3.5530e+08, device='cuda:0')
c= tensor(3.5530e+08, device='cuda:0')
c= tensor(3.5530e+08, device='cuda:0')
c= tensor(3.5530e+08, device='cuda:0')
c= tensor(3.5550e+08, device='cuda:0')
c= tensor(3.5564e+08, device='cuda:0')
c= tensor(3.5569e+08, device='cuda:0')
c= tensor(3.5569e+08, device='cuda:0')
c= tensor(3.5570e+08, device='cuda:0')
c= tensor(3.6427e+08, device='cuda:0')
c= tensor(3.6440e+08, device='cuda:0')
c= tensor(3.6444e+08, device='cuda:0')
c= tensor(3.6522e+08, device='cuda:0')
c= tensor(3.6556e+08, device='cuda:0')
c= tensor(3.6557e+08, device='cuda:0')
c= tensor(3.6557e+08, device='cuda:0')
c= tensor(3.6563e+08, device='cuda:0')
c= tensor(3.6563e+08, device='cuda:0')
c= tensor(3.6563e+08, device='cuda:0')
c= tensor(3.6568e+08, device='cuda:0')
c= tensor(3.6568e+08, device='cuda:0')
c= tensor(3.6569e+08, device='cuda:0')
c= tensor(3.6569e+08, device='cuda:0')
c= tensor(3.6569e+08, device='cuda:0')
c= tensor(3.9337e+08, device='cuda:0')
c= tensor(3.9342e+08, device='cuda:0')
c= tensor(3.9360e+08, device='cuda:0')
c= tensor(3.9403e+08, device='cuda:0')
c= tensor(3.9403e+08, device='cuda:0')
c= tensor(3.9405e+08, device='cuda:0')
c= tensor(4.7204e+08, device='cuda:0')
c= tensor(4.9992e+08, device='cuda:0')
c= tensor(5.0044e+08, device='cuda:0')
c= tensor(5.0051e+08, device='cuda:0')
c= tensor(5.0051e+08, device='cuda:0')
c= tensor(5.0354e+08, device='cuda:0')
c= tensor(5.0504e+08, device='cuda:0')
c= tensor(5.0859e+08, device='cuda:0')
c= tensor(5.0862e+08, device='cuda:0')
c= tensor(5.1013e+08, device='cuda:0')
c= tensor(5.1331e+08, device='cuda:0')
c= tensor(5.1353e+08, device='cuda:0')
c= tensor(5.1354e+08, device='cuda:0')
c= tensor(5.1356e+08, device='cuda:0')
c= tensor(5.1357e+08, device='cuda:0')
c= tensor(5.1357e+08, device='cuda:0')
c= tensor(5.1468e+08, device='cuda:0')
c= tensor(5.1481e+08, device='cuda:0')
c= tensor(5.1481e+08, device='cuda:0')
c= tensor(5.1485e+08, device='cuda:0')
c= tensor(5.1486e+08, device='cuda:0')
c= tensor(5.1486e+08, device='cuda:0')
c= tensor(5.1555e+08, device='cuda:0')
c= tensor(5.1676e+08, device='cuda:0')
c= tensor(5.1684e+08, device='cuda:0')
c= tensor(5.1766e+08, device='cuda:0')
c= tensor(5.1830e+08, device='cuda:0')
c= tensor(5.1831e+08, device='cuda:0')
c= tensor(5.1839e+08, device='cuda:0')
c= tensor(5.1861e+08, device='cuda:0')
c= tensor(5.2024e+08, device='cuda:0')
c= tensor(5.2024e+08, device='cuda:0')
c= tensor(5.2773e+08, device='cuda:0')
c= tensor(5.3880e+08, device='cuda:0')
c= tensor(5.3896e+08, device='cuda:0')
c= tensor(5.3911e+08, device='cuda:0')
c= tensor(5.4147e+08, device='cuda:0')
c= tensor(5.4149e+08, device='cuda:0')
c= tensor(5.4149e+08, device='cuda:0')
c= tensor(5.4170e+08, device='cuda:0')
c= tensor(5.4302e+08, device='cuda:0')
c= tensor(5.4389e+08, device='cuda:0')
c= tensor(5.7834e+08, device='cuda:0')
c= tensor(5.7894e+08, device='cuda:0')
c= tensor(5.8009e+08, device='cuda:0')
c= tensor(5.8025e+08, device='cuda:0')
c= tensor(5.8116e+08, device='cuda:0')
c= tensor(5.8116e+08, device='cuda:0')
c= tensor(5.8118e+08, device='cuda:0')
c= tensor(5.8696e+08, device='cuda:0')
c= tensor(5.8699e+08, device='cuda:0')
c= tensor(5.8700e+08, device='cuda:0')
c= tensor(5.8710e+08, device='cuda:0')
c= tensor(5.8747e+08, device='cuda:0')
c= tensor(5.8805e+08, device='cuda:0')
c= tensor(5.8827e+08, device='cuda:0')
c= tensor(5.8832e+08, device='cuda:0')
c= tensor(5.8857e+08, device='cuda:0')
c= tensor(5.8857e+08, device='cuda:0')
c= tensor(5.8868e+08, device='cuda:0')
c= tensor(5.8873e+08, device='cuda:0')
c= tensor(5.8933e+08, device='cuda:0')
c= tensor(5.8933e+08, device='cuda:0')
c= tensor(5.9138e+08, device='cuda:0')
c= tensor(5.9138e+08, device='cuda:0')
c= tensor(5.9144e+08, device='cuda:0')
c= tensor(5.9146e+08, device='cuda:0')
c= tensor(5.9150e+08, device='cuda:0')
c= tensor(5.9150e+08, device='cuda:0')
c= tensor(5.9157e+08, device='cuda:0')
c= tensor(5.9160e+08, device='cuda:0')
c= tensor(5.9167e+08, device='cuda:0')
c= tensor(5.9190e+08, device='cuda:0')
c= tensor(5.9871e+08, device='cuda:0')
c= tensor(5.9871e+08, device='cuda:0')
c= tensor(5.9874e+08, device='cuda:0')
c= tensor(6.0107e+08, device='cuda:0')
c= tensor(6.0107e+08, device='cuda:0')
c= tensor(6.1774e+08, device='cuda:0')
c= tensor(6.1774e+08, device='cuda:0')
c= tensor(6.1849e+08, device='cuda:0')
c= tensor(6.2237e+08, device='cuda:0')
c= tensor(6.2238e+08, device='cuda:0')
c= tensor(6.2799e+08, device='cuda:0')
c= tensor(6.2810e+08, device='cuda:0')
c= tensor(6.3096e+08, device='cuda:0')
c= tensor(6.3097e+08, device='cuda:0')
c= tensor(6.3181e+08, device='cuda:0')
c= tensor(6.3181e+08, device='cuda:0')
c= tensor(6.3181e+08, device='cuda:0')
c= tensor(6.3182e+08, device='cuda:0')
c= tensor(6.3218e+08, device='cuda:0')
c= tensor(6.3224e+08, device='cuda:0')
c= tensor(6.3374e+08, device='cuda:0')
c= tensor(6.3375e+08, device='cuda:0')
c= tensor(6.3376e+08, device='cuda:0')
c= tensor(6.3377e+08, device='cuda:0')
c= tensor(6.3416e+08, device='cuda:0')
c= tensor(6.3416e+08, device='cuda:0')
c= tensor(6.3522e+08, device='cuda:0')
c= tensor(6.3583e+08, device='cuda:0')
c= tensor(6.3586e+08, device='cuda:0')
c= tensor(6.3586e+08, device='cuda:0')
c= tensor(6.3592e+08, device='cuda:0')
c= tensor(6.8496e+08, device='cuda:0')
c= tensor(6.8497e+08, device='cuda:0')
c= tensor(6.8501e+08, device='cuda:0')
c= tensor(6.8603e+08, device='cuda:0')
c= tensor(6.8639e+08, device='cuda:0')
c= tensor(6.8639e+08, device='cuda:0')
c= tensor(6.8639e+08, device='cuda:0')
c= tensor(6.9952e+08, device='cuda:0')
c= tensor(6.9956e+08, device='cuda:0')
c= tensor(6.9991e+08, device='cuda:0')
c= tensor(7.0002e+08, device='cuda:0')
c= tensor(7.0012e+08, device='cuda:0')
c= tensor(7.0048e+08, device='cuda:0')
c= tensor(7.0374e+08, device='cuda:0')
c= tensor(7.0444e+08, device='cuda:0')
c= tensor(7.0444e+08, device='cuda:0')
c= tensor(7.0478e+08, device='cuda:0')
c= tensor(7.0479e+08, device='cuda:0')
c= tensor(7.0483e+08, device='cuda:0')
c= tensor(7.0491e+08, device='cuda:0')
c= tensor(7.0492e+08, device='cuda:0')
c= tensor(7.0605e+08, device='cuda:0')
c= tensor(7.0607e+08, device='cuda:0')
c= tensor(7.0607e+08, device='cuda:0')
c= tensor(7.0609e+08, device='cuda:0')
c= tensor(7.0619e+08, device='cuda:0')
c= tensor(7.0746e+08, device='cuda:0')
c= tensor(7.0933e+08, device='cuda:0')
c= tensor(7.0935e+08, device='cuda:0')
c= tensor(7.0937e+08, device='cuda:0')
c= tensor(7.0943e+08, device='cuda:0')
c= tensor(7.0943e+08, device='cuda:0')
c= tensor(7.0943e+08, device='cuda:0')
c= tensor(7.0949e+08, device='cuda:0')
c= tensor(7.1277e+08, device='cuda:0')
c= tensor(7.1277e+08, device='cuda:0')
c= tensor(7.1277e+08, device='cuda:0')
c= tensor(7.1278e+08, device='cuda:0')
c= tensor(7.1853e+08, device='cuda:0')
c= tensor(7.3150e+08, device='cuda:0')
c= tensor(7.3168e+08, device='cuda:0')
c= tensor(7.3168e+08, device='cuda:0')
c= tensor(7.3181e+08, device='cuda:0')
c= tensor(7.3209e+08, device='cuda:0')
c= tensor(7.3209e+08, device='cuda:0')
c= tensor(7.3214e+08, device='cuda:0')
c= tensor(7.3218e+08, device='cuda:0')
c= tensor(7.3931e+08, device='cuda:0')
c= tensor(7.4026e+08, device='cuda:0')
c= tensor(7.4161e+08, device='cuda:0')
c= tensor(7.4162e+08, device='cuda:0')
c= tensor(7.4162e+08, device='cuda:0')
c= tensor(7.4162e+08, device='cuda:0')
c= tensor(7.4173e+08, device='cuda:0')
c= tensor(7.4177e+08, device='cuda:0')
c= tensor(7.4238e+08, device='cuda:0')
c= tensor(7.4290e+08, device='cuda:0')
c= tensor(7.4768e+08, device='cuda:0')
c= tensor(7.4769e+08, device='cuda:0')
c= tensor(7.4771e+08, device='cuda:0')
c= tensor(7.4780e+08, device='cuda:0')
c= tensor(8.0543e+08, device='cuda:0')
c= tensor(8.0563e+08, device='cuda:0')
c= tensor(8.0564e+08, device='cuda:0')
c= tensor(8.0564e+08, device='cuda:0')
c= tensor(8.0585e+08, device='cuda:0')
c= tensor(8.0585e+08, device='cuda:0')
c= tensor(8.0591e+08, device='cuda:0')
c= tensor(8.0591e+08, device='cuda:0')
c= tensor(8.0593e+08, device='cuda:0')
c= tensor(8.0595e+08, device='cuda:0')
c= tensor(8.0595e+08, device='cuda:0')
c= tensor(8.0595e+08, device='cuda:0')
c= tensor(8.0936e+08, device='cuda:0')
c= tensor(8.2811e+08, device='cuda:0')
c= tensor(8.2946e+08, device='cuda:0')
c= tensor(8.3024e+08, device='cuda:0')
c= tensor(8.3034e+08, device='cuda:0')
c= tensor(8.3036e+08, device='cuda:0')
c= tensor(8.3038e+08, device='cuda:0')
c= tensor(8.3054e+08, device='cuda:0')
c= tensor(8.3083e+08, device='cuda:0')
c= tensor(8.3291e+08, device='cuda:0')
c= tensor(8.3292e+08, device='cuda:0')
c= tensor(8.4448e+08, device='cuda:0')
c= tensor(8.4454e+08, device='cuda:0')
c= tensor(8.4461e+08, device='cuda:0')
c= tensor(8.4875e+08, device='cuda:0')
c= tensor(8.4903e+08, device='cuda:0')
c= tensor(8.5594e+08, device='cuda:0')
c= tensor(8.5785e+08, device='cuda:0')
c= tensor(8.5810e+08, device='cuda:0')
c= tensor(8.5832e+08, device='cuda:0')
c= tensor(8.5836e+08, device='cuda:0')
c= tensor(8.5872e+08, device='cuda:0')
c= tensor(8.5875e+08, device='cuda:0')
c= tensor(8.6065e+08, device='cuda:0')
c= tensor(8.9158e+08, device='cuda:0')
c= tensor(8.9298e+08, device='cuda:0')
c= tensor(8.9344e+08, device='cuda:0')
c= tensor(8.9356e+08, device='cuda:0')
c= tensor(8.9358e+08, device='cuda:0')
c= tensor(8.9367e+08, device='cuda:0')
c= tensor(8.9380e+08, device='cuda:0')
c= tensor(8.9485e+08, device='cuda:0')
c= tensor(8.9681e+08, device='cuda:0')
c= tensor(8.9681e+08, device='cuda:0')
c= tensor(9.0709e+08, device='cuda:0')
c= tensor(9.1821e+08, device='cuda:0')
c= tensor(9.1839e+08, device='cuda:0')
c= tensor(9.1839e+08, device='cuda:0')
c= tensor(9.1898e+08, device='cuda:0')
c= tensor(9.1961e+08, device='cuda:0')
c= tensor(9.1961e+08, device='cuda:0')
c= tensor(9.3953e+08, device='cuda:0')
c= tensor(9.4026e+08, device='cuda:0')
c= tensor(9.4030e+08, device='cuda:0')
c= tensor(9.4032e+08, device='cuda:0')
c= tensor(9.4032e+08, device='cuda:0')
c= tensor(9.4032e+08, device='cuda:0')
c= tensor(9.4035e+08, device='cuda:0')
c= tensor(9.4037e+08, device='cuda:0')
c= tensor(9.4090e+08, device='cuda:0')
c= tensor(1.2164e+09, device='cuda:0')
c= tensor(1.2164e+09, device='cuda:0')
c= tensor(1.2166e+09, device='cuda:0')
c= tensor(1.2166e+09, device='cuda:0')
c= tensor(1.2166e+09, device='cuda:0')
c= tensor(1.2166e+09, device='cuda:0')
c= tensor(1.2229e+09, device='cuda:0')
c= tensor(1.2230e+09, device='cuda:0')
c= tensor(1.2671e+09, device='cuda:0')
c= tensor(1.2672e+09, device='cuda:0')
c= tensor(1.2680e+09, device='cuda:0')
c= tensor(1.2680e+09, device='cuda:0')
c= tensor(1.2691e+09, device='cuda:0')
c= tensor(1.2790e+09, device='cuda:0')
c= tensor(1.2790e+09, device='cuda:0')
c= tensor(1.2790e+09, device='cuda:0')
c= tensor(1.2794e+09, device='cuda:0')
c= tensor(1.2795e+09, device='cuda:0')
c= tensor(1.2796e+09, device='cuda:0')
c= tensor(1.2802e+09, device='cuda:0')
c= tensor(1.2808e+09, device='cuda:0')
c= tensor(1.2809e+09, device='cuda:0')
c= tensor(1.2809e+09, device='cuda:0')
c= tensor(1.2814e+09, device='cuda:0')
c= tensor(1.2864e+09, device='cuda:0')
c= tensor(1.2864e+09, device='cuda:0')
c= tensor(1.2864e+09, device='cuda:0')
c= tensor(1.2899e+09, device='cuda:0')
c= tensor(1.2900e+09, device='cuda:0')
c= tensor(1.2922e+09, device='cuda:0')
c= tensor(1.2923e+09, device='cuda:0')
c= tensor(1.2942e+09, device='cuda:0')
c= tensor(1.2943e+09, device='cuda:0')
c= tensor(1.2971e+09, device='cuda:0')
c= tensor(1.2974e+09, device='cuda:0')
c= tensor(1.2974e+09, device='cuda:0')
c= tensor(1.2974e+09, device='cuda:0')
c= tensor(1.2975e+09, device='cuda:0')
c= tensor(1.2977e+09, device='cuda:0')
c= tensor(1.2996e+09, device='cuda:0')
c= tensor(1.3006e+09, device='cuda:0')
c= tensor(1.3006e+09, device='cuda:0')
c= tensor(1.3007e+09, device='cuda:0')
c= tensor(1.3009e+09, device='cuda:0')
c= tensor(1.3078e+09, device='cuda:0')
c= tensor(1.3080e+09, device='cuda:0')
c= tensor(1.3086e+09, device='cuda:0')
c= tensor(1.3095e+09, device='cuda:0')
c= tensor(1.3096e+09, device='cuda:0')
c= tensor(1.3096e+09, device='cuda:0')
c= tensor(1.3097e+09, device='cuda:0')
c= tensor(1.3103e+09, device='cuda:0')
c= tensor(1.3108e+09, device='cuda:0')
c= tensor(1.3111e+09, device='cuda:0')
c= tensor(1.3111e+09, device='cuda:0')
c= tensor(1.3111e+09, device='cuda:0')
c= tensor(1.3143e+09, device='cuda:0')
c= tensor(1.3146e+09, device='cuda:0')
c= tensor(1.3146e+09, device='cuda:0')
c= tensor(1.3146e+09, device='cuda:0')
c= tensor(1.3146e+09, device='cuda:0')
c= tensor(1.3148e+09, device='cuda:0')
c= tensor(1.3151e+09, device='cuda:0')
c= tensor(1.3157e+09, device='cuda:0')
c= tensor(1.3157e+09, device='cuda:0')
c= tensor(1.3354e+09, device='cuda:0')
c= tensor(1.3354e+09, device='cuda:0')
c= tensor(1.3354e+09, device='cuda:0')
c= tensor(1.3451e+09, device='cuda:0')
c= tensor(1.3456e+09, device='cuda:0')
c= tensor(1.3468e+09, device='cuda:0')
c= tensor(1.3480e+09, device='cuda:0')
c= tensor(1.3481e+09, device='cuda:0')
c= tensor(1.3488e+09, device='cuda:0')
c= tensor(1.3500e+09, device='cuda:0')
c= tensor(1.3500e+09, device='cuda:0')
c= tensor(1.3506e+09, device='cuda:0')
c= tensor(1.3506e+09, device='cuda:0')
c= tensor(1.3538e+09, device='cuda:0')
c= tensor(1.3567e+09, device='cuda:0')
c= tensor(1.3568e+09, device='cuda:0')
c= tensor(1.3571e+09, device='cuda:0')
c= tensor(1.3571e+09, device='cuda:0')
c= tensor(1.3573e+09, device='cuda:0')
c= tensor(1.3582e+09, device='cuda:0')
c= tensor(1.3582e+09, device='cuda:0')
c= tensor(1.3622e+09, device='cuda:0')
c= tensor(1.3664e+09, device='cuda:0')
c= tensor(1.3665e+09, device='cuda:0')
c= tensor(1.3665e+09, device='cuda:0')
c= tensor(1.3665e+09, device='cuda:0')
c= tensor(1.3753e+09, device='cuda:0')
c= tensor(1.3753e+09, device='cuda:0')
c= tensor(1.3754e+09, device='cuda:0')
c= tensor(1.3759e+09, device='cuda:0')
c= tensor(1.3763e+09, device='cuda:0')
c= tensor(1.3764e+09, device='cuda:0')
c= tensor(1.3765e+09, device='cuda:0')
c= tensor(1.3765e+09, device='cuda:0')
time to make c is 11.346272706985474
time for making loss is 11.346358299255371
p0 True
it  0 : 1139488256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4215009280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% |  8% |
memory (bytes)
4215222272
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  335803780.0
relative error loss 0.24394627
shape of L is 
torch.Size([])
memory (bytes)
4242145280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% |  8% |
memory (bytes)
4242145280
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  331329860.0
relative error loss 0.24069618
shape of L is 
torch.Size([])
memory (bytes)
4245688320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% |  8% |
memory (bytes)
4245716992
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  324349700.0
relative error loss 0.2356254
shape of L is 
torch.Size([])
memory (bytes)
4248748032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  8% |
memory (bytes)
4248748032
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  320776580.0
relative error loss 0.2330297
shape of L is 
torch.Size([])
memory (bytes)
4251893760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4252123136
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  317780540.0
relative error loss 0.23085321
shape of L is 
torch.Size([])
memory (bytes)
4255363072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4255391744
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  314144580.0
relative error loss 0.22821184
shape of L is 
torch.Size([])
memory (bytes)
4258488320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% |  8% |
memory (bytes)
4258488320
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  312531780.0
relative error loss 0.22704022
shape of L is 
torch.Size([])
memory (bytes)
4261584896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% |  8% |
memory (bytes)
4261814272
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  310916300.0
relative error loss 0.22586663
shape of L is 
torch.Size([])
memory (bytes)
4265021440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4265021440
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  308990980.0
relative error loss 0.22446798
shape of L is 
torch.Size([])
memory (bytes)
4268232704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4268232704
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  307877200.0
relative error loss 0.22365886
time to take a step is 299.57417702674866
it  1 : 1739149312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4271468544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4271468544
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  307877200.0
relative error loss 0.22365886
shape of L is 
torch.Size([])
memory (bytes)
4274499584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4274499584
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  306638140.0
relative error loss 0.22275876
shape of L is 
torch.Size([])
memory (bytes)
4277907456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  8% |
memory (bytes)
4277911552
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  305311940.0
relative error loss 0.22179532
shape of L is 
torch.Size([])
memory (bytes)
4281028608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  8% |
memory (bytes)
4281028608
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  304644670.0
relative error loss 0.22131059
shape of L is 
torch.Size([])
memory (bytes)
4284276736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  8% |
memory (bytes)
4284276736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% |  8% |
error is  303616320.0
relative error loss 0.22056355
shape of L is 
torch.Size([])
memory (bytes)
4287500288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% |  8% |
memory (bytes)
4287582208
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  302990720.0
relative error loss 0.22010908
shape of L is 
torch.Size([])
memory (bytes)
4290744320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% |  8% |
memory (bytes)
4290744320
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  302488450.0
relative error loss 0.21974419
shape of L is 
torch.Size([])
memory (bytes)
4294004736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4294037504
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  301358850.0
relative error loss 0.21892358
shape of L is 
torch.Size([])
memory (bytes)
4297158656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  8% |
memory (bytes)
4297158656
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  300711550.0
relative error loss 0.21845336
shape of L is 
torch.Size([])
memory (bytes)
4300443648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  8% |
memory (bytes)
4300443648
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  300321280.0
relative error loss 0.21816984
time to take a step is 290.25421714782715
it  2 : 1740109824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4303634432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  8% |
memory (bytes)
4303712256
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  300321280.0
relative error loss 0.21816984
shape of L is 
torch.Size([])
memory (bytes)
4306771968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  8% |
memory (bytes)
4306771968
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  299858700.0
relative error loss 0.21783379
shape of L is 
torch.Size([])
memory (bytes)
4310171648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4310171648
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  299230600.0
relative error loss 0.2173775
shape of L is 
torch.Size([])
memory (bytes)
4313391104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  8% |
memory (bytes)
4313391104
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  298562430.0
relative error loss 0.21689212
shape of L is 
torch.Size([])
memory (bytes)
4316581888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  8% |
memory (bytes)
4316610560
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  297920900.0
relative error loss 0.21642607
shape of L is 
torch.Size([])
memory (bytes)
4319809536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% |  8% |
memory (bytes)
4319809536
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  297550080.0
relative error loss 0.21615669
shape of L is 
torch.Size([])
memory (bytes)
4322975744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% |  8% |
memory (bytes)
4322975744
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  297268350.0
relative error loss 0.21595202
shape of L is 
torch.Size([])
memory (bytes)
4326277120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% |  8% |
memory (bytes)
4326277120
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  296885630.0
relative error loss 0.215674
shape of L is 
torch.Size([])
memory (bytes)
4329484288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4329484288
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  296746750.0
relative error loss 0.2155731
shape of L is 
torch.Size([])
memory (bytes)
4332683264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% |  8% |
memory (bytes)
4332716032
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  296382980.0
relative error loss 0.21530885
time to take a step is 291.70341992378235
it  3 : 1739149312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4335861760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4335861760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% |  8% |
error is  296382980.0
relative error loss 0.21530885
shape of L is 
torch.Size([])
memory (bytes)
4339109888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% |  8% |
memory (bytes)
4339150848
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  296384500.0
relative error loss 0.21530996
shape of L is 
torch.Size([])
memory (bytes)
4342255616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4342382592
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  296208500.0
relative error loss 0.2151821
shape of L is 
torch.Size([])
memory (bytes)
4345421824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% |  9% |
memory (bytes)
4345421824
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  295981700.0
relative error loss 0.21501733
shape of L is 
torch.Size([])
memory (bytes)
4348768256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  9% |
memory (bytes)
4348829696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% |  9% |
error is  295602430.0
relative error loss 0.21474181
shape of L is 
torch.Size([])
memory (bytes)
4351795200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% |  9% |
memory (bytes)
4352049152
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  295395840.0
relative error loss 0.21459173
shape of L is 
torch.Size([])
memory (bytes)
4355244032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4355244032
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  295188860.0
relative error loss 0.21444137
shape of L is 
torch.Size([])
memory (bytes)
4358328320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4358328320
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  294991870.0
relative error loss 0.21429826
shape of L is 
torch.Size([])
memory (bytes)
4361678848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
memory (bytes)
4361707520
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  294846600.0
relative error loss 0.21419273
shape of L is 
torch.Size([])
memory (bytes)
4364779520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  9% |
memory (bytes)
4364926976
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  294471680.0
relative error loss 0.21392037
time to take a step is 292.37858486175537
it  4 : 1740109824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
shape of L is 
torch.Size([])
memory (bytes)
4368125952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4368158720
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  294471680.0
relative error loss 0.21392037
shape of L is 
torch.Size([])
memory (bytes)
4371283968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% |  8% |
memory (bytes)
4371283968
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  294428800.0
relative error loss 0.21388923
shape of L is 
torch.Size([])
memory (bytes)
4374482944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4374589440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  294203650.0
relative error loss 0.21372566
shape of L is 
torch.Size([])
memory (bytes)
4377784320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% |  8% |
memory (bytes)
4377812992
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  294136450.0
relative error loss 0.21367684
shape of L is 
torch.Size([])
memory (bytes)
4380913664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4381020160
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  293951740.0
relative error loss 0.21354266
shape of L is 
torch.Size([])
memory (bytes)
4384231424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  8% |
memory (bytes)
4384264192
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  293763330.0
relative error loss 0.21340579
shape of L is 
torch.Size([])
memory (bytes)
4387405824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  8% |
memory (bytes)
4387405824
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  293591680.0
relative error loss 0.2132811
shape of L is 
torch.Size([])
memory (bytes)
4390678528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  8% |
memory (bytes)
4390707200
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  293444220.0
relative error loss 0.21317397
shape of L is 
torch.Size([])
memory (bytes)
4393828352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4393828352
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  293337200.0
relative error loss 0.21309623
shape of L is 
torch.Size([])
memory (bytes)
4397150208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% |  8% |
memory (bytes)
4397158400
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  293077760.0
relative error loss 0.21290775
time to take a step is 292.44027972221375
it  5 : 1740109824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4400377856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  8% |
memory (bytes)
4400377856
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  293077760.0
relative error loss 0.21290775
shape of L is 
torch.Size([])
memory (bytes)
4403499008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  8% |
memory (bytes)
4403605504
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  292931600.0
relative error loss 0.21280156
shape of L is 
torch.Size([])
memory (bytes)
4406804480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  8% |
memory (bytes)
4406833152
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  292757000.0
relative error loss 0.21267472
shape of L is 
torch.Size([])
memory (bytes)
4410048512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  8% |
memory (bytes)
4410048512
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  292635000.0
relative error loss 0.2125861
shape of L is 
torch.Size([])
memory (bytes)
4413235200
| ID | GPU | MEM |
------------------
|  0 | 15% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4413263872
| ID | GPU | MEM |
------------------
|  0 | 15% |  0% |
|  1 | 99% |  8% |
error is  292545800.0
relative error loss 0.2125213
shape of L is 
torch.Size([])
memory (bytes)
4416475136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4416475136
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  292455940.0
relative error loss 0.21245602
shape of L is 
torch.Size([])
memory (bytes)
4419670016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  8% |
memory (bytes)
4419698688
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  292342400.0
relative error loss 0.21237354
shape of L is 
torch.Size([])
memory (bytes)
4422893568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4422893568
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  292581000.0
relative error loss 0.21254687
shape of L is 
torch.Size([])
memory (bytes)
4426104832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  8% |
memory (bytes)
4426133504
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  292309380.0
relative error loss 0.21234955
shape of L is 
torch.Size([])
memory (bytes)
4429230080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% |  9% |
memory (bytes)
4429230080
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  292270720.0
relative error loss 0.21232148
time to take a step is 293.9415521621704
it  6 : 1740590080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
shape of L is 
torch.Size([])
memory (bytes)
4432568320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4432568320
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  292270720.0
relative error loss 0.21232148
shape of L is 
torch.Size([])
memory (bytes)
4435685376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  8% |
memory (bytes)
4435685376
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  292240400.0
relative error loss 0.21229944
shape of L is 
torch.Size([])
memory (bytes)
4438867968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% |  8% |
memory (bytes)
4439007232
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  292190980.0
relative error loss 0.21226354
shape of L is 
torch.Size([])
memory (bytes)
4442206208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% |  8% |
memory (bytes)
4442234880
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  292133900.0
relative error loss 0.21222207
shape of L is 
torch.Size([])
memory (bytes)
4445249536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  8% |
memory (bytes)
4445249536
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  292117380.0
relative error loss 0.21221007
shape of L is 
torch.Size([])
memory (bytes)
4448677888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% |  8% |
memory (bytes)
4448681984
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  292073470.0
relative error loss 0.21217819
shape of L is 
torch.Size([])
memory (bytes)
4451803136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  8% |
memory (bytes)
4451803136
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  292047870.0
relative error loss 0.21215959
shape of L is 
torch.Size([])
memory (bytes)
4455088128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  8% |
memory (bytes)
4455129088
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  291993730.0
relative error loss 0.21212025
shape of L is 
torch.Size([])
memory (bytes)
4458332160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4458332160
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  291887360.0
relative error loss 0.21204297
shape of L is 
torch.Size([])
memory (bytes)
4461387776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  8% |
memory (bytes)
4461514752
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  291830000.0
relative error loss 0.21200132
time to take a step is 291.0236701965332
it  7 : 1739629568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4464812032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  8% |
memory (bytes)
4464816128
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  291830000.0
relative error loss 0.21200132
shape of L is 
torch.Size([])
memory (bytes)
4467912704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  8% |
memory (bytes)
4468019200
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  291750000.0
relative error loss 0.21194321
shape of L is 
torch.Size([])
memory (bytes)
4471173120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% |  8% |
memory (bytes)
4471255040
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  291700350.0
relative error loss 0.21190713
shape of L is 
torch.Size([])
memory (bytes)
4474449920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% |  8% |
memory (bytes)
4474482688
| ID | GPU  | MEM |
-------------------
|  0 |   2% |  0% |
|  1 | 100% |  8% |
error is  291621900.0
relative error loss 0.21185012
shape of L is 
torch.Size([])
memory (bytes)
4477583360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  8% |
memory (bytes)
4477583360
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  291548540.0
relative error loss 0.21179685
shape of L is 
torch.Size([])
memory (bytes)
4480921600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4480925696
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  291480580.0
relative error loss 0.21174747
shape of L is 
torch.Size([])
memory (bytes)
4484042752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  8% |
memory (bytes)
4484042752
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  291375500.0
relative error loss 0.21167113
shape of L is 
torch.Size([])
memory (bytes)
4487258112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  8% |
memory (bytes)
4487368704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% |  8% |
error is  291311500.0
relative error loss 0.21162464
shape of L is 
torch.Size([])
memory (bytes)
4490551296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% |  8% |
memory (bytes)
4490588160
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  291250800.0
relative error loss 0.21158056
shape of L is 
torch.Size([])
memory (bytes)
4493750272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  8% |
memory (bytes)
4493750272
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  291186560.0
relative error loss 0.21153387
time to take a step is 293.20925521850586
it  8 : 1739629568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4497027072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4497027072
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  291186560.0
relative error loss 0.21153387
shape of L is 
torch.Size([])
memory (bytes)
4500131840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4500131840
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  291148030.0
relative error loss 0.21150589
shape of L is 
torch.Size([])
memory (bytes)
4503367680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% |  8% |
memory (bytes)
4503367680
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  291070080.0
relative error loss 0.21144927
shape of L is 
torch.Size([])
memory (bytes)
4506677248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% |  8% |
memory (bytes)
4506677248
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  291093120.0
relative error loss 0.211466
shape of L is 
torch.Size([])
memory (bytes)
4509712384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  8% |
memory (bytes)
4509904896
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  291029900.0
relative error loss 0.21142006
shape of L is 
torch.Size([])
memory (bytes)
4513099776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% |  9% |
memory (bytes)
4513132544
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  290965000.0
relative error loss 0.21137293
shape of L is 
torch.Size([])
memory (bytes)
4516220928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% |  9% |
memory (bytes)
4516220928
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  290861700.0
relative error loss 0.21129788
shape of L is 
torch.Size([])
memory (bytes)
4519452672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% |  9% |
memory (bytes)
4519452672
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  290769150.0
relative error loss 0.21123065
shape of L is 
torch.Size([])
memory (bytes)
4522758144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% |  9% |
memory (bytes)
4522758144
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  290689400.0
relative error loss 0.21117271
shape of L is 
torch.Size([])
memory (bytes)
4525977600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
memory (bytes)
4525977600
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  290636300.0
relative error loss 0.21113414
time to take a step is 305.4902415275574
it  9 : 1740590080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
shape of L is 
torch.Size([])
memory (bytes)
4529201152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4529229824
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  290636300.0
relative error loss 0.21113414
shape of L is 
torch.Size([])
memory (bytes)
4532342784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  8% |
memory (bytes)
4532342784
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  290603650.0
relative error loss 0.21111043
shape of L is 
torch.Size([])
memory (bytes)
4535545856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4535676928
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  290518270.0
relative error loss 0.2110484
shape of L is 
torch.Size([])
memory (bytes)
4538892288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% |  8% |
memory (bytes)
4538896384
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  290484220.0
relative error loss 0.21102366
shape of L is 
torch.Size([])
memory (bytes)
4542083072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  8% |
memory (bytes)
4542111744
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  290420860.0
relative error loss 0.21097764
shape of L is 
torch.Size([])
memory (bytes)
4545257472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  8% |
memory (bytes)
4545339392
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  290401660.0
relative error loss 0.21096368
shape of L is 
torch.Size([])
memory (bytes)
4548526080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4548554752
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  290363260.0
relative error loss 0.21093579
shape of L is 
torch.Size([])
memory (bytes)
4551696384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  8% |
memory (bytes)
4551696384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  290351870.0
relative error loss 0.21092752
shape of L is 
torch.Size([])
memory (bytes)
4554969088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% |  8% |
memory (bytes)
4555001856
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  290310530.0
relative error loss 0.21089748
shape of L is 
torch.Size([])
memory (bytes)
4558069760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% |  8% |
memory (bytes)
4558069760
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  290294400.0
relative error loss 0.21088576
time to take a step is 299.2178621292114
it  10 : 1739629568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4561440768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4561444864
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  290294400.0
relative error loss 0.21088576
shape of L is 
torch.Size([])
memory (bytes)
4564574208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  8% |
memory (bytes)
4564574208
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  290275200.0
relative error loss 0.21087182
shape of L is 
torch.Size([])
memory (bytes)
4567805952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  8% |
memory (bytes)
4567887872
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  290249100.0
relative error loss 0.21085285
shape of L is 
torch.Size([])
memory (bytes)
4571078656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4571107328
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  290220160.0
relative error loss 0.21083184
shape of L is 
torch.Size([])
memory (bytes)
4574150656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  8% |
memory (bytes)
4574318592
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  290185100.0
relative error loss 0.21080635
shape of L is 
torch.Size([])
memory (bytes)
4577525760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  8% |
memory (bytes)
4577558528
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  290161400.0
relative error loss 0.21078916
shape of L is 
torch.Size([])
memory (bytes)
4580741120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  8% |
memory (bytes)
4580741120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 86% |  8% |
error is  290126080.0
relative error loss 0.21076348
shape of L is 
torch.Size([])
memory (bytes)
4583989248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4583989248
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  290115970.0
relative error loss 0.21075614
shape of L is 
torch.Size([])
memory (bytes)
4587118592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4587200512
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  290080770.0
relative error loss 0.21073057
shape of L is 
torch.Size([])
memory (bytes)
4590317568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% |  8% |
memory (bytes)
4590419968
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  290056830.0
relative error loss 0.21071318
time to take a step is 294.14352798461914
it  11 : 1740109824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
4593643520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4593643520
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  290056830.0
relative error loss 0.21071318
shape of L is 
torch.Size([])
memory (bytes)
4596752384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
memory (bytes)
4596752384
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  290028400.0
relative error loss 0.21069254
shape of L is 
torch.Size([])
memory (bytes)
4600074240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4600074240
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  289993730.0
relative error loss 0.21066734
shape of L is 
torch.Size([])
memory (bytes)
4603219968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% |  9% |
memory (bytes)
4603219968
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  289987970.0
relative error loss 0.21066315
shape of L is 
torch.Size([])
memory (bytes)
4606517248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% |  9% |
memory (bytes)
4606517248
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  289936770.0
relative error loss 0.21062596
shape of L is 
torch.Size([])
memory (bytes)
4609724416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  9% |
memory (bytes)
4609724416
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  289915260.0
relative error loss 0.21061035
shape of L is 
torch.Size([])
memory (bytes)
4612886528
| ID | GPU | MEM |
------------------
|  0 | 13% |  0% |
|  1 | 25% |  9% |
memory (bytes)
4612886528
| ID | GPU  | MEM |
-------------------
|  0 |  10% |  0% |
|  1 | 100% |  9% |
error is  289869570.0
relative error loss 0.21057715
shape of L is 
torch.Size([])
memory (bytes)
4616192000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  9% |
memory (bytes)
4616196096
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  289824000.0
relative error loss 0.21054403
shape of L is 
torch.Size([])
memory (bytes)
4619419648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% |  9% |
memory (bytes)
4619419648
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  289915140.0
relative error loss 0.21061024
shape of L is 
torch.Size([])
memory (bytes)
4622512128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% |  9% |
memory (bytes)
4622639104
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  289803400.0
relative error loss 0.21052907
time to take a step is 305.6928324699402
it  12 : 1740110336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
shape of L is 
torch.Size([])
memory (bytes)
4625858560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% |  8% |
memory (bytes)
4625858560
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  289803400.0
relative error loss 0.21052907
shape of L is 
torch.Size([])
memory (bytes)
4629069824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% |  8% |
memory (bytes)
4629069824
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  289777920.0
relative error loss 0.21051057
shape of L is 
torch.Size([])
memory (bytes)
4632272896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4632272896
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  289755520.0
relative error loss 0.2104943
shape of L is 
torch.Size([])
memory (bytes)
4635512832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% |  9% |
memory (bytes)
4635512832
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  289719170.0
relative error loss 0.21046789
shape of L is 
torch.Size([])
memory (bytes)
4638699520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  9% |
memory (bytes)
4638732288
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  289692300.0
relative error loss 0.21044835
shape of L is 
torch.Size([])
memory (bytes)
4641857536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  9% |
memory (bytes)
4641857536
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  289677200.0
relative error loss 0.21043739
shape of L is 
torch.Size([])
memory (bytes)
4645150720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% |  9% |
memory (bytes)
4645179392
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  289663100.0
relative error loss 0.21042717
shape of L is 
torch.Size([])
memory (bytes)
4648370176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% |  9% |
memory (bytes)
4648370176
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  289642880.0
relative error loss 0.21041247
shape of L is 
torch.Size([])
memory (bytes)
4651622400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4651626496
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  289606400.0
relative error loss 0.21038596
shape of L is 
torch.Size([])
memory (bytes)
4654800896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4654800896
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  289608320.0
relative error loss 0.21038736
shape of L is 
torch.Size([])
memory (bytes)
4658073600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
memory (bytes)
4658077696
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  289586050.0
relative error loss 0.21037118
time to take a step is 333.85755586624146
it  13 : 1739630080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
shape of L is 
torch.Size([])
memory (bytes)
4661272576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  8% |
memory (bytes)
4661272576
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  289586050.0
relative error loss 0.21037118
shape of L is 
torch.Size([])
memory (bytes)
4664479744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4664479744
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  289561200.0
relative error loss 0.21035314
shape of L is 
torch.Size([])
memory (bytes)
4667740160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
memory (bytes)
4667740160
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  289532930.0
relative error loss 0.21033259
shape of L is 
torch.Size([])
memory (bytes)
4670849024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4670849024
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  289489400.0
relative error loss 0.21030098
shape of L is 
torch.Size([])
memory (bytes)
4674166784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% |  9% |
memory (bytes)
4674166784
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  289450240.0
relative error loss 0.21027252
shape of L is 
torch.Size([])
memory (bytes)
4677332992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  8% |
memory (bytes)
4677332992
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  289422080.0
relative error loss 0.21025206
shape of L is 
torch.Size([])
memory (bytes)
4680597504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% |  9% |
memory (bytes)
4680601600
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  289397630.0
relative error loss 0.2102343
shape of L is 
torch.Size([])
memory (bytes)
4683796480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% |  9% |
memory (bytes)
4683825152
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  289467400.0
relative error loss 0.21028498
shape of L is 
torch.Size([])
memory (bytes)
4687015936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
memory (bytes)
4687052800
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  289378940.0
relative error loss 0.21022072
shape of L is 
torch.Size([])
memory (bytes)
4690251776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
memory (bytes)
4690284544
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  289356800.0
relative error loss 0.21020465
time to take a step is 299.9363923072815
it  14 : 1740109824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
shape of L is 
torch.Size([])
memory (bytes)
4693413888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  8% |
memory (bytes)
4693413888
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  289356800.0
relative error loss 0.21020465
shape of L is 
torch.Size([])
memory (bytes)
4696711168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4696711168
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  289346430.0
relative error loss 0.2101971
shape of L is 
torch.Size([])
memory (bytes)
4699824128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  9% |
memory (bytes)
4699824128
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  289333380.0
relative error loss 0.21018763
shape of L is 
torch.Size([])
memory (bytes)
4703068160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
memory (bytes)
4703068160
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  289317630.0
relative error loss 0.21017618
shape of L is 
torch.Size([])
memory (bytes)
4706340864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
memory (bytes)
4706373632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  289307780.0
relative error loss 0.21016903
shape of L is 
torch.Size([])
memory (bytes)
4709560320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
memory (bytes)
4709560320
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  289284860.0
relative error loss 0.21015239
shape of L is 
torch.Size([])
memory (bytes)
4712800256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% |  9% |
memory (bytes)
4712800256
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  289278720.0
relative error loss 0.21014792
shape of L is 
torch.Size([])
memory (bytes)
4715925504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4716027904
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  289270140.0
relative error loss 0.21014169
shape of L is 
torch.Size([])
memory (bytes)
4719177728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4719177728
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  289254400.0
relative error loss 0.21013026
shape of L is 
torch.Size([])
memory (bytes)
4722429952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4722454528
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  289238660.0
relative error loss 0.21011882
time to take a step is 309.19671869277954
sum tnnu_Z after tensor(5982378., device='cuda:0')
shape of features
(5907,)
shape of features
(5907,)
number of orig particles 23627
number of new particles after remove low mass 23627
tnuZ shape should be parts x labs
torch.Size([23627, 702])
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
error without small mass is  335751500.0
relative error without small mass is  0.24390829
nnu_Z shape should be number of particles by maxV
(23627, 702)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
shape of features
(23627,)
Wed Feb 1 12:13:33 EST 2023
