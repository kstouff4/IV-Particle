Thu Feb 2 01:35:46 EST 2023
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  0% |  0% |
numbers of X: 25298161
numbers of Z: 18293
shape of features
(18293,)
shape of features
(18293,)
ZX	Vol	Parts	Cubes	Eps
Z	0.018807406244451094	18293	18.293	0.10092869480301053
X	0.013994642005520802	629	0.629	0.281257243420803
X	0.014335097175235002	10392	10.392	0.11131845439986285
X	0.014331371825798283	2251	2.251	0.18534068179679955
X	0.01443924134844248	9593	9.593	0.11460317343920527
X	0.014381317022524969	6317	6.317	0.1315515373369129
X	0.014471643507401603	20734	20.734	0.08870435053544343
X	0.014763618226599901	38068	38.068	0.0729251921072472
X	0.014501073169483429	23074	23.074	0.08565625551986908
X	0.014406974136839081	6435	6.435	0.13082017982156827
X	0.014123009310854036	14605	14.605	0.09888761320771385
X	0.01436540129861877	4144	4.144	0.15134427779191453
X	0.014438665303012651	57772	57.772	0.06298974964626516
X	0.016877022058283432	6993	6.993	0.13413561246342004
X	0.014497612375794536	274141	274.141	0.03753538116299444
X	0.014488012385388256	15191	15.191	0.09843302116715794
X	0.014600519450767568	28261	28.261	0.08024049405444157
X	0.015026055838214411	43548	43.548	0.07013889637578855
X	0.014320765459680865	21027	21.027	0.08798250154182448
X	0.014464226893844618	114722	114.722	0.05014367639368184
X	0.014478337340332601	75206	75.206	0.05774158933567792
X	0.014120670814868475	12043	12.043	0.1054484533404579
X	0.01655470354584525	127370	127.37	0.050654506242065314
X	0.014439316342852894	6212	6.212	0.13246619490933575
X	0.014450169702253848	25883	25.883	0.08234162719131315
X	0.013798588219465772	1066	1.066	0.2347969673702868
X	0.014489056949280219	50674	50.674	0.06587970474490959
X	0.014444863166248819	37262	37.262	0.07291481141778153
X	0.01444628985720787	2801	2.801	0.1727750677311491
X	0.014494423652980837	26550	26.55	0.08172949369767817
X	0.017285081539871374	823809	823.809	0.027581314904945507
X	0.01438381562486724	6452	6.452	0.13063511029252173
X	0.015121369956144364	502656	502.656	0.03110093335719677
X	0.014315124629007633	11774	11.774	0.10673094809660677
X	0.014453633741581363	6323	6.323	0.13172997857690028
X	0.014404656872829942	7515	7.515	0.12421993878497022
X	0.014822532234666003	93467	93.467	0.05412792350525097
X	0.014463761934674026	85207	85.207	0.05536925287413121
X	0.014270914784103908	862	0.862	0.25486772889685605
X	0.016327230508372394	5872	5.872	0.1406184022666042
X	0.013742611781355	1532	1.532	0.20777988630476324
X	0.014544103727314994	2469	2.469	0.1806017514690007
X	0.01382136595951208	1010	1.01	0.23919009841448585
X	0.01325009179952531	374	0.374	0.32843474518644605
X	0.01430778914914616	2075	2.075	0.1903347751794836
X	0.014061278436897745	461	0.461	0.31244575428186916
X	0.013748628734082485	689	0.689	0.2712356188941047
X	0.014145392333933571	2169	2.169	0.1868322375924754
X	0.01433256193395337	1588	1.588	0.20820497837222765
X	0.014126978697492906	881	0.881	0.2521686696621207
X	0.014241055143322974	6143	6.143	0.13234896547462416
X	0.014393808613624605	8074	8.074	0.12125388421774792
X	0.014028833063250918	2170	2.17	0.18628902562935454
X	0.014187942575287599	3647	3.647	0.157275552521595
X	0.014200363598770551	1330	1.33	0.22019930363265672
X	0.014419035165436446	3560	3.56	0.15940260005729345
X	0.01435579957401081	3235	3.235	0.16433031981328994
X	0.013971346994265584	1013	1.013	0.2398149635096119
X	0.014295923093752041	2100	2.1	0.18952403883817426
X	0.014274432210849796	1605	1.605	0.2071864115724
X	0.013883067870371042	2151	2.151	0.18618656751565793
X	0.01445841987854813	3758	3.758	0.15669483652256594
X	0.01393084904006799	975	0.975	0.2426559753137003
X	0.014487563879421104	7593	7.593	0.12403016496770089
X	0.014262946917476059	3135	3.135	0.16570053357816913
X	0.01436364343401741	1385	1.385	0.2180745311274285
X	0.014359417601436962	1346	1.346	0.22013913483320094
X	0.013556831246461138	1199	1.199	0.22444658520998606
X	0.013997900739960906	2456	2.456	0.1786260723579059
X	0.01433108286047329	2233	2.233	0.1858361051245429
X	0.013835168223622851	1331	1.331	0.21824055775157541
X	0.013543435110813937	2386	2.386	0.17838272637054123
X	0.014122631354065497	1765	1.765	0.20001242299634447
X	0.014252669730112836	1484	1.484	0.21256288711672472
X	0.014062601613422925	1154	1.154	0.23011942927818027
X	0.014305917492947054	2717	2.717	0.17397039886820967
X	0.015288614631048691	3281	3.281	0.1670272386052361
X	0.013719433957185662	586	0.586	0.28607474915427594
X	0.014019697408579711	282	0.282	0.36770243903816857
X	0.01414805802207377	1847	1.847	0.1971257289243877
X	0.0167406856379019	6800	6.8	0.13502725622478465
X	0.014110912694023267	2121	2.121	0.18807806389289955
X	0.013931216876483266	689	0.689	0.2724310538821492
X	0.016018642955403894	6753	6.753	0.13336534431773805
X	0.013646110758016925	906	0.906	0.2469601601289912
X	0.013662442122040925	1569	1.569	0.20573197590792133
X	0.014426603581219987	813	0.813	0.2608301858925406
X	0.016223097970009256	2356	2.356	0.19024773789764646
X	0.014030675554590113	611	0.611	0.2842360269504988
X	0.01409127190501773	3013	3.013	0.16723109026090177
X	0.014126797170336006	1665	1.665	0.2039591330500214
X	0.014108970311690837	885	0.885	0.25168114610522996
X	0.0140187897844704	842	0.842	0.25534820580251244
X	0.014040375499765893	779	0.779	0.26218857010549246
X	0.01406392620866424	1969	1.969	0.19258403760885304
X	0.014281526268727097	3297	3.297	0.16301161819654436
X	0.014077341310659604	2857	2.857	0.1701649589641153
X	0.01596851332373315	3356	3.356	0.16819542461981532
X	0.013827963424355525	8029	8.029	0.11986680275485763
X	0.01401657616080192	2459	2.459	0.17863277194588467
X	0.014051843273496482	1111	1.111	0.23299132228066258
X	0.01439565756760929	8017	8.017	0.12154577709883775
X	0.01387841373253614	1381	1.381	0.21579864688165237
X	0.014360880469072204	3242	3.242	0.1642313334076736
X	0.0142145705498952	1173	1.173	0.22969166914664246
X	0.014102007615396232	2066	2.066	0.18969252139963624
X	0.014062324172544459	1368	1.368	0.2174321044733903
X	0.014477517740565181	3579	3.579	0.15933485976545683
X	0.01376774211675171	374	0.374	0.33265728612125606
X	0.014264375022288242	1077	1.077	0.2365986968814278
X	0.014368425577536217	1174	1.174	0.23045193655610297
X	0.01432091849882816	996	0.996	0.24316655658779987
X	0.014180693048058138	834	0.834	0.25714443815198884
X	0.014860628628925851	1055	1.055	0.24150617552216774
X	0.013941816564644244	1847	1.847	0.1961631783701465
X	0.014315816037184109	2440	2.44	0.1803607755897643
X	0.013882568000541438	988	0.988	0.24130757482505563
X	0.013876392297499708	737	0.737	0.26603351977660483
X	0.013642092014370884	503	0.503	0.3004491616103939
X	0.014373264768349173	5321	5.321	0.1392688712112522
X	0.013919899893074544	918	0.918	0.24751288383155473
X	0.014469761743251775	6140	6.14	0.13307536611511406
X	0.013592098637601841	963	0.963	0.241668516870976
X	0.014216790207869472	1204	1.204	0.22771502618999023
X	0.014259139965634465	1146	1.146	0.23172322890077623
X	0.013900240672323804	1117	1.117	0.23173397728797818
X	0.014136202861177003	2101	2.101	0.1887856177990113
X	0.013884731162451508	1334	1.334	0.21833693793113892
X	0.013950450274970617	1533	1.533	0.20877669752015826
X	0.01603997628927268	13923	13.923	0.10483113782596527
X	0.014966053382464806	3381	3.381	0.16419277381286818
X	0.014303417380292949	3759	3.759	0.156119024398922
X	0.014245740940250403	996	0.996	0.24274030949223652
X	0.014137421746158901	2121	2.121	0.18819576599668977
X	0.014077220249211405	932	0.932	0.24719157484797755
X	0.014270549895318549	2591	2.591	0.17660016233561437
X	0.014069524093808176	770	0.77	0.26338814731470095
X	0.013460609944357414	597	0.597	0.28250760032359956
X	0.014729205748613266	1012	1.012	0.24415536516830982
X	0.014404467166012085	1105	1.105	0.23534861126562687
X	0.014208766257905365	758	0.758	0.265641376153575
X	0.014322320482840602	1578	1.578	0.208594152407642
X	0.014235956467773802	828	0.828	0.25809847481705
X	0.015867175453784296	8002	8.002	0.12563202470919396
X	0.014229550499955723	5965	5.965	0.13361654705119255
X	0.014365918450867456	4675	4.675	0.14538421657318648
X	0.013926403032919253	673	0.673	0.2745414765690821
X	0.013943644920789918	1426	1.426	0.21383835085191535
X	0.013798158741180303	2065	2.065	0.18835060823626482
X	0.014107647014917842	900	0.9	0.2502672520697014
X	0.014226313533354787	1482	1.482	0.21252729961145841
X	0.013887847703317264	1489	1.489	0.2104974064237245
X	0.0163546320518841	4373	4.373	0.15522217580647021
X	0.014145304223471876	2368	2.368	0.1814443843291188
X	0.014104433136655813	7744	7.744	0.12212280306122562
X	0.013996953452479304	1933	1.933	0.193464174581174
X	0.017688056396122257	16630	16.63	0.10207732682872384
X	0.0138944287071207	1129	1.129	0.23087784013328838
X	0.01400443889845785	2157	2.157	0.18655426800764832
X	0.014019610777625466	2482	2.482	0.17809212931618737
X	0.013965245153030046	260	0.26	0.37730428603675276
X	0.016132577220747013	13500	13.5	0.106118229302992
X	0.014267035755449704	1491	1.491	0.21230099783698123
X	0.01434075989726035	2745	2.745	0.1735174960466805
X	0.014114347060673302	864	0.864	0.2537361523241054
X	0.01426836879163904	1812	1.812	0.19894760295560823
X	0.01432909825893078	765	0.765	0.2655741551346775
X	0.014144971763054003	1238	1.238	0.22523047187718495
X	0.013865431699670602	1654	1.654	0.20314180025780285
X	0.014471892540503285	2499	2.499	0.17957795612548505
X	0.013834415554551355	298	0.298	0.35940285074338707
X	0.013751477743790853	789	0.789	0.2592730972125344
X	0.013811038140434988	697	0.697	0.2706021121915315
X	0.01447885634434742	4045	4.045	0.15296963992651716
X	0.01379447341618329	678	0.678	0.2729973790591337
X	0.015141472341225024	2747	2.747	0.1766457322550525
X	0.014201918976657558	2055	2.055	0.19047810779112487
X	0.014136431787085233	2450	2.45	0.17935957761946056
X	0.014329466755500538	3029	3.029	0.16787147974170433
X	0.014046376333628801	2409	2.409	0.1799875643482917
X	0.01356441060400552	1132	1.132	0.22883274324695171
X	0.01436740199761104	1672	1.672	0.2048239120734194
X	0.014178710336249671	2515	2.515	0.17797796917705583
X	0.013965003886518735	2387	2.387	0.18018952784832643
X	0.014243539834130464	1135	1.135	0.23238460718956905
X	0.01404772148818059	3201	3.201	0.16372188568721385
X	0.014152858554464656	2561	2.561	0.17679834526617644
X	0.014300207406079866	2983	2.983	0.1686150598453187
X	0.014154093928734358	2453	2.453	0.17936106543415348
X	0.014115541885860181	2938	2.938	0.16873891589109594
X	0.014211260132353727	7336	7.336	0.12465922487941539
X	0.014076467609902976	1114	1.114	0.23291788158889443
X	0.013878079013247384	278	0.278	0.36820938464623754
X	0.013968078148711337	1028	1.028	0.2386242156181791
X	0.014333129508691402	1098	1.098	0.23545769866659014
X	0.014253135921664974	1707	1.707	0.20287364641101452
X	0.014245002370772709	2251	2.251	0.18496760657265263
X	0.013660242386236305	875	0.875	0.24992907633067554
X	0.013552137584588338	766	0.766	0.2605711586780015
X	0.013856018590274771	1775	1.775	0.19837184533313618
X	0.01412748510890334	721	0.721	0.26959377779528215
X	0.01444032410322811	2249	2.249	0.18586423496959234
X	0.014044997492066258	791	0.791	0.260884570373065
X	0.014347749275910275	4265	4.265	0.1498378666748303
X	0.014801342104022522	4710	4.71	0.14647383860463487
X	0.014439952843194517	1979	1.979	0.19395753687783726
X	0.013289247697239088	497	0.497	0.2990299368924059
X	0.01793124369890574	5323	5.323	0.14990564323125444
X	0.01414552024170309	2235	2.235	0.18497532498504388
X	0.01384424224201458	1370	1.37	0.2161969390698114
X	0.01715085273517748	8634	8.634	0.1257065453101579
X	0.014328321854556481	6482	6.482	0.13026530697534996
X	0.014272575773674367	1472	1.472	0.21323812205364293
X	0.01398427765360666	1114	1.114	0.2324082902690701
X	0.013795231267061035	875	0.875	0.2507496357564838
X	0.013863600787840345	636	0.636	0.27934418986104165
X	0.013391806723888101	769	0.769	0.2592015599949436
X	0.01409626884930504	756	0.756	0.2651718750496175
X	0.01354005878031899	397	0.397	0.32429731102677156
X	0.014075124125285794	2876	2.876	0.16978049049574487
X	0.014200264795068722	1227	1.227	0.22619549965724697
X	0.01397488017469968	2264	2.264	0.1834385280049309
X	0.014303259635984484	1277	1.277	0.22374227086213438
X	0.01407440905822617	2575	2.575	0.17615067509177834
X	0.013931837747536943	1124	1.124	0.23142700353518825
X	0.014001189381706759	3688	3.688	0.15600002664294216
X	0.014099130837367486	3137	3.137	0.16502862227114393
X	0.013454365536028797	659	0.659	0.27331235819445504
X	0.01609415815674726	631	0.631	0.2943605699765282
X	0.014166265622959804	2390	2.39	0.18097523945284108
X	0.013532950526669571	443	0.443	0.31260524174908727
X	0.014067225320897263	734	0.734	0.267611102946359
X	0.014237107756415117	1179	1.179	0.22942249000139436
X	0.014284836074400764	1967	1.967	0.19365273441709463
X	0.015305921059712266	2974	2.974	0.17265248556321988
X	0.01419806836531965	1452	1.452	0.2138393122070282
X	0.014186276464780002	937	0.937	0.2473866593961415
X	0.013894871480904064	1084	1.084	0.2340319137482323
X	0.01434177184599757	5050	5.05	0.14161322408118682
X	0.013791225214477008	536	0.536	0.2952201661545771
X	0.014353822737506659	8329	8.329	0.11989237741986612
X	0.014134384280992049	36031	36.031	0.07320378322865666
X	0.014284020966261064	1694	1.694	0.20353808822483102
X	0.014286983486368155	3248	3.248	0.16384813688658076
X	0.01333194143046188	363	0.363	0.3324009507782483
X	0.01429745775103252	3446	3.446	0.16068716290975843
X	0.01442030385851991	51912	51.912	0.06524825231718935
X	0.015642307253949855	153547	153.547	0.046703901196403275
X	0.014390469442963866	1631	1.631	0.2066364824103342
X	0.014434306036861362	74300	74.3	0.05791650750867792
X	0.014392330675973373	15530	15.53	0.09749594601967333
X	0.014173644911253442	12467	12.467	0.10436940227680805
X	0.014372994806559663	39043	39.043	0.07166955302732787
X	0.014002696547508242	743	0.743	0.2661180226658564
X	0.016848265092664596	2768	2.768	0.18258413307590327
X	0.014433306791458184	94563	94.563	0.05344185615744688
X	0.0165867332621392	257886	257.886	0.0400661578981948
X	0.014011525620984852	2184	2.184	0.18581364258762728
X	0.014492984084526764	21140	21.14	0.08817611175404914
X	0.014224898849195363	9282	9.282	0.1152928940697567
X	0.014389514801876354	26618	26.618	0.08146234540946651
X	0.01447186508833197	57497	57.497	0.06313833232289505
X	0.01540144074203484	21862	21.862	0.0889796076488982
X	0.014521219926310818	15763	15.763	0.09730191979902074
X	0.014262920857581601	12810	12.81	0.10364613559042586
X	0.014125127962881606	1527	1.527	0.20991869505534771
X	0.014437901775505486	157406	157.406	0.045098379138764
X	0.014130471873034246	1906	1.906	0.19498951372140239
X	0.01437753548537483	1653	1.653	0.2056540176051258
X	0.014426291522951475	17675	17.675	0.09345411458557377
X	0.014340898801028068	50335	50.335	0.06580144528857766
X	0.014678054957234432	101076	101.076	0.05256219433156029
X	0.014492328317463934	52714	52.714	0.06502355812786077
X	0.013893821691881185	593	0.593	0.2861468762233619
X	0.01436461810111103	11458	11.458	0.10782722341326291
X	0.0143625462255861	5970	5.97	0.13399410825531533
X	0.014394253966482239	22175	22.175	0.08658483708756494
X	0.014469214084124292	31318	31.318	0.07730667140056494
X	0.014438780229634289	3032	3.032	0.16824174714028622
X	0.014438154124760847	21454	21.454	0.08763302413965034
X	0.013598098882749609	361	0.361	0.3352151701240053
X	0.014395797427262439	3356	3.356	0.1624817513380711
X	0.014409804990784994	51819	51.819	0.06527141462628497
X	0.01416620199790786	36359	36.359	0.07303771314122742
X	0.014433514555620102	39314	39.314	0.07160471498199203
X	0.014147967301448832	3293	3.293	0.1625676416679737
X	0.015108420173545049	177803	177.803	0.0439636076422126
X	0.014505600286695152	6318	6.318	0.13192244592321603
X	0.01437513321152304	16552	16.552	0.0954085143747856
X	0.014392690678754197	51128	51.128	0.06553818285801197
X	0.014379009661451244	1935	1.935	0.19514136359120846
X	0.014405690552164804	75720	75.72	0.05751412170152337
X	0.015268976484111314	150164	150.164	0.046674681811362057
X	0.01529247449521211	141370	141.37	0.047647505510297006
X	0.014980757701227473	10054	10.054	0.11421723250767384
X	0.014959144810948989	9834	9.834	0.11500732994081182
X	0.014262690617691252	4599	4.599	0.14582971175685774
X	0.014432355474645029	2818	2.818	0.17237147957351867
X	0.015499027155861777	35446	35.446	0.07590069198203946
X	0.014276158987406844	4579	4.579	0.14608767505036824
X	0.014337497797370159	16077	16.077	0.0962548860341466
X	0.01538787918831941	30575	30.575	0.07954339088579844
X	0.014436510411155724	7279	7.279	0.12564064858883295
X	0.014139621109468842	8347	8.347	0.11920719058570385
X	0.014457723896110601	3791	3.791	0.15623633669187867
X	0.015157940748284877	49627	49.627	0.0673453158665636
X	0.01439263099899375	7437	7.437	0.12461800752134042
X	0.01440026082449277	7172	7.172	0.1261566144781978
X	0.014347334123641866	8478	8.478	0.11916789465479852
X	0.014471299608795294	133905	133.905	0.04763252240634541
X	0.014414396749613444	10135	10.135	0.11245814258426991
X	0.014427814651526162	89380	89.38	0.05444860215645706
X	0.014415629199224454	3342	3.342	0.1627830016450317
X	0.013827531785176206	29761	29.761	0.07745195453741635
X	0.014584461542070049	7772	7.772	0.12334433349492892
X	0.015065976464498835	134322	134.322	0.048226237250067396
X	0.014526660782589072	22156	22.156	0.08687433227938458
X	0.014317072378110656	2188	2.188	0.18704049116737967
X	0.014979160156868098	79622	79.622	0.05729958290434446
X	0.014483368430218399	89809	89.809	0.05443145028713345
X	0.014182882373637382	5264	5.264	0.13914989196528837
X	0.014448250608462401	56907	56.907	0.063321305219838
X	0.017702990266568065	81519	81.519	0.06010758266134111
X	0.01748650345219109	224206	224.206	0.04272531261902565
X	0.014493406422963604	23778	23.778	0.08478747773667866
X	0.013565931148173178	708	0.708	0.267591378573437
X	0.014476890755470302	5512	5.512	0.1379712808578629
X	0.014789249529156792	5582	5.582	0.1383732372716897
X	0.0143301712478144	10001	10.001	0.11273773823579011
X	0.014467693220336655	13257	13.257	0.1029559250314615
X	0.014242952425429061	1182	1.182	0.2292595922736387
X	0.014454229725050786	38433	38.433	0.07218221790990255
X	0.014473347646083295	79832	79.832	0.05659751041774654
X	0.01402211670565042	1086	1.086	0.23459995117062468
X	0.014240442104595021	2941	2.941	0.16917758299548563
X	0.014340564403130241	10518	10.518	0.11088624898574663
X	0.016806340145955702	8050	8.05	0.12780830921093883
X	0.01435089269156591	4814	4.814	0.1439210361518752
X	0.015583908701836752	6938	6.938	0.13096262353294436
X	0.014378049434263393	2369	2.369	0.18240845739158223
X	0.014373823930153802	7011	7.011	0.12703716242496055
X	0.01447855203679661	23529	23.529	0.08505644371823584
X	0.014454973964901943	9044	9.044	0.11691913312608068
X	0.016517819414716573	25730	25.73	0.08626559905102058
X	0.014436536536363983	5685	5.685	0.1364303044975767
X	0.016252701761752822	249649	249.649	0.04022832145122087
X	0.014836172477167203	4674	4.674	0.14696403088743323
X	0.014448860725261218	51631	51.631	0.065409529129593
X	0.013694770885533463	542	0.542	0.29343944648348713
X	0.014181498711735697	1764	1.764	0.20032778218955322
X	0.014461278082396275	3549	3.549	0.15972279182248086
X	0.014701591067604249	7943	7.943	0.12277970680720582
X	0.01669015950025832	2640	2.64	0.18490650365699832
X	0.014431304239944945	93532	93.532	0.053635020858743485
X	0.01438145706036501	3299	3.299	0.16335791854036516
X	0.01405942452970009	2263	2.263	0.18383477319349298
X	0.017169026039261143	156424	156.424	0.04787925939164638
X	0.014399049025454727	43473	43.473	0.06918916133469906
X	0.014345446831353862	32070	32.07	0.0764783260216508
X	0.014514559370151017	118258	118.258	0.049696344209915075
X	0.014477780366172501	180871	180.871	0.04309673288849181
X	0.014289141363852387	4837	4.837	0.1434861606738715
X	0.014205535979723863	2623	2.623	0.17561156421649804
X	0.014340370024775268	10322	10.322	0.1115832057210182
X	0.013925162092931253	987	0.987	0.24163566484595306
X	0.013985072257944276	2104	2.104	0.1880209862361628
X	0.01439804950053194	8401	8.401	0.11967154350886444
X	0.013838062605307582	1388	1.388	0.21522626792811886
X	0.015706049262535413	3385	3.385	0.16678977676969128
X	0.014326140495685088	1695	1.695	0.20369787627695846
X	0.014335305729569427	2837	2.837	0.17159944330613003
X	0.016769299166440283	118727	118.727	0.05207810099592991
X	0.013957744360671286	9494	9.494	0.11370738471735019
X	0.014415058226586053	50037	50.037	0.06604526886888734
X	0.014204591275604911	2246	2.246	0.18492955727645732
X	0.014195676235000657	4514	4.514	0.14650918202605923
X	0.014087949730125842	8112	8.112	0.12020059490026164
X	0.01735266386782868	344380	344.38	0.03693539468732778
X	0.016696246375385398	308413	308.413	0.03782933639698333
X	0.014865023290122856	5982	5.982	0.13544814504246447
X	0.014495951940862785	35713	35.713	0.074041203422542
X	0.014002067525766262	1445	1.445	0.2131938996285479
X	0.014093158381480975	6761	6.761	0.12774163354573595
X	0.014394346014708953	29206	29.206	0.07899021315293585
X	0.01444900632890306	10114	10.114	0.11262591803072833
X	0.014419676567102302	10317	10.317	0.11180657991513687
X	0.01448250177627418	43290	43.29	0.06942011597425478
X	0.016374615905845005	393641	393.641	0.03464896490158921
X	0.014218826894456015	19857	19.857	0.08946435675570948
X	0.013818324135452543	4282	4.282	0.1477756588285502
X	0.018083626591067076	16292	16.292	0.10353893887213537
X	0.014272804452377252	4513	4.513	0.1467848817782619
X	0.014022159483970082	1222	1.222	0.22555263174859655
X	0.01444057279884218	78655	78.655	0.05683545768389738
X	0.014441483494513015	10522	10.522	0.1111316690491054
X	0.013623002553519559	770	0.77	0.26057177474840065
X	0.014748902026675743	42409	42.409	0.07032351222280811
X	0.01458875506766547	6600	6.6	0.13026410675827582
X	0.0134655754707919	571	0.571	0.28676728417462377
X	0.01445382534727362	50660	50.66	0.06583232687686462
X	0.014867720895904442	75315	75.315	0.05822653856981131
X	0.014712783337071517	82008	82.008	0.056400056581011083
X	0.014448082154831668	83439	83.439	0.055737442498314964
X	0.014372049812685618	130547	130.547	0.04792740767170836
X	0.014449044430914774	12507	12.507	0.1049289496528576
X	0.014106461255046514	10085	10.085	0.11183576837365891
X	0.017029265722293317	78018	78.018	0.0602097794564937
X	0.016671530477509706	70815	70.815	0.06174712440260904
X	0.014411814011490802	4933	4.933	0.14295604057974096
X	0.015034357466284051	27485	27.485	0.08178300114950382
X	0.01618177482390011	91472	91.472	0.05613658545776402
X	0.016132374236490478	87892	87.892	0.056830696723743766
X	0.01619762749796245	63467	63.467	0.06343093974399813
X	0.014263610791315693	46604	46.604	0.06739103019516571
X	0.014475210385880972	6923	6.923	0.12787241018394918
X	0.01417581226767993	2467	2.467	0.1791126535051815
X	0.013898675798244881	3506	3.506	0.1582658341096007
X	0.014471767375426301	66979	66.979	0.06000594844931263
X	0.014827039569568051	59281	59.281	0.06300567110849456
X	0.01447019176923787	250671	250.671	0.03864767725552766
X	0.014478486021303083	67503	67.503	0.05985953776898282
X	0.014442498915395872	36461	36.461	0.07344088854151053
X	0.014438784593943507	9981	9.981	0.11309728877748178
X	0.014504037946349175	83661	83.661	0.05575989645820301
X	0.01395955364816135	2017	2.017	0.19057029721110713
X	0.01429739581747403	5059	5.059	0.1413830741266988
X	0.014484324013684253	102134	102.134	0.052148597168934545
X	0.014386557545213096	8487	8.487	0.1192342145097701
X	0.01376124849200019	2427	2.427	0.17831834316562867
X	0.014522747254914252	9936	9.936	0.11348689976858642
X	0.015669278089191732	192580	192.58	0.043332385881810154
X	0.014358674985864158	9281	9.281	0.11565733940123442
X	0.017761420588955402	92224	92.224	0.05774901502177254
X	0.014037065571631153	2525	2.525	0.1771488410404265
X	0.015129667833131822	3346	3.346	0.16536153138676582
X	0.014314995079097954	3082	3.082	0.16684743054456666
X	0.014116688284795113	3523	3.523	0.15883275195918056
X	0.0145073130796321	28953	28.953	0.07942632546892327
X	0.014195834979522946	31152	31.152	0.07695290206395751
X	0.014312577074362618	1847	1.847	0.19788687078179384
X	0.01582679783427604	71561	71.561	0.06047449048907288
X	0.0140566610639664	2110	2.11	0.1881625713403912
X	0.01771029863316953	38047	38.047	0.07750002132206328
X	0.014215235769986397	5631	5.631	0.13616208091204396
X	0.014516596833286594	41819	41.819	0.07027977956360841
X	0.014875393134827958	4631	4.631	0.14754728543023987
X	0.01433222441561131	11319	11.319	0.10818536465891544
X	0.016707180858743474	11215	11.215	0.11420925502388309
X	0.016537865830594002	13518	13.518	0.10695202391146008
X	0.014444804933729764	47287	47.287	0.06734778338789062
X	0.014486212402463526	159718	159.718	0.0449297147969948
X	0.014414965322327515	3519	3.519	0.1600042229541292
X	0.0141666013244824	6670	6.67	0.1285424616960884
X	0.0143816665365802	54389	54.389	0.06418487704449241
X	0.014409135845744762	8674	8.674	0.11843302969516034
X	0.015003837162269364	325311	325.311	0.035862002057430106
X	0.014194184907809099	1100	1.1	0.23455206334821102
X	0.014383825895773729	64355	64.355	0.0606872382825675
X	0.01503183494530272	116442	116.442	0.050539869252430786
X	0.014369475028456398	1340	1.34	0.2205186717233996
X	0.014245345157374504	104232	104.232	0.05150989150390434
X	0.014044575053611005	9010	9.01	0.11594746842465482
X	0.016561306489863415	516961	516.961	0.03175988622030084
X	0.014379849714206031	3776	3.776	0.1561615534299486
X	0.014213550335680171	6452	6.452	0.1301176080467798
X	0.015425310394793581	5202	5.202	0.14366617240912105
X	0.014272031635026885	3068	3.068	0.1669334932288853
X	0.01439718979129565	8545	8.545	0.11899312978159853
X	0.01498500442688016	18143	18.143	0.09382442791013487
X	0.014384966173815997	12033	12.033	0.10613166465141328
X	0.014990444674871707	62363	62.363	0.06217691532093569
X	0.014261529765858445	9959	9.959	0.11271545282354316
X	0.014368075023492305	1772	1.772	0.20089923815068503
X	0.01426390135386326	7786	7.786	0.12236051863306795
X	0.014436892966172697	174917	174.917	0.043539212754715464
X	0.014297516901605354	67996	67.996	0.05946467977101208
X	0.015201555232162265	173481	173.481	0.044416605846233366
X	0.014494172122196747	10727	10.727	0.11055330013130975
X	0.014334184851640102	7567	7.567	0.12373230086690241
X	0.013936420713039925	1600	1.6	0.20575185303670127
X	0.014282848407878716	3050	3.05	0.16730348817030205
X	0.017330674623296908	86998	86.998	0.05840303278311358
X	0.015176783396853454	6948	6.948	0.12974978355646322
X	0.014451193535825566	10358	10.358	0.11174015519400585
X	0.014198469271318265	18656	18.656	0.09130074701952066
X	0.014381204330776446	21971	21.971	0.08682573683733488
X	0.014220554551920915	4371	4.371	0.1481762417463495
X	0.014086218382181999	1476	1.476	0.21211400943919448
X	0.01634045487969804	151440	151.44	0.047607405466990216
X	0.014483291830711892	25728	25.728	0.08256964514725369
X	0.01390962372937501	16104	16.104	0.09523440513091433
X	0.014490365777558684	8905	8.905	0.11762017147397123
X	0.015710158103766966	57892	57.892	0.06474218226691152
X	0.014440184138371273	114239	114.239	0.050186407984408765
X	0.014458741037648815	243927	243.927	0.03899032617285261
X	0.014508066624332999	88767	88.767	0.054674648488734205
X	0.014250044770423492	5522	5.522	0.13716393701329122
X	0.01448279444883654	19163	19.163	0.09108837528296433
X	0.014373961656250933	25121	25.121	0.08301945018334715
X	0.014268049058312499	3879	3.879	0.1543647988869312
X	0.014305507358485427	7483	7.483	0.12411068519368025
X	0.014433342008404085	3555	3.555	0.15953002473537664
X	0.014438776926035313	7452	7.452	0.12466729035270437
X	0.014087395768502799	8158	8.158	0.1199726738796826
X	0.013854430170610629	685	0.685	0.27245787528084003
X	0.014189527798205624	7238	7.238	0.12515545635547629
X	0.01422466191388	4072	4.072	0.15173230706188404
X	0.014456308924068847	17936	17.936	0.09306304855288759
X	0.014209230610553562	4124	4.124	0.15103721490021219
X	0.014737087539626399	14869	14.869	0.09970339971778064
X	0.0141997949265293	4298	4.298	0.14893789805468202
X	0.014428821851001652	8298	8.298	0.12025020172921769
X	0.014091126330541569	3068	3.068	0.16622516947212107
X	0.014390755313655404	6000	6.0	0.133857932504907
X	0.014499899104519627	11079	11.079	0.1093842531695145
X	0.014497923273734307	117090	117.09	0.04984198502276031
X	0.014385840724692319	3047	3.047	0.16775968338205963
X	0.013294955906327403	639	0.639	0.2750396759842018
X	0.014895709622406106	2971	2.971	0.1711536565582656
X	0.01442822466087842	95222	95.222	0.05331202745368369
X	0.01518998650666644	145809	145.809	0.04705340092998815
X	0.014263125700161252	41041	41.041	0.07030705798226143
X	0.014152076077180437	4492	4.492	0.14659758040127696
X	0.014519710690909243	65909	65.909	0.06039547670720958
X	0.018588116855820413	60981	60.981	0.06729977579260271
X	0.014141931620337646	1231	1.231	0.2256404168400738
X	0.014404660999274002	5044	5.044	0.14187612601283095
X	0.014505642425302744	13955	13.955	0.10129835097522247
X	0.014450601668679815	32390	32.39	0.07641142998743772
X	0.014315677289586338	10058	10.058	0.11248641854452285
X	0.01449639488928438	124615	124.615	0.04881611683129562
X	0.014485390744371962	4864	4.864	0.14387286197705684
X	0.0143603632588507	2045	2.045	0.19149497015800823
X	0.013930550814040418	1342	1.342	0.21814164859861285
X	0.014325137636881848	12112	12.112	0.10575339386114829
X	0.014368946718137637	6539	6.539	0.130008335786922
X	0.014316007355450503	13895	13.895	0.10099994111506354
X	0.014453318746199524	132088	132.088	0.04783011283052091
X	0.014512380622636148	350265	350.265	0.03460301750623372
X	0.014166866441670797	4633	4.633	0.1451455860094424
X	0.014416768517646157	1643	1.643	0.2062576762699179
X	0.014467508059948784	10954	10.954	0.10971693464769736
X	0.014504748125394703	75834	75.834	0.0576167480352978
X	0.01445806875619621	17044	17.044	0.09466285243403463
X	0.014352536230828702	1356	1.356	0.21956156655258433
X	0.013876389449749277	1450	1.45	0.2123095318149022
X	0.014352911439829244	29251	29.251	0.0788738600981173
X	0.014057403105006153	1363	1.363	0.21767225727635375
X	0.014276035974329842	48223	48.223	0.06664759304458776
X	0.014252712645694348	2260	2.26	0.18475506929515015
X	0.014456206990514132	8886	8.886	0.117611381051638
X	0.014383156056188924	3734	3.734	0.15675688554717854
X	0.014052922556480638	2487	2.487	0.17811354843637991
X	0.01432654048195956	1958	1.958	0.19413755151078288
X	0.014528667436105638	83506	83.506	0.05582593863806157
X	0.014971553334958554	200582	200.582	0.04210415619706932
X	0.01500529936730464	98014	98.014	0.05349569699883918
X	0.014614144429116262	33415	33.415	0.07590621039320533
X	0.014929687843121604	14307	14.307	0.10143022372505783
X	0.014156343002516704	4633	4.633	0.14510963806692925
X	0.016157152510287195	5705	5.705	0.14148260859261014
X	0.016999083770803277	64380	64.38	0.06415416359151187
X	0.017929632276418264	20206	20.206	0.0960941636588046
X	0.015129437299238739	52356	52.356	0.06611278298206312
X	0.014404768208287877	7253	7.253	0.12569833708974607
X	0.014734556580198234	519547	519.547	0.030495615958859437
X	0.014419872210430073	7800	7.8	0.12273137590472138
X	0.01745732857271096	32407	32.407	0.08136662812391214
X	0.015772829580572464	78834	78.834	0.05848780855051645
X	0.014539529824646229	20727	20.727	0.08885283823680461
X	0.014802636579605703	28446	28.446	0.08043393028767197
X	0.015770552643312126	234949	234.949	0.04064038097690683
X	0.015290711484880825	41359	41.359	0.0717715878865151
X	0.014493204188994566	27597	27.597	0.08068030148788291
X	0.014459251535117535	4512	4.512	0.14743216416628188
X	0.014368414680563192	24329	24.329	0.08389991709721882
X	0.014317692307604114	8099	8.099	0.12091510073549508
X	0.015980339928337552	98237	98.237	0.054588811703875084
X	0.017472897434393377	499618	499.618	0.03270209810171063
X	0.015513421230612653	72995	72.995	0.05967670336256457
X	0.015993451948160167	132716	132.716	0.049393861012468884
X	0.014886553740133455	34476	34.476	0.07558328544280973
X	0.014132435734424636	21396	21.396	0.08708863360861008
X	0.017183873615414763	23557	23.557	0.09001889881236556
X	0.014680648579568656	6484	6.484	0.13131089019106384
X	0.014392002993605327	4251	4.251	0.15015623785869892
X	0.014518063734615142	166130	166.13	0.044376550919642314
X	0.014335069140226976	27476	27.476	0.08050362340905033
X	0.014636695540145815	106152	106.152	0.05166204808944973
X	0.014473769842217099	107891	107.891	0.05119161307674499
X	0.014426820956597012	22510	22.51	0.08621808696252165
X	0.014280246287997258	3544	3.544	0.1591282651941673
X	0.01623015125971299	46212	46.212	0.07055415598046354
X	0.014485268880036364	50304	50.304	0.06603507575039413
X	0.013857184853414856	2549	2.549	0.17583354214293978
X	0.01622932934226549	267674	267.674	0.039285425517609614
X	0.016736146093723397	18655	18.655	0.09644655841643668
X	0.01568798892765306	27116	27.116	0.08332600293138488
X	0.01449448214753144	2490	2.49	0.17988755495272815
X	0.014072910969430882	4203	4.203	0.1496034182482327
X	0.013642754289990964	993	0.993	0.23950666472291632
X	0.014008979174421744	815	0.815	0.2580771933266195
X	0.014328411661172205	8633	8.633	0.1183982917743968
X	0.016394653308148203	20673	20.673	0.09256203856832498
X	0.0172141742792647	2074423	2074.423	0.02024555214439521
X	0.014389286759746565	4931	4.931	0.14290083123230327
X	0.01449637178070065	34465	34.465	0.07492504490790085
X	0.0140735097265614	5021	5.021	0.14099511477836274
X	0.01437859611682368	7338	7.338	0.12513522893517776
X	0.014733683058759224	8262	8.262	0.12126683538492135
X	0.015155807431956938	152427	152.427	0.046327520702636064
X	0.0143289285288513	3983	3.983	0.1532267076257441
X	0.015025550916518533	459786	459.786	0.03197114722471276
X	0.014257390155915714	2650	2.65	0.17522580081068356
X	0.015016015093784233	122264	122.264	0.049707131167613106
X	0.016865576486469618	23885	23.885	0.08904823481681026
X	0.014461005700388217	76463	76.463	0.057400506061873885
X	0.014471789722738	145668	145.668	0.04631475404009243
X	0.0144058797769986	9006	9.006	0.11695064315209056
X	0.014306724214370643	5121	5.121	0.14084079782092235
X	0.014481140305395728	42968	42.968	0.0695909140247323
X	0.014350575327459493	4281	4.281	0.14966078824895698
X	0.014372037117466017	9999	9.999	0.1128549433916044
X	0.014415676900433242	51744	51.744	0.06531180412383715
X	0.015016221227835042	6191	6.191	0.13435895518388447
X	0.018048904798982055	33388	33.388	0.08146170264845382
X	0.014403657555935237	14368	14.368	0.10008265613171177
X	0.014428031620226165	115258	115.258	0.05002402914751584
X	0.014495625063186515	181029	181.029	0.043101885110645194
X	0.014431800890146349	23413	23.413	0.0851048847208091
X	0.01434259195225648	4098	4.098	0.1518280087682836
X	0.014401576225966667	43827	43.827	0.06900640899651915
X	0.014296244055074724	36268	36.268	0.07332174329026797
X	0.014412263950977255	12535	12.535	0.1047617206908585
X	0.014448110033039202	18011	18.011	0.09291612111529546
X	0.01422785739186322	22690	22.69	0.08559236027565563
X	0.014392643245595775	22990	22.99	0.08554616198479485
X	0.014269936426643846	58465	58.465	0.06249453314719487
X	0.014388725086797872	28059	28.059	0.0800417745733485
X	0.014335659337703551	6987	6.987	0.12706978925368506
X	0.014293774558790547	3707	3.707	0.1568103704145741
X	0.014333448264179224	5149	5.149	0.1406725188134246
X	0.014249100149040402	48874	48.874	0.06630859891475731
X	0.016291264992412197	52389	52.389	0.06774931363263721
X	0.01452795881578634	95099	95.099	0.053457611435995804
X	0.014479987596502122	7443	7.443	0.12483606105806284
X	0.014964675792403612	13287	13.287	0.10404314203095076
X	0.015508666431011672	23038	23.038	0.08764151850857493
X	0.014522161280561554	80102	80.102	0.05659733281857112
X	0.014402606426034798	30494	30.494	0.07787694367270696
X	0.014471828275172271	39039	39.039	0.07183590575241353
X	0.014394670145221238	5558	5.558	0.13732862076880847
X	0.014937346360019017	15939	15.939	0.0978597556230579
X	0.01406660917664345	1510	1.51	0.21041215515515715
X	0.01482358702462612	5261	5.261	0.14124128774107278
X	0.014520317464142049	43119	43.119	0.06957221180459139
X	0.014375297528404355	13853	13.853	0.10124128978405296
X	0.014518240685985553	25046	25.046	0.08337935641013627
X	0.014256793032197398	1149	1.149	0.23150867644768908
X	0.014443008432741662	2780	2.78	0.1731959064005127
X	0.014460598305078613	21540	21.54	0.08756156619120842
X	0.014385824364132799	25703	25.703	0.08241070811366066
X	0.014479573739667424	14904	14.904	0.09904159939376776
X	0.014355289025906869	1815	1.815	0.1992408735474692
X	0.014169742176384352	2409	2.409	0.18051295792936523
X	0.014633275609792159	18495	18.495	0.09249020433300159
X	0.014494593399556202	17306	17.306	0.09426195566991133
X	0.016386801447624274	50806	50.806	0.06857930231744055
X	0.014096815138444825	1990	1.99	0.192053684195543
X	0.014250397225943722	4214	4.214	0.1500988896891395
X	0.014191164512532573	19922	19.922	0.08930896018985506
X	0.014036147371865842	3168	3.168	0.1642432696736798
X	0.015026844915551873	118483	118.483	0.05024242436825805
X	0.014355310704908284	4081	4.081	0.15208346567438197
X	0.014723267260355334	45560	45.56	0.06862359935949879
X	0.014520390984985096	77965	77.965	0.05710746414903539
X	0.014882921590759204	52866	52.866	0.06553961924946267
X	0.014495222026312142	102207	102.207	0.05214925121389146
X	0.014526158801616998	75432	75.432	0.05774730508121596
X	0.01407769431984475	1168	1.168	0.22927826993196818
X	0.014416365804726454	27063	27.063	0.08106373664076695
X	0.01440955402225291	8203	8.203	0.12065887618820609
X	0.01468166666508756	33945	33.945	0.0756251963969618
X	0.014376964939607988	68239	68.239	0.059503820819739206
X	0.014539861542904426	14730	14.73	0.09956786039158758
X	0.015780238420865806	46101	46.101	0.06995215069817415
X	0.014362591264522807	5725	5.725	0.13587902964649273
X	0.014469479915665625	18100	18.1	0.09280928968508452
X	0.014330051762104345	4028	4.028	0.15265795223148595
X	0.0144414898593454	4952	4.952	0.14287090241492362
X	0.014510291217414964	136043	136.043	0.047424195917214716
X	0.017514973105088857	294164	294.164	0.03904870636361862
X	0.014300108810172185	11577	11.577	0.1072953999391088
X	0.014383399411791311	11900	11.9	0.10652172556552222
X	0.014339735941848576	13391	13.391	0.1023079497173401
X	0.01443379038778711	37444	37.444	0.07277787753160407
X	0.014463612047513001	18944	18.944	0.091397649381755
X	0.014430501796341696	4634	4.634	0.14602990452071857
X	0.014493758986489715	25211	25.211	0.08315026818363215
X	0.014555477265939695	91618	91.618	0.054160402237006286
X	0.01437980292378234	5528	5.528	0.13752921578027605
X	0.014478543591464247	44014	44.014	0.0690310818441013
X	0.014905473031013094	3474	3.474	0.16249449393572762
time for making epsilon is 1.3510591983795166
epsilons are
[0.281257243420803, 0.11131845439986285, 0.18534068179679955, 0.11460317343920527, 0.1315515373369129, 0.08870435053544343, 0.0729251921072472, 0.08565625551986908, 0.13082017982156827, 0.09888761320771385, 0.15134427779191453, 0.06298974964626516, 0.13413561246342004, 0.03753538116299444, 0.09843302116715794, 0.08024049405444157, 0.07013889637578855, 0.08798250154182448, 0.05014367639368184, 0.05774158933567792, 0.1054484533404579, 0.050654506242065314, 0.13246619490933575, 0.08234162719131315, 0.2347969673702868, 0.06587970474490959, 0.07291481141778153, 0.1727750677311491, 0.08172949369767817, 0.027581314904945507, 0.13063511029252173, 0.03110093335719677, 0.10673094809660677, 0.13172997857690028, 0.12421993878497022, 0.05412792350525097, 0.05536925287413121, 0.25486772889685605, 0.1406184022666042, 0.20777988630476324, 0.1806017514690007, 0.23919009841448585, 0.32843474518644605, 0.1903347751794836, 0.31244575428186916, 0.2712356188941047, 0.1868322375924754, 0.20820497837222765, 0.2521686696621207, 0.13234896547462416, 0.12125388421774792, 0.18628902562935454, 0.157275552521595, 0.22019930363265672, 0.15940260005729345, 0.16433031981328994, 0.2398149635096119, 0.18952403883817426, 0.2071864115724, 0.18618656751565793, 0.15669483652256594, 0.2426559753137003, 0.12403016496770089, 0.16570053357816913, 0.2180745311274285, 0.22013913483320094, 0.22444658520998606, 0.1786260723579059, 0.1858361051245429, 0.21824055775157541, 0.17838272637054123, 0.20001242299634447, 0.21256288711672472, 0.23011942927818027, 0.17397039886820967, 0.1670272386052361, 0.28607474915427594, 0.36770243903816857, 0.1971257289243877, 0.13502725622478465, 0.18807806389289955, 0.2724310538821492, 0.13336534431773805, 0.2469601601289912, 0.20573197590792133, 0.2608301858925406, 0.19024773789764646, 0.2842360269504988, 0.16723109026090177, 0.2039591330500214, 0.25168114610522996, 0.25534820580251244, 0.26218857010549246, 0.19258403760885304, 0.16301161819654436, 0.1701649589641153, 0.16819542461981532, 0.11986680275485763, 0.17863277194588467, 0.23299132228066258, 0.12154577709883775, 0.21579864688165237, 0.1642313334076736, 0.22969166914664246, 0.18969252139963624, 0.2174321044733903, 0.15933485976545683, 0.33265728612125606, 0.2365986968814278, 0.23045193655610297, 0.24316655658779987, 0.25714443815198884, 0.24150617552216774, 0.1961631783701465, 0.1803607755897643, 0.24130757482505563, 0.26603351977660483, 0.3004491616103939, 0.1392688712112522, 0.24751288383155473, 0.13307536611511406, 0.241668516870976, 0.22771502618999023, 0.23172322890077623, 0.23173397728797818, 0.1887856177990113, 0.21833693793113892, 0.20877669752015826, 0.10483113782596527, 0.16419277381286818, 0.156119024398922, 0.24274030949223652, 0.18819576599668977, 0.24719157484797755, 0.17660016233561437, 0.26338814731470095, 0.28250760032359956, 0.24415536516830982, 0.23534861126562687, 0.265641376153575, 0.208594152407642, 0.25809847481705, 0.12563202470919396, 0.13361654705119255, 0.14538421657318648, 0.2745414765690821, 0.21383835085191535, 0.18835060823626482, 0.2502672520697014, 0.21252729961145841, 0.2104974064237245, 0.15522217580647021, 0.1814443843291188, 0.12212280306122562, 0.193464174581174, 0.10207732682872384, 0.23087784013328838, 0.18655426800764832, 0.17809212931618737, 0.37730428603675276, 0.106118229302992, 0.21230099783698123, 0.1735174960466805, 0.2537361523241054, 0.19894760295560823, 0.2655741551346775, 0.22523047187718495, 0.20314180025780285, 0.17957795612548505, 0.35940285074338707, 0.2592730972125344, 0.2706021121915315, 0.15296963992651716, 0.2729973790591337, 0.1766457322550525, 0.19047810779112487, 0.17935957761946056, 0.16787147974170433, 0.1799875643482917, 0.22883274324695171, 0.2048239120734194, 0.17797796917705583, 0.18018952784832643, 0.23238460718956905, 0.16372188568721385, 0.17679834526617644, 0.1686150598453187, 0.17936106543415348, 0.16873891589109594, 0.12465922487941539, 0.23291788158889443, 0.36820938464623754, 0.2386242156181791, 0.23545769866659014, 0.20287364641101452, 0.18496760657265263, 0.24992907633067554, 0.2605711586780015, 0.19837184533313618, 0.26959377779528215, 0.18586423496959234, 0.260884570373065, 0.1498378666748303, 0.14647383860463487, 0.19395753687783726, 0.2990299368924059, 0.14990564323125444, 0.18497532498504388, 0.2161969390698114, 0.1257065453101579, 0.13026530697534996, 0.21323812205364293, 0.2324082902690701, 0.2507496357564838, 0.27934418986104165, 0.2592015599949436, 0.2651718750496175, 0.32429731102677156, 0.16978049049574487, 0.22619549965724697, 0.1834385280049309, 0.22374227086213438, 0.17615067509177834, 0.23142700353518825, 0.15600002664294216, 0.16502862227114393, 0.27331235819445504, 0.2943605699765282, 0.18097523945284108, 0.31260524174908727, 0.267611102946359, 0.22942249000139436, 0.19365273441709463, 0.17265248556321988, 0.2138393122070282, 0.2473866593961415, 0.2340319137482323, 0.14161322408118682, 0.2952201661545771, 0.11989237741986612, 0.07320378322865666, 0.20353808822483102, 0.16384813688658076, 0.3324009507782483, 0.16068716290975843, 0.06524825231718935, 0.046703901196403275, 0.2066364824103342, 0.05791650750867792, 0.09749594601967333, 0.10436940227680805, 0.07166955302732787, 0.2661180226658564, 0.18258413307590327, 0.05344185615744688, 0.0400661578981948, 0.18581364258762728, 0.08817611175404914, 0.1152928940697567, 0.08146234540946651, 0.06313833232289505, 0.0889796076488982, 0.09730191979902074, 0.10364613559042586, 0.20991869505534771, 0.045098379138764, 0.19498951372140239, 0.2056540176051258, 0.09345411458557377, 0.06580144528857766, 0.05256219433156029, 0.06502355812786077, 0.2861468762233619, 0.10782722341326291, 0.13399410825531533, 0.08658483708756494, 0.07730667140056494, 0.16824174714028622, 0.08763302413965034, 0.3352151701240053, 0.1624817513380711, 0.06527141462628497, 0.07303771314122742, 0.07160471498199203, 0.1625676416679737, 0.0439636076422126, 0.13192244592321603, 0.0954085143747856, 0.06553818285801197, 0.19514136359120846, 0.05751412170152337, 0.046674681811362057, 0.047647505510297006, 0.11421723250767384, 0.11500732994081182, 0.14582971175685774, 0.17237147957351867, 0.07590069198203946, 0.14608767505036824, 0.0962548860341466, 0.07954339088579844, 0.12564064858883295, 0.11920719058570385, 0.15623633669187867, 0.0673453158665636, 0.12461800752134042, 0.1261566144781978, 0.11916789465479852, 0.04763252240634541, 0.11245814258426991, 0.05444860215645706, 0.1627830016450317, 0.07745195453741635, 0.12334433349492892, 0.048226237250067396, 0.08687433227938458, 0.18704049116737967, 0.05729958290434446, 0.05443145028713345, 0.13914989196528837, 0.063321305219838, 0.06010758266134111, 0.04272531261902565, 0.08478747773667866, 0.267591378573437, 0.1379712808578629, 0.1383732372716897, 0.11273773823579011, 0.1029559250314615, 0.2292595922736387, 0.07218221790990255, 0.05659751041774654, 0.23459995117062468, 0.16917758299548563, 0.11088624898574663, 0.12780830921093883, 0.1439210361518752, 0.13096262353294436, 0.18240845739158223, 0.12703716242496055, 0.08505644371823584, 0.11691913312608068, 0.08626559905102058, 0.1364303044975767, 0.04022832145122087, 0.14696403088743323, 0.065409529129593, 0.29343944648348713, 0.20032778218955322, 0.15972279182248086, 0.12277970680720582, 0.18490650365699832, 0.053635020858743485, 0.16335791854036516, 0.18383477319349298, 0.04787925939164638, 0.06918916133469906, 0.0764783260216508, 0.049696344209915075, 0.04309673288849181, 0.1434861606738715, 0.17561156421649804, 0.1115832057210182, 0.24163566484595306, 0.1880209862361628, 0.11967154350886444, 0.21522626792811886, 0.16678977676969128, 0.20369787627695846, 0.17159944330613003, 0.05207810099592991, 0.11370738471735019, 0.06604526886888734, 0.18492955727645732, 0.14650918202605923, 0.12020059490026164, 0.03693539468732778, 0.03782933639698333, 0.13544814504246447, 0.074041203422542, 0.2131938996285479, 0.12774163354573595, 0.07899021315293585, 0.11262591803072833, 0.11180657991513687, 0.06942011597425478, 0.03464896490158921, 0.08946435675570948, 0.1477756588285502, 0.10353893887213537, 0.1467848817782619, 0.22555263174859655, 0.05683545768389738, 0.1111316690491054, 0.26057177474840065, 0.07032351222280811, 0.13026410675827582, 0.28676728417462377, 0.06583232687686462, 0.05822653856981131, 0.056400056581011083, 0.055737442498314964, 0.04792740767170836, 0.1049289496528576, 0.11183576837365891, 0.0602097794564937, 0.06174712440260904, 0.14295604057974096, 0.08178300114950382, 0.05613658545776402, 0.056830696723743766, 0.06343093974399813, 0.06739103019516571, 0.12787241018394918, 0.1791126535051815, 0.1582658341096007, 0.06000594844931263, 0.06300567110849456, 0.03864767725552766, 0.05985953776898282, 0.07344088854151053, 0.11309728877748178, 0.05575989645820301, 0.19057029721110713, 0.1413830741266988, 0.052148597168934545, 0.1192342145097701, 0.17831834316562867, 0.11348689976858642, 0.043332385881810154, 0.11565733940123442, 0.05774901502177254, 0.1771488410404265, 0.16536153138676582, 0.16684743054456666, 0.15883275195918056, 0.07942632546892327, 0.07695290206395751, 0.19788687078179384, 0.06047449048907288, 0.1881625713403912, 0.07750002132206328, 0.13616208091204396, 0.07027977956360841, 0.14754728543023987, 0.10818536465891544, 0.11420925502388309, 0.10695202391146008, 0.06734778338789062, 0.0449297147969948, 0.1600042229541292, 0.1285424616960884, 0.06418487704449241, 0.11843302969516034, 0.035862002057430106, 0.23455206334821102, 0.0606872382825675, 0.050539869252430786, 0.2205186717233996, 0.05150989150390434, 0.11594746842465482, 0.03175988622030084, 0.1561615534299486, 0.1301176080467798, 0.14366617240912105, 0.1669334932288853, 0.11899312978159853, 0.09382442791013487, 0.10613166465141328, 0.06217691532093569, 0.11271545282354316, 0.20089923815068503, 0.12236051863306795, 0.043539212754715464, 0.05946467977101208, 0.044416605846233366, 0.11055330013130975, 0.12373230086690241, 0.20575185303670127, 0.16730348817030205, 0.05840303278311358, 0.12974978355646322, 0.11174015519400585, 0.09130074701952066, 0.08682573683733488, 0.1481762417463495, 0.21211400943919448, 0.047607405466990216, 0.08256964514725369, 0.09523440513091433, 0.11762017147397123, 0.06474218226691152, 0.050186407984408765, 0.03899032617285261, 0.054674648488734205, 0.13716393701329122, 0.09108837528296433, 0.08301945018334715, 0.1543647988869312, 0.12411068519368025, 0.15953002473537664, 0.12466729035270437, 0.1199726738796826, 0.27245787528084003, 0.12515545635547629, 0.15173230706188404, 0.09306304855288759, 0.15103721490021219, 0.09970339971778064, 0.14893789805468202, 0.12025020172921769, 0.16622516947212107, 0.133857932504907, 0.1093842531695145, 0.04984198502276031, 0.16775968338205963, 0.2750396759842018, 0.1711536565582656, 0.05331202745368369, 0.04705340092998815, 0.07030705798226143, 0.14659758040127696, 0.06039547670720958, 0.06729977579260271, 0.2256404168400738, 0.14187612601283095, 0.10129835097522247, 0.07641142998743772, 0.11248641854452285, 0.04881611683129562, 0.14387286197705684, 0.19149497015800823, 0.21814164859861285, 0.10575339386114829, 0.130008335786922, 0.10099994111506354, 0.04783011283052091, 0.03460301750623372, 0.1451455860094424, 0.2062576762699179, 0.10971693464769736, 0.0576167480352978, 0.09466285243403463, 0.21956156655258433, 0.2123095318149022, 0.0788738600981173, 0.21767225727635375, 0.06664759304458776, 0.18475506929515015, 0.117611381051638, 0.15675688554717854, 0.17811354843637991, 0.19413755151078288, 0.05582593863806157, 0.04210415619706932, 0.05349569699883918, 0.07590621039320533, 0.10143022372505783, 0.14510963806692925, 0.14148260859261014, 0.06415416359151187, 0.0960941636588046, 0.06611278298206312, 0.12569833708974607, 0.030495615958859437, 0.12273137590472138, 0.08136662812391214, 0.05848780855051645, 0.08885283823680461, 0.08043393028767197, 0.04064038097690683, 0.0717715878865151, 0.08068030148788291, 0.14743216416628188, 0.08389991709721882, 0.12091510073549508, 0.054588811703875084, 0.03270209810171063, 0.05967670336256457, 0.049393861012468884, 0.07558328544280973, 0.08708863360861008, 0.09001889881236556, 0.13131089019106384, 0.15015623785869892, 0.044376550919642314, 0.08050362340905033, 0.05166204808944973, 0.05119161307674499, 0.08621808696252165, 0.1591282651941673, 0.07055415598046354, 0.06603507575039413, 0.17583354214293978, 0.039285425517609614, 0.09644655841643668, 0.08332600293138488, 0.17988755495272815, 0.1496034182482327, 0.23950666472291632, 0.2580771933266195, 0.1183982917743968, 0.09256203856832498, 0.02024555214439521, 0.14290083123230327, 0.07492504490790085, 0.14099511477836274, 0.12513522893517776, 0.12126683538492135, 0.046327520702636064, 0.1532267076257441, 0.03197114722471276, 0.17522580081068356, 0.049707131167613106, 0.08904823481681026, 0.057400506061873885, 0.04631475404009243, 0.11695064315209056, 0.14084079782092235, 0.0695909140247323, 0.14966078824895698, 0.1128549433916044, 0.06531180412383715, 0.13435895518388447, 0.08146170264845382, 0.10008265613171177, 0.05002402914751584, 0.043101885110645194, 0.0851048847208091, 0.1518280087682836, 0.06900640899651915, 0.07332174329026797, 0.1047617206908585, 0.09291612111529546, 0.08559236027565563, 0.08554616198479485, 0.06249453314719487, 0.0800417745733485, 0.12706978925368506, 0.1568103704145741, 0.1406725188134246, 0.06630859891475731, 0.06774931363263721, 0.053457611435995804, 0.12483606105806284, 0.10404314203095076, 0.08764151850857493, 0.05659733281857112, 0.07787694367270696, 0.07183590575241353, 0.13732862076880847, 0.0978597556230579, 0.21041215515515715, 0.14124128774107278, 0.06957221180459139, 0.10124128978405296, 0.08337935641013627, 0.23150867644768908, 0.1731959064005127, 0.08756156619120842, 0.08241070811366066, 0.09904159939376776, 0.1992408735474692, 0.18051295792936523, 0.09249020433300159, 0.09426195566991133, 0.06857930231744055, 0.192053684195543, 0.1500988896891395, 0.08930896018985506, 0.1642432696736798, 0.05024242436825805, 0.15208346567438197, 0.06862359935949879, 0.05710746414903539, 0.06553961924946267, 0.05214925121389146, 0.05774730508121596, 0.22927826993196818, 0.08106373664076695, 0.12065887618820609, 0.0756251963969618, 0.059503820819739206, 0.09956786039158758, 0.06995215069817415, 0.13587902964649273, 0.09280928968508452, 0.15265795223148595, 0.14287090241492362, 0.047424195917214716, 0.03904870636361862, 0.1072953999391088, 0.10652172556552222, 0.1023079497173401, 0.07277787753160407, 0.091397649381755, 0.14602990452071857, 0.08315026818363215, 0.054160402237006286, 0.13752921578027605, 0.0690310818441013, 0.16249449393572762]
0.10092869480301053
Making ranges
torch.Size([28512, 2])
We keep 5.72e+06/3.35e+08 =  1% of the original kernel matrix.

torch.Size([1580, 2])
We keep 3.80e+04/3.96e+05 =  9% of the original kernel matrix.

torch.Size([7395, 2])
We keep 5.35e+05/1.15e+07 =  4% of the original kernel matrix.

torch.Size([16520, 2])
We keep 3.40e+06/1.08e+08 =  3% of the original kernel matrix.

torch.Size([20530, 2])
We keep 3.98e+06/1.90e+08 =  2% of the original kernel matrix.

torch.Size([5105, 2])
We keep 2.67e+05/5.07e+06 =  5% of the original kernel matrix.

torch.Size([11864, 2])
We keep 1.26e+06/4.12e+07 =  3% of the original kernel matrix.

torch.Size([11643, 2])
We keep 5.34e+06/9.20e+07 =  5% of the original kernel matrix.

torch.Size([16824, 2])
We keep 3.71e+06/1.75e+08 =  2% of the original kernel matrix.

torch.Size([11939, 2])
We keep 1.45e+06/3.99e+07 =  3% of the original kernel matrix.

torch.Size([17399, 2])
We keep 2.64e+06/1.16e+08 =  2% of the original kernel matrix.

torch.Size([32851, 2])
We keep 8.74e+06/4.30e+08 =  2% of the original kernel matrix.

torch.Size([29551, 2])
We keep 6.95e+06/3.79e+08 =  1% of the original kernel matrix.

torch.Size([56984, 2])
We keep 2.54e+07/1.45e+09 =  1% of the original kernel matrix.

torch.Size([37893, 2])
We keep 1.15e+07/6.96e+08 =  1% of the original kernel matrix.

torch.Size([35503, 2])
We keep 9.19e+06/5.32e+08 =  1% of the original kernel matrix.

torch.Size([30947, 2])
We keep 7.51e+06/4.22e+08 =  1% of the original kernel matrix.

torch.Size([10628, 2])
We keep 2.16e+06/4.14e+07 =  5% of the original kernel matrix.

torch.Size([16434, 2])
We keep 2.77e+06/1.18e+08 =  2% of the original kernel matrix.

torch.Size([18399, 2])
We keep 2.72e+07/2.13e+08 = 12% of the original kernel matrix.

torch.Size([21486, 2])
We keep 5.39e+06/2.67e+08 =  2% of the original kernel matrix.

torch.Size([7614, 2])
We keep 8.14e+05/1.72e+07 =  4% of the original kernel matrix.

torch.Size([13872, 2])
We keep 1.95e+06/7.58e+07 =  2% of the original kernel matrix.

torch.Size([82377, 2])
We keep 7.65e+07/3.34e+09 =  2% of the original kernel matrix.

torch.Size([45103, 2])
We keep 1.65e+07/1.06e+09 =  1% of the original kernel matrix.

torch.Size([12331, 2])
We keep 1.60e+06/4.89e+07 =  3% of the original kernel matrix.

torch.Size([17601, 2])
We keep 3.03e+06/1.28e+08 =  2% of the original kernel matrix.

torch.Size([459390, 2])
We keep 7.62e+08/7.52e+10 =  1% of the original kernel matrix.

torch.Size([112914, 2])
We keep 6.45e+07/5.01e+09 =  1% of the original kernel matrix.

torch.Size([24453, 2])
We keep 4.75e+06/2.31e+08 =  2% of the original kernel matrix.

torch.Size([25488, 2])
We keep 5.29e+06/2.78e+08 =  1% of the original kernel matrix.

torch.Size([43270, 2])
We keep 1.49e+07/7.99e+08 =  1% of the original kernel matrix.

torch.Size([33765, 2])
We keep 8.87e+06/5.17e+08 =  1% of the original kernel matrix.

torch.Size([65813, 2])
We keep 3.86e+07/1.90e+09 =  2% of the original kernel matrix.

torch.Size([40812, 2])
We keep 1.28e+07/7.97e+08 =  1% of the original kernel matrix.

torch.Size([30853, 2])
We keep 1.18e+07/4.42e+08 =  2% of the original kernel matrix.

torch.Size([28188, 2])
We keep 6.93e+06/3.85e+08 =  1% of the original kernel matrix.

torch.Size([173502, 2])
We keep 1.93e+08/1.32e+10 =  1% of the original kernel matrix.

torch.Size([68623, 2])
We keep 2.94e+07/2.10e+09 =  1% of the original kernel matrix.

torch.Size([119434, 2])
We keep 1.17e+08/5.66e+09 =  2% of the original kernel matrix.

torch.Size([55918, 2])
We keep 2.03e+07/1.38e+09 =  1% of the original kernel matrix.

torch.Size([16052, 2])
We keep 4.65e+06/1.45e+08 =  3% of the original kernel matrix.

torch.Size([19925, 2])
We keep 4.55e+06/2.20e+08 =  2% of the original kernel matrix.

torch.Size([177045, 2])
We keep 2.35e+08/1.62e+10 =  1% of the original kernel matrix.

torch.Size([69379, 2])
We keep 3.16e+07/2.33e+09 =  1% of the original kernel matrix.

torch.Size([11886, 2])
We keep 1.44e+06/3.86e+07 =  3% of the original kernel matrix.

torch.Size([17370, 2])
We keep 2.61e+06/1.14e+08 =  2% of the original kernel matrix.

torch.Size([38276, 2])
We keep 3.23e+07/6.70e+08 =  4% of the original kernel matrix.

torch.Size([31404, 2])
We keep 8.49e+06/4.73e+08 =  1% of the original kernel matrix.

torch.Size([2186, 2])
We keep 1.73e+05/1.14e+06 = 15% of the original kernel matrix.

torch.Size([8100, 2])
We keep 6.70e+05/1.95e+07 =  3% of the original kernel matrix.

torch.Size([82892, 2])
We keep 3.47e+07/2.57e+09 =  1% of the original kernel matrix.

torch.Size([45886, 2])
We keep 1.44e+07/9.27e+08 =  1% of the original kernel matrix.

torch.Size([54917, 2])
We keep 2.71e+07/1.39e+09 =  1% of the original kernel matrix.

torch.Size([37226, 2])
We keep 1.13e+07/6.82e+08 =  1% of the original kernel matrix.

torch.Size([5977, 2])
We keep 3.84e+05/7.85e+06 =  4% of the original kernel matrix.

torch.Size([12571, 2])
We keep 1.47e+06/5.12e+07 =  2% of the original kernel matrix.

torch.Size([35618, 2])
We keep 3.99e+07/7.05e+08 =  5% of the original kernel matrix.

torch.Size([30075, 2])
We keep 7.87e+06/4.86e+08 =  1% of the original kernel matrix.

torch.Size([1263272, 2])
We keep 4.92e+09/6.79e+11 =  0% of the original kernel matrix.

torch.Size([193903, 2])
We keep 1.75e+08/1.51e+10 =  1% of the original kernel matrix.

torch.Size([10830, 2])
We keep 2.20e+06/4.16e+07 =  5% of the original kernel matrix.

torch.Size([16619, 2])
We keep 2.76e+06/1.18e+08 =  2% of the original kernel matrix.

torch.Size([748614, 2])
We keep 2.78e+09/2.53e+11 =  1% of the original kernel matrix.

torch.Size([148620, 2])
We keep 1.14e+08/9.20e+09 =  1% of the original kernel matrix.

torch.Size([16787, 2])
We keep 6.91e+06/1.39e+08 =  4% of the original kernel matrix.

torch.Size([20672, 2])
We keep 4.46e+06/2.15e+08 =  2% of the original kernel matrix.

torch.Size([12147, 2])
We keep 1.50e+06/4.00e+07 =  3% of the original kernel matrix.

torch.Size([17576, 2])
We keep 2.68e+06/1.16e+08 =  2% of the original kernel matrix.

torch.Size([12258, 2])
We keep 1.90e+06/5.65e+07 =  3% of the original kernel matrix.

torch.Size([17465, 2])
We keep 3.11e+06/1.37e+08 =  2% of the original kernel matrix.

torch.Size([132978, 2])
We keep 2.03e+08/8.74e+09 =  2% of the original kernel matrix.

torch.Size([59202, 2])
We keep 2.53e+07/1.71e+09 =  1% of the original kernel matrix.

torch.Size([131971, 2])
We keep 1.57e+08/7.26e+09 =  2% of the original kernel matrix.

torch.Size([59140, 2])
We keep 2.30e+07/1.56e+09 =  1% of the original kernel matrix.

torch.Size([2153, 2])
We keep 5.92e+04/7.43e+05 =  7% of the original kernel matrix.

torch.Size([8300, 2])
We keep 6.66e+05/1.58e+07 =  4% of the original kernel matrix.

torch.Size([10007, 2])
We keep 1.45e+06/3.45e+07 =  4% of the original kernel matrix.

torch.Size([15839, 2])
We keep 2.66e+06/1.07e+08 =  2% of the original kernel matrix.

torch.Size([3339, 2])
We keep 1.51e+05/2.35e+06 =  6% of the original kernel matrix.

torch.Size([9685, 2])
We keep 9.76e+05/2.80e+07 =  3% of the original kernel matrix.

torch.Size([5283, 2])
We keep 3.12e+05/6.10e+06 =  5% of the original kernel matrix.

torch.Size([11837, 2])
We keep 1.35e+06/4.52e+07 =  2% of the original kernel matrix.

torch.Size([1873, 2])
We keep 9.48e+04/1.02e+06 =  9% of the original kernel matrix.

torch.Size([7529, 2])
We keep 7.28e+05/1.85e+07 =  3% of the original kernel matrix.

torch.Size([1004, 2])
We keep 1.87e+04/1.40e+05 = 13% of the original kernel matrix.

torch.Size([6220, 2])
We keep 3.62e+05/6.84e+06 =  5% of the original kernel matrix.

torch.Size([4536, 2])
We keep 2.49e+05/4.31e+06 =  5% of the original kernel matrix.

torch.Size([11123, 2])
We keep 1.21e+06/3.80e+07 =  3% of the original kernel matrix.

torch.Size([1255, 2])
We keep 2.13e+04/2.13e+05 = 10% of the original kernel matrix.

torch.Size([7005, 2])
We keep 4.39e+05/8.43e+06 =  5% of the original kernel matrix.

torch.Size([1795, 2])
We keep 4.00e+04/4.75e+05 =  8% of the original kernel matrix.

torch.Size([7851, 2])
We keep 5.61e+05/1.26e+07 =  4% of the original kernel matrix.

torch.Size([4696, 2])
We keep 2.40e+05/4.70e+06 =  5% of the original kernel matrix.

torch.Size([11431, 2])
We keep 1.21e+06/3.97e+07 =  3% of the original kernel matrix.

torch.Size([3387, 2])
We keep 1.51e+05/2.52e+06 =  5% of the original kernel matrix.

torch.Size([9859, 2])
We keep 9.92e+05/2.90e+07 =  3% of the original kernel matrix.

torch.Size([2116, 2])
We keep 6.81e+04/7.76e+05 =  8% of the original kernel matrix.

torch.Size([8229, 2])
We keep 6.73e+05/1.61e+07 =  4% of the original kernel matrix.

torch.Size([10464, 2])
We keep 1.45e+06/3.77e+07 =  3% of the original kernel matrix.

torch.Size([16157, 2])
We keep 2.71e+06/1.12e+08 =  2% of the original kernel matrix.

torch.Size([13844, 2])
We keep 2.06e+06/6.52e+07 =  3% of the original kernel matrix.

torch.Size([18631, 2])
We keep 3.29e+06/1.48e+08 =  2% of the original kernel matrix.

torch.Size([4563, 2])
We keep 2.51e+05/4.71e+06 =  5% of the original kernel matrix.

torch.Size([11102, 2])
We keep 1.23e+06/3.97e+07 =  3% of the original kernel matrix.

torch.Size([6752, 2])
We keep 6.15e+05/1.33e+07 =  4% of the original kernel matrix.

torch.Size([13022, 2])
We keep 1.82e+06/6.67e+07 =  2% of the original kernel matrix.

torch.Size([2968, 2])
We keep 1.22e+05/1.77e+06 =  6% of the original kernel matrix.

torch.Size([9320, 2])
We keep 8.90e+05/2.43e+07 =  3% of the original kernel matrix.

torch.Size([7533, 2])
We keep 5.56e+05/1.27e+07 =  4% of the original kernel matrix.

torch.Size([13893, 2])
We keep 1.75e+06/6.51e+07 =  2% of the original kernel matrix.

torch.Size([5968, 2])
We keep 5.72e+05/1.05e+07 =  5% of the original kernel matrix.

torch.Size([12301, 2])
We keep 1.68e+06/5.92e+07 =  2% of the original kernel matrix.

torch.Size([2623, 2])
We keep 7.57e+04/1.03e+06 =  7% of the original kernel matrix.

torch.Size([9181, 2])
We keep 7.25e+05/1.85e+07 =  3% of the original kernel matrix.

torch.Size([4867, 2])
We keep 2.39e+05/4.41e+06 =  5% of the original kernel matrix.

torch.Size([11473, 2])
We keep 1.21e+06/3.84e+07 =  3% of the original kernel matrix.

torch.Size([3519, 2])
We keep 1.55e+05/2.58e+06 =  6% of the original kernel matrix.

torch.Size([10088, 2])
We keep 9.97e+05/2.94e+07 =  3% of the original kernel matrix.

torch.Size([4401, 2])
We keep 2.71e+05/4.63e+06 =  5% of the original kernel matrix.

torch.Size([10841, 2])
We keep 1.23e+06/3.93e+07 =  3% of the original kernel matrix.

torch.Size([7978, 2])
We keep 5.95e+05/1.41e+07 =  4% of the original kernel matrix.

torch.Size([14287, 2])
We keep 1.82e+06/6.87e+07 =  2% of the original kernel matrix.

torch.Size([2326, 2])
We keep 7.13e+04/9.51e+05 =  7% of the original kernel matrix.

torch.Size([8475, 2])
We keep 6.97e+05/1.78e+07 =  3% of the original kernel matrix.

torch.Size([13401, 2])
We keep 1.89e+06/5.77e+07 =  3% of the original kernel matrix.

torch.Size([18486, 2])
We keep 3.15e+06/1.39e+08 =  2% of the original kernel matrix.

torch.Size([6687, 2])
We keep 4.61e+05/9.83e+06 =  4% of the original kernel matrix.

torch.Size([13086, 2])
We keep 1.62e+06/5.73e+07 =  2% of the original kernel matrix.

torch.Size([2860, 2])
We keep 1.20e+05/1.92e+06 =  6% of the original kernel matrix.

torch.Size([9251, 2])
We keep 9.08e+05/2.53e+07 =  3% of the original kernel matrix.

torch.Size([3075, 2])
We keep 1.14e+05/1.81e+06 =  6% of the original kernel matrix.

torch.Size([9643, 2])
We keep 8.90e+05/2.46e+07 =  3% of the original kernel matrix.

torch.Size([2724, 2])
We keep 9.87e+04/1.44e+06 =  6% of the original kernel matrix.

torch.Size([9000, 2])
We keep 8.09e+05/2.19e+07 =  3% of the original kernel matrix.

torch.Size([5225, 2])
We keep 3.26e+05/6.03e+06 =  5% of the original kernel matrix.

torch.Size([11612, 2])
We keep 1.35e+06/4.49e+07 =  3% of the original kernel matrix.

torch.Size([4851, 2])
We keep 2.75e+05/4.99e+06 =  5% of the original kernel matrix.

torch.Size([11464, 2])
We keep 1.26e+06/4.08e+07 =  3% of the original kernel matrix.

torch.Size([3034, 2])
We keep 1.19e+05/1.77e+06 =  6% of the original kernel matrix.

torch.Size([9459, 2])
We keep 8.78e+05/2.43e+07 =  3% of the original kernel matrix.

torch.Size([4897, 2])
We keep 4.29e+05/5.69e+06 =  7% of the original kernel matrix.

torch.Size([11207, 2])
We keep 1.33e+06/4.36e+07 =  3% of the original kernel matrix.

torch.Size([3926, 2])
We keep 1.89e+05/3.12e+06 =  6% of the original kernel matrix.

torch.Size([10395, 2])
We keep 1.07e+06/3.23e+07 =  3% of the original kernel matrix.

torch.Size([3804, 2])
We keep 1.43e+05/2.20e+06 =  6% of the original kernel matrix.

torch.Size([10475, 2])
We keep 9.55e+05/2.71e+07 =  3% of the original kernel matrix.

torch.Size([2506, 2])
We keep 9.86e+04/1.33e+06 =  7% of the original kernel matrix.

torch.Size([8702, 2])
We keep 8.16e+05/2.11e+07 =  3% of the original kernel matrix.

torch.Size([5638, 2])
We keep 3.65e+05/7.38e+06 =  4% of the original kernel matrix.

torch.Size([12123, 2])
We keep 1.46e+06/4.97e+07 =  2% of the original kernel matrix.

torch.Size([5954, 2])
We keep 5.90e+05/1.08e+07 =  5% of the original kernel matrix.

torch.Size([12463, 2])
We keep 1.69e+06/6.00e+07 =  2% of the original kernel matrix.

torch.Size([1643, 2])
We keep 3.16e+04/3.43e+05 =  9% of the original kernel matrix.

torch.Size([7677, 2])
We keep 4.97e+05/1.07e+07 =  4% of the original kernel matrix.

torch.Size([804, 2])
We keep 9.55e+03/7.95e+04 = 12% of the original kernel matrix.

torch.Size([5948, 2])
We keep 3.17e+05/5.16e+06 =  6% of the original kernel matrix.

torch.Size([4017, 2])
We keep 1.94e+05/3.41e+06 =  5% of the original kernel matrix.

torch.Size([10535, 2])
We keep 1.10e+06/3.38e+07 =  3% of the original kernel matrix.

torch.Size([11179, 2])
We keep 1.66e+06/4.62e+07 =  3% of the original kernel matrix.

torch.Size([16826, 2])
We keep 3.00e+06/1.24e+08 =  2% of the original kernel matrix.

torch.Size([4617, 2])
We keep 2.60e+05/4.50e+06 =  5% of the original kernel matrix.

torch.Size([11190, 2])
We keep 1.21e+06/3.88e+07 =  3% of the original kernel matrix.

torch.Size([1595, 2])
We keep 4.94e+04/4.75e+05 = 10% of the original kernel matrix.

torch.Size([7225, 2])
We keep 5.67e+05/1.26e+07 =  4% of the original kernel matrix.

torch.Size([11798, 2])
We keep 1.65e+06/4.56e+07 =  3% of the original kernel matrix.

torch.Size([17310, 2])
We keep 2.94e+06/1.24e+08 =  2% of the original kernel matrix.

torch.Size([2148, 2])
We keep 6.48e+04/8.21e+05 =  7% of the original kernel matrix.

torch.Size([8318, 2])
We keep 6.83e+05/1.66e+07 =  4% of the original kernel matrix.

torch.Size([3610, 2])
We keep 1.64e+05/2.46e+06 =  6% of the original kernel matrix.

torch.Size([10126, 2])
We keep 9.70e+05/2.87e+07 =  3% of the original kernel matrix.

torch.Size([1999, 2])
We keep 5.23e+04/6.61e+05 =  7% of the original kernel matrix.

torch.Size([8302, 2])
We keep 6.39e+05/1.49e+07 =  4% of the original kernel matrix.

torch.Size([4513, 2])
We keep 3.19e+05/5.55e+06 =  5% of the original kernel matrix.

torch.Size([11148, 2])
We keep 1.37e+06/4.31e+07 =  3% of the original kernel matrix.

torch.Size([1589, 2])
We keep 3.49e+04/3.73e+05 =  9% of the original kernel matrix.

torch.Size([7519, 2])
We keep 5.25e+05/1.12e+07 =  4% of the original kernel matrix.

torch.Size([6531, 2])
We keep 4.36e+05/9.08e+06 =  4% of the original kernel matrix.

torch.Size([13120, 2])
We keep 1.56e+06/5.51e+07 =  2% of the original kernel matrix.

torch.Size([4058, 2])
We keep 1.75e+05/2.77e+06 =  6% of the original kernel matrix.

torch.Size([10788, 2])
We keep 1.02e+06/3.05e+07 =  3% of the original kernel matrix.

torch.Size([2198, 2])
We keep 5.96e+04/7.83e+05 =  7% of the original kernel matrix.

torch.Size([8422, 2])
We keep 6.76e+05/1.62e+07 =  4% of the original kernel matrix.

torch.Size([2010, 2])
We keep 5.82e+04/7.09e+05 =  8% of the original kernel matrix.

torch.Size([8065, 2])
We keep 6.45e+05/1.54e+07 =  4% of the original kernel matrix.

torch.Size([1945, 2])
We keep 4.86e+04/6.07e+05 =  8% of the original kernel matrix.

torch.Size([8151, 2])
We keep 6.09e+05/1.43e+07 =  4% of the original kernel matrix.

torch.Size([4351, 2])
We keep 2.33e+05/3.88e+06 =  6% of the original kernel matrix.

torch.Size([10872, 2])
We keep 1.16e+06/3.60e+07 =  3% of the original kernel matrix.

torch.Size([6926, 2])
We keep 5.16e+05/1.09e+07 =  4% of the original kernel matrix.

torch.Size([13285, 2])
We keep 1.68e+06/6.03e+07 =  2% of the original kernel matrix.

torch.Size([6235, 2])
We keep 3.80e+05/8.16e+06 =  4% of the original kernel matrix.

torch.Size([12756, 2])
We keep 1.49e+06/5.23e+07 =  2% of the original kernel matrix.

torch.Size([6490, 2])
We keep 5.49e+05/1.13e+07 =  4% of the original kernel matrix.

torch.Size([12970, 2])
We keep 1.77e+06/6.14e+07 =  2% of the original kernel matrix.

torch.Size([11869, 2])
We keep 2.46e+06/6.45e+07 =  3% of the original kernel matrix.

torch.Size([17118, 2])
We keep 3.28e+06/1.47e+08 =  2% of the original kernel matrix.

torch.Size([5165, 2])
We keep 3.08e+05/6.05e+06 =  5% of the original kernel matrix.

torch.Size([11707, 2])
We keep 1.35e+06/4.50e+07 =  3% of the original kernel matrix.

torch.Size([2688, 2])
We keep 8.10e+04/1.23e+06 =  6% of the original kernel matrix.

torch.Size([9129, 2])
We keep 7.66e+05/2.03e+07 =  3% of the original kernel matrix.

torch.Size([14109, 2])
We keep 1.93e+06/6.43e+07 =  3% of the original kernel matrix.

torch.Size([18956, 2])
We keep 3.25e+06/1.47e+08 =  2% of the original kernel matrix.

torch.Size([3319, 2])
We keep 1.26e+05/1.91e+06 =  6% of the original kernel matrix.

torch.Size([9834, 2])
We keep 9.07e+05/2.53e+07 =  3% of the original kernel matrix.

torch.Size([6745, 2])
We keep 4.92e+05/1.05e+07 =  4% of the original kernel matrix.

torch.Size([13225, 2])
We keep 1.66e+06/5.93e+07 =  2% of the original kernel matrix.

torch.Size([2733, 2])
We keep 1.06e+05/1.38e+06 =  7% of the original kernel matrix.

torch.Size([8973, 2])
We keep 8.14e+05/2.15e+07 =  3% of the original kernel matrix.

torch.Size([4060, 2])
We keep 2.64e+05/4.27e+06 =  6% of the original kernel matrix.

torch.Size([10565, 2])
We keep 1.18e+06/3.78e+07 =  3% of the original kernel matrix.

torch.Size([3159, 2])
We keep 1.22e+05/1.87e+06 =  6% of the original kernel matrix.

torch.Size([9719, 2])
We keep 8.99e+05/2.50e+07 =  3% of the original kernel matrix.

torch.Size([7778, 2])
We keep 5.61e+05/1.28e+07 =  4% of the original kernel matrix.

torch.Size([14123, 2])
We keep 1.76e+06/6.55e+07 =  2% of the original kernel matrix.

torch.Size([1066, 2])
We keep 1.62e+04/1.40e+05 = 11% of the original kernel matrix.

torch.Size([6542, 2])
We keep 3.72e+05/6.84e+06 =  5% of the original kernel matrix.

torch.Size([2422, 2])
We keep 8.20e+04/1.16e+06 =  7% of the original kernel matrix.

torch.Size([8734, 2])
We keep 7.68e+05/1.97e+07 =  3% of the original kernel matrix.

torch.Size([2678, 2])
We keep 9.57e+04/1.38e+06 =  6% of the original kernel matrix.

torch.Size([9084, 2])
We keep 8.07e+05/2.15e+07 =  3% of the original kernel matrix.

torch.Size([2306, 2])
We keep 6.96e+04/9.92e+05 =  7% of the original kernel matrix.

torch.Size([8572, 2])
We keep 7.20e+05/1.82e+07 =  3% of the original kernel matrix.

torch.Size([2058, 2])
We keep 6.73e+04/6.96e+05 =  9% of the original kernel matrix.

torch.Size([8227, 2])
We keep 6.36e+05/1.53e+07 =  4% of the original kernel matrix.

torch.Size([2315, 2])
We keep 8.25e+04/1.11e+06 =  7% of the original kernel matrix.

torch.Size([8511, 2])
We keep 7.69e+05/1.93e+07 =  3% of the original kernel matrix.

torch.Size([3999, 2])
We keep 1.89e+05/3.41e+06 =  5% of the original kernel matrix.

torch.Size([10649, 2])
We keep 1.08e+06/3.38e+07 =  3% of the original kernel matrix.

torch.Size([5310, 2])
We keep 3.19e+05/5.95e+06 =  5% of the original kernel matrix.

torch.Size([11914, 2])
We keep 1.33e+06/4.46e+07 =  2% of the original kernel matrix.

torch.Size([2422, 2])
We keep 7.53e+04/9.76e+05 =  7% of the original kernel matrix.

torch.Size([8690, 2])
We keep 7.28e+05/1.81e+07 =  4% of the original kernel matrix.

torch.Size([1969, 2])
We keep 4.50e+04/5.43e+05 =  8% of the original kernel matrix.

torch.Size([8208, 2])
We keep 5.88e+05/1.35e+07 =  4% of the original kernel matrix.

torch.Size([1297, 2])
We keep 2.52e+04/2.53e+05 =  9% of the original kernel matrix.

torch.Size([7018, 2])
We keep 4.56e+05/9.20e+06 =  4% of the original kernel matrix.

torch.Size([9318, 2])
We keep 1.20e+06/2.83e+07 =  4% of the original kernel matrix.

torch.Size([15125, 2])
We keep 2.40e+06/9.73e+07 =  2% of the original kernel matrix.

torch.Size([2123, 2])
We keep 6.47e+04/8.43e+05 =  7% of the original kernel matrix.

torch.Size([8214, 2])
We keep 6.82e+05/1.68e+07 =  4% of the original kernel matrix.

torch.Size([12065, 2])
We keep 1.29e+06/3.77e+07 =  3% of the original kernel matrix.

torch.Size([17453, 2])
We keep 2.62e+06/1.12e+08 =  2% of the original kernel matrix.

torch.Size([2481, 2])
We keep 7.04e+04/9.27e+05 =  7% of the original kernel matrix.

torch.Size([8727, 2])
We keep 6.88e+05/1.76e+07 =  3% of the original kernel matrix.

torch.Size([2808, 2])
We keep 9.51e+04/1.45e+06 =  6% of the original kernel matrix.

torch.Size([9286, 2])
We keep 8.27e+05/2.20e+07 =  3% of the original kernel matrix.

torch.Size([2647, 2])
We keep 9.83e+04/1.31e+06 =  7% of the original kernel matrix.

torch.Size([8973, 2])
We keep 7.88e+05/2.10e+07 =  3% of the original kernel matrix.

torch.Size([2784, 2])
We keep 9.08e+04/1.25e+06 =  7% of the original kernel matrix.

torch.Size([9321, 2])
We keep 7.84e+05/2.04e+07 =  3% of the original kernel matrix.

torch.Size([4561, 2])
We keep 2.37e+05/4.41e+06 =  5% of the original kernel matrix.

torch.Size([11074, 2])
We keep 1.21e+06/3.84e+07 =  3% of the original kernel matrix.

torch.Size([3078, 2])
We keep 1.15e+05/1.78e+06 =  6% of the original kernel matrix.

torch.Size([9669, 2])
We keep 8.74e+05/2.44e+07 =  3% of the original kernel matrix.

torch.Size([3419, 2])
We keep 1.55e+05/2.35e+06 =  6% of the original kernel matrix.

torch.Size([9912, 2])
We keep 9.72e+05/2.80e+07 =  3% of the original kernel matrix.

torch.Size([21183, 2])
We keep 4.28e+06/1.94e+08 =  2% of the original kernel matrix.

torch.Size([23950, 2])
We keep 5.09e+06/2.55e+08 =  1% of the original kernel matrix.

torch.Size([6984, 2])
We keep 5.26e+05/1.14e+07 =  4% of the original kernel matrix.

torch.Size([13446, 2])
We keep 1.73e+06/6.18e+07 =  2% of the original kernel matrix.

torch.Size([7380, 2])
We keep 6.27e+05/1.41e+07 =  4% of the original kernel matrix.

torch.Size([13687, 2])
We keep 1.85e+06/6.88e+07 =  2% of the original kernel matrix.

torch.Size([2622, 2])
We keep 7.05e+04/9.92e+05 =  7% of the original kernel matrix.

torch.Size([9143, 2])
We keep 7.24e+05/1.82e+07 =  3% of the original kernel matrix.

torch.Size([4514, 2])
We keep 2.54e+05/4.50e+06 =  5% of the original kernel matrix.

torch.Size([11073, 2])
We keep 1.22e+06/3.88e+07 =  3% of the original kernel matrix.

torch.Size([1968, 2])
We keep 7.38e+04/8.69e+05 =  8% of the original kernel matrix.

torch.Size([7987, 2])
We keep 7.07e+05/1.70e+07 =  4% of the original kernel matrix.

torch.Size([5758, 2])
We keep 3.41e+05/6.71e+06 =  5% of the original kernel matrix.

torch.Size([12363, 2])
We keep 1.41e+06/4.74e+07 =  2% of the original kernel matrix.

torch.Size([1815, 2])
We keep 4.73e+04/5.93e+05 =  7% of the original kernel matrix.

torch.Size([7923, 2])
We keep 6.00e+05/1.41e+07 =  4% of the original kernel matrix.

torch.Size([1508, 2])
We keep 3.14e+04/3.56e+05 =  8% of the original kernel matrix.

torch.Size([7298, 2])
We keep 5.07e+05/1.09e+07 =  4% of the original kernel matrix.

torch.Size([2206, 2])
We keep 7.50e+04/1.02e+06 =  7% of the original kernel matrix.

torch.Size([8404, 2])
We keep 7.50e+05/1.85e+07 =  4% of the original kernel matrix.

torch.Size([2730, 2])
We keep 8.39e+04/1.22e+06 =  6% of the original kernel matrix.

torch.Size([9264, 2])
We keep 7.87e+05/2.02e+07 =  3% of the original kernel matrix.

torch.Size([1897, 2])
We keep 4.51e+04/5.75e+05 =  7% of the original kernel matrix.

torch.Size([8003, 2])
We keep 5.93e+05/1.39e+07 =  4% of the original kernel matrix.

torch.Size([3332, 2])
We keep 1.57e+05/2.49e+06 =  6% of the original kernel matrix.

torch.Size([9717, 2])
We keep 9.89e+05/2.89e+07 =  3% of the original kernel matrix.

torch.Size([2088, 2])
We keep 5.53e+04/6.86e+05 =  8% of the original kernel matrix.

torch.Size([8300, 2])
We keep 6.41e+05/1.51e+07 =  4% of the original kernel matrix.

torch.Size([13062, 2])
We keep 2.20e+06/6.40e+07 =  3% of the original kernel matrix.

torch.Size([18234, 2])
We keep 3.34e+06/1.46e+08 =  2% of the original kernel matrix.

torch.Size([10780, 2])
We keep 1.24e+06/3.56e+07 =  3% of the original kernel matrix.

torch.Size([16360, 2])
We keep 2.57e+06/1.09e+08 =  2% of the original kernel matrix.

torch.Size([9228, 2])
We keep 8.81e+05/2.19e+07 =  4% of the original kernel matrix.

torch.Size([15151, 2])
We keep 2.17e+06/8.55e+07 =  2% of the original kernel matrix.

torch.Size([1821, 2])
We keep 3.92e+04/4.53e+05 =  8% of the original kernel matrix.

torch.Size([7939, 2])
We keep 5.60e+05/1.23e+07 =  4% of the original kernel matrix.

torch.Size([3452, 2])
We keep 1.40e+05/2.03e+06 =  6% of the original kernel matrix.

torch.Size([10068, 2])
We keep 9.10e+05/2.61e+07 =  3% of the original kernel matrix.

torch.Size([4330, 2])
We keep 2.62e+05/4.26e+06 =  6% of the original kernel matrix.

torch.Size([10809, 2])
We keep 1.16e+06/3.78e+07 =  3% of the original kernel matrix.

torch.Size([2050, 2])
We keep 6.72e+04/8.10e+05 =  8% of the original kernel matrix.

torch.Size([8085, 2])
We keep 6.81e+05/1.65e+07 =  4% of the original kernel matrix.

torch.Size([3444, 2])
We keep 1.42e+05/2.20e+06 =  6% of the original kernel matrix.

torch.Size([10063, 2])
We keep 9.56e+05/2.71e+07 =  3% of the original kernel matrix.

torch.Size([3352, 2])
We keep 1.41e+05/2.22e+06 =  6% of the original kernel matrix.

torch.Size([9846, 2])
We keep 9.43e+05/2.72e+07 =  3% of the original kernel matrix.

torch.Size([7828, 2])
We keep 8.50e+05/1.91e+07 =  4% of the original kernel matrix.

torch.Size([14199, 2])
We keep 2.14e+06/8.00e+07 =  2% of the original kernel matrix.

torch.Size([5475, 2])
We keep 2.82e+05/5.61e+06 =  5% of the original kernel matrix.

torch.Size([12181, 2])
We keep 1.31e+06/4.33e+07 =  3% of the original kernel matrix.

torch.Size([11002, 2])
We keep 2.05e+06/6.00e+07 =  3% of the original kernel matrix.

torch.Size([16311, 2])
We keep 3.21e+06/1.42e+08 =  2% of the original kernel matrix.

torch.Size([4127, 2])
We keep 2.24e+05/3.74e+06 =  6% of the original kernel matrix.

torch.Size([10581, 2])
We keep 1.15e+06/3.54e+07 =  3% of the original kernel matrix.

torch.Size([22333, 2])
We keep 6.46e+06/2.77e+08 =  2% of the original kernel matrix.

torch.Size([24799, 2])
We keep 6.11e+06/3.04e+08 =  2% of the original kernel matrix.

torch.Size([2774, 2])
We keep 8.38e+04/1.27e+06 =  6% of the original kernel matrix.

torch.Size([9202, 2])
We keep 7.74e+05/2.07e+07 =  3% of the original kernel matrix.

torch.Size([4259, 2])
We keep 3.32e+05/4.65e+06 =  7% of the original kernel matrix.

torch.Size([10697, 2])
We keep 1.24e+06/3.95e+07 =  3% of the original kernel matrix.

torch.Size([5315, 2])
We keep 3.11e+05/6.16e+06 =  5% of the original kernel matrix.

torch.Size([11964, 2])
We keep 1.35e+06/4.54e+07 =  2% of the original kernel matrix.

torch.Size([772, 2])
We keep 9.58e+03/6.76e+04 = 14% of the original kernel matrix.

torch.Size([5769, 2])
We keep 3.06e+05/4.76e+06 =  6% of the original kernel matrix.

torch.Size([19980, 2])
We keep 3.99e+06/1.82e+08 =  2% of the original kernel matrix.

torch.Size([23280, 2])
We keep 4.95e+06/2.47e+08 =  2% of the original kernel matrix.

torch.Size([3122, 2])
We keep 1.60e+05/2.22e+06 =  7% of the original kernel matrix.

torch.Size([9506, 2])
We keep 9.62e+05/2.73e+07 =  3% of the original kernel matrix.

torch.Size([5472, 2])
We keep 3.87e+05/7.54e+06 =  5% of the original kernel matrix.

torch.Size([12006, 2])
We keep 1.48e+06/5.02e+07 =  2% of the original kernel matrix.

torch.Size([2107, 2])
We keep 6.25e+04/7.46e+05 =  8% of the original kernel matrix.

torch.Size([8212, 2])
We keep 6.73e+05/1.58e+07 =  4% of the original kernel matrix.

torch.Size([3513, 2])
We keep 1.84e+05/3.28e+06 =  5% of the original kernel matrix.

torch.Size([10010, 2])
We keep 1.06e+06/3.31e+07 =  3% of the original kernel matrix.

torch.Size([1934, 2])
We keep 4.66e+04/5.85e+05 =  7% of the original kernel matrix.

torch.Size([8210, 2])
We keep 6.05e+05/1.40e+07 =  4% of the original kernel matrix.

torch.Size([3069, 2])
We keep 1.01e+05/1.53e+06 =  6% of the original kernel matrix.

torch.Size([9733, 2])
We keep 8.20e+05/2.26e+07 =  3% of the original kernel matrix.

torch.Size([4056, 2])
We keep 1.72e+05/2.74e+06 =  6% of the original kernel matrix.

torch.Size([10698, 2])
We keep 1.02e+06/3.03e+07 =  3% of the original kernel matrix.

torch.Size([5459, 2])
We keep 3.39e+05/6.25e+06 =  5% of the original kernel matrix.

torch.Size([11936, 2])
We keep 1.36e+06/4.57e+07 =  2% of the original kernel matrix.

torch.Size([901, 2])
We keep 1.10e+04/8.88e+04 = 12% of the original kernel matrix.

torch.Size([6299, 2])
We keep 3.31e+05/5.45e+06 =  6% of the original kernel matrix.

torch.Size([1834, 2])
We keep 5.56e+04/6.23e+05 =  8% of the original kernel matrix.

torch.Size([7651, 2])
We keep 6.18e+05/1.44e+07 =  4% of the original kernel matrix.

torch.Size([1829, 2])
We keep 4.07e+04/4.86e+05 =  8% of the original kernel matrix.

torch.Size([7852, 2])
We keep 5.63e+05/1.28e+07 =  4% of the original kernel matrix.

torch.Size([8444, 2])
We keep 7.00e+05/1.64e+07 =  4% of the original kernel matrix.

torch.Size([14734, 2])
We keep 1.93e+06/7.40e+07 =  2% of the original kernel matrix.

torch.Size([1792, 2])
We keep 3.80e+04/4.60e+05 =  8% of the original kernel matrix.

torch.Size([7762, 2])
We keep 5.50e+05/1.24e+07 =  4% of the original kernel matrix.

torch.Size([5741, 2])
We keep 3.68e+05/7.55e+06 =  4% of the original kernel matrix.

torch.Size([12471, 2])
We keep 1.48e+06/5.03e+07 =  2% of the original kernel matrix.

torch.Size([4581, 2])
We keep 2.32e+05/4.22e+06 =  5% of the original kernel matrix.

torch.Size([11205, 2])
We keep 1.18e+06/3.76e+07 =  3% of the original kernel matrix.

torch.Size([4907, 2])
We keep 3.28e+05/6.00e+06 =  5% of the original kernel matrix.

torch.Size([11385, 2])
We keep 1.35e+06/4.48e+07 =  3% of the original kernel matrix.

torch.Size([6567, 2])
We keep 4.29e+05/9.17e+06 =  4% of the original kernel matrix.

torch.Size([13157, 2])
We keep 1.57e+06/5.54e+07 =  2% of the original kernel matrix.

torch.Size([4782, 2])
We keep 3.19e+05/5.80e+06 =  5% of the original kernel matrix.

torch.Size([11227, 2])
We keep 1.32e+06/4.41e+07 =  3% of the original kernel matrix.

torch.Size([2931, 2])
We keep 8.72e+04/1.28e+06 =  6% of the original kernel matrix.

torch.Size([9334, 2])
We keep 7.71e+05/2.07e+07 =  3% of the original kernel matrix.

torch.Size([3916, 2])
We keep 1.64e+05/2.80e+06 =  5% of the original kernel matrix.

torch.Size([10659, 2])
We keep 1.02e+06/3.06e+07 =  3% of the original kernel matrix.

torch.Size([5315, 2])
We keep 3.23e+05/6.33e+06 =  5% of the original kernel matrix.

torch.Size([11882, 2])
We keep 1.39e+06/4.60e+07 =  3% of the original kernel matrix.

torch.Size([4690, 2])
We keep 3.16e+05/5.70e+06 =  5% of the original kernel matrix.

torch.Size([11116, 2])
We keep 1.33e+06/4.37e+07 =  3% of the original kernel matrix.

torch.Size([2652, 2])
We keep 9.31e+04/1.29e+06 =  7% of the original kernel matrix.

torch.Size([9086, 2])
We keep 7.85e+05/2.08e+07 =  3% of the original kernel matrix.

torch.Size([6887, 2])
We keep 4.49e+05/1.02e+07 =  4% of the original kernel matrix.

torch.Size([13244, 2])
We keep 1.61e+06/5.86e+07 =  2% of the original kernel matrix.

torch.Size([5714, 2])
We keep 3.35e+05/6.56e+06 =  5% of the original kernel matrix.

torch.Size([12323, 2])
We keep 1.38e+06/4.68e+07 =  2% of the original kernel matrix.

torch.Size([6145, 2])
We keep 4.24e+05/8.90e+06 =  4% of the original kernel matrix.

torch.Size([12663, 2])
We keep 1.58e+06/5.46e+07 =  2% of the original kernel matrix.

torch.Size([4514, 2])
We keep 3.58e+05/6.02e+06 =  5% of the original kernel matrix.

torch.Size([10856, 2])
We keep 1.37e+06/4.49e+07 =  3% of the original kernel matrix.

torch.Size([6625, 2])
We keep 4.19e+05/8.63e+06 =  4% of the original kernel matrix.

torch.Size([13127, 2])
We keep 1.52e+06/5.37e+07 =  2% of the original kernel matrix.

torch.Size([12664, 2])
We keep 2.03e+06/5.38e+07 =  3% of the original kernel matrix.

torch.Size([17740, 2])
We keep 3.04e+06/1.34e+08 =  2% of the original kernel matrix.

torch.Size([2527, 2])
We keep 8.98e+04/1.24e+06 =  7% of the original kernel matrix.

torch.Size([8793, 2])
We keep 7.86e+05/2.04e+07 =  3% of the original kernel matrix.

torch.Size([921, 2])
We keep 1.04e+04/7.73e+04 = 13% of the original kernel matrix.

torch.Size([6326, 2])
We keep 3.17e+05/5.09e+06 =  6% of the original kernel matrix.

torch.Size([2529, 2])
We keep 7.25e+04/1.06e+06 =  6% of the original kernel matrix.

torch.Size([8920, 2])
We keep 7.30e+05/1.88e+07 =  3% of the original kernel matrix.

torch.Size([2628, 2])
We keep 8.67e+04/1.21e+06 =  7% of the original kernel matrix.

torch.Size([9084, 2])
We keep 7.67e+05/2.01e+07 =  3% of the original kernel matrix.

torch.Size([3545, 2])
We keep 1.82e+05/2.91e+06 =  6% of the original kernel matrix.

torch.Size([9967, 2])
We keep 1.06e+06/3.12e+07 =  3% of the original kernel matrix.

torch.Size([4847, 2])
We keep 2.74e+05/5.07e+06 =  5% of the original kernel matrix.

torch.Size([11398, 2])
We keep 1.26e+06/4.12e+07 =  3% of the original kernel matrix.

torch.Size([2250, 2])
We keep 6.04e+04/7.66e+05 =  7% of the original kernel matrix.

torch.Size([8546, 2])
We keep 6.57e+05/1.60e+07 =  4% of the original kernel matrix.

torch.Size([1916, 2])
We keep 5.33e+04/5.87e+05 =  9% of the original kernel matrix.

torch.Size([7972, 2])
We keep 6.15e+05/1.40e+07 =  4% of the original kernel matrix.

torch.Size([4097, 2])
We keep 1.78e+05/3.15e+06 =  5% of the original kernel matrix.

torch.Size([10753, 2])
We keep 1.08e+06/3.25e+07 =  3% of the original kernel matrix.

torch.Size([1913, 2])
We keep 4.53e+04/5.20e+05 =  8% of the original kernel matrix.

torch.Size([8091, 2])
We keep 5.78e+05/1.32e+07 =  4% of the original kernel matrix.

torch.Size([5174, 2])
We keep 2.67e+05/5.06e+06 =  5% of the original kernel matrix.

torch.Size([11794, 2])
We keep 1.27e+06/4.11e+07 =  3% of the original kernel matrix.

torch.Size([1905, 2])
We keep 5.12e+04/6.26e+05 =  8% of the original kernel matrix.

torch.Size([8050, 2])
We keep 6.19e+05/1.45e+07 =  4% of the original kernel matrix.

torch.Size([9059, 2])
We keep 7.24e+05/1.82e+07 =  3% of the original kernel matrix.

torch.Size([15057, 2])
We keep 2.02e+06/7.80e+07 =  2% of the original kernel matrix.

torch.Size([8926, 2])
We keep 8.74e+05/2.22e+07 =  3% of the original kernel matrix.

torch.Size([15168, 2])
We keep 2.20e+06/8.62e+07 =  2% of the original kernel matrix.

torch.Size([4542, 2])
We keep 2.15e+05/3.92e+06 =  5% of the original kernel matrix.

torch.Size([11291, 2])
We keep 1.13e+06/3.62e+07 =  3% of the original kernel matrix.

torch.Size([1379, 2])
We keep 2.49e+04/2.47e+05 = 10% of the original kernel matrix.

torch.Size([7096, 2])
We keep 4.51e+05/9.09e+06 =  4% of the original kernel matrix.

torch.Size([8784, 2])
We keep 1.29e+06/2.83e+07 =  4% of the original kernel matrix.

torch.Size([15427, 2])
We keep 2.54e+06/9.74e+07 =  2% of the original kernel matrix.

torch.Size([4956, 2])
We keep 2.82e+05/5.00e+06 =  5% of the original kernel matrix.

torch.Size([11579, 2])
We keep 1.27e+06/4.09e+07 =  3% of the original kernel matrix.

torch.Size([3034, 2])
We keep 1.18e+05/1.88e+06 =  6% of the original kernel matrix.

torch.Size([9540, 2])
We keep 8.95e+05/2.51e+07 =  3% of the original kernel matrix.

torch.Size([14047, 2])
We keep 2.33e+06/7.45e+07 =  3% of the original kernel matrix.

torch.Size([19377, 2])
We keep 3.59e+06/1.58e+08 =  2% of the original kernel matrix.

torch.Size([12818, 2])
We keep 1.39e+06/4.20e+07 =  3% of the original kernel matrix.

torch.Size([17936, 2])
We keep 2.78e+06/1.19e+08 =  2% of the original kernel matrix.

torch.Size([3482, 2])
We keep 1.45e+05/2.17e+06 =  6% of the original kernel matrix.

torch.Size([10129, 2])
We keep 9.56e+05/2.69e+07 =  3% of the original kernel matrix.

torch.Size([2715, 2])
We keep 8.57e+04/1.24e+06 =  6% of the original kernel matrix.

torch.Size([9096, 2])
We keep 7.73e+05/2.04e+07 =  3% of the original kernel matrix.

torch.Size([2143, 2])
We keep 6.28e+04/7.66e+05 =  8% of the original kernel matrix.

torch.Size([8240, 2])
We keep 6.67e+05/1.60e+07 =  4% of the original kernel matrix.

torch.Size([1755, 2])
We keep 3.43e+04/4.04e+05 =  8% of the original kernel matrix.

torch.Size([7913, 2])
We keep 5.27e+05/1.16e+07 =  4% of the original kernel matrix.

torch.Size([1723, 2])
We keep 4.95e+04/5.91e+05 =  8% of the original kernel matrix.

torch.Size([7538, 2])
We keep 6.08e+05/1.41e+07 =  4% of the original kernel matrix.

torch.Size([2056, 2])
We keep 4.68e+04/5.72e+05 =  8% of the original kernel matrix.

torch.Size([8420, 2])
We keep 5.86e+05/1.38e+07 =  4% of the original kernel matrix.

torch.Size([1125, 2])
We keep 1.84e+04/1.58e+05 = 11% of the original kernel matrix.

torch.Size([6691, 2])
We keep 4.00e+05/7.26e+06 =  5% of the original kernel matrix.

torch.Size([6206, 2])
We keep 4.49e+05/8.27e+06 =  5% of the original kernel matrix.

torch.Size([12769, 2])
We keep 1.52e+06/5.26e+07 =  2% of the original kernel matrix.

torch.Size([2784, 2])
We keep 1.00e+05/1.51e+06 =  6% of the original kernel matrix.

torch.Size([9160, 2])
We keep 8.38e+05/2.24e+07 =  3% of the original kernel matrix.

torch.Size([4189, 2])
We keep 2.98e+05/5.13e+06 =  5% of the original kernel matrix.

torch.Size([10526, 2])
We keep 1.28e+06/4.14e+07 =  3% of the original kernel matrix.

torch.Size([2960, 2])
We keep 1.07e+05/1.63e+06 =  6% of the original kernel matrix.

torch.Size([9528, 2])
We keep 8.58e+05/2.34e+07 =  3% of the original kernel matrix.

torch.Size([5498, 2])
We keep 3.38e+05/6.63e+06 =  5% of the original kernel matrix.

torch.Size([12034, 2])
We keep 1.40e+06/4.71e+07 =  2% of the original kernel matrix.

torch.Size([2511, 2])
We keep 9.23e+04/1.26e+06 =  7% of the original kernel matrix.

torch.Size([8818, 2])
We keep 7.88e+05/2.06e+07 =  3% of the original kernel matrix.

torch.Size([6706, 2])
We keep 7.02e+05/1.36e+07 =  5% of the original kernel matrix.

torch.Size([12888, 2])
We keep 1.82e+06/6.75e+07 =  2% of the original kernel matrix.

torch.Size([6123, 2])
We keep 5.75e+05/9.84e+06 =  5% of the original kernel matrix.

torch.Size([12505, 2])
We keep 1.63e+06/5.74e+07 =  2% of the original kernel matrix.

torch.Size([1669, 2])
We keep 3.75e+04/4.34e+05 =  8% of the original kernel matrix.

torch.Size([7544, 2])
We keep 5.49e+05/1.21e+07 =  4% of the original kernel matrix.

torch.Size([1499, 2])
We keep 3.66e+04/3.98e+05 =  9% of the original kernel matrix.

torch.Size([7450, 2])
We keep 5.58e+05/1.15e+07 =  4% of the original kernel matrix.

torch.Size([4937, 2])
We keep 3.04e+05/5.71e+06 =  5% of the original kernel matrix.

torch.Size([11528, 2])
We keep 1.33e+06/4.37e+07 =  3% of the original kernel matrix.

torch.Size([1215, 2])
We keep 2.14e+04/1.96e+05 = 10% of the original kernel matrix.

torch.Size([6622, 2])
We keep 4.29e+05/8.10e+06 =  5% of the original kernel matrix.

torch.Size([1707, 2])
We keep 4.90e+04/5.39e+05 =  9% of the original kernel matrix.

torch.Size([7665, 2])
We keep 5.88e+05/1.34e+07 =  4% of the original kernel matrix.

torch.Size([2607, 2])
We keep 1.03e+05/1.39e+06 =  7% of the original kernel matrix.

torch.Size([8985, 2])
We keep 8.11e+05/2.16e+07 =  3% of the original kernel matrix.

torch.Size([4385, 2])
We keep 2.10e+05/3.87e+06 =  5% of the original kernel matrix.

torch.Size([11108, 2])
We keep 1.16e+06/3.60e+07 =  3% of the original kernel matrix.

torch.Size([5783, 2])
We keep 4.63e+05/8.84e+06 =  5% of the original kernel matrix.

torch.Size([12376, 2])
We keep 1.57e+06/5.44e+07 =  2% of the original kernel matrix.

torch.Size([3436, 2])
We keep 1.40e+05/2.11e+06 =  6% of the original kernel matrix.

torch.Size([9947, 2])
We keep 9.43e+05/2.66e+07 =  3% of the original kernel matrix.

torch.Size([2078, 2])
We keep 6.94e+04/8.78e+05 =  7% of the original kernel matrix.

torch.Size([8172, 2])
We keep 6.99e+05/1.71e+07 =  4% of the original kernel matrix.

torch.Size([2592, 2])
We keep 8.48e+04/1.18e+06 =  7% of the original kernel matrix.

torch.Size([8860, 2])
We keep 7.50e+05/1.98e+07 =  3% of the original kernel matrix.

torch.Size([10299, 2])
We keep 9.51e+05/2.55e+07 =  3% of the original kernel matrix.

torch.Size([16179, 2])
We keep 2.29e+06/9.24e+07 =  2% of the original kernel matrix.

torch.Size([1379, 2])
We keep 2.99e+04/2.87e+05 = 10% of the original kernel matrix.

torch.Size([7020, 2])
We keep 4.84e+05/9.81e+06 =  4% of the original kernel matrix.

torch.Size([14520, 2])
We keep 2.94e+06/6.94e+07 =  4% of the original kernel matrix.

torch.Size([19302, 2])
We keep 3.41e+06/1.52e+08 =  2% of the original kernel matrix.

torch.Size([38692, 2])
We keep 5.08e+07/1.30e+09 =  3% of the original kernel matrix.

torch.Size([30576, 2])
We keep 1.10e+07/6.59e+08 =  1% of the original kernel matrix.

torch.Size([3898, 2])
We keep 1.76e+05/2.87e+06 =  6% of the original kernel matrix.

torch.Size([10720, 2])
We keep 9.63e+05/3.10e+07 =  3% of the original kernel matrix.

torch.Size([6226, 2])
We keep 5.70e+05/1.05e+07 =  5% of the original kernel matrix.

torch.Size([12661, 2])
We keep 1.66e+06/5.94e+07 =  2% of the original kernel matrix.

torch.Size([1025, 2])
We keep 1.51e+04/1.32e+05 = 11% of the original kernel matrix.

torch.Size([6363, 2])
We keep 3.67e+05/6.64e+06 =  5% of the original kernel matrix.

torch.Size([7264, 2])
We keep 5.16e+05/1.19e+07 =  4% of the original kernel matrix.

torch.Size([13565, 2])
We keep 1.71e+06/6.30e+07 =  2% of the original kernel matrix.

torch.Size([64825, 2])
We keep 2.62e+08/2.69e+09 =  9% of the original kernel matrix.

torch.Size([40001, 2])
We keep 1.52e+07/9.50e+08 =  1% of the original kernel matrix.

torch.Size([174013, 2])
We keep 7.74e+08/2.36e+10 =  3% of the original kernel matrix.

torch.Size([67839, 2])
We keep 3.94e+07/2.81e+09 =  1% of the original kernel matrix.

torch.Size([3472, 2])
We keep 1.60e+05/2.66e+06 =  6% of the original kernel matrix.

torch.Size([10043, 2])
We keep 1.02e+06/2.98e+07 =  3% of the original kernel matrix.

torch.Size([98766, 2])
We keep 1.02e+08/5.52e+09 =  1% of the original kernel matrix.

torch.Size([50132, 2])
We keep 2.04e+07/1.36e+09 =  1% of the original kernel matrix.

torch.Size([23106, 2])
We keep 1.31e+07/2.41e+08 =  5% of the original kernel matrix.

torch.Size([24313, 2])
We keep 5.20e+06/2.84e+08 =  1% of the original kernel matrix.

torch.Size([16366, 2])
We keep 7.12e+06/1.55e+08 =  4% of the original kernel matrix.

torch.Size([20225, 2])
We keep 4.58e+06/2.28e+08 =  2% of the original kernel matrix.

torch.Size([51719, 2])
We keep 1.79e+08/1.52e+09 = 11% of the original kernel matrix.

torch.Size([36109, 2])
We keep 1.11e+07/7.14e+08 =  1% of the original kernel matrix.

torch.Size([1915, 2])
We keep 4.43e+04/5.52e+05 =  8% of the original kernel matrix.

torch.Size([8132, 2])
We keep 5.84e+05/1.36e+07 =  4% of the original kernel matrix.

torch.Size([5101, 2])
We keep 4.44e+05/7.66e+06 =  5% of the original kernel matrix.

torch.Size([11651, 2])
We keep 1.56e+06/5.06e+07 =  3% of the original kernel matrix.

torch.Size([137786, 2])
We keep 1.30e+08/8.94e+09 =  1% of the original kernel matrix.

torch.Size([60088, 2])
We keep 2.50e+07/1.73e+09 =  1% of the original kernel matrix.

torch.Size([374312, 2])
We keep 7.60e+08/6.65e+10 =  1% of the original kernel matrix.

torch.Size([102169, 2])
We keep 6.12e+07/4.72e+09 =  1% of the original kernel matrix.

torch.Size([4871, 2])
We keep 2.74e+05/4.77e+06 =  5% of the original kernel matrix.

torch.Size([11532, 2])
We keep 1.24e+06/4.00e+07 =  3% of the original kernel matrix.

torch.Size([32837, 2])
We keep 1.11e+07/4.47e+08 =  2% of the original kernel matrix.

torch.Size([29669, 2])
We keep 7.00e+06/3.87e+08 =  1% of the original kernel matrix.

torch.Size([13729, 2])
We keep 3.58e+06/8.62e+07 =  4% of the original kernel matrix.

torch.Size([18514, 2])
We keep 3.66e+06/1.70e+08 =  2% of the original kernel matrix.

torch.Size([36038, 2])
We keep 6.10e+07/7.09e+08 =  8% of the original kernel matrix.

torch.Size([30152, 2])
We keep 8.29e+06/4.87e+08 =  1% of the original kernel matrix.

torch.Size([66715, 2])
We keep 8.17e+07/3.31e+09 =  2% of the original kernel matrix.

torch.Size([39583, 2])
We keep 1.62e+07/1.05e+09 =  1% of the original kernel matrix.

torch.Size([25652, 2])
We keep 2.38e+07/4.78e+08 =  4% of the original kernel matrix.

torch.Size([25181, 2])
We keep 7.40e+06/4.00e+08 =  1% of the original kernel matrix.

torch.Size([23815, 2])
We keep 8.54e+06/2.48e+08 =  3% of the original kernel matrix.

torch.Size([24915, 2])
We keep 5.47e+06/2.88e+08 =  1% of the original kernel matrix.

torch.Size([20372, 2])
We keep 5.97e+06/1.64e+08 =  3% of the original kernel matrix.

torch.Size([23212, 2])
We keep 4.76e+06/2.34e+08 =  2% of the original kernel matrix.

torch.Size([3551, 2])
We keep 1.53e+05/2.33e+06 =  6% of the original kernel matrix.

torch.Size([10103, 2])
We keep 9.71e+05/2.79e+07 =  3% of the original kernel matrix.

torch.Size([185367, 2])
We keep 5.62e+08/2.48e+10 =  2% of the original kernel matrix.

torch.Size([69313, 2])
We keep 3.95e+07/2.88e+09 =  1% of the original kernel matrix.

torch.Size([4284, 2])
We keep 2.17e+05/3.63e+06 =  5% of the original kernel matrix.

torch.Size([10911, 2])
We keep 1.14e+06/3.49e+07 =  3% of the original kernel matrix.

torch.Size([3760, 2])
We keep 1.62e+05/2.73e+06 =  5% of the original kernel matrix.

torch.Size([10486, 2])
We keep 1.02e+06/3.02e+07 =  3% of the original kernel matrix.

torch.Size([26543, 2])
We keep 8.61e+06/3.12e+08 =  2% of the original kernel matrix.

torch.Size([26438, 2])
We keep 5.90e+06/3.23e+08 =  1% of the original kernel matrix.

torch.Size([58394, 2])
We keep 6.38e+07/2.53e+09 =  2% of the original kernel matrix.

torch.Size([36123, 2])
We keep 1.46e+07/9.21e+08 =  1% of the original kernel matrix.

torch.Size([142240, 2])
We keep 1.54e+08/1.02e+10 =  1% of the original kernel matrix.

torch.Size([61220, 2])
We keep 2.64e+07/1.85e+09 =  1% of the original kernel matrix.

torch.Size([87126, 2])
We keep 4.15e+07/2.78e+09 =  1% of the original kernel matrix.

torch.Size([46929, 2])
We keep 1.49e+07/9.64e+08 =  1% of the original kernel matrix.

torch.Size([1432, 2])
We keep 3.38e+04/3.52e+05 =  9% of the original kernel matrix.

torch.Size([7181, 2])
We keep 5.07e+05/1.08e+07 =  4% of the original kernel matrix.

torch.Size([17865, 2])
We keep 8.99e+06/1.31e+08 =  6% of the original kernel matrix.

torch.Size([21307, 2])
We keep 4.31e+06/2.10e+08 =  2% of the original kernel matrix.

torch.Size([10612, 2])
We keep 1.37e+06/3.56e+07 =  3% of the original kernel matrix.

torch.Size([16265, 2])
We keep 2.61e+06/1.09e+08 =  2% of the original kernel matrix.

torch.Size([29796, 2])
We keep 1.57e+07/4.92e+08 =  3% of the original kernel matrix.

torch.Size([27347, 2])
We keep 7.36e+06/4.06e+08 =  1% of the original kernel matrix.

torch.Size([49059, 2])
We keep 1.56e+07/9.81e+08 =  1% of the original kernel matrix.

torch.Size([35705, 2])
We keep 9.54e+06/5.73e+08 =  1% of the original kernel matrix.

torch.Size([6646, 2])
We keep 4.35e+05/9.19e+06 =  4% of the original kernel matrix.

torch.Size([13283, 2])
We keep 1.58e+06/5.55e+07 =  2% of the original kernel matrix.

torch.Size([31677, 2])
We keep 1.05e+07/4.60e+08 =  2% of the original kernel matrix.

torch.Size([28839, 2])
We keep 7.20e+06/3.92e+08 =  1% of the original kernel matrix.

torch.Size([1007, 2])
We keep 1.42e+04/1.30e+05 = 10% of the original kernel matrix.

torch.Size([6442, 2])
We keep 3.69e+05/6.60e+06 =  5% of the original kernel matrix.

torch.Size([7409, 2])
We keep 5.01e+05/1.13e+07 =  4% of the original kernel matrix.

torch.Size([13949, 2])
We keep 1.68e+06/6.14e+07 =  2% of the original kernel matrix.

torch.Size([69422, 2])
We keep 5.74e+07/2.69e+09 =  2% of the original kernel matrix.

torch.Size([40808, 2])
We keep 1.50e+07/9.48e+08 =  1% of the original kernel matrix.

torch.Size([45062, 2])
We keep 3.68e+07/1.32e+09 =  2% of the original kernel matrix.

torch.Size([32864, 2])
We keep 1.07e+07/6.65e+08 =  1% of the original kernel matrix.

torch.Size([50885, 2])
We keep 3.11e+07/1.55e+09 =  2% of the original kernel matrix.

torch.Size([34813, 2])
We keep 1.18e+07/7.19e+08 =  1% of the original kernel matrix.

torch.Size([6049, 2])
We keep 5.34e+05/1.08e+07 =  4% of the original kernel matrix.

torch.Size([12533, 2])
We keep 1.66e+06/6.02e+07 =  2% of the original kernel matrix.

torch.Size([241187, 2])
We keep 3.02e+08/3.16e+10 =  0% of the original kernel matrix.

torch.Size([81606, 2])
We keep 4.28e+07/3.25e+09 =  1% of the original kernel matrix.

torch.Size([12351, 2])
We keep 1.25e+06/3.99e+07 =  3% of the original kernel matrix.

torch.Size([17717, 2])
We keep 2.69e+06/1.16e+08 =  2% of the original kernel matrix.

torch.Size([26714, 2])
We keep 5.64e+06/2.74e+08 =  2% of the original kernel matrix.

torch.Size([26543, 2])
We keep 5.71e+06/3.03e+08 =  1% of the original kernel matrix.

torch.Size([69579, 2])
We keep 5.23e+07/2.61e+09 =  2% of the original kernel matrix.

torch.Size([41700, 2])
We keep 1.45e+07/9.35e+08 =  1% of the original kernel matrix.

torch.Size([4255, 2])
We keep 2.10e+05/3.74e+06 =  5% of the original kernel matrix.

torch.Size([10955, 2])
We keep 1.14e+06/3.54e+07 =  3% of the original kernel matrix.

torch.Size([111066, 2])
We keep 1.12e+08/5.73e+09 =  1% of the original kernel matrix.

torch.Size([53581, 2])
We keep 2.05e+07/1.39e+09 =  1% of the original kernel matrix.

torch.Size([198999, 2])
We keep 4.79e+08/2.25e+10 =  2% of the original kernel matrix.

torch.Size([73510, 2])
We keep 3.83e+07/2.75e+09 =  1% of the original kernel matrix.

torch.Size([196213, 2])
We keep 2.64e+08/2.00e+10 =  1% of the original kernel matrix.

torch.Size([73309, 2])
We keep 3.55e+07/2.59e+09 =  1% of the original kernel matrix.

torch.Size([14163, 2])
We keep 3.02e+06/1.01e+08 =  2% of the original kernel matrix.

torch.Size([18702, 2])
We keep 3.89e+06/1.84e+08 =  2% of the original kernel matrix.

torch.Size([15127, 2])
We keep 3.48e+06/9.67e+07 =  3% of the original kernel matrix.

torch.Size([19581, 2])
We keep 3.87e+06/1.80e+08 =  2% of the original kernel matrix.

torch.Size([7691, 2])
We keep 1.10e+06/2.12e+07 =  5% of the original kernel matrix.

torch.Size([13650, 2])
We keep 2.16e+06/8.41e+07 =  2% of the original kernel matrix.

torch.Size([6141, 2])
We keep 3.84e+05/7.94e+06 =  4% of the original kernel matrix.

torch.Size([12664, 2])
We keep 1.49e+06/5.15e+07 =  2% of the original kernel matrix.

torch.Size([37685, 2])
We keep 4.72e+07/1.26e+09 =  3% of the original kernel matrix.

torch.Size([30289, 2])
We keep 1.08e+07/6.48e+08 =  1% of the original kernel matrix.

torch.Size([9220, 2])
We keep 8.03e+05/2.10e+07 =  3% of the original kernel matrix.

torch.Size([15200, 2])
We keep 2.11e+06/8.38e+07 =  2% of the original kernel matrix.

torch.Size([21953, 2])
We keep 7.97e+06/2.58e+08 =  3% of the original kernel matrix.

torch.Size([23685, 2])
We keep 5.51e+06/2.94e+08 =  1% of the original kernel matrix.

torch.Size([39146, 2])
We keep 5.62e+07/9.35e+08 =  6% of the original kernel matrix.

torch.Size([31756, 2])
We keep 8.58e+06/5.59e+08 =  1% of the original kernel matrix.

torch.Size([13136, 2])
We keep 2.04e+06/5.30e+07 =  3% of the original kernel matrix.

torch.Size([18399, 2])
We keep 2.89e+06/1.33e+08 =  2% of the original kernel matrix.

torch.Size([10455, 2])
We keep 3.41e+06/6.97e+07 =  4% of the original kernel matrix.

torch.Size([15664, 2])
We keep 3.39e+06/1.53e+08 =  2% of the original kernel matrix.

torch.Size([8114, 2])
We keep 5.83e+05/1.44e+07 =  4% of the original kernel matrix.

torch.Size([14434, 2])
We keep 1.82e+06/6.93e+07 =  2% of the original kernel matrix.

torch.Size([28672, 2])
We keep 1.19e+08/2.46e+09 =  4% of the original kernel matrix.

torch.Size([24727, 2])
We keep 1.45e+07/9.08e+08 =  1% of the original kernel matrix.

torch.Size([10818, 2])
We keep 3.23e+06/5.53e+07 =  5% of the original kernel matrix.

torch.Size([16389, 2])
We keep 3.10e+06/1.36e+08 =  2% of the original kernel matrix.

torch.Size([11231, 2])
We keep 4.06e+06/5.14e+07 =  7% of the original kernel matrix.

torch.Size([16901, 2])
We keep 3.01e+06/1.31e+08 =  2% of the original kernel matrix.

torch.Size([13969, 2])
We keep 2.18e+06/7.19e+07 =  3% of the original kernel matrix.

torch.Size([18893, 2])
We keep 3.38e+06/1.55e+08 =  2% of the original kernel matrix.

torch.Size([160187, 2])
We keep 6.25e+08/1.79e+10 =  3% of the original kernel matrix.

torch.Size([64709, 2])
We keep 3.32e+07/2.45e+09 =  1% of the original kernel matrix.

torch.Size([15963, 2])
We keep 3.82e+06/1.03e+08 =  3% of the original kernel matrix.

torch.Size([20364, 2])
We keep 3.99e+06/1.85e+08 =  2% of the original kernel matrix.

torch.Size([96658, 2])
We keep 2.15e+08/7.99e+09 =  2% of the original kernel matrix.

torch.Size([47384, 2])
We keep 2.35e+07/1.64e+09 =  1% of the original kernel matrix.

torch.Size([6633, 2])
We keep 6.09e+05/1.12e+07 =  5% of the original kernel matrix.

torch.Size([13303, 2])
We keep 1.64e+06/6.11e+07 =  2% of the original kernel matrix.

torch.Size([32674, 2])
We keep 2.91e+07/8.86e+08 =  3% of the original kernel matrix.

torch.Size([26882, 2])
We keep 9.39e+06/5.44e+08 =  1% of the original kernel matrix.

torch.Size([14044, 2])
We keep 2.39e+06/6.04e+07 =  3% of the original kernel matrix.

torch.Size([18913, 2])
We keep 3.14e+06/1.42e+08 =  2% of the original kernel matrix.

torch.Size([143433, 2])
We keep 4.89e+08/1.80e+10 =  2% of the original kernel matrix.

torch.Size([58654, 2])
We keep 3.48e+07/2.46e+09 =  1% of the original kernel matrix.

torch.Size([31899, 2])
We keep 9.06e+06/4.91e+08 =  1% of the original kernel matrix.

torch.Size([28486, 2])
We keep 7.28e+06/4.05e+08 =  1% of the original kernel matrix.

torch.Size([4892, 2])
We keep 2.48e+05/4.79e+06 =  5% of the original kernel matrix.

torch.Size([11547, 2])
We keep 1.22e+06/4.00e+07 =  3% of the original kernel matrix.

torch.Size([122540, 2])
We keep 7.36e+07/6.34e+09 =  1% of the original kernel matrix.

torch.Size([56708, 2])
We keep 2.13e+07/1.46e+09 =  1% of the original kernel matrix.

torch.Size([144532, 2])
We keep 9.26e+07/8.07e+09 =  1% of the original kernel matrix.

torch.Size([62106, 2])
We keep 2.36e+07/1.64e+09 =  1% of the original kernel matrix.

torch.Size([8991, 2])
We keep 2.87e+06/2.77e+07 = 10% of the original kernel matrix.

torch.Size([15072, 2])
We keep 2.37e+06/9.63e+07 =  2% of the original kernel matrix.

torch.Size([84360, 2])
We keep 1.09e+08/3.24e+09 =  3% of the original kernel matrix.

torch.Size([46225, 2])
We keep 1.59e+07/1.04e+09 =  1% of the original kernel matrix.

torch.Size([102124, 2])
We keep 8.72e+07/6.65e+09 =  1% of the original kernel matrix.

torch.Size([51438, 2])
We keep 2.23e+07/1.49e+09 =  1% of the original kernel matrix.

torch.Size([292335, 2])
We keep 5.35e+08/5.03e+10 =  1% of the original kernel matrix.

torch.Size([91146, 2])
We keep 5.29e+07/4.10e+09 =  1% of the original kernel matrix.

torch.Size([30911, 2])
We keep 6.53e+07/5.65e+08 = 11% of the original kernel matrix.

torch.Size([28160, 2])
We keep 7.84e+06/4.35e+08 =  1% of the original kernel matrix.

torch.Size([1713, 2])
We keep 4.29e+04/5.01e+05 =  8% of the original kernel matrix.

torch.Size([7677, 2])
We keep 5.45e+05/1.30e+07 =  4% of the original kernel matrix.

torch.Size([11322, 2])
We keep 1.04e+06/3.04e+07 =  3% of the original kernel matrix.

torch.Size([16834, 2])
We keep 2.42e+06/1.01e+08 =  2% of the original kernel matrix.

torch.Size([7757, 2])
We keep 2.85e+06/3.12e+07 =  9% of the original kernel matrix.

torch.Size([14087, 2])
We keep 2.35e+06/1.02e+08 =  2% of the original kernel matrix.

torch.Size([17332, 2])
We keep 2.94e+06/1.00e+08 =  2% of the original kernel matrix.

torch.Size([21137, 2])
We keep 3.85e+06/1.83e+08 =  2% of the original kernel matrix.

torch.Size([21229, 2])
We keep 7.69e+06/1.76e+08 =  4% of the original kernel matrix.

torch.Size([23835, 2])
We keep 4.86e+06/2.43e+08 =  2% of the original kernel matrix.

torch.Size([2862, 2])
We keep 9.37e+04/1.40e+06 =  6% of the original kernel matrix.

torch.Size([9368, 2])
We keep 8.06e+05/2.16e+07 =  3% of the original kernel matrix.

torch.Size([60071, 2])
We keep 3.43e+07/1.48e+09 =  2% of the original kernel matrix.

torch.Size([38954, 2])
We keep 1.15e+07/7.03e+08 =  1% of the original kernel matrix.

torch.Size([121240, 2])
We keep 8.80e+07/6.37e+09 =  1% of the original kernel matrix.

torch.Size([56364, 2])
We keep 2.15e+07/1.46e+09 =  1% of the original kernel matrix.

torch.Size([2403, 2])
We keep 2.02e+05/1.18e+06 = 17% of the original kernel matrix.

torch.Size([8580, 2])
We keep 6.60e+05/1.99e+07 =  3% of the original kernel matrix.

torch.Size([5886, 2])
We keep 4.90e+05/8.65e+06 =  5% of the original kernel matrix.

torch.Size([12370, 2])
We keep 1.58e+06/5.38e+07 =  2% of the original kernel matrix.

torch.Size([16188, 2])
We keep 3.03e+06/1.11e+08 =  2% of the original kernel matrix.

torch.Size([20322, 2])
We keep 3.97e+06/1.92e+08 =  2% of the original kernel matrix.

torch.Size([12940, 2])
We keep 2.06e+06/6.48e+07 =  3% of the original kernel matrix.

torch.Size([18193, 2])
We keep 3.35e+06/1.47e+08 =  2% of the original kernel matrix.

torch.Size([9621, 2])
We keep 8.85e+05/2.32e+07 =  3% of the original kernel matrix.

torch.Size([15515, 2])
We keep 2.22e+06/8.81e+07 =  2% of the original kernel matrix.

torch.Size([11906, 2])
We keep 1.60e+06/4.81e+07 =  3% of the original kernel matrix.

torch.Size([17315, 2])
We keep 2.95e+06/1.27e+08 =  2% of the original kernel matrix.

torch.Size([5477, 2])
We keep 3.42e+05/5.61e+06 =  6% of the original kernel matrix.

torch.Size([12184, 2])
We keep 1.31e+06/4.33e+07 =  3% of the original kernel matrix.

torch.Size([11124, 2])
We keep 2.79e+06/4.92e+07 =  5% of the original kernel matrix.

torch.Size([16627, 2])
We keep 2.79e+06/1.28e+08 =  2% of the original kernel matrix.

torch.Size([38055, 2])
We keep 9.11e+06/5.54e+08 =  1% of the original kernel matrix.

torch.Size([31883, 2])
We keep 7.55e+06/4.30e+08 =  1% of the original kernel matrix.

torch.Size([16266, 2])
We keep 2.28e+06/8.18e+07 =  2% of the original kernel matrix.

torch.Size([20555, 2])
We keep 3.53e+06/1.65e+08 =  2% of the original kernel matrix.

torch.Size([30974, 2])
We keep 2.64e+07/6.62e+08 =  3% of the original kernel matrix.

torch.Size([28224, 2])
We keep 8.52e+06/4.71e+08 =  1% of the original kernel matrix.

torch.Size([9379, 2])
We keep 1.53e+06/3.23e+07 =  4% of the original kernel matrix.

torch.Size([15176, 2])
We keep 2.54e+06/1.04e+08 =  2% of the original kernel matrix.

torch.Size([342466, 2])
We keep 7.02e+08/6.23e+10 =  1% of the original kernel matrix.

torch.Size([97452, 2])
We keep 5.97e+07/4.57e+09 =  1% of the original kernel matrix.

torch.Size([8726, 2])
We keep 8.80e+05/2.18e+07 =  4% of the original kernel matrix.

torch.Size([14836, 2])
We keep 2.19e+06/8.55e+07 =  2% of the original kernel matrix.

torch.Size([79628, 2])
We keep 4.20e+07/2.67e+09 =  1% of the original kernel matrix.

torch.Size([44953, 2])
We keep 1.47e+07/9.44e+08 =  1% of the original kernel matrix.

torch.Size([1261, 2])
We keep 2.67e+04/2.94e+05 =  9% of the original kernel matrix.

torch.Size([6898, 2])
We keep 4.63e+05/9.91e+06 =  4% of the original kernel matrix.

torch.Size([3823, 2])
We keep 1.90e+05/3.11e+06 =  6% of the original kernel matrix.

torch.Size([10298, 2])
We keep 1.07e+06/3.23e+07 =  3% of the original kernel matrix.

torch.Size([7671, 2])
We keep 5.66e+05/1.26e+07 =  4% of the original kernel matrix.

torch.Size([13980, 2])
We keep 1.77e+06/6.49e+07 =  2% of the original kernel matrix.

torch.Size([14282, 2])
We keep 2.66e+06/6.31e+07 =  4% of the original kernel matrix.

torch.Size([19262, 2])
We keep 3.16e+06/1.45e+08 =  2% of the original kernel matrix.

torch.Size([5177, 2])
We keep 3.79e+05/6.97e+06 =  5% of the original kernel matrix.

torch.Size([11825, 2])
We keep 1.50e+06/4.83e+07 =  3% of the original kernel matrix.

torch.Size([127362, 2])
We keep 2.11e+08/8.75e+09 =  2% of the original kernel matrix.

torch.Size([57373, 2])
We keep 2.53e+07/1.71e+09 =  1% of the original kernel matrix.

torch.Size([7016, 2])
We keep 5.10e+05/1.09e+07 =  4% of the original kernel matrix.

torch.Size([13440, 2])
We keep 1.68e+06/6.03e+07 =  2% of the original kernel matrix.

torch.Size([5034, 2])
We keep 2.60e+05/5.12e+06 =  5% of the original kernel matrix.

torch.Size([11610, 2])
We keep 1.26e+06/4.14e+07 =  3% of the original kernel matrix.

torch.Size([194210, 2])
We keep 3.18e+08/2.45e+10 =  1% of the original kernel matrix.

torch.Size([72859, 2])
We keep 3.94e+07/2.86e+09 =  1% of the original kernel matrix.

torch.Size([61656, 2])
We keep 4.40e+07/1.89e+09 =  2% of the original kernel matrix.

torch.Size([39283, 2])
We keep 1.28e+07/7.95e+08 =  1% of the original kernel matrix.

torch.Size([44884, 2])
We keep 2.61e+07/1.03e+09 =  2% of the original kernel matrix.

torch.Size([33695, 2])
We keep 1.00e+07/5.87e+08 =  1% of the original kernel matrix.

torch.Size([173646, 2])
We keep 2.09e+08/1.40e+10 =  1% of the original kernel matrix.

torch.Size([68492, 2])
We keep 3.04e+07/2.16e+09 =  1% of the original kernel matrix.

torch.Size([261742, 2])
We keep 3.86e+08/3.27e+10 =  1% of the original kernel matrix.

torch.Size([84751, 2])
We keep 4.44e+07/3.31e+09 =  1% of the original kernel matrix.

torch.Size([10039, 2])
We keep 8.79e+05/2.34e+07 =  3% of the original kernel matrix.

torch.Size([15857, 2])
We keep 2.23e+06/8.85e+07 =  2% of the original kernel matrix.

torch.Size([6029, 2])
We keep 3.38e+05/6.88e+06 =  4% of the original kernel matrix.

torch.Size([12713, 2])
We keep 1.42e+06/4.80e+07 =  2% of the original kernel matrix.

torch.Size([14861, 2])
We keep 5.02e+06/1.07e+08 =  4% of the original kernel matrix.

torch.Size([19417, 2])
We keep 3.99e+06/1.89e+08 =  2% of the original kernel matrix.

torch.Size([2012, 2])
We keep 7.88e+04/9.74e+05 =  8% of the original kernel matrix.

torch.Size([7985, 2])
We keep 7.06e+05/1.81e+07 =  3% of the original kernel matrix.

torch.Size([4603, 2])
We keep 2.53e+05/4.43e+06 =  5% of the original kernel matrix.

torch.Size([11076, 2])
We keep 1.22e+06/3.85e+07 =  3% of the original kernel matrix.

torch.Size([13892, 2])
We keep 2.40e+06/7.06e+07 =  3% of the original kernel matrix.

torch.Size([18870, 2])
We keep 3.36e+06/1.54e+08 =  2% of the original kernel matrix.

torch.Size([3167, 2])
We keep 1.27e+05/1.93e+06 =  6% of the original kernel matrix.

torch.Size([9657, 2])
We keep 8.94e+05/2.54e+07 =  3% of the original kernel matrix.

torch.Size([6586, 2])
We keep 5.44e+05/1.15e+07 =  4% of the original kernel matrix.

torch.Size([13086, 2])
We keep 1.77e+06/6.19e+07 =  2% of the original kernel matrix.

torch.Size([4135, 2])
We keep 1.77e+05/2.87e+06 =  6% of the original kernel matrix.

torch.Size([10849, 2])
We keep 1.02e+06/3.10e+07 =  3% of the original kernel matrix.

torch.Size([5940, 2])
We keep 3.90e+05/8.05e+06 =  4% of the original kernel matrix.

torch.Size([12523, 2])
We keep 1.48e+06/5.19e+07 =  2% of the original kernel matrix.

torch.Size([147820, 2])
We keep 2.27e+08/1.41e+10 =  1% of the original kernel matrix.

torch.Size([62810, 2])
We keep 3.04e+07/2.17e+09 =  1% of the original kernel matrix.

torch.Size([13237, 2])
We keep 3.21e+06/9.01e+07 =  3% of the original kernel matrix.

torch.Size([17970, 2])
We keep 3.75e+06/1.74e+08 =  2% of the original kernel matrix.

torch.Size([66295, 2])
We keep 5.72e+07/2.50e+09 =  2% of the original kernel matrix.

torch.Size([40395, 2])
We keep 1.44e+07/9.15e+08 =  1% of the original kernel matrix.

torch.Size([4227, 2])
We keep 3.25e+05/5.04e+06 =  6% of the original kernel matrix.

torch.Size([10784, 2])
We keep 1.23e+06/4.11e+07 =  2% of the original kernel matrix.

torch.Size([7317, 2])
We keep 3.60e+06/2.04e+07 = 17% of the original kernel matrix.

torch.Size([13443, 2])
We keep 1.98e+06/8.26e+07 =  2% of the original kernel matrix.

torch.Size([12785, 2])
We keep 2.31e+06/6.58e+07 =  3% of the original kernel matrix.

torch.Size([17888, 2])
We keep 3.30e+06/1.48e+08 =  2% of the original kernel matrix.

torch.Size([447715, 2])
We keep 2.15e+09/1.19e+11 =  1% of the original kernel matrix.

torch.Size([111671, 2])
We keep 7.80e+07/6.30e+09 =  1% of the original kernel matrix.

torch.Size([447091, 2])
We keep 8.39e+08/9.51e+10 =  0% of the original kernel matrix.

torch.Size([112016, 2])
We keep 7.06e+07/5.64e+09 =  1% of the original kernel matrix.

torch.Size([10452, 2])
We keep 1.93e+06/3.58e+07 =  5% of the original kernel matrix.

torch.Size([16355, 2])
We keep 2.56e+06/1.09e+08 =  2% of the original kernel matrix.

torch.Size([56064, 2])
We keep 6.60e+07/1.28e+09 =  5% of the original kernel matrix.

torch.Size([37911, 2])
We keep 1.10e+07/6.53e+08 =  1% of the original kernel matrix.

torch.Size([3739, 2])
We keep 1.32e+05/2.09e+06 =  6% of the original kernel matrix.

torch.Size([10485, 2])
We keep 9.14e+05/2.64e+07 =  3% of the original kernel matrix.

torch.Size([7069, 2])
We keep 8.24e+06/4.57e+07 = 18% of the original kernel matrix.

torch.Size([13095, 2])
We keep 2.81e+06/1.24e+08 =  2% of the original kernel matrix.

torch.Size([29625, 2])
We keep 3.77e+07/8.53e+08 =  4% of the original kernel matrix.

torch.Size([26240, 2])
We keep 9.10e+06/5.34e+08 =  1% of the original kernel matrix.

torch.Size([17260, 2])
We keep 3.48e+06/1.02e+08 =  3% of the original kernel matrix.

torch.Size([21163, 2])
We keep 3.79e+06/1.85e+08 =  2% of the original kernel matrix.

torch.Size([15726, 2])
We keep 1.17e+07/1.06e+08 = 10% of the original kernel matrix.

torch.Size([20098, 2])
We keep 4.11e+06/1.89e+08 =  2% of the original kernel matrix.

torch.Size([60937, 2])
We keep 4.43e+07/1.87e+09 =  2% of the original kernel matrix.

torch.Size([39254, 2])
We keep 1.26e+07/7.92e+08 =  1% of the original kernel matrix.

torch.Size([512475, 2])
We keep 2.11e+09/1.55e+11 =  1% of the original kernel matrix.

torch.Size([119883, 2])
We keep 9.14e+07/7.20e+09 =  1% of the original kernel matrix.

torch.Size([25032, 2])
We keep 1.19e+07/3.94e+08 =  3% of the original kernel matrix.

torch.Size([24791, 2])
We keep 6.55e+06/3.63e+08 =  1% of the original kernel matrix.

torch.Size([6997, 2])
We keep 9.76e+05/1.83e+07 =  5% of the original kernel matrix.

torch.Size([13092, 2])
We keep 2.03e+06/7.83e+07 =  2% of the original kernel matrix.

torch.Size([22432, 2])
We keep 6.85e+06/2.65e+08 =  2% of the original kernel matrix.

torch.Size([25881, 2])
We keep 6.03e+06/2.98e+08 =  2% of the original kernel matrix.

torch.Size([8771, 2])
We keep 7.64e+05/2.04e+07 =  3% of the original kernel matrix.

torch.Size([14922, 2])
We keep 2.10e+06/8.26e+07 =  2% of the original kernel matrix.

torch.Size([3091, 2])
We keep 9.95e+04/1.49e+06 =  6% of the original kernel matrix.

torch.Size([9608, 2])
We keep 8.20e+05/2.24e+07 =  3% of the original kernel matrix.

torch.Size([119156, 2])
We keep 1.34e+08/6.19e+09 =  2% of the original kernel matrix.

torch.Size([55909, 2])
We keep 2.12e+07/1.44e+09 =  1% of the original kernel matrix.

torch.Size([15115, 2])
We keep 5.30e+06/1.11e+08 =  4% of the original kernel matrix.

torch.Size([19375, 2])
We keep 4.02e+06/1.92e+08 =  2% of the original kernel matrix.

torch.Size([1709, 2])
We keep 5.46e+04/5.93e+05 =  9% of the original kernel matrix.

torch.Size([7491, 2])
We keep 6.09e+05/1.41e+07 =  4% of the original kernel matrix.

torch.Size([67860, 2])
We keep 3.11e+07/1.80e+09 =  1% of the original kernel matrix.

torch.Size([41380, 2])
We keep 1.23e+07/7.76e+08 =  1% of the original kernel matrix.

torch.Size([11851, 2])
We keep 1.51e+06/4.36e+07 =  3% of the original kernel matrix.

torch.Size([17294, 2])
We keep 2.83e+06/1.21e+08 =  2% of the original kernel matrix.

torch.Size([1630, 2])
We keep 3.01e+04/3.26e+05 =  9% of the original kernel matrix.

torch.Size([7591, 2])
We keep 4.89e+05/1.04e+07 =  4% of the original kernel matrix.

torch.Size([79881, 2])
We keep 6.48e+07/2.57e+09 =  2% of the original kernel matrix.

torch.Size([44753, 2])
We keep 1.47e+07/9.27e+08 =  1% of the original kernel matrix.

torch.Size([110674, 2])
We keep 8.22e+07/5.67e+09 =  1% of the original kernel matrix.

torch.Size([53537, 2])
We keep 2.04e+07/1.38e+09 =  1% of the original kernel matrix.

torch.Size([107938, 2])
We keep 1.98e+08/6.73e+09 =  2% of the original kernel matrix.

torch.Size([52561, 2])
We keep 2.25e+07/1.50e+09 =  1% of the original kernel matrix.

torch.Size([122751, 2])
We keep 1.64e+08/6.96e+09 =  2% of the original kernel matrix.

torch.Size([56582, 2])
We keep 2.26e+07/1.53e+09 =  1% of the original kernel matrix.

torch.Size([169654, 2])
We keep 3.28e+08/1.70e+10 =  1% of the original kernel matrix.

torch.Size([66923, 2])
We keep 3.32e+07/2.39e+09 =  1% of the original kernel matrix.

torch.Size([16176, 2])
We keep 5.81e+06/1.56e+08 =  3% of the original kernel matrix.

torch.Size([19873, 2])
We keep 4.72e+06/2.29e+08 =  2% of the original kernel matrix.

torch.Size([15565, 2])
We keep 5.46e+06/1.02e+08 =  5% of the original kernel matrix.

torch.Size([19817, 2])
We keep 3.73e+06/1.84e+08 =  2% of the original kernel matrix.

torch.Size([108617, 2])
We keep 8.42e+07/6.09e+09 =  1% of the original kernel matrix.

torch.Size([52994, 2])
We keep 2.14e+07/1.43e+09 =  1% of the original kernel matrix.

torch.Size([100949, 2])
We keep 6.59e+07/5.01e+09 =  1% of the original kernel matrix.

torch.Size([50792, 2])
We keep 1.92e+07/1.30e+09 =  1% of the original kernel matrix.

torch.Size([9839, 2])
We keep 9.26e+05/2.43e+07 =  3% of the original kernel matrix.

torch.Size([15736, 2])
We keep 2.26e+06/9.02e+07 =  2% of the original kernel matrix.

torch.Size([36791, 2])
We keep 2.83e+07/7.55e+08 =  3% of the original kernel matrix.

torch.Size([30806, 2])
We keep 8.03e+06/5.03e+08 =  1% of the original kernel matrix.

torch.Size([108067, 2])
We keep 1.45e+08/8.37e+09 =  1% of the original kernel matrix.

torch.Size([52860, 2])
We keep 2.36e+07/1.67e+09 =  1% of the original kernel matrix.

torch.Size([115754, 2])
We keep 1.59e+08/7.73e+09 =  2% of the original kernel matrix.

torch.Size([54867, 2])
We keep 2.41e+07/1.61e+09 =  1% of the original kernel matrix.

torch.Size([70047, 2])
We keep 1.03e+08/4.03e+09 =  2% of the original kernel matrix.

torch.Size([41449, 2])
We keep 1.80e+07/1.16e+09 =  1% of the original kernel matrix.

torch.Size([43595, 2])
We keep 1.02e+08/2.17e+09 =  4% of the original kernel matrix.

torch.Size([30477, 2])
We keep 1.37e+07/8.53e+08 =  1% of the original kernel matrix.

torch.Size([11350, 2])
We keep 2.13e+06/4.79e+07 =  4% of the original kernel matrix.

torch.Size([16988, 2])
We keep 2.97e+06/1.27e+08 =  2% of the original kernel matrix.

torch.Size([5375, 2])
We keep 3.13e+05/6.09e+06 =  5% of the original kernel matrix.

torch.Size([11971, 2])
We keep 1.34e+06/4.51e+07 =  2% of the original kernel matrix.

torch.Size([6511, 2])
We keep 6.49e+05/1.23e+07 =  5% of the original kernel matrix.

torch.Size([12767, 2])
We keep 1.76e+06/6.41e+07 =  2% of the original kernel matrix.

torch.Size([108287, 2])
We keep 5.86e+07/4.49e+09 =  1% of the original kernel matrix.

torch.Size([52859, 2])
We keep 1.83e+07/1.23e+09 =  1% of the original kernel matrix.

torch.Size([92640, 2])
We keep 4.73e+07/3.51e+09 =  1% of the original kernel matrix.

torch.Size([48710, 2])
We keep 1.64e+07/1.08e+09 =  1% of the original kernel matrix.

torch.Size([344880, 2])
We keep 8.31e+08/6.28e+10 =  1% of the original kernel matrix.

torch.Size([96038, 2])
We keep 5.84e+07/4.59e+09 =  1% of the original kernel matrix.

torch.Size([92447, 2])
We keep 1.16e+08/4.56e+09 =  2% of the original kernel matrix.

torch.Size([48500, 2])
We keep 1.87e+07/1.23e+09 =  1% of the original kernel matrix.

torch.Size([54478, 2])
We keep 2.63e+07/1.33e+09 =  1% of the original kernel matrix.

torch.Size([37304, 2])
We keep 1.11e+07/6.67e+08 =  1% of the original kernel matrix.

torch.Size([14038, 2])
We keep 2.55e+06/9.96e+07 =  2% of the original kernel matrix.

torch.Size([18767, 2])
We keep 3.80e+06/1.83e+08 =  2% of the original kernel matrix.

torch.Size([133162, 2])
We keep 1.19e+08/7.00e+09 =  1% of the original kernel matrix.

torch.Size([59494, 2])
We keep 2.25e+07/1.53e+09 =  1% of the original kernel matrix.

torch.Size([3941, 2])
We keep 2.78e+05/4.07e+06 =  6% of the original kernel matrix.

torch.Size([10359, 2])
We keep 1.17e+06/3.69e+07 =  3% of the original kernel matrix.

torch.Size([8613, 2])
We keep 1.17e+06/2.56e+07 =  4% of the original kernel matrix.

torch.Size([14604, 2])
We keep 2.33e+06/9.25e+07 =  2% of the original kernel matrix.

torch.Size([158900, 2])
We keep 1.69e+08/1.04e+10 =  1% of the original kernel matrix.

torch.Size([65123, 2])
We keep 2.67e+07/1.87e+09 =  1% of the original kernel matrix.

torch.Size([14447, 2])
We keep 2.66e+06/7.20e+07 =  3% of the original kernel matrix.

torch.Size([19259, 2])
We keep 3.37e+06/1.55e+08 =  2% of the original kernel matrix.

torch.Size([5077, 2])
We keep 3.32e+05/5.89e+06 =  5% of the original kernel matrix.

torch.Size([11565, 2])
We keep 1.33e+06/4.44e+07 =  2% of the original kernel matrix.

torch.Size([16257, 2])
We keep 2.69e+06/9.87e+07 =  2% of the original kernel matrix.

torch.Size([20531, 2])
We keep 3.80e+06/1.82e+08 =  2% of the original kernel matrix.

torch.Size([196953, 2])
We keep 7.41e+08/3.71e+10 =  1% of the original kernel matrix.

torch.Size([71238, 2])
We keep 4.76e+07/3.52e+09 =  1% of the original kernel matrix.

torch.Size([15052, 2])
We keep 2.69e+06/8.61e+07 =  3% of the original kernel matrix.

torch.Size([19561, 2])
We keep 3.69e+06/1.70e+08 =  2% of the original kernel matrix.

torch.Size([109693, 2])
We keep 1.55e+08/8.51e+09 =  1% of the original kernel matrix.

torch.Size([53582, 2])
We keep 2.54e+07/1.69e+09 =  1% of the original kernel matrix.

torch.Size([4641, 2])
We keep 1.11e+06/6.38e+06 = 17% of the original kernel matrix.

torch.Size([11022, 2])
We keep 1.31e+06/4.62e+07 =  2% of the original kernel matrix.

torch.Size([5940, 2])
We keep 1.48e+06/1.12e+07 = 13% of the original kernel matrix.

torch.Size([12407, 2])
We keep 1.63e+06/6.12e+07 =  2% of the original kernel matrix.

torch.Size([6237, 2])
We keep 4.57e+05/9.50e+06 =  4% of the original kernel matrix.

torch.Size([12795, 2])
We keep 1.61e+06/5.64e+07 =  2% of the original kernel matrix.

torch.Size([7040, 2])
We keep 6.31e+05/1.24e+07 =  5% of the original kernel matrix.

torch.Size([13386, 2])
We keep 1.75e+06/6.44e+07 =  2% of the original kernel matrix.

torch.Size([44707, 2])
We keep 2.85e+07/8.38e+08 =  3% of the original kernel matrix.

torch.Size([33859, 2])
We keep 9.18e+06/5.30e+08 =  1% of the original kernel matrix.

torch.Size([44082, 2])
We keep 2.57e+07/9.70e+08 =  2% of the original kernel matrix.

torch.Size([33119, 2])
We keep 9.81e+06/5.70e+08 =  1% of the original kernel matrix.

torch.Size([4160, 2])
We keep 1.92e+05/3.41e+06 =  5% of the original kernel matrix.

torch.Size([10766, 2])
We keep 1.10e+06/3.38e+07 =  3% of the original kernel matrix.

torch.Size([99029, 2])
We keep 1.10e+08/5.12e+09 =  2% of the original kernel matrix.

torch.Size([50381, 2])
We keep 1.99e+07/1.31e+09 =  1% of the original kernel matrix.

torch.Size([4106, 2])
We keep 2.87e+05/4.45e+06 =  6% of the original kernel matrix.

torch.Size([10666, 2])
We keep 1.18e+06/3.86e+07 =  3% of the original kernel matrix.

torch.Size([50141, 2])
We keep 3.63e+07/1.45e+09 =  2% of the original kernel matrix.

torch.Size([36516, 2])
We keep 1.20e+07/6.96e+08 =  1% of the original kernel matrix.

torch.Size([8826, 2])
We keep 1.60e+06/3.17e+07 =  5% of the original kernel matrix.

torch.Size([14787, 2])
We keep 2.53e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([66547, 2])
We keep 3.24e+07/1.75e+09 =  1% of the original kernel matrix.

torch.Size([40922, 2])
We keep 1.23e+07/7.65e+08 =  1% of the original kernel matrix.

torch.Size([8104, 2])
We keep 1.22e+06/2.14e+07 =  5% of the original kernel matrix.

torch.Size([14335, 2])
We keep 2.22e+06/8.47e+07 =  2% of the original kernel matrix.

torch.Size([18322, 2])
We keep 4.04e+06/1.28e+08 =  3% of the original kernel matrix.

torch.Size([21610, 2])
We keep 4.25e+06/2.07e+08 =  2% of the original kernel matrix.

torch.Size([17197, 2])
We keep 3.79e+06/1.26e+08 =  3% of the original kernel matrix.

torch.Size([21207, 2])
We keep 4.36e+06/2.05e+08 =  2% of the original kernel matrix.

torch.Size([18437, 2])
We keep 5.57e+06/1.83e+08 =  3% of the original kernel matrix.

torch.Size([21994, 2])
We keep 5.11e+06/2.47e+08 =  2% of the original kernel matrix.

torch.Size([72834, 2])
We keep 4.06e+07/2.24e+09 =  1% of the original kernel matrix.

torch.Size([43041, 2])
We keep 1.38e+07/8.65e+08 =  1% of the original kernel matrix.

torch.Size([215552, 2])
We keep 5.19e+08/2.55e+10 =  2% of the original kernel matrix.

torch.Size([76041, 2])
We keep 3.97e+07/2.92e+09 =  1% of the original kernel matrix.

torch.Size([7733, 2])
We keep 5.42e+05/1.24e+07 =  4% of the original kernel matrix.

torch.Size([14128, 2])
We keep 1.75e+06/6.44e+07 =  2% of the original kernel matrix.

torch.Size([9367, 2])
We keep 2.67e+06/4.45e+07 =  6% of the original kernel matrix.

torch.Size([15229, 2])
We keep 2.82e+06/1.22e+08 =  2% of the original kernel matrix.

torch.Size([80199, 2])
We keep 1.07e+08/2.96e+09 =  3% of the original kernel matrix.

torch.Size([45056, 2])
We keep 1.53e+07/9.95e+08 =  1% of the original kernel matrix.

torch.Size([14062, 2])
We keep 2.67e+06/7.52e+07 =  3% of the original kernel matrix.

torch.Size([18879, 2])
We keep 3.53e+06/1.59e+08 =  2% of the original kernel matrix.

torch.Size([527378, 2])
We keep 8.54e+08/1.06e+11 =  0% of the original kernel matrix.

torch.Size([122454, 2])
We keep 7.44e+07/5.95e+09 =  1% of the original kernel matrix.

torch.Size([2412, 2])
We keep 9.21e+04/1.21e+06 =  7% of the original kernel matrix.

torch.Size([8572, 2])
We keep 7.79e+05/2.01e+07 =  3% of the original kernel matrix.

torch.Size([92035, 2])
We keep 6.89e+07/4.14e+09 =  1% of the original kernel matrix.

torch.Size([48383, 2])
We keep 1.79e+07/1.18e+09 =  1% of the original kernel matrix.

torch.Size([178795, 2])
We keep 1.45e+08/1.36e+10 =  1% of the original kernel matrix.

torch.Size([69572, 2])
We keep 2.96e+07/2.13e+09 =  1% of the original kernel matrix.

torch.Size([3103, 2])
We keep 1.21e+05/1.80e+06 =  6% of the original kernel matrix.

torch.Size([9700, 2])
We keep 8.87e+05/2.45e+07 =  3% of the original kernel matrix.

torch.Size([120112, 2])
We keep 2.40e+08/1.09e+10 =  2% of the original kernel matrix.

torch.Size([54368, 2])
We keep 2.72e+07/1.91e+09 =  1% of the original kernel matrix.

torch.Size([10974, 2])
We keep 3.21e+06/8.12e+07 =  3% of the original kernel matrix.

torch.Size([16335, 2])
We keep 3.65e+06/1.65e+08 =  2% of the original kernel matrix.

torch.Size([698268, 2])
We keep 2.78e+09/2.67e+11 =  1% of the original kernel matrix.

torch.Size([145405, 2])
We keep 1.17e+08/9.46e+09 =  1% of the original kernel matrix.

torch.Size([7968, 2])
We keep 6.17e+05/1.43e+07 =  4% of the original kernel matrix.

torch.Size([14152, 2])
We keep 1.85e+06/6.91e+07 =  2% of the original kernel matrix.

torch.Size([10901, 2])
We keep 2.27e+06/4.16e+07 =  5% of the original kernel matrix.

torch.Size([16538, 2])
We keep 2.67e+06/1.18e+08 =  2% of the original kernel matrix.

torch.Size([7368, 2])
We keep 1.55e+06/2.71e+07 =  5% of the original kernel matrix.

torch.Size([13521, 2])
We keep 2.44e+06/9.52e+07 =  2% of the original kernel matrix.

torch.Size([6489, 2])
We keep 4.28e+05/9.41e+06 =  4% of the original kernel matrix.

torch.Size([12926, 2])
We keep 1.60e+06/5.61e+07 =  2% of the original kernel matrix.

torch.Size([11930, 2])
We keep 2.49e+06/7.30e+07 =  3% of the original kernel matrix.

torch.Size([17221, 2])
We keep 3.47e+06/1.56e+08 =  2% of the original kernel matrix.

torch.Size([26348, 2])
We keep 1.02e+07/3.29e+08 =  3% of the original kernel matrix.

torch.Size([26165, 2])
We keep 5.98e+06/3.32e+08 =  1% of the original kernel matrix.

torch.Size([15758, 2])
We keep 1.17e+07/1.45e+08 =  8% of the original kernel matrix.

torch.Size([19936, 2])
We keep 4.55e+06/2.20e+08 =  2% of the original kernel matrix.

torch.Size([88134, 2])
We keep 7.63e+07/3.89e+09 =  1% of the original kernel matrix.

torch.Size([47210, 2])
We keep 1.74e+07/1.14e+09 =  1% of the original kernel matrix.

torch.Size([15762, 2])
We keep 2.72e+06/9.92e+07 =  2% of the original kernel matrix.

torch.Size([20109, 2])
We keep 3.88e+06/1.82e+08 =  2% of the original kernel matrix.

torch.Size([3894, 2])
We keep 1.80e+05/3.14e+06 =  5% of the original kernel matrix.

torch.Size([10422, 2])
We keep 1.06e+06/3.24e+07 =  3% of the original kernel matrix.

torch.Size([10982, 2])
We keep 3.12e+06/6.06e+07 =  5% of the original kernel matrix.

torch.Size([16411, 2])
We keep 3.22e+06/1.42e+08 =  2% of the original kernel matrix.

torch.Size([249326, 2])
We keep 5.28e+08/3.06e+10 =  1% of the original kernel matrix.

torch.Size([83193, 2])
We keep 4.34e+07/3.20e+09 =  1% of the original kernel matrix.

torch.Size([86573, 2])
We keep 1.08e+08/4.62e+09 =  2% of the original kernel matrix.

torch.Size([46092, 2])
We keep 1.87e+07/1.24e+09 =  1% of the original kernel matrix.

torch.Size([257285, 2])
We keep 3.89e+08/3.01e+10 =  1% of the original kernel matrix.

torch.Size([85620, 2])
We keep 4.30e+07/3.17e+09 =  1% of the original kernel matrix.

torch.Size([17786, 2])
We keep 4.22e+06/1.15e+08 =  3% of the original kernel matrix.

torch.Size([21374, 2])
We keep 3.83e+06/1.96e+08 =  1% of the original kernel matrix.

torch.Size([12416, 2])
We keep 1.98e+06/5.73e+07 =  3% of the original kernel matrix.

torch.Size([17835, 2])
We keep 3.06e+06/1.38e+08 =  2% of the original kernel matrix.

torch.Size([3780, 2])
We keep 1.51e+05/2.56e+06 =  5% of the original kernel matrix.

torch.Size([10449, 2])
We keep 9.94e+05/2.93e+07 =  3% of the original kernel matrix.

torch.Size([5696, 2])
We keep 5.44e+05/9.30e+06 =  5% of the original kernel matrix.

torch.Size([12186, 2])
We keep 1.59e+06/5.58e+07 =  2% of the original kernel matrix.

torch.Size([117288, 2])
We keep 1.61e+08/7.57e+09 =  2% of the original kernel matrix.

torch.Size([55576, 2])
We keep 2.22e+07/1.59e+09 =  1% of the original kernel matrix.

torch.Size([12431, 2])
We keep 1.66e+06/4.83e+07 =  3% of the original kernel matrix.

torch.Size([17697, 2])
We keep 2.98e+06/1.27e+08 =  2% of the original kernel matrix.

torch.Size([16972, 2])
We keep 3.69e+06/1.07e+08 =  3% of the original kernel matrix.

torch.Size([20864, 2])
We keep 3.98e+06/1.89e+08 =  2% of the original kernel matrix.

torch.Size([27795, 2])
We keep 1.17e+07/3.48e+08 =  3% of the original kernel matrix.

torch.Size([26749, 2])
We keep 6.31e+06/3.41e+08 =  1% of the original kernel matrix.

torch.Size([30310, 2])
We keep 1.12e+07/4.83e+08 =  2% of the original kernel matrix.

torch.Size([27659, 2])
We keep 7.12e+06/4.02e+08 =  1% of the original kernel matrix.

torch.Size([7681, 2])
We keep 1.16e+06/1.91e+07 =  6% of the original kernel matrix.

torch.Size([13896, 2])
We keep 2.07e+06/8.00e+07 =  2% of the original kernel matrix.

torch.Size([2879, 2])
We keep 1.58e+05/2.18e+06 =  7% of the original kernel matrix.

torch.Size([8987, 2])
We keep 9.39e+05/2.70e+07 =  3% of the original kernel matrix.

torch.Size([165009, 2])
We keep 4.32e+08/2.29e+10 =  1% of the original kernel matrix.

torch.Size([65847, 2])
We keep 3.86e+07/2.77e+09 =  1% of the original kernel matrix.

torch.Size([35786, 2])
We keep 1.76e+07/6.62e+08 =  2% of the original kernel matrix.

torch.Size([30148, 2])
We keep 8.31e+06/4.71e+08 =  1% of the original kernel matrix.

torch.Size([16478, 2])
We keep 2.57e+07/2.59e+08 =  9% of the original kernel matrix.

torch.Size([19732, 2])
We keep 5.54e+06/2.95e+08 =  1% of the original kernel matrix.

torch.Size([15347, 2])
We keep 3.38e+06/7.93e+07 =  4% of the original kernel matrix.

torch.Size([19793, 2])
We keep 3.35e+06/1.63e+08 =  2% of the original kernel matrix.

torch.Size([81923, 2])
We keep 7.81e+07/3.35e+09 =  2% of the original kernel matrix.

torch.Size([45518, 2])
We keep 1.67e+07/1.06e+09 =  1% of the original kernel matrix.

torch.Size([170224, 2])
We keep 2.24e+08/1.31e+10 =  1% of the original kernel matrix.

torch.Size([67722, 2])
We keep 2.94e+07/2.09e+09 =  1% of the original kernel matrix.

torch.Size([369810, 2])
We keep 7.90e+08/5.95e+10 =  1% of the original kernel matrix.

torch.Size([100544, 2])
We keep 5.81e+07/4.46e+09 =  1% of the original kernel matrix.

torch.Size([132721, 2])
We keep 1.49e+08/7.88e+09 =  1% of the original kernel matrix.

torch.Size([59187, 2])
We keep 2.36e+07/1.62e+09 =  1% of the original kernel matrix.

torch.Size([8889, 2])
We keep 4.92e+06/3.05e+07 = 16% of the original kernel matrix.

torch.Size([15079, 2])
We keep 2.51e+06/1.01e+08 =  2% of the original kernel matrix.

torch.Size([30889, 2])
We keep 6.91e+06/3.67e+08 =  1% of the original kernel matrix.

torch.Size([28472, 2])
We keep 6.38e+06/3.51e+08 =  1% of the original kernel matrix.

torch.Size([32928, 2])
We keep 1.96e+07/6.31e+08 =  3% of the original kernel matrix.

torch.Size([28719, 2])
We keep 8.19e+06/4.60e+08 =  1% of the original kernel matrix.

torch.Size([7510, 2])
We keep 1.47e+06/1.50e+07 =  9% of the original kernel matrix.

torch.Size([13927, 2])
We keep 1.88e+06/7.10e+07 =  2% of the original kernel matrix.

torch.Size([12538, 2])
We keep 1.89e+06/5.60e+07 =  3% of the original kernel matrix.

torch.Size([17776, 2])
We keep 3.08e+06/1.37e+08 =  2% of the original kernel matrix.

torch.Size([7737, 2])
We keep 5.19e+05/1.26e+07 =  4% of the original kernel matrix.

torch.Size([14159, 2])
We keep 1.74e+06/6.50e+07 =  2% of the original kernel matrix.

torch.Size([11817, 2])
We keep 5.10e+06/5.55e+07 =  9% of the original kernel matrix.

torch.Size([17180, 2])
We keep 3.20e+06/1.36e+08 =  2% of the original kernel matrix.

torch.Size([12672, 2])
We keep 2.23e+06/6.66e+07 =  3% of the original kernel matrix.

torch.Size([17772, 2])
We keep 3.29e+06/1.49e+08 =  2% of the original kernel matrix.

torch.Size([1813, 2])
We keep 4.37e+04/4.69e+05 =  9% of the original kernel matrix.

torch.Size([7680, 2])
We keep 5.68e+05/1.25e+07 =  4% of the original kernel matrix.

torch.Size([12393, 2])
We keep 1.69e+06/5.24e+07 =  3% of the original kernel matrix.

torch.Size([17573, 2])
We keep 3.02e+06/1.32e+08 =  2% of the original kernel matrix.

torch.Size([6604, 2])
We keep 8.77e+05/1.66e+07 =  5% of the original kernel matrix.

torch.Size([12990, 2])
We keep 1.84e+06/7.45e+07 =  2% of the original kernel matrix.

torch.Size([17417, 2])
We keep 1.99e+07/3.22e+08 =  6% of the original kernel matrix.

torch.Size([20534, 2])
We keep 5.85e+06/3.28e+08 =  1% of the original kernel matrix.

torch.Size([6905, 2])
We keep 7.88e+05/1.70e+07 =  4% of the original kernel matrix.

torch.Size([13048, 2])
We keep 1.98e+06/7.54e+07 =  2% of the original kernel matrix.

torch.Size([23935, 2])
We keep 6.22e+06/2.21e+08 =  2% of the original kernel matrix.

torch.Size([25502, 2])
We keep 5.29e+06/2.72e+08 =  1% of the original kernel matrix.

torch.Size([6101, 2])
We keep 3.50e+06/1.85e+07 = 18% of the original kernel matrix.

torch.Size([12201, 2])
We keep 1.86e+06/7.86e+07 =  2% of the original kernel matrix.

torch.Size([13945, 2])
We keep 2.50e+06/6.89e+07 =  3% of the original kernel matrix.

torch.Size([18834, 2])
We keep 3.30e+06/1.52e+08 =  2% of the original kernel matrix.

torch.Size([6120, 2])
We keep 5.60e+05/9.41e+06 =  5% of the original kernel matrix.

torch.Size([12523, 2])
We keep 1.60e+06/5.61e+07 =  2% of the original kernel matrix.

torch.Size([11525, 2])
We keep 1.31e+06/3.60e+07 =  3% of the original kernel matrix.

torch.Size([17018, 2])
We keep 2.63e+06/1.10e+08 =  2% of the original kernel matrix.

torch.Size([18779, 2])
We keep 3.03e+06/1.23e+08 =  2% of the original kernel matrix.

torch.Size([22364, 2])
We keep 4.15e+06/2.03e+08 =  2% of the original kernel matrix.

torch.Size([162672, 2])
We keep 2.59e+08/1.37e+10 =  1% of the original kernel matrix.

torch.Size([66174, 2])
We keep 3.03e+07/2.14e+09 =  1% of the original kernel matrix.

torch.Size([6564, 2])
We keep 4.33e+05/9.28e+06 =  4% of the original kernel matrix.

torch.Size([13145, 2])
We keep 1.59e+06/5.57e+07 =  2% of the original kernel matrix.

torch.Size([1875, 2])
We keep 3.39e+04/4.08e+05 =  8% of the original kernel matrix.

torch.Size([7906, 2])
We keep 5.20e+05/1.17e+07 =  4% of the original kernel matrix.

torch.Size([5562, 2])
We keep 4.57e+05/8.83e+06 =  5% of the original kernel matrix.

torch.Size([12083, 2])
We keep 1.59e+06/5.43e+07 =  2% of the original kernel matrix.

torch.Size([130025, 2])
We keep 2.48e+08/9.07e+09 =  2% of the original kernel matrix.

torch.Size([58653, 2])
We keep 2.52e+07/1.74e+09 =  1% of the original kernel matrix.

torch.Size([210772, 2])
We keep 3.45e+08/2.13e+10 =  1% of the original kernel matrix.

torch.Size([76380, 2])
We keep 3.55e+07/2.67e+09 =  1% of the original kernel matrix.

torch.Size([45789, 2])
We keep 4.41e+07/1.68e+09 =  2% of the original kernel matrix.

torch.Size([31918, 2])
We keep 1.23e+07/7.51e+08 =  1% of the original kernel matrix.

torch.Size([7826, 2])
We keep 9.34e+05/2.02e+07 =  4% of the original kernel matrix.

torch.Size([13800, 2])
We keep 2.11e+06/8.22e+07 =  2% of the original kernel matrix.

torch.Size([87793, 2])
We keep 8.40e+07/4.34e+09 =  1% of the original kernel matrix.

torch.Size([46927, 2])
We keep 1.85e+07/1.21e+09 =  1% of the original kernel matrix.

torch.Size([79778, 2])
We keep 5.68e+07/3.72e+09 =  1% of the original kernel matrix.

torch.Size([44336, 2])
We keep 1.75e+07/1.12e+09 =  1% of the original kernel matrix.

torch.Size([2872, 2])
We keep 1.06e+05/1.52e+06 =  6% of the original kernel matrix.

torch.Size([9293, 2])
We keep 8.32e+05/2.25e+07 =  3% of the original kernel matrix.

torch.Size([8750, 2])
We keep 1.40e+06/2.54e+07 =  5% of the original kernel matrix.

torch.Size([14928, 2])
We keep 2.31e+06/9.23e+07 =  2% of the original kernel matrix.

torch.Size([22980, 2])
We keep 3.86e+06/1.95e+08 =  1% of the original kernel matrix.

torch.Size([23771, 2])
We keep 4.71e+06/2.55e+08 =  1% of the original kernel matrix.

torch.Size([36653, 2])
We keep 4.49e+07/1.05e+09 =  4% of the original kernel matrix.

torch.Size([30024, 2])
We keep 1.03e+07/5.93e+08 =  1% of the original kernel matrix.

torch.Size([15043, 2])
We keep 3.58e+06/1.01e+08 =  3% of the original kernel matrix.

torch.Size([19531, 2])
We keep 3.83e+06/1.84e+08 =  2% of the original kernel matrix.

torch.Size([188029, 2])
We keep 2.08e+08/1.55e+10 =  1% of the original kernel matrix.

torch.Size([71619, 2])
We keep 3.20e+07/2.28e+09 =  1% of the original kernel matrix.

torch.Size([10363, 2])
We keep 9.07e+05/2.37e+07 =  3% of the original kernel matrix.

torch.Size([16149, 2])
We keep 2.19e+06/8.90e+07 =  2% of the original kernel matrix.

torch.Size([4564, 2])
We keep 2.24e+05/4.18e+06 =  5% of the original kernel matrix.

torch.Size([11205, 2])
We keep 1.18e+06/3.74e+07 =  3% of the original kernel matrix.

torch.Size([3239, 2])
We keep 1.18e+05/1.80e+06 =  6% of the original kernel matrix.

torch.Size([9831, 2])
We keep 8.88e+05/2.45e+07 =  3% of the original kernel matrix.

torch.Size([19488, 2])
We keep 5.57e+06/1.47e+08 =  3% of the original kernel matrix.

torch.Size([22459, 2])
We keep 4.39e+06/2.22e+08 =  1% of the original kernel matrix.

torch.Size([13009, 2])
We keep 1.41e+06/4.28e+07 =  3% of the original kernel matrix.

torch.Size([18195, 2])
We keep 2.76e+06/1.20e+08 =  2% of the original kernel matrix.

torch.Size([20004, 2])
We keep 4.79e+06/1.93e+08 =  2% of the original kernel matrix.

torch.Size([22027, 2])
We keep 4.69e+06/2.54e+08 =  1% of the original kernel matrix.

torch.Size([187047, 2])
We keep 4.18e+08/1.74e+10 =  2% of the original kernel matrix.

torch.Size([71424, 2])
We keep 3.38e+07/2.42e+09 =  1% of the original kernel matrix.

torch.Size([595976, 2])
We keep 1.08e+09/1.23e+11 =  0% of the original kernel matrix.

torch.Size([130543, 2])
We keep 8.02e+07/6.41e+09 =  1% of the original kernel matrix.

torch.Size([8781, 2])
We keep 9.95e+05/2.15e+07 =  4% of the original kernel matrix.

torch.Size([14731, 2])
We keep 2.16e+06/8.48e+07 =  2% of the original kernel matrix.

torch.Size([3637, 2])
We keep 1.63e+05/2.70e+06 =  6% of the original kernel matrix.

torch.Size([10254, 2])
We keep 1.02e+06/3.01e+07 =  3% of the original kernel matrix.

torch.Size([18540, 2])
We keep 2.98e+06/1.20e+08 =  2% of the original kernel matrix.

torch.Size([22090, 2])
We keep 4.17e+06/2.00e+08 =  2% of the original kernel matrix.

torch.Size([107789, 2])
We keep 1.10e+09/5.75e+09 = 19% of the original kernel matrix.

torch.Size([53122, 2])
We keep 1.79e+07/1.39e+09 =  1% of the original kernel matrix.

torch.Size([27018, 2])
We keep 7.19e+06/2.90e+08 =  2% of the original kernel matrix.

torch.Size([26604, 2])
We keep 5.96e+06/3.12e+08 =  1% of the original kernel matrix.

torch.Size([2960, 2])
We keep 1.46e+05/1.84e+06 =  7% of the original kernel matrix.

torch.Size([9360, 2])
We keep 8.53e+05/2.48e+07 =  3% of the original kernel matrix.

torch.Size([3078, 2])
We keep 1.40e+05/2.10e+06 =  6% of the original kernel matrix.

torch.Size([9303, 2])
We keep 9.45e+05/2.65e+07 =  3% of the original kernel matrix.

torch.Size([42714, 2])
We keep 8.91e+07/8.56e+08 = 10% of the original kernel matrix.

torch.Size([33053, 2])
We keep 9.35e+06/5.35e+08 =  1% of the original kernel matrix.

torch.Size([3148, 2])
We keep 1.38e+05/1.86e+06 =  7% of the original kernel matrix.

torch.Size([9633, 2])
We keep 8.98e+05/2.49e+07 =  3% of the original kernel matrix.

torch.Size([61207, 2])
We keep 5.63e+07/2.33e+09 =  2% of the original kernel matrix.

torch.Size([38225, 2])
We keep 1.42e+07/8.82e+08 =  1% of the original kernel matrix.

torch.Size([4944, 2])
We keep 2.77e+05/5.11e+06 =  5% of the original kernel matrix.

torch.Size([11564, 2])
We keep 1.28e+06/4.13e+07 =  3% of the original kernel matrix.

torch.Size([16320, 2])
We keep 2.05e+06/7.90e+07 =  2% of the original kernel matrix.

torch.Size([20551, 2])
We keep 3.49e+06/1.63e+08 =  2% of the original kernel matrix.

torch.Size([6477, 2])
We keep 6.73e+05/1.39e+07 =  4% of the original kernel matrix.

torch.Size([12755, 2])
We keep 1.82e+06/6.83e+07 =  2% of the original kernel matrix.

torch.Size([5560, 2])
We keep 3.07e+05/6.19e+06 =  4% of the original kernel matrix.

torch.Size([12112, 2])
We keep 1.35e+06/4.55e+07 =  2% of the original kernel matrix.

torch.Size([4559, 2])
We keep 2.22e+05/3.83e+06 =  5% of the original kernel matrix.

torch.Size([11188, 2])
We keep 1.16e+06/3.58e+07 =  3% of the original kernel matrix.

torch.Size([131637, 2])
We keep 9.81e+07/6.97e+09 =  1% of the original kernel matrix.

torch.Size([58996, 2])
We keep 2.20e+07/1.53e+09 =  1% of the original kernel matrix.

torch.Size([312734, 2])
We keep 4.72e+08/4.02e+10 =  1% of the original kernel matrix.

torch.Size([93861, 2])
We keep 4.87e+07/3.67e+09 =  1% of the original kernel matrix.

torch.Size([134496, 2])
We keep 2.03e+08/9.61e+09 =  2% of the original kernel matrix.

torch.Size([59456, 2])
We keep 2.60e+07/1.79e+09 =  1% of the original kernel matrix.

torch.Size([47228, 2])
We keep 2.81e+07/1.12e+09 =  2% of the original kernel matrix.

torch.Size([34402, 2])
We keep 1.04e+07/6.11e+08 =  1% of the original kernel matrix.

torch.Size([22727, 2])
We keep 5.06e+06/2.05e+08 =  2% of the original kernel matrix.

torch.Size([23805, 2])
We keep 5.02e+06/2.62e+08 =  1% of the original kernel matrix.

torch.Size([8213, 2])
We keep 1.13e+06/2.15e+07 =  5% of the original kernel matrix.

torch.Size([14413, 2])
We keep 2.14e+06/8.48e+07 =  2% of the original kernel matrix.

torch.Size([9941, 2])
We keep 1.43e+06/3.25e+07 =  4% of the original kernel matrix.

torch.Size([15859, 2])
We keep 2.63e+06/1.04e+08 =  2% of the original kernel matrix.

torch.Size([91641, 2])
We keep 6.71e+07/4.14e+09 =  1% of the original kernel matrix.

torch.Size([47758, 2])
We keep 1.83e+07/1.18e+09 =  1% of the original kernel matrix.

torch.Size([27168, 2])
We keep 1.41e+07/4.08e+08 =  3% of the original kernel matrix.

torch.Size([28819, 2])
We keep 6.85e+06/3.70e+08 =  1% of the original kernel matrix.

torch.Size([64565, 2])
We keep 1.28e+08/2.74e+09 =  4% of the original kernel matrix.

torch.Size([40160, 2])
We keep 1.50e+07/9.58e+08 =  1% of the original kernel matrix.

torch.Size([11992, 2])
We keep 2.32e+06/5.26e+07 =  4% of the original kernel matrix.

torch.Size([17303, 2])
We keep 3.03e+06/1.33e+08 =  2% of the original kernel matrix.

torch.Size([778043, 2])
We keep 2.88e+09/2.70e+11 =  1% of the original kernel matrix.

torch.Size([150844, 2])
We keep 1.16e+08/9.50e+09 =  1% of the original kernel matrix.

torch.Size([14009, 2])
We keep 2.17e+06/6.08e+07 =  3% of the original kernel matrix.

torch.Size([19037, 2])
We keep 3.20e+06/1.43e+08 =  2% of the original kernel matrix.

torch.Size([39991, 2])
We keep 2.47e+07/1.05e+09 =  2% of the original kernel matrix.

torch.Size([32319, 2])
We keep 1.04e+07/5.93e+08 =  1% of the original kernel matrix.

torch.Size([103996, 2])
We keep 1.10e+08/6.21e+09 =  1% of the original kernel matrix.

torch.Size([51896, 2])
We keep 2.10e+07/1.44e+09 =  1% of the original kernel matrix.

torch.Size([32820, 2])
We keep 7.19e+06/4.30e+08 =  1% of the original kernel matrix.

torch.Size([29785, 2])
We keep 6.76e+06/3.79e+08 =  1% of the original kernel matrix.

torch.Size([36093, 2])
We keep 7.98e+07/8.09e+08 =  9% of the original kernel matrix.

torch.Size([30134, 2])
We keep 8.24e+06/5.20e+08 =  1% of the original kernel matrix.

torch.Size([296378, 2])
We keep 1.01e+09/5.52e+10 =  1% of the original kernel matrix.

torch.Size([89458, 2])
We keep 5.66e+07/4.30e+09 =  1% of the original kernel matrix.

torch.Size([62856, 2])
We keep 2.55e+07/1.71e+09 =  1% of the original kernel matrix.

torch.Size([40035, 2])
We keep 1.22e+07/7.57e+08 =  1% of the original kernel matrix.

torch.Size([37669, 2])
We keep 2.12e+07/7.62e+08 =  2% of the original kernel matrix.

torch.Size([31097, 2])
We keep 8.71e+06/5.05e+08 =  1% of the original kernel matrix.

torch.Size([8592, 2])
We keep 1.68e+06/2.04e+07 =  8% of the original kernel matrix.

torch.Size([14886, 2])
We keep 2.05e+06/8.25e+07 =  2% of the original kernel matrix.

torch.Size([34306, 2])
We keep 1.81e+07/5.92e+08 =  3% of the original kernel matrix.

torch.Size([29675, 2])
We keep 7.62e+06/4.45e+08 =  1% of the original kernel matrix.

torch.Size([12996, 2])
We keep 4.04e+06/6.56e+07 =  6% of the original kernel matrix.

torch.Size([18194, 2])
We keep 3.35e+06/1.48e+08 =  2% of the original kernel matrix.

torch.Size([140021, 2])
We keep 2.47e+08/9.65e+09 =  2% of the original kernel matrix.

torch.Size([61231, 2])
We keep 2.60e+07/1.80e+09 =  1% of the original kernel matrix.

torch.Size([680270, 2])
We keep 2.26e+09/2.50e+11 =  0% of the original kernel matrix.

torch.Size([143367, 2])
We keep 1.13e+08/9.14e+09 =  1% of the original kernel matrix.

torch.Size([94815, 2])
We keep 2.12e+08/5.33e+09 =  3% of the original kernel matrix.

torch.Size([49138, 2])
We keep 2.06e+07/1.34e+09 =  1% of the original kernel matrix.

torch.Size([157354, 2])
We keep 2.90e+08/1.76e+10 =  1% of the original kernel matrix.

torch.Size([63937, 2])
We keep 3.46e+07/2.43e+09 =  1% of the original kernel matrix.

torch.Size([37434, 2])
We keep 6.45e+07/1.19e+09 =  5% of the original kernel matrix.

torch.Size([30479, 2])
We keep 1.06e+07/6.31e+08 =  1% of the original kernel matrix.

torch.Size([27660, 2])
We keep 1.67e+07/4.58e+08 =  3% of the original kernel matrix.

torch.Size([26118, 2])
We keep 7.17e+06/3.91e+08 =  1% of the original kernel matrix.

torch.Size([30917, 2])
We keep 1.31e+07/5.55e+08 =  2% of the original kernel matrix.

torch.Size([29125, 2])
We keep 7.99e+06/4.31e+08 =  1% of the original kernel matrix.

torch.Size([10351, 2])
We keep 1.66e+06/4.20e+07 =  3% of the original kernel matrix.

torch.Size([16081, 2])
We keep 2.81e+06/1.19e+08 =  2% of the original kernel matrix.

torch.Size([8808, 2])
We keep 7.75e+05/1.81e+07 =  4% of the original kernel matrix.

torch.Size([15052, 2])
We keep 1.93e+06/7.78e+07 =  2% of the original kernel matrix.

torch.Size([262705, 2])
We keep 2.47e+08/2.76e+10 =  0% of the original kernel matrix.

torch.Size([86162, 2])
We keep 4.04e+07/3.04e+09 =  1% of the original kernel matrix.

torch.Size([29453, 2])
We keep 2.11e+07/7.55e+08 =  2% of the original kernel matrix.

torch.Size([26359, 2])
We keep 8.76e+06/5.03e+08 =  1% of the original kernel matrix.

torch.Size([121701, 2])
We keep 2.42e+08/1.13e+10 =  2% of the original kernel matrix.

torch.Size([54826, 2])
We keep 2.78e+07/1.94e+09 =  1% of the original kernel matrix.

torch.Size([122412, 2])
We keep 2.91e+08/1.16e+10 =  2% of the original kernel matrix.

torch.Size([55782, 2])
We keep 2.83e+07/1.97e+09 =  1% of the original kernel matrix.

torch.Size([34788, 2])
We keep 9.56e+06/5.07e+08 =  1% of the original kernel matrix.

torch.Size([30187, 2])
We keep 7.26e+06/4.12e+08 =  1% of the original kernel matrix.

torch.Size([6859, 2])
We keep 7.42e+05/1.26e+07 =  5% of the original kernel matrix.

torch.Size([13261, 2])
We keep 1.79e+06/6.48e+07 =  2% of the original kernel matrix.

torch.Size([62139, 2])
We keep 1.25e+08/2.14e+09 =  5% of the original kernel matrix.

torch.Size([39867, 2])
We keep 1.38e+07/8.45e+08 =  1% of the original kernel matrix.

torch.Size([83466, 2])
We keep 3.87e+07/2.53e+09 =  1% of the original kernel matrix.

torch.Size([45757, 2])
We keep 1.43e+07/9.20e+08 =  1% of the original kernel matrix.

torch.Size([4522, 2])
We keep 4.26e+05/6.50e+06 =  6% of the original kernel matrix.

torch.Size([10838, 2])
We keep 1.40e+06/4.66e+07 =  2% of the original kernel matrix.

torch.Size([398313, 2])
We keep 6.93e+08/7.16e+10 =  0% of the original kernel matrix.

torch.Size([104861, 2])
We keep 6.25e+07/4.90e+09 =  1% of the original kernel matrix.

torch.Size([23888, 2])
We keep 1.44e+07/3.48e+08 =  4% of the original kernel matrix.

torch.Size([25099, 2])
We keep 6.11e+06/3.41e+08 =  1% of the original kernel matrix.

torch.Size([37297, 2])
We keep 2.82e+07/7.35e+08 =  3% of the original kernel matrix.

torch.Size([30925, 2])
We keep 8.92e+06/4.96e+08 =  1% of the original kernel matrix.

torch.Size([5496, 2])
We keep 3.21e+05/6.20e+06 =  5% of the original kernel matrix.

torch.Size([12087, 2])
We keep 1.36e+06/4.55e+07 =  2% of the original kernel matrix.

torch.Size([8444, 2])
We keep 7.76e+05/1.77e+07 =  4% of the original kernel matrix.

torch.Size([14563, 2])
We keep 1.98e+06/7.69e+07 =  2% of the original kernel matrix.

torch.Size([2460, 2])
We keep 7.60e+04/9.86e+05 =  7% of the original kernel matrix.

torch.Size([8709, 2])
We keep 7.09e+05/1.82e+07 =  3% of the original kernel matrix.

torch.Size([1861, 2])
We keep 5.20e+04/6.64e+05 =  7% of the original kernel matrix.

torch.Size([7712, 2])
We keep 6.28e+05/1.49e+07 =  4% of the original kernel matrix.

torch.Size([15016, 2])
We keep 3.61e+06/7.45e+07 =  4% of the original kernel matrix.

torch.Size([19613, 2])
We keep 3.49e+06/1.58e+08 =  2% of the original kernel matrix.

torch.Size([24703, 2])
We keep 1.80e+07/4.27e+08 =  4% of the original kernel matrix.

torch.Size([24987, 2])
We keep 6.69e+06/3.78e+08 =  1% of the original kernel matrix.

torch.Size([3229444, 2])
We keep 3.41e+10/4.30e+12 =  0% of the original kernel matrix.

torch.Size([314515, 2])
We keep 4.27e+08/3.79e+10 =  1% of the original kernel matrix.

torch.Size([7866, 2])
We keep 1.19e+06/2.43e+07 =  4% of the original kernel matrix.

torch.Size([13954, 2])
We keep 2.26e+06/9.02e+07 =  2% of the original kernel matrix.

torch.Size([56252, 2])
We keep 1.84e+07/1.19e+09 =  1% of the original kernel matrix.

torch.Size([38055, 2])
We keep 1.03e+07/6.30e+08 =  1% of the original kernel matrix.

torch.Size([8688, 2])
We keep 1.28e+06/2.52e+07 =  5% of the original kernel matrix.

torch.Size([14689, 2])
We keep 2.30e+06/9.18e+07 =  2% of the original kernel matrix.

torch.Size([13358, 2])
We keep 1.78e+06/5.38e+07 =  3% of the original kernel matrix.

torch.Size([18283, 2])
We keep 3.06e+06/1.34e+08 =  2% of the original kernel matrix.

torch.Size([13158, 2])
We keep 2.46e+06/6.83e+07 =  3% of the original kernel matrix.

torch.Size([18289, 2])
We keep 3.42e+06/1.51e+08 =  2% of the original kernel matrix.

torch.Size([183408, 2])
We keep 4.24e+08/2.32e+10 =  1% of the original kernel matrix.

torch.Size([69905, 2])
We keep 3.85e+07/2.79e+09 =  1% of the original kernel matrix.

torch.Size([7547, 2])
We keep 1.19e+06/1.59e+07 =  7% of the original kernel matrix.

torch.Size([13952, 2])
We keep 1.90e+06/7.29e+07 =  2% of the original kernel matrix.

torch.Size([739960, 2])
We keep 1.68e+09/2.11e+11 =  0% of the original kernel matrix.

torch.Size([149085, 2])
We keep 1.02e+08/8.41e+09 =  1% of the original kernel matrix.

torch.Size([5965, 2])
We keep 3.60e+05/7.02e+06 =  5% of the original kernel matrix.

torch.Size([12616, 2])
We keep 1.43e+06/4.85e+07 =  2% of the original kernel matrix.

torch.Size([178622, 2])
We keep 2.36e+08/1.49e+10 =  1% of the original kernel matrix.

torch.Size([69671, 2])
We keep 3.15e+07/2.24e+09 =  1% of the original kernel matrix.

torch.Size([28610, 2])
We keep 5.49e+07/5.70e+08 =  9% of the original kernel matrix.

torch.Size([27117, 2])
We keep 8.02e+06/4.37e+08 =  1% of the original kernel matrix.

torch.Size([102331, 2])
We keep 1.11e+08/5.85e+09 =  1% of the original kernel matrix.

torch.Size([51119, 2])
We keep 2.08e+07/1.40e+09 =  1% of the original kernel matrix.

torch.Size([199147, 2])
We keep 3.28e+08/2.12e+10 =  1% of the original kernel matrix.

torch.Size([73289, 2])
We keep 3.66e+07/2.66e+09 =  1% of the original kernel matrix.

torch.Size([15854, 2])
We keep 2.55e+06/8.11e+07 =  3% of the original kernel matrix.

torch.Size([20235, 2])
We keep 3.59e+06/1.65e+08 =  2% of the original kernel matrix.

torch.Size([9749, 2])
We keep 1.19e+06/2.62e+07 =  4% of the original kernel matrix.

torch.Size([15697, 2])
We keep 2.35e+06/9.37e+07 =  2% of the original kernel matrix.

torch.Size([70997, 2])
We keep 2.80e+07/1.85e+09 =  1% of the original kernel matrix.

torch.Size([42460, 2])
We keep 1.26e+07/7.86e+08 =  1% of the original kernel matrix.

torch.Size([8625, 2])
We keep 7.52e+05/1.83e+07 =  4% of the original kernel matrix.

torch.Size([14952, 2])
We keep 1.99e+06/7.83e+07 =  2% of the original kernel matrix.

torch.Size([15103, 2])
We keep 3.39e+06/1.00e+08 =  3% of the original kernel matrix.

torch.Size([19481, 2])
We keep 3.84e+06/1.83e+08 =  2% of the original kernel matrix.

torch.Size([78269, 2])
We keep 5.12e+07/2.68e+09 =  1% of the original kernel matrix.

torch.Size([44063, 2])
We keep 1.46e+07/9.47e+08 =  1% of the original kernel matrix.

torch.Size([9275, 2])
We keep 1.85e+06/3.83e+07 =  4% of the original kernel matrix.

torch.Size([15378, 2])
We keep 2.52e+06/1.13e+08 =  2% of the original kernel matrix.

torch.Size([41544, 2])
We keep 2.73e+07/1.11e+09 =  2% of the original kernel matrix.

torch.Size([34343, 2])
We keep 1.07e+07/6.11e+08 =  1% of the original kernel matrix.

torch.Size([21067, 2])
We keep 1.01e+07/2.06e+08 =  4% of the original kernel matrix.

torch.Size([22876, 2])
We keep 5.21e+06/2.63e+08 =  1% of the original kernel matrix.

torch.Size([165701, 2])
We keep 3.02e+08/1.33e+10 =  2% of the original kernel matrix.

torch.Size([66856, 2])
We keep 3.02e+07/2.11e+09 =  1% of the original kernel matrix.

torch.Size([253506, 2])
We keep 6.91e+08/3.28e+10 =  2% of the original kernel matrix.

torch.Size([83337, 2])
We keep 4.42e+07/3.31e+09 =  1% of the original kernel matrix.

torch.Size([36402, 2])
We keep 1.55e+07/5.48e+08 =  2% of the original kernel matrix.

torch.Size([30808, 2])
We keep 7.70e+06/4.28e+08 =  1% of the original kernel matrix.

torch.Size([7635, 2])
We keep 7.71e+05/1.68e+07 =  4% of the original kernel matrix.

torch.Size([13891, 2])
We keep 1.97e+06/7.50e+07 =  2% of the original kernel matrix.

torch.Size([54148, 2])
We keep 7.23e+07/1.92e+09 =  3% of the original kernel matrix.

torch.Size([36408, 2])
We keep 1.25e+07/8.02e+08 =  1% of the original kernel matrix.

torch.Size([49396, 2])
We keep 3.12e+07/1.32e+09 =  2% of the original kernel matrix.

torch.Size([34674, 2])
We keep 1.10e+07/6.63e+08 =  1% of the original kernel matrix.

torch.Size([19885, 2])
We keep 8.02e+06/1.57e+08 =  5% of the original kernel matrix.

torch.Size([22636, 2])
We keep 4.49e+06/2.29e+08 =  1% of the original kernel matrix.

torch.Size([26043, 2])
We keep 1.69e+07/3.24e+08 =  5% of the original kernel matrix.

torch.Size([25497, 2])
We keep 6.22e+06/3.29e+08 =  1% of the original kernel matrix.

torch.Size([26907, 2])
We keep 2.09e+07/5.15e+08 =  4% of the original kernel matrix.

torch.Size([25557, 2])
We keep 7.52e+06/4.15e+08 =  1% of the original kernel matrix.

torch.Size([33014, 2])
We keep 1.97e+07/5.29e+08 =  3% of the original kernel matrix.

torch.Size([29259, 2])
We keep 7.56e+06/4.21e+08 =  1% of the original kernel matrix.

torch.Size([54469, 2])
We keep 1.03e+08/3.42e+09 =  3% of the original kernel matrix.

torch.Size([34143, 2])
We keep 1.66e+07/1.07e+09 =  1% of the original kernel matrix.

torch.Size([33860, 2])
We keep 4.85e+07/7.87e+08 =  6% of the original kernel matrix.

torch.Size([28824, 2])
We keep 9.05e+06/5.13e+08 =  1% of the original kernel matrix.

torch.Size([13178, 2])
We keep 1.59e+06/4.88e+07 =  3% of the original kernel matrix.

torch.Size([18226, 2])
We keep 2.96e+06/1.28e+08 =  2% of the original kernel matrix.

torch.Size([7203, 2])
We keep 6.48e+05/1.37e+07 =  4% of the original kernel matrix.

torch.Size([13395, 2])
We keep 1.81e+06/6.78e+07 =  2% of the original kernel matrix.

torch.Size([10093, 2])
We keep 1.08e+06/2.65e+07 =  4% of the original kernel matrix.

torch.Size([15991, 2])
We keep 2.30e+06/9.42e+07 =  2% of the original kernel matrix.

torch.Size([55241, 2])
We keep 5.86e+07/2.39e+09 =  2% of the original kernel matrix.

torch.Size([35924, 2])
We keep 1.42e+07/8.94e+08 =  1% of the original kernel matrix.

torch.Size([62752, 2])
We keep 8.93e+07/2.74e+09 =  3% of the original kernel matrix.

torch.Size([39306, 2])
We keep 1.53e+07/9.58e+08 =  1% of the original kernel matrix.

torch.Size([146414, 2])
We keep 2.37e+08/9.04e+09 =  2% of the original kernel matrix.

torch.Size([62376, 2])
We keep 2.49e+07/1.74e+09 =  1% of the original kernel matrix.

torch.Size([13030, 2])
We keep 1.92e+06/5.54e+07 =  3% of the original kernel matrix.

torch.Size([18122, 2])
We keep 3.08e+06/1.36e+08 =  2% of the original kernel matrix.

torch.Size([20741, 2])
We keep 3.89e+06/1.77e+08 =  2% of the original kernel matrix.

torch.Size([23270, 2])
We keep 4.76e+06/2.43e+08 =  1% of the original kernel matrix.

torch.Size([31930, 2])
We keep 1.39e+07/5.31e+08 =  2% of the original kernel matrix.

torch.Size([28957, 2])
We keep 7.64e+06/4.21e+08 =  1% of the original kernel matrix.

torch.Size([121066, 2])
We keep 3.81e+08/6.42e+09 =  5% of the original kernel matrix.

torch.Size([56388, 2])
We keep 2.03e+07/1.47e+09 =  1% of the original kernel matrix.

torch.Size([42989, 2])
We keep 3.15e+07/9.30e+08 =  3% of the original kernel matrix.

torch.Size([33032, 2])
We keep 9.58e+06/5.58e+08 =  1% of the original kernel matrix.

torch.Size([62593, 2])
We keep 2.58e+07/1.52e+09 =  1% of the original kernel matrix.

torch.Size([39919, 2])
We keep 1.17e+07/7.14e+08 =  1% of the original kernel matrix.

torch.Size([9685, 2])
We keep 1.96e+06/3.09e+07 =  6% of the original kernel matrix.

torch.Size([15627, 2])
We keep 2.48e+06/1.02e+08 =  2% of the original kernel matrix.

torch.Size([23346, 2])
We keep 8.74e+06/2.54e+08 =  3% of the original kernel matrix.

torch.Size([24798, 2])
We keep 5.65e+06/2.92e+08 =  1% of the original kernel matrix.

torch.Size([3129, 2])
We keep 1.56e+05/2.28e+06 =  6% of the original kernel matrix.

torch.Size([9563, 2])
We keep 9.50e+05/2.76e+07 =  3% of the original kernel matrix.

torch.Size([9762, 2])
We keep 1.06e+06/2.77e+07 =  3% of the original kernel matrix.

torch.Size([15600, 2])
We keep 2.39e+06/9.62e+07 =  2% of the original kernel matrix.

torch.Size([71494, 2])
We keep 2.76e+07/1.86e+09 =  1% of the original kernel matrix.

torch.Size([42568, 2])
We keep 1.24e+07/7.89e+08 =  1% of the original kernel matrix.

torch.Size([18655, 2])
We keep 6.65e+06/1.92e+08 =  3% of the original kernel matrix.

torch.Size([21152, 2])
We keep 4.81e+06/2.53e+08 =  1% of the original kernel matrix.

torch.Size([39885, 2])
We keep 1.01e+07/6.27e+08 =  1% of the original kernel matrix.

torch.Size([32662, 2])
We keep 7.95e+06/4.58e+08 =  1% of the original kernel matrix.

torch.Size([2875, 2])
We keep 9.01e+04/1.32e+06 =  6% of the original kernel matrix.

torch.Size([9516, 2])
We keep 7.91e+05/2.10e+07 =  3% of the original kernel matrix.

torch.Size([5707, 2])
We keep 4.19e+05/7.73e+06 =  5% of the original kernel matrix.

torch.Size([12285, 2])
We keep 1.47e+06/5.09e+07 =  2% of the original kernel matrix.

torch.Size([31200, 2])
We keep 1.82e+07/4.64e+08 =  3% of the original kernel matrix.

torch.Size([28459, 2])
We keep 6.87e+06/3.94e+08 =  1% of the original kernel matrix.

torch.Size([36822, 2])
We keep 1.33e+07/6.61e+08 =  2% of the original kernel matrix.

torch.Size([30671, 2])
We keep 8.25e+06/4.70e+08 =  1% of the original kernel matrix.

torch.Size([24138, 2])
We keep 5.01e+06/2.22e+08 =  2% of the original kernel matrix.

torch.Size([25619, 2])
We keep 5.34e+06/2.73e+08 =  1% of the original kernel matrix.

torch.Size([4030, 2])
We keep 1.93e+05/3.29e+06 =  5% of the original kernel matrix.

torch.Size([10678, 2])
We keep 1.10e+06/3.32e+07 =  3% of the original kernel matrix.

torch.Size([4782, 2])
We keep 3.15e+05/5.80e+06 =  5% of the original kernel matrix.

torch.Size([11333, 2])
We keep 1.34e+06/4.41e+07 =  3% of the original kernel matrix.

torch.Size([25867, 2])
We keep 1.01e+07/3.42e+08 =  2% of the original kernel matrix.

torch.Size([25864, 2])
We keep 6.33e+06/3.38e+08 =  1% of the original kernel matrix.

torch.Size([27279, 2])
We keep 6.63e+06/2.99e+08 =  2% of the original kernel matrix.

torch.Size([26889, 2])
We keep 5.91e+06/3.17e+08 =  1% of the original kernel matrix.

torch.Size([68387, 2])
We keep 7.74e+07/2.58e+09 =  2% of the original kernel matrix.

torch.Size([41635, 2])
We keep 1.51e+07/9.29e+08 =  1% of the original kernel matrix.

torch.Size([4019, 2])
We keep 2.56e+05/3.96e+06 =  6% of the original kernel matrix.

torch.Size([10574, 2])
We keep 1.15e+06/3.64e+07 =  3% of the original kernel matrix.

torch.Size([7627, 2])
We keep 8.22e+05/1.78e+07 =  4% of the original kernel matrix.

torch.Size([13763, 2])
We keep 2.03e+06/7.71e+07 =  2% of the original kernel matrix.

torch.Size([24532, 2])
We keep 1.16e+07/3.97e+08 =  2% of the original kernel matrix.

torch.Size([24656, 2])
We keep 6.71e+06/3.64e+08 =  1% of the original kernel matrix.

torch.Size([5704, 2])
We keep 5.62e+05/1.00e+07 =  5% of the original kernel matrix.

torch.Size([12059, 2])
We keep 1.62e+06/5.80e+07 =  2% of the original kernel matrix.

torch.Size([168731, 2])
We keep 2.27e+08/1.40e+10 =  1% of the original kernel matrix.

torch.Size([67654, 2])
We keep 3.00e+07/2.17e+09 =  1% of the original kernel matrix.

torch.Size([7891, 2])
We keep 1.17e+06/1.67e+07 =  7% of the original kernel matrix.

torch.Size([14233, 2])
We keep 1.92e+06/7.47e+07 =  2% of the original kernel matrix.

torch.Size([59139, 2])
We keep 5.41e+07/2.08e+09 =  2% of the original kernel matrix.

torch.Size([38014, 2])
We keep 1.32e+07/8.33e+08 =  1% of the original kernel matrix.

torch.Size([122572, 2])
We keep 1.02e+08/6.08e+09 =  1% of the original kernel matrix.

torch.Size([56969, 2])
We keep 2.11e+07/1.43e+09 =  1% of the original kernel matrix.

torch.Size([71624, 2])
We keep 1.31e+08/2.79e+09 =  4% of the original kernel matrix.

torch.Size([42309, 2])
We keep 1.49e+07/9.67e+08 =  1% of the original kernel matrix.

torch.Size([148537, 2])
We keep 1.78e+08/1.04e+10 =  1% of the original kernel matrix.

torch.Size([62768, 2])
We keep 2.68e+07/1.87e+09 =  1% of the original kernel matrix.

torch.Size([120735, 2])
We keep 8.71e+07/5.69e+09 =  1% of the original kernel matrix.

torch.Size([56251, 2])
We keep 2.02e+07/1.38e+09 =  1% of the original kernel matrix.

torch.Size([2978, 2])
We keep 9.06e+04/1.36e+06 =  6% of the original kernel matrix.

torch.Size([9517, 2])
We keep 7.93e+05/2.14e+07 =  3% of the original kernel matrix.

torch.Size([43287, 2])
We keep 1.41e+07/7.32e+08 =  1% of the original kernel matrix.

torch.Size([33546, 2])
We keep 8.65e+06/4.95e+08 =  1% of the original kernel matrix.

torch.Size([15319, 2])
We keep 1.85e+06/6.73e+07 =  2% of the original kernel matrix.

torch.Size([19793, 2])
We keep 3.26e+06/1.50e+08 =  2% of the original kernel matrix.

torch.Size([51081, 2])
We keep 4.21e+07/1.15e+09 =  3% of the original kernel matrix.

torch.Size([36448, 2])
We keep 1.01e+07/6.21e+08 =  1% of the original kernel matrix.

torch.Size([97737, 2])
We keep 9.00e+07/4.66e+09 =  1% of the original kernel matrix.

torch.Size([50132, 2])
We keep 1.88e+07/1.25e+09 =  1% of the original kernel matrix.

torch.Size([18060, 2])
We keep 9.40e+06/2.17e+08 =  4% of the original kernel matrix.

torch.Size([21339, 2])
We keep 5.32e+06/2.69e+08 =  1% of the original kernel matrix.

torch.Size([68689, 2])
We keep 4.67e+07/2.13e+09 =  2% of the original kernel matrix.

torch.Size([41738, 2])
We keep 1.36e+07/8.43e+08 =  1% of the original kernel matrix.

torch.Size([10744, 2])
We keep 1.11e+06/3.28e+07 =  3% of the original kernel matrix.

torch.Size([16568, 2])
We keep 2.52e+06/1.05e+08 =  2% of the original kernel matrix.

torch.Size([17167, 2])
We keep 1.48e+07/3.28e+08 =  4% of the original kernel matrix.

torch.Size([20241, 2])
We keep 6.07e+06/3.31e+08 =  1% of the original kernel matrix.

torch.Size([7032, 2])
We keep 4.78e+06/1.62e+07 = 29% of the original kernel matrix.

torch.Size([13493, 2])
We keep 1.75e+06/7.37e+07 =  2% of the original kernel matrix.

torch.Size([8414, 2])
We keep 1.30e+06/2.45e+07 =  5% of the original kernel matrix.

torch.Size([14394, 2])
We keep 2.26e+06/9.06e+07 =  2% of the original kernel matrix.

torch.Size([191834, 2])
We keep 4.65e+08/1.85e+10 =  2% of the original kernel matrix.

torch.Size([72831, 2])
We keep 3.43e+07/2.49e+09 =  1% of the original kernel matrix.

torch.Size([369796, 2])
We keep 2.21e+09/8.65e+10 =  2% of the original kernel matrix.

torch.Size([100559, 2])
We keep 7.08e+07/5.38e+09 =  1% of the original kernel matrix.

torch.Size([15264, 2])
We keep 5.05e+06/1.34e+08 =  3% of the original kernel matrix.

torch.Size([19443, 2])
We keep 4.44e+06/2.12e+08 =  2% of the original kernel matrix.

torch.Size([18518, 2])
We keep 3.51e+06/1.42e+08 =  2% of the original kernel matrix.

torch.Size([21970, 2])
We keep 4.32e+06/2.18e+08 =  1% of the original kernel matrix.

torch.Size([21252, 2])
We keep 4.85e+06/1.79e+08 =  2% of the original kernel matrix.

torch.Size([23873, 2])
We keep 4.98e+06/2.45e+08 =  2% of the original kernel matrix.

torch.Size([53814, 2])
We keep 5.56e+07/1.40e+09 =  3% of the original kernel matrix.

torch.Size([37090, 2])
We keep 1.07e+07/6.85e+08 =  1% of the original kernel matrix.

torch.Size([29302, 2])
We keep 8.37e+06/3.59e+08 =  2% of the original kernel matrix.

torch.Size([28069, 2])
We keep 6.51e+06/3.47e+08 =  1% of the original kernel matrix.

torch.Size([8550, 2])
We keep 1.41e+06/2.15e+07 =  6% of the original kernel matrix.

torch.Size([14856, 2])
We keep 2.06e+06/8.48e+07 =  2% of the original kernel matrix.

torch.Size([39067, 2])
We keep 2.06e+07/6.36e+08 =  3% of the original kernel matrix.

torch.Size([31933, 2])
We keep 7.85e+06/4.61e+08 =  1% of the original kernel matrix.

torch.Size([143448, 2])
We keep 1.24e+08/8.39e+09 =  1% of the original kernel matrix.

torch.Size([61865, 2])
We keep 2.44e+07/1.68e+09 =  1% of the original kernel matrix.

torch.Size([9886, 2])
We keep 1.35e+06/3.06e+07 =  4% of the original kernel matrix.

torch.Size([15782, 2])
We keep 2.42e+06/1.01e+08 =  2% of the original kernel matrix.

torch.Size([63805, 2])
We keep 5.63e+07/1.94e+09 =  2% of the original kernel matrix.

torch.Size([39935, 2])
We keep 1.31e+07/8.05e+08 =  1% of the original kernel matrix.

torch.Size([6900, 2])
We keep 1.14e+06/1.21e+07 =  9% of the original kernel matrix.

torch.Size([13430, 2])
We keep 1.61e+06/6.35e+07 =  2% of the original kernel matrix.

time for making ranges is 3.166195869445801
Sorting X and nu_X
time for sorting X is 0.07394266128540039
Sorting Z and nu_Z
time for sorting Z is 0.00027108192443847656
Starting Optim
sum tnu_Z before tensor(22326498., device='cuda:0')
c= tensor(757.4586, device='cuda:0')
c= tensor(48163.8633, device='cuda:0')
c= tensor(51936.9062, device='cuda:0')
c= tensor(118880.9453, device='cuda:0')
c= tensor(138956.7344, device='cuda:0')
c= tensor(307147.1250, device='cuda:0')
c= tensor(837062.5000, device='cuda:0')
c= tensor(997769.8750, device='cuda:0')
c= tensor(1025872.9375, device='cuda:0')
c= tensor(3764424., device='cuda:0')
c= tensor(3777815.5000, device='cuda:0')
c= tensor(6124959.5000, device='cuda:0')
c= tensor(6143182.5000, device='cuda:0')
c= tensor(30124346., device='cuda:0')
c= tensor(30236636., device='cuda:0')
c= tensor(30519084., device='cuda:0')
c= tensor(31561838., device='cuda:0')
c= tensor(31895860., device='cuda:0')
c= tensor(37028620., device='cuda:0')
c= tensor(40151712., device='cuda:0')
c= tensor(40238588., device='cuda:0')
c= tensor(47485000., device='cuda:0')
c= tensor(47503056., device='cuda:0')
c= tensor(48118060., device='cuda:0')
c= tensor(48124896., device='cuda:0')
c= tensor(48877360., device='cuda:0')
c= tensor(49671860., device='cuda:0')
c= tensor(49678236., device='cuda:0')
c= tensor(50845884., device='cuda:0')
c= tensor(2.1737e+08, device='cuda:0')
c= tensor(2.1740e+08, device='cuda:0')
c= tensor(3.3903e+08, device='cuda:0')
c= tensor(3.3914e+08, device='cuda:0')
c= tensor(3.3917e+08, device='cuda:0')
c= tensor(3.3920e+08, device='cuda:0')
c= tensor(3.4526e+08, device='cuda:0')
c= tensor(3.5068e+08, device='cuda:0')
c= tensor(3.5068e+08, device='cuda:0')
c= tensor(3.5070e+08, device='cuda:0')
c= tensor(3.5070e+08, device='cuda:0')
c= tensor(3.5070e+08, device='cuda:0')
c= tensor(3.5070e+08, device='cuda:0')
c= tensor(3.5071e+08, device='cuda:0')
c= tensor(3.5071e+08, device='cuda:0')
c= tensor(3.5071e+08, device='cuda:0')
c= tensor(3.5071e+08, device='cuda:0')
c= tensor(3.5071e+08, device='cuda:0')
c= tensor(3.5072e+08, device='cuda:0')
c= tensor(3.5072e+08, device='cuda:0')
c= tensor(3.5074e+08, device='cuda:0')
c= tensor(3.5077e+08, device='cuda:0')
c= tensor(3.5077e+08, device='cuda:0')
c= tensor(3.5078e+08, device='cuda:0')
c= tensor(3.5078e+08, device='cuda:0')
c= tensor(3.5079e+08, device='cuda:0')
c= tensor(3.5080e+08, device='cuda:0')
c= tensor(3.5080e+08, device='cuda:0')
c= tensor(3.5080e+08, device='cuda:0')
c= tensor(3.5080e+08, device='cuda:0')
c= tensor(3.5081e+08, device='cuda:0')
c= tensor(3.5082e+08, device='cuda:0')
c= tensor(3.5082e+08, device='cuda:0')
c= tensor(3.5084e+08, device='cuda:0')
c= tensor(3.5085e+08, device='cuda:0')
c= tensor(3.5085e+08, device='cuda:0')
c= tensor(3.5085e+08, device='cuda:0')
c= tensor(3.5086e+08, device='cuda:0')
c= tensor(3.5086e+08, device='cuda:0')
c= tensor(3.5086e+08, device='cuda:0')
c= tensor(3.5087e+08, device='cuda:0')
c= tensor(3.5087e+08, device='cuda:0')
c= tensor(3.5087e+08, device='cuda:0')
c= tensor(3.5088e+08, device='cuda:0')
c= tensor(3.5088e+08, device='cuda:0')
c= tensor(3.5088e+08, device='cuda:0')
c= tensor(3.5089e+08, device='cuda:0')
c= tensor(3.5089e+08, device='cuda:0')
c= tensor(3.5089e+08, device='cuda:0')
c= tensor(3.5089e+08, device='cuda:0')
c= tensor(3.5092e+08, device='cuda:0')
c= tensor(3.5092e+08, device='cuda:0')
c= tensor(3.5092e+08, device='cuda:0')
c= tensor(3.5094e+08, device='cuda:0')
c= tensor(3.5094e+08, device='cuda:0')
c= tensor(3.5095e+08, device='cuda:0')
c= tensor(3.5095e+08, device='cuda:0')
c= tensor(3.5095e+08, device='cuda:0')
c= tensor(3.5095e+08, device='cuda:0')
c= tensor(3.5096e+08, device='cuda:0')
c= tensor(3.5096e+08, device='cuda:0')
c= tensor(3.5096e+08, device='cuda:0')
c= tensor(3.5096e+08, device='cuda:0')
c= tensor(3.5096e+08, device='cuda:0')
c= tensor(3.5097e+08, device='cuda:0')
c= tensor(3.5097e+08, device='cuda:0')
c= tensor(3.5098e+08, device='cuda:0')
c= tensor(3.5099e+08, device='cuda:0')
c= tensor(3.5102e+08, device='cuda:0')
c= tensor(3.5103e+08, device='cuda:0')
c= tensor(3.5103e+08, device='cuda:0')
c= tensor(3.5106e+08, device='cuda:0')
c= tensor(3.5106e+08, device='cuda:0')
c= tensor(3.5107e+08, device='cuda:0')
c= tensor(3.5107e+08, device='cuda:0')
c= tensor(3.5107e+08, device='cuda:0')
c= tensor(3.5108e+08, device='cuda:0')
c= tensor(3.5108e+08, device='cuda:0')
c= tensor(3.5108e+08, device='cuda:0')
c= tensor(3.5108e+08, device='cuda:0')
c= tensor(3.5109e+08, device='cuda:0')
c= tensor(3.5109e+08, device='cuda:0')
c= tensor(3.5109e+08, device='cuda:0')
c= tensor(3.5109e+08, device='cuda:0')
c= tensor(3.5109e+08, device='cuda:0')
c= tensor(3.5110e+08, device='cuda:0')
c= tensor(3.5110e+08, device='cuda:0')
c= tensor(3.5110e+08, device='cuda:0')
c= tensor(3.5110e+08, device='cuda:0')
c= tensor(3.5112e+08, device='cuda:0')
c= tensor(3.5112e+08, device='cuda:0')
c= tensor(3.5113e+08, device='cuda:0')
c= tensor(3.5113e+08, device='cuda:0')
c= tensor(3.5114e+08, device='cuda:0')
c= tensor(3.5114e+08, device='cuda:0')
c= tensor(3.5114e+08, device='cuda:0')
c= tensor(3.5114e+08, device='cuda:0')
c= tensor(3.5114e+08, device='cuda:0')
c= tensor(3.5115e+08, device='cuda:0')
c= tensor(3.5121e+08, device='cuda:0')
c= tensor(3.5122e+08, device='cuda:0')
c= tensor(3.5123e+08, device='cuda:0')
c= tensor(3.5123e+08, device='cuda:0')
c= tensor(3.5123e+08, device='cuda:0')
c= tensor(3.5123e+08, device='cuda:0')
c= tensor(3.5124e+08, device='cuda:0')
c= tensor(3.5124e+08, device='cuda:0')
c= tensor(3.5124e+08, device='cuda:0')
c= tensor(3.5124e+08, device='cuda:0')
c= tensor(3.5124e+08, device='cuda:0')
c= tensor(3.5124e+08, device='cuda:0')
c= tensor(3.5125e+08, device='cuda:0')
c= tensor(3.5125e+08, device='cuda:0')
c= tensor(3.5128e+08, device='cuda:0')
c= tensor(3.5130e+08, device='cuda:0')
c= tensor(3.5131e+08, device='cuda:0')
c= tensor(3.5131e+08, device='cuda:0')
c= tensor(3.5131e+08, device='cuda:0')
c= tensor(3.5132e+08, device='cuda:0')
c= tensor(3.5132e+08, device='cuda:0')
c= tensor(3.5132e+08, device='cuda:0')
c= tensor(3.5132e+08, device='cuda:0')
c= tensor(3.5133e+08, device='cuda:0')
c= tensor(3.5134e+08, device='cuda:0')
c= tensor(3.5137e+08, device='cuda:0')
c= tensor(3.5137e+08, device='cuda:0')
c= tensor(3.5148e+08, device='cuda:0')
c= tensor(3.5148e+08, device='cuda:0')
c= tensor(3.5148e+08, device='cuda:0')
c= tensor(3.5149e+08, device='cuda:0')
c= tensor(3.5149e+08, device='cuda:0')
c= tensor(3.5154e+08, device='cuda:0')
c= tensor(3.5154e+08, device='cuda:0')
c= tensor(3.5155e+08, device='cuda:0')
c= tensor(3.5155e+08, device='cuda:0')
c= tensor(3.5155e+08, device='cuda:0')
c= tensor(3.5156e+08, device='cuda:0')
c= tensor(3.5156e+08, device='cuda:0')
c= tensor(3.5156e+08, device='cuda:0')
c= tensor(3.5156e+08, device='cuda:0')
c= tensor(3.5156e+08, device='cuda:0')
c= tensor(3.5157e+08, device='cuda:0')
c= tensor(3.5157e+08, device='cuda:0')
c= tensor(3.5157e+08, device='cuda:0')
c= tensor(3.5158e+08, device='cuda:0')
c= tensor(3.5158e+08, device='cuda:0')
c= tensor(3.5158e+08, device='cuda:0')
c= tensor(3.5159e+08, device='cuda:0')
c= tensor(3.5159e+08, device='cuda:0')
c= tensor(3.5160e+08, device='cuda:0')
c= tensor(3.5160e+08, device='cuda:0')
c= tensor(3.5160e+08, device='cuda:0')
c= tensor(3.5161e+08, device='cuda:0')
c= tensor(3.5161e+08, device='cuda:0')
c= tensor(3.5161e+08, device='cuda:0')
c= tensor(3.5162e+08, device='cuda:0')
c= tensor(3.5163e+08, device='cuda:0')
c= tensor(3.5163e+08, device='cuda:0')
c= tensor(3.5164e+08, device='cuda:0')
c= tensor(3.5164e+08, device='cuda:0')
c= tensor(3.5169e+08, device='cuda:0')
c= tensor(3.5169e+08, device='cuda:0')
c= tensor(3.5169e+08, device='cuda:0')
c= tensor(3.5169e+08, device='cuda:0')
c= tensor(3.5169e+08, device='cuda:0')
c= tensor(3.5170e+08, device='cuda:0')
c= tensor(3.5170e+08, device='cuda:0')
c= tensor(3.5170e+08, device='cuda:0')
c= tensor(3.5170e+08, device='cuda:0')
c= tensor(3.5171e+08, device='cuda:0')
c= tensor(3.5171e+08, device='cuda:0')
c= tensor(3.5171e+08, device='cuda:0')
c= tensor(3.5171e+08, device='cuda:0')
c= tensor(3.5172e+08, device='cuda:0')
c= tensor(3.5173e+08, device='cuda:0')
c= tensor(3.5174e+08, device='cuda:0')
c= tensor(3.5174e+08, device='cuda:0')
c= tensor(3.5175e+08, device='cuda:0')
c= tensor(3.5176e+08, device='cuda:0')
c= tensor(3.5176e+08, device='cuda:0')
c= tensor(3.5179e+08, device='cuda:0')
c= tensor(3.5180e+08, device='cuda:0')
c= tensor(3.5181e+08, device='cuda:0')
c= tensor(3.5181e+08, device='cuda:0')
c= tensor(3.5181e+08, device='cuda:0')
c= tensor(3.5181e+08, device='cuda:0')
c= tensor(3.5181e+08, device='cuda:0')
c= tensor(3.5181e+08, device='cuda:0')
c= tensor(3.5181e+08, device='cuda:0')
c= tensor(3.5182e+08, device='cuda:0')
c= tensor(3.5182e+08, device='cuda:0')
c= tensor(3.5183e+08, device='cuda:0')
c= tensor(3.5183e+08, device='cuda:0')
c= tensor(3.5183e+08, device='cuda:0')
c= tensor(3.5183e+08, device='cuda:0')
c= tensor(3.5184e+08, device='cuda:0')
c= tensor(3.5185e+08, device='cuda:0')
c= tensor(3.5185e+08, device='cuda:0')
c= tensor(3.5185e+08, device='cuda:0')
c= tensor(3.5185e+08, device='cuda:0')
c= tensor(3.5186e+08, device='cuda:0')
c= tensor(3.5186e+08, device='cuda:0')
c= tensor(3.5186e+08, device='cuda:0')
c= tensor(3.5186e+08, device='cuda:0')
c= tensor(3.5187e+08, device='cuda:0')
c= tensor(3.5187e+08, device='cuda:0')
c= tensor(3.5187e+08, device='cuda:0')
c= tensor(3.5187e+08, device='cuda:0')
c= tensor(3.5189e+08, device='cuda:0')
c= tensor(3.5189e+08, device='cuda:0')
c= tensor(3.5194e+08, device='cuda:0')
c= tensor(3.5326e+08, device='cuda:0')
c= tensor(3.5326e+08, device='cuda:0')
c= tensor(3.5327e+08, device='cuda:0')
c= tensor(3.5327e+08, device='cuda:0')
c= tensor(3.5328e+08, device='cuda:0')
c= tensor(3.6217e+08, device='cuda:0')
c= tensor(3.9706e+08, device='cuda:0')
c= tensor(3.9706e+08, device='cuda:0')
c= tensor(3.9938e+08, device='cuda:0')
c= tensor(3.9965e+08, device='cuda:0')
c= tensor(3.9980e+08, device='cuda:0')
c= tensor(4.0568e+08, device='cuda:0')
c= tensor(4.0568e+08, device='cuda:0')
c= tensor(4.0568e+08, device='cuda:0')
c= tensor(4.0896e+08, device='cuda:0')
c= tensor(4.3384e+08, device='cuda:0')
c= tensor(4.3384e+08, device='cuda:0')
c= tensor(4.3400e+08, device='cuda:0')
c= tensor(4.3407e+08, device='cuda:0')
c= tensor(4.3622e+08, device='cuda:0')
c= tensor(4.3818e+08, device='cuda:0')
c= tensor(4.3866e+08, device='cuda:0')
c= tensor(4.3889e+08, device='cuda:0')
c= tensor(4.3901e+08, device='cuda:0')
c= tensor(4.3902e+08, device='cuda:0')
c= tensor(4.6424e+08, device='cuda:0')
c= tensor(4.6425e+08, device='cuda:0')
c= tensor(4.6425e+08, device='cuda:0')
c= tensor(4.6442e+08, device='cuda:0')
c= tensor(4.6564e+08, device='cuda:0')
c= tensor(4.7213e+08, device='cuda:0')
c= tensor(4.7298e+08, device='cuda:0')
c= tensor(4.7298e+08, device='cuda:0')
c= tensor(4.7318e+08, device='cuda:0')
c= tensor(4.7321e+08, device='cuda:0')
c= tensor(4.7370e+08, device='cuda:0')
c= tensor(4.7415e+08, device='cuda:0')
c= tensor(4.7416e+08, device='cuda:0')
c= tensor(4.7438e+08, device='cuda:0')
c= tensor(4.7438e+08, device='cuda:0')
c= tensor(4.7438e+08, device='cuda:0')
c= tensor(4.7581e+08, device='cuda:0')
c= tensor(4.7681e+08, device='cuda:0')
c= tensor(4.7750e+08, device='cuda:0')
c= tensor(4.7752e+08, device='cuda:0')
c= tensor(4.9528e+08, device='cuda:0')
c= tensor(4.9530e+08, device='cuda:0')
c= tensor(4.9544e+08, device='cuda:0')
c= tensor(4.9671e+08, device='cuda:0')
c= tensor(4.9672e+08, device='cuda:0')
c= tensor(4.9981e+08, device='cuda:0')
c= tensor(5.1513e+08, device='cuda:0')
c= tensor(5.3441e+08, device='cuda:0')
c= tensor(5.3446e+08, device='cuda:0')
c= tensor(5.3451e+08, device='cuda:0')
c= tensor(5.3452e+08, device='cuda:0')
c= tensor(5.3453e+08, device='cuda:0')
c= tensor(5.3553e+08, device='cuda:0')
c= tensor(5.3554e+08, device='cuda:0')
c= tensor(5.3581e+08, device='cuda:0')
c= tensor(5.3995e+08, device='cuda:0')
c= tensor(5.4001e+08, device='cuda:0')
c= tensor(5.4009e+08, device='cuda:0')
c= tensor(5.4010e+08, device='cuda:0')
c= tensor(5.4335e+08, device='cuda:0')
c= tensor(5.4343e+08, device='cuda:0')
c= tensor(5.4358e+08, device='cuda:0')
c= tensor(5.4362e+08, device='cuda:0')
c= tensor(5.7628e+08, device='cuda:0')
c= tensor(5.7636e+08, device='cuda:0')
c= tensor(5.8159e+08, device='cuda:0')
c= tensor(5.8161e+08, device='cuda:0')
c= tensor(5.8439e+08, device='cuda:0')
c= tensor(5.8450e+08, device='cuda:0')
c= tensor(6.0515e+08, device='cuda:0')
c= tensor(6.0558e+08, device='cuda:0')
c= tensor(6.0559e+08, device='cuda:0')
c= tensor(6.0802e+08, device='cuda:0')
c= tensor(6.1019e+08, device='cuda:0')
c= tensor(6.1024e+08, device='cuda:0')
c= tensor(6.1216e+08, device='cuda:0')
c= tensor(6.1570e+08, device='cuda:0')
c= tensor(6.3379e+08, device='cuda:0')
c= tensor(6.3522e+08, device='cuda:0')
c= tensor(6.3522e+08, device='cuda:0')
c= tensor(6.3523e+08, device='cuda:0')
c= tensor(6.3537e+08, device='cuda:0')
c= tensor(6.3544e+08, device='cuda:0')
c= tensor(6.3557e+08, device='cuda:0')
c= tensor(6.3557e+08, device='cuda:0')
c= tensor(6.3627e+08, device='cuda:0')
c= tensor(6.3827e+08, device='cuda:0')
c= tensor(6.3828e+08, device='cuda:0')
c= tensor(6.3829e+08, device='cuda:0')
c= tensor(6.3841e+08, device='cuda:0')
c= tensor(6.3844e+08, device='cuda:0')
c= tensor(6.3845e+08, device='cuda:0')
c= tensor(6.3848e+08, device='cuda:0')
c= tensor(6.3848e+08, device='cuda:0')
c= tensor(6.3861e+08, device='cuda:0')
c= tensor(6.3880e+08, device='cuda:0')
c= tensor(6.3885e+08, device='cuda:0')
c= tensor(6.3942e+08, device='cuda:0')
c= tensor(6.3944e+08, device='cuda:0')
c= tensor(6.6375e+08, device='cuda:0')
c= tensor(6.6376e+08, device='cuda:0')
c= tensor(6.6521e+08, device='cuda:0')
c= tensor(6.6521e+08, device='cuda:0')
c= tensor(6.6521e+08, device='cuda:0')
c= tensor(6.6522e+08, device='cuda:0')
c= tensor(6.6529e+08, device='cuda:0')
c= tensor(6.6529e+08, device='cuda:0')
c= tensor(6.7079e+08, device='cuda:0')
c= tensor(6.7080e+08, device='cuda:0')
c= tensor(6.7080e+08, device='cuda:0')
c= tensor(6.7951e+08, device='cuda:0')
c= tensor(6.8043e+08, device='cuda:0')
c= tensor(6.8109e+08, device='cuda:0')
c= tensor(6.8690e+08, device='cuda:0')
c= tensor(6.9857e+08, device='cuda:0')
c= tensor(6.9858e+08, device='cuda:0')
c= tensor(6.9859e+08, device='cuda:0')
c= tensor(6.9869e+08, device='cuda:0')
c= tensor(6.9869e+08, device='cuda:0')
c= tensor(6.9870e+08, device='cuda:0')
c= tensor(6.9873e+08, device='cuda:0')
c= tensor(6.9874e+08, device='cuda:0')
c= tensor(6.9874e+08, device='cuda:0')
c= tensor(6.9875e+08, device='cuda:0')
c= tensor(6.9875e+08, device='cuda:0')
c= tensor(7.0848e+08, device='cuda:0')
c= tensor(7.0856e+08, device='cuda:0')
c= tensor(7.0998e+08, device='cuda:0')
c= tensor(7.0999e+08, device='cuda:0')
c= tensor(7.1004e+08, device='cuda:0')
c= tensor(7.1007e+08, device='cuda:0')
c= tensor(7.9659e+08, device='cuda:0')
c= tensor(8.2415e+08, device='cuda:0')
c= tensor(8.2419e+08, device='cuda:0')
c= tensor(8.2646e+08, device='cuda:0')
c= tensor(8.2646e+08, device='cuda:0')
c= tensor(8.2666e+08, device='cuda:0')
c= tensor(8.2738e+08, device='cuda:0')
c= tensor(8.2748e+08, device='cuda:0')
c= tensor(8.2785e+08, device='cuda:0')
c= tensor(8.2922e+08, device='cuda:0')
c= tensor(9.2637e+08, device='cuda:0')
c= tensor(9.2665e+08, device='cuda:0')
c= tensor(9.2666e+08, device='cuda:0')
c= tensor(9.2679e+08, device='cuda:0')
c= tensor(9.2681e+08, device='cuda:0')
c= tensor(9.2681e+08, device='cuda:0')
c= tensor(9.3020e+08, device='cuda:0')
c= tensor(9.3029e+08, device='cuda:0')
c= tensor(9.3029e+08, device='cuda:0')
c= tensor(9.3080e+08, device='cuda:0')
c= tensor(9.3083e+08, device='cuda:0')
c= tensor(9.3083e+08, device='cuda:0')
c= tensor(9.3218e+08, device='cuda:0')
c= tensor(9.3436e+08, device='cuda:0')
c= tensor(9.3874e+08, device='cuda:0')
c= tensor(9.4278e+08, device='cuda:0')
c= tensor(9.5238e+08, device='cuda:0')
c= tensor(9.5248e+08, device='cuda:0')
c= tensor(9.5258e+08, device='cuda:0')
c= tensor(9.5425e+08, device='cuda:0')
c= tensor(9.5563e+08, device='cuda:0')
c= tensor(9.5564e+08, device='cuda:0')
c= tensor(9.5792e+08, device='cuda:0')
c= tensor(9.6409e+08, device='cuda:0')
c= tensor(9.6859e+08, device='cuda:0')
c= tensor(9.7083e+08, device='cuda:0')
c= tensor(9.7335e+08, device='cuda:0')
c= tensor(9.7339e+08, device='cuda:0')
c= tensor(9.7339e+08, device='cuda:0')
c= tensor(9.7340e+08, device='cuda:0')
c= tensor(9.7456e+08, device='cuda:0')
c= tensor(9.7588e+08, device='cuda:0')
c= tensor(1.0125e+09, device='cuda:0')
c= tensor(1.0164e+09, device='cuda:0')
c= tensor(1.0172e+09, device='cuda:0')
c= tensor(1.0173e+09, device='cuda:0')
c= tensor(1.0200e+09, device='cuda:0')
c= tensor(1.0200e+09, device='cuda:0')
c= tensor(1.0200e+09, device='cuda:0')
c= tensor(1.0255e+09, device='cuda:0')
c= tensor(1.0256e+09, device='cuda:0')
c= tensor(1.0256e+09, device='cuda:0')
c= tensor(1.0256e+09, device='cuda:0')
c= tensor(1.0499e+09, device='cuda:0')
c= tensor(1.0500e+09, device='cuda:0')
c= tensor(1.0535e+09, device='cuda:0')
c= tensor(1.0536e+09, device='cuda:0')
c= tensor(1.0536e+09, device='cuda:0')
c= tensor(1.0536e+09, device='cuda:0')
c= tensor(1.0536e+09, device='cuda:0')
c= tensor(1.0544e+09, device='cuda:0')
c= tensor(1.0550e+09, device='cuda:0')
c= tensor(1.0550e+09, device='cuda:0')
c= tensor(1.0574e+09, device='cuda:0')
c= tensor(1.0574e+09, device='cuda:0')
c= tensor(1.0583e+09, device='cuda:0')
c= tensor(1.0584e+09, device='cuda:0')
c= tensor(1.0589e+09, device='cuda:0')
c= tensor(1.0589e+09, device='cuda:0')
c= tensor(1.0590e+09, device='cuda:0')
c= tensor(1.0590e+09, device='cuda:0')
c= tensor(1.0591e+09, device='cuda:0')
c= tensor(1.0603e+09, device='cuda:0')
c= tensor(1.0788e+09, device='cuda:0')
c= tensor(1.0788e+09, device='cuda:0')
c= tensor(1.0789e+09, device='cuda:0')
c= tensor(1.0831e+09, device='cuda:0')
c= tensor(1.0832e+09, device='cuda:0')
c= tensor(1.1142e+09, device='cuda:0')
c= tensor(1.1142e+09, device='cuda:0')
c= tensor(1.1156e+09, device='cuda:0')
c= tensor(1.1189e+09, device='cuda:0')
c= tensor(1.1189e+09, device='cuda:0')
c= tensor(1.1271e+09, device='cuda:0')
c= tensor(1.1272e+09, device='cuda:0')
c= tensor(1.2260e+09, device='cuda:0')
c= tensor(1.2260e+09, device='cuda:0')
c= tensor(1.2261e+09, device='cuda:0')
c= tensor(1.2261e+09, device='cuda:0')
c= tensor(1.2261e+09, device='cuda:0')
c= tensor(1.2262e+09, device='cuda:0')
c= tensor(1.2267e+09, device='cuda:0')
c= tensor(1.2270e+09, device='cuda:0')
c= tensor(1.2287e+09, device='cuda:0')
c= tensor(1.2287e+09, device='cuda:0')
c= tensor(1.2287e+09, device='cuda:0')
c= tensor(1.2287e+09, device='cuda:0')
c= tensor(1.2489e+09, device='cuda:0')
c= tensor(1.2528e+09, device='cuda:0')
c= tensor(1.2656e+09, device='cuda:0')
c= tensor(1.2657e+09, device='cuda:0')
c= tensor(1.2658e+09, device='cuda:0')
c= tensor(1.2658e+09, device='cuda:0')
c= tensor(1.2658e+09, device='cuda:0')
c= tensor(1.2709e+09, device='cuda:0')
c= tensor(1.2710e+09, device='cuda:0')
c= tensor(1.2710e+09, device='cuda:0')
c= tensor(1.2715e+09, device='cuda:0')
c= tensor(1.2722e+09, device='cuda:0')
c= tensor(1.2722e+09, device='cuda:0')
c= tensor(1.2722e+09, device='cuda:0')
c= tensor(1.2902e+09, device='cuda:0')
c= tensor(1.2906e+09, device='cuda:0')
c= tensor(1.2915e+09, device='cuda:0')
c= tensor(1.2915e+09, device='cuda:0')
c= tensor(1.2938e+09, device='cuda:0')
c= tensor(1.3012e+09, device='cuda:0')
c= tensor(1.3255e+09, device='cuda:0')
c= tensor(1.3303e+09, device='cuda:0')
c= tensor(1.3304e+09, device='cuda:0')
c= tensor(1.3306e+09, device='cuda:0')
c= tensor(1.3310e+09, device='cuda:0')
c= tensor(1.3311e+09, device='cuda:0')
c= tensor(1.3311e+09, device='cuda:0')
c= tensor(1.3311e+09, device='cuda:0')
c= tensor(1.3314e+09, device='cuda:0')
c= tensor(1.3314e+09, device='cuda:0')
c= tensor(1.3314e+09, device='cuda:0')
c= tensor(1.3315e+09, device='cuda:0')
c= tensor(1.3315e+09, device='cuda:0')
c= tensor(1.3320e+09, device='cuda:0')
c= tensor(1.3321e+09, device='cuda:0')
c= tensor(1.3322e+09, device='cuda:0')
c= tensor(1.3322e+09, device='cuda:0')
c= tensor(1.3323e+09, device='cuda:0')
c= tensor(1.3323e+09, device='cuda:0')
c= tensor(1.3323e+09, device='cuda:0')
c= tensor(1.3324e+09, device='cuda:0')
c= tensor(1.3417e+09, device='cuda:0')
c= tensor(1.3417e+09, device='cuda:0')
c= tensor(1.3417e+09, device='cuda:0')
c= tensor(1.3417e+09, device='cuda:0')
c= tensor(1.3515e+09, device='cuda:0')
c= tensor(1.3624e+09, device='cuda:0')
c= tensor(1.3632e+09, device='cuda:0')
c= tensor(1.3632e+09, device='cuda:0')
c= tensor(1.3652e+09, device='cuda:0')
c= tensor(1.3662e+09, device='cuda:0')
c= tensor(1.3662e+09, device='cuda:0')
c= tensor(1.3662e+09, device='cuda:0')
c= tensor(1.3663e+09, device='cuda:0')
c= tensor(1.3695e+09, device='cuda:0')
c= tensor(1.3697e+09, device='cuda:0')
c= tensor(1.3753e+09, device='cuda:0')
c= tensor(1.3753e+09, device='cuda:0')
c= tensor(1.3753e+09, device='cuda:0')
c= tensor(1.3753e+09, device='cuda:0')
c= tensor(1.3755e+09, device='cuda:0')
c= tensor(1.3755e+09, device='cuda:0')
c= tensor(1.3757e+09, device='cuda:0')
c= tensor(1.3893e+09, device='cuda:0')
c= tensor(1.4196e+09, device='cuda:0')
c= tensor(1.4196e+09, device='cuda:0')
c= tensor(1.4196e+09, device='cuda:0')
c= tensor(1.4196e+09, device='cuda:0')
c= tensor(1.4492e+09, device='cuda:0')
c= tensor(1.4493e+09, device='cuda:0')
c= tensor(1.4493e+09, device='cuda:0')
c= tensor(1.4493e+09, device='cuda:0')
c= tensor(1.4521e+09, device='cuda:0')
c= tensor(1.4521e+09, device='cuda:0')
c= tensor(1.4535e+09, device='cuda:0')
c= tensor(1.4535e+09, device='cuda:0')
c= tensor(1.4535e+09, device='cuda:0')
c= tensor(1.4535e+09, device='cuda:0')
c= tensor(1.4535e+09, device='cuda:0')
c= tensor(1.4535e+09, device='cuda:0')
c= tensor(1.4561e+09, device='cuda:0')
c= tensor(1.4704e+09, device='cuda:0')
c= tensor(1.4774e+09, device='cuda:0')
c= tensor(1.4781e+09, device='cuda:0')
c= tensor(1.4783e+09, device='cuda:0')
c= tensor(1.4783e+09, device='cuda:0')
c= tensor(1.4783e+09, device='cuda:0')
c= tensor(1.4795e+09, device='cuda:0')
c= tensor(1.4798e+09, device='cuda:0')
c= tensor(1.4843e+09, device='cuda:0')
c= tensor(1.4843e+09, device='cuda:0')
c= tensor(1.6022e+09, device='cuda:0')
c= tensor(1.6022e+09, device='cuda:0')
c= tensor(1.6027e+09, device='cuda:0')
c= tensor(1.6083e+09, device='cuda:0')
c= tensor(1.6085e+09, device='cuda:0')
c= tensor(1.6102e+09, device='cuda:0')
c= tensor(1.6384e+09, device='cuda:0')
c= tensor(1.6391e+09, device='cuda:0')
c= tensor(1.6395e+09, device='cuda:0')
c= tensor(1.6395e+09, device='cuda:0')
c= tensor(1.6399e+09, device='cuda:0')
c= tensor(1.6400e+09, device='cuda:0')
c= tensor(1.6456e+09, device='cuda:0')
c= tensor(1.7157e+09, device='cuda:0')
c= tensor(1.7213e+09, device='cuda:0')
c= tensor(1.7305e+09, device='cuda:0')
c= tensor(1.7319e+09, device='cuda:0')
c= tensor(1.7323e+09, device='cuda:0')
c= tensor(1.7326e+09, device='cuda:0')
c= tensor(1.7326e+09, device='cuda:0')
c= tensor(1.7326e+09, device='cuda:0')
c= tensor(1.7398e+09, device='cuda:0')
c= tensor(1.7402e+09, device='cuda:0')
c= tensor(1.7487e+09, device='cuda:0')
c= tensor(1.7571e+09, device='cuda:0')
c= tensor(1.7574e+09, device='cuda:0')
c= tensor(1.7574e+09, device='cuda:0')
c= tensor(1.7611e+09, device='cuda:0')
c= tensor(1.7618e+09, device='cuda:0')
c= tensor(1.7618e+09, device='cuda:0')
c= tensor(1.7793e+09, device='cuda:0')
c= tensor(1.7799e+09, device='cuda:0')
c= tensor(1.7809e+09, device='cuda:0')
c= tensor(1.7809e+09, device='cuda:0')
c= tensor(1.7809e+09, device='cuda:0')
c= tensor(1.7809e+09, device='cuda:0')
c= tensor(1.7809e+09, device='cuda:0')
c= tensor(1.7810e+09, device='cuda:0')
c= tensor(1.7813e+09, device='cuda:0')
c= tensor(3.2870e+09, device='cuda:0')
c= tensor(3.2871e+09, device='cuda:0')
c= tensor(3.2874e+09, device='cuda:0')
c= tensor(3.2874e+09, device='cuda:0')
c= tensor(3.2874e+09, device='cuda:0')
c= tensor(3.2875e+09, device='cuda:0')
c= tensor(3.3017e+09, device='cuda:0')
c= tensor(3.3017e+09, device='cuda:0')
c= tensor(3.3692e+09, device='cuda:0')
c= tensor(3.3692e+09, device='cuda:0')
c= tensor(3.3762e+09, device='cuda:0')
c= tensor(3.3777e+09, device='cuda:0')
c= tensor(3.3815e+09, device='cuda:0')
c= tensor(3.3930e+09, device='cuda:0')
c= tensor(3.3930e+09, device='cuda:0')
c= tensor(3.3930e+09, device='cuda:0')
c= tensor(3.3936e+09, device='cuda:0')
c= tensor(3.3937e+09, device='cuda:0')
c= tensor(3.3938e+09, device='cuda:0')
c= tensor(3.3949e+09, device='cuda:0')
c= tensor(3.3950e+09, device='cuda:0')
c= tensor(3.3955e+09, device='cuda:0')
c= tensor(3.3957e+09, device='cuda:0')
c= tensor(3.4078e+09, device='cuda:0')
c= tensor(3.4300e+09, device='cuda:0')
c= tensor(3.4303e+09, device='cuda:0')
c= tensor(3.4303e+09, device='cuda:0')
c= tensor(3.4332e+09, device='cuda:0')
c= tensor(3.4337e+09, device='cuda:0')
c= tensor(3.4339e+09, device='cuda:0')
c= tensor(3.4344e+09, device='cuda:0')
c= tensor(3.4350e+09, device='cuda:0')
c= tensor(3.4355e+09, device='cuda:0')
c= tensor(3.4399e+09, device='cuda:0')
c= tensor(3.4410e+09, device='cuda:0')
c= tensor(3.4411e+09, device='cuda:0')
c= tensor(3.4411e+09, device='cuda:0')
c= tensor(3.4411e+09, device='cuda:0')
c= tensor(3.4430e+09, device='cuda:0')
c= tensor(3.4451e+09, device='cuda:0')
c= tensor(3.4533e+09, device='cuda:0')
c= tensor(3.4533e+09, device='cuda:0')
c= tensor(3.4534e+09, device='cuda:0')
c= tensor(3.4537e+09, device='cuda:0')
c= tensor(3.4621e+09, device='cuda:0')
c= tensor(3.4627e+09, device='cuda:0')
c= tensor(3.4633e+09, device='cuda:0')
c= tensor(3.4635e+09, device='cuda:0')
c= tensor(3.4638e+09, device='cuda:0')
c= tensor(3.4638e+09, device='cuda:0')
c= tensor(3.4638e+09, device='cuda:0')
c= tensor(3.4644e+09, device='cuda:0')
c= tensor(3.4646e+09, device='cuda:0')
c= tensor(3.4648e+09, device='cuda:0')
c= tensor(3.4648e+09, device='cuda:0')
c= tensor(3.4648e+09, device='cuda:0')
c= tensor(3.4651e+09, device='cuda:0')
c= tensor(3.4654e+09, device='cuda:0')
c= tensor(3.4655e+09, device='cuda:0')
c= tensor(3.4655e+09, device='cuda:0')
c= tensor(3.4655e+09, device='cuda:0')
c= tensor(3.4657e+09, device='cuda:0')
c= tensor(3.4658e+09, device='cuda:0')
c= tensor(3.4680e+09, device='cuda:0')
c= tensor(3.4680e+09, device='cuda:0')
c= tensor(3.4680e+09, device='cuda:0')
c= tensor(3.4682e+09, device='cuda:0')
c= tensor(3.4682e+09, device='cuda:0')
c= tensor(3.4762e+09, device='cuda:0')
c= tensor(3.4762e+09, device='cuda:0')
c= tensor(3.4774e+09, device='cuda:0')
c= tensor(3.4795e+09, device='cuda:0')
c= tensor(3.4816e+09, device='cuda:0')
c= tensor(3.4856e+09, device='cuda:0')
c= tensor(3.4876e+09, device='cuda:0')
c= tensor(3.4876e+09, device='cuda:0')
c= tensor(3.4879e+09, device='cuda:0')
c= tensor(3.4880e+09, device='cuda:0')
c= tensor(3.4886e+09, device='cuda:0')
c= tensor(3.4911e+09, device='cuda:0')
c= tensor(3.4913e+09, device='cuda:0')
c= tensor(3.4926e+09, device='cuda:0')
c= tensor(3.4926e+09, device='cuda:0')
c= tensor(3.4938e+09, device='cuda:0')
c= tensor(3.4939e+09, device='cuda:0')
c= tensor(3.4939e+09, device='cuda:0')
c= tensor(3.5071e+09, device='cuda:0')
c= tensor(3.5949e+09, device='cuda:0')
c= tensor(3.5951e+09, device='cuda:0')
c= tensor(3.5951e+09, device='cuda:0')
c= tensor(3.5952e+09, device='cuda:0')
c= tensor(3.5965e+09, device='cuda:0')
c= tensor(3.5966e+09, device='cuda:0')
c= tensor(3.5967e+09, device='cuda:0')
c= tensor(3.5971e+09, device='cuda:0')
c= tensor(3.5997e+09, device='cuda:0')
c= tensor(3.5997e+09, device='cuda:0')
c= tensor(3.6010e+09, device='cuda:0')
c= tensor(3.6010e+09, device='cuda:0')
memory (bytes)
4211425280
time for making loss 2 is 17.28659200668335
p0 True
it  0 : 1448908288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 49% |
shape of L is 
torch.Size([])
memory (bytes)
4211757056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  8% |
memory (bytes)
4212387840
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  30377351000.0
relative error loss 8.435853
shape of L is 
torch.Size([])
memory (bytes)
4438257664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  9% |
memory (bytes)
4438388736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  9% |
error is  30377180000.0
relative error loss 8.435805
shape of L is 
torch.Size([])
memory (bytes)
4444602368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  9% |
memory (bytes)
4444602368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  30376548000.0
relative error loss 8.43563
shape of L is 
torch.Size([])
memory (bytes)
4446736384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% |  9% |
memory (bytes)
4446736384
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  30372910000.0
relative error loss 8.43462
shape of L is 
torch.Size([])
memory (bytes)
4448690176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% |  9% |
memory (bytes)
4448690176
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  30348734000.0
relative error loss 8.427906
shape of L is 
torch.Size([])
memory (bytes)
4450983936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  9% |
memory (bytes)
4450983936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  30220180000.0
relative error loss 8.392206
shape of L is 
torch.Size([])
memory (bytes)
4452925440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% |  9% |
memory (bytes)
4452925440
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  29521017000.0
relative error loss 8.198048
shape of L is 
torch.Size([])
memory (bytes)
4455104512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% |  9% |
memory (bytes)
4455104512
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  22738821000.0
relative error loss 6.3146176
shape of L is 
torch.Size([])
memory (bytes)
4457369600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% |  9% |
memory (bytes)
4457369600
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  7909044700.0
relative error loss 2.196358
shape of L is 
torch.Size([])
memory (bytes)
4459388928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  9% |
memory (bytes)
4459388928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  5109364000.0
relative error loss 1.4188809
time to take a step is 168.4526059627533
it  1 : 1808479232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  9% |
shape of L is 
torch.Size([])
memory (bytes)
4461641728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% |  9% |
memory (bytes)
4461641728
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  5109364000.0
relative error loss 1.4188809
shape of L is 
torch.Size([])
memory (bytes)
4463640576
| ID | GPU | MEM |
------------------
|  0 |  3% |  0% |
|  1 |  9% |  9% |
memory (bytes)
4463640576
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  4398904300.0
relative error loss 1.2215848
shape of L is 
torch.Size([])
memory (bytes)
4465926144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  9% |
memory (bytes)
4465926144
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  4196769300.0
relative error loss 1.1654515
shape of L is 
torch.Size([])
memory (bytes)
4468056064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% |  9% |
memory (bytes)
4468056064
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  3656157400.0
relative error loss 1.0153224
shape of L is 
torch.Size([])
memory (bytes)
4470181888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  9% |
memory (bytes)
4470181888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  9% |
error is  3482183000.0
relative error loss 0.96700937
shape of L is 
torch.Size([])
memory (bytes)
4472315904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  9% |
memory (bytes)
4472315904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% |  9% |
error is  3319052800.0
relative error loss 0.9217078
shape of L is 
torch.Size([])
memory (bytes)
4474458112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  9% |
memory (bytes)
4474458112
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  3204961800.0
relative error loss 0.8900245
shape of L is 
torch.Size([])
memory (bytes)
4476571648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  9% |
memory (bytes)
4476571648
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  3069159000.0
relative error loss 0.8523118
shape of L is 
torch.Size([])
memory (bytes)
4478681088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  9% |
memory (bytes)
4478681088
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  2850559000.0
relative error loss 0.7916061
shape of L is 
torch.Size([])
memory (bytes)
4480679936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  9% |
memory (bytes)
4480679936
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  2646133800.0
relative error loss 0.7348368
time to take a step is 227.1950535774231
it  2 : 1911214080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  9% |
shape of L is 
torch.Size([])
memory (bytes)
4482891776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  9% |
memory (bytes)
4482891776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  2646133800.0
relative error loss 0.7348368
shape of L is 
torch.Size([])
memory (bytes)
4485013504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  9% |
memory (bytes)
4485021696
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  2404041500.0
relative error loss 0.6676073
shape of L is 
torch.Size([])
memory (bytes)
4487155712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  9% |
memory (bytes)
4487155712
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  2171013600.0
relative error loss 0.60289496
shape of L is 
torch.Size([])
memory (bytes)
4489175040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  9% |
memory (bytes)
4489175040
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1960172400.0
relative error loss 0.54434395
shape of L is 
torch.Size([])
memory (bytes)
4491407360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% |  9% |
memory (bytes)
4491407360
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1845951500.0
relative error loss 0.51262456
shape of L is 
torch.Size([])
memory (bytes)
4493303808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% |  9% |
memory (bytes)
4493303808
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1715164400.0
relative error loss 0.4763047
shape of L is 
torch.Size([])
memory (bytes)
4495659008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% |  9% |
memory (bytes)
4495659008
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1648966100.0
relative error loss 0.4579213
shape of L is 
torch.Size([])
memory (bytes)
4497629184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  9% |
memory (bytes)
4497788928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% |  9% |
error is  1534570000.0
relative error loss 0.42615324
shape of L is 
torch.Size([])
memory (bytes)
4499890176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  9% |
memory (bytes)
4499927040
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1416798500.0
relative error loss 0.39344785
shape of L is 
torch.Size([])
memory (bytes)
4502069248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  9% |
memory (bytes)
4502069248
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1288187900.0
relative error loss 0.35773245
time to take a step is 227.90915989875793
it  3 : 1911214080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  9% |
shape of L is 
torch.Size([])
memory (bytes)
4504002560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% |  9% |
memory (bytes)
4504002560
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1288187900.0
relative error loss 0.35773245
shape of L is 
torch.Size([])
memory (bytes)
4506361856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
memory (bytes)
4506361856
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1185699300.0
relative error loss 0.32927114
shape of L is 
torch.Size([])
memory (bytes)
4508356608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% |  9% |
memory (bytes)
4508356608
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1072738050.0
relative error loss 0.29790157
shape of L is 
torch.Size([])
memory (bytes)
4510289920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 10% |
memory (bytes)
4510289920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  998702100.0
relative error loss 0.27734163
shape of L is 
torch.Size([])
memory (bytes)
4512763904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 10% |
memory (bytes)
4512763904
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  927212800.0
relative error loss 0.2574889
shape of L is 
torch.Size([])
memory (bytes)
4514951168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 10% |
memory (bytes)
4514951168
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  861943550.0
relative error loss 0.2393635
shape of L is 
torch.Size([])
memory (bytes)
4517007360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 10% |
memory (bytes)
4517007360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  782200060.0
relative error loss 0.21721858
shape of L is 
torch.Size([])
memory (bytes)
4519006208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 10% |
memory (bytes)
4519243776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  732253440.0
relative error loss 0.2033483
shape of L is 
torch.Size([])
memory (bytes)
4521390080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 10% |
memory (bytes)
4521390080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  674414340.0
relative error loss 0.18728626
shape of L is 
torch.Size([])
memory (bytes)
4523536384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4523536384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  666941440.0
relative error loss 0.18521102
shape of L is 
torch.Size([])
memory (bytes)
4525338624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 10% |
memory (bytes)
4525592576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  636783360.0
relative error loss 0.17683606
time to take a step is 249.03949522972107
c= tensor(757.4586, device='cuda:0')
c= tensor(48163.8633, device='cuda:0')
c= tensor(51936.9062, device='cuda:0')
c= tensor(118880.9453, device='cuda:0')
c= tensor(138956.7344, device='cuda:0')
c= tensor(307147.1250, device='cuda:0')
c= tensor(837062.5000, device='cuda:0')
c= tensor(997769.8750, device='cuda:0')
c= tensor(1025872.9375, device='cuda:0')
c= tensor(3764424., device='cuda:0')
c= tensor(3777815.5000, device='cuda:0')
c= tensor(6124959.5000, device='cuda:0')
c= tensor(6143182.5000, device='cuda:0')
c= tensor(30124346., device='cuda:0')
c= tensor(30236636., device='cuda:0')
c= tensor(30519084., device='cuda:0')
c= tensor(31561838., device='cuda:0')
c= tensor(31895860., device='cuda:0')
c= tensor(37028620., device='cuda:0')
c= tensor(40151712., device='cuda:0')
c= tensor(40238588., device='cuda:0')
c= tensor(47485000., device='cuda:0')
c= tensor(47503056., device='cuda:0')
c= tensor(48118060., device='cuda:0')
c= tensor(48124896., device='cuda:0')
c= tensor(48877360., device='cuda:0')
c= tensor(49671860., device='cuda:0')
c= tensor(49678236., device='cuda:0')
c= tensor(50845884., device='cuda:0')
c= tensor(2.1737e+08, device='cuda:0')
c= tensor(2.1740e+08, device='cuda:0')
c= tensor(3.3903e+08, device='cuda:0')
c= tensor(3.3914e+08, device='cuda:0')
c= tensor(3.3917e+08, device='cuda:0')
c= tensor(3.3920e+08, device='cuda:0')
c= tensor(3.4526e+08, device='cuda:0')
c= tensor(3.5068e+08, device='cuda:0')
c= tensor(3.5068e+08, device='cuda:0')
c= tensor(3.5070e+08, device='cuda:0')
c= tensor(3.5070e+08, device='cuda:0')
c= tensor(3.5070e+08, device='cuda:0')
c= tensor(3.5070e+08, device='cuda:0')
c= tensor(3.5071e+08, device='cuda:0')
c= tensor(3.5071e+08, device='cuda:0')
c= tensor(3.5071e+08, device='cuda:0')
c= tensor(3.5071e+08, device='cuda:0')
c= tensor(3.5071e+08, device='cuda:0')
c= tensor(3.5072e+08, device='cuda:0')
c= tensor(3.5072e+08, device='cuda:0')
c= tensor(3.5074e+08, device='cuda:0')
c= tensor(3.5077e+08, device='cuda:0')
c= tensor(3.5077e+08, device='cuda:0')
c= tensor(3.5078e+08, device='cuda:0')
c= tensor(3.5078e+08, device='cuda:0')
c= tensor(3.5079e+08, device='cuda:0')
c= tensor(3.5080e+08, device='cuda:0')
c= tensor(3.5080e+08, device='cuda:0')
c= tensor(3.5080e+08, device='cuda:0')
c= tensor(3.5080e+08, device='cuda:0')
c= tensor(3.5081e+08, device='cuda:0')
c= tensor(3.5082e+08, device='cuda:0')
c= tensor(3.5082e+08, device='cuda:0')
c= tensor(3.5084e+08, device='cuda:0')
c= tensor(3.5085e+08, device='cuda:0')
c= tensor(3.5085e+08, device='cuda:0')
c= tensor(3.5085e+08, device='cuda:0')
c= tensor(3.5086e+08, device='cuda:0')
c= tensor(3.5086e+08, device='cuda:0')
c= tensor(3.5086e+08, device='cuda:0')
c= tensor(3.5087e+08, device='cuda:0')
c= tensor(3.5087e+08, device='cuda:0')
c= tensor(3.5087e+08, device='cuda:0')
c= tensor(3.5088e+08, device='cuda:0')
c= tensor(3.5088e+08, device='cuda:0')
c= tensor(3.5088e+08, device='cuda:0')
c= tensor(3.5089e+08, device='cuda:0')
c= tensor(3.5089e+08, device='cuda:0')
c= tensor(3.5089e+08, device='cuda:0')
c= tensor(3.5089e+08, device='cuda:0')
c= tensor(3.5092e+08, device='cuda:0')
c= tensor(3.5092e+08, device='cuda:0')
c= tensor(3.5092e+08, device='cuda:0')
c= tensor(3.5094e+08, device='cuda:0')
c= tensor(3.5094e+08, device='cuda:0')
c= tensor(3.5095e+08, device='cuda:0')
c= tensor(3.5095e+08, device='cuda:0')
c= tensor(3.5095e+08, device='cuda:0')
c= tensor(3.5095e+08, device='cuda:0')
c= tensor(3.5096e+08, device='cuda:0')
c= tensor(3.5096e+08, device='cuda:0')
c= tensor(3.5096e+08, device='cuda:0')
c= tensor(3.5096e+08, device='cuda:0')
c= tensor(3.5096e+08, device='cuda:0')
c= tensor(3.5097e+08, device='cuda:0')
c= tensor(3.5097e+08, device='cuda:0')
c= tensor(3.5098e+08, device='cuda:0')
c= tensor(3.5099e+08, device='cuda:0')
c= tensor(3.5102e+08, device='cuda:0')
c= tensor(3.5103e+08, device='cuda:0')
c= tensor(3.5103e+08, device='cuda:0')
c= tensor(3.5106e+08, device='cuda:0')
c= tensor(3.5106e+08, device='cuda:0')
c= tensor(3.5107e+08, device='cuda:0')
c= tensor(3.5107e+08, device='cuda:0')
c= tensor(3.5107e+08, device='cuda:0')
c= tensor(3.5108e+08, device='cuda:0')
c= tensor(3.5108e+08, device='cuda:0')
c= tensor(3.5108e+08, device='cuda:0')
c= tensor(3.5108e+08, device='cuda:0')
c= tensor(3.5109e+08, device='cuda:0')
c= tensor(3.5109e+08, device='cuda:0')
c= tensor(3.5109e+08, device='cuda:0')
c= tensor(3.5109e+08, device='cuda:0')
c= tensor(3.5109e+08, device='cuda:0')
c= tensor(3.5110e+08, device='cuda:0')
c= tensor(3.5110e+08, device='cuda:0')
c= tensor(3.5110e+08, device='cuda:0')
c= tensor(3.5110e+08, device='cuda:0')
c= tensor(3.5112e+08, device='cuda:0')
c= tensor(3.5112e+08, device='cuda:0')
c= tensor(3.5113e+08, device='cuda:0')
c= tensor(3.5113e+08, device='cuda:0')
c= tensor(3.5114e+08, device='cuda:0')
c= tensor(3.5114e+08, device='cuda:0')
c= tensor(3.5114e+08, device='cuda:0')
c= tensor(3.5114e+08, device='cuda:0')
c= tensor(3.5114e+08, device='cuda:0')
c= tensor(3.5115e+08, device='cuda:0')
c= tensor(3.5121e+08, device='cuda:0')
c= tensor(3.5122e+08, device='cuda:0')
c= tensor(3.5123e+08, device='cuda:0')
c= tensor(3.5123e+08, device='cuda:0')
c= tensor(3.5123e+08, device='cuda:0')
c= tensor(3.5123e+08, device='cuda:0')
c= tensor(3.5124e+08, device='cuda:0')
c= tensor(3.5124e+08, device='cuda:0')
c= tensor(3.5124e+08, device='cuda:0')
c= tensor(3.5124e+08, device='cuda:0')
c= tensor(3.5124e+08, device='cuda:0')
c= tensor(3.5124e+08, device='cuda:0')
c= tensor(3.5125e+08, device='cuda:0')
c= tensor(3.5125e+08, device='cuda:0')
c= tensor(3.5128e+08, device='cuda:0')
c= tensor(3.5130e+08, device='cuda:0')
c= tensor(3.5131e+08, device='cuda:0')
c= tensor(3.5131e+08, device='cuda:0')
c= tensor(3.5131e+08, device='cuda:0')
c= tensor(3.5132e+08, device='cuda:0')
c= tensor(3.5132e+08, device='cuda:0')
c= tensor(3.5132e+08, device='cuda:0')
c= tensor(3.5132e+08, device='cuda:0')
c= tensor(3.5133e+08, device='cuda:0')
c= tensor(3.5134e+08, device='cuda:0')
c= tensor(3.5137e+08, device='cuda:0')
c= tensor(3.5137e+08, device='cuda:0')
c= tensor(3.5148e+08, device='cuda:0')
c= tensor(3.5148e+08, device='cuda:0')
c= tensor(3.5148e+08, device='cuda:0')
c= tensor(3.5149e+08, device='cuda:0')
c= tensor(3.5149e+08, device='cuda:0')
c= tensor(3.5154e+08, device='cuda:0')
c= tensor(3.5154e+08, device='cuda:0')
c= tensor(3.5155e+08, device='cuda:0')
c= tensor(3.5155e+08, device='cuda:0')
c= tensor(3.5155e+08, device='cuda:0')
c= tensor(3.5156e+08, device='cuda:0')
c= tensor(3.5156e+08, device='cuda:0')
c= tensor(3.5156e+08, device='cuda:0')
c= tensor(3.5156e+08, device='cuda:0')
c= tensor(3.5156e+08, device='cuda:0')
c= tensor(3.5157e+08, device='cuda:0')
c= tensor(3.5157e+08, device='cuda:0')
c= tensor(3.5157e+08, device='cuda:0')
c= tensor(3.5158e+08, device='cuda:0')
c= tensor(3.5158e+08, device='cuda:0')
c= tensor(3.5158e+08, device='cuda:0')
c= tensor(3.5159e+08, device='cuda:0')
c= tensor(3.5159e+08, device='cuda:0')
c= tensor(3.5160e+08, device='cuda:0')
c= tensor(3.5160e+08, device='cuda:0')
c= tensor(3.5160e+08, device='cuda:0')
c= tensor(3.5161e+08, device='cuda:0')
c= tensor(3.5161e+08, device='cuda:0')
c= tensor(3.5161e+08, device='cuda:0')
c= tensor(3.5162e+08, device='cuda:0')
c= tensor(3.5163e+08, device='cuda:0')
c= tensor(3.5163e+08, device='cuda:0')
c= tensor(3.5164e+08, device='cuda:0')
c= tensor(3.5164e+08, device='cuda:0')
c= tensor(3.5169e+08, device='cuda:0')
c= tensor(3.5169e+08, device='cuda:0')
c= tensor(3.5169e+08, device='cuda:0')
c= tensor(3.5169e+08, device='cuda:0')
c= tensor(3.5169e+08, device='cuda:0')
c= tensor(3.5170e+08, device='cuda:0')
c= tensor(3.5170e+08, device='cuda:0')
c= tensor(3.5170e+08, device='cuda:0')
c= tensor(3.5170e+08, device='cuda:0')
c= tensor(3.5171e+08, device='cuda:0')
c= tensor(3.5171e+08, device='cuda:0')
c= tensor(3.5171e+08, device='cuda:0')
c= tensor(3.5171e+08, device='cuda:0')
c= tensor(3.5172e+08, device='cuda:0')
c= tensor(3.5173e+08, device='cuda:0')
c= tensor(3.5174e+08, device='cuda:0')
c= tensor(3.5174e+08, device='cuda:0')
c= tensor(3.5175e+08, device='cuda:0')
c= tensor(3.5176e+08, device='cuda:0')
c= tensor(3.5176e+08, device='cuda:0')
c= tensor(3.5179e+08, device='cuda:0')
c= tensor(3.5180e+08, device='cuda:0')
c= tensor(3.5181e+08, device='cuda:0')
c= tensor(3.5181e+08, device='cuda:0')
c= tensor(3.5181e+08, device='cuda:0')
c= tensor(3.5181e+08, device='cuda:0')
c= tensor(3.5181e+08, device='cuda:0')
c= tensor(3.5181e+08, device='cuda:0')
c= tensor(3.5181e+08, device='cuda:0')
c= tensor(3.5182e+08, device='cuda:0')
c= tensor(3.5182e+08, device='cuda:0')
c= tensor(3.5183e+08, device='cuda:0')
c= tensor(3.5183e+08, device='cuda:0')
c= tensor(3.5183e+08, device='cuda:0')
c= tensor(3.5183e+08, device='cuda:0')
c= tensor(3.5184e+08, device='cuda:0')
c= tensor(3.5185e+08, device='cuda:0')
c= tensor(3.5185e+08, device='cuda:0')
c= tensor(3.5185e+08, device='cuda:0')
c= tensor(3.5185e+08, device='cuda:0')
c= tensor(3.5186e+08, device='cuda:0')
c= tensor(3.5186e+08, device='cuda:0')
c= tensor(3.5186e+08, device='cuda:0')
c= tensor(3.5186e+08, device='cuda:0')
c= tensor(3.5187e+08, device='cuda:0')
c= tensor(3.5187e+08, device='cuda:0')
c= tensor(3.5187e+08, device='cuda:0')
c= tensor(3.5187e+08, device='cuda:0')
c= tensor(3.5189e+08, device='cuda:0')
c= tensor(3.5189e+08, device='cuda:0')
c= tensor(3.5194e+08, device='cuda:0')
c= tensor(3.5326e+08, device='cuda:0')
c= tensor(3.5326e+08, device='cuda:0')
c= tensor(3.5327e+08, device='cuda:0')
c= tensor(3.5327e+08, device='cuda:0')
c= tensor(3.5328e+08, device='cuda:0')
c= tensor(3.6217e+08, device='cuda:0')
c= tensor(3.9706e+08, device='cuda:0')
c= tensor(3.9706e+08, device='cuda:0')
c= tensor(3.9938e+08, device='cuda:0')
c= tensor(3.9965e+08, device='cuda:0')
c= tensor(3.9980e+08, device='cuda:0')
c= tensor(4.0568e+08, device='cuda:0')
c= tensor(4.0568e+08, device='cuda:0')
c= tensor(4.0568e+08, device='cuda:0')
c= tensor(4.0896e+08, device='cuda:0')
c= tensor(4.3384e+08, device='cuda:0')
c= tensor(4.3384e+08, device='cuda:0')
c= tensor(4.3400e+08, device='cuda:0')
c= tensor(4.3407e+08, device='cuda:0')
c= tensor(4.3622e+08, device='cuda:0')
c= tensor(4.3818e+08, device='cuda:0')
c= tensor(4.3866e+08, device='cuda:0')
c= tensor(4.3889e+08, device='cuda:0')
c= tensor(4.3901e+08, device='cuda:0')
c= tensor(4.3902e+08, device='cuda:0')
c= tensor(4.6424e+08, device='cuda:0')
c= tensor(4.6425e+08, device='cuda:0')
c= tensor(4.6425e+08, device='cuda:0')
c= tensor(4.6442e+08, device='cuda:0')
c= tensor(4.6564e+08, device='cuda:0')
c= tensor(4.7213e+08, device='cuda:0')
c= tensor(4.7298e+08, device='cuda:0')
c= tensor(4.7298e+08, device='cuda:0')
c= tensor(4.7318e+08, device='cuda:0')
c= tensor(4.7321e+08, device='cuda:0')
c= tensor(4.7370e+08, device='cuda:0')
c= tensor(4.7415e+08, device='cuda:0')
c= tensor(4.7416e+08, device='cuda:0')
c= tensor(4.7438e+08, device='cuda:0')
c= tensor(4.7438e+08, device='cuda:0')
c= tensor(4.7438e+08, device='cuda:0')
c= tensor(4.7581e+08, device='cuda:0')
c= tensor(4.7681e+08, device='cuda:0')
c= tensor(4.7750e+08, device='cuda:0')
c= tensor(4.7752e+08, device='cuda:0')
c= tensor(4.9528e+08, device='cuda:0')
c= tensor(4.9530e+08, device='cuda:0')
c= tensor(4.9544e+08, device='cuda:0')
c= tensor(4.9671e+08, device='cuda:0')
c= tensor(4.9672e+08, device='cuda:0')
c= tensor(4.9981e+08, device='cuda:0')
c= tensor(5.1513e+08, device='cuda:0')
c= tensor(5.3441e+08, device='cuda:0')
c= tensor(5.3446e+08, device='cuda:0')
c= tensor(5.3451e+08, device='cuda:0')
c= tensor(5.3452e+08, device='cuda:0')
c= tensor(5.3453e+08, device='cuda:0')
c= tensor(5.3553e+08, device='cuda:0')
c= tensor(5.3554e+08, device='cuda:0')
c= tensor(5.3581e+08, device='cuda:0')
c= tensor(5.3995e+08, device='cuda:0')
c= tensor(5.4001e+08, device='cuda:0')
c= tensor(5.4009e+08, device='cuda:0')
c= tensor(5.4010e+08, device='cuda:0')
c= tensor(5.4335e+08, device='cuda:0')
c= tensor(5.4343e+08, device='cuda:0')
c= tensor(5.4358e+08, device='cuda:0')
c= tensor(5.4362e+08, device='cuda:0')
c= tensor(5.7628e+08, device='cuda:0')
c= tensor(5.7636e+08, device='cuda:0')
c= tensor(5.8159e+08, device='cuda:0')
c= tensor(5.8161e+08, device='cuda:0')
c= tensor(5.8439e+08, device='cuda:0')
c= tensor(5.8450e+08, device='cuda:0')
c= tensor(6.0515e+08, device='cuda:0')
c= tensor(6.0558e+08, device='cuda:0')
c= tensor(6.0559e+08, device='cuda:0')
c= tensor(6.0802e+08, device='cuda:0')
c= tensor(6.1019e+08, device='cuda:0')
c= tensor(6.1024e+08, device='cuda:0')
c= tensor(6.1216e+08, device='cuda:0')
c= tensor(6.1570e+08, device='cuda:0')
c= tensor(6.3379e+08, device='cuda:0')
c= tensor(6.3522e+08, device='cuda:0')
c= tensor(6.3522e+08, device='cuda:0')
c= tensor(6.3523e+08, device='cuda:0')
c= tensor(6.3537e+08, device='cuda:0')
c= tensor(6.3544e+08, device='cuda:0')
c= tensor(6.3557e+08, device='cuda:0')
c= tensor(6.3557e+08, device='cuda:0')
c= tensor(6.3627e+08, device='cuda:0')
c= tensor(6.3827e+08, device='cuda:0')
c= tensor(6.3828e+08, device='cuda:0')
c= tensor(6.3829e+08, device='cuda:0')
c= tensor(6.3841e+08, device='cuda:0')
c= tensor(6.3844e+08, device='cuda:0')
c= tensor(6.3845e+08, device='cuda:0')
c= tensor(6.3848e+08, device='cuda:0')
c= tensor(6.3848e+08, device='cuda:0')
c= tensor(6.3861e+08, device='cuda:0')
c= tensor(6.3880e+08, device='cuda:0')
c= tensor(6.3885e+08, device='cuda:0')
c= tensor(6.3942e+08, device='cuda:0')
c= tensor(6.3944e+08, device='cuda:0')
c= tensor(6.6375e+08, device='cuda:0')
c= tensor(6.6376e+08, device='cuda:0')
c= tensor(6.6521e+08, device='cuda:0')
c= tensor(6.6521e+08, device='cuda:0')
c= tensor(6.6521e+08, device='cuda:0')
c= tensor(6.6522e+08, device='cuda:0')
c= tensor(6.6529e+08, device='cuda:0')
c= tensor(6.6529e+08, device='cuda:0')
c= tensor(6.7079e+08, device='cuda:0')
c= tensor(6.7080e+08, device='cuda:0')
c= tensor(6.7080e+08, device='cuda:0')
c= tensor(6.7951e+08, device='cuda:0')
c= tensor(6.8043e+08, device='cuda:0')
c= tensor(6.8109e+08, device='cuda:0')
c= tensor(6.8690e+08, device='cuda:0')
c= tensor(6.9857e+08, device='cuda:0')
c= tensor(6.9858e+08, device='cuda:0')
c= tensor(6.9859e+08, device='cuda:0')
c= tensor(6.9869e+08, device='cuda:0')
c= tensor(6.9869e+08, device='cuda:0')
c= tensor(6.9870e+08, device='cuda:0')
c= tensor(6.9873e+08, device='cuda:0')
c= tensor(6.9874e+08, device='cuda:0')
c= tensor(6.9874e+08, device='cuda:0')
c= tensor(6.9875e+08, device='cuda:0')
c= tensor(6.9875e+08, device='cuda:0')
c= tensor(7.0848e+08, device='cuda:0')
c= tensor(7.0856e+08, device='cuda:0')
c= tensor(7.0998e+08, device='cuda:0')
c= tensor(7.0999e+08, device='cuda:0')
c= tensor(7.1004e+08, device='cuda:0')
c= tensor(7.1007e+08, device='cuda:0')
c= tensor(7.9659e+08, device='cuda:0')
c= tensor(8.2415e+08, device='cuda:0')
c= tensor(8.2419e+08, device='cuda:0')
c= tensor(8.2646e+08, device='cuda:0')
c= tensor(8.2646e+08, device='cuda:0')
c= tensor(8.2666e+08, device='cuda:0')
c= tensor(8.2738e+08, device='cuda:0')
c= tensor(8.2748e+08, device='cuda:0')
c= tensor(8.2785e+08, device='cuda:0')
c= tensor(8.2922e+08, device='cuda:0')
c= tensor(9.2637e+08, device='cuda:0')
c= tensor(9.2665e+08, device='cuda:0')
c= tensor(9.2666e+08, device='cuda:0')
c= tensor(9.2679e+08, device='cuda:0')
c= tensor(9.2681e+08, device='cuda:0')
c= tensor(9.2681e+08, device='cuda:0')
c= tensor(9.3020e+08, device='cuda:0')
c= tensor(9.3029e+08, device='cuda:0')
c= tensor(9.3029e+08, device='cuda:0')
c= tensor(9.3080e+08, device='cuda:0')
c= tensor(9.3083e+08, device='cuda:0')
c= tensor(9.3083e+08, device='cuda:0')
c= tensor(9.3218e+08, device='cuda:0')
c= tensor(9.3436e+08, device='cuda:0')
c= tensor(9.3874e+08, device='cuda:0')
c= tensor(9.4278e+08, device='cuda:0')
c= tensor(9.5238e+08, device='cuda:0')
c= tensor(9.5248e+08, device='cuda:0')
c= tensor(9.5258e+08, device='cuda:0')
c= tensor(9.5425e+08, device='cuda:0')
c= tensor(9.5563e+08, device='cuda:0')
c= tensor(9.5564e+08, device='cuda:0')
c= tensor(9.5792e+08, device='cuda:0')
c= tensor(9.6409e+08, device='cuda:0')
c= tensor(9.6859e+08, device='cuda:0')
c= tensor(9.7083e+08, device='cuda:0')
c= tensor(9.7335e+08, device='cuda:0')
c= tensor(9.7339e+08, device='cuda:0')
c= tensor(9.7339e+08, device='cuda:0')
c= tensor(9.7340e+08, device='cuda:0')
c= tensor(9.7456e+08, device='cuda:0')
c= tensor(9.7588e+08, device='cuda:0')
c= tensor(1.0125e+09, device='cuda:0')
c= tensor(1.0164e+09, device='cuda:0')
c= tensor(1.0172e+09, device='cuda:0')
c= tensor(1.0173e+09, device='cuda:0')
c= tensor(1.0200e+09, device='cuda:0')
c= tensor(1.0200e+09, device='cuda:0')
c= tensor(1.0200e+09, device='cuda:0')
c= tensor(1.0255e+09, device='cuda:0')
c= tensor(1.0256e+09, device='cuda:0')
c= tensor(1.0256e+09, device='cuda:0')
c= tensor(1.0256e+09, device='cuda:0')
c= tensor(1.0499e+09, device='cuda:0')
c= tensor(1.0500e+09, device='cuda:0')
c= tensor(1.0535e+09, device='cuda:0')
c= tensor(1.0536e+09, device='cuda:0')
c= tensor(1.0536e+09, device='cuda:0')
c= tensor(1.0536e+09, device='cuda:0')
c= tensor(1.0536e+09, device='cuda:0')
c= tensor(1.0544e+09, device='cuda:0')
c= tensor(1.0550e+09, device='cuda:0')
c= tensor(1.0550e+09, device='cuda:0')
c= tensor(1.0574e+09, device='cuda:0')
c= tensor(1.0574e+09, device='cuda:0')
c= tensor(1.0583e+09, device='cuda:0')
c= tensor(1.0584e+09, device='cuda:0')
c= tensor(1.0589e+09, device='cuda:0')
c= tensor(1.0589e+09, device='cuda:0')
c= tensor(1.0590e+09, device='cuda:0')
c= tensor(1.0590e+09, device='cuda:0')
c= tensor(1.0591e+09, device='cuda:0')
c= tensor(1.0603e+09, device='cuda:0')
c= tensor(1.0788e+09, device='cuda:0')
c= tensor(1.0788e+09, device='cuda:0')
c= tensor(1.0789e+09, device='cuda:0')
c= tensor(1.0831e+09, device='cuda:0')
c= tensor(1.0832e+09, device='cuda:0')
c= tensor(1.1142e+09, device='cuda:0')
c= tensor(1.1142e+09, device='cuda:0')
c= tensor(1.1156e+09, device='cuda:0')
c= tensor(1.1189e+09, device='cuda:0')
c= tensor(1.1189e+09, device='cuda:0')
c= tensor(1.1271e+09, device='cuda:0')
c= tensor(1.1272e+09, device='cuda:0')
c= tensor(1.2260e+09, device='cuda:0')
c= tensor(1.2260e+09, device='cuda:0')
c= tensor(1.2261e+09, device='cuda:0')
c= tensor(1.2261e+09, device='cuda:0')
c= tensor(1.2261e+09, device='cuda:0')
c= tensor(1.2262e+09, device='cuda:0')
c= tensor(1.2267e+09, device='cuda:0')
c= tensor(1.2270e+09, device='cuda:0')
c= tensor(1.2287e+09, device='cuda:0')
c= tensor(1.2287e+09, device='cuda:0')
c= tensor(1.2287e+09, device='cuda:0')
c= tensor(1.2287e+09, device='cuda:0')
c= tensor(1.2489e+09, device='cuda:0')
c= tensor(1.2528e+09, device='cuda:0')
c= tensor(1.2656e+09, device='cuda:0')
c= tensor(1.2657e+09, device='cuda:0')
c= tensor(1.2658e+09, device='cuda:0')
c= tensor(1.2658e+09, device='cuda:0')
c= tensor(1.2658e+09, device='cuda:0')
c= tensor(1.2709e+09, device='cuda:0')
c= tensor(1.2710e+09, device='cuda:0')
c= tensor(1.2710e+09, device='cuda:0')
c= tensor(1.2715e+09, device='cuda:0')
c= tensor(1.2722e+09, device='cuda:0')
c= tensor(1.2722e+09, device='cuda:0')
c= tensor(1.2722e+09, device='cuda:0')
c= tensor(1.2902e+09, device='cuda:0')
c= tensor(1.2906e+09, device='cuda:0')
c= tensor(1.2915e+09, device='cuda:0')
c= tensor(1.2915e+09, device='cuda:0')
c= tensor(1.2938e+09, device='cuda:0')
c= tensor(1.3012e+09, device='cuda:0')
c= tensor(1.3255e+09, device='cuda:0')
c= tensor(1.3303e+09, device='cuda:0')
c= tensor(1.3304e+09, device='cuda:0')
c= tensor(1.3306e+09, device='cuda:0')
c= tensor(1.3310e+09, device='cuda:0')
c= tensor(1.3311e+09, device='cuda:0')
c= tensor(1.3311e+09, device='cuda:0')
c= tensor(1.3311e+09, device='cuda:0')
c= tensor(1.3314e+09, device='cuda:0')
c= tensor(1.3314e+09, device='cuda:0')
c= tensor(1.3314e+09, device='cuda:0')
c= tensor(1.3315e+09, device='cuda:0')
c= tensor(1.3315e+09, device='cuda:0')
c= tensor(1.3320e+09, device='cuda:0')
c= tensor(1.3321e+09, device='cuda:0')
c= tensor(1.3322e+09, device='cuda:0')
c= tensor(1.3322e+09, device='cuda:0')
c= tensor(1.3323e+09, device='cuda:0')
c= tensor(1.3323e+09, device='cuda:0')
c= tensor(1.3323e+09, device='cuda:0')
c= tensor(1.3324e+09, device='cuda:0')
c= tensor(1.3417e+09, device='cuda:0')
c= tensor(1.3417e+09, device='cuda:0')
c= tensor(1.3417e+09, device='cuda:0')
c= tensor(1.3417e+09, device='cuda:0')
c= tensor(1.3515e+09, device='cuda:0')
c= tensor(1.3624e+09, device='cuda:0')
c= tensor(1.3632e+09, device='cuda:0')
c= tensor(1.3632e+09, device='cuda:0')
c= tensor(1.3652e+09, device='cuda:0')
c= tensor(1.3662e+09, device='cuda:0')
c= tensor(1.3662e+09, device='cuda:0')
c= tensor(1.3662e+09, device='cuda:0')
c= tensor(1.3663e+09, device='cuda:0')
c= tensor(1.3695e+09, device='cuda:0')
c= tensor(1.3697e+09, device='cuda:0')
c= tensor(1.3753e+09, device='cuda:0')
c= tensor(1.3753e+09, device='cuda:0')
c= tensor(1.3753e+09, device='cuda:0')
c= tensor(1.3753e+09, device='cuda:0')
c= tensor(1.3755e+09, device='cuda:0')
c= tensor(1.3755e+09, device='cuda:0')
c= tensor(1.3757e+09, device='cuda:0')
c= tensor(1.3893e+09, device='cuda:0')
c= tensor(1.4196e+09, device='cuda:0')
c= tensor(1.4196e+09, device='cuda:0')
c= tensor(1.4196e+09, device='cuda:0')
c= tensor(1.4196e+09, device='cuda:0')
c= tensor(1.4492e+09, device='cuda:0')
c= tensor(1.4493e+09, device='cuda:0')
c= tensor(1.4493e+09, device='cuda:0')
c= tensor(1.4493e+09, device='cuda:0')
c= tensor(1.4521e+09, device='cuda:0')
c= tensor(1.4521e+09, device='cuda:0')
c= tensor(1.4535e+09, device='cuda:0')
c= tensor(1.4535e+09, device='cuda:0')
c= tensor(1.4535e+09, device='cuda:0')
c= tensor(1.4535e+09, device='cuda:0')
c= tensor(1.4535e+09, device='cuda:0')
c= tensor(1.4535e+09, device='cuda:0')
c= tensor(1.4561e+09, device='cuda:0')
c= tensor(1.4704e+09, device='cuda:0')
c= tensor(1.4774e+09, device='cuda:0')
c= tensor(1.4781e+09, device='cuda:0')
c= tensor(1.4783e+09, device='cuda:0')
c= tensor(1.4783e+09, device='cuda:0')
c= tensor(1.4783e+09, device='cuda:0')
c= tensor(1.4795e+09, device='cuda:0')
c= tensor(1.4798e+09, device='cuda:0')
c= tensor(1.4843e+09, device='cuda:0')
c= tensor(1.4843e+09, device='cuda:0')
c= tensor(1.6022e+09, device='cuda:0')
c= tensor(1.6022e+09, device='cuda:0')
c= tensor(1.6027e+09, device='cuda:0')
c= tensor(1.6083e+09, device='cuda:0')
c= tensor(1.6085e+09, device='cuda:0')
c= tensor(1.6102e+09, device='cuda:0')
c= tensor(1.6384e+09, device='cuda:0')
c= tensor(1.6391e+09, device='cuda:0')
c= tensor(1.6395e+09, device='cuda:0')
c= tensor(1.6395e+09, device='cuda:0')
c= tensor(1.6399e+09, device='cuda:0')
c= tensor(1.6400e+09, device='cuda:0')
c= tensor(1.6456e+09, device='cuda:0')
c= tensor(1.7157e+09, device='cuda:0')
c= tensor(1.7213e+09, device='cuda:0')
c= tensor(1.7305e+09, device='cuda:0')
c= tensor(1.7319e+09, device='cuda:0')
c= tensor(1.7323e+09, device='cuda:0')
c= tensor(1.7326e+09, device='cuda:0')
c= tensor(1.7326e+09, device='cuda:0')
c= tensor(1.7326e+09, device='cuda:0')
c= tensor(1.7398e+09, device='cuda:0')
c= tensor(1.7402e+09, device='cuda:0')
c= tensor(1.7487e+09, device='cuda:0')
c= tensor(1.7571e+09, device='cuda:0')
c= tensor(1.7574e+09, device='cuda:0')
c= tensor(1.7574e+09, device='cuda:0')
c= tensor(1.7611e+09, device='cuda:0')
c= tensor(1.7618e+09, device='cuda:0')
c= tensor(1.7618e+09, device='cuda:0')
c= tensor(1.7793e+09, device='cuda:0')
c= tensor(1.7799e+09, device='cuda:0')
c= tensor(1.7809e+09, device='cuda:0')
c= tensor(1.7809e+09, device='cuda:0')
c= tensor(1.7809e+09, device='cuda:0')
c= tensor(1.7809e+09, device='cuda:0')
c= tensor(1.7809e+09, device='cuda:0')
c= tensor(1.7810e+09, device='cuda:0')
c= tensor(1.7813e+09, device='cuda:0')
c= tensor(3.2870e+09, device='cuda:0')
c= tensor(3.2871e+09, device='cuda:0')
c= tensor(3.2874e+09, device='cuda:0')
c= tensor(3.2874e+09, device='cuda:0')
c= tensor(3.2874e+09, device='cuda:0')
c= tensor(3.2875e+09, device='cuda:0')
c= tensor(3.3017e+09, device='cuda:0')
c= tensor(3.3017e+09, device='cuda:0')
c= tensor(3.3692e+09, device='cuda:0')
c= tensor(3.3692e+09, device='cuda:0')
c= tensor(3.3762e+09, device='cuda:0')
c= tensor(3.3777e+09, device='cuda:0')
c= tensor(3.3815e+09, device='cuda:0')
c= tensor(3.3930e+09, device='cuda:0')
c= tensor(3.3930e+09, device='cuda:0')
c= tensor(3.3930e+09, device='cuda:0')
c= tensor(3.3936e+09, device='cuda:0')
c= tensor(3.3937e+09, device='cuda:0')
c= tensor(3.3938e+09, device='cuda:0')
c= tensor(3.3949e+09, device='cuda:0')
c= tensor(3.3950e+09, device='cuda:0')
c= tensor(3.3955e+09, device='cuda:0')
c= tensor(3.3957e+09, device='cuda:0')
c= tensor(3.4078e+09, device='cuda:0')
c= tensor(3.4300e+09, device='cuda:0')
c= tensor(3.4303e+09, device='cuda:0')
c= tensor(3.4303e+09, device='cuda:0')
c= tensor(3.4332e+09, device='cuda:0')
c= tensor(3.4337e+09, device='cuda:0')
c= tensor(3.4339e+09, device='cuda:0')
c= tensor(3.4344e+09, device='cuda:0')
c= tensor(3.4350e+09, device='cuda:0')
c= tensor(3.4355e+09, device='cuda:0')
c= tensor(3.4399e+09, device='cuda:0')
c= tensor(3.4410e+09, device='cuda:0')
c= tensor(3.4411e+09, device='cuda:0')
c= tensor(3.4411e+09, device='cuda:0')
c= tensor(3.4411e+09, device='cuda:0')
c= tensor(3.4430e+09, device='cuda:0')
c= tensor(3.4451e+09, device='cuda:0')
c= tensor(3.4533e+09, device='cuda:0')
c= tensor(3.4533e+09, device='cuda:0')
c= tensor(3.4534e+09, device='cuda:0')
c= tensor(3.4537e+09, device='cuda:0')
c= tensor(3.4621e+09, device='cuda:0')
c= tensor(3.4627e+09, device='cuda:0')
c= tensor(3.4633e+09, device='cuda:0')
c= tensor(3.4635e+09, device='cuda:0')
c= tensor(3.4638e+09, device='cuda:0')
c= tensor(3.4638e+09, device='cuda:0')
c= tensor(3.4638e+09, device='cuda:0')
c= tensor(3.4644e+09, device='cuda:0')
c= tensor(3.4646e+09, device='cuda:0')
c= tensor(3.4648e+09, device='cuda:0')
c= tensor(3.4648e+09, device='cuda:0')
c= tensor(3.4648e+09, device='cuda:0')
c= tensor(3.4651e+09, device='cuda:0')
c= tensor(3.4654e+09, device='cuda:0')
c= tensor(3.4655e+09, device='cuda:0')
c= tensor(3.4655e+09, device='cuda:0')
c= tensor(3.4655e+09, device='cuda:0')
c= tensor(3.4657e+09, device='cuda:0')
c= tensor(3.4658e+09, device='cuda:0')
c= tensor(3.4680e+09, device='cuda:0')
c= tensor(3.4680e+09, device='cuda:0')
c= tensor(3.4680e+09, device='cuda:0')
c= tensor(3.4682e+09, device='cuda:0')
c= tensor(3.4682e+09, device='cuda:0')
c= tensor(3.4762e+09, device='cuda:0')
c= tensor(3.4762e+09, device='cuda:0')
c= tensor(3.4774e+09, device='cuda:0')
c= tensor(3.4795e+09, device='cuda:0')
c= tensor(3.4816e+09, device='cuda:0')
c= tensor(3.4856e+09, device='cuda:0')
c= tensor(3.4876e+09, device='cuda:0')
c= tensor(3.4876e+09, device='cuda:0')
c= tensor(3.4879e+09, device='cuda:0')
c= tensor(3.4880e+09, device='cuda:0')
c= tensor(3.4886e+09, device='cuda:0')
c= tensor(3.4911e+09, device='cuda:0')
c= tensor(3.4913e+09, device='cuda:0')
c= tensor(3.4926e+09, device='cuda:0')
c= tensor(3.4926e+09, device='cuda:0')
c= tensor(3.4938e+09, device='cuda:0')
c= tensor(3.4939e+09, device='cuda:0')
c= tensor(3.4939e+09, device='cuda:0')
c= tensor(3.5071e+09, device='cuda:0')
c= tensor(3.5949e+09, device='cuda:0')
c= tensor(3.5951e+09, device='cuda:0')
c= tensor(3.5951e+09, device='cuda:0')
c= tensor(3.5952e+09, device='cuda:0')
c= tensor(3.5965e+09, device='cuda:0')
c= tensor(3.5966e+09, device='cuda:0')
c= tensor(3.5967e+09, device='cuda:0')
c= tensor(3.5971e+09, device='cuda:0')
c= tensor(3.5997e+09, device='cuda:0')
c= tensor(3.5997e+09, device='cuda:0')
c= tensor(3.6010e+09, device='cuda:0')
c= tensor(3.6010e+09, device='cuda:0')
time to make c is 11.305237531661987
time for making loss is 11.305336475372314
p0 True
it  0 : 1449128448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
4527865856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 10% |
memory (bytes)
4528001024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  636783360.0
relative error loss 0.17683606
shape of L is 
torch.Size([])
memory (bytes)
4554907648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
memory (bytes)
4554907648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% | 10% |
error is  625407740.0
relative error loss 0.17367703
shape of L is 
torch.Size([])
memory (bytes)
4558516224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4558516224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  605608450.0
relative error loss 0.16817872
shape of L is 
torch.Size([])
memory (bytes)
4561825792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 10% |
memory (bytes)
4561829888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  597275400.0
relative error loss 0.16586462
shape of L is 
torch.Size([])
memory (bytes)
4564717568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4564959232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  588654100.0
relative error loss 0.16347045
shape of L is 
torch.Size([])
memory (bytes)
4568276992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 10% |
memory (bytes)
4568276992
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  580875500.0
relative error loss 0.16131033
shape of L is 
torch.Size([])
memory (bytes)
4571181056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
4571181056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  575460860.0
relative error loss 0.15980667
shape of L is 
torch.Size([])
memory (bytes)
4574732288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 10% |
memory (bytes)
4574732288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  570846700.0
relative error loss 0.15852532
shape of L is 
torch.Size([])
memory (bytes)
4577898496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4577898496
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  567852300.0
relative error loss 0.15769376
shape of L is 
torch.Size([])
memory (bytes)
4581175296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4581175296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  565463300.0
relative error loss 0.15703033
time to take a step is 290.8056378364563
it  1 : 1913411072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
4584271872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 10% |
memory (bytes)
4584271872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  565463300.0
relative error loss 0.15703033
shape of L is 
torch.Size([])
memory (bytes)
4587442176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4587442176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% | 10% |
error is  562019300.0
relative error loss 0.15607393
shape of L is 
torch.Size([])
memory (bytes)
4590841856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
4590841856
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  561275650.0
relative error loss 0.15586741
shape of L is 
torch.Size([])
memory (bytes)
4594044928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
memory (bytes)
4594044928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  558575100.0
relative error loss 0.15511747
shape of L is 
torch.Size([])
memory (bytes)
4597272576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 10% |
memory (bytes)
4597272576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  557541400.0
relative error loss 0.1548304
shape of L is 
torch.Size([])
memory (bytes)
4600475648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4600475648
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  556026400.0
relative error loss 0.15440968
shape of L is 
torch.Size([])
memory (bytes)
4603727872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 10% |
memory (bytes)
4603727872
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  554348800.0
relative error loss 0.1539438
shape of L is 
torch.Size([])
memory (bytes)
4606935040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 10% |
memory (bytes)
4606935040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  552871200.0
relative error loss 0.15353346
shape of L is 
torch.Size([])
memory (bytes)
4610158592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
4610158592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  551915500.0
relative error loss 0.15326808
shape of L is 
torch.Size([])
memory (bytes)
4613378048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4613378048
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  550744600.0
relative error loss 0.15294291
time to take a step is 282.92626786231995
it  2 : 1913411072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
4616597504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
memory (bytes)
4616597504
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  550744600.0
relative error loss 0.15294291
shape of L is 
torch.Size([])
memory (bytes)
4619812864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
memory (bytes)
4619812864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  549353700.0
relative error loss 0.15255666
shape of L is 
torch.Size([])
memory (bytes)
4623044608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4623044608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  548375040.0
relative error loss 0.15228488
shape of L is 
torch.Size([])
memory (bytes)
4626259968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
4626259968
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  547739900.0
relative error loss 0.1521085
shape of L is 
torch.Size([])
memory (bytes)
4629483520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 10% |
memory (bytes)
4629483520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  547160600.0
relative error loss 0.15194762
shape of L is 
torch.Size([])
memory (bytes)
4632555520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
memory (bytes)
4632555520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  546340600.0
relative error loss 0.15171991
shape of L is 
torch.Size([])
memory (bytes)
4635922432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4635922432
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  545745150.0
relative error loss 0.15155455
shape of L is 
torch.Size([])
memory (bytes)
4639014912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 10% |
memory (bytes)
4639014912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  545047800.0
relative error loss 0.1513609
shape of L is 
torch.Size([])
memory (bytes)
4642361344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 10% |
memory (bytes)
4642361344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  544529400.0
relative error loss 0.15121694
shape of L is 
torch.Size([])
memory (bytes)
4645351424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
4645588992
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  543994400.0
relative error loss 0.15106836
time to take a step is 291.38211727142334
it  3 : 1913411072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
4648808448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 10% |
memory (bytes)
4648808448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  543994400.0
relative error loss 0.15106836
shape of L is 
torch.Size([])
memory (bytes)
4651896832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
4651896832
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  544008200.0
relative error loss 0.1510722
shape of L is 
torch.Size([])
memory (bytes)
4655243264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 10% |
memory (bytes)
4655243264
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  543519200.0
relative error loss 0.15093641
shape of L is 
torch.Size([])
memory (bytes)
4658450432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4658450432
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  542932500.0
relative error loss 0.15077347
shape of L is 
torch.Size([])
memory (bytes)
4661456896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4661694464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  542520800.0
relative error loss 0.15065916
shape of L is 
torch.Size([])
memory (bytes)
4664918016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
4664918016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  542125060.0
relative error loss 0.15054925
shape of L is 
torch.Size([])
memory (bytes)
4667768832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4667981824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  541736700.0
relative error loss 0.15044141
shape of L is 
torch.Size([])
memory (bytes)
4671180800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
4671365120
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  541335040.0
relative error loss 0.15032986
shape of L is 
torch.Size([])
memory (bytes)
4674576384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4674576384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  541254400.0
relative error loss 0.15030746
shape of L is 
torch.Size([])
memory (bytes)
4677636096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4677795840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  540821250.0
relative error loss 0.15018718
time to take a step is 286.79422903060913
it  4 : 1913411072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
4681011200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
4681011200
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  540821250.0
relative error loss 0.15018718
shape of L is 
torch.Size([])
memory (bytes)
4684230656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4684230656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  540611600.0
relative error loss 0.15012896
shape of L is 
torch.Size([])
memory (bytes)
4687458304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4687458304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  540359400.0
relative error loss 0.15005893
shape of L is 
torch.Size([])
memory (bytes)
4690677760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4690677760
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  540153100.0
relative error loss 0.15000163
shape of L is 
torch.Size([])
memory (bytes)
4693893120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 10% |
memory (bytes)
4693893120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  539854850.0
relative error loss 0.14991881
shape of L is 
torch.Size([])
memory (bytes)
4697108480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 10% |
memory (bytes)
4697108480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  539564000.0
relative error loss 0.14983805
shape of L is 
torch.Size([])
memory (bytes)
4700332032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
4700332032
| ID | GPU  | MEM |
-------------------
|  0 |   2% |  0% |
|  1 | 100% | 10% |
error is  539358700.0
relative error loss 0.14978103
shape of L is 
torch.Size([])
memory (bytes)
4703543296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4703543296
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  538981400.0
relative error loss 0.14967625
shape of L is 
torch.Size([])
memory (bytes)
4706762752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4706762752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  538686700.0
relative error loss 0.14959441
shape of L is 
torch.Size([])
memory (bytes)
4709982208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
4709982208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  538523650.0
relative error loss 0.14954913
time to take a step is 287.68194580078125
it  5 : 1913411072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
4713189376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4713189376
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  538523650.0
relative error loss 0.14954913
shape of L is 
torch.Size([])
memory (bytes)
4716351488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 10% |
memory (bytes)
4716412928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  538371300.0
relative error loss 0.14950682
shape of L is 
torch.Size([])
memory (bytes)
4719640576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4719640576
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  538105340.0
relative error loss 0.14943297
shape of L is 
torch.Size([])
memory (bytes)
4722860032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 10% |
memory (bytes)
4722860032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  538058500.0
relative error loss 0.14941995
shape of L is 
torch.Size([])
memory (bytes)
4726075392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4726075392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% | 10% |
error is  537727500.0
relative error loss 0.14932804
shape of L is 
torch.Size([])
memory (bytes)
4729139200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
4729286656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  537608200.0
relative error loss 0.1492949
shape of L is 
torch.Size([])
memory (bytes)
4732383232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4732518400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  537422340.0
relative error loss 0.1492433
shape of L is 
torch.Size([])
memory (bytes)
4735737856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4735737856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% | 10% |
error is  537252600.0
relative error loss 0.14919616
shape of L is 
torch.Size([])
memory (bytes)
4738809856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
4738809856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  537061400.0
relative error loss 0.14914306
shape of L is 
torch.Size([])
memory (bytes)
4742189056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4742189056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  536930300.0
relative error loss 0.14910665
time to take a step is 286.7776596546173
it  6 : 1913411072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
4745408512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
memory (bytes)
4745408512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  536930300.0
relative error loss 0.14910665
shape of L is 
torch.Size([])
memory (bytes)
4748603392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 10% |
memory (bytes)
4748623872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  536770800.0
relative error loss 0.14906237
shape of L is 
torch.Size([])
memory (bytes)
4751839232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4751839232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  536591100.0
relative error loss 0.14901246
shape of L is 
torch.Size([])
memory (bytes)
4754939904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 10% |
memory (bytes)
4754939904
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  536378880.0
relative error loss 0.14895353
shape of L is 
torch.Size([])
memory (bytes)
4758269952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 10% |
memory (bytes)
4758269952
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  536207600.0
relative error loss 0.14890596
shape of L is 
torch.Size([])
memory (bytes)
4761464832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
4761464832
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  536076300.0
relative error loss 0.1488695
shape of L is 
torch.Size([])
memory (bytes)
4764712960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
4764712960
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  535958270.0
relative error loss 0.14883672
shape of L is 
torch.Size([])
memory (bytes)
4767924224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
4767924224
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  535802370.0
relative error loss 0.14879343
shape of L is 
torch.Size([])
memory (bytes)
4771119104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
4771119104
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  535719170.0
relative error loss 0.14877032
shape of L is 
torch.Size([])
memory (bytes)
4774252544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
4774354944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  535635200.0
relative error loss 0.148747
time to take a step is 289.9920563697815
it  7 : 1913411072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
4777582592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
4777582592
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  535635200.0
relative error loss 0.148747
shape of L is 
torch.Size([])
memory (bytes)
4780707840
| ID | GPU | MEM |
------------------
|  0 |  2% |  0% |
|  1 | 15% | 10% |
memory (bytes)
4780707840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  535499780.0
relative error loss 0.14870939
shape of L is 
torch.Size([])
memory (bytes)
4784021504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4784021504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  535400450.0
relative error loss 0.1486818
shape of L is 
torch.Size([])
memory (bytes)
4787159040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 10% |
memory (bytes)
4787159040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  535332100.0
relative error loss 0.14866284
shape of L is 
torch.Size([])
memory (bytes)
4790439936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
4790439936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  535249150.0
relative error loss 0.1486398
shape of L is 
torch.Size([])
memory (bytes)
4793679872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 10% |
memory (bytes)
4793679872
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  535151360.0
relative error loss 0.14861263
shape of L is 
torch.Size([])
memory (bytes)
4796792832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4796895232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  535081730.0
relative error loss 0.1485933
shape of L is 
torch.Size([])
memory (bytes)
4800114688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 10% |
memory (bytes)
4800114688
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  534985730.0
relative error loss 0.14856665
shape of L is 
torch.Size([])
memory (bytes)
4803207168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 10% |
memory (bytes)
4803207168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  534915840.0
relative error loss 0.14854723
shape of L is 
torch.Size([])
memory (bytes)
4806541312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4806541312
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  534839300.0
relative error loss 0.14852598
time to take a step is 290.2452871799469
it  8 : 1913411072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
4809773056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4809773056
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  534839300.0
relative error loss 0.14852598
shape of L is 
torch.Size([])
memory (bytes)
4812992512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
4812992512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  534785800.0
relative error loss 0.14851111
shape of L is 
torch.Size([])
memory (bytes)
4816216064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4816216064
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  534636030.0
relative error loss 0.14846954
shape of L is 
torch.Size([])
memory (bytes)
4819410944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4819431424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  534577920.0
relative error loss 0.1484534
shape of L is 
torch.Size([])
memory (bytes)
4822495232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4822650880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  534502900.0
relative error loss 0.14843257
shape of L is 
torch.Size([])
memory (bytes)
4825866240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
4825866240
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  534401280.0
relative error loss 0.14840434
shape of L is 
torch.Size([])
memory (bytes)
4829007872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
memory (bytes)
4829007872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  534303230.0
relative error loss 0.1483771
shape of L is 
torch.Size([])
memory (bytes)
4832305152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 10% |
memory (bytes)
4832305152
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  534230530.0
relative error loss 0.14835691
shape of L is 
torch.Size([])
memory (bytes)
4835364864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4835364864
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  534154750.0
relative error loss 0.14833587
shape of L is 
torch.Size([])
memory (bytes)
4838735872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4838735872
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  534049540.0
relative error loss 0.14830665
time to take a step is 287.0887842178345
it  9 : 1913411072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
4841951232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
4841951232
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  534049540.0
relative error loss 0.14830665
shape of L is 
torch.Size([])
memory (bytes)
4845137920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4845137920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  533954050.0
relative error loss 0.14828014
shape of L is 
torch.Size([])
memory (bytes)
4848381952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 10% |
memory (bytes)
4848381952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  533896200.0
relative error loss 0.14826408
shape of L is 
torch.Size([])
memory (bytes)
4851593216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4851593216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  533839870.0
relative error loss 0.14824843
shape of L is 
torch.Size([])
memory (bytes)
4854824960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4854824960
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  533778430.0
relative error loss 0.14823137
shape of L is 
torch.Size([])
memory (bytes)
4857892864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
memory (bytes)
4857892864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  533683700.0
relative error loss 0.14820507
shape of L is 
torch.Size([])
memory (bytes)
4861272064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
4861272064
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  533642750.0
relative error loss 0.14819369
shape of L is 
torch.Size([])
memory (bytes)
4864471040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
memory (bytes)
4864471040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  533557500.0
relative error loss 0.14817002
shape of L is 
torch.Size([])
memory (bytes)
4867719168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4867719168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  533547780.0
relative error loss 0.14816731
shape of L is 
torch.Size([])
memory (bytes)
4870942720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 10% |
memory (bytes)
4870942720
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  533446900.0
relative error loss 0.14813931
time to take a step is 298.9362967014313
it  10 : 1913411072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
4874182656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
4874182656
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  533446900.0
relative error loss 0.14813931
shape of L is 
torch.Size([])
memory (bytes)
4877410304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 10% |
memory (bytes)
4877410304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  533400580.0
relative error loss 0.14812644
shape of L is 
torch.Size([])
memory (bytes)
4880580608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 10% |
memory (bytes)
4880580608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  533345020.0
relative error loss 0.14811102
shape of L is 
torch.Size([])
memory (bytes)
4883857408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4883857408
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  533287680.0
relative error loss 0.14809509
shape of L is 
torch.Size([])
memory (bytes)
4887068672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 10% |
memory (bytes)
4887068672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  533218300.0
relative error loss 0.14807582
shape of L is 
torch.Size([])
memory (bytes)
4890296320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4890296320
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  533161470.0
relative error loss 0.14806004
shape of L is 
torch.Size([])
memory (bytes)
4893462528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 10% |
memory (bytes)
4893462528
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  533085950.0
relative error loss 0.14803907
shape of L is 
torch.Size([])
memory (bytes)
4896739328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
4896739328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  533089540.0
relative error loss 0.14804007
shape of L is 
torch.Size([])
memory (bytes)
4899938304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 10% |
memory (bytes)
4899938304
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  533042940.0
relative error loss 0.14802712
shape of L is 
torch.Size([])
memory (bytes)
4903186432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 10% |
memory (bytes)
4903186432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  532995070.0
relative error loss 0.14801383
time to take a step is 304.11906933784485
it  11 : 1913411072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
4906352640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4906352640
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  532995070.0
relative error loss 0.14801383
shape of L is 
torch.Size([])
memory (bytes)
4909629440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
4909629440
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  532929020.0
relative error loss 0.14799549
shape of L is 
torch.Size([])
memory (bytes)
4912824320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4912824320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 10% |
error is  532899070.0
relative error loss 0.14798717
shape of L is 
torch.Size([])
memory (bytes)
4916072448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4916072448
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  532856580.0
relative error loss 0.14797537
shape of L is 
torch.Size([])
memory (bytes)
4919304192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
4919304192
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  532809200.0
relative error loss 0.14796221
shape of L is 
torch.Size([])
memory (bytes)
4922519552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 10% |
memory (bytes)
4922519552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  532774900.0
relative error loss 0.14795269
shape of L is 
torch.Size([])
memory (bytes)
4925739008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4925739008
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  532710900.0
relative error loss 0.14793491
shape of L is 
torch.Size([])
memory (bytes)
4928933888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
memory (bytes)
4928933888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  532666880.0
relative error loss 0.1479227
shape of L is 
torch.Size([])
memory (bytes)
4932177920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 10% |
memory (bytes)
4932177920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  532632320.0
relative error loss 0.1479131
shape of L is 
torch.Size([])
memory (bytes)
4935368704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 10% |
memory (bytes)
4935368704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  532599300.0
relative error loss 0.14790392
time to take a step is 295.0700831413269
it  12 : 1913411072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
4938612736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
memory (bytes)
4938612736
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  532599300.0
relative error loss 0.14790392
shape of L is 
torch.Size([])
memory (bytes)
4941836288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4941836288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  532563460.0
relative error loss 0.14789397
shape of L is 
torch.Size([])
memory (bytes)
4945002496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
memory (bytes)
4945002496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  532520450.0
relative error loss 0.14788203
shape of L is 
torch.Size([])
memory (bytes)
4948287488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 10% |
memory (bytes)
4948287488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  532471040.0
relative error loss 0.1478683
shape of L is 
torch.Size([])
memory (bytes)
4951478272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 10% |
memory (bytes)
4951478272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  532427260.0
relative error loss 0.14785615
shape of L is 
torch.Size([])
memory (bytes)
4954730496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4954730496
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  532393730.0
relative error loss 0.14784683
shape of L is 
torch.Size([])
memory (bytes)
4957888512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 10% |
memory (bytes)
4957888512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  532381700.0
relative error loss 0.1478435
shape of L is 
torch.Size([])
memory (bytes)
4961124352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4961124352
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  532314370.0
relative error loss 0.1478248
shape of L is 
torch.Size([])
memory (bytes)
4964388864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4964388864
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  532295170.0
relative error loss 0.14781947
shape of L is 
torch.Size([])
memory (bytes)
4967452672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
4967612416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  532256000.0
relative error loss 0.1478086
time to take a step is 304.87122082710266
it  13 : 1913411072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
4970831872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
memory (bytes)
4970831872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% | 10% |
error is  532256000.0
relative error loss 0.1478086
shape of L is 
torch.Size([])
memory (bytes)
4974026752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
memory (bytes)
4974026752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  532205300.0
relative error loss 0.14779451
shape of L is 
torch.Size([])
memory (bytes)
4977274880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
memory (bytes)
4977274880
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  532148480.0
relative error loss 0.14777873
shape of L is 
torch.Size([])
memory (bytes)
4980498432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
memory (bytes)
4980498432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% | 10% |
error is  532093200.0
relative error loss 0.14776337
shape of L is 
torch.Size([])
memory (bytes)
4983611392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
memory (bytes)
4983611392
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  532045570.0
relative error loss 0.14775015
shape of L is 
torch.Size([])
memory (bytes)
4986941440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 10% |
memory (bytes)
4986941440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  531992580.0
relative error loss 0.14773543
shape of L is 
torch.Size([])
memory (bytes)
4990050304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4990050304
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  531952400.0
relative error loss 0.14772427
shape of L is 
torch.Size([])
memory (bytes)
4993376256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
memory (bytes)
4993376256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  531914500.0
relative error loss 0.14771375
shape of L is 
torch.Size([])
memory (bytes)
4996603904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 10% |
memory (bytes)
4996603904
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  531885820.0
relative error loss 0.1477058
shape of L is 
torch.Size([])
memory (bytes)
4999712768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 10% |
memory (bytes)
4999819264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  531857150.0
relative error loss 0.14769784
time to take a step is 304.6499083042145
it  14 : 1913411072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
5003046912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 10% |
memory (bytes)
5003046912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  531857150.0
relative error loss 0.14769784
shape of L is 
torch.Size([])
memory (bytes)
5006266368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
5006266368
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  531828740.0
relative error loss 0.14768994
shape of L is 
torch.Size([])
memory (bytes)
5009477632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
5009477632
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  531813380.0
relative error loss 0.14768568
shape of L is 
torch.Size([])
memory (bytes)
5012692992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
5012692992
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  531796480.0
relative error loss 0.14768098
shape of L is 
torch.Size([])
memory (bytes)
5015810048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
memory (bytes)
5015920640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  531779070.0
relative error loss 0.14767614
shape of L is 
torch.Size([])
memory (bytes)
5019144192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
5019144192
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  531767040.0
relative error loss 0.1476728
shape of L is 
torch.Size([])
memory (bytes)
5022191616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 10% |
memory (bytes)
5022363648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  531746560.0
relative error loss 0.14766712
shape of L is 
torch.Size([])
memory (bytes)
5025583104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 10% |
memory (bytes)
5025583104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  531732220.0
relative error loss 0.14766313
shape of L is 
torch.Size([])
memory (bytes)
5028786176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
5028786176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  531720960.0
relative error loss 0.14766
shape of L is 
torch.Size([])
memory (bytes)
5032030208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
5032030208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  531705600.0
relative error loss 0.14765574
time to take a step is 308.99545526504517
sum tnnu_Z after tensor(9694798., device='cuda:0')
shape of features
(4573,)
shape of features
(4573,)
number of orig particles 18293
number of new particles after remove low mass 17211
tnuZ shape should be parts x labs
torch.Size([18293, 702])
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
error without small mass is  636742140.0
relative error without small mass is  0.17682461
nnu_Z shape should be number of particles by maxV
(18293, 702)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
shape of features
(18293,)
Thu Feb 2 03:05:42 EST 2023
