Tue Jan 31 13:32:49 EST 2023
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  0% |  0% |
numbers of X: 32611887
numbers of Z: 21003
shape of features
(21003,)
shape of features
(21003,)
ZX	Vol	Parts	Cubes	Eps
Z	0.016007120252001284	21003	21.003	0.09134342498021986
X	0.014192576124224877	1019	1.019	0.24060003311010345
X	0.014351535109623077	18353	18.353	0.09212915709081916
X	0.013871170300101302	1527	1.527	0.20865302668087304
X	0.014560668964689064	12450	12.45	0.10535876202160342
X	0.013952563380164061	27836	27.836	0.0794356997116464
X	0.014779418322950369	38512	38.512	0.07266976508483812
X	0.014942455338859472	41987	41.987	0.07086563893096395
X	0.015013251492051086	35136	35.136	0.07531950903116497
X	0.014021065654703027	18026	18.026	0.09196598331059588
X	0.013638214102530535	19953	19.953	0.08808793594547186
X	0.01351926699206178	8909	8.909	0.11491447272892602
X	0.013931607659409673	52997	52.997	0.0640594663448743
X	0.013565813795125045	6335	6.335	0.12889411159304642
X	0.015558773446723488	245118	245.118	0.03989024627196858
X	0.013635268504096276	21512	21.512	0.08590022506975861
X	0.013651203920014862	29598	29.598	0.0772626374897345
X	0.015622525988313865	50358	50.358	0.0676956756927169
X	0.014391403207767502	18747	18.747	0.09156379560515152
X	0.015842273334346412	92525	92.525	0.05552895102233932
X	0.015687882012511357	73733	73.733	0.059699056838693206
X	0.013499551277749468	33277	33.277	0.07402726802624168
X	0.0155470216686486	138879	138.879	0.04819517272394324
X	0.013755424938100948	8261	8.261	0.11852604591954528
X	0.01586649569799455	31253	31.253	0.07977431838061842
X	0.014808539150463132	2374	2.374	0.18408165111961902
X	0.015186400100517087	44582	44.582	0.06983906718511695
X	0.01549255961495519	45942	45.942	0.06960457074539615
X	0.013739867616390766	12542	12.542	0.10308732032574863
X	0.0154484516551334	121001	121.001	0.05035377089081034
X	0.015805707248653947	804298	804.298	0.026985614685348682
X	0.013486270967597282	11728	11.728	0.10476654967850389
X	0.01596743082095537	783298	783.298	0.027317189069065134
X	0.013547314415282726	16018	16.018	0.09456889336498764
X	0.014770878945122445	11216	11.216	0.10961149792484903
X	0.013646067070121731	18162	18.162	0.09091061799854759
X	0.01582715909708648	236313	236.313	0.04061051074079138
X	0.015810725160526756	65354	65.354	0.06231030425214523
X	0.013225316847234133	769	0.769	0.2581229274327997
X	0.014781108016979032	2956	2.956	0.17100186763798592
X	0.013392557467411024	2511	2.511	0.17471851720991818
X	0.014835083719467325	2374	2.374	0.18419157533994318
X	0.014400942862468812	1499	1.499	0.21258360365313778
X	0.013008422004661322	577	0.577	0.2824985793007903
X	0.013231021018945618	2962	2.962	0.16469083636013382
X	0.013543168062164752	745	0.745	0.26293879301003464
X	0.013767725136289993	772	0.772	0.26126523757552095
X	0.014082028938873834	1988	1.988	0.1920508723234773
X	0.013450101968328895	3257	3.257	0.16043628833156737
X	0.014077310539112814	3265	3.265	0.162759183243774
X	0.014156337339849744	9541	9.541	0.1140562273901104
X	0.014230969878465146	11313	11.313	0.10794906973490538
X	0.01368862503790743	1040	1.04	0.2361069816814206
X	0.013601442753648364	4497	4.497	0.14461744911263213
X	0.014223109716962177	2839	2.839	0.171110393179986
X	0.013865454866809172	3518	3.518	0.15795962551736323
X	0.014626968586219707	5296	5.296	0.1403035901475526
X	0.013556993116175768	1426	1.426	0.21184324322845233
X	0.013609326861799352	4519	4.519	0.14441027704945436
X	0.014964018237128417	2182	2.182	0.18999018290870018
X	0.014648072837892333	3205	3.205	0.1659526349198213
X	0.014665383244378756	4119	4.119	0.15269818497749832
X	0.01358034712067376	1062	1.062	0.23384537492938387
X	0.015527627564051094	4671	4.671	0.14924452662560594
X	0.013730163263324005	7994	7.994	0.11975781192664486
X	0.014593142523272772	2581	2.581	0.1781504566145205
X	0.013843577720664432	1808	1.808	0.19709849390744727
X	0.014049162106445391	2110	2.11	0.18812910502110503
X	0.014658564018224053	2221	2.221	0.1875775920571417
X	0.01360163632022535	3944	3.944	0.15108386421548461
X	0.013520665907080362	4252	4.252	0.14705109922093287
X	0.013810771691966614	4419	4.419	0.14620581570528238
X	0.014276913165804619	4322	4.322	0.14893034144083794
X	0.014430015685137947	1991	1.991	0.19352267019081842
X	0.013148968214888016	4352	4.352	0.14456647654669866
X	0.014255573847932083	4165	4.165	0.1507034632718705
X	0.013504651468428138	9199	9.199	0.11365300433541053
X	0.014788457425300612	600	0.6	0.2910206959775803
X	0.013244501224448611	918	0.918	0.243443196706808
X	0.013510592761405272	2089	2.089	0.18631392764588384
X	0.013831016401895185	8696	8.696	0.11672884502744929
X	0.0134084224261763	1864	1.864	0.1930384960007332
X	0.014459182189486892	1571	1.571	0.20956681594214996
X	0.013790115033996199	2020	2.02	0.18970211512507723
X	0.013361237598703342	1876	1.876	0.19239980061836157
X	0.013509424396644421	1327	1.327	0.21673146873736773
X	0.013150932261762047	944	0.944	0.24061803851596208
X	0.014336806684810974	2329	2.329	0.18327123802901582
X	0.013484155501934419	711	0.711	0.266676481938747
X	0.015213227712386307	2767	2.767	0.17649703021743077
X	0.013548806684942896	2673	2.673	0.17177792901613584
X	0.014106422488843326	4113	4.113	0.150806267948008
X	0.013219683585041695	1450	1.45	0.20890605090507555
X	0.014732778016797546	1240	1.24	0.22818529485513628
X	0.01345289419765068	6440	6.44	0.12783308986694528
X	0.013446285245587566	3617	3.617	0.15491183691408755
X	0.01397830754542896	2804	2.804	0.1708279230519936
X	0.013868840774343421	1750	1.75	0.1993734716191857
X	0.014570777914961903	6042	6.042	0.13410161432670956
X	0.014797954671015396	8157	8.157	0.12196177699458481
X	0.013378514848986882	3093	3.093	0.16293310291313903
X	0.01437041499808291	8046	8.046	0.12132857419468276
X	0.014291479107634984	5481	5.481	0.1376382447424247
X	0.014353482043558683	4084	4.084	0.15203976125625512
X	0.013591039563350242	1116	1.116	0.23007150179723804
X	0.014177938064818629	5650	5.65	0.13589022227235448
X	0.01344917916969321	1671	1.671	0.20040402651925704
X	0.013596452893207519	3114	3.114	0.16344400841906864
X	0.013407773034523243	961	0.961	0.24073787388211626
X	0.013377383697081559	2055	2.055	0.18671811424000678
X	0.013263732952660607	666	0.666	0.2710590093307606
X	0.014429676564768287	2216	2.216	0.186736384895651
X	0.012876034978109055	1935	1.935	0.18809058598920134
X	0.013352560955683202	1888	1.888	0.19194973921919786
X	0.013561841249614882	2731	2.731	0.17060784032857726
X	0.014260825373784812	4612	4.612	0.1456862130426282
X	0.014615785318672426	2872	2.872	0.1720069149519014
X	0.014971207871620401	1955	1.955	0.19710759528804345
X	0.013293684266454199	635	0.635	0.2756071910392662
X	0.014909292604235257	5723	5.723	0.13759767555690308
X	0.013148906079504684	749	0.749	0.2598977991570559
X	0.015068440217753805	6805	6.805	0.130340702271998
X	0.013558948827934325	1482	1.482	0.20915067603449936
X	0.013478923105304765	1840	1.84	0.19421337001254077
X	0.01491229707517689	2454	2.454	0.18248338548423323
X	0.014459344380680363	2130	2.13	0.18934608622015162
X	0.015391666713685841	2814	2.814	0.17619240589889312
X	0.013574618422638421	916	0.916	0.2456277248967488
X	0.013705755997771728	3114	3.114	0.16388082032725118
X	0.014475203018887644	6650	6.65	0.1295988077022097
X	0.014838998197878593	1597	1.597	0.21023251638962812
X	0.014147435148538884	5900	5.9	0.1338469769527182
X	0.014217533624182587	1221	1.221	0.22665720727156674
X	0.013877895683211162	3167	3.167	0.16364089712992547
X	0.01403037735906875	1223	1.223	0.22553518185421326
X	0.013388032264228047	1464	1.464	0.20911822848785058
X	0.013591234397296459	1729	1.729	0.19883277315344902
X	0.013403939608412714	1628	1.628	0.20192620689410726
X	0.014717455272573678	1143	1.143	0.2343844693966887
X	0.014552536460900014	1599	1.599	0.208783774206259
X	0.013472623858581938	1591	1.591	0.20382651002150734
X	0.013447344853907054	2167	2.167	0.1837634815619635
X	0.01328671241874769	1043	1.043	0.23354885488147292
X	0.013577814096260236	5638	5.638	0.13404015731435287
X	0.013609497828023934	9086	9.086	0.11441679791753857
X	0.015315715731827112	3512	3.512	0.1633782587969091
X	0.01384983156644464	1230	1.23	0.22413677913135438
X	0.013665578218906113	1141	1.141	0.22879548817811038
X	0.015681419274216016	1186	1.186	0.23646501315073046
X	0.01329378116916842	1322	1.322	0.21584352424762962
X	0.013574284426498127	1963	1.963	0.19051633358620812
X	0.014898917200685523	2341	2.341	0.1853180681742198
X	0.0136634244317088	2221	2.221	0.18323298932182872
X	0.013523498741191894	2825	2.825	0.1685350383735327
X	0.0134958986988553	6955	6.955	0.12472922536171017
X	0.01436714494432114	2843	2.843	0.17160550060085072
X	0.01501054372471597	15533	15.533	0.09886601451151271
X	0.014603920006464977	2066	2.066	0.19191682169948196
X	0.013547162710537805	2352	2.352	0.17925461400410464
X	0.013478354239703678	2214	2.214	0.1825939638418516
X	0.013268390979562523	391	0.391	0.3237530245330473
X	0.015620620672573578	11526	11.526	0.1106640491943828
X	0.013084399745687677	927	0.927	0.2416710839132477
X	0.013983184639668671	8270	8.27	0.11913339755854568
X	0.014635462525622532	658	0.658	0.28122905410346893
X	0.014179667863090743	3899	3.899	0.15378156431287046
X	0.014552012851533996	2586	2.586	0.17786814795685027
X	0.013595719685218709	1196	1.196	0.22484867915718745
X	0.013501522503757146	2263	2.263	0.18137025498018403
X	0.013430515406559406	2864	2.864	0.16738115303498433
X	0.012770076755668307	591	0.591	0.27852788129610284
X	0.01301061002895098	610	0.61	0.2773251617327805
X	0.01339713743071144	1505	1.505	0.20724871072105766
X	0.015282693375536836	3818	3.818	0.1587771462201669
X	0.013459991533220706	1098	1.098	0.23057602549960607
X	0.013658120900490156	3942	3.942	0.151318296773343
X	0.014070299974011748	1930	1.93	0.19390187820950497
X	0.013293642440527822	2986	2.986	0.1645071316235169
X	0.013475043850647357	2017	2.017	0.18833950716249298
X	0.013618241992739349	4719	4.719	0.14237170628195434
X	0.013107770841259934	1710	1.71	0.19717154670229212
X	0.013610524565485244	6289	6.289	0.12934940132986616
X	0.013404082398630288	1466	1.466	0.20910658343373564
X	0.013611732902858123	4571	4.571	0.14386905768968772
X	0.013888926887496851	1455	1.455	0.2121299059804802
X	0.013537394092337248	2749	2.749	0.17013230163145593
X	0.014571390154200918	3583	3.583	0.15961905164968826
X	0.013417705388568759	1791	1.791	0.19567149146643054
X	0.013490453294050356	4800	4.8	0.14112212317379028
X	0.015318365858461802	3512	3.512	0.1633876815387134
X	0.013559595085571065	7519	7.519	0.12172007520685084
X	0.015197374588466274	1722	1.722	0.20665463022257557
X	0.013211933816162223	801	0.801	0.25455284647681886
X	0.013630239590628106	1881	1.881	0.19351066175493545
X	0.014131811080031977	1917	1.917	0.19462198752711546
X	0.013495612290608958	4229	4.229	0.14722615474354644
X	0.013230303477391235	1541	1.541	0.20476499847786211
X	0.013426920281518488	1925	1.925	0.19106533151417116
X	0.013535526117259777	1295	1.295	0.2186427717268101
X	0.01323907552238472	4984	4.984	0.1384925915134302
X	0.01350907918635288	600	0.6	0.28237408219652904
X	0.013627598182586501	5499	5.499	0.13532515504010276
X	0.0134787565624458	2381	2.381	0.17822288849395743
X	0.01527211577560092	10286	10.286	0.11408211460323048
X	0.01360633197074279	1745	1.745	0.19829657381588786
X	0.014434369651763425	4038	4.038	0.15290106232152118
X	0.013417550517876976	1100	1.1	0.23019374474007945
X	0.01452666548046971	2493	2.493	0.1799483570063914
X	0.01342416504322617	3141	3.141	0.16228303286198933
X	0.013257212781501848	1870	1.87	0.19210420594516547
X	0.015390371697986351	3847	3.847	0.15874826012809706
X	0.014991917409792407	6459	6.459	0.13240283943380757
X	0.013361041727504706	2423	2.423	0.17666979376383377
X	0.013963430296921375	1821	1.821	0.19719416435579068
X	0.013946389612072906	4211	4.211	0.14905922220813878
X	0.013365754199678656	709	0.709	0.2661434271084953
X	0.014143603832707845	786	0.786	0.26204699209471227
X	0.013635142551965749	412	0.412	0.32106095184181654
X	0.012984164296833323	1221	1.221	0.2199038229271209
X	0.01460060595817415	15244	15.244	0.09857285002052252
X	0.013436716107998955	1381	1.381	0.2134845683200107
X	0.01293819872294718	2744	2.744	0.16768590925020121
X	0.013389196967573826	715	0.715	0.265551950826266
X	0.01363054497988287	5772	5.772	0.13316669845788792
X	0.014753532007874394	1764	1.764	0.20298587003636082
X	0.013329696128512768	1398	1.398	0.2120497147407127
X	0.015484288528755682	2489	2.489	0.18391710388050264
X	0.014276850450208657	1058	1.058	0.2380759800905274
X	0.014689176959349903	1493	1.493	0.21427880630026352
X	0.014914756402848077	3438	3.438	0.16309354178082808
X	0.014431568495017139	820	0.82	0.2601156969031519
X	0.01372871977795703	1583	1.583	0.20545483935136274
X	0.0128638430869064	1740	1.74	0.19480816785137853
X	0.014271016418276867	1784	1.784	0.19999540543572905
X	0.01577284559251662	3300	3.3	0.1684478106155282
X	0.013319668345599039	2076	2.076	0.1858183876372137
X	0.01330506187967776	1816	1.816	0.19422286292517277
X	0.013623785653296002	6685	6.685	0.1267842960154526
X	0.014985554490969496	3211	3.211	0.16711323145752005
X	0.01332774902168336	1094	1.094	0.23009816819511994
X	0.014989114402203681	15754	15.754	0.09835468331911598
X	0.014316249981189726	39769	39.769	0.07113688884305977
X	0.014548808813102809	4025	4.025	0.15346894844166842
X	0.013212070908087148	4820	4.82	0.1399505531057572
X	0.012816969490189008	1025	1.025	0.23210642460609168
X	0.014493346425321588	4226	4.226	0.1508040690885025
X	0.014547536936558973	6391	6.391	0.13154475921440273
X	0.01505220513748095	77381	77.381	0.057941348274474086
X	0.01499369836213028	2500	2.5	0.18168660941688108
X	0.015648399825867226	117879	117.879	0.051012643606313086
X	0.014644419245318042	18373	18.373	0.09271799384627949
X	0.013599768353574135	26772	26.772	0.07979032454746446
X	0.014869680574646287	10573	10.573	0.11203863327291863
X	0.013606414233285612	1681	1.681	0.20078225384212667
X	0.013994653928307212	3383	3.383	0.16052895867819006
X	0.01521646550179129	218254	218.254	0.04115764573123195
X	0.013671428849919261	228857	228.857	0.03909158101360689
X	0.01359970610907625	3328	3.328	0.15987555626951303
X	0.015335167135190363	31752	31.752	0.07845834494514543
X	0.01576611165228058	20593	20.593	0.09148186141523276
X	0.014106959312848633	13834	13.834	0.10065342267603745
X	0.014391869852282099	57900	57.9	0.06287522512796781
X	0.013911937576126217	32437	32.437	0.07541347961735126
X	0.013643282122531849	17857	17.857	0.09141906750025797
X	0.014268166002094504	12124	12.124	0.10557815668025768
X	0.013963458858353131	1340	1.34	0.21842184460547623
X	0.015027241324299832	104204	104.204	0.05244027456386444
X	0.013508881734775997	5344	5.344	0.1362231703162386
X	0.013421284669240894	2640	2.64	0.17194783832091812
X	0.013571788089090214	10987	10.987	0.10729660530720936
X	0.015059121852073856	53205	53.205	0.06565720634826797
X	0.015779651294323557	185535	185.535	0.043976822945482184
X	0.014895367940278645	54938	54.938	0.06472312284092661
X	0.013505388183257777	2057	2.057	0.18725104926471514
X	0.014018176104089572	31895	31.895	0.07603081310110106
X	0.01385398590056977	7357	7.357	0.12348797294925432
X	0.01363919327373696	23644	23.644	0.08324456936151385
X	0.013657265887224539	38722	38.722	0.07065376800735873
X	0.01568975144835555	15229	15.229	0.10099849268002306
X	0.01408490144693469	38812	38.812	0.0713284187791387
X	0.01365737553808268	513	0.513	0.2985955254751599
X	0.013589732446091152	5524	5.524	0.13499543381533213
X	0.014709193876780324	74752	74.752	0.05816414545519918
X	0.01399666660631391	33055	33.055	0.07509237129352318
X	0.013678766528459984	51763	51.763	0.064171545583659
X	0.013275274406332409	3441	3.441	0.1568385955930194
X	0.014989949409077427	195898	195.898	0.042454466558158835
X	0.013628699177102707	10447	10.447	0.10926664379792059
X	0.014093222560830046	21825	21.825	0.08643414745244417
X	0.015042354191193149	75820	75.82	0.05832355801182832
X	0.013449846218432917	1503	1.503	0.2076121564242133
X	0.013856249506565606	68562	68.562	0.05868413926729254
X	0.01583095266889633	146023	146.023	0.0476828195520988
X	0.015687924478117055	445211	445.211	0.032784346761196605
X	0.013489729960179372	33793	33.793	0.07363068416369127
X	0.014654216392087362	23576	23.576	0.08534226386233108
X	0.01336388196833957	4251	4.251	0.14649197766532845
X	0.013554268412388477	4944	4.944	0.13995847654437554
X	0.014668005859788811	136366	136.366	0.047557789220428436
X	0.014842012982194818	6420	6.42	0.13222673055656267
X	0.013635764505085567	19806	19.806	0.08830004112056469
X	0.015700951033009076	28629	28.629	0.08185400371646205
X	0.01366464628964947	13735	13.735	0.09982896693173657
X	0.01396699043017961	16245	16.245	0.09508843652211887
X	0.013583621273072109	2719	2.719	0.17094987423040328
X	0.014964262338746068	117138	117.138	0.050363878728239574
X	0.014829440557209212	9416	9.416	0.11634635250321745
X	0.014810446105182826	14709	14.709	0.1002293692623541
X	0.013633228018495385	9851	9.851	0.11143957824579695
X	0.01520050344056502	42272	42.272	0.07111071717953044
X	0.014917545075657663	15611	15.611	0.09849682052149018
X	0.015156945412220547	128319	128.319	0.049065199987251694
X	0.014000184113379109	4483	4.483	0.14616890809331523
X	0.01420498262005663	23116	23.116	0.0850176977607999
X	0.014734936213517522	7179	7.179	0.12708513919525205
X	0.015976923538287748	137977	137.977	0.04874111796334604
X	0.013606407964778388	26524	26.524	0.08005125710932567
X	0.013597261982855944	8520	8.52	0.11686127586214883
X	0.014323389206579924	87839	87.839	0.05463271749756355
X	0.014830233175252608	109415	109.415	0.05136772969582622
X	0.015073652067561361	3047	3.047	0.1703917947731914
X	0.014656575516858684	106430	106.43	0.051640386328571666
X	0.014972113262524499	136300	136.3	0.04789193848544398
X	0.01592632977920443	268474	268.474	0.03900058819165154
X	0.01362969755063088	15584	15.584	0.0956318211780471
X	0.013424755961501018	2306	2.306	0.17989359420866774
X	0.013613416379548004	8279	8.279	0.11803108928237523
X	0.013649245732599435	8899	8.899	0.11532473941104333
X	0.014404478491472453	14970	14.97	0.09872456898721593
X	0.01490551458654514	36082	36.082	0.07447641968314148
X	0.014104644578274254	1330	1.33	0.21970342906161838
X	0.014909804450048936	44128	44.128	0.06964969746453006
X	0.015255540697499634	90188	90.188	0.055304391240395774
X	0.014631819742166208	3683	3.683	0.1583794660354921
X	0.01422520690569356	5542	5.542	0.1369190961328996
X	0.013939451188660358	28356	28.356	0.07892239135245432
X	0.014641968900651182	6754	6.754	0.12942340095869237
X	0.015633183534847914	10075	10.075	0.11577126816017616
X	0.014400002538830347	12188	12.188	0.10571664557601844
X	0.014714786695968164	2459	2.459	0.18155093117492316
X	0.014957602725274155	31135	31.135	0.07831970470914562
X	0.014920588146861459	26226	26.226	0.08286121465189908
X	0.015290855749938395	15066	15.066	0.10049503583757291
X	0.015720226683920868	37841	37.841	0.0746160176538648
X	0.014940459212719191	8162	8.162	0.12232703574213531
X	0.0157251797340247	356475	356.475	0.03533367798591223
X	0.013611751040333613	9653	9.653	0.11213742098768952
X	0.01439225809095062	73089	73.089	0.05817802485277137
X	0.014145408419306713	2028	2.028	0.19106560487854124
X	0.014941439306781592	2927	2.927	0.17218285481234344
X	0.013535210212496896	2728	2.728	0.17055857008932587
X	0.015318430264524828	10338	10.338	0.11400552844695226
X	0.014916691658394262	4250	4.25	0.15197116549612832
X	0.015728656408600796	40472	40.472	0.0729758204460239
X	0.01445382390990995	4140	4.14	0.1517029895842486
X	0.014700946322342794	3232	3.232	0.1656880982732337
X	0.015845088117173407	166427	166.427	0.04566221325691653
X	0.015962453176730118	33580	33.58	0.07804402298960449
X	0.015310720260496777	23661	23.661	0.08649430406828312
X	0.01565142183238444	90244	90.244	0.055767154659678485
X	0.015922920534611158	250756	250.756	0.039895489503617654
X	0.015470478823198016	7596	7.596	0.12675728636693198
X	0.013551493160656129	9135	9.135	0.11404936358589761
X	0.013594665363395236	15271	15.271	0.09619821528457437
X	0.014556865311066128	1469	1.469	0.21479064193264322
X	0.013367026812926145	3807	3.807	0.15199080462861153
X	0.013629924248067159	13044	13.044	0.10147542618980522
X	0.013833502128063438	2963	2.963	0.1671347784149036
X	0.014748899970807746	3403	3.403	0.16304189974101063
X	0.013529953405553422	6009	6.009	0.13106841039736558
X	0.014037914366741067	5560	5.56	0.13616827394171732
X	0.015724865031248637	207494	207.494	0.04231819426112848
X	0.013945816754469022	12697	12.697	0.10317653646001446
X	0.014970962066525775	61279	61.279	0.06251429629406065
X	0.013625614387341434	4943	4.943	0.1402130687454446
X	0.013351635339083748	3055	3.055	0.16349623240295663
X	0.015046585568687156	60694	60.694	0.06281992298656774
X	0.015408036966074278	166560	166.56	0.0452264187825817
X	0.014243630524135974	260282	260.282	0.03796576787982503
X	0.015075735286259929	12094	12.094	0.10762244302185929
X	0.01484857373416134	25559	25.559	0.08344115249714566
X	0.013403542022445364	2189	2.189	0.18294671102087343
X	0.014199437193719067	4332	4.332	0.14854597695783203
X	0.01487192244287848	642563	642.563	0.028498180476848295
X	0.015117678772019423	12818	12.818	0.10565456880248317
X	0.01341813752553686	7921	7.921	0.11920763587903795
X	0.014715339163985175	28929	28.929	0.07982622985318348
X	0.015660727434764873	460173	460.173	0.032406361015306646
X	0.013444605249427902	31082	31.082	0.07562749224549829
X	0.01375414576465019	8477	8.477	0.11750702014573391
X	0.015833302693417598	6286	6.286	0.13606059921210809
X	0.014800910849011948	24847	24.847	0.08414044945735921
X	0.01363098392022224	3199	3.199	0.1621203957168609
X	0.015798382076221116	132109	132.109	0.04926744826496521
X	0.013521711193723739	10279	10.279	0.10957048816723167
X	0.013080468655649125	1063	1.063	0.23086779053092096
X	0.015242709748879684	32133	32.133	0.07798964951331439
X	0.015337532448043827	19006	19.006	0.09310112378469332
X	0.013404735038260491	1292	1.292	0.2181048083281231
X	0.014760476304248078	61388	61.388	0.06218308951796551
X	0.014346258436314435	63409	63.409	0.06093440600843848
X	0.015739469611979987	164621	164.621	0.045726534230692915
X	0.015601077321521101	146662	146.662	0.04738188438996416
X	0.014295060893748103	148561	148.561	0.04582407761369618
X	0.0146290540544703	16476	16.476	0.09611433241273122
X	0.013608388949573995	13155	13.155	0.10113588684194183
X	0.014382092601355282	59763	59.763	0.0622008849405969
X	0.014328302315117567	110449	110.449	0.0506225847366522
X	0.013509358874636444	6439	6.439	0.1280183143929805
X	0.013723856214040514	48316	48.316	0.06573475706021678
X	0.014235798590984203	89546	89.546	0.05417245881244382
X	0.013662267553373228	83650	83.65	0.05466201205182247
X	0.015072984958360878	60273	60.273	0.06300265030686025
X	0.014984472313777888	42355	42.355	0.07072597163546313
X	0.014720588938472638	9667	9.667	0.1150476686099692
X	0.014150730794064011	4766	4.766	0.1437280754479447
X	0.01575644725705315	12379	12.379	0.10837378603620179
X	0.015357810592045287	81615	81.615	0.057304198260745766
X	0.013647601459268358	89888	89.888	0.05334800764091296
X	0.015898640923054347	171165	171.165	0.04528784742863583
X	0.015747094277703162	81142	81.142	0.057896401741538726
X	0.014911765578317962	37422	37.422	0.07358693106315713
X	0.013605191219165683	12435	12.435	0.10304326713567179
X	0.01565211726678312	177466	177.466	0.04451290903211775
X	0.01338496415702042	3387	3.387	0.15810077735875658
X	0.0136266552699141	8303	8.303	0.1179554677380482
X	0.015037770890202116	121536	121.536	0.04983022425782989
X	0.014233586210726321	39760	39.76	0.07100506349873921
X	0.013485828172594047	4216	4.216	0.1473416989393694
X	0.01411700261091682	14482	14.482	0.09915272414899279
X	0.015925067128006773	268138	268.138	0.03901584063419852
X	0.014494171618550149	11394	11.394	0.10835253123861521
X	0.014854739826197403	50948	50.948	0.06631004203240351
X	0.013434375727626614	1445	1.445	0.21027285644784055
X	0.013527056580618202	4057	4.057	0.14939386608728888
X	0.013572560732752011	3348	3.348	0.1594503394400684
X	0.015542914766127434	5335	5.335	0.14282339332862892
X	0.015589572124520246	16294	16.294	0.0985376410434645
X	0.014088947723231816	54314	54.314	0.06377575224931328
X	0.013383930865247856	2088	2.088	0.1857595128319566
X	0.01444136071299898	73121	73.121	0.05823561477706639
X	0.013539640016682116	7003	7.003	0.12457789753456405
X	0.015716900166908132	43690	43.69	0.071120524358793
X	0.013525191414992121	7580	7.58	0.12128992798043153
X	0.013544066841574625	73043	73.043	0.057023888490685716
X	0.013460845891557886	4308	4.308	0.1461948988691348
X	0.014512207251404712	14776	14.776	0.0994013302702152
X	0.013723337010061329	23640	23.64	0.08342010883958109
X	0.01358682919407421	17056	17.056	0.09270012280139882
X	0.015025349458479062	47796	47.796	0.06799510932273496
X	0.013653872248204949	222791	222.791	0.039426303802905203
X	0.013565512030868095	7351	7.351	0.12265820062565018
X	0.014330270508934467	3607	3.607	0.15838087893036756
X	0.014983045431858195	35270	35.27	0.07517351867558393
X	0.01388058785811832	16650	16.65	0.09411623392731538
X	0.015433601110893161	331484	331.484	0.0359750633102313
X	0.013372724568469167	2260	2.26	0.18087164434240013
X	0.015042121908498731	97520	97.52	0.053629671547979735
X	0.015715258551505813	211968	211.968	0.042009780220802734
X	0.013211110962087453	4080	4.08	0.14794253601658264
X	0.013597917177750048	94318	94.318	0.05243556309951872
X	0.014106652881955133	13483	13.483	0.10151864562388901
X	0.015940940779005222	381609	381.609	0.034697520941709734
X	0.014830071814098145	8618	8.618	0.11983368691879549
X	0.013881242266049242	9597	9.597	0.1130917557052926
X	0.013988426967747923	6464	6.464	0.12934684620278586
X	0.015034391480222441	2175	2.175	0.19049146908018585
X	0.013454803817829927	6164	6.164	0.12971939462614787
X	0.014431070014539093	21614	21.614	0.0874019513496441
X	0.014144922546959305	9488	9.488	0.11423748480658497
X	0.015159036773341562	91476	91.476	0.054927286779953115
X	0.013640264543855299	12416	12.416	0.10318431762345144
X	0.01343935546087426	3382	3.382	0.15839262118769143
X	0.015289582977819922	14467	14.467	0.10186048028252788
X	0.015786201618314632	130123	130.123	0.049504102654676634
X	0.015458172014269488	64422	64.422	0.06214048578880493
X	0.01589538551146622	194314	194.314	0.04340992056579758
X	0.015461245514416558	8788	8.788	0.12072155181134629
X	0.01429025388739487	4681	4.681	0.14506648937582223
X	0.014789988365366434	4029	4.029	0.1542612531261467
X	0.013403671822534581	3308	3.308	0.1594236550021563
X	0.015300209007381025	112167	112.167	0.0514767065268091
X	0.013940275921184487	5011	5.011	0.14064220144567188
X	0.01364589744711505	16111	16.111	0.094614972719921
X	0.01349801823669708	24092	24.092	0.08243893741303666
X	0.0152928312453876	14082	14.082	0.1027877087794267
X	0.01453315831458413	8492	8.492	0.11961436516602511
X	0.014126684018784402	7838	7.838	0.1216965661085606
X	0.015756641200783496	150282	150.282	0.047154038437729695
X	0.015143184364298639	25151	25.151	0.08444111984011177
X	0.013514716062640757	6608	6.608	0.1269342916604491
X	0.013663236875997806	11272	11.272	0.10662299376926915
X	0.014970198760859598	62842	62.842	0.061990602312269424
X	0.0157877020663109	52579	52.579	0.06696309732128969
X	0.01570240567990895	191764	191.764	0.04342432114086752
X	0.014199235034414913	57186	57.186	0.06285286460093638
X	0.013648845453756877	3395	3.395	0.15900790388527045
X	0.013658033322742354	25305	25.305	0.08141928335534047
X	0.013601961719554505	26057	26.057	0.08051788868485996
X	0.013607717215996275	6877	6.877	0.1255438425502401
X	0.014977710604723669	8488	8.488	0.12084073404002749
X	0.013602585254515492	2588	2.588	0.1738677610108562
X	0.014873253987084814	21576	21.576	0.0883374829096004
X	0.013491794775345104	18372	18.372	0.09022033386885728
X	0.013240646503020299	5047	5.047	0.13791937976182211
X	0.01402019995331599	16563	16.563	0.09459577403902872
X	0.013665508048472727	6866	6.866	0.12578841970503454
X	0.014250533007082589	23637	23.637	0.08447851111450073
X	0.013361205202173569	4330	4.33	0.14558587663305705
X	0.01399832798383773	14168	14.168	0.09959920507794862
X	0.0137839876185282	2021	2.021	0.1896427249198653
X	0.013608228849746322	9683	9.683	0.11201182953067916
X	0.013628023822357272	3922	3.922	0.15146365462118308
X	0.013514206547124721	8376	8.376	0.1172873825756345
X	0.01496944050316326	17475	17.475	0.0949721190029762
X	0.014827206732831735	73818	73.818	0.05856421902198431
X	0.013870073645231771	4474	4.474	0.14581233423761952
X	0.014573254830005806	1425	1.425	0.21706038296918406
X	0.01402334711804212	5811	5.811	0.1341323714414602
X	0.014774822487287271	56459	56.459	0.06396312840010127
X	0.015944133317483688	479150	479.15	0.03216447557613876
X	0.014256832434269028	29471	29.471	0.07850115698051074
X	0.015627583714416294	4695	4.695	0.14930880255668597
X	0.014358939414772712	102891	102.891	0.051869849947204
X	0.015622018159058431	90335	90.335	0.05571348984369418
X	0.014513236563387114	1963	1.963	0.19481153045599958
X	0.01418173812603974	7763	7.763	0.12224561558042968
X	0.013657125499648185	24077	24.077	0.08277877130822203
X	0.015087399052374578	77546	77.546	0.057945314652408306
X	0.01529783565516158	251194	251.194	0.03934355293049446
X	0.015100603249877293	139631	139.631	0.04764355390402724
X	0.01435249491464888	12969	12.969	0.10343645720485746
X	0.014954200771656882	3736	3.736	0.15877621563729552
X	0.013590271180472074	1625	1.625	0.20298233891801487
X	0.013713878259133547	47683	47.683	0.06600835444228222
X	0.013604968555663143	7632	7.632	0.12125129719457439
X	0.013503257267334586	9187	9.187	0.11369855411693816
X	0.015149539042341087	106660	106.66	0.0521754191260046
X	0.015758034953819743	361189	361.189	0.03520377052203122
X	0.013300577251181701	6685	6.685	0.12577365819369898
X	0.013188789497696675	1405	1.405	0.21094838467338914
X	0.015388275066799609	9122	9.122	0.11904192951754196
X	0.01534836209524612	34009	34.009	0.07670483229879282
X	0.013610296842900821	18332	18.332	0.09054947074890923
X	0.013538011216469397	1228	1.228	0.22256259048865382
X	0.01493955729253787	3222	3.222	0.16675187682225817
X	0.014252381247019967	14991	14.991	0.09832990259979443
X	0.013870758979420122	2225	2.225	0.18404473726606452
X	0.01519987038813975	30945	30.945	0.0789011135482081
X	0.013607680075423874	3574	3.574	0.15615042862299594
X	0.014835742734986168	11587	11.587	0.1085873985821036
X	0.014999578810308337	11788	11.788	0.10836265712211783
X	0.013959072265445986	5971	5.971	0.13272003721685188
X	0.014242836928302952	5144	5.144	0.14042093074880665
X	0.015331232162135904	113350	113.35	0.051331644585153234
X	0.015611250267363212	279216	279.216	0.038238348614485676
X	0.014230547513234746	62695	62.695	0.060999962447317285
X	0.014781365529833316	57861	57.861	0.06345164326250832
X	0.015168976173139821	11763	11.763	0.10884606274148058
X	0.013589673283964876	12178	12.178	0.10372362396147339
X	0.015701184528704337	11952	11.952	0.10952096903102751
X	0.014965274894688902	93280	93.28	0.05433738329634875
X	0.014813050267797537	13024	13.024	0.10438386411976958
X	0.015807066772447895	13447	13.447	0.10553794358400923
X	0.013525745547913998	12221	12.221	0.10343912138510913
X	0.016003729037964768	844498	844.498	0.02666086207116903
X	0.013607421289486544	9346	9.346	0.11334003734263662
X	0.015085873767829524	44820	44.82	0.06956106230583241
X	0.0152743778070919	199304	199.304	0.04247654370398757
X	0.015033977901874265	30977	30.977	0.07858593968905625
X	0.015084603522181024	16600	16.6	0.0968594363095046
X	0.01590501782258957	517491	517.491	0.03132399310224649
X	0.014564971548301123	96073	96.073	0.05332154967676225
X	0.014163690354977084	51399	51.399	0.06507396229853478
X	0.013577145241696119	3379	3.379	0.15897912073631362
X	0.015052092458875744	20020	20.02	0.09093078382624209
X	0.013640495675882483	8761	8.761	0.1159023274835933
X	0.01445294313939852	132985	132.985	0.04772191675452047
X	0.015956036971969876	634593	634.593	0.02929610738771855
X	0.01582753146371485	61394	61.394	0.06364472562354127
X	0.0151487991727474	107675	107.675	0.05201011043395872
X	0.014135385293893153	45849	45.849	0.06755527418231298
X	0.014214089541483468	18142	18.142	0.09218875744857828
X	0.015353994113856682	20736	20.736	0.09046879279823446
X	0.01574391933611462	5434	5.434	0.14256001770695403
X	0.015835591234107334	277824	277.824	0.03848470610135238
X	0.015804222856660718	276392	276.392	0.03852558353184418
X	0.014287309932249553	49521	49.521	0.06607751266885933
X	0.015612524843105893	912908	912.908	0.025764123751471073
X	0.015534509421680248	114933	114.933	0.051319718635042505
X	0.013642449854974373	26922	26.922	0.07972508949076919
X	0.013825705400994799	5090	5.09	0.1395264374434945
X	0.015588869187341448	34199	34.199	0.07696035609393488
X	0.01494699225622212	80524	80.524	0.05704393676533984
X	0.013556402067194563	4971	4.971	0.13971195161757965
X	0.015616848631527791	403399	403.399	0.033828769004092804
X	0.013668221036378743	12754	12.754	0.1023344501116912
X	0.014900849102978758	19319	19.319	0.09170833134396976
X	0.013652610905827138	4563	4.563	0.14409704720486163
X	0.0134845128931308	7875	7.875	0.11963591260035329
X	0.013288702935453875	1695	1.695	0.19865719399374884
X	0.013022947599229515	5996	5.996	0.12950381320837404
X	0.01406942109904185	16438	16.438	0.09494579837810589
X	0.014696301886461857	21654	21.654	0.08787998565540339
X	0.01594590708489144	1905336	1905.336	0.020302953438957154
X	0.014884107507780625	12362	12.362	0.1063844174249951
X	0.015368053720341526	132065	132.065	0.048821416752541064
X	0.014803599416211	5403	5.403	0.1399299843930641
X	0.014313134020716007	11071	11.071	0.10893881052492484
X	0.01371050380218646	5354	5.354	0.1368122595283226
X	0.01365185482417602	119013	119.013	0.048588380017342225
X	0.014319067190087683	48666	48.666	0.06651144119448386
X	0.015675423867871525	599619	599.619	0.029678911522920876
X	0.013571985136019201	5049	5.049	0.13904199854116836
X	0.01594466661315654	121848	121.848	0.05076904716546317
X	0.015149518375065221	21953	21.953	0.0883693477716583
X	0.013776078652243236	103026	103.026	0.05113594172219965
X	0.01550478165255384	111803	111.803	0.05176116777843209
X	0.014230875114650278	12879	12.879	0.10338316446984792
X	0.01425892976712055	7696	7.696	0.12282137553736268
X	0.01564734393677996	67515	67.515	0.06142522918554941
X	0.014117826628764301	10104	10.104	0.1117956320405258
X	0.014192807591255149	29436	29.436	0.07841452336296895
X	0.0143295854451844	211359	211.359	0.040775969304074455
X	0.014660097523089641	7432	7.432	0.12541333577088437
X	0.015474291724062846	46694	46.694	0.06920166870455044
X	0.013390963567535823	14297	14.297	0.09784132011909447
X	0.014915901689237988	80531	80.531	0.057002706088550105
X	0.01585909198151921	207247	207.247	0.04245511501680467
X	0.013568411299808352	20537	20.537	0.08709582399134662
X	0.015270923236192745	6985	6.985	0.12978752814463324
X	0.014769957463590269	114148	114.148	0.050579008622195026
X	0.01496273484991004	58006	58.006	0.0636569784623688
X	0.015869274061852557	422533	422.533	0.03348874010743461
X	0.014486389021600841	38592	38.592	0.0721363702674441
X	0.015060848271211237	25223	25.223	0.08420752401144302
X	0.014747904868983877	24863	24.863	0.08402185507202341
X	0.013519272244976108	91684	91.684	0.05283072700805722
X	0.01568156183284156	141971	141.971	0.04798032931962471
X	0.01417817493068784	24119	24.119	0.08376970980369067
X	0.013303923939412632	4799	4.799	0.14047843741126717
X	0.014713949677865644	6517	6.517	0.13118792064720167
X	0.013737111501834208	37802	37.802	0.07136088299219157
X	0.01568884830615435	67794	67.794	0.0613950374033404
X	0.014249993657993943	47944	47.944	0.06673599430755571
X	0.014850032018768234	9274	9.274	0.116991263896939
X	0.014857540436677173	19327	19.327	0.09160675333099202
X	0.014086150523893396	22701	22.701	0.08529346912532194
X	0.013947988118171285	27245	27.245	0.0799972243909374
X	0.01540386664259173	18226	18.226	0.09454664621871957
X	0.015643512586676057	42174	42.174	0.0718505054140506
X	0.014406681641016243	10593	10.593	0.11079368204003255
X	0.014553597064941385	19398	19.398	0.09086663886848469
X	0.013459914252905669	2261	2.261	0.18123715811599192
X	0.014223008766661936	5358	5.358	0.13846167768624584
X	0.015406853971194859	98251	98.251	0.053925270089377225
X	0.01363306719400517	12177	12.177	0.10383675074286372
X	0.014349600977329997	42774	42.774	0.06948429464157146
X	0.014213215184632661	1985	1.985	0.19274240484385427
X	0.01341781818451843	4366	4.366	0.14538939314037963
X	0.014069110834686212	18689	18.689	0.09096902269254233
X	0.014350976161458882	42346	42.346	0.06971983489058976
X	0.01565737432636638	18619	18.619	0.09438888608524547
X	0.014720481220150213	4043	4.043	0.1538412342025927
X	0.015177882467412486	6112	6.112	0.135417860076861
X	0.01362491259731279	17118	17.118	0.092674496314198
X	0.013649235595646753	18154	18.154	0.0909310067943651
X	0.015107504288155016	72881	72.881	0.05918243108742495
X	0.01362559027443224	4440	4.44	0.14531957479266214
X	0.01361796517858024	7974	7.974	0.11953047976962895
X	0.014306573563506559	30273	30.273	0.077892096006944
X	0.013450392182597354	7072	7.072	0.12389798172276602
X	0.015652931030797526	112860	112.86	0.051762893130285835
X	0.015026408280720637	4951	4.951	0.1447840606622753
X	0.015145005632870622	43530	43.53	0.07033318057851874
X	0.015857357493472796	106208	106.208	0.053050709372305896
X	0.013633805254243927	17063	17.063	0.09279414299065018
X	0.015791434171554684	93323	93.323	0.05531093002406134
X	0.015325969095875955	82200	82.2	0.057128397598117796
X	0.014620054373388874	1451	1.451	0.21598679088319048
X	0.015031982041253066	30169	30.169	0.0792778343165619
X	0.01366211712539263	10735	10.735	0.10836906304352542
X	0.014432874039400053	123542	123.542	0.048885425300527856
X	0.01503148074998519	54712	54.712	0.06500893601296202
X	0.014810503929547898	24619	24.619	0.08441762750373005
X	0.014748115911803599	42594	42.594	0.07022030398987697
X	0.01363710908068331	22196	22.196	0.0850124500612885
X	0.013447393723557858	14863	14.863	0.09671872162631619
X	0.013540410670925849	2705	2.705	0.17106251610773793
X	0.013394930668954327	7205	7.205	0.12296140822447381
X	0.015437710193275438	106417	106.417	0.052544099972894005
X	0.015961659481229837	342692	342.692	0.03597974036876202
X	0.014933086181138292	20218	20.218	0.09039348418124869
X	0.014568256089590653	11384	11.384	0.10856859727120934
X	0.014345521499311264	26000	26.0	0.08201897374226869
X	0.015752370989112564	83286	83.286	0.05740168515671114
X	0.013940383679453919	23897	23.897	0.08355589865387844
X	0.01365859790857931	6640	6.64	0.12717820184471593
X	0.015806517121236745	35620	35.62	0.07627474369889424
X	0.015105795678387381	83881	83.881	0.05647115480629168
X	0.014428226678215845	4359	4.359	0.1490305700277952
X	0.014922850021021276	20315	20.315	0.09022875866073092
X	0.013651421703439498	7256	7.256	0.12345069412357972
time for making epsilon is 1.957956314086914
epsilons are
[0.24060003311010345, 0.09212915709081916, 0.20865302668087304, 0.10535876202160342, 0.0794356997116464, 0.07266976508483812, 0.07086563893096395, 0.07531950903116497, 0.09196598331059588, 0.08808793594547186, 0.11491447272892602, 0.0640594663448743, 0.12889411159304642, 0.03989024627196858, 0.08590022506975861, 0.0772626374897345, 0.0676956756927169, 0.09156379560515152, 0.05552895102233932, 0.059699056838693206, 0.07402726802624168, 0.04819517272394324, 0.11852604591954528, 0.07977431838061842, 0.18408165111961902, 0.06983906718511695, 0.06960457074539615, 0.10308732032574863, 0.05035377089081034, 0.026985614685348682, 0.10476654967850389, 0.027317189069065134, 0.09456889336498764, 0.10961149792484903, 0.09091061799854759, 0.04061051074079138, 0.06231030425214523, 0.2581229274327997, 0.17100186763798592, 0.17471851720991818, 0.18419157533994318, 0.21258360365313778, 0.2824985793007903, 0.16469083636013382, 0.26293879301003464, 0.26126523757552095, 0.1920508723234773, 0.16043628833156737, 0.162759183243774, 0.1140562273901104, 0.10794906973490538, 0.2361069816814206, 0.14461744911263213, 0.171110393179986, 0.15795962551736323, 0.1403035901475526, 0.21184324322845233, 0.14441027704945436, 0.18999018290870018, 0.1659526349198213, 0.15269818497749832, 0.23384537492938387, 0.14924452662560594, 0.11975781192664486, 0.1781504566145205, 0.19709849390744727, 0.18812910502110503, 0.1875775920571417, 0.15108386421548461, 0.14705109922093287, 0.14620581570528238, 0.14893034144083794, 0.19352267019081842, 0.14456647654669866, 0.1507034632718705, 0.11365300433541053, 0.2910206959775803, 0.243443196706808, 0.18631392764588384, 0.11672884502744929, 0.1930384960007332, 0.20956681594214996, 0.18970211512507723, 0.19239980061836157, 0.21673146873736773, 0.24061803851596208, 0.18327123802901582, 0.266676481938747, 0.17649703021743077, 0.17177792901613584, 0.150806267948008, 0.20890605090507555, 0.22818529485513628, 0.12783308986694528, 0.15491183691408755, 0.1708279230519936, 0.1993734716191857, 0.13410161432670956, 0.12196177699458481, 0.16293310291313903, 0.12132857419468276, 0.1376382447424247, 0.15203976125625512, 0.23007150179723804, 0.13589022227235448, 0.20040402651925704, 0.16344400841906864, 0.24073787388211626, 0.18671811424000678, 0.2710590093307606, 0.186736384895651, 0.18809058598920134, 0.19194973921919786, 0.17060784032857726, 0.1456862130426282, 0.1720069149519014, 0.19710759528804345, 0.2756071910392662, 0.13759767555690308, 0.2598977991570559, 0.130340702271998, 0.20915067603449936, 0.19421337001254077, 0.18248338548423323, 0.18934608622015162, 0.17619240589889312, 0.2456277248967488, 0.16388082032725118, 0.1295988077022097, 0.21023251638962812, 0.1338469769527182, 0.22665720727156674, 0.16364089712992547, 0.22553518185421326, 0.20911822848785058, 0.19883277315344902, 0.20192620689410726, 0.2343844693966887, 0.208783774206259, 0.20382651002150734, 0.1837634815619635, 0.23354885488147292, 0.13404015731435287, 0.11441679791753857, 0.1633782587969091, 0.22413677913135438, 0.22879548817811038, 0.23646501315073046, 0.21584352424762962, 0.19051633358620812, 0.1853180681742198, 0.18323298932182872, 0.1685350383735327, 0.12472922536171017, 0.17160550060085072, 0.09886601451151271, 0.19191682169948196, 0.17925461400410464, 0.1825939638418516, 0.3237530245330473, 0.1106640491943828, 0.2416710839132477, 0.11913339755854568, 0.28122905410346893, 0.15378156431287046, 0.17786814795685027, 0.22484867915718745, 0.18137025498018403, 0.16738115303498433, 0.27852788129610284, 0.2773251617327805, 0.20724871072105766, 0.1587771462201669, 0.23057602549960607, 0.151318296773343, 0.19390187820950497, 0.1645071316235169, 0.18833950716249298, 0.14237170628195434, 0.19717154670229212, 0.12934940132986616, 0.20910658343373564, 0.14386905768968772, 0.2121299059804802, 0.17013230163145593, 0.15961905164968826, 0.19567149146643054, 0.14112212317379028, 0.1633876815387134, 0.12172007520685084, 0.20665463022257557, 0.25455284647681886, 0.19351066175493545, 0.19462198752711546, 0.14722615474354644, 0.20476499847786211, 0.19106533151417116, 0.2186427717268101, 0.1384925915134302, 0.28237408219652904, 0.13532515504010276, 0.17822288849395743, 0.11408211460323048, 0.19829657381588786, 0.15290106232152118, 0.23019374474007945, 0.1799483570063914, 0.16228303286198933, 0.19210420594516547, 0.15874826012809706, 0.13240283943380757, 0.17666979376383377, 0.19719416435579068, 0.14905922220813878, 0.2661434271084953, 0.26204699209471227, 0.32106095184181654, 0.2199038229271209, 0.09857285002052252, 0.2134845683200107, 0.16768590925020121, 0.265551950826266, 0.13316669845788792, 0.20298587003636082, 0.2120497147407127, 0.18391710388050264, 0.2380759800905274, 0.21427880630026352, 0.16309354178082808, 0.2601156969031519, 0.20545483935136274, 0.19480816785137853, 0.19999540543572905, 0.1684478106155282, 0.1858183876372137, 0.19422286292517277, 0.1267842960154526, 0.16711323145752005, 0.23009816819511994, 0.09835468331911598, 0.07113688884305977, 0.15346894844166842, 0.1399505531057572, 0.23210642460609168, 0.1508040690885025, 0.13154475921440273, 0.057941348274474086, 0.18168660941688108, 0.051012643606313086, 0.09271799384627949, 0.07979032454746446, 0.11203863327291863, 0.20078225384212667, 0.16052895867819006, 0.04115764573123195, 0.03909158101360689, 0.15987555626951303, 0.07845834494514543, 0.09148186141523276, 0.10065342267603745, 0.06287522512796781, 0.07541347961735126, 0.09141906750025797, 0.10557815668025768, 0.21842184460547623, 0.05244027456386444, 0.1362231703162386, 0.17194783832091812, 0.10729660530720936, 0.06565720634826797, 0.043976822945482184, 0.06472312284092661, 0.18725104926471514, 0.07603081310110106, 0.12348797294925432, 0.08324456936151385, 0.07065376800735873, 0.10099849268002306, 0.0713284187791387, 0.2985955254751599, 0.13499543381533213, 0.05816414545519918, 0.07509237129352318, 0.064171545583659, 0.1568385955930194, 0.042454466558158835, 0.10926664379792059, 0.08643414745244417, 0.05832355801182832, 0.2076121564242133, 0.05868413926729254, 0.0476828195520988, 0.032784346761196605, 0.07363068416369127, 0.08534226386233108, 0.14649197766532845, 0.13995847654437554, 0.047557789220428436, 0.13222673055656267, 0.08830004112056469, 0.08185400371646205, 0.09982896693173657, 0.09508843652211887, 0.17094987423040328, 0.050363878728239574, 0.11634635250321745, 0.1002293692623541, 0.11143957824579695, 0.07111071717953044, 0.09849682052149018, 0.049065199987251694, 0.14616890809331523, 0.0850176977607999, 0.12708513919525205, 0.04874111796334604, 0.08005125710932567, 0.11686127586214883, 0.05463271749756355, 0.05136772969582622, 0.1703917947731914, 0.051640386328571666, 0.04789193848544398, 0.03900058819165154, 0.0956318211780471, 0.17989359420866774, 0.11803108928237523, 0.11532473941104333, 0.09872456898721593, 0.07447641968314148, 0.21970342906161838, 0.06964969746453006, 0.055304391240395774, 0.1583794660354921, 0.1369190961328996, 0.07892239135245432, 0.12942340095869237, 0.11577126816017616, 0.10571664557601844, 0.18155093117492316, 0.07831970470914562, 0.08286121465189908, 0.10049503583757291, 0.0746160176538648, 0.12232703574213531, 0.03533367798591223, 0.11213742098768952, 0.05817802485277137, 0.19106560487854124, 0.17218285481234344, 0.17055857008932587, 0.11400552844695226, 0.15197116549612832, 0.0729758204460239, 0.1517029895842486, 0.1656880982732337, 0.04566221325691653, 0.07804402298960449, 0.08649430406828312, 0.055767154659678485, 0.039895489503617654, 0.12675728636693198, 0.11404936358589761, 0.09619821528457437, 0.21479064193264322, 0.15199080462861153, 0.10147542618980522, 0.1671347784149036, 0.16304189974101063, 0.13106841039736558, 0.13616827394171732, 0.04231819426112848, 0.10317653646001446, 0.06251429629406065, 0.1402130687454446, 0.16349623240295663, 0.06281992298656774, 0.0452264187825817, 0.03796576787982503, 0.10762244302185929, 0.08344115249714566, 0.18294671102087343, 0.14854597695783203, 0.028498180476848295, 0.10565456880248317, 0.11920763587903795, 0.07982622985318348, 0.032406361015306646, 0.07562749224549829, 0.11750702014573391, 0.13606059921210809, 0.08414044945735921, 0.1621203957168609, 0.04926744826496521, 0.10957048816723167, 0.23086779053092096, 0.07798964951331439, 0.09310112378469332, 0.2181048083281231, 0.06218308951796551, 0.06093440600843848, 0.045726534230692915, 0.04738188438996416, 0.04582407761369618, 0.09611433241273122, 0.10113588684194183, 0.0622008849405969, 0.0506225847366522, 0.1280183143929805, 0.06573475706021678, 0.05417245881244382, 0.05466201205182247, 0.06300265030686025, 0.07072597163546313, 0.1150476686099692, 0.1437280754479447, 0.10837378603620179, 0.057304198260745766, 0.05334800764091296, 0.04528784742863583, 0.057896401741538726, 0.07358693106315713, 0.10304326713567179, 0.04451290903211775, 0.15810077735875658, 0.1179554677380482, 0.04983022425782989, 0.07100506349873921, 0.1473416989393694, 0.09915272414899279, 0.03901584063419852, 0.10835253123861521, 0.06631004203240351, 0.21027285644784055, 0.14939386608728888, 0.1594503394400684, 0.14282339332862892, 0.0985376410434645, 0.06377575224931328, 0.1857595128319566, 0.05823561477706639, 0.12457789753456405, 0.071120524358793, 0.12128992798043153, 0.057023888490685716, 0.1461948988691348, 0.0994013302702152, 0.08342010883958109, 0.09270012280139882, 0.06799510932273496, 0.039426303802905203, 0.12265820062565018, 0.15838087893036756, 0.07517351867558393, 0.09411623392731538, 0.0359750633102313, 0.18087164434240013, 0.053629671547979735, 0.042009780220802734, 0.14794253601658264, 0.05243556309951872, 0.10151864562388901, 0.034697520941709734, 0.11983368691879549, 0.1130917557052926, 0.12934684620278586, 0.19049146908018585, 0.12971939462614787, 0.0874019513496441, 0.11423748480658497, 0.054927286779953115, 0.10318431762345144, 0.15839262118769143, 0.10186048028252788, 0.049504102654676634, 0.06214048578880493, 0.04340992056579758, 0.12072155181134629, 0.14506648937582223, 0.1542612531261467, 0.1594236550021563, 0.0514767065268091, 0.14064220144567188, 0.094614972719921, 0.08243893741303666, 0.1027877087794267, 0.11961436516602511, 0.1216965661085606, 0.047154038437729695, 0.08444111984011177, 0.1269342916604491, 0.10662299376926915, 0.061990602312269424, 0.06696309732128969, 0.04342432114086752, 0.06285286460093638, 0.15900790388527045, 0.08141928335534047, 0.08051788868485996, 0.1255438425502401, 0.12084073404002749, 0.1738677610108562, 0.0883374829096004, 0.09022033386885728, 0.13791937976182211, 0.09459577403902872, 0.12578841970503454, 0.08447851111450073, 0.14558587663305705, 0.09959920507794862, 0.1896427249198653, 0.11201182953067916, 0.15146365462118308, 0.1172873825756345, 0.0949721190029762, 0.05856421902198431, 0.14581233423761952, 0.21706038296918406, 0.1341323714414602, 0.06396312840010127, 0.03216447557613876, 0.07850115698051074, 0.14930880255668597, 0.051869849947204, 0.05571348984369418, 0.19481153045599958, 0.12224561558042968, 0.08277877130822203, 0.057945314652408306, 0.03934355293049446, 0.04764355390402724, 0.10343645720485746, 0.15877621563729552, 0.20298233891801487, 0.06600835444228222, 0.12125129719457439, 0.11369855411693816, 0.0521754191260046, 0.03520377052203122, 0.12577365819369898, 0.21094838467338914, 0.11904192951754196, 0.07670483229879282, 0.09054947074890923, 0.22256259048865382, 0.16675187682225817, 0.09832990259979443, 0.18404473726606452, 0.0789011135482081, 0.15615042862299594, 0.1085873985821036, 0.10836265712211783, 0.13272003721685188, 0.14042093074880665, 0.051331644585153234, 0.038238348614485676, 0.060999962447317285, 0.06345164326250832, 0.10884606274148058, 0.10372362396147339, 0.10952096903102751, 0.05433738329634875, 0.10438386411976958, 0.10553794358400923, 0.10343912138510913, 0.02666086207116903, 0.11334003734263662, 0.06956106230583241, 0.04247654370398757, 0.07858593968905625, 0.0968594363095046, 0.03132399310224649, 0.05332154967676225, 0.06507396229853478, 0.15897912073631362, 0.09093078382624209, 0.1159023274835933, 0.04772191675452047, 0.02929610738771855, 0.06364472562354127, 0.05201011043395872, 0.06755527418231298, 0.09218875744857828, 0.09046879279823446, 0.14256001770695403, 0.03848470610135238, 0.03852558353184418, 0.06607751266885933, 0.025764123751471073, 0.051319718635042505, 0.07972508949076919, 0.1395264374434945, 0.07696035609393488, 0.05704393676533984, 0.13971195161757965, 0.033828769004092804, 0.1023344501116912, 0.09170833134396976, 0.14409704720486163, 0.11963591260035329, 0.19865719399374884, 0.12950381320837404, 0.09494579837810589, 0.08787998565540339, 0.020302953438957154, 0.1063844174249951, 0.048821416752541064, 0.1399299843930641, 0.10893881052492484, 0.1368122595283226, 0.048588380017342225, 0.06651144119448386, 0.029678911522920876, 0.13904199854116836, 0.05076904716546317, 0.0883693477716583, 0.05113594172219965, 0.05176116777843209, 0.10338316446984792, 0.12282137553736268, 0.06142522918554941, 0.1117956320405258, 0.07841452336296895, 0.040775969304074455, 0.12541333577088437, 0.06920166870455044, 0.09784132011909447, 0.057002706088550105, 0.04245511501680467, 0.08709582399134662, 0.12978752814463324, 0.050579008622195026, 0.0636569784623688, 0.03348874010743461, 0.0721363702674441, 0.08420752401144302, 0.08402185507202341, 0.05283072700805722, 0.04798032931962471, 0.08376970980369067, 0.14047843741126717, 0.13118792064720167, 0.07136088299219157, 0.0613950374033404, 0.06673599430755571, 0.116991263896939, 0.09160675333099202, 0.08529346912532194, 0.0799972243909374, 0.09454664621871957, 0.0718505054140506, 0.11079368204003255, 0.09086663886848469, 0.18123715811599192, 0.13846167768624584, 0.053925270089377225, 0.10383675074286372, 0.06948429464157146, 0.19274240484385427, 0.14538939314037963, 0.09096902269254233, 0.06971983489058976, 0.09438888608524547, 0.1538412342025927, 0.135417860076861, 0.092674496314198, 0.0909310067943651, 0.05918243108742495, 0.14531957479266214, 0.11953047976962895, 0.077892096006944, 0.12389798172276602, 0.051762893130285835, 0.1447840606622753, 0.07033318057851874, 0.053050709372305896, 0.09279414299065018, 0.05531093002406134, 0.057128397598117796, 0.21598679088319048, 0.0792778343165619, 0.10836906304352542, 0.048885425300527856, 0.06500893601296202, 0.08441762750373005, 0.07022030398987697, 0.0850124500612885, 0.09671872162631619, 0.17106251610773793, 0.12296140822447381, 0.052544099972894005, 0.03597974036876202, 0.09039348418124869, 0.10856859727120934, 0.08201897374226869, 0.05740168515671114, 0.08355589865387844, 0.12717820184471593, 0.07627474369889424, 0.05647115480629168, 0.1490305700277952, 0.09022875866073092, 0.12345069412357972]
0.09134342498021986
Making ranges
torch.Size([35882, 2])
We keep 6.05e+06/4.41e+08 =  1% of the original kernel matrix.

torch.Size([2666, 2])
We keep 8.00e+04/1.04e+06 =  7% of the original kernel matrix.

torch.Size([11167, 2])
We keep 7.32e+05/2.14e+07 =  3% of the original kernel matrix.

torch.Size([31972, 2])
We keep 6.74e+06/3.37e+08 =  2% of the original kernel matrix.

torch.Size([34408, 2])
We keep 5.90e+06/3.85e+08 =  1% of the original kernel matrix.

torch.Size([4090, 2])
We keep 1.32e+05/2.33e+06 =  5% of the original kernel matrix.

torch.Size([13223, 2])
We keep 9.52e+05/3.21e+07 =  2% of the original kernel matrix.

torch.Size([20238, 2])
We keep 7.77e+06/1.55e+08 =  5% of the original kernel matrix.

torch.Size([27347, 2])
We keep 4.45e+06/2.61e+08 =  1% of the original kernel matrix.

torch.Size([50492, 2])
We keep 1.22e+07/7.75e+08 =  1% of the original kernel matrix.

torch.Size([43250, 2])
We keep 8.29e+06/5.85e+08 =  1% of the original kernel matrix.

torch.Size([66261, 2])
We keep 2.12e+07/1.48e+09 =  1% of the original kernel matrix.

torch.Size([48760, 2])
We keep 1.10e+07/8.09e+08 =  1% of the original kernel matrix.

torch.Size([72269, 2])
We keep 2.52e+07/1.76e+09 =  1% of the original kernel matrix.

torch.Size([50870, 2])
We keep 1.18e+07/8.82e+08 =  1% of the original kernel matrix.

torch.Size([59847, 2])
We keep 1.74e+07/1.23e+09 =  1% of the original kernel matrix.

torch.Size([46722, 2])
We keep 1.02e+07/7.38e+08 =  1% of the original kernel matrix.

torch.Size([31353, 2])
We keep 1.36e+07/3.25e+08 =  4% of the original kernel matrix.

torch.Size([33310, 2])
We keep 5.69e+06/3.79e+08 =  1% of the original kernel matrix.

torch.Size([30350, 2])
We keep 1.51e+07/3.98e+08 =  3% of the original kernel matrix.

torch.Size([33004, 2])
We keep 6.06e+06/4.19e+08 =  1% of the original kernel matrix.

torch.Size([17419, 2])
We keep 2.09e+06/7.94e+07 =  2% of the original kernel matrix.

torch.Size([25119, 2])
We keep 3.36e+06/1.87e+08 =  1% of the original kernel matrix.

torch.Size([87356, 2])
We keep 6.59e+07/2.81e+09 =  2% of the original kernel matrix.

torch.Size([55310, 2])
We keep 1.42e+07/1.11e+09 =  1% of the original kernel matrix.

torch.Size([14398, 2])
We keep 1.17e+06/4.01e+07 =  2% of the original kernel matrix.

torch.Size([22995, 2])
We keep 2.60e+06/1.33e+08 =  1% of the original kernel matrix.

torch.Size([410997, 2])
We keep 7.05e+08/6.01e+10 =  1% of the original kernel matrix.

torch.Size([125916, 2])
We keep 5.50e+07/5.15e+09 =  1% of the original kernel matrix.

torch.Size([38887, 2])
We keep 7.56e+06/4.63e+08 =  1% of the original kernel matrix.

torch.Size([38175, 2])
We keep 6.75e+06/4.52e+08 =  1% of the original kernel matrix.

torch.Size([54110, 2])
We keep 1.52e+07/8.76e+08 =  1% of the original kernel matrix.

torch.Size([44712, 2])
We keep 8.74e+06/6.22e+08 =  1% of the original kernel matrix.

torch.Size([83499, 2])
We keep 4.31e+07/2.54e+09 =  1% of the original kernel matrix.

torch.Size([54648, 2])
We keep 1.39e+07/1.06e+09 =  1% of the original kernel matrix.

torch.Size([32155, 2])
We keep 6.76e+06/3.51e+08 =  1% of the original kernel matrix.

torch.Size([33070, 2])
We keep 5.79e+06/3.94e+08 =  1% of the original kernel matrix.

torch.Size([144197, 2])
We keep 1.54e+08/8.56e+09 =  1% of the original kernel matrix.

torch.Size([73065, 2])
We keep 2.35e+07/1.94e+09 =  1% of the original kernel matrix.

torch.Size([120620, 2])
We keep 7.82e+07/5.44e+09 =  1% of the original kernel matrix.

torch.Size([66276, 2])
We keep 1.90e+07/1.55e+09 =  1% of the original kernel matrix.

torch.Size([57713, 2])
We keep 2.62e+07/1.11e+09 =  2% of the original kernel matrix.

torch.Size([44903, 2])
We keep 9.68e+06/6.99e+08 =  1% of the original kernel matrix.

torch.Size([224794, 2])
We keep 1.77e+08/1.93e+10 =  0% of the original kernel matrix.

torch.Size([93992, 2])
We keep 3.28e+07/2.92e+09 =  1% of the original kernel matrix.

torch.Size([17145, 2])
We keep 1.84e+06/6.82e+07 =  2% of the original kernel matrix.

torch.Size([25115, 2])
We keep 3.20e+06/1.74e+08 =  1% of the original kernel matrix.

torch.Size([48402, 2])
We keep 3.51e+07/9.77e+08 =  3% of the original kernel matrix.

torch.Size([42456, 2])
We keep 9.34e+06/6.56e+08 =  1% of the original kernel matrix.

torch.Size([5564, 2])
We keep 3.47e+05/5.64e+06 =  6% of the original kernel matrix.

torch.Size([14951, 2])
We keep 1.29e+06/4.99e+07 =  2% of the original kernel matrix.

torch.Size([75990, 2])
We keep 2.68e+07/1.99e+09 =  1% of the original kernel matrix.

torch.Size([52487, 2])
We keep 1.24e+07/9.36e+08 =  1% of the original kernel matrix.

torch.Size([76709, 2])
We keep 3.53e+07/2.11e+09 =  1% of the original kernel matrix.

torch.Size([52875, 2])
We keep 1.27e+07/9.65e+08 =  1% of the original kernel matrix.

torch.Size([23539, 2])
We keep 3.49e+06/1.57e+08 =  2% of the original kernel matrix.

torch.Size([29755, 2])
We keep 4.36e+06/2.63e+08 =  1% of the original kernel matrix.

torch.Size([135687, 2])
We keep 2.81e+08/1.46e+10 =  1% of the original kernel matrix.

torch.Size([69904, 2])
We keep 2.95e+07/2.54e+09 =  1% of the original kernel matrix.

torch.Size([1481088, 2])
We keep 4.44e+09/6.47e+11 =  0% of the original kernel matrix.

torch.Size([247953, 2])
We keep 1.62e+08/1.69e+10 =  0% of the original kernel matrix.

torch.Size([21572, 2])
We keep 3.23e+06/1.38e+08 =  2% of the original kernel matrix.

torch.Size([28165, 2])
We keep 4.17e+06/2.46e+08 =  1% of the original kernel matrix.

torch.Size([1403570, 2])
We keep 4.21e+09/6.14e+11 =  0% of the original kernel matrix.

torch.Size([245147, 2])
We keep 1.58e+08/1.65e+10 =  0% of the original kernel matrix.

torch.Size([26305, 2])
We keep 1.03e+07/2.57e+08 =  4% of the original kernel matrix.

torch.Size([31376, 2])
We keep 5.43e+06/3.36e+08 =  1% of the original kernel matrix.

torch.Size([20415, 2])
We keep 3.67e+06/1.26e+08 =  2% of the original kernel matrix.

torch.Size([27601, 2])
We keep 4.07e+06/2.36e+08 =  1% of the original kernel matrix.

torch.Size([32393, 2])
We keep 6.39e+06/3.30e+08 =  1% of the original kernel matrix.

torch.Size([33316, 2])
We keep 5.53e+06/3.81e+08 =  1% of the original kernel matrix.

torch.Size([370785, 2])
We keep 6.62e+08/5.58e+10 =  1% of the original kernel matrix.

torch.Size([119605, 2])
We keep 5.32e+07/4.96e+09 =  1% of the original kernel matrix.

torch.Size([107921, 2])
We keep 6.81e+07/4.27e+09 =  1% of the original kernel matrix.

torch.Size([63248, 2])
We keep 1.73e+07/1.37e+09 =  1% of the original kernel matrix.

torch.Size([2338, 2])
We keep 4.52e+04/5.91e+05 =  7% of the original kernel matrix.

torch.Size([10554, 2])
We keep 5.94e+05/1.62e+07 =  3% of the original kernel matrix.

torch.Size([6701, 2])
We keep 4.37e+05/8.74e+06 =  4% of the original kernel matrix.

torch.Size([15764, 2])
We keep 1.53e+06/6.21e+07 =  2% of the original kernel matrix.

torch.Size([6435, 2])
We keep 3.14e+05/6.31e+06 =  4% of the original kernel matrix.

torch.Size([15496, 2])
We keep 1.33e+06/5.27e+07 =  2% of the original kernel matrix.

torch.Size([6072, 2])
We keep 2.85e+05/5.64e+06 =  5% of the original kernel matrix.

torch.Size([15665, 2])
We keep 1.30e+06/4.99e+07 =  2% of the original kernel matrix.

torch.Size([3131, 2])
We keep 1.88e+05/2.25e+06 =  8% of the original kernel matrix.

torch.Size([11230, 2])
We keep 9.56e+05/3.15e+07 =  3% of the original kernel matrix.

torch.Size([1803, 2])
We keep 3.01e+04/3.33e+05 =  9% of the original kernel matrix.

torch.Size([9652, 2])
We keep 4.94e+05/1.21e+07 =  4% of the original kernel matrix.

torch.Size([7222, 2])
We keep 4.27e+05/8.77e+06 =  4% of the original kernel matrix.

torch.Size([16203, 2])
We keep 1.49e+06/6.22e+07 =  2% of the original kernel matrix.

torch.Size([1914, 2])
We keep 4.64e+04/5.55e+05 =  8% of the original kernel matrix.

torch.Size([9713, 2])
We keep 5.75e+05/1.56e+07 =  3% of the original kernel matrix.

torch.Size([2121, 2])
We keep 4.69e+04/5.96e+05 =  7% of the original kernel matrix.

torch.Size([10267, 2])
We keep 5.99e+05/1.62e+07 =  3% of the original kernel matrix.

torch.Size([5215, 2])
We keep 2.18e+05/3.95e+06 =  5% of the original kernel matrix.

torch.Size([14439, 2])
We keep 1.14e+06/4.18e+07 =  2% of the original kernel matrix.

torch.Size([8243, 2])
We keep 4.38e+05/1.06e+07 =  4% of the original kernel matrix.

torch.Size([17368, 2])
We keep 1.59e+06/6.84e+07 =  2% of the original kernel matrix.

torch.Size([7592, 2])
We keep 5.24e+05/1.07e+07 =  4% of the original kernel matrix.

torch.Size([16758, 2])
We keep 1.62e+06/6.86e+07 =  2% of the original kernel matrix.

torch.Size([17835, 2])
We keep 2.97e+06/9.10e+07 =  3% of the original kernel matrix.

torch.Size([25514, 2])
We keep 3.60e+06/2.00e+08 =  1% of the original kernel matrix.

torch.Size([20857, 2])
We keep 2.84e+06/1.28e+08 =  2% of the original kernel matrix.

torch.Size([27822, 2])
We keep 4.11e+06/2.38e+08 =  1% of the original kernel matrix.

torch.Size([2860, 2])
We keep 7.35e+04/1.08e+06 =  6% of the original kernel matrix.

torch.Size([11327, 2])
We keep 7.36e+05/2.18e+07 =  3% of the original kernel matrix.

torch.Size([10339, 2])
We keep 7.43e+05/2.02e+07 =  3% of the original kernel matrix.

torch.Size([19147, 2])
We keep 2.00e+06/9.45e+07 =  2% of the original kernel matrix.

torch.Size([6585, 2])
We keep 4.16e+05/8.06e+06 =  5% of the original kernel matrix.

torch.Size([15632, 2])
We keep 1.47e+06/5.96e+07 =  2% of the original kernel matrix.

torch.Size([8419, 2])
We keep 5.36e+05/1.24e+07 =  4% of the original kernel matrix.

torch.Size([17605, 2])
We keep 1.71e+06/7.39e+07 =  2% of the original kernel matrix.

torch.Size([10847, 2])
We keep 1.23e+06/2.80e+07 =  4% of the original kernel matrix.

torch.Size([19696, 2])
We keep 2.35e+06/1.11e+08 =  2% of the original kernel matrix.

torch.Size([4170, 2])
We keep 1.10e+05/2.03e+06 =  5% of the original kernel matrix.

torch.Size([13407, 2])
We keep 8.91e+05/3.00e+07 =  2% of the original kernel matrix.

torch.Size([10742, 2])
We keep 7.28e+05/2.04e+07 =  3% of the original kernel matrix.

torch.Size([19557, 2])
We keep 2.03e+06/9.49e+07 =  2% of the original kernel matrix.

torch.Size([5286, 2])
We keep 2.58e+05/4.76e+06 =  5% of the original kernel matrix.

torch.Size([14527, 2])
We keep 1.24e+06/4.58e+07 =  2% of the original kernel matrix.

torch.Size([7118, 2])
We keep 4.77e+05/1.03e+07 =  4% of the original kernel matrix.

torch.Size([16139, 2])
We keep 1.63e+06/6.73e+07 =  2% of the original kernel matrix.

torch.Size([9551, 2])
We keep 6.66e+05/1.70e+07 =  3% of the original kernel matrix.

torch.Size([18676, 2])
We keep 1.92e+06/8.65e+07 =  2% of the original kernel matrix.

torch.Size([2865, 2])
We keep 7.96e+04/1.13e+06 =  7% of the original kernel matrix.

torch.Size([11537, 2])
We keep 7.40e+05/2.23e+07 =  3% of the original kernel matrix.

torch.Size([9834, 2])
We keep 9.63e+05/2.18e+07 =  4% of the original kernel matrix.

torch.Size([18987, 2])
We keep 2.17e+06/9.81e+07 =  2% of the original kernel matrix.

torch.Size([16799, 2])
We keep 1.64e+06/6.39e+07 =  2% of the original kernel matrix.

torch.Size([24855, 2])
We keep 3.10e+06/1.68e+08 =  1% of the original kernel matrix.

torch.Size([5965, 2])
We keep 3.11e+05/6.66e+06 =  4% of the original kernel matrix.

torch.Size([15119, 2])
We keep 1.37e+06/5.42e+07 =  2% of the original kernel matrix.

torch.Size([4756, 2])
We keep 1.75e+05/3.27e+06 =  5% of the original kernel matrix.

torch.Size([13975, 2])
We keep 1.06e+06/3.80e+07 =  2% of the original kernel matrix.

torch.Size([5243, 2])
We keep 2.32e+05/4.45e+06 =  5% of the original kernel matrix.

torch.Size([14369, 2])
We keep 1.19e+06/4.43e+07 =  2% of the original kernel matrix.

torch.Size([5087, 2])
We keep 2.87e+05/4.93e+06 =  5% of the original kernel matrix.

torch.Size([14255, 2])
We keep 1.26e+06/4.66e+07 =  2% of the original kernel matrix.

torch.Size([9319, 2])
We keep 5.93e+05/1.56e+07 =  3% of the original kernel matrix.

torch.Size([18442, 2])
We keep 1.83e+06/8.28e+07 =  2% of the original kernel matrix.

torch.Size([9780, 2])
We keep 6.84e+05/1.81e+07 =  3% of the original kernel matrix.

torch.Size([18748, 2])
We keep 1.95e+06/8.93e+07 =  2% of the original kernel matrix.

torch.Size([8638, 2])
We keep 1.31e+06/1.95e+07 =  6% of the original kernel matrix.

torch.Size([17649, 2])
We keep 1.98e+06/9.28e+07 =  2% of the original kernel matrix.

torch.Size([9538, 2])
We keep 9.03e+05/1.87e+07 =  4% of the original kernel matrix.

torch.Size([18602, 2])
We keep 2.00e+06/9.08e+07 =  2% of the original kernel matrix.

torch.Size([4993, 2])
We keep 2.20e+05/3.96e+06 =  5% of the original kernel matrix.

torch.Size([14266, 2])
We keep 1.16e+06/4.18e+07 =  2% of the original kernel matrix.

torch.Size([10182, 2])
We keep 7.14e+05/1.89e+07 =  3% of the original kernel matrix.

torch.Size([18791, 2])
We keep 1.97e+06/9.14e+07 =  2% of the original kernel matrix.

torch.Size([9321, 2])
We keep 6.96e+05/1.73e+07 =  4% of the original kernel matrix.

torch.Size([18365, 2])
We keep 1.95e+06/8.75e+07 =  2% of the original kernel matrix.

torch.Size([17432, 2])
We keep 2.35e+06/8.46e+07 =  2% of the original kernel matrix.

torch.Size([25064, 2])
We keep 3.46e+06/1.93e+08 =  1% of the original kernel matrix.

torch.Size([1642, 2])
We keep 3.40e+04/3.60e+05 =  9% of the original kernel matrix.

torch.Size([9340, 2])
We keep 5.30e+05/1.26e+07 =  4% of the original kernel matrix.

torch.Size([2466, 2])
We keep 5.73e+04/8.43e+05 =  6% of the original kernel matrix.

torch.Size([10739, 2])
We keep 6.69e+05/1.93e+07 =  3% of the original kernel matrix.

torch.Size([5229, 2])
We keep 2.15e+05/4.36e+06 =  4% of the original kernel matrix.

torch.Size([14432, 2])
We keep 1.15e+06/4.39e+07 =  2% of the original kernel matrix.

torch.Size([17048, 2])
We keep 2.04e+06/7.56e+07 =  2% of the original kernel matrix.

torch.Size([24878, 2])
We keep 3.33e+06/1.83e+08 =  1% of the original kernel matrix.

torch.Size([4895, 2])
We keep 1.89e+05/3.47e+06 =  5% of the original kernel matrix.

torch.Size([13949, 2])
We keep 1.09e+06/3.91e+07 =  2% of the original kernel matrix.

torch.Size([3809, 2])
We keep 1.61e+05/2.47e+06 =  6% of the original kernel matrix.

torch.Size([12599, 2])
We keep 9.92e+05/3.30e+07 =  3% of the original kernel matrix.

torch.Size([4984, 2])
We keep 2.52e+05/4.08e+06 =  6% of the original kernel matrix.

torch.Size([14084, 2])
We keep 1.15e+06/4.24e+07 =  2% of the original kernel matrix.

torch.Size([4808, 2])
We keep 2.03e+05/3.52e+06 =  5% of the original kernel matrix.

torch.Size([13852, 2])
We keep 1.09e+06/3.94e+07 =  2% of the original kernel matrix.

torch.Size([3539, 2])
We keep 1.14e+05/1.76e+06 =  6% of the original kernel matrix.

torch.Size([12350, 2])
We keep 8.58e+05/2.79e+07 =  3% of the original kernel matrix.

torch.Size([2555, 2])
We keep 6.17e+04/8.91e+05 =  6% of the original kernel matrix.

torch.Size([10892, 2])
We keep 6.74e+05/1.98e+07 =  3% of the original kernel matrix.

torch.Size([5611, 2])
We keep 3.10e+05/5.42e+06 =  5% of the original kernel matrix.

torch.Size([14808, 2])
We keep 1.30e+06/4.89e+07 =  2% of the original kernel matrix.

torch.Size([2138, 2])
We keep 4.26e+04/5.06e+05 =  8% of the original kernel matrix.

torch.Size([10310, 2])
We keep 5.63e+05/1.49e+07 =  3% of the original kernel matrix.

torch.Size([6479, 2])
We keep 3.68e+05/7.66e+06 =  4% of the original kernel matrix.

torch.Size([15844, 2])
We keep 1.49e+06/5.81e+07 =  2% of the original kernel matrix.

torch.Size([6542, 2])
We keep 3.40e+05/7.14e+06 =  4% of the original kernel matrix.

torch.Size([15685, 2])
We keep 1.38e+06/5.61e+07 =  2% of the original kernel matrix.

torch.Size([9507, 2])
We keep 6.34e+05/1.69e+07 =  3% of the original kernel matrix.

torch.Size([18630, 2])
We keep 1.90e+06/8.64e+07 =  2% of the original kernel matrix.

torch.Size([3969, 2])
We keep 1.20e+05/2.10e+06 =  5% of the original kernel matrix.

torch.Size([12970, 2])
We keep 9.01e+05/3.05e+07 =  2% of the original kernel matrix.

torch.Size([3316, 2])
We keep 1.00e+05/1.54e+06 =  6% of the original kernel matrix.

torch.Size([12206, 2])
We keep 8.37e+05/2.60e+07 =  3% of the original kernel matrix.

torch.Size([13020, 2])
We keep 1.51e+06/4.15e+07 =  3% of the original kernel matrix.

torch.Size([21352, 2])
We keep 2.63e+06/1.35e+08 =  1% of the original kernel matrix.

torch.Size([9017, 2])
We keep 5.56e+05/1.31e+07 =  4% of the original kernel matrix.

torch.Size([18130, 2])
We keep 1.71e+06/7.60e+07 =  2% of the original kernel matrix.

torch.Size([6658, 2])
We keep 3.39e+05/7.86e+06 =  4% of the original kernel matrix.

torch.Size([16026, 2])
We keep 1.44e+06/5.89e+07 =  2% of the original kernel matrix.

torch.Size([4737, 2])
We keep 1.69e+05/3.06e+06 =  5% of the original kernel matrix.

torch.Size([13874, 2])
We keep 1.04e+06/3.68e+07 =  2% of the original kernel matrix.

torch.Size([11387, 2])
We keep 1.68e+06/3.65e+07 =  4% of the original kernel matrix.

torch.Size([20021, 2])
We keep 2.58e+06/1.27e+08 =  2% of the original kernel matrix.

torch.Size([16172, 2])
We keep 1.90e+06/6.65e+07 =  2% of the original kernel matrix.

torch.Size([24295, 2])
We keep 3.23e+06/1.71e+08 =  1% of the original kernel matrix.

torch.Size([7931, 2])
We keep 3.82e+05/9.57e+06 =  3% of the original kernel matrix.

torch.Size([17217, 2])
We keep 1.51e+06/6.50e+07 =  2% of the original kernel matrix.

torch.Size([15792, 2])
We keep 1.85e+06/6.47e+07 =  2% of the original kernel matrix.

torch.Size([24015, 2])
We keep 3.18e+06/1.69e+08 =  1% of the original kernel matrix.

torch.Size([12158, 2])
We keep 1.06e+06/3.00e+07 =  3% of the original kernel matrix.

torch.Size([20986, 2])
We keep 2.38e+06/1.15e+08 =  2% of the original kernel matrix.

torch.Size([9308, 2])
We keep 6.94e+05/1.67e+07 =  4% of the original kernel matrix.

torch.Size([18394, 2])
We keep 1.91e+06/8.58e+07 =  2% of the original kernel matrix.

torch.Size([2943, 2])
We keep 8.58e+04/1.25e+06 =  6% of the original kernel matrix.

torch.Size([11417, 2])
We keep 7.72e+05/2.34e+07 =  3% of the original kernel matrix.

torch.Size([11428, 2])
We keep 1.21e+06/3.19e+07 =  3% of the original kernel matrix.

torch.Size([20208, 2])
We keep 2.41e+06/1.19e+08 =  2% of the original kernel matrix.

torch.Size([4475, 2])
We keep 1.68e+05/2.79e+06 =  6% of the original kernel matrix.

torch.Size([13414, 2])
We keep 1.00e+06/3.51e+07 =  2% of the original kernel matrix.

torch.Size([7596, 2])
We keep 3.99e+05/9.70e+06 =  4% of the original kernel matrix.

torch.Size([16810, 2])
We keep 1.55e+06/6.54e+07 =  2% of the original kernel matrix.

torch.Size([2700, 2])
We keep 7.34e+04/9.24e+05 =  7% of the original kernel matrix.

torch.Size([11187, 2])
We keep 6.90e+05/2.02e+07 =  3% of the original kernel matrix.

torch.Size([5228, 2])
We keep 2.43e+05/4.22e+06 =  5% of the original kernel matrix.

torch.Size([14312, 2])
We keep 1.15e+06/4.32e+07 =  2% of the original kernel matrix.

torch.Size([2018, 2])
We keep 4.14e+04/4.44e+05 =  9% of the original kernel matrix.

torch.Size([10085, 2])
We keep 5.46e+05/1.40e+07 =  3% of the original kernel matrix.

torch.Size([4956, 2])
We keep 2.66e+05/4.91e+06 =  5% of the original kernel matrix.

torch.Size([13882, 2])
We keep 1.23e+06/4.65e+07 =  2% of the original kernel matrix.

torch.Size([4594, 2])
We keep 2.78e+05/3.74e+06 =  7% of the original kernel matrix.

torch.Size([13353, 2])
We keep 1.11e+06/4.06e+07 =  2% of the original kernel matrix.

torch.Size([5028, 2])
We keep 1.89e+05/3.56e+06 =  5% of the original kernel matrix.

torch.Size([14103, 2])
We keep 1.09e+06/3.97e+07 =  2% of the original kernel matrix.

torch.Size([6388, 2])
We keep 4.01e+05/7.46e+06 =  5% of the original kernel matrix.

torch.Size([15636, 2])
We keep 1.41e+06/5.74e+07 =  2% of the original kernel matrix.

torch.Size([9867, 2])
We keep 9.64e+05/2.13e+07 =  4% of the original kernel matrix.

torch.Size([18853, 2])
We keep 2.09e+06/9.69e+07 =  2% of the original kernel matrix.

torch.Size([6255, 2])
We keep 3.89e+05/8.25e+06 =  4% of the original kernel matrix.

torch.Size([15129, 2])
We keep 1.49e+06/6.03e+07 =  2% of the original kernel matrix.

torch.Size([4839, 2])
We keep 2.20e+05/3.82e+06 =  5% of the original kernel matrix.

torch.Size([14096, 2])
We keep 1.15e+06/4.11e+07 =  2% of the original kernel matrix.

torch.Size([1992, 2])
We keep 3.25e+04/4.03e+05 =  8% of the original kernel matrix.

torch.Size([10148, 2])
We keep 5.26e+05/1.33e+07 =  3% of the original kernel matrix.

torch.Size([11849, 2])
We keep 1.12e+06/3.28e+07 =  3% of the original kernel matrix.

torch.Size([20754, 2])
We keep 2.47e+06/1.20e+08 =  2% of the original kernel matrix.

torch.Size([2235, 2])
We keep 4.31e+04/5.61e+05 =  7% of the original kernel matrix.

torch.Size([10392, 2])
We keep 5.81e+05/1.57e+07 =  3% of the original kernel matrix.

torch.Size([14033, 2])
We keep 1.38e+06/4.63e+07 =  2% of the original kernel matrix.

torch.Size([22557, 2])
We keep 2.82e+06/1.43e+08 =  1% of the original kernel matrix.

torch.Size([3933, 2])
We keep 1.47e+05/2.20e+06 =  6% of the original kernel matrix.

torch.Size([12928, 2])
We keep 9.30e+05/3.11e+07 =  2% of the original kernel matrix.

torch.Size([4688, 2])
We keep 1.87e+05/3.39e+06 =  5% of the original kernel matrix.

torch.Size([13648, 2])
We keep 1.07e+06/3.86e+07 =  2% of the original kernel matrix.

torch.Size([6081, 2])
We keep 2.81e+05/6.02e+06 =  4% of the original kernel matrix.

torch.Size([15587, 2])
We keep 1.34e+06/5.15e+07 =  2% of the original kernel matrix.

torch.Size([5107, 2])
We keep 2.83e+05/4.54e+06 =  6% of the original kernel matrix.

torch.Size([14066, 2])
We keep 1.21e+06/4.47e+07 =  2% of the original kernel matrix.

torch.Size([6697, 2])
We keep 3.73e+05/7.92e+06 =  4% of the original kernel matrix.

torch.Size([16110, 2])
We keep 1.51e+06/5.91e+07 =  2% of the original kernel matrix.

torch.Size([2646, 2])
We keep 5.96e+04/8.39e+05 =  7% of the original kernel matrix.

torch.Size([11330, 2])
We keep 6.70e+05/1.92e+07 =  3% of the original kernel matrix.

torch.Size([7542, 2])
We keep 4.17e+05/9.70e+06 =  4% of the original kernel matrix.

torch.Size([16775, 2])
We keep 1.55e+06/6.54e+07 =  2% of the original kernel matrix.

torch.Size([14016, 2])
We keep 1.40e+06/4.42e+07 =  3% of the original kernel matrix.

torch.Size([22618, 2])
We keep 2.75e+06/1.40e+08 =  1% of the original kernel matrix.

torch.Size([4123, 2])
We keep 1.45e+05/2.55e+06 =  5% of the original kernel matrix.

torch.Size([13253, 2])
We keep 9.94e+05/3.35e+07 =  2% of the original kernel matrix.

torch.Size([12788, 2])
We keep 1.07e+06/3.48e+07 =  3% of the original kernel matrix.

torch.Size([21366, 2])
We keep 2.48e+06/1.24e+08 =  2% of the original kernel matrix.

torch.Size([3186, 2])
We keep 1.09e+05/1.49e+06 =  7% of the original kernel matrix.

torch.Size([11896, 2])
We keep 8.22e+05/2.56e+07 =  3% of the original kernel matrix.

torch.Size([7552, 2])
We keep 4.85e+05/1.00e+07 =  4% of the original kernel matrix.

torch.Size([16880, 2])
We keep 1.56e+06/6.65e+07 =  2% of the original kernel matrix.

torch.Size([3122, 2])
We keep 1.13e+05/1.50e+06 =  7% of the original kernel matrix.

torch.Size([11579, 2])
We keep 8.33e+05/2.57e+07 =  3% of the original kernel matrix.

torch.Size([3970, 2])
We keep 1.31e+05/2.14e+06 =  6% of the original kernel matrix.

torch.Size([12860, 2])
We keep 9.22e+05/3.07e+07 =  2% of the original kernel matrix.

torch.Size([4698, 2])
We keep 1.57e+05/2.99e+06 =  5% of the original kernel matrix.

torch.Size([13956, 2])
We keep 1.01e+06/3.63e+07 =  2% of the original kernel matrix.

torch.Size([4216, 2])
We keep 1.68e+05/2.65e+06 =  6% of the original kernel matrix.

torch.Size([13096, 2])
We keep 9.80e+05/3.42e+07 =  2% of the original kernel matrix.

torch.Size([3143, 2])
We keep 8.77e+04/1.31e+06 =  6% of the original kernel matrix.

torch.Size([11980, 2])
We keep 8.02e+05/2.40e+07 =  3% of the original kernel matrix.

torch.Size([4025, 2])
We keep 1.49e+05/2.56e+06 =  5% of the original kernel matrix.

torch.Size([12964, 2])
We keep 9.91e+05/3.36e+07 =  2% of the original kernel matrix.

torch.Size([4162, 2])
We keep 1.41e+05/2.53e+06 =  5% of the original kernel matrix.

torch.Size([13201, 2])
We keep 9.73e+05/3.34e+07 =  2% of the original kernel matrix.

torch.Size([5612, 2])
We keep 2.43e+05/4.70e+06 =  5% of the original kernel matrix.

torch.Size([14888, 2])
We keep 1.21e+06/4.55e+07 =  2% of the original kernel matrix.

torch.Size([2858, 2])
We keep 7.97e+04/1.09e+06 =  7% of the original kernel matrix.

torch.Size([11308, 2])
We keep 7.26e+05/2.19e+07 =  3% of the original kernel matrix.

torch.Size([12194, 2])
We keep 1.07e+06/3.18e+07 =  3% of the original kernel matrix.

torch.Size([20474, 2])
We keep 2.38e+06/1.18e+08 =  2% of the original kernel matrix.

torch.Size([18190, 2])
We keep 2.01e+06/8.26e+07 =  2% of the original kernel matrix.

torch.Size([25849, 2])
We keep 3.42e+06/1.91e+08 =  1% of the original kernel matrix.

torch.Size([7246, 2])
We keep 6.34e+05/1.23e+07 =  5% of the original kernel matrix.

torch.Size([16410, 2])
We keep 1.75e+06/7.38e+07 =  2% of the original kernel matrix.

torch.Size([3499, 2])
We keep 9.68e+04/1.51e+06 =  6% of the original kernel matrix.

torch.Size([12465, 2])
We keep 8.17e+05/2.58e+07 =  3% of the original kernel matrix.

torch.Size([3159, 2])
We keep 9.06e+04/1.30e+06 =  6% of the original kernel matrix.

torch.Size([11837, 2])
We keep 7.79e+05/2.40e+07 =  3% of the original kernel matrix.

torch.Size([2697, 2])
We keep 9.34e+04/1.41e+06 =  6% of the original kernel matrix.

torch.Size([11277, 2])
We keep 8.19e+05/2.49e+07 =  3% of the original kernel matrix.

torch.Size([3617, 2])
We keep 1.07e+05/1.75e+06 =  6% of the original kernel matrix.

torch.Size([12415, 2])
We keep 8.51e+05/2.78e+07 =  3% of the original kernel matrix.

torch.Size([4953, 2])
We keep 2.24e+05/3.85e+06 =  5% of the original kernel matrix.

torch.Size([14016, 2])
We keep 1.12e+06/4.12e+07 =  2% of the original kernel matrix.

torch.Size([5403, 2])
We keep 2.88e+05/5.48e+06 =  5% of the original kernel matrix.

torch.Size([14674, 2])
We keep 1.31e+06/4.92e+07 =  2% of the original kernel matrix.

torch.Size([5680, 2])
We keep 2.49e+05/4.93e+06 =  5% of the original kernel matrix.

torch.Size([14979, 2])
We keep 1.24e+06/4.66e+07 =  2% of the original kernel matrix.

torch.Size([7354, 2])
We keep 3.74e+05/7.98e+06 =  4% of the original kernel matrix.

torch.Size([16709, 2])
We keep 1.45e+06/5.93e+07 =  2% of the original kernel matrix.

torch.Size([12163, 2])
We keep 2.10e+06/4.84e+07 =  4% of the original kernel matrix.

torch.Size([20674, 2])
We keep 2.79e+06/1.46e+08 =  1% of the original kernel matrix.

torch.Size([6426, 2])
We keep 3.98e+05/8.08e+06 =  4% of the original kernel matrix.

torch.Size([15437, 2])
We keep 1.48e+06/5.97e+07 =  2% of the original kernel matrix.

torch.Size([26095, 2])
We keep 5.69e+06/2.41e+08 =  2% of the original kernel matrix.

torch.Size([31420, 2])
We keep 5.32e+06/3.26e+08 =  1% of the original kernel matrix.

torch.Size([5324, 2])
We keep 2.12e+05/4.27e+06 =  4% of the original kernel matrix.

torch.Size([14674, 2])
We keep 1.17e+06/4.34e+07 =  2% of the original kernel matrix.

torch.Size([5849, 2])
We keep 3.21e+05/5.53e+06 =  5% of the original kernel matrix.

torch.Size([15119, 2])
We keep 1.28e+06/4.94e+07 =  2% of the original kernel matrix.

torch.Size([5904, 2])
We keep 2.63e+05/4.90e+06 =  5% of the original kernel matrix.

torch.Size([15241, 2])
We keep 1.23e+06/4.65e+07 =  2% of the original kernel matrix.

torch.Size([1220, 2])
We keep 1.75e+04/1.53e+05 = 11% of the original kernel matrix.

torch.Size([8281, 2])
We keep 3.86e+05/8.21e+06 =  4% of the original kernel matrix.

torch.Size([20051, 2])
We keep 3.21e+06/1.33e+08 =  2% of the original kernel matrix.

torch.Size([27608, 2])
We keep 4.23e+06/2.42e+08 =  1% of the original kernel matrix.

torch.Size([2666, 2])
We keep 6.46e+04/8.59e+05 =  7% of the original kernel matrix.

torch.Size([11060, 2])
We keep 6.64e+05/1.95e+07 =  3% of the original kernel matrix.

torch.Size([15020, 2])
We keep 2.96e+06/6.84e+07 =  4% of the original kernel matrix.

torch.Size([22974, 2])
We keep 3.22e+06/1.74e+08 =  1% of the original kernel matrix.

torch.Size([1736, 2])
We keep 4.32e+04/4.33e+05 =  9% of the original kernel matrix.

torch.Size([9530, 2])
We keep 5.67e+05/1.38e+07 =  4% of the original kernel matrix.

torch.Size([8586, 2])
We keep 6.54e+05/1.52e+07 =  4% of the original kernel matrix.

torch.Size([17654, 2])
We keep 1.84e+06/8.19e+07 =  2% of the original kernel matrix.

torch.Size([6009, 2])
We keep 3.07e+05/6.69e+06 =  4% of the original kernel matrix.

torch.Size([15366, 2])
We keep 1.38e+06/5.43e+07 =  2% of the original kernel matrix.

torch.Size([3408, 2])
We keep 9.30e+04/1.43e+06 =  6% of the original kernel matrix.

torch.Size([12374, 2])
We keep 7.96e+05/2.51e+07 =  3% of the original kernel matrix.

torch.Size([5822, 2])
We keep 2.74e+05/5.12e+06 =  5% of the original kernel matrix.

torch.Size([14999, 2])
We keep 1.24e+06/4.75e+07 =  2% of the original kernel matrix.

torch.Size([7353, 2])
We keep 3.71e+05/8.20e+06 =  4% of the original kernel matrix.

torch.Size([16578, 2])
We keep 1.47e+06/6.02e+07 =  2% of the original kernel matrix.

torch.Size([1755, 2])
We keep 2.89e+04/3.49e+05 =  8% of the original kernel matrix.

torch.Size([9458, 2])
We keep 4.90e+05/1.24e+07 =  3% of the original kernel matrix.

torch.Size([1804, 2])
We keep 3.31e+04/3.72e+05 =  8% of the original kernel matrix.

torch.Size([9423, 2])
We keep 5.11e+05/1.28e+07 =  3% of the original kernel matrix.

torch.Size([3704, 2])
We keep 1.65e+05/2.27e+06 =  7% of the original kernel matrix.

torch.Size([12302, 2])
We keep 9.38e+05/3.16e+07 =  2% of the original kernel matrix.

torch.Size([8486, 2])
We keep 7.00e+05/1.46e+07 =  4% of the original kernel matrix.

torch.Size([17695, 2])
We keep 1.85e+06/8.02e+07 =  2% of the original kernel matrix.

torch.Size([3107, 2])
We keep 7.65e+04/1.21e+06 =  6% of the original kernel matrix.

torch.Size([11923, 2])
We keep 7.54e+05/2.31e+07 =  3% of the original kernel matrix.

torch.Size([9002, 2])
We keep 6.03e+05/1.55e+07 =  3% of the original kernel matrix.

torch.Size([18199, 2])
We keep 1.84e+06/8.28e+07 =  2% of the original kernel matrix.

torch.Size([5088, 2])
We keep 2.04e+05/3.72e+06 =  5% of the original kernel matrix.

torch.Size([14540, 2])
We keep 1.12e+06/4.05e+07 =  2% of the original kernel matrix.

torch.Size([7171, 2])
We keep 4.22e+05/8.92e+06 =  4% of the original kernel matrix.

torch.Size([16139, 2])
We keep 1.50e+06/6.27e+07 =  2% of the original kernel matrix.

torch.Size([5264, 2])
We keep 2.09e+05/4.07e+06 =  5% of the original kernel matrix.

torch.Size([14578, 2])
We keep 1.14e+06/4.24e+07 =  2% of the original kernel matrix.

torch.Size([10600, 2])
We keep 9.76e+05/2.23e+07 =  4% of the original kernel matrix.

torch.Size([19583, 2])
We keep 2.11e+06/9.91e+07 =  2% of the original kernel matrix.

torch.Size([4626, 2])
We keep 1.59e+05/2.92e+06 =  5% of the original kernel matrix.

torch.Size([13665, 2])
We keep 1.01e+06/3.59e+07 =  2% of the original kernel matrix.

torch.Size([14106, 2])
We keep 1.15e+06/3.96e+07 =  2% of the original kernel matrix.

torch.Size([22595, 2])
We keep 2.59e+06/1.32e+08 =  1% of the original kernel matrix.

torch.Size([3867, 2])
We keep 1.51e+05/2.15e+06 =  7% of the original kernel matrix.

torch.Size([12767, 2])
We keep 9.20e+05/3.08e+07 =  2% of the original kernel matrix.

torch.Size([9693, 2])
We keep 9.73e+05/2.09e+07 =  4% of the original kernel matrix.

torch.Size([18771, 2])
We keep 2.05e+06/9.60e+07 =  2% of the original kernel matrix.

torch.Size([3870, 2])
We keep 1.25e+05/2.12e+06 =  5% of the original kernel matrix.

torch.Size([12888, 2])
We keep 9.20e+05/3.06e+07 =  3% of the original kernel matrix.

torch.Size([6926, 2])
We keep 3.44e+05/7.56e+06 =  4% of the original kernel matrix.

torch.Size([16270, 2])
We keep 1.42e+06/5.77e+07 =  2% of the original kernel matrix.

torch.Size([8622, 2])
We keep 5.03e+05/1.28e+07 =  3% of the original kernel matrix.

torch.Size([17944, 2])
We keep 1.74e+06/7.53e+07 =  2% of the original kernel matrix.

torch.Size([4629, 2])
We keep 1.86e+05/3.21e+06 =  5% of the original kernel matrix.

torch.Size([13575, 2])
We keep 1.05e+06/3.76e+07 =  2% of the original kernel matrix.

torch.Size([10420, 2])
We keep 1.03e+06/2.30e+07 =  4% of the original kernel matrix.

torch.Size([19208, 2])
We keep 2.13e+06/1.01e+08 =  2% of the original kernel matrix.

torch.Size([7557, 2])
We keep 5.60e+05/1.23e+07 =  4% of the original kernel matrix.

torch.Size([16702, 2])
We keep 1.74e+06/7.38e+07 =  2% of the original kernel matrix.

torch.Size([15593, 2])
We keep 1.55e+06/5.65e+07 =  2% of the original kernel matrix.

torch.Size([23663, 2])
We keep 2.97e+06/1.58e+08 =  1% of the original kernel matrix.

torch.Size([4131, 2])
We keep 1.72e+05/2.97e+06 =  5% of the original kernel matrix.

torch.Size([13127, 2])
We keep 1.07e+06/3.62e+07 =  2% of the original kernel matrix.

torch.Size([2388, 2])
We keep 4.71e+04/6.42e+05 =  7% of the original kernel matrix.

torch.Size([10701, 2])
We keep 6.10e+05/1.68e+07 =  3% of the original kernel matrix.

torch.Size([5061, 2])
We keep 1.76e+05/3.54e+06 =  4% of the original kernel matrix.

torch.Size([14340, 2])
We keep 1.08e+06/3.95e+07 =  2% of the original kernel matrix.

torch.Size([4687, 2])
We keep 2.03e+05/3.67e+06 =  5% of the original kernel matrix.

torch.Size([13838, 2])
We keep 1.12e+06/4.03e+07 =  2% of the original kernel matrix.

torch.Size([9395, 2])
We keep 8.52e+05/1.79e+07 =  4% of the original kernel matrix.

torch.Size([18407, 2])
We keep 1.94e+06/8.88e+07 =  2% of the original kernel matrix.

torch.Size([4097, 2])
We keep 1.42e+05/2.37e+06 =  5% of the original kernel matrix.

torch.Size([13104, 2])
We keep 9.49e+05/3.24e+07 =  2% of the original kernel matrix.

torch.Size([5056, 2])
We keep 2.13e+05/3.71e+06 =  5% of the original kernel matrix.

torch.Size([14205, 2])
We keep 1.09e+06/4.04e+07 =  2% of the original kernel matrix.

torch.Size([3667, 2])
We keep 9.92e+04/1.68e+06 =  5% of the original kernel matrix.

torch.Size([12794, 2])
We keep 8.37e+05/2.72e+07 =  3% of the original kernel matrix.

torch.Size([10905, 2])
We keep 1.24e+06/2.48e+07 =  4% of the original kernel matrix.

torch.Size([19688, 2])
We keep 2.17e+06/1.05e+08 =  2% of the original kernel matrix.

torch.Size([1772, 2])
We keep 3.19e+04/3.60e+05 =  8% of the original kernel matrix.

torch.Size([9643, 2])
We keep 5.11e+05/1.26e+07 =  4% of the original kernel matrix.

torch.Size([12787, 2])
We keep 8.98e+05/3.02e+07 =  2% of the original kernel matrix.

torch.Size([21535, 2])
We keep 2.32e+06/1.15e+08 =  2% of the original kernel matrix.

torch.Size([6036, 2])
We keep 2.83e+05/5.67e+06 =  4% of the original kernel matrix.

torch.Size([15277, 2])
We keep 1.28e+06/5.00e+07 =  2% of the original kernel matrix.

torch.Size([18988, 2])
We keep 2.40e+06/1.06e+08 =  2% of the original kernel matrix.

torch.Size([26717, 2])
We keep 3.85e+06/2.16e+08 =  1% of the original kernel matrix.

torch.Size([4731, 2])
We keep 1.74e+05/3.05e+06 =  5% of the original kernel matrix.

torch.Size([13869, 2])
We keep 1.04e+06/3.67e+07 =  2% of the original kernel matrix.

torch.Size([9127, 2])
We keep 6.48e+05/1.63e+07 =  3% of the original kernel matrix.

torch.Size([18205, 2])
We keep 1.88e+06/8.48e+07 =  2% of the original kernel matrix.

torch.Size([3045, 2])
We keep 7.73e+04/1.21e+06 =  6% of the original kernel matrix.

torch.Size([11754, 2])
We keep 7.55e+05/2.31e+07 =  3% of the original kernel matrix.

torch.Size([5744, 2])
We keep 3.63e+05/6.22e+06 =  5% of the original kernel matrix.

torch.Size([14953, 2])
We keep 1.35e+06/5.24e+07 =  2% of the original kernel matrix.

torch.Size([7837, 2])
We keep 4.23e+05/9.87e+06 =  4% of the original kernel matrix.

torch.Size([16971, 2])
We keep 1.57e+06/6.60e+07 =  2% of the original kernel matrix.

torch.Size([4853, 2])
We keep 2.07e+05/3.50e+06 =  5% of the original kernel matrix.

torch.Size([13909, 2])
We keep 1.08e+06/3.93e+07 =  2% of the original kernel matrix.

torch.Size([8465, 2])
We keep 6.11e+05/1.48e+07 =  4% of the original kernel matrix.

torch.Size([17512, 2])
We keep 1.87e+06/8.08e+07 =  2% of the original kernel matrix.

torch.Size([12733, 2])
We keep 1.40e+06/4.17e+07 =  3% of the original kernel matrix.

torch.Size([21372, 2])
We keep 2.72e+06/1.36e+08 =  2% of the original kernel matrix.

torch.Size([6150, 2])
We keep 3.25e+05/5.87e+06 =  5% of the original kernel matrix.

torch.Size([15327, 2])
We keep 1.29e+06/5.09e+07 =  2% of the original kernel matrix.

torch.Size([4634, 2])
We keep 1.74e+05/3.32e+06 =  5% of the original kernel matrix.

torch.Size([13817, 2])
We keep 1.07e+06/3.82e+07 =  2% of the original kernel matrix.

torch.Size([9865, 2])
We keep 7.12e+05/1.77e+07 =  4% of the original kernel matrix.

torch.Size([18931, 2])
We keep 1.94e+06/8.84e+07 =  2% of the original kernel matrix.

torch.Size([2153, 2])
We keep 3.79e+04/5.03e+05 =  7% of the original kernel matrix.

torch.Size([10385, 2])
We keep 5.68e+05/1.49e+07 =  3% of the original kernel matrix.

torch.Size([2074, 2])
We keep 5.70e+04/6.18e+05 =  9% of the original kernel matrix.

torch.Size([10134, 2])
We keep 6.22e+05/1.65e+07 =  3% of the original kernel matrix.

torch.Size([1226, 2])
We keep 1.81e+04/1.70e+05 = 10% of the original kernel matrix.

torch.Size([8498, 2])
We keep 4.02e+05/8.65e+06 =  4% of the original kernel matrix.

torch.Size([3493, 2])
We keep 9.20e+04/1.49e+06 =  6% of the original kernel matrix.

torch.Size([12308, 2])
We keep 7.99e+05/2.56e+07 =  3% of the original kernel matrix.

torch.Size([23835, 2])
We keep 6.25e+06/2.32e+08 =  2% of the original kernel matrix.

torch.Size([29261, 2])
We keep 5.23e+06/3.20e+08 =  1% of the original kernel matrix.

torch.Size([3932, 2])
We keep 1.15e+05/1.91e+06 =  6% of the original kernel matrix.

torch.Size([12892, 2])
We keep 8.80e+05/2.90e+07 =  3% of the original kernel matrix.

torch.Size([5931, 2])
We keep 4.92e+05/7.53e+06 =  6% of the original kernel matrix.

torch.Size([14711, 2])
We keep 1.41e+06/5.76e+07 =  2% of the original kernel matrix.

torch.Size([1959, 2])
We keep 4.40e+04/5.11e+05 =  8% of the original kernel matrix.

torch.Size([9885, 2])
We keep 5.66e+05/1.50e+07 =  3% of the original kernel matrix.

torch.Size([12695, 2])
We keep 1.10e+06/3.33e+07 =  3% of the original kernel matrix.

torch.Size([21235, 2])
We keep 2.42e+06/1.21e+08 =  1% of the original kernel matrix.

torch.Size([3838, 2])
We keep 2.21e+05/3.11e+06 =  7% of the original kernel matrix.

torch.Size([12638, 2])
We keep 1.07e+06/3.70e+07 =  2% of the original kernel matrix.

torch.Size([3872, 2])
We keep 1.24e+05/1.95e+06 =  6% of the original kernel matrix.

torch.Size([12748, 2])
We keep 8.86e+05/2.94e+07 =  3% of the original kernel matrix.

torch.Size([5647, 2])
We keep 3.71e+05/6.20e+06 =  5% of the original kernel matrix.

torch.Size([15004, 2])
We keep 1.39e+06/5.23e+07 =  2% of the original kernel matrix.

torch.Size([2920, 2])
We keep 7.60e+04/1.12e+06 =  6% of the original kernel matrix.

torch.Size([11599, 2])
We keep 7.49e+05/2.22e+07 =  3% of the original kernel matrix.

torch.Size([3798, 2])
We keep 1.34e+05/2.23e+06 =  6% of the original kernel matrix.

torch.Size([12805, 2])
We keep 9.50e+05/3.14e+07 =  3% of the original kernel matrix.

torch.Size([7480, 2])
We keep 5.33e+05/1.18e+07 =  4% of the original kernel matrix.

torch.Size([16484, 2])
We keep 1.72e+06/7.22e+07 =  2% of the original kernel matrix.

torch.Size([2157, 2])
We keep 5.23e+04/6.72e+05 =  7% of the original kernel matrix.

torch.Size([10293, 2])
We keep 6.33e+05/1.72e+07 =  3% of the original kernel matrix.

torch.Size([4036, 2])
We keep 1.59e+05/2.51e+06 =  6% of the original kernel matrix.

torch.Size([13028, 2])
We keep 9.79e+05/3.32e+07 =  2% of the original kernel matrix.

torch.Size([4426, 2])
We keep 1.75e+05/3.03e+06 =  5% of the original kernel matrix.

torch.Size([13202, 2])
We keep 1.01e+06/3.65e+07 =  2% of the original kernel matrix.

torch.Size([4607, 2])
We keep 1.97e+05/3.18e+06 =  6% of the original kernel matrix.

torch.Size([13688, 2])
We keep 1.06e+06/3.75e+07 =  2% of the original kernel matrix.

torch.Size([7430, 2])
We keep 5.59e+05/1.09e+07 =  5% of the original kernel matrix.

torch.Size([16898, 2])
We keep 1.70e+06/6.93e+07 =  2% of the original kernel matrix.

torch.Size([5018, 2])
We keep 2.48e+05/4.31e+06 =  5% of the original kernel matrix.

torch.Size([13777, 2])
We keep 1.16e+06/4.36e+07 =  2% of the original kernel matrix.

torch.Size([4625, 2])
We keep 1.92e+05/3.30e+06 =  5% of the original kernel matrix.

torch.Size([13654, 2])
We keep 1.06e+06/3.81e+07 =  2% of the original kernel matrix.

torch.Size([14474, 2])
We keep 1.31e+06/4.47e+07 =  2% of the original kernel matrix.

torch.Size([22981, 2])
We keep 2.71e+06/1.40e+08 =  1% of the original kernel matrix.

torch.Size([7603, 2])
We keep 5.32e+05/1.03e+07 =  5% of the original kernel matrix.

torch.Size([17183, 2])
We keep 1.64e+06/6.74e+07 =  2% of the original kernel matrix.

torch.Size([3078, 2])
We keep 7.66e+04/1.20e+06 =  6% of the original kernel matrix.

torch.Size([11839, 2])
We keep 7.46e+05/2.30e+07 =  3% of the original kernel matrix.

torch.Size([25114, 2])
We keep 8.34e+06/2.48e+08 =  3% of the original kernel matrix.

torch.Size([30585, 2])
We keep 5.42e+06/3.31e+08 =  1% of the original kernel matrix.

torch.Size([43937, 2])
We keep 5.73e+07/1.58e+09 =  3% of the original kernel matrix.

torch.Size([38433, 2])
We keep 1.14e+07/8.35e+08 =  1% of the original kernel matrix.

torch.Size([8980, 2])
We keep 8.26e+05/1.62e+07 =  5% of the original kernel matrix.

torch.Size([18497, 2])
We keep 1.83e+06/8.45e+07 =  2% of the original kernel matrix.

torch.Size([10275, 2])
We keep 1.17e+06/2.32e+07 =  5% of the original kernel matrix.

torch.Size([19322, 2])
We keep 2.13e+06/1.01e+08 =  2% of the original kernel matrix.

torch.Size([2657, 2])
We keep 1.02e+05/1.05e+06 =  9% of the original kernel matrix.

torch.Size([10747, 2])
We keep 7.08e+05/2.15e+07 =  3% of the original kernel matrix.

torch.Size([9449, 2])
We keep 6.89e+05/1.79e+07 =  3% of the original kernel matrix.

torch.Size([18483, 2])
We keep 1.96e+06/8.88e+07 =  2% of the original kernel matrix.

torch.Size([13102, 2])
We keep 1.27e+06/4.08e+07 =  3% of the original kernel matrix.

torch.Size([21725, 2])
We keep 2.67e+06/1.34e+08 =  1% of the original kernel matrix.

torch.Size([93794, 2])
We keep 1.67e+08/5.99e+09 =  2% of the original kernel matrix.

torch.Size([57418, 2])
We keep 2.01e+07/1.63e+09 =  1% of the original kernel matrix.

torch.Size([5955, 2])
We keep 2.95e+05/6.25e+06 =  4% of the original kernel matrix.

torch.Size([15371, 2])
We keep 1.36e+06/5.25e+07 =  2% of the original kernel matrix.

torch.Size([178508, 2])
We keep 1.60e+08/1.39e+10 =  1% of the original kernel matrix.

torch.Size([81583, 2])
We keep 2.89e+07/2.48e+09 =  1% of the original kernel matrix.

torch.Size([28565, 2])
We keep 1.31e+07/3.38e+08 =  3% of the original kernel matrix.

torch.Size([32588, 2])
We keep 5.79e+06/3.86e+08 =  1% of the original kernel matrix.

torch.Size([45995, 2])
We keep 1.55e+07/7.17e+08 =  2% of the original kernel matrix.

torch.Size([40737, 2])
We keep 8.04e+06/5.62e+08 =  1% of the original kernel matrix.

torch.Size([19316, 2])
We keep 3.09e+06/1.12e+08 =  2% of the original kernel matrix.

torch.Size([26860, 2])
We keep 3.88e+06/2.22e+08 =  1% of the original kernel matrix.

torch.Size([4469, 2])
We keep 1.48e+05/2.83e+06 =  5% of the original kernel matrix.

torch.Size([13659, 2])
We keep 1.01e+06/3.53e+07 =  2% of the original kernel matrix.

torch.Size([7914, 2])
We keep 6.32e+05/1.14e+07 =  5% of the original kernel matrix.

torch.Size([17264, 2])
We keep 1.67e+06/7.11e+07 =  2% of the original kernel matrix.

torch.Size([361625, 2])
We keep 4.96e+08/4.76e+10 =  1% of the original kernel matrix.

torch.Size([117552, 2])
We keep 4.95e+07/4.58e+09 =  1% of the original kernel matrix.

torch.Size([441932, 2])
We keep 6.21e+08/5.24e+10 =  1% of the original kernel matrix.

torch.Size([130278, 2])
We keep 5.05e+07/4.81e+09 =  1% of the original kernel matrix.

torch.Size([8459, 2])
We keep 6.40e+05/1.11e+07 =  5% of the original kernel matrix.

torch.Size([17784, 2])
We keep 1.63e+06/6.99e+07 =  2% of the original kernel matrix.

torch.Size([50815, 2])
We keep 2.04e+07/1.01e+09 =  2% of the original kernel matrix.

torch.Size([43547, 2])
We keep 9.50e+06/6.67e+08 =  1% of the original kernel matrix.

torch.Size([30745, 2])
We keep 1.83e+07/4.24e+08 =  4% of the original kernel matrix.

torch.Size([32636, 2])
We keep 6.27e+06/4.33e+08 =  1% of the original kernel matrix.

torch.Size([24015, 2])
We keep 9.48e+06/1.91e+08 =  4% of the original kernel matrix.

torch.Size([30048, 2])
We keep 4.83e+06/2.91e+08 =  1% of the original kernel matrix.

torch.Size([76921, 2])
We keep 9.23e+07/3.35e+09 =  2% of the original kernel matrix.

torch.Size([51479, 2])
We keep 1.56e+07/1.22e+09 =  1% of the original kernel matrix.

torch.Size([30032, 2])
We keep 6.47e+07/1.05e+09 =  6% of the original kernel matrix.

torch.Size([29779, 2])
We keep 9.55e+06/6.81e+08 =  1% of the original kernel matrix.

torch.Size([30733, 2])
We keep 8.73e+06/3.19e+08 =  2% of the original kernel matrix.

torch.Size([32368, 2])
We keep 5.43e+06/3.75e+08 =  1% of the original kernel matrix.

torch.Size([22323, 2])
We keep 4.27e+06/1.47e+08 =  2% of the original kernel matrix.

torch.Size([28934, 2])
We keep 4.26e+06/2.55e+08 =  1% of the original kernel matrix.

torch.Size([3550, 2])
We keep 1.08e+05/1.80e+06 =  6% of the original kernel matrix.

torch.Size([12531, 2])
We keep 8.62e+05/2.81e+07 =  3% of the original kernel matrix.

torch.Size([110298, 2])
We keep 2.52e+08/1.09e+10 =  2% of the original kernel matrix.

torch.Size([60229, 2])
We keep 2.60e+07/2.19e+09 =  1% of the original kernel matrix.

torch.Size([11093, 2])
We keep 1.87e+06/2.86e+07 =  6% of the original kernel matrix.

torch.Size([19908, 2])
We keep 2.15e+06/1.12e+08 =  1% of the original kernel matrix.

torch.Size([6342, 2])
We keep 3.34e+05/6.97e+06 =  4% of the original kernel matrix.

torch.Size([15370, 2])
We keep 1.36e+06/5.54e+07 =  2% of the original kernel matrix.

torch.Size([21148, 2])
We keep 3.39e+06/1.21e+08 =  2% of the original kernel matrix.

torch.Size([27975, 2])
We keep 3.93e+06/2.31e+08 =  1% of the original kernel matrix.

torch.Size([76315, 2])
We keep 8.61e+07/2.83e+09 =  3% of the original kernel matrix.

torch.Size([51035, 2])
We keep 1.46e+07/1.12e+09 =  1% of the original kernel matrix.

torch.Size([286149, 2])
We keep 4.18e+08/3.44e+10 =  1% of the original kernel matrix.

torch.Size([105758, 2])
We keep 4.30e+07/3.90e+09 =  1% of the original kernel matrix.

torch.Size([94129, 2])
We keep 4.64e+07/3.02e+09 =  1% of the original kernel matrix.

torch.Size([58092, 2])
We keep 1.48e+07/1.15e+09 =  1% of the original kernel matrix.

torch.Size([5307, 2])
We keep 2.13e+05/4.23e+06 =  5% of the original kernel matrix.

torch.Size([14405, 2])
We keep 1.15e+06/4.32e+07 =  2% of the original kernel matrix.

torch.Size([53020, 2])
We keep 2.12e+07/1.02e+09 =  2% of the original kernel matrix.

torch.Size([43597, 2])
We keep 9.43e+06/6.70e+08 =  1% of the original kernel matrix.

torch.Size([14611, 2])
We keep 1.88e+06/5.41e+07 =  3% of the original kernel matrix.

torch.Size([22870, 2])
We keep 2.95e+06/1.55e+08 =  1% of the original kernel matrix.

torch.Size([36251, 2])
We keep 1.39e+07/5.59e+08 =  2% of the original kernel matrix.

torch.Size([36333, 2])
We keep 7.27e+06/4.97e+08 =  1% of the original kernel matrix.

torch.Size([70679, 2])
We keep 1.94e+07/1.50e+09 =  1% of the original kernel matrix.

torch.Size([50705, 2])
We keep 1.08e+07/8.13e+08 =  1% of the original kernel matrix.

torch.Size([22848, 2])
We keep 9.67e+06/2.32e+08 =  4% of the original kernel matrix.

torch.Size([29098, 2])
We keep 5.35e+06/3.20e+08 =  1% of the original kernel matrix.

torch.Size([67802, 2])
We keep 2.79e+07/1.51e+09 =  1% of the original kernel matrix.

torch.Size([49191, 2])
We keep 1.08e+07/8.15e+08 =  1% of the original kernel matrix.

torch.Size([1590, 2])
We keep 2.38e+04/2.63e+05 =  9% of the original kernel matrix.

torch.Size([9324, 2])
We keep 4.64e+05/1.08e+07 =  4% of the original kernel matrix.

torch.Size([12525, 2])
We keep 1.07e+06/3.05e+07 =  3% of the original kernel matrix.

torch.Size([21232, 2])
We keep 2.36e+06/1.16e+08 =  2% of the original kernel matrix.

torch.Size([123136, 2])
We keep 8.32e+07/5.59e+09 =  1% of the original kernel matrix.

torch.Size([66782, 2])
We keep 1.94e+07/1.57e+09 =  1% of the original kernel matrix.

torch.Size([46103, 2])
We keep 2.91e+07/1.09e+09 =  2% of the original kernel matrix.

torch.Size([39674, 2])
We keep 9.65e+06/6.94e+08 =  1% of the original kernel matrix.

torch.Size([86980, 2])
We keep 5.76e+07/2.68e+09 =  2% of the original kernel matrix.

torch.Size([54687, 2])
We keep 1.40e+07/1.09e+09 =  1% of the original kernel matrix.

torch.Size([7820, 2])
We keep 5.25e+05/1.18e+07 =  4% of the original kernel matrix.

torch.Size([16938, 2])
We keep 1.65e+06/7.23e+07 =  2% of the original kernel matrix.

torch.Size([292422, 2])
We keep 3.31e+08/3.84e+10 =  0% of the original kernel matrix.

torch.Size([106146, 2])
We keep 4.48e+07/4.11e+09 =  1% of the original kernel matrix.

torch.Size([21081, 2])
We keep 2.27e+06/1.09e+08 =  2% of the original kernel matrix.

torch.Size([28198, 2])
We keep 3.76e+06/2.19e+08 =  1% of the original kernel matrix.

torch.Size([38383, 2])
We keep 9.05e+06/4.76e+08 =  1% of the original kernel matrix.

torch.Size([37708, 2])
We keep 6.86e+06/4.58e+08 =  1% of the original kernel matrix.

torch.Size([122274, 2])
We keep 7.82e+07/5.75e+09 =  1% of the original kernel matrix.

torch.Size([66888, 2])
We keep 1.95e+07/1.59e+09 =  1% of the original kernel matrix.

torch.Size([3994, 2])
We keep 1.32e+05/2.26e+06 =  5% of the original kernel matrix.

torch.Size([12950, 2])
We keep 9.30e+05/3.16e+07 =  2% of the original kernel matrix.

torch.Size([120848, 2])
We keep 8.31e+07/4.70e+09 =  1% of the original kernel matrix.

torch.Size([66046, 2])
We keep 1.79e+07/1.44e+09 =  1% of the original kernel matrix.

torch.Size([207241, 2])
We keep 4.87e+08/2.13e+10 =  2% of the original kernel matrix.

torch.Size([90239, 2])
We keep 3.49e+07/3.07e+09 =  1% of the original kernel matrix.

torch.Size([705996, 2])
We keep 2.58e+09/1.98e+11 =  1% of the original kernel matrix.

torch.Size([169224, 2])
We keep 9.51e+07/9.35e+09 =  1% of the original kernel matrix.

torch.Size([60795, 2])
We keep 1.85e+07/1.14e+09 =  1% of the original kernel matrix.

torch.Size([46230, 2])
We keep 9.75e+06/7.10e+08 =  1% of the original kernel matrix.

torch.Size([37344, 2])
We keep 1.07e+07/5.56e+08 =  1% of the original kernel matrix.

torch.Size([37017, 2])
We keep 7.33e+06/4.95e+08 =  1% of the original kernel matrix.

torch.Size([8996, 2])
We keep 1.08e+06/1.81e+07 =  5% of the original kernel matrix.

torch.Size([18008, 2])
We keep 1.94e+06/8.93e+07 =  2% of the original kernel matrix.

torch.Size([11468, 2])
We keep 8.04e+05/2.44e+07 =  3% of the original kernel matrix.

torch.Size([20417, 2])
We keep 2.16e+06/1.04e+08 =  2% of the original kernel matrix.

torch.Size([197318, 2])
We keep 2.66e+08/1.86e+10 =  1% of the original kernel matrix.

torch.Size([86600, 2])
We keep 3.29e+07/2.86e+09 =  1% of the original kernel matrix.

torch.Size([13392, 2])
We keep 1.23e+06/4.12e+07 =  2% of the original kernel matrix.

torch.Size([22194, 2])
We keep 2.68e+06/1.35e+08 =  1% of the original kernel matrix.

torch.Size([33320, 2])
We keep 1.12e+07/3.92e+08 =  2% of the original kernel matrix.

torch.Size([35101, 2])
We keep 6.27e+06/4.16e+08 =  1% of the original kernel matrix.

torch.Size([38032, 2])
We keep 3.26e+07/8.20e+08 =  3% of the original kernel matrix.

torch.Size([37525, 2])
We keep 8.27e+06/6.01e+08 =  1% of the original kernel matrix.

torch.Size([25464, 2])
We keep 7.06e+06/1.89e+08 =  3% of the original kernel matrix.

torch.Size([30855, 2])
We keep 4.58e+06/2.88e+08 =  1% of the original kernel matrix.

torch.Size([25978, 2])
We keep 9.78e+06/2.64e+08 =  3% of the original kernel matrix.

torch.Size([30553, 2])
We keep 5.55e+06/3.41e+08 =  1% of the original kernel matrix.

torch.Size([6904, 2])
We keep 3.12e+05/7.39e+06 =  4% of the original kernel matrix.

torch.Size([16422, 2])
We keep 1.38e+06/5.71e+07 =  2% of the original kernel matrix.

torch.Size([118195, 2])
We keep 2.54e+08/1.37e+10 =  1% of the original kernel matrix.

torch.Size([64528, 2])
We keep 2.82e+07/2.46e+09 =  1% of the original kernel matrix.

torch.Size([16510, 2])
We keep 4.31e+06/8.87e+07 =  4% of the original kernel matrix.

torch.Size([24781, 2])
We keep 3.60e+06/1.98e+08 =  1% of the original kernel matrix.

torch.Size([19896, 2])
We keep 1.52e+07/2.16e+08 =  7% of the original kernel matrix.

torch.Size([26703, 2])
We keep 5.05e+06/3.09e+08 =  1% of the original kernel matrix.

torch.Size([19880, 2])
We keep 2.51e+06/9.70e+07 =  2% of the original kernel matrix.

torch.Size([27264, 2])
We keep 3.66e+06/2.07e+08 =  1% of the original kernel matrix.

torch.Size([58609, 2])
We keep 4.93e+07/1.79e+09 =  2% of the original kernel matrix.

torch.Size([46082, 2])
We keep 1.17e+07/8.88e+08 =  1% of the original kernel matrix.

torch.Size([26474, 2])
We keep 5.09e+06/2.44e+08 =  2% of the original kernel matrix.

torch.Size([31888, 2])
We keep 5.24e+06/3.28e+08 =  1% of the original kernel matrix.

torch.Size([167791, 2])
We keep 3.97e+08/1.65e+10 =  2% of the original kernel matrix.

torch.Size([78442, 2])
We keep 3.12e+07/2.70e+09 =  1% of the original kernel matrix.

torch.Size([8211, 2])
We keep 1.44e+06/2.01e+07 =  7% of the original kernel matrix.

torch.Size([17402, 2])
We keep 2.02e+06/9.42e+07 =  2% of the original kernel matrix.

torch.Size([29295, 2])
We keep 1.70e+07/5.34e+08 =  3% of the original kernel matrix.

torch.Size([31458, 2])
We keep 7.29e+06/4.86e+08 =  1% of the original kernel matrix.

torch.Size([14549, 2])
We keep 1.43e+06/5.15e+07 =  2% of the original kernel matrix.

torch.Size([23217, 2])
We keep 2.86e+06/1.51e+08 =  1% of the original kernel matrix.

torch.Size([163045, 2])
We keep 4.34e+08/1.90e+10 =  2% of the original kernel matrix.

torch.Size([76258, 2])
We keep 3.26e+07/2.90e+09 =  1% of the original kernel matrix.

torch.Size([40283, 2])
We keep 1.08e+07/7.04e+08 =  1% of the original kernel matrix.

torch.Size([38128, 2])
We keep 7.94e+06/5.57e+08 =  1% of the original kernel matrix.

torch.Size([17435, 2])
We keep 1.79e+06/7.26e+07 =  2% of the original kernel matrix.

torch.Size([25456, 2])
We keep 3.24e+06/1.79e+08 =  1% of the original kernel matrix.

torch.Size([152591, 2])
We keep 7.90e+07/7.72e+09 =  1% of the original kernel matrix.

torch.Size([75221, 2])
We keep 2.20e+07/1.84e+09 =  1% of the original kernel matrix.

torch.Size([185671, 2])
We keep 1.19e+08/1.20e+10 =  0% of the original kernel matrix.

torch.Size([83754, 2])
We keep 2.67e+07/2.30e+09 =  1% of the original kernel matrix.

torch.Size([6677, 2])
We keep 4.50e+05/9.28e+06 =  4% of the original kernel matrix.

torch.Size([16033, 2])
We keep 1.56e+06/6.40e+07 =  2% of the original kernel matrix.

torch.Size([162292, 2])
We keep 3.32e+08/1.13e+10 =  2% of the original kernel matrix.

torch.Size([77678, 2])
We keep 2.63e+07/2.24e+09 =  1% of the original kernel matrix.

torch.Size([219589, 2])
We keep 1.72e+08/1.86e+10 =  0% of the original kernel matrix.

torch.Size([91617, 2])
We keep 3.24e+07/2.86e+09 =  1% of the original kernel matrix.

torch.Size([462728, 2])
We keep 6.30e+08/7.21e+10 =  0% of the original kernel matrix.

torch.Size([136838, 2])
We keep 5.94e+07/5.64e+09 =  1% of the original kernel matrix.

torch.Size([25385, 2])
We keep 1.22e+07/2.43e+08 =  5% of the original kernel matrix.

torch.Size([30732, 2])
We keep 5.19e+06/3.27e+08 =  1% of the original kernel matrix.

torch.Size([5691, 2])
We keep 2.77e+05/5.32e+06 =  5% of the original kernel matrix.

torch.Size([14769, 2])
We keep 1.24e+06/4.84e+07 =  2% of the original kernel matrix.

torch.Size([17657, 2])
We keep 1.72e+06/6.85e+07 =  2% of the original kernel matrix.

torch.Size([25573, 2])
We keep 3.18e+06/1.74e+08 =  1% of the original kernel matrix.

torch.Size([14595, 2])
We keep 2.39e+06/7.92e+07 =  3% of the original kernel matrix.

torch.Size([23147, 2])
We keep 3.32e+06/1.87e+08 =  1% of the original kernel matrix.

torch.Size([26227, 2])
We keep 5.02e+06/2.24e+08 =  2% of the original kernel matrix.

torch.Size([31068, 2])
We keep 5.07e+06/3.14e+08 =  1% of the original kernel matrix.

torch.Size([59452, 2])
We keep 4.15e+07/1.30e+09 =  3% of the original kernel matrix.

torch.Size([46640, 2])
We keep 1.05e+07/7.58e+08 =  1% of the original kernel matrix.

torch.Size([3461, 2])
We keep 1.14e+05/1.77e+06 =  6% of the original kernel matrix.

torch.Size([12204, 2])
We keep 8.61e+05/2.79e+07 =  3% of the original kernel matrix.

torch.Size([76656, 2])
We keep 3.70e+07/1.95e+09 =  1% of the original kernel matrix.

torch.Size([52624, 2])
We keep 1.24e+07/9.27e+08 =  1% of the original kernel matrix.

torch.Size([145269, 2])
We keep 9.74e+07/8.13e+09 =  1% of the original kernel matrix.

torch.Size([73537, 2])
We keep 2.27e+07/1.89e+09 =  1% of the original kernel matrix.

torch.Size([4849, 2])
We keep 3.88e+06/1.36e+07 = 28% of the original kernel matrix.

torch.Size([13479, 2])
We keep 1.37e+06/7.74e+07 =  1% of the original kernel matrix.

torch.Size([11333, 2])
We keep 1.23e+06/3.07e+07 =  3% of the original kernel matrix.

torch.Size([20325, 2])
We keep 2.32e+06/1.16e+08 =  1% of the original kernel matrix.

torch.Size([49402, 2])
We keep 1.71e+07/8.04e+08 =  2% of the original kernel matrix.

torch.Size([42439, 2])
We keep 8.18e+06/5.96e+08 =  1% of the original kernel matrix.

torch.Size([13929, 2])
We keep 1.53e+06/4.56e+07 =  3% of the original kernel matrix.

torch.Size([22358, 2])
We keep 2.76e+06/1.42e+08 =  1% of the original kernel matrix.

torch.Size([16565, 2])
We keep 3.86e+06/1.02e+08 =  3% of the original kernel matrix.

torch.Size([24530, 2])
We keep 3.77e+06/2.12e+08 =  1% of the original kernel matrix.

torch.Size([22331, 2])
We keep 3.15e+06/1.49e+08 =  2% of the original kernel matrix.

torch.Size([28957, 2])
We keep 4.34e+06/2.56e+08 =  1% of the original kernel matrix.

torch.Size([6029, 2])
We keep 3.03e+05/6.05e+06 =  5% of the original kernel matrix.

torch.Size([15492, 2])
We keep 1.34e+06/5.16e+07 =  2% of the original kernel matrix.

torch.Size([45854, 2])
We keep 2.86e+07/9.69e+08 =  2% of the original kernel matrix.

torch.Size([40282, 2])
We keep 9.23e+06/6.54e+08 =  1% of the original kernel matrix.

torch.Size([44755, 2])
We keep 1.13e+07/6.88e+08 =  1% of the original kernel matrix.

torch.Size([40885, 2])
We keep 7.87e+06/5.51e+08 =  1% of the original kernel matrix.

torch.Size([25478, 2])
We keep 5.69e+06/2.27e+08 =  2% of the original kernel matrix.

torch.Size([31485, 2])
We keep 5.16e+06/3.16e+08 =  1% of the original kernel matrix.

torch.Size([55750, 2])
We keep 3.46e+07/1.43e+09 =  2% of the original kernel matrix.

torch.Size([45120, 2])
We keep 1.10e+07/7.95e+08 =  1% of the original kernel matrix.

torch.Size([15295, 2])
We keep 2.28e+06/6.66e+07 =  3% of the original kernel matrix.

torch.Size([23543, 2])
We keep 3.24e+06/1.71e+08 =  1% of the original kernel matrix.

torch.Size([564169, 2])
We keep 1.32e+09/1.27e+11 =  1% of the original kernel matrix.

torch.Size([150103, 2])
We keep 7.68e+07/7.49e+09 =  1% of the original kernel matrix.

torch.Size([19141, 2])
We keep 2.25e+06/9.32e+07 =  2% of the original kernel matrix.

torch.Size([26708, 2])
We keep 3.59e+06/2.03e+08 =  1% of the original kernel matrix.

torch.Size([124830, 2])
We keep 6.87e+07/5.34e+09 =  1% of the original kernel matrix.

torch.Size([67534, 2])
We keep 1.87e+07/1.54e+09 =  1% of the original kernel matrix.

torch.Size([5346, 2])
We keep 2.10e+05/4.11e+06 =  5% of the original kernel matrix.

torch.Size([14760, 2])
We keep 1.16e+06/4.26e+07 =  2% of the original kernel matrix.

torch.Size([6907, 2])
We keep 3.91e+05/8.57e+06 =  4% of the original kernel matrix.

torch.Size([16272, 2])
We keep 1.52e+06/6.15e+07 =  2% of the original kernel matrix.

torch.Size([6583, 2])
We keep 3.25e+05/7.44e+06 =  4% of the original kernel matrix.

torch.Size([15750, 2])
We keep 1.37e+06/5.73e+07 =  2% of the original kernel matrix.

torch.Size([19180, 2])
We keep 2.68e+06/1.07e+08 =  2% of the original kernel matrix.

torch.Size([26939, 2])
We keep 3.75e+06/2.17e+08 =  1% of the original kernel matrix.

torch.Size([8822, 2])
We keep 8.87e+05/1.81e+07 =  4% of the original kernel matrix.

torch.Size([17932, 2])
We keep 1.99e+06/8.93e+07 =  2% of the original kernel matrix.

torch.Size([58754, 2])
We keep 6.60e+07/1.64e+09 =  4% of the original kernel matrix.

torch.Size([46120, 2])
We keep 1.16e+07/8.50e+08 =  1% of the original kernel matrix.

torch.Size([9286, 2])
We keep 6.64e+05/1.71e+07 =  3% of the original kernel matrix.

torch.Size([18340, 2])
We keep 1.93e+06/8.70e+07 =  2% of the original kernel matrix.

torch.Size([7515, 2])
We keep 4.52e+05/1.04e+07 =  4% of the original kernel matrix.

torch.Size([16601, 2])
We keep 1.63e+06/6.79e+07 =  2% of the original kernel matrix.

torch.Size([254969, 2])
We keep 3.25e+08/2.77e+10 =  1% of the original kernel matrix.

torch.Size([102470, 2])
We keep 3.98e+07/3.50e+09 =  1% of the original kernel matrix.

torch.Size([50682, 2])
We keep 2.71e+07/1.13e+09 =  2% of the original kernel matrix.

torch.Size([43036, 2])
We keep 1.00e+07/7.05e+08 =  1% of the original kernel matrix.

torch.Size([38731, 2])
We keep 9.95e+06/5.60e+08 =  1% of the original kernel matrix.

torch.Size([38158, 2])
We keep 7.41e+06/4.97e+08 =  1% of the original kernel matrix.

torch.Size([147886, 2])
We keep 9.61e+07/8.14e+09 =  1% of the original kernel matrix.

torch.Size([75307, 2])
We keep 2.29e+07/1.90e+09 =  1% of the original kernel matrix.

torch.Size([425055, 2])
We keep 5.24e+08/6.29e+10 =  0% of the original kernel matrix.

torch.Size([130450, 2])
We keep 5.60e+07/5.27e+09 =  1% of the original kernel matrix.

torch.Size([14577, 2])
We keep 1.64e+06/5.77e+07 =  2% of the original kernel matrix.

torch.Size([23019, 2])
We keep 3.08e+06/1.60e+08 =  1% of the original kernel matrix.

torch.Size([18591, 2])
We keep 2.03e+06/8.34e+07 =  2% of the original kernel matrix.

torch.Size([26133, 2])
We keep 3.43e+06/1.92e+08 =  1% of the original kernel matrix.

torch.Size([22349, 2])
We keep 8.77e+06/2.33e+08 =  3% of the original kernel matrix.

torch.Size([28538, 2])
We keep 5.19e+06/3.21e+08 =  1% of the original kernel matrix.

torch.Size([3465, 2])
We keep 1.32e+05/2.16e+06 =  6% of the original kernel matrix.

torch.Size([12141, 2])
We keep 9.46e+05/3.09e+07 =  3% of the original kernel matrix.

torch.Size([9183, 2])
We keep 5.52e+05/1.45e+07 =  3% of the original kernel matrix.

torch.Size([18100, 2])
We keep 1.77e+06/8.00e+07 =  2% of the original kernel matrix.

torch.Size([24409, 2])
We keep 4.33e+06/1.70e+08 =  2% of the original kernel matrix.

torch.Size([30173, 2])
We keep 4.51e+06/2.74e+08 =  1% of the original kernel matrix.

torch.Size([7022, 2])
We keep 4.33e+05/8.78e+06 =  4% of the original kernel matrix.

torch.Size([16212, 2])
We keep 1.45e+06/6.22e+07 =  2% of the original kernel matrix.

torch.Size([7678, 2])
We keep 4.91e+05/1.16e+07 =  4% of the original kernel matrix.

torch.Size([16851, 2])
We keep 1.69e+06/7.15e+07 =  2% of the original kernel matrix.

torch.Size([13592, 2])
We keep 1.09e+06/3.61e+07 =  3% of the original kernel matrix.

torch.Size([22038, 2])
We keep 2.46e+06/1.26e+08 =  1% of the original kernel matrix.

torch.Size([11762, 2])
We keep 1.24e+06/3.09e+07 =  4% of the original kernel matrix.

torch.Size([20737, 2])
We keep 2.34e+06/1.17e+08 =  2% of the original kernel matrix.

torch.Size([318509, 2])
We keep 4.53e+08/4.31e+10 =  1% of the original kernel matrix.

torch.Size([111293, 2])
We keep 4.65e+07/4.36e+09 =  1% of the original kernel matrix.

torch.Size([16221, 2])
We keep 5.63e+06/1.61e+08 =  3% of the original kernel matrix.

torch.Size([23721, 2])
We keep 4.49e+06/2.67e+08 =  1% of the original kernel matrix.

torch.Size([101445, 2])
We keep 7.28e+07/3.76e+09 =  1% of the original kernel matrix.

torch.Size([60267, 2])
We keep 1.64e+07/1.29e+09 =  1% of the original kernel matrix.

torch.Size([9812, 2])
We keep 1.29e+06/2.44e+07 =  5% of the original kernel matrix.

torch.Size([18865, 2])
We keep 2.06e+06/1.04e+08 =  1% of the original kernel matrix.

torch.Size([6892, 2])
We keep 4.94e+05/9.33e+06 =  5% of the original kernel matrix.

torch.Size([15830, 2])
We keep 1.53e+06/6.42e+07 =  2% of the original kernel matrix.

torch.Size([100868, 2])
We keep 6.02e+07/3.68e+09 =  1% of the original kernel matrix.

torch.Size([59613, 2])
We keep 1.62e+07/1.27e+09 =  1% of the original kernel matrix.

torch.Size([229259, 2])
We keep 6.45e+08/2.77e+10 =  2% of the original kernel matrix.

torch.Size([94075, 2])
We keep 3.72e+07/3.50e+09 =  1% of the original kernel matrix.

torch.Size([482321, 2])
We keep 5.44e+08/6.77e+10 =  0% of the original kernel matrix.

torch.Size([136732, 2])
We keep 5.66e+07/5.47e+09 =  1% of the original kernel matrix.

torch.Size([21652, 2])
We keep 3.56e+06/1.46e+08 =  2% of the original kernel matrix.

torch.Size([28630, 2])
We keep 4.24e+06/2.54e+08 =  1% of the original kernel matrix.

torch.Size([41703, 2])
We keep 1.38e+07/6.53e+08 =  2% of the original kernel matrix.

torch.Size([39752, 2])
We keep 7.84e+06/5.37e+08 =  1% of the original kernel matrix.

torch.Size([6011, 2])
We keep 2.31e+05/4.79e+06 =  4% of the original kernel matrix.

torch.Size([15533, 2])
We keep 1.21e+06/4.60e+07 =  2% of the original kernel matrix.

torch.Size([9477, 2])
We keep 1.08e+06/1.88e+07 =  5% of the original kernel matrix.

torch.Size([18571, 2])
We keep 1.88e+06/9.10e+07 =  2% of the original kernel matrix.

torch.Size([954670, 2])
We keep 5.55e+09/4.13e+11 =  1% of the original kernel matrix.

torch.Size([191371, 2])
We keep 1.32e+08/1.35e+10 =  0% of the original kernel matrix.

torch.Size([23313, 2])
We keep 4.39e+06/1.64e+08 =  2% of the original kernel matrix.

torch.Size([29966, 2])
We keep 4.47e+06/2.69e+08 =  1% of the original kernel matrix.

torch.Size([16803, 2])
We keep 1.66e+06/6.27e+07 =  2% of the original kernel matrix.

torch.Size([24686, 2])
We keep 3.08e+06/1.66e+08 =  1% of the original kernel matrix.

torch.Size([42314, 2])
We keep 2.00e+07/8.37e+08 =  2% of the original kernel matrix.

torch.Size([38976, 2])
We keep 8.70e+06/6.08e+08 =  1% of the original kernel matrix.

torch.Size([722083, 2])
We keep 2.04e+09/2.12e+11 =  0% of the original kernel matrix.

torch.Size([171151, 2])
We keep 9.76e+07/9.67e+09 =  1% of the original kernel matrix.

torch.Size([52934, 2])
We keep 1.61e+07/9.66e+08 =  1% of the original kernel matrix.

torch.Size([43069, 2])
We keep 9.09e+06/6.53e+08 =  1% of the original kernel matrix.

torch.Size([16327, 2])
We keep 2.29e+06/7.19e+07 =  3% of the original kernel matrix.

torch.Size([24231, 2])
We keep 3.24e+06/1.78e+08 =  1% of the original kernel matrix.

torch.Size([11291, 2])
We keep 1.87e+06/3.95e+07 =  4% of the original kernel matrix.

torch.Size([20216, 2])
We keep 2.72e+06/1.32e+08 =  2% of the original kernel matrix.

torch.Size([41914, 2])
We keep 9.86e+06/6.17e+08 =  1% of the original kernel matrix.

torch.Size([39684, 2])
We keep 7.73e+06/5.22e+08 =  1% of the original kernel matrix.

torch.Size([8091, 2])
We keep 4.39e+05/1.02e+07 =  4% of the original kernel matrix.

torch.Size([17575, 2])
We keep 1.51e+06/6.72e+07 =  2% of the original kernel matrix.

torch.Size([201262, 2])
We keep 2.22e+08/1.75e+10 =  1% of the original kernel matrix.

torch.Size([88056, 2])
We keep 3.18e+07/2.77e+09 =  1% of the original kernel matrix.

torch.Size([19584, 2])
We keep 3.55e+06/1.06e+08 =  3% of the original kernel matrix.

torch.Size([26702, 2])
We keep 3.78e+06/2.16e+08 =  1% of the original kernel matrix.

torch.Size([2919, 2])
We keep 8.85e+04/1.13e+06 =  7% of the original kernel matrix.

torch.Size([11475, 2])
We keep 7.38e+05/2.23e+07 =  3% of the original kernel matrix.

torch.Size([52963, 2])
We keep 1.99e+07/1.03e+09 =  1% of the original kernel matrix.

torch.Size([44456, 2])
We keep 9.28e+06/6.75e+08 =  1% of the original kernel matrix.

torch.Size([30890, 2])
We keep 6.39e+06/3.61e+08 =  1% of the original kernel matrix.

torch.Size([35493, 2])
We keep 6.42e+06/3.99e+08 =  1% of the original kernel matrix.

torch.Size([3386, 2])
We keep 1.08e+05/1.67e+06 =  6% of the original kernel matrix.

torch.Size([12111, 2])
We keep 8.45e+05/2.71e+07 =  3% of the original kernel matrix.

torch.Size([102135, 2])
We keep 6.47e+07/3.77e+09 =  1% of the original kernel matrix.

torch.Size([60259, 2])
We keep 1.63e+07/1.29e+09 =  1% of the original kernel matrix.

torch.Size([107183, 2])
We keep 7.63e+07/4.02e+09 =  1% of the original kernel matrix.

torch.Size([62388, 2])
We keep 1.66e+07/1.33e+09 =  1% of the original kernel matrix.

torch.Size([237384, 2])
We keep 5.36e+08/2.71e+10 =  1% of the original kernel matrix.

torch.Size([97261, 2])
We keep 3.92e+07/3.46e+09 =  1% of the original kernel matrix.

torch.Size([221279, 2])
We keep 3.16e+08/2.15e+10 =  1% of the original kernel matrix.

torch.Size([92683, 2])
We keep 3.49e+07/3.08e+09 =  1% of the original kernel matrix.

torch.Size([235660, 2])
We keep 2.88e+08/2.21e+10 =  1% of the original kernel matrix.

torch.Size([95419, 2])
We keep 3.55e+07/3.12e+09 =  1% of the original kernel matrix.

torch.Size([25915, 2])
We keep 8.36e+06/2.71e+08 =  3% of the original kernel matrix.

torch.Size([30462, 2])
We keep 5.49e+06/3.46e+08 =  1% of the original kernel matrix.

torch.Size([24079, 2])
We keep 4.64e+06/1.73e+08 =  2% of the original kernel matrix.

torch.Size([29802, 2])
We keep 4.48e+06/2.76e+08 =  1% of the original kernel matrix.

torch.Size([107037, 2])
We keep 5.03e+07/3.57e+09 =  1% of the original kernel matrix.

torch.Size([61916, 2])
We keep 1.59e+07/1.26e+09 =  1% of the original kernel matrix.

torch.Size([193493, 2])
We keep 1.44e+08/1.22e+10 =  1% of the original kernel matrix.

torch.Size([85577, 2])
We keep 2.63e+07/2.32e+09 =  1% of the original kernel matrix.

torch.Size([13335, 2])
We keep 1.49e+06/4.15e+07 =  3% of the original kernel matrix.

torch.Size([22055, 2])
We keep 2.63e+06/1.35e+08 =  1% of the original kernel matrix.

torch.Size([68673, 2])
We keep 6.69e+07/2.33e+09 =  2% of the original kernel matrix.

torch.Size([50152, 2])
We keep 1.26e+07/1.01e+09 =  1% of the original kernel matrix.

torch.Size([132612, 2])
We keep 1.32e+08/8.02e+09 =  1% of the original kernel matrix.

torch.Size([70136, 2])
We keep 2.22e+07/1.88e+09 =  1% of the original kernel matrix.

torch.Size([135254, 2])
We keep 1.65e+08/7.00e+09 =  2% of the original kernel matrix.

torch.Size([70830, 2])
We keep 2.11e+07/1.76e+09 =  1% of the original kernel matrix.

torch.Size([97148, 2])
We keep 8.20e+07/3.63e+09 =  2% of the original kernel matrix.

torch.Size([58960, 2])
We keep 1.62e+07/1.27e+09 =  1% of the original kernel matrix.

torch.Size([52705, 2])
We keep 9.27e+07/1.79e+09 =  5% of the original kernel matrix.

torch.Size([42833, 2])
We keep 1.21e+07/8.90e+08 =  1% of the original kernel matrix.

torch.Size([17126, 2])
We keep 2.81e+06/9.35e+07 =  3% of the original kernel matrix.

torch.Size([24924, 2])
We keep 3.65e+06/2.03e+08 =  1% of the original kernel matrix.

torch.Size([10925, 2])
We keep 7.79e+05/2.27e+07 =  3% of the original kernel matrix.

torch.Size([20069, 2])
We keep 2.11e+06/1.00e+08 =  2% of the original kernel matrix.

torch.Size([18279, 2])
We keep 6.31e+06/1.53e+08 =  4% of the original kernel matrix.

torch.Size([25591, 2])
We keep 4.44e+06/2.60e+08 =  1% of the original kernel matrix.

torch.Size([135908, 2])
We keep 7.49e+07/6.66e+09 =  1% of the original kernel matrix.

torch.Size([71237, 2])
We keep 2.07e+07/1.71e+09 =  1% of the original kernel matrix.

torch.Size([163173, 2])
We keep 8.62e+07/8.08e+09 =  1% of the original kernel matrix.

torch.Size([78178, 2])
We keep 2.24e+07/1.89e+09 =  1% of the original kernel matrix.

torch.Size([215953, 2])
We keep 4.55e+08/2.93e+10 =  1% of the original kernel matrix.

torch.Size([90138, 2])
We keep 4.00e+07/3.59e+09 =  1% of the original kernel matrix.

torch.Size([122729, 2])
We keep 1.09e+08/6.58e+09 =  1% of the original kernel matrix.

torch.Size([66960, 2])
We keep 2.09e+07/1.70e+09 =  1% of the original kernel matrix.

torch.Size([59617, 2])
We keep 2.84e+07/1.40e+09 =  2% of the original kernel matrix.

torch.Size([46356, 2])
We keep 1.08e+07/7.86e+08 =  1% of the original kernel matrix.

torch.Size([20779, 2])
We keep 3.37e+06/1.55e+08 =  2% of the original kernel matrix.

torch.Size([27398, 2])
We keep 4.37e+06/2.61e+08 =  1% of the original kernel matrix.

torch.Size([278265, 2])
We keep 3.38e+08/3.15e+10 =  1% of the original kernel matrix.

torch.Size([105393, 2])
We keep 4.13e+07/3.73e+09 =  1% of the original kernel matrix.

torch.Size([6881, 2])
We keep 8.64e+05/1.15e+07 =  7% of the original kernel matrix.

torch.Size([15724, 2])
We keep 1.65e+06/7.11e+07 =  2% of the original kernel matrix.

torch.Size([16496, 2])
We keep 2.16e+06/6.89e+07 =  3% of the original kernel matrix.

torch.Size([24492, 2])
We keep 3.18e+06/1.74e+08 =  1% of the original kernel matrix.

torch.Size([197752, 2])
We keep 1.62e+08/1.48e+10 =  1% of the original kernel matrix.

torch.Size([86900, 2])
We keep 2.93e+07/2.55e+09 =  1% of the original kernel matrix.

torch.Size([71239, 2])
We keep 3.00e+07/1.58e+09 =  1% of the original kernel matrix.

torch.Size([50461, 2])
We keep 1.13e+07/8.35e+08 =  1% of the original kernel matrix.

torch.Size([9496, 2])
We keep 8.41e+05/1.78e+07 =  4% of the original kernel matrix.

torch.Size([18517, 2])
We keep 1.93e+06/8.85e+07 =  2% of the original kernel matrix.

torch.Size([25660, 2])
We keep 4.11e+06/2.10e+08 =  1% of the original kernel matrix.

torch.Size([30895, 2])
We keep 4.90e+06/3.04e+08 =  1% of the original kernel matrix.

torch.Size([395989, 2])
We keep 1.49e+09/7.19e+10 =  2% of the original kernel matrix.

torch.Size([124285, 2])
We keep 6.00e+07/5.63e+09 =  1% of the original kernel matrix.

torch.Size([20884, 2])
We keep 3.30e+06/1.30e+08 =  2% of the original kernel matrix.

torch.Size([27867, 2])
We keep 4.14e+06/2.39e+08 =  1% of the original kernel matrix.

torch.Size([88185, 2])
We keep 5.33e+07/2.60e+09 =  2% of the original kernel matrix.

torch.Size([56238, 2])
We keep 1.39e+07/1.07e+09 =  1% of the original kernel matrix.

torch.Size([3968, 2])
We keep 1.23e+05/2.09e+06 =  5% of the original kernel matrix.

torch.Size([12938, 2])
We keep 9.09e+05/3.03e+07 =  2% of the original kernel matrix.

torch.Size([9664, 2])
We keep 6.48e+05/1.65e+07 =  3% of the original kernel matrix.

torch.Size([18892, 2])
We keep 1.89e+06/8.52e+07 =  2% of the original kernel matrix.

torch.Size([8279, 2])
We keep 4.67e+05/1.12e+07 =  4% of the original kernel matrix.

torch.Size([17541, 2])
We keep 1.64e+06/7.03e+07 =  2% of the original kernel matrix.

torch.Size([9808, 2])
We keep 1.50e+06/2.85e+07 =  5% of the original kernel matrix.

torch.Size([18911, 2])
We keep 2.36e+06/1.12e+08 =  2% of the original kernel matrix.

torch.Size([27228, 2])
We keep 5.58e+06/2.65e+08 =  2% of the original kernel matrix.

torch.Size([32662, 2])
We keep 5.47e+06/3.42e+08 =  1% of the original kernel matrix.

torch.Size([94233, 2])
We keep 7.07e+07/2.95e+09 =  2% of the original kernel matrix.

torch.Size([56912, 2])
We keep 1.46e+07/1.14e+09 =  1% of the original kernel matrix.

torch.Size([5140, 2])
We keep 2.33e+05/4.36e+06 =  5% of the original kernel matrix.

torch.Size([14090, 2])
We keep 1.16e+06/4.39e+07 =  2% of the original kernel matrix.

torch.Size([124563, 2])
We keep 9.99e+07/5.35e+09 =  1% of the original kernel matrix.

torch.Size([67437, 2])
We keep 1.87e+07/1.54e+09 =  1% of the original kernel matrix.

torch.Size([13445, 2])
We keep 1.81e+06/4.90e+07 =  3% of the original kernel matrix.

torch.Size([21931, 2])
We keep 2.80e+06/1.47e+08 =  1% of the original kernel matrix.

torch.Size([73786, 2])
We keep 3.15e+07/1.91e+09 =  1% of the original kernel matrix.

torch.Size([52105, 2])
We keep 1.24e+07/9.18e+08 =  1% of the original kernel matrix.

torch.Size([14630, 2])
We keep 2.10e+06/5.75e+07 =  3% of the original kernel matrix.

torch.Size([22924, 2])
We keep 3.00e+06/1.59e+08 =  1% of the original kernel matrix.

torch.Size([133679, 2])
We keep 6.65e+07/5.34e+09 =  1% of the original kernel matrix.

torch.Size([70150, 2])
We keep 1.86e+07/1.53e+09 =  1% of the original kernel matrix.

torch.Size([9101, 2])
We keep 9.78e+05/1.86e+07 =  5% of the original kernel matrix.

torch.Size([18138, 2])
We keep 1.96e+06/9.05e+07 =  2% of the original kernel matrix.

torch.Size([26071, 2])
We keep 5.93e+06/2.18e+08 =  2% of the original kernel matrix.

torch.Size([31056, 2])
We keep 4.91e+06/3.10e+08 =  1% of the original kernel matrix.

torch.Size([42076, 2])
We keep 9.63e+06/5.59e+08 =  1% of the original kernel matrix.

torch.Size([39948, 2])
We keep 7.31e+06/4.97e+08 =  1% of the original kernel matrix.

torch.Size([28812, 2])
We keep 1.02e+07/2.91e+08 =  3% of the original kernel matrix.

torch.Size([33195, 2])
We keep 5.81e+06/3.58e+08 =  1% of the original kernel matrix.

torch.Size([76253, 2])
We keep 4.18e+07/2.28e+09 =  1% of the original kernel matrix.

torch.Size([52679, 2])
We keep 1.33e+07/1.00e+09 =  1% of the original kernel matrix.

torch.Size([372374, 2])
We keep 8.94e+08/4.96e+10 =  1% of the original kernel matrix.

torch.Size([118840, 2])
We keep 4.94e+07/4.68e+09 =  1% of the original kernel matrix.

torch.Size([15964, 2])
We keep 1.41e+06/5.40e+07 =  2% of the original kernel matrix.

torch.Size([24226, 2])
We keep 2.90e+06/1.54e+08 =  1% of the original kernel matrix.

torch.Size([7385, 2])
We keep 9.39e+05/1.30e+07 =  7% of the original kernel matrix.

torch.Size([16478, 2])
We keep 1.71e+06/7.58e+07 =  2% of the original kernel matrix.

torch.Size([58621, 2])
We keep 3.69e+07/1.24e+09 =  2% of the original kernel matrix.

torch.Size([46058, 2])
We keep 1.02e+07/7.41e+08 =  1% of the original kernel matrix.

torch.Size([29267, 2])
We keep 5.78e+06/2.77e+08 =  2% of the original kernel matrix.

torch.Size([32868, 2])
We keep 5.55e+06/3.50e+08 =  1% of the original kernel matrix.

torch.Size([570696, 2])
We keep 8.46e+08/1.10e+11 =  0% of the original kernel matrix.

torch.Size([149830, 2])
We keep 7.11e+07/6.96e+09 =  1% of the original kernel matrix.

torch.Size([5552, 2])
We keep 3.22e+05/5.11e+06 =  6% of the original kernel matrix.

torch.Size([14574, 2])
We keep 1.22e+06/4.75e+07 =  2% of the original kernel matrix.

torch.Size([157279, 2])
We keep 1.17e+08/9.51e+09 =  1% of the original kernel matrix.

torch.Size([76554, 2])
We keep 2.43e+07/2.05e+09 =  1% of the original kernel matrix.

torch.Size([351602, 2])
We keep 3.48e+08/4.49e+10 =  0% of the original kernel matrix.

torch.Size([119468, 2])
We keep 4.80e+07/4.45e+09 =  1% of the original kernel matrix.

torch.Size([9702, 2])
We keep 7.33e+05/1.66e+07 =  4% of the original kernel matrix.

torch.Size([18678, 2])
We keep 1.88e+06/8.57e+07 =  2% of the original kernel matrix.

torch.Size([128086, 2])
We keep 2.05e+08/8.90e+09 =  2% of the original kernel matrix.

torch.Size([66941, 2])
We keep 2.31e+07/1.98e+09 =  1% of the original kernel matrix.

torch.Size([18360, 2])
We keep 6.50e+06/1.82e+08 =  3% of the original kernel matrix.

torch.Size([25392, 2])
We keep 4.68e+06/2.83e+08 =  1% of the original kernel matrix.

torch.Size([635874, 2])
We keep 1.57e+09/1.46e+11 =  1% of the original kernel matrix.

torch.Size([160347, 2])
We keep 8.24e+07/8.01e+09 =  1% of the original kernel matrix.

torch.Size([16601, 2])
We keep 2.00e+06/7.43e+07 =  2% of the original kernel matrix.

torch.Size([24630, 2])
We keep 3.33e+06/1.81e+08 =  1% of the original kernel matrix.

torch.Size([18614, 2])
We keep 2.71e+06/9.21e+07 =  2% of the original kernel matrix.

torch.Size([26068, 2])
We keep 3.56e+06/2.02e+08 =  1% of the original kernel matrix.

torch.Size([11976, 2])
We keep 2.51e+06/4.18e+07 =  5% of the original kernel matrix.

torch.Size([20774, 2])
We keep 2.69e+06/1.36e+08 =  1% of the original kernel matrix.

torch.Size([5187, 2])
We keep 2.48e+05/4.73e+06 =  5% of the original kernel matrix.

torch.Size([14324, 2])
We keep 1.21e+06/4.57e+07 =  2% of the original kernel matrix.

torch.Size([12972, 2])
We keep 1.37e+06/3.80e+07 =  3% of the original kernel matrix.

torch.Size([21608, 2])
We keep 2.55e+06/1.29e+08 =  1% of the original kernel matrix.

torch.Size([36931, 2])
We keep 8.69e+06/4.67e+08 =  1% of the original kernel matrix.

torch.Size([37335, 2])
We keep 6.80e+06/4.54e+08 =  1% of the original kernel matrix.

torch.Size([17517, 2])
We keep 1.07e+07/9.00e+07 = 11% of the original kernel matrix.

torch.Size([25272, 2])
We keep 3.57e+06/1.99e+08 =  1% of the original kernel matrix.

torch.Size([150487, 2])
We keep 1.03e+08/8.37e+09 =  1% of the original kernel matrix.

torch.Size([75212, 2])
We keep 2.30e+07/1.92e+09 =  1% of the original kernel matrix.

torch.Size([23259, 2])
We keep 3.09e+06/1.54e+08 =  2% of the original kernel matrix.

torch.Size([29697, 2])
We keep 4.35e+06/2.61e+08 =  1% of the original kernel matrix.

torch.Size([7900, 2])
We keep 6.31e+05/1.14e+07 =  5% of the original kernel matrix.

torch.Size([17083, 2])
We keep 1.65e+06/7.10e+07 =  2% of the original kernel matrix.

torch.Size([23171, 2])
We keep 4.78e+06/2.09e+08 =  2% of the original kernel matrix.

torch.Size([29169, 2])
We keep 5.02e+06/3.04e+08 =  1% of the original kernel matrix.

torch.Size([152065, 2])
We keep 2.97e+08/1.69e+10 =  1% of the original kernel matrix.

torch.Size([75106, 2])
We keep 3.17e+07/2.73e+09 =  1% of the original kernel matrix.

torch.Size([86260, 2])
We keep 1.09e+08/4.15e+09 =  2% of the original kernel matrix.

torch.Size([56104, 2])
We keep 1.72e+07/1.35e+09 =  1% of the original kernel matrix.

torch.Size([311203, 2])
We keep 3.92e+08/3.78e+10 =  1% of the original kernel matrix.

torch.Size([112746, 2])
We keep 4.46e+07/4.08e+09 =  1% of the original kernel matrix.

torch.Size([16463, 2])
We keep 3.60e+06/7.72e+07 =  4% of the original kernel matrix.

torch.Size([24804, 2])
We keep 3.25e+06/1.85e+08 =  1% of the original kernel matrix.

torch.Size([8773, 2])
We keep 9.02e+05/2.19e+07 =  4% of the original kernel matrix.

torch.Size([18251, 2])
We keep 2.12e+06/9.83e+07 =  2% of the original kernel matrix.

torch.Size([9462, 2])
We keep 6.19e+05/1.62e+07 =  3% of the original kernel matrix.

torch.Size([18706, 2])
We keep 1.89e+06/8.46e+07 =  2% of the original kernel matrix.

torch.Size([6922, 2])
We keep 7.31e+05/1.09e+07 =  6% of the original kernel matrix.

torch.Size([15967, 2])
We keep 1.60e+06/6.95e+07 =  2% of the original kernel matrix.

torch.Size([180113, 2])
We keep 3.10e+08/1.26e+10 =  2% of the original kernel matrix.

torch.Size([82890, 2])
We keep 2.60e+07/2.36e+09 =  1% of the original kernel matrix.

torch.Size([11358, 2])
We keep 9.08e+05/2.51e+07 =  3% of the original kernel matrix.

torch.Size([20299, 2])
We keep 2.21e+06/1.05e+08 =  2% of the original kernel matrix.

torch.Size([29948, 2])
We keep 5.70e+06/2.60e+08 =  2% of the original kernel matrix.

torch.Size([33837, 2])
We keep 5.39e+06/3.38e+08 =  1% of the original kernel matrix.

torch.Size([42741, 2])
We keep 1.12e+07/5.80e+08 =  1% of the original kernel matrix.

torch.Size([39399, 2])
We keep 7.31e+06/5.06e+08 =  1% of the original kernel matrix.

torch.Size([18130, 2])
We keep 7.72e+06/1.98e+08 =  3% of the original kernel matrix.

torch.Size([25190, 2])
We keep 4.96e+06/2.96e+08 =  1% of the original kernel matrix.

torch.Size([15609, 2])
We keep 3.04e+06/7.21e+07 =  4% of the original kernel matrix.

torch.Size([23934, 2])
We keep 3.30e+06/1.78e+08 =  1% of the original kernel matrix.

torch.Size([15717, 2])
We keep 1.73e+06/6.14e+07 =  2% of the original kernel matrix.

torch.Size([23821, 2])
We keep 3.08e+06/1.65e+08 =  1% of the original kernel matrix.

torch.Size([222766, 2])
We keep 2.91e+08/2.26e+10 =  1% of the original kernel matrix.

torch.Size([92526, 2])
We keep 3.57e+07/3.16e+09 =  1% of the original kernel matrix.

torch.Size([39772, 2])
We keep 1.53e+07/6.33e+08 =  2% of the original kernel matrix.

torch.Size([38410, 2])
We keep 7.86e+06/5.28e+08 =  1% of the original kernel matrix.

torch.Size([13025, 2])
We keep 1.44e+06/4.37e+07 =  3% of the original kernel matrix.

torch.Size([21810, 2])
We keep 2.63e+06/1.39e+08 =  1% of the original kernel matrix.

torch.Size([21222, 2])
We keep 3.19e+06/1.27e+08 =  2% of the original kernel matrix.

torch.Size([28204, 2])
We keep 4.03e+06/2.37e+08 =  1% of the original kernel matrix.

torch.Size([79613, 2])
We keep 3.26e+08/3.95e+09 =  8% of the original kernel matrix.

torch.Size([52676, 2])
We keep 1.70e+07/1.32e+09 =  1% of the original kernel matrix.

torch.Size([83750, 2])
We keep 4.49e+07/2.76e+09 =  1% of the original kernel matrix.

torch.Size([54502, 2])
We keep 1.45e+07/1.10e+09 =  1% of the original kernel matrix.

torch.Size([196848, 2])
We keep 6.28e+08/3.68e+10 =  1% of the original kernel matrix.

torch.Size([82046, 2])
We keep 4.45e+07/4.03e+09 =  1% of the original kernel matrix.

torch.Size([102943, 2])
We keep 4.98e+07/3.27e+09 =  1% of the original kernel matrix.

torch.Size([60743, 2])
We keep 1.52e+07/1.20e+09 =  1% of the original kernel matrix.

torch.Size([7856, 2])
We keep 7.05e+05/1.15e+07 =  6% of the original kernel matrix.

torch.Size([17274, 2])
We keep 1.57e+06/7.13e+07 =  2% of the original kernel matrix.

torch.Size([46864, 2])
We keep 9.82e+06/6.40e+08 =  1% of the original kernel matrix.

torch.Size([41418, 2])
We keep 7.55e+06/5.31e+08 =  1% of the original kernel matrix.

torch.Size([31391, 2])
We keep 3.07e+07/6.79e+08 =  4% of the original kernel matrix.

torch.Size([32407, 2])
We keep 7.99e+06/5.47e+08 =  1% of the original kernel matrix.

torch.Size([14869, 2])
We keep 1.76e+06/4.73e+07 =  3% of the original kernel matrix.

torch.Size([23242, 2])
We keep 2.62e+06/1.44e+08 =  1% of the original kernel matrix.

torch.Size([13967, 2])
We keep 2.11e+06/7.20e+07 =  2% of the original kernel matrix.

torch.Size([22518, 2])
We keep 3.29e+06/1.78e+08 =  1% of the original kernel matrix.

torch.Size([6708, 2])
We keep 3.04e+05/6.70e+06 =  4% of the original kernel matrix.

torch.Size([16192, 2])
We keep 1.35e+06/5.44e+07 =  2% of the original kernel matrix.

torch.Size([36213, 2])
We keep 1.75e+07/4.66e+08 =  3% of the original kernel matrix.

torch.Size([36928, 2])
We keep 6.87e+06/4.53e+08 =  1% of the original kernel matrix.

torch.Size([31234, 2])
We keep 1.02e+07/3.38e+08 =  3% of the original kernel matrix.

torch.Size([33753, 2])
We keep 6.03e+06/3.86e+08 =  1% of the original kernel matrix.

torch.Size([9370, 2])
We keep 1.71e+06/2.55e+07 =  6% of the original kernel matrix.

torch.Size([17591, 2])
We keep 2.16e+06/1.06e+08 =  2% of the original kernel matrix.

torch.Size([29533, 2])
We keep 5.11e+06/2.74e+08 =  1% of the original kernel matrix.

torch.Size([33603, 2])
We keep 5.50e+06/3.48e+08 =  1% of the original kernel matrix.

torch.Size([14296, 2])
We keep 1.60e+06/4.71e+07 =  3% of the original kernel matrix.

torch.Size([22942, 2])
We keep 2.73e+06/1.44e+08 =  1% of the original kernel matrix.

torch.Size([31966, 2])
We keep 1.56e+07/5.59e+08 =  2% of the original kernel matrix.

torch.Size([33365, 2])
We keep 7.34e+06/4.96e+08 =  1% of the original kernel matrix.

torch.Size([8658, 2])
We keep 9.86e+05/1.87e+07 =  5% of the original kernel matrix.

torch.Size([17570, 2])
We keep 1.96e+06/9.09e+07 =  2% of the original kernel matrix.

torch.Size([22705, 2])
We keep 7.74e+06/2.01e+08 =  3% of the original kernel matrix.

torch.Size([28742, 2])
We keep 4.89e+06/2.98e+08 =  1% of the original kernel matrix.

torch.Size([4503, 2])
We keep 6.86e+05/4.08e+06 = 16% of the original kernel matrix.

torch.Size([13490, 2])
We keep 1.03e+06/4.24e+07 =  2% of the original kernel matrix.

torch.Size([17409, 2])
We keep 3.57e+06/9.38e+07 =  3% of the original kernel matrix.

torch.Size([25152, 2])
We keep 3.59e+06/2.03e+08 =  1% of the original kernel matrix.

torch.Size([9015, 2])
We keep 7.13e+05/1.54e+07 =  4% of the original kernel matrix.

torch.Size([18123, 2])
We keep 1.82e+06/8.24e+07 =  2% of the original kernel matrix.

torch.Size([17280, 2])
We keep 1.77e+06/7.02e+07 =  2% of the original kernel matrix.

torch.Size([25074, 2])
We keep 3.20e+06/1.76e+08 =  1% of the original kernel matrix.

torch.Size([29417, 2])
We keep 5.44e+06/3.05e+08 =  1% of the original kernel matrix.

torch.Size([33939, 2])
We keep 5.81e+06/3.67e+08 =  1% of the original kernel matrix.

torch.Size([119780, 2])
We keep 1.02e+08/5.45e+09 =  1% of the original kernel matrix.

torch.Size([66334, 2])
We keep 1.92e+07/1.55e+09 =  1% of the original kernel matrix.

torch.Size([10144, 2])
We keep 7.67e+05/2.00e+07 =  3% of the original kernel matrix.

torch.Size([19190, 2])
We keep 1.99e+06/9.40e+07 =  2% of the original kernel matrix.

torch.Size([3558, 2])
We keep 1.24e+05/2.03e+06 =  6% of the original kernel matrix.

torch.Size([12388, 2])
We keep 9.09e+05/2.99e+07 =  3% of the original kernel matrix.

torch.Size([11738, 2])
We keep 1.51e+06/3.38e+07 =  4% of the original kernel matrix.

torch.Size([20423, 2])
We keep 2.47e+06/1.22e+08 =  2% of the original kernel matrix.

torch.Size([85467, 2])
We keep 7.92e+07/3.19e+09 =  2% of the original kernel matrix.

torch.Size([55388, 2])
We keep 1.49e+07/1.19e+09 =  1% of the original kernel matrix.

torch.Size([785348, 2])
We keep 2.28e+09/2.30e+11 =  0% of the original kernel matrix.

torch.Size([184919, 2])
We keep 1.01e+08/1.01e+10 =  0% of the original kernel matrix.

torch.Size([46023, 2])
We keep 1.93e+07/8.69e+08 =  2% of the original kernel matrix.

torch.Size([40537, 2])
We keep 8.87e+06/6.19e+08 =  1% of the original kernel matrix.

torch.Size([9925, 2])
We keep 8.70e+05/2.20e+07 =  3% of the original kernel matrix.

torch.Size([19061, 2])
We keep 2.17e+06/9.86e+07 =  2% of the original kernel matrix.

torch.Size([143049, 2])
We keep 2.18e+08/1.06e+10 =  2% of the original kernel matrix.

torch.Size([72293, 2])
We keep 2.54e+07/2.16e+09 =  1% of the original kernel matrix.

torch.Size([146183, 2])
We keep 9.17e+07/8.16e+09 =  1% of the original kernel matrix.

torch.Size([74201, 2])
We keep 2.28e+07/1.90e+09 =  1% of the original kernel matrix.

torch.Size([4880, 2])
We keep 2.15e+05/3.85e+06 =  5% of the original kernel matrix.

torch.Size([14064, 2])
We keep 1.15e+06/4.12e+07 =  2% of the original kernel matrix.

torch.Size([13964, 2])
We keep 2.14e+06/6.03e+07 =  3% of the original kernel matrix.

torch.Size([22487, 2])
We keep 3.03e+06/1.63e+08 =  1% of the original kernel matrix.

torch.Size([44691, 2])
We keep 8.38e+06/5.80e+08 =  1% of the original kernel matrix.

torch.Size([40929, 2])
We keep 7.29e+06/5.06e+08 =  1% of the original kernel matrix.

torch.Size([89539, 2])
We keep 1.39e+08/6.01e+09 =  2% of the original kernel matrix.

torch.Size([56534, 2])
We keep 1.94e+07/1.63e+09 =  1% of the original kernel matrix.

torch.Size([291340, 2])
We keep 1.16e+09/6.31e+10 =  1% of the original kernel matrix.

torch.Size([101885, 2])
We keep 5.56e+07/5.28e+09 =  1% of the original kernel matrix.

torch.Size([227186, 2])
We keep 1.86e+08/1.95e+10 =  0% of the original kernel matrix.

torch.Size([94039, 2])
We keep 3.33e+07/2.93e+09 =  1% of the original kernel matrix.

torch.Size([23441, 2])
We keep 3.99e+06/1.68e+08 =  2% of the original kernel matrix.

torch.Size([29919, 2])
We keep 4.52e+06/2.72e+08 =  1% of the original kernel matrix.

torch.Size([8470, 2])
We keep 5.69e+05/1.40e+07 =  4% of the original kernel matrix.

torch.Size([17759, 2])
We keep 1.81e+06/7.85e+07 =  2% of the original kernel matrix.

torch.Size([4250, 2])
We keep 1.50e+05/2.64e+06 =  5% of the original kernel matrix.

torch.Size([13200, 2])
We keep 9.81e+05/3.41e+07 =  2% of the original kernel matrix.

torch.Size([80005, 2])
We keep 4.02e+07/2.27e+09 =  1% of the original kernel matrix.

torch.Size([52548, 2])
We keep 1.30e+07/1.00e+09 =  1% of the original kernel matrix.

torch.Size([16348, 2])
We keep 1.56e+06/5.82e+07 =  2% of the original kernel matrix.

torch.Size([24530, 2])
We keep 2.98e+06/1.60e+08 =  1% of the original kernel matrix.

torch.Size([14304, 2])
We keep 4.36e+06/8.44e+07 =  5% of the original kernel matrix.

torch.Size([22521, 2])
We keep 3.23e+06/1.93e+08 =  1% of the original kernel matrix.

torch.Size([167187, 2])
We keep 1.61e+08/1.14e+10 =  1% of the original kernel matrix.

torch.Size([79107, 2])
We keep 2.63e+07/2.24e+09 =  1% of the original kernel matrix.

torch.Size([623795, 2])
We keep 1.07e+09/1.30e+11 =  0% of the original kernel matrix.

torch.Size([156846, 2])
We keep 7.78e+07/7.59e+09 =  1% of the original kernel matrix.

torch.Size([14153, 2])
We keep 1.87e+06/4.47e+07 =  4% of the original kernel matrix.

torch.Size([22354, 2])
We keep 2.72e+06/1.40e+08 =  1% of the original kernel matrix.

torch.Size([3915, 2])
We keep 1.30e+05/1.97e+06 =  6% of the original kernel matrix.

torch.Size([12717, 2])
We keep 8.85e+05/2.95e+07 =  2% of the original kernel matrix.

torch.Size([16383, 2])
We keep 5.22e+06/8.32e+07 =  6% of the original kernel matrix.

torch.Size([24564, 2])
We keep 3.29e+06/1.92e+08 =  1% of the original kernel matrix.

torch.Size([56312, 2])
We keep 5.56e+07/1.16e+09 =  4% of the original kernel matrix.

torch.Size([45753, 2])
We keep 9.27e+06/7.14e+08 =  1% of the original kernel matrix.

torch.Size([33098, 2])
We keep 7.34e+06/3.36e+08 =  2% of the original kernel matrix.

torch.Size([34669, 2])
We keep 5.86e+06/3.85e+08 =  1% of the original kernel matrix.

torch.Size([3135, 2])
We keep 1.25e+05/1.51e+06 =  8% of the original kernel matrix.

torch.Size([11813, 2])
We keep 7.69e+05/2.58e+07 =  2% of the original kernel matrix.

torch.Size([7031, 2])
We keep 5.65e+05/1.04e+07 =  5% of the original kernel matrix.

torch.Size([16219, 2])
We keep 1.63e+06/6.77e+07 =  2% of the original kernel matrix.

torch.Size([26402, 2])
We keep 5.92e+06/2.25e+08 =  2% of the original kernel matrix.

torch.Size([31351, 2])
We keep 5.05e+06/3.15e+08 =  1% of the original kernel matrix.

torch.Size([5633, 2])
We keep 3.69e+05/4.95e+06 =  7% of the original kernel matrix.

torch.Size([15102, 2])
We keep 1.23e+06/4.67e+07 =  2% of the original kernel matrix.

torch.Size([40263, 2])
We keep 4.27e+07/9.58e+08 =  4% of the original kernel matrix.

torch.Size([37933, 2])
We keep 9.31e+06/6.50e+08 =  1% of the original kernel matrix.

torch.Size([8549, 2])
We keep 6.09e+05/1.28e+07 =  4% of the original kernel matrix.

torch.Size([17865, 2])
We keep 1.72e+06/7.51e+07 =  2% of the original kernel matrix.

torch.Size([21501, 2])
We keep 2.96e+06/1.34e+08 =  2% of the original kernel matrix.

torch.Size([28610, 2])
We keep 4.20e+06/2.43e+08 =  1% of the original kernel matrix.

torch.Size([20525, 2])
We keep 3.38e+06/1.39e+08 =  2% of the original kernel matrix.

torch.Size([27727, 2])
We keep 4.30e+06/2.48e+08 =  1% of the original kernel matrix.

torch.Size([13133, 2])
We keep 1.05e+06/3.57e+07 =  2% of the original kernel matrix.

torch.Size([21858, 2])
We keep 2.50e+06/1.25e+08 =  1% of the original kernel matrix.

torch.Size([10500, 2])
We keep 1.18e+06/2.65e+07 =  4% of the original kernel matrix.

torch.Size([19410, 2])
We keep 2.27e+06/1.08e+08 =  2% of the original kernel matrix.

torch.Size([183992, 2])
We keep 1.47e+08/1.28e+10 =  1% of the original kernel matrix.

torch.Size([83486, 2])
We keep 2.77e+07/2.38e+09 =  1% of the original kernel matrix.

torch.Size([474055, 2])
We keep 8.08e+08/7.80e+10 =  1% of the original kernel matrix.

torch.Size([135258, 2])
We keep 6.17e+07/5.86e+09 =  1% of the original kernel matrix.

torch.Size([100007, 2])
We keep 1.25e+08/3.93e+09 =  3% of the original kernel matrix.

torch.Size([60399, 2])
We keep 1.66e+07/1.32e+09 =  1% of the original kernel matrix.

torch.Size([80890, 2])
We keep 1.13e+08/3.35e+09 =  3% of the original kernel matrix.

torch.Size([53290, 2])
We keep 1.57e+07/1.22e+09 =  1% of the original kernel matrix.

torch.Size([21282, 2])
We keep 3.08e+06/1.38e+08 =  2% of the original kernel matrix.

torch.Size([28358, 2])
We keep 4.23e+06/2.47e+08 =  1% of the original kernel matrix.

torch.Size([22434, 2])
We keep 5.18e+06/1.48e+08 =  3% of the original kernel matrix.

torch.Size([28605, 2])
We keep 4.28e+06/2.56e+08 =  1% of the original kernel matrix.

torch.Size([20787, 2])
We keep 3.65e+06/1.43e+08 =  2% of the original kernel matrix.

torch.Size([28199, 2])
We keep 4.37e+06/2.51e+08 =  1% of the original kernel matrix.

torch.Size([157180, 2])
We keep 1.01e+08/8.70e+09 =  1% of the original kernel matrix.

torch.Size([76730, 2])
We keep 2.34e+07/1.96e+09 =  1% of the original kernel matrix.

torch.Size([21281, 2])
We keep 5.54e+06/1.70e+08 =  3% of the original kernel matrix.

torch.Size([27937, 2])
We keep 4.63e+06/2.74e+08 =  1% of the original kernel matrix.

torch.Size([22580, 2])
We keep 1.09e+07/1.81e+08 =  6% of the original kernel matrix.

torch.Size([29112, 2])
We keep 4.44e+06/2.82e+08 =  1% of the original kernel matrix.

torch.Size([23219, 2])
We keep 3.30e+06/1.49e+08 =  2% of the original kernel matrix.

torch.Size([29488, 2])
We keep 4.27e+06/2.57e+08 =  1% of the original kernel matrix.

torch.Size([1561216, 2])
We keep 4.79e+09/7.13e+11 =  0% of the original kernel matrix.

torch.Size([261880, 2])
We keep 1.70e+08/1.77e+10 =  0% of the original kernel matrix.

torch.Size([18578, 2])
We keep 2.47e+06/8.73e+07 =  2% of the original kernel matrix.

torch.Size([26171, 2])
We keep 3.48e+06/1.96e+08 =  1% of the original kernel matrix.

torch.Size([74132, 2])
We keep 3.01e+07/2.01e+09 =  1% of the original kernel matrix.

torch.Size([51242, 2])
We keep 1.25e+07/9.41e+08 =  1% of the original kernel matrix.

torch.Size([301870, 2])
We keep 4.91e+08/3.97e+10 =  1% of the original kernel matrix.

torch.Size([107985, 2])
We keep 4.57e+07/4.19e+09 =  1% of the original kernel matrix.

torch.Size([52043, 2])
We keep 1.41e+07/9.60e+08 =  1% of the original kernel matrix.

torch.Size([44451, 2])
We keep 9.10e+06/6.51e+08 =  1% of the original kernel matrix.

torch.Size([27838, 2])
We keep 7.55e+06/2.76e+08 =  2% of the original kernel matrix.

torch.Size([32372, 2])
We keep 5.56e+06/3.49e+08 =  1% of the original kernel matrix.

torch.Size([837279, 2])
We keep 2.60e+09/2.68e+11 =  0% of the original kernel matrix.

torch.Size([187929, 2])
We keep 1.09e+08/1.09e+10 =  1% of the original kernel matrix.

torch.Size([162146, 2])
We keep 9.72e+07/9.23e+09 =  1% of the original kernel matrix.

torch.Size([77831, 2])
We keep 2.39e+07/2.02e+09 =  1% of the original kernel matrix.

torch.Size([91256, 2])
We keep 6.33e+07/2.64e+09 =  2% of the original kernel matrix.

torch.Size([56912, 2])
We keep 1.40e+07/1.08e+09 =  1% of the original kernel matrix.

torch.Size([8076, 2])
We keep 6.36e+05/1.14e+07 =  5% of the original kernel matrix.

torch.Size([17615, 2])
We keep 1.63e+06/7.10e+07 =  2% of the original kernel matrix.

torch.Size([31706, 2])
We keep 1.34e+07/4.01e+08 =  3% of the original kernel matrix.

torch.Size([33263, 2])
We keep 6.13e+06/4.20e+08 =  1% of the original kernel matrix.

torch.Size([16366, 2])
We keep 3.27e+06/7.68e+07 =  4% of the original kernel matrix.

torch.Size([24641, 2])
We keep 3.31e+06/1.84e+08 =  1% of the original kernel matrix.

torch.Size([227381, 2])
We keep 4.54e+08/1.77e+10 =  2% of the original kernel matrix.

torch.Size([93882, 2])
We keep 3.20e+07/2.79e+09 =  1% of the original kernel matrix.

torch.Size([1150288, 2])
We keep 3.28e+09/4.03e+11 =  0% of the original kernel matrix.

torch.Size([219912, 2])
We keep 1.31e+08/1.33e+10 =  0% of the original kernel matrix.

torch.Size([88637, 2])
We keep 1.65e+08/3.77e+09 =  4% of the original kernel matrix.

torch.Size([57072, 2])
We keep 1.65e+07/1.29e+09 =  1% of the original kernel matrix.

torch.Size([171715, 2])
We keep 1.40e+08/1.16e+10 =  1% of the original kernel matrix.

torch.Size([79802, 2])
We keep 2.65e+07/2.26e+09 =  1% of the original kernel matrix.

torch.Size([78201, 2])
We keep 1.10e+08/2.10e+09 =  5% of the original kernel matrix.

torch.Size([52751, 2])
We keep 1.28e+07/9.63e+08 =  1% of the original kernel matrix.

torch.Size([26872, 2])
We keep 1.49e+07/3.29e+08 =  4% of the original kernel matrix.

torch.Size([31057, 2])
We keep 5.91e+06/3.81e+08 =  1% of the original kernel matrix.

torch.Size([32807, 2])
We keep 8.56e+06/4.30e+08 =  1% of the original kernel matrix.

torch.Size([34953, 2])
We keep 6.75e+06/4.36e+08 =  1% of the original kernel matrix.

torch.Size([10125, 2])
We keep 1.48e+06/2.95e+07 =  5% of the original kernel matrix.

torch.Size([19239, 2])
We keep 2.43e+06/1.14e+08 =  2% of the original kernel matrix.

torch.Size([290504, 2])
We keep 1.64e+09/7.72e+10 =  2% of the original kernel matrix.

torch.Size([102067, 2])
We keep 6.20e+07/5.84e+09 =  1% of the original kernel matrix.

torch.Size([461537, 2])
We keep 5.93e+08/7.64e+10 =  0% of the original kernel matrix.

torch.Size([134484, 2])
We keep 6.08e+07/5.81e+09 =  1% of the original kernel matrix.

torch.Size([55423, 2])
We keep 6.74e+07/2.45e+09 =  2% of the original kernel matrix.

torch.Size([40785, 2])
We keep 1.37e+07/1.04e+09 =  1% of the original kernel matrix.

torch.Size([1329633, 2])
We keep 9.64e+09/8.33e+11 =  1% of the original kernel matrix.

torch.Size([226527, 2])
We keep 1.83e+08/1.92e+10 =  0% of the original kernel matrix.

torch.Size([141737, 2])
We keep 3.40e+08/1.32e+10 =  2% of the original kernel matrix.

torch.Size([71988, 2])
We keep 2.81e+07/2.41e+09 =  1% of the original kernel matrix.

torch.Size([48712, 2])
We keep 1.19e+07/7.25e+08 =  1% of the original kernel matrix.

torch.Size([42175, 2])
We keep 8.08e+06/5.65e+08 =  1% of the original kernel matrix.

torch.Size([10619, 2])
We keep 1.26e+06/2.59e+07 =  4% of the original kernel matrix.

torch.Size([19468, 2])
We keep 2.22e+06/1.07e+08 =  2% of the original kernel matrix.

torch.Size([53714, 2])
We keep 2.94e+07/1.17e+09 =  2% of the original kernel matrix.

torch.Size([44871, 2])
We keep 9.97e+06/7.18e+08 =  1% of the original kernel matrix.

torch.Size([136280, 2])
We keep 7.32e+07/6.48e+09 =  1% of the original kernel matrix.

torch.Size([71030, 2])
We keep 2.04e+07/1.69e+09 =  1% of the original kernel matrix.

torch.Size([9257, 2])
We keep 1.33e+06/2.47e+07 =  5% of the original kernel matrix.

torch.Size([18182, 2])
We keep 2.19e+06/1.04e+08 =  2% of the original kernel matrix.

torch.Size([685427, 2])
We keep 1.17e+09/1.63e+11 =  0% of the original kernel matrix.

torch.Size([167107, 2])
We keep 8.56e+07/8.47e+09 =  1% of the original kernel matrix.

torch.Size([22204, 2])
We keep 4.70e+06/1.63e+08 =  2% of the original kernel matrix.

torch.Size([28685, 2])
We keep 4.30e+06/2.68e+08 =  1% of the original kernel matrix.

torch.Size([27661, 2])
We keep 1.05e+07/3.73e+08 =  2% of the original kernel matrix.

torch.Size([30639, 2])
We keep 5.94e+06/4.06e+08 =  1% of the original kernel matrix.

torch.Size([10284, 2])
We keep 7.50e+05/2.08e+07 =  3% of the original kernel matrix.

torch.Size([19729, 2])
We keep 1.99e+06/9.58e+07 =  2% of the original kernel matrix.

torch.Size([16673, 2])
We keep 1.65e+06/6.20e+07 =  2% of the original kernel matrix.

torch.Size([24545, 2])
We keep 3.04e+06/1.65e+08 =  1% of the original kernel matrix.

torch.Size([4596, 2])
We keep 1.55e+05/2.87e+06 =  5% of the original kernel matrix.

torch.Size([13633, 2])
We keep 1.00e+06/3.56e+07 =  2% of the original kernel matrix.

torch.Size([12143, 2])
We keep 1.66e+06/3.60e+07 =  4% of the original kernel matrix.

torch.Size([20582, 2])
We keep 2.45e+06/1.26e+08 =  1% of the original kernel matrix.

torch.Size([28308, 2])
We keep 7.77e+06/2.70e+08 =  2% of the original kernel matrix.

torch.Size([32611, 2])
We keep 5.52e+06/3.45e+08 =  1% of the original kernel matrix.

torch.Size([36227, 2])
We keep 1.34e+07/4.69e+08 =  2% of the original kernel matrix.

torch.Size([36948, 2])
We keep 7.00e+06/4.55e+08 =  1% of the original kernel matrix.

torch.Size([2468225, 2])
We keep 3.43e+10/3.63e+12 =  0% of the original kernel matrix.

torch.Size([295989, 2])
We keep 3.65e+08/4.00e+10 =  0% of the original kernel matrix.

torch.Size([18214, 2])
We keep 5.39e+06/1.53e+08 =  3% of the original kernel matrix.

torch.Size([25360, 2])
We keep 4.49e+06/2.60e+08 =  1% of the original kernel matrix.

torch.Size([214179, 2])
We keep 1.63e+08/1.74e+10 =  0% of the original kernel matrix.

torch.Size([90594, 2])
We keep 3.14e+07/2.77e+09 =  1% of the original kernel matrix.

torch.Size([11596, 2])
We keep 1.09e+06/2.92e+07 =  3% of the original kernel matrix.

torch.Size([20586, 2])
We keep 2.38e+06/1.13e+08 =  2% of the original kernel matrix.

torch.Size([20267, 2])
We keep 2.96e+06/1.23e+08 =  2% of the original kernel matrix.

torch.Size([27398, 2])
We keep 4.01e+06/2.33e+08 =  1% of the original kernel matrix.

torch.Size([10985, 2])
We keep 1.28e+06/2.87e+07 =  4% of the original kernel matrix.

torch.Size([19711, 2])
We keep 2.31e+06/1.12e+08 =  2% of the original kernel matrix.

torch.Size([155528, 2])
We keep 2.95e+08/1.42e+10 =  2% of the original kernel matrix.

torch.Size([74637, 2])
We keep 2.88e+07/2.50e+09 =  1% of the original kernel matrix.

torch.Size([60248, 2])
We keep 7.31e+07/2.37e+09 =  3% of the original kernel matrix.

torch.Size([44967, 2])
We keep 1.32e+07/1.02e+09 =  1% of the original kernel matrix.

torch.Size([1070961, 2])
We keep 2.48e+09/3.60e+11 =  0% of the original kernel matrix.

torch.Size([211121, 2])
We keep 1.23e+08/1.26e+10 =  0% of the original kernel matrix.

torch.Size([11490, 2])
We keep 9.12e+05/2.55e+07 =  3% of the original kernel matrix.

torch.Size([20346, 2])
We keep 2.18e+06/1.06e+08 =  2% of the original kernel matrix.

torch.Size([192388, 2])
We keep 1.75e+08/1.48e+10 =  1% of the original kernel matrix.

torch.Size([86090, 2])
We keep 2.98e+07/2.56e+09 =  1% of the original kernel matrix.

torch.Size([33937, 2])
We keep 2.03e+07/4.82e+08 =  4% of the original kernel matrix.

torch.Size([35907, 2])
We keep 7.17e+06/4.61e+08 =  1% of the original kernel matrix.

torch.Size([177038, 2])
We keep 1.44e+08/1.06e+10 =  1% of the original kernel matrix.

torch.Size([81324, 2])
We keep 2.52e+07/2.16e+09 =  1% of the original kernel matrix.

torch.Size([150177, 2])
We keep 2.34e+08/1.25e+10 =  1% of the original kernel matrix.

torch.Size([74647, 2])
We keep 2.72e+07/2.35e+09 =  1% of the original kernel matrix.

torch.Size([23146, 2])
We keep 4.39e+06/1.66e+08 =  2% of the original kernel matrix.

torch.Size([29300, 2])
We keep 4.44e+06/2.70e+08 =  1% of the original kernel matrix.

torch.Size([14130, 2])
We keep 2.26e+06/5.92e+07 =  3% of the original kernel matrix.

torch.Size([22663, 2])
We keep 3.06e+06/1.62e+08 =  1% of the original kernel matrix.

torch.Size([111100, 2])
We keep 5.73e+07/4.56e+09 =  1% of the original kernel matrix.

torch.Size([63589, 2])
We keep 1.77e+07/1.42e+09 =  1% of the original kernel matrix.

torch.Size([19229, 2])
We keep 4.93e+06/1.02e+08 =  4% of the original kernel matrix.

torch.Size([26611, 2])
We keep 3.47e+06/2.12e+08 =  1% of the original kernel matrix.

torch.Size([45876, 2])
We keep 2.33e+07/8.66e+08 =  2% of the original kernel matrix.

torch.Size([40453, 2])
We keep 8.47e+06/6.18e+08 =  1% of the original kernel matrix.

torch.Size([357310, 2])
We keep 5.78e+08/4.47e+10 =  1% of the original kernel matrix.

torch.Size([116257, 2])
We keep 4.76e+07/4.44e+09 =  1% of the original kernel matrix.

torch.Size([13129, 2])
We keep 1.79e+06/5.52e+07 =  3% of the original kernel matrix.

torch.Size([22034, 2])
We keep 2.96e+06/1.56e+08 =  1% of the original kernel matrix.

torch.Size([71594, 2])
We keep 7.20e+07/2.18e+09 =  3% of the original kernel matrix.

torch.Size([50594, 2])
We keep 1.32e+07/9.81e+08 =  1% of the original kernel matrix.

torch.Size([24825, 2])
We keep 6.36e+06/2.04e+08 =  3% of the original kernel matrix.

torch.Size([29816, 2])
We keep 4.80e+06/3.00e+08 =  1% of the original kernel matrix.

torch.Size([132117, 2])
We keep 8.55e+07/6.49e+09 =  1% of the original kernel matrix.

torch.Size([69499, 2])
We keep 2.04e+07/1.69e+09 =  1% of the original kernel matrix.

torch.Size([319409, 2])
We keep 5.07e+08/4.30e+10 =  1% of the original kernel matrix.

torch.Size([111937, 2])
We keep 4.72e+07/4.35e+09 =  1% of the original kernel matrix.

torch.Size([36303, 2])
We keep 9.71e+06/4.22e+08 =  2% of the original kernel matrix.

torch.Size([37015, 2])
We keep 6.54e+06/4.31e+08 =  1% of the original kernel matrix.

torch.Size([13872, 2])
We keep 1.54e+06/4.88e+07 =  3% of the original kernel matrix.

torch.Size([22433, 2])
We keep 2.88e+06/1.47e+08 =  1% of the original kernel matrix.

torch.Size([166547, 2])
We keep 1.96e+08/1.30e+10 =  1% of the original kernel matrix.

torch.Size([78119, 2])
We keep 2.75e+07/2.40e+09 =  1% of the original kernel matrix.

torch.Size([94447, 2])
We keep 5.84e+07/3.36e+09 =  1% of the original kernel matrix.

torch.Size([57339, 2])
We keep 1.56e+07/1.22e+09 =  1% of the original kernel matrix.

torch.Size([468520, 2])
We keep 3.05e+09/1.79e+11 =  1% of the original kernel matrix.

torch.Size([131425, 2])
We keep 9.04e+07/8.87e+09 =  1% of the original kernel matrix.

torch.Size([63611, 2])
We keep 4.08e+07/1.49e+09 =  2% of the original kernel matrix.

torch.Size([47369, 2])
We keep 1.09e+07/8.11e+08 =  1% of the original kernel matrix.

torch.Size([36563, 2])
We keep 1.63e+07/6.36e+08 =  2% of the original kernel matrix.

torch.Size([36290, 2])
We keep 7.67e+06/5.30e+08 =  1% of the original kernel matrix.

torch.Size([37460, 2])
We keep 2.31e+07/6.18e+08 =  3% of the original kernel matrix.

torch.Size([37237, 2])
We keep 7.77e+06/5.22e+08 =  1% of the original kernel matrix.

torch.Size([131470, 2])
We keep 1.76e+08/8.41e+09 =  2% of the original kernel matrix.

torch.Size([67149, 2])
We keep 2.29e+07/1.93e+09 =  1% of the original kernel matrix.

torch.Size([185356, 2])
We keep 3.20e+08/2.02e+10 =  1% of the original kernel matrix.

torch.Size([82675, 2])
We keep 3.39e+07/2.98e+09 =  1% of the original kernel matrix.

torch.Size([41432, 2])
We keep 1.08e+07/5.82e+08 =  1% of the original kernel matrix.

torch.Size([39650, 2])
We keep 7.46e+06/5.07e+08 =  1% of the original kernel matrix.

torch.Size([11059, 2])
We keep 8.00e+05/2.30e+07 =  3% of the original kernel matrix.

torch.Size([19855, 2])
We keep 2.10e+06/1.01e+08 =  2% of the original kernel matrix.

torch.Size([13233, 2])
We keep 1.38e+06/4.25e+07 =  3% of the original kernel matrix.

torch.Size([21919, 2])
We keep 2.72e+06/1.37e+08 =  1% of the original kernel matrix.

torch.Size([60018, 2])
We keep 4.19e+07/1.43e+09 =  2% of the original kernel matrix.

torch.Size([45730, 2])
We keep 1.08e+07/7.94e+08 =  1% of the original kernel matrix.

torch.Size([93521, 2])
We keep 1.65e+08/4.60e+09 =  3% of the original kernel matrix.

torch.Size([57754, 2])
We keep 1.80e+07/1.42e+09 =  1% of the original kernel matrix.

torch.Size([87443, 2])
We keep 3.85e+07/2.30e+09 =  1% of the original kernel matrix.

torch.Size([55888, 2])
We keep 1.31e+07/1.01e+09 =  1% of the original kernel matrix.

torch.Size([17413, 2])
We keep 2.38e+06/8.60e+07 =  2% of the original kernel matrix.

torch.Size([25340, 2])
We keep 3.55e+06/1.95e+08 =  1% of the original kernel matrix.

torch.Size([32849, 2])
We keep 6.41e+06/3.74e+08 =  1% of the original kernel matrix.

torch.Size([33624, 2])
We keep 5.99e+06/4.06e+08 =  1% of the original kernel matrix.

torch.Size([38340, 2])
We keep 1.70e+07/5.15e+08 =  3% of the original kernel matrix.

torch.Size([37856, 2])
We keep 6.88e+06/4.77e+08 =  1% of the original kernel matrix.

torch.Size([48563, 2])
We keep 1.15e+07/7.42e+08 =  1% of the original kernel matrix.

torch.Size([42710, 2])
We keep 8.11e+06/5.72e+08 =  1% of the original kernel matrix.

torch.Size([30026, 2])
We keep 9.97e+06/3.32e+08 =  3% of the original kernel matrix.

torch.Size([33773, 2])
We keep 6.06e+06/3.83e+08 =  1% of the original kernel matrix.

torch.Size([69086, 2])
We keep 2.73e+07/1.78e+09 =  1% of the original kernel matrix.

torch.Size([49925, 2])
We keep 1.20e+07/8.86e+08 =  1% of the original kernel matrix.

torch.Size([15732, 2])
We keep 4.39e+06/1.12e+08 =  3% of the original kernel matrix.

torch.Size([23558, 2])
We keep 3.88e+06/2.22e+08 =  1% of the original kernel matrix.

torch.Size([33408, 2])
We keep 7.82e+06/3.76e+08 =  2% of the original kernel matrix.

torch.Size([34201, 2])
We keep 5.96e+06/4.07e+08 =  1% of the original kernel matrix.

torch.Size([5650, 2])
We keep 2.66e+05/5.11e+06 =  5% of the original kernel matrix.

torch.Size([14818, 2])
We keep 1.24e+06/4.75e+07 =  2% of the original kernel matrix.

torch.Size([11497, 2])
We keep 1.03e+06/2.87e+07 =  3% of the original kernel matrix.

torch.Size([20281, 2])
We keep 2.34e+06/1.13e+08 =  2% of the original kernel matrix.

torch.Size([159540, 2])
We keep 9.45e+07/9.65e+09 =  0% of the original kernel matrix.

torch.Size([77275, 2])
We keep 2.43e+07/2.06e+09 =  1% of the original kernel matrix.

torch.Size([22206, 2])
We keep 4.56e+06/1.48e+08 =  3% of the original kernel matrix.

torch.Size([28618, 2])
We keep 4.33e+06/2.56e+08 =  1% of the original kernel matrix.

torch.Size([77746, 2])
We keep 2.44e+07/1.83e+09 =  1% of the original kernel matrix.

torch.Size([52644, 2])
We keep 1.19e+07/8.98e+08 =  1% of the original kernel matrix.

torch.Size([5157, 2])
We keep 2.40e+05/3.94e+06 =  6% of the original kernel matrix.

torch.Size([14521, 2])
We keep 1.15e+06/4.17e+07 =  2% of the original kernel matrix.

torch.Size([9113, 2])
We keep 1.65e+06/1.91e+07 =  8% of the original kernel matrix.

torch.Size([18297, 2])
We keep 2.00e+06/9.17e+07 =  2% of the original kernel matrix.

torch.Size([31558, 2])
We keep 8.38e+06/3.49e+08 =  2% of the original kernel matrix.

torch.Size([32816, 2])
We keep 5.58e+06/3.93e+08 =  1% of the original kernel matrix.

torch.Size([75498, 2])
We keep 3.08e+07/1.79e+09 =  1% of the original kernel matrix.

torch.Size([51539, 2])
We keep 1.18e+07/8.89e+08 =  1% of the original kernel matrix.

torch.Size([30351, 2])
We keep 7.50e+06/3.47e+08 =  2% of the original kernel matrix.

torch.Size([34069, 2])
We keep 6.19e+06/3.91e+08 =  1% of the original kernel matrix.

torch.Size([9008, 2])
We keep 6.75e+05/1.63e+07 =  4% of the original kernel matrix.

torch.Size([18220, 2])
We keep 1.90e+06/8.49e+07 =  2% of the original kernel matrix.

torch.Size([11529, 2])
We keep 1.58e+06/3.74e+07 =  4% of the original kernel matrix.

torch.Size([20453, 2])
We keep 2.61e+06/1.28e+08 =  2% of the original kernel matrix.

torch.Size([30779, 2])
We keep 8.15e+06/2.93e+08 =  2% of the original kernel matrix.

torch.Size([34665, 2])
We keep 5.69e+06/3.60e+08 =  1% of the original kernel matrix.

torch.Size([32771, 2])
We keep 8.34e+06/3.30e+08 =  2% of the original kernel matrix.

torch.Size([33544, 2])
We keep 5.47e+06/3.81e+08 =  1% of the original kernel matrix.

torch.Size([115875, 2])
We keep 1.38e+08/5.31e+09 =  2% of the original kernel matrix.

torch.Size([64836, 2])
We keep 1.91e+07/1.53e+09 =  1% of the original kernel matrix.

torch.Size([10001, 2])
We keep 7.27e+05/1.97e+07 =  3% of the original kernel matrix.

torch.Size([19212, 2])
We keep 1.99e+06/9.33e+07 =  2% of the original kernel matrix.

torch.Size([14912, 2])
We keep 2.29e+06/6.36e+07 =  3% of the original kernel matrix.

torch.Size([22813, 2])
We keep 3.10e+06/1.67e+08 =  1% of the original kernel matrix.

torch.Size([36323, 2])
We keep 2.66e+07/9.16e+08 =  2% of the original kernel matrix.

torch.Size([34481, 2])
We keep 8.97e+06/6.36e+08 =  1% of the original kernel matrix.

torch.Size([14440, 2])
We keep 1.58e+06/5.00e+07 =  3% of the original kernel matrix.

torch.Size([22704, 2])
We keep 2.82e+06/1.49e+08 =  1% of the original kernel matrix.

torch.Size([155792, 2])
We keep 2.49e+08/1.27e+10 =  1% of the original kernel matrix.

torch.Size([76543, 2])
We keep 2.77e+07/2.37e+09 =  1% of the original kernel matrix.

torch.Size([10492, 2])
We keep 1.73e+06/2.45e+07 =  7% of the original kernel matrix.

torch.Size([19842, 2])
We keep 2.10e+06/1.04e+08 =  2% of the original kernel matrix.

torch.Size([65490, 2])
We keep 3.74e+07/1.89e+09 =  1% of the original kernel matrix.

torch.Size([48337, 2])
We keep 1.23e+07/9.14e+08 =  1% of the original kernel matrix.

torch.Size([167128, 2])
We keep 1.45e+08/1.13e+10 =  1% of the original kernel matrix.

torch.Size([79573, 2])
We keep 2.62e+07/2.23e+09 =  1% of the original kernel matrix.

torch.Size([31045, 2])
We keep 6.41e+06/2.91e+08 =  2% of the original kernel matrix.

torch.Size([35186, 2])
We keep 5.69e+06/3.58e+08 =  1% of the original kernel matrix.

torch.Size([142606, 2])
We keep 1.62e+08/8.71e+09 =  1% of the original kernel matrix.

torch.Size([73928, 2])
We keep 2.38e+07/1.96e+09 =  1% of the original kernel matrix.

torch.Size([136716, 2])
We keep 9.43e+07/6.76e+09 =  1% of the original kernel matrix.

torch.Size([71597, 2])
We keep 2.10e+07/1.73e+09 =  1% of the original kernel matrix.

torch.Size([3845, 2])
We keep 1.30e+05/2.11e+06 =  6% of the original kernel matrix.

torch.Size([12936, 2])
We keep 9.29e+05/3.05e+07 =  3% of the original kernel matrix.

torch.Size([50818, 2])
We keep 1.56e+07/9.10e+08 =  1% of the original kernel matrix.

torch.Size([43389, 2])
We keep 9.06e+06/6.34e+08 =  1% of the original kernel matrix.

torch.Size([21312, 2])
We keep 2.39e+06/1.15e+08 =  2% of the original kernel matrix.

torch.Size([28395, 2])
We keep 3.87e+06/2.25e+08 =  1% of the original kernel matrix.

torch.Size([206103, 2])
We keep 2.64e+08/1.53e+10 =  1% of the original kernel matrix.

torch.Size([88658, 2])
We keep 2.96e+07/2.59e+09 =  1% of the original kernel matrix.

torch.Size([84137, 2])
We keep 6.08e+07/2.99e+09 =  2% of the original kernel matrix.

torch.Size([55146, 2])
We keep 1.47e+07/1.15e+09 =  1% of the original kernel matrix.

torch.Size([33054, 2])
We keep 2.12e+07/6.06e+08 =  3% of the original kernel matrix.

torch.Size([34145, 2])
We keep 7.65e+06/5.17e+08 =  1% of the original kernel matrix.

torch.Size([74238, 2])
We keep 2.52e+07/1.81e+09 =  1% of the original kernel matrix.

torch.Size([51615, 2])
We keep 1.19e+07/8.95e+08 =  1% of the original kernel matrix.

torch.Size([37472, 2])
We keep 1.32e+07/4.93e+08 =  2% of the original kernel matrix.

torch.Size([37001, 2])
We keep 6.82e+06/4.66e+08 =  1% of the original kernel matrix.

torch.Size([17604, 2])
We keep 1.16e+07/2.21e+08 =  5% of the original kernel matrix.

torch.Size([24490, 2])
We keep 4.97e+06/3.12e+08 =  1% of the original kernel matrix.

torch.Size([6805, 2])
We keep 3.10e+05/7.32e+06 =  4% of the original kernel matrix.

torch.Size([16142, 2])
We keep 1.37e+06/5.68e+07 =  2% of the original kernel matrix.

torch.Size([14336, 2])
We keep 1.95e+06/5.19e+07 =  3% of the original kernel matrix.

torch.Size([22713, 2])
We keep 2.86e+06/1.51e+08 =  1% of the original kernel matrix.

torch.Size([170699, 2])
We keep 3.26e+08/1.13e+10 =  2% of the original kernel matrix.

torch.Size([80573, 2])
We keep 2.62e+07/2.24e+09 =  1% of the original kernel matrix.

torch.Size([586717, 2])
We keep 1.10e+09/1.17e+11 =  0% of the original kernel matrix.

torch.Size([155754, 2])
We keep 7.40e+07/7.20e+09 =  1% of the original kernel matrix.

torch.Size([31539, 2])
We keep 8.74e+06/4.09e+08 =  2% of the original kernel matrix.

torch.Size([34014, 2])
We keep 6.58e+06/4.25e+08 =  1% of the original kernel matrix.

torch.Size([21121, 2])
We keep 3.12e+06/1.30e+08 =  2% of the original kernel matrix.

torch.Size([28240, 2])
We keep 4.08e+06/2.39e+08 =  1% of the original kernel matrix.

torch.Size([43837, 2])
We keep 1.19e+07/6.76e+08 =  1% of the original kernel matrix.

torch.Size([40729, 2])
We keep 7.97e+06/5.46e+08 =  1% of the original kernel matrix.

torch.Size([125852, 2])
We keep 1.31e+08/6.94e+09 =  1% of the original kernel matrix.

torch.Size([68590, 2])
We keep 2.06e+07/1.75e+09 =  1% of the original kernel matrix.

torch.Size([42013, 2])
We keep 1.04e+07/5.71e+08 =  1% of the original kernel matrix.

torch.Size([39748, 2])
We keep 7.36e+06/5.02e+08 =  1% of the original kernel matrix.

torch.Size([13644, 2])
We keep 1.42e+06/4.41e+07 =  3% of the original kernel matrix.

torch.Size([22268, 2])
We keep 2.67e+06/1.39e+08 =  1% of the original kernel matrix.

torch.Size([57172, 2])
We keep 2.21e+07/1.27e+09 =  1% of the original kernel matrix.

torch.Size([45875, 2])
We keep 1.04e+07/7.48e+08 =  1% of the original kernel matrix.

torch.Size([139326, 2])
We keep 1.05e+08/7.04e+09 =  1% of the original kernel matrix.

torch.Size([72186, 2])
We keep 2.13e+07/1.76e+09 =  1% of the original kernel matrix.

torch.Size([8575, 2])
We keep 1.22e+06/1.90e+07 =  6% of the original kernel matrix.

torch.Size([17660, 2])
We keep 2.00e+06/9.16e+07 =  2% of the original kernel matrix.

torch.Size([32699, 2])
We keep 1.75e+07/4.13e+08 =  4% of the original kernel matrix.

torch.Size([35388, 2])
We keep 6.67e+06/4.27e+08 =  1% of the original kernel matrix.

torch.Size([15086, 2])
We keep 5.24e+06/5.26e+07 =  9% of the original kernel matrix.

torch.Size([23551, 2])
We keep 2.58e+06/1.52e+08 =  1% of the original kernel matrix.

time for making ranges is 4.075313329696655
Sorting X and nu_X
time for sorting X is 0.08431100845336914
Sorting Z and nu_Z
time for sorting Z is 0.0002574920654296875
Starting Optim
sum tnu_Z before tensor(31607418., device='cuda:0')
c= tensor(1303.6429, device='cuda:0')
c= tensor(111984.0312, device='cuda:0')
c= tensor(114174.1641, device='cuda:0')
c= tensor(216302.9062, device='cuda:0')
c= tensor(425880.5625, device='cuda:0')
c= tensor(801982.0625, device='cuda:0')
c= tensor(1255949.2500, device='cuda:0')
c= tensor(1553457.3750, device='cuda:0')
c= tensor(1946142.8750, device='cuda:0')
c= tensor(5882444.5000, device='cuda:0')
c= tensor(5913413., device='cuda:0')
c= tensor(8275076., device='cuda:0')
c= tensor(8290912.5000, device='cuda:0')
c= tensor(27483778., device='cuda:0')
c= tensor(27668938., device='cuda:0')
c= tensor(27911792., device='cuda:0')
c= tensor(28755616., device='cuda:0')
c= tensor(29004630., device='cuda:0')
c= tensor(33174780., device='cuda:0')
c= tensor(34938620., device='cuda:0')
c= tensor(35384508., device='cuda:0')
c= tensor(41404708., device='cuda:0')
c= tensor(41428484., device='cuda:0')
c= tensor(42088112., device='cuda:0')
c= tensor(42106884., device='cuda:0')
c= tensor(42736236., device='cuda:0')
c= tensor(44027656., device='cuda:0')
c= tensor(44084348., device='cuda:0')
c= tensor(51809840., device='cuda:0')
c= tensor(2.0506e+08, device='cuda:0')
c= tensor(2.0512e+08, device='cuda:0')
c= tensor(3.6097e+08, device='cuda:0')
c= tensor(3.6116e+08, device='cuda:0')
c= tensor(3.6122e+08, device='cuda:0')
c= tensor(3.6134e+08, device='cuda:0')
c= tensor(3.7841e+08, device='cuda:0')
c= tensor(3.8002e+08, device='cuda:0')
c= tensor(3.8002e+08, device='cuda:0')
c= tensor(3.8003e+08, device='cuda:0')
c= tensor(3.8003e+08, device='cuda:0')
c= tensor(3.8003e+08, device='cuda:0')
c= tensor(3.8004e+08, device='cuda:0')
c= tensor(3.8004e+08, device='cuda:0')
c= tensor(3.8004e+08, device='cuda:0')
c= tensor(3.8004e+08, device='cuda:0')
c= tensor(3.8004e+08, device='cuda:0')
c= tensor(3.8005e+08, device='cuda:0')
c= tensor(3.8005e+08, device='cuda:0')
c= tensor(3.8006e+08, device='cuda:0')
c= tensor(3.8010e+08, device='cuda:0')
c= tensor(3.8014e+08, device='cuda:0')
c= tensor(3.8014e+08, device='cuda:0')
c= tensor(3.8015e+08, device='cuda:0')
c= tensor(3.8016e+08, device='cuda:0')
c= tensor(3.8017e+08, device='cuda:0')
c= tensor(3.8018e+08, device='cuda:0')
c= tensor(3.8018e+08, device='cuda:0')
c= tensor(3.8019e+08, device='cuda:0')
c= tensor(3.8019e+08, device='cuda:0')
c= tensor(3.8020e+08, device='cuda:0')
c= tensor(3.8021e+08, device='cuda:0')
c= tensor(3.8021e+08, device='cuda:0')
c= tensor(3.8022e+08, device='cuda:0')
c= tensor(3.8024e+08, device='cuda:0')
c= tensor(3.8025e+08, device='cuda:0')
c= tensor(3.8025e+08, device='cuda:0')
c= tensor(3.8025e+08, device='cuda:0')
c= tensor(3.8026e+08, device='cuda:0')
c= tensor(3.8026e+08, device='cuda:0')
c= tensor(3.8027e+08, device='cuda:0')
c= tensor(3.8029e+08, device='cuda:0')
c= tensor(3.8031e+08, device='cuda:0')
c= tensor(3.8031e+08, device='cuda:0')
c= tensor(3.8032e+08, device='cuda:0')
c= tensor(3.8033e+08, device='cuda:0')
c= tensor(3.8036e+08, device='cuda:0')
c= tensor(3.8036e+08, device='cuda:0')
c= tensor(3.8036e+08, device='cuda:0')
c= tensor(3.8037e+08, device='cuda:0')
c= tensor(3.8040e+08, device='cuda:0')
c= tensor(3.8040e+08, device='cuda:0')
c= tensor(3.8040e+08, device='cuda:0')
c= tensor(3.8041e+08, device='cuda:0')
c= tensor(3.8041e+08, device='cuda:0')
c= tensor(3.8041e+08, device='cuda:0')
c= tensor(3.8041e+08, device='cuda:0')
c= tensor(3.8042e+08, device='cuda:0')
c= tensor(3.8042e+08, device='cuda:0')
c= tensor(3.8042e+08, device='cuda:0')
c= tensor(3.8043e+08, device='cuda:0')
c= tensor(3.8043e+08, device='cuda:0')
c= tensor(3.8044e+08, device='cuda:0')
c= tensor(3.8044e+08, device='cuda:0')
c= tensor(3.8046e+08, device='cuda:0')
c= tensor(3.8046e+08, device='cuda:0')
c= tensor(3.8047e+08, device='cuda:0')
c= tensor(3.8047e+08, device='cuda:0')
c= tensor(3.8049e+08, device='cuda:0')
c= tensor(3.8052e+08, device='cuda:0')
c= tensor(3.8052e+08, device='cuda:0')
c= tensor(3.8055e+08, device='cuda:0')
c= tensor(3.8056e+08, device='cuda:0')
c= tensor(3.8057e+08, device='cuda:0')
c= tensor(3.8057e+08, device='cuda:0')
c= tensor(3.8059e+08, device='cuda:0')
c= tensor(3.8059e+08, device='cuda:0')
c= tensor(3.8059e+08, device='cuda:0')
c= tensor(3.8060e+08, device='cuda:0')
c= tensor(3.8060e+08, device='cuda:0')
c= tensor(3.8060e+08, device='cuda:0')
c= tensor(3.8060e+08, device='cuda:0')
c= tensor(3.8061e+08, device='cuda:0')
c= tensor(3.8061e+08, device='cuda:0')
c= tensor(3.8062e+08, device='cuda:0')
c= tensor(3.8063e+08, device='cuda:0')
c= tensor(3.8063e+08, device='cuda:0')
c= tensor(3.8064e+08, device='cuda:0')
c= tensor(3.8064e+08, device='cuda:0')
c= tensor(3.8065e+08, device='cuda:0')
c= tensor(3.8065e+08, device='cuda:0')
c= tensor(3.8067e+08, device='cuda:0')
c= tensor(3.8067e+08, device='cuda:0')
c= tensor(3.8067e+08, device='cuda:0')
c= tensor(3.8068e+08, device='cuda:0')
c= tensor(3.8068e+08, device='cuda:0')
c= tensor(3.8069e+08, device='cuda:0')
c= tensor(3.8069e+08, device='cuda:0')
c= tensor(3.8069e+08, device='cuda:0')
c= tensor(3.8071e+08, device='cuda:0')
c= tensor(3.8071e+08, device='cuda:0')
c= tensor(3.8073e+08, device='cuda:0')
c= tensor(3.8073e+08, device='cuda:0')
c= tensor(3.8073e+08, device='cuda:0')
c= tensor(3.8074e+08, device='cuda:0')
c= tensor(3.8074e+08, device='cuda:0')
c= tensor(3.8074e+08, device='cuda:0')
c= tensor(3.8074e+08, device='cuda:0')
c= tensor(3.8075e+08, device='cuda:0')
c= tensor(3.8075e+08, device='cuda:0')
c= tensor(3.8075e+08, device='cuda:0')
c= tensor(3.8075e+08, device='cuda:0')
c= tensor(3.8075e+08, device='cuda:0')
c= tensor(3.8077e+08, device='cuda:0')
c= tensor(3.8080e+08, device='cuda:0')
c= tensor(3.8081e+08, device='cuda:0')
c= tensor(3.8081e+08, device='cuda:0')
c= tensor(3.8081e+08, device='cuda:0')
c= tensor(3.8082e+08, device='cuda:0')
c= tensor(3.8082e+08, device='cuda:0')
c= tensor(3.8082e+08, device='cuda:0')
c= tensor(3.8082e+08, device='cuda:0')
c= tensor(3.8083e+08, device='cuda:0')
c= tensor(3.8083e+08, device='cuda:0')
c= tensor(3.8086e+08, device='cuda:0')
c= tensor(3.8087e+08, device='cuda:0')
c= tensor(3.8095e+08, device='cuda:0')
c= tensor(3.8095e+08, device='cuda:0')
c= tensor(3.8096e+08, device='cuda:0')
c= tensor(3.8096e+08, device='cuda:0')
c= tensor(3.8096e+08, device='cuda:0')
c= tensor(3.8101e+08, device='cuda:0')
c= tensor(3.8101e+08, device='cuda:0')
c= tensor(3.8105e+08, device='cuda:0')
c= tensor(3.8105e+08, device='cuda:0')
c= tensor(3.8106e+08, device='cuda:0')
c= tensor(3.8107e+08, device='cuda:0')
c= tensor(3.8107e+08, device='cuda:0')
c= tensor(3.8107e+08, device='cuda:0')
c= tensor(3.8108e+08, device='cuda:0')
c= tensor(3.8108e+08, device='cuda:0')
c= tensor(3.8108e+08, device='cuda:0')
c= tensor(3.8108e+08, device='cuda:0')
c= tensor(3.8109e+08, device='cuda:0')
c= tensor(3.8109e+08, device='cuda:0')
c= tensor(3.8110e+08, device='cuda:0')
c= tensor(3.8110e+08, device='cuda:0')
c= tensor(3.8111e+08, device='cuda:0')
c= tensor(3.8111e+08, device='cuda:0')
c= tensor(3.8112e+08, device='cuda:0')
c= tensor(3.8113e+08, device='cuda:0')
c= tensor(3.8114e+08, device='cuda:0')
c= tensor(3.8114e+08, device='cuda:0')
c= tensor(3.8116e+08, device='cuda:0')
c= tensor(3.8116e+08, device='cuda:0')
c= tensor(3.8116e+08, device='cuda:0')
c= tensor(3.8117e+08, device='cuda:0')
c= tensor(3.8117e+08, device='cuda:0')
c= tensor(3.8118e+08, device='cuda:0')
c= tensor(3.8119e+08, device='cuda:0')
c= tensor(3.8121e+08, device='cuda:0')
c= tensor(3.8122e+08, device='cuda:0')
c= tensor(3.8122e+08, device='cuda:0')
c= tensor(3.8122e+08, device='cuda:0')
c= tensor(3.8122e+08, device='cuda:0')
c= tensor(3.8123e+08, device='cuda:0')
c= tensor(3.8124e+08, device='cuda:0')
c= tensor(3.8124e+08, device='cuda:0')
c= tensor(3.8124e+08, device='cuda:0')
c= tensor(3.8125e+08, device='cuda:0')
c= tensor(3.8125e+08, device='cuda:0')
c= tensor(3.8127e+08, device='cuda:0')
c= tensor(3.8127e+08, device='cuda:0')
c= tensor(3.8130e+08, device='cuda:0')
c= tensor(3.8131e+08, device='cuda:0')
c= tensor(3.8131e+08, device='cuda:0')
c= tensor(3.8132e+08, device='cuda:0')
c= tensor(3.8132e+08, device='cuda:0')
c= tensor(3.8133e+08, device='cuda:0')
c= tensor(3.8133e+08, device='cuda:0')
c= tensor(3.8134e+08, device='cuda:0')
c= tensor(3.8135e+08, device='cuda:0')
c= tensor(3.8136e+08, device='cuda:0')
c= tensor(3.8136e+08, device='cuda:0')
c= tensor(3.8137e+08, device='cuda:0')
c= tensor(3.8137e+08, device='cuda:0')
c= tensor(3.8137e+08, device='cuda:0')
c= tensor(3.8137e+08, device='cuda:0')
c= tensor(3.8137e+08, device='cuda:0')
c= tensor(3.8147e+08, device='cuda:0')
c= tensor(3.8147e+08, device='cuda:0')
c= tensor(3.8148e+08, device='cuda:0')
c= tensor(3.8148e+08, device='cuda:0')
c= tensor(3.8149e+08, device='cuda:0')
c= tensor(3.8150e+08, device='cuda:0')
c= tensor(3.8150e+08, device='cuda:0')
c= tensor(3.8150e+08, device='cuda:0')
c= tensor(3.8150e+08, device='cuda:0')
c= tensor(3.8151e+08, device='cuda:0')
c= tensor(3.8151e+08, device='cuda:0')
c= tensor(3.8151e+08, device='cuda:0')
c= tensor(3.8152e+08, device='cuda:0')
c= tensor(3.8152e+08, device='cuda:0')
c= tensor(3.8152e+08, device='cuda:0')
c= tensor(3.8153e+08, device='cuda:0')
c= tensor(3.8153e+08, device='cuda:0')
c= tensor(3.8153e+08, device='cuda:0')
c= tensor(3.8155e+08, device='cuda:0')
c= tensor(3.8156e+08, device='cuda:0')
c= tensor(3.8156e+08, device='cuda:0')
c= tensor(3.8172e+08, device='cuda:0')
c= tensor(3.8320e+08, device='cuda:0')
c= tensor(3.8322e+08, device='cuda:0')
c= tensor(3.8324e+08, device='cuda:0')
c= tensor(3.8324e+08, device='cuda:0')
c= tensor(3.8325e+08, device='cuda:0')
c= tensor(3.8326e+08, device='cuda:0')
c= tensor(3.8760e+08, device='cuda:0')
c= tensor(3.8761e+08, device='cuda:0')
c= tensor(3.9146e+08, device='cuda:0')
c= tensor(3.9183e+08, device='cuda:0')
c= tensor(3.9211e+08, device='cuda:0')
c= tensor(3.9234e+08, device='cuda:0')
c= tensor(3.9235e+08, device='cuda:0')
c= tensor(3.9235e+08, device='cuda:0')
c= tensor(4.0491e+08, device='cuda:0')
c= tensor(4.2864e+08, device='cuda:0')
c= tensor(4.2865e+08, device='cuda:0')
c= tensor(4.2895e+08, device='cuda:0')
c= tensor(4.2952e+08, device='cuda:0')
c= tensor(4.2971e+08, device='cuda:0')
c= tensor(4.3164e+08, device='cuda:0')
c= tensor(4.3296e+08, device='cuda:0')
c= tensor(4.3320e+08, device='cuda:0')
c= tensor(4.3331e+08, device='cuda:0')
c= tensor(4.3331e+08, device='cuda:0')
c= tensor(4.4172e+08, device='cuda:0')
c= tensor(4.4180e+08, device='cuda:0')
c= tensor(4.4180e+08, device='cuda:0')
c= tensor(4.4186e+08, device='cuda:0')
c= tensor(4.4339e+08, device='cuda:0')
c= tensor(4.5833e+08, device='cuda:0')
c= tensor(4.5935e+08, device='cuda:0')
c= tensor(4.5936e+08, device='cuda:0')
c= tensor(4.5975e+08, device='cuda:0')
c= tensor(4.5977e+08, device='cuda:0')
c= tensor(4.6022e+08, device='cuda:0')
c= tensor(4.6086e+08, device='cuda:0')
c= tensor(4.6186e+08, device='cuda:0')
c= tensor(4.6261e+08, device='cuda:0')
c= tensor(4.6262e+08, device='cuda:0')
c= tensor(4.6263e+08, device='cuda:0')
c= tensor(4.6449e+08, device='cuda:0')
c= tensor(4.6516e+08, device='cuda:0')
c= tensor(4.6631e+08, device='cuda:0')
c= tensor(4.6632e+08, device='cuda:0')
c= tensor(4.8760e+08, device='cuda:0')
c= tensor(4.8764e+08, device='cuda:0')
c= tensor(4.8788e+08, device='cuda:0')
c= tensor(4.8979e+08, device='cuda:0')
c= tensor(4.8980e+08, device='cuda:0')
c= tensor(4.9148e+08, device='cuda:0')
c= tensor(5.0408e+08, device='cuda:0')
c= tensor(6.0013e+08, device='cuda:0')
c= tensor(6.0047e+08, device='cuda:0')
c= tensor(6.0068e+08, device='cuda:0')
c= tensor(6.0070e+08, device='cuda:0')
c= tensor(6.0071e+08, device='cuda:0')
c= tensor(6.0830e+08, device='cuda:0')
c= tensor(6.0832e+08, device='cuda:0')
c= tensor(6.0877e+08, device='cuda:0')
c= tensor(6.1195e+08, device='cuda:0')
c= tensor(6.1217e+08, device='cuda:0')
c= tensor(6.1243e+08, device='cuda:0')
c= tensor(6.1244e+08, device='cuda:0')
c= tensor(6.2084e+08, device='cuda:0')
c= tensor(6.2092e+08, device='cuda:0')
c= tensor(6.2144e+08, device='cuda:0')
c= tensor(6.2148e+08, device='cuda:0')
c= tensor(6.2325e+08, device='cuda:0')
c= tensor(6.2333e+08, device='cuda:0')
c= tensor(6.3289e+08, device='cuda:0')
c= tensor(6.3293e+08, device='cuda:0')
c= tensor(6.3416e+08, device='cuda:0')
c= tensor(6.3423e+08, device='cuda:0')
c= tensor(6.4512e+08, device='cuda:0')
c= tensor(6.4578e+08, device='cuda:0')
c= tensor(6.4582e+08, device='cuda:0')
c= tensor(6.4861e+08, device='cuda:0')
c= tensor(6.5142e+08, device='cuda:0')
c= tensor(6.5143e+08, device='cuda:0')
c= tensor(6.5824e+08, device='cuda:0')
c= tensor(6.6692e+08, device='cuda:0')
c= tensor(6.8762e+08, device='cuda:0')
c= tensor(6.8796e+08, device='cuda:0')
c= tensor(6.8796e+08, device='cuda:0')
c= tensor(6.8798e+08, device='cuda:0')
c= tensor(6.8822e+08, device='cuda:0')
c= tensor(6.8836e+08, device='cuda:0')
c= tensor(6.8909e+08, device='cuda:0')
c= tensor(6.8909e+08, device='cuda:0')
c= tensor(6.8989e+08, device='cuda:0')
c= tensor(6.9193e+08, device='cuda:0')
c= tensor(6.9208e+08, device='cuda:0')
c= tensor(6.9211e+08, device='cuda:0')
c= tensor(6.9308e+08, device='cuda:0')
c= tensor(6.9311e+08, device='cuda:0')
c= tensor(6.9317e+08, device='cuda:0')
c= tensor(6.9322e+08, device='cuda:0')
c= tensor(6.9323e+08, device='cuda:0')
c= tensor(6.9719e+08, device='cuda:0')
c= tensor(6.9753e+08, device='cuda:0')
c= tensor(6.9765e+08, device='cuda:0')
c= tensor(6.9851e+08, device='cuda:0')
c= tensor(6.9855e+08, device='cuda:0')
c= tensor(7.5304e+08, device='cuda:0')
c= tensor(7.5307e+08, device='cuda:0')
c= tensor(7.5610e+08, device='cuda:0')
c= tensor(7.5611e+08, device='cuda:0')
c= tensor(7.5611e+08, device='cuda:0')
c= tensor(7.5612e+08, device='cuda:0')
c= tensor(7.5618e+08, device='cuda:0')
c= tensor(7.5619e+08, device='cuda:0')
c= tensor(7.5751e+08, device='cuda:0')
c= tensor(7.5752e+08, device='cuda:0')
c= tensor(7.5753e+08, device='cuda:0')
c= tensor(7.6559e+08, device='cuda:0')
c= tensor(7.6613e+08, device='cuda:0')
c= tensor(7.6637e+08, device='cuda:0')
c= tensor(7.6861e+08, device='cuda:0')
c= tensor(7.8224e+08, device='cuda:0')
c= tensor(7.8227e+08, device='cuda:0')
c= tensor(7.8230e+08, device='cuda:0')
c= tensor(7.8245e+08, device='cuda:0')
c= tensor(7.8245e+08, device='cuda:0')
c= tensor(7.8246e+08, device='cuda:0')
c= tensor(7.8255e+08, device='cuda:0')
c= tensor(7.8256e+08, device='cuda:0')
c= tensor(7.8256e+08, device='cuda:0')
c= tensor(7.8258e+08, device='cuda:0')
c= tensor(7.8259e+08, device='cuda:0')
c= tensor(7.9785e+08, device='cuda:0')
c= tensor(7.9794e+08, device='cuda:0')
c= tensor(7.9970e+08, device='cuda:0')
c= tensor(7.9977e+08, device='cuda:0')
c= tensor(7.9977e+08, device='cuda:0')
c= tensor(8.0090e+08, device='cuda:0')
c= tensor(8.3919e+08, device='cuda:0')
c= tensor(8.5920e+08, device='cuda:0')
c= tensor(8.5927e+08, device='cuda:0')
c= tensor(8.5958e+08, device='cuda:0')
c= tensor(8.5958e+08, device='cuda:0')
c= tensor(8.5961e+08, device='cuda:0')
c= tensor(1.0451e+09, device='cuda:0')
c= tensor(1.0452e+09, device='cuda:0')
c= tensor(1.0452e+09, device='cuda:0')
c= tensor(1.0459e+09, device='cuda:0')
c= tensor(1.1131e+09, device='cuda:0')
c= tensor(1.1134e+09, device='cuda:0')
c= tensor(1.1134e+09, device='cuda:0')
c= tensor(1.1134e+09, device='cuda:0')
c= tensor(1.1136e+09, device='cuda:0')
c= tensor(1.1136e+09, device='cuda:0')
c= tensor(1.1196e+09, device='cuda:0')
c= tensor(1.1197e+09, device='cuda:0')
c= tensor(1.1197e+09, device='cuda:0')
c= tensor(1.1200e+09, device='cuda:0')
c= tensor(1.1201e+09, device='cuda:0')
c= tensor(1.1201e+09, device='cuda:0')
c= tensor(1.1213e+09, device='cuda:0')
c= tensor(1.1233e+09, device='cuda:0')
c= tensor(1.1377e+09, device='cuda:0')
c= tensor(1.1457e+09, device='cuda:0')
c= tensor(1.1532e+09, device='cuda:0')
c= tensor(1.1533e+09, device='cuda:0')
c= tensor(1.1534e+09, device='cuda:0')
c= tensor(1.1543e+09, device='cuda:0')
c= tensor(1.1578e+09, device='cuda:0')
c= tensor(1.1578e+09, device='cuda:0')
c= tensor(1.1652e+09, device='cuda:0')
c= tensor(1.1712e+09, device='cuda:0')
c= tensor(1.1755e+09, device='cuda:0')
c= tensor(1.1770e+09, device='cuda:0')
c= tensor(1.1797e+09, device='cuda:0')
c= tensor(1.1798e+09, device='cuda:0')
c= tensor(1.1798e+09, device='cuda:0')
c= tensor(1.1799e+09, device='cuda:0')
c= tensor(1.1815e+09, device='cuda:0')
c= tensor(1.1849e+09, device='cuda:0')
c= tensor(1.2013e+09, device='cuda:0')
c= tensor(1.2042e+09, device='cuda:0')
c= tensor(1.2051e+09, device='cuda:0')
c= tensor(1.2054e+09, device='cuda:0')
c= tensor(1.2139e+09, device='cuda:0')
c= tensor(1.2139e+09, device='cuda:0')
c= tensor(1.2140e+09, device='cuda:0')
c= tensor(1.2185e+09, device='cuda:0')
c= tensor(1.2191e+09, device='cuda:0')
c= tensor(1.2191e+09, device='cuda:0')
c= tensor(1.2192e+09, device='cuda:0')
c= tensor(1.2634e+09, device='cuda:0')
c= tensor(1.2635e+09, device='cuda:0')
c= tensor(1.2645e+09, device='cuda:0')
c= tensor(1.2645e+09, device='cuda:0')
c= tensor(1.2645e+09, device='cuda:0')
c= tensor(1.2645e+09, device='cuda:0')
c= tensor(1.2646e+09, device='cuda:0')
c= tensor(1.2647e+09, device='cuda:0')
c= tensor(1.2662e+09, device='cuda:0')
c= tensor(1.2662e+09, device='cuda:0')
c= tensor(1.2687e+09, device='cuda:0')
c= tensor(1.2687e+09, device='cuda:0')
c= tensor(1.2693e+09, device='cuda:0')
c= tensor(1.2694e+09, device='cuda:0')
c= tensor(1.2707e+09, device='cuda:0')
c= tensor(1.2707e+09, device='cuda:0')
c= tensor(1.2708e+09, device='cuda:0')
c= tensor(1.2710e+09, device='cuda:0')
c= tensor(1.2712e+09, device='cuda:0')
c= tensor(1.2724e+09, device='cuda:0')
c= tensor(1.3136e+09, device='cuda:0')
c= tensor(1.3136e+09, device='cuda:0')
c= tensor(1.3136e+09, device='cuda:0')
c= tensor(1.3146e+09, device='cuda:0')
c= tensor(1.3147e+09, device='cuda:0')
c= tensor(1.3456e+09, device='cuda:0')
c= tensor(1.3456e+09, device='cuda:0')
c= tensor(1.3485e+09, device='cuda:0')
c= tensor(1.3573e+09, device='cuda:0')
c= tensor(1.3573e+09, device='cuda:0')
c= tensor(1.3656e+09, device='cuda:0')
c= tensor(1.3660e+09, device='cuda:0')
c= tensor(1.4114e+09, device='cuda:0')
c= tensor(1.4114e+09, device='cuda:0')
c= tensor(1.4114e+09, device='cuda:0')
c= tensor(1.4115e+09, device='cuda:0')
c= tensor(1.4115e+09, device='cuda:0')
c= tensor(1.4115e+09, device='cuda:0')
c= tensor(1.4120e+09, device='cuda:0')
c= tensor(1.4122e+09, device='cuda:0')
c= tensor(1.4150e+09, device='cuda:0')
c= tensor(1.4150e+09, device='cuda:0')
c= tensor(1.4150e+09, device='cuda:0')
c= tensor(1.4151e+09, device='cuda:0')
c= tensor(1.4219e+09, device='cuda:0')
c= tensor(1.4240e+09, device='cuda:0')
c= tensor(1.4353e+09, device='cuda:0')
c= tensor(1.4354e+09, device='cuda:0')
c= tensor(1.4354e+09, device='cuda:0')
c= tensor(1.4354e+09, device='cuda:0')
c= tensor(1.4354e+09, device='cuda:0')
c= tensor(1.4492e+09, device='cuda:0')
c= tensor(1.4492e+09, device='cuda:0')
c= tensor(1.4493e+09, device='cuda:0')
c= tensor(1.4504e+09, device='cuda:0')
c= tensor(1.4509e+09, device='cuda:0')
c= tensor(1.4509e+09, device='cuda:0')
c= tensor(1.4509e+09, device='cuda:0')
c= tensor(1.4628e+09, device='cuda:0')
c= tensor(1.4631e+09, device='cuda:0')
c= tensor(1.4631e+09, device='cuda:0')
c= tensor(1.4632e+09, device='cuda:0')
c= tensor(1.4717e+09, device='cuda:0')
c= tensor(1.4726e+09, device='cuda:0')
c= tensor(1.4892e+09, device='cuda:0')
c= tensor(1.4902e+09, device='cuda:0')
c= tensor(1.4902e+09, device='cuda:0')
c= tensor(1.4905e+09, device='cuda:0')
c= tensor(1.4911e+09, device='cuda:0')
c= tensor(1.4911e+09, device='cuda:0')
c= tensor(1.4912e+09, device='cuda:0')
c= tensor(1.4912e+09, device='cuda:0')
c= tensor(1.4920e+09, device='cuda:0')
c= tensor(1.4922e+09, device='cuda:0')
c= tensor(1.4922e+09, device='cuda:0')
c= tensor(1.4923e+09, device='cuda:0')
c= tensor(1.4923e+09, device='cuda:0')
c= tensor(1.4930e+09, device='cuda:0')
c= tensor(1.4931e+09, device='cuda:0')
c= tensor(1.4932e+09, device='cuda:0')
c= tensor(1.4932e+09, device='cuda:0')
c= tensor(1.4933e+09, device='cuda:0')
c= tensor(1.4933e+09, device='cuda:0')
c= tensor(1.4933e+09, device='cuda:0')
c= tensor(1.4934e+09, device='cuda:0')
c= tensor(1.4972e+09, device='cuda:0')
c= tensor(1.4972e+09, device='cuda:0')
c= tensor(1.4972e+09, device='cuda:0')
c= tensor(1.4973e+09, device='cuda:0')
c= tensor(1.4996e+09, device='cuda:0')
c= tensor(1.5731e+09, device='cuda:0')
c= tensor(1.5734e+09, device='cuda:0')
c= tensor(1.5734e+09, device='cuda:0')
c= tensor(1.5785e+09, device='cuda:0')
c= tensor(1.5803e+09, device='cuda:0')
c= tensor(1.5803e+09, device='cuda:0')
c= tensor(1.5804e+09, device='cuda:0')
c= tensor(1.5806e+09, device='cuda:0')
c= tensor(1.5877e+09, device='cuda:0')
c= tensor(1.6282e+09, device='cuda:0')
c= tensor(1.6330e+09, device='cuda:0')
c= tensor(1.6331e+09, device='cuda:0')
c= tensor(1.6331e+09, device='cuda:0')
c= tensor(1.6331e+09, device='cuda:0')
c= tensor(1.6340e+09, device='cuda:0')
c= tensor(1.6340e+09, device='cuda:0')
c= tensor(1.6342e+09, device='cuda:0')
c= tensor(1.6378e+09, device='cuda:0')
c= tensor(1.6661e+09, device='cuda:0')
c= tensor(1.6662e+09, device='cuda:0')
c= tensor(1.6662e+09, device='cuda:0')
c= tensor(1.6663e+09, device='cuda:0')
c= tensor(1.6676e+09, device='cuda:0')
c= tensor(1.6678e+09, device='cuda:0')
c= tensor(1.6678e+09, device='cuda:0')
c= tensor(1.6678e+09, device='cuda:0')
c= tensor(1.6680e+09, device='cuda:0')
c= tensor(1.6680e+09, device='cuda:0')
c= tensor(1.6689e+09, device='cuda:0')
c= tensor(1.6689e+09, device='cuda:0')
c= tensor(1.6689e+09, device='cuda:0')
c= tensor(1.6690e+09, device='cuda:0')
c= tensor(1.6690e+09, device='cuda:0')
c= tensor(1.6690e+09, device='cuda:0')
c= tensor(1.6731e+09, device='cuda:0')
c= tensor(1.6982e+09, device='cuda:0')
c= tensor(1.7012e+09, device='cuda:0')
c= tensor(1.7043e+09, device='cuda:0')
c= tensor(1.7044e+09, device='cuda:0')
c= tensor(1.7045e+09, device='cuda:0')
c= tensor(1.7045e+09, device='cuda:0')
c= tensor(1.7065e+09, device='cuda:0')
c= tensor(1.7066e+09, device='cuda:0')
c= tensor(1.7068e+09, device='cuda:0')
c= tensor(1.7069e+09, device='cuda:0')
c= tensor(1.8772e+09, device='cuda:0')
c= tensor(1.8773e+09, device='cuda:0')
c= tensor(1.8778e+09, device='cuda:0')
c= tensor(1.8952e+09, device='cuda:0')
c= tensor(1.8956e+09, device='cuda:0')
c= tensor(1.8958e+09, device='cuda:0')
c= tensor(1.9820e+09, device='cuda:0')
c= tensor(1.9862e+09, device='cuda:0')
c= tensor(1.9873e+09, device='cuda:0')
c= tensor(1.9873e+09, device='cuda:0')
c= tensor(1.9875e+09, device='cuda:0')
c= tensor(1.9876e+09, device='cuda:0')
c= tensor(1.9989e+09, device='cuda:0')
c= tensor(2.0973e+09, device='cuda:0')
c= tensor(2.1021e+09, device='cuda:0')
c= tensor(2.1053e+09, device='cuda:0')
c= tensor(2.1074e+09, device='cuda:0')
c= tensor(2.1077e+09, device='cuda:0')
c= tensor(2.1079e+09, device='cuda:0')
c= tensor(2.1079e+09, device='cuda:0')
c= tensor(2.1509e+09, device='cuda:0')
c= tensor(2.1700e+09, device='cuda:0')
c= tensor(2.1712e+09, device='cuda:0')
c= tensor(2.5318e+09, device='cuda:0')
c= tensor(2.5428e+09, device='cuda:0')
c= tensor(2.5432e+09, device='cuda:0')
c= tensor(2.5432e+09, device='cuda:0')
c= tensor(2.5438e+09, device='cuda:0')
c= tensor(2.5456e+09, device='cuda:0')
c= tensor(2.5456e+09, device='cuda:0')
c= tensor(2.5804e+09, device='cuda:0')
c= tensor(2.5808e+09, device='cuda:0')
c= tensor(2.5810e+09, device='cuda:0')
c= tensor(2.5810e+09, device='cuda:0')
c= tensor(2.5810e+09, device='cuda:0')
c= tensor(2.5810e+09, device='cuda:0')
c= tensor(2.5810e+09, device='cuda:0')
c= tensor(2.5812e+09, device='cuda:0')
c= tensor(2.5815e+09, device='cuda:0')
c= tensor(3.9546e+09, device='cuda:0')
c= tensor(3.9553e+09, device='cuda:0')
c= tensor(3.9592e+09, device='cuda:0')
c= tensor(3.9592e+09, device='cuda:0')
c= tensor(3.9593e+09, device='cuda:0')
c= tensor(3.9593e+09, device='cuda:0')
c= tensor(3.9683e+09, device='cuda:0')
c= tensor(3.9699e+09, device='cuda:0')
c= tensor(4.0681e+09, device='cuda:0')
c= tensor(4.0682e+09, device='cuda:0')
c= tensor(4.0725e+09, device='cuda:0')
c= tensor(4.0729e+09, device='cuda:0')
c= tensor(4.0770e+09, device='cuda:0')
c= tensor(4.0864e+09, device='cuda:0')
c= tensor(4.0865e+09, device='cuda:0')
c= tensor(4.0866e+09, device='cuda:0')
c= tensor(4.0880e+09, device='cuda:0')
c= tensor(4.0881e+09, device='cuda:0')
c= tensor(4.0887e+09, device='cuda:0')
c= tensor(4.1042e+09, device='cuda:0')
c= tensor(4.1043e+09, device='cuda:0')
c= tensor(4.1059e+09, device='cuda:0')
c= tensor(4.1061e+09, device='cuda:0')
c= tensor(4.1081e+09, device='cuda:0')
c= tensor(4.1222e+09, device='cuda:0')
c= tensor(4.1224e+09, device='cuda:0')
c= tensor(4.1224e+09, device='cuda:0')
c= tensor(4.1283e+09, device='cuda:0')
c= tensor(4.1294e+09, device='cuda:0')
c= tensor(4.2146e+09, device='cuda:0')
c= tensor(4.2155e+09, device='cuda:0')
c= tensor(4.2159e+09, device='cuda:0')
c= tensor(4.2164e+09, device='cuda:0')
c= tensor(4.2252e+09, device='cuda:0')
c= tensor(4.2336e+09, device='cuda:0')
c= tensor(4.2338e+09, device='cuda:0')
c= tensor(4.2338e+09, device='cuda:0')
c= tensor(4.2338e+09, device='cuda:0')
c= tensor(4.2349e+09, device='cuda:0')
c= tensor(4.2385e+09, device='cuda:0')
c= tensor(4.2392e+09, device='cuda:0')
c= tensor(4.2392e+09, device='cuda:0')
c= tensor(4.2394e+09, device='cuda:0')
c= tensor(4.2398e+09, device='cuda:0')
c= tensor(4.2403e+09, device='cuda:0')
c= tensor(4.2405e+09, device='cuda:0')
c= tensor(4.2411e+09, device='cuda:0')
c= tensor(4.2416e+09, device='cuda:0')
c= tensor(4.2418e+09, device='cuda:0')
c= tensor(4.2418e+09, device='cuda:0')
c= tensor(4.2418e+09, device='cuda:0')
c= tensor(4.2445e+09, device='cuda:0')
c= tensor(4.2447e+09, device='cuda:0')
c= tensor(4.2452e+09, device='cuda:0')
c= tensor(4.2452e+09, device='cuda:0')
c= tensor(4.2452e+09, device='cuda:0')
c= tensor(4.2454e+09, device='cuda:0')
c= tensor(4.2460e+09, device='cuda:0')
c= tensor(4.2461e+09, device='cuda:0')
c= tensor(4.2461e+09, device='cuda:0')
c= tensor(4.2462e+09, device='cuda:0')
c= tensor(4.2463e+09, device='cuda:0')
c= tensor(4.2465e+09, device='cuda:0')
c= tensor(4.2499e+09, device='cuda:0')
c= tensor(4.2499e+09, device='cuda:0')
c= tensor(4.2500e+09, device='cuda:0')
c= tensor(4.2505e+09, device='cuda:0')
c= tensor(4.2505e+09, device='cuda:0')
c= tensor(4.2584e+09, device='cuda:0')
c= tensor(4.2585e+09, device='cuda:0')
c= tensor(4.2606e+09, device='cuda:0')
c= tensor(4.2640e+09, device='cuda:0')
c= tensor(4.2642e+09, device='cuda:0')
c= tensor(4.2675e+09, device='cuda:0')
c= tensor(4.2697e+09, device='cuda:0')
c= tensor(4.2697e+09, device='cuda:0')
c= tensor(4.2700e+09, device='cuda:0')
c= tensor(4.2700e+09, device='cuda:0')
c= tensor(4.2756e+09, device='cuda:0')
c= tensor(4.2776e+09, device='cuda:0')
c= tensor(4.2779e+09, device='cuda:0')
c= tensor(4.2785e+09, device='cuda:0')
c= tensor(4.2789e+09, device='cuda:0')
c= tensor(4.2801e+09, device='cuda:0')
c= tensor(4.2801e+09, device='cuda:0')
c= tensor(4.2802e+09, device='cuda:0')
c= tensor(4.2886e+09, device='cuda:0')
c= tensor(4.3208e+09, device='cuda:0')
c= tensor(4.3210e+09, device='cuda:0')
c= tensor(4.3210e+09, device='cuda:0')
c= tensor(4.3212e+09, device='cuda:0')
c= tensor(4.3247e+09, device='cuda:0')
c= tensor(4.3249e+09, device='cuda:0')
c= tensor(4.3249e+09, device='cuda:0')
c= tensor(4.3253e+09, device='cuda:0')
c= tensor(4.3275e+09, device='cuda:0')
c= tensor(4.3275e+09, device='cuda:0')
c= tensor(4.3278e+09, device='cuda:0')
c= tensor(4.3279e+09, device='cuda:0')
memory (bytes)
4637118464
time for making loss 2 is 13.837815046310425
p0 True
it  0 : 1991524864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 57% |
shape of L is 
torch.Size([])
memory (bytes)
4637384704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% |  9% |
memory (bytes)
4637921280
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  51632402000.0
relative error loss 11.930111
shape of L is 
torch.Size([])
memory (bytes)
4807000064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  4% | 10% |
memory (bytes)
4807004160
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  51632160000.0
relative error loss 11.930055
shape of L is 
torch.Size([])
memory (bytes)
4808871936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
memory (bytes)
4808871936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 10% |
error is  51631694000.0
relative error loss 11.929947
shape of L is 
torch.Size([])
memory (bytes)
4809900032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
4809900032
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  51628510000.0
relative error loss 11.929212
shape of L is 
torch.Size([])
memory (bytes)
4811943936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
memory (bytes)
4811943936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% | 10% |
error is  51610980000.0
relative error loss 11.925161
shape of L is 
torch.Size([])
memory (bytes)
4814045184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
memory (bytes)
4814045184
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  51418685000.0
relative error loss 11.88073
shape of L is 
torch.Size([])
memory (bytes)
4816166912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 10% |
memory (bytes)
4816166912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  50465170000.0
relative error loss 11.660412
shape of L is 
torch.Size([])
memory (bytes)
4818333696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 10% |
memory (bytes)
4818333696
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  45482394000.0
relative error loss 10.509098
shape of L is 
torch.Size([])
memory (bytes)
4820459520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4820459520
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  12840898000.0
relative error loss 2.967
shape of L is 
torch.Size([])
memory (bytes)
4822544384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
memory (bytes)
4822593536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  7493038000.0
relative error loss 1.731331
time to take a step is 222.56712985038757
it  1 : 2404364800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
4824723456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4824723456
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  7493038000.0
relative error loss 1.731331
shape of L is 
torch.Size([])
memory (bytes)
4826836992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
memory (bytes)
4826836992
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  8271888400.0
relative error loss 1.911291
shape of L is 
torch.Size([])
memory (bytes)
4828848128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 10% |
memory (bytes)
4829011968
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  5778517000.0
relative error loss 1.3351761
shape of L is 
torch.Size([])
memory (bytes)
4831145984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
memory (bytes)
4831145984
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  4731422700.0
relative error loss 1.093236
shape of L is 
torch.Size([])
memory (bytes)
4833259520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 10% |
memory (bytes)
4833259520
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  4418185000.0
relative error loss 1.0208597
shape of L is 
torch.Size([])
memory (bytes)
4835385344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 10% |
memory (bytes)
4835385344
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  4187514000.0
relative error loss 0.9675612
shape of L is 
torch.Size([])
memory (bytes)
4837511168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
memory (bytes)
4837511168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  3936418000.0
relative error loss 0.9095433
shape of L is 
torch.Size([])
memory (bytes)
4839612416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 10% |
memory (bytes)
4839612416
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  3609345500.0
relative error loss 0.83397037
shape of L is 
torch.Size([])
memory (bytes)
4841717760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 10% |
memory (bytes)
4841717760
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  3503491800.0
relative error loss 0.80951196
shape of L is 
torch.Size([])
memory (bytes)
4843847680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4843847680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 10% |
error is  3418891300.0
relative error loss 0.78996426
time to take a step is 211.7769958972931
it  2 : 2522319360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
4845977600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 10% |
memory (bytes)
4845977600
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  3418891300.0
relative error loss 0.78996426
shape of L is 
torch.Size([])
memory (bytes)
4848099328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4848099328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  3128381700.0
relative error loss 0.72283953
shape of L is 
torch.Size([])
memory (bytes)
4850216960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% | 10% |
memory (bytes)
4850216960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  2969259000.0
relative error loss 0.6860728
shape of L is 
torch.Size([])
memory (bytes)
4852330496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4852330496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  2823020800.0
relative error loss 0.65228325
shape of L is 
torch.Size([])
memory (bytes)
4854452224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4854452224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  3877905700.0
relative error loss 0.89602345
shape of L is 
torch.Size([])
memory (bytes)
4856586240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 10% |
memory (bytes)
4856586240
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  2539300400.0
relative error loss 0.5867272
shape of L is 
torch.Size([])
memory (bytes)
4858589184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4858699776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  2392420600.0
relative error loss 0.5527894
shape of L is 
torch.Size([])
memory (bytes)
4860821504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4860821504
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  2411430400.0
relative error loss 0.5571818
shape of L is 
torch.Size([])
memory (bytes)
4862967808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
memory (bytes)
4862967808
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  2290275800.0
relative error loss 0.5291879
shape of L is 
torch.Size([])
memory (bytes)
4865130496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 10% |
memory (bytes)
4865130496
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  2164658000.0
relative error loss 0.50016284
time to take a step is 212.26355838775635
it  3 : 2522319360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
4867268608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4867268608
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  2164658000.0
relative error loss 0.50016284
shape of L is 
torch.Size([])
memory (bytes)
4869410816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 10% |
memory (bytes)
4869410816
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  2040897000.0
relative error loss 0.47156683
shape of L is 
torch.Size([])
memory (bytes)
4871565312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 10% |
memory (bytes)
4871565312
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  1985167900.0
relative error loss 0.45869014
shape of L is 
torch.Size([])
memory (bytes)
4873707520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 10% |
memory (bytes)
4873707520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  1847264300.0
relative error loss 0.4268263
shape of L is 
torch.Size([])
memory (bytes)
4875841536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4875841536
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  1821590800.0
relative error loss 0.42089424
shape of L is 
torch.Size([])
memory (bytes)
4877991936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 10% |
memory (bytes)
4877991936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  1567183400.0
relative error loss 0.3621112
shape of L is 
torch.Size([])
memory (bytes)
4880113664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 10% |
memory (bytes)
4880130048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  1389394700.0
relative error loss 0.3210316
shape of L is 
torch.Size([])
memory (bytes)
4882190336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4882190336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  1260724000.0
relative error loss 0.29130113
shape of L is 
torch.Size([])
memory (bytes)
4884418560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 10% |
memory (bytes)
4884418560
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  1101421600.0
relative error loss 0.25449294
shape of L is 
torch.Size([])
memory (bytes)
4886241280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
memory (bytes)
4886433792
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  1037333760.0
relative error loss 0.2396849
time to take a step is 207.76515913009644
c= tensor(1303.6429, device='cuda:0')
c= tensor(111984.0312, device='cuda:0')
c= tensor(114174.1641, device='cuda:0')
c= tensor(216302.9062, device='cuda:0')
c= tensor(425880.5625, device='cuda:0')
c= tensor(801982.0625, device='cuda:0')
c= tensor(1255949.2500, device='cuda:0')
c= tensor(1553457.3750, device='cuda:0')
c= tensor(1946142.8750, device='cuda:0')
c= tensor(5882444.5000, device='cuda:0')
c= tensor(5913413., device='cuda:0')
c= tensor(8275076., device='cuda:0')
c= tensor(8290912.5000, device='cuda:0')
c= tensor(27483778., device='cuda:0')
c= tensor(27668938., device='cuda:0')
c= tensor(27911792., device='cuda:0')
c= tensor(28755616., device='cuda:0')
c= tensor(29004630., device='cuda:0')
c= tensor(33174780., device='cuda:0')
c= tensor(34938620., device='cuda:0')
c= tensor(35384508., device='cuda:0')
c= tensor(41404708., device='cuda:0')
c= tensor(41428484., device='cuda:0')
c= tensor(42088112., device='cuda:0')
c= tensor(42106884., device='cuda:0')
c= tensor(42736236., device='cuda:0')
c= tensor(44027656., device='cuda:0')
c= tensor(44084348., device='cuda:0')
c= tensor(51809840., device='cuda:0')
c= tensor(2.0506e+08, device='cuda:0')
c= tensor(2.0512e+08, device='cuda:0')
c= tensor(3.6097e+08, device='cuda:0')
c= tensor(3.6116e+08, device='cuda:0')
c= tensor(3.6122e+08, device='cuda:0')
c= tensor(3.6134e+08, device='cuda:0')
c= tensor(3.7841e+08, device='cuda:0')
c= tensor(3.8002e+08, device='cuda:0')
c= tensor(3.8002e+08, device='cuda:0')
c= tensor(3.8003e+08, device='cuda:0')
c= tensor(3.8003e+08, device='cuda:0')
c= tensor(3.8003e+08, device='cuda:0')
c= tensor(3.8004e+08, device='cuda:0')
c= tensor(3.8004e+08, device='cuda:0')
c= tensor(3.8004e+08, device='cuda:0')
c= tensor(3.8004e+08, device='cuda:0')
c= tensor(3.8004e+08, device='cuda:0')
c= tensor(3.8005e+08, device='cuda:0')
c= tensor(3.8005e+08, device='cuda:0')
c= tensor(3.8006e+08, device='cuda:0')
c= tensor(3.8010e+08, device='cuda:0')
c= tensor(3.8014e+08, device='cuda:0')
c= tensor(3.8014e+08, device='cuda:0')
c= tensor(3.8015e+08, device='cuda:0')
c= tensor(3.8016e+08, device='cuda:0')
c= tensor(3.8017e+08, device='cuda:0')
c= tensor(3.8018e+08, device='cuda:0')
c= tensor(3.8018e+08, device='cuda:0')
c= tensor(3.8019e+08, device='cuda:0')
c= tensor(3.8019e+08, device='cuda:0')
c= tensor(3.8020e+08, device='cuda:0')
c= tensor(3.8021e+08, device='cuda:0')
c= tensor(3.8021e+08, device='cuda:0')
c= tensor(3.8022e+08, device='cuda:0')
c= tensor(3.8024e+08, device='cuda:0')
c= tensor(3.8025e+08, device='cuda:0')
c= tensor(3.8025e+08, device='cuda:0')
c= tensor(3.8025e+08, device='cuda:0')
c= tensor(3.8026e+08, device='cuda:0')
c= tensor(3.8026e+08, device='cuda:0')
c= tensor(3.8027e+08, device='cuda:0')
c= tensor(3.8029e+08, device='cuda:0')
c= tensor(3.8031e+08, device='cuda:0')
c= tensor(3.8031e+08, device='cuda:0')
c= tensor(3.8032e+08, device='cuda:0')
c= tensor(3.8033e+08, device='cuda:0')
c= tensor(3.8036e+08, device='cuda:0')
c= tensor(3.8036e+08, device='cuda:0')
c= tensor(3.8036e+08, device='cuda:0')
c= tensor(3.8037e+08, device='cuda:0')
c= tensor(3.8040e+08, device='cuda:0')
c= tensor(3.8040e+08, device='cuda:0')
c= tensor(3.8040e+08, device='cuda:0')
c= tensor(3.8041e+08, device='cuda:0')
c= tensor(3.8041e+08, device='cuda:0')
c= tensor(3.8041e+08, device='cuda:0')
c= tensor(3.8041e+08, device='cuda:0')
c= tensor(3.8042e+08, device='cuda:0')
c= tensor(3.8042e+08, device='cuda:0')
c= tensor(3.8042e+08, device='cuda:0')
c= tensor(3.8043e+08, device='cuda:0')
c= tensor(3.8043e+08, device='cuda:0')
c= tensor(3.8044e+08, device='cuda:0')
c= tensor(3.8044e+08, device='cuda:0')
c= tensor(3.8046e+08, device='cuda:0')
c= tensor(3.8046e+08, device='cuda:0')
c= tensor(3.8047e+08, device='cuda:0')
c= tensor(3.8047e+08, device='cuda:0')
c= tensor(3.8049e+08, device='cuda:0')
c= tensor(3.8052e+08, device='cuda:0')
c= tensor(3.8052e+08, device='cuda:0')
c= tensor(3.8055e+08, device='cuda:0')
c= tensor(3.8056e+08, device='cuda:0')
c= tensor(3.8057e+08, device='cuda:0')
c= tensor(3.8057e+08, device='cuda:0')
c= tensor(3.8059e+08, device='cuda:0')
c= tensor(3.8059e+08, device='cuda:0')
c= tensor(3.8059e+08, device='cuda:0')
c= tensor(3.8060e+08, device='cuda:0')
c= tensor(3.8060e+08, device='cuda:0')
c= tensor(3.8060e+08, device='cuda:0')
c= tensor(3.8060e+08, device='cuda:0')
c= tensor(3.8061e+08, device='cuda:0')
c= tensor(3.8061e+08, device='cuda:0')
c= tensor(3.8062e+08, device='cuda:0')
c= tensor(3.8063e+08, device='cuda:0')
c= tensor(3.8063e+08, device='cuda:0')
c= tensor(3.8064e+08, device='cuda:0')
c= tensor(3.8064e+08, device='cuda:0')
c= tensor(3.8065e+08, device='cuda:0')
c= tensor(3.8065e+08, device='cuda:0')
c= tensor(3.8067e+08, device='cuda:0')
c= tensor(3.8067e+08, device='cuda:0')
c= tensor(3.8067e+08, device='cuda:0')
c= tensor(3.8068e+08, device='cuda:0')
c= tensor(3.8068e+08, device='cuda:0')
c= tensor(3.8069e+08, device='cuda:0')
c= tensor(3.8069e+08, device='cuda:0')
c= tensor(3.8069e+08, device='cuda:0')
c= tensor(3.8071e+08, device='cuda:0')
c= tensor(3.8071e+08, device='cuda:0')
c= tensor(3.8073e+08, device='cuda:0')
c= tensor(3.8073e+08, device='cuda:0')
c= tensor(3.8073e+08, device='cuda:0')
c= tensor(3.8074e+08, device='cuda:0')
c= tensor(3.8074e+08, device='cuda:0')
c= tensor(3.8074e+08, device='cuda:0')
c= tensor(3.8074e+08, device='cuda:0')
c= tensor(3.8075e+08, device='cuda:0')
c= tensor(3.8075e+08, device='cuda:0')
c= tensor(3.8075e+08, device='cuda:0')
c= tensor(3.8075e+08, device='cuda:0')
c= tensor(3.8075e+08, device='cuda:0')
c= tensor(3.8077e+08, device='cuda:0')
c= tensor(3.8080e+08, device='cuda:0')
c= tensor(3.8081e+08, device='cuda:0')
c= tensor(3.8081e+08, device='cuda:0')
c= tensor(3.8081e+08, device='cuda:0')
c= tensor(3.8082e+08, device='cuda:0')
c= tensor(3.8082e+08, device='cuda:0')
c= tensor(3.8082e+08, device='cuda:0')
c= tensor(3.8082e+08, device='cuda:0')
c= tensor(3.8083e+08, device='cuda:0')
c= tensor(3.8083e+08, device='cuda:0')
c= tensor(3.8086e+08, device='cuda:0')
c= tensor(3.8087e+08, device='cuda:0')
c= tensor(3.8095e+08, device='cuda:0')
c= tensor(3.8095e+08, device='cuda:0')
c= tensor(3.8096e+08, device='cuda:0')
c= tensor(3.8096e+08, device='cuda:0')
c= tensor(3.8096e+08, device='cuda:0')
c= tensor(3.8101e+08, device='cuda:0')
c= tensor(3.8101e+08, device='cuda:0')
c= tensor(3.8105e+08, device='cuda:0')
c= tensor(3.8105e+08, device='cuda:0')
c= tensor(3.8106e+08, device='cuda:0')
c= tensor(3.8107e+08, device='cuda:0')
c= tensor(3.8107e+08, device='cuda:0')
c= tensor(3.8107e+08, device='cuda:0')
c= tensor(3.8108e+08, device='cuda:0')
c= tensor(3.8108e+08, device='cuda:0')
c= tensor(3.8108e+08, device='cuda:0')
c= tensor(3.8108e+08, device='cuda:0')
c= tensor(3.8109e+08, device='cuda:0')
c= tensor(3.8109e+08, device='cuda:0')
c= tensor(3.8110e+08, device='cuda:0')
c= tensor(3.8110e+08, device='cuda:0')
c= tensor(3.8111e+08, device='cuda:0')
c= tensor(3.8111e+08, device='cuda:0')
c= tensor(3.8112e+08, device='cuda:0')
c= tensor(3.8113e+08, device='cuda:0')
c= tensor(3.8114e+08, device='cuda:0')
c= tensor(3.8114e+08, device='cuda:0')
c= tensor(3.8116e+08, device='cuda:0')
c= tensor(3.8116e+08, device='cuda:0')
c= tensor(3.8116e+08, device='cuda:0')
c= tensor(3.8117e+08, device='cuda:0')
c= tensor(3.8117e+08, device='cuda:0')
c= tensor(3.8118e+08, device='cuda:0')
c= tensor(3.8119e+08, device='cuda:0')
c= tensor(3.8121e+08, device='cuda:0')
c= tensor(3.8122e+08, device='cuda:0')
c= tensor(3.8122e+08, device='cuda:0')
c= tensor(3.8122e+08, device='cuda:0')
c= tensor(3.8122e+08, device='cuda:0')
c= tensor(3.8123e+08, device='cuda:0')
c= tensor(3.8124e+08, device='cuda:0')
c= tensor(3.8124e+08, device='cuda:0')
c= tensor(3.8124e+08, device='cuda:0')
c= tensor(3.8125e+08, device='cuda:0')
c= tensor(3.8125e+08, device='cuda:0')
c= tensor(3.8127e+08, device='cuda:0')
c= tensor(3.8127e+08, device='cuda:0')
c= tensor(3.8130e+08, device='cuda:0')
c= tensor(3.8131e+08, device='cuda:0')
c= tensor(3.8131e+08, device='cuda:0')
c= tensor(3.8132e+08, device='cuda:0')
c= tensor(3.8132e+08, device='cuda:0')
c= tensor(3.8133e+08, device='cuda:0')
c= tensor(3.8133e+08, device='cuda:0')
c= tensor(3.8134e+08, device='cuda:0')
c= tensor(3.8135e+08, device='cuda:0')
c= tensor(3.8136e+08, device='cuda:0')
c= tensor(3.8136e+08, device='cuda:0')
c= tensor(3.8137e+08, device='cuda:0')
c= tensor(3.8137e+08, device='cuda:0')
c= tensor(3.8137e+08, device='cuda:0')
c= tensor(3.8137e+08, device='cuda:0')
c= tensor(3.8137e+08, device='cuda:0')
c= tensor(3.8147e+08, device='cuda:0')
c= tensor(3.8147e+08, device='cuda:0')
c= tensor(3.8148e+08, device='cuda:0')
c= tensor(3.8148e+08, device='cuda:0')
c= tensor(3.8149e+08, device='cuda:0')
c= tensor(3.8150e+08, device='cuda:0')
c= tensor(3.8150e+08, device='cuda:0')
c= tensor(3.8150e+08, device='cuda:0')
c= tensor(3.8150e+08, device='cuda:0')
c= tensor(3.8151e+08, device='cuda:0')
c= tensor(3.8151e+08, device='cuda:0')
c= tensor(3.8151e+08, device='cuda:0')
c= tensor(3.8152e+08, device='cuda:0')
c= tensor(3.8152e+08, device='cuda:0')
c= tensor(3.8152e+08, device='cuda:0')
c= tensor(3.8153e+08, device='cuda:0')
c= tensor(3.8153e+08, device='cuda:0')
c= tensor(3.8153e+08, device='cuda:0')
c= tensor(3.8155e+08, device='cuda:0')
c= tensor(3.8156e+08, device='cuda:0')
c= tensor(3.8156e+08, device='cuda:0')
c= tensor(3.8172e+08, device='cuda:0')
c= tensor(3.8320e+08, device='cuda:0')
c= tensor(3.8322e+08, device='cuda:0')
c= tensor(3.8324e+08, device='cuda:0')
c= tensor(3.8324e+08, device='cuda:0')
c= tensor(3.8325e+08, device='cuda:0')
c= tensor(3.8326e+08, device='cuda:0')
c= tensor(3.8760e+08, device='cuda:0')
c= tensor(3.8761e+08, device='cuda:0')
c= tensor(3.9146e+08, device='cuda:0')
c= tensor(3.9183e+08, device='cuda:0')
c= tensor(3.9211e+08, device='cuda:0')
c= tensor(3.9234e+08, device='cuda:0')
c= tensor(3.9235e+08, device='cuda:0')
c= tensor(3.9235e+08, device='cuda:0')
c= tensor(4.0491e+08, device='cuda:0')
c= tensor(4.2864e+08, device='cuda:0')
c= tensor(4.2865e+08, device='cuda:0')
c= tensor(4.2895e+08, device='cuda:0')
c= tensor(4.2952e+08, device='cuda:0')
c= tensor(4.2971e+08, device='cuda:0')
c= tensor(4.3164e+08, device='cuda:0')
c= tensor(4.3296e+08, device='cuda:0')
c= tensor(4.3320e+08, device='cuda:0')
c= tensor(4.3331e+08, device='cuda:0')
c= tensor(4.3331e+08, device='cuda:0')
c= tensor(4.4172e+08, device='cuda:0')
c= tensor(4.4180e+08, device='cuda:0')
c= tensor(4.4180e+08, device='cuda:0')
c= tensor(4.4186e+08, device='cuda:0')
c= tensor(4.4339e+08, device='cuda:0')
c= tensor(4.5833e+08, device='cuda:0')
c= tensor(4.5935e+08, device='cuda:0')
c= tensor(4.5936e+08, device='cuda:0')
c= tensor(4.5975e+08, device='cuda:0')
c= tensor(4.5977e+08, device='cuda:0')
c= tensor(4.6022e+08, device='cuda:0')
c= tensor(4.6086e+08, device='cuda:0')
c= tensor(4.6186e+08, device='cuda:0')
c= tensor(4.6261e+08, device='cuda:0')
c= tensor(4.6262e+08, device='cuda:0')
c= tensor(4.6263e+08, device='cuda:0')
c= tensor(4.6449e+08, device='cuda:0')
c= tensor(4.6516e+08, device='cuda:0')
c= tensor(4.6631e+08, device='cuda:0')
c= tensor(4.6632e+08, device='cuda:0')
c= tensor(4.8760e+08, device='cuda:0')
c= tensor(4.8764e+08, device='cuda:0')
c= tensor(4.8788e+08, device='cuda:0')
c= tensor(4.8979e+08, device='cuda:0')
c= tensor(4.8980e+08, device='cuda:0')
c= tensor(4.9148e+08, device='cuda:0')
c= tensor(5.0408e+08, device='cuda:0')
c= tensor(6.0013e+08, device='cuda:0')
c= tensor(6.0047e+08, device='cuda:0')
c= tensor(6.0068e+08, device='cuda:0')
c= tensor(6.0070e+08, device='cuda:0')
c= tensor(6.0071e+08, device='cuda:0')
c= tensor(6.0830e+08, device='cuda:0')
c= tensor(6.0832e+08, device='cuda:0')
c= tensor(6.0877e+08, device='cuda:0')
c= tensor(6.1195e+08, device='cuda:0')
c= tensor(6.1217e+08, device='cuda:0')
c= tensor(6.1243e+08, device='cuda:0')
c= tensor(6.1244e+08, device='cuda:0')
c= tensor(6.2084e+08, device='cuda:0')
c= tensor(6.2092e+08, device='cuda:0')
c= tensor(6.2144e+08, device='cuda:0')
c= tensor(6.2148e+08, device='cuda:0')
c= tensor(6.2325e+08, device='cuda:0')
c= tensor(6.2333e+08, device='cuda:0')
c= tensor(6.3289e+08, device='cuda:0')
c= tensor(6.3293e+08, device='cuda:0')
c= tensor(6.3416e+08, device='cuda:0')
c= tensor(6.3423e+08, device='cuda:0')
c= tensor(6.4512e+08, device='cuda:0')
c= tensor(6.4578e+08, device='cuda:0')
c= tensor(6.4582e+08, device='cuda:0')
c= tensor(6.4861e+08, device='cuda:0')
c= tensor(6.5142e+08, device='cuda:0')
c= tensor(6.5143e+08, device='cuda:0')
c= tensor(6.5824e+08, device='cuda:0')
c= tensor(6.6692e+08, device='cuda:0')
c= tensor(6.8762e+08, device='cuda:0')
c= tensor(6.8796e+08, device='cuda:0')
c= tensor(6.8796e+08, device='cuda:0')
c= tensor(6.8798e+08, device='cuda:0')
c= tensor(6.8822e+08, device='cuda:0')
c= tensor(6.8836e+08, device='cuda:0')
c= tensor(6.8909e+08, device='cuda:0')
c= tensor(6.8909e+08, device='cuda:0')
c= tensor(6.8989e+08, device='cuda:0')
c= tensor(6.9193e+08, device='cuda:0')
c= tensor(6.9208e+08, device='cuda:0')
c= tensor(6.9211e+08, device='cuda:0')
c= tensor(6.9308e+08, device='cuda:0')
c= tensor(6.9311e+08, device='cuda:0')
c= tensor(6.9317e+08, device='cuda:0')
c= tensor(6.9322e+08, device='cuda:0')
c= tensor(6.9323e+08, device='cuda:0')
c= tensor(6.9719e+08, device='cuda:0')
c= tensor(6.9753e+08, device='cuda:0')
c= tensor(6.9765e+08, device='cuda:0')
c= tensor(6.9851e+08, device='cuda:0')
c= tensor(6.9855e+08, device='cuda:0')
c= tensor(7.5304e+08, device='cuda:0')
c= tensor(7.5307e+08, device='cuda:0')
c= tensor(7.5610e+08, device='cuda:0')
c= tensor(7.5611e+08, device='cuda:0')
c= tensor(7.5611e+08, device='cuda:0')
c= tensor(7.5612e+08, device='cuda:0')
c= tensor(7.5618e+08, device='cuda:0')
c= tensor(7.5619e+08, device='cuda:0')
c= tensor(7.5751e+08, device='cuda:0')
c= tensor(7.5752e+08, device='cuda:0')
c= tensor(7.5753e+08, device='cuda:0')
c= tensor(7.6559e+08, device='cuda:0')
c= tensor(7.6613e+08, device='cuda:0')
c= tensor(7.6637e+08, device='cuda:0')
c= tensor(7.6861e+08, device='cuda:0')
c= tensor(7.8224e+08, device='cuda:0')
c= tensor(7.8227e+08, device='cuda:0')
c= tensor(7.8230e+08, device='cuda:0')
c= tensor(7.8245e+08, device='cuda:0')
c= tensor(7.8245e+08, device='cuda:0')
c= tensor(7.8246e+08, device='cuda:0')
c= tensor(7.8255e+08, device='cuda:0')
c= tensor(7.8256e+08, device='cuda:0')
c= tensor(7.8256e+08, device='cuda:0')
c= tensor(7.8258e+08, device='cuda:0')
c= tensor(7.8259e+08, device='cuda:0')
c= tensor(7.9785e+08, device='cuda:0')
c= tensor(7.9794e+08, device='cuda:0')
c= tensor(7.9970e+08, device='cuda:0')
c= tensor(7.9977e+08, device='cuda:0')
c= tensor(7.9977e+08, device='cuda:0')
c= tensor(8.0090e+08, device='cuda:0')
c= tensor(8.3919e+08, device='cuda:0')
c= tensor(8.5920e+08, device='cuda:0')
c= tensor(8.5927e+08, device='cuda:0')
c= tensor(8.5958e+08, device='cuda:0')
c= tensor(8.5958e+08, device='cuda:0')
c= tensor(8.5961e+08, device='cuda:0')
c= tensor(1.0451e+09, device='cuda:0')
c= tensor(1.0452e+09, device='cuda:0')
c= tensor(1.0452e+09, device='cuda:0')
c= tensor(1.0459e+09, device='cuda:0')
c= tensor(1.1131e+09, device='cuda:0')
c= tensor(1.1134e+09, device='cuda:0')
c= tensor(1.1134e+09, device='cuda:0')
c= tensor(1.1134e+09, device='cuda:0')
c= tensor(1.1136e+09, device='cuda:0')
c= tensor(1.1136e+09, device='cuda:0')
c= tensor(1.1196e+09, device='cuda:0')
c= tensor(1.1197e+09, device='cuda:0')
c= tensor(1.1197e+09, device='cuda:0')
c= tensor(1.1200e+09, device='cuda:0')
c= tensor(1.1201e+09, device='cuda:0')
c= tensor(1.1201e+09, device='cuda:0')
c= tensor(1.1213e+09, device='cuda:0')
c= tensor(1.1233e+09, device='cuda:0')
c= tensor(1.1377e+09, device='cuda:0')
c= tensor(1.1457e+09, device='cuda:0')
c= tensor(1.1532e+09, device='cuda:0')
c= tensor(1.1533e+09, device='cuda:0')
c= tensor(1.1534e+09, device='cuda:0')
c= tensor(1.1543e+09, device='cuda:0')
c= tensor(1.1578e+09, device='cuda:0')
c= tensor(1.1578e+09, device='cuda:0')
c= tensor(1.1652e+09, device='cuda:0')
c= tensor(1.1712e+09, device='cuda:0')
c= tensor(1.1755e+09, device='cuda:0')
c= tensor(1.1770e+09, device='cuda:0')
c= tensor(1.1797e+09, device='cuda:0')
c= tensor(1.1798e+09, device='cuda:0')
c= tensor(1.1798e+09, device='cuda:0')
c= tensor(1.1799e+09, device='cuda:0')
c= tensor(1.1815e+09, device='cuda:0')
c= tensor(1.1849e+09, device='cuda:0')
c= tensor(1.2013e+09, device='cuda:0')
c= tensor(1.2042e+09, device='cuda:0')
c= tensor(1.2051e+09, device='cuda:0')
c= tensor(1.2054e+09, device='cuda:0')
c= tensor(1.2139e+09, device='cuda:0')
c= tensor(1.2139e+09, device='cuda:0')
c= tensor(1.2140e+09, device='cuda:0')
c= tensor(1.2185e+09, device='cuda:0')
c= tensor(1.2191e+09, device='cuda:0')
c= tensor(1.2191e+09, device='cuda:0')
c= tensor(1.2192e+09, device='cuda:0')
c= tensor(1.2634e+09, device='cuda:0')
c= tensor(1.2635e+09, device='cuda:0')
c= tensor(1.2645e+09, device='cuda:0')
c= tensor(1.2645e+09, device='cuda:0')
c= tensor(1.2645e+09, device='cuda:0')
c= tensor(1.2645e+09, device='cuda:0')
c= tensor(1.2646e+09, device='cuda:0')
c= tensor(1.2647e+09, device='cuda:0')
c= tensor(1.2662e+09, device='cuda:0')
c= tensor(1.2662e+09, device='cuda:0')
c= tensor(1.2687e+09, device='cuda:0')
c= tensor(1.2687e+09, device='cuda:0')
c= tensor(1.2693e+09, device='cuda:0')
c= tensor(1.2694e+09, device='cuda:0')
c= tensor(1.2707e+09, device='cuda:0')
c= tensor(1.2707e+09, device='cuda:0')
c= tensor(1.2708e+09, device='cuda:0')
c= tensor(1.2710e+09, device='cuda:0')
c= tensor(1.2712e+09, device='cuda:0')
c= tensor(1.2724e+09, device='cuda:0')
c= tensor(1.3136e+09, device='cuda:0')
c= tensor(1.3136e+09, device='cuda:0')
c= tensor(1.3136e+09, device='cuda:0')
c= tensor(1.3146e+09, device='cuda:0')
c= tensor(1.3147e+09, device='cuda:0')
c= tensor(1.3456e+09, device='cuda:0')
c= tensor(1.3456e+09, device='cuda:0')
c= tensor(1.3485e+09, device='cuda:0')
c= tensor(1.3573e+09, device='cuda:0')
c= tensor(1.3573e+09, device='cuda:0')
c= tensor(1.3656e+09, device='cuda:0')
c= tensor(1.3660e+09, device='cuda:0')
c= tensor(1.4114e+09, device='cuda:0')
c= tensor(1.4114e+09, device='cuda:0')
c= tensor(1.4114e+09, device='cuda:0')
c= tensor(1.4115e+09, device='cuda:0')
c= tensor(1.4115e+09, device='cuda:0')
c= tensor(1.4115e+09, device='cuda:0')
c= tensor(1.4120e+09, device='cuda:0')
c= tensor(1.4122e+09, device='cuda:0')
c= tensor(1.4150e+09, device='cuda:0')
c= tensor(1.4150e+09, device='cuda:0')
c= tensor(1.4150e+09, device='cuda:0')
c= tensor(1.4151e+09, device='cuda:0')
c= tensor(1.4219e+09, device='cuda:0')
c= tensor(1.4240e+09, device='cuda:0')
c= tensor(1.4353e+09, device='cuda:0')
c= tensor(1.4354e+09, device='cuda:0')
c= tensor(1.4354e+09, device='cuda:0')
c= tensor(1.4354e+09, device='cuda:0')
c= tensor(1.4354e+09, device='cuda:0')
c= tensor(1.4492e+09, device='cuda:0')
c= tensor(1.4492e+09, device='cuda:0')
c= tensor(1.4493e+09, device='cuda:0')
c= tensor(1.4504e+09, device='cuda:0')
c= tensor(1.4509e+09, device='cuda:0')
c= tensor(1.4509e+09, device='cuda:0')
c= tensor(1.4509e+09, device='cuda:0')
c= tensor(1.4628e+09, device='cuda:0')
c= tensor(1.4631e+09, device='cuda:0')
c= tensor(1.4631e+09, device='cuda:0')
c= tensor(1.4632e+09, device='cuda:0')
c= tensor(1.4717e+09, device='cuda:0')
c= tensor(1.4726e+09, device='cuda:0')
c= tensor(1.4892e+09, device='cuda:0')
c= tensor(1.4902e+09, device='cuda:0')
c= tensor(1.4902e+09, device='cuda:0')
c= tensor(1.4905e+09, device='cuda:0')
c= tensor(1.4911e+09, device='cuda:0')
c= tensor(1.4911e+09, device='cuda:0')
c= tensor(1.4912e+09, device='cuda:0')
c= tensor(1.4912e+09, device='cuda:0')
c= tensor(1.4920e+09, device='cuda:0')
c= tensor(1.4922e+09, device='cuda:0')
c= tensor(1.4922e+09, device='cuda:0')
c= tensor(1.4923e+09, device='cuda:0')
c= tensor(1.4923e+09, device='cuda:0')
c= tensor(1.4930e+09, device='cuda:0')
c= tensor(1.4931e+09, device='cuda:0')
c= tensor(1.4932e+09, device='cuda:0')
c= tensor(1.4932e+09, device='cuda:0')
c= tensor(1.4933e+09, device='cuda:0')
c= tensor(1.4933e+09, device='cuda:0')
c= tensor(1.4933e+09, device='cuda:0')
c= tensor(1.4934e+09, device='cuda:0')
c= tensor(1.4972e+09, device='cuda:0')
c= tensor(1.4972e+09, device='cuda:0')
c= tensor(1.4972e+09, device='cuda:0')
c= tensor(1.4973e+09, device='cuda:0')
c= tensor(1.4996e+09, device='cuda:0')
c= tensor(1.5731e+09, device='cuda:0')
c= tensor(1.5734e+09, device='cuda:0')
c= tensor(1.5734e+09, device='cuda:0')
c= tensor(1.5785e+09, device='cuda:0')
c= tensor(1.5803e+09, device='cuda:0')
c= tensor(1.5803e+09, device='cuda:0')
c= tensor(1.5804e+09, device='cuda:0')
c= tensor(1.5806e+09, device='cuda:0')
c= tensor(1.5877e+09, device='cuda:0')
c= tensor(1.6282e+09, device='cuda:0')
c= tensor(1.6330e+09, device='cuda:0')
c= tensor(1.6331e+09, device='cuda:0')
c= tensor(1.6331e+09, device='cuda:0')
c= tensor(1.6331e+09, device='cuda:0')
c= tensor(1.6340e+09, device='cuda:0')
c= tensor(1.6340e+09, device='cuda:0')
c= tensor(1.6342e+09, device='cuda:0')
c= tensor(1.6378e+09, device='cuda:0')
c= tensor(1.6661e+09, device='cuda:0')
c= tensor(1.6662e+09, device='cuda:0')
c= tensor(1.6662e+09, device='cuda:0')
c= tensor(1.6663e+09, device='cuda:0')
c= tensor(1.6676e+09, device='cuda:0')
c= tensor(1.6678e+09, device='cuda:0')
c= tensor(1.6678e+09, device='cuda:0')
c= tensor(1.6678e+09, device='cuda:0')
c= tensor(1.6680e+09, device='cuda:0')
c= tensor(1.6680e+09, device='cuda:0')
c= tensor(1.6689e+09, device='cuda:0')
c= tensor(1.6689e+09, device='cuda:0')
c= tensor(1.6689e+09, device='cuda:0')
c= tensor(1.6690e+09, device='cuda:0')
c= tensor(1.6690e+09, device='cuda:0')
c= tensor(1.6690e+09, device='cuda:0')
c= tensor(1.6731e+09, device='cuda:0')
c= tensor(1.6982e+09, device='cuda:0')
c= tensor(1.7012e+09, device='cuda:0')
c= tensor(1.7043e+09, device='cuda:0')
c= tensor(1.7044e+09, device='cuda:0')
c= tensor(1.7045e+09, device='cuda:0')
c= tensor(1.7045e+09, device='cuda:0')
c= tensor(1.7065e+09, device='cuda:0')
c= tensor(1.7066e+09, device='cuda:0')
c= tensor(1.7068e+09, device='cuda:0')
c= tensor(1.7069e+09, device='cuda:0')
c= tensor(1.8772e+09, device='cuda:0')
c= tensor(1.8773e+09, device='cuda:0')
c= tensor(1.8778e+09, device='cuda:0')
c= tensor(1.8952e+09, device='cuda:0')
c= tensor(1.8956e+09, device='cuda:0')
c= tensor(1.8958e+09, device='cuda:0')
c= tensor(1.9820e+09, device='cuda:0')
c= tensor(1.9862e+09, device='cuda:0')
c= tensor(1.9873e+09, device='cuda:0')
c= tensor(1.9873e+09, device='cuda:0')
c= tensor(1.9875e+09, device='cuda:0')
c= tensor(1.9876e+09, device='cuda:0')
c= tensor(1.9989e+09, device='cuda:0')
c= tensor(2.0973e+09, device='cuda:0')
c= tensor(2.1021e+09, device='cuda:0')
c= tensor(2.1053e+09, device='cuda:0')
c= tensor(2.1074e+09, device='cuda:0')
c= tensor(2.1077e+09, device='cuda:0')
c= tensor(2.1079e+09, device='cuda:0')
c= tensor(2.1079e+09, device='cuda:0')
c= tensor(2.1509e+09, device='cuda:0')
c= tensor(2.1700e+09, device='cuda:0')
c= tensor(2.1712e+09, device='cuda:0')
c= tensor(2.5318e+09, device='cuda:0')
c= tensor(2.5428e+09, device='cuda:0')
c= tensor(2.5432e+09, device='cuda:0')
c= tensor(2.5432e+09, device='cuda:0')
c= tensor(2.5438e+09, device='cuda:0')
c= tensor(2.5456e+09, device='cuda:0')
c= tensor(2.5456e+09, device='cuda:0')
c= tensor(2.5804e+09, device='cuda:0')
c= tensor(2.5808e+09, device='cuda:0')
c= tensor(2.5810e+09, device='cuda:0')
c= tensor(2.5810e+09, device='cuda:0')
c= tensor(2.5810e+09, device='cuda:0')
c= tensor(2.5810e+09, device='cuda:0')
c= tensor(2.5810e+09, device='cuda:0')
c= tensor(2.5812e+09, device='cuda:0')
c= tensor(2.5815e+09, device='cuda:0')
c= tensor(3.9546e+09, device='cuda:0')
c= tensor(3.9553e+09, device='cuda:0')
c= tensor(3.9592e+09, device='cuda:0')
c= tensor(3.9592e+09, device='cuda:0')
c= tensor(3.9593e+09, device='cuda:0')
c= tensor(3.9593e+09, device='cuda:0')
c= tensor(3.9683e+09, device='cuda:0')
c= tensor(3.9699e+09, device='cuda:0')
c= tensor(4.0681e+09, device='cuda:0')
c= tensor(4.0682e+09, device='cuda:0')
c= tensor(4.0725e+09, device='cuda:0')
c= tensor(4.0729e+09, device='cuda:0')
c= tensor(4.0770e+09, device='cuda:0')
c= tensor(4.0864e+09, device='cuda:0')
c= tensor(4.0865e+09, device='cuda:0')
c= tensor(4.0866e+09, device='cuda:0')
c= tensor(4.0880e+09, device='cuda:0')
c= tensor(4.0881e+09, device='cuda:0')
c= tensor(4.0887e+09, device='cuda:0')
c= tensor(4.1042e+09, device='cuda:0')
c= tensor(4.1043e+09, device='cuda:0')
c= tensor(4.1059e+09, device='cuda:0')
c= tensor(4.1061e+09, device='cuda:0')
c= tensor(4.1081e+09, device='cuda:0')
c= tensor(4.1222e+09, device='cuda:0')
c= tensor(4.1224e+09, device='cuda:0')
c= tensor(4.1224e+09, device='cuda:0')
c= tensor(4.1283e+09, device='cuda:0')
c= tensor(4.1294e+09, device='cuda:0')
c= tensor(4.2146e+09, device='cuda:0')
c= tensor(4.2155e+09, device='cuda:0')
c= tensor(4.2159e+09, device='cuda:0')
c= tensor(4.2164e+09, device='cuda:0')
c= tensor(4.2252e+09, device='cuda:0')
c= tensor(4.2336e+09, device='cuda:0')
c= tensor(4.2338e+09, device='cuda:0')
c= tensor(4.2338e+09, device='cuda:0')
c= tensor(4.2338e+09, device='cuda:0')
c= tensor(4.2349e+09, device='cuda:0')
c= tensor(4.2385e+09, device='cuda:0')
c= tensor(4.2392e+09, device='cuda:0')
c= tensor(4.2392e+09, device='cuda:0')
c= tensor(4.2394e+09, device='cuda:0')
c= tensor(4.2398e+09, device='cuda:0')
c= tensor(4.2403e+09, device='cuda:0')
c= tensor(4.2405e+09, device='cuda:0')
c= tensor(4.2411e+09, device='cuda:0')
c= tensor(4.2416e+09, device='cuda:0')
c= tensor(4.2418e+09, device='cuda:0')
c= tensor(4.2418e+09, device='cuda:0')
c= tensor(4.2418e+09, device='cuda:0')
c= tensor(4.2445e+09, device='cuda:0')
c= tensor(4.2447e+09, device='cuda:0')
c= tensor(4.2452e+09, device='cuda:0')
c= tensor(4.2452e+09, device='cuda:0')
c= tensor(4.2452e+09, device='cuda:0')
c= tensor(4.2454e+09, device='cuda:0')
c= tensor(4.2460e+09, device='cuda:0')
c= tensor(4.2461e+09, device='cuda:0')
c= tensor(4.2461e+09, device='cuda:0')
c= tensor(4.2462e+09, device='cuda:0')
c= tensor(4.2463e+09, device='cuda:0')
c= tensor(4.2465e+09, device='cuda:0')
c= tensor(4.2499e+09, device='cuda:0')
c= tensor(4.2499e+09, device='cuda:0')
c= tensor(4.2500e+09, device='cuda:0')
c= tensor(4.2505e+09, device='cuda:0')
c= tensor(4.2505e+09, device='cuda:0')
c= tensor(4.2584e+09, device='cuda:0')
c= tensor(4.2585e+09, device='cuda:0')
c= tensor(4.2606e+09, device='cuda:0')
c= tensor(4.2640e+09, device='cuda:0')
c= tensor(4.2642e+09, device='cuda:0')
c= tensor(4.2675e+09, device='cuda:0')
c= tensor(4.2697e+09, device='cuda:0')
c= tensor(4.2697e+09, device='cuda:0')
c= tensor(4.2700e+09, device='cuda:0')
c= tensor(4.2700e+09, device='cuda:0')
c= tensor(4.2756e+09, device='cuda:0')
c= tensor(4.2776e+09, device='cuda:0')
c= tensor(4.2779e+09, device='cuda:0')
c= tensor(4.2785e+09, device='cuda:0')
c= tensor(4.2789e+09, device='cuda:0')
c= tensor(4.2801e+09, device='cuda:0')
c= tensor(4.2801e+09, device='cuda:0')
c= tensor(4.2802e+09, device='cuda:0')
c= tensor(4.2886e+09, device='cuda:0')
c= tensor(4.3208e+09, device='cuda:0')
c= tensor(4.3210e+09, device='cuda:0')
c= tensor(4.3210e+09, device='cuda:0')
c= tensor(4.3212e+09, device='cuda:0')
c= tensor(4.3247e+09, device='cuda:0')
c= tensor(4.3249e+09, device='cuda:0')
c= tensor(4.3249e+09, device='cuda:0')
c= tensor(4.3253e+09, device='cuda:0')
c= tensor(4.3275e+09, device='cuda:0')
c= tensor(4.3275e+09, device='cuda:0')
c= tensor(4.3278e+09, device='cuda:0')
c= tensor(4.3279e+09, device='cuda:0')
time to make c is 10.931968927383423
time for making loss is 10.932116985321045
p0 True
it  0 : 1991777280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
4888690688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
4888936448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  1037333760.0
relative error loss 0.2396849
shape of L is 
torch.Size([])
memory (bytes)
4915539968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
memory (bytes)
4915539968
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  1033386500.0
relative error loss 0.23877284
shape of L is 
torch.Size([])
memory (bytes)
4919160832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 10% |
memory (bytes)
4919160832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  1006313500.0
relative error loss 0.23251739
shape of L is 
torch.Size([])
memory (bytes)
4922392576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
4922392576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  978438140.0
relative error loss 0.22607656
shape of L is 
torch.Size([])
memory (bytes)
4925599744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 10% |
memory (bytes)
4925599744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  969632000.0
relative error loss 0.22404182
shape of L is 
torch.Size([])
memory (bytes)
4928811008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4928811008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  962127100.0
relative error loss 0.22230774
shape of L is 
torch.Size([])
memory (bytes)
4932034560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 10% |
memory (bytes)
4932034560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  958978050.0
relative error loss 0.22158013
shape of L is 
torch.Size([])
memory (bytes)
4935233536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 10% |
memory (bytes)
4935233536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  954560500.0
relative error loss 0.22055942
shape of L is 
torch.Size([])
memory (bytes)
4938440704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 10% |
memory (bytes)
4938440704
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  951515140.0
relative error loss 0.21985576
shape of L is 
torch.Size([])
memory (bytes)
4941660160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
4941660160
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  949014000.0
relative error loss 0.21927786
time to take a step is 280.6784167289734
it  1 : 2524838912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
4944871424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 10% |
memory (bytes)
4944871424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  949014000.0
relative error loss 0.21927786
shape of L is 
torch.Size([])
memory (bytes)
4948008960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 10% |
memory (bytes)
4948082688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  946789600.0
relative error loss 0.21876389
shape of L is 
torch.Size([])
memory (bytes)
4951314432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 10% |
memory (bytes)
4951314432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  944652540.0
relative error loss 0.2182701
shape of L is 
torch.Size([])
memory (bytes)
4954439680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 10% |
memory (bytes)
4954550272
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  942918900.0
relative error loss 0.21786954
shape of L is 
torch.Size([])
memory (bytes)
4957761536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4957761536
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  941796350.0
relative error loss 0.21761015
shape of L is 
torch.Size([])
memory (bytes)
4960739328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
4960972800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  940710660.0
relative error loss 0.21735929
shape of L is 
torch.Size([])
memory (bytes)
4964184064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 10% |
memory (bytes)
4964184064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  939740900.0
relative error loss 0.21713524
shape of L is 
torch.Size([])
memory (bytes)
4967276544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 10% |
memory (bytes)
4967276544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  938851300.0
relative error loss 0.21692967
shape of L is 
torch.Size([])
memory (bytes)
4970610688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 10% |
memory (bytes)
4970610688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  938299650.0
relative error loss 0.21680221
shape of L is 
torch.Size([])
memory (bytes)
4973826048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4973826048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 10% |
error is  937685500.0
relative error loss 0.2166603
time to take a step is 270.71607637405396
it  2 : 2524838912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
4977033216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 10% |
memory (bytes)
4977033216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  937685500.0
relative error loss 0.2166603
shape of L is 
torch.Size([])
memory (bytes)
4980240384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 10% |
memory (bytes)
4980240384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  937037300.0
relative error loss 0.21651053
shape of L is 
torch.Size([])
memory (bytes)
4983451648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
4983451648
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  936507650.0
relative error loss 0.21638815
shape of L is 
torch.Size([])
memory (bytes)
4986675200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 10% |
memory (bytes)
4986675200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  935908100.0
relative error loss 0.21624961
shape of L is 
torch.Size([])
memory (bytes)
4989878272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 10% |
memory (bytes)
4989878272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  935471900.0
relative error loss 0.21614882
shape of L is 
torch.Size([])
memory (bytes)
4993089536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 10% |
memory (bytes)
4993089536
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  935000300.0
relative error loss 0.21603987
shape of L is 
torch.Size([])
memory (bytes)
4996300800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 10% |
memory (bytes)
4996300800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  934501400.0
relative error loss 0.21592459
shape of L is 
torch.Size([])
memory (bytes)
4999368704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
4999516160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  934148860.0
relative error loss 0.21584314
shape of L is 
torch.Size([])
memory (bytes)
5002539008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
5002735616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  933732600.0
relative error loss 0.21574695
shape of L is 
torch.Size([])
memory (bytes)
5005950976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 10% |
memory (bytes)
5005950976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  933398000.0
relative error loss 0.21566965
time to take a step is 283.0998513698578
it  3 : 2524838912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
5009158144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
5009158144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  933398000.0
relative error loss 0.21566965
shape of L is 
torch.Size([])
memory (bytes)
5012369408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
5012369408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 10% |
error is  933087200.0
relative error loss 0.21559784
shape of L is 
torch.Size([])
memory (bytes)
5015592960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
5015592960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 10% |
error is  932666900.0
relative error loss 0.21550071
shape of L is 
torch.Size([])
memory (bytes)
5018636288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
5018804224
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  932282600.0
relative error loss 0.21541193
shape of L is 
torch.Size([])
memory (bytes)
5022019584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 10% |
memory (bytes)
5022019584
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  931969300.0
relative error loss 0.21533953
shape of L is 
torch.Size([])
memory (bytes)
5025161216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
5025161216
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  931709440.0
relative error loss 0.21527949
shape of L is 
torch.Size([])
memory (bytes)
5028454400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 10% |
memory (bytes)
5028454400
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  931415040.0
relative error loss 0.21521147
shape of L is 
torch.Size([])
memory (bytes)
5031682048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 10% |
memory (bytes)
5031682048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  931196900.0
relative error loss 0.21516107
shape of L is 
torch.Size([])
memory (bytes)
5034897408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 10% |
memory (bytes)
5034897408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  930965760.0
relative error loss 0.21510765
shape of L is 
torch.Size([])
memory (bytes)
5038120960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 10% |
memory (bytes)
5038120960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  930799100.0
relative error loss 0.21506914
time to take a step is 276.7207291126251
it  4 : 2524838912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
5041324032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
5041324032
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  930799100.0
relative error loss 0.21506914
shape of L is 
torch.Size([])
memory (bytes)
5044551680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
5044551680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  930617860.0
relative error loss 0.21502726
shape of L is 
torch.Size([])
memory (bytes)
5047758848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
5047758848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  930485760.0
relative error loss 0.21499674
shape of L is 
torch.Size([])
memory (bytes)
5050986496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
5050986496
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  930358500.0
relative error loss 0.21496734
shape of L is 
torch.Size([])
memory (bytes)
5054197760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
5054197760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  930120960.0
relative error loss 0.21491246
shape of L is 
torch.Size([])
memory (bytes)
5057413120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
5057413120
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  930140160.0
relative error loss 0.21491688
shape of L is 
torch.Size([])
memory (bytes)
5060562944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 10% |
memory (bytes)
5060624384
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  930019840.0
relative error loss 0.2148891
shape of L is 
torch.Size([])
memory (bytes)
5063847936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
5063847936
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  929901800.0
relative error loss 0.21486183
shape of L is 
torch.Size([])
memory (bytes)
5067055104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
5067055104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 10% |
error is  929681400.0
relative error loss 0.2148109
shape of L is 
torch.Size([])
memory (bytes)
5070274560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
5070274560
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  929538560.0
relative error loss 0.21477789
time to take a step is 272.80681681632996
it  5 : 2524838912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
5073489920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
5073489920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  929538560.0
relative error loss 0.21477789
shape of L is 
torch.Size([])
memory (bytes)
5076684800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
5076684800
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  929366800.0
relative error loss 0.21473819
shape of L is 
torch.Size([])
memory (bytes)
5079904256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
5079904256
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  929194240.0
relative error loss 0.21469833
shape of L is 
torch.Size([])
memory (bytes)
5083123712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
5083123712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  929050100.0
relative error loss 0.21466503
shape of L is 
torch.Size([])
memory (bytes)
5086339072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
5086339072
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  928968450.0
relative error loss 0.21464616
shape of L is 
torch.Size([])
memory (bytes)
5089550336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
5089550336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  928740600.0
relative error loss 0.21459351
shape of L is 
torch.Size([])
memory (bytes)
5092765696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 10% |
memory (bytes)
5092765696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  928676100.0
relative error loss 0.2145786
shape of L is 
torch.Size([])
memory (bytes)
5095981056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
5095981056
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  928552700.0
relative error loss 0.2145501
shape of L is 
torch.Size([])
memory (bytes)
5099196416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
5099196416
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  928415200.0
relative error loss 0.21451832
shape of L is 
torch.Size([])
memory (bytes)
5102268416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 10% |
memory (bytes)
5102419968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  928295940.0
relative error loss 0.21449077
time to take a step is 275.3489978313446
it  6 : 2524838912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
5105635328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
5105635328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  928295940.0
relative error loss 0.21449077
shape of L is 
torch.Size([])
memory (bytes)
5108838400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 10% |
memory (bytes)
5108838400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  928216300.0
relative error loss 0.21447237
shape of L is 
torch.Size([])
memory (bytes)
5112057856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 10% |
memory (bytes)
5112057856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 10% |
error is  928049660.0
relative error loss 0.21443386
shape of L is 
torch.Size([])
memory (bytes)
5115265024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 10% |
memory (bytes)
5115265024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  927947800.0
relative error loss 0.21441032
shape of L is 
torch.Size([])
memory (bytes)
5118480384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 10% |
memory (bytes)
5118480384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  927823360.0
relative error loss 0.21438158
shape of L is 
torch.Size([])
memory (bytes)
5121662976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 10% |
memory (bytes)
5121703936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  927714560.0
relative error loss 0.21435644
shape of L is 
torch.Size([])
memory (bytes)
5124919296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
5124919296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  927649540.0
relative error loss 0.2143414
shape of L is 
torch.Size([])
memory (bytes)
5128130560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 10% |
memory (bytes)
5128130560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  927498000.0
relative error loss 0.2143064
shape of L is 
torch.Size([])
memory (bytes)
5131337728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 10% |
memory (bytes)
5131337728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  927405060.0
relative error loss 0.21428493
shape of L is 
torch.Size([])
memory (bytes)
5134565376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
5134565376
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  927336700.0
relative error loss 0.21426913
time to take a step is 282.89359283447266
it  7 : 2524838912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
5137772544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 10% |
memory (bytes)
5137772544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  927336700.0
relative error loss 0.21426913
shape of L is 
torch.Size([])
memory (bytes)
5140918272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 10% |
memory (bytes)
5141004288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  927268350.0
relative error loss 0.21425334
shape of L is 
torch.Size([])
memory (bytes)
5144219648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 10% |
memory (bytes)
5144219648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  927181300.0
relative error loss 0.21423322
shape of L is 
torch.Size([])
memory (bytes)
5147430912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
5147430912
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  927104800.0
relative error loss 0.21421553
shape of L is 
torch.Size([])
memory (bytes)
5150642176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
5150642176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  927018000.0
relative error loss 0.21419549
shape of L is 
torch.Size([])
memory (bytes)
5153857536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 10% |
memory (bytes)
5153857536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  926953500.0
relative error loss 0.21418057
shape of L is 
torch.Size([])
memory (bytes)
5157072896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 10% |
memory (bytes)
5157072896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  926879000.0
relative error loss 0.21416336
shape of L is 
torch.Size([])
memory (bytes)
5160263680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
5160263680
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  926864400.0
relative error loss 0.21416
shape of L is 
torch.Size([])
memory (bytes)
5163466752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
5163499520
| ID | GPU | MEM |
------------------
|  0 |  9% |  0% |
|  1 | 93% | 10% |
error is  926735600.0
relative error loss 0.21413024
shape of L is 
torch.Size([])
memory (bytes)
5166706688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
5166706688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 10% |
error is  926693400.0
relative error loss 0.21412048
time to take a step is 276.61374497413635
it  8 : 2524838912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
5169922048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
memory (bytes)
5169922048
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  926693400.0
relative error loss 0.21412048
shape of L is 
torch.Size([])
memory (bytes)
5173145600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 10% |
memory (bytes)
5173145600
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  926636000.0
relative error loss 0.21410723
shape of L is 
torch.Size([])
memory (bytes)
5176360960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 10% |
memory (bytes)
5176360960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  926555900.0
relative error loss 0.21408872
shape of L is 
torch.Size([])
memory (bytes)
5179568128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
5179568128
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  926461200.0
relative error loss 0.21406683
shape of L is 
torch.Size([])
memory (bytes)
5182771200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
5182771200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  926374660.0
relative error loss 0.21404684
shape of L is 
torch.Size([])
memory (bytes)
5185990656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
5185990656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  926306560.0
relative error loss 0.2140311
shape of L is 
torch.Size([])
memory (bytes)
5189038080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
5189201920
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  926221800.0
relative error loss 0.21401152
shape of L is 
torch.Size([])
memory (bytes)
5192417280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
5192417280
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  926161660.0
relative error loss 0.21399762
shape of L is 
torch.Size([])
memory (bytes)
5195624448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
5195624448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  926090500.0
relative error loss 0.21398118
shape of L is 
torch.Size([])
memory (bytes)
5198839808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 10% |
memory (bytes)
5198839808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  926025200.0
relative error loss 0.2139661
time to take a step is 279.3983647823334
it  9 : 2524838912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
5202059264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
5202059264
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  926025200.0
relative error loss 0.2139661
shape of L is 
torch.Size([])
memory (bytes)
5205266432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 10% |
memory (bytes)
5205266432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  925995800.0
relative error loss 0.21395929
shape of L is 
torch.Size([])
memory (bytes)
5208363008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 10% |
memory (bytes)
5208363008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  925966100.0
relative error loss 0.21395244
shape of L is 
torch.Size([])
memory (bytes)
5211693056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 10% |
memory (bytes)
5211693056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  925901800.0
relative error loss 0.21393758
shape of L is 
torch.Size([])
memory (bytes)
5214912512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 10% |
memory (bytes)
5214912512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% | 10% |
error is  925860350.0
relative error loss 0.213928
shape of L is 
torch.Size([])
memory (bytes)
5218111488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
5218111488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  925800960.0
relative error loss 0.21391428
shape of L is 
torch.Size([])
memory (bytes)
5221339136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
5221339136
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  925702140.0
relative error loss 0.21389145
shape of L is 
torch.Size([])
memory (bytes)
5224550400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
5224550400
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  925676300.0
relative error loss 0.21388547
shape of L is 
torch.Size([])
memory (bytes)
5227761664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 10% |
memory (bytes)
5227761664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  925623800.0
relative error loss 0.21387334
shape of L is 
torch.Size([])
memory (bytes)
5230964736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 10% |
memory (bytes)
5230964736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  925583100.0
relative error loss 0.21386394
time to take a step is 283.6417543888092
it  10 : 2524838912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
5234180096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 10% |
memory (bytes)
5234180096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 10% |
error is  925583100.0
relative error loss 0.21386394
shape of L is 
torch.Size([])
memory (bytes)
5237387264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 10% |
memory (bytes)
5237387264
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  925541600.0
relative error loss 0.21385436
shape of L is 
torch.Size([])
memory (bytes)
5240602624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 10% |
memory (bytes)
5240602624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  925487900.0
relative error loss 0.21384194
shape of L is 
torch.Size([])
memory (bytes)
5243785216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 10% |
memory (bytes)
5243785216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  925422600.0
relative error loss 0.21382685
shape of L is 
torch.Size([])
memory (bytes)
5247037440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 10% |
memory (bytes)
5247037440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  925384450.0
relative error loss 0.21381804
shape of L is 
torch.Size([])
memory (bytes)
5250256896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
5250256896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  925341200.0
relative error loss 0.21380804
shape of L is 
torch.Size([])
memory (bytes)
5253332992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 10% |
memory (bytes)
5253332992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  925327600.0
relative error loss 0.21380492
shape of L is 
torch.Size([])
memory (bytes)
5256687616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 10% |
memory (bytes)
5256687616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  925266700.0
relative error loss 0.21379083
shape of L is 
torch.Size([])
memory (bytes)
5259898880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 10% |
memory (bytes)
5259898880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  925237250.0
relative error loss 0.21378402
shape of L is 
torch.Size([])
memory (bytes)
5263052800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 10% |
memory (bytes)
5263052800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  925207550.0
relative error loss 0.21377717
time to take a step is 272.14586067199707
it  11 : 2524838912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
5266329600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 10% |
memory (bytes)
5266329600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 10% |
error is  925207550.0
relative error loss 0.21377717
shape of L is 
torch.Size([])
memory (bytes)
5269544960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 10% |
memory (bytes)
5269544960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  925166100.0
relative error loss 0.21376759
shape of L is 
torch.Size([])
memory (bytes)
5272756224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
5272756224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% | 10% |
error is  925117950.0
relative error loss 0.21375647
shape of L is 
torch.Size([])
memory (bytes)
5275971584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
5275971584
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  925078500.0
relative error loss 0.21374735
shape of L is 
torch.Size([])
memory (bytes)
5279195136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
5279195136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  925040400.0
relative error loss 0.21373855
shape of L is 
torch.Size([])
memory (bytes)
5282406400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 10% |
memory (bytes)
5282406400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  925010700.0
relative error loss 0.21373168
shape of L is 
torch.Size([])
memory (bytes)
5285617664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 10% |
memory (bytes)
5285617664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  924969700.0
relative error loss 0.21372221
shape of L is 
torch.Size([])
memory (bytes)
5288824832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 10% |
memory (bytes)
5288824832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  924943900.0
relative error loss 0.21371624
shape of L is 
torch.Size([])
memory (bytes)
5292036096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 10% |
memory (bytes)
5292036096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 10% |
error is  924923900.0
relative error loss 0.21371163
shape of L is 
torch.Size([])
memory (bytes)
5295140864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 10% |
memory (bytes)
5295267840
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  924878340.0
relative error loss 0.2137011
time to take a step is 254.3152756690979
it  12 : 2524838912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
5298479104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
5298479104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% | 10% |
error is  924878340.0
relative error loss 0.2137011
shape of L is 
torch.Size([])
memory (bytes)
5301673984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 10% |
memory (bytes)
5301673984
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  924845300.0
relative error loss 0.21369347
shape of L is 
torch.Size([])
memory (bytes)
5304922112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 10% |
memory (bytes)
5304922112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  924825860.0
relative error loss 0.21368897
shape of L is 
torch.Size([])
memory (bytes)
5308137472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
5308137472
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  924801300.0
relative error loss 0.21368329
shape of L is 
torch.Size([])
memory (bytes)
5311344640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
5311344640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% | 10% |
error is  924763650.0
relative error loss 0.2136746
shape of L is 
torch.Size([])
memory (bytes)
5314564096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
5314564096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  924792060.0
relative error loss 0.21368116
shape of L is 
torch.Size([])
memory (bytes)
5317783552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 10% |
memory (bytes)
5317783552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  924736000.0
relative error loss 0.21366821
shape of L is 
torch.Size([])
memory (bytes)
5320990720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 10% |
memory (bytes)
5321007104
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  924686340.0
relative error loss 0.21365674
shape of L is 
torch.Size([])
memory (bytes)
5324214272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
5324214272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  924650000.0
relative error loss 0.21364833
shape of L is 
torch.Size([])
memory (bytes)
5327454208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 10% |
memory (bytes)
5327454208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  924612860.0
relative error loss 0.21363977
time to take a step is 273.1325616836548
it  13 : 2524838912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
5330673664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 10% |
memory (bytes)
5330673664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  924612860.0
relative error loss 0.21363977
shape of L is 
torch.Size([])
memory (bytes)
5333880832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 10% |
memory (bytes)
5333880832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  924584960.0
relative error loss 0.21363331
shape of L is 
torch.Size([])
memory (bytes)
5337096192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 10% |
memory (bytes)
5337096192
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  924559100.0
relative error loss 0.21362734
shape of L is 
torch.Size([])
memory (bytes)
5340299264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 10% |
memory (bytes)
5340299264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  924533250.0
relative error loss 0.21362136
shape of L is 
torch.Size([])
memory (bytes)
5343514624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
5343514624
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  924514300.0
relative error loss 0.21361698
shape of L is 
torch.Size([])
memory (bytes)
5346725888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
5346725888
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  924495360.0
relative error loss 0.21361262
shape of L is 
torch.Size([])
memory (bytes)
5349953536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
5349953536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 10% |
error is  924467460.0
relative error loss 0.21360616
shape of L is 
torch.Size([])
memory (bytes)
5353160704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
5353160704
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  924456960.0
relative error loss 0.21360373
shape of L is 
torch.Size([])
memory (bytes)
5356376064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
5356376064
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  924444400.0
relative error loss 0.21360084
shape of L is 
torch.Size([])
memory (bytes)
5359599616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
5359599616
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  924422900.0
relative error loss 0.21359587
time to take a step is 279.49078845977783
it  14 : 2524838912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
shape of L is 
torch.Size([])
memory (bytes)
5362819072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
5362819072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  924422900.0
relative error loss 0.21359587
shape of L is 
torch.Size([])
memory (bytes)
5366026240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 10% |
memory (bytes)
5366026240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  924408600.0
relative error loss 0.21359256
shape of L is 
torch.Size([])
memory (bytes)
5369237504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
5369237504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% | 10% |
error is  924393200.0
relative error loss 0.21358901
shape of L is 
torch.Size([])
memory (bytes)
5372456960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
5372456960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  924356350.0
relative error loss 0.21358049
shape of L is 
torch.Size([])
memory (bytes)
5375672320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 10% |
memory (bytes)
5375672320
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  924333600.0
relative error loss 0.21357523
shape of L is 
torch.Size([])
memory (bytes)
5378887680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 10% |
memory (bytes)
5378887680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  924315900.0
relative error loss 0.21357115
shape of L is 
torch.Size([])
memory (bytes)
5382103040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 10% |
memory (bytes)
5382103040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  924302850.0
relative error loss 0.21356812
shape of L is 
torch.Size([])
memory (bytes)
5385318400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 10% |
memory (bytes)
5385318400
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  924270600.0
relative error loss 0.21356067
shape of L is 
torch.Size([])
memory (bytes)
5388537856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
5388537856
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 10% |
error is  924255500.0
relative error loss 0.21355718
shape of L is 
torch.Size([])
memory (bytes)
5391745024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 10% |
memory (bytes)
5391745024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 10% |
error is  924240100.0
relative error loss 0.21355364
time to take a step is 280.20444560050964
sum tnnu_Z after tensor(10160411., device='cuda:0')
shape of features
(5251,)
shape of features
(5251,)
number of orig particles 21003
number of new particles after remove low mass 18776
tnuZ shape should be parts x labs
torch.Size([21003, 702])
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
error without small mass is  1037197630.0
relative error without small mass is  0.23965344
nnu_Z shape should be number of particles by maxV
(21003, 702)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
shape of features
(21003,)
Tue Jan 31 14:58:14 EST 2023
