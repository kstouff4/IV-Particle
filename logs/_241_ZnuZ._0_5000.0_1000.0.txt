Thu Feb 2 07:27:59 EST 2023
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  0% |  0% |
numbers of X: 13809173
numbers of Z: 26947
shape of features
(26947,)
shape of features
(26947,)
ZX	Vol	Parts	Cubes	Eps
Z	0.020005949417170855	26947	26.947	0.09054884492887691
X	0.017036538061654262	137	0.137	0.4991375698448514
X	0.01884254981522826	6724	6.724	0.14098426865153096
X	0.019860114476053767	3097	3.097	0.1857862265735094
X	0.017305431969972546	1128	1.128	0.2484798149330856
X	0.018057946087851266	18881	18.881	0.0985253046951447
X	0.01751797564614151	12425	12.425	0.11213189808109203
X	0.017640458534826216	22058	22.058	0.09282135681925544
X	0.01743812473462526	12899	12.899	0.11057269946436467
X	0.01743094791095834	1812	1.812	0.21267720160079231
X	0.01735298022686402	4499	4.499	0.15682646730336045
X	0.016915510426787288	1826	1.826	0.21002038228718084
X	0.017152637120203387	13949	13.949	0.10713448226168093
X	0.017549931975827895	1767	1.767	0.21495437855338242
X	0.017600013296111966	106108	106.108	0.05494417065597821
X	0.017386832192611355	10933	10.933	0.11672405632601815
X	0.0176152483789077	16728	16.728	0.10173762235612895
X	0.01752159911336016	26136	26.136	0.08752085252425357
X	0.01748106802016635	17845	17.845	0.09931552364965386
X	0.017511953702493293	56452	56.452	0.06769424608564681
X	0.01759517371366191	58230	58.23	0.0671040794418054
X	0.01713034636449501	5861	5.861	0.14297654809201238
X	0.01864679080514117	107537	107.537	0.0557633334455131
X	0.017360255061801683	3927	3.927	0.16412134476489793
X	0.017527960659169017	8665	8.665	0.12647002592447001
X	0.017166438727323038	6663	6.663	0.1370892703145461
X	0.018102392356397372	27180	27.18	0.08732974132567624
X	0.017451316547912696	13246	13.246	0.10962623067538109
X	0.017249338018240412	2841	2.841	0.18243173970204446
X	0.01744545056658361	24946	24.946	0.08876200236557669
X	0.017747701453293973	537018	537.018	0.03209108670395392
X	0.017390274273359898	1468	1.468	0.22796077529437692
X	0.01755780712231604	221594	221.594	0.04295075070454102
X	0.01735425032906519	2231	2.231	0.19813845055185708
X	0.017444829324726702	2168	2.168	0.20038681796955254
X	0.017434498676373502	3903	3.903	0.1646911659520821
X	0.017552510228911133	45691	45.691	0.07269471817466053
X	0.01761552163428381	40983	40.983	0.0754681876582231
X	0.01841136922152779	1070	1.07	0.2581670248470334
X	0.017403321871326306	2834	2.834	0.18312350921755466
X	0.01737681150819657	1588	1.588	0.22201033767092604
X	0.017271798286777416	1540	1.54	0.2238406434145195
X	0.014513309955551704	123	0.123	0.4904790398974679
X	0.01457324502787185	162	0.162	0.44807125554192745
X	0.016825315806738978	516	0.516	0.3194757077895192
X	0.017204537015575335	146	0.146	0.49026404570348847
X	0.015391892131506863	128	0.128	0.4935835536970381
X	0.01672322038635705	1158	1.158	0.243521392186981
X	0.017084542494761504	1179	1.179	0.24379783769003888
X	0.016704901179404333	403	0.403	0.3460824858492996
X	0.01717998506764605	5247	5.247	0.1484922903729974
X	0.017480814783461838	6835	6.835	0.1367543188770398
X	0.017385053631633215	1659	1.659	0.21883152300005618
X	0.01728155499104695	1089	1.089	0.251295654859514
X	0.017465999814083956	1019	1.019	0.25783374440901047
X	0.01716332325496458	1920	1.92	0.20753915832873598
X	0.01735578187094134	4108	4.108	0.16166074756629367
X	0.017326222252873568	1024	1.024	0.2567248890253226
X	0.017388140466372475	827	0.827	0.2760043181656967
X	0.017341899067701394	470	0.47	0.33291466937640746
X	0.01695572501590357	471	0.471	0.33019093991189435
X	0.01784708163079174	6427	6.427	0.1405571588765186
X	0.01654657340798356	413	0.413	0.34217846433922716
X	0.01800956315346108	4781	4.781	0.1555947043065867
X	0.01731479155830985	1487	1.487	0.2266568069980132
X	0.016943208353366754	672	0.672	0.2932302649621038
X	0.017176428315613817	1137	1.137	0.2472051223318173
X	0.01627422430852916	401	0.401	0.34365181474729906
X	0.01682042779004741	571	0.571	0.3088400704778475
X	0.017119918204770306	556	0.556	0.313431252330394
X	0.016915035542522844	625	0.625	0.300237060284791
X	0.016876242701321913	999	0.999	0.25658824605275876
X	0.01735551432252491	776	0.776	0.2817465384677429
X	0.01531200396529656	234	0.234	0.402969390548716
X	0.017006191874970624	754	0.754	0.2825391122766
X	0.01735047482714304	827	0.827	0.2758048834887799
X	0.01742768045233988	2056	2.056	0.20389449140423452
X	0.01609823308860306	345	0.345	0.3600142617404443
X	0.015034993114109076	204	0.204	0.41926732928013677
X	0.01590864112493336	461	0.461	0.3255697553509754
X	0.019097246888166882	2197	2.197	0.20561126352205997
X	0.017424763086196258	1129	1.129	0.24897609247952732
X	0.015055016815028797	118	0.118	0.5034230299385536
X	0.01836596344917313	3066	3.066	0.18161310703330266
X	0.01666057753072999	235	0.235	0.4138796405370903
X	0.016988453046124934	407	0.407	0.3468857623909616
X	0.016985228205018164	621	0.621	0.30129592783810755
X	0.016896475527261083	911	0.911	0.26470323496119336
X	0.016499739285447512	174	0.174	0.4560116568904372
X	0.016942881259412745	1274	1.274	0.23692230346595944
X	0.01711109400875861	1600	1.6	0.22031932797848464
X	0.015716468307477003	383	0.383	0.34492027925882474
X	0.016808191476537896	200	0.2	0.4380230824395156
X	0.017196714546467262	243	0.243	0.41363099012119553
X	0.01747220861588073	1803	1.803	0.21319844196643933
X	0.01620188083325322	696	0.696	0.2855302099383806
X	0.01732430050714436	833	0.833	0.27500266487957936
X	0.019821885891585417	6286	6.286	0.14664149008052316
X	0.017396116790938405	2825	2.825	0.18329246879813033
X	0.017529351478519976	2537	2.537	0.19046497642597715
X	0.017433872893556336	1065	1.065	0.2539109454004491
X	0.017205337734177074	4586	4.586	0.15538522365368437
X	0.017118591433545945	1151	1.151	0.2459221050927745
X	0.016356235704498512	515	0.515	0.3166834569674619
X	0.016762690105178477	524	0.524	0.31744669487794547
X	0.016608902766391327	826	0.826	0.27192781419476836
X	0.016756386492179486	266	0.266	0.39789294988223733
X	0.017165741440112223	1965	1.965	0.20595231151705698
X	0.016502895353307238	92	0.092	0.5639718659139755
X	0.016401670620287245	553	0.553	0.30954299829149395
X	0.017370786110687108	763	0.763	0.28342075130063543
X	0.015764962457045516	685	0.685	0.2844466280242219
X	0.016393302242610132	341	0.341	0.36361128684328453
X	0.01729552017498817	824	0.824	0.27584734324898763
X	0.01739570032535281	1371	1.371	0.23323917015440968
X	0.017415794279024323	1308	1.308	0.2370164636348924
X	0.01705164503145913	486	0.486	0.32737336164731057
X	0.01707872376564203	356	0.356	0.363358344972872
X	0.016314088067376907	205	0.205	0.4301332152636365
X	0.017486051663498623	2327	2.327	0.19586873777613661
X	0.016370756462599466	198	0.198	0.43564679084469377
X	0.017375677159813117	2955	2.955	0.18049343820455446
X	0.016819983132009206	665	0.665	0.2935404598241655
X	0.016619011030270463	471	0.471	0.3279906194038071
X	0.017405200884313632	1016	1.016	0.2577874540599672
X	0.017399974677382215	621	0.621	0.30372858860163593
X	0.017361663643610553	1029	1.029	0.2564830411163045
X	0.016863867007363174	380	0.38	0.3540445042513388
X	0.01668669799073203	255	0.255	0.4029737360660656
X	0.018291676505825773	6633	6.633	0.14023222933985938
X	0.017223137582327037	1056	1.056	0.25360011776368285
X	0.017520211324626028	2708	2.708	0.1863360536208934
X	0.017211942762901222	579	0.579	0.3097779084310659
X	0.017302701724508433	1246	1.246	0.24036171437855527
X	0.01682509474967127	388	0.388	0.35132463009699416
X	0.016833055097035823	607	0.607	0.30268536141787833
X	0.01636352308843865	476	0.476	0.32515463834814434
X	0.016303749632703122	243	0.243	0.4063438969644119
X	0.016764631069134943	1183	1.183	0.24199315699479879
X	0.01740547985733141	445	0.445	0.3394496224758766
X	0.01685300491179128	792	0.792	0.27710905794371193
X	0.017269248742750574	870	0.87	0.2707601405136168
X	0.016511455509686076	166	0.166	0.4633321363201014
X	0.017276597430146628	1785	1.785	0.2131113913112435
X	0.01741942988917014	9912	9.912	0.12067691925113826
X	0.0185525618620867	2832	2.832	0.18711282100420124
X	0.0165133651444146	387	0.387	0.3494418155023057
X	0.01735569888054663	512	0.512	0.3236364150903587
X	0.017370583626852107	626	0.626	0.30274712368066387
X	0.017070651925885465	573	0.573	0.310002475379959
X	0.016856363614242982	511	0.511	0.3207113502777285
X	0.01675513508642749	716	0.716	0.28603003359196155
X	0.019660114433071186	4778	4.778	0.16024333143992708
X	0.01744240921543857	1751	1.751	0.21516589865442004
X	0.0166611891609812	2351	2.351	0.19208102091232435
X	0.017479441924361412	755	0.755	0.28501006923596756
X	0.01734364610492191	5020	5.02	0.15117462339153165
X	0.017026233504047316	593	0.593	0.30621135923072096
X	0.01711193650288124	710	0.71	0.28885525339397855
X	0.017132367974254072	751	0.751	0.28361279438255416
X	0.014625608309318475	84	0.084	0.5583993243012455
X	0.01725319132188137	1859	1.859	0.21015030089332407
X	0.015142452183695365	225	0.225	0.40675968056962497
X	0.017158410569203968	1427	1.427	0.22909609897102162
X	0.016773506986721896	438	0.438	0.33706727327614555
X	0.01669330774768106	693	0.693	0.28880390095732555
X	0.017053089435441787	446	0.446	0.33689099585730886
X	0.017263082019665085	483	0.483	0.32940010585478613
X	0.016226645905215848	306	0.306	0.37569531345222024
X	0.017078901456572686	654	0.654	0.2966839561201988
X	0.015420937432271048	69	0.069	0.6068582049208221
X	0.01587228613723006	197	0.197	0.43190783855048276
X	0.017192799654729713	627	0.627	0.3015502460718463
X	0.0174411801234518	1507	1.507	0.2261973633598887
X	0.017040229251113828	758	0.758	0.2822292852311678
X	0.01822961320823539	2099	2.099	0.2055519834463736
X	0.0170792498753845	486	0.486	0.32754992756835605
X	0.016579614035313845	486	0.486	0.32432422001869304
X	0.017561930054622393	2175	2.175	0.20061850693929723
X	0.01724512201969575	1355	1.355	0.23347604368737554
X	0.01696518098579883	529	0.529	0.31771248030897137
X	0.017620420138627465	2373	2.373	0.19509202072042328
X	0.016983408324798468	822	0.822	0.2744001500893026
X	0.016667905336461637	1108	1.108	0.24685793303983114
X	0.01699894939565651	408	0.408	0.34667349537754233
X	0.017428829417056167	3666	3.666	0.16814808266230016
X	0.017316374474091692	1705	1.705	0.21655964387148569
X	0.016983131751270166	838	0.838	0.27264105485225
X	0.017057206851530827	900	0.9	0.2666173876901628
X	0.018639626767931003	2354	2.354	0.19931665243302418
X	0.01732328747268359	7965	7.965	0.12956315373208832
X	0.016935678553513786	411	0.411	0.3453983511672256
X	0.01718653034115238	262	0.262	0.4033006970196763
X	0.016934312369633112	393	0.393	0.3505836611311105
X	0.01718197487482144	167	0.167	0.4685819292934765
X	0.017233271527156202	1052	1.052	0.25397092281321854
X	0.01713719595434413	596	0.596	0.3063593699148467
X	0.01641208573741267	226	0.226	0.41720718520947886
X	0.016693575440481893	382	0.382	0.35223188509908276
X	0.01707234171589297	1146	1.146	0.24605724613611005
X	0.01669954187477266	250	0.25	0.40574656281138294
X	0.018783057383257442	6354	6.354	0.1435179949284066
X	0.01616067823960664	328	0.328	0.36660235172581357
X	0.017463559949450442	4589	4.589	0.1561246845530774
X	0.019761580227829964	2936	2.936	0.1888086438548235
X	0.017416766974237784	811	0.811	0.27795995264851964
X	0.016003729077327306	89	0.089	0.5644303974638416
X	0.01795316163555398	2130	2.13	0.2035106364567614
X	0.01736859865227904	1420	1.42	0.23040513034027582
X	0.016687061252663123	507	0.507	0.32047240059511034
X	0.01936665148760145	3789	3.789	0.17225690665512677
X	0.017374976741871117	6550	6.55	0.1384291792542299
X	0.016549156461431243	229	0.229	0.4165304861387015
X	0.016699887221236033	206	0.206	0.4327948716014836
X	0.01647590236431989	275	0.275	0.3912960873134974
X	0.016898185343070932	439	0.439	0.33764359549560075
X	0.016563915217002796	261	0.261	0.3988786860194255
X	0.017090446465633177	145	0.145	0.49029989699113635
X	0.016913563816465894	136	0.136	0.49915114614573375
X	0.016618788631461004	699	0.699	0.2875460434169172
X	0.017217821081397074	594	0.594	0.30718305025453063
X	0.016196364499371577	890	0.89	0.2630323581020914
X	0.017046850441672652	533	0.533	0.31742343031082637
X	0.01738806638533455	1246	1.246	0.240756349188118
X	0.01655787818885062	603	0.603	0.30169107119788635
X	0.01680689510321135	1625	1.625	0.21787699397795893
X	0.017203712277987674	703	0.703	0.2903280174976604
X	0.01685164964998713	634	0.634	0.29843589287528544
X	0.016236183652758092	139	0.139	0.488828349692093
X	0.01698450755264431	1754	1.754	0.21314466012645916
X	0.015723456771623606	253	0.253	0.39610394696819967
X	0.016267705067048093	132	0.132	0.4976424882831473
X	0.016969860066038032	635	0.635	0.29897497984310806
X	0.01583646514735266	390	0.39	0.34371452138525627
X	0.01934045643348051	4115	4.115	0.16750674258448986
X	0.017303476036414595	868	0.868	0.27114683756940555
X	0.016313521205180776	441	0.441	0.3331985024205715
X	0.01739324534945695	854	0.854	0.27309112350794
X	0.017412494204242006	1752	1.752	0.21500189826649477
X	0.016963943929532314	276	0.276	0.39464437352962417
X	0.017435572904097713	2843	2.843	0.1830429982035974
X	0.01758776980900996	16713	16.713	0.10171510534700982
X	0.017605910144994394	1947	1.947	0.2083359304145808
X	0.01720352465247932	1706	1.706	0.21604595315080363
X	0.014124117192873627	110	0.11	0.5044942364958976
X	0.01744709272596652	2805	2.805	0.18390635403652947
X	0.017465529494189915	22061	22.061	0.09250932681167973
X	0.017394606642552053	72719	72.719	0.062075787539809205
X	0.01738376009795853	605	0.605	0.3062876236977265
X	0.017166833128657067	15502	15.502	0.10345881132551929
X	0.01746403315148631	10609	10.609	0.1180746587898025
X	0.01733717742906042	2497	2.497	0.19077433592724158
X	0.017387539609202772	80579	80.579	0.05997985612049316
X	0.017363765847857024	249	0.249	0.4116058366705479
X	0.017114590824778352	1210	1.21	0.2418393963063839
X	0.017321496285403064	46551	46.551	0.07192592625364479
X	0.017678999138924855	90822	90.822	0.057954727897112215
X	0.01710578223278459	969	0.969	0.26037925932995515
X	0.01761314298965284	10429	10.429	0.11908710319552372
X	0.01742936014245489	4503	4.503	0.15700970548153778
X	0.017431724034711694	10920	10.92	0.1168707688657045
X	0.017141885310590674	10067	10.067	0.11941333809291883
X	0.01735731483906063	5238	5.238	0.14908673907245357
X	0.018073202717279836	11601	11.601	0.1159257455628544
X	0.017599828774129873	5879	5.879	0.1441235643314563
X	0.01705218450772139	409	0.409	0.3467519444015926
X	0.018632569120377928	169953	169.953	0.04786094859014693
X	0.016968209348765806	651	0.651	0.29649566070169936
X	0.01693907292011179	793	0.793	0.27746325923207443
X	0.01721313596893234	7253	7.253	0.13338718499304236
X	0.01696228064032883	8462	8.462	0.12608702791097104
X	0.018326817391300838	136073	136.073	0.05125918509363448
X	0.017509122747888323	32746	32.746	0.08116498448227538
X	0.015076543102046555	153	0.153	0.46188810811885833
X	0.01743014337385585	7899	7.899	0.13018959676950437
X	0.01702388050545635	3159	3.159	0.17532195795230143
X	0.017383733405945682	8426	8.426	0.12730355532456347
X	0.017526861971116987	24796	24.796	0.08907876342385827
X	0.017297650475898846	1199	1.199	0.24343852693211376
X	0.017446388510715925	8551	8.551	0.1268322054510215
X	0.015477576679104243	155	0.155	0.4639349477358142
X	0.01733814192149743	1553	1.553	0.22349974533118444
X	0.017309496707233314	27338	27.338	0.08586955925003177
X	0.01738060780659525	9059	9.059	0.12425910848769238
X	0.017410807785878002	6850	6.85	0.1364717570832213
X	0.017084016843790423	6432	6.432	0.13848879987126259
X	0.018921591673579617	93943	93.943	0.058618011317897324
X	0.017441559747835803	1615	1.615	0.22104003013573023
X	0.017435024524959757	9032	9.032	0.12451247843778465
X	0.017657988887353163	28406	28.406	0.08534479747681617
X	0.01686026988974079	903	0.903	0.2652928776960843
X	0.0174604192951272	19144	19.144	0.09697817217250927
X	0.018525107878723718	109117	109.117	0.05537190215808156
X	0.017434168308163717	104617	104.617	0.055030018324718565
X	0.017443254954264582	6707	6.707	0.1375201721740003
X	0.01735496286216922	4023	4.023	0.16278881134185044
X	0.016405883678101203	739	0.739	0.28105095360470345
X	0.01741373569426789	1461	1.461	0.22842689906179348
X	0.017519465399475048	7809	7.809	0.13091069914949208
X	0.0179656334591018	4103	4.103	0.1635988849431182
X	0.01745111765824449	9822	9.822	0.12111774384045917
X	0.017679055417321912	29785	29.785	0.08404021424942329
X	0.017593517964507133	10603	10.603	0.11838807959105134
X	0.017337473568270718	3139	3.139	0.17676576838661057
X	0.01804550038746608	2533	2.533	0.19241750534438892
X	0.017613519025681475	38578	38.578	0.07700202410625873
X	0.01759513019503297	5108	5.108	0.1510244277176689
X	0.017366895130060286	2260	2.26	0.19733520941122498
X	0.018435805120356703	10630	10.63	0.12014608250351076
X	0.01781586825729367	272990	272.99	0.040261203364568246
X	0.017375374864612773	4115	4.115	0.16162980457489884
X	0.017398732628241074	23549	23.549	0.09040276523257708
X	0.01694450266297118	1036	1.036	0.2538378361801792
X	0.01715438097532391	24376	24.376	0.0889482763255611
X	0.017536798848130437	2789	2.789	0.18457261647310202
X	0.01744267823430131	134197	134.197	0.050655132311456734
X	0.01744725282408637	14986	14.986	0.1051995241492748
X	0.01741208331056065	1821	1.821	0.2122496243168792
X	0.01765341326652147	35800	35.8	0.07900390422497233
X	0.01755973005195135	37143	37.143	0.07790170307933798
X	0.01746041541248713	1758	1.758	0.2149538528156332
X	0.017417589094339762	11241	11.241	0.11571625352109906
X	0.019865150977644058	72914	72.914	0.06482765547306021
X	0.018867228736757082	167102	167.102	0.04833282229311176
X	0.01748998766317026	9624	9.624	0.12203318968322804
X	0.01643177288358	592	0.592	0.30277562243927925
X	0.016985968883147406	1511	1.511	0.22401406484549843
X	0.01789748943286429	17820	17.82	0.10014473883969967
X	0.017615646153820705	29170	29.17	0.08452542886531235
X	0.017542390530601606	4022	4.022	0.16338627428052152
X	0.01682669075487676	543	0.543	0.3140988125463805
X	0.017578666378557872	16736	16.736	0.10165094464081788
X	0.01863507053380987	27015	27.015	0.08835721590728284
X	0.01667912449452035	13191	13.191	0.10813472413853993
X	0.017101451134007187	1372	1.372	0.23186024163532273
X	0.017313412633070093	5061	5.061	0.15067763007771817
X	0.019651570340812715	7210	7.21	0.13968635935978546
X	0.0173685330764611	2595	2.595	0.18845540277167305
X	0.01740400300201562	5616	5.616	0.14579434646507627
X	0.017434638969127718	1236	1.236	0.24161922998402294
X	0.01744003479120135	13120	13.12	0.10995234697498545
X	0.017476571265284485	13249	13.249	0.10967080822984088
X	0.01752639783360956	6182	6.182	0.1415319710556649
X	0.01731752124696671	7572	7.572	0.1317523513269533
X	0.01708397577680393	2377	2.377	0.19298338585736102
X	0.017622778138569495	132001	132.001	0.051109196199355625
X	0.017072600202855736	2461	2.461	0.1907199037509683
X	0.0174199033568998	37434	37.434	0.0774925291047342
X	0.01722719172785773	481	0.481	0.32962727252173174
X	0.016941609906464525	570	0.57	0.3097609070160074
X	0.01716151412548994	1866	1.866	0.2095147746963646
X	0.01754683318386411	5421	5.421	0.14792438760476445
X	0.017422587171433628	951	0.951	0.2636194055594241
X	0.017385371064919778	20364	20.364	0.09486520700128499
X	0.017332362973129095	1256	1.256	0.23985901469986404
X	0.017210355634487914	1375	1.375	0.23218226783197585
X	0.01753569980484538	87686	87.686	0.05847868717675548
X	0.01733046223407832	8351	8.351	0.1275529637811959
X	0.017423114086267108	8955	8.955	0.12483989557928075
X	0.017606212009872734	50970	50.97	0.07016444557893135
X	0.01751532724769856	54562	54.562	0.06847141846521236
X	0.017436502806144255	4087	4.087	0.16218782461879291
X	0.017251433871366472	1734	1.734	0.21507604161679095
X	0.01742116241844135	4763	4.763	0.154075002247251
X	0.01655317196771947	138	0.138	0.49317461417932795
X	0.017330004238596063	810	0.81	0.27761177527286546
X	0.017492951561353342	8705	8.705	0.12619188838958684
X	0.01704253921576148	629	0.629	0.30035017236755773
X	0.016906465723282433	473	0.473	0.3294052848439323
X	0.0170542544984639	801	0.801	0.2771618221393586
X	0.01715197641275863	1002	1.002	0.25772041168128973
X	0.01767672174602684	71463	71.463	0.06277308807008128
X	0.01716491141619372	3336	3.336	0.17263883958180454
X	0.017344106661628576	21981	21.981	0.09240628197162648
X	0.01742701083919184	2016	2.016	0.20523154970355567
X	0.016620223188092306	1515	1.515	0.2221986449996387
X	0.017369067702396806	7674	7.674	0.13129602290289644
X	0.017753270160747403	244731	244.731	0.04170580027215995
X	0.01809036818624874	194501	194.501	0.045308031016990886
X	0.017535313776595854	8679	8.679	0.1264196599266261
X	0.01746463016610668	27383	27.383	0.08607812313659274
X	0.01714765888566459	366	0.366	0.36050219779968246
X	0.01715590226994324	16955	16.955	0.10039342169626352
X	0.017410480465171966	8993	8.993	0.12463367115521852
X	0.017987538799869498	21059	21.059	0.0948807261281412
X	0.017323683070561244	3793	3.793	0.16591505514651203
X	0.01727279450567255	27156	27.156	0.08600009587722696
X	0.018765680884219142	139522	139.522	0.05123584202024025
X	0.017193866124874422	6447	6.447	0.13867727213361367
X	0.01710893650703672	1687	1.687	0.21645584152222566
X	0.019750957340520677	9630	9.63	0.12705369153228208
X	0.017428456557093082	4976	4.976	0.15186563794977118
X	0.01719100045234395	686	0.686	0.29263460305266675
X	0.01746150247020964	32581	32.581	0.0812279949768937
X	0.016962593698666442	5955	5.955	0.14175449242874466
X	0.015991236848861126	272	0.272	0.3888401039556946
X	0.017683273026196253	11153	11.153	0.11660673898269762
X	0.019849058096858094	8920	8.92	0.13055445099989857
X	0.01557798957518182	136	0.136	0.48565086526096335
X	0.0177125635371132	48932	48.932	0.07126825928435786
X	0.017628850740789445	28802	28.802	0.08490510138160823
X	0.017496266672716032	28972	28.972	0.08452573793556022
X	0.018319124744783744	26536	26.536	0.08838049314341243
X	0.01737199658801587	28302	28.302	0.08498537653287111
X	0.016650395845569096	2134	2.134	0.19833988142334402
X	0.017238757428406148	2783	2.783	0.18365281270350772
X	0.017483803895198338	10222	10.222	0.11959141836058772
X	0.01748723958926195	15884	15.884	0.10325722767732752
X	0.017225117232778787	1160	1.16	0.2457921823974431
X	0.018070773658740956	17303	17.303	0.10145772336128603
X	0.01899990000311254	50819	50.819	0.07204022603290273
X	0.01762620263230598	14333	14.333	0.10713726426832283
X	0.017504820058917202	5335	5.335	0.14859621328944522
X	0.016894012110960466	11305	11.305	0.11432838749179634
X	0.017485047463293504	3304	3.304	0.17426450043429642
X	0.01696067741855382	938	0.938	0.2624702703686612
X	0.017072581621725334	828	0.828	0.27421399771038
X	0.017594927692578855	41935	41.935	0.07486352627153314
X	0.017523127927242574	25933	25.933	0.08775117837914359
X	0.017432569000036945	148948	148.948	0.048915020393043865
X	0.01746775485981636	77731	77.731	0.06079683128174002
X	0.01745372160615512	18230	18.23	0.09855994639606198
X	0.01744898282858695	6989	6.989	0.13565996216169837
X	0.01917744529801929	36257	36.257	0.08087226910863783
X	0.017268666239618157	507	0.507	0.32415317428336454
X	0.01682075275217697	2051	2.051	0.20166316964429723
X	0.017488293967801653	98396	98.396	0.05622420634441522
X	0.017338913340353545	6114	6.114	0.14154639530580398
X	0.01734682013942126	635	0.635	0.3011725481366476
X	0.017541435893108175	6551	6.551	0.13886277730876392
X	0.01727773729081757	15658	15.658	0.10333565402038757
X	0.01736027665040507	5622	5.622	0.14562030404365042
X	0.018959135486807097	23746	23.746	0.09277038427964948
X	0.016785108235584434	4239	4.239	0.15820501387809593
X	0.01697367564717944	4035	4.035	0.16142747484000047
X	0.01736291397284679	1244	1.244	0.2407690972349658
X	0.018204016239075527	2262	2.262	0.20039714865246905
X	0.01748676583727299	20576	20.576	0.09472170666357692
X	0.016949226620151175	9840	9.84	0.11987214113302164
X	0.016444260005405734	2045	2.045	0.20034277001322545
X	0.017491230275275896	28703	28.703	0.08478083484598349
X	0.016363829786039914	916	0.916	0.26141459236364156
X	0.017484740826554927	7760	7.76	0.13109893369153613
X	0.01733437218935721	699	0.699	0.29161530516642975
X	0.017614805778886636	11885	11.885	0.11401436102116885
X	0.017925834803166844	1471	1.471	0.2301206395630238
X	0.017292216475259557	7415	7.415	0.1326110873553019
X	0.017392424838300027	5197	5.197	0.14957848820024053
X	0.017751480480463554	6682	6.682	0.1384977450971237
X	0.017657013630586256	23449	23.449	0.09097686666528255
X	0.01746100681270081	29552	29.552	0.08391265606115521
X	0.017435106415874993	3781	3.781	0.16644572365920332
X	0.016489976562200512	3680	3.68	0.16486333209412113
X	0.017507012671799375	47488	47.488	0.07170389084312413
X	0.017394436407792462	2620	2.62	0.18794741498935832
X	0.017640443682312427	266575	266.575	0.04044796676264787
X	0.016183910408090137	339	0.339	0.3627671244158594
X	0.017356424681647435	26293	26.293	0.08707096856872515
X	0.01900932378662341	156503	156.503	0.04952388050822672
X	0.016708441512769467	487	0.487	0.32493936481618296
X	0.017376620318772268	32675	32.675	0.0810183209586439
X	0.01739897370630666	12819	12.819	0.1107192566694179
X	0.017598736333056465	216583	216.583	0.043313077237089756
X	0.01729917156223027	2115	2.115	0.2014829548342991
X	0.01692139764108446	10662	10.662	0.11664492575095209
X	0.01672365647286618	923	0.923	0.26264946004117384
X	0.01978095229527798	3040	3.04	0.18669149664906565
X	0.01706711803543886	3636	3.636	0.1674346887564132
X	0.017473102019921284	19559	19.559	0.09631068135548367
X	0.01695602440333834	5551	5.551	0.145094494976125
X	0.017446766784688723	25943	25.943	0.08761226741004229
X	0.0172102019139319	4086	4.086	0.16149628425592918
X	0.016645215618561988	395	0.395	0.34798787329507586
X	0.017458418969120516	1703	1.703	0.21723514484876225
X	0.01745917125428266	48382	48.382	0.07119452594815211
X	0.017318063403844577	17925	17.925	0.09885835669728879
X	0.017710189418214582	149580	149.58	0.0491039647246405
X	0.01896900693671017	12207	12.207	0.11582755357558021
X	0.017445530961785083	2811	2.811	0.18376992951955776
X	0.017188801581602457	1239	1.239	0.24028395074147418
X	0.016590037790304414	2545	2.545	0.18680415402201625
X	0.01926083549437677	165456	165.456	0.04882746758724812
X	0.017461446630993006	4508	4.508	0.15704788951454932
X	0.01736297485124446	2923	2.923	0.1811055558781012
X	0.01734143081671854	23923	23.923	0.08983035082553277
X	0.017213173953971456	14683	14.683	0.10544242521551103
X	0.017327598082097505	1547	1.547	0.22374294528788521
X	0.017996813153051232	2647	2.647	0.18944386933895946
X	0.017516876762728516	83972	83.972	0.05930719614885079
X	0.017437750952766853	5038	5.038	0.15126696819364654
X	0.01716025996149616	20130	20.13	0.09481856156328801
X	0.017447395751294385	8170	8.17	0.12877637187463384
X	0.01816990335071562	16184	16.184	0.10393350716052235
X	0.01760920696658056	52580	52.58	0.06944480115429758
X	0.017476267545456132	50304	50.304	0.07029893109632358
X	0.017632561719052357	51759	51.759	0.06984090289030666
X	0.017496151260759867	3752	3.752	0.16706798385575808
X	0.017613833013948622	20443	20.443	0.09515605208675909
X	0.017415315150691706	4625	4.625	0.15557502052535058
X	0.018558540917243984	2139	2.139	0.20548384394770852
X	0.017403242880973828	6119	6.119	0.1416826189499698
X	0.017505624592427754	1160	1.16	0.24711922911748466
X	0.017455832345248293	9008	9.008	0.12467251720085779
X	0.017353338280562007	2513	2.513	0.1904277260557487
X	0.015195198083164807	708	0.708	0.27790159286867677
X	0.017310573815560615	6143	6.143	0.1412462683989505
X	0.01756668079076086	2491	2.491	0.19176616979909925
X	0.017611832745585258	20587	20.587	0.09493007616197764
X	0.017349962820410177	2647	2.647	0.1871464251661299
X	0.017438391340292497	8079	8.079	0.1292358302318142
X	0.01577782764186398	2933	2.933	0.17521786390098842
X	0.01766525566370155	8922	8.922	0.12557000246358316
X	0.017289213133796017	973	0.973	0.26094809413687364
X	0.017370106258773423	1756	1.756	0.21466405118752377
X	0.017471491555272875	6499	6.499	0.13904684803205947
X	0.01743940848402366	56877	56.877	0.06743184196420952
X	0.017025931792203394	2387	2.387	0.19249501628966298
X	0.01628102181772971	190	0.19	0.4408687870773296
X	0.015692762370438915	573	0.573	0.30142661200222837
X	0.01749723807406536	99414	99.414	0.05604118598477953
X	0.017575906365762447	115925	115.925	0.05332287312663657
X	0.017100009583325554	7615	7.615	0.1309509988333517
X	0.017451753786752504	2127	2.127	0.20169286223242006
X	0.017444288004634108	34491	34.491	0.07967385313175245
X	0.01976726861054014	31075	31.075	0.08600265174055473
X	0.016869041517307715	526	0.526	0.3177129284839431
X	0.017458419111771782	3155	3.155	0.17687584072803064
X	0.017604163014931475	6061	6.061	0.14267800641936218
X	0.017456852825530524	12077	12.077	0.11306693721834295
X	0.017180460983335106	1796	1.796	0.2122802025830505
X	0.017603519978155943	61865	61.865	0.06577358292194405
X	0.017330970901227885	2205	2.205	0.198825202337702
X	0.01710995475305754	4278	4.278	0.15873378067503502
X	0.017134717627084718	487	0.487	0.3276795437075492
X	0.017300855326139456	6701	6.701	0.13718585011364465
X	0.01744023144221927	4599	4.599	0.15594194432008923
X	0.017431726768718746	6182	6.182	0.14127667652531156
X	0.017585060189677676	24226	24.226	0.08987123617227205
X	0.01767924794930673	141782	141.782	0.04995905668090003
X	0.017379608418047518	2055	2.055	0.20373988356307018
X	0.016911120609250388	968	0.968	0.2594770847047167
X	0.017444293472281658	5843	5.843	0.14399226592850556
X	0.017635446303932666	55742	55.742	0.06813987031148458
X	0.01736387629295617	6929	6.929	0.13582885526976976
X	0.016557113551704184	542	0.542	0.3126044194529187
X	0.01697971879096574	630	0.63	0.29982187421692996
X	0.017559870711657245	21583	21.583	0.09335476737374268
X	0.01673989653118256	403	0.403	0.3463239884412394
X	0.01739358985040998	18566	18.566	0.09784912797784016
X	0.01696649571335487	973	0.973	0.2593142823028641
X	0.01752859781054062	5330	5.33	0.14870993681952802
X	0.017451582751288237	3832	3.832	0.1657561968051438
X	0.017601124844292666	1315	1.315	0.2374314383850926
X	0.017292170904622806	870	0.87	0.2708798844561754
X	0.01764222924993317	61965	61.965	0.06578633148654282
X	0.01768976120189488	138649	138.649	0.05034253448560091
X	0.01768240444218386	41953	41.953	0.07497666160985705
X	0.01794277066925694	7674	7.674	0.13272596873273634
X	0.01746522175076413	9465	9.465	0.12265478492468677
X	0.01733818910527832	2476	2.476	0.19131588526608273
X	0.018360523693532464	3363	3.363	0.17608379878796007
X	0.017535225386630715	19632	19.632	0.09630502219591924
X	0.01990238570096497	10681	10.681	0.12305456828983825
X	0.017370891277048196	14841	14.841	0.10538682297627412
X	0.017273726082241283	5401	5.401	0.1473343292918123
X	0.01834260335802131	132242	132.242	0.05176432104397398
X	0.01747773777255749	6244	6.244	0.14093129465972384
X	0.019792388999713792	10861	10.861	0.12214512400790077
X	0.019485934605642043	68715	68.715	0.06569866780403008
X	0.017513206743837797	19960	19.96	0.09573447923461965
X	0.017458978845824587	16707	16.707	0.10147836360924399
X	0.017433422870428893	46195	46.195	0.07226520367918456
X	0.01742222464593525	19327	19.327	0.09660057709018129
X	0.017497407184885724	12560	12.56	0.11168495829148843
X	0.01736923105184604	2710	2.71	0.18575354060236487
X	0.017427267912787835	7591	7.591	0.1319198368224168
X	0.017431042766440162	3502	3.502	0.1707401828657178
X	0.01768058717515103	30660	30.66	0.08323542047944744
X	0.01760783259620888	145467	145.467	0.04946679877460526
X	0.018252040217342592	33984	33.984	0.08128535510235886
X	0.017971017670237434	26266	26.266	0.0881169744756001
X	0.01759302387196312	3393	3.393	0.17308212189588235
X	0.01749414237335986	11482	11.482	0.11506900860288856
X	0.01981649584154257	26252	26.252	0.09105172016283705
X	0.016761619948959874	5554	5.554	0.14451182022078593
X	0.017556583063570663	23802	23.802	0.09035291771980765
X	0.017672709216603116	87621	87.621	0.05864508797415371
X	0.016189639071242677	4751	4.751	0.1504818235732337
X	0.017498055263111323	39636	39.636	0.07614358515753814
X	0.017486736966286096	35099	35.099	0.0792753213973619
X	0.017364670378495574	9832	9.832	0.12087641136284444
X	0.017296287110658552	1370	1.37	0.23285064192394536
X	0.01747445620576808	23785	23.785	0.09023329944916787
X	0.018044167122862675	28668	28.668	0.08569980130443138
X	0.016695053604789047	921	0.921	0.26268950927061596
X	0.01769629940433561	178487	178.487	0.04628340365001763
X	0.017693191165148236	14078	14.078	0.10791663700317664
X	0.017441936491650083	14413	14.413	0.10656471752162353
X	0.017427898103733283	2194	2.194	0.19952751618336248
X	0.016639647362089907	752	0.752	0.2807428854291463
X	0.016942817067336937	550	0.55	0.3134784163382725
X	0.016503771450877038	797	0.797	0.27460490675432886
X	0.017344115599119615	6307	6.307	0.1401016046476819
X	0.01743196859469841	10680	10.68	0.11774027903170721
X	0.019296918362435578	1286014	1286.014	0.024664979244585838
X	0.017328337329868812	2757	2.757	0.1845468808171495
X	0.017489920333071007	14903	14.903	0.10548030492778039
X	0.017982194094883792	2282	2.282	0.19899508367937635
X	0.018112185255101672	3747	3.747	0.1690813549749409
X	0.0184350950587786	6809	6.809	0.13937579225261648
X	0.017624213542868035	41361	41.361	0.07524995397814817
X	0.017400846808943715	2554	2.554	0.18957591620617478
X	0.0176860832486983	281996	281.996	0.03973100697260696
X	0.016701476533379535	1307	1.307	0.23379026686662102
X	0.017573560075508893	70654	70.654	0.06288894831429152
X	0.017503317492716688	15865	15.865	0.1033300797785702
X	0.01743270061557127	28894	28.894	0.08449914886254392
X	0.017553198898618665	76359	76.359	0.061258357387560386
X	0.017488040448093534	4023	4.023	0.16320383969202762
X	0.01729356768566842	1146	1.146	0.2471155037855036
X	0.01817326512104851	41697	41.697	0.07581865063474537
X	0.017541099145302587	2657	2.657	0.18759521197878515
X	0.016813521413948186	5029	5.029	0.14952908710966745
X	0.017545164510263053	19258	19.258	0.09694276907769445
X	0.01885165949606951	6925	6.925	0.13962931357941213
X	0.02000855877345079	121835	121.835	0.0547622453351139
X	0.017325282433660952	4615	4.615	0.15541855707169336
X	0.01980051090024774	63103	63.103	0.06795302530518639
X	0.01749261760661878	139158	139.158	0.05009359589808991
X	0.017374288807605352	3822	3.822	0.16565534420838607
X	0.01891830372036965	1934	1.934	0.21386607935492102
X	0.01763715094919869	33359	33.359	0.08086088113682183
X	0.01712005811687627	5788	5.788	0.1435463775556668
X	0.017388722489172205	12174	12.174	0.11261894632089259
X	0.01717600531934292	6636	6.636	0.13730044016560344
X	0.017324124294243456	18130	18.13	0.09849582653424885
X	0.01746383240086677	17678	17.678	0.09959452739330014
X	0.017115820355231362	49620	49.62	0.0701315848351009
X	0.017490922640559344	8724	8.724	0.1260953352611369
X	0.017917894237019106	5021	5.021	0.15281485809086784
X	0.01706148410057729	1046	1.046	0.25360727469743516
X	0.019236857205197848	6741	6.741	0.14184148345401762
X	0.017125288450193444	12110	12.11	0.1122443744832308
X	0.01734562753306113	26146	26.146	0.08721575001328165
X	0.017507219041879643	49863	49.863	0.07054716843515495
X	0.017076804917709562	2277	2.277	0.19574072633006165
X	0.017486581020782467	9316	9.316	0.1233554877576795
X	0.01759029042667657	10241	10.241	0.11975956631177845
X	0.017607898225207953	27083	27.083	0.08663038607209121
X	0.01749149253936296	15049	15.049	0.10514123540585969
X	0.017514012911851343	23675	23.675	0.09044097298868582
X	0.017507400509891285	81089	81.089	0.05999106530954775
X	0.017598228361819856	10718	10.718	0.11797366334183443
X	0.01717376136228499	499	0.499	0.325278201035073
X	0.01756166675299013	5088	5.088	0.15112612369010583
X	0.01870097203859015	20331	20.331	0.09725273272544663
X	0.017327585273090397	6085	6.085	0.14174001924989538
X	0.017640906615362006	15090	15.09	0.10534418164929285
X	0.017371213746406794	511	0.511	0.32394386140900666
X	0.017381613141047848	1625	1.625	0.22033267381074245
X	0.017434325682720363	31176	31.176	0.0823875498197658
X	0.01755629575287663	12944	12.944	0.11069333223963032
X	0.01749173662549289	10683	10.683	0.11786365384838707
X	0.016652199343544633	645	0.645	0.2955543090270558
X	0.01699047406586197	1226	1.226	0.24019926574556544
X	0.01743837960937961	5278	5.278	0.14894030785401804
X	0.017515459992919893	11846	11.846	0.11392436231658418
X	0.01746262426444686	19402	19.402	0.09655045759767364
X	0.017421712463763853	634	0.634	0.3017638379318834
X	0.017197008086246347	9595	9.595	0.12147007663069524
X	0.017298908014154124	6209	6.209	0.1407123911580402
X	0.017243568034327007	6915	6.915	0.13560581613069592
X	0.01770031976793585	97434	97.434	0.05663568966175057
X	0.01844470416474306	4422	4.422	0.16097223916283177
X	0.01740742485224018	11202	11.202	0.11582784782883038
X	0.017619931966581646	52349	52.349	0.06956091388347001
X	0.019652172660502433	9917	9.917	0.1256059133798971
X	0.017503482527957085	35105	35.105	0.07929610023559565
X	0.017595320477648793	59520	59.52	0.06661592861829282
X	0.016640019060430177	480	0.48	0.3260650075984711
X	0.017468230994554107	7268	7.268	0.13395060183262839
X	0.0175841779114145	3418	3.418	0.17263015888853414
X	0.01747956402621906	28408	28.408	0.08505437265827676
X	0.01742688130469211	30154	30.154	0.08329614726262057
X	0.01731944655896844	7991	7.991	0.12941291701299054
X	0.017636776210970217	19518	19.518	0.0966780681316985
X	0.01737476356272246	4063	4.063	0.16231451572531297
X	0.017012752653732796	5056	5.056	0.14984968725155778
X	0.017376213631430434	3265	3.265	0.17459185820316775
X	0.017482661202635943	2326	2.326	0.19588414125508377
X	0.017674538456807824	55743	55.743	0.06818977349866624
X	0.0174361105499954	115363	115.363	0.053267341735601645
X	0.016913931426142503	1510	1.51	0.22374630537788745
X	0.01744543322003416	4096	4.096	0.1620966117029425
X	0.01748686430982912	4945	4.945	0.15235213620612345
X	0.01894800747636835	40202	40.202	0.0778225211312337
X	0.01735883371527352	5177	5.177	0.14967437706820144
X	0.017298167730513313	2618	2.618	0.1876478043888227
X	0.0182451822825553	15514	15.514	0.1055540269385633
X	0.017524515795122275	26369	26.369	0.08726714922169743
X	0.016933318106572063	2067	2.067	0.20158917002781768
X	0.01748763193529684	24762	24.762	0.08905297411991567
X	0.017429040376975905	1089	1.089	0.2520085054518817
time for making epsilon is 0.757025957107544
epsilons are
[0.4991375698448514, 0.14098426865153096, 0.1857862265735094, 0.2484798149330856, 0.0985253046951447, 0.11213189808109203, 0.09282135681925544, 0.11057269946436467, 0.21267720160079231, 0.15682646730336045, 0.21002038228718084, 0.10713448226168093, 0.21495437855338242, 0.05494417065597821, 0.11672405632601815, 0.10173762235612895, 0.08752085252425357, 0.09931552364965386, 0.06769424608564681, 0.0671040794418054, 0.14297654809201238, 0.0557633334455131, 0.16412134476489793, 0.12647002592447001, 0.1370892703145461, 0.08732974132567624, 0.10962623067538109, 0.18243173970204446, 0.08876200236557669, 0.03209108670395392, 0.22796077529437692, 0.04295075070454102, 0.19813845055185708, 0.20038681796955254, 0.1646911659520821, 0.07269471817466053, 0.0754681876582231, 0.2581670248470334, 0.18312350921755466, 0.22201033767092604, 0.2238406434145195, 0.4904790398974679, 0.44807125554192745, 0.3194757077895192, 0.49026404570348847, 0.4935835536970381, 0.243521392186981, 0.24379783769003888, 0.3460824858492996, 0.1484922903729974, 0.1367543188770398, 0.21883152300005618, 0.251295654859514, 0.25783374440901047, 0.20753915832873598, 0.16166074756629367, 0.2567248890253226, 0.2760043181656967, 0.33291466937640746, 0.33019093991189435, 0.1405571588765186, 0.34217846433922716, 0.1555947043065867, 0.2266568069980132, 0.2932302649621038, 0.2472051223318173, 0.34365181474729906, 0.3088400704778475, 0.313431252330394, 0.300237060284791, 0.25658824605275876, 0.2817465384677429, 0.402969390548716, 0.2825391122766, 0.2758048834887799, 0.20389449140423452, 0.3600142617404443, 0.41926732928013677, 0.3255697553509754, 0.20561126352205997, 0.24897609247952732, 0.5034230299385536, 0.18161310703330266, 0.4138796405370903, 0.3468857623909616, 0.30129592783810755, 0.26470323496119336, 0.4560116568904372, 0.23692230346595944, 0.22031932797848464, 0.34492027925882474, 0.4380230824395156, 0.41363099012119553, 0.21319844196643933, 0.2855302099383806, 0.27500266487957936, 0.14664149008052316, 0.18329246879813033, 0.19046497642597715, 0.2539109454004491, 0.15538522365368437, 0.2459221050927745, 0.3166834569674619, 0.31744669487794547, 0.27192781419476836, 0.39789294988223733, 0.20595231151705698, 0.5639718659139755, 0.30954299829149395, 0.28342075130063543, 0.2844466280242219, 0.36361128684328453, 0.27584734324898763, 0.23323917015440968, 0.2370164636348924, 0.32737336164731057, 0.363358344972872, 0.4301332152636365, 0.19586873777613661, 0.43564679084469377, 0.18049343820455446, 0.2935404598241655, 0.3279906194038071, 0.2577874540599672, 0.30372858860163593, 0.2564830411163045, 0.3540445042513388, 0.4029737360660656, 0.14023222933985938, 0.25360011776368285, 0.1863360536208934, 0.3097779084310659, 0.24036171437855527, 0.35132463009699416, 0.30268536141787833, 0.32515463834814434, 0.4063438969644119, 0.24199315699479879, 0.3394496224758766, 0.27710905794371193, 0.2707601405136168, 0.4633321363201014, 0.2131113913112435, 0.12067691925113826, 0.18711282100420124, 0.3494418155023057, 0.3236364150903587, 0.30274712368066387, 0.310002475379959, 0.3207113502777285, 0.28603003359196155, 0.16024333143992708, 0.21516589865442004, 0.19208102091232435, 0.28501006923596756, 0.15117462339153165, 0.30621135923072096, 0.28885525339397855, 0.28361279438255416, 0.5583993243012455, 0.21015030089332407, 0.40675968056962497, 0.22909609897102162, 0.33706727327614555, 0.28880390095732555, 0.33689099585730886, 0.32940010585478613, 0.37569531345222024, 0.2966839561201988, 0.6068582049208221, 0.43190783855048276, 0.3015502460718463, 0.2261973633598887, 0.2822292852311678, 0.2055519834463736, 0.32754992756835605, 0.32432422001869304, 0.20061850693929723, 0.23347604368737554, 0.31771248030897137, 0.19509202072042328, 0.2744001500893026, 0.24685793303983114, 0.34667349537754233, 0.16814808266230016, 0.21655964387148569, 0.27264105485225, 0.2666173876901628, 0.19931665243302418, 0.12956315373208832, 0.3453983511672256, 0.4033006970196763, 0.3505836611311105, 0.4685819292934765, 0.25397092281321854, 0.3063593699148467, 0.41720718520947886, 0.35223188509908276, 0.24605724613611005, 0.40574656281138294, 0.1435179949284066, 0.36660235172581357, 0.1561246845530774, 0.1888086438548235, 0.27795995264851964, 0.5644303974638416, 0.2035106364567614, 0.23040513034027582, 0.32047240059511034, 0.17225690665512677, 0.1384291792542299, 0.4165304861387015, 0.4327948716014836, 0.3912960873134974, 0.33764359549560075, 0.3988786860194255, 0.49029989699113635, 0.49915114614573375, 0.2875460434169172, 0.30718305025453063, 0.2630323581020914, 0.31742343031082637, 0.240756349188118, 0.30169107119788635, 0.21787699397795893, 0.2903280174976604, 0.29843589287528544, 0.488828349692093, 0.21314466012645916, 0.39610394696819967, 0.4976424882831473, 0.29897497984310806, 0.34371452138525627, 0.16750674258448986, 0.27114683756940555, 0.3331985024205715, 0.27309112350794, 0.21500189826649477, 0.39464437352962417, 0.1830429982035974, 0.10171510534700982, 0.2083359304145808, 0.21604595315080363, 0.5044942364958976, 0.18390635403652947, 0.09250932681167973, 0.062075787539809205, 0.3062876236977265, 0.10345881132551929, 0.1180746587898025, 0.19077433592724158, 0.05997985612049316, 0.4116058366705479, 0.2418393963063839, 0.07192592625364479, 0.057954727897112215, 0.26037925932995515, 0.11908710319552372, 0.15700970548153778, 0.1168707688657045, 0.11941333809291883, 0.14908673907245357, 0.1159257455628544, 0.1441235643314563, 0.3467519444015926, 0.04786094859014693, 0.29649566070169936, 0.27746325923207443, 0.13338718499304236, 0.12608702791097104, 0.05125918509363448, 0.08116498448227538, 0.46188810811885833, 0.13018959676950437, 0.17532195795230143, 0.12730355532456347, 0.08907876342385827, 0.24343852693211376, 0.1268322054510215, 0.4639349477358142, 0.22349974533118444, 0.08586955925003177, 0.12425910848769238, 0.1364717570832213, 0.13848879987126259, 0.058618011317897324, 0.22104003013573023, 0.12451247843778465, 0.08534479747681617, 0.2652928776960843, 0.09697817217250927, 0.05537190215808156, 0.055030018324718565, 0.1375201721740003, 0.16278881134185044, 0.28105095360470345, 0.22842689906179348, 0.13091069914949208, 0.1635988849431182, 0.12111774384045917, 0.08404021424942329, 0.11838807959105134, 0.17676576838661057, 0.19241750534438892, 0.07700202410625873, 0.1510244277176689, 0.19733520941122498, 0.12014608250351076, 0.040261203364568246, 0.16162980457489884, 0.09040276523257708, 0.2538378361801792, 0.0889482763255611, 0.18457261647310202, 0.050655132311456734, 0.1051995241492748, 0.2122496243168792, 0.07900390422497233, 0.07790170307933798, 0.2149538528156332, 0.11571625352109906, 0.06482765547306021, 0.04833282229311176, 0.12203318968322804, 0.30277562243927925, 0.22401406484549843, 0.10014473883969967, 0.08452542886531235, 0.16338627428052152, 0.3140988125463805, 0.10165094464081788, 0.08835721590728284, 0.10813472413853993, 0.23186024163532273, 0.15067763007771817, 0.13968635935978546, 0.18845540277167305, 0.14579434646507627, 0.24161922998402294, 0.10995234697498545, 0.10967080822984088, 0.1415319710556649, 0.1317523513269533, 0.19298338585736102, 0.051109196199355625, 0.1907199037509683, 0.0774925291047342, 0.32962727252173174, 0.3097609070160074, 0.2095147746963646, 0.14792438760476445, 0.2636194055594241, 0.09486520700128499, 0.23985901469986404, 0.23218226783197585, 0.05847868717675548, 0.1275529637811959, 0.12483989557928075, 0.07016444557893135, 0.06847141846521236, 0.16218782461879291, 0.21507604161679095, 0.154075002247251, 0.49317461417932795, 0.27761177527286546, 0.12619188838958684, 0.30035017236755773, 0.3294052848439323, 0.2771618221393586, 0.25772041168128973, 0.06277308807008128, 0.17263883958180454, 0.09240628197162648, 0.20523154970355567, 0.2221986449996387, 0.13129602290289644, 0.04170580027215995, 0.045308031016990886, 0.1264196599266261, 0.08607812313659274, 0.36050219779968246, 0.10039342169626352, 0.12463367115521852, 0.0948807261281412, 0.16591505514651203, 0.08600009587722696, 0.05123584202024025, 0.13867727213361367, 0.21645584152222566, 0.12705369153228208, 0.15186563794977118, 0.29263460305266675, 0.0812279949768937, 0.14175449242874466, 0.3888401039556946, 0.11660673898269762, 0.13055445099989857, 0.48565086526096335, 0.07126825928435786, 0.08490510138160823, 0.08452573793556022, 0.08838049314341243, 0.08498537653287111, 0.19833988142334402, 0.18365281270350772, 0.11959141836058772, 0.10325722767732752, 0.2457921823974431, 0.10145772336128603, 0.07204022603290273, 0.10713726426832283, 0.14859621328944522, 0.11432838749179634, 0.17426450043429642, 0.2624702703686612, 0.27421399771038, 0.07486352627153314, 0.08775117837914359, 0.048915020393043865, 0.06079683128174002, 0.09855994639606198, 0.13565996216169837, 0.08087226910863783, 0.32415317428336454, 0.20166316964429723, 0.05622420634441522, 0.14154639530580398, 0.3011725481366476, 0.13886277730876392, 0.10333565402038757, 0.14562030404365042, 0.09277038427964948, 0.15820501387809593, 0.16142747484000047, 0.2407690972349658, 0.20039714865246905, 0.09472170666357692, 0.11987214113302164, 0.20034277001322545, 0.08478083484598349, 0.26141459236364156, 0.13109893369153613, 0.29161530516642975, 0.11401436102116885, 0.2301206395630238, 0.1326110873553019, 0.14957848820024053, 0.1384977450971237, 0.09097686666528255, 0.08391265606115521, 0.16644572365920332, 0.16486333209412113, 0.07170389084312413, 0.18794741498935832, 0.04044796676264787, 0.3627671244158594, 0.08707096856872515, 0.04952388050822672, 0.32493936481618296, 0.0810183209586439, 0.1107192566694179, 0.043313077237089756, 0.2014829548342991, 0.11664492575095209, 0.26264946004117384, 0.18669149664906565, 0.1674346887564132, 0.09631068135548367, 0.145094494976125, 0.08761226741004229, 0.16149628425592918, 0.34798787329507586, 0.21723514484876225, 0.07119452594815211, 0.09885835669728879, 0.0491039647246405, 0.11582755357558021, 0.18376992951955776, 0.24028395074147418, 0.18680415402201625, 0.04882746758724812, 0.15704788951454932, 0.1811055558781012, 0.08983035082553277, 0.10544242521551103, 0.22374294528788521, 0.18944386933895946, 0.05930719614885079, 0.15126696819364654, 0.09481856156328801, 0.12877637187463384, 0.10393350716052235, 0.06944480115429758, 0.07029893109632358, 0.06984090289030666, 0.16706798385575808, 0.09515605208675909, 0.15557502052535058, 0.20548384394770852, 0.1416826189499698, 0.24711922911748466, 0.12467251720085779, 0.1904277260557487, 0.27790159286867677, 0.1412462683989505, 0.19176616979909925, 0.09493007616197764, 0.1871464251661299, 0.1292358302318142, 0.17521786390098842, 0.12557000246358316, 0.26094809413687364, 0.21466405118752377, 0.13904684803205947, 0.06743184196420952, 0.19249501628966298, 0.4408687870773296, 0.30142661200222837, 0.05604118598477953, 0.05332287312663657, 0.1309509988333517, 0.20169286223242006, 0.07967385313175245, 0.08600265174055473, 0.3177129284839431, 0.17687584072803064, 0.14267800641936218, 0.11306693721834295, 0.2122802025830505, 0.06577358292194405, 0.198825202337702, 0.15873378067503502, 0.3276795437075492, 0.13718585011364465, 0.15594194432008923, 0.14127667652531156, 0.08987123617227205, 0.04995905668090003, 0.20373988356307018, 0.2594770847047167, 0.14399226592850556, 0.06813987031148458, 0.13582885526976976, 0.3126044194529187, 0.29982187421692996, 0.09335476737374268, 0.3463239884412394, 0.09784912797784016, 0.2593142823028641, 0.14870993681952802, 0.1657561968051438, 0.2374314383850926, 0.2708798844561754, 0.06578633148654282, 0.05034253448560091, 0.07497666160985705, 0.13272596873273634, 0.12265478492468677, 0.19131588526608273, 0.17608379878796007, 0.09630502219591924, 0.12305456828983825, 0.10538682297627412, 0.1473343292918123, 0.05176432104397398, 0.14093129465972384, 0.12214512400790077, 0.06569866780403008, 0.09573447923461965, 0.10147836360924399, 0.07226520367918456, 0.09660057709018129, 0.11168495829148843, 0.18575354060236487, 0.1319198368224168, 0.1707401828657178, 0.08323542047944744, 0.04946679877460526, 0.08128535510235886, 0.0881169744756001, 0.17308212189588235, 0.11506900860288856, 0.09105172016283705, 0.14451182022078593, 0.09035291771980765, 0.05864508797415371, 0.1504818235732337, 0.07614358515753814, 0.0792753213973619, 0.12087641136284444, 0.23285064192394536, 0.09023329944916787, 0.08569980130443138, 0.26268950927061596, 0.04628340365001763, 0.10791663700317664, 0.10656471752162353, 0.19952751618336248, 0.2807428854291463, 0.3134784163382725, 0.27460490675432886, 0.1401016046476819, 0.11774027903170721, 0.024664979244585838, 0.1845468808171495, 0.10548030492778039, 0.19899508367937635, 0.1690813549749409, 0.13937579225261648, 0.07524995397814817, 0.18957591620617478, 0.03973100697260696, 0.23379026686662102, 0.06288894831429152, 0.1033300797785702, 0.08449914886254392, 0.061258357387560386, 0.16320383969202762, 0.2471155037855036, 0.07581865063474537, 0.18759521197878515, 0.14952908710966745, 0.09694276907769445, 0.13962931357941213, 0.0547622453351139, 0.15541855707169336, 0.06795302530518639, 0.05009359589808991, 0.16565534420838607, 0.21386607935492102, 0.08086088113682183, 0.1435463775556668, 0.11261894632089259, 0.13730044016560344, 0.09849582653424885, 0.09959452739330014, 0.0701315848351009, 0.1260953352611369, 0.15281485809086784, 0.25360727469743516, 0.14184148345401762, 0.1122443744832308, 0.08721575001328165, 0.07054716843515495, 0.19574072633006165, 0.1233554877576795, 0.11975956631177845, 0.08663038607209121, 0.10514123540585969, 0.09044097298868582, 0.05999106530954775, 0.11797366334183443, 0.325278201035073, 0.15112612369010583, 0.09725273272544663, 0.14174001924989538, 0.10534418164929285, 0.32394386140900666, 0.22033267381074245, 0.0823875498197658, 0.11069333223963032, 0.11786365384838707, 0.2955543090270558, 0.24019926574556544, 0.14894030785401804, 0.11392436231658418, 0.09655045759767364, 0.3017638379318834, 0.12147007663069524, 0.1407123911580402, 0.13560581613069592, 0.05663568966175057, 0.16097223916283177, 0.11582784782883038, 0.06956091388347001, 0.1256059133798971, 0.07929610023559565, 0.06661592861829282, 0.3260650075984711, 0.13395060183262839, 0.17263015888853414, 0.08505437265827676, 0.08329614726262057, 0.12941291701299054, 0.0966780681316985, 0.16231451572531297, 0.14984968725155778, 0.17459185820316775, 0.19588414125508377, 0.06818977349866624, 0.053267341735601645, 0.22374630537788745, 0.1620966117029425, 0.15235213620612345, 0.0778225211312337, 0.14967437706820144, 0.1876478043888227, 0.1055540269385633, 0.08726714922169743, 0.20158917002781768, 0.08905297411991567, 0.2520085054518817]
0.09054884492887691
Making ranges
torch.Size([47293, 2])
We keep 7.80e+06/7.26e+08 =  1% of the original kernel matrix.

torch.Size([395, 2])
We keep 2.85e+03/1.88e+04 = 15% of the original kernel matrix.

torch.Size([6679, 2])
We keep 2.34e+05/3.69e+06 =  6% of the original kernel matrix.

torch.Size([13766, 2])
We keep 1.55e+06/4.52e+07 =  3% of the original kernel matrix.

torch.Size([25740, 2])
We keep 2.97e+06/1.81e+08 =  1% of the original kernel matrix.

torch.Size([7887, 2])
We keep 3.67e+05/9.59e+06 =  3% of the original kernel matrix.

torch.Size([20763, 2])
We keep 1.72e+06/8.35e+07 =  2% of the original kernel matrix.

torch.Size([2240, 2])
We keep 2.37e+05/1.27e+06 = 18% of the original kernel matrix.

torch.Size([11377, 2])
We keep 8.39e+05/3.04e+07 =  2% of the original kernel matrix.

torch.Size([31276, 2])
We keep 7.10e+06/3.56e+08 =  1% of the original kernel matrix.

torch.Size([38453, 2])
We keep 6.43e+06/5.09e+08 =  1% of the original kernel matrix.

torch.Size([22238, 2])
We keep 3.62e+06/1.54e+08 =  2% of the original kernel matrix.

torch.Size([31961, 2])
We keep 4.62e+06/3.35e+08 =  1% of the original kernel matrix.

torch.Size([35483, 2])
We keep 8.27e+06/4.87e+08 =  1% of the original kernel matrix.

torch.Size([40300, 2])
We keep 7.24e+06/5.94e+08 =  1% of the original kernel matrix.

torch.Size([23963, 2])
We keep 3.17e+06/1.66e+08 =  1% of the original kernel matrix.

torch.Size([33429, 2])
We keep 4.74e+06/3.48e+08 =  1% of the original kernel matrix.

torch.Size([4612, 2])
We keep 1.85e+05/3.28e+06 =  5% of the original kernel matrix.

torch.Size([15803, 2])
We keep 1.15e+06/4.88e+07 =  2% of the original kernel matrix.

torch.Size([5928, 2])
We keep 1.16e+06/2.02e+07 =  5% of the original kernel matrix.

torch.Size([16466, 2])
We keep 2.13e+06/1.21e+08 =  1% of the original kernel matrix.

torch.Size([4174, 2])
We keep 2.05e+05/3.33e+06 =  6% of the original kernel matrix.

torch.Size([14923, 2])
We keep 1.16e+06/4.92e+07 =  2% of the original kernel matrix.

torch.Size([20509, 2])
We keep 5.75e+06/1.95e+08 =  2% of the original kernel matrix.

torch.Size([29743, 2])
We keep 4.99e+06/3.76e+08 =  1% of the original kernel matrix.

torch.Size([4441, 2])
We keep 1.60e+05/3.12e+06 =  5% of the original kernel matrix.

torch.Size([15388, 2])
We keep 1.13e+06/4.76e+07 =  2% of the original kernel matrix.

torch.Size([174452, 2])
We keep 1.31e+08/1.13e+10 =  1% of the original kernel matrix.

torch.Size([89097, 2])
We keep 2.66e+07/2.86e+09 =  0% of the original kernel matrix.

torch.Size([20438, 2])
We keep 2.91e+06/1.20e+08 =  2% of the original kernel matrix.

torch.Size([30513, 2])
We keep 4.16e+06/2.95e+08 =  1% of the original kernel matrix.

torch.Size([28503, 2])
We keep 6.25e+06/2.80e+08 =  2% of the original kernel matrix.

torch.Size([36616, 2])
We keep 5.87e+06/4.51e+08 =  1% of the original kernel matrix.

torch.Size([43119, 2])
We keep 1.45e+07/6.83e+08 =  2% of the original kernel matrix.

torch.Size([45083, 2])
We keep 8.39e+06/7.04e+08 =  1% of the original kernel matrix.

torch.Size([28756, 2])
We keep 6.64e+06/3.18e+08 =  2% of the original kernel matrix.

torch.Size([36199, 2])
We keep 6.07e+06/4.81e+08 =  1% of the original kernel matrix.

torch.Size([95199, 2])
We keep 5.04e+07/3.19e+09 =  1% of the original kernel matrix.

torch.Size([63859, 2])
We keep 1.54e+07/1.52e+09 =  1% of the original kernel matrix.

torch.Size([98312, 2])
We keep 5.00e+07/3.39e+09 =  1% of the original kernel matrix.

torch.Size([65193, 2])
We keep 1.60e+07/1.57e+09 =  1% of the original kernel matrix.

torch.Size([7881, 2])
We keep 1.71e+06/3.44e+07 =  4% of the original kernel matrix.

torch.Size([18295, 2])
We keep 2.63e+06/1.58e+08 =  1% of the original kernel matrix.

torch.Size([170025, 2])
We keep 1.24e+08/1.16e+10 =  1% of the original kernel matrix.

torch.Size([87985, 2])
We keep 2.71e+07/2.90e+09 =  0% of the original kernel matrix.

torch.Size([9002, 2])
We keep 5.73e+05/1.54e+07 =  3% of the original kernel matrix.

torch.Size([20452, 2])
We keep 1.94e+06/1.06e+08 =  1% of the original kernel matrix.

torch.Size([16098, 2])
We keep 5.35e+06/7.51e+07 =  7% of the original kernel matrix.

torch.Size([27021, 2])
We keep 3.51e+06/2.33e+08 =  1% of the original kernel matrix.

torch.Size([7759, 2])
We keep 4.50e+06/4.44e+07 = 10% of the original kernel matrix.

torch.Size([17631, 2])
We keep 2.84e+06/1.80e+08 =  1% of the original kernel matrix.

torch.Size([44476, 2])
We keep 9.84e+06/7.39e+08 =  1% of the original kernel matrix.

torch.Size([45934, 2])
We keep 8.64e+06/7.32e+08 =  1% of the original kernel matrix.

torch.Size([16573, 2])
We keep 5.79e+06/1.75e+08 =  3% of the original kernel matrix.

torch.Size([25612, 2])
We keep 4.91e+06/3.57e+08 =  1% of the original kernel matrix.

torch.Size([6769, 2])
We keep 4.95e+05/8.07e+06 =  6% of the original kernel matrix.

torch.Size([18448, 2])
We keep 1.55e+06/7.66e+07 =  2% of the original kernel matrix.

torch.Size([38327, 2])
We keep 1.39e+07/6.22e+08 =  2% of the original kernel matrix.

torch.Size([41340, 2])
We keep 8.05e+06/6.72e+08 =  1% of the original kernel matrix.

torch.Size([917960, 2])
We keep 1.74e+09/2.88e+11 =  0% of the original kernel matrix.

torch.Size([215409, 2])
We keep 1.12e+08/1.45e+10 =  0% of the original kernel matrix.

torch.Size([3358, 2])
We keep 1.70e+05/2.16e+06 =  7% of the original kernel matrix.

torch.Size([13649, 2])
We keep 1.01e+06/3.96e+07 =  2% of the original kernel matrix.

torch.Size([311158, 2])
We keep 7.04e+08/4.91e+10 =  1% of the original kernel matrix.

torch.Size([120211, 2])
We keep 5.08e+07/5.97e+09 =  0% of the original kernel matrix.

torch.Size([4451, 2])
We keep 4.08e+05/4.98e+06 =  8% of the original kernel matrix.

torch.Size([14984, 2])
We keep 1.33e+06/6.01e+07 =  2% of the original kernel matrix.

torch.Size([5432, 2])
We keep 2.27e+05/4.70e+06 =  4% of the original kernel matrix.

torch.Size([16895, 2])
We keep 1.30e+06/5.84e+07 =  2% of the original kernel matrix.

torch.Size([6819, 2])
We keep 7.69e+05/1.52e+07 =  5% of the original kernel matrix.

torch.Size([17743, 2])
We keep 1.96e+06/1.05e+08 =  1% of the original kernel matrix.

torch.Size([57542, 2])
We keep 4.77e+07/2.09e+09 =  2% of the original kernel matrix.

torch.Size([49097, 2])
We keep 1.32e+07/1.23e+09 =  1% of the original kernel matrix.

torch.Size([68691, 2])
We keep 3.31e+07/1.68e+09 =  1% of the original kernel matrix.

torch.Size([55640, 2])
We keep 1.19e+07/1.10e+09 =  1% of the original kernel matrix.

torch.Size([2534, 2])
We keep 7.34e+04/1.14e+06 =  6% of the original kernel matrix.

torch.Size([12575, 2])
We keep 8.40e+05/2.88e+07 =  2% of the original kernel matrix.

torch.Size([6261, 2])
We keep 3.77e+05/8.03e+06 =  4% of the original kernel matrix.

torch.Size([17619, 2])
We keep 1.56e+06/7.64e+07 =  2% of the original kernel matrix.

torch.Size([3390, 2])
We keep 1.95e+05/2.52e+06 =  7% of the original kernel matrix.

torch.Size([13515, 2])
We keep 1.07e+06/4.28e+07 =  2% of the original kernel matrix.

torch.Size([4026, 2])
We keep 1.23e+05/2.37e+06 =  5% of the original kernel matrix.

torch.Size([14961, 2])
We keep 1.03e+06/4.15e+07 =  2% of the original kernel matrix.

torch.Size([386, 2])
We keep 3.62e+03/1.51e+04 = 23% of the original kernel matrix.

torch.Size([6033, 2])
We keep 2.16e+05/3.31e+06 =  6% of the original kernel matrix.

torch.Size([519, 2])
We keep 5.49e+03/2.62e+04 = 20% of the original kernel matrix.

torch.Size([6858, 2])
We keep 2.49e+05/4.37e+06 =  5% of the original kernel matrix.

torch.Size([1439, 2])
We keep 2.54e+04/2.66e+05 =  9% of the original kernel matrix.

torch.Size([10103, 2])
We keep 5.12e+05/1.39e+07 =  3% of the original kernel matrix.

torch.Size([447, 2])
We keep 3.35e+03/2.13e+04 = 15% of the original kernel matrix.

torch.Size([6981, 2])
We keep 2.45e+05/3.93e+06 =  6% of the original kernel matrix.

torch.Size([405, 2])
We keep 3.21e+03/1.64e+04 = 19% of the original kernel matrix.

torch.Size([6451, 2])
We keep 2.24e+05/3.45e+06 =  6% of the original kernel matrix.

torch.Size([2458, 2])
We keep 1.19e+05/1.34e+06 =  8% of the original kernel matrix.

torch.Size([11879, 2])
We keep 8.67e+05/3.12e+07 =  2% of the original kernel matrix.

torch.Size([2968, 2])
We keep 8.60e+04/1.39e+06 =  6% of the original kernel matrix.

torch.Size([13226, 2])
We keep 8.68e+05/3.18e+07 =  2% of the original kernel matrix.

torch.Size([1125, 2])
We keep 1.62e+04/1.62e+05 = 10% of the original kernel matrix.

torch.Size([9147, 2])
We keep 4.36e+05/1.09e+07 =  4% of the original kernel matrix.

torch.Size([7364, 2])
We keep 1.40e+06/2.75e+07 =  5% of the original kernel matrix.

torch.Size([17143, 2])
We keep 2.43e+06/1.41e+08 =  1% of the original kernel matrix.

torch.Size([13515, 2])
We keep 1.58e+06/4.67e+07 =  3% of the original kernel matrix.

torch.Size([24770, 2])
We keep 2.96e+06/1.84e+08 =  1% of the original kernel matrix.

torch.Size([3858, 2])
We keep 1.58e+05/2.75e+06 =  5% of the original kernel matrix.

torch.Size([14602, 2])
We keep 1.07e+06/4.47e+07 =  2% of the original kernel matrix.

torch.Size([2783, 2])
We keep 8.41e+04/1.19e+06 =  7% of the original kernel matrix.

torch.Size([12972, 2])
We keep 8.30e+05/2.93e+07 =  2% of the original kernel matrix.

torch.Size([2610, 2])
We keep 6.80e+04/1.04e+06 =  6% of the original kernel matrix.

torch.Size([12804, 2])
We keep 7.89e+05/2.75e+07 =  2% of the original kernel matrix.

torch.Size([4456, 2])
We keep 1.88e+05/3.69e+06 =  5% of the original kernel matrix.

torch.Size([15291, 2])
We keep 1.20e+06/5.17e+07 =  2% of the original kernel matrix.

torch.Size([6401, 2])
We keep 9.54e+05/1.69e+07 =  5% of the original kernel matrix.

torch.Size([16541, 2])
We keep 2.04e+06/1.11e+08 =  1% of the original kernel matrix.

torch.Size([2849, 2])
We keep 7.26e+04/1.05e+06 =  6% of the original kernel matrix.

torch.Size([13353, 2])
We keep 7.99e+05/2.76e+07 =  2% of the original kernel matrix.

torch.Size([2227, 2])
We keep 4.58e+04/6.84e+05 =  6% of the original kernel matrix.

torch.Size([12105, 2])
We keep 6.80e+05/2.23e+07 =  3% of the original kernel matrix.

torch.Size([1422, 2])
We keep 2.07e+04/2.21e+05 =  9% of the original kernel matrix.

torch.Size([10406, 2])
We keep 4.90e+05/1.27e+07 =  3% of the original kernel matrix.

torch.Size([1336, 2])
We keep 2.42e+04/2.22e+05 = 10% of the original kernel matrix.

torch.Size([9734, 2])
We keep 4.88e+05/1.27e+07 =  3% of the original kernel matrix.

torch.Size([13310, 2])
We keep 1.32e+06/4.13e+07 =  3% of the original kernel matrix.

torch.Size([24966, 2])
We keep 2.84e+06/1.73e+08 =  1% of the original kernel matrix.

torch.Size([1113, 2])
We keep 1.76e+04/1.71e+05 = 10% of the original kernel matrix.

torch.Size([9201, 2])
We keep 4.47e+05/1.11e+07 =  4% of the original kernel matrix.

torch.Size([10059, 2])
We keep 9.29e+05/2.29e+07 =  4% of the original kernel matrix.

torch.Size([21876, 2])
We keep 2.30e+06/1.29e+08 =  1% of the original kernel matrix.

torch.Size([3771, 2])
We keep 1.24e+05/2.21e+06 =  5% of the original kernel matrix.

torch.Size([14462, 2])
We keep 1.01e+06/4.01e+07 =  2% of the original kernel matrix.

torch.Size([1673, 2])
We keep 3.64e+04/4.52e+05 =  8% of the original kernel matrix.

torch.Size([10655, 2])
We keep 6.05e+05/1.81e+07 =  3% of the original kernel matrix.

torch.Size([2815, 2])
We keep 8.02e+04/1.29e+06 =  6% of the original kernel matrix.

torch.Size([12826, 2])
We keep 8.47e+05/3.06e+07 =  2% of the original kernel matrix.

torch.Size([1049, 2])
We keep 1.74e+04/1.61e+05 = 10% of the original kernel matrix.

torch.Size([8885, 2])
We keep 4.37e+05/1.08e+07 =  4% of the original kernel matrix.

torch.Size([1476, 2])
We keep 2.89e+04/3.26e+05 =  8% of the original kernel matrix.

torch.Size([9938, 2])
We keep 5.46e+05/1.54e+07 =  3% of the original kernel matrix.

torch.Size([1562, 2])
We keep 2.47e+04/3.09e+05 =  8% of the original kernel matrix.

torch.Size([10601, 2])
We keep 5.25e+05/1.50e+07 =  3% of the original kernel matrix.

torch.Size([1606, 2])
We keep 3.47e+04/3.91e+05 =  8% of the original kernel matrix.

torch.Size([10382, 2])
We keep 5.73e+05/1.68e+07 =  3% of the original kernel matrix.

torch.Size([2196, 2])
We keep 8.24e+04/9.98e+05 =  8% of the original kernel matrix.

torch.Size([10988, 2])
We keep 7.64e+05/2.69e+07 =  2% of the original kernel matrix.

torch.Size([1978, 2])
We keep 4.73e+04/6.02e+05 =  7% of the original kernel matrix.

torch.Size([11110, 2])
We keep 6.57e+05/2.09e+07 =  3% of the original kernel matrix.

torch.Size([886, 2])
We keep 6.84e+03/5.48e+04 = 12% of the original kernel matrix.

torch.Size([8697, 2])
We keep 3.07e+05/6.31e+06 =  4% of the original kernel matrix.

torch.Size([1679, 2])
We keep 5.44e+04/5.69e+05 =  9% of the original kernel matrix.

torch.Size([10171, 2])
We keep 6.52e+05/2.03e+07 =  3% of the original kernel matrix.

torch.Size([2079, 2])
We keep 4.85e+04/6.84e+05 =  7% of the original kernel matrix.

torch.Size([11532, 2])
We keep 6.92e+05/2.23e+07 =  3% of the original kernel matrix.

torch.Size([4751, 2])
We keep 2.31e+05/4.23e+06 =  5% of the original kernel matrix.

torch.Size([15744, 2])
We keep 1.26e+06/5.54e+07 =  2% of the original kernel matrix.

torch.Size([1140, 2])
We keep 1.28e+04/1.19e+05 = 10% of the original kernel matrix.

torch.Size([9492, 2])
We keep 3.97e+05/9.30e+06 =  4% of the original kernel matrix.

torch.Size([570, 2])
We keep 6.26e+03/4.16e+04 = 15% of the original kernel matrix.

torch.Size([7029, 2])
We keep 2.86e+05/5.50e+06 =  5% of the original kernel matrix.

torch.Size([1173, 2])
We keep 2.18e+04/2.13e+05 = 10% of the original kernel matrix.

torch.Size([9195, 2])
We keep 4.63e+05/1.24e+07 =  3% of the original kernel matrix.

torch.Size([5113, 2])
We keep 2.39e+05/4.83e+06 =  4% of the original kernel matrix.

torch.Size([16714, 2])
We keep 1.36e+06/5.92e+07 =  2% of the original kernel matrix.

torch.Size([2974, 2])
We keep 8.10e+04/1.27e+06 =  6% of the original kernel matrix.

torch.Size([13289, 2])
We keep 8.51e+05/3.04e+07 =  2% of the original kernel matrix.

torch.Size([428, 2])
We keep 2.39e+03/1.39e+04 = 17% of the original kernel matrix.

torch.Size([6743, 2])
We keep 2.04e+05/3.18e+06 =  6% of the original kernel matrix.

torch.Size([6829, 2])
We keep 4.32e+05/9.40e+06 =  4% of the original kernel matrix.

torch.Size([18618, 2])
We keep 1.70e+06/8.26e+07 =  2% of the original kernel matrix.

torch.Size([758, 2])
We keep 8.40e+03/5.52e+04 = 15% of the original kernel matrix.

torch.Size([8183, 2])
We keep 3.26e+05/6.33e+06 =  5% of the original kernel matrix.

torch.Size([1097, 2])
We keep 1.68e+04/1.66e+05 = 10% of the original kernel matrix.

torch.Size([9273, 2])
We keep 4.50e+05/1.10e+07 =  4% of the original kernel matrix.

torch.Size([1639, 2])
We keep 3.12e+04/3.86e+05 =  8% of the original kernel matrix.

torch.Size([10660, 2])
We keep 5.73e+05/1.67e+07 =  3% of the original kernel matrix.

torch.Size([2039, 2])
We keep 7.60e+04/8.30e+05 =  9% of the original kernel matrix.

torch.Size([10989, 2])
We keep 7.43e+05/2.45e+07 =  3% of the original kernel matrix.

torch.Size([496, 2])
We keep 5.51e+03/3.03e+04 = 18% of the original kernel matrix.

torch.Size([6957, 2])
We keep 2.74e+05/4.69e+06 =  5% of the original kernel matrix.

torch.Size([3216, 2])
We keep 1.16e+05/1.62e+06 =  7% of the original kernel matrix.

torch.Size([13523, 2])
We keep 9.13e+05/3.43e+07 =  2% of the original kernel matrix.

torch.Size([4035, 2])
We keep 1.42e+05/2.56e+06 =  5% of the original kernel matrix.

torch.Size([14917, 2])
We keep 1.06e+06/4.31e+07 =  2% of the original kernel matrix.

torch.Size([1067, 2])
We keep 1.43e+04/1.47e+05 =  9% of the original kernel matrix.

torch.Size([8750, 2])
We keep 4.11e+05/1.03e+07 =  3% of the original kernel matrix.

torch.Size([695, 2])
We keep 5.28e+03/4.00e+04 = 13% of the original kernel matrix.

torch.Size([8169, 2])
We keep 2.89e+05/5.39e+06 =  5% of the original kernel matrix.

torch.Size([831, 2])
We keep 8.17e+03/5.90e+04 = 13% of the original kernel matrix.

torch.Size([8628, 2])
We keep 3.32e+05/6.55e+06 =  5% of the original kernel matrix.

torch.Size([4340, 2])
We keep 1.76e+05/3.25e+06 =  5% of the original kernel matrix.

torch.Size([15288, 2])
We keep 1.15e+06/4.86e+07 =  2% of the original kernel matrix.

torch.Size([1791, 2])
We keep 3.90e+04/4.84e+05 =  8% of the original kernel matrix.

torch.Size([10593, 2])
We keep 6.11e+05/1.88e+07 =  3% of the original kernel matrix.

torch.Size([2202, 2])
We keep 5.02e+04/6.94e+05 =  7% of the original kernel matrix.

torch.Size([11884, 2])
We keep 6.99e+05/2.24e+07 =  3% of the original kernel matrix.

torch.Size([13510, 2])
We keep 1.31e+06/3.95e+07 =  3% of the original kernel matrix.

torch.Size([26214, 2])
We keep 2.86e+06/1.69e+08 =  1% of the original kernel matrix.

torch.Size([5220, 2])
We keep 4.86e+05/7.98e+06 =  6% of the original kernel matrix.

torch.Size([15841, 2])
We keep 1.58e+06/7.61e+07 =  2% of the original kernel matrix.

torch.Size([5512, 2])
We keep 3.00e+05/6.44e+06 =  4% of the original kernel matrix.

torch.Size([16626, 2])
We keep 1.45e+06/6.84e+07 =  2% of the original kernel matrix.

torch.Size([3112, 2])
We keep 6.54e+04/1.13e+06 =  5% of the original kernel matrix.

torch.Size([14012, 2])
We keep 8.05e+05/2.87e+07 =  2% of the original kernel matrix.

torch.Size([9654, 2])
We keep 7.85e+05/2.10e+07 =  3% of the original kernel matrix.

torch.Size([21005, 2])
We keep 2.20e+06/1.24e+08 =  1% of the original kernel matrix.

torch.Size([2751, 2])
We keep 9.65e+04/1.32e+06 =  7% of the original kernel matrix.

torch.Size([12634, 2])
We keep 8.56e+05/3.10e+07 =  2% of the original kernel matrix.

torch.Size([1203, 2])
We keep 2.50e+04/2.65e+05 =  9% of the original kernel matrix.

torch.Size([9107, 2])
We keep 5.06e+05/1.39e+07 =  3% of the original kernel matrix.

torch.Size([1491, 2])
We keep 2.51e+04/2.75e+05 =  9% of the original kernel matrix.

torch.Size([10084, 2])
We keep 5.21e+05/1.41e+07 =  3% of the original kernel matrix.

torch.Size([2050, 2])
We keep 5.25e+04/6.82e+05 =  7% of the original kernel matrix.

torch.Size([11222, 2])
We keep 6.87e+05/2.23e+07 =  3% of the original kernel matrix.

torch.Size([768, 2])
We keep 1.10e+04/7.08e+04 = 15% of the original kernel matrix.

torch.Size([8084, 2])
We keep 3.49e+05/7.17e+06 =  4% of the original kernel matrix.

torch.Size([4679, 2])
We keep 2.10e+05/3.86e+06 =  5% of the original kernel matrix.

torch.Size([15632, 2])
We keep 1.22e+06/5.30e+07 =  2% of the original kernel matrix.

torch.Size([375, 2])
We keep 1.64e+03/8.46e+03 = 19% of the original kernel matrix.

torch.Size([6807, 2])
We keep 1.83e+05/2.48e+06 =  7% of the original kernel matrix.

torch.Size([1456, 2])
We keep 2.90e+04/3.06e+05 =  9% of the original kernel matrix.

torch.Size([10080, 2])
We keep 5.33e+05/1.49e+07 =  3% of the original kernel matrix.

torch.Size([1934, 2])
We keep 5.32e+04/5.82e+05 =  9% of the original kernel matrix.

torch.Size([11107, 2])
We keep 6.65e+05/2.06e+07 =  3% of the original kernel matrix.

torch.Size([1847, 2])
We keep 4.00e+04/4.69e+05 =  8% of the original kernel matrix.

torch.Size([10721, 2])
We keep 5.99e+05/1.85e+07 =  3% of the original kernel matrix.

torch.Size([960, 2])
We keep 1.37e+04/1.16e+05 = 11% of the original kernel matrix.

torch.Size([8553, 2])
We keep 4.00e+05/9.19e+06 =  4% of the original kernel matrix.

torch.Size([1835, 2])
We keep 5.50e+04/6.79e+05 =  8% of the original kernel matrix.

torch.Size([10775, 2])
We keep 6.94e+05/2.22e+07 =  3% of the original kernel matrix.

torch.Size([3156, 2])
We keep 1.28e+05/1.88e+06 =  6% of the original kernel matrix.

torch.Size([13356, 2])
We keep 9.67e+05/3.69e+07 =  2% of the original kernel matrix.

torch.Size([3132, 2])
We keep 1.07e+05/1.71e+06 =  6% of the original kernel matrix.

torch.Size([13292, 2])
We keep 9.18e+05/3.52e+07 =  2% of the original kernel matrix.

torch.Size([1129, 2])
We keep 2.51e+04/2.36e+05 = 10% of the original kernel matrix.

torch.Size([8722, 2])
We keep 4.98e+05/1.31e+07 =  3% of the original kernel matrix.

torch.Size([1006, 2])
We keep 1.43e+04/1.27e+05 = 11% of the original kernel matrix.

torch.Size([9030, 2])
We keep 4.06e+05/9.59e+06 =  4% of the original kernel matrix.

torch.Size([686, 2])
We keep 5.87e+03/4.20e+04 = 13% of the original kernel matrix.

torch.Size([8096, 2])
We keep 2.99e+05/5.52e+06 =  5% of the original kernel matrix.

torch.Size([5301, 2])
We keep 2.71e+05/5.41e+06 =  5% of the original kernel matrix.

torch.Size([16442, 2])
We keep 1.37e+06/6.27e+07 =  2% of the original kernel matrix.

torch.Size([679, 2])
We keep 5.52e+03/3.92e+04 = 14% of the original kernel matrix.

torch.Size([8037, 2])
We keep 2.90e+05/5.34e+06 =  5% of the original kernel matrix.

torch.Size([6820, 2])
We keep 3.95e+05/8.73e+06 =  4% of the original kernel matrix.

torch.Size([18277, 2])
We keep 1.61e+06/7.96e+07 =  2% of the original kernel matrix.

torch.Size([1563, 2])
We keep 3.95e+04/4.42e+05 =  8% of the original kernel matrix.

torch.Size([10213, 2])
We keep 6.05e+05/1.79e+07 =  3% of the original kernel matrix.

torch.Size([1526, 2])
We keep 2.17e+04/2.22e+05 =  9% of the original kernel matrix.

torch.Size([10513, 2])
We keep 4.84e+05/1.27e+07 =  3% of the original kernel matrix.

torch.Size([2680, 2])
We keep 6.30e+04/1.03e+06 =  6% of the original kernel matrix.

torch.Size([13051, 2])
We keep 7.96e+05/2.74e+07 =  2% of the original kernel matrix.

torch.Size([1842, 2])
We keep 3.27e+04/3.86e+05 =  8% of the original kernel matrix.

torch.Size([11264, 2])
We keep 5.67e+05/1.67e+07 =  3% of the original kernel matrix.

torch.Size([2755, 2])
We keep 6.73e+04/1.06e+06 =  6% of the original kernel matrix.

torch.Size([12791, 2])
We keep 7.88e+05/2.77e+07 =  2% of the original kernel matrix.

torch.Size([1193, 2])
We keep 1.42e+04/1.44e+05 =  9% of the original kernel matrix.

torch.Size([9871, 2])
We keep 4.21e+05/1.02e+07 =  4% of the original kernel matrix.

torch.Size([763, 2])
We keep 8.08e+03/6.50e+04 = 12% of the original kernel matrix.

torch.Size([8311, 2])
We keep 3.34e+05/6.87e+06 =  4% of the original kernel matrix.

torch.Size([14023, 2])
We keep 1.26e+06/4.40e+07 =  2% of the original kernel matrix.

torch.Size([25322, 2])
We keep 2.89e+06/1.79e+08 =  1% of the original kernel matrix.

torch.Size([2659, 2])
We keep 7.40e+04/1.12e+06 =  6% of the original kernel matrix.

torch.Size([12628, 2])
We keep 8.00e+05/2.85e+07 =  2% of the original kernel matrix.

torch.Size([5749, 2])
We keep 3.78e+05/7.33e+06 =  5% of the original kernel matrix.

torch.Size([17010, 2])
We keep 1.53e+06/7.30e+07 =  2% of the original kernel matrix.

torch.Size([1516, 2])
We keep 3.25e+04/3.35e+05 =  9% of the original kernel matrix.

torch.Size([10382, 2])
We keep 5.58e+05/1.56e+07 =  3% of the original kernel matrix.

torch.Size([3074, 2])
We keep 1.05e+05/1.55e+06 =  6% of the original kernel matrix.

torch.Size([13211, 2])
We keep 9.01e+05/3.36e+07 =  2% of the original kernel matrix.

torch.Size([1052, 2])
We keep 2.01e+04/1.51e+05 = 13% of the original kernel matrix.

torch.Size([8749, 2])
We keep 4.35e+05/1.05e+07 =  4% of the original kernel matrix.

torch.Size([1802, 2])
We keep 2.88e+04/3.68e+05 =  7% of the original kernel matrix.

torch.Size([11123, 2])
We keep 5.65e+05/1.64e+07 =  3% of the original kernel matrix.

torch.Size([1350, 2])
We keep 2.16e+04/2.27e+05 =  9% of the original kernel matrix.

torch.Size([9806, 2])
We keep 4.89e+05/1.28e+07 =  3% of the original kernel matrix.

torch.Size([743, 2])
We keep 8.36e+03/5.90e+04 = 14% of the original kernel matrix.

torch.Size([8081, 2])
We keep 3.28e+05/6.55e+06 =  5% of the original kernel matrix.

torch.Size([2647, 2])
We keep 9.85e+04/1.40e+06 =  7% of the original kernel matrix.

torch.Size([12297, 2])
We keep 8.70e+05/3.19e+07 =  2% of the original kernel matrix.

torch.Size([1464, 2])
We keep 1.83e+04/1.98e+05 =  9% of the original kernel matrix.

torch.Size([10452, 2])
We keep 4.69e+05/1.20e+07 =  3% of the original kernel matrix.

torch.Size([2015, 2])
We keep 6.32e+04/6.27e+05 = 10% of the original kernel matrix.

torch.Size([11348, 2])
We keep 6.64e+05/2.13e+07 =  3% of the original kernel matrix.

torch.Size([2488, 2])
We keep 5.08e+04/7.57e+05 =  6% of the original kernel matrix.

torch.Size([12513, 2])
We keep 7.12e+05/2.34e+07 =  3% of the original kernel matrix.

torch.Size([519, 2])
We keep 4.46e+03/2.76e+04 = 16% of the original kernel matrix.

torch.Size([7356, 2])
We keep 2.62e+05/4.47e+06 =  5% of the original kernel matrix.

torch.Size([4145, 2])
We keep 1.65e+05/3.19e+06 =  5% of the original kernel matrix.

torch.Size([14957, 2])
We keep 1.14e+06/4.81e+07 =  2% of the original kernel matrix.

torch.Size([16017, 2])
We keep 3.24e+06/9.82e+07 =  3% of the original kernel matrix.

torch.Size([26652, 2])
We keep 3.93e+06/2.67e+08 =  1% of the original kernel matrix.

torch.Size([6056, 2])
We keep 3.77e+05/8.02e+06 =  4% of the original kernel matrix.

torch.Size([17358, 2])
We keep 1.59e+06/7.63e+07 =  2% of the original kernel matrix.

torch.Size([970, 2])
We keep 1.70e+04/1.50e+05 = 11% of the original kernel matrix.

torch.Size([8669, 2])
We keep 4.32e+05/1.04e+07 =  4% of the original kernel matrix.

torch.Size([1551, 2])
We keep 2.26e+04/2.62e+05 =  8% of the original kernel matrix.

torch.Size([10688, 2])
We keep 5.11e+05/1.38e+07 =  3% of the original kernel matrix.

torch.Size([1621, 2])
We keep 3.79e+04/3.92e+05 =  9% of the original kernel matrix.

torch.Size([10574, 2])
We keep 5.89e+05/1.69e+07 =  3% of the original kernel matrix.

torch.Size([1556, 2])
We keep 2.80e+04/3.28e+05 =  8% of the original kernel matrix.

torch.Size([10369, 2])
We keep 5.47e+05/1.54e+07 =  3% of the original kernel matrix.

torch.Size([1417, 2])
We keep 2.49e+04/2.61e+05 =  9% of the original kernel matrix.

torch.Size([10030, 2])
We keep 5.09e+05/1.38e+07 =  3% of the original kernel matrix.

torch.Size([1811, 2])
We keep 4.06e+04/5.13e+05 =  7% of the original kernel matrix.

torch.Size([10586, 2])
We keep 6.03e+05/1.93e+07 =  3% of the original kernel matrix.

torch.Size([9507, 2])
We keep 8.79e+05/2.28e+07 =  3% of the original kernel matrix.

torch.Size([21549, 2])
We keep 2.36e+06/1.29e+08 =  1% of the original kernel matrix.

torch.Size([4498, 2])
We keep 1.46e+05/3.07e+06 =  4% of the original kernel matrix.

torch.Size([15669, 2])
We keep 1.11e+06/4.72e+07 =  2% of the original kernel matrix.

torch.Size([3745, 2])
We keep 3.88e+05/5.53e+06 =  7% of the original kernel matrix.

torch.Size([12822, 2])
We keep 1.37e+06/6.34e+07 =  2% of the original kernel matrix.

torch.Size([2026, 2])
We keep 4.06e+04/5.70e+05 =  7% of the original kernel matrix.

torch.Size([11512, 2])
We keep 6.43e+05/2.03e+07 =  3% of the original kernel matrix.

torch.Size([11325, 2])
We keep 8.10e+05/2.52e+07 =  3% of the original kernel matrix.

torch.Size([22742, 2])
We keep 2.33e+06/1.35e+08 =  1% of the original kernel matrix.

torch.Size([1821, 2])
We keep 2.85e+04/3.52e+05 =  8% of the original kernel matrix.

torch.Size([11265, 2])
We keep 5.61e+05/1.60e+07 =  3% of the original kernel matrix.

torch.Size([2037, 2])
We keep 4.04e+04/5.04e+05 =  8% of the original kernel matrix.

torch.Size([11618, 2])
We keep 6.26e+05/1.91e+07 =  3% of the original kernel matrix.

torch.Size([1965, 2])
We keep 4.96e+04/5.64e+05 =  8% of the original kernel matrix.

torch.Size([11290, 2])
We keep 6.51e+05/2.02e+07 =  3% of the original kernel matrix.

torch.Size([243, 2])
We keep 1.82e+03/7.06e+03 = 25% of the original kernel matrix.

torch.Size([5375, 2])
We keep 1.77e+05/2.26e+06 =  7% of the original kernel matrix.

torch.Size([4177, 2])
We keep 2.09e+05/3.46e+06 =  6% of the original kernel matrix.

torch.Size([14802, 2])
We keep 1.17e+06/5.01e+07 =  2% of the original kernel matrix.

torch.Size([718, 2])
We keep 7.28e+03/5.06e+04 = 14% of the original kernel matrix.

torch.Size([7678, 2])
We keep 3.07e+05/6.06e+06 =  5% of the original kernel matrix.

torch.Size([3276, 2])
We keep 1.30e+05/2.04e+06 =  6% of the original kernel matrix.

torch.Size([13428, 2])
We keep 9.72e+05/3.85e+07 =  2% of the original kernel matrix.

torch.Size([1117, 2])
We keep 2.46e+04/1.92e+05 = 12% of the original kernel matrix.

torch.Size([8966, 2])
We keep 4.73e+05/1.18e+07 =  4% of the original kernel matrix.

torch.Size([1813, 2])
We keep 4.02e+04/4.80e+05 =  8% of the original kernel matrix.

torch.Size([10800, 2])
We keep 6.17e+05/1.87e+07 =  3% of the original kernel matrix.

torch.Size([1298, 2])
We keep 1.88e+04/1.99e+05 =  9% of the original kernel matrix.

torch.Size([9839, 2])
We keep 4.74e+05/1.20e+07 =  3% of the original kernel matrix.

torch.Size([1343, 2])
We keep 2.18e+04/2.33e+05 =  9% of the original kernel matrix.

torch.Size([10136, 2])
We keep 4.97e+05/1.30e+07 =  3% of the original kernel matrix.

torch.Size([861, 2])
We keep 1.02e+04/9.36e+04 = 10% of the original kernel matrix.

torch.Size([8418, 2])
We keep 3.62e+05/8.25e+06 =  4% of the original kernel matrix.

torch.Size([1737, 2])
We keep 3.44e+04/4.28e+05 =  8% of the original kernel matrix.

torch.Size([10940, 2])
We keep 5.95e+05/1.76e+07 =  3% of the original kernel matrix.

torch.Size([277, 2])
We keep 1.10e+03/4.76e+03 = 22% of the original kernel matrix.

torch.Size([6041, 2])
We keep 1.54e+05/1.86e+06 =  8% of the original kernel matrix.

torch.Size([629, 2])
We keep 5.99e+03/3.88e+04 = 15% of the original kernel matrix.

torch.Size([7534, 2])
We keep 2.88e+05/5.31e+06 =  5% of the original kernel matrix.

torch.Size([1654, 2])
We keep 3.35e+04/3.93e+05 =  8% of the original kernel matrix.

torch.Size([10544, 2])
We keep 5.88e+05/1.69e+07 =  3% of the original kernel matrix.

torch.Size([3978, 2])
We keep 1.22e+05/2.27e+06 =  5% of the original kernel matrix.

torch.Size([14973, 2])
We keep 1.02e+06/4.06e+07 =  2% of the original kernel matrix.

torch.Size([1950, 2])
We keep 4.60e+04/5.75e+05 =  8% of the original kernel matrix.

torch.Size([11254, 2])
We keep 6.59e+05/2.04e+07 =  3% of the original kernel matrix.

torch.Size([4500, 2])
We keep 2.50e+05/4.41e+06 =  5% of the original kernel matrix.

torch.Size([15507, 2])
We keep 1.30e+06/5.66e+07 =  2% of the original kernel matrix.

torch.Size([1499, 2])
We keep 2.12e+04/2.36e+05 =  8% of the original kernel matrix.

torch.Size([10571, 2])
We keep 4.98e+05/1.31e+07 =  3% of the original kernel matrix.

torch.Size([1179, 2])
We keep 2.47e+04/2.36e+05 = 10% of the original kernel matrix.

torch.Size([9105, 2])
We keep 4.95e+05/1.31e+07 =  3% of the original kernel matrix.

torch.Size([5334, 2])
We keep 2.33e+05/4.73e+06 =  4% of the original kernel matrix.

torch.Size([16602, 2])
We keep 1.30e+06/5.86e+07 =  2% of the original kernel matrix.

torch.Size([2722, 2])
We keep 1.36e+05/1.84e+06 =  7% of the original kernel matrix.

torch.Size([12173, 2])
We keep 9.57e+05/3.65e+07 =  2% of the original kernel matrix.

torch.Size([1314, 2])
We keep 2.90e+04/2.80e+05 = 10% of the original kernel matrix.

torch.Size([9575, 2])
We keep 5.32e+05/1.43e+07 =  3% of the original kernel matrix.

torch.Size([5968, 2])
We keep 2.41e+05/5.63e+06 =  4% of the original kernel matrix.

torch.Size([17496, 2])
We keep 1.37e+06/6.39e+07 =  2% of the original kernel matrix.

torch.Size([2258, 2])
We keep 4.68e+04/6.76e+05 =  6% of the original kernel matrix.

torch.Size([12136, 2])
We keep 6.85e+05/2.22e+07 =  3% of the original kernel matrix.

torch.Size([1996, 2])
We keep 1.11e+05/1.23e+06 =  9% of the original kernel matrix.

torch.Size([10367, 2])
We keep 8.36e+05/2.99e+07 =  2% of the original kernel matrix.

torch.Size([1092, 2])
We keep 1.91e+04/1.66e+05 = 11% of the original kernel matrix.

torch.Size([9038, 2])
We keep 4.48e+05/1.10e+07 =  4% of the original kernel matrix.

torch.Size([8547, 2])
We keep 5.47e+05/1.34e+07 =  4% of the original kernel matrix.

torch.Size([20325, 2])
We keep 1.88e+06/9.88e+07 =  1% of the original kernel matrix.

torch.Size([4106, 2])
We keep 1.61e+05/2.91e+06 =  5% of the original kernel matrix.

torch.Size([14940, 2])
We keep 1.11e+06/4.59e+07 =  2% of the original kernel matrix.

torch.Size([2200, 2])
We keep 5.02e+04/7.02e+05 =  7% of the original kernel matrix.

torch.Size([11740, 2])
We keep 6.96e+05/2.26e+07 =  3% of the original kernel matrix.

torch.Size([1860, 2])
We keep 7.61e+04/8.10e+05 =  9% of the original kernel matrix.

torch.Size([10581, 2])
We keep 7.36e+05/2.43e+07 =  3% of the original kernel matrix.

torch.Size([5297, 2])
We keep 2.55e+05/5.54e+06 =  4% of the original kernel matrix.

torch.Size([16877, 2])
We keep 1.40e+06/6.34e+07 =  2% of the original kernel matrix.

torch.Size([13721, 2])
We keep 2.50e+06/6.34e+07 =  3% of the original kernel matrix.

torch.Size([24576, 2])
We keep 3.33e+06/2.15e+08 =  1% of the original kernel matrix.

torch.Size([1050, 2])
We keep 2.02e+04/1.69e+05 = 11% of the original kernel matrix.

torch.Size([8869, 2])
We keep 4.42e+05/1.11e+07 =  3% of the original kernel matrix.

torch.Size([721, 2])
We keep 9.68e+03/6.86e+04 = 14% of the original kernel matrix.

torch.Size([8055, 2])
We keep 3.50e+05/7.06e+06 =  4% of the original kernel matrix.

torch.Size([1192, 2])
We keep 1.66e+04/1.54e+05 = 10% of the original kernel matrix.

torch.Size([9657, 2])
We keep 4.40e+05/1.06e+07 =  4% of the original kernel matrix.

torch.Size([554, 2])
We keep 4.06e+03/2.79e+04 = 14% of the original kernel matrix.

torch.Size([7733, 2])
We keep 2.70e+05/4.50e+06 =  5% of the original kernel matrix.

torch.Size([2237, 2])
We keep 9.29e+04/1.11e+06 =  8% of the original kernel matrix.

torch.Size([11223, 2])
We keep 8.12e+05/2.83e+07 =  2% of the original kernel matrix.

torch.Size([1563, 2])
We keep 3.21e+04/3.55e+05 =  9% of the original kernel matrix.

torch.Size([10286, 2])
We keep 5.72e+05/1.61e+07 =  3% of the original kernel matrix.

torch.Size([706, 2])
We keep 5.91e+03/5.11e+04 = 11% of the original kernel matrix.

torch.Size([8161, 2])
We keep 3.04e+05/6.09e+06 =  4% of the original kernel matrix.

torch.Size([830, 2])
We keep 1.93e+04/1.46e+05 = 13% of the original kernel matrix.

torch.Size([7965, 2])
We keep 4.30e+05/1.03e+07 =  4% of the original kernel matrix.

torch.Size([2923, 2])
We keep 7.84e+04/1.31e+06 =  5% of the original kernel matrix.

torch.Size([13163, 2])
We keep 8.52e+05/3.09e+07 =  2% of the original kernel matrix.

torch.Size([729, 2])
We keep 8.29e+03/6.25e+04 = 13% of the original kernel matrix.

torch.Size([7999, 2])
We keep 3.32e+05/6.74e+06 =  4% of the original kernel matrix.

torch.Size([13016, 2])
We keep 1.14e+06/4.04e+07 =  2% of the original kernel matrix.

torch.Size([24790, 2])
We keep 2.84e+06/1.71e+08 =  1% of the original kernel matrix.

torch.Size([1010, 2])
We keep 1.26e+04/1.08e+05 = 11% of the original kernel matrix.

torch.Size([8834, 2])
We keep 3.80e+05/8.84e+06 =  4% of the original kernel matrix.

torch.Size([10522, 2])
We keep 7.87e+05/2.11e+07 =  3% of the original kernel matrix.

torch.Size([22070, 2])
We keep 2.21e+06/1.24e+08 =  1% of the original kernel matrix.

torch.Size([6454, 2])
We keep 3.99e+05/8.62e+06 =  4% of the original kernel matrix.

torch.Size([18640, 2])
We keep 1.68e+06/7.91e+07 =  2% of the original kernel matrix.

torch.Size([2164, 2])
We keep 5.01e+04/6.58e+05 =  7% of the original kernel matrix.

torch.Size([11797, 2])
We keep 6.91e+05/2.19e+07 =  3% of the original kernel matrix.

torch.Size([323, 2])
We keep 1.58e+03/7.92e+03 = 19% of the original kernel matrix.

torch.Size([6409, 2])
We keep 1.81e+05/2.40e+06 =  7% of the original kernel matrix.

torch.Size([5153, 2])
We keep 2.25e+05/4.54e+06 =  4% of the original kernel matrix.

torch.Size([16802, 2])
We keep 1.31e+06/5.74e+07 =  2% of the original kernel matrix.

torch.Size([3530, 2])
We keep 1.19e+05/2.02e+06 =  5% of the original kernel matrix.

torch.Size([14275, 2])
We keep 9.79e+05/3.83e+07 =  2% of the original kernel matrix.

torch.Size([974, 2])
We keep 3.17e+04/2.57e+05 = 12% of the original kernel matrix.

torch.Size([8249, 2])
We keep 5.12e+05/1.37e+07 =  3% of the original kernel matrix.

torch.Size([8190, 2])
We keep 5.62e+05/1.44e+07 =  3% of the original kernel matrix.

torch.Size([20151, 2])
We keep 1.97e+06/1.02e+08 =  1% of the original kernel matrix.

torch.Size([13630, 2])
We keep 1.21e+06/4.29e+07 =  2% of the original kernel matrix.

torch.Size([24821, 2])
We keep 2.82e+06/1.77e+08 =  1% of the original kernel matrix.

torch.Size([732, 2])
We keep 7.28e+03/5.24e+04 = 13% of the original kernel matrix.

torch.Size([8310, 2])
We keep 3.16e+05/6.17e+06 =  5% of the original kernel matrix.

torch.Size([689, 2])
We keep 6.22e+03/4.24e+04 = 14% of the original kernel matrix.

torch.Size([8026, 2])
We keep 3.03e+05/5.55e+06 =  5% of the original kernel matrix.

torch.Size([891, 2])
We keep 9.86e+03/7.56e+04 = 13% of the original kernel matrix.

torch.Size([8725, 2])
We keep 3.55e+05/7.41e+06 =  4% of the original kernel matrix.

torch.Size([1231, 2])
We keep 2.00e+04/1.93e+05 = 10% of the original kernel matrix.

torch.Size([9614, 2])
We keep 4.67e+05/1.18e+07 =  3% of the original kernel matrix.

torch.Size([854, 2])
We keep 9.77e+03/6.81e+04 = 14% of the original kernel matrix.

torch.Size([8348, 2])
We keep 3.46e+05/7.03e+06 =  4% of the original kernel matrix.

torch.Size([446, 2])
We keep 3.42e+03/2.10e+04 = 16% of the original kernel matrix.

torch.Size([7064, 2])
We keep 2.45e+05/3.91e+06 =  6% of the original kernel matrix.

torch.Size([469, 2])
We keep 2.89e+03/1.85e+04 = 15% of the original kernel matrix.

torch.Size([7192, 2])
We keep 2.35e+05/3.66e+06 =  6% of the original kernel matrix.

torch.Size([1962, 2])
We keep 3.99e+04/4.89e+05 =  8% of the original kernel matrix.

torch.Size([11144, 2])
We keep 6.00e+05/1.88e+07 =  3% of the original kernel matrix.

torch.Size([1548, 2])
We keep 3.30e+04/3.53e+05 =  9% of the original kernel matrix.

torch.Size([10440, 2])
We keep 5.61e+05/1.60e+07 =  3% of the original kernel matrix.

torch.Size([1421, 2])
We keep 8.18e+04/7.92e+05 = 10% of the original kernel matrix.

torch.Size([8670, 2])
We keep 7.27e+05/2.40e+07 =  3% of the original kernel matrix.

torch.Size([1340, 2])
We keep 2.64e+04/2.84e+05 =  9% of the original kernel matrix.

torch.Size([9915, 2])
We keep 5.25e+05/1.44e+07 =  3% of the original kernel matrix.

torch.Size([3131, 2])
We keep 1.02e+05/1.55e+06 =  6% of the original kernel matrix.

torch.Size([13447, 2])
We keep 9.06e+05/3.36e+07 =  2% of the original kernel matrix.

torch.Size([1604, 2])
We keep 3.59e+04/3.64e+05 =  9% of the original kernel matrix.

torch.Size([10282, 2])
We keep 5.63e+05/1.62e+07 =  3% of the original kernel matrix.

torch.Size([3405, 2])
We keep 1.76e+05/2.64e+06 =  6% of the original kernel matrix.

torch.Size([13339, 2])
We keep 1.06e+06/4.38e+07 =  2% of the original kernel matrix.

torch.Size([1934, 2])
We keep 3.79e+04/4.94e+05 =  7% of the original kernel matrix.

torch.Size([11313, 2])
We keep 6.13e+05/1.89e+07 =  3% of the original kernel matrix.

torch.Size([1497, 2])
We keep 3.42e+04/4.02e+05 =  8% of the original kernel matrix.

torch.Size([9913, 2])
We keep 5.82e+05/1.71e+07 =  3% of the original kernel matrix.

torch.Size([406, 2])
We keep 3.76e+03/1.93e+04 = 19% of the original kernel matrix.

torch.Size([6440, 2])
We keep 2.36e+05/3.75e+06 =  6% of the original kernel matrix.

torch.Size([4115, 2])
We keep 1.75e+05/3.08e+06 =  5% of the original kernel matrix.

torch.Size([14629, 2])
We keep 1.12e+06/4.73e+07 =  2% of the original kernel matrix.

torch.Size([717, 2])
We keep 8.90e+03/6.40e+04 = 13% of the original kernel matrix.

torch.Size([7624, 2])
We keep 3.30e+05/6.82e+06 =  4% of the original kernel matrix.

torch.Size([507, 2])
We keep 2.99e+03/1.74e+04 = 17% of the original kernel matrix.

torch.Size([7451, 2])
We keep 2.26e+05/3.56e+06 =  6% of the original kernel matrix.

torch.Size([1641, 2])
We keep 4.09e+04/4.03e+05 = 10% of the original kernel matrix.

torch.Size([10555, 2])
We keep 5.84e+05/1.71e+07 =  3% of the original kernel matrix.

torch.Size([1149, 2])
We keep 1.61e+04/1.52e+05 = 10% of the original kernel matrix.

torch.Size([9270, 2])
We keep 4.26e+05/1.05e+07 =  4% of the original kernel matrix.

torch.Size([9204, 2])
We keep 5.89e+05/1.69e+07 =  3% of the original kernel matrix.

torch.Size([21731, 2])
We keep 2.08e+06/1.11e+08 =  1% of the original kernel matrix.

torch.Size([2342, 2])
We keep 5.37e+04/7.53e+05 =  7% of the original kernel matrix.

torch.Size([12070, 2])
We keep 7.17e+05/2.34e+07 =  3% of the original kernel matrix.

torch.Size([917, 2])
We keep 2.50e+04/1.94e+05 = 12% of the original kernel matrix.

torch.Size([7779, 2])
We keep 4.67e+05/1.19e+07 =  3% of the original kernel matrix.

torch.Size([2280, 2])
We keep 5.70e+04/7.29e+05 =  7% of the original kernel matrix.

torch.Size([12089, 2])
We keep 7.10e+05/2.30e+07 =  3% of the original kernel matrix.

torch.Size([4029, 2])
We keep 1.68e+05/3.07e+06 =  5% of the original kernel matrix.

torch.Size([14814, 2])
We keep 1.14e+06/4.72e+07 =  2% of the original kernel matrix.

torch.Size([774, 2])
We keep 1.01e+04/7.62e+04 = 13% of the original kernel matrix.

torch.Size([8111, 2])
We keep 3.56e+05/7.44e+06 =  4% of the original kernel matrix.

torch.Size([6327, 2])
We keep 3.97e+05/8.08e+06 =  4% of the original kernel matrix.

torch.Size([17515, 2])
We keep 1.57e+06/7.66e+07 =  2% of the original kernel matrix.

torch.Size([16166, 2])
We keep 1.44e+07/2.79e+08 =  5% of the original kernel matrix.

torch.Size([25513, 2])
We keep 5.88e+06/4.50e+08 =  1% of the original kernel matrix.

torch.Size([3093, 2])
We keep 4.46e+05/3.79e+06 = 11% of the original kernel matrix.

torch.Size([13049, 2])
We keep 1.18e+06/5.25e+07 =  2% of the original kernel matrix.

torch.Size([4215, 2])
We keep 1.53e+05/2.91e+06 =  5% of the original kernel matrix.

torch.Size([15075, 2])
We keep 1.09e+06/4.60e+07 =  2% of the original kernel matrix.

torch.Size([375, 2])
We keep 2.36e+03/1.21e+04 = 19% of the original kernel matrix.

torch.Size([6313, 2])
We keep 1.95e+05/2.96e+06 =  6% of the original kernel matrix.

torch.Size([6575, 2])
We keep 3.79e+05/7.87e+06 =  4% of the original kernel matrix.

torch.Size([17824, 2])
We keep 1.52e+06/7.56e+07 =  2% of the original kernel matrix.

torch.Size([33537, 2])
We keep 2.85e+07/4.87e+08 =  5% of the original kernel matrix.

torch.Size([38935, 2])
We keep 7.11e+06/5.94e+08 =  1% of the original kernel matrix.

torch.Size([71271, 2])
We keep 1.42e+08/5.29e+09 =  2% of the original kernel matrix.

torch.Size([51267, 2])
We keep 1.94e+07/1.96e+09 =  0% of the original kernel matrix.

torch.Size([1712, 2])
We keep 2.92e+04/3.66e+05 =  7% of the original kernel matrix.

torch.Size([10981, 2])
We keep 5.68e+05/1.63e+07 =  3% of the original kernel matrix.

torch.Size([23104, 2])
We keep 7.71e+06/2.40e+08 =  3% of the original kernel matrix.

torch.Size([32035, 2])
We keep 5.49e+06/4.18e+08 =  1% of the original kernel matrix.

torch.Size([16542, 2])
We keep 5.69e+06/1.13e+08 =  5% of the original kernel matrix.

torch.Size([26815, 2])
We keep 4.07e+06/2.86e+08 =  1% of the original kernel matrix.

torch.Size([5186, 2])
We keep 3.44e+05/6.24e+06 =  5% of the original kernel matrix.

torch.Size([16126, 2])
We keep 1.44e+06/6.73e+07 =  2% of the original kernel matrix.

torch.Size([101312, 2])
We keep 2.00e+08/6.49e+09 =  3% of the original kernel matrix.

torch.Size([66354, 2])
We keep 2.05e+07/2.17e+09 =  0% of the original kernel matrix.

torch.Size([728, 2])
We keep 7.84e+03/6.20e+04 = 12% of the original kernel matrix.

torch.Size([8226, 2])
We keep 3.29e+05/6.71e+06 =  4% of the original kernel matrix.

torch.Size([3136, 2])
We keep 9.54e+04/1.46e+06 =  6% of the original kernel matrix.

torch.Size([13258, 2])
We keep 8.66e+05/3.26e+07 =  2% of the original kernel matrix.

torch.Size([72857, 2])
We keep 4.15e+07/2.17e+09 =  1% of the original kernel matrix.

torch.Size([55702, 2])
We keep 1.33e+07/1.25e+09 =  1% of the original kernel matrix.

torch.Size([152039, 2])
We keep 9.42e+07/8.25e+09 =  1% of the original kernel matrix.

torch.Size([82584, 2])
We keep 2.32e+07/2.45e+09 =  0% of the original kernel matrix.

torch.Size([2077, 2])
We keep 8.31e+04/9.39e+05 =  8% of the original kernel matrix.

torch.Size([11342, 2])
We keep 7.66e+05/2.61e+07 =  2% of the original kernel matrix.

torch.Size([19557, 2])
We keep 2.79e+06/1.09e+08 =  2% of the original kernel matrix.

torch.Size([29864, 2])
We keep 4.06e+06/2.81e+08 =  1% of the original kernel matrix.

torch.Size([8524, 2])
We keep 9.43e+05/2.03e+07 =  4% of the original kernel matrix.

torch.Size([19879, 2])
We keep 2.11e+06/1.21e+08 =  1% of the original kernel matrix.

torch.Size([17404, 2])
We keep 7.97e+06/1.19e+08 =  6% of the original kernel matrix.

torch.Size([27671, 2])
We keep 3.96e+06/2.94e+08 =  1% of the original kernel matrix.

torch.Size([14703, 2])
We keep 9.15e+06/1.01e+08 =  9% of the original kernel matrix.

torch.Size([25069, 2])
We keep 3.88e+06/2.71e+08 =  1% of the original kernel matrix.

torch.Size([9663, 2])
We keep 1.15e+06/2.74e+07 =  4% of the original kernel matrix.

torch.Size([20676, 2])
We keep 2.39e+06/1.41e+08 =  1% of the original kernel matrix.

torch.Size([15734, 2])
We keep 5.10e+06/1.35e+08 =  3% of the original kernel matrix.

torch.Size([26209, 2])
We keep 4.49e+06/3.13e+08 =  1% of the original kernel matrix.

torch.Size([12413, 2])
We keep 1.41e+06/3.46e+07 =  4% of the original kernel matrix.

torch.Size([24059, 2])
We keep 2.62e+06/1.58e+08 =  1% of the original kernel matrix.

torch.Size([1064, 2])
We keep 1.57e+04/1.67e+05 =  9% of the original kernel matrix.

torch.Size([8896, 2])
We keep 4.22e+05/1.10e+07 =  3% of the original kernel matrix.

torch.Size([120120, 2])
We keep 6.68e+08/2.89e+10 =  2% of the original kernel matrix.

torch.Size([62918, 2])
We keep 4.08e+07/4.58e+09 =  0% of the original kernel matrix.

torch.Size([1597, 2])
We keep 4.93e+04/4.24e+05 = 11% of the original kernel matrix.

torch.Size([10335, 2])
We keep 5.99e+05/1.75e+07 =  3% of the original kernel matrix.

torch.Size([1973, 2])
We keep 5.12e+04/6.29e+05 =  8% of the original kernel matrix.

torch.Size([11272, 2])
We keep 6.70e+05/2.14e+07 =  3% of the original kernel matrix.

torch.Size([14222, 2])
We keep 1.71e+06/5.26e+07 =  3% of the original kernel matrix.

torch.Size([25140, 2])
We keep 2.97e+06/1.95e+08 =  1% of the original kernel matrix.

torch.Size([12558, 2])
We keep 2.62e+06/7.16e+07 =  3% of the original kernel matrix.

torch.Size([22706, 2])
We keep 3.37e+06/2.28e+08 =  1% of the original kernel matrix.

torch.Size([192115, 2])
We keep 2.84e+08/1.85e+10 =  1% of the original kernel matrix.

torch.Size([93675, 2])
We keep 3.33e+07/3.67e+09 =  0% of the original kernel matrix.

torch.Size([53342, 2])
We keep 1.55e+07/1.07e+09 =  1% of the original kernel matrix.

torch.Size([49503, 2])
We keep 9.85e+06/8.82e+08 =  1% of the original kernel matrix.

torch.Size([554, 2])
We keep 3.73e+03/2.34e+04 = 15% of the original kernel matrix.

torch.Size([7403, 2])
We keep 2.37e+05/4.12e+06 =  5% of the original kernel matrix.

torch.Size([13427, 2])
We keep 2.99e+06/6.24e+07 =  4% of the original kernel matrix.

torch.Size([24641, 2])
We keep 3.28e+06/2.13e+08 =  1% of the original kernel matrix.

torch.Size([5019, 2])
We keep 6.33e+05/9.98e+06 =  6% of the original kernel matrix.

torch.Size([14982, 2])
We keep 1.70e+06/8.51e+07 =  1% of the original kernel matrix.

torch.Size([12232, 2])
We keep 3.37e+06/7.10e+07 =  4% of the original kernel matrix.

torch.Size([23105, 2])
We keep 3.42e+06/2.27e+08 =  1% of the original kernel matrix.

torch.Size([41762, 2])
We keep 8.60e+06/6.15e+08 =  1% of the original kernel matrix.

torch.Size([44816, 2])
We keep 8.03e+06/6.68e+08 =  1% of the original kernel matrix.

torch.Size([2990, 2])
We keep 8.47e+04/1.44e+06 =  5% of the original kernel matrix.

torch.Size([13246, 2])
We keep 8.71e+05/3.23e+07 =  2% of the original kernel matrix.

torch.Size([15468, 2])
We keep 2.17e+06/7.31e+07 =  2% of the original kernel matrix.

torch.Size([26231, 2])
We keep 3.43e+06/2.30e+08 =  1% of the original kernel matrix.

torch.Size([619, 2])
We keep 4.08e+03/2.40e+04 = 16% of the original kernel matrix.

torch.Size([7624, 2])
We keep 2.49e+05/4.18e+06 =  5% of the original kernel matrix.

torch.Size([3660, 2])
We keep 1.43e+05/2.41e+06 =  5% of the original kernel matrix.

torch.Size([14284, 2])
We keep 1.05e+06/4.18e+07 =  2% of the original kernel matrix.

torch.Size([29450, 2])
We keep 1.99e+07/7.47e+08 =  2% of the original kernel matrix.

torch.Size([33445, 2])
We keep 8.61e+06/7.37e+08 =  1% of the original kernel matrix.

torch.Size([12618, 2])
We keep 5.18e+06/8.21e+07 =  6% of the original kernel matrix.

torch.Size([22991, 2])
We keep 3.57e+06/2.44e+08 =  1% of the original kernel matrix.

torch.Size([11571, 2])
We keep 1.84e+06/4.69e+07 =  3% of the original kernel matrix.

torch.Size([22445, 2])
We keep 2.93e+06/1.85e+08 =  1% of the original kernel matrix.

torch.Size([6500, 2])
We keep 9.74e+06/4.14e+07 = 23% of the original kernel matrix.

torch.Size([15924, 2])
We keep 2.82e+06/1.73e+08 =  1% of the original kernel matrix.

torch.Size([123377, 2])
We keep 8.57e+07/8.83e+09 =  0% of the original kernel matrix.

torch.Size([74581, 2])
We keep 2.43e+07/2.53e+09 =  0% of the original kernel matrix.

torch.Size([4280, 2])
We keep 1.31e+05/2.61e+06 =  5% of the original kernel matrix.

torch.Size([15583, 2])
We keep 1.07e+06/4.35e+07 =  2% of the original kernel matrix.

torch.Size([17739, 2])
We keep 1.82e+06/8.16e+07 =  2% of the original kernel matrix.

torch.Size([28368, 2])
We keep 3.59e+06/2.43e+08 =  1% of the original kernel matrix.

torch.Size([42249, 2])
We keep 1.50e+07/8.07e+08 =  1% of the original kernel matrix.

torch.Size([43773, 2])
We keep 8.84e+06/7.65e+08 =  1% of the original kernel matrix.

torch.Size([2376, 2])
We keep 6.32e+04/8.15e+05 =  7% of the original kernel matrix.

torch.Size([12092, 2])
We keep 7.26e+05/2.43e+07 =  2% of the original kernel matrix.

torch.Size([31398, 2])
We keep 9.16e+06/3.66e+08 =  2% of the original kernel matrix.

torch.Size([38013, 2])
We keep 6.48e+06/5.16e+08 =  1% of the original kernel matrix.

torch.Size([127389, 2])
We keep 2.26e+08/1.19e+10 =  1% of the original kernel matrix.

torch.Size([73093, 2])
We keep 2.76e+07/2.94e+09 =  0% of the original kernel matrix.

torch.Size([141781, 2])
We keep 1.81e+08/1.09e+10 =  1% of the original kernel matrix.

torch.Size([78244, 2])
We keep 2.65e+07/2.82e+09 =  0% of the original kernel matrix.

torch.Size([11920, 2])
We keep 1.66e+06/4.50e+07 =  3% of the original kernel matrix.

torch.Size([23187, 2])
We keep 2.90e+06/1.81e+08 =  1% of the original kernel matrix.

torch.Size([8202, 2])
We keep 8.55e+05/1.62e+07 =  5% of the original kernel matrix.

torch.Size([19224, 2])
We keep 1.97e+06/1.08e+08 =  1% of the original kernel matrix.

torch.Size([1945, 2])
We keep 5.66e+04/5.46e+05 = 10% of the original kernel matrix.

torch.Size([10973, 2])
We keep 6.34e+05/1.99e+07 =  3% of the original kernel matrix.

torch.Size([4075, 2])
We keep 1.09e+05/2.13e+06 =  5% of the original kernel matrix.

torch.Size([15126, 2])
We keep 9.93e+05/3.94e+07 =  2% of the original kernel matrix.

torch.Size([10990, 2])
We keep 3.67e+06/6.10e+07 =  6% of the original kernel matrix.

torch.Size([22072, 2])
We keep 3.21e+06/2.10e+08 =  1% of the original kernel matrix.

torch.Size([9434, 2])
We keep 6.31e+05/1.68e+07 =  3% of the original kernel matrix.

torch.Size([21181, 2])
We keep 2.05e+06/1.11e+08 =  1% of the original kernel matrix.

torch.Size([17263, 2])
We keep 3.49e+06/9.65e+07 =  3% of the original kernel matrix.

torch.Size([27994, 2])
We keep 3.79e+06/2.65e+08 =  1% of the original kernel matrix.

torch.Size([29401, 2])
We keep 4.45e+07/8.87e+08 =  5% of the original kernel matrix.

torch.Size([36098, 2])
We keep 9.16e+06/8.03e+08 =  1% of the original kernel matrix.

torch.Size([15582, 2])
We keep 5.97e+06/1.12e+08 =  5% of the original kernel matrix.

torch.Size([26589, 2])
We keep 4.15e+06/2.86e+08 =  1% of the original kernel matrix.

torch.Size([4832, 2])
We keep 1.01e+06/9.85e+06 = 10% of the original kernel matrix.

torch.Size([14900, 2])
We keep 1.70e+06/8.46e+07 =  2% of the original kernel matrix.

torch.Size([6063, 2])
We keep 2.88e+05/6.42e+06 =  4% of the original kernel matrix.

torch.Size([17820, 2])
We keep 1.47e+06/6.83e+07 =  2% of the original kernel matrix.

torch.Size([22568, 2])
We keep 8.06e+07/1.49e+09 =  5% of the original kernel matrix.

torch.Size([28912, 2])
We keep 1.16e+07/1.04e+09 =  1% of the original kernel matrix.

torch.Size([9479, 2])
We keep 1.24e+06/2.61e+07 =  4% of the original kernel matrix.

torch.Size([21121, 2])
We keep 2.33e+06/1.38e+08 =  1% of the original kernel matrix.

torch.Size([4506, 2])
We keep 6.44e+05/5.11e+06 = 12% of the original kernel matrix.

torch.Size([15189, 2])
We keep 1.34e+06/6.09e+07 =  2% of the original kernel matrix.

torch.Size([16770, 2])
We keep 3.81e+06/1.13e+08 =  3% of the original kernel matrix.

torch.Size([27457, 2])
We keep 4.21e+06/2.86e+08 =  1% of the original kernel matrix.

torch.Size([327652, 2])
We keep 1.57e+09/7.45e+10 =  2% of the original kernel matrix.

torch.Size([117923, 2])
We keep 6.21e+07/7.36e+09 =  0% of the original kernel matrix.

torch.Size([8400, 2])
We keep 8.56e+05/1.69e+07 =  5% of the original kernel matrix.

torch.Size([19907, 2])
We keep 2.04e+06/1.11e+08 =  1% of the original kernel matrix.

torch.Size([25877, 2])
We keep 2.90e+07/5.55e+08 =  5% of the original kernel matrix.

torch.Size([31410, 2])
We keep 7.04e+06/6.35e+08 =  1% of the original kernel matrix.

torch.Size([2650, 2])
We keep 7.35e+04/1.07e+06 =  6% of the original kernel matrix.

torch.Size([12676, 2])
We keep 7.98e+05/2.79e+07 =  2% of the original kernel matrix.

torch.Size([20320, 2])
We keep 2.27e+07/5.94e+08 =  3% of the original kernel matrix.

torch.Size([26643, 2])
We keep 7.98e+06/6.57e+08 =  1% of the original kernel matrix.

torch.Size([6232, 2])
We keep 3.92e+05/7.78e+06 =  5% of the original kernel matrix.

torch.Size([18023, 2])
We keep 1.53e+06/7.52e+07 =  2% of the original kernel matrix.

torch.Size([137487, 2])
We keep 4.01e+08/1.80e+10 =  2% of the original kernel matrix.

torch.Size([69684, 2])
We keep 3.31e+07/3.62e+09 =  0% of the original kernel matrix.

torch.Size([24062, 2])
We keep 4.37e+06/2.25e+08 =  1% of the original kernel matrix.

torch.Size([32844, 2])
We keep 5.36e+06/4.04e+08 =  1% of the original kernel matrix.

torch.Size([4623, 2])
We keep 1.73e+05/3.32e+06 =  5% of the original kernel matrix.

torch.Size([15954, 2])
We keep 1.17e+06/4.91e+07 =  2% of the original kernel matrix.

torch.Size([59625, 2])
We keep 1.72e+07/1.28e+09 =  1% of the original kernel matrix.

torch.Size([51860, 2])
We keep 1.07e+07/9.65e+08 =  1% of the original kernel matrix.

torch.Size([63112, 2])
We keep 1.69e+07/1.38e+09 =  1% of the original kernel matrix.

torch.Size([53564, 2])
We keep 1.10e+07/1.00e+09 =  1% of the original kernel matrix.

torch.Size([3324, 2])
We keep 3.96e+05/3.09e+06 = 12% of the original kernel matrix.

torch.Size([13125, 2])
We keep 1.12e+06/4.74e+07 =  2% of the original kernel matrix.

torch.Size([19736, 2])
We keep 4.84e+06/1.26e+08 =  3% of the original kernel matrix.

torch.Size([30008, 2])
We keep 4.28e+06/3.03e+08 =  1% of the original kernel matrix.

torch.Size([109683, 2])
We keep 5.60e+07/5.32e+09 =  1% of the original kernel matrix.

torch.Size([71025, 2])
We keep 1.97e+07/1.96e+09 =  1% of the original kernel matrix.

torch.Size([255499, 2])
We keep 2.22e+08/2.79e+10 =  0% of the original kernel matrix.

torch.Size([109780, 2])
We keep 3.99e+07/4.50e+09 =  0% of the original kernel matrix.

torch.Size([12720, 2])
We keep 9.42e+06/9.26e+07 = 10% of the original kernel matrix.

torch.Size([23878, 2])
We keep 3.83e+06/2.59e+08 =  1% of the original kernel matrix.

torch.Size([1525, 2])
We keep 3.01e+04/3.50e+05 =  8% of the original kernel matrix.

torch.Size([9984, 2])
We keep 5.43e+05/1.60e+07 =  3% of the original kernel matrix.

torch.Size([3650, 2])
We keep 1.45e+05/2.28e+06 =  6% of the original kernel matrix.

torch.Size([14105, 2])
We keep 1.02e+06/4.07e+07 =  2% of the original kernel matrix.

torch.Size([15365, 2])
We keep 2.01e+07/3.18e+08 =  6% of the original kernel matrix.

torch.Size([25078, 2])
We keep 6.28e+06/4.80e+08 =  1% of the original kernel matrix.

torch.Size([37301, 2])
We keep 1.35e+08/8.51e+08 = 15% of the original kernel matrix.

torch.Size([40455, 2])
We keep 9.18e+06/7.86e+08 =  1% of the original kernel matrix.

torch.Size([9524, 2])
We keep 6.58e+05/1.62e+07 =  4% of the original kernel matrix.

torch.Size([21336, 2])
We keep 2.01e+06/1.08e+08 =  1% of the original kernel matrix.

torch.Size([1295, 2])
We keep 2.43e+04/2.95e+05 =  8% of the original kernel matrix.

torch.Size([9567, 2])
We keep 5.28e+05/1.46e+07 =  3% of the original kernel matrix.

torch.Size([28808, 2])
We keep 5.77e+06/2.80e+08 =  2% of the original kernel matrix.

torch.Size([36876, 2])
We keep 5.76e+06/4.51e+08 =  1% of the original kernel matrix.

torch.Size([40828, 2])
We keep 1.19e+07/7.30e+08 =  1% of the original kernel matrix.

torch.Size([43277, 2])
We keep 8.60e+06/7.28e+08 =  1% of the original kernel matrix.

torch.Size([10836, 2])
We keep 2.18e+07/1.74e+08 = 12% of the original kernel matrix.

torch.Size([18526, 2])
We keep 4.82e+06/3.55e+08 =  1% of the original kernel matrix.

torch.Size([2757, 2])
We keep 1.40e+05/1.88e+06 =  7% of the original kernel matrix.

torch.Size([12222, 2])
We keep 9.59e+05/3.70e+07 =  2% of the original kernel matrix.

torch.Size([8002, 2])
We keep 1.14e+06/2.56e+07 =  4% of the original kernel matrix.

torch.Size([18240, 2])
We keep 2.37e+06/1.36e+08 =  1% of the original kernel matrix.

torch.Size([13695, 2])
We keep 1.59e+06/5.20e+07 =  3% of the original kernel matrix.

torch.Size([25319, 2])
We keep 3.17e+06/1.94e+08 =  1% of the original kernel matrix.

torch.Size([6169, 2])
We keep 3.06e+05/6.73e+06 =  4% of the original kernel matrix.

torch.Size([17484, 2])
We keep 1.47e+06/6.99e+07 =  2% of the original kernel matrix.

torch.Size([11768, 2])
We keep 9.98e+05/3.15e+07 =  3% of the original kernel matrix.

torch.Size([22971, 2])
We keep 2.54e+06/1.51e+08 =  1% of the original kernel matrix.

torch.Size([3099, 2])
We keep 1.16e+05/1.53e+06 =  7% of the original kernel matrix.

torch.Size([13586, 2])
We keep 8.56e+05/3.33e+07 =  2% of the original kernel matrix.

torch.Size([16553, 2])
We keep 1.73e+07/1.72e+08 = 10% of the original kernel matrix.

torch.Size([26535, 2])
We keep 4.84e+06/3.54e+08 =  1% of the original kernel matrix.

torch.Size([24203, 2])
We keep 3.24e+06/1.76e+08 =  1% of the original kernel matrix.

torch.Size([33608, 2])
We keep 4.83e+06/3.57e+08 =  1% of the original kernel matrix.

torch.Size([13129, 2])
We keep 1.21e+06/3.82e+07 =  3% of the original kernel matrix.

torch.Size([24595, 2])
We keep 2.73e+06/1.67e+08 =  1% of the original kernel matrix.

torch.Size([9666, 2])
We keep 4.74e+06/5.73e+07 =  8% of the original kernel matrix.

torch.Size([20434, 2])
We keep 3.17e+06/2.04e+08 =  1% of the original kernel matrix.

torch.Size([3679, 2])
We keep 4.06e+05/5.65e+06 =  7% of the original kernel matrix.

torch.Size([13016, 2])
We keep 1.39e+06/6.41e+07 =  2% of the original kernel matrix.

torch.Size([179378, 2])
We keep 2.21e+08/1.74e+10 =  1% of the original kernel matrix.

torch.Size([90331, 2])
We keep 3.19e+07/3.56e+09 =  0% of the original kernel matrix.

torch.Size([5492, 2])
We keep 3.09e+05/6.06e+06 =  5% of the original kernel matrix.

torch.Size([16503, 2])
We keep 1.41e+06/6.63e+07 =  2% of the original kernel matrix.

torch.Size([60884, 2])
We keep 2.10e+07/1.40e+09 =  1% of the original kernel matrix.

torch.Size([51797, 2])
We keep 1.10e+07/1.01e+09 =  1% of the original kernel matrix.

torch.Size([1412, 2])
We keep 2.23e+04/2.31e+05 =  9% of the original kernel matrix.

torch.Size([10252, 2])
We keep 4.99e+05/1.30e+07 =  3% of the original kernel matrix.

torch.Size([1522, 2])
We keep 3.19e+04/3.25e+05 =  9% of the original kernel matrix.

torch.Size([10305, 2])
We keep 5.57e+05/1.54e+07 =  3% of the original kernel matrix.

torch.Size([4715, 2])
We keep 1.68e+05/3.48e+06 =  4% of the original kernel matrix.

torch.Size([15778, 2])
We keep 1.16e+06/5.03e+07 =  2% of the original kernel matrix.

torch.Size([10999, 2])
We keep 1.30e+06/2.94e+07 =  4% of the original kernel matrix.

torch.Size([22813, 2])
We keep 2.48e+06/1.46e+08 =  1% of the original kernel matrix.

torch.Size([2361, 2])
We keep 6.78e+04/9.04e+05 =  7% of the original kernel matrix.

torch.Size([12150, 2])
We keep 7.67e+05/2.56e+07 =  2% of the original kernel matrix.

torch.Size([23127, 2])
We keep 1.70e+07/4.15e+08 =  4% of the original kernel matrix.

torch.Size([30257, 2])
We keep 6.86e+06/5.49e+08 =  1% of the original kernel matrix.

torch.Size([3247, 2])
We keep 9.64e+04/1.58e+06 =  6% of the original kernel matrix.

torch.Size([13602, 2])
We keep 8.98e+05/3.38e+07 =  2% of the original kernel matrix.

torch.Size([3396, 2])
We keep 1.10e+05/1.89e+06 =  5% of the original kernel matrix.

torch.Size([13813, 2])
We keep 9.53e+05/3.71e+07 =  2% of the original kernel matrix.

torch.Size([138969, 2])
We keep 1.09e+08/7.69e+09 =  1% of the original kernel matrix.

torch.Size([78574, 2])
We keep 2.24e+07/2.36e+09 =  0% of the original kernel matrix.

torch.Size([14860, 2])
We keep 3.28e+06/6.97e+07 =  4% of the original kernel matrix.

torch.Size([25919, 2])
We keep 3.39e+06/2.25e+08 =  1% of the original kernel matrix.

torch.Size([15495, 2])
We keep 3.97e+06/8.02e+07 =  4% of the original kernel matrix.

torch.Size([26643, 2])
We keep 3.60e+06/2.41e+08 =  1% of the original kernel matrix.

torch.Size([77170, 2])
We keep 5.64e+07/2.60e+09 =  2% of the original kernel matrix.

torch.Size([58031, 2])
We keep 1.44e+07/1.37e+09 =  1% of the original kernel matrix.

torch.Size([64345, 2])
We keep 6.08e+07/2.98e+09 =  2% of the original kernel matrix.

torch.Size([47246, 2])
We keep 1.53e+07/1.47e+09 =  1% of the original kernel matrix.

torch.Size([7854, 2])
We keep 2.31e+06/1.67e+07 = 13% of the original kernel matrix.

torch.Size([18853, 2])
We keep 2.03e+06/1.10e+08 =  1% of the original kernel matrix.

torch.Size([4156, 2])
We keep 1.63e+05/3.01e+06 =  5% of the original kernel matrix.

torch.Size([14923, 2])
We keep 1.12e+06/4.67e+07 =  2% of the original kernel matrix.

torch.Size([7825, 2])
We keep 1.42e+06/2.27e+07 =  6% of the original kernel matrix.

torch.Size([18839, 2])
We keep 2.28e+06/1.28e+08 =  1% of the original kernel matrix.

torch.Size([407, 2])
We keep 3.94e+03/1.90e+04 = 20% of the original kernel matrix.

torch.Size([6330, 2])
We keep 2.38e+05/3.72e+06 =  6% of the original kernel matrix.

torch.Size([2121, 2])
We keep 4.74e+04/6.56e+05 =  7% of the original kernel matrix.

torch.Size([11765, 2])
We keep 6.82e+05/2.18e+07 =  3% of the original kernel matrix.

torch.Size([14434, 2])
We keep 2.79e+06/7.58e+07 =  3% of the original kernel matrix.

torch.Size([25565, 2])
We keep 3.52e+06/2.35e+08 =  1% of the original kernel matrix.

torch.Size([1708, 2])
We keep 4.12e+04/3.96e+05 = 10% of the original kernel matrix.

torch.Size([10831, 2])
We keep 5.95e+05/1.69e+07 =  3% of the original kernel matrix.

torch.Size([1334, 2])
We keep 2.32e+04/2.24e+05 = 10% of the original kernel matrix.

torch.Size([9945, 2])
We keep 4.93e+05/1.27e+07 =  3% of the original kernel matrix.

torch.Size([2038, 2])
We keep 4.86e+04/6.42e+05 =  7% of the original kernel matrix.

torch.Size([11511, 2])
We keep 6.82e+05/2.16e+07 =  3% of the original kernel matrix.

torch.Size([2783, 2])
We keep 6.31e+04/1.00e+06 =  6% of the original kernel matrix.

torch.Size([13136, 2])
We keep 7.79e+05/2.70e+07 =  2% of the original kernel matrix.

torch.Size([102349, 2])
We keep 6.92e+07/5.11e+09 =  1% of the original kernel matrix.

torch.Size([66652, 2])
We keep 1.90e+07/1.93e+09 =  0% of the original kernel matrix.

torch.Size([4693, 2])
We keep 7.23e+05/1.11e+07 =  6% of the original kernel matrix.

torch.Size([13965, 2])
We keep 1.77e+06/8.99e+07 =  1% of the original kernel matrix.

torch.Size([34080, 2])
We keep 1.20e+07/4.83e+08 =  2% of the original kernel matrix.

torch.Size([39607, 2])
We keep 7.36e+06/5.92e+08 =  1% of the original kernel matrix.

torch.Size([3720, 2])
We keep 3.19e+05/4.06e+06 =  7% of the original kernel matrix.

torch.Size([13791, 2])
We keep 1.24e+06/5.43e+07 =  2% of the original kernel matrix.

torch.Size([3161, 2])
We keep 2.07e+05/2.30e+06 =  9% of the original kernel matrix.

torch.Size([12882, 2])
We keep 1.00e+06/4.08e+07 =  2% of the original kernel matrix.

torch.Size([10126, 2])
We keep 2.80e+06/5.89e+07 =  4% of the original kernel matrix.

torch.Size([19926, 2])
We keep 3.24e+06/2.07e+08 =  1% of the original kernel matrix.

torch.Size([343457, 2])
We keep 8.06e+08/5.99e+10 =  1% of the original kernel matrix.

torch.Size([125559, 2])
We keep 5.53e+07/6.59e+09 =  0% of the original kernel matrix.

torch.Size([308618, 2])
We keep 2.84e+08/3.78e+10 =  0% of the original kernel matrix.

torch.Size([122637, 2])
We keep 4.58e+07/5.24e+09 =  0% of the original kernel matrix.

torch.Size([15576, 2])
We keep 2.99e+06/7.53e+07 =  3% of the original kernel matrix.

torch.Size([26910, 2])
We keep 3.55e+06/2.34e+08 =  1% of the original kernel matrix.

torch.Size([38560, 2])
We keep 2.31e+07/7.50e+08 =  3% of the original kernel matrix.

torch.Size([41575, 2])
We keep 8.61e+06/7.38e+08 =  1% of the original kernel matrix.

torch.Size([1148, 2])
We keep 1.35e+04/1.34e+05 = 10% of the original kernel matrix.

torch.Size([9602, 2])
We keep 4.10e+05/9.86e+06 =  4% of the original kernel matrix.

torch.Size([14667, 2])
We keep 5.28e+07/2.87e+08 = 18% of the original kernel matrix.

torch.Size([23824, 2])
We keep 5.98e+06/4.57e+08 =  1% of the original kernel matrix.

torch.Size([11118, 2])
We keep 6.96e+06/8.09e+07 =  8% of the original kernel matrix.

torch.Size([21817, 2])
We keep 3.62e+06/2.42e+08 =  1% of the original kernel matrix.

torch.Size([29811, 2])
We keep 2.35e+07/4.43e+08 =  5% of the original kernel matrix.

torch.Size([37001, 2])
We keep 6.81e+06/5.67e+08 =  1% of the original kernel matrix.

torch.Size([7643, 2])
We keep 1.06e+06/1.44e+07 =  7% of the original kernel matrix.

torch.Size([18799, 2])
We keep 1.90e+06/1.02e+08 =  1% of the original kernel matrix.

torch.Size([41865, 2])
We keep 1.36e+07/7.37e+08 =  1% of the original kernel matrix.

torch.Size([42871, 2])
We keep 8.49e+06/7.32e+08 =  1% of the original kernel matrix.

torch.Size([180330, 2])
We keep 3.31e+08/1.95e+10 =  1% of the original kernel matrix.

torch.Size([89277, 2])
We keep 3.42e+07/3.76e+09 =  0% of the original kernel matrix.

torch.Size([11198, 2])
We keep 1.54e+06/4.16e+07 =  3% of the original kernel matrix.

torch.Size([21822, 2])
We keep 2.79e+06/1.74e+08 =  1% of the original kernel matrix.

torch.Size([3435, 2])
We keep 2.48e+05/2.85e+06 =  8% of the original kernel matrix.

torch.Size([13231, 2])
We keep 1.09e+06/4.55e+07 =  2% of the original kernel matrix.

torch.Size([19127, 2])
We keep 2.86e+06/9.27e+07 =  3% of the original kernel matrix.

torch.Size([31008, 2])
We keep 3.91e+06/2.59e+08 =  1% of the original kernel matrix.

torch.Size([10520, 2])
We keep 8.52e+05/2.48e+07 =  3% of the original kernel matrix.

torch.Size([21913, 2])
We keep 2.34e+06/1.34e+08 =  1% of the original kernel matrix.

torch.Size([1986, 2])
We keep 4.06e+04/4.71e+05 =  8% of the original kernel matrix.

torch.Size([11546, 2])
We keep 6.13e+05/1.85e+07 =  3% of the original kernel matrix.

torch.Size([51059, 2])
We keep 2.46e+07/1.06e+09 =  2% of the original kernel matrix.

torch.Size([47924, 2])
We keep 9.91e+06/8.78e+08 =  1% of the original kernel matrix.

torch.Size([8522, 2])
We keep 2.11e+06/3.55e+07 =  5% of the original kernel matrix.

torch.Size([18418, 2])
We keep 2.54e+06/1.60e+08 =  1% of the original kernel matrix.

torch.Size([816, 2])
We keep 8.22e+03/7.40e+04 = 11% of the original kernel matrix.

torch.Size([8303, 2])
We keep 3.44e+05/7.33e+06 =  4% of the original kernel matrix.

torch.Size([19058, 2])
We keep 3.51e+06/1.24e+08 =  2% of the original kernel matrix.

torch.Size([29621, 2])
We keep 4.32e+06/3.01e+08 =  1% of the original kernel matrix.

torch.Size([16332, 2])
We keep 2.26e+06/7.96e+07 =  2% of the original kernel matrix.

torch.Size([27920, 2])
We keep 3.70e+06/2.40e+08 =  1% of the original kernel matrix.

torch.Size([424, 2])
We keep 3.54e+03/1.85e+04 = 19% of the original kernel matrix.

torch.Size([6567, 2])
We keep 2.29e+05/3.66e+06 =  6% of the original kernel matrix.

torch.Size([63208, 2])
We keep 5.10e+07/2.39e+09 =  2% of the original kernel matrix.

torch.Size([50525, 2])
We keep 1.40e+07/1.32e+09 =  1% of the original kernel matrix.

torch.Size([46139, 2])
We keep 1.60e+07/8.30e+08 =  1% of the original kernel matrix.

torch.Size([46006, 2])
We keep 9.00e+06/7.76e+08 =  1% of the original kernel matrix.

torch.Size([46081, 2])
We keep 2.13e+07/8.39e+08 =  2% of the original kernel matrix.

torch.Size([45907, 2])
We keep 8.97e+06/7.81e+08 =  1% of the original kernel matrix.

torch.Size([38854, 2])
We keep 2.15e+07/7.04e+08 =  3% of the original kernel matrix.

torch.Size([42102, 2])
We keep 8.58e+06/7.15e+08 =  1% of the original kernel matrix.

torch.Size([40971, 2])
We keep 2.17e+07/8.01e+08 =  2% of the original kernel matrix.

torch.Size([42395, 2])
We keep 8.89e+06/7.63e+08 =  1% of the original kernel matrix.

torch.Size([3964, 2])
We keep 3.62e+05/4.55e+06 =  7% of the original kernel matrix.

torch.Size([13735, 2])
We keep 1.26e+06/5.75e+07 =  2% of the original kernel matrix.

torch.Size([5664, 2])
We keep 4.14e+05/7.75e+06 =  5% of the original kernel matrix.

torch.Size([16480, 2])
We keep 1.52e+06/7.50e+07 =  2% of the original kernel matrix.

torch.Size([19846, 2])
We keep 2.27e+06/1.04e+08 =  2% of the original kernel matrix.

torch.Size([30166, 2])
We keep 3.98e+06/2.75e+08 =  1% of the original kernel matrix.

torch.Size([26828, 2])
We keep 5.01e+06/2.52e+08 =  1% of the original kernel matrix.

torch.Size([35481, 2])
We keep 5.63e+06/4.28e+08 =  1% of the original kernel matrix.

torch.Size([2904, 2])
We keep 8.46e+04/1.35e+06 =  6% of the original kernel matrix.

torch.Size([13119, 2])
We keep 8.58e+05/3.13e+07 =  2% of the original kernel matrix.

torch.Size([23273, 2])
We keep 1.31e+07/2.99e+08 =  4% of the original kernel matrix.

torch.Size([32778, 2])
We keep 6.02e+06/4.66e+08 =  1% of the original kernel matrix.

torch.Size([70323, 2])
We keep 3.99e+07/2.58e+09 =  1% of the original kernel matrix.

torch.Size([55616, 2])
We keep 1.45e+07/1.37e+09 =  1% of the original kernel matrix.

torch.Size([24587, 2])
We keep 5.11e+06/2.05e+08 =  2% of the original kernel matrix.

torch.Size([33783, 2])
We keep 5.20e+06/3.86e+08 =  1% of the original kernel matrix.

torch.Size([8062, 2])
We keep 1.40e+06/2.85e+07 =  4% of the original kernel matrix.

torch.Size([19121, 2])
We keep 2.45e+06/1.44e+08 =  1% of the original kernel matrix.

torch.Size([10504, 2])
We keep 1.41e+07/1.28e+08 = 11% of the original kernel matrix.

torch.Size([20071, 2])
We keep 4.32e+06/3.05e+08 =  1% of the original kernel matrix.

torch.Size([7107, 2])
We keep 4.89e+05/1.09e+07 =  4% of the original kernel matrix.

torch.Size([18547, 2])
We keep 1.73e+06/8.90e+07 =  1% of the original kernel matrix.

torch.Size([2513, 2])
We keep 6.13e+04/8.80e+05 =  6% of the original kernel matrix.

torch.Size([12406, 2])
We keep 7.48e+05/2.53e+07 =  2% of the original kernel matrix.

torch.Size([1298, 2])
We keep 1.21e+05/6.86e+05 = 17% of the original kernel matrix.

torch.Size([8815, 2])
We keep 6.51e+05/2.23e+07 =  2% of the original kernel matrix.

torch.Size([70966, 2])
We keep 2.20e+07/1.76e+09 =  1% of the original kernel matrix.

torch.Size([56406, 2])
We keep 1.21e+07/1.13e+09 =  1% of the original kernel matrix.

torch.Size([44633, 2])
We keep 9.54e+06/6.73e+08 =  1% of the original kernel matrix.

torch.Size([45601, 2])
We keep 8.22e+06/6.99e+08 =  1% of the original kernel matrix.

torch.Size([214186, 2])
We keep 2.97e+08/2.22e+10 =  1% of the original kernel matrix.

torch.Size([98056, 2])
We keep 3.58e+07/4.01e+09 =  0% of the original kernel matrix.

torch.Size([89961, 2])
We keep 1.42e+08/6.04e+09 =  2% of the original kernel matrix.

torch.Size([60671, 2])
We keep 2.07e+07/2.09e+09 =  0% of the original kernel matrix.

torch.Size([29830, 2])
We keep 6.34e+06/3.32e+08 =  1% of the original kernel matrix.

torch.Size([37026, 2])
We keep 6.15e+06/4.91e+08 =  1% of the original kernel matrix.

torch.Size([11413, 2])
We keep 1.23e+06/4.88e+07 =  2% of the original kernel matrix.

torch.Size([22923, 2])
We keep 2.98e+06/1.88e+08 =  1% of the original kernel matrix.

torch.Size([55299, 2])
We keep 2.06e+07/1.31e+09 =  1% of the original kernel matrix.

torch.Size([50440, 2])
We keep 1.09e+07/9.77e+08 =  1% of the original kernel matrix.

torch.Size([1359, 2])
We keep 2.64e+04/2.57e+05 = 10% of the original kernel matrix.

torch.Size([9799, 2])
We keep 5.18e+05/1.37e+07 =  3% of the original kernel matrix.

torch.Size([4854, 2])
We keep 2.22e+05/4.21e+06 =  5% of the original kernel matrix.

torch.Size([15543, 2])
We keep 1.22e+06/5.53e+07 =  2% of the original kernel matrix.

torch.Size([157652, 2])
We keep 1.24e+08/9.68e+09 =  1% of the original kernel matrix.

torch.Size([84016, 2])
We keep 2.50e+07/2.65e+09 =  0% of the original kernel matrix.

torch.Size([12573, 2])
We keep 1.49e+06/3.74e+07 =  3% of the original kernel matrix.

torch.Size([23837, 2])
We keep 2.72e+06/1.65e+08 =  1% of the original kernel matrix.

torch.Size([1552, 2])
We keep 3.64e+04/4.03e+05 =  9% of the original kernel matrix.

torch.Size([10224, 2])
We keep 5.90e+05/1.71e+07 =  3% of the original kernel matrix.

torch.Size([12441, 2])
We keep 1.45e+06/4.29e+07 =  3% of the original kernel matrix.

torch.Size([23907, 2])
We keep 2.84e+06/1.77e+08 =  1% of the original kernel matrix.

torch.Size([23372, 2])
We keep 1.02e+07/2.45e+08 =  4% of the original kernel matrix.

torch.Size([32341, 2])
We keep 5.51e+06/4.22e+08 =  1% of the original kernel matrix.

torch.Size([11025, 2])
We keep 1.15e+06/3.16e+07 =  3% of the original kernel matrix.

torch.Size([22126, 2])
We keep 2.52e+06/1.51e+08 =  1% of the original kernel matrix.

torch.Size([35154, 2])
We keep 1.48e+07/5.64e+08 =  2% of the original kernel matrix.

torch.Size([40495, 2])
We keep 7.81e+06/6.40e+08 =  1% of the original kernel matrix.

torch.Size([4569, 2])
We keep 1.64e+06/1.80e+07 =  9% of the original kernel matrix.

torch.Size([13236, 2])
We keep 1.98e+06/1.14e+08 =  1% of the original kernel matrix.

torch.Size([6243, 2])
We keep 1.26e+06/1.63e+07 =  7% of the original kernel matrix.

torch.Size([16634, 2])
We keep 1.91e+06/1.09e+08 =  1% of the original kernel matrix.

torch.Size([3195, 2])
We keep 9.65e+04/1.55e+06 =  6% of the original kernel matrix.

torch.Size([13579, 2])
We keep 9.04e+05/3.35e+07 =  2% of the original kernel matrix.

torch.Size([5298, 2])
We keep 2.41e+05/5.12e+06 =  4% of the original kernel matrix.

torch.Size([16816, 2])
We keep 1.36e+06/6.10e+07 =  2% of the original kernel matrix.

torch.Size([33910, 2])
We keep 8.15e+06/4.23e+08 =  1% of the original kernel matrix.

torch.Size([39652, 2])
We keep 6.84e+06/5.54e+08 =  1% of the original kernel matrix.

torch.Size([15826, 2])
We keep 3.34e+06/9.68e+07 =  3% of the original kernel matrix.

torch.Size([26195, 2])
We keep 3.86e+06/2.65e+08 =  1% of the original kernel matrix.

torch.Size([2198, 2])
We keep 1.59e+06/4.18e+06 = 37% of the original kernel matrix.

torch.Size([10007, 2])
We keep 1.13e+06/5.51e+07 =  2% of the original kernel matrix.

torch.Size([38209, 2])
We keep 1.86e+07/8.24e+08 =  2% of the original kernel matrix.

torch.Size([41293, 2])
We keep 8.87e+06/7.73e+08 =  1% of the original kernel matrix.

torch.Size([1886, 2])
We keep 8.13e+04/8.39e+05 =  9% of the original kernel matrix.

torch.Size([10387, 2])
We keep 7.37e+05/2.47e+07 =  2% of the original kernel matrix.

torch.Size([15772, 2])
We keep 2.28e+06/6.02e+07 =  3% of the original kernel matrix.

torch.Size([26760, 2])
We keep 3.18e+06/2.09e+08 =  1% of the original kernel matrix.

torch.Size([1744, 2])
We keep 3.94e+04/4.89e+05 =  8% of the original kernel matrix.

torch.Size([10733, 2])
We keep 6.12e+05/1.88e+07 =  3% of the original kernel matrix.

torch.Size([22119, 2])
We keep 3.44e+06/1.41e+08 =  2% of the original kernel matrix.

torch.Size([31881, 2])
We keep 4.48e+06/3.20e+08 =  1% of the original kernel matrix.

torch.Size([2748, 2])
We keep 2.19e+05/2.16e+06 = 10% of the original kernel matrix.

torch.Size([12194, 2])
We keep 1.03e+06/3.96e+07 =  2% of the original kernel matrix.

torch.Size([11519, 2])
We keep 3.37e+06/5.50e+07 =  6% of the original kernel matrix.

torch.Size([22416, 2])
We keep 3.14e+06/2.00e+08 =  1% of the original kernel matrix.

torch.Size([10297, 2])
We keep 1.19e+06/2.70e+07 =  4% of the original kernel matrix.

torch.Size([21643, 2])
We keep 2.40e+06/1.40e+08 =  1% of the original kernel matrix.

torch.Size([12624, 2])
We keep 1.71e+06/4.46e+07 =  3% of the original kernel matrix.

torch.Size([23904, 2])
We keep 2.89e+06/1.80e+08 =  1% of the original kernel matrix.

torch.Size([38524, 2])
We keep 1.03e+07/5.50e+08 =  1% of the original kernel matrix.

torch.Size([40594, 2])
We keep 7.30e+06/6.32e+08 =  1% of the original kernel matrix.

torch.Size([40289, 2])
We keep 2.01e+07/8.73e+08 =  2% of the original kernel matrix.

torch.Size([41925, 2])
We keep 9.05e+06/7.96e+08 =  1% of the original kernel matrix.

torch.Size([7944, 2])
We keep 7.12e+05/1.43e+07 =  4% of the original kernel matrix.

torch.Size([19254, 2])
We keep 1.93e+06/1.02e+08 =  1% of the original kernel matrix.

torch.Size([5193, 2])
We keep 7.95e+05/1.35e+07 =  5% of the original kernel matrix.

torch.Size([14787, 2])
We keep 1.83e+06/9.92e+07 =  1% of the original kernel matrix.

torch.Size([72462, 2])
We keep 5.19e+07/2.26e+09 =  2% of the original kernel matrix.

torch.Size([55769, 2])
We keep 1.36e+07/1.28e+09 =  1% of the original kernel matrix.

torch.Size([5416, 2])
We keep 3.34e+05/6.86e+06 =  4% of the original kernel matrix.

torch.Size([16367, 2])
We keep 1.48e+06/7.06e+07 =  2% of the original kernel matrix.

torch.Size([450508, 2])
We keep 5.84e+08/7.11e+10 =  0% of the original kernel matrix.

torch.Size([146019, 2])
We keep 6.01e+07/7.18e+09 =  0% of the original kernel matrix.

torch.Size([1048, 2])
We keep 1.33e+04/1.15e+05 = 11% of the original kernel matrix.

torch.Size([8947, 2])
We keep 3.87e+05/9.14e+06 =  4% of the original kernel matrix.

torch.Size([40835, 2])
We keep 1.18e+07/6.91e+08 =  1% of the original kernel matrix.

torch.Size([43167, 2])
We keep 8.38e+06/7.09e+08 =  1% of the original kernel matrix.

torch.Size([239211, 2])
We keep 1.97e+08/2.45e+10 =  0% of the original kernel matrix.

torch.Size([105449, 2])
We keep 3.77e+07/4.22e+09 =  0% of the original kernel matrix.

torch.Size([1469, 2])
We keep 2.25e+04/2.37e+05 =  9% of the original kernel matrix.

torch.Size([10206, 2])
We keep 4.85e+05/1.31e+07 =  3% of the original kernel matrix.

torch.Size([31477, 2])
We keep 2.77e+07/1.07e+09 =  2% of the original kernel matrix.

torch.Size([34104, 2])
We keep 9.77e+06/8.80e+08 =  1% of the original kernel matrix.

torch.Size([12937, 2])
We keep 6.43e+06/1.64e+08 =  3% of the original kernel matrix.

torch.Size([22034, 2])
We keep 4.79e+06/3.45e+08 =  1% of the original kernel matrix.

torch.Size([311768, 2])
We keep 7.82e+08/4.69e+10 =  1% of the original kernel matrix.

torch.Size([120298, 2])
We keep 5.01e+07/5.84e+09 =  0% of the original kernel matrix.

torch.Size([5329, 2])
We keep 2.05e+05/4.47e+06 =  4% of the original kernel matrix.

torch.Size([16623, 2])
We keep 1.27e+06/5.70e+07 =  2% of the original kernel matrix.

torch.Size([17340, 2])
We keep 3.64e+06/1.14e+08 =  3% of the original kernel matrix.

torch.Size([27313, 2])
We keep 4.03e+06/2.87e+08 =  1% of the original kernel matrix.

torch.Size([1646, 2])
We keep 1.56e+05/8.52e+05 = 18% of the original kernel matrix.

torch.Size([9747, 2])
We keep 7.41e+05/2.49e+07 =  2% of the original kernel matrix.

torch.Size([6688, 2])
We keep 4.20e+05/9.24e+06 =  4% of the original kernel matrix.

torch.Size([18700, 2])
We keep 1.70e+06/8.19e+07 =  2% of the original kernel matrix.

torch.Size([6271, 2])
We keep 7.06e+05/1.32e+07 =  5% of the original kernel matrix.

torch.Size([16834, 2])
We keep 1.87e+06/9.80e+07 =  1% of the original kernel matrix.

torch.Size([29349, 2])
We keep 1.25e+07/3.83e+08 =  3% of the original kernel matrix.

torch.Size([36548, 2])
We keep 6.70e+06/5.27e+08 =  1% of the original kernel matrix.

torch.Size([9487, 2])
We keep 1.37e+06/3.08e+07 =  4% of the original kernel matrix.

torch.Size([20485, 2])
We keep 2.47e+06/1.50e+08 =  1% of the original kernel matrix.

torch.Size([36459, 2])
We keep 1.65e+07/6.73e+08 =  2% of the original kernel matrix.

torch.Size([40336, 2])
We keep 8.33e+06/6.99e+08 =  1% of the original kernel matrix.

torch.Size([8525, 2])
We keep 6.61e+05/1.67e+07 =  3% of the original kernel matrix.

torch.Size([19823, 2])
We keep 2.02e+06/1.10e+08 =  1% of the original kernel matrix.

torch.Size([1155, 2])
We keep 1.54e+04/1.56e+05 =  9% of the original kernel matrix.

torch.Size([9365, 2])
We keep 4.31e+05/1.06e+07 =  4% of the original kernel matrix.

torch.Size([3785, 2])
We keep 1.92e+05/2.90e+06 =  6% of the original kernel matrix.

torch.Size([14200, 2])
We keep 1.10e+06/4.59e+07 =  2% of the original kernel matrix.

torch.Size([57590, 2])
We keep 5.36e+07/2.34e+09 =  2% of the original kernel matrix.

torch.Size([46862, 2])
We keep 1.38e+07/1.30e+09 =  1% of the original kernel matrix.

torch.Size([17592, 2])
We keep 1.47e+07/3.21e+08 =  4% of the original kernel matrix.

torch.Size([26411, 2])
We keep 6.14e+06/4.83e+08 =  1% of the original kernel matrix.

torch.Size([245351, 2])
We keep 2.65e+08/2.24e+10 =  1% of the original kernel matrix.

torch.Size([107216, 2])
We keep 3.60e+07/4.03e+09 =  0% of the original kernel matrix.

torch.Size([19396, 2])
We keep 6.68e+06/1.49e+08 =  4% of the original kernel matrix.

torch.Size([30043, 2])
We keep 4.64e+06/3.29e+08 =  1% of the original kernel matrix.

torch.Size([5433, 2])
We keep 5.06e+05/7.90e+06 =  6% of the original kernel matrix.

torch.Size([16109, 2])
We keep 1.57e+06/7.57e+07 =  2% of the original kernel matrix.

torch.Size([2817, 2])
We keep 1.17e+05/1.54e+06 =  7% of the original kernel matrix.

torch.Size([12632, 2])
We keep 8.85e+05/3.34e+07 =  2% of the original kernel matrix.

torch.Size([3992, 2])
We keep 6.76e+05/6.48e+06 = 10% of the original kernel matrix.

torch.Size([13663, 2])
We keep 1.47e+06/6.86e+07 =  2% of the original kernel matrix.

torch.Size([236763, 2])
We keep 3.22e+08/2.74e+10 =  1% of the original kernel matrix.

torch.Size([105501, 2])
We keep 4.00e+07/4.46e+09 =  0% of the original kernel matrix.

torch.Size([9976, 2])
We keep 7.18e+05/2.03e+07 =  3% of the original kernel matrix.

torch.Size([21840, 2])
We keep 2.18e+06/1.21e+08 =  1% of the original kernel matrix.

torch.Size([6348, 2])
We keep 4.48e+05/8.54e+06 =  5% of the original kernel matrix.

torch.Size([17828, 2])
We keep 1.62e+06/7.88e+07 =  2% of the original kernel matrix.

torch.Size([32440, 2])
We keep 1.44e+07/5.72e+08 =  2% of the original kernel matrix.

torch.Size([37252, 2])
We keep 7.60e+06/6.45e+08 =  1% of the original kernel matrix.

torch.Size([21975, 2])
We keep 4.92e+06/2.16e+08 =  2% of the original kernel matrix.

torch.Size([30396, 2])
We keep 5.23e+06/3.96e+08 =  1% of the original kernel matrix.

torch.Size([3540, 2])
We keep 2.35e+05/2.39e+06 =  9% of the original kernel matrix.

torch.Size([13897, 2])
We keep 1.00e+06/4.17e+07 =  2% of the original kernel matrix.

torch.Size([5083, 2])
We keep 3.83e+05/7.01e+06 =  5% of the original kernel matrix.

torch.Size([15858, 2])
We keep 1.52e+06/7.13e+07 =  2% of the original kernel matrix.

torch.Size([86530, 2])
We keep 2.01e+08/7.05e+09 =  2% of the original kernel matrix.

torch.Size([58770, 2])
We keep 2.20e+07/2.26e+09 =  0% of the original kernel matrix.

torch.Size([9975, 2])
We keep 1.01e+06/2.54e+07 =  3% of the original kernel matrix.

torch.Size([21627, 2])
We keep 2.35e+06/1.36e+08 =  1% of the original kernel matrix.

torch.Size([17452, 2])
We keep 2.21e+07/4.05e+08 =  5% of the original kernel matrix.

torch.Size([26174, 2])
We keep 6.86e+06/5.42e+08 =  1% of the original kernel matrix.

torch.Size([15669, 2])
We keep 2.61e+06/6.67e+07 =  3% of the original kernel matrix.

torch.Size([26672, 2])
We keep 3.40e+06/2.20e+08 =  1% of the original kernel matrix.

torch.Size([22521, 2])
We keep 1.21e+07/2.62e+08 =  4% of the original kernel matrix.

torch.Size([31760, 2])
We keep 5.71e+06/4.36e+08 =  1% of the original kernel matrix.

torch.Size([71392, 2])
We keep 6.76e+07/2.76e+09 =  2% of the original kernel matrix.

torch.Size([54396, 2])
We keep 1.49e+07/1.42e+09 =  1% of the original kernel matrix.

torch.Size([74945, 2])
We keep 4.80e+07/2.53e+09 =  1% of the original kernel matrix.

torch.Size([56315, 2])
We keep 1.42e+07/1.36e+09 =  1% of the original kernel matrix.

torch.Size([86764, 2])
We keep 5.97e+07/2.68e+09 =  2% of the original kernel matrix.

torch.Size([61709, 2])
We keep 1.44e+07/1.39e+09 =  1% of the original kernel matrix.

torch.Size([6569, 2])
We keep 1.01e+06/1.41e+07 =  7% of the original kernel matrix.

torch.Size([17800, 2])
We keep 1.91e+06/1.01e+08 =  1% of the original kernel matrix.

torch.Size([33657, 2])
We keep 1.27e+07/4.18e+08 =  3% of the original kernel matrix.

torch.Size([39582, 2])
We keep 6.86e+06/5.51e+08 =  1% of the original kernel matrix.

torch.Size([7238, 2])
We keep 2.15e+06/2.14e+07 = 10% of the original kernel matrix.

torch.Size([18250, 2])
We keep 2.23e+06/1.25e+08 =  1% of the original kernel matrix.

torch.Size([5050, 2])
We keep 2.30e+05/4.58e+06 =  5% of the original kernel matrix.

torch.Size([16374, 2])
We keep 1.30e+06/5.76e+07 =  2% of the original kernel matrix.

torch.Size([12267, 2])
We keep 1.27e+06/3.74e+07 =  3% of the original kernel matrix.

torch.Size([23771, 2])
We keep 2.71e+06/1.65e+08 =  1% of the original kernel matrix.

torch.Size([2909, 2])
We keep 8.35e+04/1.35e+06 =  6% of the original kernel matrix.

torch.Size([13354, 2])
We keep 8.70e+05/3.13e+07 =  2% of the original kernel matrix.

torch.Size([12930, 2])
We keep 7.24e+06/8.11e+07 =  8% of the original kernel matrix.

torch.Size([23953, 2])
We keep 3.41e+06/2.43e+08 =  1% of the original kernel matrix.

torch.Size([5578, 2])
We keep 3.43e+05/6.32e+06 =  5% of the original kernel matrix.

torch.Size([16694, 2])
We keep 1.44e+06/6.77e+07 =  2% of the original kernel matrix.

torch.Size([1355, 2])
We keep 7.35e+04/5.01e+05 = 14% of the original kernel matrix.

torch.Size([8789, 2])
We keep 6.00e+05/1.91e+07 =  3% of the original kernel matrix.

torch.Size([11790, 2])
We keep 1.34e+06/3.77e+07 =  3% of the original kernel matrix.

torch.Size([22924, 2])
We keep 2.73e+06/1.66e+08 =  1% of the original kernel matrix.

torch.Size([3583, 2])
We keep 4.78e+05/6.21e+06 =  7% of the original kernel matrix.

torch.Size([12967, 2])
We keep 1.44e+06/6.71e+07 =  2% of the original kernel matrix.

torch.Size([20723, 2])
We keep 2.38e+07/4.24e+08 =  5% of the original kernel matrix.

torch.Size([29025, 2])
We keep 6.97e+06/5.55e+08 =  1% of the original kernel matrix.

torch.Size([4559, 2])
We keep 4.24e+05/7.01e+06 =  6% of the original kernel matrix.

torch.Size([14771, 2])
We keep 1.49e+06/7.13e+07 =  2% of the original kernel matrix.

torch.Size([16067, 2])
We keep 2.03e+06/6.53e+07 =  3% of the original kernel matrix.

torch.Size([27088, 2])
We keep 3.33e+06/2.18e+08 =  1% of the original kernel matrix.

torch.Size([3659, 2])
We keep 7.96e+05/8.60e+06 =  9% of the original kernel matrix.

torch.Size([11395, 2])
We keep 1.50e+06/7.90e+07 =  1% of the original kernel matrix.

torch.Size([14579, 2])
We keep 3.17e+06/7.96e+07 =  3% of the original kernel matrix.

torch.Size([25545, 2])
We keep 3.64e+06/2.40e+08 =  1% of the original kernel matrix.

torch.Size([2556, 2])
We keep 6.99e+04/9.47e+05 =  7% of the original kernel matrix.

torch.Size([12692, 2])
We keep 7.68e+05/2.62e+07 =  2% of the original kernel matrix.

torch.Size([4271, 2])
We keep 1.59e+05/3.08e+06 =  5% of the original kernel matrix.

torch.Size([15230, 2])
We keep 1.13e+06/4.73e+07 =  2% of the original kernel matrix.

torch.Size([14220, 2])
We keep 1.15e+06/4.22e+07 =  2% of the original kernel matrix.

torch.Size([25436, 2])
We keep 2.82e+06/1.75e+08 =  1% of the original kernel matrix.

torch.Size([87178, 2])
We keep 5.98e+07/3.23e+09 =  1% of the original kernel matrix.

torch.Size([61463, 2])
We keep 1.58e+07/1.53e+09 =  1% of the original kernel matrix.

torch.Size([5197, 2])
We keep 3.30e+05/5.70e+06 =  5% of the original kernel matrix.

torch.Size([16048, 2])
We keep 1.39e+06/6.43e+07 =  2% of the original kernel matrix.

torch.Size([709, 2])
We keep 5.22e+03/3.61e+04 = 14% of the original kernel matrix.

torch.Size([8347, 2])
We keep 2.81e+05/5.12e+06 =  5% of the original kernel matrix.

torch.Size([1438, 2])
We keep 3.15e+04/3.28e+05 =  9% of the original kernel matrix.

torch.Size([9637, 2])
We keep 5.33e+05/1.54e+07 =  3% of the original kernel matrix.

torch.Size([146313, 2])
We keep 1.61e+08/9.88e+09 =  1% of the original kernel matrix.

torch.Size([80539, 2])
We keep 2.53e+07/2.68e+09 =  0% of the original kernel matrix.

torch.Size([185011, 2])
We keep 2.03e+08/1.34e+10 =  1% of the original kernel matrix.

torch.Size([91988, 2])
We keep 2.84e+07/3.12e+09 =  0% of the original kernel matrix.

torch.Size([11161, 2])
We keep 2.23e+06/5.80e+07 =  3% of the original kernel matrix.

torch.Size([21526, 2])
We keep 3.15e+06/2.05e+08 =  1% of the original kernel matrix.

torch.Size([4600, 2])
We keep 2.77e+05/4.52e+06 =  6% of the original kernel matrix.

torch.Size([15308, 2])
We keep 1.30e+06/5.73e+07 =  2% of the original kernel matrix.

torch.Size([39570, 2])
We keep 3.00e+07/1.19e+09 =  2% of the original kernel matrix.

torch.Size([40783, 2])
We keep 1.05e+07/9.29e+08 =  1% of the original kernel matrix.

torch.Size([48419, 2])
We keep 1.41e+07/9.66e+08 =  1% of the original kernel matrix.

torch.Size([49379, 2])
We keep 9.76e+06/8.37e+08 =  1% of the original kernel matrix.

torch.Size([1490, 2])
We keep 2.47e+04/2.77e+05 =  8% of the original kernel matrix.

torch.Size([10323, 2])
We keep 5.17e+05/1.42e+07 =  3% of the original kernel matrix.

torch.Size([6891, 2])
We keep 4.14e+05/9.95e+06 =  4% of the original kernel matrix.

torch.Size([18510, 2])
We keep 1.68e+06/8.50e+07 =  1% of the original kernel matrix.

torch.Size([13175, 2])
We keep 1.04e+06/3.67e+07 =  2% of the original kernel matrix.

torch.Size([24465, 2])
We keep 2.70e+06/1.63e+08 =  1% of the original kernel matrix.

torch.Size([9391, 2])
We keep 9.73e+06/1.46e+08 =  6% of the original kernel matrix.

torch.Size([19096, 2])
We keep 4.41e+06/3.25e+08 =  1% of the original kernel matrix.

torch.Size([4289, 2])
We keep 1.96e+05/3.23e+06 =  6% of the original kernel matrix.

torch.Size([15251, 2])
We keep 1.13e+06/4.84e+07 =  2% of the original kernel matrix.

torch.Size([103643, 2])
We keep 5.88e+07/3.83e+09 =  1% of the original kernel matrix.

torch.Size([67373, 2])
We keep 1.68e+07/1.67e+09 =  1% of the original kernel matrix.

torch.Size([5236, 2])
We keep 2.62e+05/4.86e+06 =  5% of the original kernel matrix.

torch.Size([16362, 2])
We keep 1.32e+06/5.94e+07 =  2% of the original kernel matrix.

torch.Size([7660, 2])
We keep 2.81e+06/1.83e+07 = 15% of the original kernel matrix.

torch.Size([18946, 2])
We keep 1.91e+06/1.15e+08 =  1% of the original kernel matrix.

torch.Size([1353, 2])
We keep 2.33e+04/2.37e+05 =  9% of the original kernel matrix.

torch.Size([9785, 2])
We keep 5.06e+05/1.31e+07 =  3% of the original kernel matrix.

torch.Size([12781, 2])
We keep 1.74e+06/4.49e+07 =  3% of the original kernel matrix.

torch.Size([24023, 2])
We keep 2.86e+06/1.81e+08 =  1% of the original kernel matrix.

torch.Size([10005, 2])
We keep 8.14e+05/2.12e+07 =  3% of the original kernel matrix.

torch.Size([21594, 2])
We keep 2.22e+06/1.24e+08 =  1% of the original kernel matrix.

torch.Size([10691, 2])
We keep 1.47e+06/3.82e+07 =  3% of the original kernel matrix.

torch.Size([21884, 2])
We keep 2.67e+06/1.67e+08 =  1% of the original kernel matrix.

torch.Size([35683, 2])
We keep 2.35e+07/5.87e+08 =  3% of the original kernel matrix.

torch.Size([39415, 2])
We keep 7.68e+06/6.53e+08 =  1% of the original kernel matrix.

torch.Size([231613, 2])
We keep 1.77e+08/2.01e+10 =  0% of the original kernel matrix.

torch.Size([103399, 2])
We keep 3.42e+07/3.82e+09 =  0% of the original kernel matrix.

torch.Size([4958, 2])
We keep 2.32e+05/4.22e+06 =  5% of the original kernel matrix.

torch.Size([16077, 2])
We keep 1.26e+06/5.54e+07 =  2% of the original kernel matrix.

torch.Size([2241, 2])
We keep 6.80e+04/9.37e+05 =  7% of the original kernel matrix.

torch.Size([11571, 2])
We keep 7.59e+05/2.61e+07 =  2% of the original kernel matrix.

torch.Size([12642, 2])
We keep 1.02e+06/3.41e+07 =  2% of the original kernel matrix.

torch.Size([23917, 2])
We keep 2.62e+06/1.57e+08 =  1% of the original kernel matrix.

torch.Size([84277, 2])
We keep 5.22e+08/3.11e+09 = 16% of the original kernel matrix.

torch.Size([61140, 2])
We keep 1.56e+07/1.50e+09 =  1% of the original kernel matrix.

torch.Size([13969, 2])
We keep 1.40e+06/4.80e+07 =  2% of the original kernel matrix.

torch.Size([25082, 2])
We keep 2.95e+06/1.87e+08 =  1% of the original kernel matrix.

torch.Size([1389, 2])
We keep 3.99e+04/2.94e+05 = 13% of the original kernel matrix.

torch.Size([9438, 2])
We keep 5.12e+05/1.46e+07 =  3% of the original kernel matrix.

torch.Size([1289, 2])
We keep 3.89e+04/3.97e+05 =  9% of the original kernel matrix.

torch.Size([9160, 2])
We keep 5.90e+05/1.70e+07 =  3% of the original kernel matrix.

torch.Size([33440, 2])
We keep 3.11e+07/4.66e+08 =  6% of the original kernel matrix.

torch.Size([38834, 2])
We keep 6.92e+06/5.82e+08 =  1% of the original kernel matrix.

torch.Size([1166, 2])
We keep 1.64e+04/1.62e+05 = 10% of the original kernel matrix.

torch.Size([9634, 2])
We keep 4.36e+05/1.09e+07 =  4% of the original kernel matrix.

torch.Size([23353, 2])
We keep 1.28e+07/3.45e+08 =  3% of the original kernel matrix.

torch.Size([30989, 2])
We keep 6.28e+06/5.00e+08 =  1% of the original kernel matrix.

torch.Size([2302, 2])
We keep 7.35e+04/9.47e+05 =  7% of the original kernel matrix.

torch.Size([11811, 2])
We keep 7.73e+05/2.62e+07 =  2% of the original kernel matrix.

torch.Size([11777, 2])
We keep 8.47e+05/2.84e+07 =  2% of the original kernel matrix.

torch.Size([23345, 2])
We keep 2.45e+06/1.44e+08 =  1% of the original kernel matrix.

torch.Size([7338, 2])
We keep 8.86e+05/1.47e+07 =  6% of the original kernel matrix.

torch.Size([18550, 2])
We keep 1.94e+06/1.03e+08 =  1% of the original kernel matrix.

torch.Size([3565, 2])
We keep 9.65e+04/1.73e+06 =  5% of the original kernel matrix.

torch.Size([14402, 2])
We keep 9.40e+05/3.54e+07 =  2% of the original kernel matrix.

torch.Size([2409, 2])
We keep 5.20e+04/7.57e+05 =  6% of the original kernel matrix.

torch.Size([12329, 2])
We keep 7.18e+05/2.34e+07 =  3% of the original kernel matrix.

torch.Size([103854, 2])
We keep 4.68e+07/3.84e+09 =  1% of the original kernel matrix.

torch.Size([67678, 2])
We keep 1.67e+07/1.67e+09 =  1% of the original kernel matrix.

torch.Size([225417, 2])
We keep 2.03e+08/1.92e+10 =  1% of the original kernel matrix.

torch.Size([102328, 2])
We keep 3.36e+07/3.74e+09 =  0% of the original kernel matrix.

torch.Size([63100, 2])
We keep 5.04e+07/1.76e+09 =  2% of the original kernel matrix.

torch.Size([52903, 2])
We keep 1.23e+07/1.13e+09 =  1% of the original kernel matrix.

torch.Size([12782, 2])
We keep 3.35e+06/5.89e+07 =  5% of the original kernel matrix.

torch.Size([23563, 2])
We keep 3.21e+06/2.07e+08 =  1% of the original kernel matrix.

torch.Size([18519, 2])
We keep 2.30e+06/8.96e+07 =  2% of the original kernel matrix.

torch.Size([29094, 2])
We keep 3.76e+06/2.55e+08 =  1% of the original kernel matrix.

torch.Size([5263, 2])
We keep 3.01e+05/6.13e+06 =  4% of the original kernel matrix.

torch.Size([16044, 2])
We keep 1.43e+06/6.67e+07 =  2% of the original kernel matrix.

torch.Size([7498, 2])
We keep 5.52e+05/1.13e+07 =  4% of the original kernel matrix.

torch.Size([19231, 2])
We keep 1.79e+06/9.06e+07 =  1% of the original kernel matrix.

torch.Size([33091, 2])
We keep 6.66e+06/3.85e+08 =  1% of the original kernel matrix.

torch.Size([39754, 2])
We keep 6.64e+06/5.29e+08 =  1% of the original kernel matrix.

torch.Size([22239, 2])
We keep 2.70e+06/1.14e+08 =  2% of the original kernel matrix.

torch.Size([33765, 2])
We keep 4.20e+06/2.88e+08 =  1% of the original kernel matrix.

torch.Size([22778, 2])
We keep 1.51e+07/2.20e+08 =  6% of the original kernel matrix.

torch.Size([31962, 2])
We keep 5.33e+06/4.00e+08 =  1% of the original kernel matrix.

torch.Size([10334, 2])
We keep 1.14e+06/2.92e+07 =  3% of the original kernel matrix.

torch.Size([21675, 2])
We keep 2.47e+06/1.46e+08 =  1% of the original kernel matrix.

torch.Size([149011, 2])
We keep 3.58e+08/1.75e+10 =  2% of the original kernel matrix.

torch.Size([78668, 2])
We keep 3.26e+07/3.56e+09 =  0% of the original kernel matrix.

torch.Size([12298, 2])
We keep 1.94e+06/3.90e+07 =  4% of the original kernel matrix.

torch.Size([23659, 2])
We keep 2.69e+06/1.68e+08 =  1% of the original kernel matrix.

torch.Size([18452, 2])
We keep 3.32e+06/1.18e+08 =  2% of the original kernel matrix.

torch.Size([29903, 2])
We keep 4.27e+06/2.93e+08 =  1% of the original kernel matrix.

torch.Size([96781, 2])
We keep 9.23e+07/4.72e+09 =  1% of the original kernel matrix.

torch.Size([65822, 2])
We keep 1.89e+07/1.85e+09 =  1% of the original kernel matrix.

torch.Size([34631, 2])
We keep 6.07e+06/3.98e+08 =  1% of the original kernel matrix.

torch.Size([40647, 2])
We keep 6.69e+06/5.38e+08 =  1% of the original kernel matrix.

torch.Size([21722, 2])
We keep 4.41e+07/2.79e+08 = 15% of the original kernel matrix.

torch.Size([31235, 2])
We keep 5.89e+06/4.50e+08 =  1% of the original kernel matrix.

torch.Size([70548, 2])
We keep 4.78e+07/2.13e+09 =  2% of the original kernel matrix.

torch.Size([55189, 2])
We keep 1.32e+07/1.24e+09 =  1% of the original kernel matrix.

torch.Size([33084, 2])
We keep 5.90e+06/3.74e+08 =  1% of the original kernel matrix.

torch.Size([38959, 2])
We keep 6.53e+06/5.21e+08 =  1% of the original kernel matrix.

torch.Size([21220, 2])
We keep 5.06e+06/1.58e+08 =  3% of the original kernel matrix.

torch.Size([30944, 2])
We keep 4.58e+06/3.38e+08 =  1% of the original kernel matrix.

torch.Size([5323, 2])
We keep 4.99e+05/7.34e+06 =  6% of the original kernel matrix.

torch.Size([15911, 2])
We keep 1.46e+06/7.30e+07 =  2% of the original kernel matrix.

torch.Size([14360, 2])
We keep 2.64e+06/5.76e+07 =  4% of the original kernel matrix.

torch.Size([25618, 2])
We keep 3.14e+06/2.05e+08 =  1% of the original kernel matrix.

torch.Size([6934, 2])
We keep 7.91e+05/1.23e+07 =  6% of the original kernel matrix.

torch.Size([18334, 2])
We keep 1.79e+06/9.44e+07 =  1% of the original kernel matrix.

torch.Size([50582, 2])
We keep 1.58e+07/9.40e+08 =  1% of the original kernel matrix.

torch.Size([48598, 2])
We keep 9.48e+06/8.26e+08 =  1% of the original kernel matrix.

torch.Size([235222, 2])
We keep 2.31e+08/2.12e+10 =  1% of the original kernel matrix.

torch.Size([103747, 2])
We keep 3.50e+07/3.92e+09 =  0% of the original kernel matrix.

torch.Size([46634, 2])
We keep 3.86e+07/1.15e+09 =  3% of the original kernel matrix.

torch.Size([45414, 2])
We keep 1.02e+07/9.16e+08 =  1% of the original kernel matrix.

torch.Size([25580, 2])
We keep 1.98e+07/6.90e+08 =  2% of the original kernel matrix.

torch.Size([30512, 2])
We keep 8.49e+06/7.08e+08 =  1% of the original kernel matrix.

torch.Size([7195, 2])
We keep 6.59e+05/1.15e+07 =  5% of the original kernel matrix.

torch.Size([18872, 2])
We keep 1.76e+06/9.14e+07 =  1% of the original kernel matrix.

torch.Size([12212, 2])
We keep 6.35e+06/1.32e+08 =  4% of the original kernel matrix.

torch.Size([22426, 2])
We keep 4.40e+06/3.09e+08 =  1% of the original kernel matrix.

torch.Size([45340, 2])
We keep 9.93e+06/6.89e+08 =  1% of the original kernel matrix.

torch.Size([47565, 2])
We keep 8.12e+06/7.07e+08 =  1% of the original kernel matrix.

torch.Size([8864, 2])
We keep 1.95e+06/3.08e+07 =  6% of the original kernel matrix.

torch.Size([19575, 2])
We keep 2.52e+06/1.50e+08 =  1% of the original kernel matrix.

torch.Size([34304, 2])
We keep 1.57e+07/5.67e+08 =  2% of the original kernel matrix.

torch.Size([37536, 2])
We keep 7.27e+06/6.41e+08 =  1% of the original kernel matrix.

torch.Size([146304, 2])
We keep 7.52e+07/7.68e+09 =  0% of the original kernel matrix.

torch.Size([81002, 2])
We keep 2.27e+07/2.36e+09 =  0% of the original kernel matrix.

torch.Size([5042, 2])
We keep 2.56e+06/2.26e+07 = 11% of the original kernel matrix.

torch.Size([13750, 2])
We keep 2.21e+06/1.28e+08 =  1% of the original kernel matrix.

torch.Size([33931, 2])
We keep 7.55e+07/1.57e+09 =  4% of the original kernel matrix.

torch.Size([35758, 2])
We keep 1.13e+07/1.07e+09 =  1% of the original kernel matrix.

torch.Size([46443, 2])
We keep 3.73e+07/1.23e+09 =  3% of the original kernel matrix.

torch.Size([44719, 2])
We keep 1.05e+07/9.46e+08 =  1% of the original kernel matrix.

torch.Size([18710, 2])
We keep 2.35e+06/9.67e+07 =  2% of the original kernel matrix.

torch.Size([28992, 2])
We keep 3.84e+06/2.65e+08 =  1% of the original kernel matrix.

torch.Size([3127, 2])
We keep 1.41e+05/1.88e+06 =  7% of the original kernel matrix.

torch.Size([13369, 2])
We keep 9.67e+05/3.69e+07 =  2% of the original kernel matrix.

torch.Size([37898, 2])
We keep 2.02e+07/5.66e+08 =  3% of the original kernel matrix.

torch.Size([40127, 2])
We keep 7.19e+06/6.41e+08 =  1% of the original kernel matrix.

torch.Size([46870, 2])
We keep 1.16e+07/8.22e+08 =  1% of the original kernel matrix.

torch.Size([46864, 2])
We keep 8.95e+06/7.73e+08 =  1% of the original kernel matrix.

torch.Size([2163, 2])
We keep 7.14e+04/8.48e+05 =  8% of the original kernel matrix.

torch.Size([11332, 2])
We keep 7.41e+05/2.48e+07 =  2% of the original kernel matrix.

torch.Size([286436, 2])
We keep 2.36e+08/3.19e+10 =  0% of the original kernel matrix.

torch.Size([117887, 2])
We keep 4.19e+07/4.81e+09 =  0% of the original kernel matrix.

torch.Size([20559, 2])
We keep 5.48e+06/1.98e+08 =  2% of the original kernel matrix.

torch.Size([30761, 2])
We keep 5.03e+06/3.79e+08 =  1% of the original kernel matrix.

torch.Size([22536, 2])
We keep 7.33e+06/2.08e+08 =  3% of the original kernel matrix.

torch.Size([31515, 2])
We keep 5.23e+06/3.88e+08 =  1% of the original kernel matrix.

torch.Size([5279, 2])
We keep 5.11e+05/4.81e+06 = 10% of the original kernel matrix.

torch.Size([16820, 2])
We keep 1.29e+06/5.91e+07 =  2% of the original kernel matrix.

torch.Size([1933, 2])
We keep 4.40e+04/5.66e+05 =  7% of the original kernel matrix.

torch.Size([11142, 2])
We keep 6.43e+05/2.03e+07 =  3% of the original kernel matrix.

torch.Size([1538, 2])
We keep 2.94e+04/3.02e+05 =  9% of the original kernel matrix.

torch.Size([10374, 2])
We keep 5.42e+05/1.48e+07 =  3% of the original kernel matrix.

torch.Size([1652, 2])
We keep 6.53e+04/6.35e+05 = 10% of the original kernel matrix.

torch.Size([9979, 2])
We keep 6.72e+05/2.15e+07 =  3% of the original kernel matrix.

torch.Size([11333, 2])
We keep 2.59e+06/3.98e+07 =  6% of the original kernel matrix.

torch.Size([22757, 2])
We keep 2.78e+06/1.70e+08 =  1% of the original kernel matrix.

torch.Size([17079, 2])
We keep 5.25e+06/1.14e+08 =  4% of the original kernel matrix.

torch.Size([27349, 2])
We keep 3.93e+06/2.88e+08 =  1% of the original kernel matrix.

torch.Size([1509620, 2])
We keep 1.69e+10/1.65e+12 =  1% of the original kernel matrix.

torch.Size([239437, 2])
We keep 2.55e+08/3.47e+10 =  0% of the original kernel matrix.

torch.Size([5927, 2])
We keep 3.94e+05/7.60e+06 =  5% of the original kernel matrix.

torch.Size([16985, 2])
We keep 1.52e+06/7.43e+07 =  2% of the original kernel matrix.

torch.Size([25208, 2])
We keep 4.70e+06/2.22e+08 =  2% of the original kernel matrix.

torch.Size([34052, 2])
We keep 5.33e+06/4.02e+08 =  1% of the original kernel matrix.

torch.Size([4764, 2])
We keep 3.02e+05/5.21e+06 =  5% of the original kernel matrix.

torch.Size([15658, 2])
We keep 1.36e+06/6.15e+07 =  2% of the original kernel matrix.

torch.Size([8500, 2])
We keep 5.34e+05/1.40e+07 =  3% of the original kernel matrix.

torch.Size([20039, 2])
We keep 1.91e+06/1.01e+08 =  1% of the original kernel matrix.

torch.Size([12711, 2])
We keep 1.74e+06/4.64e+07 =  3% of the original kernel matrix.

torch.Size([23981, 2])
We keep 2.99e+06/1.83e+08 =  1% of the original kernel matrix.

torch.Size([51588, 2])
We keep 5.14e+07/1.71e+09 =  3% of the original kernel matrix.

torch.Size([46162, 2])
We keep 1.21e+07/1.11e+09 =  1% of the original kernel matrix.

torch.Size([5317, 2])
We keep 3.47e+05/6.52e+06 =  5% of the original kernel matrix.

torch.Size([16409, 2])
We keep 1.45e+06/6.88e+07 =  2% of the original kernel matrix.

torch.Size([485844, 2])
We keep 5.73e+08/7.95e+10 =  0% of the original kernel matrix.

torch.Size([151138, 2])
We keep 6.28e+07/7.60e+09 =  0% of the original kernel matrix.

torch.Size([3413, 2])
We keep 1.03e+05/1.71e+06 =  6% of the original kernel matrix.

torch.Size([13918, 2])
We keep 9.27e+05/3.52e+07 =  2% of the original kernel matrix.

torch.Size([117907, 2])
We keep 8.80e+07/4.99e+09 =  1% of the original kernel matrix.

torch.Size([72268, 2])
We keep 1.88e+07/1.90e+09 =  0% of the original kernel matrix.

torch.Size([17914, 2])
We keep 1.51e+07/2.52e+08 =  5% of the original kernel matrix.

torch.Size([28231, 2])
We keep 5.62e+06/4.28e+08 =  1% of the original kernel matrix.

torch.Size([42629, 2])
We keep 1.79e+07/8.35e+08 =  2% of the original kernel matrix.

torch.Size([43252, 2])
We keep 8.97e+06/7.79e+08 =  1% of the original kernel matrix.

torch.Size([107857, 2])
We keep 7.57e+07/5.83e+09 =  1% of the original kernel matrix.

torch.Size([68640, 2])
We keep 2.00e+07/2.06e+09 =  0% of the original kernel matrix.

torch.Size([9187, 2])
We keep 6.36e+05/1.62e+07 =  3% of the original kernel matrix.

torch.Size([20561, 2])
We keep 2.00e+06/1.08e+08 =  1% of the original kernel matrix.

torch.Size([2549, 2])
We keep 1.12e+05/1.31e+06 =  8% of the original kernel matrix.

torch.Size([12270, 2])
We keep 8.64e+05/3.09e+07 =  2% of the original kernel matrix.

torch.Size([68323, 2])
We keep 2.33e+07/1.74e+09 =  1% of the original kernel matrix.

torch.Size([55554, 2])
We keep 1.22e+07/1.12e+09 =  1% of the original kernel matrix.

torch.Size([5304, 2])
We keep 4.11e+05/7.06e+06 =  5% of the original kernel matrix.

torch.Size([16338, 2])
We keep 1.53e+06/7.16e+07 =  2% of the original kernel matrix.

torch.Size([8364, 2])
We keep 1.44e+06/2.53e+07 =  5% of the original kernel matrix.

torch.Size([18392, 2])
We keep 2.36e+06/1.36e+08 =  1% of the original kernel matrix.

torch.Size([30006, 2])
We keep 7.81e+06/3.71e+08 =  2% of the original kernel matrix.

torch.Size([37116, 2])
We keep 6.49e+06/5.19e+08 =  1% of the original kernel matrix.

torch.Size([9251, 2])
We keep 3.96e+06/4.80e+07 =  8% of the original kernel matrix.

torch.Size([20927, 2])
We keep 3.03e+06/1.87e+08 =  1% of the original kernel matrix.

torch.Size([219840, 2])
We keep 1.06e+08/1.48e+10 =  0% of the original kernel matrix.

torch.Size([108428, 2])
We keep 2.96e+07/3.28e+09 =  0% of the original kernel matrix.

torch.Size([8480, 2])
We keep 1.17e+06/2.13e+07 =  5% of the original kernel matrix.

torch.Size([19301, 2])
We keep 2.20e+06/1.24e+08 =  1% of the original kernel matrix.

torch.Size([92541, 2])
We keep 8.56e+07/3.98e+09 =  2% of the original kernel matrix.

torch.Size([66266, 2])
We keep 1.76e+07/1.70e+09 =  1% of the original kernel matrix.

torch.Size([168611, 2])
We keep 3.36e+08/1.94e+10 =  1% of the original kernel matrix.

torch.Size([82145, 2])
We keep 3.41e+07/3.75e+09 =  0% of the original kernel matrix.

torch.Size([7729, 2])
We keep 7.39e+05/1.46e+07 =  5% of the original kernel matrix.

torch.Size([18919, 2])
We keep 1.92e+06/1.03e+08 =  1% of the original kernel matrix.

torch.Size([4240, 2])
We keep 2.16e+05/3.74e+06 =  5% of the original kernel matrix.

torch.Size([15194, 2])
We keep 1.23e+06/5.21e+07 =  2% of the original kernel matrix.

torch.Size([41326, 2])
We keep 6.71e+07/1.11e+09 =  6% of the original kernel matrix.

torch.Size([41952, 2])
We keep 1.01e+07/8.99e+08 =  1% of the original kernel matrix.

torch.Size([10401, 2])
We keep 1.28e+06/3.35e+07 =  3% of the original kernel matrix.

torch.Size([21203, 2])
We keep 2.56e+06/1.56e+08 =  1% of the original kernel matrix.

torch.Size([21531, 2])
We keep 5.25e+06/1.48e+08 =  3% of the original kernel matrix.

torch.Size([31396, 2])
We keep 4.51e+06/3.28e+08 =  1% of the original kernel matrix.

torch.Size([10388, 2])
We keep 2.63e+06/4.40e+07 =  5% of the original kernel matrix.

torch.Size([21089, 2])
We keep 2.86e+06/1.79e+08 =  1% of the original kernel matrix.

torch.Size([18700, 2])
We keep 1.18e+07/3.29e+08 =  3% of the original kernel matrix.

torch.Size([27164, 2])
We keep 6.19e+06/4.89e+08 =  1% of the original kernel matrix.

torch.Size([25841, 2])
We keep 7.98e+06/3.13e+08 =  2% of the original kernel matrix.

torch.Size([33866, 2])
We keep 6.14e+06/4.76e+08 =  1% of the original kernel matrix.

torch.Size([40220, 2])
We keep 1.01e+08/2.46e+09 =  4% of the original kernel matrix.

torch.Size([37216, 2])
We keep 1.35e+07/1.34e+09 =  1% of the original kernel matrix.

torch.Size([16080, 2])
We keep 2.34e+06/7.61e+07 =  3% of the original kernel matrix.

torch.Size([27112, 2])
We keep 3.52e+06/2.35e+08 =  1% of the original kernel matrix.

torch.Size([10546, 2])
We keep 9.86e+05/2.52e+07 =  3% of the original kernel matrix.

torch.Size([21917, 2])
We keep 2.39e+06/1.35e+08 =  1% of the original kernel matrix.

torch.Size([2571, 2])
We keep 8.33e+04/1.09e+06 =  7% of the original kernel matrix.

torch.Size([12261, 2])
We keep 8.00e+05/2.82e+07 =  2% of the original kernel matrix.

torch.Size([13314, 2])
We keep 1.40e+06/4.54e+07 =  3% of the original kernel matrix.

torch.Size([25077, 2])
We keep 2.98e+06/1.82e+08 =  1% of the original kernel matrix.

torch.Size([13497, 2])
We keep 6.39e+06/1.47e+08 =  4% of the original kernel matrix.

torch.Size([23279, 2])
We keep 4.55e+06/3.26e+08 =  1% of the original kernel matrix.

torch.Size([39260, 2])
We keep 1.68e+07/6.84e+08 =  2% of the original kernel matrix.

torch.Size([41966, 2])
We keep 8.37e+06/7.05e+08 =  1% of the original kernel matrix.

torch.Size([82359, 2])
We keep 5.41e+07/2.49e+09 =  2% of the original kernel matrix.

torch.Size([60378, 2])
We keep 1.41e+07/1.34e+09 =  1% of the original kernel matrix.

torch.Size([5459, 2])
We keep 2.88e+05/5.18e+06 =  5% of the original kernel matrix.

torch.Size([16562, 2])
We keep 1.34e+06/6.14e+07 =  2% of the original kernel matrix.

torch.Size([17997, 2])
We keep 1.94e+06/8.68e+07 =  2% of the original kernel matrix.

torch.Size([28707, 2])
We keep 3.69e+06/2.51e+08 =  1% of the original kernel matrix.

torch.Size([18882, 2])
We keep 3.11e+06/1.05e+08 =  2% of the original kernel matrix.

torch.Size([29273, 2])
We keep 3.99e+06/2.76e+08 =  1% of the original kernel matrix.

torch.Size([43168, 2])
We keep 2.95e+07/7.33e+08 =  4% of the original kernel matrix.

torch.Size([44767, 2])
We keep 8.58e+06/7.30e+08 =  1% of the original kernel matrix.

torch.Size([22841, 2])
We keep 6.59e+06/2.26e+08 =  2% of the original kernel matrix.

torch.Size([32050, 2])
We keep 5.24e+06/4.06e+08 =  1% of the original kernel matrix.

torch.Size([39016, 2])
We keep 8.95e+06/5.61e+08 =  1% of the original kernel matrix.

torch.Size([40328, 2])
We keep 7.08e+06/6.38e+08 =  1% of the original kernel matrix.

torch.Size([35290, 2])
We keep 1.29e+09/6.58e+09 = 19% of the original kernel matrix.

torch.Size([32792, 2])
We keep 2.16e+07/2.19e+09 =  0% of the original kernel matrix.

torch.Size([19131, 2])
We keep 2.85e+06/1.15e+08 =  2% of the original kernel matrix.

torch.Size([29675, 2])
We keep 4.10e+06/2.89e+08 =  1% of the original kernel matrix.

torch.Size([1323, 2])
We keep 2.61e+04/2.49e+05 = 10% of the original kernel matrix.

torch.Size([9816, 2])
We keep 5.11e+05/1.34e+07 =  3% of the original kernel matrix.

torch.Size([11073, 2])
We keep 8.69e+05/2.59e+07 =  3% of the original kernel matrix.

torch.Size([22413, 2])
We keep 2.37e+06/1.37e+08 =  1% of the original kernel matrix.

torch.Size([32821, 2])
We keep 6.34e+06/4.13e+08 =  1% of the original kernel matrix.

torch.Size([39794, 2])
We keep 6.90e+06/5.48e+08 =  1% of the original kernel matrix.

torch.Size([10966, 2])
We keep 1.34e+06/3.70e+07 =  3% of the original kernel matrix.

torch.Size([22338, 2])
We keep 2.68e+06/1.64e+08 =  1% of the original kernel matrix.

torch.Size([27003, 2])
We keep 4.06e+06/2.28e+08 =  1% of the original kernel matrix.

torch.Size([35766, 2])
We keep 5.40e+06/4.07e+08 =  1% of the original kernel matrix.

torch.Size([1558, 2])
We keep 2.12e+04/2.61e+05 =  8% of the original kernel matrix.

torch.Size([10773, 2])
We keep 5.09e+05/1.38e+07 =  3% of the original kernel matrix.

torch.Size([4116, 2])
We keep 1.35e+05/2.64e+06 =  5% of the original kernel matrix.

torch.Size([15170, 2])
We keep 1.07e+06/4.38e+07 =  2% of the original kernel matrix.

torch.Size([48650, 2])
We keep 2.35e+07/9.72e+08 =  2% of the original kernel matrix.

torch.Size([46638, 2])
We keep 9.55e+06/8.40e+08 =  1% of the original kernel matrix.

torch.Size([21632, 2])
We keep 3.95e+06/1.68e+08 =  2% of the original kernel matrix.

torch.Size([31086, 2])
We keep 4.77e+06/3.49e+08 =  1% of the original kernel matrix.

torch.Size([20171, 2])
We keep 2.56e+06/1.14e+08 =  2% of the original kernel matrix.

torch.Size([30376, 2])
We keep 4.10e+06/2.88e+08 =  1% of the original kernel matrix.

torch.Size([1820, 2])
We keep 3.60e+04/4.16e+05 =  8% of the original kernel matrix.

torch.Size([11028, 2])
We keep 5.93e+05/1.74e+07 =  3% of the original kernel matrix.

torch.Size([2667, 2])
We keep 1.20e+05/1.50e+06 =  7% of the original kernel matrix.

torch.Size([12102, 2])
We keep 8.91e+05/3.30e+07 =  2% of the original kernel matrix.

torch.Size([11073, 2])
We keep 1.24e+06/2.79e+07 =  4% of the original kernel matrix.

torch.Size([22575, 2])
We keep 2.43e+06/1.42e+08 =  1% of the original kernel matrix.

torch.Size([21116, 2])
We keep 3.81e+06/1.40e+08 =  2% of the original kernel matrix.

torch.Size([30831, 2])
We keep 4.46e+06/3.19e+08 =  1% of the original kernel matrix.

torch.Size([29142, 2])
We keep 1.21e+07/3.76e+08 =  3% of the original kernel matrix.

torch.Size([36235, 2])
We keep 6.45e+06/5.23e+08 =  1% of the original kernel matrix.

torch.Size([1650, 2])
We keep 3.76e+04/4.02e+05 =  9% of the original kernel matrix.

torch.Size([10758, 2])
We keep 5.84e+05/1.71e+07 =  3% of the original kernel matrix.

torch.Size([11652, 2])
We keep 1.86e+07/9.21e+07 = 20% of the original kernel matrix.

torch.Size([22304, 2])
We keep 3.83e+06/2.59e+08 =  1% of the original kernel matrix.

torch.Size([9961, 2])
We keep 2.18e+06/3.86e+07 =  5% of the original kernel matrix.

torch.Size([20610, 2])
We keep 2.73e+06/1.67e+08 =  1% of the original kernel matrix.

torch.Size([9646, 2])
We keep 2.38e+06/4.78e+07 =  4% of the original kernel matrix.

torch.Size([20019, 2])
We keep 3.00e+06/1.86e+08 =  1% of the original kernel matrix.

torch.Size([150562, 2])
We keep 1.58e+08/9.49e+09 =  1% of the original kernel matrix.

torch.Size([82591, 2])
We keep 2.49e+07/2.63e+09 =  0% of the original kernel matrix.

torch.Size([6684, 2])
We keep 1.79e+06/1.96e+07 =  9% of the original kernel matrix.

torch.Size([17680, 2])
We keep 2.22e+06/1.19e+08 =  1% of the original kernel matrix.

torch.Size([19598, 2])
We keep 4.28e+06/1.25e+08 =  3% of the original kernel matrix.

torch.Size([29822, 2])
We keep 4.25e+06/3.02e+08 =  1% of the original kernel matrix.

torch.Size([89679, 2])
We keep 5.18e+07/2.74e+09 =  1% of the original kernel matrix.

torch.Size([62621, 2])
We keep 1.47e+07/1.41e+09 =  1% of the original kernel matrix.

torch.Size([17751, 2])
We keep 2.62e+06/9.83e+07 =  2% of the original kernel matrix.

torch.Size([28785, 2])
We keep 3.99e+06/2.67e+08 =  1% of the original kernel matrix.

torch.Size([58198, 2])
We keep 2.82e+07/1.23e+09 =  2% of the original kernel matrix.

torch.Size([51230, 2])
We keep 1.05e+07/9.46e+08 =  1% of the original kernel matrix.

torch.Size([103813, 2])
We keep 5.30e+07/3.54e+09 =  1% of the original kernel matrix.

torch.Size([67156, 2])
We keep 1.63e+07/1.60e+09 =  1% of the original kernel matrix.

torch.Size([1352, 2])
We keep 2.10e+04/2.30e+05 =  9% of the original kernel matrix.

torch.Size([9947, 2])
We keep 4.86e+05/1.29e+07 =  3% of the original kernel matrix.

torch.Size([14697, 2])
We keep 1.44e+06/5.28e+07 =  2% of the original kernel matrix.

torch.Size([25809, 2])
We keep 3.08e+06/1.96e+08 =  1% of the original kernel matrix.

torch.Size([8081, 2])
We keep 4.23e+05/1.17e+07 =  3% of the original kernel matrix.

torch.Size([19647, 2])
We keep 1.77e+06/9.21e+07 =  1% of the original kernel matrix.

torch.Size([45697, 2])
We keep 1.61e+07/8.07e+08 =  1% of the original kernel matrix.

torch.Size([45994, 2])
We keep 8.72e+06/7.66e+08 =  1% of the original kernel matrix.

torch.Size([45911, 2])
We keep 1.75e+07/9.09e+08 =  1% of the original kernel matrix.

torch.Size([45196, 2])
We keep 9.24e+06/8.13e+08 =  1% of the original kernel matrix.

torch.Size([12335, 2])
We keep 2.84e+06/6.39e+07 =  4% of the original kernel matrix.

torch.Size([22887, 2])
We keep 3.32e+06/2.15e+08 =  1% of the original kernel matrix.

torch.Size([32759, 2])
We keep 8.56e+06/3.81e+08 =  2% of the original kernel matrix.

torch.Size([39140, 2])
We keep 6.57e+06/5.26e+08 =  1% of the original kernel matrix.

torch.Size([9087, 2])
We keep 5.46e+05/1.65e+07 =  3% of the original kernel matrix.

torch.Size([20774, 2])
We keep 2.01e+06/1.09e+08 =  1% of the original kernel matrix.

torch.Size([4568, 2])
We keep 1.83e+06/2.56e+07 =  7% of the original kernel matrix.

torch.Size([13316, 2])
We keep 2.35e+06/1.36e+08 =  1% of the original kernel matrix.

torch.Size([6115, 2])
We keep 2.50e+06/1.07e+07 = 23% of the original kernel matrix.

torch.Size([17334, 2])
We keep 1.74e+06/8.80e+07 =  1% of the original kernel matrix.

torch.Size([4832, 2])
We keep 2.93e+05/5.41e+06 =  5% of the original kernel matrix.

torch.Size([15580, 2])
We keep 1.37e+06/6.27e+07 =  2% of the original kernel matrix.

torch.Size([91675, 2])
We keep 5.64e+07/3.11e+09 =  1% of the original kernel matrix.

torch.Size([63172, 2])
We keep 1.54e+07/1.50e+09 =  1% of the original kernel matrix.

torch.Size([157666, 2])
We keep 2.77e+08/1.33e+10 =  2% of the original kernel matrix.

torch.Size([82965, 2])
We keep 2.86e+07/3.11e+09 =  0% of the original kernel matrix.

torch.Size([3044, 2])
We keep 2.36e+05/2.28e+06 = 10% of the original kernel matrix.

torch.Size([12703, 2])
We keep 1.02e+06/4.07e+07 =  2% of the original kernel matrix.

torch.Size([8937, 2])
We keep 6.27e+05/1.68e+07 =  3% of the original kernel matrix.

torch.Size([20758, 2])
We keep 2.03e+06/1.10e+08 =  1% of the original kernel matrix.

torch.Size([11091, 2])
We keep 8.72e+05/2.45e+07 =  3% of the original kernel matrix.

torch.Size([22764, 2])
We keep 2.30e+06/1.33e+08 =  1% of the original kernel matrix.

torch.Size([58004, 2])
We keep 2.90e+07/1.62e+09 =  1% of the original kernel matrix.

torch.Size([50991, 2])
We keep 1.19e+07/1.08e+09 =  1% of the original kernel matrix.

torch.Size([10603, 2])
We keep 1.10e+06/2.68e+07 =  4% of the original kernel matrix.

torch.Size([22006, 2])
We keep 2.40e+06/1.40e+08 =  1% of the original kernel matrix.

torch.Size([5932, 2])
We keep 3.28e+05/6.85e+06 =  4% of the original kernel matrix.

torch.Size([17185, 2])
We keep 1.47e+06/7.05e+07 =  2% of the original kernel matrix.

torch.Size([26139, 2])
We keep 5.51e+06/2.41e+08 =  2% of the original kernel matrix.

torch.Size([34979, 2])
We keep 5.44e+06/4.18e+08 =  1% of the original kernel matrix.

torch.Size([44054, 2])
We keep 1.16e+07/6.95e+08 =  1% of the original kernel matrix.

torch.Size([45796, 2])
We keep 8.41e+06/7.11e+08 =  1% of the original kernel matrix.

torch.Size([4783, 2])
We keep 2.69e+05/4.27e+06 =  6% of the original kernel matrix.

torch.Size([15556, 2])
We keep 1.24e+06/5.57e+07 =  2% of the original kernel matrix.

torch.Size([40162, 2])
We keep 1.74e+07/6.13e+08 =  2% of the original kernel matrix.

torch.Size([43494, 2])
We keep 8.08e+06/6.67e+08 =  1% of the original kernel matrix.

torch.Size([2798, 2])
We keep 1.21e+05/1.19e+06 = 10% of the original kernel matrix.

torch.Size([13164, 2])
We keep 8.09e+05/2.93e+07 =  2% of the original kernel matrix.

time for making ranges is 2.3480758666992188
Sorting X and nu_X
time for sorting X is 0.05996298789978027
Sorting Z and nu_Z
time for sorting Z is 0.00026988983154296875
Starting Optim
sum tnu_Z before tensor(11914732., device='cuda:0')
c= tensor(142.4813, device='cuda:0')
c= tensor(18901.8711, device='cuda:0')
c= tensor(24175.5703, device='cuda:0')
c= tensor(26572.4648, device='cuda:0')
c= tensor(152346.5312, device='cuda:0')
c= tensor(216520.8125, device='cuda:0')
c= tensor(386456.5625, device='cuda:0')
c= tensor(432503.9062, device='cuda:0')
c= tensor(435500.6875, device='cuda:0')
c= tensor(563553.2500, device='cuda:0')
c= tensor(566657.5625, device='cuda:0')
c= tensor(677282.1875, device='cuda:0')
c= tensor(679739.6250, device='cuda:0')
c= tensor(4005467.5000, device='cuda:0')
c= tensor(4064969.7500, device='cuda:0')
c= tensor(4170752., device='cuda:0')
c= tensor(4497765.5000, device='cuda:0')
c= tensor(4660050., device='cuda:0')
c= tensor(5621120., device='cuda:0')
c= tensor(6778846., device='cuda:0')
c= tensor(6812350.5000, device='cuda:0')
c= tensor(10478250., device='cuda:0')
c= tensor(10486526., device='cuda:0')
c= tensor(10589324., device='cuda:0')
c= tensor(10686426., device='cuda:0')
c= tensor(10871092., device='cuda:0')
c= tensor(10970779., device='cuda:0')
c= tensor(10980045., device='cuda:0')
c= tensor(11378534., device='cuda:0')
c= tensor(69574960., device='cuda:0')
c= tensor(69577392., device='cuda:0')
c= tensor(96607832., device='cuda:0')
c= tensor(96613776., device='cuda:0')
c= tensor(96617472., device='cuda:0')
c= tensor(96628264., device='cuda:0')
c= tensor(97616584., device='cuda:0')
c= tensor(98507768., device='cuda:0')
c= tensor(98509168., device='cuda:0')
c= tensor(98514920., device='cuda:0')
c= tensor(98517664., device='cuda:0')
c= tensor(98519880., device='cuda:0')
c= tensor(98520016., device='cuda:0')
c= tensor(98520192., device='cuda:0')
c= tensor(98520776., device='cuda:0')
c= tensor(98520928., device='cuda:0')
c= tensor(98521056., device='cuda:0')
c= tensor(98522928., device='cuda:0')
c= tensor(98524512., device='cuda:0')
c= tensor(98524968., device='cuda:0')
c= tensor(98542352., device='cuda:0')
c= tensor(98564424., device='cuda:0')
c= tensor(98566960., device='cuda:0')
c= tensor(98568552., device='cuda:0')
c= tensor(98569912., device='cuda:0')
c= tensor(98572776., device='cuda:0')
c= tensor(98585160., device='cuda:0')
c= tensor(98586424., device='cuda:0')
c= tensor(98587392., device='cuda:0')
c= tensor(98587928., device='cuda:0')
c= tensor(98588496., device='cuda:0')
c= tensor(98608368., device='cuda:0')
c= tensor(98608848., device='cuda:0')
c= tensor(98620248., device='cuda:0')
c= tensor(98622304., device='cuda:0')
c= tensor(98623152., device='cuda:0')
c= tensor(98624640., device='cuda:0')
c= tensor(98625104., device='cuda:0')
c= tensor(98625768., device='cuda:0')
c= tensor(98626400., device='cuda:0')
c= tensor(98627160., device='cuda:0')
c= tensor(98628600., device='cuda:0')
c= tensor(98629520., device='cuda:0')
c= tensor(98629768., device='cuda:0')
c= tensor(98630824., device='cuda:0')
c= tensor(98631832., device='cuda:0')
c= tensor(98635416., device='cuda:0')
c= tensor(98635792., device='cuda:0')
c= tensor(98636008., device='cuda:0')
c= tensor(98636520., device='cuda:0')
c= tensor(98639808., device='cuda:0')
c= tensor(98641288., device='cuda:0')
c= tensor(98641408., device='cuda:0')
c= tensor(98646736., device='cuda:0')
c= tensor(98646984., device='cuda:0')
c= tensor(98647448., device='cuda:0')
c= tensor(98648168., device='cuda:0')
c= tensor(98649384., device='cuda:0')
c= tensor(98649568., device='cuda:0')
c= tensor(98651576., device='cuda:0')
c= tensor(98653952., device='cuda:0')
c= tensor(98654384., device='cuda:0')
c= tensor(98654592., device='cuda:0')
c= tensor(98654848., device='cuda:0')
c= tensor(98657624., device='cuda:0')
c= tensor(98658472., device='cuda:0')
c= tensor(98659472., device='cuda:0')
c= tensor(98678488., device='cuda:0')
c= tensor(98684624., device='cuda:0')
c= tensor(98689160., device='cuda:0')
c= tensor(98690456., device='cuda:0')
c= tensor(98703424., device='cuda:0')
c= tensor(98704984., device='cuda:0')
c= tensor(98705600., device='cuda:0')
c= tensor(98706208., device='cuda:0')
c= tensor(98707352., device='cuda:0')
c= tensor(98707632., device='cuda:0')
c= tensor(98710816., device='cuda:0')
c= tensor(98710912., device='cuda:0')
c= tensor(98711648., device='cuda:0')
c= tensor(98712640., device='cuda:0')
c= tensor(98713488., device='cuda:0')
c= tensor(98713872., device='cuda:0')
c= tensor(98714920., device='cuda:0')
c= tensor(98717040., device='cuda:0')
c= tensor(98718976., device='cuda:0')
c= tensor(98719576., device='cuda:0')
c= tensor(98719968., device='cuda:0')
c= tensor(98720184., device='cuda:0')
c= tensor(98724216., device='cuda:0')
c= tensor(98724424., device='cuda:0')
c= tensor(98729440., device='cuda:0')
c= tensor(98730288., device='cuda:0')
c= tensor(98730824., device='cuda:0')
c= tensor(98732240., device='cuda:0')
c= tensor(98732968., device='cuda:0')
c= tensor(98734232., device='cuda:0')
c= tensor(98734632., device='cuda:0')
c= tensor(98734896., device='cuda:0')
c= tensor(98750672., device='cuda:0')
c= tensor(98752072., device='cuda:0')
c= tensor(98757776., device='cuda:0')
c= tensor(98758456., device='cuda:0')
c= tensor(98760296., device='cuda:0')
c= tensor(98760752., device='cuda:0')
c= tensor(98761464., device='cuda:0')
c= tensor(98762016., device='cuda:0')
c= tensor(98762280., device='cuda:0')
c= tensor(98763960., device='cuda:0')
c= tensor(98764440., device='cuda:0')
c= tensor(98765520., device='cuda:0')
c= tensor(98766560., device='cuda:0')
c= tensor(98766728., device='cuda:0')
c= tensor(98771880., device='cuda:0')
c= tensor(98829528., device='cuda:0')
c= tensor(98834400., device='cuda:0')
c= tensor(98834840., device='cuda:0')
c= tensor(98835416., device='cuda:0')
c= tensor(98836200., device='cuda:0')
c= tensor(98836856., device='cuda:0')
c= tensor(98837432., device='cuda:0')
c= tensor(98838296., device='cuda:0')
c= tensor(98849960., device='cuda:0')
c= tensor(98852360., device='cuda:0')
c= tensor(98857472., device='cuda:0')
c= tensor(98858384., device='cuda:0')
c= tensor(98868696., device='cuda:0')
c= tensor(98869368., device='cuda:0')
c= tensor(98870232., device='cuda:0')
c= tensor(98871160., device='cuda:0')
c= tensor(98871248., device='cuda:0')
c= tensor(98874368., device='cuda:0')
c= tensor(98874616., device='cuda:0')
c= tensor(98877008., device='cuda:0')
c= tensor(98877536., device='cuda:0')
c= tensor(98878728., device='cuda:0')
c= tensor(98879224., device='cuda:0')
c= tensor(98879800., device='cuda:0')
c= tensor(98880136., device='cuda:0')
c= tensor(98880896., device='cuda:0')
c= tensor(98880968., device='cuda:0')
c= tensor(98881176., device='cuda:0')
c= tensor(98881944., device='cuda:0')
c= tensor(98884032., device='cuda:0')
c= tensor(98884944., device='cuda:0')
c= tensor(98888792., device='cuda:0')
c= tensor(98889328., device='cuda:0')
c= tensor(98889896., device='cuda:0')
c= tensor(98893528., device='cuda:0')
c= tensor(98895672., device='cuda:0')
c= tensor(98896320., device='cuda:0')
c= tensor(98899968., device='cuda:0')
c= tensor(98900968., device='cuda:0')
c= tensor(98902792., device='cuda:0')
c= tensor(98903256., device='cuda:0')
c= tensor(98910696., device='cuda:0')
c= tensor(98913176., device='cuda:0')
c= tensor(98914224., device='cuda:0')
c= tensor(98915456., device='cuda:0')
c= tensor(98919224., device='cuda:0')
c= tensor(98962840., device='cuda:0')
c= tensor(98963328., device='cuda:0')
c= tensor(98963616., device='cuda:0')
c= tensor(98964056., device='cuda:0')
c= tensor(98964240., device='cuda:0')
c= tensor(98965712., device='cuda:0')
c= tensor(98966400., device='cuda:0')
c= tensor(98966648., device='cuda:0')
c= tensor(98967104., device='cuda:0')
c= tensor(98968560., device='cuda:0')
c= tensor(98968832., device='cuda:0')
c= tensor(98984848., device='cuda:0')
c= tensor(98985208., device='cuda:0')
c= tensor(98994824., device='cuda:0')
c= tensor(98999800., device='cuda:0')
c= tensor(99000784., device='cuda:0')
c= tensor(99000872., device='cuda:0')
c= tensor(99004224., device='cuda:0')
c= tensor(99006312., device='cuda:0')
c= tensor(99006960., device='cuda:0')
c= tensor(99018000., device='cuda:0')
c= tensor(99034216., device='cuda:0')
c= tensor(99034464., device='cuda:0')
c= tensor(99034680., device='cuda:0')
c= tensor(99034992., device='cuda:0')
c= tensor(99035488., device='cuda:0')
c= tensor(99035776., device='cuda:0')
c= tensor(99035928., device='cuda:0')
c= tensor(99036072., device='cuda:0')
c= tensor(99036912., device='cuda:0')
c= tensor(99037624., device='cuda:0')
c= tensor(99039120., device='cuda:0')
c= tensor(99039800., device='cuda:0')
c= tensor(99041504., device='cuda:0')
c= tensor(99042240., device='cuda:0')
c= tensor(99045408., device='cuda:0')
c= tensor(99046216., device='cuda:0')
c= tensor(99047000., device='cuda:0')
c= tensor(99047152., device='cuda:0')
c= tensor(99050024., device='cuda:0')
c= tensor(99050304., device='cuda:0')
c= tensor(99050440., device='cuda:0')
c= tensor(99051304., device='cuda:0')
c= tensor(99051736., device='cuda:0')
c= tensor(99059864., device='cuda:0')
c= tensor(99060984., device='cuda:0')
c= tensor(99061536., device='cuda:0')
c= tensor(99062744., device='cuda:0')
c= tensor(99065568., device='cuda:0')
c= tensor(99065880., device='cuda:0')
c= tensor(99072264., device='cuda:0')
c= tensor(99358664., device='cuda:0')
c= tensor(99367776., device='cuda:0')
c= tensor(99370336., device='cuda:0')
c= tensor(99370448., device='cuda:0')
c= tensor(99378576., device='cuda:0')
c= tensor(1.0029e+08, device='cuda:0')
c= tensor(1.0451e+08, device='cuda:0')
c= tensor(1.0451e+08, device='cuda:0')
c= tensor(1.0464e+08, device='cuda:0')
c= tensor(1.0472e+08, device='cuda:0')
c= tensor(1.0473e+08, device='cuda:0')
c= tensor(1.1166e+08, device='cuda:0')
c= tensor(1.1166e+08, device='cuda:0')
c= tensor(1.1167e+08, device='cuda:0')
c= tensor(1.1241e+08, device='cuda:0')
c= tensor(1.1464e+08, device='cuda:0')
c= tensor(1.1465e+08, device='cuda:0')
c= tensor(1.1468e+08, device='cuda:0')
c= tensor(1.1470e+08, device='cuda:0')
c= tensor(1.1487e+08, device='cuda:0')
c= tensor(1.1503e+08, device='cuda:0')
c= tensor(1.1505e+08, device='cuda:0')
c= tensor(1.1514e+08, device='cuda:0')
c= tensor(1.1517e+08, device='cuda:0')
c= tensor(1.1517e+08, device='cuda:0')
c= tensor(1.3727e+08, device='cuda:0')
c= tensor(1.3727e+08, device='cuda:0')
c= tensor(1.3727e+08, device='cuda:0')
c= tensor(1.3730e+08, device='cuda:0')
c= tensor(1.3735e+08, device='cuda:0')
c= tensor(1.4668e+08, device='cuda:0')
c= tensor(1.4695e+08, device='cuda:0')
c= tensor(1.4695e+08, device='cuda:0')
c= tensor(1.4701e+08, device='cuda:0')
c= tensor(1.4702e+08, device='cuda:0')
c= tensor(1.4710e+08, device='cuda:0')
c= tensor(1.4735e+08, device='cuda:0')
c= tensor(1.4736e+08, device='cuda:0')
c= tensor(1.4740e+08, device='cuda:0')
c= tensor(1.4740e+08, device='cuda:0')
c= tensor(1.4740e+08, device='cuda:0')
c= tensor(1.4783e+08, device='cuda:0')
c= tensor(1.4792e+08, device='cuda:0')
c= tensor(1.4795e+08, device='cuda:0')
c= tensor(1.4811e+08, device='cuda:0')
c= tensor(1.5249e+08, device='cuda:0')
c= tensor(1.5250e+08, device='cuda:0')
c= tensor(1.5253e+08, device='cuda:0')
c= tensor(1.5288e+08, device='cuda:0')
c= tensor(1.5288e+08, device='cuda:0')
c= tensor(1.5306e+08, device='cuda:0')
c= tensor(1.5910e+08, device='cuda:0')
c= tensor(1.6660e+08, device='cuda:0')
c= tensor(1.6663e+08, device='cuda:0')
c= tensor(1.6664e+08, device='cuda:0')
c= tensor(1.6664e+08, device='cuda:0')
c= tensor(1.6664e+08, device='cuda:0')
c= tensor(1.6671e+08, device='cuda:0')
c= tensor(1.6672e+08, device='cuda:0')
c= tensor(1.6684e+08, device='cuda:0')
c= tensor(1.7042e+08, device='cuda:0')
c= tensor(1.7054e+08, device='cuda:0')
c= tensor(1.7057e+08, device='cuda:0')
c= tensor(1.7057e+08, device='cuda:0')
c= tensor(1.7355e+08, device='cuda:0')
c= tensor(1.7358e+08, device='cuda:0')
c= tensor(1.7359e+08, device='cuda:0')
c= tensor(1.7366e+08, device='cuda:0')
c= tensor(2.3840e+08, device='cuda:0')
c= tensor(2.3842e+08, device='cuda:0')
c= tensor(2.3905e+08, device='cuda:0')
c= tensor(2.3905e+08, device='cuda:0')
c= tensor(2.4038e+08, device='cuda:0')
c= tensor(2.4040e+08, device='cuda:0')
c= tensor(2.5433e+08, device='cuda:0')
c= tensor(2.5449e+08, device='cuda:0')
c= tensor(2.5449e+08, device='cuda:0')
c= tensor(2.5495e+08, device='cuda:0')
c= tensor(2.5530e+08, device='cuda:0')
c= tensor(2.5531e+08, device='cuda:0')
c= tensor(2.5536e+08, device='cuda:0')
c= tensor(2.5703e+08, device='cuda:0')
c= tensor(2.6302e+08, device='cuda:0')
c= tensor(2.6322e+08, device='cuda:0')
c= tensor(2.6322e+08, device='cuda:0')
c= tensor(2.6322e+08, device='cuda:0')
c= tensor(2.6379e+08, device='cuda:0')
c= tensor(2.6695e+08, device='cuda:0')
c= tensor(2.6696e+08, device='cuda:0')
c= tensor(2.6696e+08, device='cuda:0')
c= tensor(2.6706e+08, device='cuda:0')
c= tensor(2.6727e+08, device='cuda:0')
c= tensor(2.6765e+08, device='cuda:0')
c= tensor(2.6765e+08, device='cuda:0')
c= tensor(2.6767e+08, device='cuda:0')
c= tensor(2.6769e+08, device='cuda:0')
c= tensor(2.6770e+08, device='cuda:0')
c= tensor(2.6771e+08, device='cuda:0')
c= tensor(2.6771e+08, device='cuda:0')
c= tensor(2.6818e+08, device='cuda:0')
c= tensor(2.6823e+08, device='cuda:0')
c= tensor(2.6825e+08, device='cuda:0')
c= tensor(2.6834e+08, device='cuda:0')
c= tensor(2.6835e+08, device='cuda:0')
c= tensor(2.7525e+08, device='cuda:0')
c= tensor(2.7526e+08, device='cuda:0')
c= tensor(2.7578e+08, device='cuda:0')
c= tensor(2.7578e+08, device='cuda:0')
c= tensor(2.7578e+08, device='cuda:0')
c= tensor(2.7578e+08, device='cuda:0')
c= tensor(2.7581e+08, device='cuda:0')
c= tensor(2.7581e+08, device='cuda:0')
c= tensor(2.7614e+08, device='cuda:0')
c= tensor(2.7614e+08, device='cuda:0')
c= tensor(2.7614e+08, device='cuda:0')
c= tensor(2.7866e+08, device='cuda:0')
c= tensor(2.7871e+08, device='cuda:0')
c= tensor(2.7878e+08, device='cuda:0')
c= tensor(2.8001e+08, device='cuda:0')
c= tensor(2.8114e+08, device='cuda:0')
c= tensor(2.8117e+08, device='cuda:0')
c= tensor(2.8117e+08, device='cuda:0')
c= tensor(2.8120e+08, device='cuda:0')
c= tensor(2.8120e+08, device='cuda:0')
c= tensor(2.8120e+08, device='cuda:0')
c= tensor(2.8125e+08, device='cuda:0')
c= tensor(2.8125e+08, device='cuda:0')
c= tensor(2.8125e+08, device='cuda:0')
c= tensor(2.8125e+08, device='cuda:0')
c= tensor(2.8125e+08, device='cuda:0')
c= tensor(2.8356e+08, device='cuda:0')
c= tensor(2.8357e+08, device='cuda:0')
c= tensor(2.8383e+08, device='cuda:0')
c= tensor(2.8384e+08, device='cuda:0')
c= tensor(2.8384e+08, device='cuda:0')
c= tensor(2.8387e+08, device='cuda:0')
c= tensor(3.2390e+08, device='cuda:0')
c= tensor(3.3270e+08, device='cuda:0')
c= tensor(3.3276e+08, device='cuda:0')
c= tensor(3.3347e+08, device='cuda:0')
c= tensor(3.3347e+08, device='cuda:0')
c= tensor(3.3470e+08, device='cuda:0')
c= tensor(3.3480e+08, device='cuda:0')
c= tensor(3.3527e+08, device='cuda:0')
c= tensor(3.3529e+08, device='cuda:0')
c= tensor(3.3572e+08, device='cuda:0')
c= tensor(3.4719e+08, device='cuda:0')
c= tensor(3.4722e+08, device='cuda:0')
c= tensor(3.4722e+08, device='cuda:0')
c= tensor(3.4726e+08, device='cuda:0')
c= tensor(3.4727e+08, device='cuda:0')
c= tensor(3.4727e+08, device='cuda:0')
c= tensor(3.4785e+08, device='cuda:0')
c= tensor(3.4788e+08, device='cuda:0')
c= tensor(3.4788e+08, device='cuda:0')
c= tensor(3.4792e+08, device='cuda:0')
c= tensor(3.4796e+08, device='cuda:0')
c= tensor(3.4796e+08, device='cuda:0')
c= tensor(3.4884e+08, device='cuda:0')
c= tensor(3.4917e+08, device='cuda:0')
c= tensor(3.4957e+08, device='cuda:0')
c= tensor(3.5000e+08, device='cuda:0')
c= tensor(3.5046e+08, device='cuda:0')
c= tensor(3.5046e+08, device='cuda:0')
c= tensor(3.5047e+08, device='cuda:0')
c= tensor(3.5050e+08, device='cuda:0')
c= tensor(3.5058e+08, device='cuda:0')
c= tensor(3.5058e+08, device='cuda:0')
c= tensor(3.5145e+08, device='cuda:0')
c= tensor(3.5325e+08, device='cuda:0')
c= tensor(3.5338e+08, device='cuda:0')
c= tensor(3.5341e+08, device='cuda:0')
c= tensor(3.5377e+08, device='cuda:0')
c= tensor(3.5378e+08, device='cuda:0')
c= tensor(3.5378e+08, device='cuda:0')
c= tensor(3.5379e+08, device='cuda:0')
c= tensor(3.5416e+08, device='cuda:0')
c= tensor(3.5444e+08, device='cuda:0')
c= tensor(3.6444e+08, device='cuda:0')
c= tensor(3.6823e+08, device='cuda:0')
c= tensor(3.6836e+08, device='cuda:0')
c= tensor(3.6844e+08, device='cuda:0')
c= tensor(3.6882e+08, device='cuda:0')
c= tensor(3.6882e+08, device='cuda:0')
c= tensor(3.6882e+08, device='cuda:0')
c= tensor(3.7268e+08, device='cuda:0')
c= tensor(3.7270e+08, device='cuda:0')
c= tensor(3.7270e+08, device='cuda:0')
c= tensor(3.7273e+08, device='cuda:0')
c= tensor(3.7294e+08, device='cuda:0')
c= tensor(3.7296e+08, device='cuda:0')
c= tensor(3.7321e+08, device='cuda:0')
c= tensor(3.7323e+08, device='cuda:0')
c= tensor(3.7325e+08, device='cuda:0')
c= tensor(3.7325e+08, device='cuda:0')
c= tensor(3.7326e+08, device='cuda:0')
c= tensor(3.7341e+08, device='cuda:0')
c= tensor(3.7346e+08, device='cuda:0')
c= tensor(3.7348e+08, device='cuda:0')
c= tensor(3.7382e+08, device='cuda:0')
c= tensor(3.7382e+08, device='cuda:0')
c= tensor(3.7386e+08, device='cuda:0')
c= tensor(3.7386e+08, device='cuda:0')
c= tensor(3.7391e+08, device='cuda:0')
c= tensor(3.7391e+08, device='cuda:0')
c= tensor(3.7396e+08, device='cuda:0')
c= tensor(3.7397e+08, device='cuda:0')
c= tensor(3.7400e+08, device='cuda:0')
c= tensor(3.7424e+08, device='cuda:0')
c= tensor(3.7481e+08, device='cuda:0')
c= tensor(3.7481e+08, device='cuda:0')
c= tensor(3.7483e+08, device='cuda:0')
c= tensor(3.7626e+08, device='cuda:0')
c= tensor(3.7627e+08, device='cuda:0')
c= tensor(3.9585e+08, device='cuda:0')
c= tensor(3.9585e+08, device='cuda:0')
c= tensor(3.9608e+08, device='cuda:0')
c= tensor(4.0053e+08, device='cuda:0')
c= tensor(4.0054e+08, device='cuda:0')
c= tensor(4.0117e+08, device='cuda:0')
c= tensor(4.0138e+08, device='cuda:0')
c= tensor(4.2274e+08, device='cuda:0')
c= tensor(4.2274e+08, device='cuda:0')
c= tensor(4.2281e+08, device='cuda:0')
c= tensor(4.2281e+08, device='cuda:0')
c= tensor(4.2282e+08, device='cuda:0')
c= tensor(4.2283e+08, device='cuda:0')
c= tensor(4.2326e+08, device='cuda:0')
c= tensor(4.2329e+08, device='cuda:0')
c= tensor(4.2361e+08, device='cuda:0')
c= tensor(4.2362e+08, device='cuda:0')
c= tensor(4.2362e+08, device='cuda:0')
c= tensor(4.2362e+08, device='cuda:0')
c= tensor(4.2488e+08, device='cuda:0')
c= tensor(4.2518e+08, device='cuda:0')
c= tensor(4.3321e+08, device='cuda:0')
c= tensor(4.3332e+08, device='cuda:0')
c= tensor(4.3333e+08, device='cuda:0')
c= tensor(4.3333e+08, device='cuda:0')
c= tensor(4.3335e+08, device='cuda:0')
c= tensor(4.4276e+08, device='cuda:0')
c= tensor(4.4277e+08, device='cuda:0')
c= tensor(4.4277e+08, device='cuda:0')
c= tensor(4.4306e+08, device='cuda:0')
c= tensor(4.4323e+08, device='cuda:0')
c= tensor(4.4323e+08, device='cuda:0')
c= tensor(4.4324e+08, device='cuda:0')
c= tensor(4.5233e+08, device='cuda:0')
c= tensor(4.5235e+08, device='cuda:0')
c= tensor(4.5301e+08, device='cuda:0')
c= tensor(4.5304e+08, device='cuda:0')
c= tensor(4.5327e+08, device='cuda:0')
c= tensor(4.5468e+08, device='cuda:0')
c= tensor(4.5572e+08, device='cuda:0')
c= tensor(4.5752e+08, device='cuda:0')
c= tensor(4.5754e+08, device='cuda:0')
c= tensor(4.5780e+08, device='cuda:0')
c= tensor(4.5784e+08, device='cuda:0')
c= tensor(4.5785e+08, device='cuda:0')
c= tensor(4.5787e+08, device='cuda:0')
c= tensor(4.5787e+08, device='cuda:0')
c= tensor(4.5804e+08, device='cuda:0')
c= tensor(4.5805e+08, device='cuda:0')
c= tensor(4.5805e+08, device='cuda:0')
c= tensor(4.5807e+08, device='cuda:0')
c= tensor(4.5808e+08, device='cuda:0')
c= tensor(4.5875e+08, device='cuda:0')
c= tensor(4.5875e+08, device='cuda:0')
c= tensor(4.5879e+08, device='cuda:0')
c= tensor(4.5880e+08, device='cuda:0')
c= tensor(4.5885e+08, device='cuda:0')
c= tensor(4.5885e+08, device='cuda:0')
c= tensor(4.5885e+08, device='cuda:0')
c= tensor(4.5887e+08, device='cuda:0')
c= tensor(4.6042e+08, device='cuda:0')
c= tensor(4.6042e+08, device='cuda:0')
c= tensor(4.6042e+08, device='cuda:0')
c= tensor(4.6042e+08, device='cuda:0')
c= tensor(4.6528e+08, device='cuda:0')
c= tensor(4.7035e+08, device='cuda:0')
c= tensor(4.7038e+08, device='cuda:0')
c= tensor(4.7039e+08, device='cuda:0')
c= tensor(4.7105e+08, device='cuda:0')
c= tensor(4.7130e+08, device='cuda:0')
c= tensor(4.7130e+08, device='cuda:0')
c= tensor(4.7131e+08, device='cuda:0')
c= tensor(4.7132e+08, device='cuda:0')
c= tensor(4.7177e+08, device='cuda:0')
c= tensor(4.7177e+08, device='cuda:0')
c= tensor(4.7306e+08, device='cuda:0')
c= tensor(4.7307e+08, device='cuda:0')
c= tensor(4.7311e+08, device='cuda:0')
c= tensor(4.7311e+08, device='cuda:0')
c= tensor(4.7314e+08, device='cuda:0')
c= tensor(4.7315e+08, device='cuda:0')
c= tensor(4.7320e+08, device='cuda:0')
c= tensor(4.7357e+08, device='cuda:0')
c= tensor(4.7749e+08, device='cuda:0')
c= tensor(4.7750e+08, device='cuda:0')
c= tensor(4.7750e+08, device='cuda:0')
c= tensor(4.7751e+08, device='cuda:0')
c= tensor(4.8749e+08, device='cuda:0')
c= tensor(4.8751e+08, device='cuda:0')
c= tensor(4.8751e+08, device='cuda:0')
c= tensor(4.8751e+08, device='cuda:0')
c= tensor(4.8853e+08, device='cuda:0')
c= tensor(4.8853e+08, device='cuda:0')
c= tensor(4.8881e+08, device='cuda:0')
c= tensor(4.8881e+08, device='cuda:0')
c= tensor(4.8883e+08, device='cuda:0')
c= tensor(4.8884e+08, device='cuda:0')
c= tensor(4.8884e+08, device='cuda:0')
c= tensor(4.8884e+08, device='cuda:0')
c= tensor(4.8994e+08, device='cuda:0')
c= tensor(4.9565e+08, device='cuda:0')
c= tensor(4.9680e+08, device='cuda:0')
c= tensor(4.9684e+08, device='cuda:0')
c= tensor(4.9689e+08, device='cuda:0')
c= tensor(4.9689e+08, device='cuda:0')
c= tensor(4.9690e+08, device='cuda:0')
c= tensor(4.9701e+08, device='cuda:0')
c= tensor(4.9705e+08, device='cuda:0')
c= tensor(4.9732e+08, device='cuda:0')
c= tensor(4.9733e+08, device='cuda:0')
c= tensor(5.0710e+08, device='cuda:0')
c= tensor(5.0713e+08, device='cuda:0')
c= tensor(5.0718e+08, device='cuda:0')
c= tensor(5.1101e+08, device='cuda:0')
c= tensor(5.1111e+08, device='cuda:0')
c= tensor(5.1176e+08, device='cuda:0')
c= tensor(5.1276e+08, device='cuda:0')
c= tensor(5.1294e+08, device='cuda:0')
c= tensor(5.1301e+08, device='cuda:0')
c= tensor(5.1301e+08, device='cuda:0')
c= tensor(5.1305e+08, device='cuda:0')
c= tensor(5.1306e+08, device='cuda:0')
c= tensor(5.1333e+08, device='cuda:0')
c= tensor(5.1848e+08, device='cuda:0')
c= tensor(5.1972e+08, device='cuda:0')
c= tensor(5.2010e+08, device='cuda:0')
c= tensor(5.2011e+08, device='cuda:0')
c= tensor(5.2023e+08, device='cuda:0')
c= tensor(5.2039e+08, device='cuda:0')
c= tensor(5.2043e+08, device='cuda:0')
c= tensor(5.2070e+08, device='cuda:0')
c= tensor(5.2244e+08, device='cuda:0')
c= tensor(5.2247e+08, device='cuda:0')
c= tensor(5.2483e+08, device='cuda:0')
c= tensor(5.2573e+08, device='cuda:0')
c= tensor(5.2578e+08, device='cuda:0')
c= tensor(5.2578e+08, device='cuda:0')
c= tensor(5.2634e+08, device='cuda:0')
c= tensor(5.2654e+08, device='cuda:0')
c= tensor(5.2654e+08, device='cuda:0')
c= tensor(5.3217e+08, device='cuda:0')
c= tensor(5.3238e+08, device='cuda:0')
c= tensor(5.3253e+08, device='cuda:0')
c= tensor(5.3253e+08, device='cuda:0')
c= tensor(5.3253e+08, device='cuda:0')
c= tensor(5.3254e+08, device='cuda:0')
c= tensor(5.3254e+08, device='cuda:0')
c= tensor(5.3258e+08, device='cuda:0')
c= tensor(5.3267e+08, device='cuda:0')
c= tensor(1.2587e+09, device='cuda:0')
c= tensor(1.2587e+09, device='cuda:0')
c= tensor(1.2588e+09, device='cuda:0')
c= tensor(1.2588e+09, device='cuda:0')
c= tensor(1.2588e+09, device='cuda:0')
c= tensor(1.2588e+09, device='cuda:0')
c= tensor(1.2601e+09, device='cuda:0')
c= tensor(1.2601e+09, device='cuda:0')
c= tensor(1.2781e+09, device='cuda:0')
c= tensor(1.2781e+09, device='cuda:0')
c= tensor(1.2805e+09, device='cuda:0')
c= tensor(1.2809e+09, device='cuda:0')
c= tensor(1.2813e+09, device='cuda:0')
c= tensor(1.2835e+09, device='cuda:0')
c= tensor(1.2835e+09, device='cuda:0')
c= tensor(1.2835e+09, device='cuda:0')
c= tensor(1.2840e+09, device='cuda:0')
c= tensor(1.2840e+09, device='cuda:0')
c= tensor(1.2840e+09, device='cuda:0')
c= tensor(1.2841e+09, device='cuda:0')
c= tensor(1.2843e+09, device='cuda:0')
c= tensor(1.2864e+09, device='cuda:0')
c= tensor(1.2864e+09, device='cuda:0')
c= tensor(1.2883e+09, device='cuda:0')
c= tensor(1.2961e+09, device='cuda:0')
c= tensor(1.2962e+09, device='cuda:0')
c= tensor(1.2962e+09, device='cuda:0')
c= tensor(1.2981e+09, device='cuda:0')
c= tensor(1.2981e+09, device='cuda:0')
c= tensor(1.2982e+09, device='cuda:0')
c= tensor(1.2982e+09, device='cuda:0')
c= tensor(1.2985e+09, device='cuda:0')
c= tensor(1.2986e+09, device='cuda:0')
c= tensor(1.3017e+09, device='cuda:0')
c= tensor(1.3018e+09, device='cuda:0')
c= tensor(1.3018e+09, device='cuda:0')
c= tensor(1.3018e+09, device='cuda:0')
c= tensor(1.3018e+09, device='cuda:0')
c= tensor(1.3019e+09, device='cuda:0')
c= tensor(1.3023e+09, device='cuda:0')
c= tensor(1.3037e+09, device='cuda:0')
c= tensor(1.3038e+09, device='cuda:0')
c= tensor(1.3038e+09, device='cuda:0')
c= tensor(1.3039e+09, device='cuda:0')
c= tensor(1.3043e+09, device='cuda:0')
c= tensor(1.3044e+09, device='cuda:0')
c= tensor(1.3046e+09, device='cuda:0')
c= tensor(1.3406e+09, device='cuda:0')
c= tensor(1.3407e+09, device='cuda:0')
c= tensor(1.3407e+09, device='cuda:0')
c= tensor(1.3407e+09, device='cuda:0')
c= tensor(1.3408e+09, device='cuda:0')
c= tensor(1.3409e+09, device='cuda:0')
c= tensor(1.3409e+09, device='cuda:0')
c= tensor(1.3409e+09, device='cuda:0')
c= tensor(1.3409e+09, device='cuda:0')
c= tensor(1.3415e+09, device='cuda:0')
c= tensor(1.3415e+09, device='cuda:0')
c= tensor(1.3416e+09, device='cuda:0')
c= tensor(1.3416e+09, device='cuda:0')
c= tensor(1.3416e+09, device='cuda:0')
c= tensor(1.3416e+09, device='cuda:0')
c= tensor(1.3417e+09, device='cuda:0')
c= tensor(1.3419e+09, device='cuda:0')
c= tensor(1.3419e+09, device='cuda:0')
c= tensor(1.3423e+09, device='cuda:0')
c= tensor(1.3424e+09, device='cuda:0')
c= tensor(1.3424e+09, device='cuda:0')
c= tensor(1.3469e+09, device='cuda:0')
c= tensor(1.3470e+09, device='cuda:0')
c= tensor(1.3471e+09, device='cuda:0')
c= tensor(1.3481e+09, device='cuda:0')
c= tensor(1.3481e+09, device='cuda:0')
c= tensor(1.3487e+09, device='cuda:0')
c= tensor(1.3497e+09, device='cuda:0')
c= tensor(1.3497e+09, device='cuda:0')
c= tensor(1.3497e+09, device='cuda:0')
c= tensor(1.3497e+09, device='cuda:0')
c= tensor(1.3500e+09, device='cuda:0')
c= tensor(1.3504e+09, device='cuda:0')
c= tensor(1.3504e+09, device='cuda:0')
c= tensor(1.3506e+09, device='cuda:0')
c= tensor(1.3506e+09, device='cuda:0')
c= tensor(1.3508e+09, device='cuda:0')
c= tensor(1.3508e+09, device='cuda:0')
c= tensor(1.3508e+09, device='cuda:0')
c= tensor(1.3519e+09, device='cuda:0')
c= tensor(1.3601e+09, device='cuda:0')
c= tensor(1.3601e+09, device='cuda:0')
c= tensor(1.3601e+09, device='cuda:0')
c= tensor(1.3601e+09, device='cuda:0')
c= tensor(1.3608e+09, device='cuda:0')
c= tensor(1.3608e+09, device='cuda:0')
c= tensor(1.3608e+09, device='cuda:0')
c= tensor(1.3609e+09, device='cuda:0')
c= tensor(1.3611e+09, device='cuda:0')
c= tensor(1.3611e+09, device='cuda:0')
c= tensor(1.3615e+09, device='cuda:0')
c= tensor(1.3615e+09, device='cuda:0')
memory (bytes)
3693957120
time for making loss 2 is 14.51073169708252
p0 True
it  0 : 1034771456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 27% |
shape of L is 
torch.Size([])
memory (bytes)
3694215168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% |  6% |
memory (bytes)
3694977024
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  7317050400.0
relative error loss 5.3742046
shape of L is 
torch.Size([])
memory (bytes)
3940986880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  4% |  7% |
memory (bytes)
3941052416
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  7316995000.0
relative error loss 5.3741636
shape of L is 
torch.Size([])
memory (bytes)
3943096320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
3943096320
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  7316856000.0
relative error loss 5.3740616
shape of L is 
torch.Size([])
memory (bytes)
3944144896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
3944144896
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  7315980300.0
relative error loss 5.3734183
shape of L is 
torch.Size([])
memory (bytes)
3946094592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
3946094592
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  7306370000.0
relative error loss 5.36636
shape of L is 
torch.Size([])
memory (bytes)
3948224512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  7% |
memory (bytes)
3948224512
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  7258494000.0
relative error loss 5.3311963
shape of L is 
torch.Size([])
memory (bytes)
3950202880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  7% |
memory (bytes)
3950346240
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  6756462600.0
relative error loss 4.962466
shape of L is 
torch.Size([])
memory (bytes)
3952451584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% |  7% |
memory (bytes)
3952451584
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  4826112000.0
relative error loss 3.5446677
shape of L is 
torch.Size([])
memory (bytes)
3954466816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  8% |
memory (bytes)
3954565120
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  2464250400.0
relative error loss 1.809935
shape of L is 
torch.Size([])
memory (bytes)
3956707328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% |  8% |
memory (bytes)
3956707328
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  1707352000.0
relative error loss 1.2540107
time to take a step is 251.876451253891
it  1 : 1564446208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  6% |  8% |
shape of L is 
torch.Size([])
memory (bytes)
3958726656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% |  7% |
memory (bytes)
3958726656
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  1707352000.0
relative error loss 1.2540107
shape of L is 
torch.Size([])
memory (bytes)
3960963072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  8% |
memory (bytes)
3960963072
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1329890700.0
relative error loss 0.976774
shape of L is 
torch.Size([])
memory (bytes)
3962900480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  8% |
memory (bytes)
3962900480
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  69744990000.0
relative error loss 51.226086
shape of L is 
torch.Size([])
memory (bytes)
3965235200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  9% |
memory (bytes)
3965235200
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1695993300.0
relative error loss 1.2456679
shape of L is 
torch.Size([])
memory (bytes)
3967229952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  9% |
memory (bytes)
3967229952
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1209922600.0
relative error loss 0.8886602
shape of L is 
torch.Size([])
memory (bytes)
3969499136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  9% |
memory (bytes)
3969499136
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  1116819500.0
relative error loss 0.8202781
shape of L is 
torch.Size([])
memory (bytes)
3971637248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 30% |  9% |
memory (bytes)
3971637248
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  990203140.0
relative error loss 0.72728133
shape of L is 
torch.Size([])
memory (bytes)
3973701632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  9% |
memory (bytes)
3973701632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% |  9% |
error is  863427840.0
relative error loss 0.6341678
shape of L is 
torch.Size([])
memory (bytes)
3975737344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  9% |
memory (bytes)
3975737344
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  761110500.0
relative error loss 0.5590181
shape of L is 
torch.Size([])
memory (bytes)
3977940992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  9% |
memory (bytes)
3977977856
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  681879400.0
relative error loss 0.5008247
time to take a step is 242.82596564292908
it  2 : 1715782144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  9% |
shape of L is 
torch.Size([])
memory (bytes)
3980079104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
3980079104
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  681879400.0
relative error loss 0.5008247
shape of L is 
torch.Size([])
memory (bytes)
3981938688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% |  8% |
memory (bytes)
3982168064
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  624261250.0
relative error loss 0.45850545
shape of L is 
torch.Size([])
memory (bytes)
3984236544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% |  8% |
memory (bytes)
3984236544
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  580592640.0
relative error loss 0.4264319
shape of L is 
torch.Size([])
memory (bytes)
3986452480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  8% |
memory (bytes)
3986452480
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  541989900.0
relative error loss 0.39807904
shape of L is 
torch.Size([])
memory (bytes)
3988463616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  9% |
memory (bytes)
3988463616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  9% |
error is  518174600.0
relative error loss 0.38058728
shape of L is 
torch.Size([])
memory (bytes)
3990605824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  9% |
memory (bytes)
3990605824
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  478979200.0
relative error loss 0.35179916
shape of L is 
torch.Size([])
memory (bytes)
3992629248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  9% |
memory (bytes)
3992629248
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  447590660.0
relative error loss 0.328745
shape of L is 
torch.Size([])
memory (bytes)
3994943488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  9% |
memory (bytes)
3994943488
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  422061200.0
relative error loss 0.3099942
shape of L is 
torch.Size([])
memory (bytes)
3996942336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% |  9% |
memory (bytes)
3996942336
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  396793920.0
relative error loss 0.29143596
shape of L is 
torch.Size([])
memory (bytes)
3998986240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  9% |
memory (bytes)
3998986240
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  373940860.0
relative error loss 0.27465093
time to take a step is 233.0793514251709
it  3 : 1715782144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  9% |
shape of L is 
torch.Size([])
memory (bytes)
4001341440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  8% |
memory (bytes)
4001341440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  8% |
error is  373940860.0
relative error loss 0.27465093
shape of L is 
torch.Size([])
memory (bytes)
4003467264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% |  8% |
memory (bytes)
4003467264
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  358494900.0
relative error loss 0.26330623
shape of L is 
torch.Size([])
memory (bytes)
4005388288
| ID | GPU | MEM |
------------------
|  0 |  2% |  0% |
|  1 |  7% |  8% |
memory (bytes)
4005629952
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  326626050.0
relative error loss 0.2398993
shape of L is 
torch.Size([])
memory (bytes)
4007575552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  8% |
memory (bytes)
4007575552
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  305044480.0
relative error loss 0.22404812
shape of L is 
torch.Size([])
memory (bytes)
4009922560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% |  9% |
memory (bytes)
4009922560
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  293344060.0
relative error loss 0.21545444
shape of L is 
torch.Size([])
memory (bytes)
4012015616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  9% |
memory (bytes)
4012015616
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  278955000.0
relative error loss 0.204886
shape of L is 
torch.Size([])
memory (bytes)
4014116864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  9% |
memory (bytes)
4014116864
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  251703420.0
relative error loss 0.18487035
shape of L is 
torch.Size([])
memory (bytes)
4016324608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  9% |
memory (bytes)
4016357376
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  242664580.0
relative error loss 0.17823152
shape of L is 
torch.Size([])
memory (bytes)
4018511872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  9% |
memory (bytes)
4018511872
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  233276290.0
relative error loss 0.17133604
shape of L is 
torch.Size([])
memory (bytes)
4020420608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  9% |
memory (bytes)
4020637696
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  215321730.0
relative error loss 0.15814884
time to take a step is 234.25384974479675
c= tensor(142.4813, device='cuda:0')
c= tensor(18901.8711, device='cuda:0')
c= tensor(24175.5703, device='cuda:0')
c= tensor(26572.4648, device='cuda:0')
c= tensor(152346.5312, device='cuda:0')
c= tensor(216520.8125, device='cuda:0')
c= tensor(386456.5625, device='cuda:0')
c= tensor(432503.9062, device='cuda:0')
c= tensor(435500.6875, device='cuda:0')
c= tensor(563553.2500, device='cuda:0')
c= tensor(566657.5625, device='cuda:0')
c= tensor(677282.1875, device='cuda:0')
c= tensor(679739.6250, device='cuda:0')
c= tensor(4005467.5000, device='cuda:0')
c= tensor(4064969.7500, device='cuda:0')
c= tensor(4170752., device='cuda:0')
c= tensor(4497765.5000, device='cuda:0')
c= tensor(4660050., device='cuda:0')
c= tensor(5621120., device='cuda:0')
c= tensor(6778846., device='cuda:0')
c= tensor(6812350.5000, device='cuda:0')
c= tensor(10478250., device='cuda:0')
c= tensor(10486526., device='cuda:0')
c= tensor(10589324., device='cuda:0')
c= tensor(10686426., device='cuda:0')
c= tensor(10871092., device='cuda:0')
c= tensor(10970779., device='cuda:0')
c= tensor(10980045., device='cuda:0')
c= tensor(11378534., device='cuda:0')
c= tensor(69574960., device='cuda:0')
c= tensor(69577392., device='cuda:0')
c= tensor(96607832., device='cuda:0')
c= tensor(96613776., device='cuda:0')
c= tensor(96617472., device='cuda:0')
c= tensor(96628264., device='cuda:0')
c= tensor(97616584., device='cuda:0')
c= tensor(98507768., device='cuda:0')
c= tensor(98509168., device='cuda:0')
c= tensor(98514920., device='cuda:0')
c= tensor(98517664., device='cuda:0')
c= tensor(98519880., device='cuda:0')
c= tensor(98520016., device='cuda:0')
c= tensor(98520192., device='cuda:0')
c= tensor(98520776., device='cuda:0')
c= tensor(98520928., device='cuda:0')
c= tensor(98521056., device='cuda:0')
c= tensor(98522928., device='cuda:0')
c= tensor(98524512., device='cuda:0')
c= tensor(98524968., device='cuda:0')
c= tensor(98542352., device='cuda:0')
c= tensor(98564424., device='cuda:0')
c= tensor(98566960., device='cuda:0')
c= tensor(98568552., device='cuda:0')
c= tensor(98569912., device='cuda:0')
c= tensor(98572776., device='cuda:0')
c= tensor(98585160., device='cuda:0')
c= tensor(98586424., device='cuda:0')
c= tensor(98587392., device='cuda:0')
c= tensor(98587928., device='cuda:0')
c= tensor(98588496., device='cuda:0')
c= tensor(98608368., device='cuda:0')
c= tensor(98608848., device='cuda:0')
c= tensor(98620248., device='cuda:0')
c= tensor(98622304., device='cuda:0')
c= tensor(98623152., device='cuda:0')
c= tensor(98624640., device='cuda:0')
c= tensor(98625104., device='cuda:0')
c= tensor(98625768., device='cuda:0')
c= tensor(98626400., device='cuda:0')
c= tensor(98627160., device='cuda:0')
c= tensor(98628600., device='cuda:0')
c= tensor(98629520., device='cuda:0')
c= tensor(98629768., device='cuda:0')
c= tensor(98630824., device='cuda:0')
c= tensor(98631832., device='cuda:0')
c= tensor(98635416., device='cuda:0')
c= tensor(98635792., device='cuda:0')
c= tensor(98636008., device='cuda:0')
c= tensor(98636520., device='cuda:0')
c= tensor(98639808., device='cuda:0')
c= tensor(98641288., device='cuda:0')
c= tensor(98641408., device='cuda:0')
c= tensor(98646736., device='cuda:0')
c= tensor(98646984., device='cuda:0')
c= tensor(98647448., device='cuda:0')
c= tensor(98648168., device='cuda:0')
c= tensor(98649384., device='cuda:0')
c= tensor(98649568., device='cuda:0')
c= tensor(98651576., device='cuda:0')
c= tensor(98653952., device='cuda:0')
c= tensor(98654384., device='cuda:0')
c= tensor(98654592., device='cuda:0')
c= tensor(98654848., device='cuda:0')
c= tensor(98657624., device='cuda:0')
c= tensor(98658472., device='cuda:0')
c= tensor(98659472., device='cuda:0')
c= tensor(98678488., device='cuda:0')
c= tensor(98684624., device='cuda:0')
c= tensor(98689160., device='cuda:0')
c= tensor(98690456., device='cuda:0')
c= tensor(98703424., device='cuda:0')
c= tensor(98704984., device='cuda:0')
c= tensor(98705600., device='cuda:0')
c= tensor(98706208., device='cuda:0')
c= tensor(98707352., device='cuda:0')
c= tensor(98707632., device='cuda:0')
c= tensor(98710816., device='cuda:0')
c= tensor(98710912., device='cuda:0')
c= tensor(98711648., device='cuda:0')
c= tensor(98712640., device='cuda:0')
c= tensor(98713488., device='cuda:0')
c= tensor(98713872., device='cuda:0')
c= tensor(98714920., device='cuda:0')
c= tensor(98717040., device='cuda:0')
c= tensor(98718976., device='cuda:0')
c= tensor(98719576., device='cuda:0')
c= tensor(98719968., device='cuda:0')
c= tensor(98720184., device='cuda:0')
c= tensor(98724216., device='cuda:0')
c= tensor(98724424., device='cuda:0')
c= tensor(98729440., device='cuda:0')
c= tensor(98730288., device='cuda:0')
c= tensor(98730824., device='cuda:0')
c= tensor(98732240., device='cuda:0')
c= tensor(98732968., device='cuda:0')
c= tensor(98734232., device='cuda:0')
c= tensor(98734632., device='cuda:0')
c= tensor(98734896., device='cuda:0')
c= tensor(98750672., device='cuda:0')
c= tensor(98752072., device='cuda:0')
c= tensor(98757776., device='cuda:0')
c= tensor(98758456., device='cuda:0')
c= tensor(98760296., device='cuda:0')
c= tensor(98760752., device='cuda:0')
c= tensor(98761464., device='cuda:0')
c= tensor(98762016., device='cuda:0')
c= tensor(98762280., device='cuda:0')
c= tensor(98763960., device='cuda:0')
c= tensor(98764440., device='cuda:0')
c= tensor(98765520., device='cuda:0')
c= tensor(98766560., device='cuda:0')
c= tensor(98766728., device='cuda:0')
c= tensor(98771880., device='cuda:0')
c= tensor(98829528., device='cuda:0')
c= tensor(98834400., device='cuda:0')
c= tensor(98834840., device='cuda:0')
c= tensor(98835416., device='cuda:0')
c= tensor(98836200., device='cuda:0')
c= tensor(98836856., device='cuda:0')
c= tensor(98837432., device='cuda:0')
c= tensor(98838296., device='cuda:0')
c= tensor(98849960., device='cuda:0')
c= tensor(98852360., device='cuda:0')
c= tensor(98857472., device='cuda:0')
c= tensor(98858384., device='cuda:0')
c= tensor(98868696., device='cuda:0')
c= tensor(98869368., device='cuda:0')
c= tensor(98870232., device='cuda:0')
c= tensor(98871160., device='cuda:0')
c= tensor(98871248., device='cuda:0')
c= tensor(98874368., device='cuda:0')
c= tensor(98874616., device='cuda:0')
c= tensor(98877008., device='cuda:0')
c= tensor(98877536., device='cuda:0')
c= tensor(98878728., device='cuda:0')
c= tensor(98879224., device='cuda:0')
c= tensor(98879800., device='cuda:0')
c= tensor(98880136., device='cuda:0')
c= tensor(98880896., device='cuda:0')
c= tensor(98880968., device='cuda:0')
c= tensor(98881176., device='cuda:0')
c= tensor(98881944., device='cuda:0')
c= tensor(98884032., device='cuda:0')
c= tensor(98884944., device='cuda:0')
c= tensor(98888792., device='cuda:0')
c= tensor(98889328., device='cuda:0')
c= tensor(98889896., device='cuda:0')
c= tensor(98893528., device='cuda:0')
c= tensor(98895672., device='cuda:0')
c= tensor(98896320., device='cuda:0')
c= tensor(98899968., device='cuda:0')
c= tensor(98900968., device='cuda:0')
c= tensor(98902792., device='cuda:0')
c= tensor(98903256., device='cuda:0')
c= tensor(98910696., device='cuda:0')
c= tensor(98913176., device='cuda:0')
c= tensor(98914224., device='cuda:0')
c= tensor(98915456., device='cuda:0')
c= tensor(98919224., device='cuda:0')
c= tensor(98962840., device='cuda:0')
c= tensor(98963328., device='cuda:0')
c= tensor(98963616., device='cuda:0')
c= tensor(98964056., device='cuda:0')
c= tensor(98964240., device='cuda:0')
c= tensor(98965712., device='cuda:0')
c= tensor(98966400., device='cuda:0')
c= tensor(98966648., device='cuda:0')
c= tensor(98967104., device='cuda:0')
c= tensor(98968560., device='cuda:0')
c= tensor(98968832., device='cuda:0')
c= tensor(98984848., device='cuda:0')
c= tensor(98985208., device='cuda:0')
c= tensor(98994824., device='cuda:0')
c= tensor(98999800., device='cuda:0')
c= tensor(99000784., device='cuda:0')
c= tensor(99000872., device='cuda:0')
c= tensor(99004224., device='cuda:0')
c= tensor(99006312., device='cuda:0')
c= tensor(99006960., device='cuda:0')
c= tensor(99018000., device='cuda:0')
c= tensor(99034216., device='cuda:0')
c= tensor(99034464., device='cuda:0')
c= tensor(99034680., device='cuda:0')
c= tensor(99034992., device='cuda:0')
c= tensor(99035488., device='cuda:0')
c= tensor(99035776., device='cuda:0')
c= tensor(99035928., device='cuda:0')
c= tensor(99036072., device='cuda:0')
c= tensor(99036912., device='cuda:0')
c= tensor(99037624., device='cuda:0')
c= tensor(99039120., device='cuda:0')
c= tensor(99039800., device='cuda:0')
c= tensor(99041504., device='cuda:0')
c= tensor(99042240., device='cuda:0')
c= tensor(99045408., device='cuda:0')
c= tensor(99046216., device='cuda:0')
c= tensor(99047000., device='cuda:0')
c= tensor(99047152., device='cuda:0')
c= tensor(99050024., device='cuda:0')
c= tensor(99050304., device='cuda:0')
c= tensor(99050440., device='cuda:0')
c= tensor(99051304., device='cuda:0')
c= tensor(99051736., device='cuda:0')
c= tensor(99059864., device='cuda:0')
c= tensor(99060984., device='cuda:0')
c= tensor(99061536., device='cuda:0')
c= tensor(99062744., device='cuda:0')
c= tensor(99065568., device='cuda:0')
c= tensor(99065880., device='cuda:0')
c= tensor(99072264., device='cuda:0')
c= tensor(99358664., device='cuda:0')
c= tensor(99367776., device='cuda:0')
c= tensor(99370336., device='cuda:0')
c= tensor(99370448., device='cuda:0')
c= tensor(99378576., device='cuda:0')
c= tensor(1.0029e+08, device='cuda:0')
c= tensor(1.0451e+08, device='cuda:0')
c= tensor(1.0451e+08, device='cuda:0')
c= tensor(1.0464e+08, device='cuda:0')
c= tensor(1.0472e+08, device='cuda:0')
c= tensor(1.0473e+08, device='cuda:0')
c= tensor(1.1166e+08, device='cuda:0')
c= tensor(1.1166e+08, device='cuda:0')
c= tensor(1.1167e+08, device='cuda:0')
c= tensor(1.1241e+08, device='cuda:0')
c= tensor(1.1464e+08, device='cuda:0')
c= tensor(1.1465e+08, device='cuda:0')
c= tensor(1.1468e+08, device='cuda:0')
c= tensor(1.1470e+08, device='cuda:0')
c= tensor(1.1487e+08, device='cuda:0')
c= tensor(1.1503e+08, device='cuda:0')
c= tensor(1.1505e+08, device='cuda:0')
c= tensor(1.1514e+08, device='cuda:0')
c= tensor(1.1517e+08, device='cuda:0')
c= tensor(1.1517e+08, device='cuda:0')
c= tensor(1.3727e+08, device='cuda:0')
c= tensor(1.3727e+08, device='cuda:0')
c= tensor(1.3727e+08, device='cuda:0')
c= tensor(1.3730e+08, device='cuda:0')
c= tensor(1.3735e+08, device='cuda:0')
c= tensor(1.4668e+08, device='cuda:0')
c= tensor(1.4695e+08, device='cuda:0')
c= tensor(1.4695e+08, device='cuda:0')
c= tensor(1.4701e+08, device='cuda:0')
c= tensor(1.4702e+08, device='cuda:0')
c= tensor(1.4710e+08, device='cuda:0')
c= tensor(1.4735e+08, device='cuda:0')
c= tensor(1.4736e+08, device='cuda:0')
c= tensor(1.4740e+08, device='cuda:0')
c= tensor(1.4740e+08, device='cuda:0')
c= tensor(1.4740e+08, device='cuda:0')
c= tensor(1.4783e+08, device='cuda:0')
c= tensor(1.4792e+08, device='cuda:0')
c= tensor(1.4795e+08, device='cuda:0')
c= tensor(1.4811e+08, device='cuda:0')
c= tensor(1.5249e+08, device='cuda:0')
c= tensor(1.5250e+08, device='cuda:0')
c= tensor(1.5253e+08, device='cuda:0')
c= tensor(1.5288e+08, device='cuda:0')
c= tensor(1.5288e+08, device='cuda:0')
c= tensor(1.5306e+08, device='cuda:0')
c= tensor(1.5910e+08, device='cuda:0')
c= tensor(1.6660e+08, device='cuda:0')
c= tensor(1.6663e+08, device='cuda:0')
c= tensor(1.6664e+08, device='cuda:0')
c= tensor(1.6664e+08, device='cuda:0')
c= tensor(1.6664e+08, device='cuda:0')
c= tensor(1.6671e+08, device='cuda:0')
c= tensor(1.6672e+08, device='cuda:0')
c= tensor(1.6684e+08, device='cuda:0')
c= tensor(1.7042e+08, device='cuda:0')
c= tensor(1.7054e+08, device='cuda:0')
c= tensor(1.7057e+08, device='cuda:0')
c= tensor(1.7057e+08, device='cuda:0')
c= tensor(1.7355e+08, device='cuda:0')
c= tensor(1.7358e+08, device='cuda:0')
c= tensor(1.7359e+08, device='cuda:0')
c= tensor(1.7366e+08, device='cuda:0')
c= tensor(2.3840e+08, device='cuda:0')
c= tensor(2.3842e+08, device='cuda:0')
c= tensor(2.3905e+08, device='cuda:0')
c= tensor(2.3905e+08, device='cuda:0')
c= tensor(2.4038e+08, device='cuda:0')
c= tensor(2.4040e+08, device='cuda:0')
c= tensor(2.5433e+08, device='cuda:0')
c= tensor(2.5449e+08, device='cuda:0')
c= tensor(2.5449e+08, device='cuda:0')
c= tensor(2.5495e+08, device='cuda:0')
c= tensor(2.5530e+08, device='cuda:0')
c= tensor(2.5531e+08, device='cuda:0')
c= tensor(2.5536e+08, device='cuda:0')
c= tensor(2.5703e+08, device='cuda:0')
c= tensor(2.6302e+08, device='cuda:0')
c= tensor(2.6322e+08, device='cuda:0')
c= tensor(2.6322e+08, device='cuda:0')
c= tensor(2.6322e+08, device='cuda:0')
c= tensor(2.6379e+08, device='cuda:0')
c= tensor(2.6695e+08, device='cuda:0')
c= tensor(2.6696e+08, device='cuda:0')
c= tensor(2.6696e+08, device='cuda:0')
c= tensor(2.6706e+08, device='cuda:0')
c= tensor(2.6727e+08, device='cuda:0')
c= tensor(2.6765e+08, device='cuda:0')
c= tensor(2.6765e+08, device='cuda:0')
c= tensor(2.6767e+08, device='cuda:0')
c= tensor(2.6769e+08, device='cuda:0')
c= tensor(2.6770e+08, device='cuda:0')
c= tensor(2.6771e+08, device='cuda:0')
c= tensor(2.6771e+08, device='cuda:0')
c= tensor(2.6818e+08, device='cuda:0')
c= tensor(2.6823e+08, device='cuda:0')
c= tensor(2.6825e+08, device='cuda:0')
c= tensor(2.6834e+08, device='cuda:0')
c= tensor(2.6835e+08, device='cuda:0')
c= tensor(2.7525e+08, device='cuda:0')
c= tensor(2.7526e+08, device='cuda:0')
c= tensor(2.7578e+08, device='cuda:0')
c= tensor(2.7578e+08, device='cuda:0')
c= tensor(2.7578e+08, device='cuda:0')
c= tensor(2.7578e+08, device='cuda:0')
c= tensor(2.7581e+08, device='cuda:0')
c= tensor(2.7581e+08, device='cuda:0')
c= tensor(2.7614e+08, device='cuda:0')
c= tensor(2.7614e+08, device='cuda:0')
c= tensor(2.7614e+08, device='cuda:0')
c= tensor(2.7866e+08, device='cuda:0')
c= tensor(2.7871e+08, device='cuda:0')
c= tensor(2.7878e+08, device='cuda:0')
c= tensor(2.8001e+08, device='cuda:0')
c= tensor(2.8114e+08, device='cuda:0')
c= tensor(2.8117e+08, device='cuda:0')
c= tensor(2.8117e+08, device='cuda:0')
c= tensor(2.8120e+08, device='cuda:0')
c= tensor(2.8120e+08, device='cuda:0')
c= tensor(2.8120e+08, device='cuda:0')
c= tensor(2.8125e+08, device='cuda:0')
c= tensor(2.8125e+08, device='cuda:0')
c= tensor(2.8125e+08, device='cuda:0')
c= tensor(2.8125e+08, device='cuda:0')
c= tensor(2.8125e+08, device='cuda:0')
c= tensor(2.8356e+08, device='cuda:0')
c= tensor(2.8357e+08, device='cuda:0')
c= tensor(2.8383e+08, device='cuda:0')
c= tensor(2.8384e+08, device='cuda:0')
c= tensor(2.8384e+08, device='cuda:0')
c= tensor(2.8387e+08, device='cuda:0')
c= tensor(3.2390e+08, device='cuda:0')
c= tensor(3.3270e+08, device='cuda:0')
c= tensor(3.3276e+08, device='cuda:0')
c= tensor(3.3347e+08, device='cuda:0')
c= tensor(3.3347e+08, device='cuda:0')
c= tensor(3.3470e+08, device='cuda:0')
c= tensor(3.3480e+08, device='cuda:0')
c= tensor(3.3527e+08, device='cuda:0')
c= tensor(3.3529e+08, device='cuda:0')
c= tensor(3.3572e+08, device='cuda:0')
c= tensor(3.4719e+08, device='cuda:0')
c= tensor(3.4722e+08, device='cuda:0')
c= tensor(3.4722e+08, device='cuda:0')
c= tensor(3.4726e+08, device='cuda:0')
c= tensor(3.4727e+08, device='cuda:0')
c= tensor(3.4727e+08, device='cuda:0')
c= tensor(3.4785e+08, device='cuda:0')
c= tensor(3.4788e+08, device='cuda:0')
c= tensor(3.4788e+08, device='cuda:0')
c= tensor(3.4792e+08, device='cuda:0')
c= tensor(3.4796e+08, device='cuda:0')
c= tensor(3.4796e+08, device='cuda:0')
c= tensor(3.4884e+08, device='cuda:0')
c= tensor(3.4917e+08, device='cuda:0')
c= tensor(3.4957e+08, device='cuda:0')
c= tensor(3.5000e+08, device='cuda:0')
c= tensor(3.5046e+08, device='cuda:0')
c= tensor(3.5046e+08, device='cuda:0')
c= tensor(3.5047e+08, device='cuda:0')
c= tensor(3.5050e+08, device='cuda:0')
c= tensor(3.5058e+08, device='cuda:0')
c= tensor(3.5058e+08, device='cuda:0')
c= tensor(3.5145e+08, device='cuda:0')
c= tensor(3.5325e+08, device='cuda:0')
c= tensor(3.5338e+08, device='cuda:0')
c= tensor(3.5341e+08, device='cuda:0')
c= tensor(3.5377e+08, device='cuda:0')
c= tensor(3.5378e+08, device='cuda:0')
c= tensor(3.5378e+08, device='cuda:0')
c= tensor(3.5379e+08, device='cuda:0')
c= tensor(3.5416e+08, device='cuda:0')
c= tensor(3.5444e+08, device='cuda:0')
c= tensor(3.6444e+08, device='cuda:0')
c= tensor(3.6823e+08, device='cuda:0')
c= tensor(3.6836e+08, device='cuda:0')
c= tensor(3.6844e+08, device='cuda:0')
c= tensor(3.6882e+08, device='cuda:0')
c= tensor(3.6882e+08, device='cuda:0')
c= tensor(3.6882e+08, device='cuda:0')
c= tensor(3.7268e+08, device='cuda:0')
c= tensor(3.7270e+08, device='cuda:0')
c= tensor(3.7270e+08, device='cuda:0')
c= tensor(3.7273e+08, device='cuda:0')
c= tensor(3.7294e+08, device='cuda:0')
c= tensor(3.7296e+08, device='cuda:0')
c= tensor(3.7321e+08, device='cuda:0')
c= tensor(3.7323e+08, device='cuda:0')
c= tensor(3.7325e+08, device='cuda:0')
c= tensor(3.7325e+08, device='cuda:0')
c= tensor(3.7326e+08, device='cuda:0')
c= tensor(3.7341e+08, device='cuda:0')
c= tensor(3.7346e+08, device='cuda:0')
c= tensor(3.7348e+08, device='cuda:0')
c= tensor(3.7382e+08, device='cuda:0')
c= tensor(3.7382e+08, device='cuda:0')
c= tensor(3.7386e+08, device='cuda:0')
c= tensor(3.7386e+08, device='cuda:0')
c= tensor(3.7391e+08, device='cuda:0')
c= tensor(3.7391e+08, device='cuda:0')
c= tensor(3.7396e+08, device='cuda:0')
c= tensor(3.7397e+08, device='cuda:0')
c= tensor(3.7400e+08, device='cuda:0')
c= tensor(3.7424e+08, device='cuda:0')
c= tensor(3.7481e+08, device='cuda:0')
c= tensor(3.7481e+08, device='cuda:0')
c= tensor(3.7483e+08, device='cuda:0')
c= tensor(3.7626e+08, device='cuda:0')
c= tensor(3.7627e+08, device='cuda:0')
c= tensor(3.9585e+08, device='cuda:0')
c= tensor(3.9585e+08, device='cuda:0')
c= tensor(3.9608e+08, device='cuda:0')
c= tensor(4.0053e+08, device='cuda:0')
c= tensor(4.0054e+08, device='cuda:0')
c= tensor(4.0117e+08, device='cuda:0')
c= tensor(4.0138e+08, device='cuda:0')
c= tensor(4.2274e+08, device='cuda:0')
c= tensor(4.2274e+08, device='cuda:0')
c= tensor(4.2281e+08, device='cuda:0')
c= tensor(4.2281e+08, device='cuda:0')
c= tensor(4.2282e+08, device='cuda:0')
c= tensor(4.2283e+08, device='cuda:0')
c= tensor(4.2326e+08, device='cuda:0')
c= tensor(4.2329e+08, device='cuda:0')
c= tensor(4.2361e+08, device='cuda:0')
c= tensor(4.2362e+08, device='cuda:0')
c= tensor(4.2362e+08, device='cuda:0')
c= tensor(4.2362e+08, device='cuda:0')
c= tensor(4.2488e+08, device='cuda:0')
c= tensor(4.2518e+08, device='cuda:0')
c= tensor(4.3321e+08, device='cuda:0')
c= tensor(4.3332e+08, device='cuda:0')
c= tensor(4.3333e+08, device='cuda:0')
c= tensor(4.3333e+08, device='cuda:0')
c= tensor(4.3335e+08, device='cuda:0')
c= tensor(4.4276e+08, device='cuda:0')
c= tensor(4.4277e+08, device='cuda:0')
c= tensor(4.4277e+08, device='cuda:0')
c= tensor(4.4306e+08, device='cuda:0')
c= tensor(4.4323e+08, device='cuda:0')
c= tensor(4.4323e+08, device='cuda:0')
c= tensor(4.4324e+08, device='cuda:0')
c= tensor(4.5233e+08, device='cuda:0')
c= tensor(4.5235e+08, device='cuda:0')
c= tensor(4.5301e+08, device='cuda:0')
c= tensor(4.5304e+08, device='cuda:0')
c= tensor(4.5327e+08, device='cuda:0')
c= tensor(4.5468e+08, device='cuda:0')
c= tensor(4.5572e+08, device='cuda:0')
c= tensor(4.5752e+08, device='cuda:0')
c= tensor(4.5754e+08, device='cuda:0')
c= tensor(4.5780e+08, device='cuda:0')
c= tensor(4.5784e+08, device='cuda:0')
c= tensor(4.5785e+08, device='cuda:0')
c= tensor(4.5787e+08, device='cuda:0')
c= tensor(4.5787e+08, device='cuda:0')
c= tensor(4.5804e+08, device='cuda:0')
c= tensor(4.5805e+08, device='cuda:0')
c= tensor(4.5805e+08, device='cuda:0')
c= tensor(4.5807e+08, device='cuda:0')
c= tensor(4.5808e+08, device='cuda:0')
c= tensor(4.5875e+08, device='cuda:0')
c= tensor(4.5875e+08, device='cuda:0')
c= tensor(4.5879e+08, device='cuda:0')
c= tensor(4.5880e+08, device='cuda:0')
c= tensor(4.5885e+08, device='cuda:0')
c= tensor(4.5885e+08, device='cuda:0')
c= tensor(4.5885e+08, device='cuda:0')
c= tensor(4.5887e+08, device='cuda:0')
c= tensor(4.6042e+08, device='cuda:0')
c= tensor(4.6042e+08, device='cuda:0')
c= tensor(4.6042e+08, device='cuda:0')
c= tensor(4.6042e+08, device='cuda:0')
c= tensor(4.6528e+08, device='cuda:0')
c= tensor(4.7035e+08, device='cuda:0')
c= tensor(4.7038e+08, device='cuda:0')
c= tensor(4.7039e+08, device='cuda:0')
c= tensor(4.7105e+08, device='cuda:0')
c= tensor(4.7130e+08, device='cuda:0')
c= tensor(4.7130e+08, device='cuda:0')
c= tensor(4.7131e+08, device='cuda:0')
c= tensor(4.7132e+08, device='cuda:0')
c= tensor(4.7177e+08, device='cuda:0')
c= tensor(4.7177e+08, device='cuda:0')
c= tensor(4.7306e+08, device='cuda:0')
c= tensor(4.7307e+08, device='cuda:0')
c= tensor(4.7311e+08, device='cuda:0')
c= tensor(4.7311e+08, device='cuda:0')
c= tensor(4.7314e+08, device='cuda:0')
c= tensor(4.7315e+08, device='cuda:0')
c= tensor(4.7320e+08, device='cuda:0')
c= tensor(4.7357e+08, device='cuda:0')
c= tensor(4.7749e+08, device='cuda:0')
c= tensor(4.7750e+08, device='cuda:0')
c= tensor(4.7750e+08, device='cuda:0')
c= tensor(4.7751e+08, device='cuda:0')
c= tensor(4.8749e+08, device='cuda:0')
c= tensor(4.8751e+08, device='cuda:0')
c= tensor(4.8751e+08, device='cuda:0')
c= tensor(4.8751e+08, device='cuda:0')
c= tensor(4.8853e+08, device='cuda:0')
c= tensor(4.8853e+08, device='cuda:0')
c= tensor(4.8881e+08, device='cuda:0')
c= tensor(4.8881e+08, device='cuda:0')
c= tensor(4.8883e+08, device='cuda:0')
c= tensor(4.8884e+08, device='cuda:0')
c= tensor(4.8884e+08, device='cuda:0')
c= tensor(4.8884e+08, device='cuda:0')
c= tensor(4.8994e+08, device='cuda:0')
c= tensor(4.9565e+08, device='cuda:0')
c= tensor(4.9680e+08, device='cuda:0')
c= tensor(4.9684e+08, device='cuda:0')
c= tensor(4.9689e+08, device='cuda:0')
c= tensor(4.9689e+08, device='cuda:0')
c= tensor(4.9690e+08, device='cuda:0')
c= tensor(4.9701e+08, device='cuda:0')
c= tensor(4.9705e+08, device='cuda:0')
c= tensor(4.9732e+08, device='cuda:0')
c= tensor(4.9733e+08, device='cuda:0')
c= tensor(5.0710e+08, device='cuda:0')
c= tensor(5.0713e+08, device='cuda:0')
c= tensor(5.0718e+08, device='cuda:0')
c= tensor(5.1101e+08, device='cuda:0')
c= tensor(5.1111e+08, device='cuda:0')
c= tensor(5.1176e+08, device='cuda:0')
c= tensor(5.1276e+08, device='cuda:0')
c= tensor(5.1294e+08, device='cuda:0')
c= tensor(5.1301e+08, device='cuda:0')
c= tensor(5.1301e+08, device='cuda:0')
c= tensor(5.1305e+08, device='cuda:0')
c= tensor(5.1306e+08, device='cuda:0')
c= tensor(5.1333e+08, device='cuda:0')
c= tensor(5.1848e+08, device='cuda:0')
c= tensor(5.1972e+08, device='cuda:0')
c= tensor(5.2010e+08, device='cuda:0')
c= tensor(5.2011e+08, device='cuda:0')
c= tensor(5.2023e+08, device='cuda:0')
c= tensor(5.2039e+08, device='cuda:0')
c= tensor(5.2043e+08, device='cuda:0')
c= tensor(5.2070e+08, device='cuda:0')
c= tensor(5.2244e+08, device='cuda:0')
c= tensor(5.2247e+08, device='cuda:0')
c= tensor(5.2483e+08, device='cuda:0')
c= tensor(5.2573e+08, device='cuda:0')
c= tensor(5.2578e+08, device='cuda:0')
c= tensor(5.2578e+08, device='cuda:0')
c= tensor(5.2634e+08, device='cuda:0')
c= tensor(5.2654e+08, device='cuda:0')
c= tensor(5.2654e+08, device='cuda:0')
c= tensor(5.3217e+08, device='cuda:0')
c= tensor(5.3238e+08, device='cuda:0')
c= tensor(5.3253e+08, device='cuda:0')
c= tensor(5.3253e+08, device='cuda:0')
c= tensor(5.3253e+08, device='cuda:0')
c= tensor(5.3254e+08, device='cuda:0')
c= tensor(5.3254e+08, device='cuda:0')
c= tensor(5.3258e+08, device='cuda:0')
c= tensor(5.3267e+08, device='cuda:0')
c= tensor(1.2587e+09, device='cuda:0')
c= tensor(1.2587e+09, device='cuda:0')
c= tensor(1.2588e+09, device='cuda:0')
c= tensor(1.2588e+09, device='cuda:0')
c= tensor(1.2588e+09, device='cuda:0')
c= tensor(1.2588e+09, device='cuda:0')
c= tensor(1.2601e+09, device='cuda:0')
c= tensor(1.2601e+09, device='cuda:0')
c= tensor(1.2781e+09, device='cuda:0')
c= tensor(1.2781e+09, device='cuda:0')
c= tensor(1.2805e+09, device='cuda:0')
c= tensor(1.2809e+09, device='cuda:0')
c= tensor(1.2813e+09, device='cuda:0')
c= tensor(1.2835e+09, device='cuda:0')
c= tensor(1.2835e+09, device='cuda:0')
c= tensor(1.2835e+09, device='cuda:0')
c= tensor(1.2840e+09, device='cuda:0')
c= tensor(1.2840e+09, device='cuda:0')
c= tensor(1.2840e+09, device='cuda:0')
c= tensor(1.2841e+09, device='cuda:0')
c= tensor(1.2843e+09, device='cuda:0')
c= tensor(1.2864e+09, device='cuda:0')
c= tensor(1.2864e+09, device='cuda:0')
c= tensor(1.2883e+09, device='cuda:0')
c= tensor(1.2961e+09, device='cuda:0')
c= tensor(1.2962e+09, device='cuda:0')
c= tensor(1.2962e+09, device='cuda:0')
c= tensor(1.2981e+09, device='cuda:0')
c= tensor(1.2981e+09, device='cuda:0')
c= tensor(1.2982e+09, device='cuda:0')
c= tensor(1.2982e+09, device='cuda:0')
c= tensor(1.2985e+09, device='cuda:0')
c= tensor(1.2986e+09, device='cuda:0')
c= tensor(1.3017e+09, device='cuda:0')
c= tensor(1.3018e+09, device='cuda:0')
c= tensor(1.3018e+09, device='cuda:0')
c= tensor(1.3018e+09, device='cuda:0')
c= tensor(1.3018e+09, device='cuda:0')
c= tensor(1.3019e+09, device='cuda:0')
c= tensor(1.3023e+09, device='cuda:0')
c= tensor(1.3037e+09, device='cuda:0')
c= tensor(1.3038e+09, device='cuda:0')
c= tensor(1.3038e+09, device='cuda:0')
c= tensor(1.3039e+09, device='cuda:0')
c= tensor(1.3043e+09, device='cuda:0')
c= tensor(1.3044e+09, device='cuda:0')
c= tensor(1.3046e+09, device='cuda:0')
c= tensor(1.3406e+09, device='cuda:0')
c= tensor(1.3407e+09, device='cuda:0')
c= tensor(1.3407e+09, device='cuda:0')
c= tensor(1.3407e+09, device='cuda:0')
c= tensor(1.3408e+09, device='cuda:0')
c= tensor(1.3409e+09, device='cuda:0')
c= tensor(1.3409e+09, device='cuda:0')
c= tensor(1.3409e+09, device='cuda:0')
c= tensor(1.3409e+09, device='cuda:0')
c= tensor(1.3415e+09, device='cuda:0')
c= tensor(1.3415e+09, device='cuda:0')
c= tensor(1.3416e+09, device='cuda:0')
c= tensor(1.3416e+09, device='cuda:0')
c= tensor(1.3416e+09, device='cuda:0')
c= tensor(1.3416e+09, device='cuda:0')
c= tensor(1.3417e+09, device='cuda:0')
c= tensor(1.3419e+09, device='cuda:0')
c= tensor(1.3419e+09, device='cuda:0')
c= tensor(1.3423e+09, device='cuda:0')
c= tensor(1.3424e+09, device='cuda:0')
c= tensor(1.3424e+09, device='cuda:0')
c= tensor(1.3469e+09, device='cuda:0')
c= tensor(1.3470e+09, device='cuda:0')
c= tensor(1.3471e+09, device='cuda:0')
c= tensor(1.3481e+09, device='cuda:0')
c= tensor(1.3481e+09, device='cuda:0')
c= tensor(1.3487e+09, device='cuda:0')
c= tensor(1.3497e+09, device='cuda:0')
c= tensor(1.3497e+09, device='cuda:0')
c= tensor(1.3497e+09, device='cuda:0')
c= tensor(1.3497e+09, device='cuda:0')
c= tensor(1.3500e+09, device='cuda:0')
c= tensor(1.3504e+09, device='cuda:0')
c= tensor(1.3504e+09, device='cuda:0')
c= tensor(1.3506e+09, device='cuda:0')
c= tensor(1.3506e+09, device='cuda:0')
c= tensor(1.3508e+09, device='cuda:0')
c= tensor(1.3508e+09, device='cuda:0')
c= tensor(1.3508e+09, device='cuda:0')
c= tensor(1.3519e+09, device='cuda:0')
c= tensor(1.3601e+09, device='cuda:0')
c= tensor(1.3601e+09, device='cuda:0')
c= tensor(1.3601e+09, device='cuda:0')
c= tensor(1.3601e+09, device='cuda:0')
c= tensor(1.3608e+09, device='cuda:0')
c= tensor(1.3608e+09, device='cuda:0')
c= tensor(1.3608e+09, device='cuda:0')
c= tensor(1.3609e+09, device='cuda:0')
c= tensor(1.3611e+09, device='cuda:0')
c= tensor(1.3611e+09, device='cuda:0')
c= tensor(1.3615e+09, device='cuda:0')
c= tensor(1.3615e+09, device='cuda:0')
time to make c is 11.203302145004272
time for making loss is 11.203348875045776
p0 True
it  0 : 1035095552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  6% |
shape of L is 
torch.Size([])
memory (bytes)
4022714368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  6% |
memory (bytes)
4022996992
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  215321730.0
relative error loss 0.15814884
shape of L is 
torch.Size([])
memory (bytes)
4049448960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  7% |
memory (bytes)
4049604608
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  7% |
error is  211094910.0
relative error loss 0.15504433
shape of L is 
torch.Size([])
memory (bytes)
4053139456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% |  7% |
memory (bytes)
4053348352
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  202564100.0
relative error loss 0.14877865
shape of L is 
torch.Size([])
memory (bytes)
4056563712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  8% |
memory (bytes)
4056563712
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  198821500.0
relative error loss 0.1460298
shape of L is 
torch.Size([])
memory (bytes)
4059783168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% |  8% |
memory (bytes)
4059783168
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  196109950.0
relative error loss 0.14403823
shape of L is 
torch.Size([])
memory (bytes)
4062994432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4062994432
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  193852290.0
relative error loss 0.14238003
shape of L is 
torch.Size([])
memory (bytes)
4066205696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4066205696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% |  9% |
error is  191717890.0
relative error loss 0.14081235
shape of L is 
torch.Size([])
memory (bytes)
4069416960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
4069416960
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  189998200.0
relative error loss 0.1395493
shape of L is 
torch.Size([])
memory (bytes)
4072648704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
4072648704
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  188827000.0
relative error loss 0.13868907
shape of L is 
torch.Size([])
memory (bytes)
4075864064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% |  8% |
memory (bytes)
4075864064
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  187938820.0
relative error loss 0.13803671
time to take a step is 318.53372073173523
it  1 : 1719018496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  9% |
shape of L is 
torch.Size([])
memory (bytes)
4079091712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  8% |
memory (bytes)
4079091712
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  187938820.0
relative error loss 0.13803671
shape of L is 
torch.Size([])
memory (bytes)
4082225152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4082307072
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  187159800.0
relative error loss 0.13746455
shape of L is 
torch.Size([])
memory (bytes)
4085522432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4085522432
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  186463620.0
relative error loss 0.13695322
shape of L is 
torch.Size([])
memory (bytes)
4088713216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4088741888
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  185893500.0
relative error loss 0.13653448
shape of L is 
torch.Size([])
memory (bytes)
4091953152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% |  8% |
memory (bytes)
4091953152
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  185352060.0
relative error loss 0.13613681
shape of L is 
torch.Size([])
memory (bytes)
4095115264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 32% |  8% |
memory (bytes)
4095180800
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  184993280.0
relative error loss 0.13587329
shape of L is 
torch.Size([])
memory (bytes)
4098400256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 32% |  8% |
memory (bytes)
4098400256
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  184605570.0
relative error loss 0.13558853
shape of L is 
torch.Size([])
memory (bytes)
4101517312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4101517312
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  184304130.0
relative error loss 0.13536713
shape of L is 
torch.Size([])
memory (bytes)
4104839168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 32% |  8% |
memory (bytes)
4104839168
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  183928580.0
relative error loss 0.13509129
shape of L is 
torch.Size([])
memory (bytes)
4107837440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4108025856
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  183585400.0
relative error loss 0.13483924
time to take a step is 321.5518870353699
it  2 : 1719018496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  9% |
shape of L is 
torch.Size([])
memory (bytes)
4111273984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% |  8% |
memory (bytes)
4111273984
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  183585400.0
relative error loss 0.13483924
shape of L is 
torch.Size([])
memory (bytes)
4114378752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% |  8% |
memory (bytes)
4114378752
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  182990460.0
relative error loss 0.13440228
shape of L is 
torch.Size([])
memory (bytes)
4117737472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4117737472
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  182640130.0
relative error loss 0.13414495
shape of L is 
torch.Size([])
memory (bytes)
4120903680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% |  8% |
memory (bytes)
4120903680
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  182235260.0
relative error loss 0.1338476
shape of L is 
torch.Size([])
memory (bytes)
4123889664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% |  8% |
memory (bytes)
4124155904
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  181900290.0
relative error loss 0.13360156
shape of L is 
torch.Size([])
memory (bytes)
4127383552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4127383552
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  181367800.0
relative error loss 0.13321047
shape of L is 
torch.Size([])
memory (bytes)
4130566144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4130566144
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  181059200.0
relative error loss 0.1329838
shape of L is 
torch.Size([])
memory (bytes)
4133826560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4133826560
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  180827400.0
relative error loss 0.13281354
shape of L is 
torch.Size([])
memory (bytes)
4137054208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4137054208
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  180438020.0
relative error loss 0.13252756
shape of L is 
torch.Size([])
memory (bytes)
4140265472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4140265472
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  180221820.0
relative error loss 0.13236877
time to take a step is 321.53770184516907
it  3 : 1719018496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
shape of L is 
torch.Size([])
memory (bytes)
4143443968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  8% |
memory (bytes)
4143443968
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  180221820.0
relative error loss 0.13236877
shape of L is 
torch.Size([])
memory (bytes)
4146593792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  8% |
memory (bytes)
4146696192
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  180080510.0
relative error loss 0.13226497
shape of L is 
torch.Size([])
memory (bytes)
4149932032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% |  8% |
memory (bytes)
4149932032
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  179966720.0
relative error loss 0.13218139
shape of L is 
torch.Size([])
memory (bytes)
4153061376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 32% |  8% |
memory (bytes)
4153061376
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  179724160.0
relative error loss 0.13200325
shape of L is 
torch.Size([])
memory (bytes)
4156391424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  8% |
memory (bytes)
4156391424
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  179525120.0
relative error loss 0.13185705
shape of L is 
torch.Size([])
memory (bytes)
4159512576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4159512576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% |  9% |
error is  179261820.0
relative error loss 0.13166367
shape of L is 
torch.Size([])
memory (bytes)
4162826240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% |  8% |
memory (bytes)
4162826240
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  179055870.0
relative error loss 0.1315124
shape of L is 
torch.Size([])
memory (bytes)
4166041600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4166041600
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  178906750.0
relative error loss 0.13140288
shape of L is 
torch.Size([])
memory (bytes)
4169252864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4169269248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% |  9% |
error is  178807940.0
relative error loss 0.1313303
shape of L is 
torch.Size([])
memory (bytes)
4172488704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4172488704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  9% |
error is  178603520.0
relative error loss 0.13118015
time to take a step is 292.92033982276917
it  4 : 1719018496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  9% |
shape of L is 
torch.Size([])
memory (bytes)
4175712256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% |  8% |
memory (bytes)
4175712256
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  178603520.0
relative error loss 0.13118015
shape of L is 
torch.Size([])
memory (bytes)
4178927616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% |  8% |
memory (bytes)
4178927616
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  178520320.0
relative error loss 0.13111906
shape of L is 
torch.Size([])
memory (bytes)
4182159360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4182159360
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  178433150.0
relative error loss 0.13105503
shape of L is 
torch.Size([])
memory (bytes)
4185382912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4185382912
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  178303360.0
relative error loss 0.1309597
shape of L is 
torch.Size([])
memory (bytes)
4188606464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  8% |
memory (bytes)
4188606464
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  178164480.0
relative error loss 0.13085769
shape of L is 
torch.Size([])
memory (bytes)
4191817728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% |  8% |
memory (bytes)
4191817728
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  178068860.0
relative error loss 0.13078746
shape of L is 
torch.Size([])
memory (bytes)
4194963456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4195045376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% |  9% |
error is  177966080.0
relative error loss 0.13071197
shape of L is 
torch.Size([])
memory (bytes)
4198264832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4198264832
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  177793280.0
relative error loss 0.13058506
shape of L is 
torch.Size([])
memory (bytes)
4201324544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% |  8% |
memory (bytes)
4201451520
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  177655040.0
relative error loss 0.13048352
shape of L is 
torch.Size([])
memory (bytes)
4204703744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4204703744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% |  9% |
error is  177568900.0
relative error loss 0.13042025
time to take a step is 302.33299374580383
it  5 : 1719018496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
shape of L is 
torch.Size([])
memory (bytes)
4207915008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4207915008
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  177568900.0
relative error loss 0.13042025
shape of L is 
torch.Size([])
memory (bytes)
4211027968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  8% |
memory (bytes)
4211134464
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  177498240.0
relative error loss 0.13036835
shape of L is 
torch.Size([])
memory (bytes)
4214353920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% |  8% |
memory (bytes)
4214353920
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  177399680.0
relative error loss 0.13029596
shape of L is 
torch.Size([])
memory (bytes)
4217544704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% |  8% |
memory (bytes)
4217544704
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  177323650.0
relative error loss 0.13024013
shape of L is 
torch.Size([])
memory (bytes)
4220809216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  8% |
memory (bytes)
4220809216
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  177275260.0
relative error loss 0.13020459
shape of L is 
torch.Size([])
memory (bytes)
4223913984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% |  8% |
memory (bytes)
4223913984
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  177177090.0
relative error loss 0.13013248
shape of L is 
torch.Size([])
memory (bytes)
4227166208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% |  8% |
memory (bytes)
4227244032
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  177125890.0
relative error loss 0.13009487
shape of L is 
torch.Size([])
memory (bytes)
4230463488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4230463488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% |  9% |
error is  177073920.0
relative error loss 0.13005671
shape of L is 
torch.Size([])
memory (bytes)
4233617408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  8% |
memory (bytes)
4233617408
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  176904320.0
relative error loss 0.12993214
shape of L is 
torch.Size([])
memory (bytes)
4236902400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% |  8% |
memory (bytes)
4236902400
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  176887680.0
relative error loss 0.12991992
time to take a step is 292.82302498817444
it  6 : 1719018496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  9% |
shape of L is 
torch.Size([])
memory (bytes)
4240117760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% |  8% |
memory (bytes)
4240117760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% |  8% |
error is  176887680.0
relative error loss 0.12991992
shape of L is 
torch.Size([])
memory (bytes)
4243189760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4243324928
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  176811900.0
relative error loss 0.12986426
shape of L is 
torch.Size([])
memory (bytes)
4246548480
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4246548480
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  176771200.0
relative error loss 0.12983437
shape of L is 
torch.Size([])
memory (bytes)
4249665536
| ID | GPU | MEM |
------------------
|  0 | 18% |  0% |
|  1 | 11% |  8% |
memory (bytes)
4249665536
| ID | GPU  | MEM |
-------------------
|  0 |  15% |  0% |
|  1 | 100% |  9% |
error is  176700670.0
relative error loss 0.12978256
shape of L is 
torch.Size([])
memory (bytes)
4252987392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% |  8% |
memory (bytes)
4252987392
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  176630530.0
relative error loss 0.12973104
shape of L is 
torch.Size([])
memory (bytes)
4256153600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4256153600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% |  9% |
error is  176534400.0
relative error loss 0.12966044
shape of L is 
torch.Size([])
memory (bytes)
4259426304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4259426304
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  176449920.0
relative error loss 0.1295984
shape of L is 
torch.Size([])
memory (bytes)
4262641664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4262641664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% |  9% |
error is  176412670.0
relative error loss 0.12957104
shape of L is 
torch.Size([])
memory (bytes)
4265836544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% |  8% |
memory (bytes)
4265836544
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  176372350.0
relative error loss 0.12954141
shape of L is 
torch.Size([])
memory (bytes)
4269068288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% |  9% |
memory (bytes)
4269068288
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  176324220.0
relative error loss 0.12950607
time to take a step is 291.1414244174957
it  7 : 1719018496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
shape of L is 
torch.Size([])
memory (bytes)
4272205824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  8% |
memory (bytes)
4272287744
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  176324220.0
relative error loss 0.12950607
shape of L is 
torch.Size([])
memory (bytes)
4275441664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% |  8% |
memory (bytes)
4275441664
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  176262270.0
relative error loss 0.12946057
shape of L is 
torch.Size([])
memory (bytes)
4278730752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4278730752
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  176200580.0
relative error loss 0.12941526
shape of L is 
torch.Size([])
memory (bytes)
4281917440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
memory (bytes)
4281917440
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  176141060.0
relative error loss 0.12937154
shape of L is 
torch.Size([])
memory (bytes)
4285161472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% |  9% |
memory (bytes)
4285161472
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  176085760.0
relative error loss 0.12933092
shape of L is 
torch.Size([])
memory (bytes)
4288385024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% |  9% |
memory (bytes)
4288385024
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  176043780.0
relative error loss 0.12930009
shape of L is 
torch.Size([])
memory (bytes)
4291530752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
memory (bytes)
4291608576
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  176011140.0
relative error loss 0.12927611
shape of L is 
torch.Size([])
memory (bytes)
4294815744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% |  9% |
memory (bytes)
4294815744
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  175981820.0
relative error loss 0.12925458
shape of L is 
torch.Size([])
memory (bytes)
4298039296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% |  9% |
memory (bytes)
4298039296
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  175956740.0
relative error loss 0.12923616
shape of L is 
torch.Size([])
memory (bytes)
4301168640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4301271040
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  175941120.0
relative error loss 0.12922469
time to take a step is 291.01730728149414
it  8 : 1719018496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
shape of L is 
torch.Size([])
memory (bytes)
4304474112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4304474112
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  175941120.0
relative error loss 0.12922469
shape of L is 
torch.Size([])
memory (bytes)
4307644416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 30% |  8% |
memory (bytes)
4307644416
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  175920260.0
relative error loss 0.12920937
shape of L is 
torch.Size([])
memory (bytes)
4310859776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% |  8% |
memory (bytes)
4310913024
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  175893500.0
relative error loss 0.12918971
shape of L is 
torch.Size([])
memory (bytes)
4314140672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4314140672
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  175870080.0
relative error loss 0.1291725
shape of L is 
torch.Size([])
memory (bytes)
4317302784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% |  9% |
memory (bytes)
4317302784
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  175815940.0
relative error loss 0.12913275
shape of L is 
torch.Size([])
memory (bytes)
4320583680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4320583680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% |  9% |
error is  175778820.0
relative error loss 0.12910548
shape of L is 
torch.Size([])
memory (bytes)
4323782656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4323782656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  9% |
error is  175733000.0
relative error loss 0.12907182
shape of L is 
torch.Size([])
memory (bytes)
4326912000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4327014400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% |  9% |
error is  175701890.0
relative error loss 0.12904897
shape of L is 
torch.Size([])
memory (bytes)
4330229760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  9% |
memory (bytes)
4330229760
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  175669500.0
relative error loss 0.12902519
shape of L is 
torch.Size([])
memory (bytes)
4333367296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4333367296
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  175662080.0
relative error loss 0.12901974
time to take a step is 290.8050286769867
it  9 : 1719018496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
shape of L is 
torch.Size([])
memory (bytes)
4336668672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4336668672
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  175662080.0
relative error loss 0.12901974
shape of L is 
torch.Size([])
memory (bytes)
4339826688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4339826688
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  175614720.0
relative error loss 0.12898496
shape of L is 
torch.Size([])
memory (bytes)
4343062528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  8% |
memory (bytes)
4343099392
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  175598720.0
relative error loss 0.1289732
shape of L is 
torch.Size([])
memory (bytes)
4346322944
| ID | GPU | MEM |
------------------
|  0 |  2% |  0% |
|  1 | 11% |  9% |
memory (bytes)
4346322944
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  175577220.0
relative error loss 0.1289574
shape of L is 
torch.Size([])
memory (bytes)
4349497344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% |  9% |
memory (bytes)
4349497344
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  175550080.0
relative error loss 0.12893748
shape of L is 
torch.Size([])
memory (bytes)
4352765952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% |  9% |
memory (bytes)
4352765952
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  175534850.0
relative error loss 0.12892629
shape of L is 
torch.Size([])
memory (bytes)
4355969024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
memory (bytes)
4355969024
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  175500930.0
relative error loss 0.12890138
shape of L is 
torch.Size([])
memory (bytes)
4359208960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4359208960
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  175486720.0
relative error loss 0.12889095
shape of L is 
torch.Size([])
memory (bytes)
4362424320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  9% |
memory (bytes)
4362424320
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  175458940.0
relative error loss 0.12887055
shape of L is 
torch.Size([])
memory (bytes)
4365484032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4365484032
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  175424000.0
relative error loss 0.12884487
time to take a step is 290.4002456665039
it  10 : 1719018496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
shape of L is 
torch.Size([])
memory (bytes)
4368879616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% |  8% |
memory (bytes)
4368879616
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  175424000.0
relative error loss 0.12884487
shape of L is 
torch.Size([])
memory (bytes)
4371984384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4372086784
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  175401340.0
relative error loss 0.12882823
shape of L is 
torch.Size([])
memory (bytes)
4375302144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% |  8% |
memory (bytes)
4375302144
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  175374720.0
relative error loss 0.12880868
shape of L is 
torch.Size([])
memory (bytes)
4378529792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 33% |  9% |
memory (bytes)
4378529792
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  175351040.0
relative error loss 0.12879129
shape of L is 
torch.Size([])
memory (bytes)
4381704192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  9% |
memory (bytes)
4381704192
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  175319170.0
relative error loss 0.12876788
shape of L is 
torch.Size([])
memory (bytes)
4384976896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% |  9% |
memory (bytes)
4384976896
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  175301760.0
relative error loss 0.1287551
shape of L is 
torch.Size([])
memory (bytes)
4388192256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
memory (bytes)
4388192256
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  175248640.0
relative error loss 0.12871608
shape of L is 
torch.Size([])
memory (bytes)
4391305216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  9% |
memory (bytes)
4391407616
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  175227780.0
relative error loss 0.12870075
shape of L is 
torch.Size([])
memory (bytes)
4394622976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4394622976
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  175201660.0
relative error loss 0.12868157
shape of L is 
torch.Size([])
memory (bytes)
4397842432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% |  9% |
memory (bytes)
4397842432
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  175170050.0
relative error loss 0.12865835
time to take a step is 290.54644775390625
it  11 : 1719018496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
shape of L is 
torch.Size([])
memory (bytes)
4401065984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 32% |  8% |
memory (bytes)
4401065984
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  175170050.0
relative error loss 0.12865835
shape of L is 
torch.Size([])
memory (bytes)
4404269056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% |  8% |
memory (bytes)
4404269056
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  175137150.0
relative error loss 0.12863418
shape of L is 
torch.Size([])
memory (bytes)
4407476224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  8% |
memory (bytes)
4407517184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  9% |
error is  175112450.0
relative error loss 0.12861605
shape of L is 
torch.Size([])
memory (bytes)
4410736640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  9% |
memory (bytes)
4410736640
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  175088770.0
relative error loss 0.12859866
shape of L is 
torch.Size([])
memory (bytes)
4413902848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% |  9% |
memory (bytes)
4413902848
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  175056260.0
relative error loss 0.12857477
shape of L is 
torch.Size([])
memory (bytes)
4417171456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 32% |  9% |
memory (bytes)
4417171456
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  175023870.0
relative error loss 0.12855099
shape of L is 
torch.Size([])
memory (bytes)
4420378624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4420378624
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  175004800.0
relative error loss 0.12853698
shape of L is 
torch.Size([])
memory (bytes)
4423593984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4423593984
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  174984700.0
relative error loss 0.12852222
shape of L is 
torch.Size([])
memory (bytes)
4426809344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4426809344
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  174976130.0
relative error loss 0.12851593
shape of L is 
torch.Size([])
memory (bytes)
4430041088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
memory (bytes)
4430041088
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  174950020.0
relative error loss 0.12849675
time to take a step is 292.0327377319336
it  12 : 1719018496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
shape of L is 
torch.Size([])
memory (bytes)
4433256448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4433256448
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  174950020.0
relative error loss 0.12849675
shape of L is 
torch.Size([])
memory (bytes)
4436475904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4436475904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% |  9% |
error is  174939520.0
relative error loss 0.12848903
shape of L is 
torch.Size([])
memory (bytes)
4439601152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4439699456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% |  9% |
error is  174925570.0
relative error loss 0.1284788
shape of L is 
torch.Size([])
memory (bytes)
4442918912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% |  9% |
memory (bytes)
4442918912
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  174902660.0
relative error loss 0.12846196
shape of L is 
torch.Size([])
memory (bytes)
4446035968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% |  9% |
memory (bytes)
4446138368
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  174891900.0
relative error loss 0.12845406
shape of L is 
torch.Size([])
memory (bytes)
4449361920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  9% |
memory (bytes)
4449361920
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  174877440.0
relative error loss 0.12844343
shape of L is 
torch.Size([])
memory (bytes)
4452581376
| ID | GPU | MEM |
------------------
|  0 | 16% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4452581376
| ID | GPU | MEM |
------------------
|  0 | 13% |  0% |
|  1 | 98% |  9% |
error is  174868350.0
relative error loss 0.12843676
shape of L is 
torch.Size([])
memory (bytes)
4455698432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 30% |  9% |
memory (bytes)
4455698432
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  174847230.0
relative error loss 0.12842125
shape of L is 
torch.Size([])
memory (bytes)
4459008000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
memory (bytes)
4459008000
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  174833020.0
relative error loss 0.12841082
shape of L is 
torch.Size([])
memory (bytes)
4462227456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  9% |
memory (bytes)
4462227456
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  174813950.0
relative error loss 0.12839681
time to take a step is 291.3058023452759
it  13 : 1719018496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
shape of L is 
torch.Size([])
memory (bytes)
4465455104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% |  8% |
memory (bytes)
4465455104
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  174813950.0
relative error loss 0.12839681
shape of L is 
torch.Size([])
memory (bytes)
4468666368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4468666368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% |  9% |
error is  174798850.0
relative error loss 0.12838572
shape of L is 
torch.Size([])
memory (bytes)
4471762944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% |  8% |
memory (bytes)
4471885824
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  174786820.0
relative error loss 0.12837687
shape of L is 
torch.Size([])
memory (bytes)
4475105280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
memory (bytes)
4475105280
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  174768260.0
relative error loss 0.12836325
shape of L is 
torch.Size([])
memory (bytes)
4478230528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4478230528
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  174750850.0
relative error loss 0.12835047
shape of L is 
torch.Size([])
memory (bytes)
4481523712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 32% |  9% |
memory (bytes)
4481548288
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  174741250.0
relative error loss 0.1283434
shape of L is 
torch.Size([])
memory (bytes)
4484767744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
memory (bytes)
4484767744
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  174730620.0
relative error loss 0.12833561
shape of L is 
torch.Size([])
memory (bytes)
4487987200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  9% |
memory (bytes)
4487987200
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  174711680.0
relative error loss 0.12832169
shape of L is 
torch.Size([])
memory (bytes)
4491202560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  9% |
memory (bytes)
4491202560
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  174716420.0
relative error loss 0.12832516
shape of L is 
torch.Size([])
memory (bytes)
4494401536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% |  9% |
memory (bytes)
4494401536
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  174699520.0
relative error loss 0.12831277
time to take a step is 291.7764205932617
it  14 : 1719019008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  9% |
shape of L is 
torch.Size([])
memory (bytes)
4497608704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% |  8% |
memory (bytes)
4497645568
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  8% |
error is  174699520.0
relative error loss 0.12831277
shape of L is 
torch.Size([])
memory (bytes)
4500848640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  8% |
memory (bytes)
4500848640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% |  9% |
error is  174680450.0
relative error loss 0.12829876
shape of L is 
torch.Size([])
memory (bytes)
4504039424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4504039424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% |  9% |
error is  174664580.0
relative error loss 0.12828709
shape of L is 
torch.Size([])
memory (bytes)
4507299840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% |  9% |
memory (bytes)
4507299840
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  174642820.0
relative error loss 0.12827112
shape of L is 
torch.Size([])
memory (bytes)
4510412800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4510412800
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  174628740.0
relative error loss 0.12826078
shape of L is 
torch.Size([])
memory (bytes)
4513591296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 32% |  9% |
memory (bytes)
4513751040
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  174615040.0
relative error loss 0.12825072
shape of L is 
torch.Size([])
memory (bytes)
4516966400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4516966400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% |  9% |
error is  174604290.0
relative error loss 0.12824282
shape of L is 
torch.Size([])
memory (bytes)
4520083456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4520185856
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  174595200.0
relative error loss 0.12823614
shape of L is 
torch.Size([])
memory (bytes)
4523413504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% |  9% |
memory (bytes)
4523413504
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  174580100.0
relative error loss 0.12822504
shape of L is 
torch.Size([])
memory (bytes)
4526592000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% |  9% |
memory (bytes)
4526592000
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% |  9% |
error is  174565250.0
relative error loss 0.12821414
time to take a step is 297.3367385864258
sum tnnu_Z after tensor(5331275., device='cuda:0')
shape of features
(6737,)
shape of features
(6737,)
number of orig particles 26947
number of new particles after remove low mass 26947
tnuZ shape should be parts x labs
torch.Size([26947, 702])
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
error without small mass is  215304660.0
relative error without small mass is  0.1581363
nnu_Z shape should be number of particles by maxV
(26947, 702)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
shape of features
(26947,)
Thu Feb 2 09:00:08 EST 2023
