Thu Feb 2 03:05:42 EST 2023
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  0% |  0% |
numbers of X: 43929661
numbers of Z: 31254
shape of features
(31254,)
shape of features
(31254,)
ZX	Vol	Parts	Cubes	Eps
Z	0.023974188170124342	31254	31.254	0.09154039188283526
X	0.021776244045009083	881	0.881	0.2912967832912536
X	0.02316218198742011	23535	23.535	0.0994691535637805
X	0.023861313727981472	4241	4.241	0.17785880066796053
X	0.022017950911681853	2646	2.646	0.20264179312700215
X	0.023114139276527088	68068	68.068	0.0697661748197639
X	0.02215626809071744	45973	45.973	0.07840274270191346
X	0.022119863637637446	77260	77.26	0.06590867494777405
X	0.022276194721462333	54723	54.723	0.07411231327359326
X	0.023564443732521815	11183	11.183	0.12820342345113211
X	0.022576958548426364	28714	28.714	0.0922976364634755
X	0.02211105161132228	8297	8.297	0.13864241117850398
X	0.022927060173108842	108060	108.06	0.05964322740151296
X	0.022957836713267503	8576	8.576	0.13885090991407853
X	0.022429983719745607	267855	267.855	0.04374983306407167
X	0.02215539440987872	42817	42.817	0.08028253616504379
X	0.023017934390343817	58957	58.957	0.07308759260205352
X	0.022347771659793344	80052	80.052	0.06535628323193925
X	0.022056770334918156	69858	69.858	0.06809395997145203
X	0.023846905643427726	195097	195.097	0.04962804271226695
X	0.02328870876364252	149631	149.631	0.053790792628012445
X	0.02195932858179607	12956	12.956	0.11922922086531812
X	0.023078207496556015	353364	353.364	0.04027107817763362
X	0.022244269778522053	18941	18.941	0.10550468613066323
X	0.022746701852507514	17950	17.95	0.10821430156466719
X	0.023362183175256068	90295	90.295	0.06372109655358502
X	0.022781253476345218	81942	81.942	0.06526650427384531
X	0.02374424389354806	53397	53.397	0.07632740468799164
X	0.021978482309327675	37508	37.508	0.08368056376346898
X	0.022070451122659304	41712	41.712	0.080881661756888
X	0.023873610925244444	1817673	1817.673	0.023593957087786912
X	0.021994362592763366	7727	7.727	0.14172076546765588
X	0.023910922392554494	500488	500.488	0.036285592608383155
X	0.0229885435875004	21942	21.942	0.1015652360946926
X	0.022217917988621307	10477	10.477	0.1284760389771542
X	0.022068367964442937	8117	8.117	0.13956983237713835
X	0.02268680339354508	95008	95.008	0.06203989353107273
X	0.023356687492547636	77850	77.85	0.06694490727669328
X	0.021427323471694817	1743	1.743	0.2307934679048279
X	0.02287241323634351	6648	6.648	0.15096409204934363
X	0.021947323835868608	2440	2.44	0.20796835109841955
X	0.02262702301528523	7019	7.019	0.14772388660824273
X	0.021155353256597512	2134	2.134	0.2148202852166738
X	0.02148027792567966	1484	1.484	0.24370737119932312
X	0.021898691441351457	5336	5.336	0.16010348343198985
X	0.021537844956396955	1023	1.023	0.27612699768392845
X	0.022053826538693894	1839	1.839	0.22889382416850435
X	0.022320475086985325	5744	5.744	0.1572158522592377
X	0.02268007547575647	5565	5.565	0.15973243380008145
X	0.021367209197180444	2121	2.121	0.21597445056222433
X	0.022201474768701813	9648	9.648	0.13202256811727026
X	0.02291215015382675	18160	18.16	0.10805627717386516
X	0.02148577064840225	2309	2.309	0.21033376744025675
X	0.0215441893656761	3902	3.902	0.1767452011036029
X	0.021610807708607783	2802	2.802	0.1975760457810524
X	0.022955835464262407	8904	8.904	0.137120582862877
X	0.02217791581028662	3007	3.007	0.19465359730934517
X	0.021714828551015117	3170	3.17	0.18991781897310528
X	0.021703139155647026	4226	4.226	0.17252989344686134
X	0.021946482292218225	3443	3.443	0.18541404648892615
X	0.02176583846098936	2344	2.344	0.2101870386650776
X	0.022975088561217733	9297	9.297	0.1351983696139168
X	0.02209025969999201	2156	2.156	0.21719543266155972
X	0.02371982598094529	13045	13.045	0.12205506463395345
X	0.022101278045038152	7807	7.807	0.14146350131726407
X	0.022072188894367538	2128	2.128	0.21808439636394802
X	0.02200884539024956	2613	2.613	0.2034632371472397
X	0.022067140452340105	2480	2.48	0.2072198998444356
X	0.021536753898468406	5355	5.355	0.15902799741886922
X	0.022031800305667915	4773	4.773	0.166502024085809
X	0.02196023105710213	2033	2.033	0.22105474531752298
X	0.02203270431707369	3620	3.62	0.1825799477885046
X	0.021860829070998223	4914	4.914	0.16446638959583917
X	0.02180687654866317	3685	3.685	0.18087782381450893
X	0.021926127010497936	2049	2.049	0.2203636661464838
X	0.022154433935809823	3371	3.371	0.18731273355566255
X	0.02247381432843406	4989	4.989	0.16515349226635517
X	0.022902744043060697	1632	1.632	0.24120656374050015
X	0.021526611654971117	1074	1.074	0.27163796291460074
X	0.021858680071387287	2850	2.85	0.1972088306234573
X	0.023679029002112384	18400	18.4	0.1087715595637351
X	0.022985410615738784	3955	3.955	0.17979126698085718
X	0.021148538911564867	1326	1.326	0.2517168460633839
X	0.02291734136170465	8043	8.043	0.14176925430768325
X	0.02096080399703516	1556	1.556	0.237939264482379
X	0.02191468478738788	2301	2.301	0.21196925327241967
X	0.022342776845776485	1843	1.843	0.22972271350531656
X	0.022655075280714195	2496	2.496	0.20859648030721992
X	0.021113718246215923	1204	1.204	0.2598041465658648
X	0.022933741870327696	5571	5.571	0.1602681610797707
X	0.021912473049452295	3173	3.173	0.1904322246658315
X	0.022156576203515348	3146	3.146	0.19168199060954222
X	0.021486016697495215	1716	1.716	0.23220926391872468
X	0.021750234057131865	1343	1.343	0.25300511849181123
X	0.021829116315576127	3840	3.84	0.1784712684043269
X	0.022500623867980372	5011	5.011	0.16497699381576086
X	0.022079736845354064	4453	4.453	0.17052199276723756
X	0.022554306974220003	3477	3.477	0.18649832349920353
X	0.022135149879442016	6983	6.983	0.14689718551471193
X	0.022976984888007024	5195	5.195	0.16414808818151316
X	0.021902162412123866	3551	3.551	0.18339131907428266
X	0.023709229751429653	9865	9.865	0.13394919825963486
X	0.02219545879657664	3246	3.246	0.18980393140511892
X	0.02275439743966611	7522	7.522	0.14462521029184183
X	0.021970235420211703	1747	1.747	0.2325487102558469
X	0.02152910777065195	4845	4.845	0.16440338306120708
X	0.02211451018720098	2659	2.659	0.20260632344001747
X	0.02250366641852264	6024	6.024	0.15516345429420206
X	0.021359036303681547	896	0.896	0.2878002817387854
X	0.021924953203205647	2943	2.943	0.19530624832748758
X	0.02205082438717073	1491	1.491	0.2454609415078867
X	0.021794181431852862	2378	2.378	0.20927125104151426
X	0.02154224132930364	1468	1.468	0.24482453806067322
X	0.021108791036431415	1713	1.713	0.23097696256153533
X	0.021730757654465867	5119	5.119	0.1619185401683256
X	0.0221288455445424	5840	5.84	0.1559009051916277
X	0.021355020378519614	1279	1.279	0.25558958250365144
X	0.02193204786607948	2431	2.431	0.20817635912910393
X	0.02201066174438319	1116	1.116	0.27018193126172974
X	0.02301060208655172	9805	9.805	0.13289035496251755
X	0.02151426737505958	1821	1.821	0.22775724732435995
X	0.022134141634096185	11147	11.147	0.12569024163954143
X	0.021754541459843926	4031	4.031	0.1754065116999883
X	0.02297333191289995	2493	2.493	0.209652752983554
X	0.02157057751379515	2491	2.491	0.20535061840937419
X	0.021817845168154812	1755	1.755	0.23165672459519698
X	0.021742272559080813	1985	1.985	0.2220827406052172
X	0.021997871262191007	2185	2.185	0.21592838585901136
X	0.021808108495106917	3334	3.334	0.1870183127486827
X	0.0232118205772263	21021	21.021	0.10335986757391867
X	0.02163023251774184	2630	2.63	0.20185297122459706
X	0.02226206379833924	9866	9.866	0.13116202224422085
X	0.021691831993425583	1721	1.721	0.23272254302516246
X	0.02181894859281356	4504	4.504	0.16920446239153103
X	0.020950738284288114	1236	1.236	0.25687802508267277
X	0.021726121511451337	5106	5.106	0.16204431448274959
X	0.021900357144850162	2606	2.606	0.20331008813400506
X	0.021324640872864477	1260	1.26	0.256746024098531
X	0.02176161551189612	1666	1.666	0.2355078450561717
X	0.021929150836096287	2176	2.176	0.21600027691462148
X	0.022496781076289975	3584	3.584	0.18446647989409454
X	0.022310373248537425	5040	5.04	0.16419456086825326
X	0.021549382620207738	1504	1.504	0.24288218732559536
X	0.023753617412708705	12601	12.601	0.12353070364646399
X	0.02307218855199648	10229	10.229	0.1311450403920368
X	0.02302740408980218	9568	9.568	0.13401125422790558
X	0.021874313733385553	2249	2.249	0.21345925716711767
X	0.022945266074851055	2501	2.501	0.2093436548829138
X	0.022394437960994034	3852	3.852	0.17981170659758214
X	0.021771493170047302	2158	2.158	0.21607884930476634
X	0.022348179321948207	2427	2.427	0.20959976451560006
X	0.022174417382684157	2683	2.683	0.20218263318135749
X	0.02270221070644899	4434	4.434	0.17235510597748996
X	0.021948365069532152	4436	4.436	0.17040024411145582
X	0.02151975786440432	8190	8.19	0.13799112634268756
X	0.021718511259293163	2824	2.824	0.1973884726807403
X	0.023892174440503105	18065	18.065	0.10976719600946747
X	0.022018336043908304	3310	3.31	0.18806970093639586
X	0.021986065748623895	2580	2.58	0.2042565249003885
X	0.023137764403612565	4399	4.399	0.17390916806672113
X	0.021453294333008334	692	0.692	0.3141443830819159
X	0.02357650622948728	55872	55.872	0.07500583959185839
X	0.021531515064346654	1761	1.761	0.2303766248635836
X	0.022822175930982333	5478	5.478	0.16090826065413924
X	0.02162890666565382	1322	1.322	0.25386395231073067
X	0.0216931398697574	2990	2.99	0.19359036680468275
X	0.02126105079212235	1640	1.64	0.2349165562264226
X	0.02197441015222261	3273	3.273	0.1886501147407789
X	0.021796649328892918	3332	3.332	0.18702295507089503
X	0.022381198718395605	5464	5.464	0.16000156019326156
X	0.021257176540023865	773	0.773	0.3018390058691986
X	0.021894512203425576	2021	2.021	0.22127022973714588
X	0.021025630413220933	1341	1.341	0.25028803158617746
X	0.021987260327948538	7221	7.221	0.14494100149434105
X	0.02146516569124227	2604	2.604	0.20200607101246057
X	0.02294421737311139	6696	6.696	0.15075993256128684
X	0.022970519105714085	10049	10.049	0.13172938205201848
X	0.022100119698570456	5816	5.816	0.15604747389951662
X	0.022037372781215127	2530	2.53	0.20575312134915758
X	0.021800327491387685	5537	5.537	0.15790510280840905
X	0.021322557109416083	3306	3.306	0.18614247281585722
X	0.022827030828428158	4033	4.033	0.17821341857013256
X	0.02174400702013559	2269	2.269	0.21240677314226253
X	0.021934217915455984	3380	3.38	0.18652418505779256
X	0.021865838855688033	4058	4.058	0.17531457914644058
X	0.02282358941510281	9477	9.477	0.13404100903210822
X	0.02265468363006861	5368	5.368	0.1616026574460918
X	0.02188660634462753	2907	2.907	0.19599476203457475
X	0.022391221443386043	4872	4.872	0.16626091584280592
X	0.02266607660207629	10622	10.622	0.12874291899795923
X	0.023295337397392975	16691	16.691	0.111753778492809
X	0.021289671044751587	2193	2.193	0.21332582790769378
X	0.021173164920310527	668	0.668	0.3164728362746434
X	0.02194980897035596	3510	3.51	0.18423601987479513
X	0.021676804837792163	2491	2.491	0.20568715917288113
X	0.021148857700029732	2528	2.528	0.20300338600958295
X	0.02247210910886135	4082	4.082	0.1765727217738133
X	0.02205063367342716	2295	2.295	0.21259161353706044
X	0.021603906660036695	1288	1.288	0.2559796709145404
X	0.02118484082391134	1814	1.814	0.2268798749640483
X	0.021631506842916433	1414	1.414	0.24824420622229795
X	0.023006507946329642	6293	6.293	0.1540509298683687
X	0.02153187697013054	1621	1.621	0.23682795139414917
X	0.022914517060376553	9153	9.153	0.13578415128433355
X	0.02269318841063765	4347	4.347	0.173474359591745
X	0.02196905696200383	3603	3.603	0.1826903961778285
X	0.02150633039778636	894	0.894	0.28867528166620465
X	0.02348515670714012	6617	6.617	0.1525377874147461
X	0.02288129343420436	4316	4.316	0.17436783618106874
X	0.021859584384188193	3212	3.212	0.18950563191395342
X	0.023395664993242662	10250	10.25	0.13166505034463258
X	0.022050374943653254	7062	7.062	0.14616029866581345
X	0.02193447349350645	3573	3.573	0.1831040987503805
X	0.021684499359957865	3186	3.186	0.18951105215187197
X	0.021965011646091187	2029	2.029	0.2212159635340891
X	0.021775217676704584	1468	1.468	0.2457039564990795
X	0.02117856612165015	1608	1.608	0.2361584571885985
X	0.021571855238659402	1246	1.246	0.258696062345425
X	0.021017921410716	922	0.922	0.28354344669688586
X	0.0220074831929214	4433	4.433	0.1705915633382165
X	0.021575205094938342	2879	2.879	0.19569110669326834
X	0.022110213124631708	2438	2.438	0.20853857866089112
X	0.021827870223351097	1612	1.612	0.23835015674001864
X	0.021714618576957757	4547	4.547	0.16840012790390116
X	0.021638735394346	2365	2.365	0.209154354289505
X	0.022038216145638098	5304	5.304	0.16076480359214454
X	0.022726254867713052	6170	6.17	0.15443565330099698
X	0.021471352455030245	2050	2.05	0.2187938850527847
X	0.02192912882424552	1383	1.383	0.25122705049180394
X	0.02162238082226861	7041	7.041	0.14535268409905547
X	0.020629589137822747	724	0.724	0.30543397024130037
X	0.021973513297809864	2079	2.079	0.21945644371103765
X	0.02204392905042196	2313	2.313	0.21201721426294048
X	0.022741876261225296	3275	3.275	0.19078240943518446
X	0.023204032831688714	6967	6.967	0.14933876434661184
X	0.021589366999758	2934	2.934	0.19450313238447464
X	0.02155493970987133	1672	1.672	0.23447876235937973
X	0.02186973887324207	3539	3.539	0.1835077223796618
X	0.023429131535223968	7779	7.779	0.14441450180153953
X	0.021279133902211624	3096	3.096	0.19013015334836392
X	0.022296764586578605	9358	9.358	0.1335630319546129
X	0.02219160162833557	70924	70.924	0.06788885366670117
X	0.02217639787629373	9699	9.699	0.13174111915861775
X	0.021997549070936297	5787	5.787	0.15606551060670723
X	0.021210721595465873	1943	1.943	0.22183371263462726
X	0.023047734221970505	18028	18.028	0.10853260554633354
X	0.022715842401897203	24250	24.25	0.09784508946513014
X	0.0221965500019087	210620	210.62	0.04723478646412669
X	0.022319027685279533	4513	4.513	0.17037399712720613
X	0.02240130323195131	83218	83.218	0.06456819862186756
X	0.022090657344287258	79423	79.423	0.06527608770592953
X	0.022254242495976555	35522	35.522	0.08556676940106134
X	0.023677236937456235	222001	222.001	0.04742332820269893
X	0.021612630070791415	1824	1.824	0.22797869430077114
X	0.02146064100940219	15815	15.815	0.11071109966249494
X	0.02228118542806837	155439	155.439	0.052335064824213844
X	0.02243340194096877	462818	462.818	0.036460976754931776
X	0.021622761136610616	4554	4.554	0.16807613067013533
X	0.023235613667734776	35462	35.462	0.08685542982349233
X	0.023414350552554836	80192	80.192	0.06634125613768349
X	0.02309677210115924	290613	290.613	0.04299441892782514
X	0.022078829003754685	119277	119.277	0.05699102170576235
X	0.02291416694951849	142667	142.667	0.05435758846483935
X	0.02222189095718784	38795	38.795	0.08304912962989194
X	0.022631053257821485	19230	19.23	0.10557842058026513
X	0.022116296848463217	7814	7.814	0.14145327313494488
X	0.023810848211662718	203039	203.039	0.048947644847972385
X	0.021918842702293825	5282	5.282	0.1606964962487849
X	0.02214529602230699	5430	5.43	0.15976950394908584
X	0.021925780104348376	74823	74.823	0.06642118901681807
X	0.021663183760134815	34471	34.471	0.0856556734036632
X	0.02318773869119858	440139	440.139	0.03748776208726447
X	0.022211094943588992	102238	102.238	0.06011541844851615
X	0.0218931668619001	1249	1.249	0.25976584606279207
X	0.02218790229104845	18308	18.308	0.1066166699358026
X	0.022874639822033575	10469	10.469	0.12976264466890583
X	0.022749855030430914	23409	23.409	0.09905245804921647
X	0.022352021881452546	88859	88.859	0.06312554356862503
X	0.02237828154893266	6088	6.088	0.15433012096794832
X	0.022249744030212838	54598	54.598	0.07413946167238218
X	0.02155403481566131	1234	1.234	0.25946043394737944
X	0.022136747836030462	6480	6.48	0.15060738596247555
X	0.02313668856675146	63751	63.751	0.07132986461912921
X	0.022292986292196244	73004	73.004	0.0673401166228403
X	0.022477268462431466	45297	45.297	0.07916952348375089
X	0.021708038614849805	6460	6.46	0.14978305667832492
X	0.0236211042033313	269805	269.805	0.04440345196981782
X	0.022202313860539686	12988	12.988	0.11956901527847746
X	0.022853509081648557	22637	22.637	0.10031780192766913
X	0.022847656817617818	104363	104.363	0.060269608543299796
X	0.021574598341618612	4010	4.01	0.17522636380718906
X	0.02229113535968155	81261	81.261	0.0649755566573313
X	0.023529383813813948	234937	234.937	0.046439376307508454
X	0.02360526701673693	271117	271.117	0.04432179993714727
X	0.023023273982005415	17825	17.825	0.10890450559109982
X	0.021944059967952546	27680	27.68	0.09255153623662667
X	0.021988570312656483	14670	14.67	0.11444292773216484
X	0.022299737233440257	5607	5.607	0.15843692194759662
X	0.02267436585838511	62600	62.6	0.07128315918598178
X	0.022901124535213525	14130	14.13	0.11746402607629232
X	0.022339694512524072	44363	44.363	0.07955829017858458
X	0.02266253240456142	56632	56.632	0.0736911803540477
X	0.02240479517554067	58103	58.103	0.0727859364230317
X	0.022097257419900727	11528	11.528	0.12422093745251571
X	0.02297898273230267	12139	12.139	0.12370368275927936
X	0.023014781048662784	108383	108.383	0.05965981374914385
X	0.022342316566032753	36758	36.758	0.08470819789125697
X	0.022350970464239225	7155	7.155	0.14618256641390268
X	0.023580304858231267	17395	17.395	0.11067299448422756
X	0.023908044577088863	364373	364.373	0.040333504272086616
X	0.021907191610897093	14205	14.205	0.11553542234097293
X	0.02217194437894908	120915	120.915	0.056812146735010914
X	0.02213243368670293	5280	5.28	0.16123713886470498
X	0.022825210646646284	30149	30.149	0.09114103598020064
X	0.022907253733252652	16139	16.139	0.11238252914681517
X	0.02355551787737292	101843	101.843	0.06138380765023575
X	0.022198234356841297	50122	50.122	0.07622486782899877
X	0.02162477121245496	6596	6.596	0.14855603226916
X	0.023165062362640234	145188	145.188	0.05423765658777047
X	0.02308403473574774	118827	118.827	0.05791604036698981
X	0.022020220247452413	38546	38.546	0.08297503554839998
X	0.022538218576098046	90500	90.5	0.06291540583921285
X	0.023013652292860516	125461	125.461	0.056818827496610075
X	0.02384379909588009	271649	271.649	0.04444154149355492
X	0.023103991273257404	85608	85.608	0.06462356119959867
X	0.021702459336366334	2164	2.164	0.2156505474135733
X	0.021895152106182855	10158	10.158	0.12917543103050702
X	0.022362129652728487	19831	19.831	0.10408532809774158
X	0.0220183353090799	37611	37.611	0.08365460768762135
X	0.02211735402173373	9652	9.652	0.13183739671071368
X	0.021815411806408985	2197	2.197	0.21493696864614634
X	0.023211583685104446	71448	71.448	0.06874454345524558
X	0.022252756499811115	146277	146.277	0.053382952532096614
X	0.022167987321930645	144903	144.903	0.05348306598698587
X	0.021736869373520692	6341	6.341	0.15078092876551313
X	0.022311395368709158	23880	23.88	0.09776066262603013
X	0.023022103178250958	15467	15.467	0.11417726528879886
X	0.022836127632447866	7656	7.656	0.14394840119533336
X	0.022960336806478868	10583	10.583	0.12945628882838414
X	0.022020589039601825	5266	5.266	0.1611075991312786
X	0.023579820059740818	224215	224.215	0.04720180513976989
X	0.02225579528095399	42979	42.979	0.08030247615723504
X	0.02290035356014068	16907	16.907	0.11064332324358453
X	0.02293219900202265	22736	22.736	0.10028682460931929
X	0.022211163438343572	8810	8.81	0.1361021961778892
X	0.02354570139583262	738419	738.419	0.03171048781965381
X	0.022359232564774397	7385	7.385	0.14466677427902644
X	0.023084207827861573	129627	129.627	0.05626087753433181
X	0.021677252130344275	1979	1.979	0.2220851287134155
X	0.02236867865068406	4659	4.659	0.16870030716438905
X	0.022244015335467427	3543	3.543	0.18447917360918928
X	0.02225212229743412	17469	17.469	0.10840129349586115
X	0.023112153464878216	26135	26.135	0.09598557467077891
X	0.023028707617880895	32446	32.446	0.08920094746203164
X	0.02269686089159883	6447	6.447	0.15212574165803117
X	0.022613905961935795	5741	5.741	0.1579292883288566
X	0.022225005248504334	238464	238.464	0.045339112625390254
X	0.022654176754737836	37479	37.479	0.0845512624797022
X	0.023821260946212657	27795	27.795	0.09498740490318834
X	0.023274817485734636	68536	68.536	0.06976793090684912
X	0.02380675789341057	111500	111.5	0.05976888159481246
X	0.02380242651011447	7172	7.172	0.1491627279612954
X	0.021968836471990192	5058	5.058	0.16315838970074686
X	0.022125199720261727	16338	16.338	0.11063590752911677
X	0.021518656306938538	1304	1.304	0.25459264472628507
X	0.022889460287609624	6802	6.802	0.14985329899679092
X	0.02242916705836451	26748	26.748	0.09429904121078865
X	0.02168024313912255	3391	3.391	0.18560034801128952
X	0.02212396527029006	5388	5.388	0.16013212257825604
X	0.021678483787733356	7057	7.057	0.14536826113160353
X	0.022014119599329945	5778	5.778	0.1561856974815479
X	0.023315977422318623	378924	378.924	0.039479093402841815
X	0.022052653884814723	38675	38.675	0.08292335144649604
X	0.02272501765888427	84541	84.541	0.06453752152734742
X	0.02167360535392003	24428	24.428	0.09609063941058663
X	0.022485571849097323	5704	5.704	0.15797006858421478
X	0.02243870246941684	28974	28.974	0.09183250435250766
X	0.022996897887853827	533323	533.323	0.035066545231914346
X	0.023189095567224774	646944	646.944	0.032971525434168016
X	0.022288379906073453	50567	50.567	0.07610334938564357
X	0.02305802276012305	51257	51.257	0.07662249565571809
X	0.021999600659924345	2512	2.512	0.20612549937902278
X	0.022596836188773384	96692	96.692	0.06159597933974458
X	0.02307887041920304	126457	126.457	0.056722743051699956
X	0.023897676791055297	191381	191.381	0.04998261277308286
X	0.02223743815619113	7993	7.993	0.14064522109618602
X	0.022117876694741672	111906	111.906	0.058250101533688474
X	0.02373021944243222	317000	317.0	0.04214513966331446
X	0.021988216741372923	80932	80.932	0.06476741723267086
X	0.021495683355491874	11935	11.935	0.12166797435688068
X	0.02384948399178329	28453	28.453	0.0942866721843058
X	0.02300150695907057	13183	13.183	0.12038730137161947
X	0.022102031381114936	4139	4.139	0.174788258351811
X	0.02307338827597724	140217	140.217	0.05479869731459246
X	0.02228484406331987	26433	26.433	0.09446865116884237
X	0.021674672812297523	2068	2.068	0.21884367808345756
X	0.022334047300897008	42704	42.704	0.0805686880314566
X	0.023659775362988036	20445	20.445	0.1049883637678558
X	0.021763863374091183	1636	1.636	0.23694682690821853
X	0.02253145479161278	109878	109.878	0.058969325823620855
X	0.02228381063850617	123027	123.027	0.05658000851235676
X	0.02326294338842076	54493	54.493	0.07529647560329904
X	0.022847780706651312	128507	128.507	0.056230560459951
X	0.022121807031322133	106562	106.562	0.05921150214635186
X	0.022041240742030496	20382	20.382	0.1026430969126479
X	0.022581204960440263	53422	53.422	0.07504855302342366
X	0.02315410720841904	104162	104.162	0.060576788207616604
X	0.02263026877630039	153106	153.106	0.052872823189220604
X	0.022401049942041246	8587	8.587	0.13766038203173295
X	0.023214259044094857	89923	89.923	0.06367388464946665
X	0.023527372268344244	211785	211.785	0.04807206102647145
X	0.022300486787384926	62779	62.779	0.07082174933519494
X	0.022308546287891758	34975	34.975	0.08608044746880607
X	0.022627505914944713	34037	34.037	0.08727606106400917
X	0.022281938781138336	13307	13.307	0.11874744479798999
X	0.021986614855075602	6686	6.686	0.1487067442971036
X	0.022076931230650715	29789	29.789	0.09049562701373688
X	0.022949261613616836	185635	185.635	0.04981608779885762
X	0.022423075075036696	121189	121.189	0.0569828267762474
X	0.02390850789741901	399926	399.926	0.03910127730786722
X	0.023949238496096124	137733	137.733	0.05581503918040614
X	0.02287757560115492	84552	84.552	0.06467881256241825
X	0.0219082594168991	17404	17.404	0.10797410181772467
X	0.02319826107875367	126135	126.135	0.05686869644869147
X	0.02188577897451002	2511	2.511	0.20579671213734885
X	0.021923485076276443	11579	11.579	0.12371229009362811
X	0.023086815513473913	252392	252.392	0.04505705641796944
X	0.02221669527212259	27190	27.19	0.09348818970002265
X	0.021716740725549764	4238	4.238	0.17240289913489124
X	0.022219471827037294	16062	16.062	0.1114238182129866
X	0.023001491624557495	86616	86.616	0.0642765599639183
X	0.02199053101404282	22641	22.641	0.0990330232553515
X	0.0237229246489819	102955	102.955	0.06130655730453004
X	0.021713476832263757	11842	11.842	0.12239624782722516
X	0.02167886210192	17266	17.266	0.1078818160836239
X	0.022684727547124382	4013	4.013	0.17813727420846795
X	0.023384895410189704	4884	4.884	0.16854657258893885
X	0.023301268350620175	45850	45.85	0.0798019322642454
X	0.02279748480326145	62088	62.088	0.07160770497410275
X	0.022068185251771986	11331	11.331	0.124881889319959
X	0.022208456354624845	145350	145.35	0.05346067595340228
X	0.02189154199904938	3411	3.411	0.18583672560711342
X	0.023081637475344623	34535	34.535	0.0874317259660618
X	0.021892512357351825	6100	6.1	0.1531047234969232
X	0.022345553856938243	100589	100.589	0.060563859642477354
X	0.022784188335719845	5072	5.072	0.1650003146257318
X	0.022078549396079283	66988	66.988	0.0690755748968776
X	0.022394299519133964	32351	32.351	0.08846060887211489
X	0.023732405295600695	65380	65.38	0.07133438916671021
X	0.023394933627234334	46613	46.613	0.0794703125081559
X	0.022468853424566764	403642	403.642	0.03818225032587854
X	0.023450258984610783	8829	8.829	0.13848796774027247
X	0.021997975596356377	11634	11.634	0.123656766368785
X	0.022773323552069683	92251	92.251	0.06273141896812592
X	0.02208964863207292	9980	9.98	0.13032250589813113
X	0.023529432458204762	497217	497.217	0.03617055403307056
X	0.021876259628680665	3341	3.341	0.18708208343599317
X	0.0226209760127534	119910	119.91	0.05735245049980015
X	0.02310798297418462	297798	297.798	0.04265272219420639
X	0.021987067858632144	3262	3.262	0.18889818569707326
X	0.021969315884254586	228924	228.924	0.045783438068674034
X	0.022133916241721716	28088	28.088	0.09236607346476146
X	0.023138555651605948	270374	270.374	0.04406804388920862
X	0.02206021877712125	8883	8.883	0.13542017925772262
X	0.02204696327439723	57830	57.83	0.07251010412613261
X	0.021901925444675258	2261	2.261	0.21317056910348336
X	0.02313874351089215	7185	7.185	0.1476741001550621
X	0.021906665038825986	5554	5.554	0.15799989467679884
X	0.022233350857070407	38679	38.679	0.08314635745506058
X	0.022385516536122984	23171	23.171	0.09885700472196958
X	0.023079263230462832	101601	101.601	0.061015660842887884
X	0.022125615730012586	16080	16.08	0.11122517737617248
X	0.02223400624205264	4880	4.88	0.1657801902693538
X	0.021861638980366062	6700	6.7	0.14832099781535568
X	0.023851893001389776	83835	83.835	0.06577070461854155
X	0.022860944209616	19298	19.298	0.10581013956839333
X	0.023873980635554827	209614	209.614	0.04847318060631221
X	0.023348274561915697	109488	109.488	0.059744246713280255
X	0.022126661569391668	10120	10.12	0.1297911638867333
X	0.02205504610142492	15091	15.091	0.11348278454414783
X	0.021823656778820293	13556	13.556	0.11720118089561567
X	0.023829738839926853	951807	951.807	0.02925432820418236
X	0.022880270859706772	9494	9.494	0.13407176024832246
X	0.02213450784758528	19167	19.167	0.10491523934514589
X	0.022981824627514715	101782	101.782	0.06089353342445221
X	0.022015839374854077	30632	30.632	0.0895749617844352
X	0.021890436230031736	4159	4.159	0.17394895816003939
X	0.02237774833054224	4871	4.871	0.16623893651737423
X	0.023765379271461662	275353	275.353	0.04419281291333656
X	0.022618702313399634	34912	34.912	0.08652952479319509
X	0.021695507258261096	48681	48.681	0.0763841832604738
X	0.02239478779330464	85155	85.155	0.06406865389717856
X	0.023910242977247254	57162	57.162	0.07478689879671756
X	0.02306362857325593	75166	75.166	0.06744794651548024
X	0.023405523337838602	197252	197.252	0.04913967882500326
X	0.02273839133214737	94208	94.208	0.06226213147245924
X	0.02201350523707799	7884	7.884	0.14081454900216886
X	0.02252956231911999	59687	59.687	0.07226993564386108
X	0.022256889738547246	8813	8.813	0.13618007469625626
X	0.02207419194426189	14040	14.04	0.11628023539352365
X	0.022179984887123035	26536	26.536	0.09419805360069217
X	0.022887507376559025	8559	8.559	0.138800753461359
X	0.02203772169185996	72452	72.452	0.06725203627825747
X	0.022601032255158783	24218	24.218	0.09772297205911475
X	0.02105869516602034	3644	3.644	0.17945312680252173
X	0.023040298940232778	21729	21.729	0.10197243498887312
X	0.022211518156560663	5165	5.165	0.16261825852606301
X	0.02365213876142579	134881	134.881	0.05597230779826549
X	0.02218530180530619	41448	41.448	0.08119337369300894
X	0.022498647948804375	14948	14.948	0.11460171327286306
X	0.02167148758126618	18735	18.735	0.10497317716819544
X	0.0228250046607335	27354	27.354	0.09414487376215216
X	0.022365125735244067	4280	4.28	0.173530516073041
X	0.022144402130665375	9532	9.532	0.13244227797714803
X	0.02224807617696065	28845	28.845	0.09170801778497621
X	0.02283356917421322	143435	143.435	0.054196705590392885
X	0.023864755173971002	10014	10.014	0.1335723188266935
X	0.022159211016730605	1765	1.765	0.23241805430426368
X	0.022126679051416222	4842	4.842	0.16594486460705343
X	0.022115551293411636	242189	242.189	0.04503129660408135
X	0.02375564091019784	295721	295.721	0.04314808529579638
X	0.021696066827214568	48203	48.203	0.0766364973538144
X	0.023165823908305124	8748	8.748	0.13835015779440382
X	0.023135757672378736	79860	79.86	0.06616852804550578
X	0.023956171349621837	143562	143.562	0.055054476442019086
X	0.0228610606701287	3986	3.986	0.17899999847611386
X	0.02195812520115561	9138	9.138	0.13394135900770668
X	0.02225404081995388	33334	33.334	0.08739913809075395
X	0.023833193322928107	134924	134.924	0.0561088037134277
X	0.022948611196615197	127487	127.487	0.05646294185416216
X	0.02318854841784506	128216	128.216	0.05655145453593792
X	0.022926094285319547	21935	21.935	0.101483977059874
X	0.022150445007954777	26488	26.488	0.09421305702213933
X	0.02262200354065992	3298	3.298	0.1900028600262031
X	0.022287351271917024	38597	38.597	0.08327250629772191
X	0.02276996491985359	22442	22.442	0.10048477579413162
X	0.022095044879025368	51285	51.285	0.07552687390302375
X	0.02231274534144413	36801	36.801	0.08463781885784225
X	0.023841046143686834	316047	316.047	0.042253032010346615
X	0.02300435793486468	12988	12.988	0.12099180213823191
X	0.022458443410699108	13849	13.849	0.11748626001282454
X	0.022216188476561323	16224	16.224	0.11104624336506415
X	0.02352079521467844	492337	492.337	0.036285226794252494
X	0.022188908776102974	43421	43.421	0.07994881686687451
X	0.021917474245609768	15966	15.966	0.11113856714066446
X	0.022022430082783127	2792	2.792	0.199059667954848
X	0.02220237697204713	51186	51.186	0.07569771322431676
X	0.022162802065114947	2751	2.751	0.2004678314635712
X	0.02302805108602959	35196	35.196	0.08681363846309993
X	0.02287499765193697	5244	5.244	0.16339275355037977
X	0.022353443734680054	14666	14.666	0.11508293241084024
X	0.022609638305511708	14107	14.107	0.11702706579708752
X	0.021912244831270383	5065	5.065	0.16294303727890683
X	0.021583332204898883	3351	3.351	0.186057860434975
X	0.022455573983275412	181277	181.277	0.04984946308992304
X	0.022639039094160145	371705	371.705	0.03934472632603878
X	0.023169037401628498	149093	149.093	0.053763010457165104
X	0.02200745958903548	25992	25.992	0.09460412463538492
X	0.022728133370469653	27695	27.695	0.09362405918628301
X	0.021957481001836504	12828	12.828	0.11962111737039735
X	0.022856960927548006	12228	12.228	0.12318401408632391
X	0.02235214043048218	116498	116.498	0.057676675011754684
X	0.0238101610735758	55045	55.045	0.07562780326449174
X	0.022636228608017846	58811	58.811	0.07274143021801002
X	0.021991340044624493	10985	10.985	0.126032884948286
X	0.023941630756195254	484205	484.205	0.036703556870904916
X	0.022148846268059557	23153	23.153	0.09853290316700906
X	0.023809719610725352	48261	48.261	0.0790167788483768
X	0.02390681771431734	126955	126.955	0.05731795054850347
X	0.023532074653192788	57596	57.596	0.07420319947422159
X	0.02320178230096464	147362	147.362	0.053998116741285004
X	0.023372872490204713	223714	223.714	0.04709841682467324
X	0.023172170894180975	100205	100.205	0.06137984350208267
X	0.022214976052774518	36745	36.745	0.08455692854652254
X	0.021632330803944597	30664	30.664	0.0890208021096779
X	0.022192536225724282	57214	57.214	0.07292921551062206
X	0.021976580333235685	8417	8.417	0.13770010219946452
X	0.02236023402751275	127770	127.77	0.055934850710489904
X	0.02396739657563457	713533	713.533	0.03226529197639626
X	0.02383319300911745	159843	159.843	0.05302689561407942
X	0.023117109353467592	94823	94.823	0.06247025499479236
X	0.021922628168663354	28601	28.601	0.0915174266743276
X	0.022343462672169134	23157	23.157	0.09881496648312557
X	0.023720377738174953	30979	30.979	0.09148531800302777
X	0.02193059150400347	23333	23.333	0.09795499530508861
X	0.02322323471131107	63736	63.736	0.07142429611538949
X	0.023444062924795714	300597	300.597	0.0427250663147722
X	0.021772570098223535	10862	10.862	0.1260859320009222
X	0.02221326960578386	99321	99.321	0.06070024836492066
X	0.023184159675303663	120709	120.709	0.05769664618730414
X	0.02222489805090989	38976	38.976	0.0829241134157307
X	0.021907064744732557	3124	3.124	0.1914069584616892
X	0.02291590105483603	113056	113.056	0.058741870865122385
X	0.02232847932108241	138941	138.941	0.05436794385915434
X	0.021587203966626986	3905	3.905	0.17681744832508978
X	0.023895920115237272	752142	752.142	0.031671938034128766
X	0.023046751067882638	32163	32.163	0.08948516709908098
X	0.0229730215690737	30866	30.866	0.09062456246835016
X	0.022096788514085683	14554	14.554	0.11493410796878328
X	0.022083762123494284	12069	12.069	0.12231149026483346
X	0.022066499160861373	4303	4.303	0.17244635957913423
X	0.022029755854855684	6967	6.967	0.1467758568957044
X	0.022377432496141672	19160	19.16	0.10531047791216143
X	0.022995669241767613	121316	121.316	0.05744373327112164
X	0.023906379998647857	1747004	1747.004	0.023918832252591043
X	0.022498110154171417	10748	10.748	0.12792029737890107
X	0.02237335997630734	100538	100.538	0.060599213772180825
X	0.02221312370152184	4801	4.801	0.16663234637262023
X	0.022709466661470903	23281	23.281	0.09917489925788647
X	0.022154152762410766	7148	7.148	0.14579977957447723
X	0.022681856473996457	177988	177.988	0.050322548254603965
X	0.02194557333454315	28319	28.319	0.09185222323436232
X	0.023653592663986353	1005381	1005.381	0.028654234672493277
X	0.02298101614947892	8137	8.137	0.14135189571114556
X	0.02322439158857599	148520	148.52	0.05387489891872375
X	0.022635838687229445	28431	28.431	0.09268329925012969
X	0.022897772665408888	153179	153.179	0.05307190414264266
X	0.023073129124535494	260702	260.702	0.04456432962456525
X	0.022154193017384097	24021	24.021	0.09733931461772466
X	0.02210592272579864	8265	8.265	0.13881037576804983
X	0.02313684414629541	81307	81.307	0.06577467759488417
X	0.021994713050662775	10947	10.947	0.12618499866136046
X	0.02295992092789791	47253	47.253	0.0786164931422813
X	0.02206159339921845	134213	134.213	0.05477899968494251
X	0.021988217914943393	19720	19.72	0.10369576994423298
X	0.023921398152263244	72452	72.452	0.06911602086236315
X	0.02207409105504093	32027	32.027	0.08833283923632206
X	0.02317763685711331	133542	133.542	0.0557806742259403
X	0.022740563348365602	331311	331.311	0.04094382898944773
X	0.02227222528312642	49069	49.069	0.0768514621197513
X	0.022681628622530425	12741	12.741	0.12119654512585466
X	0.0222374206060379	165160	165.16	0.051253852190479565
X	0.021838485721523946	50178	50.178	0.0757826389819776
X	0.022918672635895455	78298	78.298	0.06639666472060604
X	0.022366600485331362	50864	50.864	0.07604368542654041
X	0.02343802867313044	74534	74.534	0.06800208322360027
X	0.02289383153535277	45483	45.483	0.07954687567814091
X	0.021657240574295297	104361	104.361	0.059204533433401714
X	0.023142961134442867	47192	47.192	0.07885880306005935
X	0.02229534617778688	23325	23.325	0.09850634136503632
X	0.022248708731274117	5788	5.788	0.1566482073796709
X	0.02389930609150904	19070	19.07	0.10781475479130405
X	0.022289211463728257	64659	64.659	0.07011677085381282
X	0.02206473209780936	86413	86.413	0.06344146335940248
X	0.022203996711611315	109152	109.152	0.058812072717373466
X	0.021968417428255848	8609	8.609	0.13665179940363784
X	0.02217331699811897	35623	35.623	0.08538207892197623
X	0.023604035515747357	60072	60.072	0.07324388791654073
X	0.022879222446598735	260645	260.645	0.044442377861201164
X	0.023211470721625944	48986	48.986	0.07796090889376867
X	0.022275853188287904	67493	67.493	0.06910750356117891
X	0.02233449506871565	32935	32.935	0.08785628265178025
X	0.02214935781410984	23977	23.977	0.09739173407701969
X	0.023006181054787904	2667	2.667	0.2050881208745333
X	0.022849226051055688	10159	10.159	0.13102077921578578
X	0.022785457029971666	167034	167.034	0.05147752057967958
X	0.02268615172256192	51044	51.044	0.07631420378543433
X	0.02271043356737531	70731	70.731	0.06847602509453402
X	0.02208722397235101	1811	1.811	0.2301835206302064
X	0.022060830972316908	5414	5.414	0.15972315194765888
X	0.02209388754804945	205129	205.129	0.047578968834365874
X	0.022073556312721825	102481	102.481	0.05994362252163227
X	0.022911929632452845	45772	45.772	0.07940001647057279
X	0.02184644786693988	3978	3.978	0.17642983094600911
X	0.02185496908850551	3376	3.376	0.18637282414049414
X	0.02223632098130398	37288	37.288	0.08417141242427043
X	0.02344093929914934	52537	52.537	0.07641346853510259
X	0.02282590498326372	98550	98.55	0.061412549260012855
X	0.022167357744469145	4415	4.415	0.17123573703486486
X	0.022230925174618336	30416	30.416	0.0900779469334157
X	0.02173793430337321	14799	14.799	0.11367421590460232
X	0.02206200843430074	18860	18.86	0.10536615187960276
X	0.023809639018276353	348352	348.352	0.04088633357822465
X	0.023688125690084386	15075	15.075	0.11625843148423069
X	0.022431057233692746	65126	65.126	0.07009684096859572
X	0.022369625019387662	136497	136.497	0.05472408799872177
X	0.022999135063339855	52829	52.829	0.07579019734263027
X	0.023917913532256238	70883	70.883	0.06961888559513692
X	0.0229842910768747	133501	133.501	0.055630828952083995
X	0.0218012159630428	2621	2.621	0.20261483116942586
X	0.022227881473874302	42199	42.199	0.08076043278500376
X	0.022242369983896712	30685	30.685	0.08982936113516969
X	0.022570907053500226	167046	167.046	0.05131420961075305
X	0.022360125274741537	136162	136.162	0.05476117651815059
X	0.021854951095507433	52564	52.564	0.07463693743929217
X	0.023110423092519736	60035	60.035	0.07274466146656394
X	0.02201781286689825	13665	13.665	0.11723434106229043
X	0.0220556699625291	10212	10.212	0.12926168595246265
X	0.02168488849957442	15750	15.75	0.1112479999285803
X	0.022792424401823712	11611	11.611	0.12521037011912492
X	0.023404273667143358	197149	197.149	0.049147360243884025
X	0.023897772061855484	190994	190.994	0.050016415413439194
X	0.022589776186797573	7605	7.605	0.1437488028563515
X	0.022391421450006888	20963	20.963	0.10222161622986493
X	0.0220609537063819	13079	13.079	0.11903734938477228
X	0.02385558717727505	450377	450.377	0.037555305772117624
X	0.022369953220958288	23179	23.179	0.09882271777572438
X	0.021980666991501314	21247	21.247	0.10113801281987672
X	0.023186254092177144	91768	91.768	0.06321882815805345
X	0.0232171584091198	91036	91.036	0.0634159689386585
X	0.021759855620998973	11687	11.687	0.12302243584365004
X	0.022827377833145056	37905	37.905	0.08444744391869986
X	0.022144194507254723	11133	11.133	0.12576193941512048
time for making epsilon is 2.29518723487854
epsilons are
[0.2912967832912536, 0.0994691535637805, 0.17785880066796053, 0.20264179312700215, 0.0697661748197639, 0.07840274270191346, 0.06590867494777405, 0.07411231327359326, 0.12820342345113211, 0.0922976364634755, 0.13864241117850398, 0.05964322740151296, 0.13885090991407853, 0.04374983306407167, 0.08028253616504379, 0.07308759260205352, 0.06535628323193925, 0.06809395997145203, 0.04962804271226695, 0.053790792628012445, 0.11922922086531812, 0.04027107817763362, 0.10550468613066323, 0.10821430156466719, 0.06372109655358502, 0.06526650427384531, 0.07632740468799164, 0.08368056376346898, 0.080881661756888, 0.023593957087786912, 0.14172076546765588, 0.036285592608383155, 0.1015652360946926, 0.1284760389771542, 0.13956983237713835, 0.06203989353107273, 0.06694490727669328, 0.2307934679048279, 0.15096409204934363, 0.20796835109841955, 0.14772388660824273, 0.2148202852166738, 0.24370737119932312, 0.16010348343198985, 0.27612699768392845, 0.22889382416850435, 0.1572158522592377, 0.15973243380008145, 0.21597445056222433, 0.13202256811727026, 0.10805627717386516, 0.21033376744025675, 0.1767452011036029, 0.1975760457810524, 0.137120582862877, 0.19465359730934517, 0.18991781897310528, 0.17252989344686134, 0.18541404648892615, 0.2101870386650776, 0.1351983696139168, 0.21719543266155972, 0.12205506463395345, 0.14146350131726407, 0.21808439636394802, 0.2034632371472397, 0.2072198998444356, 0.15902799741886922, 0.166502024085809, 0.22105474531752298, 0.1825799477885046, 0.16446638959583917, 0.18087782381450893, 0.2203636661464838, 0.18731273355566255, 0.16515349226635517, 0.24120656374050015, 0.27163796291460074, 0.1972088306234573, 0.1087715595637351, 0.17979126698085718, 0.2517168460633839, 0.14176925430768325, 0.237939264482379, 0.21196925327241967, 0.22972271350531656, 0.20859648030721992, 0.2598041465658648, 0.1602681610797707, 0.1904322246658315, 0.19168199060954222, 0.23220926391872468, 0.25300511849181123, 0.1784712684043269, 0.16497699381576086, 0.17052199276723756, 0.18649832349920353, 0.14689718551471193, 0.16414808818151316, 0.18339131907428266, 0.13394919825963486, 0.18980393140511892, 0.14462521029184183, 0.2325487102558469, 0.16440338306120708, 0.20260632344001747, 0.15516345429420206, 0.2878002817387854, 0.19530624832748758, 0.2454609415078867, 0.20927125104151426, 0.24482453806067322, 0.23097696256153533, 0.1619185401683256, 0.1559009051916277, 0.25558958250365144, 0.20817635912910393, 0.27018193126172974, 0.13289035496251755, 0.22775724732435995, 0.12569024163954143, 0.1754065116999883, 0.209652752983554, 0.20535061840937419, 0.23165672459519698, 0.2220827406052172, 0.21592838585901136, 0.1870183127486827, 0.10335986757391867, 0.20185297122459706, 0.13116202224422085, 0.23272254302516246, 0.16920446239153103, 0.25687802508267277, 0.16204431448274959, 0.20331008813400506, 0.256746024098531, 0.2355078450561717, 0.21600027691462148, 0.18446647989409454, 0.16419456086825326, 0.24288218732559536, 0.12353070364646399, 0.1311450403920368, 0.13401125422790558, 0.21345925716711767, 0.2093436548829138, 0.17981170659758214, 0.21607884930476634, 0.20959976451560006, 0.20218263318135749, 0.17235510597748996, 0.17040024411145582, 0.13799112634268756, 0.1973884726807403, 0.10976719600946747, 0.18806970093639586, 0.2042565249003885, 0.17390916806672113, 0.3141443830819159, 0.07500583959185839, 0.2303766248635836, 0.16090826065413924, 0.25386395231073067, 0.19359036680468275, 0.2349165562264226, 0.1886501147407789, 0.18702295507089503, 0.16000156019326156, 0.3018390058691986, 0.22127022973714588, 0.25028803158617746, 0.14494100149434105, 0.20200607101246057, 0.15075993256128684, 0.13172938205201848, 0.15604747389951662, 0.20575312134915758, 0.15790510280840905, 0.18614247281585722, 0.17821341857013256, 0.21240677314226253, 0.18652418505779256, 0.17531457914644058, 0.13404100903210822, 0.1616026574460918, 0.19599476203457475, 0.16626091584280592, 0.12874291899795923, 0.111753778492809, 0.21332582790769378, 0.3164728362746434, 0.18423601987479513, 0.20568715917288113, 0.20300338600958295, 0.1765727217738133, 0.21259161353706044, 0.2559796709145404, 0.2268798749640483, 0.24824420622229795, 0.1540509298683687, 0.23682795139414917, 0.13578415128433355, 0.173474359591745, 0.1826903961778285, 0.28867528166620465, 0.1525377874147461, 0.17436783618106874, 0.18950563191395342, 0.13166505034463258, 0.14616029866581345, 0.1831040987503805, 0.18951105215187197, 0.2212159635340891, 0.2457039564990795, 0.2361584571885985, 0.258696062345425, 0.28354344669688586, 0.1705915633382165, 0.19569110669326834, 0.20853857866089112, 0.23835015674001864, 0.16840012790390116, 0.209154354289505, 0.16076480359214454, 0.15443565330099698, 0.2187938850527847, 0.25122705049180394, 0.14535268409905547, 0.30543397024130037, 0.21945644371103765, 0.21201721426294048, 0.19078240943518446, 0.14933876434661184, 0.19450313238447464, 0.23447876235937973, 0.1835077223796618, 0.14441450180153953, 0.19013015334836392, 0.1335630319546129, 0.06788885366670117, 0.13174111915861775, 0.15606551060670723, 0.22183371263462726, 0.10853260554633354, 0.09784508946513014, 0.04723478646412669, 0.17037399712720613, 0.06456819862186756, 0.06527608770592953, 0.08556676940106134, 0.04742332820269893, 0.22797869430077114, 0.11071109966249494, 0.052335064824213844, 0.036460976754931776, 0.16807613067013533, 0.08685542982349233, 0.06634125613768349, 0.04299441892782514, 0.05699102170576235, 0.05435758846483935, 0.08304912962989194, 0.10557842058026513, 0.14145327313494488, 0.048947644847972385, 0.1606964962487849, 0.15976950394908584, 0.06642118901681807, 0.0856556734036632, 0.03748776208726447, 0.06011541844851615, 0.25976584606279207, 0.1066166699358026, 0.12976264466890583, 0.09905245804921647, 0.06312554356862503, 0.15433012096794832, 0.07413946167238218, 0.25946043394737944, 0.15060738596247555, 0.07132986461912921, 0.0673401166228403, 0.07916952348375089, 0.14978305667832492, 0.04440345196981782, 0.11956901527847746, 0.10031780192766913, 0.060269608543299796, 0.17522636380718906, 0.0649755566573313, 0.046439376307508454, 0.04432179993714727, 0.10890450559109982, 0.09255153623662667, 0.11444292773216484, 0.15843692194759662, 0.07128315918598178, 0.11746402607629232, 0.07955829017858458, 0.0736911803540477, 0.0727859364230317, 0.12422093745251571, 0.12370368275927936, 0.05965981374914385, 0.08470819789125697, 0.14618256641390268, 0.11067299448422756, 0.040333504272086616, 0.11553542234097293, 0.056812146735010914, 0.16123713886470498, 0.09114103598020064, 0.11238252914681517, 0.06138380765023575, 0.07622486782899877, 0.14855603226916, 0.05423765658777047, 0.05791604036698981, 0.08297503554839998, 0.06291540583921285, 0.056818827496610075, 0.04444154149355492, 0.06462356119959867, 0.2156505474135733, 0.12917543103050702, 0.10408532809774158, 0.08365460768762135, 0.13183739671071368, 0.21493696864614634, 0.06874454345524558, 0.053382952532096614, 0.05348306598698587, 0.15078092876551313, 0.09776066262603013, 0.11417726528879886, 0.14394840119533336, 0.12945628882838414, 0.1611075991312786, 0.04720180513976989, 0.08030247615723504, 0.11064332324358453, 0.10028682460931929, 0.1361021961778892, 0.03171048781965381, 0.14466677427902644, 0.05626087753433181, 0.2220851287134155, 0.16870030716438905, 0.18447917360918928, 0.10840129349586115, 0.09598557467077891, 0.08920094746203164, 0.15212574165803117, 0.1579292883288566, 0.045339112625390254, 0.0845512624797022, 0.09498740490318834, 0.06976793090684912, 0.05976888159481246, 0.1491627279612954, 0.16315838970074686, 0.11063590752911677, 0.25459264472628507, 0.14985329899679092, 0.09429904121078865, 0.18560034801128952, 0.16013212257825604, 0.14536826113160353, 0.1561856974815479, 0.039479093402841815, 0.08292335144649604, 0.06453752152734742, 0.09609063941058663, 0.15797006858421478, 0.09183250435250766, 0.035066545231914346, 0.032971525434168016, 0.07610334938564357, 0.07662249565571809, 0.20612549937902278, 0.06159597933974458, 0.056722743051699956, 0.04998261277308286, 0.14064522109618602, 0.058250101533688474, 0.04214513966331446, 0.06476741723267086, 0.12166797435688068, 0.0942866721843058, 0.12038730137161947, 0.174788258351811, 0.05479869731459246, 0.09446865116884237, 0.21884367808345756, 0.0805686880314566, 0.1049883637678558, 0.23694682690821853, 0.058969325823620855, 0.05658000851235676, 0.07529647560329904, 0.056230560459951, 0.05921150214635186, 0.1026430969126479, 0.07504855302342366, 0.060576788207616604, 0.052872823189220604, 0.13766038203173295, 0.06367388464946665, 0.04807206102647145, 0.07082174933519494, 0.08608044746880607, 0.08727606106400917, 0.11874744479798999, 0.1487067442971036, 0.09049562701373688, 0.04981608779885762, 0.0569828267762474, 0.03910127730786722, 0.05581503918040614, 0.06467881256241825, 0.10797410181772467, 0.05686869644869147, 0.20579671213734885, 0.12371229009362811, 0.04505705641796944, 0.09348818970002265, 0.17240289913489124, 0.1114238182129866, 0.0642765599639183, 0.0990330232553515, 0.06130655730453004, 0.12239624782722516, 0.1078818160836239, 0.17813727420846795, 0.16854657258893885, 0.0798019322642454, 0.07160770497410275, 0.124881889319959, 0.05346067595340228, 0.18583672560711342, 0.0874317259660618, 0.1531047234969232, 0.060563859642477354, 0.1650003146257318, 0.0690755748968776, 0.08846060887211489, 0.07133438916671021, 0.0794703125081559, 0.03818225032587854, 0.13848796774027247, 0.123656766368785, 0.06273141896812592, 0.13032250589813113, 0.03617055403307056, 0.18708208343599317, 0.05735245049980015, 0.04265272219420639, 0.18889818569707326, 0.045783438068674034, 0.09236607346476146, 0.04406804388920862, 0.13542017925772262, 0.07251010412613261, 0.21317056910348336, 0.1476741001550621, 0.15799989467679884, 0.08314635745506058, 0.09885700472196958, 0.061015660842887884, 0.11122517737617248, 0.1657801902693538, 0.14832099781535568, 0.06577070461854155, 0.10581013956839333, 0.04847318060631221, 0.059744246713280255, 0.1297911638867333, 0.11348278454414783, 0.11720118089561567, 0.02925432820418236, 0.13407176024832246, 0.10491523934514589, 0.06089353342445221, 0.0895749617844352, 0.17394895816003939, 0.16623893651737423, 0.04419281291333656, 0.08652952479319509, 0.0763841832604738, 0.06406865389717856, 0.07478689879671756, 0.06744794651548024, 0.04913967882500326, 0.06226213147245924, 0.14081454900216886, 0.07226993564386108, 0.13618007469625626, 0.11628023539352365, 0.09419805360069217, 0.138800753461359, 0.06725203627825747, 0.09772297205911475, 0.17945312680252173, 0.10197243498887312, 0.16261825852606301, 0.05597230779826549, 0.08119337369300894, 0.11460171327286306, 0.10497317716819544, 0.09414487376215216, 0.173530516073041, 0.13244227797714803, 0.09170801778497621, 0.054196705590392885, 0.1335723188266935, 0.23241805430426368, 0.16594486460705343, 0.04503129660408135, 0.04314808529579638, 0.0766364973538144, 0.13835015779440382, 0.06616852804550578, 0.055054476442019086, 0.17899999847611386, 0.13394135900770668, 0.08739913809075395, 0.0561088037134277, 0.05646294185416216, 0.05655145453593792, 0.101483977059874, 0.09421305702213933, 0.1900028600262031, 0.08327250629772191, 0.10048477579413162, 0.07552687390302375, 0.08463781885784225, 0.042253032010346615, 0.12099180213823191, 0.11748626001282454, 0.11104624336506415, 0.036285226794252494, 0.07994881686687451, 0.11113856714066446, 0.199059667954848, 0.07569771322431676, 0.2004678314635712, 0.08681363846309993, 0.16339275355037977, 0.11508293241084024, 0.11702706579708752, 0.16294303727890683, 0.186057860434975, 0.04984946308992304, 0.03934472632603878, 0.053763010457165104, 0.09460412463538492, 0.09362405918628301, 0.11962111737039735, 0.12318401408632391, 0.057676675011754684, 0.07562780326449174, 0.07274143021801002, 0.126032884948286, 0.036703556870904916, 0.09853290316700906, 0.0790167788483768, 0.05731795054850347, 0.07420319947422159, 0.053998116741285004, 0.04709841682467324, 0.06137984350208267, 0.08455692854652254, 0.0890208021096779, 0.07292921551062206, 0.13770010219946452, 0.055934850710489904, 0.03226529197639626, 0.05302689561407942, 0.06247025499479236, 0.0915174266743276, 0.09881496648312557, 0.09148531800302777, 0.09795499530508861, 0.07142429611538949, 0.0427250663147722, 0.1260859320009222, 0.06070024836492066, 0.05769664618730414, 0.0829241134157307, 0.1914069584616892, 0.058741870865122385, 0.05436794385915434, 0.17681744832508978, 0.031671938034128766, 0.08948516709908098, 0.09062456246835016, 0.11493410796878328, 0.12231149026483346, 0.17244635957913423, 0.1467758568957044, 0.10531047791216143, 0.05744373327112164, 0.023918832252591043, 0.12792029737890107, 0.060599213772180825, 0.16663234637262023, 0.09917489925788647, 0.14579977957447723, 0.050322548254603965, 0.09185222323436232, 0.028654234672493277, 0.14135189571114556, 0.05387489891872375, 0.09268329925012969, 0.05307190414264266, 0.04456432962456525, 0.09733931461772466, 0.13881037576804983, 0.06577467759488417, 0.12618499866136046, 0.0786164931422813, 0.05477899968494251, 0.10369576994423298, 0.06911602086236315, 0.08833283923632206, 0.0557806742259403, 0.04094382898944773, 0.0768514621197513, 0.12119654512585466, 0.051253852190479565, 0.0757826389819776, 0.06639666472060604, 0.07604368542654041, 0.06800208322360027, 0.07954687567814091, 0.059204533433401714, 0.07885880306005935, 0.09850634136503632, 0.1566482073796709, 0.10781475479130405, 0.07011677085381282, 0.06344146335940248, 0.058812072717373466, 0.13665179940363784, 0.08538207892197623, 0.07324388791654073, 0.044442377861201164, 0.07796090889376867, 0.06910750356117891, 0.08785628265178025, 0.09739173407701969, 0.2050881208745333, 0.13102077921578578, 0.05147752057967958, 0.07631420378543433, 0.06847602509453402, 0.2301835206302064, 0.15972315194765888, 0.047578968834365874, 0.05994362252163227, 0.07940001647057279, 0.17642983094600911, 0.18637282414049414, 0.08417141242427043, 0.07641346853510259, 0.061412549260012855, 0.17123573703486486, 0.0900779469334157, 0.11367421590460232, 0.10536615187960276, 0.04088633357822465, 0.11625843148423069, 0.07009684096859572, 0.05472408799872177, 0.07579019734263027, 0.06961888559513692, 0.055630828952083995, 0.20261483116942586, 0.08076043278500376, 0.08982936113516969, 0.05131420961075305, 0.05476117651815059, 0.07463693743929217, 0.07274466146656394, 0.11723434106229043, 0.12926168595246265, 0.1112479999285803, 0.12521037011912492, 0.049147360243884025, 0.050016415413439194, 0.1437488028563515, 0.10222161622986493, 0.11903734938477228, 0.037555305772117624, 0.09882271777572438, 0.10113801281987672, 0.06321882815805345, 0.0634159689386585, 0.12302243584365004, 0.08444744391869986, 0.12576193941512048]
0.09154039188283526
Making ranges
torch.Size([53899, 2])
We keep 9.13e+06/9.77e+08 =  0% of the original kernel matrix.

torch.Size([2632, 2])
We keep 4.47e+04/7.76e+05 =  5% of the original kernel matrix.

torch.Size([14594, 2])
We keep 7.75e+05/2.75e+07 =  2% of the original kernel matrix.

torch.Size([40207, 2])
We keep 7.95e+06/5.54e+08 =  1% of the original kernel matrix.

torch.Size([48532, 2])
We keep 8.11e+06/7.36e+08 =  1% of the original kernel matrix.

torch.Size([9712, 2])
We keep 5.53e+05/1.80e+07 =  3% of the original kernel matrix.

torch.Size([24304, 2])
We keep 2.26e+06/1.33e+08 =  1% of the original kernel matrix.

torch.Size([6582, 2])
We keep 4.49e+05/7.00e+06 =  6% of the original kernel matrix.

torch.Size([20330, 2])
We keep 1.57e+06/8.27e+07 =  1% of the original kernel matrix.

torch.Size([102402, 2])
We keep 9.90e+07/4.63e+09 =  2% of the original kernel matrix.

torch.Size([74506, 2])
We keep 1.91e+07/2.13e+09 =  0% of the original kernel matrix.

torch.Size([80445, 2])
We keep 2.29e+07/2.11e+09 =  1% of the original kernel matrix.

torch.Size([67238, 2])
We keep 1.37e+07/1.44e+09 =  0% of the original kernel matrix.

torch.Size([134726, 2])
We keep 5.76e+07/5.97e+09 =  0% of the original kernel matrix.

torch.Size([85407, 2])
We keep 2.12e+07/2.41e+09 =  0% of the original kernel matrix.

torch.Size([96630, 2])
We keep 2.88e+07/2.99e+09 =  0% of the original kernel matrix.

torch.Size([73216, 2])
We keep 1.58e+07/1.71e+09 =  0% of the original kernel matrix.

torch.Size([20338, 2])
We keep 5.87e+06/1.25e+08 =  4% of the original kernel matrix.

torch.Size([33687, 2])
We keep 4.63e+06/3.50e+08 =  1% of the original kernel matrix.

torch.Size([35189, 2])
We keep 6.41e+07/8.24e+08 =  7% of the original kernel matrix.

torch.Size([43686, 2])
We keep 9.51e+06/8.97e+08 =  1% of the original kernel matrix.

torch.Size([17516, 2])
We keep 1.65e+06/6.88e+07 =  2% of the original kernel matrix.

torch.Size([31086, 2])
We keep 3.60e+06/2.59e+08 =  1% of the original kernel matrix.

torch.Size([157513, 2])
We keep 1.78e+08/1.17e+10 =  1% of the original kernel matrix.

torch.Size([91813, 2])
We keep 2.79e+07/3.38e+09 =  0% of the original kernel matrix.

torch.Size([18704, 2])
We keep 1.58e+06/7.35e+07 =  2% of the original kernel matrix.

torch.Size([32133, 2])
We keep 3.73e+06/2.68e+08 =  1% of the original kernel matrix.

torch.Size([450554, 2])
We keep 5.03e+08/7.17e+10 =  0% of the original kernel matrix.

torch.Size([164602, 2])
We keep 6.17e+07/8.37e+09 =  0% of the original kernel matrix.

torch.Size([67727, 2])
We keep 3.48e+07/1.83e+09 =  1% of the original kernel matrix.

torch.Size([61100, 2])
We keep 1.28e+07/1.34e+09 =  0% of the original kernel matrix.

torch.Size([99189, 2])
We keep 3.74e+07/3.48e+09 =  1% of the original kernel matrix.

torch.Size([73898, 2])
We keep 1.69e+07/1.84e+09 =  0% of the original kernel matrix.

torch.Size([140648, 2])
We keep 6.48e+07/6.41e+09 =  1% of the original kernel matrix.

torch.Size([87503, 2])
We keep 2.15e+07/2.50e+09 =  0% of the original kernel matrix.

torch.Size([110516, 2])
We keep 9.23e+07/4.88e+09 =  1% of the original kernel matrix.

torch.Size([77201, 2])
We keep 1.92e+07/2.18e+09 =  0% of the original kernel matrix.

torch.Size([301778, 2])
We keep 3.64e+08/3.81e+10 =  0% of the original kernel matrix.

torch.Size([132289, 2])
We keep 4.68e+07/6.10e+09 =  0% of the original kernel matrix.

torch.Size([244865, 2])
We keep 2.06e+08/2.24e+10 =  0% of the original kernel matrix.

torch.Size([118732, 2])
We keep 3.75e+07/4.68e+09 =  0% of the original kernel matrix.

torch.Size([22253, 2])
We keep 4.84e+06/1.68e+08 =  2% of the original kernel matrix.

torch.Size([34890, 2])
We keep 5.06e+06/4.05e+08 =  1% of the original kernel matrix.

torch.Size([608088, 2])
We keep 8.35e+08/1.25e+11 =  0% of the original kernel matrix.

torch.Size([190486, 2])
We keep 7.91e+07/1.10e+10 =  0% of the original kernel matrix.

torch.Size([34096, 2])
We keep 5.58e+06/3.59e+08 =  1% of the original kernel matrix.

torch.Size([44271, 2])
We keep 6.70e+06/5.92e+08 =  1% of the original kernel matrix.

torch.Size([32978, 2])
We keep 1.17e+07/3.22e+08 =  3% of the original kernel matrix.

torch.Size([43389, 2])
We keep 6.41e+06/5.61e+08 =  1% of the original kernel matrix.

torch.Size([76041, 2])
We keep 2.59e+08/8.15e+09 =  3% of the original kernel matrix.

torch.Size([58848, 2])
We keep 2.40e+07/2.82e+09 =  0% of the original kernel matrix.

torch.Size([142422, 2])
We keep 5.82e+07/6.71e+09 =  0% of the original kernel matrix.

torch.Size([88334, 2])
We keep 2.22e+07/2.56e+09 =  0% of the original kernel matrix.

torch.Size([77333, 2])
We keep 5.45e+07/2.85e+09 =  1% of the original kernel matrix.

torch.Size([65234, 2])
We keep 1.59e+07/1.67e+09 =  0% of the original kernel matrix.

torch.Size([50763, 2])
We keep 7.89e+07/1.41e+09 =  5% of the original kernel matrix.

torch.Size([52763, 2])
We keep 1.15e+07/1.17e+09 =  0% of the original kernel matrix.

torch.Size([63456, 2])
We keep 3.37e+07/1.74e+09 =  1% of the original kernel matrix.

torch.Size([59027, 2])
We keep 1.27e+07/1.30e+09 =  0% of the original kernel matrix.

torch.Size([3488030, 2])
We keep 1.30e+10/3.30e+12 =  0% of the original kernel matrix.

torch.Size([479200, 2])
We keep 3.54e+08/5.68e+10 =  0% of the original kernel matrix.

torch.Size([14968, 2])
We keep 2.47e+06/5.97e+07 =  4% of the original kernel matrix.

torch.Size([28832, 2])
We keep 3.45e+06/2.41e+08 =  1% of the original kernel matrix.

torch.Size([753721, 2])
We keep 2.53e+09/2.50e+11 =  1% of the original kernel matrix.

torch.Size([213097, 2])
We keep 1.10e+08/1.56e+10 =  0% of the original kernel matrix.

torch.Size([35473, 2])
We keep 2.76e+07/4.81e+08 =  5% of the original kernel matrix.

torch.Size([44582, 2])
We keep 7.80e+06/6.86e+08 =  1% of the original kernel matrix.

torch.Size([21929, 2])
We keep 2.10e+06/1.10e+08 =  1% of the original kernel matrix.

torch.Size([34816, 2])
We keep 4.27e+06/3.27e+08 =  1% of the original kernel matrix.

torch.Size([16103, 2])
We keep 1.89e+06/6.59e+07 =  2% of the original kernel matrix.

torch.Size([29806, 2])
We keep 3.53e+06/2.54e+08 =  1% of the original kernel matrix.

torch.Size([143468, 2])
We keep 1.66e+08/9.03e+09 =  1% of the original kernel matrix.

torch.Size([88515, 2])
We keep 2.55e+07/2.97e+09 =  0% of the original kernel matrix.

torch.Size([132763, 2])
We keep 6.18e+07/6.06e+09 =  1% of the original kernel matrix.

torch.Size([85147, 2])
We keep 2.14e+07/2.43e+09 =  0% of the original kernel matrix.

torch.Size([4525, 2])
We keep 1.29e+05/3.04e+06 =  4% of the original kernel matrix.

torch.Size([17550, 2])
We keep 1.19e+06/5.45e+07 =  2% of the original kernel matrix.

torch.Size([14651, 2])
We keep 1.15e+06/4.42e+07 =  2% of the original kernel matrix.

torch.Size([28514, 2])
We keep 3.07e+06/2.08e+08 =  1% of the original kernel matrix.

torch.Size([5961, 2])
We keep 2.38e+05/5.95e+06 =  3% of the original kernel matrix.

torch.Size([19558, 2])
We keep 1.51e+06/7.63e+07 =  1% of the original kernel matrix.

torch.Size([15444, 2])
We keep 1.40e+06/4.93e+07 =  2% of the original kernel matrix.

torch.Size([29359, 2])
We keep 3.16e+06/2.19e+08 =  1% of the original kernel matrix.

torch.Size([4018, 2])
We keep 2.97e+05/4.55e+06 =  6% of the original kernel matrix.

torch.Size([15663, 2])
We keep 1.37e+06/6.67e+07 =  2% of the original kernel matrix.

torch.Size([3417, 2])
We keep 2.60e+05/2.20e+06 = 11% of the original kernel matrix.

torch.Size([15410, 2])
We keep 1.07e+06/4.64e+07 =  2% of the original kernel matrix.

torch.Size([12425, 2])
We keep 8.61e+05/2.85e+07 =  3% of the original kernel matrix.

torch.Size([26326, 2])
We keep 2.59e+06/1.67e+08 =  1% of the original kernel matrix.

torch.Size([2864, 2])
We keep 5.86e+04/1.05e+06 =  5% of the original kernel matrix.

torch.Size([14975, 2])
We keep 8.49e+05/3.20e+07 =  2% of the original kernel matrix.

torch.Size([5021, 2])
We keep 1.52e+05/3.38e+06 =  4% of the original kernel matrix.

torch.Size([18494, 2])
We keep 1.25e+06/5.75e+07 =  2% of the original kernel matrix.

torch.Size([12988, 2])
We keep 1.02e+06/3.30e+07 =  3% of the original kernel matrix.

torch.Size([27085, 2])
We keep 2.76e+06/1.80e+08 =  1% of the original kernel matrix.

torch.Size([12543, 2])
We keep 8.67e+05/3.10e+07 =  2% of the original kernel matrix.

torch.Size([26491, 2])
We keep 2.72e+06/1.74e+08 =  1% of the original kernel matrix.

torch.Size([5219, 2])
We keep 1.97e+05/4.50e+06 =  4% of the original kernel matrix.

torch.Size([18470, 2])
We keep 1.36e+06/6.63e+07 =  2% of the original kernel matrix.

torch.Size([18886, 2])
We keep 2.80e+06/9.31e+07 =  3% of the original kernel matrix.

torch.Size([32187, 2])
We keep 4.05e+06/3.02e+08 =  1% of the original kernel matrix.

torch.Size([32933, 2])
We keep 5.13e+06/3.30e+08 =  1% of the original kernel matrix.

torch.Size([43753, 2])
We keep 6.58e+06/5.68e+08 =  1% of the original kernel matrix.

torch.Size([6041, 2])
We keep 2.03e+05/5.33e+06 =  3% of the original kernel matrix.

torch.Size([19688, 2])
We keep 1.44e+06/7.22e+07 =  1% of the original kernel matrix.

torch.Size([9114, 2])
We keep 5.73e+05/1.52e+07 =  3% of the original kernel matrix.

torch.Size([22968, 2])
We keep 2.08e+06/1.22e+08 =  1% of the original kernel matrix.

torch.Size([6974, 2])
We keep 3.13e+05/7.85e+06 =  3% of the original kernel matrix.

torch.Size([20801, 2])
We keep 1.65e+06/8.76e+07 =  1% of the original kernel matrix.

torch.Size([18775, 2])
We keep 1.71e+06/7.93e+07 =  2% of the original kernel matrix.

torch.Size([32145, 2])
We keep 3.81e+06/2.78e+08 =  1% of the original kernel matrix.

torch.Size([7028, 2])
We keep 4.22e+05/9.04e+06 =  4% of the original kernel matrix.

torch.Size([20853, 2])
We keep 1.75e+06/9.40e+07 =  1% of the original kernel matrix.

torch.Size([8022, 2])
We keep 3.36e+05/1.00e+07 =  3% of the original kernel matrix.

torch.Size([22156, 2])
We keep 1.79e+06/9.91e+07 =  1% of the original kernel matrix.

torch.Size([10271, 2])
We keep 5.56e+05/1.79e+07 =  3% of the original kernel matrix.

torch.Size([24223, 2])
We keep 2.20e+06/1.32e+08 =  1% of the original kernel matrix.

torch.Size([8331, 2])
We keep 3.89e+05/1.19e+07 =  3% of the original kernel matrix.

torch.Size([22265, 2])
We keep 1.90e+06/1.08e+08 =  1% of the original kernel matrix.

torch.Size([5809, 2])
We keep 2.23e+05/5.49e+06 =  4% of the original kernel matrix.

torch.Size([19271, 2])
We keep 1.47e+06/7.33e+07 =  2% of the original kernel matrix.

torch.Size([19266, 2])
We keep 2.71e+06/8.64e+07 =  3% of the original kernel matrix.

torch.Size([32759, 2])
We keep 3.87e+06/2.91e+08 =  1% of the original kernel matrix.

torch.Size([5348, 2])
We keep 1.99e+05/4.65e+06 =  4% of the original kernel matrix.

torch.Size([18798, 2])
We keep 1.39e+06/6.74e+07 =  2% of the original kernel matrix.

torch.Size([25647, 2])
We keep 3.22e+06/1.70e+08 =  1% of the original kernel matrix.

torch.Size([38103, 2])
We keep 5.11e+06/4.08e+08 =  1% of the original kernel matrix.

torch.Size([16849, 2])
We keep 1.71e+06/6.09e+07 =  2% of the original kernel matrix.

torch.Size([30268, 2])
We keep 3.42e+06/2.44e+08 =  1% of the original kernel matrix.

torch.Size([5178, 2])
We keep 2.01e+05/4.53e+06 =  4% of the original kernel matrix.

torch.Size([18476, 2])
We keep 1.38e+06/6.65e+07 =  2% of the original kernel matrix.

torch.Size([6911, 2])
We keep 2.48e+05/6.83e+06 =  3% of the original kernel matrix.

torch.Size([20877, 2])
We keep 1.58e+06/8.17e+07 =  1% of the original kernel matrix.

torch.Size([5840, 2])
We keep 2.36e+05/6.15e+06 =  3% of the original kernel matrix.

torch.Size([19322, 2])
We keep 1.52e+06/7.75e+07 =  1% of the original kernel matrix.

torch.Size([11855, 2])
We keep 1.03e+06/2.87e+07 =  3% of the original kernel matrix.

torch.Size([25670, 2])
We keep 2.60e+06/1.67e+08 =  1% of the original kernel matrix.

torch.Size([11267, 2])
We keep 7.69e+05/2.28e+07 =  3% of the original kernel matrix.

torch.Size([25435, 2])
We keep 2.41e+06/1.49e+08 =  1% of the original kernel matrix.

torch.Size([5171, 2])
We keep 1.72e+05/4.13e+06 =  4% of the original kernel matrix.

torch.Size([18469, 2])
We keep 1.34e+06/6.35e+07 =  2% of the original kernel matrix.

torch.Size([7544, 2])
We keep 8.25e+05/1.31e+07 =  6% of the original kernel matrix.

torch.Size([21090, 2])
We keep 1.99e+06/1.13e+08 =  1% of the original kernel matrix.

torch.Size([11343, 2])
We keep 6.80e+05/2.41e+07 =  2% of the original kernel matrix.

torch.Size([25331, 2])
We keep 2.45e+06/1.54e+08 =  1% of the original kernel matrix.

torch.Size([8800, 2])
We keep 5.45e+05/1.36e+07 =  4% of the original kernel matrix.

torch.Size([22769, 2])
We keep 2.01e+06/1.15e+08 =  1% of the original kernel matrix.

torch.Size([4972, 2])
We keep 2.09e+05/4.20e+06 =  4% of the original kernel matrix.

torch.Size([18191, 2])
We keep 1.35e+06/6.40e+07 =  2% of the original kernel matrix.

torch.Size([7737, 2])
We keep 3.79e+05/1.14e+07 =  3% of the original kernel matrix.

torch.Size([21666, 2])
We keep 1.89e+06/1.05e+08 =  1% of the original kernel matrix.

torch.Size([11742, 2])
We keep 7.99e+05/2.49e+07 =  3% of the original kernel matrix.

torch.Size([25832, 2])
We keep 2.48e+06/1.56e+08 =  1% of the original kernel matrix.

torch.Size([4084, 2])
We keep 1.37e+05/2.66e+06 =  5% of the original kernel matrix.

torch.Size([17096, 2])
We keep 1.17e+06/5.10e+07 =  2% of the original kernel matrix.

torch.Size([2902, 2])
We keep 6.41e+04/1.15e+06 =  5% of the original kernel matrix.

torch.Size([14859, 2])
We keep 8.63e+05/3.36e+07 =  2% of the original kernel matrix.

torch.Size([7134, 2])
We keep 3.23e+05/8.12e+06 =  3% of the original kernel matrix.

torch.Size([21030, 2])
We keep 1.67e+06/8.91e+07 =  1% of the original kernel matrix.

torch.Size([32125, 2])
We keep 6.11e+06/3.39e+08 =  1% of the original kernel matrix.

torch.Size([42755, 2])
We keep 6.65e+06/5.75e+08 =  1% of the original kernel matrix.

torch.Size([9319, 2])
We keep 4.89e+05/1.56e+07 =  3% of the original kernel matrix.

torch.Size([23313, 2])
We keep 2.13e+06/1.24e+08 =  1% of the original kernel matrix.

torch.Size([3202, 2])
We keep 1.06e+05/1.76e+06 =  6% of the original kernel matrix.

torch.Size([15184, 2])
We keep 9.96e+05/4.14e+07 =  2% of the original kernel matrix.

torch.Size([16738, 2])
We keep 1.63e+06/6.47e+07 =  2% of the original kernel matrix.

torch.Size([30371, 2])
We keep 3.54e+06/2.51e+08 =  1% of the original kernel matrix.

torch.Size([4282, 2])
We keep 1.12e+05/2.42e+06 =  4% of the original kernel matrix.

torch.Size([17164, 2])
We keep 1.10e+06/4.86e+07 =  2% of the original kernel matrix.

torch.Size([5804, 2])
We keep 1.97e+05/5.29e+06 =  3% of the original kernel matrix.

torch.Size([19367, 2])
We keep 1.44e+06/7.19e+07 =  1% of the original kernel matrix.

torch.Size([4958, 2])
We keep 1.42e+05/3.40e+06 =  4% of the original kernel matrix.

torch.Size([18432, 2])
We keep 1.25e+06/5.76e+07 =  2% of the original kernel matrix.

torch.Size([6121, 2])
We keep 2.52e+05/6.23e+06 =  4% of the original kernel matrix.

torch.Size([19804, 2])
We keep 1.55e+06/7.80e+07 =  1% of the original kernel matrix.

torch.Size([3439, 2])
We keep 7.77e+04/1.45e+06 =  5% of the original kernel matrix.

torch.Size([15920, 2])
We keep 9.29e+05/3.76e+07 =  2% of the original kernel matrix.

torch.Size([12842, 2])
We keep 9.23e+05/3.10e+07 =  2% of the original kernel matrix.

torch.Size([26831, 2])
We keep 2.72e+06/1.74e+08 =  1% of the original kernel matrix.

torch.Size([7853, 2])
We keep 3.39e+05/1.01e+07 =  3% of the original kernel matrix.

torch.Size([21798, 2])
We keep 1.80e+06/9.92e+07 =  1% of the original kernel matrix.

torch.Size([7620, 2])
We keep 3.82e+05/9.90e+06 =  3% of the original kernel matrix.

torch.Size([21423, 2])
We keep 1.80e+06/9.83e+07 =  1% of the original kernel matrix.

torch.Size([4328, 2])
We keep 1.37e+05/2.94e+06 =  4% of the original kernel matrix.

torch.Size([17189, 2])
We keep 1.18e+06/5.36e+07 =  2% of the original kernel matrix.

torch.Size([3749, 2])
We keep 9.24e+04/1.80e+06 =  5% of the original kernel matrix.

torch.Size([16583, 2])
We keep 1.01e+06/4.20e+07 =  2% of the original kernel matrix.

torch.Size([9145, 2])
We keep 5.22e+05/1.47e+07 =  3% of the original kernel matrix.

torch.Size([23248, 2])
We keep 2.07e+06/1.20e+08 =  1% of the original kernel matrix.

torch.Size([11503, 2])
We keep 7.21e+05/2.51e+07 =  2% of the original kernel matrix.

torch.Size([25595, 2])
We keep 2.49e+06/1.57e+08 =  1% of the original kernel matrix.

torch.Size([10547, 2])
We keep 5.74e+05/1.98e+07 =  2% of the original kernel matrix.

torch.Size([24708, 2])
We keep 2.29e+06/1.39e+08 =  1% of the original kernel matrix.

torch.Size([8491, 2])
We keep 3.88e+05/1.21e+07 =  3% of the original kernel matrix.

torch.Size([22649, 2])
We keep 1.94e+06/1.09e+08 =  1% of the original kernel matrix.

torch.Size([13997, 2])
We keep 1.92e+06/4.88e+07 =  3% of the original kernel matrix.

torch.Size([27832, 2])
We keep 3.19e+06/2.18e+08 =  1% of the original kernel matrix.

torch.Size([11528, 2])
We keep 7.76e+05/2.70e+07 =  2% of the original kernel matrix.

torch.Size([25632, 2])
We keep 2.60e+06/1.62e+08 =  1% of the original kernel matrix.

torch.Size([8946, 2])
We keep 4.04e+05/1.26e+07 =  3% of the original kernel matrix.

torch.Size([23253, 2])
We keep 1.96e+06/1.11e+08 =  1% of the original kernel matrix.

torch.Size([19495, 2])
We keep 2.05e+06/9.73e+07 =  2% of the original kernel matrix.

torch.Size([32771, 2])
We keep 4.18e+06/3.08e+08 =  1% of the original kernel matrix.

torch.Size([8148, 2])
We keep 3.66e+05/1.05e+07 =  3% of the original kernel matrix.

torch.Size([22319, 2])
We keep 1.84e+06/1.01e+08 =  1% of the original kernel matrix.

torch.Size([16643, 2])
We keep 1.31e+06/5.66e+07 =  2% of the original kernel matrix.

torch.Size([30168, 2])
We keep 3.36e+06/2.35e+08 =  1% of the original kernel matrix.

torch.Size([4518, 2])
We keep 1.44e+05/3.05e+06 =  4% of the original kernel matrix.

torch.Size([17678, 2])
We keep 1.22e+06/5.46e+07 =  2% of the original kernel matrix.

torch.Size([10382, 2])
We keep 1.12e+06/2.35e+07 =  4% of the original kernel matrix.

torch.Size([24372, 2])
We keep 2.41e+06/1.51e+08 =  1% of the original kernel matrix.

torch.Size([6387, 2])
We keep 3.89e+05/7.07e+06 =  5% of the original kernel matrix.

torch.Size([20035, 2])
We keep 1.63e+06/8.31e+07 =  1% of the original kernel matrix.

torch.Size([13967, 2])
We keep 9.05e+05/3.63e+07 =  2% of the original kernel matrix.

torch.Size([27961, 2])
We keep 2.85e+06/1.88e+08 =  1% of the original kernel matrix.

torch.Size([2633, 2])
We keep 4.88e+04/8.03e+05 =  6% of the original kernel matrix.

torch.Size([14661, 2])
We keep 7.79e+05/2.80e+07 =  2% of the original kernel matrix.

torch.Size([7182, 2])
We keep 3.22e+05/8.66e+06 =  3% of the original kernel matrix.

torch.Size([21068, 2])
We keep 1.72e+06/9.20e+07 =  1% of the original kernel matrix.

torch.Size([3891, 2])
We keep 1.07e+05/2.22e+06 =  4% of the original kernel matrix.

torch.Size([16645, 2])
We keep 1.08e+06/4.66e+07 =  2% of the original kernel matrix.

torch.Size([5915, 2])
We keep 2.72e+05/5.65e+06 =  4% of the original kernel matrix.

torch.Size([19459, 2])
We keep 1.48e+06/7.43e+07 =  1% of the original kernel matrix.

torch.Size([4040, 2])
We keep 9.61e+04/2.16e+06 =  4% of the original kernel matrix.

torch.Size([16996, 2])
We keep 1.06e+06/4.59e+07 =  2% of the original kernel matrix.

torch.Size([4349, 2])
We keep 1.38e+05/2.93e+06 =  4% of the original kernel matrix.

torch.Size([17206, 2])
We keep 1.16e+06/5.35e+07 =  2% of the original kernel matrix.

torch.Size([11994, 2])
We keep 7.51e+05/2.62e+07 =  2% of the original kernel matrix.

torch.Size([26058, 2])
We keep 2.52e+06/1.60e+08 =  1% of the original kernel matrix.

torch.Size([13419, 2])
We keep 9.35e+05/3.41e+07 =  2% of the original kernel matrix.

torch.Size([27442, 2])
We keep 2.80e+06/1.83e+08 =  1% of the original kernel matrix.

torch.Size([3402, 2])
We keep 8.96e+04/1.64e+06 =  5% of the original kernel matrix.

torch.Size([15594, 2])
We keep 9.70e+05/4.00e+07 =  2% of the original kernel matrix.

torch.Size([6024, 2])
We keep 2.21e+05/5.91e+06 =  3% of the original kernel matrix.

torch.Size([19655, 2])
We keep 1.50e+06/7.60e+07 =  1% of the original kernel matrix.

torch.Size([3285, 2])
We keep 6.47e+04/1.25e+06 =  5% of the original kernel matrix.

torch.Size([15769, 2])
We keep 8.99e+05/3.49e+07 =  2% of the original kernel matrix.

torch.Size([20118, 2])
We keep 2.10e+06/9.61e+07 =  2% of the original kernel matrix.

torch.Size([33640, 2])
We keep 4.13e+06/3.06e+08 =  1% of the original kernel matrix.

torch.Size([4936, 2])
We keep 1.41e+05/3.32e+06 =  4% of the original kernel matrix.

torch.Size([18304, 2])
We keep 1.24e+06/5.69e+07 =  2% of the original kernel matrix.

torch.Size([23193, 2])
We keep 2.37e+06/1.24e+08 =  1% of the original kernel matrix.

torch.Size([35755, 2])
We keep 4.47e+06/3.48e+08 =  1% of the original kernel matrix.

torch.Size([9137, 2])
We keep 6.27e+05/1.62e+07 =  3% of the original kernel matrix.

torch.Size([22924, 2])
We keep 2.08e+06/1.26e+08 =  1% of the original kernel matrix.

torch.Size([5987, 2])
We keep 2.63e+05/6.22e+06 =  4% of the original kernel matrix.

torch.Size([19520, 2])
We keep 1.56e+06/7.79e+07 =  2% of the original kernel matrix.

torch.Size([6473, 2])
We keep 2.30e+05/6.21e+06 =  3% of the original kernel matrix.

torch.Size([20257, 2])
We keep 1.52e+06/7.79e+07 =  1% of the original kernel matrix.

torch.Size([4521, 2])
We keep 1.36e+05/3.08e+06 =  4% of the original kernel matrix.

torch.Size([17648, 2])
We keep 1.21e+06/5.49e+07 =  2% of the original kernel matrix.

torch.Size([5133, 2])
We keep 1.66e+05/3.94e+06 =  4% of the original kernel matrix.

torch.Size([18457, 2])
We keep 1.31e+06/6.20e+07 =  2% of the original kernel matrix.

torch.Size([5793, 2])
We keep 1.84e+05/4.77e+06 =  3% of the original kernel matrix.

torch.Size([19553, 2])
We keep 1.40e+06/6.83e+07 =  2% of the original kernel matrix.

torch.Size([8201, 2])
We keep 3.75e+05/1.11e+07 =  3% of the original kernel matrix.

torch.Size([22201, 2])
We keep 1.85e+06/1.04e+08 =  1% of the original kernel matrix.

torch.Size([36274, 2])
We keep 6.38e+06/4.42e+08 =  1% of the original kernel matrix.

torch.Size([45969, 2])
We keep 7.34e+06/6.57e+08 =  1% of the original kernel matrix.

torch.Size([6940, 2])
We keep 2.58e+05/6.92e+06 =  3% of the original kernel matrix.

torch.Size([20852, 2])
We keep 1.57e+06/8.22e+07 =  1% of the original kernel matrix.

torch.Size([19705, 2])
We keep 2.06e+06/9.73e+07 =  2% of the original kernel matrix.

torch.Size([32847, 2])
We keep 4.09e+06/3.08e+08 =  1% of the original kernel matrix.

torch.Size([4298, 2])
We keep 1.67e+05/2.96e+06 =  5% of the original kernel matrix.

torch.Size([17244, 2])
We keep 1.19e+06/5.38e+07 =  2% of the original kernel matrix.

torch.Size([10657, 2])
We keep 6.83e+05/2.03e+07 =  3% of the original kernel matrix.

torch.Size([24785, 2])
We keep 2.32e+06/1.41e+08 =  1% of the original kernel matrix.

torch.Size([3099, 2])
We keep 9.73e+04/1.53e+06 =  6% of the original kernel matrix.

torch.Size([14876, 2])
We keep 9.49e+05/3.86e+07 =  2% of the original kernel matrix.

torch.Size([10421, 2])
We keep 4.91e+06/2.61e+07 = 18% of the original kernel matrix.

torch.Size([24448, 2])
We keep 2.58e+06/1.60e+08 =  1% of the original kernel matrix.

torch.Size([6661, 2])
We keep 2.49e+05/6.79e+06 =  3% of the original kernel matrix.

torch.Size([20458, 2])
We keep 1.58e+06/8.14e+07 =  1% of the original kernel matrix.

torch.Size([3496, 2])
We keep 8.14e+04/1.59e+06 =  5% of the original kernel matrix.

torch.Size([15963, 2])
We keep 9.69e+05/3.94e+07 =  2% of the original kernel matrix.

torch.Size([4481, 2])
We keep 1.24e+05/2.78e+06 =  4% of the original kernel matrix.

torch.Size([17648, 2])
We keep 1.17e+06/5.21e+07 =  2% of the original kernel matrix.

torch.Size([5668, 2])
We keep 1.80e+05/4.73e+06 =  3% of the original kernel matrix.

torch.Size([19309, 2])
We keep 1.40e+06/6.80e+07 =  2% of the original kernel matrix.

torch.Size([8302, 2])
We keep 7.13e+05/1.28e+07 =  5% of the original kernel matrix.

torch.Size([22393, 2])
We keep 1.99e+06/1.12e+08 =  1% of the original kernel matrix.

torch.Size([11316, 2])
We keep 7.16e+05/2.54e+07 =  2% of the original kernel matrix.

torch.Size([25348, 2])
We keep 2.51e+06/1.58e+08 =  1% of the original kernel matrix.

torch.Size([4015, 2])
We keep 1.69e+05/2.26e+06 =  7% of the original kernel matrix.

torch.Size([16849, 2])
We keep 1.10e+06/4.70e+07 =  2% of the original kernel matrix.

torch.Size([22965, 2])
We keep 4.24e+06/1.59e+08 =  2% of the original kernel matrix.

torch.Size([35715, 2])
We keep 5.05e+06/3.94e+08 =  1% of the original kernel matrix.

torch.Size([20170, 2])
We keep 2.28e+06/1.05e+08 =  2% of the original kernel matrix.

torch.Size([33330, 2])
We keep 4.22e+06/3.20e+08 =  1% of the original kernel matrix.

torch.Size([20249, 2])
We keep 1.89e+06/9.15e+07 =  2% of the original kernel matrix.

torch.Size([33712, 2])
We keep 4.04e+06/2.99e+08 =  1% of the original kernel matrix.

torch.Size([5871, 2])
We keep 1.93e+05/5.06e+06 =  3% of the original kernel matrix.

torch.Size([19416, 2])
We keep 1.42e+06/7.03e+07 =  2% of the original kernel matrix.

torch.Size([6153, 2])
We keep 2.44e+05/6.26e+06 =  3% of the original kernel matrix.

torch.Size([20018, 2])
We keep 1.55e+06/7.82e+07 =  1% of the original kernel matrix.

torch.Size([8731, 2])
We keep 6.20e+05/1.48e+07 =  4% of the original kernel matrix.

torch.Size([22811, 2])
We keep 2.07e+06/1.20e+08 =  1% of the original kernel matrix.

torch.Size([5736, 2])
We keep 1.82e+05/4.66e+06 =  3% of the original kernel matrix.

torch.Size([19414, 2])
We keep 1.39e+06/6.74e+07 =  2% of the original kernel matrix.

torch.Size([5956, 2])
We keep 2.41e+05/5.89e+06 =  4% of the original kernel matrix.

torch.Size([19705, 2])
We keep 1.53e+06/7.59e+07 =  2% of the original kernel matrix.

torch.Size([6309, 2])
We keep 2.74e+05/7.20e+06 =  3% of the original kernel matrix.

torch.Size([19866, 2])
We keep 1.61e+06/8.39e+07 =  1% of the original kernel matrix.

torch.Size([10483, 2])
We keep 6.09e+05/1.97e+07 =  3% of the original kernel matrix.

torch.Size([24763, 2])
We keep 2.28e+06/1.39e+08 =  1% of the original kernel matrix.

torch.Size([10848, 2])
We keep 5.85e+05/1.97e+07 =  2% of the original kernel matrix.

torch.Size([25061, 2])
We keep 2.27e+06/1.39e+08 =  1% of the original kernel matrix.

torch.Size([14823, 2])
We keep 2.96e+06/6.71e+07 =  4% of the original kernel matrix.

torch.Size([28475, 2])
We keep 3.59e+06/2.56e+08 =  1% of the original kernel matrix.

torch.Size([6992, 2])
We keep 3.03e+05/7.97e+06 =  3% of the original kernel matrix.

torch.Size([20872, 2])
We keep 1.65e+06/8.83e+07 =  1% of the original kernel matrix.

torch.Size([31026, 2])
We keep 5.81e+06/3.26e+08 =  1% of the original kernel matrix.

torch.Size([42280, 2])
We keep 6.63e+06/5.65e+08 =  1% of the original kernel matrix.

torch.Size([8084, 2])
We keep 3.64e+05/1.10e+07 =  3% of the original kernel matrix.

torch.Size([21926, 2])
We keep 1.84e+06/1.03e+08 =  1% of the original kernel matrix.

torch.Size([6604, 2])
We keep 2.56e+05/6.66e+06 =  3% of the original kernel matrix.

torch.Size([20417, 2])
We keep 1.56e+06/8.06e+07 =  1% of the original kernel matrix.

torch.Size([10218, 2])
We keep 6.34e+05/1.94e+07 =  3% of the original kernel matrix.

torch.Size([24391, 2])
We keep 2.29e+06/1.37e+08 =  1% of the original kernel matrix.

torch.Size([2071, 2])
We keep 3.28e+04/4.79e+05 =  6% of the original kernel matrix.

torch.Size([13261, 2])
We keep 6.55e+05/2.16e+07 =  3% of the original kernel matrix.

torch.Size([93844, 2])
We keep 2.89e+07/3.12e+09 =  0% of the original kernel matrix.

torch.Size([73119, 2])
We keep 1.62e+07/1.75e+09 =  0% of the original kernel matrix.

torch.Size([4369, 2])
We keep 1.44e+05/3.10e+06 =  4% of the original kernel matrix.

torch.Size([17273, 2])
We keep 1.22e+06/5.50e+07 =  2% of the original kernel matrix.

torch.Size([12271, 2])
We keep 8.95e+05/3.00e+07 =  2% of the original kernel matrix.

torch.Size([26389, 2])
We keep 2.67e+06/1.71e+08 =  1% of the original kernel matrix.

torch.Size([3617, 2])
We keep 8.22e+04/1.75e+06 =  4% of the original kernel matrix.

torch.Size([16320, 2])
We keep 1.00e+06/4.13e+07 =  2% of the original kernel matrix.

torch.Size([7032, 2])
We keep 3.64e+05/8.94e+06 =  4% of the original kernel matrix.

torch.Size([20741, 2])
We keep 1.74e+06/9.34e+07 =  1% of the original kernel matrix.

torch.Size([4450, 2])
We keep 1.20e+05/2.69e+06 =  4% of the original kernel matrix.

torch.Size([17586, 2])
We keep 1.14e+06/5.13e+07 =  2% of the original kernel matrix.

torch.Size([8095, 2])
We keep 3.69e+05/1.07e+07 =  3% of the original kernel matrix.

torch.Size([22453, 2])
We keep 1.83e+06/1.02e+08 =  1% of the original kernel matrix.

torch.Size([7904, 2])
We keep 3.77e+05/1.11e+07 =  3% of the original kernel matrix.

torch.Size([21849, 2])
We keep 1.88e+06/1.04e+08 =  1% of the original kernel matrix.

torch.Size([12504, 2])
We keep 9.41e+05/2.99e+07 =  3% of the original kernel matrix.

torch.Size([26347, 2])
We keep 2.67e+06/1.71e+08 =  1% of the original kernel matrix.

torch.Size([2335, 2])
We keep 4.15e+04/5.98e+05 =  6% of the original kernel matrix.

torch.Size([13894, 2])
We keep 7.15e+05/2.42e+07 =  2% of the original kernel matrix.

torch.Size([4768, 2])
We keep 2.26e+05/4.08e+06 =  5% of the original kernel matrix.

torch.Size([17832, 2])
We keep 1.32e+06/6.32e+07 =  2% of the original kernel matrix.

torch.Size([3322, 2])
We keep 9.55e+04/1.80e+06 =  5% of the original kernel matrix.

torch.Size([15493, 2])
We keep 9.91e+05/4.19e+07 =  2% of the original kernel matrix.

torch.Size([16369, 2])
We keep 1.35e+06/5.21e+07 =  2% of the original kernel matrix.

torch.Size([30085, 2])
We keep 3.24e+06/2.26e+08 =  1% of the original kernel matrix.

torch.Size([6322, 2])
We keep 3.92e+05/6.78e+06 =  5% of the original kernel matrix.

torch.Size([19884, 2])
We keep 1.58e+06/8.14e+07 =  1% of the original kernel matrix.

torch.Size([14848, 2])
We keep 1.23e+06/4.48e+07 =  2% of the original kernel matrix.

torch.Size([28767, 2])
We keep 3.10e+06/2.09e+08 =  1% of the original kernel matrix.

torch.Size([16556, 2])
We keep 3.14e+07/1.01e+08 = 31% of the original kernel matrix.

torch.Size([30284, 2])
We keep 4.29e+06/3.14e+08 =  1% of the original kernel matrix.

torch.Size([13425, 2])
We keep 1.04e+06/3.38e+07 =  3% of the original kernel matrix.

torch.Size([27508, 2])
We keep 2.81e+06/1.82e+08 =  1% of the original kernel matrix.

torch.Size([6452, 2])
We keep 2.44e+05/6.40e+06 =  3% of the original kernel matrix.

torch.Size([20282, 2])
We keep 1.55e+06/7.91e+07 =  1% of the original kernel matrix.

torch.Size([11687, 2])
We keep 1.17e+06/3.07e+07 =  3% of the original kernel matrix.

torch.Size([25601, 2])
We keep 2.69e+06/1.73e+08 =  1% of the original kernel matrix.

torch.Size([8253, 2])
We keep 4.39e+05/1.09e+07 =  4% of the original kernel matrix.

torch.Size([22169, 2])
We keep 1.83e+06/1.03e+08 =  1% of the original kernel matrix.

torch.Size([9529, 2])
We keep 5.24e+05/1.63e+07 =  3% of the original kernel matrix.

torch.Size([23660, 2])
We keep 2.16e+06/1.26e+08 =  1% of the original kernel matrix.

torch.Size([5916, 2])
We keep 2.08e+05/5.15e+06 =  4% of the original kernel matrix.

torch.Size([19548, 2])
We keep 1.41e+06/7.09e+07 =  1% of the original kernel matrix.

torch.Size([7354, 2])
We keep 5.75e+05/1.14e+07 =  5% of the original kernel matrix.

torch.Size([21037, 2])
We keep 1.89e+06/1.06e+08 =  1% of the original kernel matrix.

torch.Size([9936, 2])
We keep 6.62e+05/1.65e+07 =  4% of the original kernel matrix.

torch.Size([23965, 2])
We keep 2.13e+06/1.27e+08 =  1% of the original kernel matrix.

torch.Size([19892, 2])
We keep 1.91e+06/8.98e+07 =  2% of the original kernel matrix.

torch.Size([33161, 2])
We keep 4.01e+06/2.96e+08 =  1% of the original kernel matrix.

torch.Size([12548, 2])
We keep 8.00e+05/2.88e+07 =  2% of the original kernel matrix.

torch.Size([26549, 2])
We keep 2.64e+06/1.68e+08 =  1% of the original kernel matrix.

torch.Size([7365, 2])
We keep 2.84e+05/8.45e+06 =  3% of the original kernel matrix.

torch.Size([21368, 2])
We keep 1.70e+06/9.09e+07 =  1% of the original kernel matrix.

torch.Size([10939, 2])
We keep 8.37e+05/2.37e+07 =  3% of the original kernel matrix.

torch.Size([25011, 2])
We keep 2.47e+06/1.52e+08 =  1% of the original kernel matrix.

torch.Size([21487, 2])
We keep 2.50e+06/1.13e+08 =  2% of the original kernel matrix.

torch.Size([34522, 2])
We keep 4.30e+06/3.32e+08 =  1% of the original kernel matrix.

torch.Size([29721, 2])
We keep 4.99e+06/2.79e+08 =  1% of the original kernel matrix.

torch.Size([41126, 2])
We keep 6.19e+06/5.22e+08 =  1% of the original kernel matrix.

torch.Size([5686, 2])
We keep 2.43e+05/4.81e+06 =  5% of the original kernel matrix.

torch.Size([19196, 2])
We keep 1.40e+06/6.85e+07 =  2% of the original kernel matrix.

torch.Size([1998, 2])
We keep 2.99e+04/4.46e+05 =  6% of the original kernel matrix.

torch.Size([13104, 2])
We keep 6.46e+05/2.09e+07 =  3% of the original kernel matrix.

torch.Size([8691, 2])
We keep 4.24e+05/1.23e+07 =  3% of the original kernel matrix.

torch.Size([22853, 2])
We keep 1.94e+06/1.10e+08 =  1% of the original kernel matrix.

torch.Size([5838, 2])
We keep 8.64e+05/6.21e+06 = 13% of the original kernel matrix.

torch.Size([19299, 2])
We keep 1.55e+06/7.79e+07 =  1% of the original kernel matrix.

torch.Size([6016, 2])
We keep 3.20e+05/6.39e+06 =  5% of the original kernel matrix.

torch.Size([19178, 2])
We keep 1.52e+06/7.90e+07 =  1% of the original kernel matrix.

torch.Size([9822, 2])
We keep 5.21e+05/1.67e+07 =  3% of the original kernel matrix.

torch.Size([24131, 2])
We keep 2.17e+06/1.28e+08 =  1% of the original kernel matrix.

torch.Size([6033, 2])
We keep 2.00e+05/5.27e+06 =  3% of the original kernel matrix.

torch.Size([19794, 2])
We keep 1.45e+06/7.17e+07 =  2% of the original kernel matrix.

torch.Size([3430, 2])
We keep 8.98e+04/1.66e+06 =  5% of the original kernel matrix.

torch.Size([15910, 2])
We keep 9.91e+05/4.03e+07 =  2% of the original kernel matrix.

torch.Size([5151, 2])
We keep 1.65e+05/3.29e+06 =  5% of the original kernel matrix.

torch.Size([18701, 2])
We keep 1.23e+06/5.67e+07 =  2% of the original kernel matrix.

torch.Size([3874, 2])
We keep 9.37e+04/2.00e+06 =  4% of the original kernel matrix.

torch.Size([16878, 2])
We keep 1.04e+06/4.42e+07 =  2% of the original kernel matrix.

torch.Size([14311, 2])
We keep 1.01e+06/3.96e+07 =  2% of the original kernel matrix.

torch.Size([28368, 2])
We keep 2.95e+06/1.97e+08 =  1% of the original kernel matrix.

torch.Size([4557, 2])
We keep 1.13e+05/2.63e+06 =  4% of the original kernel matrix.

torch.Size([17792, 2])
We keep 1.14e+06/5.07e+07 =  2% of the original kernel matrix.

torch.Size([19013, 2])
We keep 1.79e+06/8.38e+07 =  2% of the original kernel matrix.

torch.Size([32315, 2])
We keep 3.89e+06/2.86e+08 =  1% of the original kernel matrix.

torch.Size([10420, 2])
We keep 5.91e+05/1.89e+07 =  3% of the original kernel matrix.

torch.Size([24705, 2])
We keep 2.27e+06/1.36e+08 =  1% of the original kernel matrix.

torch.Size([8332, 2])
We keep 5.74e+05/1.30e+07 =  4% of the original kernel matrix.

torch.Size([22384, 2])
We keep 1.98e+06/1.13e+08 =  1% of the original kernel matrix.

torch.Size([2645, 2])
We keep 4.59e+04/7.99e+05 =  5% of the original kernel matrix.

torch.Size([14546, 2])
We keep 7.70e+05/2.79e+07 =  2% of the original kernel matrix.

torch.Size([14515, 2])
We keep 1.27e+06/4.38e+07 =  2% of the original kernel matrix.

torch.Size([28689, 2])
We keep 3.10e+06/2.07e+08 =  1% of the original kernel matrix.

torch.Size([10359, 2])
We keep 5.57e+05/1.86e+07 =  2% of the original kernel matrix.

torch.Size([24572, 2])
We keep 2.26e+06/1.35e+08 =  1% of the original kernel matrix.

torch.Size([7772, 2])
We keep 3.76e+05/1.03e+07 =  3% of the original kernel matrix.

torch.Size([21740, 2])
We keep 1.80e+06/1.00e+08 =  1% of the original kernel matrix.

torch.Size([20338, 2])
We keep 2.75e+06/1.05e+08 =  2% of the original kernel matrix.

torch.Size([33522, 2])
We keep 4.26e+06/3.20e+08 =  1% of the original kernel matrix.

torch.Size([15249, 2])
We keep 1.21e+06/4.99e+07 =  2% of the original kernel matrix.

torch.Size([29002, 2])
We keep 3.18e+06/2.21e+08 =  1% of the original kernel matrix.

torch.Size([8111, 2])
We keep 7.18e+05/1.28e+07 =  5% of the original kernel matrix.

torch.Size([22024, 2])
We keep 1.99e+06/1.12e+08 =  1% of the original kernel matrix.

torch.Size([8082, 2])
We keep 3.43e+05/1.02e+07 =  3% of the original kernel matrix.

torch.Size([22081, 2])
We keep 1.80e+06/9.96e+07 =  1% of the original kernel matrix.

torch.Size([5306, 2])
We keep 1.65e+05/4.12e+06 =  3% of the original kernel matrix.

torch.Size([18771, 2])
We keep 1.33e+06/6.34e+07 =  2% of the original kernel matrix.

torch.Size([3714, 2])
We keep 1.06e+05/2.16e+06 =  4% of the original kernel matrix.

torch.Size([16360, 2])
We keep 1.08e+06/4.59e+07 =  2% of the original kernel matrix.

torch.Size([4181, 2])
We keep 1.34e+05/2.59e+06 =  5% of the original kernel matrix.

torch.Size([17081, 2])
We keep 1.13e+06/5.03e+07 =  2% of the original kernel matrix.

torch.Size([3481, 2])
We keep 8.44e+04/1.55e+06 =  5% of the original kernel matrix.

torch.Size([16158, 2])
We keep 9.60e+05/3.89e+07 =  2% of the original kernel matrix.

torch.Size([2484, 2])
We keep 5.27e+04/8.50e+05 =  6% of the original kernel matrix.

torch.Size([14045, 2])
We keep 7.80e+05/2.88e+07 =  2% of the original kernel matrix.

torch.Size([10760, 2])
We keep 7.38e+05/1.97e+07 =  3% of the original kernel matrix.

torch.Size([25040, 2])
We keep 2.27e+06/1.39e+08 =  1% of the original kernel matrix.

torch.Size([7177, 2])
We keep 3.12e+05/8.29e+06 =  3% of the original kernel matrix.

torch.Size([21137, 2])
We keep 1.67e+06/9.00e+07 =  1% of the original kernel matrix.

torch.Size([5282, 2])
We keep 3.22e+05/5.94e+06 =  5% of the original kernel matrix.

torch.Size([18408, 2])
We keep 1.53e+06/7.62e+07 =  2% of the original kernel matrix.

torch.Size([4366, 2])
We keep 1.20e+05/2.60e+06 =  4% of the original kernel matrix.

torch.Size([17530, 2])
We keep 1.14e+06/5.04e+07 =  2% of the original kernel matrix.

torch.Size([11069, 2])
We keep 6.17e+05/2.07e+07 =  2% of the original kernel matrix.

torch.Size([25141, 2])
We keep 2.32e+06/1.42e+08 =  1% of the original kernel matrix.

torch.Size([5640, 2])
We keep 2.97e+05/5.59e+06 =  5% of the original kernel matrix.

torch.Size([19008, 2])
We keep 1.49e+06/7.39e+07 =  2% of the original kernel matrix.

torch.Size([11833, 2])
We keep 8.07e+05/2.81e+07 =  2% of the original kernel matrix.

torch.Size([25739, 2])
We keep 2.60e+06/1.66e+08 =  1% of the original kernel matrix.

torch.Size([13500, 2])
We keep 1.01e+06/3.81e+07 =  2% of the original kernel matrix.

torch.Size([27443, 2])
We keep 2.91e+06/1.93e+08 =  1% of the original kernel matrix.

torch.Size([5239, 2])
We keep 1.68e+05/4.20e+06 =  3% of the original kernel matrix.

torch.Size([18565, 2])
We keep 1.33e+06/6.41e+07 =  2% of the original kernel matrix.

torch.Size([3404, 2])
We keep 1.08e+05/1.91e+06 =  5% of the original kernel matrix.

torch.Size([15648, 2])
We keep 1.04e+06/4.32e+07 =  2% of the original kernel matrix.

torch.Size([15719, 2])
We keep 1.19e+06/4.96e+07 =  2% of the original kernel matrix.

torch.Size([29378, 2])
We keep 3.17e+06/2.20e+08 =  1% of the original kernel matrix.

torch.Size([2090, 2])
We keep 3.45e+04/5.24e+05 =  6% of the original kernel matrix.

torch.Size([13159, 2])
We keep 6.68e+05/2.26e+07 =  2% of the original kernel matrix.

torch.Size([5104, 2])
We keep 1.80e+05/4.32e+06 =  4% of the original kernel matrix.

torch.Size([18360, 2])
We keep 1.36e+06/6.50e+07 =  2% of the original kernel matrix.

torch.Size([5711, 2])
We keep 2.20e+05/5.35e+06 =  4% of the original kernel matrix.

torch.Size([19120, 2])
We keep 1.45e+06/7.23e+07 =  2% of the original kernel matrix.

torch.Size([7814, 2])
We keep 3.92e+05/1.07e+07 =  3% of the original kernel matrix.

torch.Size([21886, 2])
We keep 1.88e+06/1.02e+08 =  1% of the original kernel matrix.

torch.Size([15813, 2])
We keep 1.19e+06/4.85e+07 =  2% of the original kernel matrix.

torch.Size([29905, 2])
We keep 3.20e+06/2.18e+08 =  1% of the original kernel matrix.

torch.Size([7453, 2])
We keep 3.10e+05/8.61e+06 =  3% of the original kernel matrix.

torch.Size([21446, 2])
We keep 1.70e+06/9.17e+07 =  1% of the original kernel matrix.

torch.Size([4217, 2])
We keep 1.37e+05/2.80e+06 =  4% of the original kernel matrix.

torch.Size([17083, 2])
We keep 1.16e+06/5.23e+07 =  2% of the original kernel matrix.

torch.Size([8522, 2])
We keep 4.36e+05/1.25e+07 =  3% of the original kernel matrix.

torch.Size([22727, 2])
We keep 1.94e+06/1.11e+08 =  1% of the original kernel matrix.

torch.Size([17052, 2])
We keep 1.44e+06/6.05e+07 =  2% of the original kernel matrix.

torch.Size([30815, 2])
We keep 3.46e+06/2.43e+08 =  1% of the original kernel matrix.

torch.Size([5770, 2])
We keep 3.21e+06/9.59e+06 = 33% of the original kernel matrix.

torch.Size([18661, 2])
We keep 1.81e+06/9.68e+07 =  1% of the original kernel matrix.

torch.Size([18540, 2])
We keep 2.25e+06/8.76e+07 =  2% of the original kernel matrix.

torch.Size([32120, 2])
We keep 3.95e+06/2.92e+08 =  1% of the original kernel matrix.

torch.Size([82775, 2])
We keep 1.42e+08/5.03e+09 =  2% of the original kernel matrix.

torch.Size([65424, 2])
We keep 2.00e+07/2.22e+09 =  0% of the original kernel matrix.

torch.Size([18221, 2])
We keep 5.82e+06/9.41e+07 =  6% of the original kernel matrix.

torch.Size([31956, 2])
We keep 3.94e+06/3.03e+08 =  1% of the original kernel matrix.

torch.Size([13161, 2])
We keep 9.53e+05/3.35e+07 =  2% of the original kernel matrix.

torch.Size([27238, 2])
We keep 2.75e+06/1.81e+08 =  1% of the original kernel matrix.

torch.Size([3445, 2])
We keep 4.53e+05/3.78e+06 = 12% of the original kernel matrix.

torch.Size([14694, 2])
We keep 1.26e+06/6.07e+07 =  2% of the original kernel matrix.

torch.Size([29663, 2])
We keep 1.97e+07/3.25e+08 =  6% of the original kernel matrix.

torch.Size([40730, 2])
We keep 6.48e+06/5.63e+08 =  1% of the original kernel matrix.

torch.Size([41558, 2])
We keep 3.27e+07/5.88e+08 =  5% of the original kernel matrix.

torch.Size([48620, 2])
We keep 8.24e+06/7.58e+08 =  1% of the original kernel matrix.

torch.Size([241575, 2])
We keep 1.24e+09/4.44e+10 =  2% of the original kernel matrix.

torch.Size([114218, 2])
We keep 5.06e+07/6.58e+09 =  0% of the original kernel matrix.

torch.Size([10936, 2])
We keep 5.79e+05/2.04e+07 =  2% of the original kernel matrix.

torch.Size([25079, 2])
We keep 2.31e+06/1.41e+08 =  1% of the original kernel matrix.

torch.Size([124800, 2])
We keep 1.12e+08/6.93e+09 =  1% of the original kernel matrix.

torch.Size([81379, 2])
We keep 2.27e+07/2.60e+09 =  0% of the original kernel matrix.

torch.Size([118215, 2])
We keep 1.06e+08/6.31e+09 =  1% of the original kernel matrix.

torch.Size([79278, 2])
We keep 2.14e+07/2.48e+09 =  0% of the original kernel matrix.

torch.Size([55363, 2])
We keep 2.08e+07/1.26e+09 =  1% of the original kernel matrix.

torch.Size([55734, 2])
We keep 1.12e+07/1.11e+09 =  1% of the original kernel matrix.

torch.Size([292001, 2])
We keep 7.60e+08/4.93e+10 =  1% of the original kernel matrix.

torch.Size([130862, 2])
We keep 5.26e+07/6.94e+09 =  0% of the original kernel matrix.

torch.Size([4961, 2])
We keep 1.61e+05/3.33e+06 =  4% of the original kernel matrix.

torch.Size([18385, 2])
We keep 1.25e+06/5.70e+07 =  2% of the original kernel matrix.

torch.Size([23638, 2])
We keep 1.48e+07/2.50e+08 =  5% of the original kernel matrix.

torch.Size([35739, 2])
We keep 5.78e+06/4.94e+08 =  1% of the original kernel matrix.

torch.Size([230690, 2])
We keep 2.79e+08/2.42e+10 =  1% of the original kernel matrix.

torch.Size([112927, 2])
We keep 3.86e+07/4.86e+09 =  0% of the original kernel matrix.

torch.Size([840766, 2])
We keep 1.77e+09/2.14e+11 =  0% of the original kernel matrix.

torch.Size([223748, 2])
We keep 9.99e+07/1.45e+10 =  0% of the original kernel matrix.

torch.Size([10730, 2])
We keep 8.32e+05/2.07e+07 =  4% of the original kernel matrix.

torch.Size([24968, 2])
We keep 2.31e+06/1.42e+08 =  1% of the original kernel matrix.

torch.Size([57805, 2])
We keep 1.57e+07/1.26e+09 =  1% of the original kernel matrix.

torch.Size([57821, 2])
We keep 1.11e+07/1.11e+09 =  0% of the original kernel matrix.

torch.Size([108523, 2])
We keep 3.01e+08/6.43e+09 =  4% of the original kernel matrix.

torch.Size([76916, 2])
We keep 2.21e+07/2.51e+09 =  0% of the original kernel matrix.

torch.Size([315863, 2])
We keep 3.13e+09/8.45e+10 =  3% of the original kernel matrix.

torch.Size([132308, 2])
We keep 6.41e+07/9.08e+09 =  0% of the original kernel matrix.

torch.Size([141800, 2])
We keep 4.03e+08/1.42e+10 =  2% of the original kernel matrix.

torch.Size([85592, 2])
We keep 2.96e+07/3.73e+09 =  0% of the original kernel matrix.

torch.Size([162543, 2])
We keep 9.03e+08/2.04e+10 =  4% of the original kernel matrix.

torch.Size([91930, 2])
We keep 3.45e+07/4.46e+09 =  0% of the original kernel matrix.

torch.Size([62248, 2])
We keep 3.30e+07/1.51e+09 =  2% of the original kernel matrix.

torch.Size([59083, 2])
We keep 1.20e+07/1.21e+09 =  0% of the original kernel matrix.

torch.Size([34276, 2])
We keep 9.64e+06/3.70e+08 =  2% of the original kernel matrix.

torch.Size([44207, 2])
We keep 6.88e+06/6.01e+08 =  1% of the original kernel matrix.

torch.Size([13885, 2])
We keep 2.69e+06/6.11e+07 =  4% of the original kernel matrix.

torch.Size([27634, 2])
We keep 3.42e+06/2.44e+08 =  1% of the original kernel matrix.

torch.Size([203127, 2])
We keep 8.72e+08/4.12e+10 =  2% of the original kernel matrix.

torch.Size([103732, 2])
We keep 4.96e+07/6.35e+09 =  0% of the original kernel matrix.

torch.Size([11747, 2])
We keep 1.28e+06/2.79e+07 =  4% of the original kernel matrix.

torch.Size([25899, 2])
We keep 2.60e+06/1.65e+08 =  1% of the original kernel matrix.

torch.Size([10743, 2])
We keep 1.41e+06/2.95e+07 =  4% of the original kernel matrix.

torch.Size([24593, 2])
We keep 2.67e+06/1.70e+08 =  1% of the original kernel matrix.

torch.Size([122570, 2])
We keep 8.31e+07/5.60e+09 =  1% of the original kernel matrix.

torch.Size([80951, 2])
We keep 2.04e+07/2.34e+09 =  0% of the original kernel matrix.

torch.Size([49468, 2])
We keep 1.85e+07/1.19e+09 =  1% of the original kernel matrix.

torch.Size([51296, 2])
We keep 1.08e+07/1.08e+09 =  1% of the original kernel matrix.

torch.Size([690113, 2])
We keep 1.90e+09/1.94e+11 =  0% of the original kernel matrix.

torch.Size([201763, 2])
We keep 9.59e+07/1.38e+10 =  0% of the original kernel matrix.

torch.Size([179121, 2])
We keep 9.11e+07/1.05e+10 =  0% of the original kernel matrix.

torch.Size([99482, 2])
We keep 2.66e+07/3.20e+09 =  0% of the original kernel matrix.

torch.Size([3346, 2])
We keep 7.95e+04/1.56e+06 =  5% of the original kernel matrix.

torch.Size([15609, 2])
We keep 9.61e+05/3.90e+07 =  2% of the original kernel matrix.

torch.Size([32692, 2])
We keep 7.52e+06/3.35e+08 =  2% of the original kernel matrix.

torch.Size([43017, 2])
We keep 6.61e+06/5.72e+08 =  1% of the original kernel matrix.

torch.Size([20476, 2])
We keep 3.02e+06/1.10e+08 =  2% of the original kernel matrix.

torch.Size([33601, 2])
We keep 4.28e+06/3.27e+08 =  1% of the original kernel matrix.

torch.Size([37317, 2])
We keep 1.02e+07/5.48e+08 =  1% of the original kernel matrix.

torch.Size([45810, 2])
We keep 8.05e+06/7.32e+08 =  1% of the original kernel matrix.

torch.Size([144264, 2])
We keep 2.35e+08/7.90e+09 =  2% of the original kernel matrix.

torch.Size([89513, 2])
We keep 2.39e+07/2.78e+09 =  0% of the original kernel matrix.

torch.Size([13173, 2])
We keep 1.38e+06/3.71e+07 =  3% of the original kernel matrix.

torch.Size([27327, 2])
We keep 2.87e+06/1.90e+08 =  1% of the original kernel matrix.

torch.Size([91096, 2])
We keep 3.83e+07/2.98e+09 =  1% of the original kernel matrix.

torch.Size([70517, 2])
We keep 1.57e+07/1.71e+09 =  0% of the original kernel matrix.

torch.Size([3382, 2])
We keep 1.05e+05/1.52e+06 =  6% of the original kernel matrix.

torch.Size([15579, 2])
We keep 9.53e+05/3.86e+07 =  2% of the original kernel matrix.

torch.Size([14911, 2])
We keep 1.10e+06/4.20e+07 =  2% of the original kernel matrix.

torch.Size([28896, 2])
We keep 3.01e+06/2.03e+08 =  1% of the original kernel matrix.

torch.Size([82013, 2])
We keep 9.10e+07/4.06e+09 =  2% of the original kernel matrix.

torch.Size([65565, 2])
We keep 1.83e+07/1.99e+09 =  0% of the original kernel matrix.

torch.Size([112248, 2])
We keep 8.02e+07/5.33e+09 =  1% of the original kernel matrix.

torch.Size([77256, 2])
We keep 2.01e+07/2.28e+09 =  0% of the original kernel matrix.

torch.Size([67684, 2])
We keep 3.25e+07/2.05e+09 =  1% of the original kernel matrix.

torch.Size([60524, 2])
We keep 1.36e+07/1.42e+09 =  0% of the original kernel matrix.

torch.Size([13866, 2])
We keep 1.52e+06/4.17e+07 =  3% of the original kernel matrix.

torch.Size([27706, 2])
We keep 2.91e+06/2.02e+08 =  1% of the original kernel matrix.

torch.Size([385683, 2])
We keep 4.56e+08/7.28e+10 =  0% of the original kernel matrix.

torch.Size([150390, 2])
We keep 6.22e+07/8.43e+09 =  0% of the original kernel matrix.

torch.Size([25752, 2])
We keep 3.00e+06/1.69e+08 =  1% of the original kernel matrix.

torch.Size([37985, 2])
We keep 5.02e+06/4.06e+08 =  1% of the original kernel matrix.

torch.Size([39588, 2])
We keep 6.73e+06/5.12e+08 =  1% of the original kernel matrix.

torch.Size([47531, 2])
We keep 7.74e+06/7.07e+08 =  1% of the original kernel matrix.

torch.Size([164991, 2])
We keep 1.03e+08/1.09e+10 =  0% of the original kernel matrix.

torch.Size([95117, 2])
We keep 2.74e+07/3.26e+09 =  0% of the original kernel matrix.

torch.Size([9159, 2])
We keep 6.32e+05/1.61e+07 =  3% of the original kernel matrix.

torch.Size([23139, 2])
We keep 2.11e+06/1.25e+08 =  1% of the original kernel matrix.

torch.Size([135289, 2])
We keep 7.31e+07/6.60e+09 =  1% of the original kernel matrix.

torch.Size([85214, 2])
We keep 2.20e+07/2.54e+09 =  0% of the original kernel matrix.

torch.Size([317902, 2])
We keep 7.05e+08/5.52e+10 =  1% of the original kernel matrix.

torch.Size([136365, 2])
We keep 5.57e+07/7.34e+09 =  0% of the original kernel matrix.

torch.Size([380630, 2])
We keep 8.09e+08/7.35e+10 =  1% of the original kernel matrix.

torch.Size([148512, 2])
We keep 6.33e+07/8.47e+09 =  0% of the original kernel matrix.

torch.Size([30481, 2])
We keep 5.59e+06/3.18e+08 =  1% of the original kernel matrix.

torch.Size([41296, 2])
We keep 6.46e+06/5.57e+08 =  1% of the original kernel matrix.

torch.Size([44650, 2])
We keep 1.13e+07/7.66e+08 =  1% of the original kernel matrix.

torch.Size([50656, 2])
We keep 9.17e+06/8.65e+08 =  1% of the original kernel matrix.

torch.Size([24825, 2])
We keep 4.96e+06/2.15e+08 =  2% of the original kernel matrix.

torch.Size([36402, 2])
We keep 5.46e+06/4.58e+08 =  1% of the original kernel matrix.

torch.Size([12722, 2])
We keep 8.56e+05/3.14e+07 =  2% of the original kernel matrix.

torch.Size([26783, 2])
We keep 2.70e+06/1.75e+08 =  1% of the original kernel matrix.

torch.Size([86274, 2])
We keep 7.90e+07/3.92e+09 =  2% of the original kernel matrix.

torch.Size([67625, 2])
We keep 1.79e+07/1.96e+09 =  0% of the original kernel matrix.

torch.Size([26688, 2])
We keep 3.50e+06/2.00e+08 =  1% of the original kernel matrix.

torch.Size([38779, 2])
We keep 5.42e+06/4.42e+08 =  1% of the original kernel matrix.

torch.Size([64824, 2])
We keep 5.30e+07/1.97e+09 =  2% of the original kernel matrix.

torch.Size([59667, 2])
We keep 1.31e+07/1.39e+09 =  0% of the original kernel matrix.

torch.Size([85865, 2])
We keep 9.13e+07/3.21e+09 =  2% of the original kernel matrix.

torch.Size([69261, 2])
We keep 1.61e+07/1.77e+09 =  0% of the original kernel matrix.

torch.Size([81754, 2])
We keep 3.24e+08/3.38e+09 =  9% of the original kernel matrix.

torch.Size([66870, 2])
We keep 1.69e+07/1.82e+09 =  0% of the original kernel matrix.

torch.Size([17617, 2])
We keep 7.73e+06/1.33e+08 =  5% of the original kernel matrix.

torch.Size([30693, 2])
We keep 4.63e+06/3.60e+08 =  1% of the original kernel matrix.

torch.Size([23640, 2])
We keep 3.39e+06/1.47e+08 =  2% of the original kernel matrix.

torch.Size([36555, 2])
We keep 4.77e+06/3.79e+08 =  1% of the original kernel matrix.

torch.Size([73432, 2])
We keep 3.30e+08/1.17e+10 =  2% of the original kernel matrix.

torch.Size([58233, 2])
We keep 2.86e+07/3.39e+09 =  0% of the original kernel matrix.

torch.Size([47964, 2])
We keep 2.39e+08/1.35e+09 = 17% of the original kernel matrix.

torch.Size([50877, 2])
We keep 1.15e+07/1.15e+09 =  0% of the original kernel matrix.

torch.Size([13564, 2])
We keep 4.32e+06/5.12e+07 =  8% of the original kernel matrix.

torch.Size([27490, 2])
We keep 3.19e+06/2.24e+08 =  1% of the original kernel matrix.

torch.Size([31425, 2])
We keep 5.33e+06/3.03e+08 =  1% of the original kernel matrix.

torch.Size([42607, 2])
We keep 6.32e+06/5.44e+08 =  1% of the original kernel matrix.

torch.Size([510495, 2])
We keep 1.18e+09/1.33e+11 =  0% of the original kernel matrix.

torch.Size([171678, 2])
We keep 8.15e+07/1.14e+10 =  0% of the original kernel matrix.

torch.Size([26875, 2])
We keep 4.39e+06/2.02e+08 =  2% of the original kernel matrix.

torch.Size([38773, 2])
We keep 5.39e+06/4.44e+08 =  1% of the original kernel matrix.

torch.Size([149939, 2])
We keep 3.19e+08/1.46e+10 =  2% of the original kernel matrix.

torch.Size([87964, 2])
We keep 3.00e+07/3.78e+09 =  0% of the original kernel matrix.

torch.Size([11619, 2])
We keep 9.06e+05/2.79e+07 =  3% of the original kernel matrix.

torch.Size([25862, 2])
We keep 2.60e+06/1.65e+08 =  1% of the original kernel matrix.

torch.Size([31058, 2])
We keep 3.03e+07/9.09e+08 =  3% of the original kernel matrix.

torch.Size([38396, 2])
We keep 9.68e+06/9.42e+08 =  1% of the original kernel matrix.

torch.Size([28468, 2])
We keep 6.92e+06/2.60e+08 =  2% of the original kernel matrix.

torch.Size([40016, 2])
We keep 5.95e+06/5.04e+08 =  1% of the original kernel matrix.

torch.Size([100885, 2])
We keep 2.58e+08/1.04e+10 =  2% of the original kernel matrix.

torch.Size([69746, 2])
We keep 2.72e+07/3.18e+09 =  0% of the original kernel matrix.

torch.Size([77210, 2])
We keep 2.45e+07/2.51e+09 =  0% of the original kernel matrix.

torch.Size([65349, 2])
We keep 1.47e+07/1.57e+09 =  0% of the original kernel matrix.

torch.Size([15203, 2])
We keep 1.07e+06/4.35e+07 =  2% of the original kernel matrix.

torch.Size([29023, 2])
We keep 3.02e+06/2.06e+08 =  1% of the original kernel matrix.

torch.Size([240300, 2])
We keep 1.45e+08/2.11e+10 =  0% of the original kernel matrix.

torch.Size([117047, 2])
We keep 3.64e+07/4.54e+09 =  0% of the original kernel matrix.

torch.Size([199280, 2])
We keep 1.38e+08/1.41e+10 =  0% of the original kernel matrix.

torch.Size([105929, 2])
We keep 3.06e+07/3.71e+09 =  0% of the original kernel matrix.

torch.Size([40635, 2])
We keep 1.35e+08/1.49e+09 =  9% of the original kernel matrix.

torch.Size([45989, 2])
We keep 1.10e+07/1.20e+09 =  0% of the original kernel matrix.

torch.Size([158904, 2])
We keep 9.45e+07/8.19e+09 =  1% of the original kernel matrix.

torch.Size([92775, 2])
We keep 2.41e+07/2.83e+09 =  0% of the original kernel matrix.

torch.Size([200708, 2])
We keep 1.16e+08/1.57e+10 =  0% of the original kernel matrix.

torch.Size([107173, 2])
We keep 3.19e+07/3.92e+09 =  0% of the original kernel matrix.

torch.Size([430849, 2])
We keep 4.29e+08/7.38e+10 =  0% of the original kernel matrix.

torch.Size([162877, 2])
We keep 6.27e+07/8.49e+09 =  0% of the original kernel matrix.

torch.Size([90541, 2])
We keep 4.09e+08/7.33e+09 =  5% of the original kernel matrix.

torch.Size([69546, 2])
We keep 2.37e+07/2.68e+09 =  0% of the original kernel matrix.

torch.Size([5305, 2])
We keep 2.18e+05/4.68e+06 =  4% of the original kernel matrix.

torch.Size([18589, 2])
We keep 1.39e+06/6.76e+07 =  2% of the original kernel matrix.

torch.Size([21620, 2])
We keep 1.96e+06/1.03e+08 =  1% of the original kernel matrix.

torch.Size([34396, 2])
We keep 4.16e+06/3.17e+08 =  1% of the original kernel matrix.

torch.Size([25336, 2])
We keep 1.35e+07/3.93e+08 =  3% of the original kernel matrix.

torch.Size([37308, 2])
We keep 7.00e+06/6.20e+08 =  1% of the original kernel matrix.

torch.Size([62668, 2])
We keep 2.22e+07/1.41e+09 =  1% of the original kernel matrix.

torch.Size([59308, 2])
We keep 1.16e+07/1.18e+09 =  0% of the original kernel matrix.

torch.Size([20097, 2])
We keep 2.21e+06/9.32e+07 =  2% of the original kernel matrix.

torch.Size([33442, 2])
We keep 4.04e+06/3.02e+08 =  1% of the original kernel matrix.

torch.Size([5552, 2])
We keep 2.41e+05/4.83e+06 =  4% of the original kernel matrix.

torch.Size([19034, 2])
We keep 1.40e+06/6.87e+07 =  2% of the original kernel matrix.

torch.Size([122137, 2])
We keep 8.29e+07/5.10e+09 =  1% of the original kernel matrix.

torch.Size([81881, 2])
We keep 1.97e+07/2.23e+09 =  0% of the original kernel matrix.

torch.Size([242753, 2])
We keep 1.61e+08/2.14e+10 =  0% of the original kernel matrix.

torch.Size([117317, 2])
We keep 3.63e+07/4.57e+09 =  0% of the original kernel matrix.

torch.Size([113298, 2])
We keep 5.46e+08/2.10e+10 =  2% of the original kernel matrix.

torch.Size([69081, 2])
We keep 3.60e+07/4.53e+09 =  0% of the original kernel matrix.

torch.Size([13805, 2])
We keep 1.22e+06/4.02e+07 =  3% of the original kernel matrix.

torch.Size([27700, 2])
We keep 2.96e+06/1.98e+08 =  1% of the original kernel matrix.

torch.Size([37941, 2])
We keep 1.07e+07/5.70e+08 =  1% of the original kernel matrix.

torch.Size([46068, 2])
We keep 8.16e+06/7.46e+08 =  1% of the original kernel matrix.

torch.Size([28822, 2])
We keep 4.25e+06/2.39e+08 =  1% of the original kernel matrix.

torch.Size([40446, 2])
We keep 5.81e+06/4.83e+08 =  1% of the original kernel matrix.

torch.Size([16182, 2])
We keep 1.87e+06/5.86e+07 =  3% of the original kernel matrix.

torch.Size([30129, 2])
We keep 3.39e+06/2.39e+08 =  1% of the original kernel matrix.

torch.Size([21593, 2])
We keep 2.18e+06/1.12e+08 =  1% of the original kernel matrix.

torch.Size([34587, 2])
We keep 4.32e+06/3.31e+08 =  1% of the original kernel matrix.

torch.Size([12626, 2])
We keep 9.02e+05/2.77e+07 =  3% of the original kernel matrix.

torch.Size([26925, 2])
We keep 2.59e+06/1.65e+08 =  1% of the original kernel matrix.

torch.Size([257167, 2])
We keep 1.19e+09/5.03e+10 =  2% of the original kernel matrix.

torch.Size([118480, 2])
We keep 5.33e+07/7.01e+09 =  0% of the original kernel matrix.

torch.Size([74499, 2])
We keep 1.86e+07/1.85e+09 =  1% of the original kernel matrix.

torch.Size([65153, 2])
We keep 1.29e+07/1.34e+09 =  0% of the original kernel matrix.

torch.Size([31279, 2])
We keep 6.00e+06/2.86e+08 =  2% of the original kernel matrix.

torch.Size([42291, 2])
We keep 6.21e+06/5.28e+08 =  1% of the original kernel matrix.

torch.Size([37894, 2])
We keep 1.30e+07/5.17e+08 =  2% of the original kernel matrix.

torch.Size([46422, 2])
We keep 7.94e+06/7.11e+08 =  1% of the original kernel matrix.

torch.Size([16511, 2])
We keep 3.01e+06/7.76e+07 =  3% of the original kernel matrix.

torch.Size([30300, 2])
We keep 3.78e+06/2.75e+08 =  1% of the original kernel matrix.

torch.Size([1153396, 2])
We keep 1.59e+10/5.45e+11 =  2% of the original kernel matrix.

torch.Size([266181, 2])
We keep 1.55e+08/2.31e+10 =  0% of the original kernel matrix.

torch.Size([15607, 2])
We keep 1.33e+06/5.45e+07 =  2% of the original kernel matrix.

torch.Size([29130, 2])
We keep 3.30e+06/2.31e+08 =  1% of the original kernel matrix.

torch.Size([210948, 2])
We keep 1.45e+08/1.68e+10 =  0% of the original kernel matrix.

torch.Size([108898, 2])
We keep 3.31e+07/4.05e+09 =  0% of the original kernel matrix.

torch.Size([4740, 2])
We keep 1.80e+05/3.92e+06 =  4% of the original kernel matrix.

torch.Size([17803, 2])
We keep 1.29e+06/6.19e+07 =  2% of the original kernel matrix.

torch.Size([11251, 2])
We keep 6.52e+05/2.17e+07 =  3% of the original kernel matrix.

torch.Size([25400, 2])
We keep 2.36e+06/1.46e+08 =  1% of the original kernel matrix.

torch.Size([8991, 2])
We keep 4.10e+05/1.26e+07 =  3% of the original kernel matrix.

torch.Size([23153, 2])
We keep 1.94e+06/1.11e+08 =  1% of the original kernel matrix.

torch.Size([32342, 2])
We keep 8.54e+06/3.05e+08 =  2% of the original kernel matrix.

torch.Size([42863, 2])
We keep 6.30e+06/5.46e+08 =  1% of the original kernel matrix.

torch.Size([35346, 2])
We keep 7.99e+07/6.83e+08 = 11% of the original kernel matrix.

torch.Size([44473, 2])
We keep 8.91e+06/8.17e+08 =  1% of the original kernel matrix.

torch.Size([44698, 2])
We keep 4.76e+07/1.05e+09 =  4% of the original kernel matrix.

torch.Size([50015, 2])
We keep 1.06e+07/1.01e+09 =  1% of the original kernel matrix.

torch.Size([14361, 2])
We keep 1.07e+06/4.16e+07 =  2% of the original kernel matrix.

torch.Size([28217, 2])
We keep 3.00e+06/2.01e+08 =  1% of the original kernel matrix.

torch.Size([12527, 2])
We keep 9.43e+05/3.30e+07 =  2% of the original kernel matrix.

torch.Size([26375, 2])
We keep 2.73e+06/1.79e+08 =  1% of the original kernel matrix.

torch.Size([383276, 2])
We keep 6.68e+08/5.69e+10 =  1% of the original kernel matrix.

torch.Size([150981, 2])
We keep 5.55e+07/7.45e+09 =  0% of the original kernel matrix.

torch.Size([61195, 2])
We keep 1.83e+07/1.40e+09 =  1% of the original kernel matrix.

torch.Size([58464, 2])
We keep 1.16e+07/1.17e+09 =  0% of the original kernel matrix.

torch.Size([42843, 2])
We keep 1.63e+07/7.73e+08 =  2% of the original kernel matrix.

torch.Size([49358, 2])
We keep 9.36e+06/8.69e+08 =  1% of the original kernel matrix.

torch.Size([112187, 2])
We keep 6.72e+07/4.70e+09 =  1% of the original kernel matrix.

torch.Size([79014, 2])
We keep 1.94e+07/2.14e+09 =  0% of the original kernel matrix.

torch.Size([96683, 2])
We keep 2.23e+08/1.24e+10 =  1% of the original kernel matrix.

torch.Size([65345, 2])
We keep 2.94e+07/3.48e+09 =  0% of the original kernel matrix.

torch.Size([15973, 2])
We keep 1.22e+06/5.14e+07 =  2% of the original kernel matrix.

torch.Size([29809, 2])
We keep 3.28e+06/2.24e+08 =  1% of the original kernel matrix.

torch.Size([11775, 2])
We keep 7.53e+05/2.56e+07 =  2% of the original kernel matrix.

torch.Size([25782, 2])
We keep 2.49e+06/1.58e+08 =  1% of the original kernel matrix.

torch.Size([26721, 2])
We keep 1.60e+07/2.67e+08 =  6% of the original kernel matrix.

torch.Size([38328, 2])
We keep 6.08e+06/5.11e+08 =  1% of the original kernel matrix.

torch.Size([3361, 2])
We keep 9.44e+04/1.70e+06 =  5% of the original kernel matrix.

torch.Size([15612, 2])
We keep 9.91e+05/4.08e+07 =  2% of the original kernel matrix.

torch.Size([15027, 2])
We keep 1.10e+06/4.63e+07 =  2% of the original kernel matrix.

torch.Size([28811, 2])
We keep 3.13e+06/2.13e+08 =  1% of the original kernel matrix.

torch.Size([40903, 2])
We keep 1.19e+07/7.15e+08 =  1% of the original kernel matrix.

torch.Size([48207, 2])
We keep 8.99e+06/8.36e+08 =  1% of the original kernel matrix.

torch.Size([6942, 2])
We keep 7.19e+05/1.15e+07 =  6% of the original kernel matrix.

torch.Size([20471, 2])
We keep 1.92e+06/1.06e+08 =  1% of the original kernel matrix.

torch.Size([12782, 2])
We keep 8.13e+05/2.90e+07 =  2% of the original kernel matrix.

torch.Size([26702, 2])
We keep 2.63e+06/1.68e+08 =  1% of the original kernel matrix.

torch.Size([15068, 2])
We keep 1.46e+06/4.98e+07 =  2% of the original kernel matrix.

torch.Size([28698, 2])
We keep 3.17e+06/2.21e+08 =  1% of the original kernel matrix.

torch.Size([12782, 2])
We keep 1.02e+06/3.34e+07 =  3% of the original kernel matrix.

torch.Size([26914, 2])
We keep 2.77e+06/1.81e+08 =  1% of the original kernel matrix.

torch.Size([513908, 2])
We keep 1.74e+09/1.44e+11 =  1% of the original kernel matrix.

torch.Size([170712, 2])
We keep 8.52e+07/1.18e+10 =  0% of the original kernel matrix.

torch.Size([50667, 2])
We keep 8.49e+07/1.50e+09 =  5% of the original kernel matrix.

torch.Size([51699, 2])
We keep 1.18e+07/1.21e+09 =  0% of the original kernel matrix.

torch.Size([130156, 2])
We keep 1.09e+08/7.15e+09 =  1% of the original kernel matrix.

torch.Size([83516, 2])
We keep 2.25e+07/2.64e+09 =  0% of the original kernel matrix.

torch.Size([30152, 2])
We keep 2.14e+07/5.97e+08 =  3% of the original kernel matrix.

torch.Size([39860, 2])
We keep 8.28e+06/7.63e+08 =  1% of the original kernel matrix.

torch.Size([12092, 2])
We keep 1.14e+06/3.25e+07 =  3% of the original kernel matrix.

torch.Size([26127, 2])
We keep 2.71e+06/1.78e+08 =  1% of the original kernel matrix.

torch.Size([43137, 2])
We keep 1.70e+07/8.39e+08 =  2% of the original kernel matrix.

torch.Size([47270, 2])
We keep 8.92e+06/9.06e+08 =  0% of the original kernel matrix.

torch.Size([876890, 2])
We keep 5.79e+09/2.84e+11 =  2% of the original kernel matrix.

torch.Size([230133, 2])
We keep 1.14e+08/1.67e+10 =  0% of the original kernel matrix.

torch.Size([1131762, 2])
We keep 2.01e+09/4.19e+11 =  0% of the original kernel matrix.

torch.Size([266159, 2])
We keep 1.37e+08/2.02e+10 =  0% of the original kernel matrix.

torch.Size([80929, 2])
We keep 4.47e+07/2.56e+09 =  1% of the original kernel matrix.

torch.Size([66946, 2])
We keep 1.49e+07/1.58e+09 =  0% of the original kernel matrix.

torch.Size([83118, 2])
We keep 4.73e+07/2.63e+09 =  1% of the original kernel matrix.

torch.Size([67903, 2])
We keep 1.51e+07/1.60e+09 =  0% of the original kernel matrix.

torch.Size([6334, 2])
We keep 2.31e+05/6.31e+06 =  3% of the original kernel matrix.

torch.Size([20115, 2])
We keep 1.54e+06/7.85e+07 =  1% of the original kernel matrix.

torch.Size([91015, 2])
We keep 2.31e+08/9.35e+09 =  2% of the original kernel matrix.

torch.Size([65521, 2])
We keep 2.56e+07/3.02e+09 =  0% of the original kernel matrix.

torch.Size([148049, 2])
We keep 7.46e+08/1.60e+10 =  4% of the original kernel matrix.

torch.Size([88590, 2])
We keep 3.24e+07/3.95e+09 =  0% of the original kernel matrix.

torch.Size([237845, 2])
We keep 7.69e+08/3.66e+10 =  2% of the original kernel matrix.

torch.Size([116504, 2])
We keep 4.62e+07/5.98e+09 =  0% of the original kernel matrix.

torch.Size([16304, 2])
We keep 1.61e+06/6.39e+07 =  2% of the original kernel matrix.

torch.Size([29748, 2])
We keep 3.50e+06/2.50e+08 =  1% of the original kernel matrix.

torch.Size([175543, 2])
We keep 1.33e+08/1.25e+10 =  1% of the original kernel matrix.

torch.Size([98705, 2])
We keep 2.89e+07/3.50e+09 =  0% of the original kernel matrix.

torch.Size([471550, 2])
We keep 8.74e+08/1.00e+11 =  0% of the original kernel matrix.

torch.Size([166043, 2])
We keep 7.24e+07/9.91e+09 =  0% of the original kernel matrix.

torch.Size([121339, 2])
We keep 1.06e+08/6.55e+09 =  1% of the original kernel matrix.

torch.Size([78546, 2])
We keep 2.16e+07/2.53e+09 =  0% of the original kernel matrix.

torch.Size([21011, 2])
We keep 3.50e+06/1.42e+08 =  2% of the original kernel matrix.

torch.Size([33453, 2])
We keep 4.70e+06/3.73e+08 =  1% of the original kernel matrix.

torch.Size([48948, 2])
We keep 1.54e+07/8.10e+08 =  1% of the original kernel matrix.

torch.Size([54753, 2])
We keep 9.51e+06/8.89e+08 =  1% of the original kernel matrix.

torch.Size([24333, 2])
We keep 3.99e+06/1.74e+08 =  2% of the original kernel matrix.

torch.Size([36732, 2])
We keep 5.06e+06/4.12e+08 =  1% of the original kernel matrix.

torch.Size([9839, 2])
We keep 6.33e+05/1.71e+07 =  3% of the original kernel matrix.

torch.Size([24043, 2])
We keep 2.18e+06/1.29e+08 =  1% of the original kernel matrix.

torch.Size([220172, 2])
We keep 2.97e+08/1.97e+10 =  1% of the original kernel matrix.

torch.Size([111856, 2])
We keep 3.57e+07/4.38e+09 =  0% of the original kernel matrix.

torch.Size([38533, 2])
We keep 1.90e+07/6.99e+08 =  2% of the original kernel matrix.

torch.Size([46065, 2])
We keep 8.82e+06/8.26e+08 =  1% of the original kernel matrix.

torch.Size([5317, 2])
We keep 2.01e+05/4.28e+06 =  4% of the original kernel matrix.

torch.Size([18626, 2])
We keep 1.34e+06/6.46e+07 =  2% of the original kernel matrix.

torch.Size([72654, 2])
We keep 2.31e+07/1.82e+09 =  1% of the original kernel matrix.

torch.Size([64569, 2])
We keep 1.28e+07/1.33e+09 =  0% of the original kernel matrix.

torch.Size([36507, 2])
We keep 5.87e+06/4.18e+08 =  1% of the original kernel matrix.

torch.Size([46479, 2])
We keep 7.22e+06/6.39e+08 =  1% of the original kernel matrix.

torch.Size([3956, 2])
We keep 2.27e+05/2.68e+06 =  8% of the original kernel matrix.

torch.Size([16460, 2])
We keep 1.09e+06/5.11e+07 =  2% of the original kernel matrix.

torch.Size([159648, 2])
We keep 2.29e+08/1.21e+10 =  1% of the original kernel matrix.

torch.Size([93540, 2])
We keep 2.89e+07/3.43e+09 =  0% of the original kernel matrix.

torch.Size([204100, 2])
We keep 1.50e+08/1.51e+10 =  0% of the original kernel matrix.

torch.Size([107183, 2])
We keep 3.11e+07/3.85e+09 =  0% of the original kernel matrix.

torch.Size([86860, 2])
We keep 9.01e+07/2.97e+09 =  3% of the original kernel matrix.

torch.Size([69591, 2])
We keep 1.59e+07/1.70e+09 =  0% of the original kernel matrix.

torch.Size([203352, 2])
We keep 2.15e+08/1.65e+10 =  1% of the original kernel matrix.

torch.Size([106745, 2])
We keep 3.22e+07/4.02e+09 =  0% of the original kernel matrix.

torch.Size([166983, 2])
We keep 1.14e+08/1.14e+10 =  1% of the original kernel matrix.

torch.Size([94828, 2])
We keep 2.76e+07/3.33e+09 =  0% of the original kernel matrix.

torch.Size([30894, 2])
We keep 1.24e+07/4.15e+08 =  2% of the original kernel matrix.

torch.Size([40967, 2])
We keep 7.09e+06/6.37e+08 =  1% of the original kernel matrix.

torch.Size([84865, 2])
We keep 5.56e+07/2.85e+09 =  1% of the original kernel matrix.

torch.Size([67908, 2])
We keep 1.57e+07/1.67e+09 =  0% of the original kernel matrix.

torch.Size([175800, 2])
We keep 8.35e+07/1.08e+10 =  0% of the original kernel matrix.

torch.Size([98600, 2])
We keep 2.72e+07/3.26e+09 =  0% of the original kernel matrix.

torch.Size([262407, 2])
We keep 2.06e+08/2.34e+10 =  0% of the original kernel matrix.

torch.Size([122946, 2])
We keep 3.78e+07/4.79e+09 =  0% of the original kernel matrix.

torch.Size([18066, 2])
We keep 1.87e+06/7.37e+07 =  2% of the original kernel matrix.

torch.Size([31577, 2])
We keep 3.67e+06/2.68e+08 =  1% of the original kernel matrix.

torch.Size([120140, 2])
We keep 4.22e+08/8.09e+09 =  5% of the original kernel matrix.

torch.Size([81757, 2])
We keep 2.42e+07/2.81e+09 =  0% of the original kernel matrix.

torch.Size([263224, 2])
We keep 3.66e+09/4.49e+10 =  8% of the original kernel matrix.

torch.Size([122414, 2])
We keep 5.08e+07/6.62e+09 =  0% of the original kernel matrix.

torch.Size([102549, 2])
We keep 6.56e+07/3.94e+09 =  1% of the original kernel matrix.

torch.Size([75290, 2])
We keep 1.76e+07/1.96e+09 =  0% of the original kernel matrix.

torch.Size([44927, 2])
We keep 2.81e+07/1.22e+09 =  2% of the original kernel matrix.

torch.Size([49751, 2])
We keep 1.11e+07/1.09e+09 =  1% of the original kernel matrix.

torch.Size([28363, 2])
We keep 1.17e+08/1.16e+09 = 10% of the original kernel matrix.

torch.Size([37320, 2])
We keep 1.11e+07/1.06e+09 =  1% of the original kernel matrix.

torch.Size([24545, 2])
We keep 3.72e+06/1.77e+08 =  2% of the original kernel matrix.

torch.Size([36914, 2])
We keep 5.15e+06/4.16e+08 =  1% of the original kernel matrix.

torch.Size([14962, 2])
We keep 1.11e+06/4.47e+07 =  2% of the original kernel matrix.

torch.Size([28734, 2])
We keep 3.06e+06/2.09e+08 =  1% of the original kernel matrix.

torch.Size([34856, 2])
We keep 3.92e+07/8.87e+08 =  4% of the original kernel matrix.

torch.Size([43230, 2])
We keep 9.65e+06/9.31e+08 =  1% of the original kernel matrix.

torch.Size([313090, 2])
We keep 2.30e+08/3.45e+10 =  0% of the original kernel matrix.

torch.Size([135060, 2])
We keep 4.49e+07/5.80e+09 =  0% of the original kernel matrix.

torch.Size([207928, 2])
We keep 1.15e+08/1.47e+10 =  0% of the original kernel matrix.

torch.Size([108213, 2])
We keep 3.08e+07/3.79e+09 =  0% of the original kernel matrix.

torch.Size([598245, 2])
We keep 1.82e+09/1.60e+11 =  1% of the original kernel matrix.

torch.Size([185880, 2])
We keep 8.88e+07/1.25e+10 =  0% of the original kernel matrix.

torch.Size([174589, 2])
We keep 5.44e+08/1.90e+10 =  2% of the original kernel matrix.

torch.Size([98743, 2])
We keep 3.56e+07/4.30e+09 =  0% of the original kernel matrix.

torch.Size([132629, 2])
We keep 9.48e+07/7.15e+09 =  1% of the original kernel matrix.

torch.Size([84760, 2])
We keep 2.28e+07/2.64e+09 =  0% of the original kernel matrix.

torch.Size([26515, 2])
We keep 4.76e+06/3.03e+08 =  1% of the original kernel matrix.

torch.Size([38066, 2])
We keep 6.24e+06/5.44e+08 =  1% of the original kernel matrix.

torch.Size([210238, 2])
We keep 1.43e+08/1.59e+10 =  0% of the original kernel matrix.

torch.Size([109253, 2])
We keep 3.21e+07/3.94e+09 =  0% of the original kernel matrix.

torch.Size([6185, 2])
We keep 2.73e+05/6.31e+06 =  4% of the original kernel matrix.

torch.Size([19829, 2])
We keep 1.54e+06/7.85e+07 =  1% of the original kernel matrix.

torch.Size([21591, 2])
We keep 3.28e+06/1.34e+08 =  2% of the original kernel matrix.

torch.Size([34278, 2])
We keep 4.64e+06/3.62e+08 =  1% of the original kernel matrix.

torch.Size([407687, 2])
We keep 5.47e+08/6.37e+10 =  0% of the original kernel matrix.

torch.Size([155519, 2])
We keep 5.88e+07/7.89e+09 =  0% of the original kernel matrix.

torch.Size([45733, 2])
We keep 1.76e+07/7.39e+08 =  2% of the original kernel matrix.

torch.Size([51037, 2])
We keep 8.67e+06/8.50e+08 =  1% of the original kernel matrix.

torch.Size([10142, 2])
We keep 5.53e+05/1.80e+07 =  3% of the original kernel matrix.

torch.Size([24010, 2])
We keep 2.18e+06/1.32e+08 =  1% of the original kernel matrix.

torch.Size([24937, 2])
We keep 7.39e+06/2.58e+08 =  2% of the original kernel matrix.

torch.Size([36970, 2])
We keep 5.81e+06/5.02e+08 =  1% of the original kernel matrix.

torch.Size([125586, 2])
We keep 1.58e+08/7.50e+09 =  2% of the original kernel matrix.

torch.Size([82457, 2])
We keep 2.35e+07/2.71e+09 =  0% of the original kernel matrix.

torch.Size([38052, 2])
We keep 1.00e+07/5.13e+08 =  1% of the original kernel matrix.

torch.Size([46369, 2])
We keep 7.76e+06/7.08e+08 =  1% of the original kernel matrix.

torch.Size([160343, 2])
We keep 1.41e+08/1.06e+10 =  1% of the original kernel matrix.

torch.Size([94428, 2])
We keep 2.73e+07/3.22e+09 =  0% of the original kernel matrix.

torch.Size([13716, 2])
We keep 9.06e+06/1.40e+08 =  6% of the original kernel matrix.

torch.Size([26283, 2])
We keep 4.66e+06/3.70e+08 =  1% of the original kernel matrix.

torch.Size([24839, 2])
We keep 1.25e+07/2.98e+08 =  4% of the original kernel matrix.

torch.Size([36812, 2])
We keep 6.22e+06/5.40e+08 =  1% of the original kernel matrix.

torch.Size([9236, 2])
We keep 5.85e+05/1.61e+07 =  3% of the original kernel matrix.

torch.Size([23338, 2])
We keep 2.16e+06/1.25e+08 =  1% of the original kernel matrix.

torch.Size([11173, 2])
We keep 8.27e+05/2.39e+07 =  3% of the original kernel matrix.

torch.Size([25483, 2])
We keep 2.50e+06/1.53e+08 =  1% of the original kernel matrix.

torch.Size([74797, 2])
We keep 4.43e+07/2.10e+09 =  2% of the original kernel matrix.

torch.Size([65102, 2])
We keep 1.35e+07/1.43e+09 =  0% of the original kernel matrix.

torch.Size([82043, 2])
We keep 1.08e+08/3.85e+09 =  2% of the original kernel matrix.

torch.Size([65116, 2])
We keep 1.79e+07/1.94e+09 =  0% of the original kernel matrix.

torch.Size([17228, 2])
We keep 1.19e+07/1.28e+08 =  9% of the original kernel matrix.

torch.Size([30601, 2])
We keep 4.60e+06/3.54e+08 =  1% of the original kernel matrix.

torch.Size([230021, 2])
We keep 2.68e+08/2.11e+10 =  1% of the original kernel matrix.

torch.Size([113442, 2])
We keep 3.58e+07/4.54e+09 =  0% of the original kernel matrix.

torch.Size([8076, 2])
We keep 4.60e+05/1.16e+07 =  3% of the original kernel matrix.

torch.Size([22039, 2])
We keep 1.91e+06/1.07e+08 =  1% of the original kernel matrix.

torch.Size([57640, 2])
We keep 3.88e+07/1.19e+09 =  3% of the original kernel matrix.

torch.Size([58430, 2])
We keep 1.10e+07/1.08e+09 =  1% of the original kernel matrix.

torch.Size([11022, 2])
We keep 4.71e+06/3.72e+07 = 12% of the original kernel matrix.

torch.Size([25032, 2])
We keep 2.95e+06/1.91e+08 =  1% of the original kernel matrix.

torch.Size([173525, 2])
We keep 8.33e+07/1.01e+10 =  0% of the original kernel matrix.

torch.Size([97407, 2])
We keep 2.63e+07/3.14e+09 =  0% of the original kernel matrix.

torch.Size([11269, 2])
We keep 8.86e+05/2.57e+07 =  3% of the original kernel matrix.

torch.Size([25264, 2])
We keep 2.54e+06/1.59e+08 =  1% of the original kernel matrix.

torch.Size([93423, 2])
We keep 2.45e+08/4.49e+09 =  5% of the original kernel matrix.

torch.Size([70167, 2])
We keep 1.91e+07/2.09e+09 =  0% of the original kernel matrix.

torch.Size([50712, 2])
We keep 5.40e+07/1.05e+09 =  5% of the original kernel matrix.

torch.Size([53483, 2])
We keep 9.79e+06/1.01e+09 =  0% of the original kernel matrix.

torch.Size([90496, 2])
We keep 2.26e+08/4.27e+09 =  5% of the original kernel matrix.

torch.Size([69906, 2])
We keep 1.76e+07/2.04e+09 =  0% of the original kernel matrix.

torch.Size([74545, 2])
We keep 2.76e+07/2.17e+09 =  1% of the original kernel matrix.

torch.Size([64914, 2])
We keep 1.41e+07/1.46e+09 =  0% of the original kernel matrix.

torch.Size([560508, 2])
We keep 1.98e+09/1.63e+11 =  1% of the original kernel matrix.

torch.Size([178710, 2])
We keep 8.85e+07/1.26e+10 =  0% of the original kernel matrix.

torch.Size([18752, 2])
We keep 1.86e+06/7.80e+07 =  2% of the original kernel matrix.

torch.Size([32270, 2])
We keep 3.84e+06/2.76e+08 =  1% of the original kernel matrix.

torch.Size([17032, 2])
We keep 6.77e+06/1.35e+08 =  5% of the original kernel matrix.

torch.Size([30037, 2])
We keep 4.70e+06/3.64e+08 =  1% of the original kernel matrix.

torch.Size([136859, 2])
We keep 1.20e+08/8.51e+09 =  1% of the original kernel matrix.

torch.Size([85853, 2])
We keep 2.47e+07/2.88e+09 =  0% of the original kernel matrix.

torch.Size([20193, 2])
We keep 2.38e+06/9.96e+07 =  2% of the original kernel matrix.

torch.Size([33295, 2])
We keep 4.15e+06/3.12e+08 =  1% of the original kernel matrix.

torch.Size([854802, 2])
We keep 1.42e+09/2.47e+11 =  0% of the original kernel matrix.

torch.Size([227034, 2])
We keep 1.08e+08/1.55e+10 =  0% of the original kernel matrix.

torch.Size([7609, 2])
We keep 4.51e+05/1.12e+07 =  4% of the original kernel matrix.

torch.Size([21297, 2])
We keep 1.86e+06/1.04e+08 =  1% of the original kernel matrix.

torch.Size([184396, 2])
We keep 1.33e+08/1.44e+10 =  0% of the original kernel matrix.

torch.Size([100722, 2])
We keep 3.07e+07/3.75e+09 =  0% of the original kernel matrix.

torch.Size([496046, 2])
We keep 4.66e+08/8.87e+10 =  0% of the original kernel matrix.

torch.Size([173467, 2])
We keep 6.80e+07/9.31e+09 =  0% of the original kernel matrix.

torch.Size([7724, 2])
We keep 4.29e+05/1.06e+07 =  4% of the original kernel matrix.

torch.Size([21515, 2])
We keep 1.82e+06/1.02e+08 =  1% of the original kernel matrix.

torch.Size([258681, 2])
We keep 7.63e+08/5.24e+10 =  1% of the original kernel matrix.

torch.Size([117051, 2])
We keep 5.38e+07/7.15e+09 =  0% of the original kernel matrix.

torch.Size([28566, 2])
We keep 2.51e+07/7.89e+08 =  3% of the original kernel matrix.

torch.Size([37398, 2])
We keep 9.39e+06/8.78e+08 =  1% of the original kernel matrix.

torch.Size([394835, 2])
We keep 1.06e+09/7.31e+10 =  1% of the original kernel matrix.

torch.Size([152905, 2])
We keep 6.31e+07/8.45e+09 =  0% of the original kernel matrix.

torch.Size([19225, 2])
We keep 1.73e+06/7.89e+07 =  2% of the original kernel matrix.

torch.Size([32423, 2])
We keep 3.76e+06/2.78e+08 =  1% of the original kernel matrix.

torch.Size([90745, 2])
We keep 5.21e+07/3.34e+09 =  1% of the original kernel matrix.

torch.Size([69793, 2])
We keep 1.66e+07/1.81e+09 =  0% of the original kernel matrix.

torch.Size([5208, 2])
We keep 3.32e+05/5.11e+06 =  6% of the original kernel matrix.

torch.Size([18387, 2])
We keep 1.44e+06/7.07e+07 =  2% of the original kernel matrix.

torch.Size([15274, 2])
We keep 1.40e+06/5.16e+07 =  2% of the original kernel matrix.

torch.Size([29374, 2])
We keep 3.31e+06/2.25e+08 =  1% of the original kernel matrix.

torch.Size([11478, 2])
We keep 1.32e+06/3.08e+07 =  4% of the original kernel matrix.

torch.Size([25557, 2])
We keep 2.71e+06/1.74e+08 =  1% of the original kernel matrix.

torch.Size([64003, 2])
We keep 3.99e+07/1.50e+09 =  2% of the original kernel matrix.

torch.Size([59828, 2])
We keep 1.19e+07/1.21e+09 =  0% of the original kernel matrix.

torch.Size([35911, 2])
We keep 1.19e+07/5.37e+08 =  2% of the original kernel matrix.

torch.Size([44781, 2])
We keep 8.02e+06/7.24e+08 =  1% of the original kernel matrix.

torch.Size([160545, 2])
We keep 1.43e+08/1.03e+10 =  1% of the original kernel matrix.

torch.Size([94114, 2])
We keep 2.69e+07/3.18e+09 =  0% of the original kernel matrix.

torch.Size([28850, 2])
We keep 4.10e+06/2.59e+08 =  1% of the original kernel matrix.

torch.Size([40143, 2])
We keep 5.92e+06/5.03e+08 =  1% of the original kernel matrix.

torch.Size([11199, 2])
We keep 7.55e+05/2.38e+07 =  3% of the original kernel matrix.

torch.Size([25168, 2])
We keep 2.46e+06/1.53e+08 =  1% of the original kernel matrix.

torch.Size([14063, 2])
We keep 1.52e+06/4.49e+07 =  3% of the original kernel matrix.

torch.Size([28019, 2])
We keep 3.10e+06/2.09e+08 =  1% of the original kernel matrix.

torch.Size([112722, 2])
We keep 1.77e+08/7.03e+09 =  2% of the original kernel matrix.

torch.Size([78216, 2])
We keep 2.31e+07/2.62e+09 =  0% of the original kernel matrix.

torch.Size([23249, 2])
We keep 2.13e+07/3.72e+08 =  5% of the original kernel matrix.

torch.Size([35247, 2])
We keep 6.92e+06/6.03e+08 =  1% of the original kernel matrix.

torch.Size([338522, 2])
We keep 3.04e+08/4.39e+10 =  0% of the original kernel matrix.

torch.Size([142261, 2])
We keep 5.03e+07/6.55e+09 =  0% of the original kernel matrix.

torch.Size([162798, 2])
We keep 2.32e+08/1.20e+10 =  1% of the original kernel matrix.

torch.Size([95113, 2])
We keep 2.87e+07/3.42e+09 =  0% of the original kernel matrix.

torch.Size([18300, 2])
We keep 4.02e+06/1.02e+08 =  3% of the original kernel matrix.

torch.Size([31845, 2])
We keep 4.15e+06/3.16e+08 =  1% of the original kernel matrix.

torch.Size([22498, 2])
We keep 1.60e+07/2.28e+08 =  7% of the original kernel matrix.

torch.Size([35090, 2])
We keep 5.64e+06/4.72e+08 =  1% of the original kernel matrix.

torch.Size([20392, 2])
We keep 8.79e+06/1.84e+08 =  4% of the original kernel matrix.

torch.Size([33029, 2])
We keep 5.26e+06/4.24e+08 =  1% of the original kernel matrix.

torch.Size([1709214, 2])
We keep 8.37e+09/9.06e+11 =  0% of the original kernel matrix.

torch.Size([326895, 2])
We keep 1.96e+08/2.97e+10 =  0% of the original kernel matrix.

torch.Size([19678, 2])
We keep 2.00e+06/9.01e+07 =  2% of the original kernel matrix.

torch.Size([32782, 2])
We keep 4.00e+06/2.97e+08 =  1% of the original kernel matrix.

torch.Size([34236, 2])
We keep 5.78e+06/3.67e+08 =  1% of the original kernel matrix.

torch.Size([44171, 2])
We keep 6.79e+06/5.99e+08 =  1% of the original kernel matrix.

torch.Size([140233, 2])
We keep 3.03e+08/1.04e+10 =  2% of the original kernel matrix.

torch.Size([85934, 2])
We keep 2.71e+07/3.18e+09 =  0% of the original kernel matrix.

torch.Size([39448, 2])
We keep 2.74e+07/9.38e+08 =  2% of the original kernel matrix.

torch.Size([45897, 2])
We keep 1.01e+07/9.57e+08 =  1% of the original kernel matrix.

torch.Size([9613, 2])
We keep 6.29e+05/1.73e+07 =  3% of the original kernel matrix.

torch.Size([23784, 2])
We keep 2.16e+06/1.30e+08 =  1% of the original kernel matrix.

torch.Size([10528, 2])
We keep 8.55e+05/2.37e+07 =  3% of the original kernel matrix.

torch.Size([24467, 2])
We keep 2.47e+06/1.52e+08 =  1% of the original kernel matrix.

torch.Size([339709, 2])
We keep 1.12e+09/7.58e+10 =  1% of the original kernel matrix.

torch.Size([138181, 2])
We keep 6.38e+07/8.61e+09 =  0% of the original kernel matrix.

torch.Size([57422, 2])
We keep 1.83e+07/1.22e+09 =  1% of the original kernel matrix.

torch.Size([57001, 2])
We keep 1.10e+07/1.09e+09 =  1% of the original kernel matrix.

torch.Size([61526, 2])
We keep 6.13e+07/2.37e+09 =  2% of the original kernel matrix.

torch.Size([56371, 2])
We keep 1.44e+07/1.52e+09 =  0% of the original kernel matrix.

torch.Size([149120, 2])
We keep 9.74e+07/7.25e+09 =  1% of the original kernel matrix.

torch.Size([89867, 2])
We keep 2.31e+07/2.66e+09 =  0% of the original kernel matrix.

torch.Size([90951, 2])
We keep 1.70e+08/3.27e+09 =  5% of the original kernel matrix.

torch.Size([71784, 2])
We keep 1.66e+07/1.79e+09 =  0% of the original kernel matrix.

torch.Size([119473, 2])
We keep 1.38e+08/5.65e+09 =  2% of the original kernel matrix.

torch.Size([80796, 2])
We keep 2.10e+07/2.35e+09 =  0% of the original kernel matrix.

torch.Size([265225, 2])
We keep 5.14e+08/3.89e+10 =  1% of the original kernel matrix.

torch.Size([122553, 2])
We keep 4.78e+07/6.16e+09 =  0% of the original kernel matrix.

torch.Size([163065, 2])
We keep 8.80e+07/8.88e+09 =  0% of the original kernel matrix.

torch.Size([94762, 2])
We keep 2.50e+07/2.94e+09 =  0% of the original kernel matrix.

torch.Size([15492, 2])
We keep 2.33e+06/6.22e+07 =  3% of the original kernel matrix.

torch.Size([29243, 2])
We keep 3.48e+06/2.46e+08 =  1% of the original kernel matrix.

torch.Size([99361, 2])
We keep 4.05e+07/3.56e+09 =  1% of the original kernel matrix.

torch.Size([74183, 2])
We keep 1.70e+07/1.87e+09 =  0% of the original kernel matrix.

torch.Size([16541, 2])
We keep 2.86e+06/7.77e+07 =  3% of the original kernel matrix.

torch.Size([30121, 2])
We keep 3.78e+06/2.75e+08 =  1% of the original kernel matrix.

torch.Size([25646, 2])
We keep 6.44e+06/1.97e+08 =  3% of the original kernel matrix.

torch.Size([37786, 2])
We keep 5.39e+06/4.39e+08 =  1% of the original kernel matrix.

torch.Size([41235, 2])
We keep 1.28e+07/7.04e+08 =  1% of the original kernel matrix.

torch.Size([48068, 2])
We keep 8.71e+06/8.29e+08 =  1% of the original kernel matrix.

torch.Size([18098, 2])
We keep 2.13e+06/7.33e+07 =  2% of the original kernel matrix.

torch.Size([31608, 2])
We keep 3.73e+06/2.68e+08 =  1% of the original kernel matrix.

torch.Size([76297, 2])
We keep 2.39e+08/5.25e+09 =  4% of the original kernel matrix.

torch.Size([62809, 2])
We keep 2.02e+07/2.26e+09 =  0% of the original kernel matrix.

torch.Size([39813, 2])
We keep 9.24e+06/5.87e+08 =  1% of the original kernel matrix.

torch.Size([47609, 2])
We keep 8.19e+06/7.57e+08 =  1% of the original kernel matrix.

torch.Size([7992, 2])
We keep 5.81e+05/1.33e+07 =  4% of the original kernel matrix.

torch.Size([21464, 2])
We keep 1.95e+06/1.14e+08 =  1% of the original kernel matrix.

torch.Size([35286, 2])
We keep 7.76e+06/4.72e+08 =  1% of the original kernel matrix.

torch.Size([44881, 2])
We keep 7.62e+06/6.79e+08 =  1% of the original kernel matrix.

torch.Size([7601, 2])
We keep 1.99e+06/2.67e+07 =  7% of the original kernel matrix.

torch.Size([20803, 2])
We keep 2.53e+06/1.61e+08 =  1% of the original kernel matrix.

torch.Size([158693, 2])
We keep 3.28e+08/1.82e+10 =  1% of the original kernel matrix.

torch.Size([90760, 2])
We keep 3.46e+07/4.22e+09 =  0% of the original kernel matrix.

torch.Size([48515, 2])
We keep 8.97e+07/1.72e+09 =  5% of the original kernel matrix.

torch.Size([50132, 2])
We keep 1.26e+07/1.30e+09 =  0% of the original kernel matrix.

torch.Size([28153, 2])
We keep 5.05e+06/2.23e+08 =  2% of the original kernel matrix.

torch.Size([39672, 2])
We keep 5.61e+06/4.67e+08 =  1% of the original kernel matrix.

torch.Size([22463, 2])
We keep 4.54e+07/3.51e+08 = 12% of the original kernel matrix.

torch.Size([33249, 2])
We keep 6.70e+06/5.86e+08 =  1% of the original kernel matrix.

torch.Size([41568, 2])
We keep 1.39e+07/7.48e+08 =  1% of the original kernel matrix.

torch.Size([48479, 2])
We keep 9.24e+06/8.55e+08 =  1% of the original kernel matrix.

torch.Size([10187, 2])
We keep 6.05e+05/1.83e+07 =  3% of the original kernel matrix.

torch.Size([24256, 2])
We keep 2.24e+06/1.34e+08 =  1% of the original kernel matrix.

torch.Size([19671, 2])
We keep 2.17e+06/9.09e+07 =  2% of the original kernel matrix.

torch.Size([32957, 2])
We keep 3.95e+06/2.98e+08 =  1% of the original kernel matrix.

torch.Size([49876, 2])
We keep 1.18e+07/8.32e+08 =  1% of the original kernel matrix.

torch.Size([50597, 2])
We keep 8.80e+06/9.02e+08 =  0% of the original kernel matrix.

torch.Size([226432, 2])
We keep 2.02e+08/2.06e+10 =  0% of the original kernel matrix.

torch.Size([113371, 2])
We keep 3.60e+07/4.48e+09 =  0% of the original kernel matrix.

torch.Size([19585, 2])
We keep 2.29e+06/1.00e+08 =  2% of the original kernel matrix.

torch.Size([32971, 2])
We keep 4.21e+06/3.13e+08 =  1% of the original kernel matrix.

torch.Size([4565, 2])
We keep 1.39e+05/3.12e+06 =  4% of the original kernel matrix.

torch.Size([17752, 2])
We keep 1.21e+06/5.52e+07 =  2% of the original kernel matrix.

torch.Size([10997, 2])
We keep 7.24e+05/2.34e+07 =  3% of the original kernel matrix.

torch.Size([24937, 2])
We keep 2.43e+06/1.51e+08 =  1% of the original kernel matrix.

torch.Size([366089, 2])
We keep 5.74e+08/5.87e+10 =  0% of the original kernel matrix.

torch.Size([145734, 2])
We keep 5.71e+07/7.57e+09 =  0% of the original kernel matrix.

torch.Size([461305, 2])
We keep 1.18e+09/8.75e+10 =  1% of the original kernel matrix.

torch.Size([166874, 2])
We keep 6.77e+07/9.24e+09 =  0% of the original kernel matrix.

torch.Size([70399, 2])
We keep 3.45e+07/2.32e+09 =  1% of the original kernel matrix.

torch.Size([60561, 2])
We keep 1.42e+07/1.51e+09 =  0% of the original kernel matrix.

torch.Size([18454, 2])
We keep 1.75e+06/7.65e+07 =  2% of the original kernel matrix.

torch.Size([32130, 2])
We keep 3.79e+06/2.73e+08 =  1% of the original kernel matrix.

torch.Size([116122, 2])
We keep 1.07e+08/6.38e+09 =  1% of the original kernel matrix.

torch.Size([78483, 2])
We keep 2.21e+07/2.50e+09 =  0% of the original kernel matrix.

torch.Size([231185, 2])
We keep 1.42e+08/2.06e+10 =  0% of the original kernel matrix.

torch.Size([115756, 2])
We keep 3.61e+07/4.49e+09 =  0% of the original kernel matrix.

torch.Size([9165, 2])
We keep 5.35e+05/1.59e+07 =  3% of the original kernel matrix.

torch.Size([23165, 2])
We keep 2.10e+06/1.25e+08 =  1% of the original kernel matrix.

torch.Size([17925, 2])
We keep 2.78e+06/8.35e+07 =  3% of the original kernel matrix.

torch.Size([31472, 2])
We keep 3.90e+06/2.86e+08 =  1% of the original kernel matrix.

torch.Size([57535, 2])
We keep 1.21e+07/1.11e+09 =  1% of the original kernel matrix.

torch.Size([58324, 2])
We keep 1.05e+07/1.04e+09 =  1% of the original kernel matrix.

torch.Size([126432, 2])
We keep 5.05e+08/1.82e+10 =  2% of the original kernel matrix.

torch.Size([79982, 2])
We keep 3.42e+07/4.22e+09 =  0% of the original kernel matrix.

torch.Size([154526, 2])
We keep 7.42e+08/1.63e+10 =  4% of the original kernel matrix.

torch.Size([91889, 2])
We keep 3.29e+07/3.98e+09 =  0% of the original kernel matrix.

torch.Size([208602, 2])
We keep 1.42e+08/1.64e+10 =  0% of the original kernel matrix.

torch.Size([108626, 2])
We keep 3.28e+07/4.01e+09 =  0% of the original kernel matrix.

torch.Size([37423, 2])
We keep 7.11e+06/4.81e+08 =  1% of the original kernel matrix.

torch.Size([46668, 2])
We keep 7.62e+06/6.86e+08 =  1% of the original kernel matrix.

torch.Size([37543, 2])
We keep 8.48e+07/7.02e+08 = 12% of the original kernel matrix.

torch.Size([45717, 2])
We keep 8.95e+06/8.28e+08 =  1% of the original kernel matrix.

torch.Size([8066, 2])
We keep 3.72e+05/1.09e+07 =  3% of the original kernel matrix.

torch.Size([22171, 2])
We keep 1.88e+06/1.03e+08 =  1% of the original kernel matrix.

torch.Size([60771, 2])
We keep 2.41e+07/1.49e+09 =  1% of the original kernel matrix.

torch.Size([58062, 2])
We keep 1.19e+07/1.21e+09 =  0% of the original kernel matrix.

torch.Size([38904, 2])
We keep 8.79e+06/5.04e+08 =  1% of the original kernel matrix.

torch.Size([47243, 2])
We keep 7.74e+06/7.01e+08 =  1% of the original kernel matrix.

torch.Size([69112, 2])
We keep 7.62e+07/2.63e+09 =  2% of the original kernel matrix.

torch.Size([60472, 2])
We keep 1.51e+07/1.60e+09 =  0% of the original kernel matrix.

torch.Size([60379, 2])
We keep 1.73e+07/1.35e+09 =  1% of the original kernel matrix.

torch.Size([58080, 2])
We keep 1.14e+07/1.15e+09 =  0% of the original kernel matrix.

torch.Size([511424, 2])
We keep 6.00e+08/9.99e+10 =  0% of the original kernel matrix.

torch.Size([175408, 2])
We keep 7.15e+07/9.88e+09 =  0% of the original kernel matrix.

torch.Size([22253, 2])
We keep 5.39e+06/1.69e+08 =  3% of the original kernel matrix.

torch.Size([34976, 2])
We keep 5.10e+06/4.06e+08 =  1% of the original kernel matrix.

torch.Size([19995, 2])
We keep 2.68e+07/1.92e+08 = 13% of the original kernel matrix.

torch.Size([32858, 2])
We keep 5.37e+06/4.33e+08 =  1% of the original kernel matrix.

torch.Size([29745, 2])
We keep 8.18e+06/2.63e+08 =  3% of the original kernel matrix.

torch.Size([40916, 2])
We keep 5.91e+06/5.07e+08 =  1% of the original kernel matrix.

torch.Size([560896, 2])
We keep 9.56e+09/2.42e+11 =  3% of the original kernel matrix.

torch.Size([181867, 2])
We keep 1.05e+08/1.54e+10 =  0% of the original kernel matrix.

torch.Size([74090, 2])
We keep 2.97e+07/1.89e+09 =  1% of the original kernel matrix.

torch.Size([64310, 2])
We keep 1.30e+07/1.36e+09 =  0% of the original kernel matrix.

torch.Size([22110, 2])
We keep 1.42e+07/2.55e+08 =  5% of the original kernel matrix.

torch.Size([34013, 2])
We keep 5.93e+06/4.99e+08 =  1% of the original kernel matrix.

torch.Size([6701, 2])
We keep 3.42e+05/7.80e+06 =  4% of the original kernel matrix.

torch.Size([20465, 2])
We keep 1.66e+06/8.73e+07 =  1% of the original kernel matrix.

torch.Size([82602, 2])
We keep 3.53e+07/2.62e+09 =  1% of the original kernel matrix.

torch.Size([67245, 2])
We keep 1.48e+07/1.60e+09 =  0% of the original kernel matrix.

torch.Size([6614, 2])
We keep 2.88e+05/7.57e+06 =  3% of the original kernel matrix.

torch.Size([20254, 2])
We keep 1.63e+06/8.60e+07 =  1% of the original kernel matrix.

torch.Size([44289, 2])
We keep 4.56e+07/1.24e+09 =  3% of the original kernel matrix.

torch.Size([48486, 2])
We keep 1.12e+07/1.10e+09 =  1% of the original kernel matrix.

torch.Size([11817, 2])
We keep 8.41e+05/2.75e+07 =  3% of the original kernel matrix.

torch.Size([25970, 2])
We keep 2.62e+06/1.64e+08 =  1% of the original kernel matrix.

torch.Size([28853, 2])
We keep 3.49e+06/2.15e+08 =  1% of the original kernel matrix.

torch.Size([40435, 2])
We keep 5.52e+06/4.58e+08 =  1% of the original kernel matrix.

torch.Size([24960, 2])
We keep 4.76e+06/1.99e+08 =  2% of the original kernel matrix.

torch.Size([37266, 2])
We keep 5.36e+06/4.41e+08 =  1% of the original kernel matrix.

torch.Size([12228, 2])
We keep 7.13e+05/2.57e+07 =  2% of the original kernel matrix.

torch.Size([26384, 2])
We keep 2.51e+06/1.58e+08 =  1% of the original kernel matrix.

torch.Size([8586, 2])
We keep 3.60e+05/1.12e+07 =  3% of the original kernel matrix.

torch.Size([22606, 2])
We keep 1.86e+06/1.05e+08 =  1% of the original kernel matrix.

torch.Size([304368, 2])
We keep 2.28e+08/3.29e+10 =  0% of the original kernel matrix.

torch.Size([133315, 2])
We keep 4.37e+07/5.67e+09 =  0% of the original kernel matrix.

torch.Size([646467, 2])
We keep 1.08e+09/1.38e+11 =  0% of the original kernel matrix.

torch.Size([195774, 2])
We keep 8.34e+07/1.16e+10 =  0% of the original kernel matrix.

torch.Size([224261, 2])
We keep 2.41e+08/2.22e+10 =  1% of the original kernel matrix.

torch.Size([112251, 2])
We keep 3.73e+07/4.66e+09 =  0% of the original kernel matrix.

torch.Size([38320, 2])
We keep 3.74e+07/6.76e+08 =  5% of the original kernel matrix.

torch.Size([45879, 2])
We keep 8.55e+06/8.12e+08 =  1% of the original kernel matrix.

torch.Size([47311, 2])
We keep 1.17e+07/7.67e+08 =  1% of the original kernel matrix.

torch.Size([52401, 2])
We keep 9.23e+06/8.66e+08 =  1% of the original kernel matrix.

torch.Size([23083, 2])
We keep 5.18e+06/1.65e+08 =  3% of the original kernel matrix.

torch.Size([35679, 2])
We keep 5.00e+06/4.01e+08 =  1% of the original kernel matrix.

torch.Size([23191, 2])
We keep 3.14e+06/1.50e+08 =  2% of the original kernel matrix.

torch.Size([36137, 2])
We keep 4.84e+06/3.82e+08 =  1% of the original kernel matrix.

torch.Size([200940, 2])
We keep 9.81e+07/1.36e+10 =  0% of the original kernel matrix.

torch.Size([106870, 2])
We keep 2.99e+07/3.64e+09 =  0% of the original kernel matrix.

torch.Size([83125, 2])
We keep 1.36e+08/3.03e+09 =  4% of the original kernel matrix.

torch.Size([68810, 2])
We keep 1.58e+07/1.72e+09 =  0% of the original kernel matrix.

torch.Size([83995, 2])
We keep 1.55e+08/3.46e+09 =  4% of the original kernel matrix.

torch.Size([67498, 2])
We keep 1.67e+07/1.84e+09 =  0% of the original kernel matrix.

torch.Size([21441, 2])
We keep 3.02e+06/1.21e+08 =  2% of the original kernel matrix.

torch.Size([34372, 2])
We keep 4.44e+06/3.43e+08 =  1% of the original kernel matrix.

torch.Size([673199, 2])
We keep 2.67e+09/2.34e+11 =  1% of the original kernel matrix.

torch.Size([199178, 2])
We keep 1.06e+08/1.51e+10 =  0% of the original kernel matrix.

torch.Size([38840, 2])
We keep 1.40e+07/5.36e+08 =  2% of the original kernel matrix.

torch.Size([47176, 2])
We keep 7.91e+06/7.24e+08 =  1% of the original kernel matrix.

torch.Size([75960, 2])
We keep 3.04e+07/2.33e+09 =  1% of the original kernel matrix.

torch.Size([65960, 2])
We keep 1.45e+07/1.51e+09 =  0% of the original kernel matrix.

torch.Size([202143, 2])
We keep 2.17e+08/1.61e+10 =  1% of the original kernel matrix.

torch.Size([107368, 2])
We keep 3.26e+07/3.97e+09 =  0% of the original kernel matrix.

torch.Size([95926, 2])
We keep 3.84e+07/3.32e+09 =  1% of the original kernel matrix.

torch.Size([73283, 2])
We keep 1.67e+07/1.80e+09 =  0% of the original kernel matrix.

torch.Size([179967, 2])
We keep 6.67e+08/2.17e+10 =  3% of the original kernel matrix.

torch.Size([99107, 2])
We keep 3.59e+07/4.61e+09 =  0% of the original kernel matrix.

torch.Size([336970, 2])
We keep 6.52e+08/5.00e+10 =  1% of the original kernel matrix.

torch.Size([140692, 2])
We keep 5.23e+07/6.99e+09 =  0% of the original kernel matrix.

torch.Size([165109, 2])
We keep 8.44e+07/1.00e+10 =  0% of the original kernel matrix.

torch.Size([95588, 2])
We keep 2.63e+07/3.13e+09 =  0% of the original kernel matrix.

torch.Size([61671, 2])
We keep 2.44e+07/1.35e+09 =  1% of the original kernel matrix.

torch.Size([59160, 2])
We keep 1.14e+07/1.15e+09 =  0% of the original kernel matrix.

torch.Size([46710, 2])
We keep 1.77e+07/9.40e+08 =  1% of the original kernel matrix.

torch.Size([50623, 2])
We keep 9.93e+06/9.58e+08 =  1% of the original kernel matrix.

torch.Size([94525, 2])
We keep 4.84e+07/3.27e+09 =  1% of the original kernel matrix.

torch.Size([71723, 2])
We keep 1.63e+07/1.79e+09 =  0% of the original kernel matrix.

torch.Size([17590, 2])
We keep 2.25e+06/7.08e+07 =  3% of the original kernel matrix.

torch.Size([31340, 2])
We keep 3.65e+06/2.63e+08 =  1% of the original kernel matrix.

torch.Size([219278, 2])
We keep 1.76e+08/1.63e+10 =  1% of the original kernel matrix.

torch.Size([111505, 2])
We keep 3.21e+07/3.99e+09 =  0% of the original kernel matrix.

torch.Size([1171168, 2])
We keep 2.94e+09/5.09e+11 =  0% of the original kernel matrix.

torch.Size([276598, 2])
We keep 1.51e+08/2.23e+10 =  0% of the original kernel matrix.

torch.Size([229049, 2])
We keep 4.51e+08/2.55e+10 =  1% of the original kernel matrix.

torch.Size([115307, 2])
We keep 3.95e+07/5.00e+09 =  0% of the original kernel matrix.

torch.Size([118519, 2])
We keep 1.60e+08/8.99e+09 =  1% of the original kernel matrix.

torch.Size([78817, 2])
We keep 2.56e+07/2.96e+09 =  0% of the original kernel matrix.

torch.Size([44252, 2])
We keep 2.20e+07/8.18e+08 =  2% of the original kernel matrix.

torch.Size([47534, 2])
We keep 8.60e+06/8.94e+08 =  0% of the original kernel matrix.

torch.Size([28701, 2])
We keep 3.07e+07/5.36e+08 =  5% of the original kernel matrix.

torch.Size([39225, 2])
We keep 7.94e+06/7.24e+08 =  1% of the original kernel matrix.

torch.Size([51972, 2])
We keep 1.25e+07/9.60e+08 =  1% of the original kernel matrix.

torch.Size([52156, 2])
We keep 9.33e+06/9.68e+08 =  0% of the original kernel matrix.

torch.Size([34528, 2])
We keep 1.17e+07/5.44e+08 =  2% of the original kernel matrix.

torch.Size([43424, 2])
We keep 7.95e+06/7.29e+08 =  1% of the original kernel matrix.

torch.Size([95866, 2])
We keep 7.51e+07/4.06e+09 =  1% of the original kernel matrix.

torch.Size([72424, 2])
We keep 1.83e+07/1.99e+09 =  0% of the original kernel matrix.

torch.Size([500632, 2])
We keep 4.79e+08/9.04e+10 =  0% of the original kernel matrix.

torch.Size([173559, 2])
We keep 6.86e+07/9.39e+09 =  0% of the original kernel matrix.

torch.Size([16216, 2])
We keep 8.54e+06/1.18e+08 =  7% of the original kernel matrix.

torch.Size([29605, 2])
We keep 4.31e+06/3.39e+08 =  1% of the original kernel matrix.

torch.Size([101781, 2])
We keep 3.37e+08/9.86e+09 =  3% of the original kernel matrix.

torch.Size([71352, 2])
We keep 2.59e+07/3.10e+09 =  0% of the original kernel matrix.

torch.Size([163367, 2])
We keep 4.07e+08/1.46e+10 =  2% of the original kernel matrix.

torch.Size([95129, 2])
We keep 3.15e+07/3.77e+09 =  0% of the original kernel matrix.

torch.Size([65610, 2])
We keep 1.80e+07/1.52e+09 =  1% of the original kernel matrix.

torch.Size([60571, 2])
We keep 1.19e+07/1.22e+09 =  0% of the original kernel matrix.

torch.Size([7574, 2])
We keep 3.57e+05/9.76e+06 =  3% of the original kernel matrix.

torch.Size([21460, 2])
We keep 1.78e+06/9.76e+07 =  1% of the original kernel matrix.

torch.Size([183417, 2])
We keep 1.48e+08/1.28e+10 =  1% of the original kernel matrix.

torch.Size([101453, 2])
We keep 2.90e+07/3.53e+09 =  0% of the original kernel matrix.

torch.Size([240861, 2])
We keep 1.41e+08/1.93e+10 =  0% of the original kernel matrix.

torch.Size([117223, 2])
We keep 3.47e+07/4.34e+09 =  0% of the original kernel matrix.

torch.Size([8935, 2])
We keep 5.22e+05/1.52e+07 =  3% of the original kernel matrix.

torch.Size([22876, 2])
We keep 2.08e+06/1.22e+08 =  1% of the original kernel matrix.

torch.Size([1260896, 2])
We keep 2.71e+09/5.66e+11 =  0% of the original kernel matrix.

torch.Size([287157, 2])
We keep 1.57e+08/2.35e+10 =  0% of the original kernel matrix.

torch.Size([48798, 2])
We keep 2.20e+07/1.03e+09 =  2% of the original kernel matrix.

torch.Size([52497, 2])
We keep 1.01e+07/1.01e+09 =  1% of the original kernel matrix.

torch.Size([50366, 2])
We keep 1.64e+07/9.53e+08 =  1% of the original kernel matrix.

torch.Size([54044, 2])
We keep 1.01e+07/9.65e+08 =  1% of the original kernel matrix.

torch.Size([25562, 2])
We keep 1.10e+07/2.12e+08 =  5% of the original kernel matrix.

torch.Size([37618, 2])
We keep 5.47e+06/4.55e+08 =  1% of the original kernel matrix.

torch.Size([23588, 2])
We keep 2.71e+06/1.46e+08 =  1% of the original kernel matrix.

torch.Size([35985, 2])
We keep 4.75e+06/3.77e+08 =  1% of the original kernel matrix.

torch.Size([9261, 2])
We keep 9.43e+05/1.85e+07 =  5% of the original kernel matrix.

torch.Size([23108, 2])
We keep 2.22e+06/1.34e+08 =  1% of the original kernel matrix.

torch.Size([13427, 2])
We keep 2.52e+06/4.85e+07 =  5% of the original kernel matrix.

torch.Size([27430, 2])
We keep 3.17e+06/2.18e+08 =  1% of the original kernel matrix.

torch.Size([33210, 2])
We keep 1.29e+07/3.67e+08 =  3% of the original kernel matrix.

torch.Size([43154, 2])
We keep 6.79e+06/5.99e+08 =  1% of the original kernel matrix.

torch.Size([167765, 2])
We keep 2.14e+08/1.47e+10 =  1% of the original kernel matrix.

torch.Size([95202, 2])
We keep 3.06e+07/3.79e+09 =  0% of the original kernel matrix.

torch.Size([1910898, 2])
We keep 3.61e+10/3.05e+12 =  1% of the original kernel matrix.

torch.Size([314121, 2])
We keep 3.46e+08/5.46e+10 =  0% of the original kernel matrix.

torch.Size([18249, 2])
We keep 5.76e+06/1.16e+08 =  4% of the original kernel matrix.

torch.Size([31772, 2])
We keep 4.39e+06/3.36e+08 =  1% of the original kernel matrix.

torch.Size([172807, 2])
We keep 8.54e+07/1.01e+10 =  0% of the original kernel matrix.

torch.Size([97565, 2])
We keep 2.64e+07/3.14e+09 =  0% of the original kernel matrix.

torch.Size([10858, 2])
We keep 7.49e+05/2.30e+07 =  3% of the original kernel matrix.

torch.Size([25033, 2])
We keep 2.42e+06/1.50e+08 =  1% of the original kernel matrix.

torch.Size([39424, 2])
We keep 7.84e+06/5.42e+08 =  1% of the original kernel matrix.

torch.Size([47598, 2])
We keep 7.91e+06/7.28e+08 =  1% of the original kernel matrix.

torch.Size([14733, 2])
We keep 1.56e+06/5.11e+07 =  3% of the original kernel matrix.

torch.Size([28433, 2])
We keep 3.24e+06/2.23e+08 =  1% of the original kernel matrix.

torch.Size([154542, 2])
We keep 9.64e+08/3.17e+10 =  3% of the original kernel matrix.

torch.Size([87350, 2])
We keep 4.39e+07/5.56e+09 =  0% of the original kernel matrix.

torch.Size([42205, 2])
We keep 2.93e+07/8.02e+08 =  3% of the original kernel matrix.

torch.Size([46598, 2])
We keep 8.77e+06/8.85e+08 =  0% of the original kernel matrix.

torch.Size([1846437, 2])
We keep 4.69e+09/1.01e+12 =  0% of the original kernel matrix.

torch.Size([340036, 2])
We keep 2.05e+08/3.14e+10 =  0% of the original kernel matrix.

torch.Size([16241, 2])
We keep 2.74e+06/6.62e+07 =  4% of the original kernel matrix.

torch.Size([29952, 2])
We keep 3.62e+06/2.54e+08 =  1% of the original kernel matrix.

torch.Size([244910, 2])
We keep 2.62e+08/2.21e+10 =  1% of the original kernel matrix.

torch.Size([118548, 2])
We keep 3.74e+07/4.64e+09 =  0% of the original kernel matrix.

torch.Size([46220, 2])
We keep 2.58e+07/8.08e+08 =  3% of the original kernel matrix.

torch.Size([51997, 2])
We keep 9.43e+06/8.89e+08 =  1% of the original kernel matrix.

torch.Size([237274, 2])
We keep 2.03e+08/2.35e+10 =  0% of the original kernel matrix.

torch.Size([115855, 2])
We keep 3.82e+07/4.79e+09 =  0% of the original kernel matrix.

torch.Size([372614, 2])
We keep 6.83e+08/6.80e+10 =  1% of the original kernel matrix.

torch.Size([147393, 2])
We keep 6.03e+07/8.15e+09 =  0% of the original kernel matrix.

torch.Size([42013, 2])
We keep 8.97e+06/5.77e+08 =  1% of the original kernel matrix.

torch.Size([49004, 2])
We keep 8.11e+06/7.51e+08 =  1% of the original kernel matrix.

torch.Size([17932, 2])
We keep 1.71e+06/6.83e+07 =  2% of the original kernel matrix.

torch.Size([31503, 2])
We keep 3.61e+06/2.58e+08 =  1% of the original kernel matrix.

torch.Size([139495, 2])
We keep 5.90e+07/6.61e+09 =  0% of the original kernel matrix.

torch.Size([87239, 2])
We keep 2.22e+07/2.54e+09 =  0% of the original kernel matrix.

torch.Size([21395, 2])
We keep 4.77e+06/1.20e+08 =  3% of the original kernel matrix.

torch.Size([34508, 2])
We keep 4.48e+06/3.42e+08 =  1% of the original kernel matrix.

torch.Size([61824, 2])
We keep 6.80e+07/2.23e+09 =  3% of the original kernel matrix.

torch.Size([56618, 2])
We keep 1.42e+07/1.48e+09 =  0% of the original kernel matrix.

torch.Size([217749, 2])
We keep 1.58e+08/1.80e+10 =  0% of the original kernel matrix.

torch.Size([110020, 2])
We keep 3.35e+07/4.19e+09 =  0% of the original kernel matrix.

torch.Size([26803, 2])
We keep 1.68e+07/3.89e+08 =  4% of the original kernel matrix.

torch.Size([38401, 2])
We keep 7.01e+06/6.16e+08 =  1% of the original kernel matrix.

torch.Size([126160, 2])
We keep 7.15e+07/5.25e+09 =  1% of the original kernel matrix.

torch.Size([85148, 2])
We keep 2.02e+07/2.26e+09 =  0% of the original kernel matrix.

torch.Size([50098, 2])
We keep 1.75e+07/1.03e+09 =  1% of the original kernel matrix.

torch.Size([52490, 2])
We keep 1.02e+07/1.00e+09 =  1% of the original kernel matrix.

torch.Size([202948, 2])
We keep 2.44e+08/1.78e+10 =  1% of the original kernel matrix.

torch.Size([106759, 2])
We keep 3.41e+07/4.17e+09 =  0% of the original kernel matrix.

torch.Size([487862, 2])
We keep 1.16e+09/1.10e+11 =  1% of the original kernel matrix.

torch.Size([166495, 2])
We keep 7.50e+07/1.04e+10 =  0% of the original kernel matrix.

torch.Size([80643, 2])
We keep 7.23e+07/2.41e+09 =  3% of the original kernel matrix.

torch.Size([66748, 2])
We keep 1.40e+07/1.53e+09 =  0% of the original kernel matrix.

torch.Size([24850, 2])
We keep 4.30e+06/1.62e+08 =  2% of the original kernel matrix.

torch.Size([37252, 2])
We keep 5.03e+06/3.98e+08 =  1% of the original kernel matrix.

torch.Size([206643, 2])
We keep 4.31e+08/2.73e+10 =  1% of the original kernel matrix.

torch.Size([105821, 2])
We keep 4.09e+07/5.16e+09 =  0% of the original kernel matrix.

torch.Size([79333, 2])
We keep 3.39e+07/2.52e+09 =  1% of the original kernel matrix.

torch.Size([65378, 2])
We keep 1.45e+07/1.57e+09 =  0% of the original kernel matrix.

torch.Size([126349, 2])
We keep 1.32e+08/6.13e+09 =  2% of the original kernel matrix.

torch.Size([82465, 2])
We keep 2.11e+07/2.45e+09 =  0% of the original kernel matrix.

torch.Size([74961, 2])
We keep 5.13e+07/2.59e+09 =  1% of the original kernel matrix.

torch.Size([63881, 2])
We keep 1.50e+07/1.59e+09 =  0% of the original kernel matrix.

torch.Size([98219, 2])
We keep 9.38e+07/5.56e+09 =  1% of the original kernel matrix.

torch.Size([71541, 2])
We keep 2.09e+07/2.33e+09 =  0% of the original kernel matrix.

torch.Size([64044, 2])
We keep 4.37e+07/2.07e+09 =  2% of the original kernel matrix.

torch.Size([58953, 2])
We keep 1.38e+07/1.42e+09 =  0% of the original kernel matrix.

torch.Size([97985, 2])
We keep 4.81e+08/1.09e+10 =  4% of the original kernel matrix.

torch.Size([68261, 2])
We keep 2.70e+07/3.26e+09 =  0% of the original kernel matrix.

torch.Size([68132, 2])
We keep 4.46e+07/2.23e+09 =  2% of the original kernel matrix.

torch.Size([61254, 2])
We keep 1.39e+07/1.47e+09 =  0% of the original kernel matrix.

torch.Size([39482, 2])
We keep 8.17e+06/5.44e+08 =  1% of the original kernel matrix.

torch.Size([48014, 2])
We keep 8.02e+06/7.29e+08 =  1% of the original kernel matrix.

torch.Size([13288, 2])
We keep 9.14e+05/3.35e+07 =  2% of the original kernel matrix.

torch.Size([27373, 2])
We keep 2.78e+06/1.81e+08 =  1% of the original kernel matrix.

torch.Size([33950, 2])
We keep 6.63e+06/3.64e+08 =  1% of the original kernel matrix.

torch.Size([44703, 2])
We keep 6.91e+06/5.96e+08 =  1% of the original kernel matrix.

torch.Size([89613, 2])
We keep 1.11e+08/4.18e+09 =  2% of the original kernel matrix.

torch.Size([68985, 2])
We keep 1.84e+07/2.02e+09 =  0% of the original kernel matrix.

torch.Size([136864, 2])
We keep 1.60e+08/7.47e+09 =  2% of the original kernel matrix.

torch.Size([85591, 2])
We keep 2.34e+07/2.70e+09 =  0% of the original kernel matrix.

torch.Size([184676, 2])
We keep 1.55e+08/1.19e+10 =  1% of the original kernel matrix.

torch.Size([101473, 2])
We keep 2.81e+07/3.41e+09 =  0% of the original kernel matrix.

torch.Size([18019, 2])
We keep 1.75e+06/7.41e+07 =  2% of the original kernel matrix.

torch.Size([31296, 2])
We keep 3.70e+06/2.69e+08 =  1% of the original kernel matrix.

torch.Size([55557, 2])
We keep 6.20e+07/1.27e+09 =  4% of the original kernel matrix.

torch.Size([55889, 2])
We keep 1.10e+07/1.11e+09 =  0% of the original kernel matrix.

torch.Size([93565, 2])
We keep 5.31e+07/3.61e+09 =  1% of the original kernel matrix.

torch.Size([71623, 2])
We keep 1.73e+07/1.88e+09 =  0% of the original kernel matrix.

torch.Size([395544, 2])
We keep 1.23e+09/6.79e+10 =  1% of the original kernel matrix.

torch.Size([153110, 2])
We keep 5.87e+07/8.15e+09 =  0% of the original kernel matrix.

torch.Size([77443, 2])
We keep 3.48e+07/2.40e+09 =  1% of the original kernel matrix.

torch.Size([65657, 2])
We keep 1.45e+07/1.53e+09 =  0% of the original kernel matrix.

torch.Size([119287, 2])
We keep 4.73e+07/4.56e+09 =  1% of the original kernel matrix.

torch.Size([80514, 2])
We keep 1.88e+07/2.11e+09 =  0% of the original kernel matrix.

torch.Size([37486, 2])
We keep 6.73e+07/1.08e+09 =  6% of the original kernel matrix.

torch.Size([44895, 2])
We keep 1.05e+07/1.03e+09 =  1% of the original kernel matrix.

torch.Size([41750, 2])
We keep 9.86e+06/5.75e+08 =  1% of the original kernel matrix.

torch.Size([49016, 2])
We keep 8.10e+06/7.49e+08 =  1% of the original kernel matrix.

torch.Size([6365, 2])
We keep 2.75e+05/7.11e+06 =  3% of the original kernel matrix.

torch.Size([20134, 2])
We keep 1.62e+06/8.34e+07 =  1% of the original kernel matrix.

torch.Size([20473, 2])
We keep 2.45e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([33570, 2])
We keep 4.18e+06/3.18e+08 =  1% of the original kernel matrix.

torch.Size([282388, 2])
We keep 2.38e+08/2.79e+10 =  0% of the original kernel matrix.

torch.Size([127646, 2])
We keep 4.05e+07/5.22e+09 =  0% of the original kernel matrix.

torch.Size([72630, 2])
We keep 4.78e+07/2.61e+09 =  1% of the original kernel matrix.

torch.Size([62223, 2])
We keep 1.46e+07/1.60e+09 =  0% of the original kernel matrix.

torch.Size([125092, 2])
We keep 5.33e+07/5.00e+09 =  1% of the original kernel matrix.

torch.Size([82360, 2])
We keep 1.96e+07/2.21e+09 =  0% of the original kernel matrix.

torch.Size([4812, 2])
We keep 1.38e+05/3.28e+06 =  4% of the original kernel matrix.

torch.Size([18193, 2])
We keep 1.24e+06/5.66e+07 =  2% of the original kernel matrix.

torch.Size([12164, 2])
We keep 1.11e+06/2.93e+07 =  3% of the original kernel matrix.

torch.Size([26261, 2])
We keep 2.64e+06/1.69e+08 =  1% of the original kernel matrix.

torch.Size([313285, 2])
We keep 4.71e+08/4.21e+10 =  1% of the original kernel matrix.

torch.Size([134589, 2])
We keep 4.87e+07/6.41e+09 =  0% of the original kernel matrix.

torch.Size([165711, 2])
We keep 1.39e+08/1.05e+10 =  1% of the original kernel matrix.

torch.Size([95160, 2])
We keep 2.69e+07/3.20e+09 =  0% of the original kernel matrix.

torch.Size([75416, 2])
We keep 4.07e+07/2.10e+09 =  1% of the original kernel matrix.

torch.Size([65196, 2])
We keep 1.35e+07/1.43e+09 =  0% of the original kernel matrix.

torch.Size([9574, 2])
We keep 5.55e+05/1.58e+07 =  3% of the original kernel matrix.

torch.Size([23744, 2])
We keep 2.12e+06/1.24e+08 =  1% of the original kernel matrix.

torch.Size([8306, 2])
We keep 4.15e+05/1.14e+07 =  3% of the original kernel matrix.

torch.Size([22253, 2])
We keep 1.88e+06/1.06e+08 =  1% of the original kernel matrix.

torch.Size([62523, 2])
We keep 1.95e+07/1.39e+09 =  1% of the original kernel matrix.

torch.Size([59899, 2])
We keep 1.16e+07/1.17e+09 =  0% of the original kernel matrix.

torch.Size([83756, 2])
We keep 3.91e+07/2.76e+09 =  1% of the original kernel matrix.

torch.Size([68263, 2])
We keep 1.54e+07/1.64e+09 =  0% of the original kernel matrix.

torch.Size([141248, 2])
We keep 1.71e+08/9.71e+09 =  1% of the original kernel matrix.

torch.Size([87469, 2])
We keep 2.56e+07/3.08e+09 =  0% of the original kernel matrix.

torch.Size([9089, 2])
We keep 8.89e+05/1.95e+07 =  4% of the original kernel matrix.

torch.Size([23254, 2])
We keep 2.30e+06/1.38e+08 =  1% of the original kernel matrix.

torch.Size([39177, 2])
We keep 6.53e+07/9.25e+08 =  7% of the original kernel matrix.

torch.Size([45937, 2])
We keep 9.92e+06/9.51e+08 =  1% of the original kernel matrix.

torch.Size([25958, 2])
We keep 4.71e+06/2.19e+08 =  2% of the original kernel matrix.

torch.Size([37825, 2])
We keep 5.55e+06/4.63e+08 =  1% of the original kernel matrix.

torch.Size([29813, 2])
We keep 1.23e+07/3.56e+08 =  3% of the original kernel matrix.

torch.Size([40231, 2])
We keep 6.77e+06/5.89e+08 =  1% of the original kernel matrix.

torch.Size([540223, 2])
We keep 1.02e+09/1.21e+11 =  0% of the original kernel matrix.

torch.Size([178821, 2])
We keep 7.87e+07/1.09e+10 =  0% of the original kernel matrix.

torch.Size([23132, 2])
We keep 1.79e+07/2.27e+08 =  7% of the original kernel matrix.

torch.Size([35792, 2])
We keep 5.80e+06/4.71e+08 =  1% of the original kernel matrix.

torch.Size([101373, 2])
We keep 7.68e+07/4.24e+09 =  1% of the original kernel matrix.

torch.Size([74008, 2])
We keep 1.82e+07/2.04e+09 =  0% of the original kernel matrix.

torch.Size([233320, 2])
We keep 1.90e+08/1.86e+10 =  1% of the original kernel matrix.

torch.Size([115445, 2])
We keep 3.44e+07/4.27e+09 =  0% of the original kernel matrix.

torch.Size([86563, 2])
We keep 3.92e+07/2.79e+09 =  1% of the original kernel matrix.

torch.Size([69444, 2])
We keep 1.55e+07/1.65e+09 =  0% of the original kernel matrix.

torch.Size([115773, 2])
We keep 7.69e+07/5.02e+09 =  1% of the original kernel matrix.

torch.Size([80171, 2])
We keep 1.99e+07/2.22e+09 =  0% of the original kernel matrix.

torch.Size([222864, 2])
We keep 1.72e+08/1.78e+10 =  0% of the original kernel matrix.

torch.Size([112688, 2])
We keep 3.37e+07/4.17e+09 =  0% of the original kernel matrix.

torch.Size([6627, 2])
We keep 2.62e+05/6.87e+06 =  3% of the original kernel matrix.

torch.Size([20374, 2])
We keep 1.56e+06/8.19e+07 =  1% of the original kernel matrix.

torch.Size([72438, 2])
We keep 2.49e+07/1.78e+09 =  1% of the original kernel matrix.

torch.Size([64128, 2])
We keep 1.28e+07/1.32e+09 =  0% of the original kernel matrix.

torch.Size([51770, 2])
We keep 1.73e+07/9.42e+08 =  1% of the original kernel matrix.

torch.Size([55292, 2])
We keep 9.86e+06/9.59e+08 =  1% of the original kernel matrix.

torch.Size([277793, 2])
We keep 2.75e+08/2.79e+10 =  0% of the original kernel matrix.

torch.Size([125879, 2])
We keep 4.09e+07/5.22e+09 =  0% of the original kernel matrix.

torch.Size([208962, 2])
We keep 2.07e+08/1.85e+10 =  1% of the original kernel matrix.

torch.Size([108341, 2])
We keep 3.41e+07/4.26e+09 =  0% of the original kernel matrix.

torch.Size([76176, 2])
We keep 4.70e+07/2.76e+09 =  1% of the original kernel matrix.

torch.Size([63086, 2])
We keep 1.54e+07/1.64e+09 =  0% of the original kernel matrix.

torch.Size([102560, 2])
We keep 3.59e+07/3.60e+09 =  0% of the original kernel matrix.

torch.Size([75294, 2])
We keep 1.73e+07/1.88e+09 =  0% of the original kernel matrix.

torch.Size([26468, 2])
We keep 3.25e+06/1.87e+08 =  1% of the original kernel matrix.

torch.Size([38501, 2])
We keep 5.24e+06/4.27e+08 =  1% of the original kernel matrix.

torch.Size([12985, 2])
We keep 6.72e+06/1.04e+08 =  6% of the original kernel matrix.

torch.Size([26054, 2])
We keep 4.25e+06/3.19e+08 =  1% of the original kernel matrix.

torch.Size([27187, 2])
We keep 1.92e+07/2.48e+08 =  7% of the original kernel matrix.

torch.Size([38766, 2])
We keep 5.72e+06/4.92e+08 =  1% of the original kernel matrix.

torch.Size([20952, 2])
We keep 2.82e+06/1.35e+08 =  2% of the original kernel matrix.

torch.Size([33640, 2])
We keep 4.63e+06/3.63e+08 =  1% of the original kernel matrix.

torch.Size([311309, 2])
We keep 3.74e+08/3.89e+10 =  0% of the original kernel matrix.

torch.Size([135225, 2])
We keep 4.74e+07/6.16e+09 =  0% of the original kernel matrix.

torch.Size([281572, 2])
We keep 4.64e+08/3.65e+10 =  1% of the original kernel matrix.

torch.Size([128828, 2])
We keep 4.66e+07/5.97e+09 =  0% of the original kernel matrix.

torch.Size([15163, 2])
We keep 2.02e+06/5.78e+07 =  3% of the original kernel matrix.

torch.Size([28945, 2])
We keep 3.35e+06/2.38e+08 =  1% of the original kernel matrix.

torch.Size([37010, 2])
We keep 7.84e+06/4.39e+08 =  1% of the original kernel matrix.

torch.Size([46002, 2])
We keep 7.28e+06/6.55e+08 =  1% of the original kernel matrix.

torch.Size([26220, 2])
We keep 3.02e+06/1.71e+08 =  1% of the original kernel matrix.

torch.Size([38348, 2])
We keep 5.06e+06/4.09e+08 =  1% of the original kernel matrix.

torch.Size([699829, 2])
We keep 1.79e+09/2.03e+11 =  0% of the original kernel matrix.

torch.Size([203417, 2])
We keep 9.90e+07/1.41e+10 =  0% of the original kernel matrix.

torch.Size([39996, 2])
We keep 7.97e+06/5.37e+08 =  1% of the original kernel matrix.

torch.Size([48375, 2])
We keep 7.87e+06/7.24e+08 =  1% of the original kernel matrix.

torch.Size([35948, 2])
We keep 7.62e+06/4.51e+08 =  1% of the original kernel matrix.

torch.Size([45038, 2])
We keep 7.23e+06/6.64e+08 =  1% of the original kernel matrix.

torch.Size([145272, 2])
We keep 1.34e+08/8.42e+09 =  1% of the original kernel matrix.

torch.Size([89105, 2])
We keep 2.45e+07/2.87e+09 =  0% of the original kernel matrix.

torch.Size([157549, 2])
We keep 7.71e+07/8.29e+09 =  0% of the original kernel matrix.

torch.Size([92706, 2])
We keep 2.44e+07/2.85e+09 =  0% of the original kernel matrix.

torch.Size([21204, 2])
We keep 3.47e+06/1.37e+08 =  2% of the original kernel matrix.

torch.Size([33991, 2])
We keep 4.64e+06/3.65e+08 =  1% of the original kernel matrix.

torch.Size([61513, 2])
We keep 2.57e+07/1.44e+09 =  1% of the original kernel matrix.

torch.Size([59142, 2])
We keep 1.18e+07/1.18e+09 =  0% of the original kernel matrix.

torch.Size([20302, 2])
We keep 5.58e+06/1.24e+08 =  4% of the original kernel matrix.

torch.Size([33356, 2])
We keep 4.34e+06/3.48e+08 =  1% of the original kernel matrix.

time for making ranges is 5.959513187408447
Sorting X and nu_X
time for sorting X is 0.09130287170410156
Sorting Z and nu_Z
time for sorting Z is 0.0002434253692626953
Starting Optim
sum tnu_Z before tensor(43254112., device='cuda:0')
c= tensor(1057.0842, device='cuda:0')
c= tensor(133237.3281, device='cuda:0')
c= tensor(140896.9219, device='cuda:0')
c= tensor(146551.1094, device='cuda:0')
c= tensor(1962816., device='cuda:0')
c= tensor(2391131.7500, device='cuda:0')
c= tensor(3546683., device='cuda:0')
c= tensor(4052200.5000, device='cuda:0')
c= tensor(4164014.5000, device='cuda:0')
c= tensor(10063810., device='cuda:0')
c= tensor(10087060., device='cuda:0')
c= tensor(15185586., device='cuda:0')
c= tensor(15205295., device='cuda:0')
c= tensor(30119488., device='cuda:0')
c= tensor(30828332., device='cuda:0')
c= tensor(31437178., device='cuda:0')
c= tensor(32831296., device='cuda:0')
c= tensor(35213196., device='cuda:0')
c= tensor(44292064., device='cuda:0')
c= tensor(49219720., device='cuda:0')
c= tensor(49293480., device='cuda:0')
c= tensor(81726416., device='cuda:0')
c= tensor(81811760., device='cuda:0')
c= tensor(82018144., device='cuda:0')
c= tensor(89880136., device='cuda:0')
c= tensor(91241880., device='cuda:0')
c= tensor(92427176., device='cuda:0')
c= tensor(94354784., device='cuda:0')
c= tensor(95538064., device='cuda:0')
c= tensor(5.9621e+08, device='cuda:0')
c= tensor(5.9624e+08, device='cuda:0')
c= tensor(6.8451e+08, device='cuda:0')
c= tensor(6.8517e+08, device='cuda:0')
c= tensor(6.8521e+08, device='cuda:0')
c= tensor(6.8523e+08, device='cuda:0')
c= tensor(6.8837e+08, device='cuda:0')
c= tensor(6.8969e+08, device='cuda:0')
c= tensor(6.8969e+08, device='cuda:0')
c= tensor(6.8971e+08, device='cuda:0')
c= tensor(6.8971e+08, device='cuda:0')
c= tensor(6.8973e+08, device='cuda:0')
c= tensor(6.8973e+08, device='cuda:0')
c= tensor(6.8973e+08, device='cuda:0')
c= tensor(6.8975e+08, device='cuda:0')
c= tensor(6.8975e+08, device='cuda:0')
c= tensor(6.8975e+08, device='cuda:0')
c= tensor(6.8976e+08, device='cuda:0')
c= tensor(6.8977e+08, device='cuda:0')
c= tensor(6.8978e+08, device='cuda:0')
c= tensor(6.8981e+08, device='cuda:0')
c= tensor(6.8989e+08, device='cuda:0')
c= tensor(6.8989e+08, device='cuda:0')
c= tensor(6.8990e+08, device='cuda:0')
c= tensor(6.8990e+08, device='cuda:0')
c= tensor(6.8993e+08, device='cuda:0')
c= tensor(6.8993e+08, device='cuda:0')
c= tensor(6.8994e+08, device='cuda:0')
c= tensor(6.8994e+08, device='cuda:0')
c= tensor(6.8995e+08, device='cuda:0')
c= tensor(6.8995e+08, device='cuda:0')
c= tensor(6.8999e+08, device='cuda:0')
c= tensor(6.8999e+08, device='cuda:0')
c= tensor(6.9003e+08, device='cuda:0')
c= tensor(6.9005e+08, device='cuda:0')
c= tensor(6.9006e+08, device='cuda:0')
c= tensor(6.9006e+08, device='cuda:0')
c= tensor(6.9006e+08, device='cuda:0')
c= tensor(6.9008e+08, device='cuda:0')
c= tensor(6.9009e+08, device='cuda:0')
c= tensor(6.9009e+08, device='cuda:0')
c= tensor(6.9010e+08, device='cuda:0')
c= tensor(6.9011e+08, device='cuda:0')
c= tensor(6.9012e+08, device='cuda:0')
c= tensor(6.9012e+08, device='cuda:0')
c= tensor(6.9012e+08, device='cuda:0')
c= tensor(6.9013e+08, device='cuda:0')
c= tensor(6.9014e+08, device='cuda:0')
c= tensor(6.9014e+08, device='cuda:0')
c= tensor(6.9014e+08, device='cuda:0')
c= tensor(6.9024e+08, device='cuda:0')
c= tensor(6.9024e+08, device='cuda:0')
c= tensor(6.9024e+08, device='cuda:0')
c= tensor(6.9027e+08, device='cuda:0')
c= tensor(6.9027e+08, device='cuda:0')
c= tensor(6.9027e+08, device='cuda:0')
c= tensor(6.9027e+08, device='cuda:0')
c= tensor(6.9028e+08, device='cuda:0')
c= tensor(6.9028e+08, device='cuda:0')
c= tensor(6.9029e+08, device='cuda:0')
c= tensor(6.9030e+08, device='cuda:0')
c= tensor(6.9030e+08, device='cuda:0')
c= tensor(6.9030e+08, device='cuda:0')
c= tensor(6.9031e+08, device='cuda:0')
c= tensor(6.9031e+08, device='cuda:0')
c= tensor(6.9032e+08, device='cuda:0')
c= tensor(6.9033e+08, device='cuda:0')
c= tensor(6.9034e+08, device='cuda:0')
c= tensor(6.9036e+08, device='cuda:0')
c= tensor(6.9037e+08, device='cuda:0')
c= tensor(6.9038e+08, device='cuda:0')
c= tensor(6.9040e+08, device='cuda:0')
c= tensor(6.9041e+08, device='cuda:0')
c= tensor(6.9042e+08, device='cuda:0')
c= tensor(6.9043e+08, device='cuda:0')
c= tensor(6.9044e+08, device='cuda:0')
c= tensor(6.9045e+08, device='cuda:0')
c= tensor(6.9046e+08, device='cuda:0')
c= tensor(6.9046e+08, device='cuda:0')
c= tensor(6.9047e+08, device='cuda:0')
c= tensor(6.9047e+08, device='cuda:0')
c= tensor(6.9047e+08, device='cuda:0')
c= tensor(6.9047e+08, device='cuda:0')
c= tensor(6.9048e+08, device='cuda:0')
c= tensor(6.9049e+08, device='cuda:0')
c= tensor(6.9050e+08, device='cuda:0')
c= tensor(6.9050e+08, device='cuda:0')
c= tensor(6.9050e+08, device='cuda:0')
c= tensor(6.9050e+08, device='cuda:0')
c= tensor(6.9053e+08, device='cuda:0')
c= tensor(6.9054e+08, device='cuda:0')
c= tensor(6.9057e+08, device='cuda:0')
c= tensor(6.9057e+08, device='cuda:0')
c= tensor(6.9058e+08, device='cuda:0')
c= tensor(6.9058e+08, device='cuda:0')
c= tensor(6.9058e+08, device='cuda:0')
c= tensor(6.9059e+08, device='cuda:0')
c= tensor(6.9059e+08, device='cuda:0')
c= tensor(6.9060e+08, device='cuda:0')
c= tensor(6.9069e+08, device='cuda:0')
c= tensor(6.9069e+08, device='cuda:0')
c= tensor(6.9072e+08, device='cuda:0')
c= tensor(6.9072e+08, device='cuda:0')
c= tensor(6.9073e+08, device='cuda:0')
c= tensor(6.9074e+08, device='cuda:0')
c= tensor(6.9083e+08, device='cuda:0')
c= tensor(6.9084e+08, device='cuda:0')
c= tensor(6.9084e+08, device='cuda:0')
c= tensor(6.9084e+08, device='cuda:0')
c= tensor(6.9084e+08, device='cuda:0')
c= tensor(6.9085e+08, device='cuda:0')
c= tensor(6.9086e+08, device='cuda:0')
c= tensor(6.9087e+08, device='cuda:0')
c= tensor(6.9097e+08, device='cuda:0')
c= tensor(6.9100e+08, device='cuda:0')
c= tensor(6.9102e+08, device='cuda:0')
c= tensor(6.9103e+08, device='cuda:0')
c= tensor(6.9103e+08, device='cuda:0')
c= tensor(6.9104e+08, device='cuda:0')
c= tensor(6.9104e+08, device='cuda:0')
c= tensor(6.9105e+08, device='cuda:0')
c= tensor(6.9105e+08, device='cuda:0')
c= tensor(6.9106e+08, device='cuda:0')
c= tensor(6.9107e+08, device='cuda:0')
c= tensor(6.9111e+08, device='cuda:0')
c= tensor(6.9111e+08, device='cuda:0')
c= tensor(6.9119e+08, device='cuda:0')
c= tensor(6.9120e+08, device='cuda:0')
c= tensor(6.9120e+08, device='cuda:0')
c= tensor(6.9121e+08, device='cuda:0')
c= tensor(6.9121e+08, device='cuda:0')
c= tensor(6.9165e+08, device='cuda:0')
c= tensor(6.9166e+08, device='cuda:0')
c= tensor(6.9167e+08, device='cuda:0')
c= tensor(6.9167e+08, device='cuda:0')
c= tensor(6.9168e+08, device='cuda:0')
c= tensor(6.9168e+08, device='cuda:0')
c= tensor(6.9168e+08, device='cuda:0')
c= tensor(6.9169e+08, device='cuda:0')
c= tensor(6.9170e+08, device='cuda:0')
c= tensor(6.9170e+08, device='cuda:0')
c= tensor(6.9171e+08, device='cuda:0')
c= tensor(6.9171e+08, device='cuda:0')
c= tensor(6.9173e+08, device='cuda:0')
c= tensor(6.9173e+08, device='cuda:0')
c= tensor(6.9175e+08, device='cuda:0')
c= tensor(6.9255e+08, device='cuda:0')
c= tensor(6.9256e+08, device='cuda:0')
c= tensor(6.9257e+08, device='cuda:0')
c= tensor(6.9258e+08, device='cuda:0')
c= tensor(6.9259e+08, device='cuda:0')
c= tensor(6.9260e+08, device='cuda:0')
c= tensor(6.9260e+08, device='cuda:0')
c= tensor(6.9261e+08, device='cuda:0')
c= tensor(6.9261e+08, device='cuda:0')
c= tensor(6.9264e+08, device='cuda:0')
c= tensor(6.9265e+08, device='cuda:0')
c= tensor(6.9265e+08, device='cuda:0')
c= tensor(6.9266e+08, device='cuda:0')
c= tensor(6.9269e+08, device='cuda:0')
c= tensor(6.9277e+08, device='cuda:0')
c= tensor(6.9277e+08, device='cuda:0')
c= tensor(6.9277e+08, device='cuda:0')
c= tensor(6.9278e+08, device='cuda:0')
c= tensor(6.9280e+08, device='cuda:0')
c= tensor(6.9280e+08, device='cuda:0')
c= tensor(6.9281e+08, device='cuda:0')
c= tensor(6.9281e+08, device='cuda:0')
c= tensor(6.9281e+08, device='cuda:0')
c= tensor(6.9282e+08, device='cuda:0')
c= tensor(6.9282e+08, device='cuda:0')
c= tensor(6.9283e+08, device='cuda:0')
c= tensor(6.9283e+08, device='cuda:0')
c= tensor(6.9285e+08, device='cuda:0')
c= tensor(6.9286e+08, device='cuda:0')
c= tensor(6.9287e+08, device='cuda:0')
c= tensor(6.9287e+08, device='cuda:0')
c= tensor(6.9288e+08, device='cuda:0')
c= tensor(6.9289e+08, device='cuda:0')
c= tensor(6.9290e+08, device='cuda:0')
c= tensor(6.9294e+08, device='cuda:0')
c= tensor(6.9296e+08, device='cuda:0')
c= tensor(6.9297e+08, device='cuda:0')
c= tensor(6.9297e+08, device='cuda:0')
c= tensor(6.9297e+08, device='cuda:0')
c= tensor(6.9298e+08, device='cuda:0')
c= tensor(6.9298e+08, device='cuda:0')
c= tensor(6.9298e+08, device='cuda:0')
c= tensor(6.9298e+08, device='cuda:0')
c= tensor(6.9299e+08, device='cuda:0')
c= tensor(6.9300e+08, device='cuda:0')
c= tensor(6.9300e+08, device='cuda:0')
c= tensor(6.9300e+08, device='cuda:0')
c= tensor(6.9301e+08, device='cuda:0')
c= tensor(6.9302e+08, device='cuda:0')
c= tensor(6.9303e+08, device='cuda:0')
c= tensor(6.9304e+08, device='cuda:0')
c= tensor(6.9304e+08, device='cuda:0')
c= tensor(6.9305e+08, device='cuda:0')
c= tensor(6.9306e+08, device='cuda:0')
c= tensor(6.9306e+08, device='cuda:0')
c= tensor(6.9307e+08, device='cuda:0')
c= tensor(6.9307e+08, device='cuda:0')
c= tensor(6.9307e+08, device='cuda:0')
c= tensor(6.9309e+08, device='cuda:0')
c= tensor(6.9309e+08, device='cuda:0')
c= tensor(6.9310e+08, device='cuda:0')
c= tensor(6.9310e+08, device='cuda:0')
c= tensor(6.9312e+08, device='cuda:0')
c= tensor(6.9319e+08, device='cuda:0')
c= tensor(6.9323e+08, device='cuda:0')
c= tensor(6.9678e+08, device='cuda:0')
c= tensor(6.9687e+08, device='cuda:0')
c= tensor(6.9689e+08, device='cuda:0')
c= tensor(6.9689e+08, device='cuda:0')
c= tensor(6.9721e+08, device='cuda:0')
c= tensor(6.9780e+08, device='cuda:0')
c= tensor(7.3480e+08, device='cuda:0')
c= tensor(7.3481e+08, device='cuda:0')
c= tensor(7.3736e+08, device='cuda:0')
c= tensor(7.3997e+08, device='cuda:0')
c= tensor(7.4038e+08, device='cuda:0')
c= tensor(7.6380e+08, device='cuda:0')
c= tensor(7.6381e+08, device='cuda:0')
c= tensor(7.6413e+08, device='cuda:0')
c= tensor(7.7034e+08, device='cuda:0')
c= tensor(8.2959e+08, device='cuda:0')
c= tensor(8.2960e+08, device='cuda:0')
c= tensor(8.2989e+08, device='cuda:0')
c= tensor(8.4415e+08, device='cuda:0')
c= tensor(9.3989e+08, device='cuda:0')
c= tensor(9.4971e+08, device='cuda:0')
c= tensor(9.7153e+08, device='cuda:0')
c= tensor(9.7239e+08, device='cuda:0')
c= tensor(9.7267e+08, device='cuda:0')
c= tensor(9.7274e+08, device='cuda:0')
c= tensor(1.0010e+09, device='cuda:0')
c= tensor(1.0011e+09, device='cuda:0')
c= tensor(1.0011e+09, device='cuda:0')
c= tensor(1.0027e+09, device='cuda:0')
c= tensor(1.0031e+09, device='cuda:0')
c= tensor(1.0656e+09, device='cuda:0')
c= tensor(1.0676e+09, device='cuda:0')
c= tensor(1.0676e+09, device='cuda:0')
c= tensor(1.0677e+09, device='cuda:0')
c= tensor(1.0677e+09, device='cuda:0')
c= tensor(1.0681e+09, device='cuda:0')
c= tensor(1.0766e+09, device='cuda:0')
c= tensor(1.0768e+09, device='cuda:0')
c= tensor(1.0776e+09, device='cuda:0')
c= tensor(1.0776e+09, device='cuda:0')
c= tensor(1.0777e+09, device='cuda:0')
c= tensor(1.0796e+09, device='cuda:0')
c= tensor(1.0814e+09, device='cuda:0')
c= tensor(1.0822e+09, device='cuda:0')
c= tensor(1.0822e+09, device='cuda:0')
c= tensor(1.1150e+09, device='cuda:0')
c= tensor(1.1151e+09, device='cuda:0')
c= tensor(1.1153e+09, device='cuda:0')
c= tensor(1.1178e+09, device='cuda:0')
c= tensor(1.1178e+09, device='cuda:0')
c= tensor(1.1193e+09, device='cuda:0')
c= tensor(1.1390e+09, device='cuda:0')
c= tensor(1.1705e+09, device='cuda:0')
c= tensor(1.1706e+09, device='cuda:0')
c= tensor(1.1708e+09, device='cuda:0')
c= tensor(1.1709e+09, device='cuda:0')
c= tensor(1.1709e+09, device='cuda:0')
c= tensor(1.1736e+09, device='cuda:0')
c= tensor(1.1737e+09, device='cuda:0')
c= tensor(1.1752e+09, device='cuda:0')
c= tensor(1.1832e+09, device='cuda:0')
c= tensor(1.1986e+09, device='cuda:0')
c= tensor(1.1988e+09, device='cuda:0')
c= tensor(1.1989e+09, device='cuda:0')
c= tensor(1.2100e+09, device='cuda:0')
c= tensor(1.2177e+09, device='cuda:0')
c= tensor(1.2178e+09, device='cuda:0')
c= tensor(1.2179e+09, device='cuda:0')
c= tensor(1.2577e+09, device='cuda:0')
c= tensor(1.2578e+09, device='cuda:0')
c= tensor(1.2653e+09, device='cuda:0')
c= tensor(1.2654e+09, device='cuda:0')
c= tensor(1.2676e+09, device='cuda:0')
c= tensor(1.2680e+09, device='cuda:0')
c= tensor(1.2758e+09, device='cuda:0')
c= tensor(1.2771e+09, device='cuda:0')
c= tensor(1.2771e+09, device='cuda:0')
c= tensor(1.2823e+09, device='cuda:0')
c= tensor(1.2860e+09, device='cuda:0')
c= tensor(1.2883e+09, device='cuda:0')
c= tensor(1.2900e+09, device='cuda:0')
c= tensor(1.2965e+09, device='cuda:0')
c= tensor(1.3087e+09, device='cuda:0')
c= tensor(1.3176e+09, device='cuda:0')
c= tensor(1.3176e+09, device='cuda:0')
c= tensor(1.3176e+09, device='cuda:0')
c= tensor(1.3185e+09, device='cuda:0')
c= tensor(1.3190e+09, device='cuda:0')
c= tensor(1.3190e+09, device='cuda:0')
c= tensor(1.3190e+09, device='cuda:0')
c= tensor(1.3209e+09, device='cuda:0')
c= tensor(1.3247e+09, device='cuda:0')
c= tensor(1.3405e+09, device='cuda:0')
c= tensor(1.3405e+09, device='cuda:0')
c= tensor(1.3409e+09, device='cuda:0')
c= tensor(1.3409e+09, device='cuda:0')
c= tensor(1.3410e+09, device='cuda:0')
c= tensor(1.3410e+09, device='cuda:0')
c= tensor(1.3410e+09, device='cuda:0')
c= tensor(1.3811e+09, device='cuda:0')
c= tensor(1.3816e+09, device='cuda:0')
c= tensor(1.3817e+09, device='cuda:0')
c= tensor(1.3820e+09, device='cuda:0')
c= tensor(1.3821e+09, device='cuda:0')
c= tensor(2.0901e+09, device='cuda:0')
c= tensor(2.0901e+09, device='cuda:0')
c= tensor(2.0943e+09, device='cuda:0')
c= tensor(2.0943e+09, device='cuda:0')
c= tensor(2.0943e+09, device='cuda:0')
c= tensor(2.0943e+09, device='cuda:0')
c= tensor(2.0946e+09, device='cuda:0')
c= tensor(2.0960e+09, device='cuda:0')
c= tensor(2.0968e+09, device='cuda:0')
c= tensor(2.0968e+09, device='cuda:0')
c= tensor(2.0969e+09, device='cuda:0')
c= tensor(2.1157e+09, device='cuda:0')
c= tensor(2.1162e+09, device='cuda:0')
c= tensor(2.1167e+09, device='cuda:0')
c= tensor(2.1183e+09, device='cuda:0')
c= tensor(2.1225e+09, device='cuda:0')
c= tensor(2.1225e+09, device='cuda:0')
c= tensor(2.1225e+09, device='cuda:0')
c= tensor(2.1231e+09, device='cuda:0')
c= tensor(2.1231e+09, device='cuda:0')
c= tensor(2.1231e+09, device='cuda:0')
c= tensor(2.1233e+09, device='cuda:0')
c= tensor(2.1233e+09, device='cuda:0')
c= tensor(2.1233e+09, device='cuda:0')
c= tensor(2.1234e+09, device='cuda:0')
c= tensor(2.1234e+09, device='cuda:0')
c= tensor(2.1831e+09, device='cuda:0')
c= tensor(2.1860e+09, device='cuda:0')
c= tensor(2.1885e+09, device='cuda:0')
c= tensor(2.1890e+09, device='cuda:0')
c= tensor(2.1890e+09, device='cuda:0')
c= tensor(2.1893e+09, device='cuda:0')
c= tensor(2.4498e+09, device='cuda:0')
c= tensor(2.5313e+09, device='cuda:0')
c= tensor(2.5326e+09, device='cuda:0')
c= tensor(2.5338e+09, device='cuda:0')
c= tensor(2.5338e+09, device='cuda:0')
c= tensor(2.5405e+09, device='cuda:0')
c= tensor(2.5569e+09, device='cuda:0')
c= tensor(2.5779e+09, device='cuda:0')
c= tensor(2.5779e+09, device='cuda:0')
c= tensor(2.5823e+09, device='cuda:0')
c= tensor(2.6104e+09, device='cuda:0')
c= tensor(2.6125e+09, device='cuda:0')
c= tensor(2.6125e+09, device='cuda:0')
c= tensor(2.6128e+09, device='cuda:0')
c= tensor(2.6129e+09, device='cuda:0')
c= tensor(2.6129e+09, device='cuda:0')
c= tensor(2.6205e+09, device='cuda:0')
c= tensor(2.6208e+09, device='cuda:0')
c= tensor(2.6208e+09, device='cuda:0')
c= tensor(2.6213e+09, device='cuda:0')
c= tensor(2.6213e+09, device='cuda:0')
c= tensor(2.6214e+09, device='cuda:0')
c= tensor(2.6262e+09, device='cuda:0')
c= tensor(2.6300e+09, device='cuda:0')
c= tensor(2.6318e+09, device='cuda:0')
c= tensor(2.6369e+09, device='cuda:0')
c= tensor(2.6397e+09, device='cuda:0')
c= tensor(2.6399e+09, device='cuda:0')
c= tensor(2.6413e+09, device='cuda:0')
c= tensor(2.6431e+09, device='cuda:0')
c= tensor(2.6481e+09, device='cuda:0')
c= tensor(2.6481e+09, device='cuda:0')
c= tensor(2.6707e+09, device='cuda:0')
c= tensor(2.8275e+09, device='cuda:0')
c= tensor(2.8302e+09, device='cuda:0')
c= tensor(2.8310e+09, device='cuda:0')
c= tensor(2.8351e+09, device='cuda:0')
c= tensor(2.8352e+09, device='cuda:0')
c= tensor(2.8352e+09, device='cuda:0')
c= tensor(2.8361e+09, device='cuda:0')
c= tensor(2.8415e+09, device='cuda:0')
c= tensor(2.8457e+09, device='cuda:0')
c= tensor(2.9065e+09, device='cuda:0')
c= tensor(2.9204e+09, device='cuda:0')
c= tensor(2.9225e+09, device='cuda:0')
c= tensor(2.9229e+09, device='cuda:0')
c= tensor(2.9262e+09, device='cuda:0')
c= tensor(2.9262e+09, device='cuda:0')
c= tensor(2.9262e+09, device='cuda:0')
c= tensor(2.9432e+09, device='cuda:0')
c= tensor(2.9434e+09, device='cuda:0')
c= tensor(2.9434e+09, device='cuda:0')
c= tensor(2.9436e+09, device='cuda:0')
c= tensor(2.9481e+09, device='cuda:0')
c= tensor(2.9483e+09, device='cuda:0')
c= tensor(2.9512e+09, device='cuda:0')
c= tensor(2.9513e+09, device='cuda:0')
c= tensor(2.9515e+09, device='cuda:0')
c= tensor(2.9515e+09, device='cuda:0')
c= tensor(2.9515e+09, device='cuda:0')
c= tensor(2.9523e+09, device='cuda:0')
c= tensor(2.9546e+09, device='cuda:0')
c= tensor(2.9548e+09, device='cuda:0')
c= tensor(2.9611e+09, device='cuda:0')
c= tensor(2.9611e+09, device='cuda:0')
c= tensor(2.9620e+09, device='cuda:0')
c= tensor(2.9620e+09, device='cuda:0')
c= tensor(2.9635e+09, device='cuda:0')
c= tensor(2.9636e+09, device='cuda:0')
c= tensor(2.9677e+09, device='cuda:0')
c= tensor(2.9686e+09, device='cuda:0')
c= tensor(2.9730e+09, device='cuda:0')
c= tensor(2.9739e+09, device='cuda:0')
c= tensor(3.0447e+09, device='cuda:0')
c= tensor(3.0447e+09, device='cuda:0')
c= tensor(3.0448e+09, device='cuda:0')
c= tensor(3.0489e+09, device='cuda:0')
c= tensor(3.0489e+09, device='cuda:0')
c= tensor(3.1007e+09, device='cuda:0')
c= tensor(3.1007e+09, device='cuda:0')
c= tensor(3.1040e+09, device='cuda:0')
c= tensor(3.1157e+09, device='cuda:0')
c= tensor(3.1157e+09, device='cuda:0')
c= tensor(3.1409e+09, device='cuda:0')
c= tensor(3.1421e+09, device='cuda:0')
c= tensor(3.1710e+09, device='cuda:0')
c= tensor(3.1711e+09, device='cuda:0')
c= tensor(3.1722e+09, device='cuda:0')
c= tensor(3.1722e+09, device='cuda:0')
c= tensor(3.1722e+09, device='cuda:0')
c= tensor(3.1722e+09, device='cuda:0')
c= tensor(3.1740e+09, device='cuda:0')
c= tensor(3.1743e+09, device='cuda:0')
c= tensor(3.1773e+09, device='cuda:0')
c= tensor(3.1774e+09, device='cuda:0')
c= tensor(3.1774e+09, device='cuda:0')
c= tensor(3.1774e+09, device='cuda:0')
c= tensor(3.1811e+09, device='cuda:0')
c= tensor(3.1814e+09, device='cuda:0')
c= tensor(3.1893e+09, device='cuda:0')
c= tensor(3.1943e+09, device='cuda:0')
c= tensor(3.1944e+09, device='cuda:0')
c= tensor(3.1947e+09, device='cuda:0')
c= tensor(3.1949e+09, device='cuda:0')
c= tensor(3.5302e+09, device='cuda:0')
c= tensor(3.5302e+09, device='cuda:0')
c= tensor(3.5303e+09, device='cuda:0')
c= tensor(3.5365e+09, device='cuda:0')
c= tensor(3.5374e+09, device='cuda:0')
c= tensor(3.5374e+09, device='cuda:0')
c= tensor(3.5374e+09, device='cuda:0')
c= tensor(3.5854e+09, device='cuda:0')
c= tensor(3.5858e+09, device='cuda:0')
c= tensor(3.5877e+09, device='cuda:0')
c= tensor(3.5894e+09, device='cuda:0')
c= tensor(3.5931e+09, device='cuda:0')
c= tensor(3.5960e+09, device='cuda:0')
c= tensor(3.6106e+09, device='cuda:0')
c= tensor(3.6127e+09, device='cuda:0')
c= tensor(3.6127e+09, device='cuda:0')
c= tensor(3.6137e+09, device='cuda:0')
c= tensor(3.6137e+09, device='cuda:0')
c= tensor(3.6139e+09, device='cuda:0')
c= tensor(3.6142e+09, device='cuda:0')
c= tensor(3.6142e+09, device='cuda:0')
c= tensor(3.6209e+09, device='cuda:0')
c= tensor(3.6210e+09, device='cuda:0')
c= tensor(3.6210e+09, device='cuda:0')
c= tensor(3.6212e+09, device='cuda:0')
c= tensor(3.6213e+09, device='cuda:0')
c= tensor(3.6307e+09, device='cuda:0')
c= tensor(3.6333e+09, device='cuda:0')
c= tensor(3.6334e+09, device='cuda:0')
c= tensor(3.6344e+09, device='cuda:0')
c= tensor(3.6348e+09, device='cuda:0')
c= tensor(3.6348e+09, device='cuda:0')
c= tensor(3.6348e+09, device='cuda:0')
c= tensor(3.6350e+09, device='cuda:0')
c= tensor(3.6417e+09, device='cuda:0')
c= tensor(3.6418e+09, device='cuda:0')
c= tensor(3.6418e+09, device='cuda:0')
c= tensor(3.6418e+09, device='cuda:0')
c= tensor(3.6617e+09, device='cuda:0')
c= tensor(3.6950e+09, device='cuda:0')
c= tensor(3.6959e+09, device='cuda:0')
c= tensor(3.6959e+09, device='cuda:0')
c= tensor(3.6980e+09, device='cuda:0')
c= tensor(3.7009e+09, device='cuda:0')
c= tensor(3.7009e+09, device='cuda:0')
c= tensor(3.7010e+09, device='cuda:0')
c= tensor(3.7012e+09, device='cuda:0')
c= tensor(3.7228e+09, device='cuda:0')
c= tensor(3.7422e+09, device='cuda:0')
c= tensor(3.7456e+09, device='cuda:0')
c= tensor(3.7457e+09, device='cuda:0')
c= tensor(3.7476e+09, device='cuda:0')
c= tensor(3.7476e+09, device='cuda:0')
c= tensor(3.7481e+09, device='cuda:0')
c= tensor(3.7483e+09, device='cuda:0')
c= tensor(3.7507e+09, device='cuda:0')
c= tensor(3.7511e+09, device='cuda:0')
c= tensor(3.7664e+09, device='cuda:0')
c= tensor(3.7665e+09, device='cuda:0')
c= tensor(3.7672e+09, device='cuda:0')
c= tensor(3.7674e+09, device='cuda:0')
c= tensor(4.0765e+09, device='cuda:0')
c= tensor(4.0772e+09, device='cuda:0')
c= tensor(4.0775e+09, device='cuda:0')
c= tensor(4.0775e+09, device='cuda:0')
c= tensor(4.0782e+09, device='cuda:0')
c= tensor(4.0782e+09, device='cuda:0')
c= tensor(4.0792e+09, device='cuda:0')
c= tensor(4.0792e+09, device='cuda:0')
c= tensor(4.0793e+09, device='cuda:0')
c= tensor(4.0794e+09, device='cuda:0')
c= tensor(4.0794e+09, device='cuda:0')
c= tensor(4.0794e+09, device='cuda:0')
c= tensor(4.0858e+09, device='cuda:0')
c= tensor(4.1223e+09, device='cuda:0')
c= tensor(4.1288e+09, device='cuda:0')
c= tensor(4.1295e+09, device='cuda:0')
c= tensor(4.1299e+09, device='cuda:0')
c= tensor(4.1300e+09, device='cuda:0')
c= tensor(4.1301e+09, device='cuda:0')
c= tensor(4.1320e+09, device='cuda:0')
c= tensor(4.1344e+09, device='cuda:0')
c= tensor(4.1374e+09, device='cuda:0')
c= tensor(4.1374e+09, device='cuda:0')
c= tensor(4.2206e+09, device='cuda:0')
c= tensor(4.2209e+09, device='cuda:0')
c= tensor(4.2214e+09, device='cuda:0')
c= tensor(4.2321e+09, device='cuda:0')
c= tensor(4.2328e+09, device='cuda:0')
c= tensor(4.2486e+09, device='cuda:0')
c= tensor(4.2664e+09, device='cuda:0')
c= tensor(4.2691e+09, device='cuda:0')
c= tensor(4.2696e+09, device='cuda:0')
c= tensor(4.2699e+09, device='cuda:0')
c= tensor(4.2709e+09, device='cuda:0')
c= tensor(4.2709e+09, device='cuda:0')
c= tensor(4.2748e+09, device='cuda:0')
c= tensor(4.3674e+09, device='cuda:0')
c= tensor(4.3828e+09, device='cuda:0')
c= tensor(4.3861e+09, device='cuda:0')
c= tensor(4.3864e+09, device='cuda:0')
c= tensor(4.3870e+09, device='cuda:0')
c= tensor(4.3872e+09, device='cuda:0')
c= tensor(4.3875e+09, device='cuda:0')
c= tensor(4.3889e+09, device='cuda:0')
c= tensor(4.4035e+09, device='cuda:0')
c= tensor(4.4036e+09, device='cuda:0')
c= tensor(4.4141e+09, device='cuda:0')
c= tensor(4.4285e+09, device='cuda:0')
c= tensor(4.4290e+09, device='cuda:0')
c= tensor(4.4290e+09, device='cuda:0')
c= tensor(4.4325e+09, device='cuda:0')
c= tensor(4.4357e+09, device='cuda:0')
c= tensor(4.4357e+09, device='cuda:0')
c= tensor(4.5203e+09, device='cuda:0')
c= tensor(4.5210e+09, device='cuda:0')
c= tensor(4.5214e+09, device='cuda:0')
c= tensor(4.5216e+09, device='cuda:0')
c= tensor(4.5216e+09, device='cuda:0')
c= tensor(4.5216e+09, device='cuda:0')
c= tensor(4.5217e+09, device='cuda:0')
c= tensor(4.5220e+09, device='cuda:0')
c= tensor(4.5270e+09, device='cuda:0')
c= tensor(6.0072e+09, device='cuda:0')
c= tensor(6.0074e+09, device='cuda:0')
c= tensor(6.0090e+09, device='cuda:0')
c= tensor(6.0090e+09, device='cuda:0')
c= tensor(6.0091e+09, device='cuda:0')
c= tensor(6.0092e+09, device='cuda:0')
c= tensor(6.0419e+09, device='cuda:0')
c= tensor(6.0426e+09, device='cuda:0')
c= tensor(6.2118e+09, device='cuda:0')
c= tensor(6.2119e+09, device='cuda:0')
c= tensor(6.2187e+09, device='cuda:0')
c= tensor(6.2193e+09, device='cuda:0')
c= tensor(6.2250e+09, device='cuda:0')
c= tensor(6.2484e+09, device='cuda:0')
c= tensor(6.2485e+09, device='cuda:0')
c= tensor(6.2485e+09, device='cuda:0')
c= tensor(6.2498e+09, device='cuda:0')
c= tensor(6.2499e+09, device='cuda:0')
c= tensor(6.2512e+09, device='cuda:0')
c= tensor(6.2548e+09, device='cuda:0')
c= tensor(6.2563e+09, device='cuda:0')
c= tensor(6.2578e+09, device='cuda:0')
c= tensor(6.2581e+09, device='cuda:0')
c= tensor(6.2641e+09, device='cuda:0')
c= tensor(6.2955e+09, device='cuda:0')
c= tensor(6.2968e+09, device='cuda:0')
c= tensor(6.2968e+09, device='cuda:0')
c= tensor(6.3103e+09, device='cuda:0')
c= tensor(6.3109e+09, device='cuda:0')
c= tensor(6.3132e+09, device='cuda:0')
c= tensor(6.3144e+09, device='cuda:0')
c= tensor(6.3170e+09, device='cuda:0')
c= tensor(6.3178e+09, device='cuda:0')
c= tensor(6.3322e+09, device='cuda:0')
c= tensor(6.3332e+09, device='cuda:0')
c= tensor(6.3333e+09, device='cuda:0')
c= tensor(6.3333e+09, device='cuda:0')
c= tensor(6.3335e+09, device='cuda:0')
c= tensor(6.3364e+09, device='cuda:0')
c= tensor(6.3408e+09, device='cuda:0')
c= tensor(6.3440e+09, device='cuda:0')
c= tensor(6.3441e+09, device='cuda:0')
c= tensor(6.3455e+09, device='cuda:0')
c= tensor(6.3466e+09, device='cuda:0')
c= tensor(6.3795e+09, device='cuda:0')
c= tensor(6.3801e+09, device='cuda:0')
c= tensor(6.3813e+09, device='cuda:0')
c= tensor(6.3850e+09, device='cuda:0')
c= tensor(6.3854e+09, device='cuda:0')
c= tensor(6.3854e+09, device='cuda:0')
c= tensor(6.3854e+09, device='cuda:0')
c= tensor(6.3918e+09, device='cuda:0')
c= tensor(6.3930e+09, device='cuda:0')
c= tensor(6.3941e+09, device='cuda:0')
c= tensor(6.3941e+09, device='cuda:0')
c= tensor(6.3941e+09, device='cuda:0')
c= tensor(6.4071e+09, device='cuda:0')
c= tensor(6.4103e+09, device='cuda:0')
c= tensor(6.4110e+09, device='cuda:0')
c= tensor(6.4110e+09, device='cuda:0')
c= tensor(6.4111e+09, device='cuda:0')
c= tensor(6.4114e+09, device='cuda:0')
c= tensor(6.4122e+09, device='cuda:0')
c= tensor(6.4168e+09, device='cuda:0')
c= tensor(6.4168e+09, device='cuda:0')
c= tensor(6.4211e+09, device='cuda:0')
c= tensor(6.4212e+09, device='cuda:0')
c= tensor(6.4214e+09, device='cuda:0')
c= tensor(6.4541e+09, device='cuda:0')
c= tensor(6.4545e+09, device='cuda:0')
c= tensor(6.4563e+09, device='cuda:0')
c= tensor(6.4617e+09, device='cuda:0')
c= tensor(6.4623e+09, device='cuda:0')
c= tensor(6.4640e+09, device='cuda:0')
c= tensor(6.4680e+09, device='cuda:0')
c= tensor(6.4680e+09, device='cuda:0')
c= tensor(6.4685e+09, device='cuda:0')
c= tensor(6.4688e+09, device='cuda:0')
c= tensor(6.4746e+09, device='cuda:0')
c= tensor(6.4802e+09, device='cuda:0')
c= tensor(6.4812e+09, device='cuda:0')
c= tensor(6.4821e+09, device='cuda:0')
c= tensor(6.4822e+09, device='cuda:0')
c= tensor(6.4828e+09, device='cuda:0')
c= tensor(6.4831e+09, device='cuda:0')
c= tensor(6.4831e+09, device='cuda:0')
c= tensor(6.4934e+09, device='cuda:0')
c= tensor(6.5058e+09, device='cuda:0')
c= tensor(6.5058e+09, device='cuda:0')
c= tensor(6.5061e+09, device='cuda:0')
c= tensor(6.5061e+09, device='cuda:0')
c= tensor(6.5591e+09, device='cuda:0')
c= tensor(6.5592e+09, device='cuda:0')
c= tensor(6.5593e+09, device='cuda:0')
c= tensor(6.5619e+09, device='cuda:0')
c= tensor(6.5635e+09, device='cuda:0')
c= tensor(6.5635e+09, device='cuda:0')
c= tensor(6.5640e+09, device='cuda:0')
c= tensor(6.5641e+09, device='cuda:0')
memory (bytes)
5272788992
time for making loss 2 is 11.919631242752075
p0 True
it  0 : 2700215808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 69% |
shape of L is 
torch.Size([])
memory (bytes)
5273051136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 44% | 12% |
memory (bytes)
5273554944
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  65834676000.0
relative error loss 10.02955
shape of L is 
torch.Size([])
memory (bytes)
5400862720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
5400862720
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  65834467000.0
relative error loss 10.029518
shape of L is 
torch.Size([])
memory (bytes)
5407354880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
5407412224
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  65833697000.0
relative error loss 10.029401
shape of L is 
torch.Size([])
memory (bytes)
5409415168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
memory (bytes)
5409509376
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  65824903000.0
relative error loss 10.028061
shape of L is 
torch.Size([])
memory (bytes)
5411631104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
5411643392
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  65737073000.0
relative error loss 10.014681
shape of L is 
torch.Size([])
memory (bytes)
5413818368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
5413818368
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  64864555000.0
relative error loss 9.881757
shape of L is 
torch.Size([])
memory (bytes)
5415936000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
5415936000
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  60646715000.0
relative error loss 9.239193
shape of L is 
torch.Size([])
memory (bytes)
5418057728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 13% |
memory (bytes)
5418057728
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  41495510000.0
relative error loss 6.3216124
shape of L is 
torch.Size([])
memory (bytes)
5420179456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 13% |
memory (bytes)
5420195840
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  15147381000.0
relative error loss 2.3076198
shape of L is 
torch.Size([])
memory (bytes)
5422346240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 13% |
memory (bytes)
5422346240
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  9496552000.0
relative error loss 1.4467474
time to take a step is 197.82039093971252
it  1 : 3314867200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 13% |
shape of L is 
torch.Size([])
memory (bytes)
5424463872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% | 13% |
memory (bytes)
5424463872
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  9496552000.0
relative error loss 1.4467474
shape of L is 
torch.Size([])
memory (bytes)
5426614272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 13% |
memory (bytes)
5426614272
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  7174345700.0
relative error loss 1.092972
shape of L is 
torch.Size([])
memory (bytes)
5428613120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 13% |
memory (bytes)
5428613120
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  26830602000.0
relative error loss 4.0874944
shape of L is 
torch.Size([])
memory (bytes)
5430755328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
5430853632
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  6400026600.0
relative error loss 0.9750088
shape of L is 
torch.Size([])
memory (bytes)
5433024512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 13% |
memory (bytes)
5433024512
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  5877462500.0
relative error loss 0.895399
shape of L is 
torch.Size([])
memory (bytes)
5435121664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% | 13% |
memory (bytes)
5435121664
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  5626292700.0
relative error loss 0.85713464
shape of L is 
torch.Size([])
memory (bytes)
5437177856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 13% |
memory (bytes)
5437267968
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  5282410500.0
relative error loss 0.8047461
shape of L is 
torch.Size([])
memory (bytes)
5439397888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
5439397888
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  4931494400.0
relative error loss 0.751286
shape of L is 
torch.Size([])
memory (bytes)
5441474560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
5441474560
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  4716189700.0
relative error loss 0.7184855
shape of L is 
torch.Size([])
memory (bytes)
5443555328
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 13% |
memory (bytes)
5443555328
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  4487460400.0
relative error loss 0.6836398
time to take a step is 188.8118667602539
it  2 : 3490072064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 13% |
shape of L is 
torch.Size([])
memory (bytes)
5445754880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 13% |
memory (bytes)
5445754880
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  4487460400.0
relative error loss 0.6836398
shape of L is 
torch.Size([])
memory (bytes)
5447778304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
5447778304
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  4179225600.0
relative error loss 0.636682
shape of L is 
torch.Size([])
memory (bytes)
5449887744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 33% | 13% |
memory (bytes)
5449887744
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  3919781400.0
relative error loss 0.59715706
shape of L is 
torch.Size([])
memory (bytes)
5452091392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 13% |
memory (bytes)
5452091392
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  3743936800.0
relative error loss 0.5703681
shape of L is 
torch.Size([])
memory (bytes)
5454217216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 34% | 13% |
memory (bytes)
5454217216
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  3516583000.0
relative error loss 0.53573203
shape of L is 
torch.Size([])
memory (bytes)
5456113664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 13% |
memory (bytes)
5456334848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% | 13% |
error is  3293771500.0
relative error loss 0.5017879
shape of L is 
torch.Size([])
memory (bytes)
5458264064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
5458452480
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  3106211800.0
relative error loss 0.47321424
shape of L is 
torch.Size([])
memory (bytes)
5460574208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
5460574208
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  2884160800.0
relative error loss 0.43938598
shape of L is 
torch.Size([])
memory (bytes)
5462659072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 13% |
memory (bytes)
5462659072
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  2656802300.0
relative error loss 0.40474918
shape of L is 
torch.Size([])
memory (bytes)
5464780800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
5464850432
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  2465704000.0
relative error loss 0.3756364
time to take a step is 189.25593185424805
it  3 : 3490391040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 13% |
shape of L is 
torch.Size([])
memory (bytes)
5466988544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 13% |
memory (bytes)
5466988544
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  2465704000.0
relative error loss 0.3756364
shape of L is 
torch.Size([])
memory (bytes)
5468962816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
5468962816
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  2283728100.0
relative error loss 0.34791338
shape of L is 
torch.Size([])
memory (bytes)
5471150080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 13% |
memory (bytes)
5471150080
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  2100303900.0
relative error loss 0.3199697
shape of L is 
torch.Size([])
memory (bytes)
5473402880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
5473419264
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1938945500.0
relative error loss 0.29538766
shape of L is 
torch.Size([])
memory (bytes)
5475553280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 34% | 13% |
memory (bytes)
5475553280
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1806024700.0
relative error loss 0.2751379
shape of L is 
torch.Size([])
memory (bytes)
5477539840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 13% |
memory (bytes)
5477539840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 13% |
error is  1700609000.0
relative error loss 0.2590784
shape of L is 
torch.Size([])
memory (bytes)
5479829504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 13% |
memory (bytes)
5479829504
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1540827600.0
relative error loss 0.23473659
shape of L is 
torch.Size([])
memory (bytes)
5481975808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 32% | 13% |
memory (bytes)
5481975808
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1455115800.0
relative error loss 0.22167887
shape of L is 
torch.Size([])
memory (bytes)
5483921408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 13% |
memory (bytes)
5483921408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% | 13% |
error is  1327179300.0
relative error loss 0.20218845
shape of L is 
torch.Size([])
memory (bytes)
5485961216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 13% |
memory (bytes)
5486170112
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1273684500.0
relative error loss 0.19403881
time to take a step is 190.4539499282837
c= tensor(1057.0842, device='cuda:0')
c= tensor(133237.3281, device='cuda:0')
c= tensor(140896.9219, device='cuda:0')
c= tensor(146551.1094, device='cuda:0')
c= tensor(1962816., device='cuda:0')
c= tensor(2391131.7500, device='cuda:0')
c= tensor(3546683., device='cuda:0')
c= tensor(4052200.5000, device='cuda:0')
c= tensor(4164014.5000, device='cuda:0')
c= tensor(10063810., device='cuda:0')
c= tensor(10087060., device='cuda:0')
c= tensor(15185586., device='cuda:0')
c= tensor(15205295., device='cuda:0')
c= tensor(30119488., device='cuda:0')
c= tensor(30828332., device='cuda:0')
c= tensor(31437178., device='cuda:0')
c= tensor(32831296., device='cuda:0')
c= tensor(35213196., device='cuda:0')
c= tensor(44292064., device='cuda:0')
c= tensor(49219720., device='cuda:0')
c= tensor(49293480., device='cuda:0')
c= tensor(81726416., device='cuda:0')
c= tensor(81811760., device='cuda:0')
c= tensor(82018144., device='cuda:0')
c= tensor(89880136., device='cuda:0')
c= tensor(91241880., device='cuda:0')
c= tensor(92427176., device='cuda:0')
c= tensor(94354784., device='cuda:0')
c= tensor(95538064., device='cuda:0')
c= tensor(5.9621e+08, device='cuda:0')
c= tensor(5.9624e+08, device='cuda:0')
c= tensor(6.8451e+08, device='cuda:0')
c= tensor(6.8517e+08, device='cuda:0')
c= tensor(6.8521e+08, device='cuda:0')
c= tensor(6.8523e+08, device='cuda:0')
c= tensor(6.8837e+08, device='cuda:0')
c= tensor(6.8969e+08, device='cuda:0')
c= tensor(6.8969e+08, device='cuda:0')
c= tensor(6.8971e+08, device='cuda:0')
c= tensor(6.8971e+08, device='cuda:0')
c= tensor(6.8973e+08, device='cuda:0')
c= tensor(6.8973e+08, device='cuda:0')
c= tensor(6.8973e+08, device='cuda:0')
c= tensor(6.8975e+08, device='cuda:0')
c= tensor(6.8975e+08, device='cuda:0')
c= tensor(6.8975e+08, device='cuda:0')
c= tensor(6.8976e+08, device='cuda:0')
c= tensor(6.8977e+08, device='cuda:0')
c= tensor(6.8978e+08, device='cuda:0')
c= tensor(6.8981e+08, device='cuda:0')
c= tensor(6.8989e+08, device='cuda:0')
c= tensor(6.8989e+08, device='cuda:0')
c= tensor(6.8990e+08, device='cuda:0')
c= tensor(6.8990e+08, device='cuda:0')
c= tensor(6.8993e+08, device='cuda:0')
c= tensor(6.8993e+08, device='cuda:0')
c= tensor(6.8994e+08, device='cuda:0')
c= tensor(6.8994e+08, device='cuda:0')
c= tensor(6.8995e+08, device='cuda:0')
c= tensor(6.8995e+08, device='cuda:0')
c= tensor(6.8999e+08, device='cuda:0')
c= tensor(6.8999e+08, device='cuda:0')
c= tensor(6.9003e+08, device='cuda:0')
c= tensor(6.9005e+08, device='cuda:0')
c= tensor(6.9006e+08, device='cuda:0')
c= tensor(6.9006e+08, device='cuda:0')
c= tensor(6.9006e+08, device='cuda:0')
c= tensor(6.9008e+08, device='cuda:0')
c= tensor(6.9009e+08, device='cuda:0')
c= tensor(6.9009e+08, device='cuda:0')
c= tensor(6.9010e+08, device='cuda:0')
c= tensor(6.9011e+08, device='cuda:0')
c= tensor(6.9012e+08, device='cuda:0')
c= tensor(6.9012e+08, device='cuda:0')
c= tensor(6.9012e+08, device='cuda:0')
c= tensor(6.9013e+08, device='cuda:0')
c= tensor(6.9014e+08, device='cuda:0')
c= tensor(6.9014e+08, device='cuda:0')
c= tensor(6.9014e+08, device='cuda:0')
c= tensor(6.9024e+08, device='cuda:0')
c= tensor(6.9024e+08, device='cuda:0')
c= tensor(6.9024e+08, device='cuda:0')
c= tensor(6.9027e+08, device='cuda:0')
c= tensor(6.9027e+08, device='cuda:0')
c= tensor(6.9027e+08, device='cuda:0')
c= tensor(6.9027e+08, device='cuda:0')
c= tensor(6.9028e+08, device='cuda:0')
c= tensor(6.9028e+08, device='cuda:0')
c= tensor(6.9029e+08, device='cuda:0')
c= tensor(6.9030e+08, device='cuda:0')
c= tensor(6.9030e+08, device='cuda:0')
c= tensor(6.9030e+08, device='cuda:0')
c= tensor(6.9031e+08, device='cuda:0')
c= tensor(6.9031e+08, device='cuda:0')
c= tensor(6.9032e+08, device='cuda:0')
c= tensor(6.9033e+08, device='cuda:0')
c= tensor(6.9034e+08, device='cuda:0')
c= tensor(6.9036e+08, device='cuda:0')
c= tensor(6.9037e+08, device='cuda:0')
c= tensor(6.9038e+08, device='cuda:0')
c= tensor(6.9040e+08, device='cuda:0')
c= tensor(6.9041e+08, device='cuda:0')
c= tensor(6.9042e+08, device='cuda:0')
c= tensor(6.9043e+08, device='cuda:0')
c= tensor(6.9044e+08, device='cuda:0')
c= tensor(6.9045e+08, device='cuda:0')
c= tensor(6.9046e+08, device='cuda:0')
c= tensor(6.9046e+08, device='cuda:0')
c= tensor(6.9047e+08, device='cuda:0')
c= tensor(6.9047e+08, device='cuda:0')
c= tensor(6.9047e+08, device='cuda:0')
c= tensor(6.9047e+08, device='cuda:0')
c= tensor(6.9048e+08, device='cuda:0')
c= tensor(6.9049e+08, device='cuda:0')
c= tensor(6.9050e+08, device='cuda:0')
c= tensor(6.9050e+08, device='cuda:0')
c= tensor(6.9050e+08, device='cuda:0')
c= tensor(6.9050e+08, device='cuda:0')
c= tensor(6.9053e+08, device='cuda:0')
c= tensor(6.9054e+08, device='cuda:0')
c= tensor(6.9057e+08, device='cuda:0')
c= tensor(6.9057e+08, device='cuda:0')
c= tensor(6.9058e+08, device='cuda:0')
c= tensor(6.9058e+08, device='cuda:0')
c= tensor(6.9058e+08, device='cuda:0')
c= tensor(6.9059e+08, device='cuda:0')
c= tensor(6.9059e+08, device='cuda:0')
c= tensor(6.9060e+08, device='cuda:0')
c= tensor(6.9069e+08, device='cuda:0')
c= tensor(6.9069e+08, device='cuda:0')
c= tensor(6.9072e+08, device='cuda:0')
c= tensor(6.9072e+08, device='cuda:0')
c= tensor(6.9073e+08, device='cuda:0')
c= tensor(6.9074e+08, device='cuda:0')
c= tensor(6.9083e+08, device='cuda:0')
c= tensor(6.9084e+08, device='cuda:0')
c= tensor(6.9084e+08, device='cuda:0')
c= tensor(6.9084e+08, device='cuda:0')
c= tensor(6.9084e+08, device='cuda:0')
c= tensor(6.9085e+08, device='cuda:0')
c= tensor(6.9086e+08, device='cuda:0')
c= tensor(6.9087e+08, device='cuda:0')
c= tensor(6.9097e+08, device='cuda:0')
c= tensor(6.9100e+08, device='cuda:0')
c= tensor(6.9102e+08, device='cuda:0')
c= tensor(6.9103e+08, device='cuda:0')
c= tensor(6.9103e+08, device='cuda:0')
c= tensor(6.9104e+08, device='cuda:0')
c= tensor(6.9104e+08, device='cuda:0')
c= tensor(6.9105e+08, device='cuda:0')
c= tensor(6.9105e+08, device='cuda:0')
c= tensor(6.9106e+08, device='cuda:0')
c= tensor(6.9107e+08, device='cuda:0')
c= tensor(6.9111e+08, device='cuda:0')
c= tensor(6.9111e+08, device='cuda:0')
c= tensor(6.9119e+08, device='cuda:0')
c= tensor(6.9120e+08, device='cuda:0')
c= tensor(6.9120e+08, device='cuda:0')
c= tensor(6.9121e+08, device='cuda:0')
c= tensor(6.9121e+08, device='cuda:0')
c= tensor(6.9165e+08, device='cuda:0')
c= tensor(6.9166e+08, device='cuda:0')
c= tensor(6.9167e+08, device='cuda:0')
c= tensor(6.9167e+08, device='cuda:0')
c= tensor(6.9168e+08, device='cuda:0')
c= tensor(6.9168e+08, device='cuda:0')
c= tensor(6.9168e+08, device='cuda:0')
c= tensor(6.9169e+08, device='cuda:0')
c= tensor(6.9170e+08, device='cuda:0')
c= tensor(6.9170e+08, device='cuda:0')
c= tensor(6.9171e+08, device='cuda:0')
c= tensor(6.9171e+08, device='cuda:0')
c= tensor(6.9173e+08, device='cuda:0')
c= tensor(6.9173e+08, device='cuda:0')
c= tensor(6.9175e+08, device='cuda:0')
c= tensor(6.9255e+08, device='cuda:0')
c= tensor(6.9256e+08, device='cuda:0')
c= tensor(6.9257e+08, device='cuda:0')
c= tensor(6.9258e+08, device='cuda:0')
c= tensor(6.9259e+08, device='cuda:0')
c= tensor(6.9260e+08, device='cuda:0')
c= tensor(6.9260e+08, device='cuda:0')
c= tensor(6.9261e+08, device='cuda:0')
c= tensor(6.9261e+08, device='cuda:0')
c= tensor(6.9264e+08, device='cuda:0')
c= tensor(6.9265e+08, device='cuda:0')
c= tensor(6.9265e+08, device='cuda:0')
c= tensor(6.9266e+08, device='cuda:0')
c= tensor(6.9269e+08, device='cuda:0')
c= tensor(6.9277e+08, device='cuda:0')
c= tensor(6.9277e+08, device='cuda:0')
c= tensor(6.9277e+08, device='cuda:0')
c= tensor(6.9278e+08, device='cuda:0')
c= tensor(6.9280e+08, device='cuda:0')
c= tensor(6.9280e+08, device='cuda:0')
c= tensor(6.9281e+08, device='cuda:0')
c= tensor(6.9281e+08, device='cuda:0')
c= tensor(6.9281e+08, device='cuda:0')
c= tensor(6.9282e+08, device='cuda:0')
c= tensor(6.9282e+08, device='cuda:0')
c= tensor(6.9283e+08, device='cuda:0')
c= tensor(6.9283e+08, device='cuda:0')
c= tensor(6.9285e+08, device='cuda:0')
c= tensor(6.9286e+08, device='cuda:0')
c= tensor(6.9287e+08, device='cuda:0')
c= tensor(6.9287e+08, device='cuda:0')
c= tensor(6.9288e+08, device='cuda:0')
c= tensor(6.9289e+08, device='cuda:0')
c= tensor(6.9290e+08, device='cuda:0')
c= tensor(6.9294e+08, device='cuda:0')
c= tensor(6.9296e+08, device='cuda:0')
c= tensor(6.9297e+08, device='cuda:0')
c= tensor(6.9297e+08, device='cuda:0')
c= tensor(6.9297e+08, device='cuda:0')
c= tensor(6.9298e+08, device='cuda:0')
c= tensor(6.9298e+08, device='cuda:0')
c= tensor(6.9298e+08, device='cuda:0')
c= tensor(6.9298e+08, device='cuda:0')
c= tensor(6.9299e+08, device='cuda:0')
c= tensor(6.9300e+08, device='cuda:0')
c= tensor(6.9300e+08, device='cuda:0')
c= tensor(6.9300e+08, device='cuda:0')
c= tensor(6.9301e+08, device='cuda:0')
c= tensor(6.9302e+08, device='cuda:0')
c= tensor(6.9303e+08, device='cuda:0')
c= tensor(6.9304e+08, device='cuda:0')
c= tensor(6.9304e+08, device='cuda:0')
c= tensor(6.9305e+08, device='cuda:0')
c= tensor(6.9306e+08, device='cuda:0')
c= tensor(6.9306e+08, device='cuda:0')
c= tensor(6.9307e+08, device='cuda:0')
c= tensor(6.9307e+08, device='cuda:0')
c= tensor(6.9307e+08, device='cuda:0')
c= tensor(6.9309e+08, device='cuda:0')
c= tensor(6.9309e+08, device='cuda:0')
c= tensor(6.9310e+08, device='cuda:0')
c= tensor(6.9310e+08, device='cuda:0')
c= tensor(6.9312e+08, device='cuda:0')
c= tensor(6.9319e+08, device='cuda:0')
c= tensor(6.9323e+08, device='cuda:0')
c= tensor(6.9678e+08, device='cuda:0')
c= tensor(6.9687e+08, device='cuda:0')
c= tensor(6.9689e+08, device='cuda:0')
c= tensor(6.9689e+08, device='cuda:0')
c= tensor(6.9721e+08, device='cuda:0')
c= tensor(6.9780e+08, device='cuda:0')
c= tensor(7.3480e+08, device='cuda:0')
c= tensor(7.3481e+08, device='cuda:0')
c= tensor(7.3736e+08, device='cuda:0')
c= tensor(7.3997e+08, device='cuda:0')
c= tensor(7.4038e+08, device='cuda:0')
c= tensor(7.6380e+08, device='cuda:0')
c= tensor(7.6381e+08, device='cuda:0')
c= tensor(7.6413e+08, device='cuda:0')
c= tensor(7.7034e+08, device='cuda:0')
c= tensor(8.2959e+08, device='cuda:0')
c= tensor(8.2960e+08, device='cuda:0')
c= tensor(8.2989e+08, device='cuda:0')
c= tensor(8.4415e+08, device='cuda:0')
c= tensor(9.3989e+08, device='cuda:0')
c= tensor(9.4971e+08, device='cuda:0')
c= tensor(9.7153e+08, device='cuda:0')
c= tensor(9.7239e+08, device='cuda:0')
c= tensor(9.7267e+08, device='cuda:0')
c= tensor(9.7274e+08, device='cuda:0')
c= tensor(1.0010e+09, device='cuda:0')
c= tensor(1.0011e+09, device='cuda:0')
c= tensor(1.0011e+09, device='cuda:0')
c= tensor(1.0027e+09, device='cuda:0')
c= tensor(1.0031e+09, device='cuda:0')
c= tensor(1.0656e+09, device='cuda:0')
c= tensor(1.0676e+09, device='cuda:0')
c= tensor(1.0676e+09, device='cuda:0')
c= tensor(1.0677e+09, device='cuda:0')
c= tensor(1.0677e+09, device='cuda:0')
c= tensor(1.0681e+09, device='cuda:0')
c= tensor(1.0766e+09, device='cuda:0')
c= tensor(1.0768e+09, device='cuda:0')
c= tensor(1.0776e+09, device='cuda:0')
c= tensor(1.0776e+09, device='cuda:0')
c= tensor(1.0777e+09, device='cuda:0')
c= tensor(1.0796e+09, device='cuda:0')
c= tensor(1.0814e+09, device='cuda:0')
c= tensor(1.0822e+09, device='cuda:0')
c= tensor(1.0822e+09, device='cuda:0')
c= tensor(1.1150e+09, device='cuda:0')
c= tensor(1.1151e+09, device='cuda:0')
c= tensor(1.1153e+09, device='cuda:0')
c= tensor(1.1178e+09, device='cuda:0')
c= tensor(1.1178e+09, device='cuda:0')
c= tensor(1.1193e+09, device='cuda:0')
c= tensor(1.1390e+09, device='cuda:0')
c= tensor(1.1705e+09, device='cuda:0')
c= tensor(1.1706e+09, device='cuda:0')
c= tensor(1.1708e+09, device='cuda:0')
c= tensor(1.1709e+09, device='cuda:0')
c= tensor(1.1709e+09, device='cuda:0')
c= tensor(1.1736e+09, device='cuda:0')
c= tensor(1.1737e+09, device='cuda:0')
c= tensor(1.1752e+09, device='cuda:0')
c= tensor(1.1832e+09, device='cuda:0')
c= tensor(1.1986e+09, device='cuda:0')
c= tensor(1.1988e+09, device='cuda:0')
c= tensor(1.1989e+09, device='cuda:0')
c= tensor(1.2100e+09, device='cuda:0')
c= tensor(1.2177e+09, device='cuda:0')
c= tensor(1.2178e+09, device='cuda:0')
c= tensor(1.2179e+09, device='cuda:0')
c= tensor(1.2577e+09, device='cuda:0')
c= tensor(1.2578e+09, device='cuda:0')
c= tensor(1.2653e+09, device='cuda:0')
c= tensor(1.2654e+09, device='cuda:0')
c= tensor(1.2676e+09, device='cuda:0')
c= tensor(1.2680e+09, device='cuda:0')
c= tensor(1.2758e+09, device='cuda:0')
c= tensor(1.2771e+09, device='cuda:0')
c= tensor(1.2771e+09, device='cuda:0')
c= tensor(1.2823e+09, device='cuda:0')
c= tensor(1.2860e+09, device='cuda:0')
c= tensor(1.2883e+09, device='cuda:0')
c= tensor(1.2900e+09, device='cuda:0')
c= tensor(1.2965e+09, device='cuda:0')
c= tensor(1.3087e+09, device='cuda:0')
c= tensor(1.3176e+09, device='cuda:0')
c= tensor(1.3176e+09, device='cuda:0')
c= tensor(1.3176e+09, device='cuda:0')
c= tensor(1.3185e+09, device='cuda:0')
c= tensor(1.3190e+09, device='cuda:0')
c= tensor(1.3190e+09, device='cuda:0')
c= tensor(1.3190e+09, device='cuda:0')
c= tensor(1.3209e+09, device='cuda:0')
c= tensor(1.3247e+09, device='cuda:0')
c= tensor(1.3405e+09, device='cuda:0')
c= tensor(1.3405e+09, device='cuda:0')
c= tensor(1.3409e+09, device='cuda:0')
c= tensor(1.3409e+09, device='cuda:0')
c= tensor(1.3410e+09, device='cuda:0')
c= tensor(1.3410e+09, device='cuda:0')
c= tensor(1.3410e+09, device='cuda:0')
c= tensor(1.3811e+09, device='cuda:0')
c= tensor(1.3816e+09, device='cuda:0')
c= tensor(1.3817e+09, device='cuda:0')
c= tensor(1.3820e+09, device='cuda:0')
c= tensor(1.3821e+09, device='cuda:0')
c= tensor(2.0901e+09, device='cuda:0')
c= tensor(2.0901e+09, device='cuda:0')
c= tensor(2.0943e+09, device='cuda:0')
c= tensor(2.0943e+09, device='cuda:0')
c= tensor(2.0943e+09, device='cuda:0')
c= tensor(2.0943e+09, device='cuda:0')
c= tensor(2.0946e+09, device='cuda:0')
c= tensor(2.0960e+09, device='cuda:0')
c= tensor(2.0968e+09, device='cuda:0')
c= tensor(2.0968e+09, device='cuda:0')
c= tensor(2.0969e+09, device='cuda:0')
c= tensor(2.1157e+09, device='cuda:0')
c= tensor(2.1162e+09, device='cuda:0')
c= tensor(2.1167e+09, device='cuda:0')
c= tensor(2.1183e+09, device='cuda:0')
c= tensor(2.1225e+09, device='cuda:0')
c= tensor(2.1225e+09, device='cuda:0')
c= tensor(2.1225e+09, device='cuda:0')
c= tensor(2.1231e+09, device='cuda:0')
c= tensor(2.1231e+09, device='cuda:0')
c= tensor(2.1231e+09, device='cuda:0')
c= tensor(2.1233e+09, device='cuda:0')
c= tensor(2.1233e+09, device='cuda:0')
c= tensor(2.1233e+09, device='cuda:0')
c= tensor(2.1234e+09, device='cuda:0')
c= tensor(2.1234e+09, device='cuda:0')
c= tensor(2.1831e+09, device='cuda:0')
c= tensor(2.1860e+09, device='cuda:0')
c= tensor(2.1885e+09, device='cuda:0')
c= tensor(2.1890e+09, device='cuda:0')
c= tensor(2.1890e+09, device='cuda:0')
c= tensor(2.1893e+09, device='cuda:0')
c= tensor(2.4498e+09, device='cuda:0')
c= tensor(2.5313e+09, device='cuda:0')
c= tensor(2.5326e+09, device='cuda:0')
c= tensor(2.5338e+09, device='cuda:0')
c= tensor(2.5338e+09, device='cuda:0')
c= tensor(2.5405e+09, device='cuda:0')
c= tensor(2.5569e+09, device='cuda:0')
c= tensor(2.5779e+09, device='cuda:0')
c= tensor(2.5779e+09, device='cuda:0')
c= tensor(2.5823e+09, device='cuda:0')
c= tensor(2.6104e+09, device='cuda:0')
c= tensor(2.6125e+09, device='cuda:0')
c= tensor(2.6125e+09, device='cuda:0')
c= tensor(2.6128e+09, device='cuda:0')
c= tensor(2.6129e+09, device='cuda:0')
c= tensor(2.6129e+09, device='cuda:0')
c= tensor(2.6205e+09, device='cuda:0')
c= tensor(2.6208e+09, device='cuda:0')
c= tensor(2.6208e+09, device='cuda:0')
c= tensor(2.6213e+09, device='cuda:0')
c= tensor(2.6213e+09, device='cuda:0')
c= tensor(2.6214e+09, device='cuda:0')
c= tensor(2.6262e+09, device='cuda:0')
c= tensor(2.6300e+09, device='cuda:0')
c= tensor(2.6318e+09, device='cuda:0')
c= tensor(2.6369e+09, device='cuda:0')
c= tensor(2.6397e+09, device='cuda:0')
c= tensor(2.6399e+09, device='cuda:0')
c= tensor(2.6413e+09, device='cuda:0')
c= tensor(2.6431e+09, device='cuda:0')
c= tensor(2.6481e+09, device='cuda:0')
c= tensor(2.6481e+09, device='cuda:0')
c= tensor(2.6707e+09, device='cuda:0')
c= tensor(2.8275e+09, device='cuda:0')
c= tensor(2.8302e+09, device='cuda:0')
c= tensor(2.8310e+09, device='cuda:0')
c= tensor(2.8351e+09, device='cuda:0')
c= tensor(2.8352e+09, device='cuda:0')
c= tensor(2.8352e+09, device='cuda:0')
c= tensor(2.8361e+09, device='cuda:0')
c= tensor(2.8415e+09, device='cuda:0')
c= tensor(2.8457e+09, device='cuda:0')
c= tensor(2.9065e+09, device='cuda:0')
c= tensor(2.9204e+09, device='cuda:0')
c= tensor(2.9225e+09, device='cuda:0')
c= tensor(2.9229e+09, device='cuda:0')
c= tensor(2.9262e+09, device='cuda:0')
c= tensor(2.9262e+09, device='cuda:0')
c= tensor(2.9262e+09, device='cuda:0')
c= tensor(2.9432e+09, device='cuda:0')
c= tensor(2.9434e+09, device='cuda:0')
c= tensor(2.9434e+09, device='cuda:0')
c= tensor(2.9436e+09, device='cuda:0')
c= tensor(2.9481e+09, device='cuda:0')
c= tensor(2.9483e+09, device='cuda:0')
c= tensor(2.9512e+09, device='cuda:0')
c= tensor(2.9513e+09, device='cuda:0')
c= tensor(2.9515e+09, device='cuda:0')
c= tensor(2.9515e+09, device='cuda:0')
c= tensor(2.9515e+09, device='cuda:0')
c= tensor(2.9523e+09, device='cuda:0')
c= tensor(2.9546e+09, device='cuda:0')
c= tensor(2.9548e+09, device='cuda:0')
c= tensor(2.9611e+09, device='cuda:0')
c= tensor(2.9611e+09, device='cuda:0')
c= tensor(2.9620e+09, device='cuda:0')
c= tensor(2.9620e+09, device='cuda:0')
c= tensor(2.9635e+09, device='cuda:0')
c= tensor(2.9636e+09, device='cuda:0')
c= tensor(2.9677e+09, device='cuda:0')
c= tensor(2.9686e+09, device='cuda:0')
c= tensor(2.9730e+09, device='cuda:0')
c= tensor(2.9739e+09, device='cuda:0')
c= tensor(3.0447e+09, device='cuda:0')
c= tensor(3.0447e+09, device='cuda:0')
c= tensor(3.0448e+09, device='cuda:0')
c= tensor(3.0489e+09, device='cuda:0')
c= tensor(3.0489e+09, device='cuda:0')
c= tensor(3.1007e+09, device='cuda:0')
c= tensor(3.1007e+09, device='cuda:0')
c= tensor(3.1040e+09, device='cuda:0')
c= tensor(3.1157e+09, device='cuda:0')
c= tensor(3.1157e+09, device='cuda:0')
c= tensor(3.1409e+09, device='cuda:0')
c= tensor(3.1421e+09, device='cuda:0')
c= tensor(3.1710e+09, device='cuda:0')
c= tensor(3.1711e+09, device='cuda:0')
c= tensor(3.1722e+09, device='cuda:0')
c= tensor(3.1722e+09, device='cuda:0')
c= tensor(3.1722e+09, device='cuda:0')
c= tensor(3.1722e+09, device='cuda:0')
c= tensor(3.1740e+09, device='cuda:0')
c= tensor(3.1743e+09, device='cuda:0')
c= tensor(3.1773e+09, device='cuda:0')
c= tensor(3.1774e+09, device='cuda:0')
c= tensor(3.1774e+09, device='cuda:0')
c= tensor(3.1774e+09, device='cuda:0')
c= tensor(3.1811e+09, device='cuda:0')
c= tensor(3.1814e+09, device='cuda:0')
c= tensor(3.1893e+09, device='cuda:0')
c= tensor(3.1943e+09, device='cuda:0')
c= tensor(3.1944e+09, device='cuda:0')
c= tensor(3.1947e+09, device='cuda:0')
c= tensor(3.1949e+09, device='cuda:0')
c= tensor(3.5302e+09, device='cuda:0')
c= tensor(3.5302e+09, device='cuda:0')
c= tensor(3.5303e+09, device='cuda:0')
c= tensor(3.5365e+09, device='cuda:0')
c= tensor(3.5374e+09, device='cuda:0')
c= tensor(3.5374e+09, device='cuda:0')
c= tensor(3.5374e+09, device='cuda:0')
c= tensor(3.5854e+09, device='cuda:0')
c= tensor(3.5858e+09, device='cuda:0')
c= tensor(3.5877e+09, device='cuda:0')
c= tensor(3.5894e+09, device='cuda:0')
c= tensor(3.5931e+09, device='cuda:0')
c= tensor(3.5960e+09, device='cuda:0')
c= tensor(3.6106e+09, device='cuda:0')
c= tensor(3.6127e+09, device='cuda:0')
c= tensor(3.6127e+09, device='cuda:0')
c= tensor(3.6137e+09, device='cuda:0')
c= tensor(3.6137e+09, device='cuda:0')
c= tensor(3.6139e+09, device='cuda:0')
c= tensor(3.6142e+09, device='cuda:0')
c= tensor(3.6142e+09, device='cuda:0')
c= tensor(3.6209e+09, device='cuda:0')
c= tensor(3.6210e+09, device='cuda:0')
c= tensor(3.6210e+09, device='cuda:0')
c= tensor(3.6212e+09, device='cuda:0')
c= tensor(3.6213e+09, device='cuda:0')
c= tensor(3.6307e+09, device='cuda:0')
c= tensor(3.6333e+09, device='cuda:0')
c= tensor(3.6334e+09, device='cuda:0')
c= tensor(3.6344e+09, device='cuda:0')
c= tensor(3.6348e+09, device='cuda:0')
c= tensor(3.6348e+09, device='cuda:0')
c= tensor(3.6348e+09, device='cuda:0')
c= tensor(3.6350e+09, device='cuda:0')
c= tensor(3.6417e+09, device='cuda:0')
c= tensor(3.6418e+09, device='cuda:0')
c= tensor(3.6418e+09, device='cuda:0')
c= tensor(3.6418e+09, device='cuda:0')
c= tensor(3.6617e+09, device='cuda:0')
c= tensor(3.6950e+09, device='cuda:0')
c= tensor(3.6959e+09, device='cuda:0')
c= tensor(3.6959e+09, device='cuda:0')
c= tensor(3.6980e+09, device='cuda:0')
c= tensor(3.7009e+09, device='cuda:0')
c= tensor(3.7009e+09, device='cuda:0')
c= tensor(3.7010e+09, device='cuda:0')
c= tensor(3.7012e+09, device='cuda:0')
c= tensor(3.7228e+09, device='cuda:0')
c= tensor(3.7422e+09, device='cuda:0')
c= tensor(3.7456e+09, device='cuda:0')
c= tensor(3.7457e+09, device='cuda:0')
c= tensor(3.7476e+09, device='cuda:0')
c= tensor(3.7476e+09, device='cuda:0')
c= tensor(3.7481e+09, device='cuda:0')
c= tensor(3.7483e+09, device='cuda:0')
c= tensor(3.7507e+09, device='cuda:0')
c= tensor(3.7511e+09, device='cuda:0')
c= tensor(3.7664e+09, device='cuda:0')
c= tensor(3.7665e+09, device='cuda:0')
c= tensor(3.7672e+09, device='cuda:0')
c= tensor(3.7674e+09, device='cuda:0')
c= tensor(4.0765e+09, device='cuda:0')
c= tensor(4.0772e+09, device='cuda:0')
c= tensor(4.0775e+09, device='cuda:0')
c= tensor(4.0775e+09, device='cuda:0')
c= tensor(4.0782e+09, device='cuda:0')
c= tensor(4.0782e+09, device='cuda:0')
c= tensor(4.0792e+09, device='cuda:0')
c= tensor(4.0792e+09, device='cuda:0')
c= tensor(4.0793e+09, device='cuda:0')
c= tensor(4.0794e+09, device='cuda:0')
c= tensor(4.0794e+09, device='cuda:0')
c= tensor(4.0794e+09, device='cuda:0')
c= tensor(4.0858e+09, device='cuda:0')
c= tensor(4.1223e+09, device='cuda:0')
c= tensor(4.1288e+09, device='cuda:0')
c= tensor(4.1295e+09, device='cuda:0')
c= tensor(4.1299e+09, device='cuda:0')
c= tensor(4.1300e+09, device='cuda:0')
c= tensor(4.1301e+09, device='cuda:0')
c= tensor(4.1320e+09, device='cuda:0')
c= tensor(4.1344e+09, device='cuda:0')
c= tensor(4.1374e+09, device='cuda:0')
c= tensor(4.1374e+09, device='cuda:0')
c= tensor(4.2206e+09, device='cuda:0')
c= tensor(4.2209e+09, device='cuda:0')
c= tensor(4.2214e+09, device='cuda:0')
c= tensor(4.2321e+09, device='cuda:0')
c= tensor(4.2328e+09, device='cuda:0')
c= tensor(4.2486e+09, device='cuda:0')
c= tensor(4.2664e+09, device='cuda:0')
c= tensor(4.2691e+09, device='cuda:0')
c= tensor(4.2696e+09, device='cuda:0')
c= tensor(4.2699e+09, device='cuda:0')
c= tensor(4.2709e+09, device='cuda:0')
c= tensor(4.2709e+09, device='cuda:0')
c= tensor(4.2748e+09, device='cuda:0')
c= tensor(4.3674e+09, device='cuda:0')
c= tensor(4.3828e+09, device='cuda:0')
c= tensor(4.3861e+09, device='cuda:0')
c= tensor(4.3864e+09, device='cuda:0')
c= tensor(4.3870e+09, device='cuda:0')
c= tensor(4.3872e+09, device='cuda:0')
c= tensor(4.3875e+09, device='cuda:0')
c= tensor(4.3889e+09, device='cuda:0')
c= tensor(4.4035e+09, device='cuda:0')
c= tensor(4.4036e+09, device='cuda:0')
c= tensor(4.4141e+09, device='cuda:0')
c= tensor(4.4285e+09, device='cuda:0')
c= tensor(4.4290e+09, device='cuda:0')
c= tensor(4.4290e+09, device='cuda:0')
c= tensor(4.4325e+09, device='cuda:0')
c= tensor(4.4357e+09, device='cuda:0')
c= tensor(4.4357e+09, device='cuda:0')
c= tensor(4.5203e+09, device='cuda:0')
c= tensor(4.5210e+09, device='cuda:0')
c= tensor(4.5214e+09, device='cuda:0')
c= tensor(4.5216e+09, device='cuda:0')
c= tensor(4.5216e+09, device='cuda:0')
c= tensor(4.5216e+09, device='cuda:0')
c= tensor(4.5217e+09, device='cuda:0')
c= tensor(4.5220e+09, device='cuda:0')
c= tensor(4.5270e+09, device='cuda:0')
c= tensor(6.0072e+09, device='cuda:0')
c= tensor(6.0074e+09, device='cuda:0')
c= tensor(6.0090e+09, device='cuda:0')
c= tensor(6.0090e+09, device='cuda:0')
c= tensor(6.0091e+09, device='cuda:0')
c= tensor(6.0092e+09, device='cuda:0')
c= tensor(6.0419e+09, device='cuda:0')
c= tensor(6.0426e+09, device='cuda:0')
c= tensor(6.2118e+09, device='cuda:0')
c= tensor(6.2119e+09, device='cuda:0')
c= tensor(6.2187e+09, device='cuda:0')
c= tensor(6.2193e+09, device='cuda:0')
c= tensor(6.2250e+09, device='cuda:0')
c= tensor(6.2484e+09, device='cuda:0')
c= tensor(6.2485e+09, device='cuda:0')
c= tensor(6.2485e+09, device='cuda:0')
c= tensor(6.2498e+09, device='cuda:0')
c= tensor(6.2499e+09, device='cuda:0')
c= tensor(6.2512e+09, device='cuda:0')
c= tensor(6.2548e+09, device='cuda:0')
c= tensor(6.2563e+09, device='cuda:0')
c= tensor(6.2578e+09, device='cuda:0')
c= tensor(6.2581e+09, device='cuda:0')
c= tensor(6.2641e+09, device='cuda:0')
c= tensor(6.2955e+09, device='cuda:0')
c= tensor(6.2968e+09, device='cuda:0')
c= tensor(6.2968e+09, device='cuda:0')
c= tensor(6.3103e+09, device='cuda:0')
c= tensor(6.3109e+09, device='cuda:0')
c= tensor(6.3132e+09, device='cuda:0')
c= tensor(6.3144e+09, device='cuda:0')
c= tensor(6.3170e+09, device='cuda:0')
c= tensor(6.3178e+09, device='cuda:0')
c= tensor(6.3322e+09, device='cuda:0')
c= tensor(6.3332e+09, device='cuda:0')
c= tensor(6.3333e+09, device='cuda:0')
c= tensor(6.3333e+09, device='cuda:0')
c= tensor(6.3335e+09, device='cuda:0')
c= tensor(6.3364e+09, device='cuda:0')
c= tensor(6.3408e+09, device='cuda:0')
c= tensor(6.3440e+09, device='cuda:0')
c= tensor(6.3441e+09, device='cuda:0')
c= tensor(6.3455e+09, device='cuda:0')
c= tensor(6.3466e+09, device='cuda:0')
c= tensor(6.3795e+09, device='cuda:0')
c= tensor(6.3801e+09, device='cuda:0')
c= tensor(6.3813e+09, device='cuda:0')
c= tensor(6.3850e+09, device='cuda:0')
c= tensor(6.3854e+09, device='cuda:0')
c= tensor(6.3854e+09, device='cuda:0')
c= tensor(6.3854e+09, device='cuda:0')
c= tensor(6.3918e+09, device='cuda:0')
c= tensor(6.3930e+09, device='cuda:0')
c= tensor(6.3941e+09, device='cuda:0')
c= tensor(6.3941e+09, device='cuda:0')
c= tensor(6.3941e+09, device='cuda:0')
c= tensor(6.4071e+09, device='cuda:0')
c= tensor(6.4103e+09, device='cuda:0')
c= tensor(6.4110e+09, device='cuda:0')
c= tensor(6.4110e+09, device='cuda:0')
c= tensor(6.4111e+09, device='cuda:0')
c= tensor(6.4114e+09, device='cuda:0')
c= tensor(6.4122e+09, device='cuda:0')
c= tensor(6.4168e+09, device='cuda:0')
c= tensor(6.4168e+09, device='cuda:0')
c= tensor(6.4211e+09, device='cuda:0')
c= tensor(6.4212e+09, device='cuda:0')
c= tensor(6.4214e+09, device='cuda:0')
c= tensor(6.4541e+09, device='cuda:0')
c= tensor(6.4545e+09, device='cuda:0')
c= tensor(6.4563e+09, device='cuda:0')
c= tensor(6.4617e+09, device='cuda:0')
c= tensor(6.4623e+09, device='cuda:0')
c= tensor(6.4640e+09, device='cuda:0')
c= tensor(6.4680e+09, device='cuda:0')
c= tensor(6.4680e+09, device='cuda:0')
c= tensor(6.4685e+09, device='cuda:0')
c= tensor(6.4688e+09, device='cuda:0')
c= tensor(6.4746e+09, device='cuda:0')
c= tensor(6.4802e+09, device='cuda:0')
c= tensor(6.4812e+09, device='cuda:0')
c= tensor(6.4821e+09, device='cuda:0')
c= tensor(6.4822e+09, device='cuda:0')
c= tensor(6.4828e+09, device='cuda:0')
c= tensor(6.4831e+09, device='cuda:0')
c= tensor(6.4831e+09, device='cuda:0')
c= tensor(6.4934e+09, device='cuda:0')
c= tensor(6.5058e+09, device='cuda:0')
c= tensor(6.5058e+09, device='cuda:0')
c= tensor(6.5061e+09, device='cuda:0')
c= tensor(6.5061e+09, device='cuda:0')
c= tensor(6.5591e+09, device='cuda:0')
c= tensor(6.5592e+09, device='cuda:0')
c= tensor(6.5593e+09, device='cuda:0')
c= tensor(6.5619e+09, device='cuda:0')
c= tensor(6.5635e+09, device='cuda:0')
c= tensor(6.5635e+09, device='cuda:0')
c= tensor(6.5640e+09, device='cuda:0')
c= tensor(6.5641e+09, device='cuda:0')
time to make c is 8.462402582168579
time for making loss is 8.462428331375122
p0 True
it  0 : 2700591616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 13% |
shape of L is 
torch.Size([])
memory (bytes)
5488431104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 13% |
memory (bytes)
5488640000
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1273684500.0
relative error loss 0.19403881
shape of L is 
torch.Size([])
memory (bytes)
5514207232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 13% |
memory (bytes)
5514424320
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1259872800.0
relative error loss 0.19193466
shape of L is 
torch.Size([])
memory (bytes)
5518270464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
memory (bytes)
5518270464
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1229250600.0
relative error loss 0.18726954
shape of L is 
torch.Size([])
memory (bytes)
5521362944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 13% |
memory (bytes)
5521453056
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1217736200.0
relative error loss 0.18551539
shape of L is 
torch.Size([])
memory (bytes)
5524656128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 13% |
memory (bytes)
5524656128
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1208393200.0
relative error loss 0.18409204
shape of L is 
torch.Size([])
memory (bytes)
5527842816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 13% |
memory (bytes)
5527859200
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1193836500.0
relative error loss 0.18187441
shape of L is 
torch.Size([])
memory (bytes)
5531074560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
memory (bytes)
5531074560
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1185503200.0
relative error loss 0.18060488
shape of L is 
torch.Size([])
memory (bytes)
5534277632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
memory (bytes)
5534277632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% | 13% |
error is  1175992300.0
relative error loss 0.17915595
shape of L is 
torch.Size([])
memory (bytes)
5537386496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 13% |
memory (bytes)
5537386496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% | 13% |
error is  1167660000.0
relative error loss 0.17788656
shape of L is 
torch.Size([])
memory (bytes)
5540687872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% | 13% |
memory (bytes)
5540687872
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1163045400.0
relative error loss 0.17718355
time to take a step is 271.23780846595764
it  1 : 3493825536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 13% |
shape of L is 
torch.Size([])
memory (bytes)
5543890944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 13% |
memory (bytes)
5543890944
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1163045400.0
relative error loss 0.17718355
shape of L is 
torch.Size([])
memory (bytes)
5547122688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 13% |
memory (bytes)
5547122688
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1157638100.0
relative error loss 0.17635979
shape of L is 
torch.Size([])
memory (bytes)
5550252032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 13% |
memory (bytes)
5550338048
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1152088000.0
relative error loss 0.17551427
shape of L is 
torch.Size([])
memory (bytes)
5553532928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 13% |
memory (bytes)
5553541120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% | 13% |
error is  1148696600.0
relative error loss 0.17499758
shape of L is 
torch.Size([])
memory (bytes)
5556703232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 13% |
memory (bytes)
5556760576
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1143404500.0
relative error loss 0.17419137
shape of L is 
torch.Size([])
memory (bytes)
5559967744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
memory (bytes)
5559967744
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1140507600.0
relative error loss 0.17375004
shape of L is 
torch.Size([])
memory (bytes)
5563097088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
memory (bytes)
5563187200
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1137840600.0
relative error loss 0.17334375
shape of L is 
torch.Size([])
memory (bytes)
5566390272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
memory (bytes)
5566390272
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1136779300.0
relative error loss 0.17318206
shape of L is 
torch.Size([])
memory (bytes)
5569511424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 13% |
memory (bytes)
5569601536
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1132388400.0
relative error loss 0.17251311
shape of L is 
torch.Size([])
memory (bytes)
5572804608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 13% |
memory (bytes)
5572804608
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1130786800.0
relative error loss 0.17226914
time to take a step is 270.6609501838684
it  2 : 3493825536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
shape of L is 
torch.Size([])
memory (bytes)
5575917568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
memory (bytes)
5576032256
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1130786800.0
relative error loss 0.17226914
shape of L is 
torch.Size([])
memory (bytes)
5579227136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
memory (bytes)
5579227136
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1129300000.0
relative error loss 0.17204262
shape of L is 
torch.Size([])
memory (bytes)
5582446592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 13% |
memory (bytes)
5582446592
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1126440400.0
relative error loss 0.17160699
shape of L is 
torch.Size([])
memory (bytes)
5585649664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 13% |
memory (bytes)
5585649664
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1127784400.0
relative error loss 0.17181174
shape of L is 
torch.Size([])
memory (bytes)
5588865024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 13% |
memory (bytes)
5588865024
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1124719600.0
relative error loss 0.17134483
shape of L is 
torch.Size([])
memory (bytes)
5592014848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 13% |
memory (bytes)
5592076288
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1122742800.0
relative error loss 0.17104366
shape of L is 
torch.Size([])
memory (bytes)
5595299840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
memory (bytes)
5595299840
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1120987100.0
relative error loss 0.1707762
shape of L is 
torch.Size([])
memory (bytes)
5598412800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 37% | 13% |
memory (bytes)
5598498816
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1119649800.0
relative error loss 0.17057247
shape of L is 
torch.Size([])
memory (bytes)
5601705984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% | 13% |
memory (bytes)
5601705984
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1118121500.0
relative error loss 0.17033964
shape of L is 
torch.Size([])
memory (bytes)
5604823040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 37% | 13% |
memory (bytes)
5604913152
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1117162000.0
relative error loss 0.17019346
time to take a step is 271.52677845954895
it  3 : 3493825536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
shape of L is 
torch.Size([])
memory (bytes)
5608120320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
memory (bytes)
5608120320
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1117162000.0
relative error loss 0.17019346
shape of L is 
torch.Size([])
memory (bytes)
5611204608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 13% |
memory (bytes)
5611323392
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1116188700.0
relative error loss 0.17004518
shape of L is 
torch.Size([])
memory (bytes)
5614538752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 13% |
memory (bytes)
5614538752
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1114728400.0
relative error loss 0.16982274
shape of L is 
torch.Size([])
memory (bytes)
5617590272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 36% | 13% |
memory (bytes)
5617741824
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1113630200.0
relative error loss 0.16965543
shape of L is 
torch.Size([])
memory (bytes)
5620953088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 13% |
memory (bytes)
5620953088
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1112993300.0
relative error loss 0.16955839
shape of L is 
torch.Size([])
memory (bytes)
5624164352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 37% | 13% |
memory (bytes)
5624164352
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1112335900.0
relative error loss 0.16945824
shape of L is 
torch.Size([])
memory (bytes)
5627305984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
memory (bytes)
5627305984
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1111525400.0
relative error loss 0.16933475
shape of L is 
torch.Size([])
memory (bytes)
5630582784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 37% | 13% |
memory (bytes)
5630582784
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1110798800.0
relative error loss 0.16922408
shape of L is 
torch.Size([])
memory (bytes)
5633785856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 13% |
memory (bytes)
5633785856
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1110172200.0
relative error loss 0.16912861
shape of L is 
torch.Size([])
memory (bytes)
5636997120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 13% |
memory (bytes)
5636997120
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1109128200.0
relative error loss 0.16896956
time to take a step is 272.8685231208801
it  4 : 3493825536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 13% |
shape of L is 
torch.Size([])
memory (bytes)
5640126464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 13% |
memory (bytes)
5640220672
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1109128200.0
relative error loss 0.16896956
shape of L is 
torch.Size([])
memory (bytes)
5643423744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 13% |
memory (bytes)
5643423744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% | 13% |
error is  1108375000.0
relative error loss 0.16885482
shape of L is 
torch.Size([])
memory (bytes)
5646585856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 13% |
memory (bytes)
5646635008
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1107431400.0
relative error loss 0.16871107
shape of L is 
torch.Size([])
memory (bytes)
5649858560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 13% |
memory (bytes)
5649858560
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1106675700.0
relative error loss 0.16859594
shape of L is 
torch.Size([])
memory (bytes)
5653073920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 13% |
memory (bytes)
5653073920
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1106031100.0
relative error loss 0.16849774
shape of L is 
torch.Size([])
memory (bytes)
5656190976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 13% |
memory (bytes)
5656190976
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1105442800.0
relative error loss 0.16840811
shape of L is 
torch.Size([])
memory (bytes)
5659488256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 13% |
memory (bytes)
5659488256
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1104772100.0
relative error loss 0.16830593
shape of L is 
torch.Size([])
memory (bytes)
5662646272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 13% |
memory (bytes)
5662703616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% | 13% |
error is  1104151000.0
relative error loss 0.16821133
shape of L is 
torch.Size([])
memory (bytes)
5665914880
| ID | GPU | MEM |
------------------
|  0 | 19% |  0% |
|  1 | 16% | 13% |
memory (bytes)
5665914880
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1103830000.0
relative error loss 0.16816242
shape of L is 
torch.Size([])
memory (bytes)
5669117952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 13% |
memory (bytes)
5669117952
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1103210000.0
relative error loss 0.16806796
time to take a step is 269.1682243347168
it  5 : 3493825536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 13% |
shape of L is 
torch.Size([])
memory (bytes)
5672312832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 13% |
memory (bytes)
5672312832
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1103210000.0
relative error loss 0.16806796
shape of L is 
torch.Size([])
memory (bytes)
5675515904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 13% |
memory (bytes)
5675515904
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1102830600.0
relative error loss 0.16801016
shape of L is 
torch.Size([])
memory (bytes)
5678735360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 32% | 13% |
memory (bytes)
5678735360
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1102461400.0
relative error loss 0.16795392
shape of L is 
torch.Size([])
memory (bytes)
5681954816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
memory (bytes)
5681954816
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1102003700.0
relative error loss 0.16788419
shape of L is 
torch.Size([])
memory (bytes)
5685157888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 32% | 13% |
memory (bytes)
5685157888
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1101388300.0
relative error loss 0.16779043
shape of L is 
torch.Size([])
memory (bytes)
5688369152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 13% |
memory (bytes)
5688369152
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1100926500.0
relative error loss 0.16772008
shape of L is 
torch.Size([])
memory (bytes)
5691572224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% | 13% |
memory (bytes)
5691572224
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1100520400.0
relative error loss 0.16765822
shape of L is 
torch.Size([])
memory (bytes)
5694795776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 37% | 13% |
memory (bytes)
5694795776
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1100060200.0
relative error loss 0.1675881
shape of L is 
torch.Size([])
memory (bytes)
5697900544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 13% |
memory (bytes)
5697900544
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1099316700.0
relative error loss 0.16747484
shape of L is 
torch.Size([])
memory (bytes)
5701201920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
memory (bytes)
5701201920
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1099044900.0
relative error loss 0.16743343
time to take a step is 251.7543749809265
it  6 : 3493825536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
shape of L is 
torch.Size([])
memory (bytes)
5704286208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
memory (bytes)
5704396800
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1099044900.0
relative error loss 0.16743343
shape of L is 
torch.Size([])
memory (bytes)
5707603968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 13% |
memory (bytes)
5707603968
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1098710500.0
relative error loss 0.1673825
shape of L is 
torch.Size([])
memory (bytes)
5710798848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 13% |
memory (bytes)
5710819328
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1098129900.0
relative error loss 0.16729404
shape of L is 
torch.Size([])
memory (bytes)
5713915904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 13% |
memory (bytes)
5714026496
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1097599000.0
relative error loss 0.16721316
shape of L is 
torch.Size([])
memory (bytes)
5717245952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
memory (bytes)
5717245952
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1097270300.0
relative error loss 0.16716307
shape of L is 
torch.Size([])
memory (bytes)
5720375296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
memory (bytes)
5720465408
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1097000400.0
relative error loss 0.16712196
shape of L is 
torch.Size([])
memory (bytes)
5723664384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 13% |
memory (bytes)
5723664384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 13% |
error is  1096546300.0
relative error loss 0.16705278
shape of L is 
torch.Size([])
memory (bytes)
5726760960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 36% | 13% |
memory (bytes)
5726875648
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1096130000.0
relative error loss 0.16698937
shape of L is 
torch.Size([])
memory (bytes)
5730091008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 13% |
memory (bytes)
5730091008
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1095965200.0
relative error loss 0.16696425
shape of L is 
torch.Size([])
memory (bytes)
5733191680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 38% | 13% |
memory (bytes)
5733302272
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1095460400.0
relative error loss 0.16688734
time to take a step is 250.3757450580597
it  7 : 3493825536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 13% |
shape of L is 
torch.Size([])
memory (bytes)
5736509440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 13% |
memory (bytes)
5736509440
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1095460400.0
relative error loss 0.16688734
shape of L is 
torch.Size([])
memory (bytes)
5739626496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 13% |
memory (bytes)
5739626496
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1095243800.0
relative error loss 0.16685435
shape of L is 
torch.Size([])
memory (bytes)
5742915584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
memory (bytes)
5742915584
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1094956500.0
relative error loss 0.16681059
shape of L is 
torch.Size([])
memory (bytes)
5746110464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 13% |
memory (bytes)
5746110464
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1094611500.0
relative error loss 0.16675802
shape of L is 
torch.Size([])
memory (bytes)
5749248000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 13% |
memory (bytes)
5749338112
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1094290400.0
relative error loss 0.16670911
shape of L is 
torch.Size([])
memory (bytes)
5752553472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 13% |
memory (bytes)
5752553472
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1095626800.0
relative error loss 0.16691269
shape of L is 
torch.Size([])
memory (bytes)
5755740160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 13% |
memory (bytes)
5755756544
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1094171100.0
relative error loss 0.16669093
shape of L is 
torch.Size([])
memory (bytes)
5758963712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 13% |
memory (bytes)
5758963712
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1093956600.0
relative error loss 0.16665825
shape of L is 
torch.Size([])
memory (bytes)
5762101248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 34% | 13% |
memory (bytes)
5762187264
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1093707300.0
relative error loss 0.16662027
shape of L is 
torch.Size([])
memory (bytes)
5765390336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 13% |
memory (bytes)
5765390336
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1093620200.0
relative error loss 0.166607
time to take a step is 250.84550952911377
it  8 : 3493825536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 13% |
shape of L is 
torch.Size([])
memory (bytes)
5768519680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 13% |
memory (bytes)
5768613888
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1093620200.0
relative error loss 0.166607
shape of L is 
torch.Size([])
memory (bytes)
5771821056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 13% |
memory (bytes)
5771821056
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1093325300.0
relative error loss 0.16656208
shape of L is 
torch.Size([])
memory (bytes)
5774958592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
memory (bytes)
5774958592
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1093198300.0
relative error loss 0.16654274
shape of L is 
torch.Size([])
memory (bytes)
5778116608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 13% |
memory (bytes)
5778231296
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1093037000.0
relative error loss 0.16651817
shape of L is 
torch.Size([])
memory (bytes)
5781434368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
memory (bytes)
5781434368
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1092782600.0
relative error loss 0.1664794
shape of L is 
torch.Size([])
memory (bytes)
5784543232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 38% | 13% |
memory (bytes)
5784649728
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1092836900.0
relative error loss 0.16648766
shape of L is 
torch.Size([])
memory (bytes)
5787865088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
memory (bytes)
5787865088
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1092627500.0
relative error loss 0.16645576
shape of L is 
torch.Size([])
memory (bytes)
5791051776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 13% |
memory (bytes)
5791076352
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1092436500.0
relative error loss 0.16642667
shape of L is 
torch.Size([])
memory (bytes)
5794291712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 13% |
memory (bytes)
5794291712
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1092331500.0
relative error loss 0.16641068
shape of L is 
torch.Size([])
memory (bytes)
5797359616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 36% | 13% |
memory (bytes)
5797482496
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1092133900.0
relative error loss 0.16638057
time to take a step is 250.6468276977539
it  9 : 3493825536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 13% |
shape of L is 
torch.Size([])
memory (bytes)
5800701952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 13% |
memory (bytes)
5800701952
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1092133900.0
relative error loss 0.16638057
shape of L is 
torch.Size([])
memory (bytes)
5803819008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 13% |
memory (bytes)
5803819008
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1091975700.0
relative error loss 0.16635647
shape of L is 
torch.Size([])
memory (bytes)
5807132672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 36% | 13% |
memory (bytes)
5807132672
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1091856400.0
relative error loss 0.1663383
shape of L is 
torch.Size([])
memory (bytes)
5810348032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 34% | 13% |
memory (bytes)
5810348032
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1091734000.0
relative error loss 0.16631965
shape of L is 
torch.Size([])
memory (bytes)
5813547008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 13% |
memory (bytes)
5813547008
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1091660800.0
relative error loss 0.1663085
shape of L is 
torch.Size([])
memory (bytes)
5816754176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
memory (bytes)
5816754176
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1091594800.0
relative error loss 0.16629843
shape of L is 
torch.Size([])
memory (bytes)
5819965440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 37% | 13% |
memory (bytes)
5819965440
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1091541500.0
relative error loss 0.16629033
shape of L is 
torch.Size([])
memory (bytes)
5823176704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 13% |
memory (bytes)
5823176704
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1091502600.0
relative error loss 0.1662844
shape of L is 
torch.Size([])
memory (bytes)
5826293760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 13% |
memory (bytes)
5826383872
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1091488300.0
relative error loss 0.16628222
shape of L is 
torch.Size([])
memory (bytes)
5829595136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 13% |
memory (bytes)
5829595136
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1091386900.0
relative error loss 0.16626677
time to take a step is 251.02893137931824
it  10 : 3493825536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 13% |
shape of L is 
torch.Size([])
memory (bytes)
5832794112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 13% |
memory (bytes)
5832794112
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1091386900.0
relative error loss 0.16626677
shape of L is 
torch.Size([])
memory (bytes)
5835984896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 13% |
memory (bytes)
5836001280
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1091361300.0
relative error loss 0.16626287
shape of L is 
torch.Size([])
memory (bytes)
5839204352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 13% |
memory (bytes)
5839204352
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1091311100.0
relative error loss 0.16625522
shape of L is 
torch.Size([])
memory (bytes)
5842317312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 13% |
memory (bytes)
5842411520
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1091238900.0
relative error loss 0.16624422
shape of L is 
torch.Size([])
memory (bytes)
5845630976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 13% |
memory (bytes)
5845630976
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1091165700.0
relative error loss 0.16623308
shape of L is 
torch.Size([])
memory (bytes)
5848797184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 13% |
memory (bytes)
5848825856
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1091010000.0
relative error loss 0.16620937
shape of L is 
torch.Size([])
memory (bytes)
5852041216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 13% |
memory (bytes)
5852041216
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1090883100.0
relative error loss 0.16619001
shape of L is 
torch.Size([])
memory (bytes)
5855158272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
memory (bytes)
5855158272
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1090686000.0
relative error loss 0.16615999
shape of L is 
torch.Size([])
memory (bytes)
5858467840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 13% |
memory (bytes)
5858467840
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1090409000.0
relative error loss 0.16611779
shape of L is 
torch.Size([])
memory (bytes)
5861654528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 37% | 13% |
memory (bytes)
5861654528
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1090116600.0
relative error loss 0.16607325
time to take a step is 255.5168399810791
it  11 : 3493825536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
shape of L is 
torch.Size([])
memory (bytes)
5864882176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
memory (bytes)
5864882176
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1090116600.0
relative error loss 0.16607325
shape of L is 
torch.Size([])
memory (bytes)
5868093440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 34% | 13% |
memory (bytes)
5868093440
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1089916400.0
relative error loss 0.16604276
shape of L is 
torch.Size([])
memory (bytes)
5871316992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 13% |
memory (bytes)
5871316992
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1089754600.0
relative error loss 0.16601811
shape of L is 
torch.Size([])
memory (bytes)
5874528256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 13% |
memory (bytes)
5874528256
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1089517000.0
relative error loss 0.16598192
shape of L is 
torch.Size([])
memory (bytes)
5877600256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 13% |
memory (bytes)
5877735424
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1089389600.0
relative error loss 0.16596249
shape of L is 
torch.Size([])
memory (bytes)
5880950784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 13% |
memory (bytes)
5880950784
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1089272800.0
relative error loss 0.16594471
shape of L is 
torch.Size([])
memory (bytes)
5884145664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 13% |
memory (bytes)
5884162048
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1089191400.0
relative error loss 0.16593231
shape of L is 
torch.Size([])
memory (bytes)
5887377408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
memory (bytes)
5887377408
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1089108500.0
relative error loss 0.16591968
shape of L is 
torch.Size([])
memory (bytes)
5890428928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 13% |
memory (bytes)
5890596864
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1089043500.0
relative error loss 0.16590977
shape of L is 
torch.Size([])
memory (bytes)
5893808128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
memory (bytes)
5893808128
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1088939000.0
relative error loss 0.16589385
time to take a step is 258.8487403392792
it  12 : 3493825536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
shape of L is 
torch.Size([])
memory (bytes)
5896921088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
memory (bytes)
5896921088
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1088939000.0
relative error loss 0.16589385
shape of L is 
torch.Size([])
memory (bytes)
5900214272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
memory (bytes)
5900214272
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1088811500.0
relative error loss 0.16587444
shape of L is 
torch.Size([])
memory (bytes)
5903421440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 13% |
memory (bytes)
5903421440
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1088716300.0
relative error loss 0.16585992
shape of L is 
torch.Size([])
memory (bytes)
5906624512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
memory (bytes)
5906624512
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1088526800.0
relative error loss 0.16583106
shape of L is 
torch.Size([])
memory (bytes)
5909843968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
memory (bytes)
5909843968
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1088363500.0
relative error loss 0.16580617
shape of L is 
torch.Size([])
memory (bytes)
5912895488
| ID | GPU | MEM |
------------------
|  0 |  1% |  0% |
|  1 | 16% | 13% |
memory (bytes)
5913063424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% | 13% |
error is  1088233000.0
relative error loss 0.1657863
shape of L is 
torch.Size([])
memory (bytes)
5916254208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 13% |
memory (bytes)
5916254208
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1088155600.0
relative error loss 0.16577451
shape of L is 
torch.Size([])
memory (bytes)
5919322112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 32% | 13% |
memory (bytes)
5919465472
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1088013800.0
relative error loss 0.1657529
shape of L is 
torch.Size([])
memory (bytes)
5922672640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 13% |
memory (bytes)
5922672640
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1088144900.0
relative error loss 0.16577287
shape of L is 
torch.Size([])
memory (bytes)
5925888000
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 13% |
memory (bytes)
5925888000
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1087934000.0
relative error loss 0.16574074
time to take a step is 250.26140141487122
it  13 : 3493826048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
shape of L is 
torch.Size([])
memory (bytes)
5929103360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
memory (bytes)
5929103360
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1087934000.0
relative error loss 0.16574074
shape of L is 
torch.Size([])
memory (bytes)
5932244992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
memory (bytes)
5932244992
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1087812600.0
relative error loss 0.16572225
shape of L is 
torch.Size([])
memory (bytes)
5935501312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 13% |
memory (bytes)
5935501312
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1087663600.0
relative error loss 0.16569956
shape of L is 
torch.Size([])
memory (bytes)
5938728960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 13% |
memory (bytes)
5938728960
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1087601200.0
relative error loss 0.16569003
shape of L is 
torch.Size([])
memory (bytes)
5941874688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 35% | 13% |
memory (bytes)
5941923840
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1087481900.0
relative error loss 0.16567187
shape of L is 
torch.Size([])
memory (bytes)
5945139200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 13% |
memory (bytes)
5945139200
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1087415300.0
relative error loss 0.16566172
shape of L is 
torch.Size([])
memory (bytes)
5948252160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
memory (bytes)
5948342272
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1087362000.0
relative error loss 0.16565362
shape of L is 
torch.Size([])
memory (bytes)
5951557632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 13% |
memory (bytes)
5951557632
| ID | GPU  | MEM |
-------------------
|  0 |   2% |  0% |
|  1 | 100% | 13% |
error is  1087221200.0
relative error loss 0.16563216
shape of L is 
torch.Size([])
memory (bytes)
5954760704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 13% |
memory (bytes)
5954760704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 13% |
error is  1087147500.0
relative error loss 0.16562092
shape of L is 
torch.Size([])
memory (bytes)
5957980160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 13% |
memory (bytes)
5957980160
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1087025200.0
relative error loss 0.16560228
time to take a step is 248.91766023635864
it  14 : 3493825536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
shape of L is 
torch.Size([])
memory (bytes)
5961183232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
memory (bytes)
5961183232
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1087025200.0
relative error loss 0.16560228
shape of L is 
torch.Size([])
memory (bytes)
5964386304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 13% |
memory (bytes)
5964386304
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1086962200.0
relative error loss 0.16559269
shape of L is 
torch.Size([])
memory (bytes)
5967523840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
memory (bytes)
5967523840
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1086752300.0
relative error loss 0.1655607
shape of L is 
torch.Size([])
memory (bytes)
5970812928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 13% |
memory (bytes)
5970812928
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1086924800.0
relative error loss 0.165587
shape of L is 
torch.Size([])
memory (bytes)
5974011904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 13% |
memory (bytes)
5974011904
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1086665200.0
relative error loss 0.16554745
shape of L is 
torch.Size([])
memory (bytes)
5977190400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 13% |
memory (bytes)
5977223168
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1086560800.0
relative error loss 0.16553155
shape of L is 
torch.Size([])
memory (bytes)
5980442624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 13% |
memory (bytes)
5980442624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 13% |
error is  1086381600.0
relative error loss 0.16550425
shape of L is 
torch.Size([])
memory (bytes)
5983592448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 13% |
memory (bytes)
5983653888
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1086339100.0
relative error loss 0.16549776
shape of L is 
torch.Size([])
memory (bytes)
5986873344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 13% |
memory (bytes)
5986873344
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1086245900.0
relative error loss 0.16548356
shape of L is 
torch.Size([])
memory (bytes)
5990006784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 13% |
memory (bytes)
5990006784
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 13% |
error is  1086192600.0
relative error loss 0.16547546
time to take a step is 249.50547814369202
sum tnnu_Z after tensor(14526168., device='cuda:0')
shape of features
(7814,)
shape of features
(7814,)
number of orig particles 31254
number of new particles after remove low mass 28731
tnuZ shape should be parts x labs
torch.Size([31254, 702])
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
error without small mass is  1273568800.0
relative error without small mass is  0.19402118
nnu_Z shape should be number of particles by maxV
(31254, 702)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
shape of features
(31254,)
Thu Feb 2 04:25:45 EST 2023
