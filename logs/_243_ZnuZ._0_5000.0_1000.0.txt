Thu Feb 2 10:29:59 EST 2023
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  0% |  0% |
numbers of X: 37530855
numbers of Z: 19271
shape of features
(19271,)
shape of features
(19271,)
ZX	Vol	Parts	Cubes	Eps
Z	0.01403146902484901	19271	19.271	0.08996349039830227
X	0.013540491016982438	1126	1.126	0.22910364965880953
X	0.013720428389516008	14288	14.288	0.09865794762858679
X	0.013689409978746129	2644	2.644	0.17299800429212447
X	0.01352655019991905	1933	1.933	0.1912721464480944
X	0.013584464856854321	38646	38.646	0.07057420506290295
X	0.013595003461809264	47216	47.216	0.06603335704760951
X	0.013611354105574501	55516	55.516	0.06258847767959139
X	0.013610399014022301	44823	44.823	0.06721352085276995
X	0.013635032530159408	5733	5.733	0.13348262711099618
X	0.01354113129583282	59336	59.336	0.06110999211544253
X	0.013599366001968758	8434	8.434	0.11726318569200232
X	0.01353223016453547	77029	77.029	0.05600649210375873
X	0.013559588198577653	5476	5.476	0.13528854320092568
X	0.01361113449775848	221925	221.925	0.039436288248258594
X	0.013599312379989835	21720	21.72	0.08554980874043781
X	0.013612315589139978	69466	69.466	0.058083561636274
X	0.013614039884696455	58238	58.238	0.06160181698882184
X	0.013591898485412089	36203	36.203	0.07214039578162346
X	0.013710483805493244	221947	221.947	0.03953069959325042
X	0.013738061201281888	112781	112.781	0.04957124599691408
X	0.013564610255965217	13405	13.405	0.10039532744622659
X	0.01373491820477947	414998	414.998	0.03210652906458049
X	0.013597578359422931	17019	17.019	0.09279170982418763
X	0.013669803763724652	11513	11.513	0.10589072581941812
X	0.013541857143278551	22093	22.093	0.08494565676448453
X	0.013676074265198923	70293	70.293	0.05794506538317663
X	0.013571491368649548	34667	34.667	0.07315383607322822
X	0.013595788665807794	8464	8.464	0.11711420739085657
X	0.013581182474809165	56576	56.576	0.062149138817176854
X	0.013953926529282743	1680015	1680.015	0.020251681607358445
X	0.013549176833303573	5691	5.691	0.13352874742442977
X	0.013959468304010408	384007	384.007	0.033126578255702616
X	0.013581888812250418	10245	10.245	0.10985404914690233
X	0.013605246516630115	9423	9.423	0.11302445148459746
X	0.013592818809128522	7591	7.591	0.12143303385625717
X	0.013607695028071532	97354	97.354	0.051897159612162125
X	0.01380110366366534	71422	71.422	0.057813234832473304
X	0.013378262204309446	1678	1.678	0.19977259714113832
X	0.013420227726193903	4668	4.668	0.14219238738443762
X	0.01343266283570648	2822	2.822	0.16821641308289653
X	0.013668374869018977	5568	5.568	0.13489811617658923
X	0.013154712721510465	1689	1.689	0.19822141539411559
X	0.013500403552315646	949	0.949	0.24230377446821016
X	0.013414599492907542	2657	2.657	0.17155184197289536
X	0.013513385458855668	1195	1.195	0.22445644828565395
X	0.01352074904619857	1632	1.632	0.20234549164920484
X	0.013714106278219682	6895	6.895	0.12576054576283854
X	0.013541801127297715	5370	5.37	0.13611334760965416
X	0.013509828167236206	1340	1.34	0.21603046923024088
X	0.013580722418535288	7948	7.948	0.11955149316963748
X	0.013802951349659948	14350	14.35	0.09871277298986325
X	0.013314459730436667	1772	1.772	0.19586339450106657
X	0.013544134240505616	4370	4.37	0.1457996912130357
X	0.013558116250934062	2656	2.656	0.17218306230449396
X	0.013648850097813733	7066	7.066	0.1245396030027484
X	0.013482804375566048	2839	2.839	0.168088607845996
X	0.013554778367516318	2746	2.746	0.17026705693858704
X	0.013671431652257099	3120	3.12	0.16363886825389976
X	0.013514972880153388	3436	3.436	0.15785342443024294
X	0.01350719704540475	1890	1.89	0.19261989587982276
X	0.013728350093333529	5313	5.313	0.1372227683620401
X	0.013595253794791992	2300	2.3	0.18080890988142934
X	0.013721978023380781	8189	8.189	0.11877597828100332
X	0.013575963526689055	7237	7.237	0.12333055805605923
X	0.013539370384089339	2534	2.534	0.1748224837771482
X	0.013612188536796565	3392	3.392	0.15891224778101304
X	0.013458223285659932	2490	2.49	0.17549421062510634
X	0.01351944357619503	6707	6.707	0.1263213729022976
X	0.01357234879785284	4356	4.356	0.14605700145411898
X	0.013481500509632326	2129	2.129	0.18500672856484363
X	0.013271558467395986	3378	3.378	0.157792888571872
X	0.013533396428361391	3554	3.554	0.1561576003785287
X	0.013724860805364027	2959	2.959	0.166771177250497
X	0.013276339001592752	2538	2.538	0.17359167957713073
X	0.013333025783789245	2960	2.96	0.1651501614662375
X	0.013552149002158155	3323	3.323	0.15976903382947577
X	0.013547307408465844	1468	1.468	0.20975338427830936
X	0.013501663793807465	879	0.879	0.24857997391114453
X	0.01353541307534155	2404	2.404	0.1779012571007828
X	0.01363435280581938	11411	11.411	0.106113410761743
X	0.01350532537669975	3155	3.155	0.16236854393638472
X	0.01350417017548327	1325	1.325	0.21681234562723156
X	0.013696820343691822	7101	7.101	0.1244801440512181
X	0.013704643586513967	1555	1.555	0.20655979029784802
X	0.013508861856487258	1795	1.795	0.19596782350727218
X	0.01353953629316814	1469	1.469	0.20966567259687383
X	0.013562784106037023	2394	2.394	0.1782686192999789
X	0.013430922857387236	1249	1.249	0.22072310933251865
X	0.013594567368870323	5433	5.433	0.13576106266026494
X	0.013580866384911662	3295	3.295	0.1603334062522409
X	0.01357564145975179	2978	2.978	0.16581044646539417
X	0.013303638075568608	1553	1.553	0.2046128895638377
X	0.013558056152449201	1483	1.483	0.20909906575387113
X	0.013578034995257228	3299	3.299	0.16025744066257985
X	0.013542666542533756	4362	4.362	0.14588350012169338
X	0.01359884788013958	3702	3.702	0.1542962011891737
X	0.013560737467488343	2857	2.857	0.16805741894071732
X	0.013507924375052134	5185	5.185	0.1375983691635647
X	0.013605187953856712	4771	4.771	0.14180723086571867
X	0.013568233124890751	3615	3.615	0.1554073894155896
X	0.013659547646108782	10004	10.004	0.11093984244471337
X	0.013612488710116633	3075	3.075	0.1641965914915035
X	0.013563430401848379	5427	5.427	0.1357073095582248
X	0.013507924249198389	1304	1.304	0.21799021934078494
X	0.01349077526129768	3595	3.595	0.15539820725852865
X	0.0135355919026723	2103	2.103	0.18601415473427751
X	0.013571025264740369	7128	7.128	0.12394099813260818
X	0.01355858938381288	1026	1.026	0.2364226327568536
X	0.013542913017196597	2295	2.295	0.18070762013490962
X	0.0132312980850344	1563	1.563	0.20380483923787218
X	0.013385120085825776	1564	1.564	0.2045479702821509
X	0.013510612138661168	1050	1.05	0.23433036867254378
X	0.013452466813932215	1526	1.526	0.20657723741296577
X	0.013571900270727742	3601	3.601	0.1556225447949309
X	0.013500822432224759	4528	4.528	0.14392998132431478
X	0.013514238587810775	1713	1.713	0.19907256227707168
X	0.013589270789221404	2259	2.259	0.1818695423484311
X	0.013448620271026978	1091	1.091	0.23100302136532666
X	0.013686866501112926	6911	6.911	0.12558016406112782
X	0.013374526833299243	1536	1.536	0.2057291050755603
X	0.013575996565894453	14132	14.132	0.09867096226209039
X	0.013554166015501945	2510	2.51	0.17544178058359472
X	0.013391189779481038	1907	1.907	0.19149443490340798
X	0.01357021454368706	2344	2.344	0.1795600435957693
X	0.013459580613333891	1569	1.569	0.20470865025506835
X	0.013312705070265147	1986	1.986	0.1885510790696066
X	0.013524509147622397	1799	1.799	0.1958980514570907
X	0.013538774382043222	3251	3.251	0.16088694079840002
X	0.013741295480424872	14659	14.659	0.09786808966221319
X	0.013538076498923166	2299	2.299	0.1805812538493614
X	0.013596370429794623	6225	6.225	0.12974616924992974
X	0.013549146734436086	1567	1.567	0.20524896850553698
X	0.01356696187355735	4377	4.377	0.14580374731700016
X	0.013486588245493127	1410	1.41	0.21227279024670664
X	0.01354857702202652	2954	2.954	0.16614772456030127
X	0.01353530437575507	1748	1.748	0.197837614192894
X	0.013391757055846894	1416	1.416	0.21147454678939717
X	0.013258259227137399	1629	1.629	0.2011508199756996
X	0.013573167649892776	1934	1.934	0.19145861570694858
X	0.0135772919970925	3156	3.156	0.16263925934744286
X	0.013418174005264695	4436	4.436	0.14462186596387935
X	0.01354637532316801	953	0.953	0.24223862975392801
X	0.01356995312891022	9026	9.026	0.11455859346922957
X	0.013659520946887289	8367	8.367	0.11774845927939083
X	0.013614223939956226	5076	5.076	0.13893886886852772
X	0.013571280227344943	1648	1.648	0.20193946604558302
X	0.013543620969345565	1525	1.525	0.20708802320144562
X	0.01351500035755389	3046	3.046	0.1643218771550257
X	0.013577779393986047	2035	2.035	0.18825857984390112
X	0.013500737527815844	2614	2.614	0.17285580551258267
X	0.01337162494122617	2390	2.39	0.1775260521294247
X	0.013695703017602678	3796	3.796	0.15337434066321992
X	0.01349909005645752	3438	3.438	0.1577609601506831
X	0.013366088590019868	8593	8.593	0.11586524238610374
X	0.013590997893863331	2510	2.51	0.17560055110537928
X	0.013622229466862859	12068	12.068	0.10412083207064442
X	0.013547322712460853	3638	3.638	0.15499948690524937
X	0.013464149789914757	2619	2.619	0.17258954297376314
X	0.01353763638600433	3071	3.071	0.16396620417551425
X	0.013108967709022642	526	0.526	0.2920973272945473
X	0.01366688628661619	5040	5.04	0.13944823277622206
X	0.013418618461080554	1661	1.661	0.2006531848203508
X	0.01356279798020566	3147	3.147	0.16273620473190814
X	0.013188523181108072	1392	1.392	0.21160161462131422
X	0.01323590836220963	3012	3.012	0.16379460563231651
X	0.013503419913200976	1596	1.596	0.20376845942070032
X	0.0135789208315992	3043	3.043	0.16463459563161592
X	0.013565052555277705	3636	3.636	0.15509550172755532
X	0.013343308347237319	3385	3.385	0.15796769568939978
X	0.013348204001952312	799	0.799	0.2556379651938828
X	0.013136624445299025	1482	1.482	0.20695624067345306
X	0.013182046758348323	1354	1.354	0.21352794863960528
X	0.013596485159340652	6387	6.387	0.1286401621836804
X	0.0135514124458515	2424	2.424	0.17748050236758053
X	0.013735813801941086	6163	6.163	0.13062332739465005
X	0.013592908292960363	3056	3.056	0.16445724563190375
X	0.013485829936829405	4751	4.751	0.14158944608931923
X	0.013574807772618068	2855	2.855	0.1681547702377048
X	0.01374360073384502	4900	4.9	0.14102674536320703
X	0.013485131512639081	2731	2.731	0.17028556266451667
X	0.013575643790734783	2797	2.797	0.16931261703086298
X	0.013436073101784934	2175	2.175	0.18348660519780305
X	0.013568693096205862	4312	4.312	0.1465389546390942
X	0.013570093898412783	2771	2.771	0.16981736938128994
X	0.013670806497984746	7124	7.124	0.12426726549925292
X	0.013571303311774804	4140	4.14	0.14855037024871642
X	0.013650761774457298	1910	1.91	0.1926228659009052
X	0.013558974336026895	3774	3.774	0.15315869160596524
X	0.013730843481004739	5940	5.94	0.13222195996434846
X	0.013712524613731528	17567	17.567	0.09207462094409868
X	0.013319108460690132	1676	1.676	0.1995570370494033
X	0.013488089842038243	769	0.769	0.2598212712904103
X	0.01349509099121578	3245	3.245	0.1608127104885239
X	0.013498585751746438	1912	1.912	0.1918374798991587
X	0.013410973399074851	2395	2.395	0.1775762693497744
X	0.013565053415603667	2384	2.384	0.1785274845086898
X	0.01357947785361761	2453	2.453	0.17690027483925191
X	0.013485314149735505	1533	1.533	0.20643007534578897
X	0.013419519379873115	1766	1.766	0.19659935448627003
X	0.013552750876622504	1794	1.794	0.19621626541739473
X	0.01357949261602769	4601	4.601	0.1434422547866
X	0.013398058764346832	1715	1.715	0.198423261840704
X	0.013653545627559169	9345	9.345	0.11347200100374583
X	0.013552802288461131	4221	4.221	0.1475269114069658
X	0.013608367741999083	3505	3.505	0.15717109232994447
X	0.013334191625283575	1133	1.133	0.22746378458070535
X	0.013700216689277421	4610	4.61	0.1437723937471426
X	0.013490241993799952	3516	3.516	0.1565514007967504
X	0.013559588245214334	2852	2.852	0.1681508215552983
X	0.013580056658638755	5896	5.896	0.1320630639645246
X	0.013656298013403376	5104	5.104	0.1388270536387954
X	0.013411944980737217	3247	3.247	0.16044880671265294
X	0.013563251190347007	3149	3.149	0.1627035572164504
X	0.013565143499393782	2325	2.325	0.1800254112511151
X	0.013573096910151882	1338	1.338	0.2164749321328098
X	0.013352293901516063	1055	1.055	0.23304214793935918
X	0.013421714570136434	1194	1.194	0.2240102499605465
X	0.013394856186591681	1064	1.064	0.23262986879500294
X	0.013564350938513176	3574	3.574	0.15598451601070357
X	0.013490040439403628	2460	2.46	0.17634346964063247
X	0.013365406235206519	3284	3.284	0.15965885302271912
X	0.013558683493042253	1470	1.47	0.20971688388635457
X	0.013514569844465363	4304	4.304	0.14643447060624504
X	0.013463341476464882	2674	2.674	0.1713946070717265
X	0.013727585464944642	3916	3.916	0.15190911176502025
X	0.01359459997030046	4246	4.246	0.1473880074491544
X	0.013526487327910173	1779	1.779	0.19663901675694936
X	0.01340442039704787	1051	1.051	0.23364066910605433
X	0.013430489497426496	5849	5.849	0.13192792751673063
X	0.013131048363225161	643	0.643	0.27333556446602064
X	0.01357982043692832	1813	1.813	0.19565850963311515
X	0.013483573452468575	1752	1.752	0.19743476834140147
X	0.01371975485605347	2670	2.67	0.17256194613189432
X	0.013721694499452854	5277	5.277	0.13751187926111255
X	0.013586595874667524	2229	2.229	0.1826698461898127
X	0.0133790923059794	2176	2.176	0.1831987838635964
X	0.013608305250106851	2953	2.953	0.16641029806855628
X	0.01363771482305191	7016	7.016	0.12480079200416748
X	0.013479419465591717	1278	1.278	0.2193041136638459
X	0.013572042265978558	6980	6.98	0.12481398813928767
X	0.01359470548994308	35218	35.218	0.07281178949617395
X	0.013619241339025307	11786	11.786	0.10493704402043404
X	0.013613080637243485	5272	5.272	0.13719143701833475
X	0.01340400695876799	665	0.665	0.27214748994318044
X	0.013578153049543054	6708	6.708	0.12649767651965613
X	0.013747599264306059	36402	36.402	0.07228261784861464
X	0.013584070805280577	208549	208.549	0.040235302883648176
X	0.013576332481799038	3789	3.789	0.15302155818805904
X	0.013595584641864816	69476	69.476	0.05805696923728813
X	0.013608263515117658	24694	24.694	0.08198550741602342
X	0.013596709883863913	43321	43.321	0.06795870888170998
X	0.01400452748034645	242968	242.968	0.03862837692115261
X	0.013488987164441496	1852	1.852	0.19384121338696048
X	0.013598861822341376	3691	3.691	0.15444938094137964
X	0.013553440857995288	167856	167.856	0.04322193757711848
X	0.013838736256722979	458927	458.927	0.03112559109718721
X	0.013646042684055809	4476	4.476	0.14500140682557738
X	0.013750164398012878	30699	30.699	0.07651170628691094
X	0.013733956692470932	186637	186.637	0.04190501266003635
X	0.013695469244925595	78613	78.613	0.05585055789486212
X	0.013745460106636277	50350	50.35	0.06487139975088071
X	0.013737398372472306	25477	25.477	0.0813926899693799
X	0.01361096539962114	38009	38.009	0.07101239532251216
X	0.013601450588347951	10942	10.942	0.10752171244741016
X	0.013497854096445236	2953	2.953	0.16595885409436695
X	0.013623724631381043	324854	324.854	0.03474315170979735
X	0.013553922103589433	6248	6.248	0.1294517685759158
X	0.01362621724855633	4622	4.622	0.14338876445353704
X	0.013603836930387966	36485	36.485	0.07197511303533838
X	0.013455951977017145	28896	28.896	0.07751020757680509
X	0.013747571098722908	236952	236.952	0.03871285479177844
X	0.013606187694287824	91837	91.837	0.05291428389886577
X	0.01345842195173408	1155	1.155	0.2267100217936565
X	0.013596982914607455	30102	30.102	0.07672715000102544
X	0.013488927316323633	8803	8.803	0.11528750422361415
X	0.013607091773869186	20026	20.026	0.08791379626674949
X	0.01362364838093364	69836	69.836	0.05799688788917487
X	0.013530060666932556	4543	4.543	0.14387511011794749
X	0.01354619363547989	40725	40.725	0.06928704858394073
X	0.012927442262706446	807	0.807	0.25208462448106866
X	0.013532240115630059	5067	5.067	0.13874146106210067
X	0.013579166724936125	60491	60.491	0.060775347704981324
X	0.013580291360798462	34221	34.221	0.07348614473410998
X	0.013413771478697501	25532	25.532	0.08069042863965006
X	0.013574502532549644	37932	37.932	0.07099690122800503
X	0.013682905505137457	253287	253.287	0.03780261203633219
X	0.013606878965022579	12279	12.279	0.10348208586563061
X	0.01358674259310942	19672	19.672	0.08839389625248821
X	0.013614794252195942	65334	65.334	0.05928670009696807
X	0.013510739975219658	5150	5.15	0.13791895849745345
X	0.01355429654242702	99804	99.804	0.051401565376094735
X	0.013757274916559379	217117	217.117	0.03986693629526769
X	0.013724043891523112	272831	272.831	0.036914426686241786
X	0.013730139362041306	15952	15.952	0.09512323278773918
X	0.013588741189972131	21041	21.041	0.0864379152074343
X	0.013517466296354304	10085	10.085	0.11025707432684254
X	0.013564418553179875	5285	5.285	0.13691532558966862
X	0.013621956111236793	24047	24.047	0.08274203369501199
X	0.013597698194131825	9399	9.399	0.1130996469757128
X	0.013609289736916223	16261	16.261	0.09423872537675816
X	0.013626289861478178	87352	87.352	0.05383130687072374
X	0.013643651895111457	45870	45.87	0.06675246330603225
X	0.013557787728750912	6026	6.026	0.13103477507186032
X	0.013611348542034976	9426	9.426	0.11302935253797715
X	0.013622661242133276	99857	99.857	0.051478728821429016
X	0.01362299724988283	14133	14.133	0.09878236887657671
X	0.013544565428914556	4213	4.213	0.1475903192327411
X	0.013734893151834912	9711	9.711	0.11225019661502098
X	0.013852651462665075	493442	493.442	0.030392442124376164
X	0.013556722906252162	11797	11.797	0.1047436523221584
X	0.013652877081115325	135958	135.958	0.046480751129326155
X	0.013572887545074554	5459	5.459	0.13547309482380024
X	0.013607006767980935	53705	53.705	0.06327749720176873
X	0.01368748691022517	21339	21.339	0.08624154955435996
X	0.013897028574949944	309460	309.46	0.035544505483109924
X	0.01361533336215589	43356	43.356	0.06797142223454126
X	0.013593443829607811	8254	8.254	0.11809232619713411
X	0.013599498572217558	124919	124.919	0.0477490569833712
X	0.013644758938099624	92518	92.518	0.05283396577246199
X	0.013605467517313244	13601	13.601	0.10001094779367782
X	0.013691454216995465	55863	55.863	0.0625808956700498
X	0.013745155701068856	100520	100.52	0.0515187987270929
X	0.013999219557995355	286890	286.89	0.036542321828884554
X	0.013761173592825684	31197	31.197	0.07612270518486354
X	0.01357311261410267	2759	2.759	0.1700758240855396
X	0.013534479756125617	13304	13.304	0.10057416683886548
X	0.013669763690906932	88087	88.087	0.05373819346770549
X	0.013840988042823703	123096	123.096	0.04826597136423666
X	0.013589053068630228	12050	12.05	0.10408801247732956
X	0.013349544752004257	1562	1.562	0.2044537789119534
X	0.013857250188067266	34819	34.819	0.07355636423737712
X	0.013723312292793744	100078	100.078	0.05156718720391164
X	0.013575154060152536	93736	93.736	0.052514526018273534
X	0.013568667578908886	4879	4.879	0.1406270268215729
X	0.01359828467588954	22557	22.557	0.08447617925427546
X	0.013733390318105868	13633	13.633	0.10024485905303125
X	0.013574381488963196	4663	4.663	0.14278575368302773
X	0.013658988755384656	10347	10.347	0.10969867100278075
X	0.01358287527671763	8278	8.278	0.11794750638908462
X	0.013551414725945463	70896	70.896	0.05760422012443253
X	0.01362007811024309	40637	40.637	0.06946285880182237
X	0.013612847767453	17507	17.507	0.09195579117650976
X	0.013743750339833704	16715	16.715	0.09368418697349905
X	0.013630766124411546	7976	7.976	0.11955792628461127
X	0.013623852220607245	421740	421.74	0.031848211118605925
X	0.013615693209117425	6442	6.442	0.12833339365549434
X	0.013601239312878583	119404	119.404	0.04847522892987615
X	0.013604389770091537	1920	1.92	0.19206988623361554
X	0.013591547297383731	4163	4.163	0.14834998154559406
X	0.013532463220370917	2888	2.888	0.16733747910562802
X	0.013619180843407125	21837	21.837	0.08543831425071859
X	0.013700573409836062	3915	3.915	0.15182233245116739
X	0.013557390066965836	42093	42.093	0.06854704498535648
X	0.013391468386713968	5204	5.204	0.1370346213720283
X	0.013569081165974198	4271	4.271	0.14700776997049564
X	0.01383475974299699	242259	242.259	0.03850914714008509
X	0.013670004846292497	24966	24.966	0.0818100329262052
X	0.013716837597951953	17918	17.918	0.09147901678673782
X	0.013687037869862365	90445	90.445	0.05328949875588751
X	0.013746539726447187	134525	134.525	0.04675163427716832
X	0.013520503007741606	3745	3.745	0.15340764712092775
X	0.013507660392954047	7752	7.752	0.12033412093401381
X	0.013560308761787024	15799	15.799	0.09503421529802397
X	0.013125662895946136	1143	1.143	0.22560999639503093
X	0.013693849856136715	4430	4.43	0.14567128296773577
X	0.013582045826990295	15830	15.83	0.09502285880136761
X	0.013491022790074953	2424	2.424	0.17721647215407846
X	0.013567003605817564	3906	3.906	0.1514434752894512
X	0.013469509893829357	8240	8.24	0.1177989792726988
X	0.013599381195647338	4790	4.79	0.14159933420541593
X	0.013585741849722149	301185	301.185	0.03559726839685498
X	0.013506570824862063	7491	7.491	0.12171247703213574
X	0.013763848632824946	50858	50.858	0.06468350378463716
X	0.013570167633211904	7985	7.985	0.1193356209788889
X	0.01357427341947368	21703	21.703	0.08551958947463349
X	0.013408444925403542	28837	28.837	0.07747164511834224
X	0.013862279903216576	642288	642.288	0.027842076252380846
X	0.013634073855178027	608900	608.9	0.028185552738264143
X	0.013617872008194165	38620	38.62	0.07064785702645016
X	0.013614619976056688	61979	61.979	0.06033745475888849
X	0.013559018410167073	2335	2.335	0.17974098686002962
X	0.013589574793892197	63679	63.679	0.0597589848278127
X	0.013593742469706589	20668	20.668	0.08696547266461603
X	0.013937055874967224	103405	103.405	0.05127155635886097
X	0.013635145652041582	8074	8.074	0.11908498660120759
X	0.013779149022897227	61312	61.312	0.06079842160934866
X	0.013752625962144603	300350	300.35	0.03577552357434323
X	0.013598132446656355	48903	48.903	0.06527014661713545
X	0.013429163319359729	5564	5.564	0.1341386541848365
X	0.01393700346394398	29820	29.82	0.07760456274875709
X	0.013545753114859681	9053	9.053	0.11437651996329129
X	0.013598957632856793	3355	3.355	0.15944259915832248
X	0.013707495716803231	116927	116.927	0.04894193139482092
X	0.013567670738396737	19793	19.793	0.0881721079368913
X	0.0132087972557689	1733	1.733	0.19679840328634113
X	0.01364790655298096	93188	93.188	0.05271109235777563
X	0.013753994600042424	11059	11.059	0.10754023895464909
X	0.013510728860230765	956	0.956	0.2417725338500996
X	0.013533760692958688	110418	110.418	0.049673665157262056
X	0.013639803287248313	75556	75.556	0.056517080099641466
X	0.01372740223126201	25842	25.842	0.0809880133374811
X	0.013610183991390894	88669	88.669	0.05354234588045782
X	0.013603303472842642	88022	88.022	0.05366416606295679
X	0.013478050889235167	8416	8.416	0.11699674743813473
X	0.013584464158672682	38288	38.288	0.07079348252747454
X	0.014004286932785888	127923	127.923	0.04783775477567576
X	0.013612773171118756	112190	112.19	0.04950671995583544
X	0.013584716172143203	10287	10.287	0.10971195224276427
X	0.013748127498493183	118212	118.212	0.04881208160405383
X	0.0136660713611206	188024	188.024	0.041732730316884965
X	0.01369881801441798	68854	68.854	0.05837828158427483
X	0.013623727534283122	32071	32.071	0.0751728730922999
X	0.013520790051565725	20412	20.412	0.08717102844837049
X	0.01359859782187506	8520	8.52	0.11686510268013654
X	0.013583420453555583	7180	7.18	0.12367869663172497
X	0.013501993785826434	4864	4.864	0.14054048431979746
X	0.013682708543740162	155993	155.993	0.044431296756350894
X	0.013619017481279803	122579	122.579	0.04807396036747723
X	0.01357703267630556	275643	275.643	0.03665663808869892
X	0.01372800128462867	151763	151.763	0.04488974476246155
X	0.01359173664289488	61067	61.067	0.06060234859765055
X	0.013590260035286591	14896	14.896	0.09698830458125952
X	0.013761350190917294	69924	69.924	0.05816721369035483
X	0.013353779317573242	2246	2.246	0.18116104380560652
X	0.013579618560312947	11391	11.391	0.1060332093160911
X	0.013594015080121036	154860	154.86	0.04444294512063952
X	0.013594998479835781	21120	21.12	0.08634325493576202
X	0.013567889444159079	3151	3.151	0.1626876668991726
X	0.013482389754845652	11493	11.493	0.10546567996697964
X	0.013833379167005841	45740	45.74	0.06712392964946401
X	0.013587741072655889	26555	26.555	0.07998348422761142
X	0.013979656903463786	72182	72.182	0.05785699418419525
X	0.01348981102764689	33451	33.451	0.07388091297208876
X	0.013533087536899012	24385	24.385	0.0821784685601931
X	0.013733917185679753	4000	4.0	0.1508614037021069
X	0.01358528582465552	3124	3.124	0.16322471474552366
X	0.013637400056667602	46891	46.891	0.06625429372539549
X	0.013574520161496947	46794	46.794	0.06619798528959417
X	0.01355042385737526	10507	10.507	0.10884906828570559
X	0.013619197355989715	112483	112.483	0.04947147671534312
X	0.013556749395407313	5396	5.396	0.13594436453864955
X	0.013599930479672381	16283	16.283	0.09417466597404803
X	0.013598912032411408	4203	4.203	0.1479045694270968
X	0.013608600555149697	123551	123.551	0.047935331371210727
X	0.013571211448016762	5952	5.952	0.1316189933389746
X	0.013572855384489832	37868	37.868	0.07103400221005797
X	0.013571034695713838	11613	11.613	0.10533099294561461
X	0.013600478943506718	16152	16.152	0.09442984978355237
X	0.013762320165825528	45426	45.426	0.06716284021124423
X	0.013668786406648219	302631	302.631	0.03561275030745519
X	0.013648670934110107	12351	12.351	0.1033862414674978
X	0.01359493205844625	8070	8.07	0.1189874531777251
X	0.013599176741159172	98661	98.661	0.051656189708541383
X	0.013826532141674841	10701	10.701	0.1089171581240844
X	0.013703320371994012	439145	439.145	0.03148274726444108
X	0.013515853931852933	4134	4.134	0.1484195142131307
X	0.013592752925645659	93789	93.789	0.052527311448936624
X	0.013682463039204111	310642	310.642	0.03531571770663856
X	0.013255047578895392	2985	2.985	0.164366125718987
X	0.013543285416993133	190395	190.395	0.04143393343368367
X	0.013566318681179319	25170	25.17	0.08138160793581284
X	0.01400919322813987	332928	332.928	0.034781977346293826
X	0.01358531815930125	8454	8.454	0.11713028232562873
X	0.013548388494788663	41545	41.545	0.06883187912125775
X	0.013578688234197268	1874	1.874	0.19350673125861337
X	0.013543663000322748	5683	5.683	0.13357325039251744
X	0.013282011494753908	5026	5.026	0.13825487561932728
X	0.013615064828480605	56752	56.752	0.06213641329366986
X	0.013601372998802933	22900	22.9	0.08405865230803265
X	0.013650221956142369	91530	91.53	0.05303046276199451
X	0.01359119582900757	14980	14.98	0.09680890002765223
X	0.013583834263294388	4221	4.221	0.14763942348152406
X	0.01358458005391294	3092	3.092	0.16378303354105803
X	0.013786077510776762	122670	122.67	0.048257792335723165
X	0.013796097654095344	28204	28.204	0.07879195043959177
X	0.0137653004703027	291906	291.906	0.036128302613707974
X	0.013709028066498874	37755	37.755	0.07134180016093399
X	0.013602255166531385	7963	7.963	0.11953949054409262
X	0.013668486652110068	6175	6.175	0.13032502703480414
X	0.013546431585737297	9752	9.752	0.11157761442909547
X	0.013994805383395138	941698	941.698	0.024585841177017155
X	0.01360036205156268	10595	10.595	0.10868005167357259
X	0.013600944418943566	17314	17.314	0.09226929823702192
X	0.013759593314166844	103130	103.13	0.05109834803897763
X	0.013509997556325334	40905	40.905	0.06912359089119716
X	0.013595897303363651	4124	4.124	0.14883203031435194
X	0.013392320670558722	2418	2.418	0.1769293360296404
X	0.013710655209493262	118524	118.524	0.04872486410825131
X	0.01360354396459968	29866	29.866	0.0769410900111754
X	0.013558832828926873	53220	53.22	0.06339414448432452
X	0.013611675966423984	62495	62.495	0.060166596241004774
X	0.013987890365037732	63012	63.012	0.060549373207946934
X	0.013746212123446462	91092	91.092	0.053239536757052223
X	0.013776582458896501	256835	256.835	0.03771340657709184
X	0.013694623544499532	120220	120.22	0.04847574120083166
X	0.013671129918843812	13979	13.979	0.09926041848586419
X	0.013622560595233658	85651	85.651	0.05418038709050515
X	0.013574226795164929	13058	13.058	0.10130078533584759
X	0.013672158909524603	9479	9.479	0.11298605835859561
X	0.013556039949200762	12975	12.975	0.10147097292457362
X	0.013584527132866111	8708	8.708	0.11597793408545552
X	0.013608532355448847	93734	93.734	0.05255790512131216
X	0.01361990805699571	16755	16.755	0.0933275606393169
X	0.01344208196637596	3796	3.796	0.152421692065125
X	0.013560644617342162	16292	16.292	0.0940665705763841
X	0.013609588673215222	11611	11.611	0.10543669678768221
X	0.013583832983656187	52092	52.092	0.0638876774795238
X	0.013410791913629524	6443	6.443	0.12767977171752198
X	0.013602749899731299	15293	15.293	0.09617112023435181
X	0.013547857873697944	12636	12.636	0.10234980154451924
X	0.01365629189757294	33290	33.29	0.07430299711222396
X	0.013608922289968005	3852	3.852	0.15230441233161815
X	0.013508754791330633	5903	5.903	0.13177939572715472
X	0.013602798141659782	23591	23.591	0.08323269704568266
X	0.013608977429630178	162000	162.0	0.043796239941228304
X	0.01371161529713834	6802	6.802	0.12632345561813726
X	0.013400936504157	1593	1.593	0.20337916403074233
X	0.013553460805491609	3990	3.99	0.15032311598853682
X	0.013590219206567701	217327	217.327	0.039692116643076675
X	0.013885198807158773	220719	220.719	0.03977139223704301
X	0.013448499100702027	41141	41.141	0.06888632106846009
X	0.013714503355345995	5973	5.973	0.1319256340456574
X	0.013849104953489587	112023	112.023	0.04981630394739104
X	0.014012316480374827	137295	137.295	0.04673243444381724
X	0.01351499659710534	3035	3.035	0.16452014430099474
X	0.0135566788542384	10274	10.274	0.10968264516314592
X	0.013610769683547455	32700	32.7	0.07466406727554485
X	0.013634502193422042	10230	10.23	0.1100494519614521
X	0.013694915205742564	27319	27.319	0.07943858807289741
X	0.01360897602806125	84535	84.535	0.054399674030544604
X	0.013610369187428722	18864	18.864	0.08969027852566372
X	0.013682690108475698	45441	45.441	0.06702567612041806
X	0.013496976288205313	3177	3.177	0.16195949851274385
X	0.01388779583683244	29452	29.452	0.0778346261623867
X	0.013731062768371553	20111	20.111	0.0880555692125181
X	0.013606508672923387	30558	30.558	0.07636140761825211
X	0.013602601104321745	47317	47.317	0.06599862996532393
X	0.013762589349493294	273329	273.329	0.03692650003691665
X	0.013578751168778302	6197	6.197	0.12988513335388865
X	0.013547447770768448	2665	2.665	0.17194389375670097
X	0.013671966792303182	8573	8.573	0.11683314218538501
X	0.013762352133809906	37086	37.086	0.07186116008981834
X	0.013553997246512647	29535	29.535	0.07713357713920502
X	0.0135796697552752	4213	4.213	0.14771771569460654
X	0.013463392502978499	3352	3.352	0.15895840597749522
X	0.013592686810943073	31056	31.056	0.075925314136887
X	0.013574687634106136	2949	2.949	0.1663482963581558
X	0.01376137493010411	20157	20.157	0.08805323385091748
X	0.013572847131784813	4719	4.719	0.14221333688535143
X	0.013739508508443653	15887	15.887	0.09527444639813512
X	0.0137522482713144	8160	8.16	0.11900389656174884
X	0.01354867478638187	6681	6.681	0.12657612096382917
X	0.013389795883484408	3497	3.497	0.15644419086764358
X	0.013611884452463299	110183	110.183	0.04980442001215117
X	0.013862421156187563	210700	210.7	0.04036994748753253
X	0.013620323314586243	164621	164.621	0.043574665349435465
X	0.01358533341398035	105395	105.395	0.05051458209203601
X	0.013595445071334403	28447	28.447	0.07818419799901524
X	0.013559695077639738	7471	7.471	0.1219804962044828
X	0.01369324217294159	9995	9.995	0.11106430334958219
X	0.013614526376713176	132723	132.723	0.046811452397496345
X	0.013913626488266575	9310	9.31	0.11433088182476478
X	0.013616877850557308	22060	22.06	0.08514464606550694
X	0.01357836053635136	10106	10.106	0.11034585244417597
X	0.013985532921888321	247785	247.785	0.038359064102206034
X	0.013579338725448677	27810	27.81	0.07874552405090016
X	0.013757057238050653	30924	30.924	0.07633844218811678
X	0.013761168320445792	133001	133.001	0.04694616637478513
X	0.013718337768794643	42005	42.005	0.0688652562039782
X	0.013766634564090474	22392	22.392	0.08503119833475781
X	0.013739802940286991	165738	165.738	0.04360331115753288
X	0.013719697013793107	115860	115.86	0.049106275813213376
X	0.013604290163133713	43537	43.537	0.06785874063846127
X	0.013574716840835643	16789	16.789	0.09316125206619805
X	0.013617223484999521	37235	37.235	0.07151202382918653
X	0.013613315410347937	8335	8.335	0.11776586714057642
X	0.013619564969799275	85919	85.919	0.054120027344412534
X	0.014019040059371908	488214	488.214	0.03062217371791511
X	0.013859987971275823	197142	197.142	0.04127254329119204
X	0.013599013588277012	86567	86.567	0.05395748270229117
X	0.013599879332004716	9964	9.964	0.11092610884235954
X	0.01350727798680964	24875	24.875	0.08158335908655401
X	0.01380784034698746	18841	18.841	0.09015862279824643
X	0.01358548092884068	19430	19.43	0.08875661665441024
X	0.01373203105872791	42277	42.277	0.06874010719139514
X	0.01373156073832404	301753	301.753	0.03570174430390181
X	0.013574672092595315	5781	5.781	0.1329154479973404
X	0.013822672681976053	139156	139.156	0.04631233689980402
X	0.013796637727819121	57174	57.174	0.06225748335422323
X	0.01361088409759127	35132	35.132	0.07290004905760313
X	0.013525027534104618	5586	5.586	0.13428033277443222
X	0.013671027191892454	50484	50.484	0.06469674974807005
X	0.013624106073150651	128396	128.396	0.047342607506477126
X	0.013508631583082293	4020	4.02	0.14978273902816855
X	0.013742027236775695	729496	729.496	0.026607820170341516
X	0.013611515320190424	46223	46.223	0.06652978435882344
X	0.013595912937607854	74606	74.606	0.056695014798274865
X	0.013685640969972658	19416	19.416	0.08899558449766953
X	0.013527960439880984	13601	13.601	0.09982067324382796
X	0.013534457678333828	2505	2.505	0.1754732996406596
X	0.013561185248090253	4100	4.1	0.14899485227731266
X	0.013642460394008267	16952	16.952	0.09301591287010591
X	0.013581958695711733	126618	126.618	0.04751407935603292
X	0.013940525479200572	1614614	1614.614	0.020514934918892606
X	0.01355337656034445	4698	4.698	0.1423567826400582
X	0.013602627577462261	103331	103.331	0.05087027184754627
X	0.013571390772330817	4652	4.652	0.14288771235940967
X	0.013592343149253503	20200	20.2	0.08762895921936537
X	0.013567919074117626	12864	12.864	0.10179170865110536
X	0.013586651332546133	154343	154.343	0.044484477890013474
X	0.013464652243651232	9999	9.999	0.11042808078140823
X	0.013691938421687847	977629	977.629	0.02410443486645871
X	0.013724335731982213	5552	5.552	0.13521160208906305
X	0.01367370826032201	111205	111.205	0.049726435793908716
X	0.013618362191487533	27808	27.808	0.07882277285265415
X	0.013617304778953638	69282	69.282	0.05814203754242197
X	0.013626935968990613	196210	196.21	0.04110478293661493
X	0.013565226535680854	19330	19.33	0.08886520196350627
X	0.013595123348512674	8045	8.045	0.1191111363530963
X	0.013713478964781462	71227	71.227	0.05774321987310316
X	0.013731263278373592	6440	6.44	0.1287087904334239
X	0.01355551266910158	17158	17.158	0.09244492928588163
X	0.013621113816430039	107211	107.211	0.05027179877523988
X	0.013615689468317196	56716	56.716	0.06215050779676045
X	0.013947158158793205	30091	30.091	0.0773896771732896
X	0.013490162696343807	29867	29.867	0.07672587710331526
X	0.013751487962396305	178608	178.608	0.04254183921525727
X	0.013622933871177169	360575	360.575	0.0335550851652942
X	0.013572372280065756	31520	31.52	0.07551325394909743
X	0.013610224917792672	7955	7.955	0.11960289890285515
X	0.013575972118789413	120248	120.248	0.04833158360316323
X	0.013591359450028252	32737	32.737	0.07460043158456951
X	0.013617777889530903	36639	36.639	0.07189867354803789
X	0.013550445040055297	28955	28.955	0.07763841547318373
X	0.013698971532505	51003	51.003	0.0645204538581941
X	0.013576755596968198	79351	79.351	0.055515535321878785
X	0.013560667539030725	169816	169.816	0.043062656798644475
X	0.013612144410243821	25044	25.044	0.08160954394445029
X	0.013561790869205089	17155	17.155	0.09246458828002288
X	0.01348171893749713	3874	3.874	0.15154044241878392
X	0.01372003341593878	17517	17.517	0.0921789619354163
X	0.013722570544860993	91370	91.37	0.053154978773045994
X	0.013563562411996202	71720	71.72	0.05739990360651091
X	0.013612105777207157	135697	135.697	0.04646418847135658
X	0.013552159556272233	6813	6.813	0.12576410535981666
X	0.013606695133411464	22143	22.143	0.0850169260312224
X	0.01368503257059659	49366	49.366	0.06520375879168223
X	0.013806679833257363	107994	107.994	0.05037671862411395
X	0.013586376204869917	35620	35.62	0.0725220224528198
X	0.013621940359457995	57247	57.247	0.061967229637006994
X	0.013923073256577998	272617	272.617	0.037101721230551124
X	0.013618062050644884	28376	28.376	0.07829271935383476
X	0.013694427551094015	2032	2.032	0.1888890256519287
X	0.013699060733444738	6626	6.626	0.12739326052944047
X	0.013700206776940624	146333	146.333	0.04540757217478327
X	0.013599129421064105	28804	28.804	0.07786687904195108
X	0.013619697112924533	60269	60.269	0.06091035835759964
X	0.013579872354613984	2147	2.147	0.1849358954374134
X	0.0135794404705197	4009	4.009	0.150181097274404
X	0.013575998561229242	159724	159.724	0.04396771222172633
X	0.013577900121252944	70355	70.355	0.05778909333330271
X	0.01359741045685818	37525	37.525	0.07129271743848849
X	0.01360408507101522	4631	4.631	0.14321819172701722
X	0.013547406059780321	3909	3.909	0.15133178657993154
X	0.013584431742340607	24738	24.738	0.08188901179514929
X	0.013564217636623618	37649	37.649	0.07115636726569419
X	0.01360538739115163	56751	56.751	0.06212205272255783
X	0.013569359040856906	6486	6.486	0.1278971292534331
X	0.013721461076258792	61200	61.2	0.06075047042654462
X	0.01358504239164328	11175	11.175	0.10672622673551692
X	0.013480057011115477	21170	21.17	0.08603139340203447
X	0.013732782952154797	233160	233.16	0.03890763530859809
X	0.013624158141619743	7034	7.034	0.12465291455355149
X	0.013602216072651751	37524	37.524	0.071301748616533
X	0.013609849210496197	105325	105.325	0.05055614474253393
X	0.013597869433300293	25454	25.454	0.08114061137091791
X	0.013680547345155671	42832	42.832	0.06835627262334494
X	0.013618974977169622	120197	120.197	0.04838940305495805
X	0.01373488915910726	1925	1.925	0.19251510405244718
X	0.013598085677973078	42057	42.057	0.06863513531771614
X	0.013609485733380701	21605	21.605	0.08572269470128616
X	0.013643242456979138	97669	97.669	0.05188640947337856
X	0.013599491420806959	96486	96.486	0.052041857400510545
X	0.013542082765615424	34745	34.745	0.07304621477916057
X	0.013775374484902394	65332	65.332	0.05951948346199167
X	0.013609291881574245	13692	13.692	0.09979823932328584
X	0.01345437929240306	12381	12.381	0.10281014721930849
X	0.013580703503104242	7733	7.733	0.12064928500067165
X	0.013567356490618106	8822	8.822	0.11542752785689747
X	0.013955540074620632	103880	103.88	0.05121591103586747
X	0.013878021964648197	189509	189.509	0.041837517804999926
X	0.013429847377449289	5162	5.162	0.1375364125984486
X	0.013613589561676432	19462	19.462	0.08876908190521127
X	0.013619453124696788	12260	12.26	0.10356739825752194
X	0.01369484462421002	271427	271.427	0.03695172248144725
X	0.013603178968353561	34634	34.634	0.07323397145152365
X	0.013546612315802278	14495	14.495	0.09776967258547463
X	0.013753248879296348	85978	85.978	0.054284101584745253
X	0.01361334618070184	90341	90.341	0.0532140930891111
X	0.013460267374159379	8469	8.469	0.11670080486389428
X	0.01358391163152373	43544	43.544	0.06782120609791971
X	0.013613336397981332	5482	5.482	0.1354176215789648
time for making epsilon is 1.9529120922088623
epsilons are
[0.22910364965880953, 0.09865794762858679, 0.17299800429212447, 0.1912721464480944, 0.07057420506290295, 0.06603335704760951, 0.06258847767959139, 0.06721352085276995, 0.13348262711099618, 0.06110999211544253, 0.11726318569200232, 0.05600649210375873, 0.13528854320092568, 0.039436288248258594, 0.08554980874043781, 0.058083561636274, 0.06160181698882184, 0.07214039578162346, 0.03953069959325042, 0.04957124599691408, 0.10039532744622659, 0.03210652906458049, 0.09279170982418763, 0.10589072581941812, 0.08494565676448453, 0.05794506538317663, 0.07315383607322822, 0.11711420739085657, 0.062149138817176854, 0.020251681607358445, 0.13352874742442977, 0.033126578255702616, 0.10985404914690233, 0.11302445148459746, 0.12143303385625717, 0.051897159612162125, 0.057813234832473304, 0.19977259714113832, 0.14219238738443762, 0.16821641308289653, 0.13489811617658923, 0.19822141539411559, 0.24230377446821016, 0.17155184197289536, 0.22445644828565395, 0.20234549164920484, 0.12576054576283854, 0.13611334760965416, 0.21603046923024088, 0.11955149316963748, 0.09871277298986325, 0.19586339450106657, 0.1457996912130357, 0.17218306230449396, 0.1245396030027484, 0.168088607845996, 0.17026705693858704, 0.16363886825389976, 0.15785342443024294, 0.19261989587982276, 0.1372227683620401, 0.18080890988142934, 0.11877597828100332, 0.12333055805605923, 0.1748224837771482, 0.15891224778101304, 0.17549421062510634, 0.1263213729022976, 0.14605700145411898, 0.18500672856484363, 0.157792888571872, 0.1561576003785287, 0.166771177250497, 0.17359167957713073, 0.1651501614662375, 0.15976903382947577, 0.20975338427830936, 0.24857997391114453, 0.1779012571007828, 0.106113410761743, 0.16236854393638472, 0.21681234562723156, 0.1244801440512181, 0.20655979029784802, 0.19596782350727218, 0.20966567259687383, 0.1782686192999789, 0.22072310933251865, 0.13576106266026494, 0.1603334062522409, 0.16581044646539417, 0.2046128895638377, 0.20909906575387113, 0.16025744066257985, 0.14588350012169338, 0.1542962011891737, 0.16805741894071732, 0.1375983691635647, 0.14180723086571867, 0.1554073894155896, 0.11093984244471337, 0.1641965914915035, 0.1357073095582248, 0.21799021934078494, 0.15539820725852865, 0.18601415473427751, 0.12394099813260818, 0.2364226327568536, 0.18070762013490962, 0.20380483923787218, 0.2045479702821509, 0.23433036867254378, 0.20657723741296577, 0.1556225447949309, 0.14392998132431478, 0.19907256227707168, 0.1818695423484311, 0.23100302136532666, 0.12558016406112782, 0.2057291050755603, 0.09867096226209039, 0.17544178058359472, 0.19149443490340798, 0.1795600435957693, 0.20470865025506835, 0.1885510790696066, 0.1958980514570907, 0.16088694079840002, 0.09786808966221319, 0.1805812538493614, 0.12974616924992974, 0.20524896850553698, 0.14580374731700016, 0.21227279024670664, 0.16614772456030127, 0.197837614192894, 0.21147454678939717, 0.2011508199756996, 0.19145861570694858, 0.16263925934744286, 0.14462186596387935, 0.24223862975392801, 0.11455859346922957, 0.11774845927939083, 0.13893886886852772, 0.20193946604558302, 0.20708802320144562, 0.1643218771550257, 0.18825857984390112, 0.17285580551258267, 0.1775260521294247, 0.15337434066321992, 0.1577609601506831, 0.11586524238610374, 0.17560055110537928, 0.10412083207064442, 0.15499948690524937, 0.17258954297376314, 0.16396620417551425, 0.2920973272945473, 0.13944823277622206, 0.2006531848203508, 0.16273620473190814, 0.21160161462131422, 0.16379460563231651, 0.20376845942070032, 0.16463459563161592, 0.15509550172755532, 0.15796769568939978, 0.2556379651938828, 0.20695624067345306, 0.21352794863960528, 0.1286401621836804, 0.17748050236758053, 0.13062332739465005, 0.16445724563190375, 0.14158944608931923, 0.1681547702377048, 0.14102674536320703, 0.17028556266451667, 0.16931261703086298, 0.18348660519780305, 0.1465389546390942, 0.16981736938128994, 0.12426726549925292, 0.14855037024871642, 0.1926228659009052, 0.15315869160596524, 0.13222195996434846, 0.09207462094409868, 0.1995570370494033, 0.2598212712904103, 0.1608127104885239, 0.1918374798991587, 0.1775762693497744, 0.1785274845086898, 0.17690027483925191, 0.20643007534578897, 0.19659935448627003, 0.19621626541739473, 0.1434422547866, 0.198423261840704, 0.11347200100374583, 0.1475269114069658, 0.15717109232994447, 0.22746378458070535, 0.1437723937471426, 0.1565514007967504, 0.1681508215552983, 0.1320630639645246, 0.1388270536387954, 0.16044880671265294, 0.1627035572164504, 0.1800254112511151, 0.2164749321328098, 0.23304214793935918, 0.2240102499605465, 0.23262986879500294, 0.15598451601070357, 0.17634346964063247, 0.15965885302271912, 0.20971688388635457, 0.14643447060624504, 0.1713946070717265, 0.15190911176502025, 0.1473880074491544, 0.19663901675694936, 0.23364066910605433, 0.13192792751673063, 0.27333556446602064, 0.19565850963311515, 0.19743476834140147, 0.17256194613189432, 0.13751187926111255, 0.1826698461898127, 0.1831987838635964, 0.16641029806855628, 0.12480079200416748, 0.2193041136638459, 0.12481398813928767, 0.07281178949617395, 0.10493704402043404, 0.13719143701833475, 0.27214748994318044, 0.12649767651965613, 0.07228261784861464, 0.040235302883648176, 0.15302155818805904, 0.05805696923728813, 0.08198550741602342, 0.06795870888170998, 0.03862837692115261, 0.19384121338696048, 0.15444938094137964, 0.04322193757711848, 0.03112559109718721, 0.14500140682557738, 0.07651170628691094, 0.04190501266003635, 0.05585055789486212, 0.06487139975088071, 0.0813926899693799, 0.07101239532251216, 0.10752171244741016, 0.16595885409436695, 0.03474315170979735, 0.1294517685759158, 0.14338876445353704, 0.07197511303533838, 0.07751020757680509, 0.03871285479177844, 0.05291428389886577, 0.2267100217936565, 0.07672715000102544, 0.11528750422361415, 0.08791379626674949, 0.05799688788917487, 0.14387511011794749, 0.06928704858394073, 0.25208462448106866, 0.13874146106210067, 0.060775347704981324, 0.07348614473410998, 0.08069042863965006, 0.07099690122800503, 0.03780261203633219, 0.10348208586563061, 0.08839389625248821, 0.05928670009696807, 0.13791895849745345, 0.051401565376094735, 0.03986693629526769, 0.036914426686241786, 0.09512323278773918, 0.0864379152074343, 0.11025707432684254, 0.13691532558966862, 0.08274203369501199, 0.1130996469757128, 0.09423872537675816, 0.05383130687072374, 0.06675246330603225, 0.13103477507186032, 0.11302935253797715, 0.051478728821429016, 0.09878236887657671, 0.1475903192327411, 0.11225019661502098, 0.030392442124376164, 0.1047436523221584, 0.046480751129326155, 0.13547309482380024, 0.06327749720176873, 0.08624154955435996, 0.035544505483109924, 0.06797142223454126, 0.11809232619713411, 0.0477490569833712, 0.05283396577246199, 0.10001094779367782, 0.0625808956700498, 0.0515187987270929, 0.036542321828884554, 0.07612270518486354, 0.1700758240855396, 0.10057416683886548, 0.05373819346770549, 0.04826597136423666, 0.10408801247732956, 0.2044537789119534, 0.07355636423737712, 0.05156718720391164, 0.052514526018273534, 0.1406270268215729, 0.08447617925427546, 0.10024485905303125, 0.14278575368302773, 0.10969867100278075, 0.11794750638908462, 0.05760422012443253, 0.06946285880182237, 0.09195579117650976, 0.09368418697349905, 0.11955792628461127, 0.031848211118605925, 0.12833339365549434, 0.04847522892987615, 0.19206988623361554, 0.14834998154559406, 0.16733747910562802, 0.08543831425071859, 0.15182233245116739, 0.06854704498535648, 0.1370346213720283, 0.14700776997049564, 0.03850914714008509, 0.0818100329262052, 0.09147901678673782, 0.05328949875588751, 0.04675163427716832, 0.15340764712092775, 0.12033412093401381, 0.09503421529802397, 0.22560999639503093, 0.14567128296773577, 0.09502285880136761, 0.17721647215407846, 0.1514434752894512, 0.1177989792726988, 0.14159933420541593, 0.03559726839685498, 0.12171247703213574, 0.06468350378463716, 0.1193356209788889, 0.08551958947463349, 0.07747164511834224, 0.027842076252380846, 0.028185552738264143, 0.07064785702645016, 0.06033745475888849, 0.17974098686002962, 0.0597589848278127, 0.08696547266461603, 0.05127155635886097, 0.11908498660120759, 0.06079842160934866, 0.03577552357434323, 0.06527014661713545, 0.1341386541848365, 0.07760456274875709, 0.11437651996329129, 0.15944259915832248, 0.04894193139482092, 0.0881721079368913, 0.19679840328634113, 0.05271109235777563, 0.10754023895464909, 0.2417725338500996, 0.049673665157262056, 0.056517080099641466, 0.0809880133374811, 0.05354234588045782, 0.05366416606295679, 0.11699674743813473, 0.07079348252747454, 0.04783775477567576, 0.04950671995583544, 0.10971195224276427, 0.04881208160405383, 0.041732730316884965, 0.05837828158427483, 0.0751728730922999, 0.08717102844837049, 0.11686510268013654, 0.12367869663172497, 0.14054048431979746, 0.044431296756350894, 0.04807396036747723, 0.03665663808869892, 0.04488974476246155, 0.06060234859765055, 0.09698830458125952, 0.05816721369035483, 0.18116104380560652, 0.1060332093160911, 0.04444294512063952, 0.08634325493576202, 0.1626876668991726, 0.10546567996697964, 0.06712392964946401, 0.07998348422761142, 0.05785699418419525, 0.07388091297208876, 0.0821784685601931, 0.1508614037021069, 0.16322471474552366, 0.06625429372539549, 0.06619798528959417, 0.10884906828570559, 0.04947147671534312, 0.13594436453864955, 0.09417466597404803, 0.1479045694270968, 0.047935331371210727, 0.1316189933389746, 0.07103400221005797, 0.10533099294561461, 0.09442984978355237, 0.06716284021124423, 0.03561275030745519, 0.1033862414674978, 0.1189874531777251, 0.051656189708541383, 0.1089171581240844, 0.03148274726444108, 0.1484195142131307, 0.052527311448936624, 0.03531571770663856, 0.164366125718987, 0.04143393343368367, 0.08138160793581284, 0.034781977346293826, 0.11713028232562873, 0.06883187912125775, 0.19350673125861337, 0.13357325039251744, 0.13825487561932728, 0.06213641329366986, 0.08405865230803265, 0.05303046276199451, 0.09680890002765223, 0.14763942348152406, 0.16378303354105803, 0.048257792335723165, 0.07879195043959177, 0.036128302613707974, 0.07134180016093399, 0.11953949054409262, 0.13032502703480414, 0.11157761442909547, 0.024585841177017155, 0.10868005167357259, 0.09226929823702192, 0.05109834803897763, 0.06912359089119716, 0.14883203031435194, 0.1769293360296404, 0.04872486410825131, 0.0769410900111754, 0.06339414448432452, 0.060166596241004774, 0.060549373207946934, 0.053239536757052223, 0.03771340657709184, 0.04847574120083166, 0.09926041848586419, 0.05418038709050515, 0.10130078533584759, 0.11298605835859561, 0.10147097292457362, 0.11597793408545552, 0.05255790512131216, 0.0933275606393169, 0.152421692065125, 0.0940665705763841, 0.10543669678768221, 0.0638876774795238, 0.12767977171752198, 0.09617112023435181, 0.10234980154451924, 0.07430299711222396, 0.15230441233161815, 0.13177939572715472, 0.08323269704568266, 0.043796239941228304, 0.12632345561813726, 0.20337916403074233, 0.15032311598853682, 0.039692116643076675, 0.03977139223704301, 0.06888632106846009, 0.1319256340456574, 0.04981630394739104, 0.04673243444381724, 0.16452014430099474, 0.10968264516314592, 0.07466406727554485, 0.1100494519614521, 0.07943858807289741, 0.054399674030544604, 0.08969027852566372, 0.06702567612041806, 0.16195949851274385, 0.0778346261623867, 0.0880555692125181, 0.07636140761825211, 0.06599862996532393, 0.03692650003691665, 0.12988513335388865, 0.17194389375670097, 0.11683314218538501, 0.07186116008981834, 0.07713357713920502, 0.14771771569460654, 0.15895840597749522, 0.075925314136887, 0.1663482963581558, 0.08805323385091748, 0.14221333688535143, 0.09527444639813512, 0.11900389656174884, 0.12657612096382917, 0.15644419086764358, 0.04980442001215117, 0.04036994748753253, 0.043574665349435465, 0.05051458209203601, 0.07818419799901524, 0.1219804962044828, 0.11106430334958219, 0.046811452397496345, 0.11433088182476478, 0.08514464606550694, 0.11034585244417597, 0.038359064102206034, 0.07874552405090016, 0.07633844218811678, 0.04694616637478513, 0.0688652562039782, 0.08503119833475781, 0.04360331115753288, 0.049106275813213376, 0.06785874063846127, 0.09316125206619805, 0.07151202382918653, 0.11776586714057642, 0.054120027344412534, 0.03062217371791511, 0.04127254329119204, 0.05395748270229117, 0.11092610884235954, 0.08158335908655401, 0.09015862279824643, 0.08875661665441024, 0.06874010719139514, 0.03570174430390181, 0.1329154479973404, 0.04631233689980402, 0.06225748335422323, 0.07290004905760313, 0.13428033277443222, 0.06469674974807005, 0.047342607506477126, 0.14978273902816855, 0.026607820170341516, 0.06652978435882344, 0.056695014798274865, 0.08899558449766953, 0.09982067324382796, 0.1754732996406596, 0.14899485227731266, 0.09301591287010591, 0.04751407935603292, 0.020514934918892606, 0.1423567826400582, 0.05087027184754627, 0.14288771235940967, 0.08762895921936537, 0.10179170865110536, 0.044484477890013474, 0.11042808078140823, 0.02410443486645871, 0.13521160208906305, 0.049726435793908716, 0.07882277285265415, 0.05814203754242197, 0.04110478293661493, 0.08886520196350627, 0.1191111363530963, 0.05774321987310316, 0.1287087904334239, 0.09244492928588163, 0.05027179877523988, 0.06215050779676045, 0.0773896771732896, 0.07672587710331526, 0.04254183921525727, 0.0335550851652942, 0.07551325394909743, 0.11960289890285515, 0.04833158360316323, 0.07460043158456951, 0.07189867354803789, 0.07763841547318373, 0.0645204538581941, 0.055515535321878785, 0.043062656798644475, 0.08160954394445029, 0.09246458828002288, 0.15154044241878392, 0.0921789619354163, 0.053154978773045994, 0.05739990360651091, 0.04646418847135658, 0.12576410535981666, 0.0850169260312224, 0.06520375879168223, 0.05037671862411395, 0.0725220224528198, 0.061967229637006994, 0.037101721230551124, 0.07829271935383476, 0.1888890256519287, 0.12739326052944047, 0.04540757217478327, 0.07786687904195108, 0.06091035835759964, 0.1849358954374134, 0.150181097274404, 0.04396771222172633, 0.05778909333330271, 0.07129271743848849, 0.14321819172701722, 0.15133178657993154, 0.08188901179514929, 0.07115636726569419, 0.06212205272255783, 0.1278971292534331, 0.06075047042654462, 0.10672622673551692, 0.08603139340203447, 0.03890763530859809, 0.12465291455355149, 0.071301748616533, 0.05055614474253393, 0.08114061137091791, 0.06835627262334494, 0.04838940305495805, 0.19251510405244718, 0.06863513531771614, 0.08572269470128616, 0.05188640947337856, 0.052041857400510545, 0.07304621477916057, 0.05951948346199167, 0.09979823932328584, 0.10281014721930849, 0.12064928500067165, 0.11542752785689747, 0.05121591103586747, 0.041837517804999926, 0.1375364125984486, 0.08876908190521127, 0.10356739825752194, 0.03695172248144725, 0.07323397145152365, 0.09776967258547463, 0.054284101584745253, 0.0532140930891111, 0.11670080486389428, 0.06782120609791971, 0.1354176215789648]
0.08996349039830227
Making ranges
torch.Size([35650, 2])
We keep 5.45e+06/3.71e+08 =  1% of the original kernel matrix.

torch.Size([2993, 2])
We keep 8.84e+04/1.27e+06 =  6% of the original kernel matrix.

torch.Size([11709, 2])
We keep 7.42e+05/2.17e+07 =  3% of the original kernel matrix.

torch.Size([25200, 2])
We keep 4.68e+06/2.04e+08 =  2% of the original kernel matrix.

torch.Size([31348, 2])
We keep 4.73e+06/2.75e+08 =  1% of the original kernel matrix.

torch.Size([6069, 2])
We keep 3.30e+05/6.99e+06 =  4% of the original kernel matrix.

torch.Size([15437, 2])
We keep 1.33e+06/5.10e+07 =  2% of the original kernel matrix.

torch.Size([4684, 2])
We keep 2.16e+05/3.74e+06 =  5% of the original kernel matrix.

torch.Size([13925, 2])
We keep 1.08e+06/3.73e+07 =  2% of the original kernel matrix.

torch.Size([63765, 2])
We keep 4.27e+07/1.49e+09 =  2% of the original kernel matrix.

torch.Size([47784, 2])
We keep 1.05e+07/7.45e+08 =  1% of the original kernel matrix.

torch.Size([83916, 2])
We keep 3.55e+07/2.23e+09 =  1% of the original kernel matrix.

torch.Size([54648, 2])
We keep 1.26e+07/9.10e+08 =  1% of the original kernel matrix.

torch.Size([93612, 2])
We keep 4.79e+07/3.08e+09 =  1% of the original kernel matrix.

torch.Size([57910, 2])
We keep 1.45e+07/1.07e+09 =  1% of the original kernel matrix.

torch.Size([81055, 2])
We keep 2.95e+07/2.01e+09 =  1% of the original kernel matrix.

torch.Size([53572, 2])
We keep 1.20e+07/8.64e+08 =  1% of the original kernel matrix.

torch.Size([11744, 2])
We keep 1.11e+06/3.29e+07 =  3% of the original kernel matrix.

torch.Size([20931, 2])
We keep 2.31e+06/1.10e+08 =  2% of the original kernel matrix.

torch.Size([59600, 2])
We keep 4.00e+08/3.52e+09 = 11% of the original kernel matrix.

torch.Size([45638, 2])
We keep 1.54e+07/1.14e+09 =  1% of the original kernel matrix.

torch.Size([15704, 2])
We keep 2.23e+06/7.11e+07 =  3% of the original kernel matrix.

torch.Size([24070, 2])
We keep 3.16e+06/1.63e+08 =  1% of the original kernel matrix.

torch.Size([111349, 2])
We keep 1.11e+08/5.93e+09 =  1% of the original kernel matrix.

torch.Size([62602, 2])
We keep 1.90e+07/1.48e+09 =  1% of the original kernel matrix.

torch.Size([11728, 2])
We keep 1.01e+06/3.00e+07 =  3% of the original kernel matrix.

torch.Size([20544, 2])
We keep 2.25e+06/1.06e+08 =  2% of the original kernel matrix.

torch.Size([396753, 2])
We keep 6.14e+08/4.93e+10 =  1% of the original kernel matrix.

torch.Size([123332, 2])
We keep 4.81e+07/4.28e+09 =  1% of the original kernel matrix.

torch.Size([36939, 2])
We keep 8.39e+06/4.72e+08 =  1% of the original kernel matrix.

torch.Size([37015, 2])
We keep 6.59e+06/4.19e+08 =  1% of the original kernel matrix.

torch.Size([121951, 2])
We keep 8.52e+07/4.83e+09 =  1% of the original kernel matrix.

torch.Size([66882, 2])
We keep 1.76e+07/1.34e+09 =  1% of the original kernel matrix.

torch.Size([102729, 2])
We keep 6.28e+07/3.39e+09 =  1% of the original kernel matrix.

torch.Size([61040, 2])
We keep 1.51e+07/1.12e+09 =  1% of the original kernel matrix.

torch.Size([59525, 2])
We keep 2.55e+07/1.31e+09 =  1% of the original kernel matrix.

torch.Size([46340, 2])
We keep 1.01e+07/6.98e+08 =  1% of the original kernel matrix.

torch.Size([384893, 2])
We keep 7.57e+08/4.93e+10 =  1% of the original kernel matrix.

torch.Size([121406, 2])
We keep 4.84e+07/4.28e+09 =  1% of the original kernel matrix.

torch.Size([190047, 2])
We keep 1.72e+08/1.27e+10 =  1% of the original kernel matrix.

torch.Size([85223, 2])
We keep 2.68e+07/2.17e+09 =  1% of the original kernel matrix.

torch.Size([19929, 2])
We keep 6.85e+06/1.80e+08 =  3% of the original kernel matrix.

torch.Size([26973, 2])
We keep 4.58e+06/2.58e+08 =  1% of the original kernel matrix.

torch.Size([742974, 2])
We keep 1.84e+09/1.72e+11 =  1% of the original kernel matrix.

torch.Size([175965, 2])
We keep 8.46e+07/8.00e+09 =  1% of the original kernel matrix.

torch.Size([28795, 2])
We keep 6.08e+06/2.90e+08 =  2% of the original kernel matrix.

torch.Size([33111, 2])
We keep 5.41e+06/3.28e+08 =  1% of the original kernel matrix.

torch.Size([21646, 2])
We keep 2.94e+06/1.33e+08 =  2% of the original kernel matrix.

torch.Size([28787, 2])
We keep 4.01e+06/2.22e+08 =  1% of the original kernel matrix.

torch.Size([27134, 2])
We keep 2.42e+07/4.88e+08 =  4% of the original kernel matrix.

torch.Size([30283, 2])
We keep 6.61e+06/4.26e+08 =  1% of the original kernel matrix.

torch.Size([122371, 2])
We keep 5.90e+07/4.94e+09 =  1% of the original kernel matrix.

torch.Size([67091, 2])
We keep 1.76e+07/1.35e+09 =  1% of the original kernel matrix.

torch.Size([47439, 2])
We keep 3.82e+07/1.20e+09 =  3% of the original kernel matrix.

torch.Size([40912, 2])
We keep 9.91e+06/6.68e+08 =  1% of the original kernel matrix.

torch.Size([15505, 2])
We keep 3.62e+06/7.16e+07 =  5% of the original kernel matrix.

torch.Size([23945, 2])
We keep 3.16e+06/1.63e+08 =  1% of the original kernel matrix.

torch.Size([87929, 2])
We keep 5.75e+07/3.20e+09 =  1% of the original kernel matrix.

torch.Size([56090, 2])
We keep 1.46e+07/1.09e+09 =  1% of the original kernel matrix.

torch.Size([3646953, 2])
We keep 1.79e+10/2.82e+12 =  0% of the original kernel matrix.

torch.Size([398544, 2])
We keep 3.10e+08/3.24e+10 =  0% of the original kernel matrix.

torch.Size([10784, 2])
We keep 2.52e+06/3.24e+07 =  7% of the original kernel matrix.

torch.Size([20003, 2])
We keep 2.37e+06/1.10e+08 =  2% of the original kernel matrix.

torch.Size([623555, 2])
We keep 2.46e+09/1.47e+11 =  1% of the original kernel matrix.

torch.Size([159852, 2])
We keep 8.05e+07/7.40e+09 =  1% of the original kernel matrix.

torch.Size([18856, 2])
We keep 3.01e+06/1.05e+08 =  2% of the original kernel matrix.

torch.Size([26600, 2])
We keep 3.68e+06/1.97e+08 =  1% of the original kernel matrix.

torch.Size([18367, 2])
We keep 2.21e+06/8.88e+07 =  2% of the original kernel matrix.

torch.Size([26329, 2])
We keep 3.44e+06/1.82e+08 =  1% of the original kernel matrix.

torch.Size([13755, 2])
We keep 2.32e+06/5.76e+07 =  4% of the original kernel matrix.

torch.Size([22510, 2])
We keep 2.92e+06/1.46e+08 =  1% of the original kernel matrix.

torch.Size([146111, 2])
We keep 2.54e+08/9.48e+09 =  2% of the original kernel matrix.

torch.Size([73497, 2])
We keep 2.38e+07/1.88e+09 =  1% of the original kernel matrix.

torch.Size([122460, 2])
We keep 7.20e+07/5.10e+09 =  1% of the original kernel matrix.

torch.Size([67317, 2])
We keep 1.79e+07/1.38e+09 =  1% of the original kernel matrix.

torch.Size([3666, 2])
We keep 1.91e+05/2.82e+06 =  6% of the original kernel matrix.

torch.Size([12228, 2])
We keep 9.81e+05/3.23e+07 =  3% of the original kernel matrix.

torch.Size([9702, 2])
We keep 1.09e+06/2.18e+07 =  5% of the original kernel matrix.

torch.Size([18805, 2])
We keep 2.04e+06/9.00e+07 =  2% of the original kernel matrix.

torch.Size([6257, 2])
We keep 4.93e+05/7.96e+06 =  6% of the original kernel matrix.

torch.Size([15306, 2])
We keep 1.41e+06/5.44e+07 =  2% of the original kernel matrix.

torch.Size([11694, 2])
We keep 1.11e+06/3.10e+07 =  3% of the original kernel matrix.

torch.Size([20779, 2])
We keep 2.29e+06/1.07e+08 =  2% of the original kernel matrix.

torch.Size([2924, 2])
We keep 2.89e+05/2.85e+06 = 10% of the original kernel matrix.

torch.Size([10798, 2])
We keep 9.90e+05/3.25e+07 =  3% of the original kernel matrix.

torch.Size([2554, 2])
We keep 6.25e+04/9.01e+05 =  6% of the original kernel matrix.

torch.Size([11014, 2])
We keep 6.67e+05/1.83e+07 =  3% of the original kernel matrix.

torch.Size([6236, 2])
We keep 3.75e+05/7.06e+06 =  5% of the original kernel matrix.

torch.Size([15433, 2])
We keep 1.34e+06/5.12e+07 =  2% of the original kernel matrix.

torch.Size([3024, 2])
We keep 9.75e+04/1.43e+06 =  6% of the original kernel matrix.

torch.Size([11663, 2])
We keep 7.77e+05/2.30e+07 =  3% of the original kernel matrix.

torch.Size([4138, 2])
We keep 1.70e+05/2.66e+06 =  6% of the original kernel matrix.

torch.Size([13246, 2])
We keep 9.63e+05/3.15e+07 =  3% of the original kernel matrix.

torch.Size([12135, 2])
We keep 2.50e+06/4.75e+07 =  5% of the original kernel matrix.

torch.Size([21327, 2])
We keep 2.77e+06/1.33e+08 =  2% of the original kernel matrix.

torch.Size([11101, 2])
We keep 9.99e+05/2.88e+07 =  3% of the original kernel matrix.

torch.Size([19996, 2])
We keep 2.22e+06/1.03e+08 =  2% of the original kernel matrix.

torch.Size([3518, 2])
We keep 1.09e+05/1.80e+06 =  6% of the original kernel matrix.

torch.Size([12431, 2])
We keep 8.38e+05/2.58e+07 =  3% of the original kernel matrix.

torch.Size([14632, 2])
We keep 2.51e+06/6.32e+07 =  3% of the original kernel matrix.

torch.Size([23216, 2])
We keep 3.03e+06/1.53e+08 =  1% of the original kernel matrix.

torch.Size([25512, 2])
We keep 4.97e+06/2.06e+08 =  2% of the original kernel matrix.

torch.Size([31426, 2])
We keep 4.81e+06/2.77e+08 =  1% of the original kernel matrix.

torch.Size([4408, 2])
We keep 1.82e+05/3.14e+06 =  5% of the original kernel matrix.

torch.Size([13390, 2])
We keep 1.02e+06/3.41e+07 =  2% of the original kernel matrix.

torch.Size([9332, 2])
We keep 9.55e+05/1.91e+07 =  4% of the original kernel matrix.

torch.Size([18568, 2])
We keep 1.95e+06/8.42e+07 =  2% of the original kernel matrix.

torch.Size([5851, 2])
We keep 3.95e+05/7.05e+06 =  5% of the original kernel matrix.

torch.Size([14941, 2])
We keep 1.35e+06/5.12e+07 =  2% of the original kernel matrix.

torch.Size([13751, 2])
We keep 1.50e+06/4.99e+07 =  3% of the original kernel matrix.

torch.Size([22408, 2])
We keep 2.74e+06/1.36e+08 =  2% of the original kernel matrix.

torch.Size([6109, 2])
We keep 5.52e+05/8.06e+06 =  6% of the original kernel matrix.

torch.Size([15369, 2])
We keep 1.43e+06/5.47e+07 =  2% of the original kernel matrix.

torch.Size([6355, 2])
We keep 3.79e+05/7.54e+06 =  5% of the original kernel matrix.

torch.Size([15675, 2])
We keep 1.37e+06/5.29e+07 =  2% of the original kernel matrix.

torch.Size([7381, 2])
We keep 4.57e+05/9.73e+06 =  4% of the original kernel matrix.

torch.Size([16737, 2])
We keep 1.52e+06/6.01e+07 =  2% of the original kernel matrix.

torch.Size([7677, 2])
We keep 5.11e+05/1.18e+07 =  4% of the original kernel matrix.

torch.Size([16965, 2])
We keep 1.61e+06/6.62e+07 =  2% of the original kernel matrix.

torch.Size([4477, 2])
We keep 2.42e+05/3.57e+06 =  6% of the original kernel matrix.

torch.Size([13448, 2])
We keep 1.07e+06/3.64e+07 =  2% of the original kernel matrix.

torch.Size([11234, 2])
We keep 1.05e+06/2.82e+07 =  3% of the original kernel matrix.

torch.Size([20409, 2])
We keep 2.23e+06/1.02e+08 =  2% of the original kernel matrix.

torch.Size([5236, 2])
We keep 3.09e+05/5.29e+06 =  5% of the original kernel matrix.

torch.Size([14316, 2])
We keep 1.22e+06/4.43e+07 =  2% of the original kernel matrix.

torch.Size([16546, 2])
We keep 2.02e+06/6.71e+07 =  3% of the original kernel matrix.

torch.Size([25030, 2])
We keep 3.09e+06/1.58e+08 =  1% of the original kernel matrix.

torch.Size([14548, 2])
We keep 1.54e+06/5.24e+07 =  2% of the original kernel matrix.

torch.Size([23112, 2])
We keep 2.80e+06/1.39e+08 =  2% of the original kernel matrix.

torch.Size([5787, 2])
We keep 3.57e+05/6.42e+06 =  5% of the original kernel matrix.

torch.Size([15014, 2])
We keep 1.32e+06/4.88e+07 =  2% of the original kernel matrix.

torch.Size([7598, 2])
We keep 5.47e+05/1.15e+07 =  4% of the original kernel matrix.

torch.Size([16806, 2])
We keep 1.61e+06/6.54e+07 =  2% of the original kernel matrix.

torch.Size([5730, 2])
We keep 3.30e+05/6.20e+06 =  5% of the original kernel matrix.

torch.Size([14925, 2])
We keep 1.29e+06/4.80e+07 =  2% of the original kernel matrix.

torch.Size([13013, 2])
We keep 1.52e+06/4.50e+07 =  3% of the original kernel matrix.

torch.Size([21656, 2])
We keep 2.66e+06/1.29e+08 =  2% of the original kernel matrix.

torch.Size([9890, 2])
We keep 7.64e+05/1.90e+07 =  4% of the original kernel matrix.

torch.Size([19161, 2])
We keep 1.91e+06/8.39e+07 =  2% of the original kernel matrix.

torch.Size([5255, 2])
We keep 2.34e+05/4.53e+06 =  5% of the original kernel matrix.

torch.Size([14504, 2])
We keep 1.15e+06/4.10e+07 =  2% of the original kernel matrix.

torch.Size([6508, 2])
We keep 6.50e+05/1.14e+07 =  5% of the original kernel matrix.

torch.Size([15241, 2])
We keep 1.59e+06/6.51e+07 =  2% of the original kernel matrix.

torch.Size([7906, 2])
We keep 5.11e+05/1.26e+07 =  4% of the original kernel matrix.

torch.Size([17243, 2])
We keep 1.65e+06/6.85e+07 =  2% of the original kernel matrix.

torch.Size([6577, 2])
We keep 4.10e+05/8.76e+06 =  4% of the original kernel matrix.

torch.Size([15670, 2])
We keep 1.47e+06/5.70e+07 =  2% of the original kernel matrix.

torch.Size([4975, 2])
We keep 3.88e+05/6.44e+06 =  6% of the original kernel matrix.

torch.Size([13797, 2])
We keep 1.30e+06/4.89e+07 =  2% of the original kernel matrix.

torch.Size([6752, 2])
We keep 4.04e+05/8.76e+06 =  4% of the original kernel matrix.

torch.Size([16030, 2])
We keep 1.44e+06/5.70e+07 =  2% of the original kernel matrix.

torch.Size([7196, 2])
We keep 6.00e+05/1.10e+07 =  5% of the original kernel matrix.

torch.Size([16452, 2])
We keep 1.60e+06/6.40e+07 =  2% of the original kernel matrix.

torch.Size([3451, 2])
We keep 1.47e+05/2.16e+06 =  6% of the original kernel matrix.

torch.Size([12237, 2])
We keep 8.85e+05/2.83e+07 =  3% of the original kernel matrix.

torch.Size([2222, 2])
We keep 5.66e+04/7.73e+05 =  7% of the original kernel matrix.

torch.Size([10418, 2])
We keep 6.28e+05/1.69e+07 =  3% of the original kernel matrix.

torch.Size([5899, 2])
We keep 2.86e+05/5.78e+06 =  4% of the original kernel matrix.

torch.Size([15152, 2])
We keep 1.24e+06/4.63e+07 =  2% of the original kernel matrix.

torch.Size([19918, 2])
We keep 3.21e+06/1.30e+08 =  2% of the original kernel matrix.

torch.Size([27020, 2])
We keep 3.96e+06/2.20e+08 =  1% of the original kernel matrix.

torch.Size([7234, 2])
We keep 4.84e+05/9.95e+06 =  4% of the original kernel matrix.

torch.Size([16475, 2])
We keep 1.52e+06/6.08e+07 =  2% of the original kernel matrix.

torch.Size([3227, 2])
We keep 1.21e+05/1.76e+06 =  6% of the original kernel matrix.

torch.Size([11825, 2])
We keep 8.37e+05/2.55e+07 =  3% of the original kernel matrix.

torch.Size([13820, 2])
We keep 1.72e+06/5.04e+07 =  3% of the original kernel matrix.

torch.Size([22621, 2])
We keep 2.76e+06/1.37e+08 =  2% of the original kernel matrix.

torch.Size([3973, 2])
We keep 1.55e+05/2.42e+06 =  6% of the original kernel matrix.

torch.Size([12912, 2])
We keep 9.33e+05/3.00e+07 =  3% of the original kernel matrix.

torch.Size([4644, 2])
We keep 1.76e+05/3.22e+06 =  5% of the original kernel matrix.

torch.Size([13894, 2])
We keep 1.01e+06/3.46e+07 =  2% of the original kernel matrix.

torch.Size([3482, 2])
We keep 1.36e+05/2.16e+06 =  6% of the original kernel matrix.

torch.Size([12278, 2])
We keep 8.99e+05/2.83e+07 =  3% of the original kernel matrix.

torch.Size([5164, 2])
We keep 3.78e+05/5.73e+06 =  6% of the original kernel matrix.

torch.Size([14403, 2])
We keep 1.28e+06/4.61e+07 =  2% of the original kernel matrix.

torch.Size([3149, 2])
We keep 1.12e+05/1.56e+06 =  7% of the original kernel matrix.

torch.Size([11815, 2])
We keep 8.08e+05/2.41e+07 =  3% of the original kernel matrix.

torch.Size([11398, 2])
We keep 1.30e+06/2.95e+07 =  4% of the original kernel matrix.

torch.Size([20393, 2])
We keep 2.28e+06/1.05e+08 =  2% of the original kernel matrix.

torch.Size([7591, 2])
We keep 4.83e+05/1.09e+07 =  4% of the original kernel matrix.

torch.Size([16848, 2])
We keep 1.56e+06/6.35e+07 =  2% of the original kernel matrix.

torch.Size([6433, 2])
We keep 5.02e+05/8.87e+06 =  5% of the original kernel matrix.

torch.Size([15564, 2])
We keep 1.48e+06/5.74e+07 =  2% of the original kernel matrix.

torch.Size([3714, 2])
We keep 1.58e+05/2.41e+06 =  6% of the original kernel matrix.

torch.Size([12462, 2])
We keep 9.12e+05/2.99e+07 =  3% of the original kernel matrix.

torch.Size([3459, 2])
We keep 1.42e+05/2.20e+06 =  6% of the original kernel matrix.

torch.Size([12357, 2])
We keep 9.01e+05/2.86e+07 =  3% of the original kernel matrix.

torch.Size([7070, 2])
We keep 5.88e+05/1.09e+07 =  5% of the original kernel matrix.

torch.Size([16377, 2])
We keep 1.59e+06/6.36e+07 =  2% of the original kernel matrix.

torch.Size([9289, 2])
We keep 7.92e+05/1.90e+07 =  4% of the original kernel matrix.

torch.Size([18379, 2])
We keep 1.91e+06/8.41e+07 =  2% of the original kernel matrix.

torch.Size([8281, 2])
We keep 5.39e+05/1.37e+07 =  3% of the original kernel matrix.

torch.Size([17513, 2])
We keep 1.69e+06/7.13e+07 =  2% of the original kernel matrix.

torch.Size([6436, 2])
We keep 4.06e+05/8.16e+06 =  4% of the original kernel matrix.

torch.Size([15725, 2])
We keep 1.43e+06/5.51e+07 =  2% of the original kernel matrix.

torch.Size([10141, 2])
We keep 1.33e+06/2.69e+07 =  4% of the original kernel matrix.

torch.Size([19338, 2])
We keep 2.21e+06/9.99e+07 =  2% of the original kernel matrix.

torch.Size([9947, 2])
We keep 9.14e+05/2.28e+07 =  4% of the original kernel matrix.

torch.Size([19064, 2])
We keep 2.05e+06/9.19e+07 =  2% of the original kernel matrix.

torch.Size([8386, 2])
We keep 5.52e+05/1.31e+07 =  4% of the original kernel matrix.

torch.Size([17620, 2])
We keep 1.68e+06/6.97e+07 =  2% of the original kernel matrix.

torch.Size([18325, 2])
We keep 2.55e+06/1.00e+08 =  2% of the original kernel matrix.

torch.Size([26239, 2])
We keep 3.59e+06/1.93e+08 =  1% of the original kernel matrix.

torch.Size([7281, 2])
We keep 4.59e+05/9.46e+06 =  4% of the original kernel matrix.

torch.Size([16705, 2])
We keep 1.50e+06/5.93e+07 =  2% of the original kernel matrix.

torch.Size([11491, 2])
We keep 1.09e+06/2.95e+07 =  3% of the original kernel matrix.

torch.Size([20390, 2])
We keep 2.27e+06/1.05e+08 =  2% of the original kernel matrix.

torch.Size([3293, 2])
We keep 1.21e+05/1.70e+06 =  7% of the original kernel matrix.

torch.Size([11901, 2])
We keep 8.34e+05/2.51e+07 =  3% of the original kernel matrix.

torch.Size([7582, 2])
We keep 6.20e+05/1.29e+07 =  4% of the original kernel matrix.

torch.Size([16840, 2])
We keep 1.70e+06/6.93e+07 =  2% of the original kernel matrix.

torch.Size([4891, 2])
We keep 2.72e+05/4.42e+06 =  6% of the original kernel matrix.

torch.Size([13983, 2])
We keep 1.16e+06/4.05e+07 =  2% of the original kernel matrix.

torch.Size([14532, 2])
We keep 1.58e+06/5.08e+07 =  3% of the original kernel matrix.

torch.Size([23169, 2])
We keep 2.77e+06/1.37e+08 =  2% of the original kernel matrix.

torch.Size([2377, 2])
We keep 9.08e+04/1.05e+06 =  8% of the original kernel matrix.

torch.Size([10520, 2])
We keep 6.92e+05/1.98e+07 =  3% of the original kernel matrix.

torch.Size([5323, 2])
We keep 2.90e+05/5.27e+06 =  5% of the original kernel matrix.

torch.Size([14601, 2])
We keep 1.23e+06/4.42e+07 =  2% of the original kernel matrix.

torch.Size([3449, 2])
We keep 3.77e+05/2.44e+06 = 15% of the original kernel matrix.

torch.Size([11994, 2])
We keep 9.42e+05/3.01e+07 =  3% of the original kernel matrix.

torch.Size([3833, 2])
We keep 1.51e+05/2.45e+06 =  6% of the original kernel matrix.

torch.Size([12716, 2])
We keep 9.38e+05/3.01e+07 =  3% of the original kernel matrix.

torch.Size([2542, 2])
We keep 7.47e+04/1.10e+06 =  6% of the original kernel matrix.

torch.Size([10900, 2])
We keep 7.13e+05/2.02e+07 =  3% of the original kernel matrix.

torch.Size([3484, 2])
We keep 1.66e+05/2.33e+06 =  7% of the original kernel matrix.

torch.Size([12194, 2])
We keep 9.15e+05/2.94e+07 =  3% of the original kernel matrix.

torch.Size([8153, 2])
We keep 6.09e+05/1.30e+07 =  4% of the original kernel matrix.

torch.Size([17531, 2])
We keep 1.67e+06/6.94e+07 =  2% of the original kernel matrix.

torch.Size([9326, 2])
We keep 8.74e+05/2.05e+07 =  4% of the original kernel matrix.

torch.Size([18391, 2])
We keep 1.97e+06/8.73e+07 =  2% of the original kernel matrix.

torch.Size([3556, 2])
We keep 2.02e+05/2.93e+06 =  6% of the original kernel matrix.

torch.Size([12092, 2])
We keep 1.00e+06/3.30e+07 =  3% of the original kernel matrix.

torch.Size([5172, 2])
We keep 2.65e+05/5.10e+06 =  5% of the original kernel matrix.

torch.Size([14320, 2])
We keep 1.18e+06/4.35e+07 =  2% of the original kernel matrix.

torch.Size([2869, 2])
We keep 7.69e+04/1.19e+06 =  6% of the original kernel matrix.

torch.Size([11532, 2])
We keep 7.25e+05/2.10e+07 =  3% of the original kernel matrix.

torch.Size([13759, 2])
We keep 1.56e+06/4.78e+07 =  3% of the original kernel matrix.

torch.Size([22652, 2])
We keep 2.71e+06/1.33e+08 =  2% of the original kernel matrix.

torch.Size([3972, 2])
We keep 1.65e+05/2.36e+06 =  6% of the original kernel matrix.

torch.Size([13010, 2])
We keep 9.32e+05/2.96e+07 =  3% of the original kernel matrix.

torch.Size([24869, 2])
We keep 4.65e+06/2.00e+08 =  2% of the original kernel matrix.

torch.Size([31015, 2])
We keep 4.70e+06/2.72e+08 =  1% of the original kernel matrix.

torch.Size([5466, 2])
We keep 3.55e+05/6.30e+06 =  5% of the original kernel matrix.

torch.Size([14629, 2])
We keep 1.30e+06/4.84e+07 =  2% of the original kernel matrix.

torch.Size([4423, 2])
We keep 2.44e+05/3.64e+06 =  6% of the original kernel matrix.

torch.Size([13465, 2])
We keep 1.08e+06/3.67e+07 =  2% of the original kernel matrix.

torch.Size([5591, 2])
We keep 2.67e+05/5.49e+06 =  4% of the original kernel matrix.

torch.Size([15066, 2])
We keep 1.21e+06/4.52e+07 =  2% of the original kernel matrix.

torch.Size([3970, 2])
We keep 1.48e+05/2.46e+06 =  6% of the original kernel matrix.

torch.Size([12857, 2])
We keep 9.33e+05/3.02e+07 =  3% of the original kernel matrix.

torch.Size([4736, 2])
We keep 2.15e+05/3.94e+06 =  5% of the original kernel matrix.

torch.Size([13856, 2])
We keep 1.10e+06/3.83e+07 =  2% of the original kernel matrix.

torch.Size([4661, 2])
We keep 1.77e+05/3.24e+06 =  5% of the original kernel matrix.

torch.Size([13971, 2])
We keep 1.02e+06/3.47e+07 =  2% of the original kernel matrix.

torch.Size([7377, 2])
We keep 4.63e+05/1.06e+07 =  4% of the original kernel matrix.

torch.Size([16772, 2])
We keep 1.55e+06/6.27e+07 =  2% of the original kernel matrix.

torch.Size([26101, 2])
We keep 4.57e+06/2.15e+08 =  2% of the original kernel matrix.

torch.Size([31732, 2])
We keep 4.85e+06/2.82e+08 =  1% of the original kernel matrix.

torch.Size([5471, 2])
We keep 2.77e+05/5.29e+06 =  5% of the original kernel matrix.

torch.Size([14726, 2])
We keep 1.22e+06/4.43e+07 =  2% of the original kernel matrix.

torch.Size([12831, 2])
We keep 1.27e+06/3.88e+07 =  3% of the original kernel matrix.

torch.Size([21721, 2])
We keep 2.49e+06/1.20e+08 =  2% of the original kernel matrix.

torch.Size([3858, 2])
We keep 1.62e+05/2.46e+06 =  6% of the original kernel matrix.

torch.Size([12773, 2])
We keep 9.32e+05/3.02e+07 =  3% of the original kernel matrix.

torch.Size([9113, 2])
We keep 9.17e+05/1.92e+07 =  4% of the original kernel matrix.

torch.Size([18252, 2])
We keep 1.94e+06/8.43e+07 =  2% of the original kernel matrix.

torch.Size([2909, 2])
We keep 1.65e+05/1.99e+06 =  8% of the original kernel matrix.

torch.Size([11158, 2])
We keep 8.70e+05/2.72e+07 =  3% of the original kernel matrix.

torch.Size([6447, 2])
We keep 5.32e+05/8.73e+06 =  6% of the original kernel matrix.

torch.Size([15580, 2])
We keep 1.48e+06/5.69e+07 =  2% of the original kernel matrix.

torch.Size([4333, 2])
We keep 1.76e+05/3.06e+06 =  5% of the original kernel matrix.

torch.Size([13635, 2])
We keep 1.01e+06/3.37e+07 =  3% of the original kernel matrix.

torch.Size([3373, 2])
We keep 1.53e+05/2.01e+06 =  7% of the original kernel matrix.

torch.Size([12000, 2])
We keep 8.83e+05/2.73e+07 =  3% of the original kernel matrix.

torch.Size([3969, 2])
We keep 1.66e+05/2.65e+06 =  6% of the original kernel matrix.

torch.Size([12930, 2])
We keep 9.68e+05/3.14e+07 =  3% of the original kernel matrix.

torch.Size([4711, 2])
We keep 2.14e+05/3.74e+06 =  5% of the original kernel matrix.

torch.Size([13827, 2])
We keep 1.08e+06/3.73e+07 =  2% of the original kernel matrix.

torch.Size([7096, 2])
We keep 5.04e+05/9.96e+06 =  5% of the original kernel matrix.

torch.Size([16421, 2])
We keep 1.52e+06/6.08e+07 =  2% of the original kernel matrix.

torch.Size([9512, 2])
We keep 7.82e+05/1.97e+07 =  3% of the original kernel matrix.

torch.Size([18694, 2])
We keep 1.95e+06/8.55e+07 =  2% of the original kernel matrix.

torch.Size([2472, 2])
We keep 6.68e+04/9.08e+05 =  7% of the original kernel matrix.

torch.Size([10719, 2])
We keep 6.70e+05/1.84e+07 =  3% of the original kernel matrix.

torch.Size([17043, 2])
We keep 2.36e+06/8.15e+07 =  2% of the original kernel matrix.

torch.Size([25159, 2])
We keep 3.33e+06/1.74e+08 =  1% of the original kernel matrix.

torch.Size([15658, 2])
We keep 2.36e+06/7.00e+07 =  3% of the original kernel matrix.

torch.Size([24167, 2])
We keep 3.15e+06/1.61e+08 =  1% of the original kernel matrix.

torch.Size([10964, 2])
We keep 9.46e+05/2.58e+07 =  3% of the original kernel matrix.

torch.Size([20112, 2])
We keep 2.15e+06/9.78e+07 =  2% of the original kernel matrix.

torch.Size([3974, 2])
We keep 1.61e+05/2.72e+06 =  5% of the original kernel matrix.

torch.Size([12971, 2])
We keep 9.65e+05/3.18e+07 =  3% of the original kernel matrix.

torch.Size([3774, 2])
We keep 1.50e+05/2.33e+06 =  6% of the original kernel matrix.

torch.Size([12739, 2])
We keep 9.10e+05/2.94e+07 =  3% of the original kernel matrix.

torch.Size([6404, 2])
We keep 8.35e+05/9.28e+06 =  8% of the original kernel matrix.

torch.Size([15682, 2])
We keep 1.51e+06/5.87e+07 =  2% of the original kernel matrix.

torch.Size([5110, 2])
We keep 2.26e+05/4.14e+06 =  5% of the original kernel matrix.

torch.Size([14428, 2])
We keep 1.11e+06/3.92e+07 =  2% of the original kernel matrix.

torch.Size([5826, 2])
We keep 3.82e+05/6.83e+06 =  5% of the original kernel matrix.

torch.Size([14932, 2])
We keep 1.34e+06/5.04e+07 =  2% of the original kernel matrix.

torch.Size([5346, 2])
We keep 3.02e+05/5.71e+06 =  5% of the original kernel matrix.

torch.Size([14352, 2])
We keep 1.23e+06/4.61e+07 =  2% of the original kernel matrix.

torch.Size([7474, 2])
We keep 8.05e+05/1.44e+07 =  5% of the original kernel matrix.

torch.Size([16737, 2])
We keep 1.77e+06/7.32e+07 =  2% of the original kernel matrix.

torch.Size([7782, 2])
We keep 5.71e+05/1.18e+07 =  4% of the original kernel matrix.

torch.Size([17066, 2])
We keep 1.62e+06/6.63e+07 =  2% of the original kernel matrix.

torch.Size([12533, 2])
We keep 3.45e+06/7.38e+07 =  4% of the original kernel matrix.

torch.Size([21100, 2])
We keep 3.24e+06/1.66e+08 =  1% of the original kernel matrix.

torch.Size([5660, 2])
We keep 3.17e+05/6.30e+06 =  5% of the original kernel matrix.

torch.Size([14837, 2])
We keep 1.29e+06/4.84e+07 =  2% of the original kernel matrix.

torch.Size([21728, 2])
We keep 3.79e+06/1.46e+08 =  2% of the original kernel matrix.

torch.Size([28610, 2])
We keep 4.17e+06/2.33e+08 =  1% of the original kernel matrix.

torch.Size([8556, 2])
We keep 5.47e+05/1.32e+07 =  4% of the original kernel matrix.

torch.Size([17927, 2])
We keep 1.67e+06/7.01e+07 =  2% of the original kernel matrix.

torch.Size([6095, 2])
We keep 3.49e+05/6.86e+06 =  5% of the original kernel matrix.

torch.Size([15409, 2])
We keep 1.34e+06/5.05e+07 =  2% of the original kernel matrix.

torch.Size([7179, 2])
We keep 4.38e+05/9.43e+06 =  4% of the original kernel matrix.

torch.Size([16575, 2])
We keep 1.51e+06/5.92e+07 =  2% of the original kernel matrix.

torch.Size([1577, 2])
We keep 2.51e+04/2.77e+05 =  9% of the original kernel matrix.

torch.Size([9271, 2])
We keep 4.44e+05/1.01e+07 =  4% of the original kernel matrix.

torch.Size([10836, 2])
We keep 8.98e+05/2.54e+07 =  3% of the original kernel matrix.

torch.Size([19851, 2])
We keep 2.12e+06/9.71e+07 =  2% of the original kernel matrix.

torch.Size([3922, 2])
We keep 1.68e+05/2.76e+06 =  6% of the original kernel matrix.

torch.Size([12822, 2])
We keep 9.80e+05/3.20e+07 =  3% of the original kernel matrix.

torch.Size([7247, 2])
We keep 5.03e+05/9.90e+06 =  5% of the original kernel matrix.

torch.Size([16581, 2])
We keep 1.53e+06/6.06e+07 =  2% of the original kernel matrix.

torch.Size([3309, 2])
We keep 1.40e+05/1.94e+06 =  7% of the original kernel matrix.

torch.Size([11915, 2])
We keep 8.60e+05/2.68e+07 =  3% of the original kernel matrix.

torch.Size([6735, 2])
We keep 4.32e+05/9.07e+06 =  4% of the original kernel matrix.

torch.Size([15996, 2])
We keep 1.47e+06/5.80e+07 =  2% of the original kernel matrix.

torch.Size([4013, 2])
We keep 1.40e+05/2.55e+06 =  5% of the original kernel matrix.

torch.Size([12977, 2])
We keep 9.38e+05/3.08e+07 =  3% of the original kernel matrix.

torch.Size([7143, 2])
We keep 4.39e+05/9.26e+06 =  4% of the original kernel matrix.

torch.Size([16712, 2])
We keep 1.45e+06/5.86e+07 =  2% of the original kernel matrix.

torch.Size([8258, 2])
We keep 5.38e+05/1.32e+07 =  4% of the original kernel matrix.

torch.Size([17463, 2])
We keep 1.68e+06/7.01e+07 =  2% of the original kernel matrix.

torch.Size([7937, 2])
We keep 4.74e+05/1.15e+07 =  4% of the original kernel matrix.

torch.Size([17250, 2])
We keep 1.59e+06/6.52e+07 =  2% of the original kernel matrix.

torch.Size([2011, 2])
We keep 5.65e+04/6.38e+05 =  8% of the original kernel matrix.

torch.Size([9748, 2])
We keep 5.88e+05/1.54e+07 =  3% of the original kernel matrix.

torch.Size([3604, 2])
We keep 1.55e+05/2.20e+06 =  7% of the original kernel matrix.

torch.Size([12301, 2])
We keep 8.99e+05/2.86e+07 =  3% of the original kernel matrix.

torch.Size([3090, 2])
We keep 1.46e+05/1.83e+06 =  7% of the original kernel matrix.

torch.Size([11563, 2])
We keep 8.56e+05/2.61e+07 =  3% of the original kernel matrix.

torch.Size([13007, 2])
We keep 1.31e+06/4.08e+07 =  3% of the original kernel matrix.

torch.Size([21751, 2])
We keep 2.55e+06/1.23e+08 =  2% of the original kernel matrix.

torch.Size([5550, 2])
We keep 3.62e+05/5.88e+06 =  6% of the original kernel matrix.

torch.Size([14755, 2])
We keep 1.28e+06/4.67e+07 =  2% of the original kernel matrix.

torch.Size([12272, 2])
We keep 1.58e+06/3.80e+07 =  4% of the original kernel matrix.

torch.Size([21401, 2])
We keep 2.48e+06/1.19e+08 =  2% of the original kernel matrix.

torch.Size([7130, 2])
We keep 4.18e+05/9.34e+06 =  4% of the original kernel matrix.

torch.Size([16669, 2])
We keep 1.46e+06/5.89e+07 =  2% of the original kernel matrix.

torch.Size([10059, 2])
We keep 1.01e+06/2.26e+07 =  4% of the original kernel matrix.

torch.Size([19134, 2])
We keep 2.07e+06/9.16e+07 =  2% of the original kernel matrix.

torch.Size([6535, 2])
We keep 4.28e+05/8.15e+06 =  5% of the original kernel matrix.

torch.Size([15775, 2])
We keep 1.42e+06/5.50e+07 =  2% of the original kernel matrix.

torch.Size([9667, 2])
We keep 1.20e+06/2.40e+07 =  4% of the original kernel matrix.

torch.Size([18728, 2])
We keep 2.11e+06/9.44e+07 =  2% of the original kernel matrix.

torch.Size([6312, 2])
We keep 3.59e+05/7.46e+06 =  4% of the original kernel matrix.

torch.Size([15535, 2])
We keep 1.37e+06/5.26e+07 =  2% of the original kernel matrix.

torch.Size([6605, 2])
We keep 3.71e+05/7.82e+06 =  4% of the original kernel matrix.

torch.Size([16051, 2])
We keep 1.39e+06/5.39e+07 =  2% of the original kernel matrix.

torch.Size([5271, 2])
We keep 2.63e+05/4.73e+06 =  5% of the original kernel matrix.

torch.Size([14398, 2])
We keep 1.18e+06/4.19e+07 =  2% of the original kernel matrix.

torch.Size([7767, 2])
We keep 1.18e+06/1.86e+07 =  6% of the original kernel matrix.

torch.Size([16918, 2])
We keep 1.93e+06/8.31e+07 =  2% of the original kernel matrix.

torch.Size([6452, 2])
We keep 3.97e+05/7.68e+06 =  5% of the original kernel matrix.

torch.Size([15744, 2])
We keep 1.40e+06/5.34e+07 =  2% of the original kernel matrix.

torch.Size([13879, 2])
We keep 1.70e+06/5.08e+07 =  3% of the original kernel matrix.

torch.Size([22642, 2])
We keep 2.79e+06/1.37e+08 =  2% of the original kernel matrix.

torch.Size([9238, 2])
We keep 6.79e+05/1.71e+07 =  3% of the original kernel matrix.

torch.Size([18402, 2])
We keep 1.84e+06/7.98e+07 =  2% of the original kernel matrix.

torch.Size([4474, 2])
We keep 1.98e+05/3.65e+06 =  5% of the original kernel matrix.

torch.Size([13500, 2])
We keep 1.06e+06/3.68e+07 =  2% of the original kernel matrix.

torch.Size([7960, 2])
We keep 6.54e+05/1.42e+07 =  4% of the original kernel matrix.

torch.Size([17296, 2])
We keep 1.75e+06/7.27e+07 =  2% of the original kernel matrix.

torch.Size([12181, 2])
We keep 1.19e+06/3.53e+07 =  3% of the original kernel matrix.

torch.Size([21168, 2])
We keep 2.40e+06/1.14e+08 =  2% of the original kernel matrix.

torch.Size([29164, 2])
We keep 1.02e+07/3.09e+08 =  3% of the original kernel matrix.

torch.Size([33127, 2])
We keep 5.73e+06/3.39e+08 =  1% of the original kernel matrix.

torch.Size([4123, 2])
We keep 1.57e+05/2.81e+06 =  5% of the original kernel matrix.

torch.Size([13047, 2])
We keep 9.75e+05/3.23e+07 =  3% of the original kernel matrix.

torch.Size([2070, 2])
We keep 4.58e+04/5.91e+05 =  7% of the original kernel matrix.

torch.Size([10245, 2])
We keep 5.84e+05/1.48e+07 =  3% of the original kernel matrix.

torch.Size([7439, 2])
We keep 5.57e+05/1.05e+07 =  5% of the original kernel matrix.

torch.Size([16766, 2])
We keep 1.56e+06/6.25e+07 =  2% of the original kernel matrix.

torch.Size([4921, 2])
We keep 1.99e+05/3.66e+06 =  5% of the original kernel matrix.

torch.Size([14286, 2])
We keep 1.07e+06/3.68e+07 =  2% of the original kernel matrix.

torch.Size([5381, 2])
We keep 3.79e+05/5.74e+06 =  6% of the original kernel matrix.

torch.Size([14540, 2])
We keep 1.26e+06/4.62e+07 =  2% of the original kernel matrix.

torch.Size([5975, 2])
We keep 2.78e+05/5.68e+06 =  4% of the original kernel matrix.

torch.Size([15347, 2])
We keep 1.25e+06/4.59e+07 =  2% of the original kernel matrix.

torch.Size([6019, 2])
We keep 3.00e+05/6.02e+06 =  4% of the original kernel matrix.

torch.Size([15340, 2])
We keep 1.27e+06/4.73e+07 =  2% of the original kernel matrix.

torch.Size([3797, 2])
We keep 1.69e+05/2.35e+06 =  7% of the original kernel matrix.

torch.Size([12853, 2])
We keep 9.30e+05/2.95e+07 =  3% of the original kernel matrix.

torch.Size([4332, 2])
We keep 1.84e+05/3.12e+06 =  5% of the original kernel matrix.

torch.Size([13446, 2])
We keep 1.01e+06/3.40e+07 =  2% of the original kernel matrix.

torch.Size([4275, 2])
We keep 2.17e+05/3.22e+06 =  6% of the original kernel matrix.

torch.Size([13311, 2])
We keep 1.04e+06/3.46e+07 =  2% of the original kernel matrix.

torch.Size([10082, 2])
We keep 7.87e+05/2.12e+07 =  3% of the original kernel matrix.

torch.Size([19188, 2])
We keep 1.98e+06/8.87e+07 =  2% of the original kernel matrix.

torch.Size([4247, 2])
We keep 1.70e+05/2.94e+06 =  5% of the original kernel matrix.

torch.Size([13317, 2])
We keep 9.90e+05/3.30e+07 =  2% of the original kernel matrix.

torch.Size([17815, 2])
We keep 2.20e+06/8.73e+07 =  2% of the original kernel matrix.

torch.Size([25809, 2])
We keep 3.41e+06/1.80e+08 =  1% of the original kernel matrix.

torch.Size([8688, 2])
We keep 1.00e+06/1.78e+07 =  5% of the original kernel matrix.

torch.Size([17913, 2])
We keep 1.90e+06/8.13e+07 =  2% of the original kernel matrix.

torch.Size([7984, 2])
We keep 5.08e+05/1.23e+07 =  4% of the original kernel matrix.

torch.Size([17321, 2])
We keep 1.65e+06/6.75e+07 =  2% of the original kernel matrix.

torch.Size([2883, 2])
We keep 8.68e+04/1.28e+06 =  6% of the original kernel matrix.

torch.Size([11400, 2])
We keep 7.37e+05/2.18e+07 =  3% of the original kernel matrix.

torch.Size([9619, 2])
We keep 9.74e+05/2.13e+07 =  4% of the original kernel matrix.

torch.Size([18976, 2])
We keep 2.03e+06/8.88e+07 =  2% of the original kernel matrix.

torch.Size([7964, 2])
We keep 5.66e+05/1.24e+07 =  4% of the original kernel matrix.

torch.Size([17407, 2])
We keep 1.63e+06/6.78e+07 =  2% of the original kernel matrix.

torch.Size([6371, 2])
We keep 4.70e+05/8.13e+06 =  5% of the original kernel matrix.

torch.Size([15819, 2])
We keep 1.43e+06/5.50e+07 =  2% of the original kernel matrix.

torch.Size([12428, 2])
We keep 1.17e+06/3.48e+07 =  3% of the original kernel matrix.

torch.Size([21216, 2])
We keep 2.38e+06/1.14e+08 =  2% of the original kernel matrix.

torch.Size([10776, 2])
We keep 9.54e+05/2.61e+07 =  3% of the original kernel matrix.

torch.Size([19811, 2])
We keep 2.15e+06/9.84e+07 =  2% of the original kernel matrix.

torch.Size([6778, 2])
We keep 6.56e+05/1.05e+07 =  6% of the original kernel matrix.

torch.Size([15997, 2])
We keep 1.58e+06/6.26e+07 =  2% of the original kernel matrix.

torch.Size([7272, 2])
We keep 4.66e+05/9.92e+06 =  4% of the original kernel matrix.

torch.Size([16531, 2])
We keep 1.51e+06/6.07e+07 =  2% of the original kernel matrix.

torch.Size([5367, 2])
We keep 2.66e+05/5.41e+06 =  4% of the original kernel matrix.

torch.Size([14678, 2])
We keep 1.24e+06/4.48e+07 =  2% of the original kernel matrix.

torch.Size([3238, 2])
We keep 1.34e+05/1.79e+06 =  7% of the original kernel matrix.

torch.Size([11930, 2])
We keep 8.48e+05/2.58e+07 =  3% of the original kernel matrix.

torch.Size([2727, 2])
We keep 7.95e+04/1.11e+06 =  7% of the original kernel matrix.

torch.Size([11015, 2])
We keep 7.20e+05/2.03e+07 =  3% of the original kernel matrix.

torch.Size([2880, 2])
We keep 1.07e+05/1.43e+06 =  7% of the original kernel matrix.

torch.Size([11439, 2])
We keep 7.79e+05/2.30e+07 =  3% of the original kernel matrix.

torch.Size([2602, 2])
We keep 8.97e+04/1.13e+06 =  7% of the original kernel matrix.

torch.Size([10993, 2])
We keep 7.04e+05/2.05e+07 =  3% of the original kernel matrix.

torch.Size([7814, 2])
We keep 5.51e+05/1.28e+07 =  4% of the original kernel matrix.

torch.Size([17092, 2])
We keep 1.66e+06/6.89e+07 =  2% of the original kernel matrix.

torch.Size([6001, 2])
We keep 2.98e+05/6.05e+06 =  4% of the original kernel matrix.

torch.Size([15180, 2])
We keep 1.27e+06/4.74e+07 =  2% of the original kernel matrix.

torch.Size([6117, 2])
We keep 7.02e+05/1.08e+07 =  6% of the original kernel matrix.

torch.Size([15097, 2])
We keep 1.57e+06/6.33e+07 =  2% of the original kernel matrix.

torch.Size([3297, 2])
We keep 1.81e+05/2.16e+06 =  8% of the original kernel matrix.

torch.Size([12023, 2])
We keep 8.98e+05/2.83e+07 =  3% of the original kernel matrix.

torch.Size([9338, 2])
We keep 7.95e+05/1.85e+07 =  4% of the original kernel matrix.

torch.Size([18524, 2])
We keep 1.91e+06/8.29e+07 =  2% of the original kernel matrix.

torch.Size([5450, 2])
We keep 5.24e+05/7.15e+06 =  7% of the original kernel matrix.

torch.Size([14467, 2])
We keep 1.37e+06/5.15e+07 =  2% of the original kernel matrix.

torch.Size([8232, 2])
We keep 8.37e+05/1.53e+07 =  5% of the original kernel matrix.

torch.Size([17616, 2])
We keep 1.79e+06/7.55e+07 =  2% of the original kernel matrix.

torch.Size([8720, 2])
We keep 8.03e+05/1.80e+07 =  4% of the original kernel matrix.

torch.Size([17893, 2])
We keep 1.86e+06/8.18e+07 =  2% of the original kernel matrix.

torch.Size([4353, 2])
We keep 1.90e+05/3.16e+06 =  5% of the original kernel matrix.

torch.Size([13300, 2])
We keep 1.01e+06/3.43e+07 =  2% of the original kernel matrix.

torch.Size([2456, 2])
We keep 8.28e+04/1.10e+06 =  7% of the original kernel matrix.

torch.Size([10574, 2])
We keep 7.11e+05/2.03e+07 =  3% of the original kernel matrix.

torch.Size([12240, 2])
We keep 1.17e+06/3.42e+07 =  3% of the original kernel matrix.

torch.Size([21146, 2])
We keep 2.38e+06/1.13e+08 =  2% of the original kernel matrix.

torch.Size([1784, 2])
We keep 3.75e+04/4.13e+05 =  9% of the original kernel matrix.

torch.Size([9445, 2])
We keep 5.12e+05/1.24e+07 =  4% of the original kernel matrix.

torch.Size([4269, 2])
We keep 1.91e+05/3.29e+06 =  5% of the original kernel matrix.

torch.Size([13390, 2])
We keep 1.02e+06/3.49e+07 =  2% of the original kernel matrix.

torch.Size([4162, 2])
We keep 1.93e+05/3.07e+06 =  6% of the original kernel matrix.

torch.Size([13152, 2])
We keep 1.02e+06/3.38e+07 =  3% of the original kernel matrix.

torch.Size([6002, 2])
We keep 3.72e+05/7.13e+06 =  5% of the original kernel matrix.

torch.Size([15291, 2])
We keep 1.37e+06/5.15e+07 =  2% of the original kernel matrix.

torch.Size([11166, 2])
We keep 9.37e+05/2.78e+07 =  3% of the original kernel matrix.

torch.Size([20446, 2])
We keep 2.19e+06/1.02e+08 =  2% of the original kernel matrix.

torch.Size([5173, 2])
We keep 2.84e+05/4.97e+06 =  5% of the original kernel matrix.

torch.Size([14297, 2])
We keep 1.20e+06/4.30e+07 =  2% of the original kernel matrix.

torch.Size([4826, 2])
We keep 3.44e+05/4.73e+06 =  7% of the original kernel matrix.

torch.Size([13810, 2])
We keep 1.17e+06/4.19e+07 =  2% of the original kernel matrix.

torch.Size([6634, 2])
We keep 4.16e+05/8.72e+06 =  4% of the original kernel matrix.

torch.Size([16009, 2])
We keep 1.45e+06/5.69e+07 =  2% of the original kernel matrix.

torch.Size([14199, 2])
We keep 1.79e+06/4.92e+07 =  3% of the original kernel matrix.

torch.Size([22892, 2])
We keep 2.70e+06/1.35e+08 =  1% of the original kernel matrix.

torch.Size([3273, 2])
We keep 1.07e+05/1.63e+06 =  6% of the original kernel matrix.

torch.Size([12043, 2])
We keep 8.17e+05/2.46e+07 =  3% of the original kernel matrix.

torch.Size([13416, 2])
We keep 1.47e+06/4.87e+07 =  3% of the original kernel matrix.

torch.Size([22290, 2])
We keep 2.75e+06/1.35e+08 =  2% of the original kernel matrix.

torch.Size([39603, 2])
We keep 5.98e+07/1.24e+09 =  4% of the original kernel matrix.

torch.Size([37251, 2])
We keep 1.01e+07/6.79e+08 =  1% of the original kernel matrix.

torch.Size([18755, 2])
We keep 7.31e+06/1.39e+08 =  5% of the original kernel matrix.

torch.Size([26312, 2])
We keep 3.97e+06/2.27e+08 =  1% of the original kernel matrix.

torch.Size([10438, 2])
We keep 1.33e+06/2.78e+07 =  4% of the original kernel matrix.

torch.Size([19357, 2])
We keep 2.19e+06/1.02e+08 =  2% of the original kernel matrix.

torch.Size([1746, 2])
We keep 3.95e+04/4.42e+05 =  8% of the original kernel matrix.

torch.Size([9463, 2])
We keep 5.36e+05/1.28e+07 =  4% of the original kernel matrix.

torch.Size([13496, 2])
We keep 2.01e+06/4.50e+07 =  4% of the original kernel matrix.

torch.Size([22219, 2])
We keep 2.58e+06/1.29e+08 =  1% of the original kernel matrix.

torch.Size([55269, 2])
We keep 7.42e+07/1.33e+09 =  5% of the original kernel matrix.

torch.Size([44311, 2])
We keep 1.02e+07/7.02e+08 =  1% of the original kernel matrix.

torch.Size([243777, 2])
We keep 1.13e+09/4.35e+10 =  2% of the original kernel matrix.

torch.Size([93542, 2])
We keep 4.70e+07/4.02e+09 =  1% of the original kernel matrix.

torch.Size([8205, 2])
We keep 5.73e+05/1.44e+07 =  3% of the original kernel matrix.

torch.Size([17571, 2])
We keep 1.71e+06/7.30e+07 =  2% of the original kernel matrix.

torch.Size([108657, 2])
We keep 1.19e+08/4.83e+09 =  2% of the original kernel matrix.

torch.Size([62751, 2])
We keep 1.77e+07/1.34e+09 =  1% of the original kernel matrix.

torch.Size([38593, 2])
We keep 1.45e+07/6.10e+08 =  2% of the original kernel matrix.

torch.Size([37176, 2])
We keep 7.24e+06/4.76e+08 =  1% of the original kernel matrix.

torch.Size([59136, 2])
We keep 7.75e+07/1.88e+09 =  4% of the original kernel matrix.

torch.Size([45122, 2])
We keep 1.17e+07/8.35e+08 =  1% of the original kernel matrix.

torch.Size([360936, 2])
We keep 1.45e+09/5.90e+10 =  2% of the original kernel matrix.

torch.Size([117767, 2])
We keep 5.16e+07/4.68e+09 =  1% of the original kernel matrix.

torch.Size([4514, 2])
We keep 1.95e+05/3.43e+06 =  5% of the original kernel matrix.

torch.Size([13620, 2])
We keep 1.02e+06/3.57e+07 =  2% of the original kernel matrix.

torch.Size([7644, 2])
We keep 7.08e+05/1.36e+07 =  5% of the original kernel matrix.

torch.Size([16883, 2])
We keep 1.69e+06/7.11e+07 =  2% of the original kernel matrix.

torch.Size([272257, 2])
We keep 3.83e+08/2.82e+10 =  1% of the original kernel matrix.

torch.Size([102747, 2])
We keep 3.83e+07/3.23e+09 =  1% of the original kernel matrix.

torch.Size([823232, 2])
We keep 1.87e+09/2.11e+11 =  0% of the original kernel matrix.

torch.Size([184795, 2])
We keep 9.33e+07/8.84e+09 =  1% of the original kernel matrix.

torch.Size([9199, 2])
We keep 9.90e+05/2.00e+07 =  4% of the original kernel matrix.

torch.Size([18483, 2])
We keep 1.97e+06/8.63e+07 =  2% of the original kernel matrix.

torch.Size([52295, 2])
We keep 1.71e+07/9.42e+08 =  1% of the original kernel matrix.

torch.Size([43786, 2])
We keep 8.84e+06/5.92e+08 =  1% of the original kernel matrix.

torch.Size([203805, 2])
We keep 4.32e+09/3.48e+10 = 12% of the original kernel matrix.

torch.Size([84649, 2])
We keep 3.83e+07/3.60e+09 =  1% of the original kernel matrix.

torch.Size([108823, 2])
We keep 2.35e+08/6.18e+09 =  3% of the original kernel matrix.

torch.Size([62582, 2])
We keep 1.92e+07/1.51e+09 =  1% of the original kernel matrix.

torch.Size([67519, 2])
We keep 1.92e+08/2.54e+09 =  7% of the original kernel matrix.

torch.Size([48314, 2])
We keep 1.27e+07/9.70e+08 =  1% of the original kernel matrix.

torch.Size([35668, 2])
We keep 1.84e+07/6.49e+08 =  2% of the original kernel matrix.

torch.Size([35350, 2])
We keep 7.34e+06/4.91e+08 =  1% of the original kernel matrix.

torch.Size([56667, 2])
We keep 4.14e+07/1.44e+09 =  2% of the original kernel matrix.

torch.Size([45001, 2])
We keep 1.04e+07/7.32e+08 =  1% of the original kernel matrix.

torch.Size([19796, 2])
We keep 3.06e+06/1.20e+08 =  2% of the original kernel matrix.

torch.Size([27246, 2])
We keep 3.79e+06/2.11e+08 =  1% of the original kernel matrix.

torch.Size([6495, 2])
We keep 5.33e+05/8.72e+06 =  6% of the original kernel matrix.

torch.Size([15625, 2])
We keep 1.45e+06/5.69e+07 =  2% of the original kernel matrix.

torch.Size([256199, 2])
We keep 2.65e+09/1.06e+11 =  2% of the original kernel matrix.

torch.Size([85604, 2])
We keep 6.94e+07/6.26e+09 =  1% of the original kernel matrix.

torch.Size([12184, 2])
We keep 2.05e+06/3.90e+07 =  5% of the original kernel matrix.

torch.Size([21146, 2])
We keep 2.55e+06/1.20e+08 =  2% of the original kernel matrix.

torch.Size([8241, 2])
We keep 1.59e+06/2.14e+07 =  7% of the original kernel matrix.

torch.Size([17523, 2])
We keep 2.05e+06/8.91e+07 =  2% of the original kernel matrix.

torch.Size([60637, 2])
We keep 3.23e+07/1.33e+09 =  2% of the original kernel matrix.

torch.Size([46555, 2])
We keep 9.83e+06/7.03e+08 =  1% of the original kernel matrix.

torch.Size([41964, 2])
We keep 2.42e+07/8.35e+08 =  2% of the original kernel matrix.

torch.Size([38271, 2])
We keep 8.22e+06/5.57e+08 =  1% of the original kernel matrix.

torch.Size([387516, 2])
We keep 8.86e+08/5.61e+10 =  1% of the original kernel matrix.

torch.Size([122408, 2])
We keep 5.19e+07/4.57e+09 =  1% of the original kernel matrix.

torch.Size([157746, 2])
We keep 1.07e+08/8.43e+09 =  1% of the original kernel matrix.

torch.Size([76847, 2])
We keep 2.21e+07/1.77e+09 =  1% of the original kernel matrix.

torch.Size([2970, 2])
We keep 9.02e+04/1.33e+06 =  6% of the original kernel matrix.

torch.Size([11574, 2])
We keep 7.58e+05/2.23e+07 =  3% of the original kernel matrix.

torch.Size([50695, 2])
We keep 2.71e+07/9.06e+08 =  2% of the original kernel matrix.

torch.Size([43029, 2])
We keep 8.75e+06/5.80e+08 =  1% of the original kernel matrix.

torch.Size([15705, 2])
We keep 3.58e+06/7.75e+07 =  4% of the original kernel matrix.

torch.Size([23955, 2])
We keep 3.26e+06/1.70e+08 =  1% of the original kernel matrix.

torch.Size([30310, 2])
We keep 1.04e+07/4.01e+08 =  2% of the original kernel matrix.

torch.Size([33489, 2])
We keep 6.21e+06/3.86e+08 =  1% of the original kernel matrix.

torch.Size([115398, 2])
We keep 7.51e+07/4.88e+09 =  1% of the original kernel matrix.

torch.Size([65311, 2])
We keep 1.73e+07/1.35e+09 =  1% of the original kernel matrix.

torch.Size([9901, 2])
We keep 7.99e+05/2.06e+07 =  3% of the original kernel matrix.

torch.Size([19049, 2])
We keep 1.97e+06/8.75e+07 =  2% of the original kernel matrix.

torch.Size([68717, 2])
We keep 3.24e+07/1.66e+09 =  1% of the original kernel matrix.

torch.Size([49504, 2])
We keep 1.11e+07/7.85e+08 =  1% of the original kernel matrix.

torch.Size([2061, 2])
We keep 5.09e+04/6.51e+05 =  7% of the original kernel matrix.

torch.Size([10009, 2])
We keep 5.94e+05/1.56e+07 =  3% of the original kernel matrix.

torch.Size([10857, 2])
We keep 1.01e+06/2.57e+07 =  3% of the original kernel matrix.

torch.Size([20076, 2])
We keep 2.16e+06/9.76e+07 =  2% of the original kernel matrix.

torch.Size([76049, 2])
We keep 1.09e+08/3.66e+09 =  2% of the original kernel matrix.

torch.Size([51351, 2])
We keep 1.57e+07/1.17e+09 =  1% of the original kernel matrix.

torch.Size([52988, 2])
We keep 3.36e+07/1.17e+09 =  2% of the original kernel matrix.

torch.Size([43422, 2])
We keep 9.46e+06/6.59e+08 =  1% of the original kernel matrix.

torch.Size([39972, 2])
We keep 1.53e+07/6.52e+08 =  2% of the original kernel matrix.

torch.Size([37924, 2])
We keep 7.48e+06/4.92e+08 =  1% of the original kernel matrix.

torch.Size([39879, 2])
We keep 1.42e+08/1.44e+09 =  9% of the original kernel matrix.

torch.Size([36016, 2])
We keep 1.07e+07/7.31e+08 =  1% of the original kernel matrix.

torch.Size([406349, 2])
We keep 5.87e+08/6.42e+10 =  0% of the original kernel matrix.

torch.Size([126016, 2])
We keep 5.42e+07/4.88e+09 =  1% of the original kernel matrix.

torch.Size([22917, 2])
We keep 3.23e+06/1.51e+08 =  2% of the original kernel matrix.

torch.Size([29501, 2])
We keep 4.15e+06/2.37e+08 =  1% of the original kernel matrix.

torch.Size([33017, 2])
We keep 7.31e+06/3.87e+08 =  1% of the original kernel matrix.

torch.Size([35682, 2])
We keep 6.17e+06/3.79e+08 =  1% of the original kernel matrix.

torch.Size([106096, 2])
We keep 6.01e+07/4.27e+09 =  1% of the original kernel matrix.

torch.Size([62132, 2])
We keep 1.63e+07/1.26e+09 =  1% of the original kernel matrix.

torch.Size([10140, 2])
We keep 1.72e+06/2.65e+07 =  6% of the original kernel matrix.

torch.Size([19433, 2])
We keep 2.19e+06/9.92e+07 =  2% of the original kernel matrix.

torch.Size([165962, 2])
We keep 2.00e+08/9.96e+09 =  2% of the original kernel matrix.

torch.Size([78656, 2])
We keep 2.38e+07/1.92e+09 =  1% of the original kernel matrix.

torch.Size([289104, 2])
We keep 9.84e+08/4.71e+10 =  2% of the original kernel matrix.

torch.Size([103919, 2])
We keep 4.81e+07/4.18e+09 =  1% of the original kernel matrix.

torch.Size([439732, 2])
We keep 1.20e+09/7.44e+10 =  1% of the original kernel matrix.

torch.Size([129806, 2])
We keep 5.90e+07/5.26e+09 =  1% of the original kernel matrix.

torch.Size([25633, 2])
We keep 7.36e+06/2.54e+08 =  2% of the original kernel matrix.

torch.Size([31093, 2])
We keep 5.12e+06/3.07e+08 =  1% of the original kernel matrix.

torch.Size([32427, 2])
We keep 1.18e+07/4.43e+08 =  2% of the original kernel matrix.

torch.Size([34516, 2])
We keep 6.33e+06/4.05e+08 =  1% of the original kernel matrix.

torch.Size([17367, 2])
We keep 3.12e+06/1.02e+08 =  3% of the original kernel matrix.

torch.Size([25056, 2])
We keep 3.61e+06/1.94e+08 =  1% of the original kernel matrix.

torch.Size([11379, 2])
We keep 9.79e+05/2.79e+07 =  3% of the original kernel matrix.

torch.Size([20339, 2])
We keep 2.19e+06/1.02e+08 =  2% of the original kernel matrix.

torch.Size([35243, 2])
We keep 2.00e+07/5.78e+08 =  3% of the original kernel matrix.

torch.Size([36010, 2])
We keep 7.23e+06/4.63e+08 =  1% of the original kernel matrix.

torch.Size([18152, 2])
We keep 2.44e+06/8.83e+07 =  2% of the original kernel matrix.

torch.Size([26178, 2])
We keep 3.42e+06/1.81e+08 =  1% of the original kernel matrix.

torch.Size([27328, 2])
We keep 6.18e+06/2.64e+08 =  2% of the original kernel matrix.

torch.Size([31856, 2])
We keep 5.13e+06/3.13e+08 =  1% of the original kernel matrix.

torch.Size([110835, 2])
We keep 3.20e+08/7.63e+09 =  4% of the original kernel matrix.

torch.Size([64198, 2])
We keep 2.02e+07/1.68e+09 =  1% of the original kernel matrix.

torch.Size([68213, 2])
We keep 7.89e+07/2.10e+09 =  3% of the original kernel matrix.

torch.Size([49701, 2])
We keep 1.25e+07/8.84e+08 =  1% of the original kernel matrix.

torch.Size([10286, 2])
We keep 1.51e+06/3.63e+07 =  4% of the original kernel matrix.

torch.Size([19277, 2])
We keep 2.46e+06/1.16e+08 =  2% of the original kernel matrix.

torch.Size([17864, 2])
We keep 2.50e+06/8.88e+07 =  2% of the original kernel matrix.

torch.Size([25880, 2])
We keep 3.37e+06/1.82e+08 =  1% of the original kernel matrix.

torch.Size([62604, 2])
We keep 4.62e+08/9.97e+09 =  4% of the original kernel matrix.

torch.Size([43835, 2])
We keep 2.42e+07/1.92e+09 =  1% of the original kernel matrix.

torch.Size([22985, 2])
We keep 6.87e+06/2.00e+08 =  3% of the original kernel matrix.

torch.Size([29260, 2])
We keep 4.57e+06/2.72e+08 =  1% of the original kernel matrix.

torch.Size([8602, 2])
We keep 1.08e+06/1.77e+07 =  6% of the original kernel matrix.

torch.Size([17921, 2])
We keep 1.85e+06/8.12e+07 =  2% of the original kernel matrix.

torch.Size([18194, 2])
We keep 3.05e+06/9.43e+07 =  3% of the original kernel matrix.

torch.Size([26343, 2])
We keep 3.54e+06/1.87e+08 =  1% of the original kernel matrix.

torch.Size([670798, 2])
We keep 5.88e+09/2.43e+11 =  2% of the original kernel matrix.

torch.Size([160464, 2])
We keep 1.01e+08/9.51e+09 =  1% of the original kernel matrix.

torch.Size([20813, 2])
We keep 4.01e+06/1.39e+08 =  2% of the original kernel matrix.

torch.Size([27946, 2])
We keep 4.09e+06/2.27e+08 =  1% of the original kernel matrix.

torch.Size([127729, 2])
We keep 1.61e+09/1.85e+10 =  8% of the original kernel matrix.

torch.Size([65298, 2])
We keep 3.12e+07/2.62e+09 =  1% of the original kernel matrix.

torch.Size([11011, 2])
We keep 1.23e+06/2.98e+07 =  4% of the original kernel matrix.

torch.Size([20030, 2])
We keep 2.28e+06/1.05e+08 =  2% of the original kernel matrix.

torch.Size([54267, 2])
We keep 9.33e+07/2.88e+09 =  3% of the original kernel matrix.

torch.Size([41077, 2])
We keep 1.42e+07/1.03e+09 =  1% of the original kernel matrix.

torch.Size([31841, 2])
We keep 3.57e+07/4.55e+08 =  7% of the original kernel matrix.

torch.Size([34383, 2])
We keep 6.34e+06/4.11e+08 =  1% of the original kernel matrix.

torch.Size([308706, 2])
We keep 2.99e+09/9.58e+10 =  3% of the original kernel matrix.

torch.Size([98794, 2])
We keep 6.68e+07/5.96e+09 =  1% of the original kernel matrix.

torch.Size([65626, 2])
We keep 2.72e+07/1.88e+09 =  1% of the original kernel matrix.

torch.Size([48822, 2])
We keep 1.17e+07/8.36e+08 =  1% of the original kernel matrix.

torch.Size([16144, 2])
We keep 1.91e+06/6.81e+07 =  2% of the original kernel matrix.

torch.Size([24475, 2])
We keep 3.09e+06/1.59e+08 =  1% of the original kernel matrix.

torch.Size([213063, 2])
We keep 1.65e+08/1.56e+10 =  1% of the original kernel matrix.

torch.Size([90694, 2])
We keep 2.91e+07/2.41e+09 =  1% of the original kernel matrix.

torch.Size([160347, 2])
We keep 9.42e+07/8.56e+09 =  1% of the original kernel matrix.

torch.Size([77750, 2])
We keep 2.21e+07/1.78e+09 =  1% of the original kernel matrix.

torch.Size([19375, 2])
We keep 1.42e+07/1.85e+08 =  7% of the original kernel matrix.

torch.Size([26474, 2])
We keep 4.60e+06/2.62e+08 =  1% of the original kernel matrix.

torch.Size([97120, 2])
We keep 5.50e+07/3.12e+09 =  1% of the original kernel matrix.

torch.Size([58625, 2])
We keep 1.44e+07/1.08e+09 =  1% of the original kernel matrix.

torch.Size([160031, 2])
We keep 1.11e+08/1.01e+10 =  1% of the original kernel matrix.

torch.Size([77863, 2])
We keep 2.39e+07/1.94e+09 =  1% of the original kernel matrix.

torch.Size([514321, 2])
We keep 6.84e+08/8.23e+10 =  0% of the original kernel matrix.

torch.Size([141642, 2])
We keep 6.10e+07/5.53e+09 =  1% of the original kernel matrix.

torch.Size([44180, 2])
We keep 7.05e+07/9.73e+08 =  7% of the original kernel matrix.

torch.Size([39979, 2])
We keep 8.80e+06/6.01e+08 =  1% of the original kernel matrix.

torch.Size([5674, 2])
We keep 7.00e+05/7.61e+06 =  9% of the original kernel matrix.

torch.Size([14721, 2])
We keep 1.39e+06/5.32e+07 =  2% of the original kernel matrix.

torch.Size([23523, 2])
We keep 3.70e+06/1.77e+08 =  2% of the original kernel matrix.

torch.Size([30152, 2])
We keep 4.50e+06/2.56e+08 =  1% of the original kernel matrix.

torch.Size([92198, 2])
We keep 3.15e+08/7.76e+09 =  4% of the original kernel matrix.

torch.Size([56890, 2])
We keep 2.17e+07/1.70e+09 =  1% of the original kernel matrix.

torch.Size([168040, 2])
We keep 1.02e+09/1.52e+10 =  6% of the original kernel matrix.

torch.Size([79076, 2])
We keep 2.96e+07/2.37e+09 =  1% of the original kernel matrix.

torch.Size([22313, 2])
We keep 3.27e+06/1.45e+08 =  2% of the original kernel matrix.

torch.Size([29125, 2])
We keep 4.14e+06/2.32e+08 =  1% of the original kernel matrix.

torch.Size([4005, 2])
We keep 1.45e+05/2.44e+06 =  5% of the original kernel matrix.

torch.Size([12920, 2])
We keep 9.13e+05/3.01e+07 =  3% of the original kernel matrix.

torch.Size([59610, 2])
We keep 2.26e+07/1.21e+09 =  1% of the original kernel matrix.

torch.Size([46625, 2])
We keep 9.67e+06/6.71e+08 =  1% of the original kernel matrix.

torch.Size([168248, 2])
We keep 1.20e+08/1.00e+10 =  1% of the original kernel matrix.

torch.Size([79581, 2])
We keep 2.40e+07/1.93e+09 =  1% of the original kernel matrix.

torch.Size([92939, 2])
We keep 4.10e+08/8.79e+09 =  4% of the original kernel matrix.

torch.Size([54714, 2])
We keep 2.29e+07/1.81e+09 =  1% of the original kernel matrix.

torch.Size([10271, 2])
We keep 1.20e+06/2.38e+07 =  5% of the original kernel matrix.

torch.Size([19407, 2])
We keep 2.11e+06/9.40e+07 =  2% of the original kernel matrix.

torch.Size([33117, 2])
We keep 1.44e+07/5.09e+08 =  2% of the original kernel matrix.

torch.Size([34906, 2])
We keep 6.97e+06/4.35e+08 =  1% of the original kernel matrix.

torch.Size([23546, 2])
We keep 4.53e+06/1.86e+08 =  2% of the original kernel matrix.

torch.Size([30085, 2])
We keep 4.60e+06/2.63e+08 =  1% of the original kernel matrix.

torch.Size([9920, 2])
We keep 8.70e+05/2.17e+07 =  4% of the original kernel matrix.

torch.Size([19194, 2])
We keep 2.02e+06/8.99e+07 =  2% of the original kernel matrix.

torch.Size([19592, 2])
We keep 2.58e+06/1.07e+08 =  2% of the original kernel matrix.

torch.Size([27276, 2])
We keep 3.67e+06/1.99e+08 =  1% of the original kernel matrix.

torch.Size([16214, 2])
We keep 2.85e+06/6.85e+07 =  4% of the original kernel matrix.

torch.Size([24555, 2])
We keep 3.10e+06/1.60e+08 =  1% of the original kernel matrix.

torch.Size([96183, 2])
We keep 4.87e+08/5.03e+09 =  9% of the original kernel matrix.

torch.Size([58578, 2])
We keep 1.79e+07/1.37e+09 =  1% of the original kernel matrix.

torch.Size([73123, 2])
We keep 2.33e+07/1.65e+09 =  1% of the original kernel matrix.

torch.Size([51323, 2])
We keep 1.10e+07/7.83e+08 =  1% of the original kernel matrix.

torch.Size([30726, 2])
We keep 7.40e+06/3.06e+08 =  2% of the original kernel matrix.

torch.Size([34406, 2])
We keep 5.59e+06/3.37e+08 =  1% of the original kernel matrix.

torch.Size([26830, 2])
We keep 1.34e+07/2.79e+08 =  4% of the original kernel matrix.

torch.Size([31719, 2])
We keep 5.49e+06/3.22e+08 =  1% of the original kernel matrix.

torch.Size([15228, 2])
We keep 2.50e+06/6.36e+07 =  3% of the original kernel matrix.

torch.Size([23649, 2])
We keep 3.03e+06/1.54e+08 =  1% of the original kernel matrix.

torch.Size([692923, 2])
We keep 1.88e+09/1.78e+11 =  1% of the original kernel matrix.

torch.Size([166993, 2])
We keep 8.60e+07/8.13e+09 =  1% of the original kernel matrix.

torch.Size([12403, 2])
We keep 1.36e+06/4.15e+07 =  3% of the original kernel matrix.

torch.Size([21190, 2])
We keep 2.56e+06/1.24e+08 =  2% of the original kernel matrix.

torch.Size([197928, 2])
We keep 1.67e+08/1.43e+10 =  1% of the original kernel matrix.

torch.Size([86624, 2])
We keep 2.79e+07/2.30e+09 =  1% of the original kernel matrix.

torch.Size([4269, 2])
We keep 2.24e+05/3.69e+06 =  6% of the original kernel matrix.

torch.Size([13177, 2])
We keep 1.03e+06/3.70e+07 =  2% of the original kernel matrix.

torch.Size([8750, 2])
We keep 9.13e+05/1.73e+07 =  5% of the original kernel matrix.

torch.Size([18015, 2])
We keep 1.88e+06/8.02e+07 =  2% of the original kernel matrix.

torch.Size([6676, 2])
We keep 3.59e+05/8.34e+06 =  4% of the original kernel matrix.

torch.Size([15953, 2])
We keep 1.42e+06/5.57e+07 =  2% of the original kernel matrix.

torch.Size([37053, 2])
We keep 1.86e+07/4.77e+08 =  3% of the original kernel matrix.

torch.Size([37362, 2])
We keep 6.43e+06/4.21e+08 =  1% of the original kernel matrix.

torch.Size([8523, 2])
We keep 7.06e+05/1.53e+07 =  4% of the original kernel matrix.

torch.Size([17916, 2])
We keep 1.79e+06/7.54e+07 =  2% of the original kernel matrix.

torch.Size([55295, 2])
We keep 9.21e+07/1.77e+09 =  5% of the original kernel matrix.

torch.Size([43539, 2])
We keep 1.16e+07/8.11e+08 =  1% of the original kernel matrix.

torch.Size([10617, 2])
We keep 9.96e+05/2.71e+07 =  3% of the original kernel matrix.

torch.Size([19547, 2])
We keep 2.16e+06/1.00e+08 =  2% of the original kernel matrix.

torch.Size([9029, 2])
We keep 7.20e+05/1.82e+07 =  3% of the original kernel matrix.

torch.Size([18203, 2])
We keep 1.90e+06/8.23e+07 =  2% of the original kernel matrix.

torch.Size([407748, 2])
We keep 8.51e+08/5.87e+10 =  1% of the original kernel matrix.

torch.Size([124911, 2])
We keep 5.22e+07/4.67e+09 =  1% of the original kernel matrix.

torch.Size([41492, 2])
We keep 1.37e+07/6.23e+08 =  2% of the original kernel matrix.

torch.Size([39093, 2])
We keep 7.39e+06/4.81e+08 =  1% of the original kernel matrix.

torch.Size([29054, 2])
We keep 1.11e+07/3.21e+08 =  3% of the original kernel matrix.

torch.Size([33573, 2])
We keep 5.86e+06/3.45e+08 =  1% of the original kernel matrix.

torch.Size([144768, 2])
We keep 2.18e+08/8.18e+09 =  2% of the original kernel matrix.

torch.Size([73787, 2])
We keep 2.22e+07/1.74e+09 =  1% of the original kernel matrix.

torch.Size([136014, 2])
We keep 4.43e+08/1.81e+10 =  2% of the original kernel matrix.

torch.Size([67017, 2])
We keep 3.17e+07/2.59e+09 =  1% of the original kernel matrix.

torch.Size([7835, 2])
We keep 6.35e+05/1.40e+07 =  4% of the original kernel matrix.

torch.Size([17014, 2])
We keep 1.72e+06/7.22e+07 =  2% of the original kernel matrix.

torch.Size([14999, 2])
We keep 1.90e+06/6.01e+07 =  3% of the original kernel matrix.

torch.Size([23478, 2])
We keep 2.95e+06/1.49e+08 =  1% of the original kernel matrix.

torch.Size([25625, 2])
We keep 1.06e+07/2.50e+08 =  4% of the original kernel matrix.

torch.Size([30891, 2])
We keep 5.22e+06/3.04e+08 =  1% of the original kernel matrix.

torch.Size([2799, 2])
We keep 1.02e+05/1.31e+06 =  7% of the original kernel matrix.

torch.Size([11248, 2])
We keep 7.55e+05/2.20e+07 =  3% of the original kernel matrix.

torch.Size([10024, 2])
We keep 7.36e+05/1.96e+07 =  3% of the original kernel matrix.

torch.Size([19139, 2])
We keep 1.94e+06/8.54e+07 =  2% of the original kernel matrix.

torch.Size([24726, 2])
We keep 7.56e+06/2.51e+08 =  3% of the original kernel matrix.

torch.Size([30242, 2])
We keep 5.16e+06/3.05e+08 =  1% of the original kernel matrix.

torch.Size([5313, 2])
We keep 4.33e+05/5.88e+06 =  7% of the original kernel matrix.

torch.Size([14368, 2])
We keep 1.28e+06/4.67e+07 =  2% of the original kernel matrix.

torch.Size([9030, 2])
We keep 5.87e+05/1.53e+07 =  3% of the original kernel matrix.

torch.Size([18442, 2])
We keep 1.78e+06/7.53e+07 =  2% of the original kernel matrix.

torch.Size([16165, 2])
We keep 1.80e+06/6.79e+07 =  2% of the original kernel matrix.

torch.Size([24455, 2])
We keep 3.07e+06/1.59e+08 =  1% of the original kernel matrix.

torch.Size([10455, 2])
We keep 8.35e+05/2.29e+07 =  3% of the original kernel matrix.

torch.Size([19693, 2])
We keep 2.05e+06/9.23e+07 =  2% of the original kernel matrix.

torch.Size([506805, 2])
We keep 1.12e+09/9.07e+10 =  1% of the original kernel matrix.

torch.Size([140338, 2])
We keep 6.33e+07/5.80e+09 =  1% of the original kernel matrix.

torch.Size([12937, 2])
We keep 2.21e+06/5.61e+07 =  3% of the original kernel matrix.

torch.Size([21635, 2])
We keep 2.88e+06/1.44e+08 =  1% of the original kernel matrix.

torch.Size([80724, 2])
We keep 6.63e+07/2.59e+09 =  2% of the original kernel matrix.

torch.Size([53676, 2])
We keep 1.35e+07/9.80e+08 =  1% of the original kernel matrix.

torch.Size([12162, 2])
We keep 3.05e+06/6.38e+07 =  4% of the original kernel matrix.

torch.Size([21000, 2])
We keep 2.99e+06/1.54e+08 =  1% of the original kernel matrix.

torch.Size([30759, 2])
We keep 1.95e+07/4.71e+08 =  4% of the original kernel matrix.

torch.Size([32846, 2])
We keep 6.57e+06/4.18e+08 =  1% of the original kernel matrix.

torch.Size([41251, 2])
We keep 2.93e+07/8.32e+08 =  3% of the original kernel matrix.

torch.Size([38567, 2])
We keep 8.55e+06/5.56e+08 =  1% of the original kernel matrix.

torch.Size([1060204, 2])
We keep 8.44e+09/4.13e+11 =  2% of the original kernel matrix.

torch.Size([203797, 2])
We keep 1.25e+08/1.24e+10 =  1% of the original kernel matrix.

torch.Size([1187971, 2])
We keep 2.71e+09/3.71e+11 =  0% of the original kernel matrix.

torch.Size([220149, 2])
We keep 1.20e+08/1.17e+10 =  1% of the original kernel matrix.

torch.Size([61626, 2])
We keep 4.32e+07/1.49e+09 =  2% of the original kernel matrix.

torch.Size([47280, 2])
We keep 1.06e+07/7.44e+08 =  1% of the original kernel matrix.

torch.Size([103639, 2])
We keep 9.37e+07/3.84e+09 =  2% of the original kernel matrix.

torch.Size([61493, 2])
We keep 1.61e+07/1.19e+09 =  1% of the original kernel matrix.

torch.Size([5638, 2])
We keep 2.59e+05/5.45e+06 =  4% of the original kernel matrix.

torch.Size([15080, 2])
We keep 1.24e+06/4.50e+07 =  2% of the original kernel matrix.

torch.Size([60406, 2])
We keep 3.75e+08/4.06e+09 =  9% of the original kernel matrix.

torch.Size([44761, 2])
We keep 1.64e+07/1.23e+09 =  1% of the original kernel matrix.

torch.Size([26755, 2])
We keep 2.09e+07/4.27e+08 =  4% of the original kernel matrix.

torch.Size([30857, 2])
We keep 6.44e+06/3.98e+08 =  1% of the original kernel matrix.

torch.Size([139665, 2])
We keep 6.36e+08/1.07e+10 =  5% of the original kernel matrix.

torch.Size([72100, 2])
We keep 2.40e+07/1.99e+09 =  1% of the original kernel matrix.

torch.Size([14926, 2])
We keep 6.03e+06/6.52e+07 =  9% of the original kernel matrix.

torch.Size([23387, 2])
We keep 3.09e+06/1.56e+08 =  1% of the original kernel matrix.

torch.Size([98428, 2])
We keep 6.45e+07/3.76e+09 =  1% of the original kernel matrix.

torch.Size([59867, 2])
We keep 1.55e+07/1.18e+09 =  1% of the original kernel matrix.

torch.Size([496924, 2])
We keep 1.41e+09/9.02e+10 =  1% of the original kernel matrix.

torch.Size([138981, 2])
We keep 6.41e+07/5.79e+09 =  1% of the original kernel matrix.

torch.Size([73444, 2])
We keep 5.73e+07/2.39e+09 =  2% of the original kernel matrix.

torch.Size([50028, 2])
We keep 1.29e+07/9.42e+08 =  1% of the original kernel matrix.

torch.Size([10559, 2])
We keep 1.22e+06/3.10e+07 =  3% of the original kernel matrix.

torch.Size([19527, 2])
We keep 2.27e+06/1.07e+08 =  2% of the original kernel matrix.

torch.Size([49626, 2])
We keep 5.23e+07/8.89e+08 =  5% of the original kernel matrix.

torch.Size([43437, 2])
We keep 8.77e+06/5.75e+08 =  1% of the original kernel matrix.

torch.Size([16725, 2])
We keep 2.20e+06/8.20e+07 =  2% of the original kernel matrix.

torch.Size([24825, 2])
We keep 3.34e+06/1.74e+08 =  1% of the original kernel matrix.

torch.Size([7646, 2])
We keep 5.26e+05/1.13e+07 =  4% of the original kernel matrix.

torch.Size([17145, 2])
We keep 1.58e+06/6.47e+07 =  2% of the original kernel matrix.

torch.Size([196773, 2])
We keep 2.21e+08/1.37e+10 =  1% of the original kernel matrix.

torch.Size([86962, 2])
We keep 2.77e+07/2.25e+09 =  1% of the original kernel matrix.

torch.Size([29764, 2])
We keep 1.41e+07/3.92e+08 =  3% of the original kernel matrix.

torch.Size([32862, 2])
We keep 6.01e+06/3.81e+08 =  1% of the original kernel matrix.

torch.Size([4337, 2])
We keep 1.78e+05/3.00e+06 =  5% of the original kernel matrix.

torch.Size([13406, 2])
We keep 9.92e+05/3.34e+07 =  2% of the original kernel matrix.

torch.Size([158890, 2])
We keep 1.38e+08/8.68e+09 =  1% of the original kernel matrix.

torch.Size([77231, 2])
We keep 2.26e+07/1.80e+09 =  1% of the original kernel matrix.

torch.Size([20637, 2])
We keep 2.87e+06/1.22e+08 =  2% of the original kernel matrix.

torch.Size([28334, 2])
We keep 3.86e+06/2.13e+08 =  1% of the original kernel matrix.

torch.Size([2396, 2])
We keep 6.65e+04/9.14e+05 =  7% of the original kernel matrix.

torch.Size([10719, 2])
We keep 6.75e+05/1.84e+07 =  3% of the original kernel matrix.

torch.Size([148475, 2])
We keep 3.08e+08/1.22e+10 =  2% of the original kernel matrix.

torch.Size([73721, 2])
We keep 2.65e+07/2.13e+09 =  1% of the original kernel matrix.

torch.Size([125522, 2])
We keep 8.32e+07/5.71e+09 =  1% of the original kernel matrix.

torch.Size([68099, 2])
We keep 1.87e+07/1.46e+09 =  1% of the original kernel matrix.

torch.Size([43252, 2])
We keep 1.56e+07/6.68e+08 =  2% of the original kernel matrix.

torch.Size([40142, 2])
We keep 7.53e+06/4.98e+08 =  1% of the original kernel matrix.

torch.Size([145930, 2])
We keep 1.97e+08/7.86e+09 =  2% of the original kernel matrix.

torch.Size([73477, 2])
We keep 2.16e+07/1.71e+09 =  1% of the original kernel matrix.

torch.Size([136601, 2])
We keep 1.68e+08/7.75e+09 =  2% of the original kernel matrix.

torch.Size([70735, 2])
We keep 2.15e+07/1.70e+09 =  1% of the original kernel matrix.

torch.Size([14907, 2])
We keep 2.29e+06/7.08e+07 =  3% of the original kernel matrix.

torch.Size([23312, 2])
We keep 3.14e+06/1.62e+08 =  1% of the original kernel matrix.

torch.Size([61219, 2])
We keep 3.14e+07/1.47e+09 =  2% of the original kernel matrix.

torch.Size([46675, 2])
We keep 1.04e+07/7.38e+08 =  1% of the original kernel matrix.

torch.Size([213046, 2])
We keep 1.58e+08/1.64e+10 =  0% of the original kernel matrix.

torch.Size([91085, 2])
We keep 2.98e+07/2.47e+09 =  1% of the original kernel matrix.

torch.Size([194134, 2])
We keep 1.50e+08/1.26e+10 =  1% of the original kernel matrix.

torch.Size([86124, 2])
We keep 2.64e+07/2.16e+09 =  1% of the original kernel matrix.

torch.Size([19217, 2])
We keep 2.58e+06/1.06e+08 =  2% of the original kernel matrix.

torch.Size([26929, 2])
We keep 3.66e+06/1.98e+08 =  1% of the original kernel matrix.

torch.Size([152245, 2])
We keep 6.78e+08/1.40e+10 =  4% of the original kernel matrix.

torch.Size([76235, 2])
We keep 2.62e+07/2.28e+09 =  1% of the original kernel matrix.

torch.Size([294033, 2])
We keep 7.35e+08/3.54e+10 =  2% of the original kernel matrix.

torch.Size([106520, 2])
We keep 4.05e+07/3.62e+09 =  1% of the original kernel matrix.

torch.Size([112657, 2])
We keep 9.67e+07/4.74e+09 =  2% of the original kernel matrix.

torch.Size([64351, 2])
We keep 1.73e+07/1.33e+09 =  1% of the original kernel matrix.

torch.Size([43858, 2])
We keep 3.37e+07/1.03e+09 =  3% of the original kernel matrix.

torch.Size([39758, 2])
We keep 9.08e+06/6.18e+08 =  1% of the original kernel matrix.

torch.Size([20679, 2])
We keep 4.50e+07/4.17e+08 = 10% of the original kernel matrix.

torch.Size([26460, 2])
We keep 6.32e+06/3.93e+08 =  1% of the original kernel matrix.

torch.Size([14675, 2])
We keep 2.85e+06/7.26e+07 =  3% of the original kernel matrix.

torch.Size([23304, 2])
We keep 3.15e+06/1.64e+08 =  1% of the original kernel matrix.

torch.Size([14566, 2])
We keep 1.46e+06/5.16e+07 =  2% of the original kernel matrix.

torch.Size([23112, 2])
We keep 2.80e+06/1.38e+08 =  2% of the original kernel matrix.

torch.Size([8573, 2])
We keep 1.67e+06/2.37e+07 =  7% of the original kernel matrix.

torch.Size([17670, 2])
We keep 2.02e+06/9.37e+07 =  2% of the original kernel matrix.

torch.Size([262784, 2])
We keep 2.36e+08/2.43e+10 =  0% of the original kernel matrix.

torch.Size([102328, 2])
We keep 3.53e+07/3.01e+09 =  1% of the original kernel matrix.

torch.Size([208225, 2])
We keep 1.62e+08/1.50e+10 =  1% of the original kernel matrix.

torch.Size([89684, 2])
We keep 2.83e+07/2.36e+09 =  1% of the original kernel matrix.

torch.Size([461023, 2])
We keep 9.48e+08/7.60e+10 =  1% of the original kernel matrix.

torch.Size([133190, 2])
We keep 5.88e+07/5.31e+09 =  1% of the original kernel matrix.

torch.Size([205197, 2])
We keep 5.57e+08/2.30e+10 =  2% of the original kernel matrix.

torch.Size([88963, 2])
We keep 3.59e+07/2.92e+09 =  1% of the original kernel matrix.

torch.Size([101767, 2])
We keep 5.79e+07/3.73e+09 =  1% of the original kernel matrix.

torch.Size([60417, 2])
We keep 1.55e+07/1.18e+09 =  1% of the original kernel matrix.

torch.Size([22486, 2])
We keep 4.80e+06/2.22e+08 =  2% of the original kernel matrix.

torch.Size([28798, 2])
We keep 4.87e+06/2.87e+08 =  1% of the original kernel matrix.

torch.Size([121775, 2])
We keep 6.20e+07/4.89e+09 =  1% of the original kernel matrix.

torch.Size([67021, 2])
We keep 1.73e+07/1.35e+09 =  1% of the original kernel matrix.

torch.Size([4863, 2])
We keep 3.11e+05/5.04e+06 =  6% of the original kernel matrix.

torch.Size([13829, 2])
We keep 1.20e+06/4.33e+07 =  2% of the original kernel matrix.

torch.Size([19507, 2])
We keep 4.01e+06/1.30e+08 =  3% of the original kernel matrix.

torch.Size([26686, 2])
We keep 3.93e+06/2.20e+08 =  1% of the original kernel matrix.

torch.Size([261942, 2])
We keep 3.00e+08/2.40e+10 =  1% of the original kernel matrix.

torch.Size([100907, 2])
We keep 3.52e+07/2.98e+09 =  1% of the original kernel matrix.

torch.Size([35424, 2])
We keep 8.86e+06/4.46e+08 =  1% of the original kernel matrix.

torch.Size([36517, 2])
We keep 6.50e+06/4.07e+08 =  1% of the original kernel matrix.

torch.Size([7097, 2])
We keep 4.94e+05/9.93e+06 =  4% of the original kernel matrix.

torch.Size([16260, 2])
We keep 1.52e+06/6.07e+07 =  2% of the original kernel matrix.

torch.Size([19670, 2])
We keep 3.94e+06/1.32e+08 =  2% of the original kernel matrix.

torch.Size([26975, 2])
We keep 3.98e+06/2.21e+08 =  1% of the original kernel matrix.

torch.Size([65861, 2])
We keep 6.03e+07/2.09e+09 =  2% of the original kernel matrix.

torch.Size([48350, 2])
We keep 1.24e+07/8.81e+08 =  1% of the original kernel matrix.

torch.Size([43711, 2])
We keep 1.49e+07/7.05e+08 =  2% of the original kernel matrix.

torch.Size([39957, 2])
We keep 7.86e+06/5.12e+08 =  1% of the original kernel matrix.

torch.Size([119402, 2])
We keep 1.17e+08/5.21e+09 =  2% of the original kernel matrix.

torch.Size([66436, 2])
We keep 1.82e+07/1.39e+09 =  1% of the original kernel matrix.

torch.Size([30029, 2])
We keep 7.74e+07/1.12e+09 =  6% of the original kernel matrix.

torch.Size([29329, 2])
We keep 9.26e+06/6.45e+08 =  1% of the original kernel matrix.

torch.Size([31558, 2])
We keep 2.83e+07/5.95e+08 =  4% of the original kernel matrix.

torch.Size([33263, 2])
We keep 7.12e+06/4.70e+08 =  1% of the original kernel matrix.

torch.Size([8446, 2])
We keep 7.66e+05/1.60e+07 =  4% of the original kernel matrix.

torch.Size([17902, 2])
We keep 1.81e+06/7.71e+07 =  2% of the original kernel matrix.

torch.Size([7166, 2])
We keep 4.51e+05/9.76e+06 =  4% of the original kernel matrix.

torch.Size([16674, 2])
We keep 1.52e+06/6.02e+07 =  2% of the original kernel matrix.

torch.Size([82343, 2])
We keep 4.15e+07/2.20e+09 =  1% of the original kernel matrix.

torch.Size([54131, 2])
We keep 1.26e+07/9.04e+08 =  1% of the original kernel matrix.

torch.Size([75756, 2])
We keep 5.97e+07/2.19e+09 =  2% of the original kernel matrix.

torch.Size([51540, 2])
We keep 1.27e+07/9.02e+08 =  1% of the original kernel matrix.

torch.Size([15424, 2])
We keep 9.90e+06/1.10e+08 =  8% of the original kernel matrix.

torch.Size([23661, 2])
We keep 3.71e+06/2.02e+08 =  1% of the original kernel matrix.

torch.Size([165087, 2])
We keep 1.99e+08/1.27e+10 =  1% of the original kernel matrix.

torch.Size([78509, 2])
We keep 2.63e+07/2.17e+09 =  1% of the original kernel matrix.

torch.Size([10257, 2])
We keep 1.74e+06/2.91e+07 =  5% of the original kernel matrix.

torch.Size([19486, 2])
We keep 2.29e+06/1.04e+08 =  2% of the original kernel matrix.

torch.Size([29250, 2])
We keep 4.98e+06/2.65e+08 =  1% of the original kernel matrix.

torch.Size([33658, 2])
We keep 5.22e+06/3.14e+08 =  1% of the original kernel matrix.

torch.Size([7590, 2])
We keep 1.03e+06/1.77e+07 =  5% of the original kernel matrix.

torch.Size([16900, 2])
We keep 1.84e+06/8.10e+07 =  2% of the original kernel matrix.

torch.Size([207494, 2])
We keep 1.50e+08/1.53e+10 =  0% of the original kernel matrix.

torch.Size([89421, 2])
We keep 2.88e+07/2.38e+09 =  1% of the original kernel matrix.

torch.Size([10932, 2])
We keep 2.70e+06/3.54e+07 =  7% of the original kernel matrix.

torch.Size([19887, 2])
We keep 2.47e+06/1.15e+08 =  2% of the original kernel matrix.

torch.Size([58132, 2])
We keep 5.84e+07/1.43e+09 =  4% of the original kernel matrix.

torch.Size([45397, 2])
We keep 1.07e+07/7.30e+08 =  1% of the original kernel matrix.

torch.Size([21057, 2])
We keep 3.61e+06/1.35e+08 =  2% of the original kernel matrix.

torch.Size([28263, 2])
We keep 4.01e+06/2.24e+08 =  1% of the original kernel matrix.

torch.Size([26928, 2])
We keep 7.45e+06/2.61e+08 =  2% of the original kernel matrix.

torch.Size([32019, 2])
We keep 5.21e+06/3.11e+08 =  1% of the original kernel matrix.

torch.Size([77165, 2])
We keep 3.41e+07/2.06e+09 =  1% of the original kernel matrix.

torch.Size([52901, 2])
We keep 1.22e+07/8.75e+08 =  1% of the original kernel matrix.

torch.Size([424105, 2])
We keep 1.24e+09/9.16e+10 =  1% of the original kernel matrix.

torch.Size([125971, 2])
We keep 6.35e+07/5.83e+09 =  1% of the original kernel matrix.

torch.Size([22623, 2])
We keep 3.59e+06/1.53e+08 =  2% of the original kernel matrix.

torch.Size([29537, 2])
We keep 4.23e+06/2.38e+08 =  1% of the original kernel matrix.

torch.Size([12849, 2])
We keep 3.91e+06/6.51e+07 =  6% of the original kernel matrix.

torch.Size([21484, 2])
We keep 3.08e+06/1.56e+08 =  1% of the original kernel matrix.

torch.Size([160082, 2])
We keep 1.77e+08/9.73e+09 =  1% of the original kernel matrix.

torch.Size([77450, 2])
We keep 2.37e+07/1.90e+09 =  1% of the original kernel matrix.

torch.Size([18797, 2])
We keep 3.29e+06/1.15e+08 =  2% of the original kernel matrix.

torch.Size([26406, 2])
We keep 3.86e+06/2.06e+08 =  1% of the original kernel matrix.

torch.Size([798783, 2])
We keep 1.71e+09/1.93e+11 =  0% of the original kernel matrix.

torch.Size([181542, 2])
We keep 9.04e+07/8.46e+09 =  1% of the original kernel matrix.

torch.Size([8591, 2])
We keep 7.22e+05/1.71e+07 =  4% of the original kernel matrix.

torch.Size([17668, 2])
We keep 1.83e+06/7.97e+07 =  2% of the original kernel matrix.

torch.Size([152360, 2])
We keep 1.15e+08/8.80e+09 =  1% of the original kernel matrix.

torch.Size([75282, 2])
We keep 2.26e+07/1.81e+09 =  1% of the original kernel matrix.

torch.Size([574243, 2])
We keep 7.41e+08/9.65e+10 =  0% of the original kernel matrix.

torch.Size([149999, 2])
We keep 6.54e+07/5.99e+09 =  1% of the original kernel matrix.

torch.Size([6988, 2])
We keep 4.22e+05/8.91e+06 =  4% of the original kernel matrix.

torch.Size([16247, 2])
We keep 1.46e+06/5.75e+07 =  2% of the original kernel matrix.

torch.Size([219503, 2])
We keep 5.79e+08/3.63e+10 =  1% of the original kernel matrix.

torch.Size([86237, 2])
We keep 4.20e+07/3.67e+09 =  1% of the original kernel matrix.

torch.Size([25051, 2])
We keep 2.75e+07/6.34e+08 =  4% of the original kernel matrix.

torch.Size([28409, 2])
We keep 7.55e+06/4.85e+08 =  1% of the original kernel matrix.

torch.Size([556658, 2])
We keep 2.69e+09/1.11e+11 =  2% of the original kernel matrix.

torch.Size([147614, 2])
We keep 7.06e+07/6.42e+09 =  1% of the original kernel matrix.

torch.Size([15770, 2])
We keep 2.01e+06/7.15e+07 =  2% of the original kernel matrix.

torch.Size([24166, 2])
We keep 3.15e+06/1.63e+08 =  1% of the original kernel matrix.

torch.Size([65958, 2])
We keep 3.88e+07/1.73e+09 =  2% of the original kernel matrix.

torch.Size([48270, 2])
We keep 1.12e+07/8.01e+08 =  1% of the original kernel matrix.

torch.Size([4038, 2])
We keep 2.32e+05/3.51e+06 =  6% of the original kernel matrix.

torch.Size([12952, 2])
We keep 1.06e+06/3.61e+07 =  2% of the original kernel matrix.

torch.Size([11378, 2])
We keep 1.30e+06/3.23e+07 =  4% of the original kernel matrix.

torch.Size([20414, 2])
We keep 2.36e+06/1.10e+08 =  2% of the original kernel matrix.

torch.Size([9770, 2])
We keep 1.20e+06/2.53e+07 =  4% of the original kernel matrix.

torch.Size([18946, 2])
We keep 2.15e+06/9.69e+07 =  2% of the original kernel matrix.

torch.Size([88923, 2])
We keep 7.39e+07/3.22e+09 =  2% of the original kernel matrix.

torch.Size([56964, 2])
We keep 1.46e+07/1.09e+09 =  1% of the original kernel matrix.

torch.Size([35392, 2])
We keep 1.36e+07/5.24e+08 =  2% of the original kernel matrix.

torch.Size([35936, 2])
We keep 6.95e+06/4.41e+08 =  1% of the original kernel matrix.

torch.Size([148419, 2])
We keep 1.70e+08/8.38e+09 =  2% of the original kernel matrix.

torch.Size([74579, 2])
We keep 2.23e+07/1.76e+09 =  1% of the original kernel matrix.

torch.Size([25597, 2])
We keep 4.89e+06/2.24e+08 =  2% of the original kernel matrix.

torch.Size([31343, 2])
We keep 4.96e+06/2.89e+08 =  1% of the original kernel matrix.

torch.Size([8524, 2])
We keep 7.42e+05/1.78e+07 =  4% of the original kernel matrix.

torch.Size([17567, 2])
We keep 1.88e+06/8.13e+07 =  2% of the original kernel matrix.

torch.Size([7221, 2])
We keep 5.22e+05/9.56e+06 =  5% of the original kernel matrix.

torch.Size([16655, 2])
We keep 1.50e+06/5.96e+07 =  2% of the original kernel matrix.

torch.Size([157290, 2])
We keep 3.74e+08/1.50e+10 =  2% of the original kernel matrix.

torch.Size([76581, 2])
We keep 2.91e+07/2.36e+09 =  1% of the original kernel matrix.

torch.Size([35130, 2])
We keep 5.25e+07/7.95e+08 =  6% of the original kernel matrix.

torch.Size([35280, 2])
We keep 8.22e+06/5.44e+08 =  1% of the original kernel matrix.

torch.Size([535511, 2])
We keep 1.02e+09/8.52e+10 =  1% of the original kernel matrix.

torch.Size([144776, 2])
We keep 6.25e+07/5.63e+09 =  1% of the original kernel matrix.

torch.Size([62060, 2])
We keep 5.01e+07/1.43e+09 =  3% of the original kernel matrix.

torch.Size([47388, 2])
We keep 1.03e+07/7.28e+08 =  1% of the original kernel matrix.

torch.Size([13914, 2])
We keep 2.96e+06/6.34e+07 =  4% of the original kernel matrix.

torch.Size([22712, 2])
We keep 3.02e+06/1.53e+08 =  1% of the original kernel matrix.

torch.Size([11646, 2])
We keep 2.60e+06/3.81e+07 =  6% of the original kernel matrix.

torch.Size([20668, 2])
We keep 2.40e+06/1.19e+08 =  2% of the original kernel matrix.

torch.Size([14999, 2])
We keep 6.85e+06/9.51e+07 =  7% of the original kernel matrix.

torch.Size([23239, 2])
We keep 3.53e+06/1.88e+08 =  1% of the original kernel matrix.

torch.Size([1794627, 2])
We keep 7.88e+09/8.87e+11 =  0% of the original kernel matrix.

torch.Size([278601, 2])
We keep 1.80e+08/1.81e+10 =  0% of the original kernel matrix.

torch.Size([19196, 2])
We keep 3.54e+06/1.12e+08 =  3% of the original kernel matrix.

torch.Size([26823, 2])
We keep 3.77e+06/2.04e+08 =  1% of the original kernel matrix.

torch.Size([29026, 2])
We keep 7.95e+06/3.00e+08 =  2% of the original kernel matrix.

torch.Size([33051, 2])
We keep 5.59e+06/3.34e+08 =  1% of the original kernel matrix.

torch.Size([151290, 2])
We keep 2.16e+08/1.06e+10 =  2% of the original kernel matrix.

torch.Size([74561, 2])
We keep 2.47e+07/1.99e+09 =  1% of the original kernel matrix.

torch.Size([58492, 2])
We keep 4.34e+07/1.67e+09 =  2% of the original kernel matrix.

torch.Size([44875, 2])
We keep 1.13e+07/7.88e+08 =  1% of the original kernel matrix.

torch.Size([9011, 2])
We keep 7.98e+05/1.70e+07 =  4% of the original kernel matrix.

torch.Size([18381, 2])
We keep 1.81e+06/7.95e+07 =  2% of the original kernel matrix.

torch.Size([5567, 2])
We keep 3.01e+05/5.85e+06 =  5% of the original kernel matrix.

torch.Size([14656, 2])
We keep 1.25e+06/4.66e+07 =  2% of the original kernel matrix.

torch.Size([138691, 2])
We keep 5.04e+08/1.40e+10 =  3% of the original kernel matrix.

torch.Size([70610, 2])
We keep 2.82e+07/2.28e+09 =  1% of the original kernel matrix.

torch.Size([48616, 2])
We keep 1.83e+07/8.92e+08 =  2% of the original kernel matrix.

torch.Size([42232, 2])
We keep 8.67e+06/5.76e+08 =  1% of the original kernel matrix.

torch.Size([55826, 2])
We keep 1.38e+08/2.83e+09 =  4% of the original kernel matrix.

torch.Size([43380, 2])
We keep 1.42e+07/1.03e+09 =  1% of the original kernel matrix.

torch.Size([108118, 2])
We keep 7.81e+07/3.91e+09 =  1% of the original kernel matrix.

torch.Size([62707, 2])
We keep 1.61e+07/1.20e+09 =  1% of the original kernel matrix.

torch.Size([106593, 2])
We keep 1.02e+08/3.97e+09 =  2% of the original kernel matrix.

torch.Size([63198, 2])
We keep 1.62e+07/1.21e+09 =  1% of the original kernel matrix.

torch.Size([131137, 2])
We keep 2.12e+08/8.30e+09 =  2% of the original kernel matrix.

torch.Size([69556, 2])
We keep 2.23e+07/1.76e+09 =  1% of the original kernel matrix.

torch.Size([426761, 2])
We keep 8.69e+08/6.60e+10 =  1% of the original kernel matrix.

torch.Size([127509, 2])
We keep 5.55e+07/4.95e+09 =  1% of the original kernel matrix.

torch.Size([199414, 2])
We keep 2.96e+08/1.45e+10 =  2% of the original kernel matrix.

torch.Size([87646, 2])
We keep 2.80e+07/2.32e+09 =  1% of the original kernel matrix.

torch.Size([21156, 2])
We keep 7.31e+06/1.95e+08 =  3% of the original kernel matrix.

torch.Size([27882, 2])
We keep 4.76e+06/2.69e+08 =  1% of the original kernel matrix.

torch.Size([135827, 2])
We keep 2.00e+08/7.34e+09 =  2% of the original kernel matrix.

torch.Size([71156, 2])
We keep 2.09e+07/1.65e+09 =  1% of the original kernel matrix.

torch.Size([20920, 2])
We keep 9.07e+06/1.71e+08 =  5% of the original kernel matrix.

torch.Size([27851, 2])
We keep 4.49e+06/2.52e+08 =  1% of the original kernel matrix.

torch.Size([18033, 2])
We keep 3.05e+06/8.99e+07 =  3% of the original kernel matrix.

torch.Size([25964, 2])
We keep 3.40e+06/1.83e+08 =  1% of the original kernel matrix.

torch.Size([22149, 2])
We keep 3.98e+06/1.68e+08 =  2% of the original kernel matrix.

torch.Size([28547, 2])
We keep 4.36e+06/2.50e+08 =  1% of the original kernel matrix.

torch.Size([16985, 2])
We keep 2.21e+06/7.58e+07 =  2% of the original kernel matrix.

torch.Size([25181, 2])
We keep 3.23e+06/1.68e+08 =  1% of the original kernel matrix.

torch.Size([91476, 2])
We keep 6.49e+08/8.79e+09 =  7% of the original kernel matrix.

torch.Size([56094, 2])
We keep 2.23e+07/1.81e+09 =  1% of the original kernel matrix.

torch.Size([27283, 2])
We keep 6.40e+06/2.81e+08 =  2% of the original kernel matrix.

torch.Size([31954, 2])
We keep 5.38e+06/3.23e+08 =  1% of the original kernel matrix.

torch.Size([7164, 2])
We keep 1.27e+06/1.44e+07 =  8% of the original kernel matrix.

torch.Size([16033, 2])
We keep 1.75e+06/7.32e+07 =  2% of the original kernel matrix.

torch.Size([28132, 2])
We keep 6.33e+06/2.65e+08 =  2% of the original kernel matrix.

torch.Size([32594, 2])
We keep 5.26e+06/3.14e+08 =  1% of the original kernel matrix.

torch.Size([12614, 2])
We keep 7.62e+06/1.35e+08 =  5% of the original kernel matrix.

torch.Size([20840, 2])
We keep 3.76e+06/2.24e+08 =  1% of the original kernel matrix.

torch.Size([59910, 2])
We keep 1.45e+08/2.71e+09 =  5% of the original kernel matrix.

torch.Size([45459, 2])
We keep 1.40e+07/1.00e+09 =  1% of the original kernel matrix.

torch.Size([11274, 2])
We keep 2.29e+06/4.15e+07 =  5% of the original kernel matrix.

torch.Size([20146, 2])
We keep 2.59e+06/1.24e+08 =  2% of the original kernel matrix.

torch.Size([26988, 2])
We keep 5.33e+06/2.34e+08 =  2% of the original kernel matrix.

torch.Size([32134, 2])
We keep 4.97e+06/2.95e+08 =  1% of the original kernel matrix.

torch.Size([16272, 2])
We keep 6.35e+06/1.60e+08 =  3% of the original kernel matrix.

torch.Size([23698, 2])
We keep 4.23e+06/2.44e+08 =  1% of the original kernel matrix.

torch.Size([48673, 2])
We keep 3.81e+07/1.11e+09 =  3% of the original kernel matrix.

torch.Size([41809, 2])
We keep 9.59e+06/6.42e+08 =  1% of the original kernel matrix.

torch.Size([8738, 2])
We keep 5.85e+05/1.48e+07 =  3% of the original kernel matrix.

torch.Size([17972, 2])
We keep 1.74e+06/7.42e+07 =  2% of the original kernel matrix.

torch.Size([12262, 2])
We keep 1.16e+06/3.48e+07 =  3% of the original kernel matrix.

torch.Size([20993, 2])
We keep 2.39e+06/1.14e+08 =  2% of the original kernel matrix.

torch.Size([40686, 2])
We keep 9.95e+06/5.57e+08 =  1% of the original kernel matrix.

torch.Size([39257, 2])
We keep 7.02e+06/4.55e+08 =  1% of the original kernel matrix.

torch.Size([258134, 2])
We keep 3.48e+08/2.62e+10 =  1% of the original kernel matrix.

torch.Size([99405, 2])
We keep 3.69e+07/3.12e+09 =  1% of the original kernel matrix.

torch.Size([13637, 2])
We keep 1.82e+06/4.63e+07 =  3% of the original kernel matrix.

torch.Size([22527, 2])
We keep 2.71e+06/1.31e+08 =  2% of the original kernel matrix.

torch.Size([4025, 2])
We keep 1.49e+05/2.54e+06 =  5% of the original kernel matrix.

torch.Size([13080, 2])
We keep 9.40e+05/3.07e+07 =  3% of the original kernel matrix.

torch.Size([8321, 2])
We keep 7.60e+05/1.59e+07 =  4% of the original kernel matrix.

torch.Size([17568, 2])
We keep 1.81e+06/7.69e+07 =  2% of the original kernel matrix.

torch.Size([358666, 2])
We keep 8.16e+08/4.72e+10 =  1% of the original kernel matrix.

torch.Size([117291, 2])
We keep 4.85e+07/4.19e+09 =  1% of the original kernel matrix.

torch.Size([376344, 2])
We keep 6.84e+08/4.87e+10 =  1% of the original kernel matrix.

torch.Size([120974, 2])
We keep 4.80e+07/4.25e+09 =  1% of the original kernel matrix.

torch.Size([59604, 2])
We keep 3.79e+07/1.69e+09 =  2% of the original kernel matrix.

torch.Size([45184, 2])
We keep 1.11e+07/7.93e+08 =  1% of the original kernel matrix.

torch.Size([11990, 2])
We keep 1.38e+06/3.57e+07 =  3% of the original kernel matrix.

torch.Size([21099, 2])
We keep 2.44e+06/1.15e+08 =  2% of the original kernel matrix.

torch.Size([158076, 2])
We keep 2.82e+08/1.25e+10 =  2% of the original kernel matrix.

torch.Size([76870, 2])
We keep 2.68e+07/2.16e+09 =  1% of the original kernel matrix.

torch.Size([227344, 2])
We keep 1.85e+08/1.88e+10 =  0% of the original kernel matrix.

torch.Size([94879, 2])
We keep 3.17e+07/2.65e+09 =  1% of the original kernel matrix.

torch.Size([6692, 2])
We keep 4.89e+05/9.21e+06 =  5% of the original kernel matrix.

torch.Size([15953, 2])
We keep 1.45e+06/5.85e+07 =  2% of the original kernel matrix.

torch.Size([17536, 2])
We keep 5.88e+06/1.06e+08 =  5% of the original kernel matrix.

torch.Size([25478, 2])
We keep 3.69e+06/1.98e+08 =  1% of the original kernel matrix.

torch.Size([58315, 2])
We keep 1.64e+07/1.07e+09 =  1% of the original kernel matrix.

torch.Size([46404, 2])
We keep 9.24e+06/6.30e+08 =  1% of the original kernel matrix.

torch.Size([16388, 2])
We keep 5.17e+06/1.05e+08 =  4% of the original kernel matrix.

torch.Size([24587, 2])
We keep 3.43e+06/1.97e+08 =  1% of the original kernel matrix.

torch.Size([40850, 2])
We keep 2.64e+07/7.46e+08 =  3% of the original kernel matrix.

torch.Size([38538, 2])
We keep 7.98e+06/5.26e+08 =  1% of the original kernel matrix.

torch.Size([144020, 2])
We keep 1.03e+08/7.15e+09 =  1% of the original kernel matrix.

torch.Size([73356, 2])
We keep 2.08e+07/1.63e+09 =  1% of the original kernel matrix.

torch.Size([32887, 2])
We keep 6.45e+06/3.56e+08 =  1% of the original kernel matrix.

torch.Size([33821, 2])
We keep 5.57e+06/3.64e+08 =  1% of the original kernel matrix.

torch.Size([46644, 2])
We keep 2.56e+08/2.06e+09 = 12% of the original kernel matrix.

torch.Size([39795, 2])
We keep 1.22e+07/8.76e+08 =  1% of the original kernel matrix.

torch.Size([7276, 2])
We keep 4.64e+05/1.01e+07 =  4% of the original kernel matrix.

torch.Size([16579, 2])
We keep 1.54e+06/6.12e+07 =  2% of the original kernel matrix.

torch.Size([48113, 2])
We keep 2.05e+07/8.67e+08 =  2% of the original kernel matrix.

torch.Size([41892, 2])
We keep 8.42e+06/5.68e+08 =  1% of the original kernel matrix.

torch.Size([34043, 2])
We keep 8.17e+06/4.04e+08 =  2% of the original kernel matrix.

torch.Size([36063, 2])
We keep 6.30e+06/3.88e+08 =  1% of the original kernel matrix.

torch.Size([45508, 2])
We keep 2.58e+07/9.34e+08 =  2% of the original kernel matrix.

torch.Size([40417, 2])
We keep 8.64e+06/5.89e+08 =  1% of the original kernel matrix.

torch.Size([76535, 2])
We keep 8.30e+07/2.24e+09 =  3% of the original kernel matrix.

torch.Size([51705, 2])
We keep 1.28e+07/9.12e+08 =  1% of the original kernel matrix.

torch.Size([498419, 2])
We keep 6.39e+08/7.47e+10 =  0% of the original kernel matrix.

torch.Size([138346, 2])
We keep 5.81e+07/5.27e+09 =  1% of the original kernel matrix.

torch.Size([12284, 2])
We keep 1.54e+06/3.84e+07 =  4% of the original kernel matrix.

torch.Size([21291, 2])
We keep 2.51e+06/1.19e+08 =  2% of the original kernel matrix.

torch.Size([5595, 2])
We keep 4.14e+05/7.10e+06 =  5% of the original kernel matrix.

torch.Size([14650, 2])
We keep 1.33e+06/5.14e+07 =  2% of the original kernel matrix.

torch.Size([16122, 2])
We keep 1.96e+06/7.35e+07 =  2% of the original kernel matrix.

torch.Size([24456, 2])
We keep 3.20e+06/1.65e+08 =  1% of the original kernel matrix.

torch.Size([64919, 2])
We keep 2.11e+07/1.38e+09 =  1% of the original kernel matrix.

torch.Size([48679, 2])
We keep 1.02e+07/7.15e+08 =  1% of the original kernel matrix.

torch.Size([49893, 2])
We keep 1.57e+07/8.72e+08 =  1% of the original kernel matrix.

torch.Size([42622, 2])
We keep 8.48e+06/5.69e+08 =  1% of the original kernel matrix.

torch.Size([8338, 2])
We keep 9.46e+05/1.77e+07 =  5% of the original kernel matrix.

torch.Size([17619, 2])
We keep 1.82e+06/8.12e+07 =  2% of the original kernel matrix.

torch.Size([6660, 2])
We keep 6.49e+05/1.12e+07 =  5% of the original kernel matrix.

torch.Size([15714, 2])
We keep 1.60e+06/6.46e+07 =  2% of the original kernel matrix.

torch.Size([52149, 2])
We keep 2.62e+07/9.64e+08 =  2% of the original kernel matrix.

torch.Size([43588, 2])
We keep 8.85e+06/5.98e+08 =  1% of the original kernel matrix.

torch.Size([6694, 2])
We keep 4.16e+05/8.70e+06 =  4% of the original kernel matrix.

torch.Size([16009, 2])
We keep 1.44e+06/5.68e+07 =  2% of the original kernel matrix.

torch.Size([28342, 2])
We keep 1.52e+07/4.06e+08 =  3% of the original kernel matrix.

torch.Size([31740, 2])
We keep 6.25e+06/3.88e+08 =  1% of the original kernel matrix.

torch.Size([9717, 2])
We keep 8.64e+05/2.23e+07 =  3% of the original kernel matrix.

torch.Size([18854, 2])
We keep 2.04e+06/9.09e+07 =  2% of the original kernel matrix.

torch.Size([27653, 2])
We keep 5.06e+06/2.52e+08 =  2% of the original kernel matrix.

torch.Size([32600, 2])
We keep 5.13e+06/3.06e+08 =  1% of the original kernel matrix.

torch.Size([14426, 2])
We keep 2.90e+06/6.66e+07 =  4% of the original kernel matrix.

torch.Size([22974, 2])
We keep 3.04e+06/1.57e+08 =  1% of the original kernel matrix.

torch.Size([13830, 2])
We keep 1.35e+06/4.46e+07 =  3% of the original kernel matrix.

torch.Size([22544, 2])
We keep 2.65e+06/1.29e+08 =  2% of the original kernel matrix.

torch.Size([7868, 2])
We keep 5.46e+05/1.22e+07 =  4% of the original kernel matrix.

torch.Size([16996, 2])
We keep 1.64e+06/6.74e+07 =  2% of the original kernel matrix.

torch.Size([186478, 2])
We keep 1.28e+08/1.21e+10 =  1% of the original kernel matrix.

torch.Size([84410, 2])
We keep 2.57e+07/2.12e+09 =  1% of the original kernel matrix.

torch.Size([363759, 2])
We keep 5.30e+08/4.44e+10 =  1% of the original kernel matrix.

torch.Size([118418, 2])
We keep 4.60e+07/4.06e+09 =  1% of the original kernel matrix.

torch.Size([266160, 2])
We keep 5.01e+08/2.71e+10 =  1% of the original kernel matrix.

torch.Size([101389, 2])
We keep 3.75e+07/3.17e+09 =  1% of the original kernel matrix.

torch.Size([156028, 2])
We keep 3.34e+08/1.11e+10 =  3% of the original kernel matrix.

torch.Size([75142, 2])
We keep 2.53e+07/2.03e+09 =  1% of the original kernel matrix.

torch.Size([48256, 2])
We keep 1.69e+07/8.09e+08 =  2% of the original kernel matrix.

torch.Size([42126, 2])
We keep 8.29e+06/5.48e+08 =  1% of the original kernel matrix.

torch.Size([13915, 2])
We keep 1.90e+06/5.58e+07 =  3% of the original kernel matrix.

torch.Size([22526, 2])
We keep 2.85e+06/1.44e+08 =  1% of the original kernel matrix.

torch.Size([18144, 2])
We keep 2.96e+06/9.99e+07 =  2% of the original kernel matrix.

torch.Size([26136, 2])
We keep 3.57e+06/1.93e+08 =  1% of the original kernel matrix.

torch.Size([220410, 2])
We keep 1.68e+08/1.76e+10 =  0% of the original kernel matrix.

torch.Size([93197, 2])
We keep 3.07e+07/2.56e+09 =  1% of the original kernel matrix.

torch.Size([17423, 2])
We keep 2.64e+06/8.67e+07 =  3% of the original kernel matrix.

torch.Size([25817, 2])
We keep 3.42e+06/1.79e+08 =  1% of the original kernel matrix.

torch.Size([36085, 2])
We keep 1.32e+07/4.87e+08 =  2% of the original kernel matrix.

torch.Size([36527, 2])
We keep 6.64e+06/4.25e+08 =  1% of the original kernel matrix.

torch.Size([17022, 2])
We keep 5.98e+06/1.02e+08 =  5% of the original kernel matrix.

torch.Size([24953, 2])
We keep 3.59e+06/1.95e+08 =  1% of the original kernel matrix.

torch.Size([356270, 2])
We keep 1.47e+09/6.14e+10 =  2% of the original kernel matrix.

torch.Size([116671, 2])
We keep 5.37e+07/4.78e+09 =  1% of the original kernel matrix.

torch.Size([44267, 2])
We keep 2.99e+07/7.73e+08 =  3% of the original kernel matrix.

torch.Size([40254, 2])
We keep 8.09e+06/5.36e+08 =  1% of the original kernel matrix.

torch.Size([51832, 2])
We keep 2.11e+07/9.56e+08 =  2% of the original kernel matrix.

torch.Size([43807, 2])
We keep 8.88e+06/5.96e+08 =  1% of the original kernel matrix.

torch.Size([214291, 2])
We keep 3.11e+08/1.77e+10 =  1% of the original kernel matrix.

torch.Size([91027, 2])
We keep 3.04e+07/2.56e+09 =  1% of the original kernel matrix.

torch.Size([74691, 2])
We keep 2.50e+07/1.76e+09 =  1% of the original kernel matrix.

torch.Size([51539, 2])
We keep 1.14e+07/8.09e+08 =  1% of the original kernel matrix.

torch.Size([35918, 2])
We keep 1.76e+07/5.01e+08 =  3% of the original kernel matrix.

torch.Size([36704, 2])
We keep 6.58e+06/4.32e+08 =  1% of the original kernel matrix.

torch.Size([265164, 2])
We keep 4.91e+08/2.75e+10 =  1% of the original kernel matrix.

torch.Size([101645, 2])
We keep 3.74e+07/3.19e+09 =  1% of the original kernel matrix.

torch.Size([191125, 2])
We keep 1.39e+08/1.34e+10 =  1% of the original kernel matrix.

torch.Size([85291, 2])
We keep 2.71e+07/2.23e+09 =  1% of the original kernel matrix.

torch.Size([70551, 2])
We keep 5.25e+07/1.90e+09 =  2% of the original kernel matrix.

torch.Size([49913, 2])
We keep 1.18e+07/8.39e+08 =  1% of the original kernel matrix.

torch.Size([24455, 2])
We keep 1.47e+07/2.82e+08 =  5% of the original kernel matrix.

torch.Size([29760, 2])
We keep 5.28e+06/3.24e+08 =  1% of the original kernel matrix.

torch.Size([61406, 2])
We keep 3.04e+07/1.39e+09 =  2% of the original kernel matrix.

torch.Size([47136, 2])
We keep 1.03e+07/7.18e+08 =  1% of the original kernel matrix.

torch.Size([16188, 2])
We keep 2.33e+06/6.95e+07 =  3% of the original kernel matrix.

torch.Size([24607, 2])
We keep 3.10e+06/1.61e+08 =  1% of the original kernel matrix.

torch.Size([148950, 2])
We keep 1.00e+08/7.38e+09 =  1% of the original kernel matrix.

torch.Size([74616, 2])
We keep 2.08e+07/1.66e+09 =  1% of the original kernel matrix.

torch.Size([869473, 2])
We keep 2.26e+09/2.38e+11 =  0% of the original kernel matrix.

torch.Size([190422, 2])
We keep 9.85e+07/9.41e+09 =  1% of the original kernel matrix.

torch.Size([270195, 2])
We keep 1.81e+09/3.89e+10 =  4% of the original kernel matrix.

torch.Size([102106, 2])
We keep 4.35e+07/3.80e+09 =  1% of the original kernel matrix.

torch.Size([89838, 2])
We keep 2.33e+08/7.49e+09 =  3% of the original kernel matrix.

torch.Size([54273, 2])
We keep 2.15e+07/1.67e+09 =  1% of the original kernel matrix.

torch.Size([17839, 2])
We keep 4.24e+06/9.93e+07 =  4% of the original kernel matrix.

torch.Size([25767, 2])
We keep 3.49e+06/1.92e+08 =  1% of the original kernel matrix.

torch.Size([33804, 2])
We keep 2.97e+07/6.19e+08 =  4% of the original kernel matrix.

torch.Size([34716, 2])
We keep 7.47e+06/4.79e+08 =  1% of the original kernel matrix.

torch.Size([33003, 2])
We keep 7.72e+06/3.55e+08 =  2% of the original kernel matrix.

torch.Size([33978, 2])
We keep 5.56e+06/3.63e+08 =  1% of the original kernel matrix.

torch.Size([26608, 2])
We keep 1.95e+07/3.78e+08 =  5% of the original kernel matrix.

torch.Size([31281, 2])
We keep 6.28e+06/3.74e+08 =  1% of the original kernel matrix.

torch.Size([68004, 2])
We keep 4.81e+07/1.79e+09 =  2% of the original kernel matrix.

torch.Size([49263, 2])
We keep 1.17e+07/8.15e+08 =  1% of the original kernel matrix.

torch.Size([555256, 2])
We keep 7.00e+08/9.11e+10 =  0% of the original kernel matrix.

torch.Size([147894, 2])
We keep 6.36e+07/5.82e+09 =  1% of the original kernel matrix.

torch.Size([9738, 2])
We keep 3.23e+06/3.34e+07 =  9% of the original kernel matrix.

torch.Size([18777, 2])
We keep 2.37e+06/1.11e+08 =  2% of the original kernel matrix.

torch.Size([131196, 2])
We keep 8.19e+08/1.94e+10 =  4% of the original kernel matrix.

torch.Size([66686, 2])
We keep 3.22e+07/2.68e+09 =  1% of the original kernel matrix.

torch.Size([88129, 2])
We keep 7.96e+07/3.27e+09 =  2% of the original kernel matrix.

torch.Size([56361, 2])
We keep 1.47e+07/1.10e+09 =  1% of the original kernel matrix.

torch.Size([59032, 2])
We keep 2.09e+07/1.23e+09 =  1% of the original kernel matrix.

torch.Size([46247, 2])
We keep 9.78e+06/6.77e+08 =  1% of the original kernel matrix.

torch.Size([10926, 2])
We keep 1.65e+06/3.12e+07 =  5% of the original kernel matrix.

torch.Size([20074, 2])
We keep 2.33e+06/1.08e+08 =  2% of the original kernel matrix.

torch.Size([87221, 2])
We keep 4.71e+07/2.55e+09 =  1% of the original kernel matrix.

torch.Size([56240, 2])
We keep 1.32e+07/9.73e+08 =  1% of the original kernel matrix.

torch.Size([214545, 2])
We keep 1.83e+08/1.65e+10 =  1% of the original kernel matrix.

torch.Size([91241, 2])
We keep 2.96e+07/2.47e+09 =  1% of the original kernel matrix.

torch.Size([8060, 2])
We keep 1.02e+06/1.62e+07 =  6% of the original kernel matrix.

torch.Size([17210, 2])
We keep 1.84e+06/7.75e+07 =  2% of the original kernel matrix.

torch.Size([1456526, 2])
We keep 3.80e+09/5.32e+11 =  0% of the original kernel matrix.

torch.Size([245456, 2])
We keep 1.43e+08/1.41e+10 =  1% of the original kernel matrix.

torch.Size([68840, 2])
We keep 5.28e+07/2.14e+09 =  2% of the original kernel matrix.

torch.Size([49966, 2])
We keep 1.21e+07/8.91e+08 =  1% of the original kernel matrix.

torch.Size([113436, 2])
We keep 1.79e+08/5.57e+09 =  3% of the original kernel matrix.

torch.Size([64129, 2])
We keep 1.89e+07/1.44e+09 =  1% of the original kernel matrix.

torch.Size([27143, 2])
We keep 3.77e+07/3.77e+08 = 10% of the original kernel matrix.

torch.Size([31947, 2])
We keep 6.16e+06/3.74e+08 =  1% of the original kernel matrix.

torch.Size([23365, 2])
We keep 5.08e+06/1.85e+08 =  2% of the original kernel matrix.

torch.Size([29634, 2])
We keep 4.52e+06/2.62e+08 =  1% of the original kernel matrix.

torch.Size([5842, 2])
We keep 3.31e+05/6.28e+06 =  5% of the original kernel matrix.

torch.Size([15185, 2])
We keep 1.28e+06/4.83e+07 =  2% of the original kernel matrix.

torch.Size([8059, 2])
We keep 1.02e+06/1.68e+07 =  6% of the original kernel matrix.

torch.Size([17073, 2])
We keep 1.86e+06/7.90e+07 =  2% of the original kernel matrix.

torch.Size([28426, 2])
We keep 8.53e+06/2.87e+08 =  2% of the original kernel matrix.

torch.Size([32564, 2])
We keep 5.47e+06/3.27e+08 =  1% of the original kernel matrix.

torch.Size([161324, 2])
We keep 4.87e+08/1.60e+10 =  3% of the original kernel matrix.

torch.Size([76299, 2])
We keep 2.90e+07/2.44e+09 =  1% of the original kernel matrix.

torch.Size([1845806, 2])
We keep 3.92e+10/2.61e+12 =  1% of the original kernel matrix.

torch.Size([235111, 2])
We keep 3.04e+08/3.11e+10 =  0% of the original kernel matrix.

torch.Size([9549, 2])
We keep 8.90e+05/2.21e+07 =  4% of the original kernel matrix.

torch.Size([18923, 2])
We keep 2.02e+06/9.05e+07 =  2% of the original kernel matrix.

torch.Size([178355, 2])
We keep 1.29e+08/1.07e+10 =  1% of the original kernel matrix.

torch.Size([82090, 2])
We keep 2.46e+07/1.99e+09 =  1% of the original kernel matrix.

torch.Size([9260, 2])
We keep 1.00e+06/2.16e+07 =  4% of the original kernel matrix.

torch.Size([18502, 2])
We keep 2.02e+06/8.96e+07 =  2% of the original kernel matrix.

torch.Size([33554, 2])
We keep 8.50e+06/4.08e+08 =  2% of the original kernel matrix.

torch.Size([35235, 2])
We keep 6.14e+06/3.89e+08 =  1% of the original kernel matrix.

torch.Size([19155, 2])
We keep 7.67e+06/1.65e+08 =  4% of the original kernel matrix.

torch.Size([26433, 2])
We keep 4.46e+06/2.48e+08 =  1% of the original kernel matrix.

torch.Size([189700, 2])
We keep 7.58e+08/2.38e+10 =  3% of the original kernel matrix.

torch.Size([82975, 2])
We keep 3.52e+07/2.97e+09 =  1% of the original kernel matrix.

torch.Size([16827, 2])
We keep 3.49e+06/1.00e+08 =  3% of the original kernel matrix.

torch.Size([24685, 2])
We keep 3.47e+06/1.93e+08 =  1% of the original kernel matrix.

torch.Size([1966685, 2])
We keep 5.89e+09/9.56e+11 =  0% of the original kernel matrix.

torch.Size([290156, 2])
We keep 1.87e+08/1.88e+10 =  0% of the original kernel matrix.

torch.Size([11598, 2])
We keep 1.14e+06/3.08e+07 =  3% of the original kernel matrix.

torch.Size([20632, 2])
We keep 2.30e+06/1.07e+08 =  2% of the original kernel matrix.

torch.Size([186444, 2])
We keep 2.23e+08/1.24e+10 =  1% of the original kernel matrix.

torch.Size([84419, 2])
We keep 2.66e+07/2.14e+09 =  1% of the original kernel matrix.

torch.Size([41780, 2])
We keep 4.22e+07/7.73e+08 =  5% of the original kernel matrix.

torch.Size([39197, 2])
We keep 8.14e+06/5.36e+08 =  1% of the original kernel matrix.

torch.Size([112816, 2])
We keep 7.82e+07/4.80e+09 =  1% of the original kernel matrix.

torch.Size([64249, 2])
We keep 1.75e+07/1.34e+09 =  1% of the original kernel matrix.

torch.Size([299977, 2])
We keep 4.43e+08/3.85e+10 =  1% of the original kernel matrix.

torch.Size([107170, 2])
We keep 4.32e+07/3.78e+09 =  1% of the original kernel matrix.

torch.Size([32710, 2])
We keep 7.57e+06/3.74e+08 =  2% of the original kernel matrix.

torch.Size([35754, 2])
We keep 6.22e+06/3.73e+08 =  1% of the original kernel matrix.

torch.Size([15632, 2])
We keep 2.18e+06/6.47e+07 =  3% of the original kernel matrix.

torch.Size([23987, 2])
We keep 3.05e+06/1.55e+08 =  1% of the original kernel matrix.

torch.Size([122331, 2])
We keep 6.12e+07/5.07e+09 =  1% of the original kernel matrix.

torch.Size([67218, 2])
We keep 1.79e+07/1.37e+09 =  1% of the original kernel matrix.

torch.Size([12348, 2])
We keep 1.50e+06/4.15e+07 =  3% of the original kernel matrix.

torch.Size([21218, 2])
We keep 2.60e+06/1.24e+08 =  2% of the original kernel matrix.

torch.Size([22257, 2])
We keep 1.27e+07/2.94e+08 =  4% of the original kernel matrix.

torch.Size([27773, 2])
We keep 5.46e+06/3.31e+08 =  1% of the original kernel matrix.

torch.Size([170708, 2])
We keep 1.59e+08/1.15e+10 =  1% of the original kernel matrix.

torch.Size([79694, 2])
We keep 2.53e+07/2.07e+09 =  1% of the original kernel matrix.

torch.Size([64527, 2])
We keep 2.02e+08/3.22e+09 =  6% of the original kernel matrix.

torch.Size([47625, 2])
We keep 1.46e+07/1.09e+09 =  1% of the original kernel matrix.

torch.Size([51953, 2])
We keep 1.91e+07/9.05e+08 =  2% of the original kernel matrix.

torch.Size([44461, 2])
We keep 8.70e+06/5.80e+08 =  1% of the original kernel matrix.

torch.Size([45450, 2])
We keep 2.36e+07/8.92e+08 =  2% of the original kernel matrix.

torch.Size([40092, 2])
We keep 8.58e+06/5.76e+08 =  1% of the original kernel matrix.

torch.Size([272035, 2])
We keep 6.89e+08/3.19e+10 =  2% of the original kernel matrix.

torch.Size([102247, 2])
We keep 4.04e+07/3.44e+09 =  1% of the original kernel matrix.

torch.Size([475133, 2])
We keep 2.38e+09/1.30e+11 =  1% of the original kernel matrix.

torch.Size([133768, 2])
We keep 7.59e+07/6.95e+09 =  1% of the original kernel matrix.

torch.Size([55233, 2])
We keep 1.88e+07/9.94e+08 =  1% of the original kernel matrix.

torch.Size([44829, 2])
We keep 8.91e+06/6.07e+08 =  1% of the original kernel matrix.

torch.Size([15597, 2])
We keep 1.79e+06/6.33e+07 =  2% of the original kernel matrix.

torch.Size([24111, 2])
We keep 3.01e+06/1.53e+08 =  1% of the original kernel matrix.

torch.Size([150805, 2])
We keep 5.97e+08/1.45e+10 =  4% of the original kernel matrix.

torch.Size([73463, 2])
We keep 2.86e+07/2.32e+09 =  1% of the original kernel matrix.

torch.Size([52791, 2])
We keep 2.39e+07/1.07e+09 =  2% of the original kernel matrix.

torch.Size([43200, 2])
We keep 9.25e+06/6.31e+08 =  1% of the original kernel matrix.

torch.Size([63210, 2])
We keep 3.55e+07/1.34e+09 =  2% of the original kernel matrix.

torch.Size([47689, 2])
We keep 9.87e+06/7.06e+08 =  1% of the original kernel matrix.

torch.Size([47424, 2])
We keep 2.01e+07/8.38e+08 =  2% of the original kernel matrix.

torch.Size([41511, 2])
We keep 8.45e+06/5.58e+08 =  1% of the original kernel matrix.

torch.Size([61616, 2])
We keep 9.65e+07/2.60e+09 =  3% of the original kernel matrix.

torch.Size([45840, 2])
We keep 1.36e+07/9.83e+08 =  1% of the original kernel matrix.

torch.Size([121964, 2])
We keep 1.30e+08/6.30e+09 =  2% of the original kernel matrix.

torch.Size([66578, 2])
We keep 2.00e+07/1.53e+09 =  1% of the original kernel matrix.

torch.Size([148988, 2])
We keep 9.88e+08/2.88e+10 =  3% of the original kernel matrix.

torch.Size([66773, 2])
We keep 3.78e+07/3.27e+09 =  1% of the original kernel matrix.

torch.Size([41273, 2])
We keep 1.48e+07/6.27e+08 =  2% of the original kernel matrix.

torch.Size([38820, 2])
We keep 7.37e+06/4.83e+08 =  1% of the original kernel matrix.

torch.Size([29100, 2])
We keep 6.08e+06/2.94e+08 =  2% of the original kernel matrix.

torch.Size([33255, 2])
We keep 5.50e+06/3.31e+08 =  1% of the original kernel matrix.

torch.Size([8854, 2])
We keep 6.57e+05/1.50e+07 =  4% of the original kernel matrix.

torch.Size([18134, 2])
We keep 1.77e+06/7.47e+07 =  2% of the original kernel matrix.

torch.Size([30431, 2])
We keep 1.18e+07/3.07e+08 =  3% of the original kernel matrix.

torch.Size([34044, 2])
We keep 5.56e+06/3.38e+08 =  1% of the original kernel matrix.

torch.Size([124653, 2])
We keep 2.13e+08/8.35e+09 =  2% of the original kernel matrix.

torch.Size([67122, 2])
We keep 2.25e+07/1.76e+09 =  1% of the original kernel matrix.

torch.Size([116091, 2])
We keep 1.03e+08/5.14e+09 =  1% of the original kernel matrix.

torch.Size([64802, 2])
We keep 1.81e+07/1.38e+09 =  1% of the original kernel matrix.

torch.Size([227825, 2])
We keep 3.16e+08/1.84e+10 =  1% of the original kernel matrix.

torch.Size([94109, 2])
We keep 3.12e+07/2.62e+09 =  1% of the original kernel matrix.

torch.Size([13583, 2])
We keep 1.55e+06/4.64e+07 =  3% of the original kernel matrix.

torch.Size([22346, 2])
We keep 2.70e+06/1.31e+08 =  2% of the original kernel matrix.

torch.Size([36362, 2])
We keep 9.51e+06/4.90e+08 =  1% of the original kernel matrix.

torch.Size([36630, 2])
We keep 6.59e+06/4.27e+08 =  1% of the original kernel matrix.

torch.Size([80546, 2])
We keep 6.08e+07/2.44e+09 =  2% of the original kernel matrix.

torch.Size([53520, 2])
We keep 1.31e+07/9.51e+08 =  1% of the original kernel matrix.

torch.Size([178668, 2])
We keep 3.92e+08/1.17e+10 =  3% of the original kernel matrix.

torch.Size([82380, 2])
We keep 2.49e+07/2.08e+09 =  1% of the original kernel matrix.

torch.Size([58570, 2])
We keep 2.89e+07/1.27e+09 =  2% of the original kernel matrix.

torch.Size([45787, 2])
We keep 9.85e+06/6.86e+08 =  1% of the original kernel matrix.

torch.Size([100826, 2])
We keep 5.43e+07/3.28e+09 =  1% of the original kernel matrix.

torch.Size([60253, 2])
We keep 1.48e+07/1.10e+09 =  1% of the original kernel matrix.

torch.Size([215955, 2])
We keep 6.07e+09/7.43e+10 =  8% of the original kernel matrix.

torch.Size([80133, 2])
We keep 6.01e+07/5.25e+09 =  1% of the original kernel matrix.

torch.Size([46493, 2])
We keep 1.79e+07/8.05e+08 =  2% of the original kernel matrix.

torch.Size([41450, 2])
We keep 8.03e+06/5.47e+08 =  1% of the original kernel matrix.

torch.Size([5041, 2])
We keep 2.19e+05/4.13e+06 =  5% of the original kernel matrix.

torch.Size([14626, 2])
We keep 1.10e+06/3.92e+07 =  2% of the original kernel matrix.

torch.Size([13442, 2])
We keep 1.43e+06/4.39e+07 =  3% of the original kernel matrix.

torch.Size([22347, 2])
We keep 2.61e+06/1.28e+08 =  2% of the original kernel matrix.

torch.Size([247724, 2])
We keep 2.33e+08/2.14e+10 =  1% of the original kernel matrix.

torch.Size([98334, 2])
We keep 3.28e+07/2.82e+09 =  1% of the original kernel matrix.

torch.Size([42487, 2])
We keep 2.14e+07/8.30e+08 =  2% of the original kernel matrix.

torch.Size([38955, 2])
We keep 8.21e+06/5.55e+08 =  1% of the original kernel matrix.

torch.Size([108319, 2])
We keep 4.49e+07/3.63e+09 =  1% of the original kernel matrix.

torch.Size([62603, 2])
We keep 1.54e+07/1.16e+09 =  1% of the original kernel matrix.

torch.Size([5272, 2])
We keep 2.43e+05/4.61e+06 =  5% of the original kernel matrix.

torch.Size([14671, 2])
We keep 1.15e+06/4.14e+07 =  2% of the original kernel matrix.

torch.Size([9100, 2])
We keep 7.31e+05/1.61e+07 =  4% of the original kernel matrix.

torch.Size([18474, 2])
We keep 1.83e+06/7.73e+07 =  2% of the original kernel matrix.

torch.Size([257809, 2])
We keep 4.10e+08/2.55e+10 =  1% of the original kernel matrix.

torch.Size([99497, 2])
We keep 3.63e+07/3.08e+09 =  1% of the original kernel matrix.

torch.Size([113858, 2])
We keep 7.23e+07/4.95e+09 =  1% of the original kernel matrix.

torch.Size([64270, 2])
We keep 1.78e+07/1.36e+09 =  1% of the original kernel matrix.

torch.Size([65857, 2])
We keep 2.41e+07/1.41e+09 =  1% of the original kernel matrix.

torch.Size([48733, 2])
We keep 1.04e+07/7.23e+08 =  1% of the original kernel matrix.

torch.Size([9922, 2])
We keep 8.27e+05/2.14e+07 =  3% of the original kernel matrix.

torch.Size([19125, 2])
We keep 2.01e+06/8.92e+07 =  2% of the original kernel matrix.

torch.Size([8502, 2])
We keep 6.83e+05/1.53e+07 =  4% of the original kernel matrix.

torch.Size([17797, 2])
We keep 1.79e+06/7.53e+07 =  2% of the original kernel matrix.

torch.Size([42539, 2])
We keep 1.55e+07/6.12e+08 =  2% of the original kernel matrix.

torch.Size([39754, 2])
We keep 7.27e+06/4.77e+08 =  1% of the original kernel matrix.

torch.Size([62903, 2])
We keep 3.46e+07/1.42e+09 =  2% of the original kernel matrix.

torch.Size([47411, 2])
We keep 1.05e+07/7.26e+08 =  1% of the original kernel matrix.

torch.Size([88228, 2])
We keep 1.10e+08/3.22e+09 =  3% of the original kernel matrix.

torch.Size([56379, 2])
We keep 1.47e+07/1.09e+09 =  1% of the original kernel matrix.

torch.Size([11854, 2])
We keep 2.21e+06/4.21e+07 =  5% of the original kernel matrix.

torch.Size([20947, 2])
We keep 2.58e+06/1.25e+08 =  2% of the original kernel matrix.

torch.Size([42566, 2])
We keep 6.44e+08/3.75e+09 = 17% of the original kernel matrix.

torch.Size([36515, 2])
We keep 1.62e+07/1.18e+09 =  1% of the original kernel matrix.

torch.Size([18894, 2])
We keep 4.12e+06/1.25e+08 =  3% of the original kernel matrix.

torch.Size([26429, 2])
We keep 3.90e+06/2.15e+08 =  1% of the original kernel matrix.

torch.Size([29215, 2])
We keep 2.09e+07/4.48e+08 =  4% of the original kernel matrix.

torch.Size([31984, 2])
We keep 6.61e+06/4.08e+08 =  1% of the original kernel matrix.

torch.Size([401667, 2])
We keep 8.35e+08/5.44e+10 =  1% of the original kernel matrix.

torch.Size([124739, 2])
We keep 5.03e+07/4.49e+09 =  1% of the original kernel matrix.

torch.Size([13167, 2])
We keep 3.11e+06/4.95e+07 =  6% of the original kernel matrix.

torch.Size([22087, 2])
We keep 2.64e+06/1.36e+08 =  1% of the original kernel matrix.

torch.Size([61220, 2])
We keep 3.86e+07/1.41e+09 =  2% of the original kernel matrix.

torch.Size([46901, 2])
We keep 1.04e+07/7.23e+08 =  1% of the original kernel matrix.

torch.Size([180509, 2])
We keep 2.08e+08/1.11e+10 =  1% of the original kernel matrix.

torch.Size([82836, 2])
We keep 2.53e+07/2.03e+09 =  1% of the original kernel matrix.

torch.Size([42293, 2])
We keep 1.48e+07/6.48e+08 =  2% of the original kernel matrix.

torch.Size([39841, 2])
We keep 7.53e+06/4.91e+08 =  1% of the original kernel matrix.

torch.Size([75738, 2])
We keep 4.24e+07/1.83e+09 =  2% of the original kernel matrix.

torch.Size([52192, 2])
We keep 1.17e+07/8.25e+08 =  1% of the original kernel matrix.

torch.Size([206986, 2])
We keep 1.84e+08/1.44e+10 =  1% of the original kernel matrix.

torch.Size([89453, 2])
We keep 2.82e+07/2.32e+09 =  1% of the original kernel matrix.

torch.Size([4547, 2])
We keep 2.05e+05/3.71e+06 =  5% of the original kernel matrix.

torch.Size([13637, 2])
We keep 1.07e+06/3.71e+07 =  2% of the original kernel matrix.

torch.Size([73183, 2])
We keep 2.82e+07/1.77e+09 =  1% of the original kernel matrix.

torch.Size([50934, 2])
We keep 1.14e+07/8.10e+08 =  1% of the original kernel matrix.

torch.Size([37830, 2])
We keep 9.95e+06/4.67e+08 =  2% of the original kernel matrix.

torch.Size([37950, 2])
We keep 6.50e+06/4.16e+08 =  1% of the original kernel matrix.

torch.Size([164266, 2])
We keep 1.72e+08/9.54e+09 =  1% of the original kernel matrix.

torch.Size([78313, 2])
We keep 2.31e+07/1.88e+09 =  1% of the original kernel matrix.

torch.Size([150315, 2])
We keep 1.35e+08/9.31e+09 =  1% of the original kernel matrix.

torch.Size([74719, 2])
We keep 2.31e+07/1.86e+09 =  1% of the original kernel matrix.

torch.Size([49415, 2])
We keep 3.52e+07/1.21e+09 =  2% of the original kernel matrix.

torch.Size([41326, 2])
We keep 9.90e+06/6.70e+08 =  1% of the original kernel matrix.

torch.Size([111677, 2])
We keep 8.27e+07/4.27e+09 =  1% of the original kernel matrix.

torch.Size([63881, 2])
We keep 1.66e+07/1.26e+09 =  1% of the original kernel matrix.

torch.Size([23994, 2])
We keep 3.96e+06/1.87e+08 =  2% of the original kernel matrix.

torch.Size([29850, 2])
We keep 4.55e+06/2.64e+08 =  1% of the original kernel matrix.

torch.Size([14381, 2])
We keep 8.72e+06/1.53e+08 =  5% of the original kernel matrix.

torch.Size([22179, 2])
We keep 4.35e+06/2.39e+08 =  1% of the original kernel matrix.

torch.Size([14408, 2])
We keep 3.19e+06/5.98e+07 =  5% of the original kernel matrix.

torch.Size([22977, 2])
We keep 2.91e+06/1.49e+08 =  1% of the original kernel matrix.

torch.Size([15556, 2])
We keep 2.85e+06/7.78e+07 =  3% of the original kernel matrix.

torch.Size([23734, 2])
We keep 3.25e+06/1.70e+08 =  1% of the original kernel matrix.

torch.Size([169608, 2])
We keep 1.57e+08/1.08e+10 =  1% of the original kernel matrix.

torch.Size([80058, 2])
We keep 2.49e+07/2.00e+09 =  1% of the original kernel matrix.

torch.Size([285339, 2])
We keep 7.14e+08/3.59e+10 =  1% of the original kernel matrix.

torch.Size([104850, 2])
We keep 4.25e+07/3.65e+09 =  1% of the original kernel matrix.

torch.Size([10017, 2])
We keep 1.15e+06/2.66e+07 =  4% of the original kernel matrix.

torch.Size([19045, 2])
We keep 2.11e+06/9.95e+07 =  2% of the original kernel matrix.

torch.Size([32745, 2])
We keep 9.14e+06/3.79e+08 =  2% of the original kernel matrix.

torch.Size([36050, 2])
We keep 6.13e+06/3.75e+08 =  1% of the original kernel matrix.

torch.Size([22550, 2])
We keep 3.62e+06/1.50e+08 =  2% of the original kernel matrix.

torch.Size([29346, 2])
We keep 4.18e+06/2.36e+08 =  1% of the original kernel matrix.

torch.Size([452916, 2])
We keep 9.54e+08/7.37e+10 =  1% of the original kernel matrix.

torch.Size([132387, 2])
We keep 5.76e+07/5.23e+09 =  1% of the original kernel matrix.

torch.Size([61918, 2])
We keep 2.27e+07/1.20e+09 =  1% of the original kernel matrix.

torch.Size([47441, 2])
We keep 9.77e+06/6.67e+08 =  1% of the original kernel matrix.

torch.Size([23977, 2])
We keep 7.49e+06/2.10e+08 =  3% of the original kernel matrix.

torch.Size([29942, 2])
We keep 4.70e+06/2.79e+08 =  1% of the original kernel matrix.

torch.Size([128844, 2])
We keep 2.26e+08/7.39e+09 =  3% of the original kernel matrix.

torch.Size([69201, 2])
We keep 2.06e+07/1.66e+09 =  1% of the original kernel matrix.

torch.Size([156023, 2])
We keep 1.26e+08/8.16e+09 =  1% of the original kernel matrix.

torch.Size([76490, 2])
We keep 2.18e+07/1.74e+09 =  1% of the original kernel matrix.

torch.Size([14308, 2])
We keep 2.59e+06/7.17e+07 =  3% of the original kernel matrix.

torch.Size([22615, 2])
We keep 3.13e+06/1.63e+08 =  1% of the original kernel matrix.

torch.Size([74035, 2])
We keep 4.13e+07/1.90e+09 =  2% of the original kernel matrix.

torch.Size([51464, 2])
We keep 1.19e+07/8.39e+08 =  1% of the original kernel matrix.

torch.Size([11568, 2])
We keep 1.10e+06/3.01e+07 =  3% of the original kernel matrix.

torch.Size([20681, 2])
We keep 2.22e+06/1.06e+08 =  2% of the original kernel matrix.

time for making ranges is 4.1354660987854
Sorting X and nu_X
time for sorting X is 0.08790421485900879
Sorting Z and nu_Z
time for sorting Z is 0.00027489662170410156
Starting Optim
sum tnu_Z before tensor(37243564., device='cuda:0')
c= tensor(1556.0684, device='cuda:0')
c= tensor(88450.0234, device='cuda:0')
c= tensor(93405.2734, device='cuda:0')
c= tensor(96790.8594, device='cuda:0')
c= tensor(1009588.3750, device='cuda:0')
c= tensor(1801832., device='cuda:0')
c= tensor(2960876.2500, device='cuda:0')
c= tensor(3520757.2500, device='cuda:0')
c= tensor(3551205., device='cuda:0')
c= tensor(38127144., device='cuda:0')
c= tensor(38166188., device='cuda:0')
c= tensor(41586860., device='cuda:0')
c= tensor(41602232., device='cuda:0')
c= tensor(60200864., device='cuda:0')
c= tensor(60439500., device='cuda:0')
c= tensor(62643928., device='cuda:0')
c= tensor(64462772., device='cuda:0')
c= tensor(65238656., device='cuda:0')
c= tensor(87884544., device='cuda:0')
c= tensor(92874600., device='cuda:0')
c= tensor(93020120., device='cuda:0')
c= tensor(1.7253e+08, device='cuda:0')
c= tensor(1.7264e+08, device='cuda:0')
c= tensor(1.7270e+08, device='cuda:0')
c= tensor(1.7351e+08, device='cuda:0')
c= tensor(1.7491e+08, device='cuda:0')
c= tensor(1.7577e+08, device='cuda:0')
c= tensor(1.7586e+08, device='cuda:0')
c= tensor(1.7825e+08, device='cuda:0')
c= tensor(9.1226e+08, device='cuda:0')
c= tensor(9.1230e+08, device='cuda:0')
c= tensor(1.0152e+09, device='cuda:0')
c= tensor(1.0152e+09, device='cuda:0')
c= tensor(1.0153e+09, device='cuda:0')
c= tensor(1.0153e+09, device='cuda:0')
c= tensor(1.0211e+09, device='cuda:0')
c= tensor(1.0230e+09, device='cuda:0')
c= tensor(1.0230e+09, device='cuda:0')
c= tensor(1.0230e+09, device='cuda:0')
c= tensor(1.0230e+09, device='cuda:0')
c= tensor(1.0230e+09, device='cuda:0')
c= tensor(1.0230e+09, device='cuda:0')
c= tensor(1.0230e+09, device='cuda:0')
c= tensor(1.0230e+09, device='cuda:0')
c= tensor(1.0230e+09, device='cuda:0')
c= tensor(1.0230e+09, device='cuda:0')
c= tensor(1.0231e+09, device='cuda:0')
c= tensor(1.0231e+09, device='cuda:0')
c= tensor(1.0231e+09, device='cuda:0')
c= tensor(1.0231e+09, device='cuda:0')
c= tensor(1.0232e+09, device='cuda:0')
c= tensor(1.0232e+09, device='cuda:0')
c= tensor(1.0232e+09, device='cuda:0')
c= tensor(1.0232e+09, device='cuda:0')
c= tensor(1.0233e+09, device='cuda:0')
c= tensor(1.0233e+09, device='cuda:0')
c= tensor(1.0233e+09, device='cuda:0')
c= tensor(1.0233e+09, device='cuda:0')
c= tensor(1.0233e+09, device='cuda:0')
c= tensor(1.0233e+09, device='cuda:0')
c= tensor(1.0233e+09, device='cuda:0')
c= tensor(1.0233e+09, device='cuda:0')
c= tensor(1.0233e+09, device='cuda:0')
c= tensor(1.0234e+09, device='cuda:0')
c= tensor(1.0234e+09, device='cuda:0')
c= tensor(1.0234e+09, device='cuda:0')
c= tensor(1.0234e+09, device='cuda:0')
c= tensor(1.0234e+09, device='cuda:0')
c= tensor(1.0234e+09, device='cuda:0')
c= tensor(1.0234e+09, device='cuda:0')
c= tensor(1.0234e+09, device='cuda:0')
c= tensor(1.0234e+09, device='cuda:0')
c= tensor(1.0234e+09, device='cuda:0')
c= tensor(1.0234e+09, device='cuda:0')
c= tensor(1.0235e+09, device='cuda:0')
c= tensor(1.0235e+09, device='cuda:0')
c= tensor(1.0235e+09, device='cuda:0')
c= tensor(1.0235e+09, device='cuda:0')
c= tensor(1.0235e+09, device='cuda:0')
c= tensor(1.0235e+09, device='cuda:0')
c= tensor(1.0235e+09, device='cuda:0')
c= tensor(1.0235e+09, device='cuda:0')
c= tensor(1.0236e+09, device='cuda:0')
c= tensor(1.0236e+09, device='cuda:0')
c= tensor(1.0236e+09, device='cuda:0')
c= tensor(1.0236e+09, device='cuda:0')
c= tensor(1.0236e+09, device='cuda:0')
c= tensor(1.0236e+09, device='cuda:0')
c= tensor(1.0236e+09, device='cuda:0')
c= tensor(1.0236e+09, device='cuda:0')
c= tensor(1.0236e+09, device='cuda:0')
c= tensor(1.0236e+09, device='cuda:0')
c= tensor(1.0236e+09, device='cuda:0')
c= tensor(1.0236e+09, device='cuda:0')
c= tensor(1.0236e+09, device='cuda:0')
c= tensor(1.0236e+09, device='cuda:0')
c= tensor(1.0237e+09, device='cuda:0')
c= tensor(1.0237e+09, device='cuda:0')
c= tensor(1.0237e+09, device='cuda:0')
c= tensor(1.0237e+09, device='cuda:0')
c= tensor(1.0237e+09, device='cuda:0')
c= tensor(1.0237e+09, device='cuda:0')
c= tensor(1.0238e+09, device='cuda:0')
c= tensor(1.0238e+09, device='cuda:0')
c= tensor(1.0238e+09, device='cuda:0')
c= tensor(1.0238e+09, device='cuda:0')
c= tensor(1.0238e+09, device='cuda:0')
c= tensor(1.0238e+09, device='cuda:0')
c= tensor(1.0238e+09, device='cuda:0')
c= tensor(1.0238e+09, device='cuda:0')
c= tensor(1.0238e+09, device='cuda:0')
c= tensor(1.0238e+09, device='cuda:0')
c= tensor(1.0238e+09, device='cuda:0')
c= tensor(1.0238e+09, device='cuda:0')
c= tensor(1.0238e+09, device='cuda:0')
c= tensor(1.0238e+09, device='cuda:0')
c= tensor(1.0238e+09, device='cuda:0')
c= tensor(1.0238e+09, device='cuda:0')
c= tensor(1.0239e+09, device='cuda:0')
c= tensor(1.0239e+09, device='cuda:0')
c= tensor(1.0239e+09, device='cuda:0')
c= tensor(1.0239e+09, device='cuda:0')
c= tensor(1.0239e+09, device='cuda:0')
c= tensor(1.0240e+09, device='cuda:0')
c= tensor(1.0240e+09, device='cuda:0')
c= tensor(1.0240e+09, device='cuda:0')
c= tensor(1.0240e+09, device='cuda:0')
c= tensor(1.0240e+09, device='cuda:0')
c= tensor(1.0240e+09, device='cuda:0')
c= tensor(1.0241e+09, device='cuda:0')
c= tensor(1.0241e+09, device='cuda:0')
c= tensor(1.0241e+09, device='cuda:0')
c= tensor(1.0241e+09, device='cuda:0')
c= tensor(1.0241e+09, device='cuda:0')
c= tensor(1.0241e+09, device='cuda:0')
c= tensor(1.0241e+09, device='cuda:0')
c= tensor(1.0241e+09, device='cuda:0')
c= tensor(1.0241e+09, device='cuda:0')
c= tensor(1.0241e+09, device='cuda:0')
c= tensor(1.0241e+09, device='cuda:0')
c= tensor(1.0241e+09, device='cuda:0')
c= tensor(1.0241e+09, device='cuda:0')
c= tensor(1.0242e+09, device='cuda:0')
c= tensor(1.0242e+09, device='cuda:0')
c= tensor(1.0242e+09, device='cuda:0')
c= tensor(1.0242e+09, device='cuda:0')
c= tensor(1.0242e+09, device='cuda:0')
c= tensor(1.0242e+09, device='cuda:0')
c= tensor(1.0242e+09, device='cuda:0')
c= tensor(1.0242e+09, device='cuda:0')
c= tensor(1.0243e+09, device='cuda:0')
c= tensor(1.0243e+09, device='cuda:0')
c= tensor(1.0243e+09, device='cuda:0')
c= tensor(1.0243e+09, device='cuda:0')
c= tensor(1.0243e+09, device='cuda:0')
c= tensor(1.0244e+09, device='cuda:0')
c= tensor(1.0244e+09, device='cuda:0')
c= tensor(1.0244e+09, device='cuda:0')
c= tensor(1.0244e+09, device='cuda:0')
c= tensor(1.0244e+09, device='cuda:0')
c= tensor(1.0244e+09, device='cuda:0')
c= tensor(1.0244e+09, device='cuda:0')
c= tensor(1.0245e+09, device='cuda:0')
c= tensor(1.0245e+09, device='cuda:0')
c= tensor(1.0245e+09, device='cuda:0')
c= tensor(1.0245e+09, device='cuda:0')
c= tensor(1.0245e+09, device='cuda:0')
c= tensor(1.0245e+09, device='cuda:0')
c= tensor(1.0245e+09, device='cuda:0')
c= tensor(1.0245e+09, device='cuda:0')
c= tensor(1.0245e+09, device='cuda:0')
c= tensor(1.0245e+09, device='cuda:0')
c= tensor(1.0245e+09, device='cuda:0')
c= tensor(1.0245e+09, device='cuda:0')
c= tensor(1.0245e+09, device='cuda:0')
c= tensor(1.0246e+09, device='cuda:0')
c= tensor(1.0246e+09, device='cuda:0')
c= tensor(1.0246e+09, device='cuda:0')
c= tensor(1.0246e+09, device='cuda:0')
c= tensor(1.0246e+09, device='cuda:0')
c= tensor(1.0246e+09, device='cuda:0')
c= tensor(1.0246e+09, device='cuda:0')
c= tensor(1.0246e+09, device='cuda:0')
c= tensor(1.0246e+09, device='cuda:0')
c= tensor(1.0247e+09, device='cuda:0')
c= tensor(1.0247e+09, device='cuda:0')
c= tensor(1.0247e+09, device='cuda:0')
c= tensor(1.0247e+09, device='cuda:0')
c= tensor(1.0247e+09, device='cuda:0')
c= tensor(1.0249e+09, device='cuda:0')
c= tensor(1.0249e+09, device='cuda:0')
c= tensor(1.0249e+09, device='cuda:0')
c= tensor(1.0250e+09, device='cuda:0')
c= tensor(1.0250e+09, device='cuda:0')
c= tensor(1.0250e+09, device='cuda:0')
c= tensor(1.0250e+09, device='cuda:0')
c= tensor(1.0250e+09, device='cuda:0')
c= tensor(1.0250e+09, device='cuda:0')
c= tensor(1.0250e+09, device='cuda:0')
c= tensor(1.0250e+09, device='cuda:0')
c= tensor(1.0250e+09, device='cuda:0')
c= tensor(1.0250e+09, device='cuda:0')
c= tensor(1.0250e+09, device='cuda:0')
c= tensor(1.0250e+09, device='cuda:0')
c= tensor(1.0250e+09, device='cuda:0')
c= tensor(1.0250e+09, device='cuda:0')
c= tensor(1.0251e+09, device='cuda:0')
c= tensor(1.0251e+09, device='cuda:0')
c= tensor(1.0251e+09, device='cuda:0')
c= tensor(1.0251e+09, device='cuda:0')
c= tensor(1.0251e+09, device='cuda:0')
c= tensor(1.0251e+09, device='cuda:0')
c= tensor(1.0251e+09, device='cuda:0')
c= tensor(1.0251e+09, device='cuda:0')
c= tensor(1.0251e+09, device='cuda:0')
c= tensor(1.0251e+09, device='cuda:0')
c= tensor(1.0251e+09, device='cuda:0')
c= tensor(1.0251e+09, device='cuda:0')
c= tensor(1.0251e+09, device='cuda:0')
c= tensor(1.0251e+09, device='cuda:0')
c= tensor(1.0252e+09, device='cuda:0')
c= tensor(1.0252e+09, device='cuda:0')
c= tensor(1.0252e+09, device='cuda:0')
c= tensor(1.0252e+09, device='cuda:0')
c= tensor(1.0252e+09, device='cuda:0')
c= tensor(1.0252e+09, device='cuda:0')
c= tensor(1.0252e+09, device='cuda:0')
c= tensor(1.0252e+09, device='cuda:0')
c= tensor(1.0252e+09, device='cuda:0')
c= tensor(1.0252e+09, device='cuda:0')
c= tensor(1.0252e+09, device='cuda:0')
c= tensor(1.0252e+09, device='cuda:0')
c= tensor(1.0252e+09, device='cuda:0')
c= tensor(1.0253e+09, device='cuda:0')
c= tensor(1.0253e+09, device='cuda:0')
c= tensor(1.0253e+09, device='cuda:0')
c= tensor(1.0253e+09, device='cuda:0')
c= tensor(1.0253e+09, device='cuda:0')
c= tensor(1.0253e+09, device='cuda:0')
c= tensor(1.0253e+09, device='cuda:0')
c= tensor(1.0271e+09, device='cuda:0')
c= tensor(1.0273e+09, device='cuda:0')
c= tensor(1.0273e+09, device='cuda:0')
c= tensor(1.0273e+09, device='cuda:0')
c= tensor(1.0274e+09, device='cuda:0')
c= tensor(1.0301e+09, device='cuda:0')
c= tensor(1.0668e+09, device='cuda:0')
c= tensor(1.0668e+09, device='cuda:0')
c= tensor(1.0699e+09, device='cuda:0')
c= tensor(1.0703e+09, device='cuda:0')
c= tensor(1.0720e+09, device='cuda:0')
c= tensor(1.1351e+09, device='cuda:0')
c= tensor(1.1351e+09, device='cuda:0')
c= tensor(1.1351e+09, device='cuda:0')
c= tensor(1.1464e+09, device='cuda:0')
c= tensor(1.2162e+09, device='cuda:0')
c= tensor(1.2162e+09, device='cuda:0')
c= tensor(1.2166e+09, device='cuda:0')
c= tensor(1.4082e+09, device='cuda:0')
c= tensor(1.4151e+09, device='cuda:0')
c= tensor(1.4198e+09, device='cuda:0')
c= tensor(1.4204e+09, device='cuda:0')
c= tensor(1.4218e+09, device='cuda:0')
c= tensor(1.4219e+09, device='cuda:0')
c= tensor(1.4219e+09, device='cuda:0')
c= tensor(1.5207e+09, device='cuda:0')
c= tensor(1.5208e+09, device='cuda:0')
c= tensor(1.5208e+09, device='cuda:0')
c= tensor(1.5215e+09, device='cuda:0')
c= tensor(1.5220e+09, device='cuda:0')
c= tensor(1.5552e+09, device='cuda:0')
c= tensor(1.5577e+09, device='cuda:0')
c= tensor(1.5577e+09, device='cuda:0')
c= tensor(1.5584e+09, device='cuda:0')
c= tensor(1.5585e+09, device='cuda:0')
c= tensor(1.5589e+09, device='cuda:0')
c= tensor(1.5615e+09, device='cuda:0')
c= tensor(1.5615e+09, device='cuda:0')
c= tensor(1.5623e+09, device='cuda:0')
c= tensor(1.5623e+09, device='cuda:0')
c= tensor(1.5623e+09, device='cuda:0')
c= tensor(1.5654e+09, device='cuda:0')
c= tensor(1.5668e+09, device='cuda:0')
c= tensor(1.5672e+09, device='cuda:0')
c= tensor(1.5717e+09, device='cuda:0')
c= tensor(1.6090e+09, device='cuda:0')
c= tensor(1.6091e+09, device='cuda:0')
c= tensor(1.6093e+09, device='cuda:0')
c= tensor(1.6112e+09, device='cuda:0')
c= tensor(1.6112e+09, device='cuda:0')
c= tensor(1.6171e+09, device='cuda:0')
c= tensor(1.6475e+09, device='cuda:0')
c= tensor(1.7059e+09, device='cuda:0')
c= tensor(1.7061e+09, device='cuda:0')
c= tensor(1.7063e+09, device='cuda:0')
c= tensor(1.7063e+09, device='cuda:0')
c= tensor(1.7064e+09, device='cuda:0')
c= tensor(1.7074e+09, device='cuda:0')
c= tensor(1.7074e+09, device='cuda:0')
c= tensor(1.7077e+09, device='cuda:0')
c= tensor(1.7318e+09, device='cuda:0')
c= tensor(1.7342e+09, device='cuda:0')
c= tensor(1.7342e+09, device='cuda:0')
c= tensor(1.7343e+09, device='cuda:0')
c= tensor(1.7545e+09, device='cuda:0')
c= tensor(1.7547e+09, device='cuda:0')
c= tensor(1.7547e+09, device='cuda:0')
c= tensor(1.7548e+09, device='cuda:0')
c= tensor(2.0443e+09, device='cuda:0')
c= tensor(2.0444e+09, device='cuda:0')
c= tensor(2.0876e+09, device='cuda:0')
c= tensor(2.0876e+09, device='cuda:0')
c= tensor(2.0932e+09, device='cuda:0')
c= tensor(2.0951e+09, device='cuda:0')
c= tensor(2.2268e+09, device='cuda:0')
c= tensor(2.2282e+09, device='cuda:0')
c= tensor(2.2282e+09, device='cuda:0')
c= tensor(2.2342e+09, device='cuda:0')
c= tensor(2.2369e+09, device='cuda:0')
c= tensor(2.2371e+09, device='cuda:0')
c= tensor(2.2382e+09, device='cuda:0')
c= tensor(2.2442e+09, device='cuda:0')
c= tensor(2.2651e+09, device='cuda:0')
c= tensor(2.2680e+09, device='cuda:0')
c= tensor(2.2680e+09, device='cuda:0')
c= tensor(2.2681e+09, device='cuda:0')
c= tensor(2.2814e+09, device='cuda:0')
c= tensor(2.3195e+09, device='cuda:0')
c= tensor(2.3196e+09, device='cuda:0')
c= tensor(2.3196e+09, device='cuda:0')
c= tensor(2.3202e+09, device='cuda:0')
c= tensor(2.3234e+09, device='cuda:0')
c= tensor(2.3376e+09, device='cuda:0')
c= tensor(2.3376e+09, device='cuda:0')
c= tensor(2.3381e+09, device='cuda:0')
c= tensor(2.3382e+09, device='cuda:0')
c= tensor(2.3382e+09, device='cuda:0')
c= tensor(2.3382e+09, device='cuda:0')
c= tensor(2.3383e+09, device='cuda:0')
c= tensor(2.3516e+09, device='cuda:0')
c= tensor(2.3521e+09, device='cuda:0')
c= tensor(2.3523e+09, device='cuda:0')
c= tensor(2.3526e+09, device='cuda:0')
c= tensor(2.3527e+09, device='cuda:0')
c= tensor(2.4313e+09, device='cuda:0')
c= tensor(2.4313e+09, device='cuda:0')
c= tensor(2.4369e+09, device='cuda:0')
c= tensor(2.4369e+09, device='cuda:0')
c= tensor(2.4369e+09, device='cuda:0')
c= tensor(2.4369e+09, device='cuda:0')
c= tensor(2.4377e+09, device='cuda:0')
c= tensor(2.4377e+09, device='cuda:0')
c= tensor(2.4398e+09, device='cuda:0')
c= tensor(2.4398e+09, device='cuda:0')
c= tensor(2.4398e+09, device='cuda:0')
c= tensor(2.4660e+09, device='cuda:0')
c= tensor(2.4665e+09, device='cuda:0')
c= tensor(2.4669e+09, device='cuda:0')
c= tensor(2.4733e+09, device='cuda:0')
c= tensor(2.4861e+09, device='cuda:0')
c= tensor(2.4861e+09, device='cuda:0')
c= tensor(2.4861e+09, device='cuda:0')
c= tensor(2.4865e+09, device='cuda:0')
c= tensor(2.4865e+09, device='cuda:0')
c= tensor(2.4865e+09, device='cuda:0')
c= tensor(2.4867e+09, device='cuda:0')
c= tensor(2.4867e+09, device='cuda:0')
c= tensor(2.4867e+09, device='cuda:0')
c= tensor(2.4867e+09, device='cuda:0')
c= tensor(2.4867e+09, device='cuda:0')
c= tensor(2.5392e+09, device='cuda:0')
c= tensor(2.5392e+09, device='cuda:0')
c= tensor(2.5412e+09, device='cuda:0')
c= tensor(2.5413e+09, device='cuda:0')
c= tensor(2.5418e+09, device='cuda:0')
c= tensor(2.5424e+09, device='cuda:0')
c= tensor(3.0094e+09, device='cuda:0')
c= tensor(3.1132e+09, device='cuda:0')
c= tensor(3.1146e+09, device='cuda:0')
c= tensor(3.1174e+09, device='cuda:0')
c= tensor(3.1174e+09, device='cuda:0')
c= tensor(3.1298e+09, device='cuda:0')
c= tensor(3.1304e+09, device='cuda:0')
c= tensor(3.1461e+09, device='cuda:0')
c= tensor(3.1462e+09, device='cuda:0')
c= tensor(3.1489e+09, device='cuda:0')
c= tensor(3.2102e+09, device='cuda:0')
c= tensor(3.2115e+09, device='cuda:0')
c= tensor(3.2115e+09, device='cuda:0')
c= tensor(3.2127e+09, device='cuda:0')
c= tensor(3.2128e+09, device='cuda:0')
c= tensor(3.2128e+09, device='cuda:0')
c= tensor(3.2198e+09, device='cuda:0')
c= tensor(3.2201e+09, device='cuda:0')
c= tensor(3.2201e+09, device='cuda:0')
c= tensor(3.2231e+09, device='cuda:0')
c= tensor(3.2231e+09, device='cuda:0')
c= tensor(3.2231e+09, device='cuda:0')
c= tensor(3.2302e+09, device='cuda:0')
c= tensor(3.2325e+09, device='cuda:0')
c= tensor(3.2329e+09, device='cuda:0')
c= tensor(3.2388e+09, device='cuda:0')
c= tensor(3.2441e+09, device='cuda:0')
c= tensor(3.2442e+09, device='cuda:0')
c= tensor(3.2450e+09, device='cuda:0')
c= tensor(3.2490e+09, device='cuda:0')
c= tensor(3.2530e+09, device='cuda:0')
c= tensor(3.2531e+09, device='cuda:0')
c= tensor(3.3088e+09, device='cuda:0')
c= tensor(3.3698e+09, device='cuda:0')
c= tensor(3.3737e+09, device='cuda:0')
c= tensor(3.3750e+09, device='cuda:0')
c= tensor(3.3769e+09, device='cuda:0')
c= tensor(3.3770e+09, device='cuda:0')
c= tensor(3.3770e+09, device='cuda:0')
c= tensor(3.3771e+09, device='cuda:0')
c= tensor(3.3831e+09, device='cuda:0')
c= tensor(3.3894e+09, device='cuda:0')
c= tensor(3.4326e+09, device='cuda:0')
c= tensor(3.4524e+09, device='cuda:0')
c= tensor(3.4541e+09, device='cuda:0')
c= tensor(3.4544e+09, device='cuda:0')
c= tensor(3.4560e+09, device='cuda:0')
c= tensor(3.4560e+09, device='cuda:0')
c= tensor(3.4561e+09, device='cuda:0')
c= tensor(3.4668e+09, device='cuda:0')
c= tensor(3.4670e+09, device='cuda:0')
c= tensor(3.4670e+09, device='cuda:0')
c= tensor(3.4671e+09, device='cuda:0')
c= tensor(3.4690e+09, device='cuda:0')
c= tensor(3.4693e+09, device='cuda:0')
c= tensor(3.4723e+09, device='cuda:0')
c= tensor(3.4738e+09, device='cuda:0')
c= tensor(3.4743e+09, device='cuda:0')
c= tensor(3.4743e+09, device='cuda:0')
c= tensor(3.4743e+09, device='cuda:0')
c= tensor(3.4753e+09, device='cuda:0')
c= tensor(3.4766e+09, device='cuda:0')
c= tensor(3.4768e+09, device='cuda:0')
c= tensor(3.4821e+09, device='cuda:0')
c= tensor(3.4821e+09, device='cuda:0')
c= tensor(3.4822e+09, device='cuda:0')
c= tensor(3.4822e+09, device='cuda:0')
c= tensor(3.4856e+09, device='cuda:0')
c= tensor(3.4857e+09, device='cuda:0')
c= tensor(3.4867e+09, device='cuda:0')
c= tensor(3.4868e+09, device='cuda:0')
c= tensor(3.4870e+09, device='cuda:0')
c= tensor(3.4880e+09, device='cuda:0')
c= tensor(3.5393e+09, device='cuda:0')
c= tensor(3.5394e+09, device='cuda:0')
c= tensor(3.5395e+09, device='cuda:0')
c= tensor(3.5463e+09, device='cuda:0')
c= tensor(3.5464e+09, device='cuda:0')
c= tensor(3.6153e+09, device='cuda:0')
c= tensor(3.6153e+09, device='cuda:0')
c= tensor(3.6185e+09, device='cuda:0')
c= tensor(3.6391e+09, device='cuda:0')
c= tensor(3.6391e+09, device='cuda:0')
c= tensor(3.6611e+09, device='cuda:0')
c= tensor(3.6620e+09, device='cuda:0')
c= tensor(3.7495e+09, device='cuda:0')
c= tensor(3.7495e+09, device='cuda:0')
c= tensor(3.7506e+09, device='cuda:0')
c= tensor(3.7507e+09, device='cuda:0')
c= tensor(3.7507e+09, device='cuda:0')
c= tensor(3.7507e+09, device='cuda:0')
c= tensor(3.7543e+09, device='cuda:0')
c= tensor(3.7546e+09, device='cuda:0')
c= tensor(3.7595e+09, device='cuda:0')
c= tensor(3.7596e+09, device='cuda:0')
c= tensor(3.7597e+09, device='cuda:0')
c= tensor(3.7597e+09, device='cuda:0')
c= tensor(3.7718e+09, device='cuda:0')
c= tensor(3.7733e+09, device='cuda:0')
c= tensor(3.8075e+09, device='cuda:0')
c= tensor(3.8085e+09, device='cuda:0')
c= tensor(3.8086e+09, device='cuda:0')
c= tensor(3.8086e+09, device='cuda:0')
c= tensor(3.8089e+09, device='cuda:0')
c= tensor(4.1998e+09, device='cuda:0')
c= tensor(4.1999e+09, device='cuda:0')
c= tensor(4.2000e+09, device='cuda:0')
c= tensor(4.2061e+09, device='cuda:0')
c= tensor(4.2080e+09, device='cuda:0')
c= tensor(4.2080e+09, device='cuda:0')
c= tensor(4.2080e+09, device='cuda:0')
c= tensor(4.2348e+09, device='cuda:0')
c= tensor(4.2353e+09, device='cuda:0')
c= tensor(4.2400e+09, device='cuda:0')
c= tensor(4.2416e+09, device='cuda:0')
c= tensor(4.2443e+09, device='cuda:0')
c= tensor(4.2509e+09, device='cuda:0')
c= tensor(4.2818e+09, device='cuda:0')
c= tensor(4.2930e+09, device='cuda:0')
c= tensor(4.2932e+09, device='cuda:0')
c= tensor(4.2988e+09, device='cuda:0')
c= tensor(4.2991e+09, device='cuda:0')
c= tensor(4.2992e+09, device='cuda:0')
c= tensor(4.2993e+09, device='cuda:0')
c= tensor(4.2994e+09, device='cuda:0')
c= tensor(4.3167e+09, device='cuda:0')
c= tensor(4.3169e+09, device='cuda:0')
c= tensor(4.3169e+09, device='cuda:0')
c= tensor(4.3170e+09, device='cuda:0')
c= tensor(4.3174e+09, device='cuda:0')
c= tensor(4.3213e+09, device='cuda:0')
c= tensor(4.3213e+09, device='cuda:0')
c= tensor(4.3214e+09, device='cuda:0')
c= tensor(4.3216e+09, device='cuda:0')
c= tensor(4.3225e+09, device='cuda:0')
c= tensor(4.3225e+09, device='cuda:0')
c= tensor(4.3225e+09, device='cuda:0')
c= tensor(4.3227e+09, device='cuda:0')
c= tensor(4.3353e+09, device='cuda:0')
c= tensor(4.3354e+09, device='cuda:0')
c= tensor(4.3354e+09, device='cuda:0')
c= tensor(4.3354e+09, device='cuda:0')
c= tensor(4.3620e+09, device='cuda:0')
c= tensor(4.3841e+09, device='cuda:0')
c= tensor(4.3852e+09, device='cuda:0')
c= tensor(4.3852e+09, device='cuda:0')
c= tensor(4.3949e+09, device='cuda:0')
c= tensor(4.3996e+09, device='cuda:0')
c= tensor(4.3996e+09, device='cuda:0')
c= tensor(4.3999e+09, device='cuda:0')
c= tensor(4.4002e+09, device='cuda:0')
c= tensor(4.4006e+09, device='cuda:0')
c= tensor(4.4022e+09, device='cuda:0')
c= tensor(4.4049e+09, device='cuda:0')
c= tensor(4.4050e+09, device='cuda:0')
c= tensor(4.4096e+09, device='cuda:0')
c= tensor(4.4096e+09, device='cuda:0')
c= tensor(4.4101e+09, device='cuda:0')
c= tensor(4.4103e+09, device='cuda:0')
c= tensor(4.4114e+09, device='cuda:0')
c= tensor(4.4133e+09, device='cuda:0')
c= tensor(4.4305e+09, device='cuda:0')
c= tensor(4.4306e+09, device='cuda:0')
c= tensor(4.4306e+09, device='cuda:0')
c= tensor(4.4306e+09, device='cuda:0')
c= tensor(4.4312e+09, device='cuda:0')
c= tensor(4.4315e+09, device='cuda:0')
c= tensor(4.4315e+09, device='cuda:0')
c= tensor(4.4315e+09, device='cuda:0')
c= tensor(4.4325e+09, device='cuda:0')
c= tensor(4.4325e+09, device='cuda:0')
c= tensor(4.4329e+09, device='cuda:0')
c= tensor(4.4329e+09, device='cuda:0')
c= tensor(4.4331e+09, device='cuda:0')
c= tensor(4.4331e+09, device='cuda:0')
c= tensor(4.4331e+09, device='cuda:0')
c= tensor(4.4332e+09, device='cuda:0')
c= tensor(4.4372e+09, device='cuda:0')
c= tensor(4.4564e+09, device='cuda:0')
c= tensor(4.4731e+09, device='cuda:0')
c= tensor(4.4817e+09, device='cuda:0')
c= tensor(4.4821e+09, device='cuda:0')
c= tensor(4.4822e+09, device='cuda:0')
c= tensor(4.4822e+09, device='cuda:0')
c= tensor(4.4862e+09, device='cuda:0')
c= tensor(4.4862e+09, device='cuda:0')
c= tensor(4.4867e+09, device='cuda:0')
c= tensor(4.4868e+09, device='cuda:0')
c= tensor(4.5428e+09, device='cuda:0')
c= tensor(4.5434e+09, device='cuda:0')
c= tensor(4.5439e+09, device='cuda:0')
c= tensor(4.5592e+09, device='cuda:0')
c= tensor(4.5597e+09, device='cuda:0')
c= tensor(4.5601e+09, device='cuda:0')
c= tensor(4.5750e+09, device='cuda:0')
c= tensor(4.5805e+09, device='cuda:0')
c= tensor(4.5815e+09, device='cuda:0')
c= tensor(4.5818e+09, device='cuda:0')
c= tensor(4.5824e+09, device='cuda:0')
c= tensor(4.5825e+09, device='cuda:0')
c= tensor(4.5847e+09, device='cuda:0')
c= tensor(4.6612e+09, device='cuda:0')
c= tensor(4.7161e+09, device='cuda:0')
c= tensor(4.7226e+09, device='cuda:0')
c= tensor(4.7227e+09, device='cuda:0')
c= tensor(4.7236e+09, device='cuda:0')
c= tensor(4.7237e+09, device='cuda:0')
c= tensor(4.7242e+09, device='cuda:0')
c= tensor(4.7252e+09, device='cuda:0')
c= tensor(4.7472e+09, device='cuda:0')
c= tensor(4.7472e+09, device='cuda:0')
c= tensor(4.7843e+09, device='cuda:0')
c= tensor(4.7874e+09, device='cuda:0')
c= tensor(4.7880e+09, device='cuda:0')
c= tensor(4.7880e+09, device='cuda:0')
c= tensor(4.7894e+09, device='cuda:0')
c= tensor(4.7942e+09, device='cuda:0')
c= tensor(4.7942e+09, device='cuda:0')
c= tensor(4.9173e+09, device='cuda:0')
c= tensor(4.9196e+09, device='cuda:0')
c= tensor(4.9245e+09, device='cuda:0')
c= tensor(4.9252e+09, device='cuda:0')
c= tensor(4.9252e+09, device='cuda:0')
c= tensor(4.9252e+09, device='cuda:0')
c= tensor(4.9253e+09, device='cuda:0')
c= tensor(4.9255e+09, device='cuda:0')
c= tensor(4.9396e+09, device='cuda:0')
c= tensor(6.8091e+09, device='cuda:0')
c= tensor(6.8091e+09, device='cuda:0')
c= tensor(6.8120e+09, device='cuda:0')
c= tensor(6.8121e+09, device='cuda:0')
c= tensor(6.8122e+09, device='cuda:0')
c= tensor(6.8124e+09, device='cuda:0')
c= tensor(6.8399e+09, device='cuda:0')
c= tensor(6.8401e+09, device='cuda:0')
c= tensor(7.0856e+09, device='cuda:0')
c= tensor(7.0856e+09, device='cuda:0')
c= tensor(7.0924e+09, device='cuda:0')
c= tensor(7.0941e+09, device='cuda:0')
c= tensor(7.0967e+09, device='cuda:0')
c= tensor(7.1132e+09, device='cuda:0')
c= tensor(7.1134e+09, device='cuda:0')
c= tensor(7.1134e+09, device='cuda:0')
c= tensor(7.1148e+09, device='cuda:0')
c= tensor(7.1148e+09, device='cuda:0')
c= tensor(7.1151e+09, device='cuda:0')
c= tensor(7.1192e+09, device='cuda:0')
c= tensor(7.1302e+09, device='cuda:0')
c= tensor(7.1307e+09, device='cuda:0')
c= tensor(7.1314e+09, device='cuda:0')
c= tensor(7.1548e+09, device='cuda:0')
c= tensor(7.2378e+09, device='cuda:0')
c= tensor(7.2382e+09, device='cuda:0')
c= tensor(7.2382e+09, device='cuda:0')
c= tensor(7.2655e+09, device='cuda:0')
c= tensor(7.2660e+09, device='cuda:0')
c= tensor(7.2668e+09, device='cuda:0')
c= tensor(7.2674e+09, device='cuda:0')
c= tensor(7.2703e+09, device='cuda:0')
c= tensor(7.2738e+09, device='cuda:0')
c= tensor(7.3178e+09, device='cuda:0')
c= tensor(7.3183e+09, device='cuda:0')
c= tensor(7.3184e+09, device='cuda:0')
c= tensor(7.3185e+09, device='cuda:0')
c= tensor(7.3188e+09, device='cuda:0')
c= tensor(7.3254e+09, device='cuda:0')
c= tensor(7.3285e+09, device='cuda:0')
c= tensor(7.3383e+09, device='cuda:0')
c= tensor(7.3383e+09, device='cuda:0')
c= tensor(7.3387e+09, device='cuda:0')
c= tensor(7.3404e+09, device='cuda:0')
c= tensor(7.3506e+09, device='cuda:0')
c= tensor(7.3512e+09, device='cuda:0')
c= tensor(7.3530e+09, device='cuda:0')
c= tensor(7.6109e+09, device='cuda:0')
c= tensor(7.6115e+09, device='cuda:0')
c= tensor(7.6115e+09, device='cuda:0')
c= tensor(7.6116e+09, device='cuda:0')
c= tensor(7.6199e+09, device='cuda:0')
c= tensor(7.6208e+09, device='cuda:0')
c= tensor(7.6218e+09, device='cuda:0')
c= tensor(7.6218e+09, device='cuda:0')
c= tensor(7.6218e+09, device='cuda:0')
c= tensor(7.6349e+09, device='cuda:0')
c= tensor(7.6368e+09, device='cuda:0')
c= tensor(7.6373e+09, device='cuda:0')
c= tensor(7.6373e+09, device='cuda:0')
c= tensor(7.6373e+09, device='cuda:0')
c= tensor(7.6376e+09, device='cuda:0')
c= tensor(7.6383e+09, device='cuda:0')
c= tensor(7.6414e+09, device='cuda:0')
c= tensor(7.6415e+09, device='cuda:0')
c= tensor(7.6561e+09, device='cuda:0')
c= tensor(7.6562e+09, device='cuda:0')
c= tensor(7.6567e+09, device='cuda:0')
c= tensor(7.6895e+09, device='cuda:0')
c= tensor(7.6897e+09, device='cuda:0')
c= tensor(7.6906e+09, device='cuda:0')
c= tensor(7.6957e+09, device='cuda:0')
c= tensor(7.6959e+09, device='cuda:0')
c= tensor(7.6969e+09, device='cuda:0')
c= tensor(7.7021e+09, device='cuda:0')
c= tensor(7.7021e+09, device='cuda:0')
c= tensor(7.7027e+09, device='cuda:0')
c= tensor(7.7029e+09, device='cuda:0')
c= tensor(7.7067e+09, device='cuda:0')
c= tensor(7.7108e+09, device='cuda:0')
c= tensor(7.7116e+09, device='cuda:0')
c= tensor(7.7142e+09, device='cuda:0')
c= tensor(7.7143e+09, device='cuda:0')
c= tensor(7.7152e+09, device='cuda:0')
c= tensor(7.7152e+09, device='cuda:0')
c= tensor(7.7153e+09, device='cuda:0')
c= tensor(7.7198e+09, device='cuda:0')
c= tensor(7.7446e+09, device='cuda:0')
c= tensor(7.7446e+09, device='cuda:0')
c= tensor(7.7449e+09, device='cuda:0')
c= tensor(7.7450e+09, device='cuda:0')
c= tensor(7.7784e+09, device='cuda:0')
c= tensor(7.7788e+09, device='cuda:0')
c= tensor(7.7789e+09, device='cuda:0')
c= tensor(7.7839e+09, device='cuda:0')
c= tensor(7.7867e+09, device='cuda:0')
c= tensor(7.7868e+09, device='cuda:0')
c= tensor(7.7878e+09, device='cuda:0')
c= tensor(7.7878e+09, device='cuda:0')
memory (bytes)
4881645568
time for making loss 2 is 12.878883838653564
p0 True
it  0 : 2225867776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 53% |
shape of L is 
torch.Size([])
memory (bytes)
4881842176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 16% |
memory (bytes)
4882477056
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  79891900000.0
relative error loss 10.258555
shape of L is 
torch.Size([])
memory (bytes)
5028642816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 17% |
memory (bytes)
5028786176
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  79891550000.0
relative error loss 10.258511
shape of L is 
torch.Size([])
memory (bytes)
5035233280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 17% |
memory (bytes)
5035261952
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% | 17% |
error is  79890694000.0
relative error loss 10.258401
shape of L is 
torch.Size([])
memory (bytes)
5037297664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 17% |
memory (bytes)
5037297664
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  79885250000.0
relative error loss 10.257701
shape of L is 
torch.Size([])
memory (bytes)
5039493120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 17% |
memory (bytes)
5039493120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  79846965000.0
relative error loss 10.252786
shape of L is 
torch.Size([])
memory (bytes)
5041467392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 17% |
memory (bytes)
5041590272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  79443900000.0
relative error loss 10.20103
shape of L is 
torch.Size([])
memory (bytes)
5043716096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 17% |
memory (bytes)
5043716096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  77452250000.0
relative error loss 9.945291
shape of L is 
torch.Size([])
memory (bytes)
5045837824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
5045837824
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  58258930000.0
relative error loss 7.4807644
shape of L is 
torch.Size([])
memory (bytes)
5047926784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 17% |
memory (bytes)
5047926784
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  19296928000.0
relative error loss 2.4778306
shape of L is 
torch.Size([])
memory (bytes)
5050019840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 17% |
memory (bytes)
5050159104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  11840271000.0
relative error loss 1.5203553
time to take a step is 211.50618052482605
it  1 : 2605075968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
5052309504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 17% |
memory (bytes)
5052309504
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  11840271000.0
relative error loss 1.5203553
shape of L is 
torch.Size([])
memory (bytes)
5054218240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 17% |
memory (bytes)
5054431232
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  11643007000.0
relative error loss 1.4950256
shape of L is 
torch.Size([])
memory (bytes)
5056577536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 17% |
memory (bytes)
5056577536
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  9613898000.0
relative error loss 1.2344769
shape of L is 
torch.Size([])
memory (bytes)
5058682880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 17% |
memory (bytes)
5058682880
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  8133481000.0
relative error loss 1.0443833
shape of L is 
torch.Size([])
memory (bytes)
5060624384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 17% |
memory (bytes)
5060624384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  7563574300.0
relative error loss 0.97120416
shape of L is 
torch.Size([])
memory (bytes)
5062823936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 17% |
memory (bytes)
5062823936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% | 17% |
error is  69330880000.0
relative error loss 8.902463
shape of L is 
torch.Size([])
memory (bytes)
5064994816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 17% |
memory (bytes)
5064994816
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  7494015000.0
relative error loss 0.9622724
shape of L is 
torch.Size([])
memory (bytes)
5067268096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 17% |
memory (bytes)
5067268096
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  7154047500.0
relative error loss 0.9186187
shape of L is 
torch.Size([])
memory (bytes)
5069017088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 17% |
memory (bytes)
5069365248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% | 17% |
error is  6921198000.0
relative error loss 0.88871956
shape of L is 
torch.Size([])
memory (bytes)
5071290368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 17% |
memory (bytes)
5071290368
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  6542008300.0
relative error loss 0.84002954
time to take a step is 203.82512164115906
it  2 : 2712890880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
5073465344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 17% |
memory (bytes)
5073465344
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  6542008300.0
relative error loss 0.84002954
shape of L is 
torch.Size([])
memory (bytes)
5075660800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 17% |
memory (bytes)
5075660800
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  6211214300.0
relative error loss 0.7975538
shape of L is 
torch.Size([])
memory (bytes)
5077839872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 17% |
memory (bytes)
5077839872
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  6036260000.0
relative error loss 0.77508867
shape of L is 
torch.Size([])
memory (bytes)
5079900160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 17% |
memory (bytes)
5079900160
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  5823246300.0
relative error loss 0.7477366
shape of L is 
torch.Size([])
memory (bytes)
5082062848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 17% |
memory (bytes)
5082062848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  5459030000.0
relative error loss 0.7009692
shape of L is 
torch.Size([])
memory (bytes)
5083996160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 17% |
memory (bytes)
5083996160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  5114033000.0
relative error loss 0.65666974
shape of L is 
torch.Size([])
memory (bytes)
5086232576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 17% |
memory (bytes)
5086310400
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  4772745000.0
relative error loss 0.6128465
shape of L is 
torch.Size([])
memory (bytes)
5088444416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 17% |
memory (bytes)
5088444416
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  4481393700.0
relative error loss 0.5754354
shape of L is 
torch.Size([])
memory (bytes)
5090480128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 17% |
memory (bytes)
5090582528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  4702464500.0
relative error loss 0.6038221
shape of L is 
torch.Size([])
memory (bytes)
5092663296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 17% |
memory (bytes)
5092663296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  4110732500.0
relative error loss 0.5278405
time to take a step is 205.02170538902283
it  3 : 2712891392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
5094756352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 17% |
memory (bytes)
5094871040
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  4110732500.0
relative error loss 0.5278405
shape of L is 
torch.Size([])
memory (bytes)
5097013248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 17% |
memory (bytes)
5097013248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  3852548900.0
relative error loss 0.49468827
shape of L is 
torch.Size([])
memory (bytes)
5098934272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 17% |
memory (bytes)
5098934272
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3569175800.0
relative error loss 0.45830163
shape of L is 
torch.Size([])
memory (bytes)
5101154304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 17% |
memory (bytes)
5101154304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  3230251000.0
relative error loss 0.41478184
shape of L is 
torch.Size([])
memory (bytes)
5103185920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 17% |
memory (bytes)
5103185920
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2996626400.0
relative error loss 0.38478318
shape of L is 
torch.Size([])
memory (bytes)
5105344512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 17% |
memory (bytes)
5105344512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  2732989400.0
relative error loss 0.35093075
shape of L is 
torch.Size([])
memory (bytes)
5107396608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 17% |
memory (bytes)
5107396608
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2483334100.0
relative error loss 0.31887364
shape of L is 
torch.Size([])
memory (bytes)
5109755904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 17% |
memory (bytes)
5109755904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  2302502400.0
relative error loss 0.29565385
shape of L is 
torch.Size([])
memory (bytes)
5111738368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 17% |
memory (bytes)
5111738368
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2105228800.0
relative error loss 0.27032286
shape of L is 
torch.Size([])
memory (bytes)
5114138624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  8% | 17% |
memory (bytes)
5114138624
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  1901168100.0
relative error loss 0.24412036
time to take a step is 205.51139497756958
c= tensor(1556.0684, device='cuda:0')
c= tensor(88450.0234, device='cuda:0')
c= tensor(93405.2734, device='cuda:0')
c= tensor(96790.8594, device='cuda:0')
c= tensor(1009588.3750, device='cuda:0')
c= tensor(1801832., device='cuda:0')
c= tensor(2960876.2500, device='cuda:0')
c= tensor(3520757.2500, device='cuda:0')
c= tensor(3551205., device='cuda:0')
c= tensor(38127144., device='cuda:0')
c= tensor(38166188., device='cuda:0')
c= tensor(41586860., device='cuda:0')
c= tensor(41602232., device='cuda:0')
c= tensor(60200864., device='cuda:0')
c= tensor(60439500., device='cuda:0')
c= tensor(62643928., device='cuda:0')
c= tensor(64462772., device='cuda:0')
c= tensor(65238656., device='cuda:0')
c= tensor(87884544., device='cuda:0')
c= tensor(92874600., device='cuda:0')
c= tensor(93020120., device='cuda:0')
c= tensor(1.7253e+08, device='cuda:0')
c= tensor(1.7264e+08, device='cuda:0')
c= tensor(1.7270e+08, device='cuda:0')
c= tensor(1.7351e+08, device='cuda:0')
c= tensor(1.7491e+08, device='cuda:0')
c= tensor(1.7577e+08, device='cuda:0')
c= tensor(1.7586e+08, device='cuda:0')
c= tensor(1.7825e+08, device='cuda:0')
c= tensor(9.1226e+08, device='cuda:0')
c= tensor(9.1230e+08, device='cuda:0')
c= tensor(1.0152e+09, device='cuda:0')
c= tensor(1.0152e+09, device='cuda:0')
c= tensor(1.0153e+09, device='cuda:0')
c= tensor(1.0153e+09, device='cuda:0')
c= tensor(1.0211e+09, device='cuda:0')
c= tensor(1.0230e+09, device='cuda:0')
c= tensor(1.0230e+09, device='cuda:0')
c= tensor(1.0230e+09, device='cuda:0')
c= tensor(1.0230e+09, device='cuda:0')
c= tensor(1.0230e+09, device='cuda:0')
c= tensor(1.0230e+09, device='cuda:0')
c= tensor(1.0230e+09, device='cuda:0')
c= tensor(1.0230e+09, device='cuda:0')
c= tensor(1.0230e+09, device='cuda:0')
c= tensor(1.0230e+09, device='cuda:0')
c= tensor(1.0231e+09, device='cuda:0')
c= tensor(1.0231e+09, device='cuda:0')
c= tensor(1.0231e+09, device='cuda:0')
c= tensor(1.0231e+09, device='cuda:0')
c= tensor(1.0232e+09, device='cuda:0')
c= tensor(1.0232e+09, device='cuda:0')
c= tensor(1.0232e+09, device='cuda:0')
c= tensor(1.0232e+09, device='cuda:0')
c= tensor(1.0233e+09, device='cuda:0')
c= tensor(1.0233e+09, device='cuda:0')
c= tensor(1.0233e+09, device='cuda:0')
c= tensor(1.0233e+09, device='cuda:0')
c= tensor(1.0233e+09, device='cuda:0')
c= tensor(1.0233e+09, device='cuda:0')
c= tensor(1.0233e+09, device='cuda:0')
c= tensor(1.0233e+09, device='cuda:0')
c= tensor(1.0233e+09, device='cuda:0')
c= tensor(1.0234e+09, device='cuda:0')
c= tensor(1.0234e+09, device='cuda:0')
c= tensor(1.0234e+09, device='cuda:0')
c= tensor(1.0234e+09, device='cuda:0')
c= tensor(1.0234e+09, device='cuda:0')
c= tensor(1.0234e+09, device='cuda:0')
c= tensor(1.0234e+09, device='cuda:0')
c= tensor(1.0234e+09, device='cuda:0')
c= tensor(1.0234e+09, device='cuda:0')
c= tensor(1.0234e+09, device='cuda:0')
c= tensor(1.0234e+09, device='cuda:0')
c= tensor(1.0235e+09, device='cuda:0')
c= tensor(1.0235e+09, device='cuda:0')
c= tensor(1.0235e+09, device='cuda:0')
c= tensor(1.0235e+09, device='cuda:0')
c= tensor(1.0235e+09, device='cuda:0')
c= tensor(1.0235e+09, device='cuda:0')
c= tensor(1.0235e+09, device='cuda:0')
c= tensor(1.0235e+09, device='cuda:0')
c= tensor(1.0236e+09, device='cuda:0')
c= tensor(1.0236e+09, device='cuda:0')
c= tensor(1.0236e+09, device='cuda:0')
c= tensor(1.0236e+09, device='cuda:0')
c= tensor(1.0236e+09, device='cuda:0')
c= tensor(1.0236e+09, device='cuda:0')
c= tensor(1.0236e+09, device='cuda:0')
c= tensor(1.0236e+09, device='cuda:0')
c= tensor(1.0236e+09, device='cuda:0')
c= tensor(1.0236e+09, device='cuda:0')
c= tensor(1.0236e+09, device='cuda:0')
c= tensor(1.0236e+09, device='cuda:0')
c= tensor(1.0236e+09, device='cuda:0')
c= tensor(1.0236e+09, device='cuda:0')
c= tensor(1.0237e+09, device='cuda:0')
c= tensor(1.0237e+09, device='cuda:0')
c= tensor(1.0237e+09, device='cuda:0')
c= tensor(1.0237e+09, device='cuda:0')
c= tensor(1.0237e+09, device='cuda:0')
c= tensor(1.0237e+09, device='cuda:0')
c= tensor(1.0238e+09, device='cuda:0')
c= tensor(1.0238e+09, device='cuda:0')
c= tensor(1.0238e+09, device='cuda:0')
c= tensor(1.0238e+09, device='cuda:0')
c= tensor(1.0238e+09, device='cuda:0')
c= tensor(1.0238e+09, device='cuda:0')
c= tensor(1.0238e+09, device='cuda:0')
c= tensor(1.0238e+09, device='cuda:0')
c= tensor(1.0238e+09, device='cuda:0')
c= tensor(1.0238e+09, device='cuda:0')
c= tensor(1.0238e+09, device='cuda:0')
c= tensor(1.0238e+09, device='cuda:0')
c= tensor(1.0238e+09, device='cuda:0')
c= tensor(1.0238e+09, device='cuda:0')
c= tensor(1.0238e+09, device='cuda:0')
c= tensor(1.0238e+09, device='cuda:0')
c= tensor(1.0239e+09, device='cuda:0')
c= tensor(1.0239e+09, device='cuda:0')
c= tensor(1.0239e+09, device='cuda:0')
c= tensor(1.0239e+09, device='cuda:0')
c= tensor(1.0239e+09, device='cuda:0')
c= tensor(1.0240e+09, device='cuda:0')
c= tensor(1.0240e+09, device='cuda:0')
c= tensor(1.0240e+09, device='cuda:0')
c= tensor(1.0240e+09, device='cuda:0')
c= tensor(1.0240e+09, device='cuda:0')
c= tensor(1.0240e+09, device='cuda:0')
c= tensor(1.0241e+09, device='cuda:0')
c= tensor(1.0241e+09, device='cuda:0')
c= tensor(1.0241e+09, device='cuda:0')
c= tensor(1.0241e+09, device='cuda:0')
c= tensor(1.0241e+09, device='cuda:0')
c= tensor(1.0241e+09, device='cuda:0')
c= tensor(1.0241e+09, device='cuda:0')
c= tensor(1.0241e+09, device='cuda:0')
c= tensor(1.0241e+09, device='cuda:0')
c= tensor(1.0241e+09, device='cuda:0')
c= tensor(1.0241e+09, device='cuda:0')
c= tensor(1.0241e+09, device='cuda:0')
c= tensor(1.0241e+09, device='cuda:0')
c= tensor(1.0242e+09, device='cuda:0')
c= tensor(1.0242e+09, device='cuda:0')
c= tensor(1.0242e+09, device='cuda:0')
c= tensor(1.0242e+09, device='cuda:0')
c= tensor(1.0242e+09, device='cuda:0')
c= tensor(1.0242e+09, device='cuda:0')
c= tensor(1.0242e+09, device='cuda:0')
c= tensor(1.0242e+09, device='cuda:0')
c= tensor(1.0243e+09, device='cuda:0')
c= tensor(1.0243e+09, device='cuda:0')
c= tensor(1.0243e+09, device='cuda:0')
c= tensor(1.0243e+09, device='cuda:0')
c= tensor(1.0243e+09, device='cuda:0')
c= tensor(1.0244e+09, device='cuda:0')
c= tensor(1.0244e+09, device='cuda:0')
c= tensor(1.0244e+09, device='cuda:0')
c= tensor(1.0244e+09, device='cuda:0')
c= tensor(1.0244e+09, device='cuda:0')
c= tensor(1.0244e+09, device='cuda:0')
c= tensor(1.0244e+09, device='cuda:0')
c= tensor(1.0245e+09, device='cuda:0')
c= tensor(1.0245e+09, device='cuda:0')
c= tensor(1.0245e+09, device='cuda:0')
c= tensor(1.0245e+09, device='cuda:0')
c= tensor(1.0245e+09, device='cuda:0')
c= tensor(1.0245e+09, device='cuda:0')
c= tensor(1.0245e+09, device='cuda:0')
c= tensor(1.0245e+09, device='cuda:0')
c= tensor(1.0245e+09, device='cuda:0')
c= tensor(1.0245e+09, device='cuda:0')
c= tensor(1.0245e+09, device='cuda:0')
c= tensor(1.0245e+09, device='cuda:0')
c= tensor(1.0245e+09, device='cuda:0')
c= tensor(1.0246e+09, device='cuda:0')
c= tensor(1.0246e+09, device='cuda:0')
c= tensor(1.0246e+09, device='cuda:0')
c= tensor(1.0246e+09, device='cuda:0')
c= tensor(1.0246e+09, device='cuda:0')
c= tensor(1.0246e+09, device='cuda:0')
c= tensor(1.0246e+09, device='cuda:0')
c= tensor(1.0246e+09, device='cuda:0')
c= tensor(1.0246e+09, device='cuda:0')
c= tensor(1.0247e+09, device='cuda:0')
c= tensor(1.0247e+09, device='cuda:0')
c= tensor(1.0247e+09, device='cuda:0')
c= tensor(1.0247e+09, device='cuda:0')
c= tensor(1.0247e+09, device='cuda:0')
c= tensor(1.0249e+09, device='cuda:0')
c= tensor(1.0249e+09, device='cuda:0')
c= tensor(1.0249e+09, device='cuda:0')
c= tensor(1.0250e+09, device='cuda:0')
c= tensor(1.0250e+09, device='cuda:0')
c= tensor(1.0250e+09, device='cuda:0')
c= tensor(1.0250e+09, device='cuda:0')
c= tensor(1.0250e+09, device='cuda:0')
c= tensor(1.0250e+09, device='cuda:0')
c= tensor(1.0250e+09, device='cuda:0')
c= tensor(1.0250e+09, device='cuda:0')
c= tensor(1.0250e+09, device='cuda:0')
c= tensor(1.0250e+09, device='cuda:0')
c= tensor(1.0250e+09, device='cuda:0')
c= tensor(1.0250e+09, device='cuda:0')
c= tensor(1.0250e+09, device='cuda:0')
c= tensor(1.0250e+09, device='cuda:0')
c= tensor(1.0251e+09, device='cuda:0')
c= tensor(1.0251e+09, device='cuda:0')
c= tensor(1.0251e+09, device='cuda:0')
c= tensor(1.0251e+09, device='cuda:0')
c= tensor(1.0251e+09, device='cuda:0')
c= tensor(1.0251e+09, device='cuda:0')
c= tensor(1.0251e+09, device='cuda:0')
c= tensor(1.0251e+09, device='cuda:0')
c= tensor(1.0251e+09, device='cuda:0')
c= tensor(1.0251e+09, device='cuda:0')
c= tensor(1.0251e+09, device='cuda:0')
c= tensor(1.0251e+09, device='cuda:0')
c= tensor(1.0251e+09, device='cuda:0')
c= tensor(1.0251e+09, device='cuda:0')
c= tensor(1.0252e+09, device='cuda:0')
c= tensor(1.0252e+09, device='cuda:0')
c= tensor(1.0252e+09, device='cuda:0')
c= tensor(1.0252e+09, device='cuda:0')
c= tensor(1.0252e+09, device='cuda:0')
c= tensor(1.0252e+09, device='cuda:0')
c= tensor(1.0252e+09, device='cuda:0')
c= tensor(1.0252e+09, device='cuda:0')
c= tensor(1.0252e+09, device='cuda:0')
c= tensor(1.0252e+09, device='cuda:0')
c= tensor(1.0252e+09, device='cuda:0')
c= tensor(1.0252e+09, device='cuda:0')
c= tensor(1.0252e+09, device='cuda:0')
c= tensor(1.0253e+09, device='cuda:0')
c= tensor(1.0253e+09, device='cuda:0')
c= tensor(1.0253e+09, device='cuda:0')
c= tensor(1.0253e+09, device='cuda:0')
c= tensor(1.0253e+09, device='cuda:0')
c= tensor(1.0253e+09, device='cuda:0')
c= tensor(1.0253e+09, device='cuda:0')
c= tensor(1.0271e+09, device='cuda:0')
c= tensor(1.0273e+09, device='cuda:0')
c= tensor(1.0273e+09, device='cuda:0')
c= tensor(1.0273e+09, device='cuda:0')
c= tensor(1.0274e+09, device='cuda:0')
c= tensor(1.0301e+09, device='cuda:0')
c= tensor(1.0668e+09, device='cuda:0')
c= tensor(1.0668e+09, device='cuda:0')
c= tensor(1.0699e+09, device='cuda:0')
c= tensor(1.0703e+09, device='cuda:0')
c= tensor(1.0720e+09, device='cuda:0')
c= tensor(1.1351e+09, device='cuda:0')
c= tensor(1.1351e+09, device='cuda:0')
c= tensor(1.1351e+09, device='cuda:0')
c= tensor(1.1464e+09, device='cuda:0')
c= tensor(1.2162e+09, device='cuda:0')
c= tensor(1.2162e+09, device='cuda:0')
c= tensor(1.2166e+09, device='cuda:0')
c= tensor(1.4082e+09, device='cuda:0')
c= tensor(1.4151e+09, device='cuda:0')
c= tensor(1.4198e+09, device='cuda:0')
c= tensor(1.4204e+09, device='cuda:0')
c= tensor(1.4218e+09, device='cuda:0')
c= tensor(1.4219e+09, device='cuda:0')
c= tensor(1.4219e+09, device='cuda:0')
c= tensor(1.5207e+09, device='cuda:0')
c= tensor(1.5208e+09, device='cuda:0')
c= tensor(1.5208e+09, device='cuda:0')
c= tensor(1.5215e+09, device='cuda:0')
c= tensor(1.5220e+09, device='cuda:0')
c= tensor(1.5552e+09, device='cuda:0')
c= tensor(1.5577e+09, device='cuda:0')
c= tensor(1.5577e+09, device='cuda:0')
c= tensor(1.5584e+09, device='cuda:0')
c= tensor(1.5585e+09, device='cuda:0')
c= tensor(1.5589e+09, device='cuda:0')
c= tensor(1.5615e+09, device='cuda:0')
c= tensor(1.5615e+09, device='cuda:0')
c= tensor(1.5623e+09, device='cuda:0')
c= tensor(1.5623e+09, device='cuda:0')
c= tensor(1.5623e+09, device='cuda:0')
c= tensor(1.5654e+09, device='cuda:0')
c= tensor(1.5668e+09, device='cuda:0')
c= tensor(1.5672e+09, device='cuda:0')
c= tensor(1.5717e+09, device='cuda:0')
c= tensor(1.6090e+09, device='cuda:0')
c= tensor(1.6091e+09, device='cuda:0')
c= tensor(1.6093e+09, device='cuda:0')
c= tensor(1.6112e+09, device='cuda:0')
c= tensor(1.6112e+09, device='cuda:0')
c= tensor(1.6171e+09, device='cuda:0')
c= tensor(1.6475e+09, device='cuda:0')
c= tensor(1.7059e+09, device='cuda:0')
c= tensor(1.7061e+09, device='cuda:0')
c= tensor(1.7063e+09, device='cuda:0')
c= tensor(1.7063e+09, device='cuda:0')
c= tensor(1.7064e+09, device='cuda:0')
c= tensor(1.7074e+09, device='cuda:0')
c= tensor(1.7074e+09, device='cuda:0')
c= tensor(1.7077e+09, device='cuda:0')
c= tensor(1.7318e+09, device='cuda:0')
c= tensor(1.7342e+09, device='cuda:0')
c= tensor(1.7342e+09, device='cuda:0')
c= tensor(1.7343e+09, device='cuda:0')
c= tensor(1.7545e+09, device='cuda:0')
c= tensor(1.7547e+09, device='cuda:0')
c= tensor(1.7547e+09, device='cuda:0')
c= tensor(1.7548e+09, device='cuda:0')
c= tensor(2.0443e+09, device='cuda:0')
c= tensor(2.0444e+09, device='cuda:0')
c= tensor(2.0876e+09, device='cuda:0')
c= tensor(2.0876e+09, device='cuda:0')
c= tensor(2.0932e+09, device='cuda:0')
c= tensor(2.0951e+09, device='cuda:0')
c= tensor(2.2268e+09, device='cuda:0')
c= tensor(2.2282e+09, device='cuda:0')
c= tensor(2.2282e+09, device='cuda:0')
c= tensor(2.2342e+09, device='cuda:0')
c= tensor(2.2369e+09, device='cuda:0')
c= tensor(2.2371e+09, device='cuda:0')
c= tensor(2.2382e+09, device='cuda:0')
c= tensor(2.2442e+09, device='cuda:0')
c= tensor(2.2651e+09, device='cuda:0')
c= tensor(2.2680e+09, device='cuda:0')
c= tensor(2.2680e+09, device='cuda:0')
c= tensor(2.2681e+09, device='cuda:0')
c= tensor(2.2814e+09, device='cuda:0')
c= tensor(2.3195e+09, device='cuda:0')
c= tensor(2.3196e+09, device='cuda:0')
c= tensor(2.3196e+09, device='cuda:0')
c= tensor(2.3202e+09, device='cuda:0')
c= tensor(2.3234e+09, device='cuda:0')
c= tensor(2.3376e+09, device='cuda:0')
c= tensor(2.3376e+09, device='cuda:0')
c= tensor(2.3381e+09, device='cuda:0')
c= tensor(2.3382e+09, device='cuda:0')
c= tensor(2.3382e+09, device='cuda:0')
c= tensor(2.3382e+09, device='cuda:0')
c= tensor(2.3383e+09, device='cuda:0')
c= tensor(2.3516e+09, device='cuda:0')
c= tensor(2.3521e+09, device='cuda:0')
c= tensor(2.3523e+09, device='cuda:0')
c= tensor(2.3526e+09, device='cuda:0')
c= tensor(2.3527e+09, device='cuda:0')
c= tensor(2.4313e+09, device='cuda:0')
c= tensor(2.4313e+09, device='cuda:0')
c= tensor(2.4369e+09, device='cuda:0')
c= tensor(2.4369e+09, device='cuda:0')
c= tensor(2.4369e+09, device='cuda:0')
c= tensor(2.4369e+09, device='cuda:0')
c= tensor(2.4377e+09, device='cuda:0')
c= tensor(2.4377e+09, device='cuda:0')
c= tensor(2.4398e+09, device='cuda:0')
c= tensor(2.4398e+09, device='cuda:0')
c= tensor(2.4398e+09, device='cuda:0')
c= tensor(2.4660e+09, device='cuda:0')
c= tensor(2.4665e+09, device='cuda:0')
c= tensor(2.4669e+09, device='cuda:0')
c= tensor(2.4733e+09, device='cuda:0')
c= tensor(2.4861e+09, device='cuda:0')
c= tensor(2.4861e+09, device='cuda:0')
c= tensor(2.4861e+09, device='cuda:0')
c= tensor(2.4865e+09, device='cuda:0')
c= tensor(2.4865e+09, device='cuda:0')
c= tensor(2.4865e+09, device='cuda:0')
c= tensor(2.4867e+09, device='cuda:0')
c= tensor(2.4867e+09, device='cuda:0')
c= tensor(2.4867e+09, device='cuda:0')
c= tensor(2.4867e+09, device='cuda:0')
c= tensor(2.4867e+09, device='cuda:0')
c= tensor(2.5392e+09, device='cuda:0')
c= tensor(2.5392e+09, device='cuda:0')
c= tensor(2.5412e+09, device='cuda:0')
c= tensor(2.5413e+09, device='cuda:0')
c= tensor(2.5418e+09, device='cuda:0')
c= tensor(2.5424e+09, device='cuda:0')
c= tensor(3.0094e+09, device='cuda:0')
c= tensor(3.1132e+09, device='cuda:0')
c= tensor(3.1146e+09, device='cuda:0')
c= tensor(3.1174e+09, device='cuda:0')
c= tensor(3.1174e+09, device='cuda:0')
c= tensor(3.1298e+09, device='cuda:0')
c= tensor(3.1304e+09, device='cuda:0')
c= tensor(3.1461e+09, device='cuda:0')
c= tensor(3.1462e+09, device='cuda:0')
c= tensor(3.1489e+09, device='cuda:0')
c= tensor(3.2102e+09, device='cuda:0')
c= tensor(3.2115e+09, device='cuda:0')
c= tensor(3.2115e+09, device='cuda:0')
c= tensor(3.2127e+09, device='cuda:0')
c= tensor(3.2128e+09, device='cuda:0')
c= tensor(3.2128e+09, device='cuda:0')
c= tensor(3.2198e+09, device='cuda:0')
c= tensor(3.2201e+09, device='cuda:0')
c= tensor(3.2201e+09, device='cuda:0')
c= tensor(3.2231e+09, device='cuda:0')
c= tensor(3.2231e+09, device='cuda:0')
c= tensor(3.2231e+09, device='cuda:0')
c= tensor(3.2302e+09, device='cuda:0')
c= tensor(3.2325e+09, device='cuda:0')
c= tensor(3.2329e+09, device='cuda:0')
c= tensor(3.2388e+09, device='cuda:0')
c= tensor(3.2441e+09, device='cuda:0')
c= tensor(3.2442e+09, device='cuda:0')
c= tensor(3.2450e+09, device='cuda:0')
c= tensor(3.2490e+09, device='cuda:0')
c= tensor(3.2530e+09, device='cuda:0')
c= tensor(3.2531e+09, device='cuda:0')
c= tensor(3.3088e+09, device='cuda:0')
c= tensor(3.3698e+09, device='cuda:0')
c= tensor(3.3737e+09, device='cuda:0')
c= tensor(3.3750e+09, device='cuda:0')
c= tensor(3.3769e+09, device='cuda:0')
c= tensor(3.3770e+09, device='cuda:0')
c= tensor(3.3770e+09, device='cuda:0')
c= tensor(3.3771e+09, device='cuda:0')
c= tensor(3.3831e+09, device='cuda:0')
c= tensor(3.3894e+09, device='cuda:0')
c= tensor(3.4326e+09, device='cuda:0')
c= tensor(3.4524e+09, device='cuda:0')
c= tensor(3.4541e+09, device='cuda:0')
c= tensor(3.4544e+09, device='cuda:0')
c= tensor(3.4560e+09, device='cuda:0')
c= tensor(3.4560e+09, device='cuda:0')
c= tensor(3.4561e+09, device='cuda:0')
c= tensor(3.4668e+09, device='cuda:0')
c= tensor(3.4670e+09, device='cuda:0')
c= tensor(3.4670e+09, device='cuda:0')
c= tensor(3.4671e+09, device='cuda:0')
c= tensor(3.4690e+09, device='cuda:0')
c= tensor(3.4693e+09, device='cuda:0')
c= tensor(3.4723e+09, device='cuda:0')
c= tensor(3.4738e+09, device='cuda:0')
c= tensor(3.4743e+09, device='cuda:0')
c= tensor(3.4743e+09, device='cuda:0')
c= tensor(3.4743e+09, device='cuda:0')
c= tensor(3.4753e+09, device='cuda:0')
c= tensor(3.4766e+09, device='cuda:0')
c= tensor(3.4768e+09, device='cuda:0')
c= tensor(3.4821e+09, device='cuda:0')
c= tensor(3.4821e+09, device='cuda:0')
c= tensor(3.4822e+09, device='cuda:0')
c= tensor(3.4822e+09, device='cuda:0')
c= tensor(3.4856e+09, device='cuda:0')
c= tensor(3.4857e+09, device='cuda:0')
c= tensor(3.4867e+09, device='cuda:0')
c= tensor(3.4868e+09, device='cuda:0')
c= tensor(3.4870e+09, device='cuda:0')
c= tensor(3.4880e+09, device='cuda:0')
c= tensor(3.5393e+09, device='cuda:0')
c= tensor(3.5394e+09, device='cuda:0')
c= tensor(3.5395e+09, device='cuda:0')
c= tensor(3.5463e+09, device='cuda:0')
c= tensor(3.5464e+09, device='cuda:0')
c= tensor(3.6153e+09, device='cuda:0')
c= tensor(3.6153e+09, device='cuda:0')
c= tensor(3.6185e+09, device='cuda:0')
c= tensor(3.6391e+09, device='cuda:0')
c= tensor(3.6391e+09, device='cuda:0')
c= tensor(3.6611e+09, device='cuda:0')
c= tensor(3.6620e+09, device='cuda:0')
c= tensor(3.7495e+09, device='cuda:0')
c= tensor(3.7495e+09, device='cuda:0')
c= tensor(3.7506e+09, device='cuda:0')
c= tensor(3.7507e+09, device='cuda:0')
c= tensor(3.7507e+09, device='cuda:0')
c= tensor(3.7507e+09, device='cuda:0')
c= tensor(3.7543e+09, device='cuda:0')
c= tensor(3.7546e+09, device='cuda:0')
c= tensor(3.7595e+09, device='cuda:0')
c= tensor(3.7596e+09, device='cuda:0')
c= tensor(3.7597e+09, device='cuda:0')
c= tensor(3.7597e+09, device='cuda:0')
c= tensor(3.7718e+09, device='cuda:0')
c= tensor(3.7733e+09, device='cuda:0')
c= tensor(3.8075e+09, device='cuda:0')
c= tensor(3.8085e+09, device='cuda:0')
c= tensor(3.8086e+09, device='cuda:0')
c= tensor(3.8086e+09, device='cuda:0')
c= tensor(3.8089e+09, device='cuda:0')
c= tensor(4.1998e+09, device='cuda:0')
c= tensor(4.1999e+09, device='cuda:0')
c= tensor(4.2000e+09, device='cuda:0')
c= tensor(4.2061e+09, device='cuda:0')
c= tensor(4.2080e+09, device='cuda:0')
c= tensor(4.2080e+09, device='cuda:0')
c= tensor(4.2080e+09, device='cuda:0')
c= tensor(4.2348e+09, device='cuda:0')
c= tensor(4.2353e+09, device='cuda:0')
c= tensor(4.2400e+09, device='cuda:0')
c= tensor(4.2416e+09, device='cuda:0')
c= tensor(4.2443e+09, device='cuda:0')
c= tensor(4.2509e+09, device='cuda:0')
c= tensor(4.2818e+09, device='cuda:0')
c= tensor(4.2930e+09, device='cuda:0')
c= tensor(4.2932e+09, device='cuda:0')
c= tensor(4.2988e+09, device='cuda:0')
c= tensor(4.2991e+09, device='cuda:0')
c= tensor(4.2992e+09, device='cuda:0')
c= tensor(4.2993e+09, device='cuda:0')
c= tensor(4.2994e+09, device='cuda:0')
c= tensor(4.3167e+09, device='cuda:0')
c= tensor(4.3169e+09, device='cuda:0')
c= tensor(4.3169e+09, device='cuda:0')
c= tensor(4.3170e+09, device='cuda:0')
c= tensor(4.3174e+09, device='cuda:0')
c= tensor(4.3213e+09, device='cuda:0')
c= tensor(4.3213e+09, device='cuda:0')
c= tensor(4.3214e+09, device='cuda:0')
c= tensor(4.3216e+09, device='cuda:0')
c= tensor(4.3225e+09, device='cuda:0')
c= tensor(4.3225e+09, device='cuda:0')
c= tensor(4.3225e+09, device='cuda:0')
c= tensor(4.3227e+09, device='cuda:0')
c= tensor(4.3353e+09, device='cuda:0')
c= tensor(4.3354e+09, device='cuda:0')
c= tensor(4.3354e+09, device='cuda:0')
c= tensor(4.3354e+09, device='cuda:0')
c= tensor(4.3620e+09, device='cuda:0')
c= tensor(4.3841e+09, device='cuda:0')
c= tensor(4.3852e+09, device='cuda:0')
c= tensor(4.3852e+09, device='cuda:0')
c= tensor(4.3949e+09, device='cuda:0')
c= tensor(4.3996e+09, device='cuda:0')
c= tensor(4.3996e+09, device='cuda:0')
c= tensor(4.3999e+09, device='cuda:0')
c= tensor(4.4002e+09, device='cuda:0')
c= tensor(4.4006e+09, device='cuda:0')
c= tensor(4.4022e+09, device='cuda:0')
c= tensor(4.4049e+09, device='cuda:0')
c= tensor(4.4050e+09, device='cuda:0')
c= tensor(4.4096e+09, device='cuda:0')
c= tensor(4.4096e+09, device='cuda:0')
c= tensor(4.4101e+09, device='cuda:0')
c= tensor(4.4103e+09, device='cuda:0')
c= tensor(4.4114e+09, device='cuda:0')
c= tensor(4.4133e+09, device='cuda:0')
c= tensor(4.4305e+09, device='cuda:0')
c= tensor(4.4306e+09, device='cuda:0')
c= tensor(4.4306e+09, device='cuda:0')
c= tensor(4.4306e+09, device='cuda:0')
c= tensor(4.4312e+09, device='cuda:0')
c= tensor(4.4315e+09, device='cuda:0')
c= tensor(4.4315e+09, device='cuda:0')
c= tensor(4.4315e+09, device='cuda:0')
c= tensor(4.4325e+09, device='cuda:0')
c= tensor(4.4325e+09, device='cuda:0')
c= tensor(4.4329e+09, device='cuda:0')
c= tensor(4.4329e+09, device='cuda:0')
c= tensor(4.4331e+09, device='cuda:0')
c= tensor(4.4331e+09, device='cuda:0')
c= tensor(4.4331e+09, device='cuda:0')
c= tensor(4.4332e+09, device='cuda:0')
c= tensor(4.4372e+09, device='cuda:0')
c= tensor(4.4564e+09, device='cuda:0')
c= tensor(4.4731e+09, device='cuda:0')
c= tensor(4.4817e+09, device='cuda:0')
c= tensor(4.4821e+09, device='cuda:0')
c= tensor(4.4822e+09, device='cuda:0')
c= tensor(4.4822e+09, device='cuda:0')
c= tensor(4.4862e+09, device='cuda:0')
c= tensor(4.4862e+09, device='cuda:0')
c= tensor(4.4867e+09, device='cuda:0')
c= tensor(4.4868e+09, device='cuda:0')
c= tensor(4.5428e+09, device='cuda:0')
c= tensor(4.5434e+09, device='cuda:0')
c= tensor(4.5439e+09, device='cuda:0')
c= tensor(4.5592e+09, device='cuda:0')
c= tensor(4.5597e+09, device='cuda:0')
c= tensor(4.5601e+09, device='cuda:0')
c= tensor(4.5750e+09, device='cuda:0')
c= tensor(4.5805e+09, device='cuda:0')
c= tensor(4.5815e+09, device='cuda:0')
c= tensor(4.5818e+09, device='cuda:0')
c= tensor(4.5824e+09, device='cuda:0')
c= tensor(4.5825e+09, device='cuda:0')
c= tensor(4.5847e+09, device='cuda:0')
c= tensor(4.6612e+09, device='cuda:0')
c= tensor(4.7161e+09, device='cuda:0')
c= tensor(4.7226e+09, device='cuda:0')
c= tensor(4.7227e+09, device='cuda:0')
c= tensor(4.7236e+09, device='cuda:0')
c= tensor(4.7237e+09, device='cuda:0')
c= tensor(4.7242e+09, device='cuda:0')
c= tensor(4.7252e+09, device='cuda:0')
c= tensor(4.7472e+09, device='cuda:0')
c= tensor(4.7472e+09, device='cuda:0')
c= tensor(4.7843e+09, device='cuda:0')
c= tensor(4.7874e+09, device='cuda:0')
c= tensor(4.7880e+09, device='cuda:0')
c= tensor(4.7880e+09, device='cuda:0')
c= tensor(4.7894e+09, device='cuda:0')
c= tensor(4.7942e+09, device='cuda:0')
c= tensor(4.7942e+09, device='cuda:0')
c= tensor(4.9173e+09, device='cuda:0')
c= tensor(4.9196e+09, device='cuda:0')
c= tensor(4.9245e+09, device='cuda:0')
c= tensor(4.9252e+09, device='cuda:0')
c= tensor(4.9252e+09, device='cuda:0')
c= tensor(4.9252e+09, device='cuda:0')
c= tensor(4.9253e+09, device='cuda:0')
c= tensor(4.9255e+09, device='cuda:0')
c= tensor(4.9396e+09, device='cuda:0')
c= tensor(6.8091e+09, device='cuda:0')
c= tensor(6.8091e+09, device='cuda:0')
c= tensor(6.8120e+09, device='cuda:0')
c= tensor(6.8121e+09, device='cuda:0')
c= tensor(6.8122e+09, device='cuda:0')
c= tensor(6.8124e+09, device='cuda:0')
c= tensor(6.8399e+09, device='cuda:0')
c= tensor(6.8401e+09, device='cuda:0')
c= tensor(7.0856e+09, device='cuda:0')
c= tensor(7.0856e+09, device='cuda:0')
c= tensor(7.0924e+09, device='cuda:0')
c= tensor(7.0941e+09, device='cuda:0')
c= tensor(7.0967e+09, device='cuda:0')
c= tensor(7.1132e+09, device='cuda:0')
c= tensor(7.1134e+09, device='cuda:0')
c= tensor(7.1134e+09, device='cuda:0')
c= tensor(7.1148e+09, device='cuda:0')
c= tensor(7.1148e+09, device='cuda:0')
c= tensor(7.1151e+09, device='cuda:0')
c= tensor(7.1192e+09, device='cuda:0')
c= tensor(7.1302e+09, device='cuda:0')
c= tensor(7.1307e+09, device='cuda:0')
c= tensor(7.1314e+09, device='cuda:0')
c= tensor(7.1548e+09, device='cuda:0')
c= tensor(7.2378e+09, device='cuda:0')
c= tensor(7.2382e+09, device='cuda:0')
c= tensor(7.2382e+09, device='cuda:0')
c= tensor(7.2655e+09, device='cuda:0')
c= tensor(7.2660e+09, device='cuda:0')
c= tensor(7.2668e+09, device='cuda:0')
c= tensor(7.2674e+09, device='cuda:0')
c= tensor(7.2703e+09, device='cuda:0')
c= tensor(7.2738e+09, device='cuda:0')
c= tensor(7.3178e+09, device='cuda:0')
c= tensor(7.3183e+09, device='cuda:0')
c= tensor(7.3184e+09, device='cuda:0')
c= tensor(7.3185e+09, device='cuda:0')
c= tensor(7.3188e+09, device='cuda:0')
c= tensor(7.3254e+09, device='cuda:0')
c= tensor(7.3285e+09, device='cuda:0')
c= tensor(7.3383e+09, device='cuda:0')
c= tensor(7.3383e+09, device='cuda:0')
c= tensor(7.3387e+09, device='cuda:0')
c= tensor(7.3404e+09, device='cuda:0')
c= tensor(7.3506e+09, device='cuda:0')
c= tensor(7.3512e+09, device='cuda:0')
c= tensor(7.3530e+09, device='cuda:0')
c= tensor(7.6109e+09, device='cuda:0')
c= tensor(7.6115e+09, device='cuda:0')
c= tensor(7.6115e+09, device='cuda:0')
c= tensor(7.6116e+09, device='cuda:0')
c= tensor(7.6199e+09, device='cuda:0')
c= tensor(7.6208e+09, device='cuda:0')
c= tensor(7.6218e+09, device='cuda:0')
c= tensor(7.6218e+09, device='cuda:0')
c= tensor(7.6218e+09, device='cuda:0')
c= tensor(7.6349e+09, device='cuda:0')
c= tensor(7.6368e+09, device='cuda:0')
c= tensor(7.6373e+09, device='cuda:0')
c= tensor(7.6373e+09, device='cuda:0')
c= tensor(7.6373e+09, device='cuda:0')
c= tensor(7.6376e+09, device='cuda:0')
c= tensor(7.6383e+09, device='cuda:0')
c= tensor(7.6414e+09, device='cuda:0')
c= tensor(7.6415e+09, device='cuda:0')
c= tensor(7.6561e+09, device='cuda:0')
c= tensor(7.6562e+09, device='cuda:0')
c= tensor(7.6567e+09, device='cuda:0')
c= tensor(7.6895e+09, device='cuda:0')
c= tensor(7.6897e+09, device='cuda:0')
c= tensor(7.6906e+09, device='cuda:0')
c= tensor(7.6957e+09, device='cuda:0')
c= tensor(7.6959e+09, device='cuda:0')
c= tensor(7.6969e+09, device='cuda:0')
c= tensor(7.7021e+09, device='cuda:0')
c= tensor(7.7021e+09, device='cuda:0')
c= tensor(7.7027e+09, device='cuda:0')
c= tensor(7.7029e+09, device='cuda:0')
c= tensor(7.7067e+09, device='cuda:0')
c= tensor(7.7108e+09, device='cuda:0')
c= tensor(7.7116e+09, device='cuda:0')
c= tensor(7.7142e+09, device='cuda:0')
c= tensor(7.7143e+09, device='cuda:0')
c= tensor(7.7152e+09, device='cuda:0')
c= tensor(7.7152e+09, device='cuda:0')
c= tensor(7.7153e+09, device='cuda:0')
c= tensor(7.7198e+09, device='cuda:0')
c= tensor(7.7446e+09, device='cuda:0')
c= tensor(7.7446e+09, device='cuda:0')
c= tensor(7.7449e+09, device='cuda:0')
c= tensor(7.7450e+09, device='cuda:0')
c= tensor(7.7784e+09, device='cuda:0')
c= tensor(7.7788e+09, device='cuda:0')
c= tensor(7.7789e+09, device='cuda:0')
c= tensor(7.7839e+09, device='cuda:0')
c= tensor(7.7867e+09, device='cuda:0')
c= tensor(7.7868e+09, device='cuda:0')
c= tensor(7.7878e+09, device='cuda:0')
c= tensor(7.7878e+09, device='cuda:0')
time to make c is 10.180093050003052
time for making loss is 10.180124044418335
p0 True
it  0 : 2226099712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
5116141568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 17% |
memory (bytes)
5116456960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1901168100.0
relative error loss 0.24412036
shape of L is 
torch.Size([])
memory (bytes)
5142986752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 17% |
memory (bytes)
5143212032
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  1874093000.0
relative error loss 0.24064377
shape of L is 
torch.Size([])
memory (bytes)
5146607616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
5146656768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1843253200.0
relative error loss 0.23668377
shape of L is 
torch.Size([])
memory (bytes)
5149855744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5149855744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1808700400.0
relative error loss 0.232247
shape of L is 
torch.Size([])
memory (bytes)
5152899072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 17% |
memory (bytes)
5152899072
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  1782449700.0
relative error loss 0.22887625
shape of L is 
torch.Size([])
memory (bytes)
5156306944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
5156306944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1764211700.0
relative error loss 0.2265344
shape of L is 
torch.Size([])
memory (bytes)
5159473152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 17% |
memory (bytes)
5159473152
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  1752947700.0
relative error loss 0.22508805
shape of L is 
torch.Size([])
memory (bytes)
5162627072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5162627072
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  1740567000.0
relative error loss 0.2234983
shape of L is 
torch.Size([])
memory (bytes)
5165559808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 17% |
memory (bytes)
5165559808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1730511400.0
relative error loss 0.2222071
shape of L is 
torch.Size([])
memory (bytes)
5169012736
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 17% |
memory (bytes)
5169012736
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  1719718900.0
relative error loss 0.22082129
time to take a step is 265.04982256889343
it  1 : 2715205632
| ID | GPU | MEM |
------------------
|  0 | 26% |  0% |
|  1 | 15% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
5172396032
| ID | GPU | MEM |
------------------
|  0 | 26% |  0% |
|  1 | 13% | 17% |
memory (bytes)
5172396032
| ID | GPU | MEM |
------------------
|  0 | 24% |  0% |
|  1 | 99% | 17% |
error is  1719718900.0
relative error loss 0.22082129
shape of L is 
torch.Size([])
memory (bytes)
5175508992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 17% |
memory (bytes)
5175595008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1708671000.0
relative error loss 0.21940267
shape of L is 
torch.Size([])
memory (bytes)
5178736640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 17% |
memory (bytes)
5178736640
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  1704506400.0
relative error loss 0.21886791
shape of L is 
torch.Size([])
memory (bytes)
5182033920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 17% |
memory (bytes)
5182033920
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  1698679800.0
relative error loss 0.21811976
shape of L is 
torch.Size([])
memory (bytes)
5185040384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
5185040384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1693447700.0
relative error loss 0.21744792
shape of L is 
torch.Size([])
memory (bytes)
5188210688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
5188456448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1688222200.0
relative error loss 0.21677694
shape of L is 
torch.Size([])
memory (bytes)
5191294976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
5191557120
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  1684593700.0
relative error loss 0.21631101
shape of L is 
torch.Size([])
memory (bytes)
5194895360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 17% |
memory (bytes)
5194895360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1682007000.0
relative error loss 0.21597888
shape of L is 
torch.Size([])
memory (bytes)
5197991936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 17% |
memory (bytes)
5197991936
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  1677198800.0
relative error loss 0.21536148
shape of L is 
torch.Size([])
memory (bytes)
5201154048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 17% |
memory (bytes)
5201154048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1673209900.0
relative error loss 0.21484926
time to take a step is 262.22262358665466
it  2 : 2715205632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
5204525056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 17% |
memory (bytes)
5204525056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1673209900.0
relative error loss 0.21484926
shape of L is 
torch.Size([])
memory (bytes)
5207654400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 17% |
memory (bytes)
5207654400
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  1670514700.0
relative error loss 0.2145032
shape of L is 
torch.Size([])
memory (bytes)
5210812416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 17% |
memory (bytes)
5210951680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1667443200.0
relative error loss 0.2141088
shape of L is 
torch.Size([])
memory (bytes)
5214158848
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 17% |
memory (bytes)
5214158848
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  1665399800.0
relative error loss 0.21384642
shape of L is 
torch.Size([])
memory (bytes)
5217267712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5217267712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 17% |
error is  1662665700.0
relative error loss 0.21349534
shape of L is 
torch.Size([])
memory (bytes)
5220593664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 17% |
memory (bytes)
5220593664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1660898300.0
relative error loss 0.2132684
shape of L is 
torch.Size([])
memory (bytes)
5223702528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 17% |
memory (bytes)
5223809024
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  1659333100.0
relative error loss 0.21306741
shape of L is 
torch.Size([])
memory (bytes)
5226983424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 17% |
memory (bytes)
5227020288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1658468400.0
relative error loss 0.21295638
shape of L is 
torch.Size([])
memory (bytes)
5230235648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
5230235648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1656065000.0
relative error loss 0.21264778
shape of L is 
torch.Size([])
memory (bytes)
5233299456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 17% |
memory (bytes)
5233385472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1655309300.0
relative error loss 0.21255074
time to take a step is 263.9584047794342
it  3 : 2715205632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
5236486144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5236486144
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  1655309300.0
relative error loss 0.21255074
shape of L is 
torch.Size([])
memory (bytes)
5239885824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5239885824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1654099000.0
relative error loss 0.21239533
shape of L is 
torch.Size([])
memory (bytes)
5242933248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
5243101184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1652290000.0
relative error loss 0.21216305
shape of L is 
torch.Size([])
memory (bytes)
5245984768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 17% |
memory (bytes)
5245984768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1651452900.0
relative error loss 0.21205556
shape of L is 
torch.Size([])
memory (bytes)
5249441792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5249519616
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  1649876000.0
relative error loss 0.21185307
shape of L is 
torch.Size([])
memory (bytes)
5252739072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 17% |
memory (bytes)
5252739072
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  1649164800.0
relative error loss 0.21176176
shape of L is 
torch.Size([])
memory (bytes)
5255794688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 17% |
memory (bytes)
5255794688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1647682000.0
relative error loss 0.21157137
shape of L is 
torch.Size([])
memory (bytes)
5258989568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 17% |
memory (bytes)
5258989568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1645837800.0
relative error loss 0.21133456
shape of L is 
torch.Size([])
memory (bytes)
5262368768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 17% |
memory (bytes)
5262368768
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  1644397600.0
relative error loss 0.21114962
shape of L is 
torch.Size([])
memory (bytes)
5265412096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 17% |
memory (bytes)
5265412096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1643285500.0
relative error loss 0.21100682
time to take a step is 263.0202214717865
it  4 : 2715205632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
5268754432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
5268754432
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  1643285500.0
relative error loss 0.21100682
shape of L is 
torch.Size([])
memory (bytes)
5271830528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 17% |
memory (bytes)
5271830528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1642157000.0
relative error loss 0.21086192
shape of L is 
torch.Size([])
memory (bytes)
5275230208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
5275230208
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  1640541700.0
relative error loss 0.2106545
shape of L is 
torch.Size([])
memory (bytes)
5278318592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5278318592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1640093200.0
relative error loss 0.2105969
shape of L is 
torch.Size([])
memory (bytes)
5281452032
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 17% |
memory (bytes)
5281660928
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  1638895600.0
relative error loss 0.21044314
shape of L is 
torch.Size([])
memory (bytes)
5284773888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 17% |
memory (bytes)
5284773888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1638230500.0
relative error loss 0.21035773
shape of L is 
torch.Size([])
memory (bytes)
5288083456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 17% |
memory (bytes)
5288083456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1637491200.0
relative error loss 0.2102628
shape of L is 
torch.Size([])
memory (bytes)
5291233280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 17% |
memory (bytes)
5291298816
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  1636166700.0
relative error loss 0.21009272
shape of L is 
torch.Size([])
memory (bytes)
5294288896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5294497792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1636017200.0
relative error loss 0.21007352
shape of L is 
torch.Size([])
memory (bytes)
5297717248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 17% |
memory (bytes)
5297717248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1634466800.0
relative error loss 0.20987445
time to take a step is 261.92658710479736
it  5 : 2715205632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
5300920320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 17% |
memory (bytes)
5300920320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1634466800.0
relative error loss 0.20987445
shape of L is 
torch.Size([])
memory (bytes)
5304057856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5304057856
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  1633941000.0
relative error loss 0.20980693
shape of L is 
torch.Size([])
memory (bytes)
5307355136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 17% |
memory (bytes)
5307355136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1632977400.0
relative error loss 0.20968321
shape of L is 
torch.Size([])
memory (bytes)
5310558208
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 17% |
memory (bytes)
5310558208
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  1632720900.0
relative error loss 0.20965026
shape of L is 
torch.Size([])
memory (bytes)
5313581056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5313581056
| ID | GPU | MEM |
------------------
|  0 |  1% |  0% |
|  1 | 99% | 17% |
error is  1631868400.0
relative error loss 0.2095408
shape of L is 
torch.Size([])
memory (bytes)
5316952064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5316984832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1631375400.0
relative error loss 0.20947748
shape of L is 
torch.Size([])
memory (bytes)
5320183808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 17% |
memory (bytes)
5320183808
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  1630915100.0
relative error loss 0.20941839
shape of L is 
torch.Size([])
memory (bytes)
5323272192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 17% |
memory (bytes)
5323350016
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1630287400.0
relative error loss 0.20933779
shape of L is 
torch.Size([])
memory (bytes)
5326397440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 17% |
memory (bytes)
5326397440
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  1630056000.0
relative error loss 0.20930807
shape of L is 
torch.Size([])
memory (bytes)
5329825792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 17% |
memory (bytes)
5329825792
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  1629171700.0
relative error loss 0.20919453
time to take a step is 265.7242293357849
it  6 : 2715205632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
5333028864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 17% |
memory (bytes)
5333045248
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1629171700.0
relative error loss 0.20919453
shape of L is 
torch.Size([])
memory (bytes)
5336113152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 17% |
memory (bytes)
5336113152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1628892700.0
relative error loss 0.2091587
shape of L is 
torch.Size([])
memory (bytes)
5339475968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
5339475968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1628264000.0
relative error loss 0.20907797
shape of L is 
torch.Size([])
memory (bytes)
5342687232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 17% |
memory (bytes)
5342687232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1627492900.0
relative error loss 0.20897895
shape of L is 
torch.Size([])
memory (bytes)
5345849344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5345849344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1627030500.0
relative error loss 0.20891958
shape of L is 
torch.Size([])
memory (bytes)
5349113856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5349113856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1626369000.0
relative error loss 0.20883465
shape of L is 
torch.Size([])
memory (bytes)
5352288256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 17% |
memory (bytes)
5352288256
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  1625941000.0
relative error loss 0.2087797
shape of L is 
torch.Size([])
memory (bytes)
5355560960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  0% | 17% |
memory (bytes)
5355560960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1625303600.0
relative error loss 0.20869784
shape of L is 
torch.Size([])
memory (bytes)
5358768128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 17% |
memory (bytes)
5358768128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1625762300.0
relative error loss 0.20875674
shape of L is 
torch.Size([])
memory (bytes)
5361983488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5361983488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1624980000.0
relative error loss 0.20865628
time to take a step is 265.8750116825104
it  7 : 2715206144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
5365190656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5365190656
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  1624980000.0
relative error loss 0.20865628
shape of L is 
torch.Size([])
memory (bytes)
5368254464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5368401920
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  1624479700.0
relative error loss 0.20859206
shape of L is 
torch.Size([])
memory (bytes)
5371625472
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5371625472
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  1623957500.0
relative error loss 0.208525
shape of L is 
torch.Size([])
memory (bytes)
5374840832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 17% |
memory (bytes)
5374840832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1623567900.0
relative error loss 0.20847496
shape of L is 
torch.Size([])
memory (bytes)
5378039808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 17% |
memory (bytes)
5378039808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1623171100.0
relative error loss 0.20842402
shape of L is 
torch.Size([])
memory (bytes)
5381255168
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 17% |
memory (bytes)
5381255168
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  1622881300.0
relative error loss 0.20838681
shape of L is 
torch.Size([])
memory (bytes)
5384474624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
5384474624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1622577200.0
relative error loss 0.20834775
shape of L is 
torch.Size([])
memory (bytes)
5387685888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 17% |
memory (bytes)
5387685888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1622897200.0
relative error loss 0.20838885
shape of L is 
torch.Size([])
memory (bytes)
5390745600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 17% |
memory (bytes)
5390745600
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  1622336000.0
relative error loss 0.20831679
shape of L is 
torch.Size([])
memory (bytes)
5394124800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5394124800
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  1621941800.0
relative error loss 0.20826617
time to take a step is 262.46563720703125
it  8 : 2715205632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
5397331968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
5397331968
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  1621941800.0
relative error loss 0.20826617
shape of L is 
torch.Size([])
memory (bytes)
5400539136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5400539136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1621553200.0
relative error loss 0.20821626
shape of L is 
torch.Size([])
memory (bytes)
5403754496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 17% |
memory (bytes)
5403754496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1621193700.0
relative error loss 0.20817012
shape of L is 
torch.Size([])
memory (bytes)
5406978048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 17% |
memory (bytes)
5406978048
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1620897300.0
relative error loss 0.20813204
shape of L is 
torch.Size([])
memory (bytes)
5410189312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 17% |
memory (bytes)
5410189312
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  1620600800.0
relative error loss 0.20809399
shape of L is 
torch.Size([])
memory (bytes)
5413285888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 17% |
memory (bytes)
5413285888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1620331500.0
relative error loss 0.2080594
shape of L is 
torch.Size([])
memory (bytes)
5416611840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 17% |
memory (bytes)
5416611840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1620137500.0
relative error loss 0.20803449
shape of L is 
torch.Size([])
memory (bytes)
5419819008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5419819008
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  1619899900.0
relative error loss 0.20800398
shape of L is 
torch.Size([])
memory (bytes)
5422866432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5423038464
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1619581400.0
relative error loss 0.2079631
shape of L is 
torch.Size([])
memory (bytes)
5426245632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
5426245632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1619390000.0
relative error loss 0.20793849
time to take a step is 262.6264817714691
it  9 : 2715205632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
5429407744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
5429407744
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  1619390000.0
relative error loss 0.20793849
shape of L is 
torch.Size([])
memory (bytes)
5432668160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 17% |
memory (bytes)
5432668160
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  1619167200.0
relative error loss 0.2079099
shape of L is 
torch.Size([])
memory (bytes)
5435809792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 17% |
memory (bytes)
5435879424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1618911200.0
relative error loss 0.20787703
shape of L is 
torch.Size([])
memory (bytes)
5439086592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
5439086592
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  1618769900.0
relative error loss 0.20785889
shape of L is 
torch.Size([])
memory (bytes)
5442297856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5442297856
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  1618588200.0
relative error loss 0.20783554
shape of L is 
torch.Size([])
memory (bytes)
5445328896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5445496832
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  1618355200.0
relative error loss 0.20780563
shape of L is 
torch.Size([])
memory (bytes)
5448548352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 17% |
memory (bytes)
5448548352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1618161700.0
relative error loss 0.20778078
shape of L is 
torch.Size([])
memory (bytes)
5451931648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
5451931648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1617938400.0
relative error loss 0.20775212
shape of L is 
torch.Size([])
memory (bytes)
5454991360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
5454991360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1617568800.0
relative error loss 0.20770465
shape of L is 
torch.Size([])
memory (bytes)
5458358272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
5458358272
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1617381400.0
relative error loss 0.20768058
time to take a step is 264.83985328674316
it  10 : 2715205632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
5461471232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
5461471232
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  1617381400.0
relative error loss 0.20768058
shape of L is 
torch.Size([])
memory (bytes)
5464772608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5464772608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1617188400.0
relative error loss 0.2076558
shape of L is 
torch.Size([])
memory (bytes)
5467897856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 17% |
memory (bytes)
5467897856
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  1616680000.0
relative error loss 0.20759052
shape of L is 
torch.Size([])
memory (bytes)
5471199232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 17% |
memory (bytes)
5471199232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1616339000.0
relative error loss 0.20754673
shape of L is 
torch.Size([])
memory (bytes)
5474414592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 17% |
memory (bytes)
5474414592
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  1616094700.0
relative error loss 0.20751537
shape of L is 
torch.Size([])
memory (bytes)
5477625856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5477625856
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  1615928300.0
relative error loss 0.207494
shape of L is 
torch.Size([])
memory (bytes)
5480828928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5480828928
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  1615530000.0
relative error loss 0.20744286
shape of L is 
torch.Size([])
memory (bytes)
5483991040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 17% |
memory (bytes)
5483991040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1615323100.0
relative error loss 0.2074163
shape of L is 
torch.Size([])
memory (bytes)
5487259648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 17% |
memory (bytes)
5487259648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1615161900.0
relative error loss 0.20739558
shape of L is 
torch.Size([])
memory (bytes)
5490470912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 17% |
memory (bytes)
5490470912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1614941200.0
relative error loss 0.20736726
time to take a step is 264.05691623687744
it  11 : 2715205632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
5493579776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
5493690368
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  1614941200.0
relative error loss 0.20736726
shape of L is 
torch.Size([])
memory (bytes)
5496893440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 17% |
memory (bytes)
5496893440
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  1614815200.0
relative error loss 0.20735107
shape of L is 
torch.Size([])
memory (bytes)
5499940864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 17% |
memory (bytes)
5499940864
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1614438900.0
relative error loss 0.20730276
shape of L is 
torch.Size([])
memory (bytes)
5503287296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
5503287296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1614032400.0
relative error loss 0.20725057
shape of L is 
torch.Size([])
memory (bytes)
5506363392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5506363392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1613729800.0
relative error loss 0.2072117
shape of L is 
torch.Size([])
memory (bytes)
5509734400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5509734400
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  1613469700.0
relative error loss 0.20717831
shape of L is 
torch.Size([])
memory (bytes)
5512785920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 17% |
memory (bytes)
5512949760
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  1613232600.0
relative error loss 0.20714787
shape of L is 
torch.Size([])
memory (bytes)
5516062720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 17% |
memory (bytes)
5516161024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1613032000.0
relative error loss 0.2071221
shape of L is 
torch.Size([])
memory (bytes)
5519368192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
5519368192
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1612871200.0
relative error loss 0.20710145
shape of L is 
torch.Size([])
memory (bytes)
5522550784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
5522550784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1612585000.0
relative error loss 0.2070647
time to take a step is 264.3150911331177
it  12 : 2715205632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
5525798912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 17% |
memory (bytes)
5525798912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1612585000.0
relative error loss 0.2070647
shape of L is 
torch.Size([])
memory (bytes)
5529006080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 17% |
memory (bytes)
5529006080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1612501500.0
relative error loss 0.20705399
shape of L is 
torch.Size([])
memory (bytes)
5532176384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5532176384
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1612255700.0
relative error loss 0.20702243
shape of L is 
torch.Size([])
memory (bytes)
5535436800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5535436800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 17% |
error is  1612131800.0
relative error loss 0.20700651
shape of L is 
torch.Size([])
memory (bytes)
5538652160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 17% |
memory (bytes)
5538652160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1611936800.0
relative error loss 0.20698147
shape of L is 
torch.Size([])
memory (bytes)
5541871616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5541871616
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  1611708900.0
relative error loss 0.20695221
shape of L is 
torch.Size([])
memory (bytes)
5545078784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5545078784
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  1611483600.0
relative error loss 0.20692329
shape of L is 
torch.Size([])
memory (bytes)
5548294144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
5548294144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1611348500.0
relative error loss 0.20690593
shape of L is 
torch.Size([])
memory (bytes)
5551407104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 17% |
memory (bytes)
5551407104
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1611222500.0
relative error loss 0.20688976
shape of L is 
torch.Size([])
memory (bytes)
5554720768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 17% |
memory (bytes)
5554720768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1610866700.0
relative error loss 0.20684406
time to take a step is 264.773964881897
it  13 : 2715205632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
5557960704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 17% |
memory (bytes)
5557960704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1610866700.0
relative error loss 0.20684406
shape of L is 
torch.Size([])
memory (bytes)
5561008128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 17% |
memory (bytes)
5561008128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1611198000.0
relative error loss 0.2068866
shape of L is 
torch.Size([])
memory (bytes)
5564379136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 17% |
memory (bytes)
5564379136
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  1610715600.0
relative error loss 0.20682468
shape of L is 
torch.Size([])
memory (bytes)
5567553536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
5567553536
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1610530800.0
relative error loss 0.20680094
shape of L is 
torch.Size([])
memory (bytes)
5570797568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5570797568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1610285600.0
relative error loss 0.20676945
shape of L is 
torch.Size([])
memory (bytes)
5573992448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 17% |
memory (bytes)
5573992448
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  1610136000.0
relative error loss 0.20675024
shape of L is 
torch.Size([])
memory (bytes)
5577093120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 17% |
memory (bytes)
5577093120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1609955300.0
relative error loss 0.20672704
shape of L is 
torch.Size([])
memory (bytes)
5580419072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 17% |
memory (bytes)
5580419072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1609832000.0
relative error loss 0.2067112
shape of L is 
torch.Size([])
memory (bytes)
5583609856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 17% |
memory (bytes)
5583609856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1609746400.0
relative error loss 0.20670022
shape of L is 
torch.Size([])
memory (bytes)
5586702336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5586702336
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1609541600.0
relative error loss 0.20667392
time to take a step is 264.287401676178
it  14 : 2715205632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
5589880832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 17% |
memory (bytes)
5589880832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1609541600.0
relative error loss 0.20667392
shape of L is 
torch.Size([])
memory (bytes)
5593239552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 17% |
memory (bytes)
5593239552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1609419800.0
relative error loss 0.20665827
shape of L is 
torch.Size([])
memory (bytes)
5596454912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 17% |
memory (bytes)
5596454912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1609359400.0
relative error loss 0.20665051
shape of L is 
torch.Size([])
memory (bytes)
5599522816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5599522816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1609282000.0
relative error loss 0.20664059
shape of L is 
torch.Size([])
memory (bytes)
5602873344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 17% |
memory (bytes)
5602873344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1609114600.0
relative error loss 0.20661908
shape of L is 
torch.Size([])
memory (bytes)
5606080512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 17% |
memory (bytes)
5606080512
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  1609041400.0
relative error loss 0.2066097
shape of L is 
torch.Size([])
memory (bytes)
5609295872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 17% |
memory (bytes)
5609295872
| ID | GPU | MEM |
------------------
|  0 |  1% |  0% |
|  1 | 99% | 17% |
error is  1608770000.0
relative error loss 0.20657484
shape of L is 
torch.Size([])
memory (bytes)
5612511232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 17% |
memory (bytes)
5612511232
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1608685600.0
relative error loss 0.206564
shape of L is 
torch.Size([])
memory (bytes)
5615730688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 17% |
memory (bytes)
5615730688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  1608550400.0
relative error loss 0.20654663
shape of L is 
torch.Size([])
memory (bytes)
5618892800
| ID | GPU | MEM |
------------------
|  0 | 22% |  0% |
|  1 | 12% | 17% |
memory (bytes)
5618892800
| ID | GPU  | MEM |
-------------------
|  0 |  22% |  0% |
|  1 | 100% | 17% |
error is  1608376800.0
relative error loss 0.20652436
time to take a step is 263.08332228660583
sum tnnu_Z after tensor(11313772., device='cuda:0')
shape of features
(4818,)
shape of features
(4818,)
number of orig particles 19271
number of new particles after remove low mass 17375
tnuZ shape should be parts x labs
torch.Size([19271, 702])
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
error without small mass is  1901052300.0
relative error without small mass is  0.24410547
nnu_Z shape should be number of particles by maxV
(19271, 702)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
shape of features
(19271,)
Thu Feb 2 11:52:01 EST 2023
